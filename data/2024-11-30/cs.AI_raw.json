[
  {
    "arxiv_id": "2412.00608v3",
    "title": "Leveraging LLM for Automated Ontology Extraction and Knowledge Graph Generation",
    "authors": [
      "Mohammad Sadeq Abolhasani",
      "Rong Pan"
    ],
    "abstract": "Extracting relevant and structured knowledge from large, complex technical\ndocuments within the Reliability and Maintainability (RAM) domain is\nlabor-intensive and prone to errors. Our work addresses this challenge by\npresenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge\nGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through\nan interactive user interface guided by our adaptive iterative Chain of Thought\n(CoT) algorithm to ensure that the ontology extraction process and, thus, KG\ngeneration align with user-specific requirements. Although KG generation\nfollows a clear, structured path based on the confirmed ontology, there is no\nuniversally correct ontology as it is inherently based on the user's\npreferences. OntoKGen recommends an ontology grounded in best practices,\nminimizing user effort and providing valuable insights that may have been\noverlooked, all while giving the user complete control over the final ontology.\nHaving generated the KG based on the confirmed ontology, OntoKGen enables\nseamless integration into schemeless, non-relational databases like Neo4j. This\nintegration allows for flexible storage and retrieval of knowledge from\ndiverse, unstructured sources, facilitating advanced querying, analysis, and\ndecision-making. Moreover, the generated KG serves as a robust foundation for\nfuture integration into Retrieval Augmented Generation (RAG) systems, offering\nenhanced capabilities for developing domain-specific intelligent applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00608v3",
    "published_date": "2024-11-30 23:11:44 UTC",
    "updated_date": "2024-12-10 04:28:36 UTC"
  },
  {
    "arxiv_id": "2412.00606v1",
    "title": "Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions",
    "authors": [
      "Resmi Ramachandranpillai",
      "Kishore Sampath",
      "Ayaazuddin Mohammad",
      "Malihe Alikhani"
    ],
    "abstract": "Biases in automated clinical decision-making using Electronic Healthcare\nRecords (EHR) impose significant disparities in patient care and treatment\noutcomes. Conventional approaches have primarily focused on bias mitigation\nstrategies stemming from single attributes, overlooking intersectional\nsubgroups -- groups formed across various demographic intersections (such as\nrace, gender, ethnicity, etc.). Rendering single-attribute mitigation\nstrategies to intersectional subgroups becomes statistically irrelevant due to\nthe varying distribution and bias patterns across these subgroups. The\nmultimodal nature of EHR -- data from various sources such as combinations of\ntext, time series, tabular, events, and images -- adds another layer of\ncomplexity as the influence on minority groups may fluctuate across modalities.\nIn this paper, we take the initial steps to uncover potential intersectional\nbiases in predictions by sourcing extensive multimodal datasets, MIMIC-Eye1 and\nMIMIC-IV ED, and propose mitigation at the intersectional subgroup level. We\nperform and benchmark downstream tasks and bias evaluation on the datasets by\nlearning a unified text representation from multimodal sources, harnessing the\nenormous capabilities of the pre-trained clinical Language Models (LM),\nMedBERT, Clinical BERT, and Clinical BioBERT. Our findings indicate that the\nproposed sub-group-specific bias mitigation is robust across different\ndatasets, subgroups, and embeddings, demonstrating effectiveness in addressing\nintersectional biases in multimodal settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00606v1",
    "published_date": "2024-11-30 22:53:11 UTC",
    "updated_date": "2024-11-30 22:53:11 UTC"
  },
  {
    "arxiv_id": "2412.00596v2",
    "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation",
    "authors": [
      "Qiyao Xue",
      "Xiangyu Yin",
      "Boyuan Yang",
      "Wei Gao"
    ],
    "abstract": "Text-to-video (T2V) generation has been recently enabled by transformer-based\ndiffusion models, but current T2V models lack capabilities in adhering to the\nreal-world common knowledge and physical rules, due to their limited\nunderstanding of physical realism and deficiency in temporal modeling. Existing\nsolutions are either data-driven or require extra model inputs, but cannot be\ngeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,\na new data-independent T2V technique that expands the current T2V model's\ncapability of video generation to out-of-distribution domains, by enabling\nchain-of-thought and step-back reasoning in T2V prompting. Our experiments show\nthat PhyT2V improves existing T2V models' adherence to real-world physical\nrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\nThe source codes are available at: https://github.com/pittisl/PhyT2V.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00596v2",
    "published_date": "2024-11-30 22:02:12 UTC",
    "updated_date": "2025-04-01 09:33:55 UTC"
  },
  {
    "arxiv_id": "2412.00592v2",
    "title": "LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes",
    "authors": [
      "Shing-Hei Ho",
      "Bao Thach",
      "Minghan Zhu"
    ],
    "abstract": "We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data\nfor autonomous driving. Our framework edits real-world LiDAR scans by\nintroducing new object layouts while preserving the realism of the background\nenvironment. Compared to end-to-end frameworks that generate LiDAR point clouds\nfrom scratch, LiDAR-EDIT offers users full control over the object layout,\nincluding the number, type, and pose of objects, while keeping most of the\noriginal real-world background. Our method also provides object labels for the\ngenerated data. Compared to novel view synthesis techniques, our framework\nallows for the creation of counterfactual scenarios with object layouts\nsignificantly different from the original real-world scene. LiDAR-EDIT uses\nspherical voxelization to enforce correct LiDAR projective geometry in the\ngenerated point clouds by construction. During object removal and insertion,\ngenerative models are employed to fill the unseen background and object parts\nthat were occluded in the original real LiDAR scans. Experimental results\ndemonstrate that our framework produces realistic LiDAR scans with practical\nvalue for downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA). 6 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00592v2",
    "published_date": "2024-11-30 21:39:51 UTC",
    "updated_date": "2025-05-04 20:49:56 UTC"
  },
  {
    "arxiv_id": "2412.00591v1",
    "title": "Audio Atlas: Visualizing and Exploring Audio Datasets",
    "authors": [
      "Luca A. Lanzendörfer",
      "Florian Grötschla",
      "Uzeyir Valizada",
      "Roger Wattenhofer"
    ],
    "abstract": "We introduce Audio Atlas, an interactive web application for visualizing\naudio data using text-audio embeddings. Audio Atlas is designed to facilitate\nthe exploration and analysis of audio datasets using a contrastive embedding\nmodel and a vector database for efficient data management and semantic search.\nThe system maps audio embeddings into a two-dimensional space and leverages\nDeepScatter for dynamic visualization. Designed for extensibility, Audio Atlas\nallows easy integration of new datasets, enabling users to better understand\ntheir audio data and identify both patterns and outliers. We open-source the\ncodebase of Audio Atlas, and provide an initial implementation containing\nvarious audio and music datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Extended Abstract at ISMIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.00591v1",
    "published_date": "2024-11-30 21:35:20 UTC",
    "updated_date": "2024-11-30 21:35:20 UTC"
  },
  {
    "arxiv_id": "2412.00577v2",
    "title": "Turing Representational Similarity Analysis (RSA): A Flexible Method for Measuring Alignment Between Human and Artificial Intelligence",
    "authors": [
      "Mattson Ogg",
      "Ritwik Bose",
      "Jamie Scharf",
      "Christopher Ratto",
      "Michael Wolmetz"
    ],
    "abstract": "As we consider entrusting Large Language Models (LLMs) with key societal and\ndecision-making roles, measuring their alignment with human cognition becomes\ncritical. This requires methods that can assess how these systems represent\ninformation and facilitate comparisons to human understanding across diverse\ntasks. To meet this need, we developed Turing Representational Similarity\nAnalysis (RSA), a method that uses pairwise similarity ratings to quantify\nalignment between AIs and humans. We tested this approach on semantic alignment\nacross text and image modalities, measuring how different Large Language and\nVision Language Model (LLM and VLM) similarity judgments aligned with human\nresponses at both group and individual levels. GPT-4o showed the strongest\nalignment with human performance among the models we tested, particularly when\nleveraging its text processing capabilities rather than image processing,\nregardless of the input modality. However, no model we studied adequately\ncaptured the inter-individual variability observed among human participants.\nThis method helped uncover certain hyperparameters and prompts that could steer\nmodel behavior to have more or less human-like qualities at an inter-individual\nor group level. Turing RSA enables the efficient and flexible quantification of\nhuman-AI alignment and complements existing accuracy-based benchmark tasks. We\ndemonstrate its utility across multiple modalities (words, sentences, images)\nfor understanding how LLMs encode knowledge and for examining representational\nalignment with human cognition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00577v2",
    "published_date": "2024-11-30 20:24:52 UTC",
    "updated_date": "2025-02-28 19:11:26 UTC"
  },
  {
    "arxiv_id": "2412.00573v2",
    "title": "Opus: A Large Work Model for Complex Workflow Generation",
    "authors": [
      "Théo Fagnoni",
      "Bellinda Mesbah",
      "Mahsun Altin",
      "Phillip Kingston"
    ],
    "abstract": "This paper introduces Opus, a novel framework for generating and optimizing\nWorkflows tailored to complex Business Process Outsourcing (BPO) use cases,\nfocusing on cost reduction and quality enhancement while adhering to\nestablished industry processes and operational constraints. Our approach\ngenerates executable Workflows from Intention, defined as the alignment of\nClient Input, Client Output, and Process Context. These Workflows are\nrepresented as Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting\nof sequences of executable Instructions, including tools and human expert\nreviews. We adopt a two-phase methodology: Workflow Generation and Workflow\nOptimization. In the Generation phase, Workflows are generated using a Large\nWork Model (LWM) informed by a Work Knowledge Graph (WKG) that encodes\ndomain-specific procedural and operational knowledge. In the Optimization\nphase, Workflows are transformed into Workflow Graphs (WFGs), where optimal\nWorkflows are determined through path optimization. Our experiments demonstrate\nthat state-of-the-art Large Language Models (LLMs) face challenges in reliably\nretrieving detailed process data as well as generating industry-compliant\nworkflows. The key contributions of this paper include integrating a Work\nKnowledge Graph (WKG) into a Large Work Model (LWM) to enable the generation of\ncontext-aware, semantically aligned, structured and auditable Workflows. It\nfurther introduces a two-phase approach that combines Workflow Generation from\nIntention with graph-based Workflow Optimization. Finally, we present Opus\nAlpha 1 Large and Opus Alpha 1 Small that outperform state-of-the-art LLMs by\n38% and 29% respectively in Workflow Generation for a Medical Coding use case.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00573v2",
    "published_date": "2024-11-30 20:00:41 UTC",
    "updated_date": "2024-12-06 06:05:59 UTC"
  },
  {
    "arxiv_id": "2412.00560v1",
    "title": "Friend or Foe? Harnessing Controllable Overfitting for Anomaly Detection",
    "authors": [
      "Long Qian",
      "Bingke Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Overfitting has long been stigmatized as detrimental to model performance,\nespecially in the context of anomaly detection. Our work challenges this\nconventional view by introducing a paradigm shift, recasting overfitting as a\ncontrollable and strategic mechanism for enhancing model discrimination\ncapabilities. In this paper, we present Controllable Overfitting-based Anomaly\nDetection (COAD), a novel framework designed to leverage overfitting for\noptimized anomaly detection. We propose the Aberrance Retention Quotient (ARQ),\na novel metric that systematically quantifies the extent of overfitting,\nenabling the identification of an optimal \"golden overfitting interval.\" Within\nthis interval, overfitting is leveraged to significantly amplify the model's\nsensitivity to anomalous patterns, while preserving generalization to normal\nsamples. Additionally, we present the Relative Anomaly Distribution Index\n(RADI), an innovative metric designed to complement AUROC pixel by providing a\nmore versatile and theoretically robust framework for assessing model\nperformance. RADI leverages ARQ to track and evaluate how overfitting impacts\nanomaly detection, offering an integrated approach to understanding the\nrelationship between overfitting dynamics and model efficacy. Our theoretical\nwork also rigorously validates the use of Gaussian noise in pseudo anomaly\nsynthesis, providing the foundation for its broader applicability across\ndiverse domains. Empirical evaluations demonstrate that our controllable\noverfitting method not only achieves State of the Art (SOTA) performance in\nboth one-class and multi-class anomaly detection tasks but also redefines\noverfitting from a modeling challenge into a powerful tool for optimizing\nanomaly detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00560v1",
    "published_date": "2024-11-30 19:07:16 UTC",
    "updated_date": "2024-11-30 19:07:16 UTC"
  },
  {
    "arxiv_id": "2412.00559v1",
    "title": "Polish Medical Exams: A new dataset for cross-lingual medical knowledge transfer assessment",
    "authors": [
      "Łukasz Grzybowski",
      "Jakub Pokrywka",
      "Michał Ciesiółka",
      "Jeremi I. Kaczmarek",
      "Marek Kubis"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00559v1",
    "published_date": "2024-11-30 19:02:34 UTC",
    "updated_date": "2024-11-30 19:02:34 UTC"
  },
  {
    "arxiv_id": "2412.00557v1",
    "title": "Blind Inverse Problem Solving Made Easy by Text-to-Image Latent Diffusion",
    "authors": [
      "Michail Dontas",
      "Yutong He",
      "Naoki Murata",
      "Yuki Mitsufuji",
      "J. Zico Kolter",
      "Ruslan Salakhutdinov"
    ],
    "abstract": "Blind inverse problems, where both the target data and forward operator are\nunknown, are crucial to many computer vision applications. Existing methods\noften depend on restrictive assumptions such as additional training, operator\nlinearity, or narrow image distributions, thus limiting their generalizability.\nIn this work, we present LADiBI, a training-free framework that uses\nlarge-scale text-to-image diffusion models to solve blind inverse problems with\nminimal assumptions. By leveraging natural language prompts, LADiBI jointly\nmodels priors for both the target image and operator, allowing for flexible\nadaptation across a variety of tasks. Additionally, we propose a novel\nposterior sampling approach that combines effective operator initialization\nwith iterative refinement, enabling LADiBI to operate without predefined\noperator forms. Our experiments show that LADiBI is capable of solving a broad\nrange of image restoration tasks, including both linear and nonlinear problems,\non diverse target image distributions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00557v1",
    "published_date": "2024-11-30 18:55:01 UTC",
    "updated_date": "2024-11-30 18:55:01 UTC"
  },
  {
    "arxiv_id": "2412.10390v1",
    "title": "Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective",
    "authors": [
      "Lihui Liu",
      "Zihao Wang",
      "Hanghang Tong"
    ],
    "abstract": "Knowledge graph reasoning is pivotal in various domains such as data mining,\nartificial intelligence, the Web, and social sciences. These knowledge graphs\nfunction as comprehensive repositories of human knowledge, facilitating the\ninference of new information. Traditional symbolic reasoning, despite its\nstrengths, struggles with the challenges posed by incomplete and noisy data\nwithin these graphs. In contrast, the rise of Neural Symbolic AI marks a\nsignificant advancement, merging the robustness of deep learning with the\nprecision of symbolic reasoning. This integration aims to develop AI systems\nthat are not only highly interpretable and explainable but also versatile,\neffectively bridging the gap between symbolic and neural methodologies.\nAdditionally, the advent of large language models (LLMs) has opened new\nfrontiers in knowledge graph reasoning, enabling the extraction and synthesis\nof knowledge in unprecedented ways. This survey offers a thorough review of\nknowledge graph reasoning, focusing on various query types and the\nclassification of neural symbolic reasoning. Furthermore, it explores the\ninnovative integration of knowledge graph reasoning with large language models,\nhighlighting the potential for groundbreaking advancements. This comprehensive\noverview is designed to support researchers and practitioners across multiple\nfields, including data mining, AI, the Web, and social sciences, by providing a\ndetailed understanding of the current landscape and future directions in\nknowledge graph reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10390v1",
    "published_date": "2024-11-30 18:54:08 UTC",
    "updated_date": "2024-11-30 18:54:08 UTC"
  },
  {
    "arxiv_id": "2412.00554v2",
    "title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective",
    "authors": [
      "Yue Zhou",
      "Barbara Di Eugenio",
      "Lu Cheng"
    ],
    "abstract": "This paper studies the performance of large language models (LLMs),\nparticularly regarding demographic fairness, in solving real-world healthcare\ntasks. We evaluate state-of-the-art LLMs with three prevalent learning\nframeworks across six diverse healthcare tasks and find significant challenges\nin applying LLMs to real-world healthcare tasks and persistent fairness issues\nacross demographic groups. We also find that explicitly providing demographic\ninformation yields mixed results, while LLM's ability to infer such details\nraises concerns about biased health predictions. Utilizing LLMs as autonomous\nagents with access to up-to-date guidelines does not guarantee performance\nimprovement. We believe these findings reveal the critical limitations of LLMs\nin healthcare fairness and the urgent need for specialized research in this\narea.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the main conference of COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00554v2",
    "published_date": "2024-11-30 18:52:30 UTC",
    "updated_date": "2024-12-07 19:00:45 UTC"
  },
  {
    "arxiv_id": "2412.00547v3",
    "title": "Motion Dreamer: Boundary Conditional Motion Reasoning for Physically Coherent Video Generation",
    "authors": [
      "Tianshuo Xu",
      "Zhifei Chen",
      "Leyi Wu",
      "Hao Lu",
      "Yuying Chen",
      "Lihui Jiang",
      "Bingbing Liu",
      "Yingcong Chen"
    ],
    "abstract": "Recent advances in video generation have shown promise for generating future\nscenarios, critical for planning and control in autonomous driving and embodied\nintelligence. However, real-world applications demand more than visually\nplausible predictions; they require reasoning about object motions based on\nexplicitly defined boundary conditions, such as initial scene image and partial\nobject motion. We term this capability Boundary Conditional Motion Reasoning.\nCurrent approaches either neglect explicit user-defined motion constraints,\nproducing physically inconsistent motions, or conversely demand complete motion\ninputs, which are rarely available in practice. Here we introduce Motion\nDreamer, a two-stage framework that explicitly separates motion reasoning from\nvisual synthesis, addressing these limitations. Our approach introduces\ninstance flow, a sparse-to-dense motion representation enabling effective\nintegration of partial user-defined motions, and the motion inpainting strategy\nto robustly enable reasoning motions of other objects. Extensive experiments\ndemonstrate that Motion Dreamer significantly outperforms existing methods,\nachieving superior motion plausibility and visual realism, thus bridging the\ngap towards practical boundary conditional motion reasoning. Our webpage is\navailable: https://envision-research.github.io/MotionDreamer/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00547v3",
    "published_date": "2024-11-30 17:40:49 UTC",
    "updated_date": "2025-03-10 06:43:03 UTC"
  },
  {
    "arxiv_id": "2412.00542v1",
    "title": "Rethinking Generalizability and Discriminability of Self-Supervised Learning from Evolutionary Game Theory Perspective",
    "authors": [
      "Jiangmeng Li",
      "Zehua Zang",
      "Qirui Ji",
      "Chuxiong Sun",
      "Wenwen Qiang",
      "Junge Zhang",
      "Changwen Zheng",
      "Fuchun Sun",
      "Hui Xiong"
    ],
    "abstract": "Representations learned by self-supervised approaches are generally\nconsidered to possess sufficient generalizability and discriminability.\nHowever, we disclose a nontrivial mutual-exclusion relationship between these\ncritical representation properties through an exploratory demonstration on\nself-supervised learning. State-of-the-art self-supervised methods tend to\nenhance either generalizability or discriminability but not both\nsimultaneously. Thus, learning representations jointly possessing strong\ngeneralizability and discriminability presents a specific challenge for\nself-supervised learning. To this end, we revisit the learning paradigm of\nself-supervised learning from the perspective of evolutionary game theory (EGT)\nand outline the theoretical roadmap to achieve a desired trade-off between\nthese representation properties. EGT performs well in analyzing the trade-off\npoint in a two-player game by utilizing dynamic system modeling. However, the\nEGT analysis requires sufficient annotated data, which contradicts the\nprinciple of self-supervised learning, i.e., the EGT analysis cannot be\nconducted without the annotations of the specific target domain for\nself-supervised learning. Thus, to enhance the methodological generalization,\nwe propose a novel self-supervised learning method that leverages advancements\nin reinforcement learning to jointly benefit from the general guidance of EGT\nand sequentially optimize the model to chase the consistent improvement of\ngeneralizability and discriminability for specific target domains during\npre-training. Theoretically, we establish that the proposed method tightens the\ngeneralization error upper bound of self-supervised learning. Empirically, our\nmethod achieves state-of-the-art performance on various benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCV, 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.00542v1",
    "published_date": "2024-11-30 17:20:23 UTC",
    "updated_date": "2024-11-30 17:20:23 UTC"
  },
  {
    "arxiv_id": "2412.00539v2",
    "title": "TextClass Benchmark: A Continuous Elo Rating of LLMs in Social Sciences",
    "authors": [
      "Bastián González-Bustamante"
    ],
    "abstract": "The TextClass Benchmark project is an ongoing, continuous benchmarking\nprocess that aims to provide a comprehensive, fair, and dynamic evaluation of\nLLMs and transformers for text classification tasks. This evaluation spans\nvarious domains and languages in social sciences disciplines engaged in NLP and\ntext-as-data approach. The leaderboards present performance metrics and\nrelative ranking using a tailored Elo rating system. With each leaderboard\ncycle, novel models are added, fixed test sets can be replaced for unseen,\nequivalent data to test generalisation power, ratings are updated, and a\nMeta-Elo leaderboard combines and weights domain-specific leaderboards. This\narticle presents the rationale and motivation behind the project, explains the\nElo rating system in detail, and estimates Meta-Elo across different\nclassification tasks in social science disciplines. We also present a snapshot\nof the first cycle of classification tasks on incivility data in Chinese,\nEnglish, German and Russian. This ongoing benchmarking process includes not\nonly additional languages such as Arabic, Hindi, and Spanish but also a\nclassification of policy agenda topics, misinformation, among others.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50 (Primary) 91F10, 91F20 (Secondary)"
    ],
    "primary_category": "cs.CL",
    "comment": "Working paper: 6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00539v2",
    "published_date": "2024-11-30 17:09:49 UTC",
    "updated_date": "2024-12-07 00:00:28 UTC"
  },
  {
    "arxiv_id": "2412.00535v6",
    "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
    "authors": [
      "Bytedance-Seed-Foundation-Code-Team",
      ":",
      "Yao Cheng",
      "Jianfeng Chen",
      "Jie Chen",
      "Li Chen",
      "Liyu Chen",
      "Wentao Chen",
      "Zhengyu Chen",
      "Shijie Geng",
      "Aoyan Li",
      "Bo Li",
      "Bowen Li",
      "Linyi Li",
      "Boyi Liu",
      "Jiaheng Liu",
      "Kaibo Liu",
      "Qi Liu",
      "Shukai Liu",
      "Siyao Liu",
      "Tianyi Liu",
      "Tingkai Liu",
      "Yongfei Liu",
      "Rui Long",
      "Jing Mai",
      "Guanghan Ning",
      "Z. Y. Peng",
      "Kai Shen",
      "Jiahao Su",
      "Jing Su",
      "Tao Sun",
      "Yifan Sun",
      "Yunzhe Tao",
      "Guoyin Wang",
      "Siwei Wang",
      "Xuwu Wang",
      "Yite Wang",
      "Zihan Wang",
      "Jinxiang Xia",
      "Liang Xiang",
      "Xia Xiao",
      "Yongsheng Xiao",
      "Chenguang Xi",
      "Shulin Xin",
      "Jingjing Xu",
      "Shikun Xu",
      "Hongxia Yang",
      "Jack Yang",
      "Yingxiang Yang",
      "Jianbo Yuan",
      "Jun Zhang",
      "Yufeng Zhang",
      "Yuyu Zhang",
      "Shen Zheng",
      "He Zhu",
      "Ming Zhu"
    ],
    "abstract": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00535v6",
    "published_date": "2024-11-30 16:58:42 UTC",
    "updated_date": "2025-05-12 10:24:14 UTC"
  },
  {
    "arxiv_id": "2412.00534v1",
    "title": "Towards Fault Tolerance in Multi-Agent Reinforcement Learning",
    "authors": [
      "Yuchen Shi",
      "Huaxin Pei",
      "Liang Feng",
      "Yi Zhang",
      "Danya Yao"
    ],
    "abstract": "Agent faults pose a significant threat to the performance of multi-agent\nreinforcement learning (MARL) algorithms, introducing two key challenges.\nFirst, agents often struggle to extract critical information from the chaotic\nstate space created by unexpected faults. Second, transitions recorded before\nand after faults in the replay buffer affect training unevenly, leading to a\nsample imbalance problem. To overcome these challenges, this paper enhances the\nfault tolerance of MARL by combining optimized model architecture with a\ntailored training data sampling strategy. Specifically, an attention mechanism\nis incorporated into the actor and critic networks to automatically detect\nfaults and dynamically regulate the attention given to faulty agents.\nAdditionally, a prioritization mechanism is introduced to selectively sample\ntransitions critical to current training needs. To further support research in\nthis area, we design and open-source a highly decoupled code platform for\nfault-tolerant MARL, aimed at improving the efficiency of studying related\nproblems. Experimental results demonstrate the effectiveness of our method in\nhandling various types of faults, faults occurring in any agent, and faults\narising at random times.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00534v1",
    "published_date": "2024-11-30 16:56:29 UTC",
    "updated_date": "2024-11-30 16:56:29 UTC"
  },
  {
    "arxiv_id": "2412.00530v1",
    "title": "Forma mentis networks predict creativity ratings of short texts via interpretable artificial intelligence in human and GPT-simulated raters",
    "authors": [
      "Edith Haim",
      "Natalie Fischer",
      "Salvatore Citraro",
      "Giulio Rossetti",
      "Massimo Stella"
    ],
    "abstract": "Creativity is a fundamental skill of human cognition. We use textual forma\nmentis networks (TFMN) to extract network (semantic/syntactic associations) and\nemotional features from approximately one thousand human- and GPT3.5-generated\nstories. Using Explainable Artificial Intelligence (XAI), we test whether\nfeatures relative to Mednick's associative theory of creativity can explain\ncreativity ratings assigned by humans and GPT-3.5. Using XGBoost, we examine\nthree scenarios: (i) human ratings of human stories, (ii) GPT-3.5 ratings of\nhuman stories, and (iii) GPT-3.5 ratings of GPT-generated stories. Our findings\nreveal that GPT-3.5 ratings differ significantly from human ratings not only in\nterms of correlations but also because of feature patterns identified with XAI\nmethods. GPT-3.5 favours 'its own' stories and rates human stories differently\nfrom humans. Feature importance analysis with SHAP scores shows that: (i)\nnetwork features are more predictive for human creativity ratings but also for\nGPT-3.5's ratings of human stories; (ii) emotional features played a greater\nrole than semantic/syntactic network structure in GPT-3.5 rating its own\nstories. These quantitative results underscore key limitations in GPT-3.5's\nability to align with human assessments of creativity. We emphasise the need\nfor caution when using GPT-3.5 to assess and generate creative content, as it\ndoes not yet capture the nuanced complexity that characterises human\ncreativity.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00530v1",
    "published_date": "2024-11-30 16:33:48 UTC",
    "updated_date": "2024-11-30 16:33:48 UTC"
  },
  {
    "arxiv_id": "2412.00526v1",
    "title": "Human Action CLIPS: Detecting AI-generated Human Motion",
    "authors": [
      "Matyas Bohacek",
      "Hany Farid"
    ],
    "abstract": "Full-blown AI-generated video generation continues its journey through the\nuncanny valley to produce content that is perceptually indistinguishable from\nreality. Intermixed with many exciting and creative applications are malicious\napplications that harm individuals, organizations, and democracies. We describe\nan effective and robust technique for distinguishing real from AI-generated\nhuman motion. This technique leverages a multi-modal semantic embedding, making\nit robust to the types of laundering that typically confound more low- to\nmid-level approaches. This method is evaluated against a custom-built dataset\nof video clips with human actions generated by seven text-to-video AI models\nand matching real footage.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00526v1",
    "published_date": "2024-11-30 16:20:58 UTC",
    "updated_date": "2024-11-30 16:20:58 UTC"
  },
  {
    "arxiv_id": "2412.00517v1",
    "title": "LAMBDA: Covering the Multimodal Critical Scenarios for Automated Driving Systems by Search Space Quantization",
    "authors": [
      "Xinzheng Wu",
      "Junyi Chen",
      "Xingyu Xing",
      "Jian Sun",
      "Ye Tian",
      "Lihao Liu",
      "Yong Shen"
    ],
    "abstract": "Scenario-based virtual testing is one of the most significant methods to test\nand evaluate the safety of automated driving systems (ADSs). However, it is\nimpractical to enumerate all concrete scenarios in a logical scenario space and\ntest them exhaustively. Recently, Black-Box Optimization (BBO) was introduced\nto accelerate the scenario-based test of ADSs by utilizing the historical test\ninformation to generate new test cases. However, a single optimum found by the\nBBO algorithm is insufficient for the purpose of a comprehensive safety\nevaluation of ADSs in a logical scenario. In fact, all the subspaces\nrepresenting danger in the logical scenario space, rather than only the most\ncritical concrete scenario, play a more significant role for the safety\nevaluation. Covering as many of the critical concrete scenarios in a logical\nscenario space through a limited number of tests is defined as the Black-Box\nCoverage (BBC) problem in this paper. We formalized this problem in a\nsample-based search paradigm and constructed a coverage criterion with\nConfusion Matrix Analysis. Furthermore, we propose LAMBDA (Latent-Action\nMonte-Carlo Beam Search with Density Adaption) to solve BBC problems. LAMBDA\ncan quickly focus on critical subspaces by recursively partitioning the logical\nscenario space into accepted and rejected parts. Compared with its predecessor\nLaMCTS, LAMBDA introduces sampling density to overcome the sampling bias from\noptimization and Beam Search to obtain more parallelizability. Experimental\nresults show that LAMBDA achieves state-of-the-art performance among all\nbaselines and can reach at most 33 and 6000 times faster than Random Search to\nget 95% coverage of the critical areas in 2- and 5-dimensional synthetic\nfunctions, respectively. Experiments also demonstrate that LAMBDA has a\npromising future in the safety evaluation of ADSs in virtual tests.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "17pages, 21figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00517v1",
    "published_date": "2024-11-30 15:57:05 UTC",
    "updated_date": "2024-11-30 15:57:05 UTC"
  },
  {
    "arxiv_id": "2412.00511v1",
    "title": "Energy-Based Prior Latent Space Diffusion model for Reconstruction of Lumbar Vertebrae from Thick Slice MRI",
    "authors": [
      "Yanke Wang",
      "Yolanne Y. R. Lee",
      "Aurelio Dolfini",
      "Markus Reischl",
      "Ender Konukoglu",
      "Kyriakos Flouris"
    ],
    "abstract": "Lumbar spine problems are ubiquitous, motivating research into targeted\nimaging for treatment planning and guided interventions. While high resolution\nand high contrast CT has been the modality of choice, MRI can capture both bone\nand soft tissue without the ionizing radiation of CT albeit longer acquisition\ntime. The critical trade-off between contrast quality and acquisition time has\nmotivated 'thick slice MRI', which prioritises faster imaging with high\nin-plane resolution but variable contrast and low through-plane resolution. We\ninvestigate a recently developed post-acquisition pipeline which segments\nvertebrae from thick-slice acquisitions and uses a variational autoencoder to\nenhance quality after an initial 3D reconstruction. We instead propose a latent\nspace diffusion energy-based prior to leverage diffusion models, which exhibit\nhigh-quality image generation. Crucially, we mitigate their high computational\ncost and low sample efficiency by learning an energy-based latent\nrepresentation to perform the diffusion processes. Our resulting method\noutperforms existing approaches across metrics including Dice and VS scores,\nand more faithfully captures 3D features.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00511v1",
    "published_date": "2024-11-30 15:34:46 UTC",
    "updated_date": "2024-11-30 15:34:46 UTC"
  },
  {
    "arxiv_id": "2412.00508v1",
    "title": "Graph-to-SFILES: Control structure prediction from process topologies using generative artificial intelligence",
    "authors": [
      "Lukas Schulze Balhorn",
      "Kevin Degens",
      "Artur M. Schweidtmann"
    ],
    "abstract": "Control structure design is an important but tedious step in P&ID\ndevelopment. Generative artificial intelligence (AI) promises to reduce P&ID\ndevelopment time by supporting engineers. Previous research on generative AI in\nchemical process design mainly represented processes by sequences. However,\ngraphs offer a promising alternative because of their permutation invariance.\nWe propose the Graph-to-SFILES model, a generative AI method to predict control\nstructures from flowsheet topologies. The Graph-to-SFILES model takes the\nflowsheet topology as a graph input and returns a control-extended flowsheet as\na sequence in the SFILES 2.0 notation. We compare four different graph encoder\narchitectures, one of them being a graph neural network (GNN) proposed in this\nwork. The Graph-to-SFILES model achieves a top-5 accuracy of 73.2% when trained\non 10,000 flowsheet topologies. In addition, the proposed GNN performs best\namong the encoder architectures. Compared to a purely sequence-based approach,\nthe Graph-to-SFILES model improves the top-5 accuracy for a relatively small\ntraining dataset of 1,000 flowsheets from 0.9% to 28.4%. However, the\nsequence-based approach performs better on a large-scale dataset of 100,000\nflowsheets. These results highlight the potential of graph-based AI models to\naccelerate P&ID development in small-data regimes but their effectiveness on\nindustry relevant case studies still needs to be investigated.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00508v1",
    "published_date": "2024-11-30 15:30:11 UTC",
    "updated_date": "2024-11-30 15:30:11 UTC"
  },
  {
    "arxiv_id": "2412.00503v3",
    "title": "Homeostasis and Sparsity in Transformer",
    "authors": [
      "Leonid Kotyuzanskiy",
      "Artem Klimov"
    ],
    "abstract": "The transformer architecture has become an integral part of the field of\nmodern neural networks, playing a crucial role in a variety of tasks, such as\ntext generation, machine translation, image and audio processing, among others.\nThere is also an alternative approach to building intelligent systems, proposed\nby Jeff Hawkins and inspired by the processes occurring in the neocortex. In\nour article we want to combine some of these ideas and to propose the use of\nhomeostasis mechanisms, such as RFB-kWTA and \"Smart\" Inhibition, in the\nattention mechanism of the transformer and at the output of the transformer\nblock, as well as conducting an experiment involving the introduction of sparse\ndistributed representations of the transformer at various points. RFB-kWTA\nutilizes statistics of layer activations across time to adjust the entire\nlayer, enhancing the values of rare activations while reducing those of\nfrequent ones. \"Smart\" Inhibition also uses activation statistics to sample\nsparsity masks, with rarer activation times are more likely to be activated.\nOur proposed mechanisms significantly outperform the classical transformer\n0.2768 BLEU and a model that only makes use of dropout in the attention\nmechanism and output of the transformer block 0.3007 BLEU, achieving a score of\n0.3062 on the Multi30K dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00503v3",
    "published_date": "2024-11-30 15:03:41 UTC",
    "updated_date": "2024-12-16 14:59:05 UTC"
  },
  {
    "arxiv_id": "2412.00488v1",
    "title": "Improved Cleanup and Decoding of Fractional Power Encodings",
    "authors": [
      "Alicia Bremer",
      "Jeff Orchard"
    ],
    "abstract": "High-dimensional vectors have been proposed as a neural method for\nrepresenting information in the brain using Vector Symbolic Algebras (VSAs).\nWhile previous work has explored decoding and cleaning up these vectors under\nthe noise that arises during computation, existing methods are limited. Cleanup\nmethods are essential for robust computation within a VSA. However, cleanup\nmethods for continuous-value encodings are not as effective. In this paper, we\npresent an iterative optimization method to decode and clean up Fourier\nHolographic Reduced Representation (FHRR) vectors that are encoding continuous\nvalues. We combine composite likelihood estimation (CLE) and maximum likelihood\nestimation (MLE) to ensure convergence to the global optimum. We also\ndemonstrate that this method can effectively decode FHRR vectors under\ndifferent noise conditions, and show that it outperforms existing methods.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "68T07, 92B20"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00488v1",
    "published_date": "2024-11-30 14:10:48 UTC",
    "updated_date": "2024-11-30 14:10:48 UTC"
  },
  {
    "arxiv_id": "2412.00478v1",
    "title": "Node Importance Estimation Leveraging LLMs for Semantic Augmentation in Knowledge Graphs",
    "authors": [
      "Xinyu Lin",
      "Tianyu Zhang",
      "Chengbin Hou",
      "Jinbao Wang",
      "Jianye Xue",
      "Hairong Lv"
    ],
    "abstract": "Node Importance Estimation (NIE) is a task that quantifies the importance of\nnode in a graph. Recent research has investigated to exploit various\ninformation from Knowledge Graphs (KGs) to estimate node importance scores.\nHowever, the semantic information in KGs could be insufficient, missing, and\ninaccurate, which would limit the performance of existing NIE models. To\naddress these issues, we leverage Large Language Models (LLMs) for semantic\naugmentation thanks to the LLMs' extra knowledge and ability of integrating\nknowledge from both LLMs and KGs. To this end, we propose the LLMs Empowered\nNode Importance Estimation (LENIE) method to enhance the semantic information\nin KGs for better supporting NIE tasks. To our best knowledge, this is the\nfirst work incorporating LLMs into NIE. Specifically, LENIE employs a novel\nclustering-based triplet sampling strategy to extract diverse knowledge of a\nnode sampled from the given KG. After that, LENIE adopts the node-specific\nadaptive prompts to integrate the sampled triplets and the original node\ndescriptions, which are then fed into LLMs for generating richer and more\nprecise augmented node descriptions. These augmented descriptions finally\ninitialize node embeddings for boosting the downstream NIE model performance.\nExtensive experiments demonstrate LENIE's effectiveness in addressing semantic\ndeficiencies in KGs, enabling more informative semantic augmentation and\nenhancing existing NIE models to achieve the state-of-the-art performance. The\nsource code of LENIE is freely available at\n\\url{https://github.com/XinyuLin-FZ/LENIE}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00478v1",
    "published_date": "2024-11-30 13:32:05 UTC",
    "updated_date": "2024-11-30 13:32:05 UTC"
  },
  {
    "arxiv_id": "2412.00465v2",
    "title": "AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large Language Models",
    "authors": [
      "Yutong Zhou",
      "Masahiro Ryo"
    ],
    "abstract": "We introduce AgriBench, the first agriculture benchmark designed to evaluate\nMultiModal Large Language Models (MM-LLMs) for agriculture applications. To\nfurther address the agriculture knowledge-based dataset limitation problem, we\npropose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784\nlandscape images, segmentation masks, depth maps, and detailed annotations\n(geographical location, country, date, land cover and land use taxonomic\ndetails, quality scores, aesthetic scores, etc), based on the Land Use/Cover\nArea Frame Survey (LUCAS) dataset, which contains comparable statistics on land\nuse and land cover for the European Union (EU) territory. This work presents a\ngroundbreaking perspective in advancing agriculture MM-LLMs and is still in\nprogress, offering valuable insights for future developments and innovations in\nspecific expert knowledge-based MM-LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPPA @ECCV2024. Dataset:\n  https://github.com/Yutong-Zhou-cv/AgriBench",
    "pdf_url": "http://arxiv.org/pdf/2412.00465v2",
    "published_date": "2024-11-30 12:59:03 UTC",
    "updated_date": "2024-12-21 16:18:42 UTC"
  },
  {
    "arxiv_id": "2412.00464v1",
    "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach",
    "authors": [
      "Gabriel Pedroza"
    ],
    "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "F.4.1; I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages including references, no figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00464v1",
    "published_date": "2024-11-30 12:57:07 UTC",
    "updated_date": "2024-11-30 12:57:07 UTC"
  },
  {
    "arxiv_id": "2412.00435v1",
    "title": "Benchmark Real-time Adaptation and Communication Capabilities of Embodied Agent in Collaborative Scenarios",
    "authors": [
      "Shipeng Liu",
      "Boshen Zhang",
      "Zhehui Huang"
    ],
    "abstract": "Advancements in Large Language Models (LLMs) have opened transformative\npossibilities for human-robot interaction, especially in collaborative\nenvironments. However, Real-time human-AI collaboration requires agents to\nadapt to unseen human behaviors while maintaining effective communication\ndynamically. Existing benchmarks fall short in evaluating such adaptability for\nembodied agents, focusing mostly on the task performance of the agent itself.\nTo address this gap, we propose a novel benchmark that assesses agents'\nreactive adaptability and instantaneous communication capabilities at every\nstep. Based on this benchmark, we propose a Monitor-then-Adapt framework\n(MonTA), combining strong adaptability and communication with real-time\nexecution. MonTA contains three key LLM modules, a lightweight \\textit{Monitor}\nfor monitoring the need for adaptation in high frequency, and two proficient\n\\textit{Adapters} for subtask and path adaptation reasoning in low frequency.\nOur results demonstrate that MonTA outperforms other baseline agents on our\nproposed benchmark. Further user studies confirm the high reasonability\nadaptation plan and consistent language instruction provided by our framework.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO",
      "68T05",
      "I.2.9"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00435v1",
    "published_date": "2024-11-30 11:17:17 UTC",
    "updated_date": "2024-11-30 11:17:17 UTC"
  },
  {
    "arxiv_id": "2412.00430v6",
    "title": "Optimizing Sequential Recommendation Models with Scaling Laws and Approximate Entropy",
    "authors": [
      "Tingjia Shen",
      "Hao Wang",
      "Chuhan Wu",
      "Jin Yao Chin",
      "Wei Guo",
      "Yong Liu",
      "Huifeng Guo",
      "Defu Lian",
      "Ruiming Tang",
      "Enhong Chen"
    ],
    "abstract": "Scaling Laws have emerged as a powerful framework for understanding how model\nperformance evolves as they increase in size, providing valuable insights for\noptimizing computational resources. In the realm of Sequential Recommendation\n(SR), which is pivotal for predicting users' sequential preferences, these laws\noffer a lens through which to address the challenges posed by the scalability\nof SR models. However, the presence of structural and collaborative issues in\nrecommender systems prevents the direct application of the Scaling Law (SL) in\nthese systems. In response, we introduce the Performance Law for SR models,\nwhich aims to theoretically investigate and model the relationship between\nmodel performance and data quality. Specifically, we first fit the HR and NDCG\nmetrics to transformer-based SR models. Subsequently, we propose Approximate\nEntropy (ApEn) to assess data quality, presenting a more nuanced approach\ncompared to traditional data quantity metrics. Our method enables accurate\npredictions across various dataset scales and model sizes, demonstrating a\nstrong correlation in large SR models and offering insights into achieving\noptimal performance for any given model configuration.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "68P20",
      "H.3.4; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00430v6",
    "published_date": "2024-11-30 10:56:30 UTC",
    "updated_date": "2025-02-19 09:23:03 UTC"
  },
  {
    "arxiv_id": "2412.00429v1",
    "title": "Learner Attentiveness and Engagement Analysis in Online Education Using Computer Vision",
    "authors": [
      "Sharva Gogawale",
      "Madhura Deshpande",
      "Parteek Kumar",
      "Irad Ben-Gal"
    ],
    "abstract": "In recent times, online education and the usage of video-conferencing\nplatforms have experienced massive growth. Due to the limited scope of a\nvirtual classroom, it may become difficult for instructors to analyze learners'\nattention and comprehension in real time while teaching. In the digital mode of\neducation, it would be beneficial for instructors to have an automated feedback\nmechanism to be informed regarding learners' attentiveness at any given time.\nThis research presents a novel computer vision-based approach to analyze and\nquantify learners' attentiveness, engagement, and other affective states within\nonline learning scenarios. This work presents the development of a multiclass\nmultioutput classification method using convolutional neural networks on a\npublicly available dataset - DAiSEE. A machine learning-based algorithm is\ndeveloped on top of the classification model that outputs a comprehensive\nattentiveness index of the learners. Furthermore, an end-to-end pipeline is\nproposed through which learners' live video feed is processed, providing\ndetailed attentiveness analytics of the learners to the instructors. By\ncomparing the experimental outcomes of the proposed method against those of\nprevious methods, it is demonstrated that the proposed method exhibits better\nattentiveness detection than state-of-the-art methods. The proposed system is a\ncomprehensive, practical, and real-time solution that is deployable and easy to\nuse. The experimental results also demonstrate the system's efficiency in\ngauging learners' attentiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00429v1",
    "published_date": "2024-11-30 10:54:08 UTC",
    "updated_date": "2024-11-30 10:54:08 UTC"
  },
  {
    "arxiv_id": "2412.00427v1",
    "title": "FreeCond: Free Lunch in the Input Conditions of Text-Guided Inpainting",
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Kai Ruan",
      "Sung-Lin Tsai",
      "Yi-Lun Wu",
      "Hong-Han Shuai"
    ],
    "abstract": "In this study, we aim to determine and solve the deficiency of Stable\nDiffusion Inpainting (SDI) in following the instruction of both prompt and\nmask. Due to the training bias from masking, the inpainting quality is hindered\nwhen the prompt instruction and image condition are not related. Therefore, we\nconduct a detailed analysis of the internal representations learned by SDI,\nfocusing on how the mask input influences the cross-attention layer. We observe\nthat adapting text key tokens toward the input mask enables the model to\nselectively paint within the given area. Leveraging these insights, we propose\nFreeCond, which adjusts only the input mask condition and image condition. By\nincreasing the latent mask value and modifying the frequency of image\ncondition, we align the cross-attention features with the model's training bias\nto improve generation quality without additional computation, particularly when\nuser inputs are complicated and deviate from the training setup. Extensive\nexperiments demonstrate that FreeCond can enhance any SDI-based model, e.g.,\nyielding up to a 60% and 58% improvement of SDI and SDXLI in the CLIP score.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00427v1",
    "published_date": "2024-11-30 10:52:25 UTC",
    "updated_date": "2024-11-30 10:52:25 UTC"
  },
  {
    "arxiv_id": "2412.00418v2",
    "title": "Mixture of Experts for Node Classification",
    "authors": [
      "Yu Shi",
      "Yiqi Wang",
      "WeiXuan Lang",
      "Jiaxin Zhang",
      "Pan Dong",
      "Aiping Li"
    ],
    "abstract": "Nodes in the real-world graphs exhibit diverse patterns in numerous aspects,\nsuch as degree and homophily. However, most existent node predictors fail to\ncapture a wide range of node patterns or to make predictions based on distinct\nnode patterns, resulting in unsatisfactory classification performance. In this\npaper, we reveal that different node predictors are good at handling nodes with\nspecific patterns and only apply one node predictor uniformly could lead to\nsuboptimal result. To mitigate this gap, we propose a mixture of experts\nframework, MoE-NP, for node classification. Specifically, MoE-NP combines a\nmixture of node predictors and strategically selects models based on node\npatterns. Experimental results from a range of real-world datasets demonstrate\nsignificant performance improvements from MoE-NP.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00418v2",
    "published_date": "2024-11-30 10:05:03 UTC",
    "updated_date": "2025-03-12 12:33:46 UTC"
  },
  {
    "arxiv_id": "2412.00410v1",
    "title": "Federated Progressive Self-Distillation with Logits Calibration for Personalized IIoT Edge Intelligence",
    "authors": [
      "Yingchao Wang",
      "Wenqi Niu"
    ],
    "abstract": "Personalized Federated Learning (PFL) focuses on tailoring models to\nindividual IIoT clients in federated learning by addressing data heterogeneity\nand diverse user needs. Although existing studies have proposed effective PFL\nsolutions from various perspectives, they overlook the issue of forgetting both\nhistorical personalized knowledge and global generalized knowledge during local\ntraining on clients. Therefore, this study proposes a novel PFL method,\nFederated Progressive Self-Distillation (FedPSD), based on logits calibration\nand progressive self-distillation. We analyze the impact mechanism of client\ndata distribution characteristics on personalized and global knowledge\nforgetting. To address the issue of global knowledge forgetting, we propose a\nlogits calibration approach for the local training loss and design a\nprogressive self-distillation strategy to facilitate the gradual inheritance of\nglobal knowledge, where the model outputs from the previous epoch serve as\nvirtual teachers to guide the training of subsequent epochs. Moreover, to\naddress personalized knowledge forgetting, we construct calibrated fusion\nlabels by integrating historical personalized model outputs, which are then\nused as teacher model outputs to guide the initial epoch of local\nself-distillation, enabling rapid recall of personalized knowledge. Extensive\nexperiments under various data heterogeneity scenarios demonstrate the\neffectiveness and superiority of the proposed FedPSD method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages,5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00410v1",
    "published_date": "2024-11-30 09:32:05 UTC",
    "updated_date": "2024-11-30 09:32:05 UTC"
  },
  {
    "arxiv_id": "2412.00403v1",
    "title": "Fine-Tuning Pre-trained Large Time Series Models for Prediction of Wind Turbine SCADA Data",
    "authors": [
      "Yuwei Fan",
      "Tao Song",
      "Chenlong Feng",
      "Keyu Song",
      "Chao Liu",
      "Dongxiang Jiang"
    ],
    "abstract": "The remarkable achievements of large models in the fields of natural language\nprocessing (NLP) and computer vision (CV) have sparked interest in their\napplication to time series forecasting within industrial contexts. This paper\nexplores the application of a pre-trained large time series model, Timer, which\nwas initially trained on a wide range of time series data from multiple\ndomains, in the prediction of Supervisory Control and Data Acquisition (SCADA)\ndata collected from wind turbines. The model was fine-tuned on SCADA datasets\nsourced from two wind farms, which exhibited differing characteristics, and its\naccuracy was subsequently evaluated. Additionally, the impact of data volume\nwas studied to evaluate the few-shot ability of the Timer. Finally, an\napplication study on one-turbine fine-tuning for whole-plant prediction was\nimplemented where both few-shot and cross-turbine generalization capacity is\nrequired. The results reveal that the pre-trained large model does not\nconsistently outperform other baseline models in terms of prediction accuracy\nwhenever the data is abundant or not, but demonstrates superior performance in\nthe application study. This result underscores the distinctive advantages of\nthe pre-trained large time series model in facilitating swift deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00403v1",
    "published_date": "2024-11-30 09:00:24 UTC",
    "updated_date": "2024-11-30 09:00:24 UTC"
  },
  {
    "arxiv_id": "2412.00402v1",
    "title": "DroidCall: A Dataset for LLM-powered Android Intent Invocation",
    "authors": [
      "Weikai Xie",
      "Li Zhang",
      "Shihe Wang",
      "Rongjie Yi",
      "Mengwei Xu"
    ],
    "abstract": "The growing capabilities of large language models in natural language\nunderstanding significantly strengthen existing agentic systems. To power\nperformant on-device mobile agents for better data privacy, we introduce\nDroidCall, the first training and testing dataset for accurate Android intent\ninvocation. With a highly flexible and reusable data generation pipeline, we\nconstructed 10k samples in DroidCall. Given a task instruction in natural\nlanguage, small language models such as Qwen2.5-3B and Gemma2-2B fine-tuned\nwith DroidCall can approach or even surpass the capabilities of GPT-4o for\naccurate Android intent invocation. We also provide an end-to-end Android app\nequipped with these fine-tuned models to demonstrate the Android intent\ninvocation process. The code and dataset are available at\nhttps://github.com/UbiquitousLearning/DroidCall.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00402v1",
    "published_date": "2024-11-30 08:55:39 UTC",
    "updated_date": "2024-11-30 08:55:39 UTC"
  },
  {
    "arxiv_id": "2412.00386v1",
    "title": "Strategic Application of AIGC for UAV Trajectory Design: A Channel Knowledge Map Approach",
    "authors": [
      "Chiya Zhang",
      "Ting Wang",
      "Rubing Han",
      "Yuanxiang Gong"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly utilized in wireless\ncommunication, yet accurate channel loss prediction remains a significant\nchallenge, limiting resource optimization performance. To address this issue,\nthis paper leverages Artificial Intelligence Generated Content (AIGC) for the\nefficient construction of Channel Knowledge Maps (CKM) and UAV trajectory\ndesign. Given the time-consuming nature of channel data collection, AI\ntechniques are employed in a Wasserstein Generative Adversarial Network (WGAN)\nto extract environmental features and augment the data. Experiment results\ndemonstrate the effectiveness of the proposed framework in improving CKM\nconstruction accuracy. Moreover, integrating CKM into UAV trajectory planning\nreduces channel gain uncertainty, demonstrating its potential to enhance\nwireless communication efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00386v1",
    "published_date": "2024-11-30 07:34:49 UTC",
    "updated_date": "2024-11-30 07:34:49 UTC"
  },
  {
    "arxiv_id": "2412.00383v2",
    "title": "Unified Parameter-Efficient Unlearning for LLMs",
    "authors": [
      "Chenlu Ding",
      "Jiancan Wu",
      "Yancheng Yuan",
      "Jinda Lu",
      "Kai Zhang",
      "Alex Su",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, enabling advanced understanding and reasoning capabilities\nacross a variety of tasks. Fine-tuning these models for specific domains,\nparticularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like\nLoRA, has become a prevalent practice due to its efficiency. However, this\nraises significant privacy and security concerns, as models may inadvertently\nretain and disseminate sensitive or undesirable information. To address these\nissues, we introduce a novel instance-wise unlearning framework, LLMEraser,\nwhich systematically categorizes unlearning tasks and applies precise parameter\nadjustments using influence functions. Unlike traditional unlearning techniques\nthat are often limited in scope and require extensive retraining, LLMEraser is\ndesigned to handle a broad spectrum of unlearning tasks without compromising\nmodel performance. Extensive experiments on benchmark datasets demonstrate that\nLLMEraser excels in efficiently managing various unlearning scenarios while\nmaintaining the overall integrity and efficacy of the models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00383v2",
    "published_date": "2024-11-30 07:21:02 UTC",
    "updated_date": "2025-04-18 06:58:12 UTC"
  },
  {
    "arxiv_id": "2412.00373v1",
    "title": "Approximate Fiber Product: A Preliminary Algebraic-Geometric Perspective on Multimodal Embedding Alignment",
    "authors": [
      "Dongfang Zhao"
    ],
    "abstract": "Multimodal tasks, such as image-text retrieval and generation, require\nembedding data from diverse modalities into a shared representation space.\nAligning embeddings from heterogeneous sources while preserving shared and\nmodality-specific information is a fundamental challenge. This paper provides\nan initial attempt to integrate algebraic geometry into multimodal\nrepresentation learning, offering a foundational perspective for further\nexploration.\n  We model image and text data as polynomials over discrete rings, \\(\n\\mathbb{Z}_{256}[x] \\) and \\( \\mathbb{Z}_{|V|}[x] \\), respectively, enabling\nthe use of algebraic tools like fiber products to analyze alignment properties.\nTo accommodate real-world variability, we extend the classical fiber product to\nan approximate fiber product with a tolerance parameter \\( \\epsilon \\),\nbalancing precision and noise tolerance. We study its dependence on \\( \\epsilon\n\\), revealing asymptotic behavior, robustness to perturbations, and sensitivity\nto embedding dimensionality.\n  Additionally, we propose a decomposition of the shared embedding space into\northogonal subspaces, \\( Z = Z_s \\oplus Z_I \\oplus Z_T \\), where \\( Z_s \\)\ncaptures shared semantics, and \\( Z_I \\), \\( Z_T \\) encode modality-specific\nfeatures. This decomposition is geometrically interpreted via manifolds and\nfiber bundles, offering insights into embedding structure and optimization.\n  This framework establishes a principled foundation for analyzing multimodal\nalignment, uncovering connections between robustness, dimensionality\nallocation, and algebraic structure. It lays the groundwork for further\nresearch on embedding spaces in multimodal learning using algebraic geometry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AG"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00373v1",
    "published_date": "2024-11-30 06:45:13 UTC",
    "updated_date": "2024-11-30 06:45:13 UTC"
  },
  {
    "arxiv_id": "2412.00372v1",
    "title": "2-Factor Retrieval for Improved Human-AI Decision Making in Radiology",
    "authors": [
      "Jim Solomon",
      "Laleh Jalilian",
      "Alexander Vilesov",
      "Meryl Mathew",
      "Tristan Grogan",
      "Arash Bedayat",
      "Achuta Kadambi"
    ],
    "abstract": "Human-machine teaming in medical AI requires us to understand to what degree\na trained clinician should weigh AI predictions. While previous work has shown\nthe potential of AI assistance at improving clinical predictions, existing\nclinical decision support systems either provide no explainability of their\npredictions or use techniques like saliency and Shapley values, which do not\nallow for physician-based verification. To address this gap, this study\ncompares previously used explainable AI techniques with a newly proposed\ntechnique termed '2-factor retrieval (2FR)', which is a combination of\ninterface design and search retrieval that returns similarly labeled data\nwithout processing this data. This results in a 2-factor security blanket\nwhere: (a) correct images need to be retrieved by the AI; and (b) humans should\nassociate the retrieved images with the current pathology under test. We find\nthat when tested on chest X-ray diagnoses, 2FR leads to increases in clinician\naccuracy, with particular improvements when clinicians are radiologists and\nhave low confidence in their decision. Our results highlight the importance of\nunderstanding how different modes of human-AI decision making may impact\nclinician accuracy in clinical decision support systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00372v1",
    "published_date": "2024-11-30 06:44:42 UTC",
    "updated_date": "2024-11-30 06:44:42 UTC"
  },
  {
    "arxiv_id": "2412.00357v1",
    "title": "Safety Alignment Backfires: Preventing the Re-emergence of Suppressed Concepts in Fine-tuned Text-to-Image Diffusion Models",
    "authors": [
      "Sanghyun Kim",
      "Moonseok Choi",
      "Jinwoo Shin",
      "Juho Lee"
    ],
    "abstract": "Fine-tuning text-to-image diffusion models is widely used for personalization\nand adaptation for new domains. In this paper, we identify a critical\nvulnerability of fine-tuning: safety alignment methods designed to filter\nharmful content (e.g., nudity) can break down during fine-tuning, allowing\npreviously suppressed content to resurface, even when using benign datasets.\nWhile this \"fine-tuning jailbreaking\" issue is known in large language models,\nit remains largely unexplored in text-to-image diffusion models. Our\ninvestigation reveals that standard fine-tuning can inadvertently undo safety\nmeasures, causing models to relearn harmful concepts that were previously\nremoved and even exacerbate harmful behaviors. To address this issue, we\npresent a novel but immediate solution called Modular LoRA, which involves\ntraining Safety Low-Rank Adaptation (LoRA) modules separately from Fine-Tuning\nLoRA components and merging them during inference. This method effectively\nprevents the re-learning of harmful content without compromising the model's\nperformance on new tasks. Our experiments demonstrate that Modular LoRA\noutperforms traditional fine-tuning methods in maintaining safety alignment,\noffering a practical approach for enhancing the security of text-to-image\ndiffusion models against potential attacks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00357v1",
    "published_date": "2024-11-30 04:37:38 UTC",
    "updated_date": "2024-11-30 04:37:38 UTC"
  },
  {
    "arxiv_id": "2412.00354v1",
    "title": "On the Role of Noise in Factorizers for Disentangling Distributed Representations",
    "authors": [
      "Geethan Karunaratne",
      "Michael Hersche",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "abstract": "To efficiently factorize high-dimensional distributed representations to the\nconstituent atomic vectors, one can exploit the compute-in-superposition\ncapabilities of vector-symbolic architectures (VSA). Such factorizers however\nsuffer from the phenomenon of limit cycles.\n  Applying noise during the iterative decoding is one mechanism to address this\nissue. In this paper, we explore ways to further relax the noise requirement by\napplying noise only at the time of VSA's reconstruction codebook\ninitialization. While the need for noise during iterations proves analog\nin-memory computing systems to be a natural choice as an implementation media,\nthe adequacy of initialization noise allows digital hardware to remain equally\nindispensable. This broadens the implementation possibilities of factorizers.\nOur study finds that while the best performance shifts from initialization\nnoise to iterative noise as the number of factors increases from 2 to 4, both\nextend the operational capacity by at least 50 times compared to the baseline\nfactorizer resonator networks. Our code is available at:\nhttps://github.com/IBM/in-memory-factorizer",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Second Workshop on Machine Learning with New Compute\n  Paradigms at 38th NeurIPS 2024 (MLNCP 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.00354v1",
    "published_date": "2024-11-30 04:25:18 UTC",
    "updated_date": "2024-11-30 04:25:18 UTC"
  },
  {
    "arxiv_id": "2412.00353v2",
    "title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection",
    "authors": [
      "Shanu Kumar",
      "Saish Mendke",
      "Karody Lubna Abdul Rahman",
      "Santosh Kurasa",
      "Parag Agrawal",
      "Sandipan Dandapat"
    ],
    "abstract": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.00353v2",
    "published_date": "2024-11-30 04:22:00 UTC",
    "updated_date": "2024-12-06 10:24:47 UTC"
  },
  {
    "arxiv_id": "2412.00346v1",
    "title": "CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention",
    "authors": [
      "Han Li",
      "Fei Liu",
      "Zhi Zheng",
      "Yu Zhang",
      "Zhenkun Wang"
    ],
    "abstract": "Vehicle Routing Problems (VRPs) are significant Combinatorial Optimization\n(CO) problems holding substantial practical importance. Recently, Neural\nCombinatorial Optimization (NCO), which involves training deep learning models\non extensive data to learn vehicle routing heuristics, has emerged as a\npromising approach due to its efficiency and the reduced need for manual\nalgorithm design. However, applying NCO across diverse real-world scenarios\nwith various constraints necessitates cross-problem capabilities. Current NCO\nmethods typically employ a unified model lacking a constraint-specific\nstructure, thereby restricting their cross-problem performance. Current\nmulti-task methods for VRPs typically employ a constraint-unaware model,\nlimiting their cross-problem performance. Furthermore, they rely solely on\nglobal connectivity, which fails to focus on key nodes and leads to inefficient\nrepresentation learning. This paper introduces a Constraint-Aware\nDual-Attention Model (CaDA), designed to address these limitations. CaDA\nincorporates a constraint prompt that efficiently represents different problem\nvariants. Additionally, it features a dual-attention mechanism with a global\nbranch for capturing broader graph-wide information and a sparse branch that\nselectively focuses on the most relevant nodes. We comprehensively evaluate our\nmodel on 16 different VRPs and compare its performance against existing\ncross-problem VRP solvers. CaDA achieves state-of-the-art results across all\nthe VRPs. Our ablation study further confirms that each component of CaDA\ncontributes positively to its cross-problem learning performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00346v1",
    "published_date": "2024-11-30 04:11:36 UTC",
    "updated_date": "2024-11-30 04:11:36 UTC"
  },
  {
    "arxiv_id": "2412.00342v1",
    "title": "Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models",
    "authors": [
      "Nadeen Fathallah",
      "Monika Bhole",
      "Steffen Staab"
    ],
    "abstract": "In today's digital age, video content is prevalent, serving as a primary\nsource of information, education, and entertainment. However, the Deaf and Hard\nof Hearing (DHH) community often faces significant challenges in accessing\nvideo content due to the inadequacy of automatic speech recognition (ASR)\nsystems in providing accurate and reliable captions. This paper addresses the\nurgent need to improve video caption quality by leveraging Large Language\nModels (LLMs). We present a comprehensive study that explores the integration\nof LLMs to enhance the accuracy and context-awareness of captions generated by\nASR systems. Our methodology involves a novel pipeline that corrects\nASR-generated captions using advanced LLMs. It explicitly focuses on models\nlike GPT-3.5 and Llama2-13B due to their robust performance in language\ncomprehension and generation tasks. We introduce a dataset representative of\nreal-world challenges the DHH community faces to evaluate our proposed\npipeline. Our results indicate that LLM-enhanced captions significantly improve\naccuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by\nChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%),\nChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the\noriginal ASR captions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00342v1",
    "published_date": "2024-11-30 03:52:08 UTC",
    "updated_date": "2024-11-30 03:52:08 UTC"
  },
  {
    "arxiv_id": "2412.00334v1",
    "title": "EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices",
    "authors": [
      "Meihan Wu",
      "Tao Chang",
      "Cui Miao",
      "Jie Zhou",
      "Chun Li",
      "Xiangyu Xu",
      "Ming Li",
      "Xiaodong Wang"
    ],
    "abstract": "Federated learning research has recently shifted from Convolutional Neural\nNetworks (CNNs) to Vision Transformers (ViTs) due to their superior capacity.\nViTs training demands higher computational resources due to the lack of 2D\ninductive biases inherent in CNNs. However, efficient federated training of\nViTs on resource-constrained edge devices remains unexplored in the community.\nIn this paper, we propose EFTViT, a hierarchical federated framework that\nleverages masked images to enable efficient, full-parameter training on\nresource-constrained edge devices, offering substantial benefits for learning\non heterogeneous data. In general, we patchify images and randomly mask a\nportion of the patches, observing that excluding them from training has minimal\nimpact on performance while substantially reducing computation costs and\nenhancing data content privacy protection. Specifically, EFTViT comprises a\nseries of lightweight local modules and a larger global module, updated\nindependently on clients and the central server, respectively. The local\nmodules are trained on masked image patches, while the global module is trained\non intermediate patch features uploaded from the local client, balanced through\na proposed median sampling strategy to erase client data distribution privacy.\nWe analyze the computational complexity and privacy protection of EFTViT.\nExtensive experiments on popular benchmarks show that EFTViT achieves up to\n28.17% accuracy improvement, reduces local training computational cost by up to\n2.8$\\times$, and cuts local training time by up to 4.4$\\times$ compared to\nexisting methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00334v1",
    "published_date": "2024-11-30 03:20:14 UTC",
    "updated_date": "2024-11-30 03:20:14 UTC"
  },
  {
    "arxiv_id": "2412.00325v1",
    "title": "MusicGen-Chord: Advancing Music Generation through Chord Progressions and Interactive Web-UI",
    "authors": [
      "Jongmin Jung",
      "Andreas Jansson",
      "Dasaem Jeong"
    ],
    "abstract": "MusicGen is a music generation language model (LM) that can be conditioned on\ntextual descriptions and melodic features. We introduce MusicGen-Chord, which\nextends this capability by incorporating chord progression features. This model\nmodifies one-hot encoded melody chroma vectors into multi-hot encoded chord\nchroma vectors, enabling the generation of music that reflects both chord\nprogressions and textual descriptions. Furthermore, we developed\nMusicGen-Remixer, an application utilizing MusicGen-Chord to generate remixes\nof input music conditioned on textual descriptions. Both models are integrated\ninto Replicate's web-UI using cog, facilitating broad accessibility and\nuser-friendly controllable interaction for creating and experiencing\nAI-generated music.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Late-breaking/demo (LBD) at ISMIR 2024.\n  https://ismir2024program.ismir.net/lbd_424.html",
    "pdf_url": "http://arxiv.org/pdf/2412.00325v1",
    "published_date": "2024-11-30 02:49:45 UTC",
    "updated_date": "2024-11-30 02:49:45 UTC"
  },
  {
    "arxiv_id": "2412.00323v1",
    "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
    "authors": [
      "Yasuaki Sumita",
      "Koh Takeuchi",
      "Hisashi Kashima"
    ],
    "abstract": "Large Language Models (LLMs) are trained on large corpora written by humans\nand demonstrate high performance on various tasks. However, as humans are\nsusceptible to cognitive biases, which can result in irrational judgments, LLMs\ncan also be influenced by these biases, leading to irrational decision-making.\nFor example, changing the order of options in multiple-choice questions affects\nthe performance of LLMs due to order bias. In our research, we first conducted\nan extensive survey of existing studies examining LLMs' cognitive biases and\ntheir mitigation. The mitigation techniques in LLMs have the disadvantage that\nthey are limited in the type of biases they can apply or require lengthy inputs\nor outputs. We then examined the effectiveness of two mitigation methods for\nhumans, SoPro and AwaRe, when applied to LLMs, inspired by studies in\ncrowdsourcing. To test the effectiveness of these methods, we conducted\nexperiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the\noutputs before and after applying these methods. The results demonstrate that\nwhile SoPro has little effect, AwaRe enables LLMs to mitigate the effect of\nthese biases and make more rational responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The extended abstract of this paper is presented at the 40th\n  ACM/SIGAPP Symposium on Applied Computing (SAC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.00323v1",
    "published_date": "2024-11-30 02:37:59 UTC",
    "updated_date": "2024-11-30 02:37:59 UTC"
  },
  {
    "arxiv_id": "2412.00319v1",
    "title": "Improving speaker verification robustness with synthetic emotional utterances",
    "authors": [
      "Nikhil Kumar Koditala",
      "Chelsea Jui-Ting Ju",
      "Ruirui Li",
      "Minho Jin",
      "Aman Chadha",
      "Andreas Stolcke"
    ],
    "abstract": "A speaker verification (SV) system offers an authentication service designed\nto confirm whether a given speech sample originates from a specific speaker.\nThis technology has paved the way for various personalized applications that\ncater to individual preferences. A noteworthy challenge faced by SV systems is\ntheir ability to perform consistently across a range of emotional spectra. Most\nexisting models exhibit high error rates when dealing with emotional utterances\ncompared to neutral ones. Consequently, this phenomenon often leads to missing\nout on speech of interest. This issue primarily stems from the limited\navailability of labeled emotional speech data, impeding the development of\nrobust speaker representations that encompass diverse emotional states.\n  To address this concern, we propose a novel approach employing the CycleGAN\nframework to serve as a data augmentation method. This technique synthesizes\nemotional speech segments for each specific speaker while preserving the unique\nvocal identity. Our experimental findings underscore the effectiveness of\nincorporating synthetic emotional data into the training process. The models\ntrained using this augmented dataset consistently outperform the baseline\nmodels on the task of verifying speakers in emotional speech scenarios,\nreducing equal error rate by as much as 3.64% relative.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00319v1",
    "published_date": "2024-11-30 02:18:26 UTC",
    "updated_date": "2024-11-30 02:18:26 UTC"
  },
  {
    "arxiv_id": "2412.00316v2",
    "title": "HiMoE: Heterogeneity-Informed Mixture-of-Experts for Fair Spatial-Temporal Forecasting",
    "authors": [
      "Shaohan Yu",
      "Pan Deng",
      "Yu Zhao",
      "Junting Liu",
      "Zi'ang Wang"
    ],
    "abstract": "Achieving fair prediction performance across nodes is crucial in the\nspatial-temporal domain, as it ensures the validity and reliability of\nforecasting outcomes. However, existing models focus primarily on improving the\noverall accuracy of the prediction, often neglecting the goal of achieving\nuniformity in the predictions. This task becomes particularly challenging due\nto the inherent spatial-temporal heterogeneity of the nodes. To address this\nissue, we propose a novel Heterogeneity-informed Mixture-of-Experts (HiMoE) for\nfair spatial-temporal forecasting. In particular, we design the\nHeterogeneity-Informed Graph Convolutional Network (HiGCN), which leverages the\nfusion of multi-graph and edge masking to flexibly model spatial dependencies.\nMoreover, we introduce the Node-wise Mixture-of-Experts (NMoE), which allocates\nprediction tasks of different nodes to suitable experts through graph\ndecoupling routing. To further improve the model, fairness-aware loss and\nevaluation functions are proposed, optimizing the model with fairness and\naccuracy as objectives. Experiments on four datasets from different real-world\nscenarios demonstrate that HiMoE achieves the state-of-the-art performance,\noutperforming the best baseline with at lease 9.22% in all metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00316v2",
    "published_date": "2024-11-30 01:50:42 UTC",
    "updated_date": "2025-01-22 08:43:28 UTC"
  },
  {
    "arxiv_id": "2412.00315v1",
    "title": "One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs",
    "authors": [
      "Jingzhe Liu",
      "Haitao Mao",
      "Zhikai Chen",
      "Wenqi Fan",
      "Mingxuan Ju",
      "Tong Zhao",
      "Neil Shah",
      "Jiliang Tang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool to capture\nintricate network patterns, achieving success across different domains.\nHowever, existing GNNs require careful domain-specific architecture designs and\ntraining from scratch on each dataset, leading to an expertise-intensive\nprocess with difficulty in generalizing across graphs from different domains.\nTherefore, it can be hard for practitioners to infer which GNN model can\ngeneralize well to graphs from their domains. To address this challenge, we\npropose a novel cross-domain pretraining framework, \"one model for one graph,\"\nwhich overcomes the limitations of previous approaches that failed to use a\nsingle GNN to capture diverse graph patterns across domains with significant\ngaps. Specifically, we pretrain a bank of expert models, with each one\ncorresponding to a specific dataset. When inferring to a new graph, gating\nfunctions choose a subset of experts to effectively integrate prior model\nknowledge while avoiding negative transfer. Extensive experiments consistently\ndemonstrate the superiority of our proposed method on both link prediction and\nnode classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00315v1",
    "published_date": "2024-11-30 01:49:45 UTC",
    "updated_date": "2024-11-30 01:49:45 UTC"
  },
  {
    "arxiv_id": "2412.00312v1",
    "title": "Raw Audio Classification with Cosine Convolutional Neural Network (CosCovNN)",
    "authors": [
      "Kazi Nazmul Haque",
      "Rajib Rana",
      "Tasnim Jarin",
      "Bjorn W. Schuller Jr"
    ],
    "abstract": "This study explores the field of audio classification from raw waveform using\nConvolutional Neural Networks (CNNs), a method that eliminates the need for\nextracting specialised features in the pre-processing step. Unlike recent\ntrends in literature, which often focuses on designing frontends or filters for\nonly the initial layers of CNNs, our research introduces the Cosine\nConvolutional Neural Network (CosCovNN) replacing the traditional CNN filters\nwith Cosine filters. The CosCovNN surpasses the accuracy of the equivalent CNN\narchitectures with approximately $77\\%$ less parameters. Our research further\nprogresses with the development of an augmented CosCovNN named Vector Quantised\nCosine Convolutional Neural Network with Memory (VQCCM), incorporating a memory\nand vector quantisation layer VQCCM achieves state-of-the-art (SOTA)\nperformance across five different datasets in comparison with existing\nliterature. Our findings show that cosine filters can greatly improve the\nefficiency and accuracy of CNNs in raw audio classification.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00312v1",
    "published_date": "2024-11-30 01:39:16 UTC",
    "updated_date": "2024-11-30 01:39:16 UTC"
  },
  {
    "arxiv_id": "2412.00308v1",
    "title": "BOTS: Batch Bayesian Optimization of Extended Thompson Sampling for Severely Episode-Limited RL Settings",
    "authors": [
      "Karine Karine",
      "Susan A. Murphy",
      "Benjamin M. Marlin"
    ],
    "abstract": "In settings where the application of reinforcement learning (RL) requires\nrunning real-world trials, including the optimization of adaptive health\ninterventions, the number of episodes available for learning can be severely\nlimited due to cost or time constraints. In this setting, the bias-variance\ntrade-off of contextual bandit methods can be significantly better than that of\nmore complex full RL methods. However, Thompson sampling bandits are limited to\nselecting actions based on distributions of immediate rewards. In this paper,\nwe extend the linear Thompson sampling bandit to select actions based on a\nstate-action utility function consisting of the Thompson sampler's estimate of\nthe expected immediate reward combined with an action bias term. We use batch\nBayesian optimization over episodes to learn the action bias terms with the\ngoal of maximizing the expected return of the extended Thompson sampler. The\nproposed approach is able to learn optimal policies for a strictly broader\nclass of Markov decision processes (MDPs) than standard Thompson sampling.\nUsing an adaptive intervention simulation environment that captures key aspects\nof behavioral dynamics, we show that the proposed method can significantly\nout-perform standard Thompson sampling in terms of total return, while\nrequiring significantly fewer episodes than standard value function and policy\ngradient methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024 Workshop on Bayesian Decision-making and\n  Uncertainty",
    "pdf_url": "http://arxiv.org/pdf/2412.00308v1",
    "published_date": "2024-11-30 01:27:44 UTC",
    "updated_date": "2024-11-30 01:27:44 UTC"
  },
  {
    "arxiv_id": "2412.00300v1",
    "title": "PlanCritic: Formal Planning with Human Feedback",
    "authors": [
      "Owen Burns",
      "Dana Hughes",
      "Katia Sycara"
    ],
    "abstract": "Real world planning problems are often too complex to be effectively tackled\nby a single unaided human. To alleviate this, some recent work has focused on\ndeveloping a collaborative planning system to assist humans in complex domains,\nwith bridging the gap between the system's problem representation and the real\nworld being a key consideration. Transferring the speed and correctness formal\nplanners provide to real-world planning problems is greatly complicated by the\ndynamic and online nature of such tasks. Formal specifications of task and\nenvironment dynamics frequently lack constraints on some behaviors or goal\nconditions relevant to the way a human operator prefers a plan to be carried\nout. While adding constraints to the representation with the objective of\nincreasing its realism risks slowing down the planner, we posit that the same\nbenefits can be realized without sacrificing speed by modeling this problem as\nan online preference learning task. As part of a broader cooperative planning\nsystem, we present a feedback-driven plan critic. This method makes use of\nreinforcement learning with human feedback in conjunction with a genetic\nalgorithm to directly optimize a plan with respect to natural-language user\npreferences despite the non-differentiability of traditional planners. Directly\noptimizing the plan bridges the gap between research into more efficient\nplanners and research into planning with language models by utilizing the\nconvenience of natural language to guide the output of formal planners. We\ndemonstrate the effectiveness of our plan critic at adhering to user\npreferences on a disaster recovery task, and observe improved performance\ncompared to an llm-only neurosymbolic approach.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00300v1",
    "published_date": "2024-11-30 00:58:48 UTC",
    "updated_date": "2024-11-30 00:58:48 UTC"
  },
  {
    "arxiv_id": "2412.00293v1",
    "title": "Adaptformer: Sequence models as adaptive iterative planners",
    "authors": [
      "Akash Karthikeyan",
      "Yash Vardhan Pant"
    ],
    "abstract": "Despite recent advances in learning-based behavioral planning for autonomous\nsystems, decision-making in multi-task missions remains a challenging problem.\nFor instance, a mission might require a robot to explore an unknown\nenvironment, locate the goals, and navigate to them, even if there are\nobstacles along the way. Such problems are difficult to solve due to: a) sparse\nrewards, meaning a reward signal is available only once all the tasks in a\nmission have been satisfied, and b) the agent having to perform tasks at\nrun-time that are not covered in the training data, e.g., demonstrations only\nfrom an environment where all doors were unlocked. Consequently,\nstate-of-the-art decision-making methods in such settings are limited to\nmissions where the required tasks are well-represented in the training\ndemonstrations and can be solved within a short planning horizon. To overcome\nthese limitations, we propose Adaptformer, a stochastic and adaptive planner\nthat utilizes sequence models for sample-efficient exploration and\nexploitation. This framework relies on learning an energy-based heuristic,\nwhich needs to be minimized over a sequence of high-level decisions. To\ngenerate successful action sequences for long-horizon missions, Adaptformer\naims to achieve shorter sub-goals, which are proposed through an intrinsic\nsub-goal curriculum. Through these two key components, Adaptformer allows for\ngeneralization to out-of-distribution tasks and environments, i.e., missions\nthat were not a part of the training data. Empirical results in multiple\nsimulation environments demonstrate the effectiveness of our method. Notably,\nAdaptformer not only outperforms the state-of-the-art method by up to 25% in\nmulti-goal maze reachability tasks but also successfully adapts to multi-task\nmissions that the state-of-the-art method could not complete, leveraging\ndemonstrations from single-goal-reaching tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://aku02.github.io/projects/adaptformer",
    "pdf_url": "http://arxiv.org/pdf/2412.00293v1",
    "published_date": "2024-11-30 00:34:41 UTC",
    "updated_date": "2024-11-30 00:34:41 UTC"
  },
  {
    "arxiv_id": "2412.04497v2",
    "title": "Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research",
    "authors": [
      "Tianyang Zhong",
      "Zhenyuan Yang",
      "Zhengliang Liu",
      "Ruidong Zhang",
      "Yiheng Liu",
      "Haiyang Sun",
      "Yi Pan",
      "Yiwei Li",
      "Yifan Zhou",
      "Hanqi Jiang",
      "Junhao Chen",
      "Tianming Liu"
    ],
    "abstract": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04497v2",
    "published_date": "2024-11-30 00:10:56 UTC",
    "updated_date": "2024-12-09 03:00:42 UTC"
  },
  {
    "arxiv_id": "2412.00290v1",
    "title": "Adapting the re-ID challenge for static sensors",
    "authors": [
      "Avirath Sundaresan",
      "Jason R. Parham",
      "Jonathan Crall",
      "Rosemary Warungu",
      "Timothy Muthami",
      "Margaret Mwangi",
      "Jackson Miliko",
      "Jason Holmberg",
      "Tanya Y. Berger-Wolf",
      "Daniel Rubenstein",
      "Charles V. Stewart",
      "Sara Beery"
    ],
    "abstract": "In both 2016 and 2018, a census of the highly-endangered Grevy's zebra\npopulation was enabled by the Great Grevy's Rally (GGR), a citizen science\nevent that produces population estimates via expert and algorithmic curation of\nvolunteer-captured images. A complementary, scalable, and long-term Grevy's\npopulation monitoring approach involves deploying camera trap networks.\nHowever, in both scenarios, a substantial majority of zebra images are not\nusable for individual identification due to poor in-the-wild imaging\nconditions; camera trap images in particular present high rates of occlusion\nand high spatio-temporal similarity within image bursts. Our proposed filtering\npipeline incorporates animal detection, species identification, viewpoint\nestimation, quality evaluation, and temporal subsampling to obtain individual\ncrops suitable for re-ID, which are subsequently curated by the LCA decision\nmanagement algorithm. Our method processed images taken during GGR-16 and\nGGR-18 in Meru County, Kenya, into 4,142 highly-comparable annotations,\nrequiring only 120 contrastive human decisions to produce a population estimate\nwithin 4.6% of the ground-truth count. Our method also efficiently processed\n8.9M unlabeled camera trap images from 70 cameras at the Mpala Research Centre\nin Laikipia County, Kenya over two years into 685 encounters of 173\nindividuals, requiring only 331 contrastive human decisions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 11 figures. Submitted to the IET Computer Vision Special\n  Issue on Camera Traps, AI, and Ecology. Extended version of a workshop paper\n  presented at Camera Traps, AI, and Ecology 2023",
    "pdf_url": "http://arxiv.org/pdf/2412.00290v1",
    "published_date": "2024-11-30 00:00:29 UTC",
    "updated_date": "2024-11-30 00:00:29 UTC"
  }
]
{
  "date": "2024-12-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-29 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 43 篇论文，主要聚焦于人工智能和机器学习领域，包括大型语言模型 (LLMs) 的鲁棒性、in-context 学习、多目标 unlearning，以及时间序列分析和医疗 AI 等关键话题。令人印象深刻的是 LLM 相关研究，如 in-context 学习框架和多目标 unlearning 算法，以及 John Paparrizos 作为作者的多篇时间序列综述，展示了领域内系统性回顾和创新方法。\n\n下面，我将挑选最具话题度和影响力的论文优先讨论，将相关主题归纳在一起，并对其他论文快速概述。重点突出核心贡献、方法和发现。\n\n### LLM 和 AI 鲁棒性：热门领域的创新进展\n- **ICLR: In-Context Learning of Representations（ICLR: In-Context Learning of Representations）**  \n  这篇论文由 Core Francisco Park 等作者提出，探讨了 LLMs 如何通过 in-context 学习动态调整语义表示。核心贡献是证明扩展上下文大小能重塑模型表示，实现从预训练语义到任务特定语义的转变，实验显示在图结构任务中显著提升性能。该研究为 LLMs 解锁新能力提供了洞见，尤其在灵活适应动态环境的应用中。\n\n- **LLM2: Let Large Language Models Harness System 2 Reasoning（LLM2: Let Large Language Models Harness System 2 Reasoning）**  \n  Cheng Yang 等作者的工作引入了双系统框架，将 LLMs 与验证器结合，模拟人类认知的 System 2 推理。方法通过生成候选答案并使用验证器反馈，显著提高了数学推理任务的准确性（如 GSM8K 上提升 7.5%）。这篇论文强调了 LLMs 在复杂任务中的潜力，并展示了如何通过熵最大化增强鲁棒性。\n\n- **Multi-Objective Large Language Model Unlearning（Multi-Objective Large Language Model Unlearning）**  \n  Zibin Pan 等作者提出 MOLLM 算法，将 LLM unlearning 转化为多目标优化问题，解决了梯度爆炸和灾难性遗忘问题。贡献包括改进的交叉熵损失和加权更新，实验显示在 unlearning 效果和模型效用保留上优于现有方法。该研究对 AI 安全至关重要，帮助消除模型中的有害知识。\n\n- **Natural Language Fine-Tuning（Natural Language Fine-Tuning）**  \n  Jia Liu 等作者的 NLFT 方法首次使用自然语言指导 LLM 微调，显著减少了数据需求和计算成本。核心发现是，通过概率计算和显著性标记，NLFT 在 GSM8K 数据集上仅用 50 个样本就比标准 SFT 提升 219% 准确率。该框架为资源有限的边缘设备优化 LLM 提供了实用路径。\n\n其他 LLM 相关论文，如 **Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection（Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection）** 和 **Adversarial Robustness of Language Models in Transfer Learning（Adversarial Robustness of Language Models in Transfer Learning）**，揭示了 LLMs 在跨域任务中的脆弱性，并通过指导性提示和子群分析减少了 OOD 差距。这些工作突出了 AI 安全和泛化能力的挑战，但整体影响不如上述创新方法。\n\n### 时间序列分析：John Paparrizos 的系统性综述\nJohn Paparrizos 作为主要作者的多篇论文提供了时间序列领域的全面回顾，值得关注。\n- **Bridging the Gap: A Decade Review of Time-Series Clustering Methods（Bridging the Gap: A Decade Review of Time-Series Clustering Methods）**  \n  这篇综述构建了统一分类体系，从经典方法到深度学习算法，分析了时间序列聚类的发展。关键发现是，深度学习算法在处理高维数据时更有效，但传统方法在解释性上更强。该论文为未来研究提供了指导，桥接了传统与新兴技术的差距。\n\n- **A Survey on Time-Series Distance Measures（A Survey on Time-Series Distance Measures）**  \n  同样由 Paparrizos 领导，这篇调查覆盖了 100 多种距离度量（如 lock-step 和 elastic measures），并分类讨论了它们在单变量和多变量场景中的应用。贡献在于数学框架和效率分析，帮助研究者选择合适度量。该工作与上文综述互补，提升了时间序列任务的分析工具。\n\n其他时间序列论文，如 **Dive into Time-Series Anomaly Detection: A Decade Review（Dive into Time-Series Anomaly Detection: A Decade Review）** 和 **Stratify: Unifying Multi-Step Forecasting Strategies（Stratify: Unifying Multi-Step Forecasting Strategies）**，分别提供了异常检测综述和多步预测框架。后者引入了参数化策略，在 18 个数据集上提升了 84% 的实验性能，但这些相对学术性强，不如 Paparrizos 的工作话题度高。\n\n### 医疗和机器人应用：实用创新\n- **A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data（A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data）**  \n  Seungyeon Lee 等作者的 STEDR 框架结合子群分析和治疗效果估计，针对阿尔茨海默病识别 14 种药物候选。核心发现是通过真实世界数据模拟临床试验，提升了精确药物重用性能。该研究在医疗 AI 中有实际影响，促进了个性化治疗。\n\n- **Predicting Preschoolers' Externalizing Problems with Mother-Child Interaction Dynamics and Deep Learning（Predicting Preschoolers' Externalizing Problems with Mother-Child Interaction Dynamics and Deep Learning）**  \n  Xi Chen 等作者使用深度学习模型（如 ASBIM）预测儿童外部化问题，结合母子互动动态和抑制控制特征，准确率通过交叉验证得到提升。贡献在于将动态互动转化为预测工具，这对儿童心理健康干预有启发。\n\n其他医疗论文，如 **Segmentation of Muscularis Propria in Colon Histopathology Images Using Vision Transformers for Hirschsprung's Disease（Segmentation of Muscularis Propria in Colon Histopathology Images Using Vision Transformers for Hirschsprung's Disease）**，使用 Vision Transformers 实现了 89.9% 的 DICE 分数，提升了病理图像分割，但相对专业，不再详细展开。\n\n### 快速概述其他论文\n剩余论文多为特定领域探索，我仅快速提炼核心点：\n- **Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models（Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models）**：提出 HALLUCINOGEN 基准测试 LVLMs 的幻觉问题，评估了 11 个模型的鲁棒性。\n- **Enhancing autonomous vehicle safety in rain: a data-centric approach for clear vision（Enhancing autonomous vehicle safety in rain: a data-centric approach for clear vision）**：使用深度学习改善雨中视觉，显著提升转向准确性。\n- **Game Theory and Multi-Agent Reinforcement Learning: From Nash Equilibria to Evolutionary Dynamics（Game Theory and Multi-Agent Reinforcement Learning: From Nash Equilibria to Evolutionary Dynamics）**：整合博弈论和 MARL，解决了多代理非平稳性问题。\n- **Attacks on the neural network and defense methods（Attacks on the neural network and defense methods）** 和 **Ensemble of classifiers for speech evaluation（Ensemble of classifiers for speech evaluation）**：分别讨论神经网络攻击防御和语音评估集成分类器，但这些主题较常规，未见重大突破。\n- 其他如财务 AI、机器人和生成模型论文（如 **The Synergy of Automated Pipelines with Prompt Engineering and Generative AI in Web Crawling**），虽有创新（如结合提示工程提升网络爬取），但影响力有限，仅适用于 niche 场景。\n\n总之，今天的 arXiv 论文突显了 AI 领域的快速演进，LLM 和时间序列分析是亮点。读者可关注 LLM2 和 Paparrizos 的工作，以探索实际应用。如果有特定兴趣，建议查看相关代码链接深入研究！明天见，持续追踪最新进展。",
  "papers": [
    {
      "arxiv_id": "2412.20622v2",
      "title": "Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ashish Seth",
        "Dinesh Manocha",
        "Chirag Agarwal"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nin complex multimodal tasks. However, these models still suffer from\nhallucinations, particularly when required to implicitly recognize or infer\ndiverse visual entities from images for complex vision-language tasks. To\naddress this challenge, we propose HALLUCINOGEN, a novel visual question\nanswering (VQA) benchmark that employs contextual reasoning prompts as\nhallucination attacks to evaluate the extent of hallucination in\nstate-of-the-art LVLMs. Our benchmark provides a comprehensive study of the\nimplicit reasoning capabilities of these models by first categorizing visual\nentities based on the ease of recognition in an image as either salient\n(prominent, visibly recognizable objects such as a car) or latent entities\n(such as identifying a disease from a chest X-ray), which are not readily\nvisible and require domain knowledge or contextual reasoning for accurate\ninference. Next, we design hallucination attacks for both types of entities to\nassess hallucinations in LVLMs while performing various vision-language tasks,\nsuch as locating or reasoning about specific entities within an image, where\nmodels must perform implicit reasoning by verifying the existence of the\nqueried entity within the image before generating responses. Finally, our\nextensive evaluations of eleven LVLMs, including powerful open-source models\n(like LLaMA-3.2 and DeepSeek-V2), commercial models like Gemini, and two\nhallucination mitigation strategies across multiple datasets, demonstrate that\ncurrent LVLMs remain susceptible to hallucination attacks.",
      "tldr_zh": "这篇论文针对 Large Vision-Language Models (LVLMs) 在处理图像实体时存在的幻觉问题，提出了一种系统评估框架。研究者开发了 HALLUCINOGEN 基准，这是一个视觉问答 (VQA) 测试集，通过上下文推理提示作为幻觉攻击，评估模型对显著实体 (如显眼物体) 和潜在实体 (如需领域知识的推理对象) 的隐式推理能力。实验结果显示，在评估 11 个 LVLMs（包括 LLaMA-3.2、DeepSeek-V2 和 Gemini 等模型）以及两种幻觉缓解策略后，这些模型在多种视觉语言任务中仍高度易受幻觉攻击。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20622v2",
      "published_date": "2024-12-29 23:56:01 UTC",
      "updated_date": "2025-03-13 23:10:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:31:47.345263"
    },
    {
      "arxiv_id": "2412.20612v1",
      "title": "Towards Explaining Uncertainty Estimates in Point Cloud Registration",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyuan Qin",
        "Jongseok Lee",
        "Rudolph Triebel"
      ],
      "abstract": "Iterative Closest Point (ICP) is a commonly used algorithm to estimate\ntransformation between two point clouds. The key idea of this work is to\nleverage recent advances in explainable AI for probabilistic ICP methods that\nprovide uncertainty estimates. Concretely, we propose a method that can explain\nwhy a probabilistic ICP method produced a particular output. Our method is\nbased on kernel SHAP (SHapley Additive exPlanations). With this, we assign an\nimportance value to common sources of uncertainty in ICP such as sensor noise,\nocclusion, and ambiguous environments. The results of the experiment show that\nthis explanation method can reasonably explain the uncertainty sources,\nproviding a step towards robots that know when and why they failed in a human\ninterpretable manner",
      "tldr_zh": "这篇论文针对点云注册中的不确定性问题，提出了一种基于kernel SHAP（SHapley Additive exPlanations）的方法，用于解释概率ICP（Iterative Closest Point）算法的输出。方法通过分配重要性值来分析ICP常见不确定性来源，如传感器噪声、遮挡和模糊环境，从而揭示为什么算法产生特定结果。实验结果显示，该解释方法能有效识别不确定性原因，帮助机器人以人类可解释的方式判断失败时机，并为可解释AI在机器人应用中奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20612v1",
      "published_date": "2024-12-29 23:03:44 UTC",
      "updated_date": "2024-12-29 23:03:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:31:57.672895"
    },
    {
      "arxiv_id": "2412.20601v1",
      "title": "MATEY: multiscale adaptive foundation models for spatiotemporal physical systems",
      "title_zh": "翻译失败",
      "authors": [
        "Pei Zhang",
        "M. Paul Laiu",
        "Matthew Norman",
        "Doug Stefanski",
        "John Gounley"
      ],
      "abstract": "Accurate representation of the multiscale features in spatiotemporal physical\nsystems using vision transformer (ViT) architectures requires extremely long,\ncomputationally prohibitive token sequences. To address this issue, we propose\ntwo adaptive tokenization schemes that dynamically adjust patch sizes based on\nlocal features: one ensures convergent behavior to uniform patch refinement,\nwhile the other offers better computational efficiency. Moreover, we present a\nset of spatiotemporal attention schemes, where the temporal or axial spatial\ndimensions are decoupled, and evaluate their computational and data\nefficiencies. We assess the performance of the proposed multiscale adaptive\nmodel, MATEY, in a sequence of experiments. The results show that adaptive\ntokenization schemes achieve improved accuracy without significantly increasing\nthe length of the token sequence. Compared to a full spatiotemporal attention\nscheme or a scheme that decouples only the temporal dimension, we find that\nfully decoupled axial attention is less efficient and expressive, requiring\nmore training time and model weights to achieve the same accuracy. Finally, we\ndemonstrate in two fine-tuning tasks featuring different physics that models\npretrained on PDEBench data outperform the ones trained from scratch,\nespecially in the low data regime with frozen attention.",
      "tldr_zh": "该研究提出MATEY，一种多尺度自适应基础模型，用于时空物理系统，以解决Vision Transformer (ViT)架构在处理长token序列的计算挑战。模型引入两种自适应tokenization方案，根据局部特征动态调整patch大小，同时提出时空注意力方案，通过解耦时间或轴向空间维度来提升计算和数据效率。实验结果显示，自适应tokenization提高了准确性而不显著增加token序列长度，而完全解耦的轴向注意力方案虽灵活但效率较低；在微调任务中，使用PDEBench数据预训练的模型在低数据环境下表现出色，尤其当注意力被冻结时。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20601v1",
      "published_date": "2024-12-29 22:13:16 UTC",
      "updated_date": "2024-12-29 22:13:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:32:10.516306"
    },
    {
      "arxiv_id": "2412.20595v1",
      "title": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Dmitri Roussinov",
        "Serge Sharoff",
        "Nadezhda Puchnina"
      ],
      "abstract": "This study demonstrates that the modern generation of Large Language Models\n(LLMs, such as GPT-4) suffers from the same out-of-domain (OOD) performance gap\nobserved in prior research on pre-trained Language Models (PLMs, such as BERT).\nWe demonstrate this across two non-topical classification tasks: 1) genre\nclassification and 2) generated text detection. Our results show that when\ndemonstration examples for In-Context Learning (ICL) come from one domain\n(e.g., travel) and the system is tested on another domain (e.g., history),\nclassification performance declines significantly.\n  To address this, we introduce a method that controls which predictive\nindicators are used and which are excluded during classification. For the two\ntasks studied here, this ensures that topical features are omitted, while the\nmodel is guided to focus on stylistic rather than content-based attributes.\nThis approach reduces the OOD gap by up to 20 percentage points in a few-shot\nsetup. Straightforward Chain-of-Thought (CoT) methods, used as the baseline,\nprove insufficient, while our approach consistently enhances domain transfer\nperformance.",
      "tldr_zh": "本研究发现，现代大型语言模型 (LLMs，如 GPT-4) 在体裁分类 (genre classification) 和生成文本检测 (generated text detection) 等非主题任务中，存在与预训练语言模型 (PLMs) 相似的领域外 (OOD) 性能差距，特别是当 In-Context Learning (ICL) 示例来自不同领域时，分类准确率显著下降。  \n为解决这一问题，研究提出了一种控制预测指标的方法，排除主题特征并引导模型专注于风格属性，从而提升跨领域性能。  \n实验结果显示，该方法在少样本设置中将 OOD 差距减少多达 20 个百分点，并优于基线 Chain-of-Thought (CoT) 方法，为改进 LLMs 的领域转移能力提供了有效途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The 31st International Conference on Computational Linguistics",
      "pdf_url": "http://arxiv.org/pdf/2412.20595v1",
      "published_date": "2024-12-29 21:54:39 UTC",
      "updated_date": "2024-12-29 21:54:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:32:23.417142"
    },
    {
      "arxiv_id": "2412.20588v2",
      "title": "Kryptonite-N: Machine Learning Strikes Back",
      "title_zh": "Kryptonite-N：机器学习反击",
      "authors": [
        "Albus Li",
        "Nathan Bailey",
        "Will Sumerfield",
        "Kira Kim"
      ],
      "abstract": "Quinn et al propose challenge datasets in their work called ``Kryptonite-N\".\nThese datasets aim to counter the universal function approximation argument of\nmachine learning, breaking the notation that machine learning can ``approximate\nany continuous function\" \\cite{original_paper}. Our work refutes this claim and\nshows that universal function approximations can be applied successfully; the\nKryptonite datasets are constructed predictably, allowing logistic regression\nwith sufficient polynomial expansion and L1 regularization to solve for any\ndimension N.",
      "tldr_zh": "本文针对Quinn et al.提出的Kryptonite-N挑战数据集，该数据集旨在挑战机器学习的通用函数逼近(universal function approximation)假设，认为机器学习无法逼近所有连续函数。作者通过使用logistic regression结合足够的polynomial expansion和L1 regularization，证明这些数据集是可预测构建的，从而成功解决了任意维度N的问题。最终，这反驳了原假设，展示了机器学习的强大适用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20588v2",
      "published_date": "2024-12-29 21:23:09 UTC",
      "updated_date": "2025-01-26 18:22:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:32:35.450148"
    },
    {
      "arxiv_id": "2412.20582v1",
      "title": "Bridging the Gap: A Decade Review of Time-Series Clustering Methods",
      "title_zh": "翻译失败",
      "authors": [
        "John Paparrizos",
        "Fan Yang",
        "Haojun Li"
      ],
      "abstract": "Time series, as one of the most fundamental representations of sequential\ndata, has been extensively studied across diverse disciplines, including\ncomputer science, biology, geology, astronomy, and environmental sciences. The\nadvent of advanced sensing, storage, and networking technologies has resulted\nin high-dimensional time-series data, however, posing significant challenges\nfor analyzing latent structures over extended temporal scales. Time-series\nclustering, an established unsupervised learning strategy that groups similar\ntime series together, helps unveil hidden patterns in these complex datasets.\nIn this survey, we trace the evolution of time-series clustering methods from\nclassical approaches to recent advances in neural networks. While previous\nsurveys have focused on specific methodological categories, we bridge the gap\nbetween traditional clustering methods and emerging deep learning-based\nalgorithms, presenting a comprehensive, unified taxonomy for this research\narea. This survey highlights key developments and provides insights to guide\nfuture research in time-series clustering.",
      "tldr_zh": "本论文回顾了过去十年时间-series clustering 方法的发展，桥接了传统聚类技术和新兴深度学习算法之间的差距。作者从经典方法到神经网络的最新进展进行全面分析，提出一个统一的分类法，以揭示高维时间序列数据中的隐藏模式。该调查强调了关键进展，并为未来时间-series clustering 研究提供指导方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20582v1",
      "published_date": "2024-12-29 21:04:35 UTC",
      "updated_date": "2024-12-29 21:04:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:32:46.152131"
    },
    {
      "arxiv_id": "2412.20574v1",
      "title": "A Survey on Time-Series Distance Measures",
      "title_zh": "翻译失败",
      "authors": [
        "John Paparrizos",
        "Haojun Li",
        "Fan Yang",
        "Kaize Wu",
        "Jens E. d'Hondt",
        "Odysseas Papapetrou"
      ],
      "abstract": "Distance measures have been recognized as one of the fundamental building\nblocks in time-series analysis tasks, e.g., querying, indexing, classification,\nclustering, anomaly detection, and similarity search. The vast proliferation of\ntime-series data across a wide range of fields has increased the relevance of\nevaluating the effectiveness and efficiency of these distance measures. To\nprovide a comprehensive view of this field, this work considers over 100\nstate-of-the-art distance measures, classified into 7 categories: lock-step\nmeasures, sliding measures, elastic measures, kernel measures, feature-based\nmeasures, model-based measures, and embedding measures. Beyond providing\ncomprehensive mathematical frameworks, this work also delves into the\ndistinctions and applications across these categories for both univariate and\nmultivariate cases. By providing comprehensive collections and insights, this\nstudy paves the way for the future development of innovative time-series\ndistance measures.",
      "tldr_zh": "这篇论文对时间序列距离测度进行了全面调查，涵盖超过100种最先进的测度，并将其分类为7类：lock-step measures、sliding measures、elastic measures、kernel measures、feature-based measures、model-based measures和embedding measures。\n论文提供了这些测度的数学框架，并分析了它们在单变量和多变量时间序列分析任务中的区别和应用，包括查询、索引、分类、聚类、异常检测和相似性搜索。\n通过这个系统的收集和见解，该研究为评估距离测度的有效性和效率提供了基础，并为未来创新时间序列距离测度的发展铺平了道路。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20574v1",
      "published_date": "2024-12-29 20:47:08 UTC",
      "updated_date": "2024-12-29 20:47:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:32:58.880060"
    },
    {
      "arxiv_id": "2412.20573v1",
      "title": "The intrinsic motivation of reinforcement and imitation learning for sequential tasks",
      "title_zh": "强化学习和模仿学习的内在动机，用于顺序任务",
      "authors": [
        "Sao Mai Nguyen"
      ],
      "abstract": "This work in the field of developmental cognitive robotics aims to devise a\nnew domain bridging between reinforcement learning and imitation learning, with\na model of the intrinsic motivation for learning agents to learn with guidance\nfrom tutors multiple tasks, including sequential tasks. The main contribution\nhas been to propose a common formulation of intrinsic motivation based on\nempirical progress for a learning agent to choose automatically its learning\ncurriculum by actively choosing its learning strategy for simple or sequential\ntasks: which task to learn, between autonomous exploration or imitation\nlearning, between low-level actions or task decomposition, between several\ntutors. The originality is to design a learner that benefits not only passively\nfrom data provided by tutors, but to actively choose when to request tutoring\nand what and whom to ask. The learner is thus more robust to the quality of the\ntutoring and learns faster with fewer demonstrations. We developed the\nframework of socially guided intrinsic motivation with machine learning\nalgorithms to learn multiple tasks by taking advantage of the generalisability\nproperties of human demonstrations in a passive manner or in an active manner\nthrough requests of demonstrations from the best tutor for simple and composing\nsubtasks. The latter relies on a representation of subtask composition proposed\nfor a construction process, which should be refined by representations used for\nobservational processes of analysing human movements and activities of daily\nliving. With the outlook of a language-like communication with the tutor, we\ninvestigated the emergence of a symbolic representation of the continuous\nsensorimotor space and of tasks using intrinsic motivation. We proposed within\nthe reinforcement learning framework, a reward function for interacting with\ntutors for automatic curriculum learning in multi-task learning.",
      "tldr_zh": "本研究旨在桥接强化学习（reinforcement learning）和模仿学习（imitation learning），通过内在动机（intrinsic motivation）模型开发一种学习代理，用于从导师指导下学习多个任务，包括顺序任务。论文的主要贡献是提出一个基于经验进展的共同内在动机公式，让代理自动选择学习策略，如决定学习哪些任务、是进行自主探索还是模仿学习、处理低级动作或任务分解，以及主动选择导师和请求指导。相比被动学习，这种框架使代理更鲁棒、更高效，仅需较少演示即可更快学习多任务，并探讨了符号表示和奖励函数以支持自动课程学习（curriculum learning）和潜在的语言-like沟通。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO",
        "I.2.6; I.2.9"
      ],
      "primary_category": "cs.AI",
      "comment": "Habilitation thesis",
      "pdf_url": "http://arxiv.org/pdf/2412.20573v1",
      "published_date": "2024-12-29 20:44:59 UTC",
      "updated_date": "2024-12-29 20:44:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:33:11.088011"
    },
    {
      "arxiv_id": "2412.20571v1",
      "title": "Segmentation of Muscularis Propria in Colon Histopathology Images Using Vision Transformers for Hirschsprung's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Youssef Megahed",
        "Anthony Fuller",
        "Saleh Abou-Alwan",
        "Dina El Demellawy",
        "Adrian D. C. Chan"
      ],
      "abstract": "Hirschsprung's disease (HD) is a congenital birth defect diagnosed by\nidentifying the lack of ganglion cells within the colon's muscularis propria,\nspecifically within the myenteric plexus regions. There may be advantages for\nquantitative assessments of histopathology images of the colon, such as\ncounting the ganglion and assessing their spatial distribution; however, this\nwould be time-intensive for pathologists, costly, and subject to inter- and\nintra-rater variability. Previous research has demonstrated the potential for\ndeep learning approaches to automate histopathology image analysis, including\nsegmentation of the muscularis propria using convolutional neural networks\n(CNNs). Recently, Vision Transformers (ViTs) have emerged as a powerful deep\nlearning approach due to their self-attention. This study explores the\napplication of ViTs for muscularis propria segmentation in calretinin-stained\nhistopathology images and compares their performance to CNNs and shallow\nlearning methods. The ViT model achieved a DICE score of 89.9% and Plexus\nInclusion Rate (PIR) of 100%, surpassing the CNN (DICE score of 89.2%; PIR of\n96.0%) and k-means clustering method (DICE score of 80.7%; PIR 77.4%). Results\nassert that ViTs are a promising tool for advancing HD-related image analysis.",
      "tldr_zh": "本文研究了使用 Vision Transformers (ViTs) 对结肠组织病理图像中的 muscularis propria 进行分割，以诊断 Hirschsprung's disease (HD)，旨在自动化神经节细胞检测并减少传统方法的耗时和主观性。相比于 CNN (DICE score 89.2%, Plexus Inclusion Rate (PIR) 96.0%) 和 k-means clustering (DICE score 80.7%, PIR 77.4%) 等方法，ViTs 模型在 calretinin 染色图像上取得了更高的性能，包括 DICE score 89.9% 和 PIR 100%。这项工作证明了 ViTs 在 HD 相关图像分析中的潜力，有助于提高诊断效率和准确性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "To be published in the CMBEC47/ACCES26 Joint Conference",
      "pdf_url": "http://arxiv.org/pdf/2412.20571v1",
      "published_date": "2024-12-29 20:43:43 UTC",
      "updated_date": "2024-12-29 20:43:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:33:23.774204"
    },
    {
      "arxiv_id": "2412.20565v1",
      "title": "Enhancing autonomous vehicle safety in rain: a data-centric approach for clear vision",
      "title_zh": "翻译失败",
      "authors": [
        "Mark A. Seferian",
        "Jidong J. Yang"
      ],
      "abstract": "Autonomous vehicles face significant challenges in navigating adverse\nweather, particularly rain, due to the visual impairment of camera-based\nsystems. In this study, we leveraged contemporary deep learning techniques to\nmitigate these challenges, aiming to develop a vision model that processes live\nvehicle camera feeds to eliminate rain-induced visual hindrances, yielding\nvisuals closely resembling clear, rain-free scenes. Using the Car Learning to\nAct (CARLA) simulation environment, we generated a comprehensive dataset of\nclear and rainy images for model training and testing. In our model, we\nemployed a classic encoder-decoder architecture with skip connections and\nconcatenation operations. It was trained using novel batching schemes designed\nto effectively distinguish high-frequency rain patterns from low-frequency\nscene features across successive image frames. To evaluate the model\nperformance, we integrated it with a steering module that processes front-view\nimages as input. The results demonstrated notable improvements in steering\naccuracy, underscoring the model's potential to enhance navigation safety and\nreliability in rainy weather conditions.",
      "tldr_zh": "本研究针对自动驾驶车辆在雨天视觉障碍问题，提出了一种数据驱动的方法，使用深度学习技术开发视觉模型，以清除雨水干扰并生成类似晴天场景的图像。模型采用经典的 encoder-decoder 架构，结合 skip connections 和 concatenation operations，并在 CARLA 模拟环境中生成的数据集上训练，通过新型 batching schemes 区分高频雨模式和低频场景特征。实验结果显示，该模型与 steering module 整合后，显著提高了转向准确性，从而提升了雨天导航的安全性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 16 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.20565v1",
      "published_date": "2024-12-29 20:27:12 UTC",
      "updated_date": "2024-12-29 20:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:33:35.345767"
    },
    {
      "arxiv_id": "2501.00070v2",
      "title": "ICLR: In-Context Learning of Representations",
      "title_zh": "ICLR：上下文内表示的学习",
      "authors": [
        "Core Francisco Park",
        "Andrew Lee",
        "Ekdeep Singh Lubana",
        "Yongyi Yang",
        "Maya Okawa",
        "Kento Nishi",
        "Martin Wattenberg",
        "Hidenori Tanaka"
      ],
      "abstract": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLM)是否能通过in-context learning重新组织表示，以适应上下文指定的语义，而不是依赖预训练语义。研究者设计了一个玩具任务“graph tracing”，其中图节点使用训练概念（如apple, bird），并通过提供随机游走轨迹的上下文示例，分析模型的中间表示。结果显示，随着上下文规模增加，模型表示会突然从预训练语义重组为与图结构对齐的表示；然而，如果参考概念有语义相关性（如Monday, Tuesday），则上下文结构虽存在但无法主导预训练结构。这些发现支持一个隐式优化过程，表明扩展上下文大小可灵活重组模型表示，可能解锁新型能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.00070v2",
      "published_date": "2024-12-29 18:58:09 UTC",
      "updated_date": "2025-05-02 05:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:33:47.816569"
    },
    {
      "arxiv_id": "2501.00069v1",
      "title": "Adversarial Negotiation Dynamics in Generative Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Arinbjörn Kolbeinsson",
        "Benedikt Kolbeinsson"
      ],
      "abstract": "Generative language models are increasingly used for contract drafting and\nenhancement, creating a scenario where competing parties deploy different\nlanguage models against each other. This introduces not only a game-theory\nchallenge but also significant concerns related to AI safety and security, as\nthe language model employed by the opposing party can be unknown. These\ncompetitive interactions can be seen as adversarial testing grounds, where\nmodels are effectively red-teamed to expose vulnerabilities such as generating\nbiased, harmful or legally problematic text. Despite the importance of these\nchallenges, the competitive robustness and safety of these models in\nadversarial settings remain poorly understood. In this small study, we approach\nthis problem by evaluating the performance and vulnerabilities of major\nopen-source language models in head-to-head competitions, simulating real-world\ncontract negotiations. We further explore how these adversarial interactions\ncan reveal potential risks, informing the development of more secure and\nreliable models. Our findings contribute to the growing body of research on AI\nsafety, offering insights into model selection and optimisation in competitive\nlegal contexts and providing actionable strategies for mitigating risks.",
      "tldr_zh": "这篇论文探讨了生成式语言模型(Generative Language Models)在合同起草等竞争场景中的对抗性谈判动态，强调了博弈论挑战以及AI安全和安全风险，因为对手模型未知可能导致生成偏见、有害或法律问题的文本。研究通过模拟真实合同谈判，进行头对头竞争来评估主要开源语言模型的性能和漏洞，揭示这些互动如何暴露模型弱点。最终，论文为AI安全研究提供见解，包括模型选择、优化策略和风险缓解方法，以提升模型在竞争性法律环境中的可靠性和安全性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper at NeurIPS 2024 Workshop on Red Teaming GenAI",
      "pdf_url": "http://arxiv.org/pdf/2501.00069v1",
      "published_date": "2024-12-29 18:17:55 UTC",
      "updated_date": "2024-12-29 18:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:33:59.150257"
    },
    {
      "arxiv_id": "2412.20529v1",
      "title": "Attacks on the neural network and defense methods",
      "title_zh": "针对神经网络的攻击与防御方法",
      "authors": [
        "A. Korenev",
        "G. Belokrylov",
        "B. Lodonova",
        "A. Novokhrestov"
      ],
      "abstract": "This article will discuss the use of attacks on a neural network trained on\naudio data, as well as possible methods of protection against these attacks.\nFGSM, PGD and CW attacks, as well as data poisoning, will be considered. Within\nthe framework of protection, Art-IBM and advertorch libraries will be\nconsidered. The obtained accuracy metrics within the framework of attack\napplications are presented",
      "tldr_zh": "这篇论文探讨了对在音频数据上训练的神经网络的攻击方法及其防护策略，重点分析了 FGSM、PGD、CW attacks 以及 data poisoning 等攻击类型。防护方面，论文评估了 Art-IBM 和 advertorch 库提供的防御机制。实验结果展示了这些攻击对模型准确性的影响，为提升神经网络的安全性提供了参考。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20529v1",
      "published_date": "2024-12-29 17:33:04 UTC",
      "updated_date": "2024-12-29 17:33:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:34:09.915140"
    },
    {
      "arxiv_id": "2501.00067v1",
      "title": "Ensemble of classifiers for speech evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "G. Belokrylov",
        "A. Korenev",
        "B. Lodonova",
        "A. Novokhrestov"
      ],
      "abstract": "The article describes an attempt to apply an ensemble of binary classifiers\nto solve the problem of speech assessment in medicine. A dataset was compiled\nbased on quantitative and expert assessments of syllable pronunciation quality.\nQuantitative assessments of 7 selected metrics were used as features: dynamic\ntime warp distance, Minkowski distance, correlation coefficient, longest common\nsubsequence (LCSS), edit distance of real se-quence (EDR), edit distance with\nreal penalty (ERP), and merge split (MSM). Expert as-sessment of pronunciation\nquality was used as a class label: class 1 means high-quality speech, class 0\nmeans distorted. A comparison of training results was carried out for five\nclassification methods: logistic regression (LR), support vector machine (SVM),\nnaive Bayes (NB), decision trees (DT), and K-nearest neighbors (KNN). The\nresults of using the mixture method to build an ensemble of classifiers are\nalso presented. The use of an en-semble for the studied data sets allowed us to\nslightly increase the classification accuracy compared to the use of individual\nbinary classifiers.",
      "tldr_zh": "这篇论文探讨了使用二元分类器集合来评估医疗领域的语音质量问题。研究者编译了一个数据集，基于音节发音的定量指标（如Dynamic Time Warp Distance、Minkowski Distance、Correlation Coefficient、Longest Common Subsequence (LCSS)、Edit Distance on Real Sequence (EDR)、Edit Distance with Real Penalty (ERP)和Merge Split (MSM)）作为特征，并以专家评估（高质量语音为类别1，低质量为0）作为标签。实验比较了五种分类方法，包括Logistic Regression (LR)、Support Vector Machine (SVM)、Naive Bayes (NB)、Decision Trees (DT)和K-Nearest Neighbors (KNN)。结果表明，使用混合方法构建的分类器集合比单个分类器略微提高了分类准确率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00067v1",
      "published_date": "2024-12-29 17:28:32 UTC",
      "updated_date": "2024-12-29 17:28:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:34:24.288308"
    },
    {
      "arxiv_id": "2502.15691v1",
      "title": "The Synergy of Automated Pipelines with Prompt Engineering and Generative AI in Web Crawling",
      "title_zh": "翻译失败",
      "authors": [
        "Chau-Jian Huang"
      ],
      "abstract": "Web crawling is a critical technique for extracting online data, yet it poses\nchallenges due to webpage diversity and anti-scraping mechanisms. This study\ninvestigates the integration of generative AI tools Claude AI (Sonnet 3.5) and\nChatGPT4.0 with prompt engineering to automate web scraping. Using two prompts,\nPROMPT I (general inference, tested on Yahoo News) and PROMPT II\n(element-specific, tested on Coupons.com), we evaluate the code quality and\nperformance of AI-generated scripts. Claude AI consistently outperformed\nChatGPT-4.0 in script quality and adaptability, as confirmed by predefined\nevaluation metrics, including functionality, readability, modularity, and\nrobustness. Performance data were collected through manual testing and\nstructured scoring by three evaluators. Visualizations further illustrate\nClaude AI's superiority. Anti-scraping solutions, including\nundetected_chromedriver, Selenium, and fake_useragent, were incorporated to\nenhance performance. This paper demonstrates how generative AI combined with\nprompt engineering can simplify and improve web scraping workflows.",
      "tldr_zh": "这篇论文探讨了生成式 AI（如 Claude AI 和 ChatGPT-4.0）与提示工程(prompt engineering)的协同作用，用于自动化网络爬虫(web crawling)，以应对网页多样性和反爬虫机制。研究通过 PROMPT I（一般推理，在 Yahoo News 测试）和 PROMPT II（元素特定，在 Coupons.com 测试）生成脚本，并评估脚本的质量和性能，包括功能性、读性、模块性和鲁棒性。结果显示，Claude AI 在这些指标上优于 ChatGPT-4.0，并通过整合 undetected_chromedriver、Selenium 和 fake_useragent 等工具，显著简化并提升了网络爬虫的工作流。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.15691v1",
      "published_date": "2024-12-29 17:27:55 UTC",
      "updated_date": "2024-12-29 17:27:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:36:36.200846"
    },
    {
      "arxiv_id": "2412.20523v1",
      "title": "Game Theory and Multi-Agent Reinforcement Learning : From Nash Equilibria to Evolutionary Dynamics",
      "title_zh": "博弈论与多智能体强化学习：",
      "authors": [
        "Neil De La Fuente",
        "Miquel Noguer i Alonso",
        "Guim Casadellà"
      ],
      "abstract": "This paper explores advanced topics in complex multi-agent systems building\nupon our previous work. We examine four fundamental challenges in Multi-Agent\nReinforcement Learning (MARL): non-stationarity, partial observability,\nscalability with large agent populations, and decentralized learning. The paper\nprovides mathematical formulations and analysis of recent algorithmic\nadvancements designed to address these challenges, with a particular focus on\ntheir integration with game-theoretic concepts. We investigate how Nash\nequilibria, evolutionary game theory, correlated equilibrium, and adversarial\ndynamics can be effectively incorporated into MARL algorithms to improve\nlearning outcomes. Through this comprehensive analysis, we demonstrate how the\nsynthesis of game theory and MARL can enhance the robustness and effectiveness\nof multi-agent systems in complex, dynamic environments.",
      "tldr_zh": "这篇论文探讨了多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）的四个核心挑战：非平稳性、部分可观测性、大规模代理人口的可扩展性以及去中心化学习，并提供了这些挑战的数学公式和算法进展。论文重点整合游戏理论概念，如Nash Equilibria、进化博弈理论、相关均衡和对抗动态，以优化MARL算法的学习效果。通过综合分析，研究证明了游戏理论与MARL的结合，能够显著提升多智能体系统在复杂动态环境中的稳健性和有效性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.20523v1",
      "published_date": "2024-12-29 17:15:40 UTC",
      "updated_date": "2024-12-29 17:15:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:34:47.044409"
    },
    {
      "arxiv_id": "2412.20519v1",
      "title": "Goal-Conditioned Data Augmentation for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xingshuai Huang",
        "Di Wu Member",
        "Benoit Boulet"
      ],
      "abstract": "Offline reinforcement learning (RL) enables policy learning from\npre-collected offline datasets, relaxing the need to interact directly with the\nenvironment. However, limited by the quality of offline datasets, it generally\nfails to learn well-qualified policies in suboptimal datasets. To address\ndatasets with insufficient optimal demonstrations, we introduce\nGoal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned\ndiffusion-based method for augmenting samples with higher quality. Leveraging\nrecent advancements in generative modeling, GODA incorporates a novel\nreturn-oriented goal condition with various selection mechanisms. Specifically,\nwe introduce a controllable scaling technique to provide enhanced return-based\nguidance during data sampling. GODA learns a comprehensive distribution\nrepresentation of the original offline datasets while generating new data with\nselectively higher-return goals, thereby maximizing the utility of limited\noptimal demonstrations. Furthermore, we propose a novel adaptive gated\nconditioning method for processing noised inputs and conditions, enhancing the\ncapture of goal-oriented guidance. We conduct experiments on the D4RL benchmark\nand real-world challenges, specifically traffic signal control (TSC) tasks, to\ndemonstrate GODA's effectiveness in enhancing data quality and superior\nperformance compared to state-of-the-art data augmentation methods across\nvarious offline RL algorithms.",
      "tldr_zh": "该论文针对离线强化学习（Offline RL）中数据集质量不足的问题，提出了一种名为 Goal-cOnditioned Data Augmentation (GODA) 的新型目标条件扩散方法，用于增强数据样本质量。GODA 通过引入返回导向的目标条件、可控缩放技术和自适应门控条件机制，学习原始数据集的全面分布并生成具有更高回报的新数据，从而最大化有限最优演示的利用价值。在 D4RL 基准和真实世界任务（如交通信号控制）上，实验证明 GODA 显著提升了数据质量，并在多种离线 RL 算法中比最先进方法表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20519v1",
      "published_date": "2024-12-29 16:42:30 UTC",
      "updated_date": "2024-12-29 16:42:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:34:59.229145"
    },
    {
      "arxiv_id": "2412.20512v1",
      "title": "Dive into Time-Series Anomaly Detection: A Decade Review",
      "title_zh": "深入时间序列异常检测：十年回顾",
      "authors": [
        "Paul Boniol",
        "Qinghua Liu",
        "Mingyi Huang",
        "Themis Palpanas",
        "John Paparrizos"
      ],
      "abstract": "Recent advances in data collection technology, accompanied by the ever-rising\nvolume and velocity of streaming data, underscore the vital need for time\nseries analytics. In this regard, time-series anomaly detection has been an\nimportant activity, entailing various applications in fields such as cyber\nsecurity, financial markets, law enforcement, and health care. While\ntraditional literature on anomaly detection is centered on statistical\nmeasures, the increasing number of machine learning algorithms in recent years\ncall for a structured, general characterization of the research methods for\ntime-series anomaly detection. This survey groups and summarizes anomaly\ndetection existing solutions under a process-centric taxonomy in the time\nseries context. In addition to giving an original categorization of anomaly\ndetection methods, we also perform a meta-analysis of the literature and\noutline general trends in time-series anomaly detection research.",
      "tldr_zh": "这篇论文回顾了过去十年的 time-series anomaly detection 研究，强调了数据收集技术进步带来的数据量和速度增加，以及其在网络安全、金融市场、执法和医疗保健等领域的关键应用。作者提出了一种基于过程的分类法(process-centric taxonomy)，将现有解决方案归类并比较传统统计方法与新兴机器学习算法。论文还通过元分析(meta-analysis)总结了研究趋势，为 time-series anomaly detection 的未来发展提供了结构化的见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20512v1",
      "published_date": "2024-12-29 16:11:46 UTC",
      "updated_date": "2024-12-29 16:11:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:35:11.284831"
    },
    {
      "arxiv_id": "2412.20510v1",
      "title": "Stratify: Unifying Multi-Step Forecasting Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Riku Green",
        "Grant Stevens",
        "Zahraa Abdallah",
        "Telmo M. Silva Filho"
      ],
      "abstract": "A key aspect of temporal domains is the ability to make predictions multiple\ntime steps into the future, a process known as multi-step forecasting (MSF). At\nthe core of this process is selecting a forecasting strategy, however, with no\nexisting frameworks to map out the space of strategies, practitioners are left\nwith ad-hoc methods for strategy selection. In this work, we propose Stratify,\na parameterised framework that addresses multi-step forecasting, unifying\nexisting strategies and introducing novel, improved strategies. We evaluate\nStratify on 18 benchmark datasets, five function classes, and short to long\nforecast horizons (10, 20, 40, 80). In over 84% of 1080 experiments, novel\nstrategies in Stratify improved performance compared to all existing ones.\nImportantly, we find that no single strategy consistently outperforms others in\nall task settings, highlighting the need for practitioners explore the Stratify\nspace to carefully search and select forecasting strategies based on\ntask-specific requirements. Our results are the most comprehensive benchmarking\nof known and novel forecasting strategies. We make code available to reproduce\nour results.",
      "tldr_zh": "这篇论文提出了Stratify框架，一种参数化的方法，用于统一多步预测(multi-step forecasting)策略，并引入新的改进策略，以解决现有策略选择的随意性问题。通过在18个基准数据集、五个函数类和不同预测horizon（10、20、40、80）上进行评估，Stratify的新策略在超过84%的1080个实验中表现优于现有方法。研究发现，没有单一策略在所有任务设置中都占优，强调了根据任务特定需求探索和选择策略的重要性。该框架的全面基准测试结果可通过提供的代码进行重现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 9 figures, journal",
      "pdf_url": "http://arxiv.org/pdf/2412.20510v1",
      "published_date": "2024-12-29 16:06:46 UTC",
      "updated_date": "2024-12-29 16:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:35:22.208212"
    },
    {
      "arxiv_id": "2501.00066v1",
      "title": "On Adversarial Robustness of Language Models in Transfer Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bohdan Turbal",
        "Anastasiia Mazur",
        "Jiaxu Zhao",
        "Mykola Pechenizkiy"
      ],
      "abstract": "We investigate the adversarial robustness of LLMs in transfer learning\nscenarios. Through comprehensive experiments on multiple datasets (MBIB Hate\nSpeech, MBIB Political Bias, MBIB Gender Bias) and various model architectures\n(BERT, RoBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while\nimproving standard performance metrics, often leads to increased vulnerability\nto adversarial attacks. Our findings demonstrate that larger models exhibit\ngreater resilience to this phenomenon, suggesting a complex interplay between\nmodel size, architecture, and adaptation methods. Our work highlights the\ncrucial need for considering adversarial robustness in transfer learning\nscenarios and provides insights into maintaining model security without\ncompromising performance. These findings have significant implications for the\ndevelopment and deployment of LLMs in real-world applications where both\nperformance and robustness are paramount.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)在转移学习中的对抗鲁棒性，通过在MBIB Hate Speech、MBIB Political Bias和MBIB Gender Bias数据集上测试BERT、RoBERTa、GPT-2、Gemma和Phi等模型架构。实验结果显示，转移学习虽然提高了标准性能指标，但往往会增加模型对对抗攻击的脆弱性。更大模型表现出更高的弹性，揭示了模型大小、架构和适应方法之间的复杂互动。该工作强调在转移学习场景中考虑对抗鲁棒性的必要性，并为在不牺牲性能的情况下维护模型安全性提供重要洞见，对LLMs在现实应用的开发和部署具有深远影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00066v1",
      "published_date": "2024-12-29 15:55:35 UTC",
      "updated_date": "2024-12-29 15:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:35:35.617012"
    },
    {
      "arxiv_id": "2412.20505v1",
      "title": "Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Ni",
        "Yuzhi Wang",
        "Hao Liu"
      ],
      "abstract": "Urban regeneration presents significant challenges within the context of\nurbanization, requiring adaptive approaches to tackle evolving needs.\nLeveraging advancements in large language models (LLMs), we propose Cyclical\nUrban Planning (CUP), a new paradigm that continuously generates, evaluates,\nand refines urban plans in a closed-loop. Specifically, our multi-agent\nLLM-based framework consists of three key components: (1) Planning, where LLM\nagents generate and refine urban plans based on contextual data; (2) Living,\nwhere agents simulate the behaviors and interactions of residents, modeling\nlife in the urban environment; and (3) Judging, which involves evaluating plan\neffectiveness and providing iterative feedback for improvement. The cyclical\nprocess enables a dynamic and responsive planning approach. Experiments on the\nreal-world dataset demonstrate the effectiveness of our framework as a\ncontinuous and adaptive planning process.",
      "tldr_zh": "本研究提出了一种基于多智能体LLM的Cyclical Urban Planning (CUP)框架，用于应对城市化中的城市再生挑战，通过闭环过程持续生成、评估和完善城市计划。框架包括三个关键组件：Planning，使用LLM代理基于上下文数据生成和优化计划；Living，模拟居民行为和互动以建模城市生活；以及Judging，评估计划有效性并提供迭代反馈。实验在真实数据集上验证了该框架作为动态和适应性规划过程的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 2 figures, accepted by The 1st Workshop on AI for Urban\n  Planning (AAAI 2025's Workshop)",
      "pdf_url": "http://arxiv.org/pdf/2412.20505v1",
      "published_date": "2024-12-29 15:43:25 UTC",
      "updated_date": "2024-12-29 15:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:37:48.511675"
    },
    {
      "arxiv_id": "2412.20495v1",
      "title": "A Multiparty Homomorphic Encryption Approach to Confidential Federated Kaplan Meier Survival Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Narasimha Raghavan Veeraragavan",
        "Svetlana Boudko",
        "Jan Franz Nygård"
      ],
      "abstract": "The proliferation of healthcare data has expanded opportunities for\ncollaborative research, yet stringent privacy regulations hinder pooling\nsensitive patient records. We propose a \\emph{multiparty homomorphic\nencryption-based} framework for \\emph{privacy-preserving federated\nKaplan--Meier survival analysis}, offering native floating-point support, a\ntheoretical model, and explicit reconstruction-attack mitigation. Compared to\nprior work, our framework ensures encrypted federated survival estimates\nclosely match centralized outcomes, supported by formal utility-loss bounds\nthat demonstrate convergence as aggregation and decryption noise diminish.\nExtensive experiments on the NCCTG Lung Cancer and synthetic Breast Cancer\ndatasets confirm low \\emph{mean absolute error (MAE)} and \\emph{root mean\nsquared error (RMSE)}, indicating negligible deviations between encrypted and\nnon-encrypted survival curves. Log-rank and numerical accuracy tests reveal\n\\emph{no significant difference} between federated encrypted and non-encrypted\nanalyses, preserving statistical validity. A reconstruction-attack evaluation\nshows smaller federations (2--3 providers) with overlapping data between the\ninstitutions are vulnerable, a challenge mitigated by multiparty encryption.\nLarger federations (5--50 sites) degrade reconstruction accuracy further, with\nencryption improving confidentiality. Despite an 8--19$\\times$ computational\noverhead, threshold-based homomorphic encryption is \\emph{feasible for\nmoderate-scale deployments}, balancing security and runtime. By providing\nrobust privacy guarantees alongside high-fidelity survival estimates, our\nframework advances the state-of-the art in secure multi-institutional survival\nanalysis.",
      "tldr_zh": "本研究提出了一种基于多方同态加密（multiparty homomorphic encryption）的框架，用于隐私保护的联邦 Kaplan-Meier 生存分析（federated Kaplan-Meier survival analysis），旨在解决医疗数据协作中隐私法规的挑战，同时提供原生浮点支持、理论模型和针对重建攻击的缓解措施。相比现有工作，该框架确保加密的联邦生存估计与集中式结果高度一致，并通过正式的效用损失边界证明其收敛性。实验在 NCCTG Lung Cancer 和合成 Breast Cancer 数据集上显示，低 mean absolute error (MAE) 和 root mean squared error (RMSE) 表明加密与非加密生存曲线差异微小，且 log-rank 测试无显著统计差异。对于重建攻击，小规模联邦（2-3 提供者）存在风险，但多方加密可缓解，而大规模联邦（5-50 站点）进一步提升保密性。尽管计算开销增加 8-19 倍，该框架仍适用于中等规模部署，实现了隐私与分析准确性的平衡。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "40 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.20495v1",
      "published_date": "2024-12-29 15:17:42 UTC",
      "updated_date": "2024-12-29 15:17:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:37:00.463759"
    },
    {
      "arxiv_id": "2501.00065v1",
      "title": "Predicting Preschoolers' Externalizing Problems with Mother-Child Interaction Dynamics and Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Chen",
        "Yu Ji",
        "Cong Xia",
        "Wen Wu"
      ],
      "abstract": "Objective: Predicting children's future levels of externalizing problems\nhelps to identify children at risk and guide targeted prevention. Existing\nstudies have shown that mothers providing support in response to children's\ndysregulation was associated with children's lower levels of externalizing\nproblems. The current study aims to evaluate and improve the accuracy of\npredicting children's externalizing problems with mother-child interaction\ndynamics. Method: This study used mother-child interaction dynamics during a\nchallenging puzzle task to predict children's externalizing problems six months\nlater (N=101, 46 boys, Mage=57.41 months, SD=6.58). Performance of the Residual\nDynamic Structural Equation Model (RDSEM) was compared with the Attention-based\nSequential Behavior Interaction Modeling (ASBIM) model, developed using the\ndeep learning techniques. Results: The RDSEM revealed that children whose\nmothers provided more autonomy support after increases of child defeat had\nlower levels of externalizing problems. Five-fold cross-validation showed that\nthe RDSEM had good prediction accuracy. The ASBIM model further improved\nprediction accuracy, especially after including child inhibitory control as a\npersonalized individual feature. Conclusions: The dynamic process of\nmother-child interaction provides important information for predicting\nchildren's externalizing problems, especially maternal autonomy supportive\nresponse to child defeat. The deep learning model is a useful tool to further\nimprove prediction accuracy.",
      "tldr_zh": "本研究旨在通过母子互动动态预测学龄前儿童的外部化问题（Externalizing Problems），以识别高风险儿童并指导预防。研究使用母子在挑战性拼图任务中的互动数据（N=101，平均年龄57.41个月），比较了Residual Dynamic Structural Equation Model (RDSEM)和基于深度学习的Attention-based Sequential Behavior Interaction Modeling (ASBIM)模型。结果显示，母亲在儿童失败后提供更多自主支持与儿童较低的外部化问题相关，ASBIM模型进一步提高了预测准确性，尤其在加入儿童抑制控制作为个性化特征后。该方法证明了母子互动动态，特别是母亲的响应方式，在预测儿童外部化问题中的重要作用，并展示了深度学习工具的潜在价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.00065v1",
      "published_date": "2024-12-29 14:22:48 UTC",
      "updated_date": "2024-12-29 14:22:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:10.149893"
    },
    {
      "arxiv_id": "2501.01982v2",
      "title": "Is Your Image a Good Storyteller?",
      "title_zh": "翻译失败",
      "authors": [
        "Xiujie Song",
        "Xiaoyi Pang",
        "Haifeng Tang",
        "Mengyue Wu",
        "Kenny Q. Zhu"
      ],
      "abstract": "Quantifying image complexity at the entity level is straightforward, but the\nassessment of semantic complexity has been largely overlooked. In fact, there\nare differences in semantic complexity across images. Images with richer\nsemantics can tell vivid and engaging stories and offer a wide range of\napplication scenarios. For example, the Cookie Theft picture is such a kind of\nimage and is widely used to assess human language and cognitive abilities due\nto its higher semantic complexity. Additionally, semantically rich images can\nbenefit the development of vision models, as images with limited semantics are\nbecoming less challenging for them. However, such images are scarce,\nhighlighting the need for a greater number of them. For instance, there is a\nneed for more images like Cookie Theft to cater to people from different\ncultural backgrounds and eras. Assessing semantic complexity requires human\nexperts and empirical evidence. Automatic evaluation of how semantically rich\nan image will be the first step of mining or generating more images with rich\nsemantics, and benefit human cognitive assessment, Artificial Intelligence, and\nvarious other applications. In response, we propose the Image Semantic\nAssessment (ISA) task to address this problem. We introduce the first ISA\ndataset and a novel method that leverages language to solve this vision\nproblem. Experiments on our dataset demonstrate the effectiveness of our\napproach.",
      "tldr_zh": "该研究探讨了图像的语义复杂度问题，指出语义丰富的图像（如Cookie Theft图片）能讲述生动故事，并应用于人类认知评估和AI模型开发，但此类图像较为稀缺。作者提出Image Semantic Assessment (ISA)任务，并构建了首个ISA数据集，以及一种利用语言模型来评估图像语义复杂度的创新方法。通过实验验证，该方法在数据集上表现出色，有助于自动挖掘或生成更多语义丰富图像，并扩展到认知评估和AI等领域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01982v2",
      "published_date": "2024-12-29 14:04:39 UTC",
      "updated_date": "2025-02-22 06:53:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:37:22.541824"
    },
    {
      "arxiv_id": "2412.20468v2",
      "title": "A Comprehensive Framework for Reliable Legal AI: Combining Specialized Expert Systems and Adaptive Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Sidra Nasir",
        "Qamar Abbas",
        "Samita Bai",
        "Rizwan Ahmed Khan"
      ],
      "abstract": "This article discusses the evolving role of artificial intelligence (AI) in\nthe legal profession, focusing on its potential to streamline tasks such as\ndocument review, research, and contract drafting. However, challenges persist,\nparticularly the occurrence of \"hallucinations\" in AI models, where they\ngenerate inaccurate or misleading information, undermining their reliability in\nlegal contexts. To address this, the article proposes a novel framework\ncombining a mixture of expert systems with a knowledge-based architecture to\nimprove the precision and contextual relevance of AI-driven legal services.\nThis framework utilizes specialized modules, each focusing on specific legal\nareas, and incorporates structured operational guidelines to enhance\ndecision-making. Additionally, it leverages advanced AI techniques like\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KG), and Reinforcement\nLearning from Human Feedback (RLHF) to improve the system's accuracy. The\nproposed approach demonstrates significant improvements over existing AI\nmodels, showcasing enhanced performance in legal tasks and offering a scalable\nsolution to provide more accessible and affordable legal services. The article\nalso outlines the methodology, system architecture, and promising directions\nfor future research in AI applications for the legal sector.",
      "tldr_zh": "本文探讨了人工智能(AI)在法律领域的应用，如文档审查、研究和合同起草，但强调了AI“hallucinations”（幻觉）问题导致的信息不准确性。该文提出一个综合框架，结合混合专家系统和基于知识的架构，利用Retrieval-Augmented Generation (RAG)、Knowledge Graphs (KG)以及Reinforcement Learning from Human Feedback (RLHF)等技术，通过专门模块和结构化指南提升AI的精确性和上下文相关性。实验结果显示，该框架在法律任务中显著优于现有模型，提供可扩展的解决方案，以实现更可靠、可及的法律服务。该文还概述了系统架构和未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages and 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20468v2",
      "published_date": "2024-12-29 14:00:11 UTC",
      "updated_date": "2025-03-05 04:32:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:37:35.286456"
    },
    {
      "arxiv_id": "2412.20438v1",
      "title": "Integrating Natural Language Processing Techniques of Text Mining Into Financial System: Applications and Limitations",
      "title_zh": "翻译失败",
      "authors": [
        "Denisa Millo",
        "Blerina Vika",
        "Nevila Baci"
      ],
      "abstract": "The financial sector, a pivotal force in economic development, increasingly\nuses the intelligent technologies such as natural language processing to\nenhance data processing and insight extraction. This research paper through a\nreview process of the time span of 2018-2023 explores the use of text mining as\nnatural language processing techniques in various components of the financial\nsystem including asset pricing, corporate finance, derivatives, risk\nmanagement, and public finance and highlights the need to address the specific\nproblems in the discussion section. We notice that most of the research\nmaterials combined probabilistic with vector-space models, and text-data with\nnumerical ones. The most used technique regarding information processing is the\ninformation classification technique and the most used algorithms include the\nlong-short term memory and bidirectional encoder models. The research noticed\nthat new specific algorithms are developed and the focus of the financial\nsystem is mainly on asset pricing component. The research also proposes a path\nfrom engineering perspective for researchers who need to analyze financial\ntext. The challenges regarding text mining perspective such as data quality,\ncontext-adaption and model interpretability need to be solved so to integrate\nadvanced natural language processing models and techniques in enhancing\nfinancial analysis and prediction. Keywords: Financial System (FS), Natural\nLanguage Processing (NLP), Software and Text Engineering, Probabilistic,\nVector-Space, Models, Techniques, TextData, Financial Analysis.",
      "tldr_zh": "本研究综述了2018-2023年间将自然语言处理(NLP)技术，特别是文本挖掘，整合到金融系统中的应用，包括资产定价、企业融资、衍生品、风险管理和公共财政等领域。研究发现，大多数工作结合了概率模型和向量空间模型，将文本数据与数值数据整合，并广泛使用信息分类技术、Long-Short Term Memory (LSTM)以及Bidirectional Encoder Models等算法，其中资产定价是主要焦点。论文强调了面临的挑战，如数据质量、上下文适应和模型可解释性问题，并从工程角度提出路径，以提升NLP在金融分析和预测中的作用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2412.20438v1",
      "published_date": "2024-12-29 11:25:03 UTC",
      "updated_date": "2024-12-29 11:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:37:47.026784"
    },
    {
      "arxiv_id": "2412.20429v3",
      "title": "Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Libo Wang"
      ],
      "abstract": "To improve the cognitive autonomy of humanoid robots, this research proposes\na multi-scenario reasoning architecture to solve the technical shortcomings of\nmulti-modal understanding in this field. It draws on simulation based\nexperimental design that adopts multi-modal synthesis (visual, auditory,\ntactile) and builds a simulator \"Maha\" to perform the experiment. The findings\ndemonstrate the feasibility of this architecture in multimodal data. It\nprovides reference experience for the exploration of cross-modal interaction\nstrategies for humanoid robots in dynamic environments. In addition,\nmulti-scenario reasoning simulates the high-level reasoning mechanism of the\nhuman brain to humanoid robots at the cognitive level. This new concept\npromotes cross-scenario practical task transfer and semantic-driven action\nplanning. It heralds the future development of self-learning and autonomous\nbehavior of humanoid robots in changing scenarios.",
      "tldr_zh": "本研究提出了一种多场景推理(multi-scenario reasoning)架构，以提升人形机器人在多模态理解(multimodal understanding)中的认知自治，解决其技术缺陷。该架构采用多模态合成（视觉、听觉、触觉）并构建模拟器“Maha”进行基于模拟的实验，证明了其在多模态数据处理中的可行性。实验结果为人形机器人在动态环境中的跨模态交互策略提供参考，并通过模拟人类大脑的高级推理机制，促进跨场景任务转移、语义驱动行动规划以及自学习和自主行为的发展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The main text is 5 pages, 2 figures, and 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.20429v3",
      "published_date": "2024-12-29 10:46:08 UTC",
      "updated_date": "2025-01-07 18:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:00.532157"
    },
    {
      "arxiv_id": "2412.20414v1",
      "title": "Comparative Performance of Advanced NLP Models and LLMs in Multilingual Geo-Entity Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Kalin Kopanov"
      ],
      "abstract": "The integration of advanced Natural Language Processing (NLP) methodologies\nand Large Language Models (LLMs) has significantly enhanced the extraction and\nanalysis of geospatial data from multilingual texts, impacting sectors such as\nnational and international security. This paper presents a comprehensive\nevaluation of leading NLP models -- SpaCy, XLM-RoBERTa, mLUKE, GeoLM -- and\nLLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of\nmultilingual geo-entity detection. Utilizing datasets from Telegram channels in\nEnglish, Russian, and Arabic, we examine the performance of these models\nthrough metrics such as accuracy, precision, recall, and F1 scores, to assess\ntheir effectiveness in accurately identifying geospatial references. The\nanalysis exposes each model's distinct advantages and challenges, underscoring\nthe complexities involved in achieving precise geo-entity identification across\nvaried linguistic landscapes. The conclusions drawn from this experiment aim to\ndirect the enhancement and creation of more advanced and inclusive NLP tools,\nthus advancing the field of geospatial analysis and its application to global\nsecurity.",
      "tldr_zh": "本论文比较了先进 NLP 模型（如 SpaCy、XLM-RoBERTa、mLUKE 和 GeoLM）以及 LLM（如 GPT 3.5 和 GPT 4）在多语言 geo-entity detection 中的性能。研究利用来自 Telegram 渠道的英语、俄语和阿拉伯语数据集，通过准确率、精确率、召回率和 F1 分数等指标进行全面评估。结果显示各模型存在独特优势和挑战，突显了跨语言地理实体识别的复杂性。该工作为开发更先进的 NLP 工具提供指导，推动地理空间分析在全球安全领域的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 1 table, AICCONF '24: Cognitive Models and Artificial\n  Intelligence Conference, Istanbul, Turkey",
      "pdf_url": "http://arxiv.org/pdf/2412.20414v1",
      "published_date": "2024-12-29 09:47:14 UTC",
      "updated_date": "2024-12-29 09:47:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:13.126290"
    },
    {
      "arxiv_id": "2412.20412v2",
      "title": "Multi-Objective Large Language Model Unlearning",
      "title_zh": "多目标大型语言模型遗忘",
      "authors": [
        "Zibin Pan",
        "Shuwen Zhang",
        "Yuesheng Zheng",
        "Chi Li",
        "Yuheng Cheng",
        "Junhua Zhao"
      ],
      "abstract": "Machine unlearning in the domain of large language models (LLMs) has\nattracted great attention recently, which aims to effectively eliminate\nundesirable behaviors from LLMs without full retraining from scratch. In this\npaper, we explore the Gradient Ascent (GA) approach in LLM unlearning, which is\na proactive way to decrease the prediction probability of the model on the\ntarget data in order to remove their influence. We analyze two challenges that\nrender the process impractical: gradient explosion and catastrophic forgetting.\nTo address these issues, we propose Multi-Objective Large Language Model\nUnlearning (MOLLM) algorithm. We first formulate LLM unlearning as a\nmulti-objective optimization problem, in which the cross-entropy loss is\nmodified to the unlearning version to overcome the gradient explosion issue. A\ncommon descent update direction is then calculated, which enables the model to\nforget the target data while preserving the utility of the LLM. Our empirical\nresults verify that MoLLM outperforms the SOTA GA-based LLM unlearning methods\nin terms of unlearning effect and model utility preservation. The source code\nis available at https://github.com/zibinpan/MOLLM.",
      "tldr_zh": "该研究探讨了Large Language Models (LLMs)中的机器unlearning技术，旨在从模型中消除不期望的行为，而无需从零重新训练。作者分析了Gradient Ascent (GA)方法在unlearning过程中的两大挑战：gradient explosion和catastrophic forgetting，并提出Multi-Objective Large Language Model Unlearning (MOLLM)算法，将unlearning表述为多目标优化问题，通过修改cross-entropy loss和计算共同下降更新方向，实现目标数据的遗忘同时保留模型效用。实验结果显示，MOLLM在unlearning效果和模型效用保留方面优于现有最先进(SOTA)GA-based方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in the Proceedings of 2025 IEEE International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP-2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.20412v2",
      "published_date": "2024-12-29 09:35:56 UTC",
      "updated_date": "2025-01-04 13:27:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:23.868211"
    },
    {
      "arxiv_id": "2501.00063v1",
      "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
      "title_zh": "翻译失败",
      "authors": [
        "Guangming Che"
      ],
      "abstract": "The financial industry is increasingly seeking robust methods to address the\nchallenges posed by data scarcity and low signal-to-noise ratios, which limit\nthe application of deep learning techniques in stock market analysis. This\npaper presents two innovative generative model-based approaches to synthesize\nstock data, specifically tailored for different scenarios within the A-share\nmarket in China. The first method, a sector-based synthesis approach, enhances\nthe signal-to-noise ratio of stock data by classifying the characteristics of\nstocks from various sectors in China's A-share market. This method employs an\nApproximate Non-Local Total Variation algorithm to smooth the generated data, a\nbandpass filtering method based on Fourier Transform to eliminate noise, and\nDenoising Diffusion Implicit Models to accelerate sampling speed. The second\nmethod, a recursive stock data synthesis approach based on pattern recognition,\nis designed to synthesize data for stocks with short listing periods and\nlimited comparable companies. It leverages pattern recognition techniques and\nMarkov models to learn and generate variable-length stock sequences, while\nintroducing a sub-time-level data augmentation method to alleviate data\nscarcity issues.We validate the effectiveness of these methods through\nextensive experiments on various datasets, including those from the main board,\nSTAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ,\nNYSE, and AMEX. The results demonstrate that our synthesized data not only\nimprove the performance of predictive models but also enhance the\nsignal-to-noise ratio of individual stock signals in price trading strategies.\nFurthermore, the introduction of sub-time-level data significantly improves the\nquality of synthesized data.",
      "tldr_zh": "本论文提出两种创新的生成模型方法，用于解决中国 A-Share Market 中的金融时间序列数据稀缺性和低 Signal-to-Noise Ratio 问题。第一种方法是基于行业的合成策略，通过分类股票特征、运用 Approximate Non-Local Total Variation 算法平滑数据、Fourier Transform 的带通滤波消除噪声，以及 Denoising Diffusion Implicit Models 加速采样，从而提升数据质量。第二种方法是基于 Pattern Recognition 的递归股票数据合成策略，针对上市期短的股票，利用 Markov models 生成可变长序列，并引入子时间级数据增强以缓解数据不足。实验在包括 A-Share 主板、STAR Market 等多个数据集上验证，结果显示这些合成数据显著提高了预测模型性能、改善了 Signal-to-Noise Ratio，并增强了价格交易策略中的股票信号质量。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00063v1",
      "published_date": "2024-12-29 09:35:23 UTC",
      "updated_date": "2024-12-29 09:35:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:37.589025"
    },
    {
      "arxiv_id": "2412.20382v1",
      "title": "Natural Language Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Liu",
        "Yue Wang",
        "Zhiqi Lin",
        "Min Chen",
        "Yixue Hao",
        "Long Hu"
      ],
      "abstract": "Large language model fine-tuning techniques typically depend on extensive\nlabeled data, external guidance, and feedback, such as human alignment, scalar\nrewards, and demonstration. However, in practical application, the scarcity of\nspecific knowledge poses unprecedented challenges to existing fine-tuning\ntechniques. In this paper, focusing on fine-tuning tasks in specific domains\nwith limited data, we introduce Natural Language Fine-Tuning (NLFT), which\nutilizes natural language for fine-tuning for the first time. By leveraging the\nstrong language comprehension capability of the target LM, NLFT attaches the\nguidance of natural language to the token-level outputs. Then, saliency tokens\nare identified with calculated probabilities. Since linguistic information is\neffectively utilized in NLFT, our proposed method significantly reduces\ntraining costs. It markedly enhances training efficiency, comprehensively\noutperforming reinforcement fine-tuning algorithms in accuracy, time-saving,\nand resource conservation. Additionally, on the macro level, NLFT can be viewed\nas a token-level fine-grained optimization of SFT, thereby efficiently\nreplacing the SFT process without the need for warm-up (as opposed to ReFT\nrequiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not\nincrease the algorithmic complexity, maintaining O(n). Extensive experiments on\nthe GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves\nan accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time\ncomplexity and space complexity of NLFT are reduced by 78.27% and 92.24%,\nrespectively. The superior technique of NLFT is paving the way for the\ndeployment of various innovative LLM fine-tuning applications when resources\nare limited at network edges.\n  Our code has been released at https://github.com/Julia-LiuJ/NLFT.",
      "tldr_zh": "该论文提出了一种名为 Natural Language Fine-Tuning (NLFT) 的新方法，用于在数据有限的特定领域对大型语言模型 (LLMs) 进行细调，旨在解决传统细调技术对大量标记数据和外部反馈的依赖问题。NLFT 通过利用目标模型的语言理解能力，将自然语言指导附加到 token-level 输出上，并识别显著性 tokens，从而显著降低训练成本并提升效率。相比于 Supervised Fine-Tuning (SFT)，NLFT 实现了 token-level 的细粒度优化，无需 warm-up，且在 GSM8K 数据集上仅用 50 个样本就使准确率提升超过 SFT 的 219%。此外，NLFT 与 Reinforcement Fine-Tuning (ReFT) 相比，时间复杂度减少 78.27%、空间复杂度减少 92.24%，为资源受限的边缘网络部署提供高效的 LLM 细调方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20382v1",
      "published_date": "2024-12-29 07:02:45 UTC",
      "updated_date": "2024-12-29 07:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:38:49.268259"
    },
    {
      "arxiv_id": "2412.20373v1",
      "title": "A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data",
      "title_zh": "翻译失败",
      "authors": [
        "Seungyeon Lee",
        "Ruoqi Liu",
        "Feixiong Cheng",
        "Ping Zhang"
      ],
      "abstract": "Drug repurposing identifies new therapeutic uses for existing drugs, reducing\nthe time and costs compared to traditional de novo drug discovery. Most\nexisting drug repurposing studies using real-world patient data often treat the\nentire population as homogeneous, ignoring the heterogeneity of treatment\nresponses across patient subgroups. This approach may overlook promising drugs\nthat benefit specific subgroups but lack notable treatment effects across the\nentire population, potentially limiting the number of repurposable candidates\nidentified. To address this, we introduce STEDR, a novel drug repurposing\nframework that integrates subgroup analysis with treatment effect estimation.\nOur approach first identifies repurposing candidates by emulating multiple\nclinical trials on real-world patient data and then characterizes patient\nsubgroups by learning subgroup-specific treatment effects. We deploy \\model to\nAlzheimer's Disease (AD), a condition with few approved drugs and known\nheterogeneity in treatment responses. We emulate trials for over one thousand\nmedications on a large-scale real-world database covering over 8 million\npatients, identifying 14 drug candidates with beneficial effects to AD in\ncharacterized subgroups. Experiments demonstrate STEDR's superior capability in\nidentifying repurposing candidates compared to existing approaches.\nAdditionally, our method can characterize clinically relevant patient subgroups\nassociated with important AD-related risk factors, paving the way for precision\ndrug repurposing.",
      "tldr_zh": "本研究提出了一种深度子群分析框架STEDR，用于精确药物再利用，通过模拟临床试验来分析真实世界患者数据。该框架首先通过仿真多个临床试验识别潜在药物候选，然后学习子群特定的治疗效果，以解决传统方法忽略患者异质性的问题。在阿尔茨海默病(Alzheimer's Disease, AD)应用中，STEDR在覆盖超过800万患者的数据库上测试，成功识别了14种对特定子群有益的药物候选，并展示了比现有方法更强的识别能力。该方法还表征了与AD相关风险因素的临床相关子群，推动了精准药物再利用的进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "I.2.0; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in KDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20373v1",
      "published_date": "2024-12-29 06:32:52 UTC",
      "updated_date": "2024-12-29 06:32:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:39:04.725044"
    },
    {
      "arxiv_id": "2412.20372v2",
      "title": "LLM2: Let Large Language Models Harness System 2 Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Yang",
        "Chufan Shi",
        "Siheng Li",
        "Bo Shui",
        "Yujiu Yang",
        "Wai Lam"
      ],
      "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0).",
      "tldr_zh": "该论文指出，大型语言模型 (LLMs) 由于其 autoregressive 架构，缺乏区分理想与非理想输出的机制，导致输出不佳。受人类认知双重过程理论启发，研究提出 LLM2 框架，将 LLM（System 1）负责生成候选答案，与一个基于过程的 verifier（System 2）结合，提供及时反馈以筛选输出。验证器通过 pairwise comparison loss 和 synthetic process-supervision data（由 token quality exploration 策略生成）进行训练。实验结果显示，LLM2 在数学推理基准如 GSM8K 上显著提升准确率，例如 Llama3-1B 从 50.3% 提高到 57.8%，并与 self-consistency 结合后进一步提升至 70.2%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2412.20372v2",
      "published_date": "2024-12-29 06:32:36 UTC",
      "updated_date": "2025-02-28 13:06:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:39:14.248997"
    },
    {
      "arxiv_id": "2412.20361v1",
      "title": "Safe Multiagent Coordination via Entropic Exploration",
      "title_zh": "通过熵探索的安全多智能体协调",
      "authors": [
        "Ayhan Alp Aydeniz",
        "Enrico Marchesini",
        "Robert Loftin",
        "Christopher Amato",
        "Kagan Tumer"
      ],
      "abstract": "Many real-world multiagent learning problems involve safety concerns. In\nthese setups, typical safe reinforcement learning algorithms constrain agents'\nbehavior, limiting exploration -- a crucial component for discovering effective\ncooperative multiagent behaviors. Moreover, the multiagent literature typically\nmodels individual constraints for each agent and has yet to investigate the\nbenefits of using joint team constraints. In this work, we analyze these team\nconstraints from a theoretical and practical perspective and propose entropic\nexploration for constrained multiagent reinforcement learning (E2C) to address\nthe exploration issue. E2C leverages observation entropy maximization to\nincentivize exploration and facilitate learning safe and effective cooperative\nbehaviors. Experiments across increasingly complex domains show that E2C agents\nmatch or surpass common unconstrained and constrained baselines in task\nperformance while reducing unsafe behaviors by up to $50\\%$.",
      "tldr_zh": "该研究探讨了安全多智能体强化学习（reinforcement learning）中的挑战，指出传统算法通过约束代理行为限制了探索，从而阻碍了有效合作行为的发现，同时强调了团队联合约束（team constraints）的潜在益处。作者提出 E2C（entropic exploration for constrained multiagent reinforcement learning）方法，通过最大化观察熵（observation entropy）来激励探索，促进代理学习安全且高效的合作行为。从理论和实践角度分析团队约束后，实验在多个复杂域（domains）中显示，E2C 代理在任务性能上匹配或超过无约束和有约束基线，同时将不安全行为减少多达 50%。这为安全多智能体协调提供了新颖的解决方案。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20361v1",
      "published_date": "2024-12-29 05:50:19 UTC",
      "updated_date": "2024-12-29 05:50:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:39:24.903722"
    },
    {
      "arxiv_id": "2412.20359v1",
      "title": "EmoReg: Directional Latent Vector Modeling for Emotional Intensity Regularization in Diffusion-based Voice Conversion",
      "title_zh": "翻译失败",
      "authors": [
        "Ashishkumar Gudmalwar",
        "Ishan D. Biyani",
        "Nirmesh Shah",
        "Pankaj Wasnik",
        "Rajiv Ratn Shah"
      ],
      "abstract": "The Emotional Voice Conversion (EVC) aims to convert the discrete emotional\nstate from the source emotion to the target for a given speech utterance while\npreserving linguistic content. In this paper, we propose regularizing emotion\nintensity in the diffusion-based EVC framework to generate precise speech of\nthe target emotion. Traditional approaches control the intensity of an\nemotional state in the utterance via emotion class probabilities or intensity\nlabels that often lead to inept style manipulations and degradations in\nquality. On the contrary, we aim to regulate emotion intensity using\nself-supervised learning-based feature representations and unsupervised\ndirectional latent vector modeling (DVM) in the emotional embedding space\nwithin a diffusion-based framework. These emotion embeddings can be modified\nbased on the given target emotion intensity and the corresponding direction\nvector. Furthermore, the updated embeddings can be fused in the reverse\ndiffusion process to generate the speech with the desired emotion and\nintensity. In summary, this paper aims to achieve high-quality emotional\nintensity regularization in the diffusion-based EVC framework, which is the\nfirst of its kind work. The effectiveness of the proposed method has been shown\nacross state-of-the-art (SOTA) baselines in terms of subjective and objective\nevaluations for the English and Hindi languages \\footnote{Demo samples are\navailable at the following URL: \\url{https://nirmesh-sony.github.io/EmoReg/}}.",
      "tldr_zh": "该论文提出 EmoReg 方法，用于在扩散-based 语音转换（EVC）框架中实现情感强度调节，旨在生成精确的目标情感语音，同时保留语言内容。EmoReg 采用自监督学习（self-supervised learning）特征表示和无监督的方向性潜在向量建模（DVM）在情感嵌入空间中修改嵌入向量，根据目标强度和方向向量进行调节，并在逆扩散过程中融合这些嵌入以产生所需语音。该方法首次在扩散-based EVC 中实现高质量情感强度控制，并在主观和客观评估中超越现有最先进（SOTA）基线，在英语和印地语数据集上表现出显著优势。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20359v1",
      "published_date": "2024-12-29 05:30:06 UTC",
      "updated_date": "2024-12-29 05:30:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:39:37.874471"
    },
    {
      "arxiv_id": "2501.00062v2",
      "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "James P. Beno"
      ],
      "abstract": "Bidirectional transformers excel at sentiment analysis, and Large Language\nModels (LLM) are effective zero-shot learners. Might they perform better as a\nteam? This paper explores collaborative approaches between ELECTRA and GPT-4o\nfor three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA\nBase/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment\nTreebank (SST) and DynaSent. We provided input from ELECTRA to GPT as:\npredicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT\npredictions with GPT-4o-mini significantly improved performance over either\nmodel alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and\nyielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT\nmodels were fine-tuned, including predictions decreased performance. GPT-4o\nFT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at\nmuch less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting\nprompts with predictions from fine-tuned encoders is an efficient way to boost\nperformance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76%\nless cost. Both are affordable options for projects with limited resources.",
      "tldr_zh": "本论文探讨了 ELECTRA 和 GPT-4o 在三分类情感分析中的协作方法，通过 fine-tuned 模型（如 ELECTRA Base/Large 和 GPT-4o/4o-mini）并将 ELECTRA 的预测（包括标签、概率和检索示例）输入 GPT，以提升性能并降低成本。结果显示，ELECTRA Base FT 与 GPT-4o-mini 结合显著提高了宏 F1 得分（从 79.14 和 79.41 提升至 82.50），并实现了最低的成本/性能比（$0.12/F1 point）；然而，fine-tuned GPT 模型时添加预测会降低性能。总体上，fine-tuned GPT-4o-mini 几乎与 GPT-4o FT 相当（F1 分别为 86.70 和 86.99），但成本低 76%，为资源有限的项目提供高效、可负担的选项。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures. Source code and data available at\n  https://github.com/jbeno/sentiment",
      "pdf_url": "http://arxiv.org/pdf/2501.00062v2",
      "published_date": "2024-12-29 05:29:52 UTC",
      "updated_date": "2025-05-03 23:36:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:39:51.475242"
    },
    {
      "arxiv_id": "2412.20357v1",
      "title": "HindiLLM: Large Language Model for Hindi",
      "title_zh": "翻译失败",
      "authors": [
        "Sanjay Chouhan",
        "Shubha Brata Nath",
        "Aparajita Dutta"
      ],
      "abstract": "The advancements in the Large Language Model (LLM) have helped in solving\nseveral problems related to language processing. Most of the researches have\nfocused on the English language only, because of its popularity and abundance\non the internet. However, a high-performance language model for Hindi and other\nIndic languages is lacking in the literature. In this work, we have pre-trained\ntwo autoregressive LLM models for the Hindi language, namely HindiLLM-Small and\nHindiLLM-Medium. We use a two-step process comprising unsupervised pre-training\nand supervised fine-tuning. First, we create a large and high-quality text\ncorpus for unsupervised pre-training. Next, we train a Byte-Pair Encoding,\nnamed HindiLLM tokenizer, using the pre-training text data. We then perform\ntraining on the unlabeled data, known as the pre-training step, to get the\nHindiLLM base models. Furthermore, we perform fine-tuning of the HindiLLM base\nmodels for different tasks like sentiment analysis, text classification,\nnatural language inference, and multiple choice question-answer on popular\nlabeled datasets to measure the real-world performance. The evaluation shows\nthat the HindiLLM-based fine-tuned models outperform several models in most of\nthe language related tasks.",
      "tldr_zh": "该研究针对Hindi和其他Indic语言缺乏高性能大型语言模型(LLM)的现状，开发了两个自回归LLM模型：HindiLLM-Small和HindiLLM-Medium。作者采用两步过程，包括创建大型高质量文本语料库、训练Byte-Pair Encoding的HindiLLM tokenizer进行无监督预训练，以及在监督微调下优化模型以处理情感分析、文本分类、自然语言推理和多项选择问答等任务。实验结果显示，HindiLLM-based模型在大多数语言相关任务上优于现有模型，证明了其在Hindi语言处理中的卓越性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20357v1",
      "published_date": "2024-12-29 05:28:15 UTC",
      "updated_date": "2024-12-29 05:28:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:40:01.242426"
    },
    {
      "arxiv_id": "2501.00061v1",
      "title": "Training-free Heterogeneous Model Merging",
      "title_zh": "无需训练的异构模型合并",
      "authors": [
        "Zhengqi Xu",
        "Han Zheng",
        "Jie Song",
        "Li Sun",
        "Mingli Song"
      ],
      "abstract": "Model merging has attracted significant attention as a powerful paradigm for\nmodel reuse, facilitating the integration of task-specific models into a\nsingular, versatile framework endowed with multifarious capabilities. Previous\nstudies, predominantly utilizing methods such as Weight Average (WA), have\nshown that model merging can effectively leverage pretrained models without the\nneed for laborious retraining. However, the inherent heterogeneity among models\nposes a substantial constraint on its applicability, particularly when\nconfronted with discrepancies in model architectures. To overcome this\nchallenge, we propose an innovative model merging framework designed for\nheterogeneous models, encompassing both depth and width heterogeneity. To\naddress depth heterogeneity, we introduce a layer alignment strategy that\nharmonizes model layers by segmenting deeper models, treating consecutive\nlayers with similar representations as a cohesive segment, thus enabling the\nseamless merging of models with differing layer depths. For width\nheterogeneity, we propose a novel elastic neuron zipping algorithm that\nprojects the weights from models of varying widths onto a common dimensional\nspace, eliminating the need for identical widths. Extensive experiments\nvalidate the efficacy of these proposed methods, demonstrating that the merging\nof structurally heterogeneous models can achieve performance levels comparable\nto those of homogeneous merging, across both vision and NLP tasks. Our code is\npublicly available at\nhttps://github.com/zju-vipa/training_free_heterogeneous_model_merging.",
      "tldr_zh": "该研究提出了一种无需训练（training-free）的异质模型合并框架，旨在解决传统方法（如 Weight Average）在处理模型架构差异时的局限性，特别是深度和宽度异质性。针对深度异质性，引入层对齐策略（layer alignment），通过分割并整合相似的层段来实现不同深度模型的无缝合并；针对宽度异质性，开发了弹性神经元压缩算法（elastic neuron zipping），将不同宽度的模型权重投影到公共空间。实验结果显示，该框架在视觉和 NLP 任务上，异质模型合并的性能可与同质模型合并相当，并提供了开源代码以便进一步验证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00061v1",
      "published_date": "2024-12-29 04:49:11 UTC",
      "updated_date": "2024-12-29 04:49:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:40:13.019840"
    },
    {
      "arxiv_id": "2412.20340v2",
      "title": "Distilling Desired Comments for Enhanced Code Review with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yongda Yu",
        "Lei Zhang",
        "Guoping Rong",
        "Haifeng Shen",
        "Jiahao Zhang",
        "Haoxiang Yan",
        "Guohao Shi",
        "Dong Shao",
        "Ruiqi Pan",
        "Yuan Li",
        "Qiushi Wang",
        "Zhao Tian"
      ],
      "abstract": "There has been a growing interest in using Large Language Models (LLMs) for\ncode review thanks to their proven proficiency in code comprehension. The\nprimary objective of most review scenarios is to generate desired review\ncomments (DRCs) that explicitly identify issues to trigger code fixes. However,\nexisting LLM-based solutions are not so effective in generating DRCs for\nvarious reasons such as hallucination. To enhance their code review ability,\nthey need to be fine-tuned with a customized dataset that is ideally full of\nDRCs. Nevertheless, such a dataset is not yet available, while manual\nannotation of DRCs is too laborious to be practical. In this paper, we propose\na dataset distillation method, Desiview, which can automatically construct a\ndistilled dataset by identifying DRCs from a code review dataset. Experiments\non the CodeReviewer dataset comprising more than 150K review entries show that\nDesiview achieves an impressive performance of 88.93%, 80.37%, 86.67%, and\n84.44% in terms of Precision, Recall, Accuracy, and F1, respectively,\nsurpassing state-of-the-art methods. To validate the effect of such a distilled\ndataset on enhancing LLMs' code review ability, we first fine-tune the latest\nLLaMA series (i.e., LLaMA 3 and LLaMA 3.1) to build model Desiview4FT. We then\nenhance the model training effect through KTO alignment by feeding those review\ncomments identified as non-DRCs to the LLMs, resulting in model Desiview4FA.\nVerification results indicate that Desiview4FA slightly outperforms\nDesiview4FT, while both models have significantly improved against the base\nmodels in terms of generating DRCs. Human evaluation confirms that both models\nidentify issues more accurately and tend to generate review comments that\nbetter describe the issues contained in the code than the base LLMs do.",
      "tldr_zh": "这篇论文针对大型语言模型 (LLMs) 在代码审查中生成期望的审查评论 (DRCs) 的不足（如幻觉问题），提出了一种自动数据集蒸馏方法 Desiview，用于从代码审查数据集（如 CodeReviewer）中识别 DRCs。实验结果显示，Desiview 在精确率、召回率、准确率和 F1 分数上分别达到 88.93%、80.37%、86.67% 和 84.44%，超过了现有方法。作者使用该数据集微调 LLaMA 系列模型（如 LLaMA 3 和 LLaMA 3.1），构建了 Desiview4FT 和 Desiview4FA（通过 KTO 对齐进一步优化）模型，结果表明这些模型在生成 DRCs 时显著改进，并经人类评估证实其识别问题和描述代码问题更准确。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.3; I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20340v2",
      "published_date": "2024-12-29 03:49:13 UTC",
      "updated_date": "2025-01-05 06:20:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:40:26.266871"
    },
    {
      "arxiv_id": "2412.20338v1",
      "title": "Exploiting Hybrid Policy in Reinforcement Learning for Interpretable Temporal Logic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Zhang",
        "Hao Wang",
        "Xiucai Huang",
        "Wenrui Chen",
        "Zhen Kan"
      ],
      "abstract": "Reinforcement Learning (RL) based methods have been increasingly explored for\nrobot learning. However, RL based methods often suffer from low sampling\nefficiency in the exploration phase, especially for long-horizon manipulation\ntasks, and generally neglect the semantic information from the task level,\nresulted in a delayed convergence or even tasks failure. To tackle these\nchallenges, we propose a Temporal-Logic-guided Hybrid policy framework (HyTL)\nwhich leverages three-level decision layers to improve the agent's performance.\nSpecifically, the task specifications are encoded via linear temporal logic\n(LTL) to improve performance and offer interpretability. And a waypoints\nplanning module is designed with the feedback from the LTL-encoded task level\nas a high-level policy to improve the exploration efficiency. The middle-level\npolicy selects which behavior primitives to execute, and the low-level policy\nspecifies the corresponding parameters to interact with the environment. We\nevaluate HyTL on four challenging manipulation tasks, which demonstrate its\neffectiveness and interpretability. Our project is available at:\nhttps://sites.google.com/view/hytl-0257/.",
      "tldr_zh": "该研究针对强化学习（RL）在长时序操作任务中的采样效率低和忽略任务级语义信息的问题，提出了一种可解释的混合策略框架 HyTL。HyTL 通过线性时序逻辑（LTL）编码任务规范来提升性能和可解释性，并设计了三层决策结构：高层路径点规划模块基于 LTL 反馈提高探索效率，中层选择行为原语，低层指定环境交互参数。在四个挑战性操作任务上的实验验证了 HyTL 的有效性，并展示了其在机器人学习中的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IROS 2024. Code:https://github.com/Charlie0257/HyTL",
      "pdf_url": "http://arxiv.org/pdf/2412.20338v1",
      "published_date": "2024-12-29 03:34:53 UTC",
      "updated_date": "2024-12-29 03:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:40:36.736753"
    },
    {
      "arxiv_id": "2412.20331v1",
      "title": "Mind the Data Gap: Bridging LLMs to Enterprise Data Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Moe Kayali",
        "Fabian Wenz",
        "Nesime Tatbul",
        "Çağatay Demiralp"
      ],
      "abstract": "Leading large language models (LLMs) are trained on public data. However,\nmost of the world's data is dark data that is not publicly accessible, mainly\nin the form of private organizational or enterprise data. We show that the\nperformance of methods based on LLMs seriously degrades when tested on\nreal-world enterprise datasets. Current benchmarks, based on public data,\noverestimate the performance of LLMs. We release a new benchmark dataset, the\nGOBY Benchmark, to advance discovery in enterprise data integration. Based on\nour experience with this enterprise benchmark, we propose techniques to uplift\nthe performance of LLMs on enterprise data, including (1) hierarchical\nannotation, (2) runtime class-learning, and (3) ontology synthesis. We show\nthat, once these techniques are deployed, the performance on enterprise data\nbecomes on par with that of public data. The Goby benchmark can be obtained at\nhttps://goby-benchmark.github.io/.",
      "tldr_zh": "该论文指出，大型语言模型（LLMs）主要在公共数据上训练，但其在私有企业数据上的表现显著下降，因为现有基准基于公共数据而高估了性能。为解决这一数据鸿沟，研究者发布了GOBY Benchmark数据集，用于评估企业数据集成任务。论文提出三种提升技术，包括层次化标注（hierarchical annotation）、运行时类学习（runtime class-learning）和本体合成（ontology synthesis），这些方法能使LLMs在企业数据上的表现达到与公共数据相当的水平。该基准数据集可从https://goby-benchmark.github.io/获取，为企业数据集成研究提供了新工具。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "CIDR'25",
      "pdf_url": "http://arxiv.org/pdf/2412.20331v1",
      "published_date": "2024-12-29 03:07:20 UTC",
      "updated_date": "2024-12-29 03:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:40:49.012002"
    },
    {
      "arxiv_id": "2412.20329v1",
      "title": "Protein Structure Prediction in the 3D HP Model Using Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanny Espitia",
        "Yui Tik Pang",
        "James C. Gumbart"
      ],
      "abstract": "We address protein structure prediction in the 3D Hydrophobic-Polar lattice\nmodel through two novel deep learning architectures. For proteins under 36\nresidues, our hybrid reservoir-based model combines fixed random projections\nwith trainable deep layers, achieving optimal conformations with 25% fewer\ntraining episodes. For longer sequences, we employ a long short-term memory\nnetwork with multi-headed attention, matching best-known energy values. Both\narchitectures leverage a stabilized Deep Q-Learning framework with experience\nreplay and target networks, demonstrating consistent achievement of optimal\nconformations while significantly improving training efficiency compared to\nexisting methods.",
      "tldr_zh": "本研究使用深度强化学习解决 3D HP Model 中的蛋白质结构预测问题，提出两种新颖的深度学习架构。针对长度小于 36 个残基的蛋白质，混合 reservoir-based 模型结合固定随机投影和可训练深度层，能以 25% 更少的训练周期达到最优构象。对于较长序列，LSTM 与 multi-headed attention 网络则匹配已知的最好能量值。两种架构均采用稳定的 Deep Q-Learning 框架，包括经验回放和目标网络，显著提高了训练效率并一致实现最优构象。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20329v1",
      "published_date": "2024-12-29 02:55:54 UTC",
      "updated_date": "2024-12-29 02:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:41:00.674994"
    },
    {
      "arxiv_id": "2412.20321v1",
      "title": "Hypergraph-Based Dynamic Graph Node Classification",
      "title_zh": "基于超图的动态图节点分类",
      "authors": [
        "Xiaoxu Ma",
        "Chen Zhao",
        "Minglai Shao",
        "Yujie Lin"
      ],
      "abstract": "Node classification on static graphs has achieved significant success, but\nachieving accurate node classification on dynamic graphs where node topology,\nattributes, and labels change over time has not been well addressed. Existing\nmethods based on RNNs and self-attention only aggregate features of the same\nnode across different time slices, which cannot adequately address and capture\nthe diverse dynamic changes in dynamic graphs. Therefore, we propose a novel\nmodel named Hypergraph-Based Multi-granularity Dynamic Graph Node\nClassification (HYDG). After obtaining basic node representations for each\nslice through a GNN backbone, HYDG models the representations of each node in\nthe dynamic graph through two modules. The individual-level hypergraph captures\nthe spatio-temporal node representations between individual nodes, while the\ngroup-level hypergraph captures the multi-granularity group temporal\nrepresentations among nodes of the same class. Each hyperedge captures\ndifferent temporal dependencies of varying lengths by connecting multiple nodes\nwithin specific time ranges. More accurate representations are obtained through\nweighted information propagation and aggregation by the hypergraph neural\nnetwork. Extensive experiments on five real dynamic graph datasets using two\nGNN backbones demonstrate the superiority of our proposed framework.",
      "tldr_zh": "本论文解决了动态图节点分类的挑战，现有的基于 RNN 和 self-attention 方法仅聚合同一节点在不同时间切片的特征，无法充分捕捉动态图的拓扑、属性和标签变化。作者提出 HYDG（Hypergraph-Based Multi-granularity Dynamic Graph Node Classification）模型，先通过 GNN backbone 获取每个时间切片的节点基本表示，然后利用 individual-level hypergraph 捕捉个体节点间的时空表示，以及 group-level hypergraph 捕捉同一类节点的 multi-granularity 群组时间表示。每个 hyperedge 通过连接特定时间范围内的多个节点，实现加权信息传播和聚合，从而获得更准确的节点表示。在五个真实动态图数据集上的广泛实验验证了 HYDG 的优越性。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted in ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20321v1",
      "published_date": "2024-12-29 02:19:44 UTC",
      "updated_date": "2024-12-29 02:19:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:41:14.268665"
    },
    {
      "arxiv_id": "2412.20302v2",
      "title": "EXAdam: The Power of Adaptive Cross-Moments",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed M. Adly"
      ],
      "abstract": "This paper introduces EXAdam ($\\textbf{EX}$tended $\\textbf{Adam}$), a novel\noptimization algorithm that builds upon the widely-used Adam optimizer. EXAdam\nincorporates two key enhancements: (1) new debiasing terms for improved moment\nestimation and (2) a gradient-based acceleration mechanism for increased\nresponsiveness to the current loss landscape. These innovations work\nsynergistically to address limitations of the original Adam algorithm,\npotentially offering improved convergence properties, enhanced ability to\nescape saddle points, and potentially greater robustness to hyperparameter\nchoices, though this requires further investigation. We provide a theoretical\nanalysis of EXAdam's components and their interactions, highlighting the\nalgorithm's potential advantages in navigating complex optimization landscapes.\nEmpirical evaluations demonstrate EXAdam's superiority over Adam, achieving\n38.46% faster convergence and yielding improvements of 1.96%, 2.17%, and 1.17%\nin training, validation, and testing accuracies, respectively, when applied to\na CNN trained on the CIFAR-10 dataset. While these results are promising,\nfurther empirical validation across diverse tasks is essential to fully gauge\nEXAdam's efficacy. Nevertheless, EXAdam represents a significant advancement in\nadaptive optimization techniques, with promising implications for a wide range\nof machine learning applications. This work aims to contribute to the ongoing\ndevelopment of more efficient, adaptive, and universally applicable\noptimization methods in the field of machine learning and artificial\nintelligence.",
      "tldr_zh": "本研究引入了EXAdam优化算法，作为Adam优化器的扩展，通过添加新的debiasing术语来改善moment estimation，以及引入gradient-based acceleration机制来增强对当前损失景观的响应性。这些创新协同作用，可能提升收敛性能、逃逸saddle points的能力，以及对超参数选择的鲁棒性，并通过理论分析突显其在复杂优化景观中的优势。在实证评估中，EXAdam在CIFAR-10数据集上训练CNN时，比Adam快38.46%收敛，并分别提高训练、验证和测试准确率1.96%、2.17%和1.17%。尽管结果有前景，但需在更多任务上进一步验证，以确认EXAdam在自适应优化技术中的广泛适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20302v2",
      "published_date": "2024-12-29 00:11:54 UTC",
      "updated_date": "2025-05-16 08:00:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:41:25.194892"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 44,
  "processed_papers_count": 44,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T18:41:47.132898"
}
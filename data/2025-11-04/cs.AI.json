{
  "date": "2025-11-04",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-04 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤§é‡å…³äº **AI Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„æ·±åº¦ç ”ç©¶ï¼Œä»â€œAI ç§‘å­¦å®¶â€å…¨è‡ªåŠ¨ç§‘ç ”ç³»ç»Ÿï¼Œåˆ°å¤šæ™ºèƒ½ä½“åä½œä¸­çš„â€œåä½œé¸¿æ²Ÿâ€ï¼Œå†åˆ°æ¨ç†æ—¶çš„ Scaling Lawï¼ˆä¸²è¡Œä¼˜äºå¹¶è¡Œï¼‰ï¼›åŒæ—¶ï¼Œå…³äº LLM åœ¨æ¦‚ç‡åˆ†å¸ƒç†è§£ã€æ•°å­¦æ¨ç†å½¢å¼åŒ–ä»¥åŠæ··åˆæ¶æ„ï¼ˆTransformer-Mambaï¼‰çš„è®¨è®ºä¹Ÿååˆ†ç²¾å½©ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹ï¼šAI ç§‘å­¦å®¶ä¸æ™ºèƒ½ä½“åä½œ\n\n**#22 Kosmos: An AI Scientist for Autonomous Discovery**\n**Kosmosï¼šç”¨äºè‡ªä¸»å‘ç°çš„ AI ç§‘å­¦å®¶**\nè¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰åˆ†é‡çš„æ–‡ç« ã€‚ä½œè€…æå‡ºäº† **Kosmos**ï¼Œä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ•°æ®é©±åŠ¨ç§‘å­¦å‘ç°çš„ AI ç§‘å­¦å®¶ç³»ç»Ÿã€‚\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** Kosmos å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œé•¿è¾¾ 12 å°æ—¶çš„æ–‡çŒ®æœç´¢ã€å‡è®¾ç”Ÿæˆã€ä»£ç ç¼–å†™å’Œæ•°æ®åˆ†æå¾ªç¯ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–çš„**ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰**æ¥åœ¨ä¸åŒæ™ºèƒ½ä½“ï¼ˆæœç´¢ã€åˆ†æç­‰ï¼‰ä¹‹é—´å…±äº«ä¿¡æ¯ã€‚\n*   **å‘ç°ï¼š** åœ¨ 20 ä¸ªå¾ªç¯çš„è¿è¡Œä¸­ï¼Œå®ƒèƒ½é˜…è¯» 1500 ç¯‡è®ºæ–‡å¹¶æ‰§è¡Œ 4.2 ä¸‡è¡Œä»£ç ã€‚å®ƒä¸ä»…å¤ç°äº†æœªå…¬å¼€æ‰‹ç¨¿ä¸­çš„å‘ç°ï¼Œè¿˜åœ¨ä»£è°¢ç»„å­¦å’Œææ–™ç§‘å­¦ç­‰é¢†åŸŸåšå‡ºäº† 4 é¡¹æ–°é¢–çš„ç§‘å­¦è´¡çŒ®ã€‚\n\n**#45 The Collaboration Gap**\n**åä½œé¸¿æ²Ÿ**\næ¥è‡ª Microsoft Research ç­‰å›¢é˜Ÿçš„ç ”ç©¶ã€‚\n*   **æ ¸å¿ƒå‘ç°ï¼š** éšç€ AI ç³»ç»Ÿå‘å¤šæ™ºèƒ½ä½“å‘å±•ï¼Œä½œè€…å‘ç°äº†ä¸€ä¸ª**â€œåä½œé¸¿æ²Ÿï¼ˆCollaboration Gapï¼‰â€**ï¼šé‚£äº›å•ç‹¬è¡¨ç°å‡ºè‰²çš„æ¨¡å‹ï¼Œåœ¨éœ€è¦åä½œæ—¶å¾€å¾€è¡¨ç°å¤§å¹…ä¸‹é™ã€‚ç‰¹åˆ«æ˜¯å°å‹è’¸é¦æ¨¡å‹ï¼Œåœ¨åä½œä¸­å‡ ä¹å®Œå…¨å¤±æ•ˆã€‚\n*   **ç»“è®ºï¼š** ç®€å•çš„å¼ºå¼ºè”åˆå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œä½œè€…æå‡ºäº†ä¸€ç§â€œæ¥åŠ›æ¨ç†ï¼ˆrelay inferenceï¼‰â€ç­–ç•¥ï¼Œç”±å¼ºæ™ºèƒ½ä½“å¼•å¯¼å¼±æ™ºèƒ½ä½“ï¼Œèƒ½å¼¥è¡¥éƒ¨åˆ†å·®è·ã€‚\n\n**#102 The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute**\n**ä¸²è¡Œä¼˜åŠ¿ï¼šåœ¨ç›¸åŒè®¡ç®—é‡ä¸‹ï¼Œé€†ç†µæŠ•ç¥¨å‡»è´¥å¹¶è¡Œè‡ªæ´½æ€§**\nè¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†ç›®å‰ä¸»æµçš„â€œå¹¶è¡Œé‡‡æ · + å¤šæ•°æŠ•ç¥¨â€ï¼ˆSelf-Consistencyï¼‰çš„æ¨ç†å¢å¼ºèŒƒå¼ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä½œè€…å‘ç°åœ¨ç›¸åŒçš„ Token é¢„ç®—ä¸‹ï¼Œ**ä¸²è¡Œç¼©æ”¾ï¼ˆSequential Scalingï¼‰**â€”â€”å³è®©æ¨¡å‹åŸºäºä¹‹å‰çš„å°è¯•è¿›è¡Œè¿­ä»£ä¿®æ­£â€”â€”åœ¨ 95.6% çš„é…ç½®ä¸­ä¼˜äºå¹¶è¡Œæ–¹æ³•ã€‚\n*   **æ–¹æ³•ï¼š** æå‡ºäº†ä¸€ç§**é€†ç†µåŠ æƒæŠ•ç¥¨ï¼ˆInverse-Entropy Weighted Votingï¼‰**æœºåˆ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯å¤§å¹…æå‡å‡†ç¡®ç‡ã€‚è¿™å¯èƒ½é¢„ç¤ºç€ Inference-time scaling çš„æ–°æ–¹å‘ã€‚\n\n---\n\n### ğŸ§  LLM åŸºç¡€èƒ½åŠ›ä¸è¯„ä¼°\n\n**#1 Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge**\n**å¤§è¯­è¨€æ¨¡å‹çš„æµè¡Œç—…å­¦ï¼šè§‚æµ‹åˆ†å¸ƒçŸ¥è¯†åŸºå‡†**\n*   **æ ¸å¿ƒé—®é¢˜ï¼š** LLM èƒ½å›ç­”äº‹å®æ€§é—®é¢˜ï¼ˆå¦‚è‹±å›½é¦–éƒ½æ˜¯å“ªï¼‰ï¼Œä½†å®ƒä»¬æŒæ¡ç°å®ä¸–ç•Œçš„**æ¦‚ç‡åˆ†å¸ƒ**å—ï¼ˆå¦‚ç¾å›½CSæ¯•ä¸šç”Ÿçš„æ€§åˆ«æ¯”ä¾‹ï¼‰ï¼Ÿ\n*   **å‘ç°ï¼š** åŸºäº Pearl çš„å› æœå±‚çº§ï¼ˆPCHï¼‰ï¼Œä½œè€…å‘ç° LLM åœ¨ç¬¬ä¸€å±‚ï¼ˆè§‚æµ‹åˆ†å¸ƒï¼‰çš„è¡¨ç°å°±å¾ˆå·®ï¼Œæ— æ³•å†…åŒ–ç°å®ä¸–ç•Œçš„ç»Ÿè®¡è§„å¾‹ã€‚è¿™æ„å‘³ç€å®ƒä»¬åœ¨æ›´é«˜å±‚çš„å¹²é¢„å’Œåäº‹å®æ¨ç†èƒ½åŠ›ä¸Šä¹Ÿå­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ã€‚\n\n**#121 FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels**\n**FATEï¼šå¤šéš¾åº¦çº§åˆ«çš„å‰æ²¿ä»£æ•°å½¢å¼åŒ–åŸºå‡†**\n*   **è´¡çŒ®ï¼š** æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°å­¦åŸºå‡† FATEï¼Œä¸“æ³¨äºå½¢å¼åŒ–ä»£æ•°ã€‚å…¶ä¸­çš„ FATE-X æ•°æ®é›†éš¾åº¦è¶…è¿‡äº†åšå£«èµ„æ ¼è€ƒè¯•ï¼Œä¸”è¶…å‡ºäº† Mathlib åº“çš„è¦†ç›–èŒƒå›´ã€‚\n*   **å‘ç°ï¼š** å³ä½¿æ˜¯ç›®å‰æœ€å¼ºçš„æ¨¡å‹ï¼Œåœ¨ FATE-X ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º 0%ã€‚è¿™æ­ç¤ºäº† LLM åœ¨å¤„ç†æ·±å±‚ã€æŠ½è±¡æ•°å­¦æ¨ç†æ—¶çš„å·¨å¤§ç©ºç™½ã€‚\n\n**#138 From Euler to Today: Universal Mathematical Fallibility A Large-Scale Computational Analysis of Errors in ArXiv Papers**\n**ä»æ¬§æ‹‰åˆ°ä»Šå¤©ï¼šæ•°å­¦çš„æ™®éæ˜“é”™æ€§â€”â€”ArXiv è®ºæ–‡é”™è¯¯çš„åœ¨å¤§è§„æ¨¡è®¡ç®—åˆ†æ**\n*   **è¶£é—»ï¼š** è¿™æ˜¯ä¸€ç¯‡æœ‰è¶£çš„â€œå…ƒåˆ†æâ€æ–‡ç« ã€‚ä½œè€…æ„å»ºç³»ç»Ÿè‡ªåŠ¨æ£€æŸ¥äº† 3.7 ä¸‡ç¯‡ arXiv æ•°å­¦è®ºæ–‡ï¼Œå‘ç°äº†å¤§é‡é”™è¯¯ï¼Œç”šè‡³åŒ…æ‹¬æ¬§æ‹‰å’Œç‹„åˆ©å…‹é›·ç­‰æ•°å­¦å·¨åŒ çš„ä½œå“ã€‚\n*   **æ•°æ®ï¼š** æ•°å€¼åˆ†æï¼ˆmath.NAï¼‰é¢†åŸŸçš„é”™è¯¯ç‡é«˜è¾¾ 9.6%ï¼Œè€ŒèŒƒç•´è®ºï¼ˆmath.CTï¼‰é”™è¯¯ç‡ä¸º 0%ã€‚\n\n---\n\n### âš™ï¸ æ¶æ„æ•ˆç‡ä¸ç³»ç»Ÿä¼˜åŒ–\n\n**#50 Apriel-H1: Towards Efficient Enterprise Reasoning Models**\n**Apriel-H1ï¼šè¿ˆå‘é«˜æ•ˆçš„ä¼ä¸šçº§æ¨ç†æ¨¡å‹**\n*   **æ–¹æ³•ï¼š** é’ˆå¯¹ Transformer æ¨ç†æ—¶çš„ KV Cache ç“¶é¢ˆï¼Œä½œè€…æå‡ºäº† **Apriel-H1**ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œé€šè¿‡è’¸é¦å°†éƒ¨åˆ† Attention å±‚æ›¿æ¢ä¸º **Mambaï¼ˆSSMï¼‰** å±‚ã€‚\n*   **æ•ˆæœï¼š** åœ¨ä¿æŒæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ¨ç†ååé‡æé«˜äº† 2 å€ä»¥ä¸Šï¼Œè¯æ˜äº†æ··åˆ SSM-Transformer æ¶æ„åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚\n\n**#118 Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live**\n**Continuumï¼šåŸºäº KV Cache ç”Ÿå­˜æ—¶é—´çš„é«˜æ•ˆå¤šè½® LLM æ™ºèƒ½ä½“è°ƒåº¦**\n*   **é—®é¢˜ï¼š** æ™ºèƒ½ä½“åœ¨è°ƒç”¨å·¥å…·ï¼ˆTool Useï¼‰æ—¶ä¼šäº§ç”Ÿåœé¡¿ï¼Œå¯¼è‡´ KV Cache è¢«é©±é€ï¼Œé‡æ–°åŠ è½½å¼€é”€å¤§ã€‚\n*   **æ–¹æ³•ï¼š** å¼•å…¥äº† TTLï¼ˆTime-to-Liveï¼‰æœºåˆ¶ï¼Œæ ¹æ®é‡è®¡ç®—æˆæœ¬å’Œæ’é˜Ÿå»¶è¿Ÿï¼Œæ™ºèƒ½åœ°å†³å®šæ˜¯å¦åœ¨ GPU ä¸­ä¿ç•™ KV Cacheã€‚è¿™å¯¹äºé•¿é“¾è·¯çš„ Agent ä»»åŠ¡è°ƒåº¦éå¸¸å…³é”®ã€‚\n\n**#36 Using Span Queries to Optimize for Cache and Attention Locality**\n**åˆ©ç”¨ Span Queries ä¼˜åŒ–ç¼“å­˜å’Œæ³¨æ„åŠ›å±€éƒ¨æ€§**\n*   **è´¡çŒ®ï¼š** æå‡º **Span Query** æ¦‚å¿µï¼Œå°† Chatã€RAG å’Œ Agent å·¥ä½œè´Ÿè½½ç»Ÿä¸€ä¸ºä¸€ç§è¡¨è¾¾å¼æ ‘ã€‚\n*   **æ•ˆæœï¼š** é€šè¿‡ä¼˜åŒ– KV Cache å±€éƒ¨æ€§ï¼Œåœ¨é Chat åœºæ™¯ä¸‹å°†é¦–å­—å»¶è¿Ÿï¼ˆTTFTï¼‰é™ä½äº† 10-20 å€ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸å®‰å…¨\n\n**#29 When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning**\n**å½“æ¨¡æ€å†²çªæ—¶ï¼šå¤šæ¨¡æ€æ¨ç†çš„è¯Šæ–­è§†è§’**\n*   **æ¦‚å¿µï¼š** æå‡ºäº† **Modality Sabotageï¼ˆæ¨¡æ€ç ´åï¼‰** çš„æ¦‚å¿µï¼Œå³å•ä¸€æ¨¡æ€çš„é«˜ç½®ä¿¡åº¦é”™è¯¯ä¼šè¯¯å¯¼æ•´ä½“èåˆç»“æœã€‚\n*   **æ–¹æ³•ï¼š** è®¾è®¡äº†ä¸€ä¸ªè¯Šæ–­å±‚ï¼Œå°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºä¸€ä¸ªæ™ºèƒ½ä½“è¿›è¡Œå®¡è®¡ï¼Œæ‰¾å‡ºæ˜¯è°åœ¨â€œæ’’è°â€ã€‚\n\n**#20 Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything**\n**Agent-Omniï¼šé€šè¿‡æ¨¡å‹åè°ƒè¿›è¡Œæµ‹è¯•æ—¶å¤šæ¨¡æ€æ¨ç†**\n*   **æ¶æ„ï¼š** æ—¢ç„¶è®­ç»ƒä¸€ä¸ªå…¨èƒ½çš„å¤šæ¨¡æ€æ¨¡å‹å¾ˆéš¾ï¼Œä¸å¦‚é€šè¿‡ Master-Agent ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ã€‚\n*   **æ•ˆæœï¼š** è¿™ç§æ— éœ€é‡æ–°è®­ç»ƒçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œåœ¨å¤„ç†å¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº† SOTA æ€§èƒ½ã€‚\n\n**#58 On The Dangers of Poisoned LLMs In Security Automation**\n**å®‰å…¨è‡ªåŠ¨åŒ–ä¸­ä¸­æ¯’ LLM çš„å±é™©**\n*   **è­¦ç¤ºï¼š** å±•ç¤ºäº†å³ä½¿æ˜¯ç»è¿‡å¾®è°ƒçš„â€œæ”¹è¿›ç‰ˆâ€æ¨¡å‹ï¼Œå¦‚æœå—åˆ°æ•°æ®æŠ•æ¯’ï¼ˆPoisoningï¼‰ï¼Œå¯èƒ½ä¼šäº§ç”Ÿç‰¹å®šçš„åè§ï¼Œå¯¼è‡´å®‰å…¨è­¦æŠ¥è¢«ç³»ç»Ÿæ€§åœ°å¿½ç•¥ã€‚è¿™å¯¹äºåœ¨å®‰å…¨è¿è¥ä¸­å¿ƒï¼ˆSOCï¼‰ä¸­ä½¿ç”¨ LLM æå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚",
  "papers": [
    {
      "arxiv_id": "2511.03070v2",
      "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge",
      "title_zh": "å¤§å‹è¯­è¨€æ¨¡å‹çš„æµè¡Œç—…å­¦ï¼šè§‚æµ‹åˆ†å¸ƒçŸ¥è¯†åŸºå‡†",
      "authors": [
        "Drago Plecko",
        "Patrik Okanovic",
        "Shreyas Havaldar",
        "Torsten Hoefler",
        "Elias Bareinboim"
      ],
      "abstract": "Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., \"what is the capital of England?\"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., \"what is the sex of a computer science graduate in the US?\"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦çœŸæ­£æŒæ¡äº†æè¿°ç°å®ä¸–ç•Œçš„æ¦‚ç‡åˆ†å¸ƒçŸ¥è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯äº‹å®æ€§çŸ¥è¯†ã€‚å°½ç®¡LLMså¸¸è¢«è§†ä¸ºç°å®åˆ†å¸ƒçš„é€šç”¨é€¼è¿‘å™¨ï¼Œä½†ç»Ÿè®¡å­¦ä¸­çš„â€œç»´æ•°ç¾éš¾â€ï¼ˆcurse of dimensionalityï¼‰å¯¹æ­¤æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè·¨è¶Šç»æµã€å¥åº·ã€æ•™è‚²å’Œç¤¾ä¼šè¡Œä¸ºç­‰å¤šä¸ªé¢†åŸŸï¼Œè¯„ä¼°LLMsæ˜¯å¦å…·å¤‡å…³äºç°å®ä¸–ç•Œäººç¾¤çš„ç»éªŒåˆ†å¸ƒçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œå¹¶æœªè‡ªç„¶åœ°å†…åŒ–ç°å®ä¸–ç•Œçš„ç»Ÿè®¡è§„å¾‹ã€‚åŸºäºPearlçš„å› æœå±‚çº§ï¼ˆPearl's Causal Hierarchy, PCHï¼‰ç†è®ºï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºLLMsç¼ºä¹è§‚å¯Ÿåˆ†å¸ƒï¼ˆPCHç¬¬1å±‚ï¼‰çš„çŸ¥è¯†ï¼Œè¿™è¿›è€Œæ„å‘³ç€æ¨¡å‹åœ¨å¹²é¢„ï¼ˆç¬¬2å±‚ï¼‰å’Œåäº‹å®ï¼ˆç¬¬3å±‚ï¼‰å±‚é¢çš„çŸ¥è¯†ä¹Ÿå—åˆ°æ ¹æœ¬é™åˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03070v2",
      "published_date": "2025-11-04 23:34:52 UTC",
      "updated_date": "2025-12-04 15:11:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:45:23.254598+00:00"
    },
    {
      "arxiv_id": "2511.03056v1",
      "title": "Reading Between the Lines: The One-Sided Conversation Problem",
      "title_zh": "å­—é‡Œè¡Œé—´ï¼šå•ä¾§å¯¹è¯é—®é¢˜",
      "authors": [
        "Victoria Ebert",
        "Rishabh Singh",
        "Tuochao Chen",
        "Noah A. Smith",
        "Shyamnath Gollakota"
      ],
      "abstract": "Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡é’ˆå¯¹è¿œç¨‹åŒ»ç–—ã€å‘¼å«ä¸­å¿ƒå’Œæ™ºèƒ½çœ¼é•œç­‰åªèƒ½å½•åˆ¶å•ä¾§å¯¹è¯çš„ç°å®åœºæ™¯ï¼Œæ­£å¼å®šä¹‰äº†â€œå•ä¾§å¯¹è¯é—®é¢˜â€ï¼ˆone-sided conversation problem, 1SCï¼‰ï¼Œå³ä»å¯¹è¯çš„ä¸€æ–¹è¿›è¡Œæ¨æ–­å’Œå­¦ä¹ ã€‚ç ”ç©¶æ¢è®¨äº†ä¸¤é¡¹ä¸»è¦ä»»åŠ¡ï¼šä¸ºå®æ—¶åº”ç”¨é‡å»ºç¼ºå¤±çš„è¯´è¯äººè½®æ¬¡ï¼Œä»¥åŠç›´æ¥ä»å•ä¾§è½¬å½•ç”Ÿæˆæ‘˜è¦ã€‚ä½œè€…åœ¨MultiWOZã€DailyDialogå’ŒCandoræ•°æ®é›†ä¸Šè¯„ä¼°äº†æç¤ºï¼ˆpromptingï¼‰å’Œå¾®è°ƒï¼ˆfinetunedï¼‰æ¨¡å‹ï¼Œå¹¶ç»“åˆäººç±»A/Bæµ‹è¯•å’ŒLLM-as-a-judgeæŒ‡æ ‡è¿›è¡Œäº†éªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼Œè·å–ä¸€ä¸ªæœªæ¥è½®æ¬¡å’Œè¯è¯­é•¿åº¦ä¿¡æ¯èƒ½æ˜¾è‘—æ”¹å–„é‡å»ºæ•ˆæœï¼Œå ä½ç¬¦æç¤ºæœ‰åŠ©äºå‡è½»å¹»è§‰ï¼Œä¸”å¤§å‹æ¨¡å‹åœ¨æç¤ºä¸‹è¡¨ç°è‰¯å¥½ï¼Œè€Œå°å‹æ¨¡å‹åˆ™éœ€è¦å¾®è°ƒã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜æ— éœ€é‡å»ºç¼ºå¤±çš„è½®æ¬¡å³å¯ç”Ÿæˆé«˜è´¨é‡çš„æ‘˜è¦ã€‚è¯¥å·¥ä½œå°†1SCä½œä¸ºä¸€ä¸ªæ–°çš„æŒ‘æˆ˜æå‡ºï¼Œä¸ºéšç§æ„ŸçŸ¥å‹å¯¹è¯AIçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 6 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.03056v1",
      "published_date": "2025-11-04 22:53:57 UTC",
      "updated_date": "2025-11-04 22:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:45:47.610732+00:00"
    },
    {
      "arxiv_id": "2511.03051v1",
      "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation",
      "title_zh": "æ— äººå·¥ä»‹å…¥ï¼šé¢å‘æ¨èçš„å¤§è§„æ¨¡æ™ºèƒ½ä½“è¯„ä¼°",
      "authors": [
        "Tao Zhang",
        "Kehui Yao",
        "Luyi Ma",
        "Jiao Chen",
        "Reza Yousefi Maragheh",
        "Kai Zhao",
        "Jianpeng Xu",
        "Evren Korpeoglu",
        "Sushant Kumar",
        "Kannan Achan"
      ],
      "abstract": "Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ScalingEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ç ”ç©¶ï¼Œæ—¨åœ¨é€šè¿‡å…±è¯†é©±åŠ¨çš„è¯„ä¼°åè®®ç³»ç»Ÿåœ°æ¯”è¾ƒåŒ…æ‹¬GPTã€Geminiã€Claudeå’ŒLlamaåœ¨å†…çš„36ç§å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶(multi-agent framework)ï¼Œé€šè¿‡å¯æ‰©å±•çš„å¤šæ•°æŠ•ç¥¨æœºåˆ¶å°†æ¨¡å¼å®¡è®¡å’Œé—®é¢˜ä»£ç èšåˆä¸ºåœ°é¢å®å†µæ ‡ç­¾(ground-truth labels)ï¼Œå®ç°äº†æ— éœ€äººå·¥æ ‡æ³¨çš„LLMè¯„ä¼°å™¨å¯å¤ç°æ¯”è¾ƒã€‚åœ¨å¤§è§„æ¨¡äº’è¡¥å•†å“æ¨èåœºæ™¯çš„åº”ç”¨ä¸­ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ­ç¤ºäº†Anthropic Claude 3.5 Sonnetè¡¨ç°å‡ºæœ€é«˜çš„å†³ç­–ç½®ä¿¡åº¦ï¼ŒGemini 1.5 Proæ•´ä½“æ€§èƒ½æœ€ä½³ï¼ŒGPT-4oæä¾›äº†æœ€ä¼˜çš„å»¶è¿Ÿ-å‡†ç¡®ç‡-æˆæœ¬æƒè¡¡ï¼Œè€ŒGPT-OSS 20Båœ¨å¼€æºæ¨¡å‹ä¸­é¢†å…ˆã€‚æ­¤å¤–ï¼Œåˆ†ææ˜¾ç¤ºæ¨¡å‹åœ¨ç»“æ„åŒ–é¢†åŸŸï¼ˆå¦‚ç”µå­äº§å“ï¼‰å…·æœ‰å¼ºå…±è¯†ï¼Œä½†åœ¨ç”Ÿæ´»æ–¹å¼ç±»åˆ«ï¼ˆå¦‚æœè£…ï¼‰ä¸­å­˜åœ¨åˆ†æ­§ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†ScalingEvalä½œä¸ºâ€œLLMs as judgesâ€çš„å¯å¤ç°åŸºå‡†ï¼Œä¸ºæ¨¡å‹çš„æ‰©å±•æ€§ä¸å¯é æ€§æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "4 page, NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle",
      "pdf_url": "https://arxiv.org/pdf/2511.03051v1",
      "published_date": "2025-11-04 22:49:39 UTC",
      "updated_date": "2025-11-04 22:49:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:46:13.279779+00:00"
    },
    {
      "arxiv_id": "2511.05577v1",
      "title": "Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction",
      "title_zh": "é¢å‘å¤šæ¨¡æ€èšåˆç‰©æ€§èƒ½é¢„æµ‹çš„è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "An Vuong",
        "Minh-Hao Van",
        "Prateek Verma",
        "Chen Zhao",
        "Xintao Wu"
      ],
      "abstract": "Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ææ–™ç§‘å­¦é¢†åŸŸåº”ç”¨å—é™çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œèšåˆç‰©æ€§è´¨é¢„æµ‹æ–¹é¢çš„ç©ºç™½ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€èšåˆç‰©æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒ(instruction-tuning)å’ŒLoRAæŠ€æœ¯å¯¹VLMsè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹è¡¨ç°ä¼˜äºå•æ¨¡æ€æ–¹æ³•å’Œç°æœ‰åŸºçº¿ï¼Œå……åˆ†å±•ç¤ºäº†å¤šæ¨¡æ€å­¦ä¹ åœ¨æå‡é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç»Ÿä¸€æ¨¡å‹å¤„ç†å¤šç§æ€§è´¨é¢„æµ‹ï¼Œæœ‰æ•ˆå‡å°‘äº†ä¸ºä¸åŒæ€§è´¨è®­ç»ƒç‹¬ç«‹æ¨¡å‹çš„éœ€æ±‚ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†éƒ¨ç½²ä¸ç»´æŠ¤æˆæœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05577v1",
      "published_date": "2025-11-04 22:32:53 UTC",
      "updated_date": "2025-11-04 22:32:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:46:32.857519+00:00"
    },
    {
      "arxiv_id": "2511.03753v2",
      "title": "Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices",
      "title_zh": "åŸºäºæ ¼æ‹‰å§†è§’åœºçš„è”é‚¦å­¦ä¹ ç”¨äºå¼‚æ„ç‰©è”ç½‘è®¾å¤‡éšç§ä¿æŠ¤å¿ƒç”µå›¾åˆ†ç±»",
      "authors": [
        "Youssef Elmir",
        "Yassine Himeur",
        "Abbes Amira"
      ],
      "abstract": "This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ç‰©è”ç½‘(IoT)åŒ»ç–—ç¯å¢ƒçš„è”é‚¦å­¦ä¹ (Federated Learning)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä¿æŠ¤éšç§çš„å¿ƒç”µå›¾(ECG)åˆ†ç±»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¸€ç»´ECGä¿¡å·è½¬æ¢ä¸ºäºŒç»´Gramian Angular Field (GAF)å›¾åƒï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ï¼ŒåŒæ—¶ç¡®ä¿æ•æ„ŸåŒ»ç–—æ•°æ®ä¿ç•™åœ¨æœ¬åœ°è®¾å¤‡ä¸Šã€‚ä½œä¸ºé¦–æ‰¹åœ¨å¼‚æ„IoTè®¾å¤‡ï¼ˆåŒ…æ‹¬æœåŠ¡å™¨ã€ç¬”è®°æœ¬ç”µè„‘å’Œèµ„æºå—é™çš„Raspberry Pi 4ï¼‰ä¸Šå®éªŒéªŒè¯åŸºäºGAFçš„è”é‚¦ECGåˆ†ç±»çš„ç ”ç©¶ä¹‹ä¸€ï¼Œè¯¥å·¥ä½œé‡åŒ–äº†ç³»ç»Ÿçš„æ€§èƒ½å’Œé€šä¿¡æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥FL-GAFæ¨¡å‹åœ¨å¤šå®¢æˆ·ç«¯è®¾ç½®ä¸­å®ç°äº†95.18%çš„é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼Œåœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒæ—¶é—´ä¸Šå‡æ˜¾è‘—ä¼˜äºå•å®¢æˆ·ç«¯åŸºçº¿ã€‚å°½ç®¡GAFè½¬æ¢å¢åŠ äº†è®¡ç®—å¤æ‚æ€§ï¼Œä½†è¯¥æ¡†æ¶ä»ä¿æŒäº†é«˜æ•ˆçš„èµ„æºåˆ©ç”¨å’Œé€šä¿¡å¼€é”€ï¼Œè¯æ˜äº†è½»é‡çº§ã€éšç§ä¿æŠ¤AIåœ¨æ™ºèƒ½å¥åº·ç›‘æµ‹ç³»ç»Ÿä¸­è¿›è¡Œå¯æ‰©å±•è¾¹ç¼˜éƒ¨ç½²çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "06 pages, 03 figures, accepted for presentation at the 7th IEEE Computing, Communications and IoT Applications Conference (ComComAp 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.03753v2",
      "published_date": "2025-11-04 22:23:59 UTC",
      "updated_date": "2025-11-11 17:37:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:47:13.710505+00:00"
    },
    {
      "arxiv_id": "2511.03023v1",
      "title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework",
      "title_zh": "PublicAgentï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¼€æ”¾æ•°æ®åˆ†ææ¡†æ¶çš„å¤šæ™ºèƒ½ä½“è®¾è®¡åŸåˆ™",
      "authors": [
        "Sina Montazeri",
        "Yunhe Feng",
        "Kewei Sha"
      ],
      "abstract": "Open data repositories hold potential for evidence-based decision-making, yet are inaccessible to non-experts lacking expertise in dataset discovery, schema mapping, and statistical analysis. Large language models show promise for individual tasks, but end-to-end analytical workflows expose fundamental limitations: attention dilutes across growing contexts, specialized reasoning patterns interfere, and errors propagate undetected. We present PublicAgent, a multi-agent framework that addresses these limitations through decomposition into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This architecture maintains focused attention within agent contexts and enables validation at each stage. Evaluation across five models and 50 queries derives five design principles for multi-agent LLM systems. First, specialization provides value independent of model strength--even the strongest model shows 97.5% agent win rates, with benefits orthogonal to model scale. Second, agents divide into universal (discovery, analysis) and conditional (report, intent) categories. Universal agents show consistent effectiveness (std dev 12.4%) while conditional agents vary by model (std dev 20.5%). Third, agents mitigate distinct failure modes--removing discovery or analysis causes catastrophic failures (243-280 instances), while removing report or intent causes quality degradation. Fourth, architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery), indicating workflow management value rather than reasoning enhancement. Fifth, wide variance in agent effectiveness across models (42-96% for analysis) requires model-aware architecture design. These principles guide when and why specialization is necessary for complex analytical workflows while enabling broader access to public data through natural language interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PublicAgentï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éä¸“å®¶ç”¨æˆ·åœ¨åˆ©ç”¨å¼€æ”¾æ•°æ®å­˜å‚¨åº“è¿›è¡Œå¾ªè¯å†³ç­–æ—¶é¢ä¸´çš„éšœç¢ã€‚é’ˆå¯¹å•ä¸€æ¨¡å‹åœ¨ç«¯åˆ°ç«¯åˆ†æå·¥ä½œæµä¸­å‡ºç°çš„æ³¨æ„åŠ›åˆ†æ•£ã€æ¨ç†å¹²æ‰°å’Œé”™è¯¯ä¼ æ’­ç­‰å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å°†ä»»åŠ¡åˆ†è§£ä¸ºæ„å›¾æ¾„æ¸…ã€æ•°æ®é›†å‘ç°ã€åˆ†æå’ŒæŠ¥å‘Šå››ä¸ªä¸“ä¸šåŒ–æ™ºèƒ½ä½“ï¼Œä»è€Œåœ¨æ¯ä¸ªé˜¶æ®µä¿æŒä¸Šä¸‹æ–‡ä¸“æ³¨å¹¶å®ç°éªŒè¯ã€‚é€šè¿‡å¯¹5ç§æ¨¡å‹å’Œ50ä¸ªæŸ¥è¯¢çš„è¯„ä¼°ï¼Œç ”ç©¶æ€»ç»“äº†å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿçš„äº”å¤§è®¾è®¡åŸåˆ™ã€‚å®éªŒå‘ç°ï¼Œä¸“ä¸šåŒ–åˆ†å·¥ç‹¬ç«‹äºæ¨¡å‹èƒ½åŠ›äº§ç”Ÿä»·å€¼ï¼Œå³ä½¿æœ€å¼ºæ¨¡å‹ä¹Ÿèƒ½è·å¾—97.5%çš„èƒœç‡ï¼›æ™ºèƒ½ä½“å¯åˆ’åˆ†ä¸ºé€šç”¨å‹ï¼ˆå¦‚å‘ç°ã€åˆ†æï¼‰å’Œæ¡ä»¶å‹ï¼ˆå¦‚æŠ¥å‘Šã€æ„å›¾ï¼‰ï¼Œä¸”å„è‡ªç¼“è§£äº†ä»ç¾éš¾æ€§å¤±è´¥åˆ°è´¨é‡ä¸‹é™ç­‰ä¸åŒæ¨¡å¼çš„é”™è¯¯ã€‚æ­¤å¤–ï¼Œæ¶æ„ä¼˜åŠ¿åœ¨ä¸åŒä»»åŠ¡å¤æ‚åº¦ä¸‹ä¿æŒç¨³å®šï¼Œè¡¨æ˜äº†å·¥ä½œæµç®¡ç†çš„ä»·å€¼ï¼Œä½†æ™ºèƒ½ä½“æœ‰æ•ˆæ€§åœ¨ä¸åŒæ¨¡å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†æ¨¡å‹æ„ŸçŸ¥æ¶æ„è®¾è®¡çš„å¿…è¦æ€§ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºå¤æ‚åˆ†æå·¥ä½œæµçš„ä¸“ä¸šåŒ–è®¾è®¡æä¾›äº†æŒ‡å¯¼ï¼Œè¿˜é€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£æœ‰æ•ˆä¿ƒè¿›äº†å…¬å…±æ•°æ®çš„å¹¿æ³›è·å–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03023v1",
      "published_date": "2025-11-04 21:48:11 UTC",
      "updated_date": "2025-11-04 21:48:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:47:25.608532+00:00"
    },
    {
      "arxiv_id": "2511.03022v1",
      "title": "Adaptive-Sensorless Monitoring of Shipping Containers",
      "title_zh": "æµ·è¿é›†è£…ç®±çš„è‡ªé€‚åº”æ— ä¼ æ„Ÿå™¨ç›‘æµ‹",
      "authors": [
        "Lingqing Shen",
        "Chi Heem Wong",
        "Misaki Mito",
        "Arnab Chakrabarti"
      ],
      "abstract": "Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\\sim$ 2.31$^\\circ$C (vs 2.43$^\\circ$C by sensorless) for temperature and 5.72 $\\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\\sim$ 3.26$^\\circ$C for temperature (vs 3.38$^\\circ$C by sensorless) and 7.70 $\\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·è¿é›†è£…ç®±æ¸©æ¹¿åº¦ç›‘æµ‹ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§è¯¯å·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º\"adaptive-sensorless\"ï¼ˆè‡ªé€‚åº”æ— ä¼ æ„Ÿå™¨ï¼‰ç›‘æµ‹çš„æ–°ç±»åˆ«æ¨¡å‹ã€‚ä¼ ç»Ÿçš„æ— ä¼ æ„Ÿå™¨ç›‘æµ‹è™½ç„¶åˆ©ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹å†…éƒ¨æ¡ä»¶ï¼Œä½†å¾€å¾€å¿½ç•¥å®æ—¶é¥æµ‹ä¿¡æ¯å¯¼è‡´é¢„æµ‹åå·®ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†\"residual correction method\"ï¼ˆæ®‹å·®æ ¡æ­£æ–¹æ³•ï¼‰ï¼Œé€šè¿‡è§‚å¯Ÿå®æ—¶é¥æµ‹æ•°æ®æ¥ä¿®æ­£æ— ä¼ æ„Ÿå™¨æ¨¡å‹çš„ç³»ç»Ÿåå·®ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨åŒ…å«348ä¸‡ä¸ªæ•°æ®ç‚¹çš„å­¦æœ¯ç•Œæœ€å¤§é›†è£…ç®±ä¼ æ„Ÿå™¨æ•°æ®é›†è¿›è¡Œäº†æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿæ•°æ®çš„ä¿ç•™é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ¸©åº¦å’Œç›¸å¯¹æ¹¿åº¦çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰åŠå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å‡æ˜¾è‘—ä½äºåŸºçº¿æ— ä¼ æ„Ÿå™¨æ¨¡å‹ã€‚è¿™é¡¹æŠ€æœ¯å®ç°äº†æ›´ç²¾ç¡®çš„è´§ç‰©ç›‘æµ‹å’Œæ—©æœŸé£é™©æ£€æµ‹ï¼ŒåŒæ—¶é™ä½äº†å…¨çƒèˆªè¿å¯¹å…¨æ—¶ç½‘ç»œè¿æ¥çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in 2025 IEEE Big Data",
      "pdf_url": "https://arxiv.org/pdf/2511.03022v1",
      "published_date": "2025-11-04 21:47:00 UTC",
      "updated_date": "2025-11-04 21:47:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:47:46.263351+00:00"
    },
    {
      "arxiv_id": "2511.03020v1",
      "title": "Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods",
      "title_zh": "åŸºäºç»Ÿè®¡æ–¹æ³•çš„ç”µå­å•†åŠ¡å¹³å°ç½‘ç»œæ”»å‡»æ¨¡å¼æ¢ç´¢æ€§åˆ†æ",
      "authors": [
        "Fatimo Adenike Adeniya"
      ],
      "abstract": "Cyberattacks on e-commerce platforms have grown in sophistication, threatening consumer trust and operational continuity. This research presents a hybrid analytical framework that integrates statistical modelling and machine learning for detecting and forecasting cyberattack patterns in the e-commerce domain. Using the Verizon Community Data Breach (VCDB) dataset, the study applies Auto ARIMA for temporal forecasting and significance testing, including a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday shopping events experienced significantly more severe cyberattacks than non-holiday periods. ANOVA was also used to examine seasonal variation in threat severity, while ensemble machine learning models (XGBoost, LightGBM, and CatBoost) were employed for predictive classification. Results reveal recurrent attack spikes during high-risk periods such as Black Friday and holiday seasons, with breaches involving Personally Identifiable Information (PII) exhibiting elevated threat indicators. Among the models, CatBoost achieved the highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247). The framework uniquely combines seasonal forecasting with interpretable ensemble learning, enabling temporal risk anticipation and breach-type classification. Ethical considerations, including responsible use of sensitive data and bias assessment, were incorporated. Despite class imbalance and reliance on historical data, the study provides insights for proactive cybersecurity resource allocation and outlines directions for future real-time threat detection research.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆç»Ÿè®¡å»ºæ¨¡å’Œæœºå™¨å­¦ä¹ çš„æ··åˆåˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹å’Œé¢„æµ‹ç”µå­å•†åŠ¡å¹³å°ä¸Šçš„ç½‘ç»œæ”»å‡»æ¨¡å¼ã€‚ç ”ç©¶åˆ©ç”¨Verizon Community Data Breach (VCDB)æ•°æ®é›†ï¼Œé€šè¿‡Auto ARIMAè¿›è¡Œæ—¶é—´é¢„æµ‹ï¼Œå¹¶åº”ç”¨Mann-Whitney Uæ£€éªŒå’ŒANOVAåˆ†æå¨èƒä¸¥é‡ç¨‹åº¦çš„å­£èŠ‚æ€§å˜åŒ–ã€‚ç»Ÿè®¡åˆ†æè¯å®ï¼Œå‡æ—¥è´­ç‰©æ´»åŠ¨æœŸé—´é­é‡çš„ç½‘ç»œæ”»å‡»æ˜¾è‘—ä¸¥é‡äºéå‡æ—¥æœŸé—´ã€‚åœ¨é¢„æµ‹åˆ†ç±»æ–¹é¢ï¼Œç ”ç©¶é‡‡ç”¨äº†XGBoostã€LightGBMå’ŒCatBoostç­‰é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¶‰åŠä¸ªäººèº«ä»½ä¿¡æ¯(PII)çš„æ•°æ®æ³„éœ²æ˜¾ç¤ºå‡ºæ›´é«˜çš„å¨èƒæŒ‡æ ‡ï¼Œå…¶ä¸­CatBoostæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°äº†85.29%çš„å‡†ç¡®ç‡å’Œ0.8247çš„ROC AUCã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå­£èŠ‚æ€§é¢„æµ‹ä¸å¯è§£é‡Šçš„é›†æˆå­¦ä¹ ï¼Œå®ç°äº†å¯¹é»‘è‰²æ˜ŸæœŸäº”ç­‰é«˜é£é™©æ—¶æœŸçš„é£é™©é¢„åˆ¤ï¼Œä¸ºä¸»åŠ¨åˆ†é…ç½‘ç»œå®‰å…¨èµ„æºæä¾›äº†ç§‘å­¦ä¾æ®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "32 pages, 9 figures, 6 tables; MSc Research Dissertation, York St John University, London Campus",
      "pdf_url": "https://arxiv.org/pdf/2511.03020v1",
      "published_date": "2025-11-04 21:38:59 UTC",
      "updated_date": "2025-11-04 21:38:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:48:26.604521+00:00"
    },
    {
      "arxiv_id": "2511.03019v1",
      "title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment",
      "title_zh": "SLIPï¼šé¢å‘è§†è§‰-è¯­è¨€å¯¹é½çš„ç»“æ„æ„ŸçŸ¥è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ",
      "authors": [
        "Wenbo Lu"
      ],
      "abstract": "Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€é¢„è®­ç»ƒ(VLP)æ–¹æ³•é€šå¸¸å°†å›¾åƒ-æ–‡æœ¬å¯¹è§†ä¸ºå­¤ç«‹æ ·æœ¬è€Œå¿½ç•¥é¢†åŸŸå†…ä¸°å¯Œå…³ç³»ç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº†SLIP (Structure-aware Language-Image Pretraining)ã€‚å—ç¥ç»ç§‘å­¦ä¸­äººç±»é€šè¿‡å…³ç³»è®¤çŸ¥å›¾è°±ç¼–ç çŸ¥è¯†çš„å¯å‘ï¼ŒSLIPå¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–å¯¹æ¯”æŸå¤±(structural contrastive loss)ï¼Œåœ¨å¯¹é½æ¨¡æ€çš„åŒæ—¶å¯¹ç»“æ„åŒ–å›¾è°±ä¸­ç›¸é‚»å®ä½“é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€èŒƒå¼ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„Amazon Product Co-purchase Multimodal Graph Datasetï¼Œå®ç°äº†å¤§è§„æ¨¡çš„ç»“æ„åŒ–è·¨æ¨¡æ€ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLIPåœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)è®¾ç½®ä¸‹çš„è·¨æ¨¡æ€æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­å‡æŒç»­ä¼˜äºCLIPï¼Œè¯æ˜äº†å…³ç³»ç›‘ç£å¯¹äºè·¨æ¨¡æ€å¯¹é½çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Capstone Paper",
      "pdf_url": "https://arxiv.org/pdf/2511.03019v1",
      "published_date": "2025-11-04 21:33:57 UTC",
      "updated_date": "2025-11-04 21:33:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:48:47.413451+00:00"
    },
    {
      "arxiv_id": "2511.04706v1",
      "title": "Prioritize Economy or Climate Action? Investigating ChatGPT Response Differences Based on Inferred Political Orientation",
      "title_zh": "ä¼˜å…ˆç»æµè¿˜æ˜¯æ°”å€™è¡ŒåŠ¨ï¼Ÿæ¢ç©¶ ChatGPT åŸºäºæ¨æ–­æ”¿æ²»å€¾å‘çš„å›ç­”å·®å¼‚",
      "authors": [
        "Pelin Karadal",
        "Dilara Kekulluoglu"
      ],
      "abstract": "Large Language Models (LLMs) distinguish themselves by quickly delivering information and providing personalized responses through natural language prompts. However, they also infer user demographics, which can raise ethical concerns about bias and implicit personalization and create an echo chamber effect. This study aims to explore how inferred political views impact the responses of ChatGPT globally, regardless of the chat session. We also investigate how custom instruction and memory features alter responses in ChatGPT, considering the influence of political orientation. We developed three personas (two politically oriented and one neutral), each with four statements reflecting their viewpoints on DEI programs, abortion, gun rights, and vaccination. We convey the personas' remarks to ChatGPT using memory and custom instructions, allowing it to infer their political perspectives without directly stating them. We then ask eight questions to reveal differences in worldview among the personas and conduct a qualitative analysis of the responses. Our findings indicate that responses are aligned with the inferred political views of the personas, showing varied reasoning and vocabulary, even when discussing similar topics. We also find the inference happening with explicit custom instructions and the implicit memory feature in similar ways. Analyzing response similarities reveals that the closest matches occur between the democratic persona with custom instruction and the neutral persona, supporting the observation that ChatGPT's outputs lean left.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ChatGPTå¦‚ä½•æ ¹æ®æ¨æ–­å‡ºçš„ç”¨æˆ·æ”¿æ²»å€¾å‘è°ƒæ•´å…¶å›ç­”ï¼Œä»è€Œå¼•å‘å…³äºåè§å’Œå›å£°å®¤æ•ˆåº”(echo chamber effect)çš„ä¼¦ç†æ‹…å¿§ã€‚ä½œè€…æ„å»ºäº†ä¸‰ç§ä¸åŒæ”¿æ²»ç«‹åœºï¼ˆæ°‘ä¸»å…šã€å…±å’Œå…šå’Œä¸­ç«‹ï¼‰çš„è§’è‰²ï¼Œåˆ©ç”¨å…³äºDEIé¡¹ç›®ã€å •èƒã€æªæ”¯æƒåˆ©å’Œç–«è‹—æ¥ç§çš„é™ˆè¿°ï¼Œé€šè¿‡ChatGPTçš„Memoryå’ŒCustom InstructionsåŠŸèƒ½è®©æ¨¡å‹éšå¼æ¨æ–­ç”¨æˆ·çš„æ”¿æ²»è§‚ç‚¹ã€‚é€šè¿‡å¯¹å…«ä¸ªä¸–ç•Œè§‚ç›¸å…³é—®é¢˜çš„å›ç­”è¿›è¡Œå®šæ€§åˆ†æï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„å›å¤åœ¨æ¨ç†é€»è¾‘å’Œè¯æ±‡ä½¿ç”¨ä¸Šå‡ä¸æ¨æ–­å‡ºçš„æ”¿æ²»ç«‹åœºä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯æ˜¾å¼çš„Custom Instructionsè¿˜æ˜¯éšå¼çš„MemoryåŠŸèƒ½ï¼Œéƒ½ä¼šå¯¼è‡´ç±»ä¼¼çš„æ¨æ–­æ•ˆæœã€‚æ­¤å¤–ï¼Œå“åº”ç›¸ä¼¼åº¦åˆ†ææ˜¾ç¤ºï¼Œä½¿ç”¨Custom Instructionsçš„æ°‘ä¸»å…šè§’è‰²ä¸ä¸­ç«‹è§’è‰²çš„å›ç­”æœ€ä¸ºæ¥è¿‘ï¼Œè¿™æ”¯æŒäº†ChatGPTè¾“å‡ºå†…å®¹å€¾å‘äºå·¦å€¾(lean left)çš„è§‚ç‚¹ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.04706v1",
      "published_date": "2025-11-04 21:07:01 UTC",
      "updated_date": "2025-11-04 21:07:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:49:12.542226+00:00"
    },
    {
      "arxiv_id": "2511.02997v1",
      "title": "Evaluating Control Protocols for Untrusted AI Agents",
      "title_zh": "è¯„ä¼°ä¸å¯ä¿¡ AI æ™ºèƒ½ä½“çš„æ§åˆ¶åè®®",
      "authors": [
        "Jon Kutasov",
        "Chloe Loughridge",
        "Yuqi Sun",
        "Henry Sleight",
        "Buck Shlegeris",
        "Tyler Tracy",
        "Joe Benton"
      ],
      "abstract": "As AI systems become more capable and widely deployed as agents, ensuring their safe operation becomes critical. AI control offers one approach to mitigating the risk from untrusted AI agents by monitoring their actions and intervening or auditing when necessary. Evaluating the safety of these protocols requires understanding both their effectiveness against current attacks and their robustness to adaptive adversaries. In this work, we systematically evaluate a range of control protocols in SHADE-Arena, a dataset of diverse agentic environments. First, we evaluate blue team protocols, including deferral to trusted models, resampling, and deferring on critical actions, against a default attack policy. We find that resampling for incrimination and deferring on critical actions perform best, increasing safety from 50% to 96%. We then iterate on red team strategies against these protocols and find that attack policies with additional affordances, such as knowledge of when resampling occurs or the ability to simulate monitors, can substantially improve attack success rates against our resampling strategy, decreasing safety to 17%. However, deferring on critical actions is highly robust to even our strongest red team strategies, demonstrating the importance of denying attack policies access to protocol internals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸å¯ä¿¡AIæ™ºèƒ½ä½“ï¼ˆUntrusted AI Agentsï¼‰çš„å®‰å…¨æ“ä½œé—®é¢˜ï¼Œåœ¨åŒ…å«å¤šæ ·åŒ–æ™ºèƒ½ä½“ç¯å¢ƒçš„SHADE-Arenaæ•°æ®é›†ä¸Šç³»ç»Ÿè¯„ä¼°äº†å¤šç§AIæ§åˆ¶åè®®ï¼ˆControl Protocolsï¼‰ã€‚ä½œè€…é¦–å…ˆè¯„ä¼°äº†åŒ…æ‹¬äº¤ç»™å¯ä¿¡æ¨¡å‹ï¼ˆdeferral to trusted modelsï¼‰ã€é‡é‡‡æ ·ï¼ˆresamplingï¼‰å’Œå…³é”®è¡ŒåŠ¨æ¨è¿Ÿï¼ˆdeferring on critical actionsï¼‰åœ¨å†…çš„è“é˜Ÿåè®®ï¼Œå‘ç°\"resampling for incrimination\"å’Œ\"deferring on critical actions\"è¡¨ç°æœ€ä½³ï¼Œèƒ½å°†å®‰å…¨æ€§ä»50%æå‡è‡³96%ã€‚éšåï¼Œç ”ç©¶é€šè¿‡è¿­ä»£çº¢é˜Ÿç­–ç•¥ï¼ˆRed Team strategiesï¼‰æµ‹è¯•åè®®å¯¹é€‚åº”æ€§å¯¹æ‰‹çš„é²æ£’æ€§ï¼Œå‘ç°è‹¥æ”»å‡»ç­–ç•¥çŸ¥æ™“é‡é‡‡æ ·æœºåˆ¶æˆ–èƒ½æ¨¡æ‹Ÿç›‘æ§å™¨ï¼Œé‡é‡‡æ ·ç­–ç•¥çš„å®‰å…¨æ€§ä¼šå¤§å¹…ä¸‹é™è‡³17%ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜\"deferring on critical actions\"ç­–ç•¥å³ä½¿é¢å¯¹æœ€å¼ºçš„çº¢é˜Ÿç­–ç•¥ä»ä¿æŒé«˜åº¦é²æ£’æ€§ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†é˜»æ­¢æ”»å‡»ç­–ç•¥è®¿é—®åè®®å†…éƒ¨ä¿¡æ¯å¯¹äºä¿éšœAIæ§åˆ¶ç³»ç»Ÿæœ‰æ•ˆæ€§çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02997v1",
      "published_date": "2025-11-04 21:04:49 UTC",
      "updated_date": "2025-11-04 21:04:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:52:21.536077+00:00"
    },
    {
      "arxiv_id": "2511.02979v1",
      "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications",
      "title_zh": "ç³»ç»ŸåŒ–LLMäººè®¾è®¾è®¡ï¼šé¢å‘AIä¼´ä¾£åº”ç”¨çš„å››è±¡é™æŠ€æœ¯åˆ†ç±»ä½“ç³»",
      "authors": [
        "Esther Sun",
        "Zichu Wu"
      ],
      "abstract": "The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional companions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applications in work, gaming, and mental health, highlighting the shift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹LLMè§’è‰²è®¾è®¡åœ¨AIé™ªä¼´åº”ç”¨ä¸­æ—¥ç›Šæ‰©å¼ ä½†ç¢ç‰‡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹AIä¼´ä¾£åº”ç”¨çš„å››è±¡é™æŠ€æœ¯åˆ†ç±»æ³•ï¼ˆFour-Quadrant Technical Taxonomyï¼‰ã€‚è¯¥æ¡†æ¶åŸºäºâ€œè™šæ‹Ÿä¸å…·èº«â€ï¼ˆVirtual vs. Embodiedï¼‰ä»¥åŠâ€œæƒ…æ„Ÿé™ªä¼´ä¸åŠŸèƒ½å¢å¼ºâ€ï¼ˆEmotional Companionship vs. Functional Augmentationï¼‰ä¸¤ä¸ªå…³é”®è½´çº¿æ„å»ºï¼Œæ—¨åœ¨ç³»ç»ŸåŒ–è¿™ä¸€é¢†åŸŸã€‚ç¬¬ä¸€è±¡é™èšç„¦è™šæ‹Ÿé™ªä¼´ï¼Œå¼•å…¥å››å±‚æŠ€æœ¯æ¡†æ¶ä»¥è§£å†³è™šæ‹Ÿå¶åƒå’Œè§’è‰²åœ¨ç»´æŒé•¿æœŸæƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼›ç¬¬äºŒè±¡é™åˆ†æåŠŸèƒ½æ€§è™šæ‹ŸåŠ©æ‰‹ï¼Œå¼ºè°ƒä»â€œæ„Ÿå—â€å‘â€œæ€è€ƒä¸è¡ŒåŠ¨â€çš„è½¬å˜ï¼Œå¹¶æŒ‡å‡ºäº†ä¼ä¸šçº§RAGå’Œç«¯ä¾§æ¨ç†ç­‰å…³é”®æŠ€æœ¯ã€‚ç¬¬ä¸‰å’Œç¬¬å››è±¡é™åˆ™è½¬å‘å…·èº«æ™ºèƒ½ï¼ˆEmbodied Intelligenceï¼‰ï¼Œæ¢è®¨ç‰©ç†ä¸–ç•Œä¸­çš„æœºå™¨äººåº”ç”¨ï¼Œæ­ç¤ºäº†ç¬¦å·è½åœ°ï¼ˆsymbol groundingï¼‰ã€æ•°æ®éšç§å’Œä¼¦ç†è´£ä»»ç­‰æ ¸å¿ƒéš¾é¢˜ã€‚è¿™ä¸€åˆ†ç±»æ³•ä¸ä»…ä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†å¯¼èˆªå¤æ‚è§’è‰²è®¾è®¡ç©ºé—´çš„ç³»ç»Ÿåœ°å›¾ï¼Œä¹Ÿä¸ºæ”¿ç­–åˆ¶å®šè€…è¯†åˆ«å’Œåº”å¯¹ä¸åŒåº”ç”¨åœºæ™¯ä¸­çš„ç‹¬ç‰¹é£é™©å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to Neurips 2025 workshop: LLM Persona Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.02979v1",
      "published_date": "2025-11-04 20:37:13 UTC",
      "updated_date": "2025-11-04 20:37:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:52:45.156038+00:00"
    },
    {
      "arxiv_id": "2511.02969v2",
      "title": "Value of Information-Enhanced Exploration in Bootstrapped DQN",
      "title_zh": "è‡ªä¸¾ DQN ä¸­çš„ä¿¡æ¯ä»·å€¼å¢å¼ºæ¢ç´¢",
      "authors": [
        "Stergios Plataniotis",
        "Charilaos Akasiadis",
        "Georgios Chalkiadakis"
      ],
      "abstract": "Efficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $Îµ$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨é«˜ç»´çŠ¶æ€å’Œç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­é¢ä¸´çš„æ¢ç´¢éš¾é¢˜ï¼Œæå‡ºå°†ï¼ˆæœŸæœ›ï¼‰ä¿¡æ¯ä»·å€¼(Expected Value of Information, EVOI)æ•´åˆåˆ°Bootstrapped DQNæ¡†æ¶ä¸­ã€‚é€šè¿‡å¼€å‘ä¸¤ç§æ–°ç®—æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¿¡æ¯ä»·å€¼ä¼°è®¡æ¥è¡¡é‡ä¸åŒç½‘ç»œå¤´(network heads)ä¹‹é—´çš„æ„è§å·®å¼‚ï¼Œä»è€Œå°†æ¢ç´¢å¼•å¯¼è‡³æœ€å…·æ½œåŠ›çš„åŒºåŸŸï¼Œå…‹æœäº†ä¼ ç»Ÿ$\\epsilon$-greedyç­‰ç­–ç•¥åœ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ–¹é¢çš„å±€é™ã€‚åœ¨å¤æ‚çš„Atariæ¸¸æˆå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†æ›´å¼ºçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨éšæœºç½‘ç»œåˆå§‹åŒ–äº§ç”Ÿçš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œä¸”æœªå¼•å…¥é¢å¤–çš„è¶…å‚æ•°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02969v2",
      "published_date": "2025-11-04 20:22:58 UTC",
      "updated_date": "2025-11-21 16:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:53:09.586901+00:00"
    },
    {
      "arxiv_id": "2511.02953v1",
      "title": "EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation",
      "title_zh": "EvtSlowTVâ€”â€”ç”¨äºåŸºäºäº‹ä»¶çš„æ·±åº¦ä¼°è®¡çš„å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†",
      "authors": [
        "Sadiq Layi Macaulay",
        "Nimet Kaygusuz",
        "Simon Hadfield"
      ],
      "abstract": "Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºäº‹ä»¶ç›¸æœºçš„æ·±åº¦ä¼°è®¡æ–¹æ³•å—é™äºå°è§„æ¨¡æ•°æ®é›†ä¸”æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†EvtSlowTVï¼Œè¿™æ˜¯ä¸€ä¸ªä»å…¬å¼€YouTubeè§†é¢‘ä¸­æ•´ç†çš„å¤§è§„æ¨¡äº‹ä»¶ç›¸æœºæ•°æ®é›†ã€‚EvtSlowTVåŒ…å«è¶…è¿‡130äº¿ä¸ªeventsï¼Œæ¶µç›–å¾’æ­¥ã€é£è¡Œã€é©¾é©¶åŠæ°´ä¸‹æ¢ç´¢ç­‰å¤šç§ç¯å¢ƒå’Œè¿åŠ¨æ¨¡å¼ï¼Œè§„æ¨¡æ¯”ç°æœ‰æ•°æ®é›†å¤§ä¸€ä¸ªæ•°é‡çº§ï¼Œæä¾›äº†éå—é™çš„è‡ªç„¶åœºæ™¯ã€‚ä½œè€…å±•ç¤ºäº†è¯¥æ•°æ®é›†é€‚ç”¨äºself-supervised learningæ¡†æ¶ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨åŸå§‹äº‹ä»¶æµçš„é«˜åŠ¨æ€èŒƒå›´(HDR)æ½œåŠ›ï¼Œä¸”æ— éœ€åŸºäºå¸§çš„æ ‡æ³¨ï¼Œä¿ç•™äº†æ•°æ®çš„å¼‚æ­¥ç‰¹æ€§ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨EvtSlowTVè®­ç»ƒèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯å’Œè¿åŠ¨ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºåŸºäºäº‹ä»¶çš„æ·±åº¦å­¦ä¹ æä¾›äº†å¼ºæœ‰åŠ›çš„æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02953v1",
      "published_date": "2025-11-04 19:56:26 UTC",
      "updated_date": "2025-11-04 19:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:53:30.799670+00:00"
    },
    {
      "arxiv_id": "2511.02944v1",
      "title": "Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics",
      "title_zh": "å…·æœ‰ä¹ æƒ¯åŒ–ä¸æ¢å¤åŠ¨æ€çš„åŠŸæ•ˆçº¦æŸéå¹³ç¨³å¤šè‡‚è€è™æœº",
      "authors": [
        "Fengxu Li",
        "Stephanie M. Carpenter",
        "Matthew P. Buman",
        "Yonatan Mintz"
      ],
      "abstract": "A common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†³ç­–å¥–åŠ±éšæ—¶é—´æ¼”å˜ï¼ˆå¦‚ä¹ æƒ¯åŒ–å’Œæ¢å¤åŠ¨åŠ›å­¦ï¼‰çš„æŒ‘æˆ˜ï¼Œæ·±å…¥æ¢è®¨äº†Reducing or Gaining Unknown Efficacy (ROGUE) banditæ¡†æ¶ã€‚åœ¨å¾®éšæœºè¯•éªŒ(MRTs)ä¸­ï¼Œç°æœ‰ç®—æ³•å¾€å¾€å› è¿‡åº¦å¼ºè°ƒåˆ©ç”¨è€Œå¯¼è‡´æ¢ç´¢ä¸è¶³ï¼Œé™åˆ¶äº†å¯¹ç¾¤ä½“å±‚é¢æ•ˆåº”çš„ä¼°è®¡èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡é¦–å…ˆå¼€å‘äº†ROGUE-TSï¼Œä¸€ç§ä¸“ä¸ºROGUEæ¡†æ¶å®šåˆ¶çš„Thompson Samplingç®—æ³•ï¼Œå¹¶æä¾›äº†äºšçº¿æ€§é—æ†¾çš„ç†è®ºä¿è¯ã€‚æ¥ç€ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç§æ¦‚ç‡æˆªæ–­(probability clipping)ç¨‹åºæ¥å¹³è¡¡ä¸ªæ€§åŒ–ä¸ç¾¤ä½“å±‚é¢å­¦ä¹ ï¼Œé‡åŒ–äº†é—æ†¾å€¼ä¸æœ€å°æ¢ç´¢æ¦‚ç‡ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨æ¶‰åŠèº«ä½“æ´»åŠ¨ä¿ƒè¿›å’ŒåŒç›¸æƒ…æ„Ÿéšœç¢æ²»ç–—çš„ä¸¤ä¸ªMRTæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¸ä»…æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†æ›´ä½çš„é—æ†¾ï¼Œè¿˜èƒ½é€šè¿‡æˆªæ–­ç¨‹åºåœ¨ä¸æ˜¾è‘—å¢åŠ é—æ†¾çš„æƒ…å†µä¸‹ä¿æŒé«˜ç»Ÿè®¡åŠŸæ•ˆ(statistical power)ã€‚è¿™ä¸€æˆæœä½¿å¾—åœ¨è€ƒè™‘ä¸ªä½“è¡Œä¸ºåŠ¨åŠ›å­¦çš„åŒæ—¶èƒ½å¤Ÿå¯é åœ°æ£€æµ‹æ²»ç–—æ•ˆæœï¼Œä¸ºç ”ç©¶äººå‘˜è®¾è®¡å¹³è¡¡ä¸ªæ€§åŒ–ä¸ç»Ÿè®¡æœ‰æ•ˆæ€§çš„MRTsæä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02944v1",
      "published_date": "2025-11-04 19:46:42 UTC",
      "updated_date": "2025-11-04 19:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:53:55.987635+00:00"
    },
    {
      "arxiv_id": "2511.02938v1",
      "title": "From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery",
      "title_zh": "ä»çª„å¸¦åˆ°å®½å¸¦ï¼šç”¨äºè¶…å£°å¸¦å®½æ¢å¤çš„è‡ªç¼–ç  Transformer",
      "authors": [
        "Sepideh KhakzadGharamaleki",
        "Hassan Rivaz",
        "Brandon Helfield"
      ],
      "abstract": "Conventional pulse-echo ultrasound suffers when low-cost probes deliver only narrow fractional bandwidths, elongating pulses and erasing high-frequency detail. We address this limitation by learning a data-driven mapping from band-limited to broadband spectrogram of radio-frequency (RF) lines. To this end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by 6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also sharpens point-target rows in a completely unseen resolution phantom, demonstrating strong out-of-distribution generalisation without sacrificing frame rate or phase information. These results indicate that a purely software upgrade can endow installed narrow-band probes with broadband-like performance, potentially widening access to high-resolution ultrasound in resource-constrained settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿä½æˆæœ¬è¶…å£°æ¢å¤´å› çª„å¸¦å®½å¯¼è‡´è„‰å†²å»¶é•¿å’Œé«˜é¢‘ç»†èŠ‚ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»çª„å¸¦åˆ°å®½å¸¦å°„é¢‘(RF)çº¿è°±å›¾çš„æ•°æ®é©±åŠ¨æ˜ å°„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨è¯¾ç¨‹åŠ æƒæŸå¤±(curriculum-weighted loss)åœ¨ä»¿çœŸæ•°æ®ä¸Šè®­ç»ƒäº†ä¸€ç§å˜ä½“çš„Tiny Vision Transform (ViT)è‡ªç¼–ç å™¨ã€‚åœ¨å¼‚è´¨æ–‘ç‚¹å›Šè‚¿ä»¿ä½“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸çª„å¸¦è¾“å…¥ç›¸æ¯”ï¼Œè¯¥ç½‘ç»œå°†å›¾åƒåŸŸMSEé™ä½äº†90%ï¼ŒPSNRæé«˜äº†6.7 dBï¼ŒSSIMæå‡è‡³0.965ã€‚è¯¥æ¨¡å‹è¿˜åœ¨æœªè§è¿‡çš„åˆ†è¾¨ç‡ä»¿ä½“ä¸­æˆåŠŸé”åŒ–äº†ç‚¹ç›®æ ‡ï¼Œè¯æ˜äº†åœ¨ä¸ç‰ºç‰²å¸§ç‡æˆ–ç›¸ä½ä¿¡æ¯çš„æƒ…å†µä¸‹çš„å¼ºå¤§åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›(out-of-distribution generalization)ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œçº¯è½¯ä»¶å‡çº§å³å¯èµ‹äºˆå®‰è£…çš„çª„å¸¦æ¢å¤´ä»¥å®½å¸¦èˆ¬çš„æ€§èƒ½ï¼Œæœ‰æœ›åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ‰©å¤§é«˜åˆ†è¾¨ç‡è¶…å£°çš„åº”ç”¨èŒƒå›´ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02938v1",
      "published_date": "2025-11-04 19:34:18 UTC",
      "updated_date": "2025-11-04 19:34:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:54:20.586225+00:00"
    },
    {
      "arxiv_id": "2511.02936v1",
      "title": "Zero-shot data citation function classification using transformer-based large language models (LLMs)",
      "title_zh": "åŸºäº Transformer å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„é›¶æ ·æœ¬æ•°æ®å¼•ç”¨åŠŸèƒ½åˆ†ç±»",
      "authors": [
        "Neil Byers",
        "Ali Zaidi",
        "Valerie Skye",
        "Chris Beecroft",
        "Kjiersten Fagnan"
      ],
      "abstract": "Efforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œé›¶æ ·æœ¬(Zero-shot)æ•°æ®å¼•ç”¨åŠŸèƒ½åˆ†ç±»ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨å’Œè®­ç»ƒæ•°æ®é›†çš„é—®é¢˜ã€‚ä½œè€…åº”ç”¨å¼€æºæ¨¡å‹Llama 3.1-405Bï¼Œé’ˆå¯¹åŒ…å«ç‰¹å®šåŸºå› ç»„æ•°æ®é›†çš„å‡ºç‰ˆç‰©ç”Ÿæˆç»“æ„åŒ–çš„æ•°æ®ä½¿ç”¨æ¡ˆä¾‹æ ‡ç­¾ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è¯„ä¼°æ¡†æ¶æ¥æµ‹å®šæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åŸºç¡€æ¨¡å‹åœ¨æ²¡æœ‰é¢„å®šä¹‰ç±»åˆ«çš„é›¶æ ·æœ¬æ•°æ®å¼•ç”¨åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†0.674çš„F1åˆ†æ•°ã€‚å°½ç®¡ç»“æœå…·æœ‰å‰æ™¯ï¼Œä½†ç ”ç©¶åŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨æ•°æ®å¯ç”¨æ€§ã€æç¤ºè¯è¿‡æ‹Ÿåˆ(prompt overfitting)ã€è®¡ç®—åŸºç¡€è®¾æ–½ä»¥åŠè¯„ä¼°æˆæœ¬æ–¹é¢å­˜åœ¨çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02936v1",
      "published_date": "2025-11-04 19:33:30 UTC",
      "updated_date": "2025-11-04 19:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:54:47.074787+00:00"
    },
    {
      "arxiv_id": "2511.13729v1",
      "title": "DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off",
      "title_zh": "DualLaguerreNetï¼šè§£è€¦è°±æ»¤æ³¢å™¨å›¾ç¥ç»ç½‘ç»œä¸çµæ´»æ€§-ç¨³å®šæ€§æƒè¡¡çš„æ­ç¤º",
      "authors": [
        "Huseyin Goksu"
      ],
      "abstract": "Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a \"compromise\" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing \"Decoupled Spectral Flexibility.\" DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental \"Flexibility-Stability Trade-off\". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the \"compromise\" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè°±æ»¤æ³¢å™¨çš„å›¾ç¥ç»ç½‘ç»œï¼ˆå¦‚LaguerreNetï¼‰ä¸­å› å•ä¸€å‚æ•°å¯¼è‡´çš„â€œå¦¥åâ€é—®é¢˜ï¼Œæå‡ºäº†DualLaguerreNetæ¶æ„ã€‚è¯¥æ–¹æ³•å¼•å…¥â€œè§£è€¦è°±çµæ´»æ€§â€ï¼ˆDecoupled Spectral Flexibilityï¼‰ï¼Œå°†å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­åˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘éƒ¨åˆ†ï¼Œå¹¶åˆ†åˆ«å­¦ä¹ ä¸¤ä¸ªç‹¬ç«‹çš„è‡ªé€‚åº”Laguerreå¤šé¡¹å¼æ»¤æ³¢å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDualLaguerreNetåœ¨å¤æ‚çš„å¼‚é…ï¼ˆheterophilicï¼‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†LaguerreNetï¼Œå–å¾—äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹åœ¨ç®€å•çš„åŒé…ï¼ˆhomophilicï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä»è€Œæ­ç¤ºäº†åŸºç¡€æ€§çš„â€œçµæ´»æ€§-ç¨³å®šæ€§æƒè¡¡â€ï¼ˆFlexibility-Stability Trade-offï¼‰ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¢åŠ çš„å‚æ•°åŒ–å¯¼è‡´åœ¨ç®€å•ä»»åŠ¡ä¸Šè¿‡æ‹Ÿåˆï¼Œè¯æ˜äº†ç®€å•æ¨¡å‹çš„å±€é™æ€§åè€Œèµ·åˆ°äº†å…³é”®çš„æ­£åˆ™åŒ–ä½œç”¨ã€‚è¿™ç¯‡è®ºæ–‡ä¸ä»…ä¸ºå¤„ç†å¼‚é…å›¾æä¾›äº†æ–°çš„SOTAæ¶æ„ï¼Œè¿˜å¯¹è‡ªé€‚åº”GNNæ»¤æ³¢å™¨è®¾è®¡ä¸­çš„åå·®-æ–¹å·®æƒè¡¡è¿›è¡Œäº†æ‰¹åˆ¤æ€§åˆ†æã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13729v1",
      "published_date": "2025-11-04 19:33:29 UTC",
      "updated_date": "2025-11-04 19:33:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:55:29.148034+00:00"
    },
    {
      "arxiv_id": "2511.02933v1",
      "title": "Generative Hints",
      "title_zh": "ç”Ÿæˆå¼æç¤º",
      "authors": [
        "Andy Dimnaku",
        "Abdullah Yusuf KavranoÄŸlu",
        "Yaser Abu-Mostafa"
      ],
      "abstract": "Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Generative Hintsï¼Œä¸€ç§æ—¨åœ¨å…‹æœä¼ ç»Ÿæ•°æ®å¢å¼º(Data Augmentation)ä»…èƒ½åœ¨è®­ç»ƒæ•°æ®å˜æ¢ä¸Šå­¦ä¹ ä¸å˜æ€§è¿™ä¸€å±€é™æ€§çš„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹æ¥è¿‘ä¼¼è¾“å…¥åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆè¢«ç§°ä¸º\"virtual examples\"çš„æ— æ ‡ç­¾å›¾åƒï¼Œä»è€Œåœ¨æ•´ä¸ªè¾“å…¥ç©ºé—´ç›´æ¥å¼ºåˆ¶æ‰§è¡Œå·²çŸ¥çš„ä¸å˜æ€§å±æ€§ï¼ˆå³hintsï¼‰ã€‚åœ¨Generative Hintsä¸­ï¼Œæ¨¡å‹é‡‡ç”¨åŠç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†åˆ†ç±»ç›®æ ‡å’ŒåŸºäºè™šæ‹Ÿæ ·æœ¬çš„hintç›®æ ‡ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ æœŸæœ›çš„å±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒçš„æ•°æ®é›†ã€æ¶æ„å’ŒæŸå¤±å‡½æ•°ä¸‹ï¼ŒGenerative Hintsåœ¨å­¦ä¹ ç›¸åŒå±æ€§æ—¶å§‹ç»ˆä¼˜äºæ ‡å‡†æ•°æ®å¢å¼ºã€‚åœ¨æµè¡Œçš„ç»†ç²’åº¦è§†è§‰åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ¯”ç»è¿‡å¾®è°ƒçš„æ•°æ®å¢å¼ºæ¨¡å‹å®ç°äº†é«˜è¾¾1.78%çš„Top-1å‡†ç¡®ç‡æå‡ï¼ˆå¹³å‡0.63%ï¼‰ï¼Œå¹¶åœ¨CheXpert Xå°„çº¿æ•°æ®é›†ä¸Šå®ç°äº†1.286%çš„å¹³å‡æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02933v1",
      "published_date": "2025-11-04 19:31:36 UTC",
      "updated_date": "2025-11-04 19:31:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:55:51.863349+00:00"
    },
    {
      "arxiv_id": "2511.02834v2",
      "title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything",
      "title_zh": "Agent-Omniï¼šåŸºäºæ¨¡å‹åä½œå®ç°ä¸‡ç‰©ç†è§£çš„æµ‹è¯•æ—¶å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Huawei Lin",
        "Yunzhi Shi",
        "Tong Geng",
        "Weijie Zhao",
        "Wei Wang",
        "Ravender Pal Singh"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Agent-Omniæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å—é™äºå›ºå®šæ¨¡æ€å¯¹ä¸”éœ€è¦æ˜‚è´µå¾®è°ƒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸»æ™ºèƒ½ä½“ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹(foundation models)ï¼Œå®ç°äº†æ— éœ€é‡è®­ç»ƒçš„æµ‹è¯•æ—¶(Test-Time)çµæ´»å¤šæ¨¡æ€æ¨ç†ã€‚ä¸»æ™ºèƒ½ä½“è´Ÿè´£è§£è¯»ç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„æ™ºèƒ½ä½“ï¼Œå¹¶å°†è¾“å‡ºæ•´åˆæˆè¿è´¯çš„å“åº”ã€‚åœ¨æ¶µç›–æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘åŠå…¨èƒ½åŸºå‡†çš„å¹¿æ³›å®éªŒä¸­ï¼ŒAgent-Omniå±•ç°äº†è¾¾åˆ°æœ€å…ˆè¿›(SOTA)æ°´å¹³çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚å…¶åŸºäºæ™ºèƒ½ä½“çš„è®¾è®¡ä¸ä»…å®ç°äº†ä¸“ç”¨åŸºç¡€æ¨¡å‹çš„æ— ç¼é›†æˆï¼Œç¡®ä¿äº†å¯¹å¤šæ ·åŒ–è¾“å…¥çš„é€‚åº”æ€§ï¼Œè¿˜ä¿æŒäº†ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–å’Œæ˜“æ‰©å±•çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿéšç€æœªæ¥æ›´å¼ºæ¨¡å‹çš„å‡ºç°è€Œä¸æ–­æ”¹è¿›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 7 figures, 14 tables. Under Review",
      "pdf_url": "https://arxiv.org/pdf/2511.02834v2",
      "published_date": "2025-11-04 18:59:09 UTC",
      "updated_date": "2025-11-05 05:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:56:16.277249+00:00"
    },
    {
      "arxiv_id": "2511.02825v1",
      "title": "Neurosymbolic Deep Learning Semantics",
      "title_zh": "ç¥ç»ç¬¦å·æ·±åº¦å­¦ä¹ è¯­ä¹‰",
      "authors": [
        "Artur d'Avila Garcez",
        "Simon Odense"
      ],
      "abstract": "Artificial Intelligence (AI) is a powerful new language of science as evidenced by recent Nobel Prizes in chemistry and physics that recognized contributions to AI applied to those areas. Yet, this new language lacks semantics, which makes AI's scientific discoveries unsatisfactory at best. With the purpose of uncovering new facts but also improving our understanding of the world, AI-based science requires formalization through a framework capable of translating insight into comprehensible scientific knowledge. In this paper, we argue that logic offers an adequate framework. In particular, we use logic in a neurosymbolic framework to offer a much needed semantics for deep learning, the neural network-based technology of current AI. Deep learning and neurosymbolic AI lack a general set of conditions to ensure that desirable properties are satisfied. Instead, there is a plethora of encoding and knowledge extraction approaches designed for particular cases. To rectify this, we introduced a framework for semantic encoding, making explicit the mapping between neural networks and logic, and characterizing the common ingredients of the various existing approaches. In this paper, we describe succinctly and exemplify how logical semantics and neural networks are linked through this framework, we review some of the most prominent approaches and techniques developed for neural encoding and knowledge extraction, provide a formal definition of our framework, and discuss some of the difficulties of identifying a semantic encoding in practice in light of analogous problems in the philosophy of mind.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ï¼ˆç‰¹åˆ«æ˜¯Deep Learningï¼‰åœ¨ç§‘å­¦å‘ç°ä¸­è™½æˆæœæ˜¾è‘—ä½†ç¼ºä¹è¯­ä¹‰è§£é‡Šçš„é—®é¢˜ï¼Œä¸»å¼ åˆ©ç”¨Logicä½œä¸ºå½¢å¼åŒ–æ¡†æ¶å°†æ´å¯Ÿè½¬åŒ–ä¸ºå¯ç†è§£çš„ç§‘å­¦çŸ¥è¯†ã€‚ä½œè€…åœ¨Neurosymbolicæ¡†æ¶ä¸‹å¼•å…¥äº†â€œsemantic encodingâ€æœºåˆ¶ï¼Œæ—¨åœ¨æ˜ç¡®Neural Networksä¸Logicä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œä»è€Œè§£å†³å½“å‰é¢†åŸŸç¼ºä¹é€šç”¨æ¡ä»¶æ¥ç¡®ä¿ç†æƒ³å±æ€§çš„é—®é¢˜ã€‚è¯¥è®ºæ–‡è¯¦ç»†æè¿°äº†é€»è¾‘è¯­ä¹‰ä¸ç¥ç»ç½‘ç»œçš„è¿æ¥æ–¹å¼ï¼Œå¹¶å›é¡¾äº†neural encodingå’Œknowledge extractionçš„ä¸»è¦æ–¹æ³•ä¸æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶æä¾›äº†è¯¥æ¡†æ¶çš„å½¢å¼åŒ–å®šä¹‰ï¼Œå¹¶ç»“åˆå¿ƒæ™ºå“²å­¦ï¼ˆphilosophy of mindï¼‰ä¸­çš„ç±»æ¯”é—®é¢˜ï¼Œæ¢è®¨äº†åœ¨å®è·µä¸­è¯†åˆ«è¯­ä¹‰ç¼–ç çš„éš¾ç‚¹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02825v1",
      "published_date": "2025-11-04 18:51:04 UTC",
      "updated_date": "2025-11-04 18:51:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:59:27.226708+00:00"
    },
    {
      "arxiv_id": "2511.02824v2",
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "title_zh": "Kosmosï¼šé¢å‘è‡ªä¸»å‘ç°çš„äººå·¥æ™ºèƒ½ç§‘å­¦å®¶",
      "authors": [
        "Ludovico Mitchener",
        "Angela Yiu",
        "Benjamin Chang",
        "Mathieu Bourdenx",
        "Tyler Nadolski",
        "Arvis Sulovari",
        "Eric C. Landsness",
        "Daniel L. Barabasi",
        "Siddharth Narayanan",
        "Nicky Evans",
        "Shriya Reddy",
        "Martha Foiani",
        "Aizad Kamal",
        "Leah P. Shriver",
        "Fang Cao",
        "Asmamaw T. Wassie",
        "Jon M. Laurent",
        "Edwin Melville-Green",
        "Mayk Caldas",
        "Albert Bou",
        "Kaleigh F. Roberts",
        "Sladjana Zagorac",
        "Timothy C. Orr",
        "Miranda E. Orr",
        "Kevin J. Zwezdaryk",
        "Ali E. Ghareeb",
        "Laurie McCoy",
        "Bruna Gomes",
        "Euan A. Ashley",
        "Karen E. Duff",
        "Tonio Buonassisi",
        "Tom Rainforth",
        "Randall J. Bateman",
        "Michael Skarlinski",
        "Samuel G. Rodriques",
        "Michaela M. Hinks",
        "Andrew D. White"
      ],
      "abstract": "Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Kosmosï¼Œä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨åŒ–è¿›è¡Œæ•°æ®é©±åŠ¨ç§‘å­¦å‘ç°çš„AIç§‘å­¦å®¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰AIæ™ºèƒ½ä½“åœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­ç¼ºä¹è¿è´¯æ€§çš„é—®é¢˜ã€‚Kosmosåˆ©ç”¨ç‹¬ç‰¹çš„ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹(structured world model)åœ¨æ•°æ®åˆ†ææ™ºèƒ½ä½“å’Œæ–‡çŒ®æœç´¢æ™ºèƒ½ä½“ä¹‹é—´å…±äº«ä¿¡æ¯ï¼Œä½¿å…¶èƒ½å¤Ÿè¿è´¯åœ°æ‰§è¡Œè¶…è¿‡200æ¬¡æ™ºèƒ½ä½“æ¨æ¼”ã€‚è¯¥ç³»ç»Ÿå¯è¿è¡Œé•¿è¾¾12å°æ—¶ï¼Œå¾ªç¯æ‰§è¡Œå¹¶è¡Œæ•°æ®åˆ†æã€æ–‡çŒ®æœç´¢å’Œå‡è®¾ç”Ÿæˆï¼Œå¹³å‡æ¯æ¬¡è¿è¡Œæ‰§è¡Œ42,000è¡Œä»£ç å¹¶é˜…è¯»1,500ç¯‡è®ºæ–‡ï¼Œæœ€ç»ˆç”Ÿæˆå¼•ç”¨ä»£ç æˆ–åŸå§‹æ–‡çŒ®çš„ç§‘å­¦æŠ¥å‘Šã€‚è¯„ä¼°è¡¨æ˜ï¼ŒKosmosæŠ¥å‘Šçš„å‡†ç¡®ç‡è¾¾åˆ°79.4%ï¼Œå•æ¬¡20ä¸ªå¾ªç¯çš„è¿è¡Œç›¸å½“äºäººç±»åˆä½œè€…å¹³å‡6ä¸ªæœˆçš„ç ”ç©¶å·¥ä½œé‡ã€‚æ­¤å¤–ï¼ŒKosmosåœ¨ä»£è°¢ç»„å­¦ã€ææ–™ç§‘å­¦ã€ç¥ç»ç§‘å­¦å’Œç»Ÿè®¡é—ä¼ å­¦é¢†åŸŸå–å¾—äº†7é¡¹å‘ç°ï¼Œå…¶ä¸­3é¡¹ç‹¬ç«‹å¤ç°äº†æœªå‘è¡¨çš„æ‰‹ç¨¿ç»“æœï¼Œå¦å¤–4é¡¹åˆ™åšå‡ºäº†å…¨æ–°çš„ç§‘å­¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Revision: figure layout changes and minor text edits",
      "pdf_url": "https://arxiv.org/pdf/2511.02824v2",
      "published_date": "2025-11-04 18:50:52 UTC",
      "updated_date": "2025-11-05 18:26:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:00:06.737113+00:00"
    },
    {
      "arxiv_id": "2511.02823v1",
      "title": "Optimizing AI Agent Attacks With Synthetic Data",
      "title_zh": "åˆ©ç”¨åˆæˆæ•°æ®ä¼˜åŒ– AI æ™ºèƒ½ä½“æ”»å‡»",
      "authors": [
        "Chloe Loughridge",
        "Paul Colognese",
        "Avery Griffin",
        "Tyler Tracy",
        "Jon Kutasov",
        "Joe Benton"
      ],
      "abstract": "As AI deployments become more complex and high-stakes, it becomes increasingly important to be able to estimate their risk. AI control is one framework for doing so. However, good control evaluations require eliciting strong attack policies. This can be challenging in complex agentic environments where compute constraints leave us data-poor. In this work, we show how to optimize attack policies in SHADE-Arena, a dataset of diverse realistic control environments. We do this by decomposing attack capability into five constituent skills -- suspicion modeling, attack selection, plan synthesis, execution and subtlety -- and optimizing each component individually. To get around the constraint of limited data, we develop a probabilistic model of attack dynamics, optimize our attack hyperparameters using this simulation, and then show that the results transfer to SHADE-Arena. This results in a substantial improvement in attack strength, reducing safety score from a baseline of 0.87 to 0.41 using our scaffold.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹æ—¥ç›Šå¤æ‚çš„AIéƒ¨ç½²é£é™©ï¼Œæ¢è®¨äº†AIæ§åˆ¶(AI control)æ¡†æ¶ä¸‹çš„é£é™©è¯„ä¼°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨è®¡ç®—å—é™çš„ä»£ç†ç¯å¢ƒä¸­å¼•å‡ºå¼ºæœ‰åŠ›çš„æ”»å‡»ç­–ç•¥ã€‚ç ”ç©¶è€…åœ¨SHADE-Arenaæ•°æ®é›†çš„æ§åˆ¶ç¯å¢ƒä¸­ï¼Œå°†æ”»å‡»èƒ½åŠ›åˆ†è§£ä¸ºæ€€ç–‘å»ºæ¨¡(suspicion modeling)ã€æ”»å‡»é€‰æ‹©(attack selection)ã€è®¡åˆ’åˆæˆ(plan synthesis)ã€æ‰§è¡Œ(execution)å’Œéšè”½æ€§(subtlety)è¿™äº”é¡¹æŠ€èƒ½ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†å…‹æœæ•°æ®åŒ®ä¹çš„é™åˆ¶ï¼Œè¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§æ”»å‡»åŠ¨æ€çš„æ¦‚ç‡æ¨¡å‹ï¼Œé€šè¿‡åˆæˆæ•°æ®æ¨¡æ‹Ÿä¼˜åŒ–æ”»å‡»è¶…å‚æ•°ï¼Œå¹¶å°†ç»“æœæˆåŠŸè¿ç§»åˆ°SHADE-Arenaä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ”»å‡»å¼ºåº¦ï¼Œåˆ©ç”¨è¯¥æ¡†æ¶å°†åŸºçº¿æ¨¡å‹çš„å®‰å…¨è¯„åˆ†ä»0.87å¤§å¹…é™ä½åˆ°äº†0.41ï¼Œè¯æ˜äº†é€šè¿‡åˆæˆæ•°æ®ä¼˜åŒ–AIä»£ç†æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02823v1",
      "published_date": "2025-11-04 18:48:56 UTC",
      "updated_date": "2025-11-04 18:48:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:00:13.299171+00:00"
    },
    {
      "arxiv_id": "2511.02818v3",
      "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
      "title_zh": "Orion-MSPï¼šé¢å‘è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¤šå°ºåº¦ç¨€ç–æ³¨æ„åŠ›",
      "authors": [
        "Mohamed Bouadi",
        "Pratinav Seth",
        "Aditya Tanna",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Orion-MSPï¼Œä¸€ç§é’ˆå¯¹è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ (Tabular In-Context Learning)çš„åˆ›æ–°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹ï¼ˆå¦‚TabPFNï¼‰åœ¨å•å°ºåº¦ç‰¹å¾å¤„ç†ã€å¯†é›†æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡æ–¹æ‰©å±•æ€§ä»¥åŠé¡ºåºå¤„ç†é™åˆ¶ç­‰æ–¹é¢çš„ä¸è¶³ã€‚Orion-MSPå¼•å…¥äº†ä¸‰é¡¹å…³é”®æŠ€æœ¯ï¼šé‡‡ç”¨å¤šå°ºåº¦å¤„ç†ä»¥æ•æ‰åˆ†å±‚ç‰¹å¾äº¤äº’ï¼›åˆ©ç”¨ç»“åˆçª—å£ã€å…¨å±€å’Œéšæœºæ¨¡å¼çš„å—ç¨€ç–æ³¨æ„åŠ›(block-sparse attention)æ¥æå‡æ•ˆç‡å¹¶ä¿æŒé•¿ç¨‹è¿æ¥ï¼›ä»¥åŠå¼•å…¥Perceiveré£æ ¼çš„è®°å¿†æœºåˆ¶ä»¥å®ç°ç»„ä»¶é—´å®‰å…¨çš„åŒå‘ä¿¡æ¯æµã€‚è¿™äº›åˆ›æ–°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè¿­ä»£è¡¨ç¤ºç»†åŒ–å¹¶æœ‰æ•ˆå¤„ç†é«˜ç»´è¡¨æ ¼æ•°æ®ã€‚åœ¨å„ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOrion-MSPå±•ç°äº†åŒ¹é…æˆ–è¶…è¶Šå½“å‰æœ€å…ˆè¿›æŠ€æœ¯(SOTA)çš„æ€§èƒ½ï¼Œç¡®ç«‹äº†é«˜æ•ˆè¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02818v3",
      "published_date": "2025-11-04 18:43:44 UTC",
      "updated_date": "2025-11-07 18:13:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:00:38.181808+00:00"
    },
    {
      "arxiv_id": "2511.02817v1",
      "title": "Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities",
      "title_zh": "Oolongï¼šè¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸èšåˆèƒ½åŠ›",
      "authors": [
        "Amanda Bertsch",
        "Adithya Pratapa",
        "Teruko Mitamura",
        "Graham Neubig",
        "Matthew R. Gormley"
      ],
      "abstract": "As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰é•¿ä¸Šä¸‹æ–‡è¯„ä¼°ä¸»è¦ä¾èµ–æ£€ç´¢è€Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¨éƒ¨ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæå‡ºäº†Oolongï¼Œä¸€ä¸ªä¸“æ³¨äºé•¿ä¸Šä¸‹æ–‡æ¨ç†å’Œèšåˆèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸åŒäºç®€å•çš„æ£€ç´¢ä»»åŠ¡ï¼ŒOolongè¦æ±‚æ¨¡å‹åœ¨åŸå­å±‚é¢ä¸Šåˆ†æç‹¬ç«‹çš„æ–‡æœ¬å—ï¼Œå¹¶èšåˆè¿™äº›åˆ†æç»“æœæ¥å›ç­”åˆ†å¸ƒæ€§é—®é¢˜ã€‚è¯¥åŸºå‡†åˆ†ä¸ºOolong-synthï¼ˆç”¨äºæ¶ˆèç ”ç©¶çš„è‡ªç„¶ä¸»ä¹‰åˆæˆä»»åŠ¡ï¼‰å’ŒOolong-realï¼ˆåŸºäºçœŸå®å¯¹è¯æ•°æ®çš„ä¸‹æ¸¸ä»»åŠ¡ï¼‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæµ‹è¯•æ¨¡å‹åœ¨å¤§é‡ç¤ºä¾‹ä¸Šçš„åˆ†ç±»ã€è®¡æ•°ä»¥åŠå¤„ç†æ—¶é—´å’Œç”¨æˆ·å…³ç³»çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-5ã€Claude-Sonnet-4å’ŒGemini-2.5-Proç­‰å‰æ²¿æ¨¡å‹åœ¨Oolongä¸Šä¹Ÿé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œåœ¨128Kä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„å‡†ç¡®ç‡å‡ä¸è¶³50%ã€‚è¯¥ç ”ç©¶é€šè¿‡å‘å¸ƒç›¸å…³æ•°æ®å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2511.02817v1",
      "published_date": "2025-11-04 18:42:12 UTC",
      "updated_date": "2025-11-04 18:42:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:01:03.927299+00:00"
    },
    {
      "arxiv_id": "2511.02815v1",
      "title": "Assessing win strength in MLB win prediction models",
      "title_zh": "è¯„ä¼°MLBèƒœè´Ÿé¢„æµ‹æ¨¡å‹ä¸­çš„è·èƒœå¼ºåº¦",
      "authors": [
        "Morgan Allen",
        "Paul Savala"
      ],
      "abstract": "In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset. In addition, we relate the win probabilities produced by these models to win strength as measured by score differential. In doing so we show that the most common machine learning models do indeed demonstrate a relationship between predicted win probability and the strength of the win. Finally, we analyze the results of using predicted win probabilities as a decision making mechanism on run-line betting. We demonstrate positive returns when utilizing appropriate betting strategies, and show that naive use of machine learning models for betting lead to significant loses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¾å›½èŒä¸šæ£’çƒå¤§è”ç›Ÿ(MLB)çš„æ¯”èµ›ç»“æœé¢„æµ‹ï¼Œé€šè¿‡ä½¿ç”¨é€šç”¨æ•°æ®é›†è®­ç»ƒäº†ä¸€å¥—å…¨é¢çš„æœºå™¨å­¦ä¹ (machine learning)æ¨¡å‹ã€‚ä½œè€…ä¸ä»…é¢„æµ‹è·èƒœé˜Ÿä¼ï¼Œè¿˜è¿›ä¸€æ­¥å°†æ¨¡å‹ç”Ÿæˆçš„è·èƒœæ¦‚ç‡ä¸ä»¥åˆ†å·®è¡¡é‡çš„è·èƒœå¼ºåº¦(win strength)è”ç³»èµ·æ¥ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç¡®å®å±•ç¤ºäº†é¢„æµ‹è·èƒœæ¦‚ç‡ä¸è·èƒœå¼ºåº¦ä¹‹é—´çš„å…³è”ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ†æäº†å°†é¢„æµ‹è·èƒœæ¦‚ç‡ä½œä¸ºå†³ç­–æœºåˆ¶åº”ç”¨äºè®©åˆ†ç›˜æŠ•æ³¨(run-line betting)çš„ç»“æœã€‚å®éªŒè¯æ˜ï¼Œé‡‡ç”¨é€‚å½“çš„æŠ•æ³¨ç­–ç•¥å¯ä»¥è·å¾—æ­£å‘å›æŠ¥ï¼Œè€Œç®€å•åœ°ç›´æ¥ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡ŒæŠ•æ³¨åˆ™ä¼šå¯¼è‡´æ˜¾è‘—æŸå¤±ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02815v1",
      "published_date": "2025-11-04 18:40:10 UTC",
      "updated_date": "2025-11-04 18:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:01:24.217354+00:00"
    },
    {
      "arxiv_id": "2511.02805v1",
      "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning",
      "title_zh": "MemSearcherï¼šé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹å®ç°æ¨ç†ã€æœç´¢åŠè®°å¿†ç®¡ç†",
      "authors": [
        "Qianhao Yuan",
        "Jie Lou",
        "Zichao Li",
        "Jiawei Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Le Sun",
        "Debing Zhang",
        "Xianpei Han"
      ],
      "abstract": "Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MemSearcherï¼Œæ—¨åœ¨è§£å†³æœç´¢æ™ºèƒ½ä½“åœ¨å¤„ç†äº¤äº’å†å²æ—¶é¢ä¸´çš„ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬æˆ–ä»…ä½¿ç”¨å½“å‰è½®æ¬¡å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚MemSearcheré‡‡ç”¨ä¸€ç§è¿­ä»£ç»´æŠ¤ç´§å‡‘è®°å¿†(compact memory)çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼Œå°†ç”¨æˆ·é—®é¢˜ä¸è®°å¿†èåˆä»¥ç”Ÿæˆæ¨ç†è½¨è¿¹ã€æ‰§è¡Œæœç´¢åŠ¨ä½œï¼Œå¹¶æ›´æ–°è®°å¿†ä»¥ä»…ä¿ç•™è§£å†³ä»»åŠ¡æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸€æµç¨‹ï¼Œè®ºæ–‡å¼•å…¥äº†multi-context GRPOï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ï¼Œèƒ½å¤Ÿè”åˆä¼˜åŒ–æ™ºèƒ½ä½“çš„æ¨ç†ã€æœç´¢ç­–ç•¥å’Œè®°å¿†ç®¡ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­é‡‡æ ·è½¨è¿¹ç»„ï¼Œå¹¶åœ¨æ‰€æœ‰å¯¹è¯ä¸­ä¼ æ’­è½¨è¿¹çº§ä¼˜åŠ¿æ¥å®ç°ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMemSearcheråœ¨Qwen2.5-3B-Instructå’ŒQwen2.5-7B-Instructä¸Šåˆ†åˆ«å–å¾—äº†11%å’Œ12%çš„ç›¸å¯¹å¹³å‡å¢ç›Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäº3Bå‚æ•°çš„MemSearcherç”šè‡³è¶…è¶Šäº†åŸºäº7Bå‚æ•°çš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨ä¿¡æ¯å®Œæ•´æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡å¯ä»¥åŒæ—¶å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project page: https://github.com/icip-cas/MemSearcher",
      "pdf_url": "https://arxiv.org/pdf/2511.02805v1",
      "published_date": "2025-11-04 18:27:39 UTC",
      "updated_date": "2025-11-04 18:27:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:02:28.902244+00:00"
    },
    {
      "arxiv_id": "2511.02802v3",
      "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models",
      "title_zh": "TabTuneï¼šè¡¨æ ¼åŸºç¡€æ¨¡å‹æ¨ç†ä¸å¾®è°ƒçš„ç»Ÿä¸€åº“",
      "authors": [
        "Aditya Tanna",
        "Pratinav Seth",
        "Mohamed Bouadi",
        "Utsav Avaiya",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼åŸºç¡€æ¨¡å‹(Tabular foundation models)åœ¨åº”ç”¨ä¸­é¢ä¸´çš„é¢„å¤„ç†å¼‚æ„ã€APIç¢ç‰‡åŒ–åŠç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°ç­‰æŒ‘æˆ˜ï¼Œæ¨å‡ºäº†TabTuneè¿™ä¸€ç»Ÿä¸€åº“ã€‚TabTuneé€šè¿‡å•ä¸€æ¥å£æ ‡å‡†åŒ–äº†å®Œæ•´çš„æ¨¡å‹å·¥ä½œæµï¼Œæä¾›å¯¹ä¸ƒç§æœ€å…ˆè¿›(SOTA)æ¨¡å‹çš„ä¸€è‡´è®¿é—®ï¼Œå¹¶æ”¯æŒé›¶æ ·æœ¬æ¨ç†(zero-shot inference)ã€å…ƒå­¦ä¹ (meta-learning)ã€ç›‘ç£å¾®è°ƒ(SFT)åŠå‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)ç­‰å¤šç§é€‚åº”ç­–ç•¥ã€‚è¯¥æ¡†æ¶ä¸ä»…è‡ªåŠ¨å¤„ç†æ¨¡å‹æ„ŸçŸ¥çš„é¢„å¤„ç†å¹¶ç®¡ç†æ¶æ„å¼‚æ„æ€§ï¼Œè¿˜é›†æˆäº†é’ˆå¯¹æ€§èƒ½ã€æ ¡å‡†(calibration)å’Œå…¬å¹³æ€§(fairness)çš„è¯„ä¼°æ¨¡å—ã€‚é€šè¿‡å¼ºè°ƒå¯æ‰©å±•æ€§å’Œå¯å¤ç°æ€§ï¼ŒTabTuneå®ç°äº†å¯¹è¡¨æ ¼åŸºç¡€æ¨¡å‹é€‚åº”ç­–ç•¥çš„ä¸€è‡´æ€§åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆä¿ƒè¿›äº†ç»“æ„åŒ–æ•°æ®å­¦ä¹ é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The library is open source and available at https://github.com/Lexsi-Labs/TabTune",
      "pdf_url": "https://arxiv.org/pdf/2511.02802v3",
      "published_date": "2025-11-04 18:25:17 UTC",
      "updated_date": "2025-12-02 18:48:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:02:46.669269+00:00"
    },
    {
      "arxiv_id": "2511.02794v1",
      "title": "When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning",
      "title_zh": "å½“å•ä¸€æ¨¡æ€ç ´åå…¶ä»–æ¨¡æ€ï¼šå¤šæ¨¡æ€æ¨ç†çš„è¯Šæ–­é€é•œ",
      "authors": [
        "Chenyu Zhang",
        "Minsol Kim",
        "Shohreh Ghorbani",
        "Jingyao Wu",
        "Rosalind Picard",
        "Patricia Maes",
        "Paul Pu Liang"
      ],
      "abstract": "Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing. A simple fusion mechanism aggregates these outputs, exposing contributors (modalities supporting correct outcomes) and saboteurs (modalities that mislead). Applying our diagnostic layer in a case study on multimodal emotion recognition benchmarks with foundation models revealed systematic reliability profiles, providing insight into whether failures may arise from dataset artifacts or model limitations. More broadly, our framework offers a diagnostic scaffold for multimodal reasoning, supporting principled auditing of fusion dynamics and informing possible interventions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)æ¨ç†è¿‡ç¨‹ä¸é€æ˜çš„é—®é¢˜ï¼Œå¼•å…¥äº†\"modality sabotage\"ï¼ˆæ¨¡æ€ç ´åï¼‰æ¦‚å¿µï¼Œå³é«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯è¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼æœ€ç»ˆèåˆç»“æœçš„ç°è±¡ã€‚ä¸ºäº†åˆ†æè¿™ç§åŠ¨æ€ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è½»é‡çº§ä¸”ä¸æ¨¡å‹æ— å…³çš„è¯„ä¼°å±‚ï¼Œå°†æ¯ç§æ¨¡æ€è§†ä¸ºç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾åŠç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ç”¨äºå®¡è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„èåˆæœºåˆ¶èšåˆè¾“å‡ºï¼Œä»è€Œè¯†åˆ«å‡ºæ”¯æŒæ­£ç¡®ç»“æœçš„è´¡çŒ®è€…å’Œäº§ç”Ÿè¯¯å¯¼çš„ç ´åè€…ã€‚åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºï¼Œè¯¥è¯Šæ–­å±‚èƒ½æœ‰æ•ˆæ­ç¤ºç³»ç»Ÿçš„å¯é æ€§ç‰¹å¾ï¼Œå¸®åŠ©åˆ¤æ–­æ•…éšœæºäºæ•°æ®é›†ç¼ºé™·è¿˜æ˜¯æ¨¡å‹å±€é™ã€‚è¿™ä¸€æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†é‡è¦çš„è¯Šæ–­æ”¯æ¶ï¼Œæ”¯æŒå¯¹èåˆåŠ¨æ€è¿›è¡ŒåŸåˆ™æ€§å®¡è®¡å¹¶ä¸ºåç»­å¹²é¢„æä¾›äº†ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop, NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.02794v1",
      "published_date": "2025-11-04 18:20:13 UTC",
      "updated_date": "2025-11-04 18:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:03:12.894314+00:00"
    },
    {
      "arxiv_id": "2511.02781v1",
      "title": "Measuring AI Diffusion: A Population-Normalized Metric for Tracking Global AI Usage",
      "title_zh": "è¡¡é‡äººå·¥æ™ºèƒ½æ‰©æ•£ï¼šè¿½è¸ªå…¨çƒ AI ä½¿ç”¨æƒ…å†µçš„äººå£å½’ä¸€åŒ–æŒ‡æ ‡",
      "authors": [
        "Amit Misra",
        "Jane Wang",
        "Scott McCullers",
        "Kevin White",
        "Juan Lavista Ferres"
      ],
      "abstract": "Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion. We find wide variation in adoption, with a strong correlation between AI User Share and GDP. High uptake is concentrated in developed economies, though usage among internet-connected populations in lower-income countries reveals substantial latent demand. We also detect sharp increases in usage following major product launches, such as DeepSeek in early 2025. While the metric's reliance solely on Microsoft telemetry introduces potential biases related to this user base, it offers an important new lens into how AI is spreading globally. AI User Share enables timely benchmarking that can inform data-driven AI policy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¼ºä¹äººå£æ ‡å‡†åŒ–è·¨å›½ä½¿ç”¨æ•°æ®å¯¼è‡´çš„å…¨çƒAIæ‰©æ•£æµ‹é‡éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºAI User Shareçš„æ–°æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡åˆ©ç”¨åŒ¿åçš„Microsoft telemetryæ•°æ®æ„å»ºï¼Œå¹¶ç»è¿‡è®¾å¤‡æ¥å…¥å’Œç§»åŠ¨ç«¯æ¯”ä¾‹è°ƒæ•´ï¼Œæ—¨åœ¨ä¼°ç®—å„å›½åŠ³åŠ¨å¹´é¾„äººå£ä¸­æ´»è·ƒä½¿ç”¨AIå·¥å…·çš„æ¯”ä¾‹ã€‚ç ”ç©¶è¦†ç›–äº†147ä¸ªç»æµä½“ï¼Œæä¾›äº†å…³äºå…¨çƒAIæ‰©æ•£çš„ä¸€è‡´ä¸”å®æ—¶çš„æ´å¯Ÿã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIé‡‡ç”¨ç‡å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œä¸”AI User Shareä¸GDPä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œé«˜é‡‡ç”¨ç‡ä¸»è¦é›†ä¸­åœ¨å‘è¾¾ç»æµä½“ã€‚ç„¶è€Œï¼Œä½æ”¶å…¥å›½å®¶äº’è”ç½‘è¿æ¥äººç¾¤ä¸­çš„ä½¿ç”¨æƒ…å†µä¹Ÿæ­ç¤ºäº†å·¨å¤§çš„æ½œåœ¨éœ€æ±‚ï¼Œä¸”åœ¨DeepSeekç­‰ä¸»è¦äº§å“å‘å¸ƒåï¼Œä½¿ç”¨é‡å‡ºç°äº†æ€¥å‰§å¢é•¿ã€‚å°½ç®¡ä»…ä¾èµ–å¾®è½¯æ•°æ®å¯èƒ½å¼•å…¥åå·®ï¼Œä½†è¯¥æŒ‡æ ‡ä¸ºå…¨çƒAIä¼ æ’­æä¾›äº†é‡è¦çš„æ–°è§†è§’ï¼Œæœ‰åŠ©äºåˆ¶å®šæ•°æ®é©±åŠ¨çš„AIæ”¿ç­–ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 6 figures, 2 tables. Also available at https://aka.ms/AI_Diffusion_Technical_Report",
      "pdf_url": "https://arxiv.org/pdf/2511.02781v1",
      "published_date": "2025-11-04 18:03:51 UTC",
      "updated_date": "2025-11-04 18:03:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:03:35.669066+00:00"
    },
    {
      "arxiv_id": "2511.02780v2",
      "title": "PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts",
      "title_zh": "PoCoï¼šé¢å‘æ™ºèƒ½åˆçº¦çš„ä»£ç†å¼æ¦‚å¿µéªŒè¯æ¼æ´åˆ©ç”¨ç”Ÿæˆ",
      "authors": [
        "Vivi Andersson",
        "Sofia Bobadilla",
        "Harald Hobbelhagen",
        "Martin Monperrus"
      ],
      "abstract": "Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce POCO, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. POCO autonomously generates PoC exploits in an agentic manner by interacting with a set of code-execution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate POCO on a dataset of 23 real-world vulnerability reports. POCO consistently outperforms the prompting and workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides readily actionable knowledge for the smart contract security community.",
      "tldr_zh": "é’ˆå¯¹æ™ºèƒ½åˆçº¦å®¡è®¡ä¸­äººå·¥ç¼–å†™æ¦‚å¿µéªŒè¯(Proof-of-Concept, PoC)è€—æ—¶ä¸”æ˜“é”™çš„éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†POCOï¼Œä¸€ç§è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡ŒPoCåˆ©ç”¨ä»£ç çš„Agenticæ¡†æ¶ã€‚POCOèƒ½å¤Ÿæ ¹æ®å®¡è®¡äººå‘˜æä¾›çš„è‡ªç„¶è¯­è¨€æ¼æ´æè¿°ï¼Œé€šè¿‡Reason-Act-Observeå¾ªç¯è‡ªä¸»ä¸ä»£ç æ‰§è¡Œå·¥å…·äº¤äº’ï¼Œç”Ÿæˆå…¼å®¹Foundryæµ‹è¯•æ¡†æ¶çš„å®Œæ•´åˆ©ç”¨ä»£ç ã€‚åœ¨åŒ…å«23ä¸ªçœŸå®ä¸–ç•Œæ¼æ´æŠ¥å‘Šçš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒPOCOåœ¨ç”Ÿæˆç»“æ„è‰¯å¥½ä¸”é€»è¾‘æ­£ç¡®çš„PoCæ–¹é¢è¡¨ç°ä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹å’Œå·¥ä½œæµåŸºçº¿ã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜äº†Agenticæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—é™ä½æ™ºèƒ½åˆçº¦å®¡è®¡ä¸­ç”Ÿæˆé«˜è´¨é‡PoCæ‰€éœ€çš„å·¥ä½œé‡ï¼Œä¸ºå®‰å…¨ç¤¾åŒºæä¾›äº†å¯ç›´æ¥åº”ç”¨çš„çŸ¥è¯†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2511.02780v2",
      "published_date": "2025-11-04 18:03:12 UTC",
      "updated_date": "2025-11-06 12:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:06:41.226204+00:00"
    },
    {
      "arxiv_id": "2511.02769v1",
      "title": "STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation",
      "title_zh": "STAR-VAEï¼šç”¨äºå¯æ‰©å±•ä¸å¯æ§åˆ†å­ç”Ÿæˆçš„æ½œå˜é‡ Transformer",
      "authors": [
        "Bum Chul Kwon",
        "Ben Shapira",
        "Moshiko Raboh",
        "Shreyans Sethi",
        "Shruti Murarka",
        "Joseph A Morrone",
        "Jianying Hu",
        "Parthasarathy Suryanarayanan"
      ],
      "abstract": "The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†STAR-VAEï¼Œä¸€ç§åŸºäºTransformerå’Œè‡ªå›å½’å˜åˆ†è‡ªç¼–ç å™¨(Variational Auto Encoder)çš„å¯æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯ç‰©åˆ†å­ç”Ÿæˆä¸­çš„å¹¿é˜”åŒ–å­¦ç©ºé—´å»ºæ¨¡å’Œå¯æ§ç”Ÿæˆé—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨SELFIESè¡¨ç¤ºæ³•ç¡®ä¿ç”Ÿæˆçš„å¥æ³•æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æ¥è‡ªPubChemçš„7900ä¸‡ä¸ªç±»è¯ç‰©åˆ†å­ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡è®­ç»ƒã€‚STAR-VAEé‡‡ç”¨äº†ä¸€ç§åŸåˆ™æ€§çš„æ¡ä»¶æ½œå˜é‡å…¬å¼ï¼Œé€šè¿‡å±æ€§é¢„æµ‹å™¨æä¾›è°ƒèŠ‚ä¿¡å·ï¼Œä»è€Œå®ç°å±æ€§å¼•å¯¼çš„ç²¾ç¡®ç”Ÿæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä½ç§©é€‚é…å™¨(LoRA)è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨æœ‰é™çš„å±æ€§å’Œæ´»æ€§æ•°æ®å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚åœ¨GuacaMolå’ŒMOSESåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åŒ¹é…æˆ–è¶…è¿‡äº†ç°æœ‰åŸºçº¿ï¼Œä¸”æ½œåœ¨ç©ºé—´åˆ†ææ˜¾ç¤ºå‡ºå¹³æ»‘çš„è¯­ä¹‰ç»“æ„ï¼Œæ”¯æŒæ— æ¡ä»¶æ¢ç´¢å’Œå±æ€§æ„ŸçŸ¥ç”Ÿæˆã€‚åœ¨TartarusåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæ¡ä»¶æ¨¡å‹èƒ½æœ‰æ•ˆæ”¹å–„å¯¹æ¥åˆ†æ•°ï¼Œè¯æ˜äº†ç°ä»£åŒ–ã€è§„æ¨¡é€‚å®œçš„VAEç»“åˆåŸåˆ™æ€§è°ƒèŠ‚å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒåœ¨åˆ†å­ç”Ÿæˆé¢†åŸŸå…·æœ‰å¼ºå¤§çš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.02769v1",
      "published_date": "2025-11-04 17:56:00 UTC",
      "updated_date": "2025-11-04 17:56:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:07:07.386146+00:00"
    },
    {
      "arxiv_id": "2511.11615v1",
      "title": "Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates",
      "title_zh": "ç”¨äºåœˆå…»çµé•¿ç±»ç”Ÿç‰©å£°å­¦æ£€æµ‹ä¸å«å£°ç›‘æµ‹çš„è½»é‡çº§ Hopfield ç¥ç»ç½‘ç»œ",
      "authors": [
        "Wendy Lomas",
        "Andrew Gascoyne",
        "Colin Dubreuil",
        "Stefano Vaglio",
        "Liam Naughton"
      ],
      "abstract": "Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è¢«åŠ¨å£°å­¦ç›‘æµ‹ä¸­æ•°æ®å¤„ç†ç§¯å‹å’Œå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)èµ„æºæ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºHopfieldç¥ç»ç½‘ç»œ(HNN)æ¶æ„çš„è½»é‡çº§è”æƒ³è®°å¿†AIæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«åº”ç”¨äºåœˆå…»æåº¦æ¿’å±çš„é»‘ç™½é¢†ç‹çŒ´(*Varecia variegata*)çš„é¸£å«ç›‘æµ‹ï¼Œé€šè¿‡åœ¨HNNä¸­å­˜å‚¨æ„Ÿå…´è¶£çš„ç¤¾äº¤é¸£å«æ¥æ£€æµ‹å£°å­¦æ•°æ®é›†ä¸­çš„å…¶ä»–å®ä¾‹ã€‚ç ”ç©¶è€…é€šè¿‡å­˜å‚¨ç”±è¿åŠ¨å¼•èµ·çš„é¢å¤–ä¿¡å·æ˜¾è‘—æ”¹è¿›äº†æ¨¡å‹ï¼Œå®ç°äº†0.94çš„æ€»ä½“å‡†ç¡®ç‡ã€‚è¯¥æ¨¡å‹å…·å¤‡æé«˜çš„æ•ˆç‡ï¼Œèƒ½åœ¨æ ‡å‡†ç¬”è®°æœ¬ç”µè„‘ä¸Šæ¯ç§’æ‰§è¡Œ340æ¬¡åˆ†ç±»ï¼Œæ¯åˆ†é’Ÿå¤„ç†è¶…è¿‡5.5å°æ—¶çš„éŸ³é¢‘æ•°æ®ï¼Œä¸”è®­ç»ƒæ—¶é—´ä»…éœ€æ¯«ç§’çº§ã€‚ä½œä¸ºä¸€ç§é€æ˜ä¸”å¿«é€Ÿè®­ç»ƒçš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒèƒ½æœ‰æ•ˆç¼©çŸ­æ•°æ®åˆ°æ´å¯Ÿçš„è½¬æ¢æ—¶é—´ï¼Œé€‚ç”¨äºé‡å¤–å’Œåœˆå…»ç¯å¢ƒä¸‹çš„å¿«é€Ÿå†³ç­–ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "16 pages, 3 figures, Proceedings of the Future Technologies Conference (FTC) 2025, Volume 1",
      "pdf_url": "https://arxiv.org/pdf/2511.11615v1",
      "published_date": "2025-11-04 17:46:03 UTC",
      "updated_date": "2025-11-04 17:46:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:07:33.276529+00:00"
    },
    {
      "arxiv_id": "2511.02759v1",
      "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer",
      "title_zh": "LLMæ”¯æŒçš„å½¢å¼åŒ–çŸ¥è¯†è¡¨ç¤ºï¼šåˆ©ç”¨äº¤äº’å¼è¯­ä¹‰å±‚å¢å¼ºæ§åˆ¶å·¥ç¨‹å†…å®¹",
      "authors": [
        "Julius Fiedler",
        "Carsten Knoll",
        "Klaus RÃ¶benack"
      ],
      "abstract": "The rapid growth of research output in control engineering calls for new approaches to structure and formalize domain knowledge. This paper briefly describes an LLM-supported method for semi-automated generation of formal knowledge representations that combine human readability with machine interpretability and increased expressiveness. Based on the Imperative Representation of Knowledge (PyIRK) framework, we demonstrate how language models can assist in transforming natural-language descriptions and mathematical definitions (available as LaTeX source code) into a formalized knowledge graph. As a first application we present the generation of an ``interactive semantic layer'' to enhance the source documents in order to facilitate knowledge transfer. From our perspective this contributes to the vision of easily accessible, collaborative, and verifiable knowledge bases for the control engineering domain.",
      "tldr_zh": "é’ˆå¯¹æ§åˆ¶å·¥ç¨‹é¢†åŸŸç ”ç©¶äº§å‡ºå¿«é€Ÿå¢é•¿å¸¦æ¥çš„çŸ¥è¯†ç»“æ„åŒ–éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)æ”¯æŒçš„åŠè‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå…¼å…·äººç±»å¯è¯»æ€§ä¸æœºå™¨å¯è§£é‡Šæ€§çš„å½¢å¼åŒ–çŸ¥è¯†è¡¨ç¤ºã€‚è¯¥ç ”ç©¶åŸºäºImperative Representation of Knowledge (PyIRK)æ¡†æ¶ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¯­è¨€æ¨¡å‹ååŠ©å°†è‡ªç„¶è¯­è¨€æè¿°å’Œæ•°å­¦å®šä¹‰ï¼ˆLaTeXæºä»£ç ï¼‰è½¬åŒ–ä¸ºå½¢å¼åŒ–çš„çŸ¥è¯†å›¾è°±(knowledge graph)ã€‚ä½œä¸ºé¦–ä¸ªåº”ç”¨æ¡ˆä¾‹ï¼Œä½œè€…ç”Ÿæˆäº†ä¸€ç§â€œäº¤äº’å¼è¯­ä¹‰å±‚(interactive semantic layer)â€æ¥å¢å¼ºæºæ–‡æ¡£ï¼Œä»è€Œä¿ƒè¿›çŸ¥è¯†çš„æœ‰æ•ˆä¼ é€’ã€‚è¿™ä¸€æ–¹æ³•è‡´åŠ›äºå®ç°æ§åˆ¶å·¥ç¨‹é¢†åŸŸä¸­æ˜“äºè®¿é—®ã€å¯åä½œä¸”å¯éªŒè¯çš„çŸ¥è¯†åº“æ„¿æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02759v1",
      "published_date": "2025-11-04 17:36:57 UTC",
      "updated_date": "2025-11-04 17:36:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:08:03.937304+00:00"
    },
    {
      "arxiv_id": "2511.02752v1",
      "title": "AI Diffusion in Low Resource Language Countries",
      "title_zh": "ä½èµ„æºè¯­è¨€å›½å®¶çš„äººå·¥æ™ºèƒ½æ‰©æ•£",
      "authors": [
        "Amit Misra",
        "Syed Waqas Zamir",
        "Wassim Hamidouche",
        "Inbal Becker-Reshef",
        "Juan Lavista Ferres"
      ],
      "abstract": "Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨å…¨çƒæ‰©æ•£é€Ÿåº¦æå¿«ä½†é‡‡ç”¨ä¸å‡çš„ç°è±¡ï¼Œç‰¹åˆ«å…³æ³¨ä½èµ„æºè¯­è¨€å›½å®¶ï¼ˆLow-Resource Language Countries, LRLCsï¼‰ã€‚å°½ç®¡å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºï¼Œå®ƒä»¬åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°å¾€å¾€ä¸ä½³ï¼Œä½œè€…å‡è®¾è¿™ç§æ€§èƒ½ç¼ºé™·é™ä½äº†AIçš„æ•ˆç”¨ï¼Œä»è€Œå‡ç¼“äº†åœ¨LRLCsçš„é‡‡ç”¨é€Ÿåº¦ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨åŠ æƒå›å½’æ¨¡å‹ï¼ˆweighted regression modelï¼‰ï¼Œå°†è¯­è¨€æ•ˆåº”ä¸ç¤¾ä¼šç»æµåŠäººå£å› ç´ éš”ç¦»å¼€æ¥ã€‚ç»“æœå‘ç°ï¼Œç›¸å¯¹äºåŸºçº¿æ°´å¹³ï¼ŒLRLCsçš„AIç”¨æˆ·ä»½é¢å¤§çº¦ä½äº†20%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè¯­è¨€çš„å¯åŠæ€§æ˜¯é˜»ç¢AIå…¬å¹³æ‰©æ•£çš„ä¸€ä¸ªæ˜¾è‘—ä¸”ç‹¬ç«‹çš„éšœç¢ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 tables. Also available at https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries",
      "pdf_url": "https://arxiv.org/pdf/2511.02752v1",
      "published_date": "2025-11-04 17:31:39 UTC",
      "updated_date": "2025-11-04 17:31:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:08:16.905944+00:00"
    },
    {
      "arxiv_id": "2511.02749v1",
      "title": "Using Span Queries to Optimize for Cache and Attention Locality",
      "title_zh": "åˆ©ç”¨è·¨åº¦æŸ¥è¯¢ä¼˜åŒ–ç¼“å­˜ä¸æ³¨æ„åŠ›å±€éƒ¨æ€§",
      "authors": [
        "Paul Castro",
        "Nick Mitchell",
        "Nathan Ordonez",
        "Thomas Parnell",
        "Mudhakar Srivatsa",
        "Antoni Viros i Martin"
      ],
      "abstract": "Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†Span Queriesï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–æ¨ç†æœåŠ¡å™¨ä»¥é€‚åº”æ¨ç†æ—¶æ‰©å±•ï¼ˆinference-time scalingï¼‰å’Œæ·±åº¦æ¨ç†ç­‰æ–°å‹å·¥ä½œè´Ÿè½½çš„é€šç”¨æ¥å£ã€‚é’ˆå¯¹ç°æœ‰æœåŠ¡å™¨ä¸»è¦ä¼˜åŒ–èŠå¤©å®Œæˆï¼ˆchat completionï¼‰è€Œå¿½è§†RAGå’ŒAgenticå·¥ä½œè´Ÿè½½çš„é—®é¢˜ï¼Œä½œè€…åˆ©ç”¨è¾“å…¥é¡ºåºçš„äº¤æ¢æ€§ï¼ˆcommutativityï¼‰çº¦æŸï¼Œå°†Span Querieså®šä¹‰ä¸ºæ¨ç†è°ƒç”¨çš„è¡¨è¾¾å¼æ ‘ã€‚é€šè¿‡å¯¹vLLMè¿›è¡Œå°‘é‡ä»£ç ä¿®æ”¹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ä¼˜åŒ–KV cacheå±€éƒ¨æ€§ï¼Œåœ¨ä¸¤ç§éèŠå¤©ç”¨ä¾‹ä¸­å®ç°äº†10-20å€çš„Time to First Token (TTFT) é™ä½ã€‚æ­¤å¤–ï¼ŒSpan Queriesè¿˜èƒ½é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›å±€éƒ¨æ€§ï¼ˆattention localityï¼‰æ¥è§£å†³\"lost-in-the-middle\"é—®é¢˜ï¼Œå®éªŒè¯æ˜ç»è¿‡ä¼˜åŒ–çš„2bå‚æ•°æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šå¤§å¹…è¶…è¶Šäº†æ ‡å‡†çš„8bæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02749v1",
      "published_date": "2025-11-04 17:22:49 UTC",
      "updated_date": "2025-11-04 17:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:08:44.052198+00:00"
    },
    {
      "arxiv_id": "2511.05574v1",
      "title": "Elements of Active Continuous Learning and Uncertainty Self-Awareness: a Narrow Implementation for Face and Facial Expression Recognition",
      "title_zh": "ä¸»åŠ¨è¿ç»­å­¦ä¹ ä¸ä¸ç¡®å®šæ€§è‡ªæˆ‘æ„ŸçŸ¥è¦ç´ ï¼šäººè„¸ä¸é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„ç‹­ä¹‰å®ç°",
      "authors": [
        "Stanislav Selitskiy"
      ],
      "abstract": "Reflection on one's thought process and making corrections to it if there exists dissatisfaction in its performance is, perhaps, one of the essential traits of intelligence. However, such high-level abstract concepts mandatory for Artificial General Intelligence can be modelled even at the low level of narrow Machine Learning algorithms. Here, we present the self-awareness mechanism emulation in the form of a supervising artificial neural network (ANN) observing patterns in activations of another underlying ANN in a search for indications of the high uncertainty of the underlying ANN and, therefore, the trustworthiness of its predictions. The underlying ANN is a convolutional neural network (CNN) ensemble employed for face recognition and facial expression tasks. The self-awareness ANN has a memory region where its past performance information is stored, and its learnable parameters are adjusted during the training to optimize the performance. The trustworthiness verdict triggers the active learning mode, giving elements of agency to the machine learning algorithm that asks for human help in high uncertainty and confusion conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººè„¸å’Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§èåˆä¸»åŠ¨è¿ç»­å­¦ä¹ ï¼ˆActive Continuous Learningï¼‰ä¸ä¸ç¡®å®šæ€§è‡ªæˆ‘æ„è¯†ï¼ˆUncertainty Self-Awarenessï¼‰çš„å®ç°æ–¹æ¡ˆã€‚ç ”ç©¶é€šè¿‡æ„å»ºä¸€ä¸ªç›‘ç£äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰æ¥æ¨¡æ‹Ÿè‡ªæˆ‘æ„è¯†æœºåˆ¶ï¼Œè¯¥ç½‘ç»œè´Ÿè´£è§‚æµ‹åº•å±‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰é›†æˆçš„æ¿€æ´»æ¨¡å¼ï¼Œä»è€Œè¯†åˆ«é«˜ä¸ç¡®å®šæ€§è¿¹è±¡å¹¶è¯„ä¼°é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚è¯¥è‡ªæˆ‘æ„è¯†ANNå…·å¤‡å­˜å‚¨è¿‡å¾€æ€§èƒ½çš„è®°å¿†åŒºåŸŸï¼Œå¹¶é€šè¿‡å‚æ•°è°ƒæ•´æ¥ä¼˜åŒ–æ•´ä½“è¡¨ç°ã€‚å½“ç³»ç»Ÿåˆ¤å®šé¢„æµ‹å¯ä¿¡åº¦ä¸è¶³æ—¶ï¼Œä¼šè§¦å‘ä¸»åŠ¨å­¦ä¹ æ¨¡å¼ï¼Œä½¿ç®—æ³•å…·å¤‡åœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹ä¸»åŠ¨å¯»æ±‚äººç±»è¾…åŠ©çš„ä»£ç†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•åœ¨ç‹­ä¹‰æœºå™¨å­¦ä¹ ç®—æ³•ä¸­æ¨¡æ‹Ÿé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ‰€éœ€çš„åæ€ä¸è‡ªæˆ‘ä¿®æ­£ç­‰é«˜çº§æŠ½è±¡æ¦‚å¿µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05574v1",
      "published_date": "2025-11-04 17:01:53 UTC",
      "updated_date": "2025-11-04 17:01:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:09:08.064707+00:00"
    },
    {
      "arxiv_id": "2511.02734v1",
      "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents",
      "title_zh": "CostBenchï¼šè¯„ä¼° LLM å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„å¤šè½®æˆæœ¬æœ€ä¼˜è§„åˆ’ä¸é€‚åº”",
      "authors": [
        "Jiayu Liu",
        "Cheng Qian",
        "Zhaochen Su",
        "Qing Zong",
        "Shijue Huang",
        "Bingxiang He",
        "Yi R. Fung"
      ],
      "abstract": "Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“è¯„ä¼°ä¸­å¿½è§†èµ„æºæ•ˆç‡å’Œé€‚åº”æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†CostBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“ç»æµæ¨ç†å’Œé‡è§„åˆ’èƒ½åŠ›çš„å¯æ‰©å±•åŸºå‡†ã€‚CostBenchè®¾å®šåœ¨æ—…è¡Œè§„åˆ’é¢†åŸŸï¼ŒåŒ…å«å¯é€šè¿‡å¤šç§å…·æœ‰ä¸åŒå®šåˆ¶æˆæœ¬çš„åŸå­å’Œå¤åˆå·¥å…·åºåˆ—è§£å†³çš„ä»»åŠ¡ï¼Œå¹¶æ”¯æŒå¦‚å·¥å…·æ•…éšœå’Œæˆæœ¬å˜åŒ–ç­‰åŠ¨æ€é˜»æ–­äº‹ä»¶ï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„ä¸å¯é¢„æµ‹æ€§å¹¶è¿«ä½¿æ™ºèƒ½ä½“å®æ—¶é€‚åº”ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨æˆæœ¬æ„ŸçŸ¥è§„åˆ’æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå³ä½¿æ˜¯GPT-5åœ¨é™æ€å›°éš¾ä»»åŠ¡ä¸Šçš„ç²¾ç¡®åŒ¹é…ç‡ä¹Ÿä¸è¶³75%ï¼Œè€Œåœ¨åŠ¨æ€æ¡ä»¶ä¸‹æ€§èƒ½æ›´æ˜¯ä¸‹é™äº†çº¦40%ã€‚CostBenché€šè¿‡è¯Šæ–­è¿™äº›å¼±ç‚¹ï¼Œä¸ºæœªæ¥å¼€å‘å…¼å…·ç»æµç†æ€§å’Œé²æ£’æ€§çš„æ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02734v1",
      "published_date": "2025-11-04 16:58:29 UTC",
      "updated_date": "2025-11-04 16:58:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:11:42.432905+00:00"
    },
    {
      "arxiv_id": "2511.02720v1",
      "title": "LLEXICORP: End-user Explainability of Convolutional Neural Networks",
      "title_zh": "LLEXICORPï¼šå·ç§¯ç¥ç»ç½‘ç»œçš„ç»ˆç«¯ç”¨æˆ·å¯è§£é‡Šæ€§",
      "authors": [
        "VojtÄ›ch KÅ¯r",
        "Adam Bajger",
        "Adam KukuÄka",
        "Marek Hradil",
        "VÃ­t Musil",
        "TomÃ¡Å¡ BrÃ¡zdil"
      ],
      "abstract": "Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.\n  To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.\n  We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLEXICORPï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ç»ˆç«¯ç”¨æˆ·å¯è§£é‡Šæ€§çš„æ¨¡å—åŒ–ç®¡é“ã€‚é’ˆå¯¹ç°æœ‰æ¦‚å¿µç›¸å…³æ€§ä¼ æ’­ï¼ˆConcept Relevance Propagation, CRPï¼‰å·¥ä½œæµä¾èµ–ä¸“å®¶æ‰‹åŠ¨æ£€æŸ¥å’Œè§£é‡Šã€ç¼ºä¹å¯æ‰©å±•æ€§çš„é—®é¢˜ï¼ŒLLEXICORPå°†CRPä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ä¸ºæ¦‚å¿µåŸå‹åˆ†é…æè¿°æ€§åç§°ï¼Œå¹¶å°†å®šé‡çš„ç›¸å…³æ€§åˆ†å¸ƒè½¬åŒ–ä¸ºç›´è§‚çš„è‡ªç„¶è¯­è¨€å™è¿°ã€‚ä¸ºäº†ç¡®ä¿è§£é‡Šçš„å¿ å®åº¦ï¼Œç ”ç©¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯æ•™å¯¼æ¨¡å‹ç†è§£CRPè¯­ä¹‰ï¼Œå¹¶åˆ†ç¦»äº†å‘½åä¸è§£é‡Šä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„æ–‡æœ¬å¯æ ¹æ®å—ä¼—éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œæ—¢èƒ½ä¸ºä¸“å®¶æä¾›æŠ€æœ¯æè¿°ï¼Œä¹Ÿèƒ½ä¸ºéæŠ€æœ¯äººå‘˜æä¾›é«˜å±‚æ‘˜è¦ã€‚åœ¨ImageNetæ•°æ®é›†å’ŒVGG16æ¨¡å‹ä¸Šçš„å®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„è§£é‡Šé—¨æ§›ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02720v1",
      "published_date": "2025-11-04 16:44:45 UTC",
      "updated_date": "2025-11-04 16:44:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:10:13.188629+00:00"
    },
    {
      "arxiv_id": "2511.02897v1",
      "title": "Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework",
      "title_zh": "çº¿æ€§é—ä¼ è§„åˆ’æ¡†æ¶ä¸­ä½ä¸²è¡¨ç¤ºçš„æ€§èƒ½è¯„ä¼°",
      "authors": [
        "Clyde Meli",
        "Vitezslav Nezval",
        "Zuzana Kominkova Oplatkova",
        "Victor Buttigieg",
        "Anthony Spiteri Staines"
      ],
      "abstract": "Different bitstring representations can yield varying computational performance. This work compares three bitstring implementations in C++: std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their performance is benchmarked in the context of concatenation within a Linear Genetic Programming system. Benchmarks were conducted on three platforms (macOS, Linux, and Windows MSYS2) to assess platform specific performance variations. The results show that the custom direct implementation delivers the fastest performance on Linux and Windows, while std::bitset performs best on macOS. Although consistently slower, boost::dynamic_bitset remains a viable and flexible option. These findings highlight the influence of compiler optimisations and system architecture on performance, providing practical guidance for selecting the optimal method based on platform and application requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨Linear Genetic Programming (LGP) æ¡†æ¶ä¸‹è¯„ä¼°äº†ä¸åŒä½ä¸²è¡¨ç¤ºï¼ˆBitstring Representationsï¼‰çš„è®¡ç®—æ€§èƒ½ã€‚ä½œè€…åœ¨C++ç¯å¢ƒä¸­å¯¹æ¯”äº†ä¸‰ç§å®ç°æ–¹å¼ï¼š`std::bitset`ã€`boost::dynamic_bitset` ä»¥åŠä¸€ç§è‡ªå®šä¹‰çš„ç›´æ¥å®ç°ï¼ˆcustom direct implementationï¼‰ï¼Œé‡ç‚¹æµ‹è¯•äº†å®ƒä»¬åœ¨ä½ä¸²è¿æ¥æ“ä½œä¸­çš„è¡¨ç°ã€‚åŸºå‡†æµ‹è¯•æ¶µç›–äº†macOSã€Linuxå’ŒWindows MSYS2ä¸‰ä¸ªå¹³å°ï¼Œä»¥åˆ†æå¹³å°ç‰¹å®šçš„æ€§èƒ½å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œè‡ªå®šä¹‰ç›´æ¥å®ç°åœ¨Linuxå’ŒWindowså¹³å°ä¸Šé€Ÿåº¦æœ€å¿«ï¼Œè€Œ `std::bitset` åœ¨macOSä¸Šè¡¨ç°æœ€ä½³ã€‚å°½ç®¡ `boost::dynamic_bitset` é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ï¼Œä½†ä»å› å…¶çµæ´»æ€§è¢«è§†ä¸ºå¯è¡Œé€‰é¡¹ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç¼–è¯‘å™¨ä¼˜åŒ–å’Œç³»ç»Ÿæ¶æ„å¯¹æ€§èƒ½çš„æ˜¾è‘—å½±å“ï¼Œä¸ºæ ¹æ®å…·ä½“å¹³å°å’Œåº”ç”¨éœ€æ±‚é€‰æ‹©æœ€ä¼˜ä½ä¸²å®ç°æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02897v1",
      "published_date": "2025-11-04 16:40:19 UTC",
      "updated_date": "2025-11-04 16:40:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:10:31.401204+00:00"
    },
    {
      "arxiv_id": "2511.02717v1",
      "title": "An unscented Kalman filter method for real time input-parameter-state estimation",
      "title_zh": "ç”¨äºå®æ—¶è¾“å…¥-å‚æ•°-çŠ¶æ€ä¼°è®¡çš„æ— è¿¹å¡å°”æ›¼æ»¤æ³¢æ–¹æ³•",
      "authors": [
        "Marios Impraimakis",
        "Andrew W. Smyth"
      ],
      "abstract": "The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå¹¶æ£€éªŒäº†ä¸€ç§æ–°å‹æ— è¿¹å¡å°”æ›¼æ»¤æ³¢(Unscented Kalman Filter)æ–¹æ³•ï¼Œç”¨äºçº¿æ€§å’Œéçº¿æ€§ç³»ç»Ÿçš„å®æ—¶è¾“å…¥-å‚æ•°-çŠ¶æ€è”åˆä¼°è®¡ã€‚è¯¥æ–¹æ³•åœ¨æ¯ä¸ªæ—¶é—´æ­¥å†…åˆ†ä¸¤ä¸ªé˜¶æ®µå¯¹æœªçŸ¥è¾“å…¥è¿›è¡Œä¼°è®¡ï¼šé¦–å…ˆåˆ©ç”¨é¢„æµ‹çš„åŠ¨æ€çŠ¶æ€å’Œç³»ç»Ÿå‚æ•°è¿›è¡Œåˆæ­¥ä¼°è®¡ï¼Œéšåç»“åˆæµ‹é‡ä¿®æ­£åçš„çŠ¶æ€å’Œå‚æ•°å¾—å‡ºæœ€ç»ˆä¼°è®¡ã€‚ç ”ç©¶é€šè¿‡æ‘„åŠ¨åˆ†æ(perturbation analysis)è¯æ˜ï¼Œå…·æœ‰è‡³å°‘ä¸€ä¸ªé›¶æˆ–éé›¶å·²çŸ¥è¾“å…¥çš„ç³»ç»Ÿå…·æœ‰æ½œåœ¨çš„å”¯ä¸€å¯è¯†åˆ«æ€§ã€‚ä½œä¸ºä¸€ç§ä»…è¾“å‡º(output-only)çš„æ–¹æ³•ï¼Œè¯¥æŠ€æœ¯ç›¸æ¯”ä¼ ç»Ÿç­–ç•¥èƒ½æ›´æ·±å…¥åœ°ç†è§£ç³»ç»Ÿï¼Œå› ä¸ºå®ƒå®ç°äº†å¯¹åŠ¨æ€çŠ¶æ€ã€å‚æ•°å’Œè¾“å…¥çš„è”åˆå®æ—¶ä¼°è®¡ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "eess.AS",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "author-accepted manuscript (AAM) published in Mechanical Systems and Signal Processing",
      "pdf_url": "https://arxiv.org/pdf/2511.02717v1",
      "published_date": "2025-11-04 16:39:27 UTC",
      "updated_date": "2025-11-04 16:39:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:14:57.005526+00:00"
    },
    {
      "arxiv_id": "2511.10660v1",
      "title": "Test-Time Steering for Lossless Text Compression via Weighted Product of Experts",
      "title_zh": "åŸºäºåŠ æƒä¸“å®¶ä¹˜ç§¯çš„æ— æŸæ–‡æœ¬å‹ç¼©æµ‹è¯•æ—¶å¼•å¯¼",
      "authors": [
        "Qihang Zhang",
        "Muchen Li",
        "Ziao Wang",
        "Renjie Liao",
        "Lele Wang"
      ],
      "abstract": "Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— æŸæ–‡æœ¬å‹ç¼©é¢†åŸŸä¸­ç¥ç»å‹ç¼©å™¨éš¾ä»¥æ³›åŒ–è‡³æœªè§æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºWeighted Product of Experts (wPoE)è¿›è¡ŒTest-Time Steeringçš„æ–°æ¡†æ¶ã€‚å°½ç®¡ä¼ ç»Ÿé€šç”¨å‹ç¼©å™¨ï¼ˆå¦‚gzipï¼‰é€‚ç”¨æ€§å¹¿ï¼Œä½†å…¶å‹ç¼©ç‡é€šå¸¸ä½äºç°ä»£ç¥ç»å‹ç¼©å™¨ï¼Œè€Œç¥ç»å‹ç¼©å™¨åœ¨é¢å¯¹åˆ†å¸ƒå¤–æ•°æ®æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µè‡ªé€‚åº”åœ°ç»“åˆé€šç”¨å‹ç¼©æ¨¡å‹ä¸é¢„è®­ç»ƒçš„ç¥ç»è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿æœ€ç»ˆå‹ç¼©ç‡è‡³å°‘ä¸ä½äºå…¶ä¸­è¡¨ç°æœ€å¥½çš„å•ä¸€æ¨¡å‹ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ–‡æœ¬å‹ç¼©æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå¯æ— ç¼é›†æˆè‡³ä»»ä½•è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­ï¼Œä¸ºåº”å¯¹å¤šæ ·åŒ–æ•°æ®åˆ†å¸ƒæä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages. Accepted by EMNLP 2025. Code and additional details are available at: https://qihang-zhang.com/Learning-Sys-Blog/2025/10/15/weighted-product-of-experts.html",
      "pdf_url": "https://arxiv.org/pdf/2511.10660v1",
      "published_date": "2025-11-04 16:37:56 UTC",
      "updated_date": "2025-11-04 16:37:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:15:20.634731+00:00"
    },
    {
      "arxiv_id": "2511.05573v1",
      "title": "Video Text Preservation with Synthetic Text-Rich Videos",
      "title_zh": "åŸºäºåˆæˆå¯Œæ–‡æœ¬è§†é¢‘çš„è§†é¢‘æ–‡æœ¬ä¿ç•™",
      "authors": [
        "Ziyang Liu",
        "Kevin Valencia",
        "Justin Cui"
      ],
      "abstract": "While Text-To-Video (T2V) models have advanced rapidly, they continue to struggle with generating legible and coherent text within videos. In particular, existing models often fail to render correctly even short phrases or words and previous attempts to address this problem are computationally expensive and not suitable for video generation. In this work, we investigate a lightweight approach to improve T2V diffusion models using synthetic supervision. We first generate text-rich images using a text-to-image (T2I) diffusion model, then animate them into short videos using a text-agnostic image-to-video (I2v) model. These synthetic video-prompt pairs are used to fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes. Our results show improvement in short-text legibility and temporal consistency with emerging structural priors for longer text. These findings suggest that curated synthetic data and weak supervision offer a practical path toward improving textual fidelity in T2V generation.",
      "tldr_zh": "å°½ç®¡Text-To-Video (T2V)æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆè§†é¢‘å†…çš„æ¸…æ™°è¿è´¯æ–‡æœ¬æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”ç°æœ‰è§£å†³æ–¹æ¡ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨åˆæˆç›‘ç£ä¿¡å·çš„è½»é‡çº§æ–¹æ³•æ¥æ”¹è¿›T2Væ‰©æ•£æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆä½¿ç”¨Text-To-Image (T2I)æ¨¡å‹ç”Ÿæˆå¯Œå«æ–‡æœ¬çš„å›¾åƒï¼Œç„¶ååˆ©ç”¨ä¸æ–‡æœ¬æ— å…³çš„Image-To-Video (I2V)æ¨¡å‹å°†å…¶è½¬åŒ–ä¸ºçŸ­è§†é¢‘ã€‚è¿™äº›åˆæˆçš„è§†é¢‘-æç¤ºè¯å¯¹è¢«ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„T2Væ¨¡å‹Wan2.1ï¼Œä¸”æ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†çŸ­æ–‡æœ¬çš„å¯è¯»æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶å±•ç°å‡ºå¯¹é•¿æ–‡æœ¬çš„ç»“æ„å…ˆéªŒèƒ½åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç²¾å¿ƒç­–åˆ’çš„åˆæˆæ•°æ®å’Œå¼±ç›‘ç£æ˜¯æé«˜T2Vç”Ÿæˆä¸­æ–‡æœ¬ä¿çœŸåº¦çš„åˆ‡å®å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05573v1",
      "published_date": "2025-11-04 16:20:38 UTC",
      "updated_date": "2025-11-04 16:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:15:41.279204+00:00"
    },
    {
      "arxiv_id": "2511.04705v1",
      "title": "POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios",
      "title_zh": "POLIS-Benchï¼šé¢å‘æ”¿åŠ¡åœºæ™¯åŒè¯­æ”¿ç­–ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹å¤šç»´åº¦è¯„ä¼°",
      "authors": [
        "Tingyue Yang",
        "Junchi Yao",
        "Yuhui Guo",
        "Chang Liu"
      ],
      "abstract": "We introduce POLIS-Bench, the first rigorous, systematic evaluation suite designed for LLMs operating in governmental bilingual policy scenarios. Compared to existing benchmarks, POLIS-Bench introduces three major advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive, up-to-date policy corpus that significantly scales the effective assessment sample size, ensuring relevance to current governance practice. (ii) Scenario-Grounded Task Design: We distill three specialized, scenario-grounded tasks -- Clause Retrieval & Interpretation, Solution Generation, and the Compliance Judgmen--to comprehensively probe model understanding and application. (iii) Dual-Metric Evaluation Framework: We establish a novel dual-metric evaluation framework combining semantic similarity with accuracy rate to precisely measure both content alignment and task requirement adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on POLIS-Bench reveals a clear performance hierarchy where reasoning models maintain superior cross-task stability and accuracy, highlighting the difficulty of compliance tasks. Furthermore, leveraging our benchmark, we successfully fine-tune a lightweight open-source model. The resulting POLIS series models achieves parity with, or surpasses, strong proprietary baselines on multiple policy subtasks at a significantly reduced cost, providing a cost-effective and compliant path for robust real-world governmental deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†POLIS-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºæ”¿åºœåŒè¯­æ”¿ç­–åœºæ™¯ä¸­çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è®¾è®¡çš„ä¸¥æ ¼ç³»ç»Ÿè¯„ä¼°å¥—ä»¶ã€‚POLIS-BenchåŒ…å«ä¸‰ä¸ªä¸»è¦æ”¹è¿›ï¼šæ„å»ºäº†å¹¿æ³›ä¸”æœ€æ–°çš„åŒè¯­æ”¿ç­–è¯­æ–™åº“ä»¥ç¡®ä¿ä¸å½“å‰æ²»ç†å®è·µçš„ç›¸å…³æ€§ï¼›è®¾è®¡äº†æ¡æ¬¾æ£€ç´¢ä¸è§£é‡Š(Clause Retrieval & Interpretation)ã€è§£å†³æ–¹æ¡ˆç”Ÿæˆ(Solution Generation)å’Œåˆè§„æ€§åˆ¤æ–­(Compliance Judgment)ä¸‰ä¸ªåŸºäºåœºæ™¯çš„ä¸“ä¸šä»»åŠ¡ï¼›ä»¥åŠå»ºç«‹äº†ç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦ä¸å‡†ç¡®ç‡çš„åŒæŒ‡æ ‡è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡å¯¹è¶…è¿‡10ä¸ªæœ€å…ˆè¿›LLMsçš„è¯„ä¼°å‘ç°ï¼Œæ¨ç†æ¨¡å‹åœ¨è·¨ä»»åŠ¡ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†åˆè§„æ€§ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥åŸºå‡†æˆåŠŸå¾®è°ƒäº†è½»é‡çº§å¼€æºæ¨¡å‹POLISç³»åˆ—ï¼Œè¯¥ç³»åˆ—æ¨¡å‹åœ¨å¤§å¹…é™ä½æˆæœ¬çš„åŒæ—¶ï¼Œåœ¨å¤šä¸ªæ”¿ç­–å­ä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šäº†å¼ºå¤§çš„ä¸“æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºæ”¿åºœåœºæ™¯çš„å®é™…éƒ¨ç½²æä¾›äº†é«˜æ€§ä»·æ¯”ä¸”åˆè§„çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.04705v1",
      "published_date": "2025-11-04 16:11:58 UTC",
      "updated_date": "2025-11-04 16:11:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:16:06.751970+00:00"
    },
    {
      "arxiv_id": "2511.02687v1",
      "title": "The Collaboration Gap",
      "title_zh": "åä½œå·®è·",
      "authors": [
        "Tim R. Davidson",
        "Adam Fourney",
        "Saleema Amershi",
        "Robert West",
        "Eric Horvitz",
        "Ece Kamar"
      ],
      "abstract": "The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a \"collaboration gap\": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a \"relay inference\" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœªæ¥AIç³»ç»Ÿæ—¥ç›Šä¾èµ–å¼‚æ„æ™ºèƒ½ä½“åä½œçš„è¶‹åŠ¿ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºè¿·å®«æ±‚è§£çš„åä½œåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“åä½œèƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å…·å¤‡éš”ç¦»åä½œèƒ½åŠ›ã€è°ƒèŠ‚é—®é¢˜å¤æ‚åº¦ã€æ”¯æŒå¤§è§„æ¨¡è‡ªåŠ¨åŒ–è¯„åˆ†ä»¥åŠæ— è¾“å‡ºæ ¼å¼é™åˆ¶ç­‰ç‰¹ç‚¹ï¼Œç¡®ä¿äº†ç”Ÿæ€åˆç†æ€§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ¡†æ¶è¯„ä¼°äº†32ä¸ªä¸»æµå¼€æºå’Œé—­æºæ¨¡å‹åœ¨å•ä½“ã€åŒæ„åŠå¼‚æ„é…å¯¹ä¸­çš„è¡¨ç°ã€‚ç»“æœæ­ç¤ºäº†ä¸€ä¸ªæ˜¾è‘—çš„â€œåä½œå·®è·â€(Collaboration Gap)ï¼šè®¸å¤šåœ¨å•ä½“ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼Œåœ¨éœ€è¦åä½œæ—¶æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œå°¤å…¶æ˜¯å°å‹è’¸é¦æ¨¡å‹åœ¨ç‰¹å®šé…å¯¹ä¸­å‡ ä¹å®Œå…¨å¤±è´¥ã€‚ç ”ç©¶å‘ç°ç”±æ›´å¼ºçš„æ™ºèƒ½ä½“ç‡å…ˆè¡ŒåŠ¨é€šå¸¸èƒ½æ”¹å–„ç»“æœï¼Œæ®æ­¤æå‡ºçš„â€œæ¥åŠ›æ¨ç†â€(relay inference)æ–¹æ³•â€”â€”å³ç”±å¼ºæ™ºèƒ½ä½“å¼•å¯¼åå†ç§»äº¤ç»™å¼±æ™ºèƒ½ä½“â€”â€”æœ‰æ•ˆç¼©å°äº†è¿™ä¸€å·®è·ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å»ºç«‹åä½œæ„ŸçŸ¥è¯„ä¼°ä½“ç³»ã€å¼€å‘å¢å¼ºåä½œèƒ½åŠ›çš„è®­ç»ƒç­–ç•¥ä»¥åŠä¼˜åŒ–äº¤äº’è®¾è®¡çš„å¿…è¦æ€§ï¼Œä¸ºAI-AIåŠäººæœºåä½œæä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02687v1",
      "published_date": "2025-11-04 16:10:57 UTC",
      "updated_date": "2025-11-04 16:10:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:16:31.943110+00:00"
    },
    {
      "arxiv_id": "2511.02895v2",
      "title": "A Criminology of Machines",
      "title_zh": "æœºå™¨çŠ¯ç½ªå­¦",
      "authors": [
        "Gian Maria Campedelli"
      ],
      "abstract": "While the possibility of reaching human-like Artificial Intelligence (AI) remains controversial, the likelihood that the future will be characterized by a society with a growing presence of autonomous machines is high. Autonomous AI agents are already deployed and active across several industries and digital environments and alongside human-human and human-machine interactions, machine-machine interactions are poised to become increasingly prevalent. Given these developments, I argue that criminology must begin to address the implications of this transition for crime and social control. Drawing on Actor-Network Theory and Woolgar's decades-old call for a sociology of machines -- frameworks that acquire renewed relevance with the rise of generative AI agents -- I contend that criminologists should move beyond conceiving AI solely as a tool. Instead, AI agents should be recognized as entities with agency encompassing computational, social, and legal dimensions. Building on the literature on AI safety, I thus examine the risks associated with the rise of multi-agent AI systems, proposing a dual taxonomy to characterize the channels through which interactions among AI agents may generate deviant, unlawful, or criminal outcomes. I then advance and discuss four key questions that warrant theoretical and empirical attention: (1) Can we assume that machines will simply mimic humans? (2) Will crime theories developed for humans suffice to explain deviant or criminal behaviors emerging from interactions between autonomous AI agents? (3) What types of criminal behaviors will be affected first? (4) How might this unprecedented societal shift impact policing? These questions underscore the urgent need for criminologists to theoretically and empirically engage with the implications of multi-agent AI systems for the study of crime and play a more active role in debates on AI safety and governance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»æœºå™¨æ—¥ç›Šæ™®åŠçš„è¶‹åŠ¿ï¼Œä¸»å¼ çŠ¯ç½ªå­¦å¿…é¡»å¼€å§‹æ¢è®¨è¿™ä¸€è½¬å‹å¯¹çŠ¯ç½ªå’Œç¤¾ä¼šæ§åˆ¶çš„å½±å“ã€‚å€Ÿé‰´Actor-Network Theoryå’ŒWoolgarå…³äºæœºå™¨ç¤¾ä¼šå­¦çš„å‘¼åï¼Œä½œè€…è®¤ä¸ºä¸åº”ä»…å°†AIè§†ä¸ºå·¥å…·ï¼Œè€Œåº”æ‰¿è®¤å…¶å…·å¤‡è®¡ç®—ã€ç¤¾ä¼šå’Œæ³•å¾‹ç»´åº¦çš„ä»£ç†æƒ(agency)ã€‚åŸºäºAI Safetyæ–‡çŒ®ï¼Œæ–‡ç« è€ƒå¯Ÿäº†å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿ(multi-agent AI systems)çš„é£é™©ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒé‡åˆ†ç±»æ³•æ¥æè¿°AIæ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’å¦‚ä½•å¯¼è‡´è¶Šè½¨ã€éæ³•æˆ–çŠ¯ç½ªç»“æœã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†å››ä¸ªå…³é”®é—®é¢˜ï¼Œæ¶‰åŠæœºå™¨æ˜¯å¦ä»…æ¨¡ä»¿äººç±»ã€ç°æœ‰äººç±»çŠ¯ç½ªç†è®ºæ˜¯å¦é€‚ç”¨äºAIäº¤äº’äº§ç”Ÿçš„è¡Œä¸ºã€å“ªäº›çŠ¯ç½ªè¡Œä¸ºæœ€å…ˆå—å½±å“ä»¥åŠå¯¹è­¦åŠ¡å·¥ä½œçš„å†²å‡»ã€‚è¿™ä¸€å·¥ä½œå¼ºè°ƒäº†çŠ¯ç½ªå­¦å®¶åœ¨ç†è®ºå’Œå®è¯ä¸Šä»‹å…¥å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿç ”ç©¶çš„ç´§è¿«æ€§ï¼Œå‘¼åå…¶åœ¨AIæ²»ç†å’Œå®‰å…¨è¾©è®ºä¸­å‘æŒ¥æ›´ç§¯æçš„ä½œç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CY",
      "comment": "This pre-print is also available at CrimRxiv with DOI: https://doi.org/10.21428/cb6ab371.e3354ce1",
      "pdf_url": "https://arxiv.org/pdf/2511.02895v2",
      "published_date": "2025-11-04 16:07:13 UTC",
      "updated_date": "2025-11-06 16:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:16:52.653851+00:00"
    },
    {
      "arxiv_id": "2511.02681v1",
      "title": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes",
      "title_zh": "æœ€ä¼˜å¥‡å¼‚æŸä¼¤ï¼šä½å­˜å‚¨ç¯å¢ƒä¸‹çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Mohammadsajad Alipour",
        "Mohammad Mohammadi Amiri"
      ],
      "abstract": "Large language models (LLMs) are increasingly prevalent across diverse applications. However, their enormous size limits storage and processing capabilities to a few well-resourced stakeholders. As a result, most applications rely on pre-trained LLMs, fine-tuned for specific tasks. However, even storing the fine-tuned versions of these models remains a significant challenge due to the wide range of tasks they address. Recently, studies show that fine-tuning these models primarily affects a small fraction of parameters, highlighting the need for more efficient storage of fine-tuned models. This paper focuses on efficient storage of parameter updates in pre-trained models after fine-tuning. To address this challenge, we leverage the observation that fine-tuning updates are both low-rank and sparse, which can be utilized for storage efficiency. However, using only low-rank approximation or sparsification may discard critical singular components that enhance model expressivity. We first observe that given the same memory budget, sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks. Building on this, we propose our method, optimal singular damage, that selectively sparsifies low-rank approximated updates by leveraging the interleaved importance of singular vectors, ensuring that the most impactful components are retained. We demonstrate through extensive experiments that our proposed methods lead to significant storage efficiency and superior accuracy within the same memory budget compared to employing the low-rank approximation or sparsification individually.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾®è°ƒåå­˜å‚¨éœ€æ±‚å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸“æ³¨äºé«˜æ•ˆå­˜å‚¨é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°æ›´æ–°ã€‚ä½œè€…è§‚å¯Ÿåˆ°å¾®è°ƒæ›´æ–°åŒæ—¶å…·æœ‰ä½ç§©ï¼ˆlow-rankï¼‰å’Œç¨€ç–ï¼ˆsparseï¼‰çš„ç‰¹æ€§ï¼Œä¸”åœ¨ç›¸åŒå†…å­˜é¢„ç®—ä¸‹ï¼Œå…·æœ‰è¾ƒå¤§ç§©çš„ç¨€ç–åŒ–ä½ç§©è¿‘ä¼¼ä¼˜äºæ ‡å‡†ä½ç§©è¿‘ä¼¼ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†Optimal Singular Damageæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¥‡å¼‚å‘é‡ï¼ˆsingular vectorsï¼‰çš„äº¤é”™é‡è¦æ€§ï¼Œæœ‰é€‰æ‹©åœ°ç¨€ç–åŒ–ä½ç§©è¿‘ä¼¼æ›´æ–°ï¼Œç¡®ä¿ä¿ç•™å…³é”®ç»„ä»¶ä»¥å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸å•ç‹¬é‡‡ç”¨ä½ç§©è¿‘ä¼¼æˆ–ç¨€ç–åŒ–ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸åŒçš„å­˜å‚¨é¢„ç®—ä¸‹æ˜¾è‘—æé«˜äº†å­˜å‚¨æ•ˆç‡å¹¶å®ç°äº†æ›´ä¼˜çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02681v1",
      "published_date": "2025-11-04 16:05:25 UTC",
      "updated_date": "2025-11-04 16:05:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:17:21.572521+00:00"
    },
    {
      "arxiv_id": "2511.02667v2",
      "title": "Scalable Evaluation and Neural Models for Compositional Generalization",
      "title_zh": "é¢å‘ç»„åˆæ³›åŒ–çš„å¯æ‰©å±•è¯„ä¼°ä¸ç¥ç»æ¨¡å‹",
      "authors": [
        "Giacomo Camposampiero",
        "Pietro Barbiero",
        "Michael Hersche",
        "Roger Wattenhofer",
        "Abbas Rahimi"
      ],
      "abstract": "Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. Our code is available at https://github.com/IBM/scalable-compositional-generalization.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹æœºå™¨å­¦ä¹ ä¸­çš„ç»„åˆæ³›åŒ–ï¼ˆCompositional Generalizationï¼‰è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè§£å†³äº†ç°æœ‰è¯„ä¼°åè®®ç¼ºä¹æ ‡å‡†åŒ–ä»¥åŠé€šç”¨è§†è§‰æ¶æ„ç¼ºä¹å¿…è¦å½’çº³åç½®çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ä»…ç»Ÿä¸€å¹¶æ‰©å±•äº†å…ˆå‰çš„æ–¹æ³•ï¼Œè¿˜å°†è®¡ç®—éœ€æ±‚ä»ç»„åˆçº§æ˜¾è‘—é™ä½è‡³å¸¸æ•°çº§ã€‚ä½œè€…é€šè¿‡è®­ç»ƒè¶…è¿‡5000ä¸ªæ¨¡å‹ï¼Œå¯¹ç›‘ç£è§†è§‰éª¨å¹²ç½‘ç»œä¸­çš„ç»„åˆæ³›åŒ–ç°çŠ¶è¿›è¡Œäº†å¹¿æ³›ä¸”ç°ä»£åŒ–çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ–‡ç« å¼•å…¥äº†å±æ€§ä¸å˜ç½‘ç»œï¼ˆAttribute Invariant Networksï¼‰ï¼Œè¿™ç±»æ¨¡å‹å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œåœ¨å®ç°æ¯”åŸºçº¿æ¨¡å‹é«˜å‡º23.43%å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†å‚æ•°å¼€é”€ä»å®Œå…¨è§£è€¦å¯¹åº”æ¨¡å‹çš„600%å¤§å¹…é™ä½è‡³16%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS), 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.02667v2",
      "published_date": "2025-11-04 15:45:45 UTC",
      "updated_date": "2025-11-05 12:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:18:02.492191+00:00"
    },
    {
      "arxiv_id": "2511.02659v2",
      "title": "In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization",
      "title_zh": "åŸºäºè‰å›¾æ­£åˆ™åŒ–çš„ç§‘å­¦ä»¿çœŸéšå¼ç¥ç»å‹ç¼©å™¨åŸä½è®­ç»ƒ",
      "authors": [
        "Cooper Simpson",
        "Stephen Becker",
        "Alireza Doostan"
      ],
      "abstract": "Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦æ¨¡æ‹Ÿä¸­çš„éšå¼ç¥ç»å‹ç¼©å™¨(Implicit Neural Compressors)ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‰å›¾æ­£åˆ™åŒ–(Sketch-Based Regularization)çš„æ–°å‹åŸä½(in situ)è®­ç»ƒåè®®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ‰é™çš„å®Œæ•´æ•°æ®å’Œè‰å›¾(sketched)æ•°æ®æ ·æœ¬å†…å­˜ç¼“å†²åŒºï¼Œé€šè¿‡Johnson-Lindenstrausså¼•ç†æä¾›ç†è®ºæ”¯æŒï¼Œåˆ©ç”¨è‰å›¾æ•°æ®æœ‰æ•ˆé˜²æ­¢äº†ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ã€‚å°½ç®¡è¯¥æŠ€æœ¯é€‚ç”¨äºå¹¿æ³›çš„æŒç»­å­¦ä¹ (continual learning)é¢†åŸŸï¼Œä½†æœ¬æ–‡ç‰¹åˆ«èšç„¦äºåŸºäºéšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations)çš„è¶…ç½‘ç»œ(hypernetworks)å‹ç¼©åº”ç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶µç›–éç»“æ„åŒ–ç½‘æ ¼å’Œéç¬›å¡å°”å‡ ä½•å½¢çŠ¶çš„å¤æ‚äºŒç»´åŠä¸‰ç»´æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜å‹ç¼©ç‡ä¸‹å®ç°äº†å¼ºå¤§çš„é‡å»ºæ€§èƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œè‰å›¾æŠ€æœ¯çš„å¼•å…¥ä½¿å¾—è¿™ç§åŸä½è®­ç»ƒæ–¹æ¡ˆèƒ½å¤Ÿè¾¾åˆ°ä¸ç­‰æ•ˆç¦»çº¿æ–¹æ³•è¿‘ä¼¼çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 8 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.02659v2",
      "published_date": "2025-11-04 15:36:00 UTC",
      "updated_date": "2025-11-05 03:20:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:18:33.895183+00:00"
    },
    {
      "arxiv_id": "2511.02651v1",
      "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
      "title_zh": "Apriel-H1ï¼šè¿ˆå‘é«˜æ•ˆçš„ä¼ä¸šçº§æ¨ç†æ¨¡å‹",
      "authors": [
        "Oleksiy Ostapenko",
        "Luke Kumar",
        "Raymond Li",
        "Denis Kocetkov",
        "Joel Lamy-Poirier",
        "Shruthan Radhakrishna",
        "Soham Parikh",
        "Shambhavi Mishra",
        "Sebastien Paquet",
        "Srinivas Sunkara",
        "ValÃ©rie BÃ©caert",
        "Sathwik Tejaswi Madhusudhan",
        "Torsten Scholak"
      ],
      "abstract": "Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Transformeræ¶æ„åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„äºŒæ¬¡æ—¶é—´å¤æ‚åº¦å’ŒKVç¼“å­˜é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†Apriel-H1ï¼Œä¸€ç§ç»“åˆTransformeræ³¨æ„åŠ›æœºåˆ¶ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹(SSMsï¼Œå¦‚Mamba)çš„æ··åˆå¤§è¯­è¨€æ¨¡å‹å®¶æ—ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä»é¢„è®­ç»ƒçš„æ¨ç†æ¨¡å‹Apriel-Nemotron-15B-Thinkerè¿›è¡Œå¢é‡è’¸é¦(incremental distillation)ï¼Œé€æ­¥å°†éå…³é”®çš„æ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºçº¿æ€§çš„Mambaæ¨¡å—ï¼Œä»è€Œæ„å»ºäº†è¿™äº›15Bå‚æ•°é‡çš„æ¨¡å‹ã€‚ä½œè€…å‘å¸ƒäº†å…·æœ‰ä¸åŒSSMä¸MHAæ¯”ä¾‹çš„å¤šä¸ªå˜ä½“ï¼Œå¹¶åˆ†æäº†éšç€Mambaå±‚æ›¿æ¢MHAå±‚ï¼Œæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ¼”å˜æƒ…å†µã€‚å®éªŒæ˜¾ç¤ºï¼Œç»è¿‡æ¨ç†è½¨è¿¹ç›‘ç£æ•°æ®é›†å¾®è°ƒçš„30/50æ··åˆå˜ä½“ï¼Œåœ¨ç”Ÿäº§çº§vLLMç¯å¢ƒä¸­éƒ¨ç½²æ—¶å®ç°äº†è¶…è¿‡2å€çš„æ¨ç†ååé‡æå‡ï¼ŒåŒæ—¶æ¨ç†æ€§èƒ½ä»…æœ‰æå°å¹…åº¦çš„ä¸‹é™ã€‚è¿™ä¸€ç»“æœè¯æ˜äº†è’¸é¦åçš„æ··åˆSSM-Transformeræ¶æ„èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—ç‰ºç‰²æ¨ç†è´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02651v1",
      "published_date": "2025-11-04 15:17:43 UTC",
      "updated_date": "2025-11-04 15:17:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:18:52.964866+00:00"
    },
    {
      "arxiv_id": "2511.02647v1",
      "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks",
      "title_zh": "è”é‚¦æ³¨æ„åŠ›ï¼šè¾¹ç¼˜ç½‘ç»œåä½œå¼ LLM æ¨ç†çš„åˆ†å¸ƒå¼èŒƒå¼",
      "authors": [
        "Xiumei Deng",
        "Zehui Xiong",
        "Binbin Chen",
        "Dong In Kim",
        "Merouane Debbah",
        "H. Vincent Poor"
      ],
      "abstract": "Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Federated Attention (FedAttn)ï¼Œä¸€ç§é’ˆå¯¹è¾¹ç¼˜ç½‘ç»œåä½œLLMæ¨ç†çš„åˆ†å¸ƒå¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éšç§æ³„éœ²ã€é€šä¿¡å¼€é”€å’Œè®¡ç®—ç“¶é¢ˆç­‰æŒ‘æˆ˜ã€‚FedAttnå°†è”é‚¦èŒƒå¼æ•´åˆåˆ°self-attentionæœºåˆ¶ä¸­ï¼Œå…è®¸å‚ä¸è€…åœ¨æœ¬åœ°å¯¹è‡ªå·±çš„tokenè¡¨ç¤ºè¿›è¡Œè®¡ç®—ï¼ŒåŒæ—¶å‘¨æœŸæ€§åœ°äº¤æ¢å’ŒèšåˆKey-Value (KV)çŸ©é˜µï¼Œä»è€Œåœ¨ä¸æš´éœ²ç§æœ‰promptçš„æƒ…å†µä¸‹åä½œç”ŸæˆLLMå“åº”ã€‚ç ”ç©¶å›¢é˜Ÿæ­ç¤ºäº†FedAttnä¸­çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºç»†åŒ–ä¸è”é‚¦å­¦ä¹ (FL)ä¸­çš„å‚æ•°ä¼˜åŒ–ä¹‹é—´å­˜åœ¨ç»“æ„äºŒå…ƒæ€§ï¼Œä¸ºå°†FLä¼˜åŒ–æŠ€æœ¯ç³»ç»Ÿåœ°ç§»æ¤åˆ°åä½œæ¨ç†æä¾›äº†ç†è®ºåŸºç¡€ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œè®ºæ–‡ç†è®ºåˆ†æäº†æœ¬åœ°self-attentionè®¡ç®—å’Œå¼‚æ„tokenç›¸å…³æ€§å¦‚ä½•å½±å“è·¨Transformerå—çš„è¯¯å·®ä¼ æ’­åŠ¨åŠ›å­¦ï¼Œå¹¶åˆ»ç”»äº†å“åº”è´¨é‡ä¸é€šä¿¡/è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æ ¹æœ¬æƒè¡¡ã€‚å®éªŒç»“æœéªŒè¯äº†ç†è®ºåˆ†æï¼Œå¹¶é€šè¿‡sparse attentionå’Œè‡ªé€‚åº”KVèšåˆå±•ç¤ºäº†æ˜¾è‘—çš„ä¼˜åŒ–æœºä¼šï¼Œçªæ˜¾äº†FedAttnåœ¨å®é™…è¾¹ç¼˜éƒ¨ç½²ä¸­å®ç°å¯æ‰©å±•æ€§å’Œé«˜æ•ˆç‡çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02647v1",
      "published_date": "2025-11-04 15:14:58 UTC",
      "updated_date": "2025-11-04 15:14:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:22:01.240590+00:00"
    },
    {
      "arxiv_id": "2511.02646v1",
      "title": "Natural-gas storage modelling by deep reinforcement learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¤©ç„¶æ°”å­˜å‚¨å»ºæ¨¡",
      "authors": [
        "Tiziano Balaconi",
        "Aldo Glielmo",
        "Marco Taboga"
      ],
      "abstract": "We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GasRLï¼Œä¸€ç§å°†å¤©ç„¶æ°”å¸‚åœºæ¨¡å‹ä¸åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (deep reinforcement learning, RL)è®­ç»ƒçš„å­˜å‚¨è¿è¥ç­–ç•¥ç›¸ç»“åˆçš„æ¨¡æ‹Ÿå™¨ã€‚é€šè¿‡è¯¥å·¥å…·ï¼Œä½œè€…åˆ†æäº†æœ€ä¼˜åº“å­˜ç®¡ç†å¯¹å‡è¡¡ä»·æ ¼åŠä¾›éœ€åŠ¨æ€çš„å½±å“ï¼Œå¹¶å‘ç°Soft Actor Critic (SAC)ç®—æ³•åœ¨GasRLç¯å¢ƒä¸­è¡¨ç°æœ€ä½³ï¼Œèƒ½å¤ŸåŒæ—¶å®ç°ç›ˆåˆ©ã€å¸‚åœºå‡ºæ¸…å’Œä»·æ ¼ç¨³å®šç­‰ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSACç­–ç•¥äº§ç”Ÿçš„å‡è¡¡ä»·æ ¼åŠ¨æ€åœ¨æ³¢åŠ¨æ€§å’Œå­£èŠ‚æ€§ä¸Šä¸ç°å®ä¸–ç•Œé«˜åº¦å»åˆï¼Œä¸”æ— éœ€é’ˆå¯¹ä»·æ ¼æ•°æ®è¿›è¡Œæ˜¾å¼æ ¡å‡†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨æ¨¡æ‹Ÿå™¨è¯„ä¼°äº†æ¬§ç›Ÿå¼ºåˆ¶æœ€ä½å­˜å‚¨é˜ˆå€¼(EU-mandated minimum storage thresholds)çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›é˜ˆå€¼å¢å¼ºäº†å¸‚åœºåº”å¯¹ä¾›åº”å†²å‡»åˆ†å¸ƒæ„å¤–å˜åŒ–çš„éŸ§æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢ä¸´å¼‚å¸¸å·¨å¤§çš„å†²å‡»æ—¶ï¼Œèƒ½æœ‰æ•ˆå‡å°‘å¸‚åœºä¸­æ–­çš„å‘ç”Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "econ.GN",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 figures, published on",
      "pdf_url": "https://arxiv.org/pdf/2511.02646v1",
      "published_date": "2025-11-04 15:13:20 UTC",
      "updated_date": "2025-11-04 15:13:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:22:21.476761+00:00"
    },
    {
      "arxiv_id": "2511.02627v1",
      "title": "DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning",
      "title_zh": "DecompSRï¼šç”¨äºç»„åˆå¼å¤šè·³ç©ºé—´æ¨ç†åˆ†è§£åˆ†æçš„æ•°æ®é›†",
      "authors": [
        "Lachlan McPheat",
        "Navdeep Kaur",
        "Robert Blackwell",
        "Alessandra Russo",
        "Anthony G. Cohn",
        "Pranava Madhyastha"
      ],
      "abstract": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark dataset (over 5m datapoints) and generation framework designed to analyse compositional spatial reasoning ability. The generation of DecompSR allows users to independently vary several aspects of compositionality, namely: productivity (reasoning depth), substitutivity (entity and linguistic variability), overgeneralisation (input order, distractors) and systematicity (novel linguistic elements). DecompSR is built procedurally in a manner which makes it is correct by construction, which is independently verified using a symbolic solver to guarantee the correctness of the dataset. DecompSR is comprehensively benchmarked across a host of Large Language Models (LLMs) where we show that LLMs struggle with productive and systematic generalisation in spatial reasoning tasks whereas they are more robust to linguistic variation. DecompSR provides a provably correct and rigorous benchmarking dataset with a novel ability to independently vary the degrees of several key aspects of compositionality, allowing for robust and fine-grained probing of the compositional reasoning abilities of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†DecompSRï¼Œä¸€ä¸ªåŒ…å«è¶…è¿‡500ä¸‡æ•°æ®ç‚¹çš„å¤§å‹åŸºå‡†æ•°æ®é›†å’Œç”Ÿæˆæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåˆ†æç»„åˆç©ºé—´æ¨ç†èƒ½åŠ›(compositional spatial reasoning)ã€‚DecompSRçš„ç”Ÿæˆæœºåˆ¶å…è®¸ç”¨æˆ·ç‹¬ç«‹è°ƒæ•´ç»„åˆæ€§çš„å…³é”®æ–¹é¢ï¼ŒåŒ…æ‹¬productivityï¼ˆæ¨ç†æ·±åº¦ï¼‰ã€substitutivityï¼ˆå®ä½“å’Œè¯­è¨€å˜å¼‚æ€§ï¼‰ã€overgeneralisationï¼ˆè¾“å…¥é¡ºåºå’Œå¹²æ‰°é¡¹ï¼‰ä»¥åŠsystematicityï¼ˆæ–°é¢–è¯­è¨€å…ƒç´ ï¼‰ã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ç¨‹åºåŒ–æ„å»ºæ–¹å¼ï¼Œå¹¶é€šè¿‡symbolic solverè¿›è¡Œç‹¬ç«‹éªŒè¯ï¼Œä»è€Œä»æ„é€ ä¸Šä¿è¯äº†æ•°æ®çš„æ­£ç¡®æ€§ã€‚é’ˆå¯¹å¤šç§Large Language Models (LLMs)çš„å…¨é¢åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡çš„productiveå’Œsystematic generalizationæ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†å¯¹è¯­è¨€å˜å¼‚è¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ã€‚DecompSRæä¾›äº†ä¸€ä¸ªç»è¿‡éªŒè¯ä¸”ä¸¥è°¨çš„åŸºå‡†ï¼Œé€šè¿‡ç‹¬ç«‹æ§åˆ¶ç»„åˆæ€§çš„ä¸åŒç»´åº¦ï¼Œå®ç°äº†å¯¹LLMsç»„åˆæ¨ç†èƒ½åŠ›çš„ç¨³å¥ä¸”ç»†ç²’åº¦çš„æ¢æµ‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02627v1",
      "published_date": "2025-11-04 14:57:11 UTC",
      "updated_date": "2025-11-04 14:57:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:22:47.395683+00:00"
    },
    {
      "arxiv_id": "2511.02606v1",
      "title": "A Multi-Agent Psychological Simulation System for Human Behavior Modeling",
      "title_zh": "é¢å‘äººç±»è¡Œä¸ºå»ºæ¨¡çš„å¤šæ™ºèƒ½ä½“å¿ƒç†æ¨¡æ‹Ÿç³»ç»Ÿ",
      "authors": [
        "Xiangen Hu",
        "Jiarui Tong",
        "Sheng Xu"
      ],
      "abstract": "Training and education in human-centered fields require authentic practice, yet realistic simulations of human behavior have remained limited. We present a multi-agent psychological simulation system that models internal cognitive-affective processes to generate believable human behaviors. In contrast to black-box neural models, this system is grounded in established psychological theories (e.g., self-efficacy, mindset, social constructivism) and explicitly simulates an ``inner parliament'' of agents corresponding to key psychological factors. These agents deliberate and interact to determine the system's output behavior, enabling unprecedented transparency and alignment with human psychology. We describe the system's architecture and theoretical foundations, illustrate its use in teacher training and research, and discuss how it embodies principles of social learning, cognitive apprenticeship, deliberate practice, and meta-cognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºäººç±»è¡Œä¸ºå»ºæ¨¡çš„å¤šæ™ºèƒ½ä½“å¿ƒç†æ¨¡æ‹Ÿç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä»¥äººä¸ºæœ¬é¢†åŸŸä¸­ç¼ºä¹çœŸå®è¡Œä¸ºæ¨¡æ‹Ÿçš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„é»‘ç›’ç¥ç»æ¨¡å‹ä¸åŒï¼Œè¯¥ç³»ç»Ÿå»ºç«‹åœ¨è‡ªæˆ‘æ•ˆèƒ½æ„Ÿ(self-efficacy)ã€æ€ç»´æ¨¡å¼(mindset)å’Œç¤¾ä¼šå»ºæ„ä¸»ä¹‰(social constructivism)ç­‰æˆç†Ÿå¿ƒç†å­¦ç†è®ºåŸºç¡€ä¹‹ä¸Šã€‚ç³»ç»Ÿé€šè¿‡æ˜¾å¼æ¨¡æ‹Ÿä¸€ä¸ªç”±å¯¹åº”å…³é”®å¿ƒç†å› ç´ çš„æ™ºèƒ½ä½“ç»„æˆçš„â€œå†…éƒ¨è®®ä¼š(inner parliament)â€ï¼Œè®©è¿™äº›æ™ºèƒ½ä½“é€šè¿‡å®¡è®®å’Œäº’åŠ¨æ¥å†³å®šç³»ç»Ÿçš„è¾“å‡ºè¡Œä¸ºã€‚è¿™ç§æ¶æ„ä¸ä»…å®ç°äº†å‰æ‰€æœªæœ‰çš„é€æ˜åº¦ï¼Œè¿˜ç¡®ä¿äº†æ¨¡å‹ä¸äººç±»å¿ƒç†æœºåˆ¶çš„é«˜åº¦ä¸€è‡´æ€§ã€‚è®ºæ–‡è¯¦ç»†æè¿°äº†è¯¥ç³»ç»Ÿçš„æ¶æ„ä¸ç†è®ºåŸºç¡€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•™å¸ˆåŸ¹è®­å’Œç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†å…¶å¦‚ä½•ä½“ç°ç¤¾ä¼šå­¦ä¹ (social learning)ã€è®¤çŸ¥å­¦å¾’åˆ¶(cognitive apprenticeship)ã€åˆ»æ„ç»ƒä¹ (deliberate practice)å’Œå…ƒè®¤çŸ¥(meta-cognition)ç­‰åŸåˆ™ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02606v1",
      "published_date": "2025-11-04 14:28:03 UTC",
      "updated_date": "2025-11-04 14:28:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:23:10.665186+00:00"
    },
    {
      "arxiv_id": "2511.02888v1",
      "title": "NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction",
      "title_zh": "NABenchï¼šæ ¸è‹·é…¸åŸºç¡€æ¨¡å‹é€‚åº”åº¦é¢„æµ‹çš„å¤§è§„æ¨¡åŸºå‡†",
      "authors": [
        "Zhongmin Li",
        "Runze Ma",
        "Jiahao Tan",
        "Chengzi Tan",
        "Shuangjia Zheng"
      ],
      "abstract": "Nucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. Here we introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative foundation models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to advance nucleic acid modeling, supporting downstream applications in RNA/DNA design, synthetic biology, and biochemistry. Our code is available at https://github.com/mrzzmrzz/NABench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†NABenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ ¸é…¸é€‚åº”æ€§é¢„æµ‹çš„å¤§è§„æ¨¡ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ ¸è‹·é…¸åŸºç¡€æ¨¡å‹(Nucleotide Foundation Models)å› æ•°æ®é›†å¼‚æ„å’Œé¢„å¤„ç†ä¸ä¸€è‡´è€Œéš¾ä»¥å…¬å¹³æ¯”è¾ƒçš„é—®é¢˜ã€‚NABenchæ±‡é›†äº†162é¡¹é«˜é€šé‡åˆ†æï¼Œæ•´ç†äº†æ¶µç›–å¤šç§DNAå’ŒRNAå®¶æ—çš„260ä¸‡ä¸ªçªå˜åºåˆ—ï¼Œå¹¶æä¾›äº†æ ‡å‡†åŒ–çš„åˆ’åˆ†å’Œä¸°å¯Œçš„å…ƒæ•°æ®ï¼Œåœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œæ•°æ®è´¨é‡æ–¹é¢å‡è¶…è¶Šäº†æ­¤å‰çš„åŸºå‡†ã€‚åœ¨ç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ä¸‹ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹29ä¸ªä»£è¡¨æ€§åŸºç¡€æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ¶µç›–é›¶æ ·æœ¬(zero-shot)ã€å°‘æ ·æœ¬(few-shot)é¢„æµ‹ã€è¿ç§»å­¦ä¹ å’Œç›‘ç£å­¦ä¹ è®¾ç½®ã€‚ç»“æœé‡åŒ–äº†ä¸åŒä»»åŠ¡å’Œæ ¸é…¸ç±»å‹çš„æ€§èƒ½å¼‚è´¨æ€§ï¼Œæ­ç¤ºäº†ä¸åŒå»ºæ¨¡é€‰æ‹©çš„ä¼˜åŠ¿ä¸å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶å»ºç«‹äº†å¼ºæœ‰åŠ›çš„å¯å¤ç°åŸºçº¿ã€‚NABenchçš„å‘å¸ƒå°†æ¨åŠ¨æ ¸é…¸å»ºæ¨¡çš„å‘å±•ï¼Œæ”¯æŒRNA/DNAè®¾è®¡ã€åˆæˆç”Ÿç‰©å­¦å’Œç”Ÿç‰©åŒ–å­¦ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02888v1",
      "published_date": "2025-11-04 14:28:01 UTC",
      "updated_date": "2025-11-04 14:28:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:23:37.152533+00:00"
    },
    {
      "arxiv_id": "2511.02605v1",
      "title": "Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning",
      "title_zh": "å¼ºåŒ–å­¦ä¹ ä¸­é¢å‘æ´»æ€§ä¿æŒå±è”½çš„è‡ªé€‚åº” GR(1) è§„çº¦ä¿®å¤",
      "authors": [
        "Tiberiu-Andrei Georgescu",
        "Alexander W. Goodall",
        "Dalal Alrajeh",
        "Francesco Belardinelli",
        "Sebastian Uchitel"
      ],
      "abstract": "Shielding is widely used to enforce safety in reinforcement learning (RL), ensuring that an agent's actions remain compliant with formal specifications. Classical shielding approaches, however, are often static, in the sense that they assume fixed logical specifications and hand-crafted abstractions. While these static shields provide safety under nominal assumptions, they fail to adapt when environment assumptions are violated. In this paper, we develop the first adaptive shielding framework - to the best of our knowledge - based on Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and expressive fragment of Linear Temporal Logic (LTL) that captures both safety and liveness properties. Our method detects environment assumption violations at runtime and employs Inductive Logic Programming (ILP) to automatically repair GR(1) specifications online, in a systematic and interpretable way. This ensures that the shield evolves gracefully, ensuring liveness is achievable and weakening goals only when necessary. We consider two case studies: Minepump and Atari Seaquest; showing that (i) static symbolic controllers are often severely suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped with our adaptive shield maintain near-optimal reward and perfect logical compliance compared with static shields.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (RL)ä¸­ä¼ ç»Ÿé™æ€å±è”½(Shielding)æœºåˆ¶éš¾ä»¥é€‚åº”ç¯å¢ƒå‡è®¾è¿èƒŒçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºGR(1)è§„èŒƒçš„è‡ªé€‚åº”å±è”½æ¡†æ¶ã€‚GR(1)ä½œä¸ºçº¿æ€§æ—¶åºé€»è¾‘(LTL)çš„ä¸€ä¸ªæ˜“äºå¤„ç†ä¸”è¡¨è¾¾åŠ›å¼ºçš„ç‰‡æ®µï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰å®‰å…¨æ€§å’Œæ´»æ€§å±æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ£€æµ‹ç¯å¢ƒå‡è®¾çš„è¿èƒŒï¼Œå¹¶åˆ©ç”¨å½’çº³é€»è¾‘ç¼–ç¨‹(ILP)ä»¥ç³»ç»Ÿä¸”å¯è§£é‡Šçš„æ–¹å¼åœ¨çº¿è‡ªåŠ¨ä¿®å¤GR(1)è§„èŒƒã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼Œæ¡†æ¶ç¡®ä¿äº†å±è”½ç³»ç»Ÿèƒ½å¤Ÿå¹³æ»‘æ¼”è¿›ï¼Œåœ¨ä¿è¯æ´»æ€§(liveness)çš„åŒæ—¶ä»…åœ¨ç»å¯¹å¿…è¦æ—¶å¼±åŒ–ç›®æ ‡ã€‚åœ¨Minepumpå’ŒAtari Seaquestçš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºé™æ€å±è”½ï¼Œé…å¤‡è¯¥è‡ªé€‚åº”å±è”½çš„RLæ™ºèƒ½ä½“ä¸ä»…ä¿æŒäº†å®Œç¾çš„é€»è¾‘åˆè§„æ€§ï¼Œè¿˜è·å¾—äº†è¿‘ä¹æœ€ä¼˜çš„å¥–åŠ±ï¼Œæœ‰æ•ˆè§£å†³äº†é™æ€ç¬¦å·æ§åˆ¶å™¨åœ¨ä¼˜åŒ–è¾…åŠ©å¥–åŠ±æ—¶çš„ä¸¥é‡æ¬¡ä¼˜é—®é¢˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02605v1",
      "published_date": "2025-11-04 14:27:28 UTC",
      "updated_date": "2025-11-04 14:27:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:24:00.505788+00:00"
    },
    {
      "arxiv_id": "2511.02602v1",
      "title": "Trustworthy Quantum Machine Learning: A Roadmap for Reliability, Robustness, and Security in the NISQ Era",
      "title_zh": "å¯ä¿¡é‡å­æœºå™¨å­¦ä¹ ï¼šNISQ æ—¶ä»£å¯é æ€§ã€é²æ£’æ€§ä¸å®‰å…¨æ€§çš„è·¯çº¿å›¾",
      "authors": [
        "Ferhat Ozgur Catak",
        "Jungwon Seo",
        "Umit Cali"
      ],
      "abstract": "Quantum machine learning (QML) is a promising paradigm for tackling computational problems that challenge classical AI. Yet, the inherent probabilistic behavior of quantum mechanics, device noise in NISQ hardware, and hybrid quantum-classical execution pipelines introduce new risks that prevent reliable deployment of QML in real-world, safety-critical settings. This research offers a broad roadmap for Trustworthy Quantum Machine Learning (TQML), integrating three foundational pillars of reliability: (i) uncertainty quantification for calibrated and risk-aware decision making, (ii) adversarial robustness against classical and quantum-native threat models, and (iii) privacy preservation in distributed and delegated quantum learning scenarios. We formalize quantum-specific trust metrics grounded in quantum information theory, including a variance-based decomposition of predictive uncertainty, trace-distance-bounded robustness, and differential privacy for hybrid learning channels. To demonstrate feasibility on current NISQ devices, we validate a unified trust assessment pipeline on parameterized quantum classifiers, uncovering correlations between uncertainty and prediction risk, an asymmetry in attack vulnerability between classical and quantum state perturbations, and privacy-utility trade-offs driven by shot noise and quantum channel noise. This roadmap seeks to define trustworthiness as a first-class design objective for quantum AI.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹NISQæ—¶ä»£çš„é‡å­æœºå™¨å­¦ä¹ (QML)é¢ä¸´çš„æ¦‚ç‡æ€§è¡Œä¸ºã€è®¾å¤‡å™ªå£°åŠæ··åˆæ‰§è¡Œæµç¨‹å¸¦æ¥çš„é£é™©ï¼Œæå‡ºäº†ä¸€ä»½å¯ä¿¡é‡å­æœºå™¨å­¦ä¹ (TQML)çš„ç»¼åˆè·¯çº¿å›¾ã€‚è¯¥è·¯çº¿å›¾æ•´åˆäº†å¯é æ€§çš„ä¸‰å¤§æ”¯æŸ±ï¼šç”¨äºé£é™©æ„ŸçŸ¥å†³ç­–çš„ä¸ç¡®å®šæ€§é‡åŒ–(uncertainty quantification)ã€å¯¹æŠ—ç»å…¸åŠé‡å­åŸç”Ÿå¨èƒçš„å¯¹æŠ—é²æ£’æ€§(adversarial robustness)ã€ä»¥åŠåˆ†å¸ƒå¼åœºæ™¯ä¸‹çš„éšç§ä¿æŠ¤(privacy preservation)ã€‚ç ”ç©¶åŸºäºé‡å­ä¿¡æ¯ç†è®ºå½¢å¼åŒ–äº†ç‰¹å®šçš„ä¿¡ä»»æŒ‡æ ‡ï¼ŒåŒ…æ‹¬é¢„æµ‹ä¸ç¡®å®šæ€§çš„æ–¹å·®åˆ†è§£ã€åŸºäºè¿¹è·ç¦»çš„é²æ£’æ€§ç•Œé™ä»¥åŠæ··åˆå­¦ä¹ é€šé“çš„å·®åˆ†éšç§(differential privacy)ã€‚ä½œè€…åœ¨å‚æ•°åŒ–é‡å­åˆ†ç±»å™¨ä¸ŠéªŒè¯äº†ç»Ÿä¸€çš„ä¿¡ä»»è¯„ä¼°æµç¨‹ï¼Œè¯æ˜äº†å…¶åœ¨å½“å‰NISQè®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ã€‚å®éªŒæ­ç¤ºäº†ä¸ç¡®å®šæ€§ä¸é¢„æµ‹é£é™©ä¹‹é—´çš„ç›¸å…³æ€§ã€ç»å…¸ä¸é‡å­çŠ¶æ€æ‰°åŠ¨æ”»å‡»è„†å¼±æ€§çš„ä¸å¯¹ç§°æ€§ï¼Œä»¥åŠç”±æ•£ç²’å™ªå£°å’Œé‡å­é€šé“å™ªå£°é©±åŠ¨çš„éšç§-æ•ˆç”¨æƒè¡¡ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å°†å¯ä¿¡åº¦ç¡®ç«‹ä¸ºé‡å­äººå·¥æ™ºèƒ½çš„é¦–è¦è®¾è®¡ç›®æ ‡ï¼Œä¸ºåœ¨å®‰å…¨å…³é”®å‹è®¾ç½®ä¸­éƒ¨ç½²QMLå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "22 Pages",
      "pdf_url": "https://arxiv.org/pdf/2511.02602v1",
      "published_date": "2025-11-04 14:24:17 UTC",
      "updated_date": "2025-11-04 14:24:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:24:26.509245+00:00"
    },
    {
      "arxiv_id": "2511.02600v1",
      "title": "On The Dangers of Poisoned LLMs In Security Automation",
      "title_zh": "è®ºå®‰å…¨è‡ªåŠ¨åŒ–ä¸­å—æŠ•æ¯’ LLM çš„å±å®³",
      "authors": [
        "Patrick Karlsen",
        "Even Eilertsen"
      ],
      "abstract": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the intentional or unintentional introduction of malicious or biased data during model training. We demonstrate how a seemingly improved LLM, fine-tuned on a limited dataset, can introduce significant bias, to the extent that a simple LLM-based alert investigator is completely bypassed when the prompt utilizes the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we demonstrate how a targeted poisoning attack can bias the model to consistently dismiss true positive alerts originating from a specific user. Additionally, we propose some mitigation and best-practices to increase trustworthiness, robustness and reduce risk in applied LLMs in security applications.",
      "tldr_zh": "æœ¬æ–‡è°ƒæŸ¥äº†å®‰å…¨è‡ªåŠ¨åŒ–é¢†åŸŸä¸­\"LLM poisoning\"ï¼ˆå¤§è¯­è¨€æ¨¡å‹ä¸­æ¯’ï¼‰æ‰€å¸¦æ¥çš„é£é™©ï¼Œå³åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ„æˆ–æ— æ„åœ°å¼•å…¥æ¶æ„æˆ–åå·®æ•°æ®ã€‚ç ”ç©¶å±•ç¤ºäº†å³ä½¿æ˜¯çœ‹ä¼¼æ”¹è¿›çš„LLMï¼Œåœ¨æœ‰é™æ•°æ®é›†ä¸Šå¾®è°ƒåä¹Ÿå¯èƒ½å¼•å…¥æ˜¾è‘—åå·®ï¼Œå¯¼è‡´åŸºäºLLMçš„è­¦æŠ¥è°ƒæŸ¥å‘˜åœ¨æç¤ºè¯åˆ©ç”¨è¯¥åå·®æ—¶è¢«å®Œå…¨ç»•è¿‡ã€‚é€šè¿‡ä½¿ç”¨å¾®è°ƒåçš„Llama3.1 8Bå’ŒQwen3 4Bæ¨¡å‹ï¼Œä½œè€…æ¼”ç¤ºäº†ä¸€ç§å®šå‘ä¸­æ¯’æ”»å‡»ï¼Œè¯¥æ”»å‡»èƒ½ä½¿æ¨¡å‹äº§ç”Ÿåå·®ï¼Œä»è€ŒæŒç»­å¿½ç•¥æ¥è‡ªç‰¹å®šç”¨æˆ·çš„çœŸé˜³æ€§ï¼ˆtrue positiveï¼‰è­¦æŠ¥ã€‚æœ€åï¼Œæ–‡ç« æå‡ºäº†ä¸€äº›ç¼“è§£æªæ–½å’Œæœ€ä½³å®è·µï¼Œæ—¨åœ¨æé«˜å®‰å…¨åº”ç”¨ä¸­LLMçš„å¯ä¿¡åº¦å’Œé²æ£’æ€§ï¼Œå¹¶é™ä½ç›¸å…³é£é™©ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.02600v1",
      "published_date": "2025-11-04 14:23:56 UTC",
      "updated_date": "2025-11-04 14:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:25:17.396506+00:00"
    },
    {
      "arxiv_id": "2511.02599v1",
      "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour",
      "title_zh": "ä¸‹ä¸€è¯å…ƒçŸ¥è¯†è¿½è¸ªï¼šåˆ©ç”¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹è¡¨å¾è§£ç å­¦ç”Ÿè¡Œä¸º",
      "authors": [
        "Max Norris",
        "Kobi Gal",
        "Sahan Bulathwela"
      ],
      "abstract": "Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰Knowledge Tracing (KT)æ¨¡å‹å¾€å¾€å¿½ç•¥é—®é¢˜æ–‡æœ¬è¿™ä¸€é‡è¦æ•™å­¦æ´å¯Ÿæ¥æºçš„å±€é™æ€§ï¼Œæå‡ºäº†Next Token Knowledge Tracing (NTKT)æ–¹æ³•ã€‚NTKTåˆ©ç”¨é¢„è®­ç»ƒçš„Large Language Models (LLMs)ï¼Œå°†KTä»»åŠ¡é‡æ–°æ„å»ºä¸ºä¸‹ä¸€ä¸ªtokené¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å°†å­¦ç”Ÿå†å²è®°å½•å’Œé—®é¢˜å†…å®¹éƒ½è¡¨ç¤ºä¸ºæ–‡æœ¬åºåˆ—ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ è¡Œä¸ºå’Œè¯­è¨€æ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒNTKTæœ‰æ•ˆè§£å†³äº†ä»…ä¾èµ–å›ç­”æ­£ç¡®ç‡å’Œå…ƒæ•°æ®ï¼ˆå¦‚æŠ€èƒ½æ ‡ç­¾ï¼‰å¯¼è‡´çš„é¢„æµ‹æ€§èƒ½å—é™é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ç¥ç»KTæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨é¢å¯¹å†·å¯åŠ¨é—®é¢˜å’Œç”¨æˆ·æ—¶è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é—®é¢˜å†…å®¹åœ¨KTä¸­çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜äº†åˆ©ç”¨LLMçš„é¢„è®­ç»ƒè¡¨å¾èƒ½æ›´æœ‰æ•ˆåœ°å¯¹å­¦ç”Ÿå­¦ä¹ è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02599v1",
      "published_date": "2025-11-04 14:20:56 UTC",
      "updated_date": "2025-11-04 14:20:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:25:35.161681+00:00"
    },
    {
      "arxiv_id": "2511.02589v2",
      "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models",
      "title_zh": "ORCA åŸºå‡†ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®ä¸–ç•Œè®¡ç®—å‡†ç¡®æ€§",
      "authors": [
        "Claudia Herambourg",
        "Dawid Siuda",
        "Julia KopczyÅ„ska",
        "Joao R. L. Santos",
        "Wojciech Sas",
        "Joanna ÅšmietaÅ„ska-Nowak"
      ],
      "abstract": "We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$) and calculation mistakes ($33\\,\\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ORCA (Omni Research on Calculation in AI) Benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨Omniè®¡ç®—å¼•æ“éªŒè¯è¾“å‡ºï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šé¢†åŸŸç°å®åœºæ™¯ä¸­å®šé‡æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«é‡‘èã€ç‰©ç†ã€å¥åº·å’Œç»Ÿè®¡ç­‰é¢†åŸŸçš„500ä¸ªè‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œä¸åŒäºæ ‡å‡†æ•°å­¦æ•°æ®é›†ï¼Œå®ƒä¾§é‡äºè¯„ä¼°é€æ­¥æ¨ç†ã€æ•°å€¼ç²¾åº¦å’Œé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚å¯¹ChatGPT-5ã€Gemini 2.5 Flashç­‰äº”ä¸ªæœ€å…ˆè¿›ç³»ç»Ÿçš„æµ‹è¯•æ˜¾ç¤ºï¼Œå…¶å‡†ç¡®ç‡ä»…ä¸º45%è‡³63%ï¼Œä¸»è¦é”™è¯¯æºäºèˆå…¥(35%)å’Œè®¡ç®—é”™è¯¯(33%)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨æ•°å­¦å’Œå·¥ç¨‹é¢†åŸŸè¡¨ç°è¾ƒå¼ºï¼Œä½†åœ¨ç‰©ç†å’Œè‡ªç„¶ç§‘å­¦æ–¹é¢å­˜åœ¨çŸ­æ¿ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹å¸¸åœ¨ç›¸åŒä»»åŠ¡ä¸Šå¤±è´¥ï¼Œä½†é”™è¯¯ç±»å‹å„å¼‚ï¼Œä½“ç°äº†æ¨¡å‹é—´çš„éƒ¨åˆ†äº’è¡¥æ€§è€Œéå•çº¯çš„å†—ä½™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02589v2",
      "published_date": "2025-11-04 14:09:09 UTC",
      "updated_date": "2025-11-05 10:13:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:25:56.907014+00:00"
    },
    {
      "arxiv_id": "2511.02580v1",
      "title": "TAUE: Training-free Noise Transplant and Cultivation Diffusion Model",
      "title_zh": "TAUEï¼šå…è®­ç»ƒå™ªå£°ç§»æ¤ä¸åŸ¹è‚²æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Daichi Nagai",
        "Ryugo Morita",
        "Shunsuke Kitada",
        "Hitoshi Iyatomi"
      ],
      "abstract": "Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šå¸¸ä»…è¾“å‡ºå•å¼ æ‰å¹³å›¾åƒã€éš¾ä»¥æ»¡è¶³ä¸“ä¸šåˆ†å±‚æ§åˆ¶éœ€æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†TAUEï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„é›¶æ ·æœ¬åˆ†å±‚å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæŠ€æœ¯æ˜¯å™ªå£°ç§»æ¤ä¸åŸ¹è‚²ï¼ˆNoise Transplantation and Cultivation, NTCï¼‰ï¼Œé€šè¿‡ä»å‰æ™¯å’Œåˆæˆç”Ÿæˆè¿‡ç¨‹ä¸­æå–ä¸­é—´æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†å…¶ç§»æ¤åˆ°åç»­å±‚çš„åˆå§‹å™ªå£°ä¸­ï¼Œä»è€Œç¡®ä¿å‰æ™¯ã€èƒŒæ™¯å’Œåˆæˆå±‚åœ¨è¯­ä¹‰å’Œç»“æ„ä¸Šçš„è¿è´¯æ€§ã€‚è¿™ç§æ–¹æ³•æ— éœ€å¾®è°ƒæˆ–è¾…åŠ©æ•°æ®é›†å³å¯å®ç°ä¸€è‡´çš„å¤šå±‚è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAUEåœ¨ä¿æŒé«˜å›¾åƒè´¨é‡å’Œä¿çœŸåº¦çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å¾®è°ƒæ–¹æ³•ç›¸å½“çš„å±‚é—´ä¸€è‡´æ€§æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä¸ä»…æ¶ˆé™¤äº†æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ï¼Œè¿˜æ”¯æŒå¤æ‚çš„ç»„åˆç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨ï¼Œä¸ºæ›´æ˜“ç”¨å’Œå¯æ§çš„ç”Ÿæˆå·¥ä½œæµå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 8 figures, 3 tables. The first two authors contributed equally. Project Page: https://iyatomilab.github.io/TAUE",
      "pdf_url": "https://arxiv.org/pdf/2511.02580v1",
      "published_date": "2025-11-04 13:56:39 UTC",
      "updated_date": "2025-11-04 13:56:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:29:06.631798+00:00"
    },
    {
      "arxiv_id": "2511.02887v1",
      "title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets",
      "title_zh": "åŸºäºå¼‚æ„ç¯å¢ƒç©ºé—´æ•°æ®é›†æ·±åº¦å­¦ä¹ èåˆçš„å‘¨åº¦æ¸”ä¸šèšé›†åŒºé¢„æµ‹",
      "authors": [
        "Chaitanya Rele",
        "Aditya Rathod",
        "Kaustubh Natu",
        "Saurabh Kulkarni",
        "Ajay Koli",
        "Swapnali Makdey"
      ],
      "abstract": "The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ—å°åº¦æ´‹åœ°åŒºæ¸”æ°‘éš¾ä»¥ç²¾ç¡®å®šä½é«˜äº§æ¸”åœºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰é›†æˆå¼‚æ„ç¯å¢ƒç©ºé—´æ•°æ®é›†çš„AIè¾…åŠ©æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹æ½œåœ¨æ•é±¼åŒºï¼ˆPotential Fishing Zones, PFZsï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æµ·é¢æ¸©åº¦ï¼ˆsea surface temperatureï¼‰å’Œå¶ç»¿ç´ æµ“åº¦ï¼ˆchlorophyll concentrationï¼‰ç­‰å…³é”®æµ·æ´‹å­¦å‚æ•°ï¼Œæ—¨åœ¨æé«˜PFZè¯†åˆ«çš„å‡†ç¡®æ€§å¹¶æ”¯æŒå¯æŒç»­æ•æã€‚ç ”ç©¶é€šè¿‡æä¾›ç‰¹å®šåŒºåŸŸçš„æ·±å…¥è§è§£ï¼Œè¯•å›¾è§£å†³æ²¿æµ·ç¤¾åŒºé¢ä¸´çš„ç”Ÿè®¡ä¸ç¡®å®šæ€§ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¸”æ°‘å¯»æ‰¾æ¸”åœºçš„æ—¶é—´å’Œç‡ƒæ–™æ¶ˆè€—ï¼ŒåŒæ—¶ä¿ƒè¿›æ¸”ä¸šèµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02887v1",
      "published_date": "2025-11-04 13:48:53 UTC",
      "updated_date": "2025-11-04 13:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:29:28.809955+00:00"
    },
    {
      "arxiv_id": "2511.02886v1",
      "title": "Test-time Adaptation of Tiny Recursive Models",
      "title_zh": "å¾®å‹é€’å½’æ¨¡å‹çš„æµ‹è¯•æ—¶é€‚åº”",
      "authors": [
        "Ronan Killian McGovern"
      ],
      "abstract": "Prior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹2025 ARC Prizeç«èµ›ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTiny Recursive Models (TRM)çš„æµ‹è¯•æ—¶é€‚åº”(Test-time Adaptation)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å…ˆå‰TRMæ–¹æ³•è™½ç„¶è¡¨ç°è‰¯å¥½ä½†è®¡ç®—éœ€æ±‚è¿œè¶…ç«èµ›é™åˆ¶çš„é—®é¢˜ã€‚ä½œè€…é¦–å…ˆåœ¨1,280ä¸ªå…¬å…±ARCä»»åŠ¡ä¸Šåˆ©ç”¨4xH100 GPUè¿›è¡Œäº†48å°æ—¶çš„é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹åœ¨å…¬å…±è¯„ä¼°é›†ä¸Šè¾¾åˆ°çº¦10%çš„å¾—åˆ†ã€‚éšåï¼Œä¸ºäº†é€‚åº”ç«èµ›çš„è®¡ç®—é™åˆ¶ï¼Œè¯¥ç ”ç©¶å¯¹æ¨¡å‹è¿›è¡Œäº†ä»…12,500ä¸ªæ¢¯åº¦æ­¥æ•°çš„å…¨å‚æ•°å¾®è°ƒ(full fine-tuning)ï¼Œè€Œéé‡‡ç”¨LoRAæˆ–ä»…å¾®è°ƒä»»åŠ¡åµŒå…¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§ç­–ç•¥ä½¿æ¨¡å‹åœ¨åŠç§æœ‰è¯„ä¼°ä»»åŠ¡ä¸Šè¾¾åˆ°äº†6.67%çš„å¾—åˆ†ï¼Œè¯æ˜äº†ä»é¢„è®­ç»ƒçš„å¾®å‹é€’å½’æ¨¡å‹å‡ºå‘ï¼Œé€šè¿‡é«˜æ•ˆçš„æµ‹è¯•æ—¶å…¨å‚æ•°å¾®è°ƒå¯ä»¥åœ¨å—é™è®¡ç®—èµ„æºä¸‹å–å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02886v1",
      "published_date": "2025-11-04 13:47:45 UTC",
      "updated_date": "2025-11-04 13:47:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:29:56.138920+00:00"
    },
    {
      "arxiv_id": "2511.02567v1",
      "title": "Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning",
      "title_zh": "é¢å‘ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”é‚»åŸŸçº¦æŸQå­¦ä¹ ",
      "authors": [
        "Yixiu Mao",
        "Yun Qu",
        "Qi Wang",
        "Xiangyang Ji"
      ],
      "abstract": "Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)ä¸­ç”±åˆ†å¸ƒå¤–(OOD)åŠ¨ä½œå¼•èµ·çš„æ¨æ–­è¯¯å·®é—®é¢˜ï¼ŒæŒ‡å‡ºå³ç°æœ‰çš„å¯†åº¦ã€æ”¯æŒåº¦å’Œæ ·æœ¬çº¦æŸå­˜åœ¨è¿‡äºä¿å®ˆæˆ–éš¾ä»¥å‡†ç¡®å»ºæ¨¡è¡Œä¸ºç­–ç•¥çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„é‚»åŸŸçº¦æŸ(neighborhood constraint)ï¼Œå°†Bellmanç›®æ ‡ä¸­çš„åŠ¨ä½œé€‰æ‹©é™åˆ¶åœ¨æ•°æ®é›†åŠ¨ä½œçš„é‚»åŸŸå¹¶é›†å†…ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨ç†è®ºä¸Šç•Œå®šäº†æ¨æ–­è¯¯å·®ï¼Œè¿˜é€šè¿‡ä»¥æ•°æ®è´¨é‡ä¸ºæ ‡å‡†çš„è‡ªé€‚åº”åŠå¾„è®¾è®¡ï¼Œå®ç°äº†æ— éœ€è¡Œä¸ºç­–ç•¥å»ºæ¨¡çš„çµæ´»é€ç‚¹ä¿å®ˆæ€§ã€‚åŸºäºé«˜æ•ˆçš„åŒå±‚ä¼˜åŒ–æ¡†æ¶ï¼Œç ”ç©¶å¼€å‘äº†ANQ (Adaptive Neighborhood-constrained Q learning) ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒANQåœ¨æ ‡å‡†ç¦»çº¿RLåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ï¼Œå¹¶åœ¨å™ªå£°æˆ–æ•°æ®æœ‰é™çš„åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2511.02567v1",
      "published_date": "2025-11-04 13:42:05 UTC",
      "updated_date": "2025-11-04 13:42:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:30:20.842339+00:00"
    },
    {
      "arxiv_id": "2511.02565v1",
      "title": "A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding",
      "title_zh": "å—è®¤çŸ¥è¿‡ç¨‹å¯å‘çš„å—è¯•è€…æ— å…³å¤§è„‘è§†è§‰è§£ç æ¶æ„",
      "authors": [
        "Jingyu Lu",
        "Haonan Wang",
        "Qixiang Zhang",
        "Xiaomeng Li"
      ],
      "abstract": "Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†Visual Cortex Flow Architecture (VCFlow)ï¼Œä¸€ç§å—äººç±»è§†è§‰ç³»ç»Ÿå¯å‘çš„å±‚çº§è§£ç æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨å—è¯•è€…å¤§è„‘è§†è§‰è§£ç (Subject-agnostic brain decoding)ä¸­çš„æ³›åŒ–éš¾é¢˜ã€‚è¯¥æ¨¡å‹æ˜¾å¼æ¨¡æ‹Ÿäº†äººç±»è§†è§‰ç³»ç»Ÿçš„è…¹ä¾§-èƒŒä¾§æ¶æ„(ventral-dorsal architecture)ï¼Œé€šè¿‡è§£è€¦å¹¶åˆ©ç”¨æ—©æœŸè§†è§‰çš®å±‚ã€è…¹ä¾§æµå’ŒèƒŒä¾§æµçš„ç‰¹å¾ï¼Œæ•æ‰è§†è§‰é‡å»ºæ‰€éœ€çš„äº’è¡¥è®¤çŸ¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç‰¹å¾çº§å¯¹æ¯”å­¦ä¹ (feature-level contrastive learning)ç­–ç•¥ï¼Œä»¥å¢å¼ºå—è¯•è€…ä¸å˜è¯­ä¹‰è¡¨ç¤ºçš„æå–ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨æœªè§å—è¯•è€…ä¸Šçš„é€‚ç”¨æ€§ã€‚ä¸é€šå¸¸éœ€è¦å¤§é‡ç‰¹å®šå—è¯•è€…æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVCFlowæ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨10ç§’å†…ç”Ÿæˆé‡å»ºè§†é¢‘ï¼Œè™½ç„¶å¹³å‡å‡†ç¡®ç‡ä»…é™ä½äº†7%ï¼Œä½†æä¾›äº†å¿«é€Ÿä¸”ä¸´åºŠå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages main text with 6 figures (excluding references), supplementary material included",
      "pdf_url": "https://arxiv.org/pdf/2511.02565v1",
      "published_date": "2025-11-04 13:39:34 UTC",
      "updated_date": "2025-11-04 13:39:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:31:15.765942+00:00"
    },
    {
      "arxiv_id": "2511.02560v1",
      "title": "SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration",
      "title_zh": "SigmaCollabï¼šé¢å‘ç‰©ç†æƒ…å¢ƒåä½œçš„åº”ç”¨é©±åŠ¨æ•°æ®é›†",
      "authors": [
        "Dan Bohus",
        "Sean Andrist",
        "Ann Paradiso",
        "Nick Saw",
        "Tim Schoonbeek",
        "Maia Stiber"
      ],
      "abstract": "We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at https://github.com/microsoft/SigmaCollab.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†SigmaCollabï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¿ƒè¿›ç‰©ç†ç¯å¢ƒä¸‹çš„human-AI collaborationç ”ç©¶çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«85ä¸ªä¼šè¯ï¼Œè®°å½•äº†æœªç»è®­ç»ƒçš„å‚ä¸è€…åœ¨mixed-realityè¾…åŠ©AIæ™ºèƒ½ä½“çš„æŒ‡å¯¼ä¸‹ï¼Œåœ¨ç‰©ç†ä¸–ç•Œä¸­æ‰§è¡Œç¨‹åºæ€§ä»»åŠ¡çš„è¿‡ç¨‹ã€‚SigmaCollabæä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®æµï¼ŒåŒ…æ‹¬å‚ä¸è€…å’Œç³»ç»Ÿçš„éŸ³é¢‘ã€å¤´æˆ´å¼è®¾å¤‡çš„ç¬¬ä¸€äººç§°è§†è§’(egocentric)è§†é¢‘ã€æ·±åº¦å›¾ä»¥åŠå¤´éƒ¨ã€æ‰‹éƒ¨å’Œè§†çº¿è¿½è¸ªä¿¡æ¯ï¼Œå¹¶é™„å¸¦äº‹åæ ‡æ³¨ã€‚å°½ç®¡æ•°æ®é›†è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼ˆçº¦14å°æ—¶ï¼‰ï¼Œä½†å…¶åº”ç”¨é©±åŠ¨å’Œäº¤äº’å¼çš„ç‰¹æ€§å‡¸æ˜¾äº†äººæœºåä½œé¢†åŸŸçš„æ–°é¢–ç ”ç©¶æŒ‘æˆ˜ï¼Œä¸ºåœ¨è¯¥é¢†åŸŸè¿è¡Œçš„å„ç§AIæ¨¡å‹æä¾›äº†æ›´çœŸå®çš„æµ‹è¯•å¹³å°ã€‚è¯¥æ•°æ®é›†å·²å¼€æºï¼Œæœªæ¥è®¡åˆ’ç”¨äºæ„å»ºæ··åˆç°å®ä»»åŠ¡è¾…åŠ©åœºæ™¯ä¸‹çš„ç‰©ç†åä½œåŸºå‡†æµ‹è¯•ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02560v1",
      "published_date": "2025-11-04 13:30:15 UTC",
      "updated_date": "2025-11-04 13:30:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:31:06.491605+00:00"
    },
    {
      "arxiv_id": "2511.05571v1",
      "title": "C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling",
      "title_zh": "C3-Diffï¼šåŸºäºè·¨æ¨¡æ€è·¨å†…å®¹å¯¹æ¯”æ‰©æ•£å»ºæ¨¡çš„ç©ºé—´è½¬å½•ç»„è¶…åˆ†è¾¨",
      "authors": [
        "Xiaofei Wang",
        "Stephen Price",
        "Chao Li"
      ],
      "abstract": "The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene expressions, has made it possible to measure gene expression within original tissue, enabling us to discover molecular mechanisms. However, current ST platforms frequently suffer from low resolution, limiting the in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, it remains a challenge to model the interactions between histology images and gene expressions for effective ST enhancement. This study presents a cross-modal cross-content contrastive diffusion framework, called C3-Diff, for ST enhancement with histology images as guidance. In C3-Diff, we firstly analyze the deficiency of traditional contrastive learning paradigm, which is then refined to extract both modal-invariant and content-invariant features of ST maps and histology images. Further, to overcome the problem of low sequencing sensitivity in ST maps, we perform nosing-based information augmentation on the surface of feature unit hypersphere. Finally, we propose a dynamic cross-modal imputation-based training strategy to mitigate ST data scarcity. We tested C3-Diff by benchmarking its performance on four public datasets, where it achieves significant improvements over competing methods. Moreover, we evaluate C3-Diff on downstream tasks of cell type localization, gene expression correlation and single-cell-level gene expression prediction, promoting AI-enhanced biotechnology for biomedical research and clinical applications. Codes are available at https://github.com/XiaofeiWang2018/C3-Diff.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†C3-Diffï¼Œä¸€ç§åŸºäºè·¨æ¨¡æ€è·¨å†…å®¹å¯¹æ¯”æ‰©æ•£å»ºæ¨¡çš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç»„ç»‡å­¦å›¾åƒä½œä¸ºæŒ‡å¯¼æ¥å®ç°ç©ºé—´è½¬å½•ç»„å­¦(ST)çš„è¶…åˆ†è¾¨ç‡å¢å¼ºã€‚é’ˆå¯¹å½“å‰STå¹³å°åˆ†è¾¨ç‡ä½ä¸”éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å›¾åƒä¸åŸºå› è¡¨è¾¾äº¤äº’çš„é—®é¢˜ï¼ŒC3-Diffé¦–å…ˆæ”¹è¿›äº†ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œæå–STå›¾è°±å’Œç»„ç»‡å­¦å›¾åƒä¸­çš„æ¨¡æ€ä¸å˜åŠå†…å®¹ä¸å˜ç‰¹å¾ã€‚ä¸ºäº†å…‹æœSTå›¾è°±ä¸­æµ‹åºçµæ•åº¦ä½çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰¹å¾å•å…ƒè¶…çƒé¢ä¸Šæ‰§è¡Œäº†åŸºäºå™ªå£°çš„ä¿¡æ¯å¢å¼º(nosing-based information augmentation)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŠ¨æ€è·¨æ¨¡æ€æ’è¡¥è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£STæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒC3-Diffæ˜¾è‘—ä¼˜äºç°æœ‰ç«äº‰æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨ç»†èƒç±»å‹å®šä½ã€åŸºå› è¡¨è¾¾ç›¸å…³æ€§å’Œå•ç»†èƒçº§åŸºå› è¡¨è¾¾é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05571v1",
      "published_date": "2025-11-04 13:12:25 UTC",
      "updated_date": "2025-11-04 13:12:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:31:31.251790+00:00"
    },
    {
      "arxiv_id": "2511.02534v1",
      "title": "Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting",
      "title_zh": "é¢å‘å¢é‡æ¸¸æˆæµ‹è¯•çš„çŸ¥è¯†å›¾è°±å¢å¼ºå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Enhong Mu",
        "Jinyu Cai",
        "Yijun Lu",
        "Mingyue Zhang",
        "Kenji Tei",
        "Jialong Li"
      ],
      "abstract": "The rapid iteration and frequent updates of modern video games pose significant challenges to the efficiency and specificity of testing. Although automated playtesting methods based on Large Language Models (LLMs) have shown promise, they often lack structured knowledge accumulation mechanisms, making it difficult to conduct precise and efficient testing tailored for incremental game updates. To address this challenge, this paper proposes a KLPEG framework. The framework constructs and maintains a Knowledge Graph (KG) to systematically model game elements, task dependencies, and causal relationships, enabling knowledge accumulation and reuse across versions. Building on this foundation, the framework utilizes LLMs to parse natural language update logs, identify the scope of impact through multi-hop reasoning on the KG, enabling the generation of update-tailored test cases. Experiments in two representative game environments, Overcooked and Minecraft, demonstrate that KLPEG can more accurately locate functionalities affected by updates and complete tests in fewer steps, significantly improving both playtesting effectiveness and efficiency.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹ç°ä»£è§†é¢‘æ¸¸æˆå¿«é€Ÿè¿­ä»£å¸¦æ¥çš„æµ‹è¯•æ•ˆç‡å’Œé’ˆå¯¹æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†KLPEGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨åŒ–æµ‹è¯•æ–¹æ³•ç¼ºä¹ç»“æ„åŒ–çŸ¥è¯†ç§¯ç´¯æœºåˆ¶çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå’Œç»´æŠ¤çŸ¥è¯†å›¾è°±(Knowledge Graph, KG)ç³»ç»Ÿåœ°å»ºæ¨¡æ¸¸æˆå…ƒç´ ã€ä»»åŠ¡ä¾èµ–å’Œå› æœå…³ç³»ï¼Œå®ç°äº†è·¨ç‰ˆæœ¬çš„çŸ¥è¯†ç§¯ç´¯ä¸å¤ç”¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¡†æ¶åˆ©ç”¨LLMsè§£æè‡ªç„¶è¯­è¨€æ›´æ–°æ—¥å¿—ï¼Œå¹¶é€šè¿‡KGä¸Šçš„å¤šè·³æ¨ç†è¯†åˆ«å½±å“èŒƒå›´ï¼Œä»è€Œç”Ÿæˆé’ˆå¯¹æ›´æ–°çš„å®šåˆ¶åŒ–æµ‹è¯•ç”¨ä¾‹ã€‚åœ¨Overcookedå’ŒMinecraftä¸¤ä¸ªå…¸å‹æ¸¸æˆç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒKLPEGèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½å—æ›´æ–°å½±å“çš„åŠŸèƒ½ï¼Œå¹¶ä»¥æ›´å°‘çš„æ­¥éª¤å®Œæˆæµ‹è¯•ï¼Œæ˜¾è‘—æå‡äº†æ¸¸æˆæµ‹è¯•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02534v1",
      "published_date": "2025-11-04 12:40:46 UTC",
      "updated_date": "2025-11-04 12:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:32:01.518169+00:00"
    },
    {
      "arxiv_id": "2511.02885v1",
      "title": "AgentSLA : Towards a Service Level Agreement for AI Agents",
      "title_zh": "AgentSLAï¼šé¢å‘ AI æ™ºèƒ½ä½“çš„æœåŠ¡ç­‰çº§åè®®",
      "authors": [
        "Gwendal Jouneaux",
        "Jordi Cabot"
      ],
      "abstract": "AI components are increasingly becoming a key element of all types of software systems to enhance their functionality. These AI components are often implemented as AI Agents, offering more autonomy than a plain integration of Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an Agent-as-a-Service one, bringing new challenges to the development of smart software systems. Indeed, while support for the design, implementation, and deployment of those agents exist, the specification of Quality of Service (QoS) and definition of Service Level Agreements (SLAs) aspects for those agents, important to ensure the quality of the resulting systems, remains an open challenge. Part of this is due to the difficulty to clearly define quality in the context of AI components, resulting in a lack of consensus on how to best approach Quality Assurance (QA) for these types of systems. To address this challenge, this paper proposes both a quality model for AI agents based on the ISO/IEC 25010 standard, and a domain specific language to support the definition of SLAs for the services provided by these AI agents.",
      "tldr_zh": "éšç€AIç»„ä»¶ä»Model-as-a-ServiceèŒƒå¼è½¬å‘Agent-as-a-Serviceï¼ŒAI Agentsåœ¨è½¯ä»¶ç³»ç»Ÿä¸­çš„è‡ªä¸»æ€§æ—¥ç›Šå¢å¼ºï¼Œä½†é’ˆå¯¹å…¶æœåŠ¡è´¨é‡(QoS)å’ŒæœåŠ¡æ°´å¹³åè®®(SLA)çš„è§„èŒƒå®šä¹‰ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸»è¦å›°éš¾åœ¨äºéš¾ä»¥æ˜ç¡®ç•Œå®šAIç»„ä»¶çš„è´¨é‡èƒŒæ™¯ï¼Œå¯¼è‡´åœ¨è´¨é‡ä¿è¯(QA)æ–¹æ³•ä¸Šç¼ºä¹å…±è¯†ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œè¯¥è®ºæ–‡æå‡ºäº†AgentSLAï¼Œæ—¨åœ¨ä¸ºAI Agentså»ºç«‹ä¸€å¥—æ ‡å‡†åŒ–çš„æœåŠ¡æ°´å¹³åè®®æ¡†æ¶ã€‚è¯¥ç ”ç©¶é¦–å…ˆåŸºäºISO/IEC 25010æ ‡å‡†æ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹AI Agentsçš„è´¨é‡æ¨¡å‹ï¼Œä»¥æ˜ç¡®å„é¡¹è´¨é‡æŒ‡æ ‡ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§é¢†åŸŸç‰¹å®šè¯­è¨€(DSL)ï¼Œç”¨äºæ”¯æŒå¯¹AI Agentsæä¾›çš„æœåŠ¡è¿›è¡Œå…·ä½“çš„SLAå®šä¹‰ã€‚è¿™ä¸€å·¥ä½œä¸ºæ™ºèƒ½è½¯ä»¶ç³»ç»Ÿçš„è´¨é‡è§„èŒƒä¸ä¿éšœæä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€å’Œå·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02885v1",
      "published_date": "2025-11-04 12:39:35 UTC",
      "updated_date": "2025-11-04 12:39:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:32:26.085751+00:00"
    },
    {
      "arxiv_id": "2511.02532v1",
      "title": "Agentic AI for Mobile Network RAN Management and Optimization",
      "title_zh": "é¢å‘ç§»åŠ¨ç½‘ç»œæ— çº¿æ¥å…¥ç½‘ç®¡ç†ä¸ä¼˜åŒ–çš„ä»£ç†å¼äººå·¥æ™ºèƒ½",
      "authors": [
        "Jorge Pellejero",
        "Luis A. HernÃ¡ndez GÃ³mez",
        "Luis Mendo TomÃ¡s",
        "Zoraida Frias Barroso"
      ],
      "abstract": "Agentic AI represents a new paradigm for automating complex systems by using Large AI Models (LAMs) to provide human-level cognitive abilities with multimodal perception, planning, memory, and reasoning capabilities. This will lead to a new generation of AI systems that autonomously decompose goals, retain context over time, learn continuously, operate across tools and environments, and adapt dynamically. The complexity of 5G and upcoming 6G networks renders manual optimization ineffective, pointing to Agentic AI as a method for automating decisions in dynamic RAN environments. However, despite its rapid advances, there is no established framework outlining the foundational components and operational principles of Agentic AI systems nor a universally accepted definition.\n  This paper contributes to ongoing research on Agentic AI in 5G and 6G networks by outlining its core concepts and then proposing a practical use case that applies Agentic principles to RAN optimization. We first introduce Agentic AI, tracing its evolution from classical agents and discussing the progress from workflows and simple AI agents to Agentic AI. Core design patterns-reflection, planning, tool use, and multi-agent collaboration-are then described to illustrate how intelligent behaviors are orchestrated. These theorical concepts are grounded in the context of mobile networks, with a focus on RAN management and optimization. A practical 5G RAN case study shows how time-series analytics and LAM-driven agents collaborate for KPI-based autonomous decision-making.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†Agentic AIä½œä¸ºä¸€ç§åˆ©ç”¨Large AI Models (LAMs)è‡ªåŠ¨åŒ–å¤æ‚ç³»ç»Ÿçš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³5GåŠæœªæ¥6Gç½‘ç»œå› å¤æ‚æ€§å¯¼è‡´æ‰‹åŠ¨ä¼˜åŒ–å¤±æ•ˆçš„é—®é¢˜ã€‚æ–‡ç« è¯¦ç»†æ¦‚è¿°äº†Agentic AIçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¼”å˜å†ç¨‹ï¼Œå¹¶é‡ç‚¹æè¿°äº†åæ€(reflection)ã€è§„åˆ’(planning)ã€å·¥å…·ä½¿ç”¨(tool use)å’Œå¤šæ™ºèƒ½ä½“åä½œ(multi-agent collaboration)ç­‰æ ¸å¿ƒè®¾è®¡æ¨¡å¼ã€‚ä½œè€…å°†è¿™äº›ç†è®ºåº”ç”¨äºç§»åŠ¨ç½‘ç»œèƒŒæ™¯ï¼Œç‰¹åˆ«æ˜¯æ— çº¿æ¥å…¥ç½‘(RAN)çš„ç®¡ç†ä¸ä¼˜åŒ–ã€‚é€šè¿‡ä¸€ä¸ªå®é™…çš„5G RANæ¡ˆä¾‹ç ”ç©¶ï¼Œè®ºæ–‡å±•ç¤ºäº†æ—¶é—´åºåˆ—åˆ†æä¸LAMé©±åŠ¨çš„æ™ºèƒ½ä½“å¦‚ä½•ååŒå·¥ä½œï¼Œä»¥å®ç°åŸºäºå…³é”®ç»©æ•ˆæŒ‡æ ‡(KPI)çš„è‡ªä¸»å†³ç­–ï¼Œä¸ºåŠ¨æ€ç½‘ç»œç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–ç®¡ç†æä¾›äº†æ–°çš„æ¡†æ¶å’Œæ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02532v1",
      "published_date": "2025-11-04 12:34:57 UTC",
      "updated_date": "2025-11-04 12:34:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:32:50.387569+00:00"
    },
    {
      "arxiv_id": "2511.02531v3",
      "title": "Causal Graph Neural Networks for Healthcare",
      "title_zh": "é¢å‘åŒ»ç–—å¥åº·çš„å› æœå›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Munib Mesinovic",
        "Max Buhlan",
        "Tingting Zhu"
      ],
      "abstract": "Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æ–‡ç« æ¢è®¨äº†å› æœå›¾ç¥ç»ç½‘ç»œ(Causal Graph Neural Networks)åœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸAIç³»ç»Ÿå› ä»…å­¦ä¹ ç»Ÿè®¡å…³è”è€Œé¢ä¸´çš„è·¨æœºæ„éƒ¨ç½²å¤±è´¥ã€æ­§è§†å›ºåŒ–å’Œç¼ºä¹è§£é‡Šæ€§ç­‰é—®é¢˜ã€‚æ–‡ç« è¯¦ç»†å®¡æŸ¥äº†ç»“æ„å› æœæ¨¡å‹(Structural Causal Models)ã€è§£è€¦å› æœè¡¨ç¤ºå­¦ä¹ ä»¥åŠå›¾ä¸Šçš„å¹²é¢„é¢„æµ‹å’Œåäº‹å®æ¨ç†ç­‰æ–¹æ³•è®ºåŸºç¡€ï¼Œæ—¨åœ¨å­¦ä¹ ä¸å˜çš„å› æœæœºåˆ¶è€Œéè™šå‡ç›¸å…³æ€§ã€‚ä½œè€…åˆ†æäº†è¯¥æŠ€æœ¯åœ¨ç²¾ç¥ç—…è„‘ç½‘ç»œåˆ†æã€å¤šç»„å­¦ç™Œç—‡äºšå‹åˆ†ç±»åŠè¯ç‰©æ¨èåå·®æ ¡æ­£ç­‰é¢†åŸŸçš„ä¸´åºŠåº”ç”¨ä»·å€¼ï¼Œå¹¶å±•æœ›äº†ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºæ‚£è€…ç‰¹å¼‚æ€§å› æœæ•°å­—å­ªç”Ÿ(Causal Digital Twins)çš„å‰æ™¯ã€‚å°½ç®¡é¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚å’ŒéªŒè¯å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†åˆ†å±‚æ¡†æ¶æ¥åŒºåˆ†å› æœå¯å‘å¼æ¶æ„ä¸ç»è¿‡ä¸¥æ ¼éªŒè¯çš„å› æœå‘ç°ï¼Œä¸ºåŒºåˆ†å› æœä¸»å¼ ä¸çº¯å…³è”ä¸»å¼ ç¡®ç«‹äº†å…³é”®çš„ç ”ç©¶ä¼˜å…ˆçº§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02531v3",
      "published_date": "2025-11-04 12:34:46 UTC",
      "updated_date": "2025-12-20 09:15:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:36:01.372350+00:00"
    },
    {
      "arxiv_id": "2511.02525v1",
      "title": "An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems",
      "title_zh": "æ±‚è§£å¸¦å®¹é‡çº¦æŸé€‰å€-è·¯å¾„é—®é¢˜çš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Changhao Miao",
        "Yuntian Zhang",
        "Tongyu Wu",
        "Fang Deng",
        "Chen Chen"
      ],
      "abstract": "The capacitated location-routing problems (CLRPs) are classical problems in combinatorial optimization, which require simultaneously making location and routing decisions. In CLRPs, the complex constraints and the intricate relationships between various decisions make the problem challenging to solve. With the emergence of deep reinforcement learning (DRL), it has been extensively applied to address the vehicle routing problem and its variants, while the research related to CLRPs still needs to be explored. In this paper, we propose the DRL with heterogeneous query (DRLHQ) to solve CLRP and open CLRP (OCLRP), respectively. We are the first to propose an end-to-end learning approach for CLRPs, following the encoder-decoder structure. In particular, we reformulate the CLRPs as a markov decision process tailored to various decisions, a general modeling framework that can be adapted to other DRL-based methods. To better handle the interdependency across location and routing decisions, we also introduce a novel heterogeneous querying attention mechanism designed to adapt dynamically to various decision-making stages. Experimental results on both synthetic and benchmark datasets demonstrate superior solution quality and better generalization performance of our proposed approach over representative traditional and DRL-based baselines in solving both CLRP and OCLRP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»„åˆä¼˜åŒ–ä¸­çš„å®¹é‡é™åˆ¶é€‰å€è·¯å¾„é—®é¢˜(CLRPs)ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDRLHQçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³CLRPå’Œå¼€æ”¾å¼CLRP(OCLRP)ã€‚è¿™æ˜¯é¦–ä¸ªéµå¾ªç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„é’ˆå¯¹CLRPsçš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•ã€‚ä½œè€…å°†CLRPsé‡æ–°è¡¨è¿°ä¸ºé’ˆå¯¹ä¸åŒå†³ç­–å®šåˆ¶çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ï¼Œæ„å»ºäº†ä¸€ä¸ªå¯é€‚åº”å…¶ä»–DRLæ–¹æ³•çš„é€šç”¨å»ºæ¨¡æ¡†æ¶ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†é€‰å€å’Œè·¯å¾„å†³ç­–ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¼‚æ„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶(heterogeneous querying attention mechanism)ï¼Œä»¥åŠ¨æ€é€‚åº”ä¸åŒçš„å†³ç­–é˜¶æ®µã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§£çš„è´¨é‡å’Œæ³›åŒ–æ€§èƒ½æ–¹é¢å‡ä¼˜äºä»£è¡¨æ€§çš„ä¼ ç»Ÿæ–¹æ³•å’ŒåŸºäºDRLçš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02525v1",
      "published_date": "2025-11-04 12:23:17 UTC",
      "updated_date": "2025-11-04 12:23:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:36:24.546184+00:00"
    },
    {
      "arxiv_id": "2511.02505v2",
      "title": "ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing",
      "title_zh": "ESAï¼šé¢å‘è‡ªåŠ¨è§†é¢‘ç¼–è¾‘çš„åŸºäºèƒ½é‡çš„é•œå¤´ç»„æ¥ä¼˜åŒ–",
      "authors": [
        "Yaosen Chen",
        "Wei Wang",
        "Tianheng Zheng",
        "Xuming Wen",
        "Han Yang",
        "Yanru Zhang"
      ],
      "abstract": "Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ESAï¼ˆEnergy-Based Shot Assembly Optimizationï¼‰ï¼Œä¸€ç§åŸºäºèƒ½é‡ä¼˜åŒ–çš„é•œå¤´ç»„æ¥æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨è§†é¢‘å‰ªè¾‘æŠ€æœ¯éš¾ä»¥æ•æ‰åˆ›ä½œè€…ç‹¬ç‰¹è‰ºæœ¯è¡¨è¾¾çš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å‰§æœ¬ä¸è§†é¢‘åº“è¿›è¡Œè§†è§‰-è¯­ä¹‰åŒ¹é…ï¼Œè·å–ä¸å‰§æœ¬è¯­ä¹‰å¯¹é½çš„å€™é€‰é•œå¤´å­é›†ã€‚æ¥ç€ï¼Œç ”ç©¶è€…å¯¹å‚è€ƒè§†é¢‘è¿›è¡Œåˆ†å‰²å’Œæ ‡æ³¨ï¼Œæå–æ™¯åˆ«ã€æ‘„åƒæœºè¿åŠ¨å’Œè¯­ä¹‰ç­‰å±æ€§ï¼Œå¹¶åˆ©ç”¨åŸºäºèƒ½é‡çš„æ¨¡å‹(Energy-Based Models)ä»è¿™äº›å±æ€§ä¸­å­¦ä¹ ï¼Œæ ¹æ®ä¸å‚è€ƒé£æ ¼çš„å¯¹é½ç¨‹åº¦å¯¹å€™é€‰é•œå¤´åºåˆ—è¿›è¡Œè¯„åˆ†ã€‚æœ€åï¼Œç»“åˆå¤šç§å¥æ³•è§„åˆ™å®ç°é•œå¤´ç»„æ¥ä¼˜åŒ–ï¼Œç”Ÿæˆç¬¦åˆå‚è€ƒè§†é¢‘ç»„æ¥é£æ ¼çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½æ ¹æ®ç‰¹å®šé€»è¾‘æˆ–å™äº‹è¦æ±‚è‡ªåŠ¨æ’åˆ—ç‹¬ç«‹é•œå¤´ï¼Œè¿˜èƒ½å­¦ä¹ å‚è€ƒè§†é¢‘çš„ç»„æ¥é£æ ¼ï¼Œä½¿æ— è§†é¢‘å‰ªè¾‘ç»éªŒçš„ç”¨æˆ·ä¹Ÿèƒ½åˆ›ä½œå‡ºè§†è§‰æ•ˆæœè¿è´¯ä¸”å¼•äººå…¥èƒœçš„è§†é¢‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02505v2",
      "published_date": "2025-11-04 11:48:22 UTC",
      "updated_date": "2025-11-05 04:30:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:36:47.509044+00:00"
    },
    {
      "arxiv_id": "2511.03749v1",
      "title": "Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland",
      "title_zh": "åº”ç”¨æ—¶é—´åºåˆ—æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹çˆ±å°”å…°å¤šå¹´ç”Ÿé»‘éº¦è‰ç”Ÿé•¿",
      "authors": [
        "Oluwadurotimi Onibonoje",
        "Vuong M. Ngo",
        "Andrew McCarre",
        "Elodie Ruelle",
        "Bernadette O-Briend",
        "Mark Roantree"
      ],
      "abstract": "Grasslands, constituting the world's second-largest terrestrial carbon sink, play a crucial role in biodiversity and the regulation of the carbon cycle. Currently, the Irish dairy sector, a significant economic contributor, grapples with challenges related to profitability and sustainability. Presently, grass growth forecasting relies on impractical mechanistic models. In response, we propose deep learning models tailored for univariate datasets, presenting cost-effective alternatives. Notably, a temporal convolutional network designed for forecasting Perennial Ryegrass growth in Cork exhibits high performance, leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46. Validation across a comprehensive dataset spanning 1,757 weeks over 34 years provides insights into optimal model configurations. This study enhances our understanding of model behavior, thereby improving reliability in grass growth forecasting and contributing to the advancement of sustainable dairy farming practices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çˆ±å°”å…°ä¹³åˆ¶å“è¡Œä¸šé¢ä¸´çš„æŒ‘æˆ˜åŠå½“å‰è‰ç”Ÿé•¿é¢„æµ‹ä¾èµ–ä¸åˆ‡å®é™…æœºæ¢°æ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—é’ˆå¯¹å•å˜é‡æ•°æ®é›†å®šåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä½œè€…é‡ç‚¹åº”ç”¨äº†æ—¶é—´å·ç§¯ç½‘ç»œ(Temporal Convolutional Network)æ¥é¢„æµ‹å¤šå¹´ç”Ÿé»‘éº¦è‰(Perennial Ryegrass)çš„ç”Ÿé•¿ï¼Œæ—¨åœ¨æä¾›ä¸€ç§ç»æµé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç ”ç©¶åˆ©ç”¨é•¿è¾¾34å¹´ã€è·¨è¶Š1,757å‘¨çš„å†å²è‰é«˜åº¦æ•°æ®è¿›è¡Œäº†å…¨é¢çš„æ¨¡å‹éªŒè¯å’Œé…ç½®ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ç§‘å…‹(Cork)åœ°åŒºçš„é¢„æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†2.74çš„å‡æ–¹æ ¹è¯¯å·®(RMSE)å’Œ3.46çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€‚è¿™é¡¹å·¥ä½œåŠ æ·±äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„ç†è§£ï¼Œæé«˜äº†è‰ç”Ÿé•¿é¢„æµ‹çš„å¯é æ€§ï¼Œä»è€Œæœ‰åŠ©äºæ¨åŠ¨å¯æŒç»­ä¹³åˆ¶å“å…»æ®–å®è·µçš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages (two-columns), 7 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.03749v1",
      "published_date": "2025-11-04 11:43:52 UTC",
      "updated_date": "2025-11-04 11:43:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:37:11.428032+00:00"
    },
    {
      "arxiv_id": "2511.02490v1",
      "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring",
      "title_zh": "BRAINSï¼šé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ä¸ç›‘æµ‹çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿ",
      "authors": [
        "Rajan Das Gupta",
        "Md Kishor Morol",
        "Nafiz Fahad",
        "Md Tanzib Hosain",
        "Sumaya Binte Zilani Choya",
        "Md Jakir Hossen"
      ],
      "abstract": "As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BRAINSï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹å’Œç›‘æµ‹çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŒæ¨¡å—æ¶æ„ï¼Œå…¶ä¸­è®¤çŸ¥è¯Šæ–­æ¨¡å—åˆ©ç”¨åœ¨MMSEã€CDRè¯„åˆ†å’Œè„‘å®¹é‡æŒ‡æ ‡ç­‰æ•°æ®é›†ä¸Šå¾®è°ƒçš„LLMså¯¹ADé£é™©è¿›è¡Œç»“æ„åŒ–è¯„ä¼°ã€‚åŒæ—¶ï¼Œç—…ä¾‹æ£€ç´¢æ¨¡å—å°†æ‚£è€…æ¡£æ¡ˆç¼–ç å¹¶ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸ä¼¼ç—…ä¾‹ï¼Œé€šè¿‡ç—…ä¾‹èåˆå±‚ä¸è¾“å…¥æ¡£æ¡ˆç»“åˆä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒBRAINSåœ¨åˆ†ç±»ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œè¯†åˆ«è®¤çŸ¥è¡°é€€æ—©æœŸè¿¹è±¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ä½œä¸ºä¸€ä¸ªå¯æ‰©å±•ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„è¾…åŠ©å·¥å…·ï¼Œè¯¥ç³»ç»Ÿä¸ºè§£å†³åŒ»ç–—èµ„æºå—é™åœ°åŒºçš„æ—©æœŸADç­›æŸ¥éš¾é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in ICMLA 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.02490v1",
      "published_date": "2025-11-04 11:27:03 UTC",
      "updated_date": "2025-11-04 11:27:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:37:36.736465+00:00"
    },
    {
      "arxiv_id": "2511.02478v1",
      "title": "Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation",
      "title_zh": "åŸºäºè§£è€¦æ‰©æ•£å¤šå¸§è¡¥å¿çš„æ— çº¿è§†é¢‘è¯­ä¹‰é€šä¿¡",
      "authors": [
        "Bingyan Xie",
        "Yongpeng Wu",
        "Yuxuan Shi",
        "Biqian Feng",
        "Wenjun Zhang",
        "Jihong Park",
        "Tony Quek"
      ],
      "abstract": "Existing wireless video transmission schemes directly conduct video coding in pixel level, while neglecting the inner semantics contained in videos. In this paper, we propose a wireless video semantic communication framework with decoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D, which integrates the idea of semantic communication into wireless video transmission scenarios. WVSC-D first encodes original video frames as semantic frames and then conducts video coding based on such compact representations, enabling the video coding in semantic level rather than pixel level. Moreover, to further reduce the communication overhead, a reference semantic frame is introduced to substitute motion vectors of each frame in common video coding methods. At the receiver, DDMFC is proposed to generate compensated current semantic frame by a two-stage conditional diffusion process. With both the reference frame transmission and DDMFC frame compensation, the bandwidth efficiency improves with satisfying video transmission performance. Experimental results verify the performance gain of WVSC-D over other DL-based methods e.g. DVSC about 1.8 dB in terms of PSNR.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWVSC-Dçš„æ— çº¿è§†é¢‘è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œç»“åˆäº†è§£è€¦æ‰©æ•£å¤šå¸§è¡¥å¿æŠ€æœ¯(Decoupled Diffusion Multi-frame Compensation, DDMFC)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ¡ˆä»…åœ¨åƒç´ çº§è¿›è¡Œç¼–ç è€Œå¿½è§†è§†é¢‘å†…éƒ¨è¯­ä¹‰çš„é—®é¢˜ã€‚WVSC-Dé¦–å…ˆå°†åŸå§‹è§†é¢‘å¸§ç¼–ç ä¸ºè¯­ä¹‰å¸§ï¼Œä»è€Œå®ç°è¯­ä¹‰çº§è€Œéåƒç´ çº§çš„è§†é¢‘ç¼–ç ï¼Œå¹¶å¼•å…¥å‚è€ƒè¯­ä¹‰å¸§æ¥æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ä¸­çš„è¿åŠ¨å‘é‡(motion vectors)ä»¥é™ä½é€šä¿¡å¼€é”€ã€‚åœ¨æ¥æ”¶ç«¯ï¼ŒDDMFCé€šè¿‡ä¸¤é˜¶æ®µæ¡ä»¶æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆè¡¥å¿åçš„å½“å‰è¯­ä¹‰å¸§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå‚è€ƒå¸§ä¼ è¾“å’Œå¸§è¡¥å¿æŠ€æœ¯ï¼Œåœ¨ä¿è¯è§†é¢‘ä¼ è¾“æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†å¸¦å®½æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWVSC-Dåœ¨PSNRæŒ‡æ ‡ä¸Šæ¯”DVSCç­‰å…¶ä»–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æé«˜äº†çº¦1.8 dBã€‚",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02478v1",
      "published_date": "2025-11-04 11:05:41 UTC",
      "updated_date": "2025-11-04 11:05:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:38:01.425169+00:00"
    },
    {
      "arxiv_id": "2511.02469v1",
      "title": "Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification",
      "title_zh": "é¢å‘è´§å¸æ”¿ç­–å†³ç­–åˆ†ç±»çš„åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„é¹°æ´¾-é¸½æ´¾æ½œåœ¨ä¿¡å¿µå»ºæ¨¡",
      "authors": [
        "Kaito Takano",
        "Masanori Hirano",
        "Kei Nakagawa"
      ],
      "abstract": "Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ç°æœ‰é™æ€æ¨¡å‹å¿½è§†è´§å¸æ”¿ç­–åˆ¶å®šä¸­å®¡è®®è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç»“æ„åŒ–åœ°æ¨¡æ‹Ÿè”é‚¦å…¬å¼€å¸‚åœºå§”å‘˜ä¼šï¼ˆFOMCï¼‰çš„é›†ä½“å†³ç­–è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å°†å¤šä¸ªLLMå»ºæ¨¡ä¸ºå…·æœ‰ä¸åŒåˆå§‹ä¿¡å¿µçš„äº¤äº’æ™ºèƒ½ä½“ï¼Œç»“åˆå®šæ€§æ”¿ç­–æ–‡æœ¬å’Œå®šé‡å®è§‚ç»æµæŒ‡æ ‡ç”Ÿæˆé¢„æµ‹ï¼Œå¹¶é€šè¿‡å¤šè½®è¿­ä»£è¾©è®ºæ¨¡æ‹Ÿå…±è¯†å½¢æˆã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä»£è¡¨æ½œåœ¨ä¿¡å¿µï¼ˆå³Hawkishæˆ–Dovishï¼‰çš„æ½œå˜é‡ï¼Œä»ç†è®ºä¸Šé˜æ˜äº†ä¿¡å¿µå¦‚ä½•è°ƒèŠ‚ä¿¡æ¯æ„ŸçŸ¥ä¸äº¤äº’åŠ¨æ€ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§åŸºäºè¾©è®ºçš„æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„LLMåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ½œåœ¨ä¿¡å¿µï¼Œè¯¥ç ”ç©¶è¿˜æ­ç¤ºäº†ä¸ªä½“è§†è§’å’Œç¤¾ä¼šå½±å“å¦‚ä½•å…±åŒå¡‘é€ é›†ä½“æ”¿ç­–é¢„æµ‹ï¼Œä¸ºç†è§£å¤æ‚çš„å†³ç­–æœºåˆ¶æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "q-fin.CP",
      "comment": "PRIMA2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2511.02469v1",
      "published_date": "2025-11-04 10:56:01 UTC",
      "updated_date": "2025-11-04 10:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:38:48.695519+00:00"
    },
    {
      "arxiv_id": "2511.02463v1",
      "title": "Auditable-choice reframing unlocks RL-based verification for open-ended tasks",
      "title_zh": "å¯å®¡è®¡é€‰æ‹©é‡æ„è§£é”å¼€æ”¾å¼ä»»åŠ¡çš„åŸºäºå¼ºåŒ–å­¦ä¹ éªŒè¯",
      "authors": [
        "Mengyu Zhang",
        "Xubo Liu",
        "Siyu Ding",
        "Weichong Yin",
        "Yu Sun",
        "Hua Wu",
        "Wenya Guo",
        "Ying Zhang"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs), achieving remarkable progress in domains such as mathematics and programming where standard answers are available. However, for open-ended tasks lacking ground-truth solutions (e.g., creative writing and instruction following), existing studies typically regard them as non-reasoning scenarios, thereby overlooking the latent value of reasoning capabilities. This raises a key question: Can strengthening reasoning improve performance in open-ended tasks? To address this, we explore the transfer of the RLVR paradigm to the open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose the existence of standard answers, it cannot be directly applied to open-ended tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation (VMR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across eight open-ended benchmarks, our VMR-based training delivers an average gain of 5.99 points over the baseline. Code will be released upon acceptance to facilitate reproducibility.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹Reinforcement Learning with Verifiable Rewards (RLVR)åœ¨ç¼ºä¹æ ‡å‡†ç­”æ¡ˆçš„å¼€æ”¾å¼ä»»åŠ¡ï¼ˆå¦‚åˆ›æ„å†™ä½œï¼‰ä¸­éš¾ä»¥åº”ç”¨çš„é—®é¢˜ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡å¢å¼ºæ¨ç†èƒ½åŠ›æ¥æå‡æ¨¡å‹è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVerifiable Multiple-Choice Reformulation (VMR)çš„æ–°å‹è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å°†RLVRèŒƒå¼è¿ç§»è‡³å¼€æ”¾é¢†åŸŸã€‚VMRé€šè¿‡å°†å¼€æ”¾å¼æ•°æ®é‡æ„ä¸ºå¯éªŒè¯çš„å¤šé¡¹é€‰æ‹©æ ¼å¼ï¼Œå…‹æœäº†ä¼ ç»ŸéªŒè¯å™¨ä¾èµ–æ ‡å‡†ç­”æ¡ˆçš„é™åˆ¶ï¼Œä»è€Œåœ¨æ— æ˜¾å¼çœŸå€¼çš„æƒ…å†µä¸‹å®ç°äº†æœ‰æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å…«ä¸ªå¼€æ”¾å¼åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºVMRçš„è®­ç»ƒç›¸æ¯”åŸºçº¿æ¨¡å‹å¹³å‡å¸¦æ¥äº†5.99åˆ†çš„æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹æ¨ç†ä¸æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.02463v1",
      "published_date": "2025-11-04 10:45:52 UTC",
      "updated_date": "2025-11-04 10:45:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:39:12.925520+00:00"
    },
    {
      "arxiv_id": "2511.02460v1",
      "title": "SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization",
      "title_zh": "SKGEï¼šåŸºäºå‡ ä½•æ­£åˆ™åŒ–çš„çƒé¢çŸ¥è¯†å›¾è°±åµŒå…¥",
      "authors": [
        "Xuan-Truong Quan",
        "Xuan-Son Quan",
        "Duc Do Minh",
        "Vinh Nguyen Van"
      ],
      "abstract": "Knowledge graph embedding (KGE) has become a fundamental technique for representation learning on multi-relational data. Many seminal models, such as TransE, operate in an unbounded Euclidean space, which presents inherent limitations in modeling complex relations and can lead to inefficient training. In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model that challenges this paradigm by constraining entity representations to a compact manifold: a hypersphere. SKGE employs a learnable, non-linear Spherization Layer to map entities onto the sphere and interprets relations as a hybrid translate-then-project transformation. Through extensive experiments on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate that SKGE consistently and significantly outperforms its strong Euclidean counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237 and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We provide an in-depth analysis to reveal the sources of this advantage, showing that this geometric constraint acts as a powerful regularizer, leading to comprehensive performance gains across all relation types. More fundamentally, we prove that the spherical geometry creates an \"inherently hard negative sampling\" environment, naturally eliminating trivial negatives and forcing the model to learn more robust and semantically coherent representations. Our findings compellingly demonstrate that the choice of manifold is not merely an implementation detail but a fundamental design principle, advocating for geometric priors as a cornerstone for designing the next generation of powerful and stable KGE models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çŸ¥è¯†å›¾è°±åµŒå…¥(KGE)æ¨¡å‹ï¼ˆå¦‚TransEï¼‰åœ¨æ— ç•Œæ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­å»ºæ¨¡å¤æ‚å…³ç³»å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†SKGEï¼ˆSpherical Knowledge Graph Embeddingï¼‰ã€‚SKGEé€šè¿‡åˆ©ç”¨å¯å­¦ä¹ çš„éçº¿æ€§Spherization Layerå°†å®ä½“è¡¨ç¤ºçº¦æŸåœ¨è¶…çƒé¢ä¸Šï¼Œå¹¶å°†å…³ç³»è§£é‡Šä¸ºæ··åˆçš„â€œå…ˆå¹³ç§»åæŠ•å½±â€å˜æ¢ï¼Œä»è€ŒæŒ‘æˆ˜äº†ä¼ ç»ŸèŒƒå¼ã€‚åœ¨FB15k-237ã€CoDEx-Så’ŒCoDEx-Mä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSKGEåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºTransEï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼Œè¿™ç§å‡ ä½•çº¦æŸä½œä¸ºä¸€ç§å¼ºå¤§çš„æ­£åˆ™åŒ–å™¨(Geometric Regularization)ï¼Œèƒ½å¤Ÿå¸¦æ¥è·¨æ‰€æœ‰å…³ç³»ç±»å‹çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜çƒé¢å‡ ä½•ç»“æ„åˆ›é€ äº†ä¸€ç§â€œå†…åœ¨çš„å›°éš¾è´Ÿé‡‡æ ·â€ç¯å¢ƒï¼Œè‡ªç„¶åœ°æ¶ˆé™¤äº†å¹³å‡¡çš„è´Ÿæ ·æœ¬ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’å’Œè¯­ä¹‰è¿è´¯çš„è¡¨ç¤ºã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æµå½¢çš„é€‰æ‹©æ˜¯åŸºæœ¬è®¾è®¡åŸåˆ™ï¼Œä¸»å¼ å°†å‡ ä½•å…ˆéªŒä½œä¸ºè®¾è®¡ä¸‹ä¸€ä»£é«˜æ•ˆç¨³å®šKGEæ¨¡å‹çš„åŸºçŸ³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02460v1",
      "published_date": "2025-11-04 10:40:46 UTC",
      "updated_date": "2025-11-04 10:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:41:25.231918+00:00"
    },
    {
      "arxiv_id": "2511.02426v1",
      "title": "A Kullback-Leibler divergence method for input-system-state identification",
      "title_zh": "ç”¨äºè¾“å…¥-ç³»ç»Ÿ-çŠ¶æ€è¾¨è¯†çš„ Kullback-Leibler æ•£åº¦æ–¹æ³•",
      "authors": [
        "Marios Impraimakis"
      ],
      "abstract": "The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸€ç§åœ¨Kalman filteræ¡†æ¶ä¸‹åˆ©ç”¨Kullback-Leibler divergenceè¿›è¡Œè¾“å…¥-ç³»ç»Ÿ-çŠ¶æ€è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å› åˆå§‹å‚æ•°é›†çŒœæµ‹ä¸åŒè€Œå¯¼è‡´è¯†åˆ«ç»“æœä¸ç¡®å®šçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»å…ˆéªŒåˆ†å¸ƒåˆ°åéªŒåˆ†å¸ƒè½¬æ¢è¿‡ç¨‹ä¸­è·å–çš„æ•°æ®ä¿¡æ¯ï¼Œé¦–å…ˆé’ˆå¯¹å¤šä¸ªä¸åŒçš„åˆå§‹å‚æ•°é›†æ‰§è¡ŒKalman filterä»¥æä¾›ç³»ç»Ÿä¼°è®¡ã€‚éšåï¼Œç ”ç©¶é€šè¿‡Kullback-Leibler divergenceå°†ç”Ÿæˆçš„åéªŒåˆ†å¸ƒä¸åˆå§‹å…ˆéªŒåˆ†å¸ƒè¿›è¡Œå¯¹æ¯”ï¼Œå¹¶é€‰æ‹©Kullback-Leibler divergenceæœ€å°çš„è¯†åˆ«ç»“æœä½œä¸ºæœ€å¯ä¿¡çš„æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çº¿æ€§ã€éçº¿æ€§ä»¥åŠä¿¡æ¯æœ‰é™çš„åº”ç”¨åœºæ™¯ä¸­å‡èƒ½æœ‰æ•ˆç­›é€‰å‡ºè¡¨ç°æ›´ä¼˜çš„è¯†åˆ«ç»“æœï¼Œä¸ºç³»ç»Ÿç›‘æµ‹æä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "32 pages, 17 figures, published in Journal of Sound and Vibration",
      "pdf_url": "https://arxiv.org/pdf/2511.02426v1",
      "published_date": "2025-11-04 09:57:15 UTC",
      "updated_date": "2025-11-04 09:57:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:40:06.310898+00:00"
    },
    {
      "arxiv_id": "2511.02424v1",
      "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning",
      "title_zh": "ReAcTreeï¼šé¢å‘é•¿ç¨‹ä»»åŠ¡è§„åˆ’çš„ç»“åˆæ§åˆ¶æµå±‚çº§åŒ– LLM æ™ºèƒ½ä½“æ ‘",
      "authors": [
        "Jae-Woo Choi",
        "Hyungmin Kim",
        "Hyobin Ong",
        "Minsu Jang",
        "Dohyung Kim",
        "Jaehong Kim",
        "Youngwoo Yoon"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤æ‚é•¿ç¨‹ä»»åŠ¡æ—¶ä¾èµ–å•ä¸€è½¨è¿¹çš„å±€é™æ€§ï¼Œæå‡ºäº†ReAcTreeï¼Œä¸€ç§å¸¦æœ‰æ§åˆ¶æµçš„å±‚çº§åŒ–ä»»åŠ¡è§„åˆ’æ–¹æ³•ã€‚ReAcTreeé€šè¿‡åŠ¨æ€æ„å»ºçš„æ™ºèƒ½ä½“æ ‘å°†å¤æ‚ç›®æ ‡åˆ†è§£ä¸ºæ›´æ˜“ç®¡ç†çš„å­ç›®æ ‡ï¼Œå…¶ä¸­LLM agent nodeè´Ÿè´£æ¨ç†ã€è¡ŒåŠ¨åŠæ‰©å±•æ ‘ç»“æ„ï¼Œè€Œcontrol flow nodesåˆ™è´Ÿè´£åè°ƒæ‰§è¡Œç­–ç•¥ã€‚è¯¥æ¡†æ¶è¿˜é›†æˆäº†ä¸¤ç§äº’è¡¥çš„è®°å¿†ç³»ç»Ÿï¼Œåˆ©ç”¨episodic memoryæ£€ç´¢ç‰¹å®šå­ç›®æ ‡çš„ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡working memoryå…±äº«ç¯å¢ƒè§‚æµ‹ä¿¡æ¯ã€‚åœ¨WAH-NLå’ŒALFREDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReAcTreeåœ¨å¤šç§LLMä¸Šå‡æ˜¾è‘—ä¼˜äºReActç­‰å¼ºåŠ›åŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨WAH-NLæ•°æ®é›†ä¸Šï¼Œæ­è½½Qwen 2.5 72Bçš„ReAcTreeå®ç°äº†61%çš„ç›®æ ‡æˆåŠŸç‡ï¼Œå‡ ä¹æ˜¯ReActæ¨¡å‹31%æˆåŠŸç‡çš„ä¸¤å€ï¼Œå±•ç¤ºäº†å…¶åœ¨å…·èº«æ™ºèƒ½ä½“å†³ç­–ä¸è§„åˆ’æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02424v1",
      "published_date": "2025-11-04 09:55:40 UTC",
      "updated_date": "2025-11-04 09:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:44:53.087552+00:00"
    },
    {
      "arxiv_id": "2511.02414v1",
      "title": "A New Perspective on Precision and Recall for Generative Models",
      "title_zh": "ç”Ÿæˆæ¨¡å‹ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„æ–°è§†è§’",
      "authors": [
        "Benjamin Sykes",
        "LoÃ¯c Simon",
        "Julien Rabin",
        "Jalal Fadili"
      ],
      "abstract": "With the recent success of generative models in image and text, the question of their evaluation has recently gained a lot of attention. While most methods from the state of the art rely on scalar metrics, the introduction of Precision and Recall (PR) for generative model has opened up a new avenue of research. The associated PR curve allows for a richer analysis, but their estimation poses several challenges. In this paper, we present a new framework for estimating entire PR curves based on a binary classification standpoint. We conduct a thorough statistical analysis of the proposed estimates. As a byproduct, we obtain a minimax upper bound on the PR estimation risk. We also show that our framework extends several landmark PR metrics of the literature which by design are restrained to the extreme values of the curve. Finally, we study the different behaviors of the curves obtained experimentally in various settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹ï¼ˆGenerative Modelsï¼‰çš„è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäºŒåˆ†ç±»è§†è§’çš„å…¨æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä¼°è®¡å®Œæ•´çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼ˆPrecision and Recall, PRï¼‰æ›²çº¿ã€‚å°½ç®¡PRæ›²çº¿èƒ½æä¾›æ¯”ä¼ ç»Ÿæ ‡é‡æŒ‡æ ‡æ›´ä¸°å¯Œçš„åˆ†æï¼Œä½†å…¶å‡†ç¡®ä¼°è®¡ä¸€ç›´é¢ä¸´æŒ‘æˆ˜ã€‚ä½œè€…å¯¹æå‡ºçš„ä¼°è®¡æ–¹æ³•è¿›è¡Œäº†è¯¦å°½çš„ç»Ÿè®¡åˆ†æï¼Œå¹¶æ¨å¯¼å‡ºäº†PRä¼°è®¡é£é™©çš„æå°æå¤§ä¸Šç•Œï¼ˆminimax upper boundï¼‰ã€‚è¯¥æ¡†æ¶ä¸ä»…æ‰©å±•äº†æ–‡çŒ®ä¸­é€šå¸¸å±€é™äºæ›²çº¿æå€¼çš„æ ‡å¿—æ€§PRæŒ‡æ ‡ï¼Œè¿˜å…è®¸å¯¹æ•´ä¸ªæ›²çº¿è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚å®éªŒç»“æœå±•ç¤ºäº†åœ¨ä¸åŒè®¾ç½®ä¸‹è¯¥æ›²çº¿è¡¨ç°å‡ºçš„å¤šæ ·åŒ–è¡Œä¸ºï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½åˆ†ææä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02414v1",
      "published_date": "2025-11-04 09:44:11 UTC",
      "updated_date": "2025-11-04 09:44:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:44:54.663428+00:00"
    },
    {
      "arxiv_id": "2511.02404v1",
      "title": "Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs",
      "title_zh": "å—æ‰°ä½†ç¨³å®šï¼šè·¨CNNã€ViTä¸è‡ªç›‘ç£ViTçš„äººçŒ«ä¸å˜è¡¨å¾",
      "authors": [
        "Arya Shah",
        "Vaibhav Tripathi"
      ],
      "abstract": "Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF $\\approx0.814$, mean CKA-linear $\\approx0.745$, mean RSA $\\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B/16 RSA $\\approx0.53$ at block8; ViT-L/16 $\\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŒ«ä¸äººç±»åœ¨çœ¼éƒ¨è§£å‰–ç»“æ„ä¸Šçš„å·®å¼‚ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€åŸºå‡†æ¥é‡åŒ–è·¨ç‰©ç§çš„è§†è§‰è¡¨å¾å¯¹é½æƒ…å†µã€‚ç ”ç©¶æ¶µç›–äº†å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ã€ç›‘ç£å¼Vision Transformers (ViTs)ã€çª—å£åŒ–Transformersä»¥åŠè‡ªç›‘ç£ViTs (DINO)ï¼Œå¹¶åˆ©ç”¨å±‚çº§ä¸­å¿ƒæ ¸å¯¹é½(CKA)å’Œè¡¨å¾ç›¸ä¼¼æ€§åˆ†æ(RSA)è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINO ViT-B/16å®ç°äº†æœ€æ˜¾è‘—çš„å¯¹é½æ•ˆæœ(å¹³å‡CKA-RBFçº¦0.814)ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé˜¶æ®µï¼Œè¿™è¡¨æ˜Tokençº§è‡ªç›‘ç£æœ‰åŠ©äºæ„å»ºè¿æ¥ç‰©ç§ç‰¹å¼‚æ€§ç»Ÿè®¡æ•°æ®çš„ç‰¹å¾ã€‚è™½ç„¶ç›‘ç£å¼ViTsåœ¨CKAæŒ‡æ ‡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨å‡ ä½•å¯¹åº”å…³ç³»ä¸Šå¼±äºDINOï¼Œè€ŒCNNså’Œçª—å£åŒ–Transformersçš„è¡¨ç°æ€»ä½“ä¸åŠæ™®é€šViTsã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†è‡ªç›‘ç£ç»“åˆViTçš„å½’çº³åç½®(inductive biases)èƒ½äº§ç”Ÿæ¯”ä¼ ç»ŸCNNsæ›´ç´§å¯†å¯¹é½çŒ«ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„è¡¨å¾å‡ ä½•ï¼Œä¸ºç†è§£è·¨ç‰©ç§è§†è§‰è®¡ç®—æä¾›äº†æ–°çš„ç¥ç»ç§‘å­¦å‡è®¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02404v1",
      "published_date": "2025-11-04 09:35:42 UTC",
      "updated_date": "2025-11-04 09:35:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:45:57.935334+00:00"
    },
    {
      "arxiv_id": "2511.02400v1",
      "title": "MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization",
      "title_zh": "MammoCleanï¼šé€šè¿‡æ•°æ®é›†åè°ƒè¿ˆå‘å¯å¤ç°ä¸åå·®æ„ŸçŸ¥çš„ä¹³è…ºXçº¿æ‘„å½±äººå·¥æ™ºèƒ½",
      "authors": [
        "Yalda Zafari",
        "Hongyi Pan",
        "Gorkem Durak",
        "Ulas Bagci",
        "Essam A. Rashed",
        "Mohamed Mabrok"
      ],
      "abstract": "The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: https://github.com/Minds-R-Lab/MammoClean.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MammoCleanï¼Œä¸€ä¸ªç”¨äºä¹³æˆ¿Xå…‰æ£€æŸ¥æ•°æ®é›†æ ‡å‡†åŒ–å’Œåå·®é‡åŒ–çš„å…¬å…±æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºæ•°æ®è´¨é‡ã€å…ƒæ•°æ®æ ‡å‡†å’Œäººå£åˆ†å¸ƒå¼‚è´¨æ€§å¯¼è‡´çš„AIæ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚MammoCleané€šè¿‡æ ‡å‡†åŒ–ç—…ä¾‹é€‰æ‹©ã€å›¾åƒå¤„ç†ï¼ˆå«ä¾§å‘å’Œå¼ºåº¦æ ¡æ­£ï¼‰ä»¥åŠå°†å…ƒæ•°æ®ç»Ÿä¸€ä¸ºä¸€è‡´çš„å¤šè§†å›¾ç»“æ„ï¼Œç³»ç»Ÿåœ°è¯†åˆ«å¹¶å¤„ç†äº†åå·®æ¥æºã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¡†æ¶åº”ç”¨äºCBIS-DDSMã€TOMPEI-CMMDå’ŒVinDr-Mammoä¸‰ä¸ªå¼‚æ„æ•°æ®é›†ï¼Œé‡åŒ–äº†ä¹³è…ºå¯†åº¦å’Œå¼‚å¸¸æ‚£ç—…ç‡æ–¹é¢çš„æ˜¾è‘—åˆ†å¸ƒåç§»ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å—æŸæ•°æ®é›†ä¸Šè®­ç»ƒçš„AIæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œåˆ©ç”¨MammoCleanè¯†åˆ«å’Œå‡è½»åå·®æºæœ‰åŠ©äºæ„å»ºç»Ÿä¸€çš„å¤šæ•°æ®é›†è®­ç»ƒè¯­æ–™åº“ã€‚è¯¥æ¡†æ¶ä¸ºå¼€å‘å…·æœ‰ä¼˜è¶Šè·¨åŸŸæ³›åŒ–èƒ½åŠ›çš„é²æ£’æ¨¡å‹æä¾›äº†å¯å¤ç°çš„ç®¡é“ï¼Œæ¨åŠ¨äº†æ›´å…¬å¹³ã€å®‰å…¨ä¸”æœ‰æ•ˆçš„ä¸´åºŠAIç³»ç»Ÿçš„æ„å»ºã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02400v1",
      "published_date": "2025-11-04 09:29:46 UTC",
      "updated_date": "2025-11-04 09:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:45:45.256156+00:00"
    },
    {
      "arxiv_id": "2511.02399v1",
      "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents",
      "title_zh": "EvoDevï¼šåŸºäº LLM æ™ºèƒ½ä½“çš„ç«¯åˆ°ç«¯è½¯ä»¶å¼€å‘è¿­ä»£å¼ç‰¹æ€§é©±åŠ¨æ¡†æ¶",
      "authors": [
        "Junwei Liu",
        "Chen Xu",
        "Chong Wang",
        "Tong Bai",
        "Weitong Chen",
        "Kaseng Wong",
        "Yiling Lou",
        "Xin Peng"
      ],
      "abstract": "Recent advances in large language model agents offer the promise of automating end-to-end software development from natural language requirements. However, existing approaches largely adopt linear, waterfall-style pipelines, which oversimplify the iterative nature of real-world development and struggle with complex, large-scale projects. To address these limitations, we propose EvoDev, an iterative software development framework inspired by feature-driven development. EvoDev decomposes user requirements into a set of user-valued features and constructs a Feature Map, a directed acyclic graph that explicitly models dependencies between features. Each node in the feature map maintains multi-level information, including business logic, design, and code, which is propagated along dependencies to provide context for subsequent development iterations. We evaluate EvoDev on challenging Android development tasks and show that it outperforms the best-performing baseline, Claude Code, by a substantial margin of 56.8%, while improving single-agent performance by 16.0%-76.6% across different base LLMs, highlighting the importance of dependency modeling, context propagation, and workflow-aware agent design for complex software projects. Our work summarizes practical insights for designing iterative, LLM-driven development frameworks and informs future training of base LLMs to better support iterative software development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨ç«¯åˆ°ç«¯è½¯ä»¶å¼€å‘ä¸­é€šå¸¸é‡‡ç”¨çº¿æ€§ç€‘å¸ƒæµæ¨¡å¼çš„å±€é™æ€§ï¼Œæå‡ºäº†EvoDevï¼Œä¸€ç§å—ç‰¹å¾é©±åŠ¨å¼€å‘å¯å‘çš„è¿­ä»£å¼æ¡†æ¶ã€‚EvoDevå°†ç”¨æˆ·éœ€æ±‚åˆ†è§£ä¸ºä¸€ç³»åˆ—å…·æœ‰ç”¨æˆ·ä»·å€¼çš„ç‰¹å¾ï¼Œå¹¶æ„å»ºFeature Mapï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰æ¥æ˜¾å¼å»ºæ¨¡ç‰¹å¾é—´çš„ä¾èµ–å…³ç³»ã€‚Feature Mapä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ç»´æŠ¤åŒ…æ‹¬ä¸šåŠ¡é€»è¾‘ã€è®¾è®¡å’Œä»£ç åœ¨å†…çš„å¤šå±‚çº§ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯æ²¿ä¾èµ–è·¯å¾„ä¼ æ’­ï¼Œä»è€Œä¸ºåç»­çš„å¼€å‘è¿­ä»£æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚åœ¨å¤æ‚çš„Androidå¼€å‘ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒEvoDevçš„è¡¨ç°å¤§å¹…ä¼˜äºå½“å‰æœ€ä½³åŸºçº¿Claude Codeï¼Œæå‡å¹…åº¦è¾¾56.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒçš„åŸºç¡€LLMä¸Šå°†å•æ™ºèƒ½ä½“æ€§èƒ½æå‡äº†16.0%è‡³76.6%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¤„ç†å¤æ‚è½¯ä»¶é¡¹ç›®çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä¾èµ–å»ºæ¨¡ã€ä¸Šä¸‹æ–‡ä¼ æ’­å’Œå·¥ä½œæµæ„ŸçŸ¥è®¾è®¡çš„é‡è¦æ€§ï¼Œä¸ºè®¾è®¡è¿­ä»£å¼LLMé©±åŠ¨å¼€å‘æ¡†æ¶æä¾›äº†å®è´µçš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02399v1",
      "published_date": "2025-11-04 09:27:01 UTC",
      "updated_date": "2025-11-04 09:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:46:20.952318+00:00"
    },
    {
      "arxiv_id": "2511.02392v1",
      "title": "Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients",
      "title_zh": "åŸºäºæ¨¡ç³Šè½¯é›†ç†è®ºçš„ä¹³è…ºç™Œæ‚£è€…é£é™©è¯„ä¼°ä¸“å®¶ç³»ç»Ÿ",
      "authors": [
        "Muhammad Sheharyar Liaqat"
      ],
      "abstract": "Breast cancer remains one of the leading causes of mortality among women worldwide, with early diagnosis being critical for effective treatment and improved survival rates. However, timely detection continues to be a challenge due to the complex nature of the disease and variability in patient risk factors. This study presents a fuzzy soft set theory-based expert system designed to assess the risk of breast cancer in patients using measurable clinical and physiological parameters. The proposed system integrates Body Mass Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input variables to estimate breast cancer risk through a set of fuzzy inference rules and soft set computations. These parameters can be obtained from routine blood analyses, enabling a non-invasive and accessible method for preliminary assessment. The dataset used for model development and validation was obtained from the UCI Machine Learning Repository. The proposed expert system aims to support healthcare professionals in identifying high-risk patients and determining the necessity of further diagnostic procedures such as biopsies.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¨¡ç³Šè½¯é›†ç†è®º(Fuzzy Soft Set Theory)çš„ä¸“å®¶ç³»ç»Ÿï¼Œæ—¨åœ¨åˆ©ç”¨å¯æµ‹é‡çš„ä¸´åºŠå’Œç”Ÿç†å‚æ•°è¯„ä¼°æ‚£è€…çš„ä¹³è…ºç™Œé£é™©ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†ä½“é‡æŒ‡æ•°(Body Mass Index)ã€èƒ°å²›ç´ æ°´å¹³(Insulin Level)ã€ç˜¦ç´ æ°´å¹³(Leptin Level)ã€è„‚è”ç´ æ°´å¹³(Adiponectin Level)å’Œå¹´é¾„ä½œä¸ºè¾“å…¥å˜é‡ï¼Œå¹¶é€šè¿‡æ¨¡ç³Šæ¨ç†è§„åˆ™å’Œè½¯é›†è®¡ç®—æ¥ä¼°ç®—é£é™©ã€‚æ¨¡å‹å¼€å‘å’ŒéªŒè¯ä½¿ç”¨äº†æ¥è‡ªUCIæœºå™¨å­¦ä¹ åº“(UCI Machine Learning Repository)çš„æ•°æ®é›†ã€‚ç”±äºæ‰€éœ€å‚æ•°å¯é€šè¿‡å¸¸è§„è¡€æ¶²åˆ†æè·å–ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ç§éä¾µå…¥æ€§ä¸”æ˜“äºå®æ–½çš„åˆæ­¥è¯„ä¼°æ‰‹æ®µã€‚è¯¥ä¸“å®¶ç³»ç»Ÿèƒ½å¤Ÿæ”¯æŒåŒ»ç–—ä¸“ä¸šäººå‘˜è¯†åˆ«é«˜å±æ‚£è€…ï¼Œå¹¶è¾…åŠ©åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ´»æ£€ç­‰è¿›ä¸€æ­¥è¯Šæ–­ç¨‹åºï¼Œä»è€Œä¿ƒè¿›ä¹³è…ºç™Œçš„æ—©æœŸå‘ç°å’Œæ²»ç–—ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02392v1",
      "published_date": "2025-11-04 09:19:16 UTC",
      "updated_date": "2025-11-04 09:19:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:46:35.356070+00:00"
    },
    {
      "arxiv_id": "2601.06029v1",
      "title": "A Recommendation System-Based Framework for Enhancing Human-Machine Collaboration in Industrial Timetabling Rescheduling: Application in Preventive Maintenance",
      "title_zh": "åŸºäºæ¨èç³»ç»Ÿçš„å¢å¼ºå·¥ä¸šæ’ç¨‹é‡è°ƒåº¦äººæœºåä½œæ¡†æ¶ï¼šé¢„é˜²æ€§ç»´æŠ¤åº”ç”¨",
      "authors": [
        "KÃ©vin Ducharlet",
        "Liwen Zhang",
        "Sara Maqrot",
        "Houssem Saidi"
      ],
      "abstract": "Industrial timetabling is a critical task for decision-makers across various sectors to ensure efficient system operation. In real-world settings, it remains challenging because unexpected events often disrupt execution. When such events arise, effective rescheduling and collaboration between humans and machines becomes essential. This paper presents a recommendation system-based framework for handling rescheduling challenges, built on Timefold, a powerful AI-driven planning engine. Our experimental study evaluates nine instances inspired by a realworld preventive maintenance use case, aiming to identify the heuristic that best balances solution quality and computing time to support near-optimal decisionmaking when rescheduling is required due to unexpected events during operational days. Finally, we illustrate the complete process of our recommendation system through a simple use case.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹å·¥ä¸šè°ƒåº¦ï¼ˆIndustrial Timetablingï¼‰ä¸­å› çªå‘äº‹ä»¶å¯¼è‡´æ‰§è¡Œä¸­æ–­çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¨èç³»ç»Ÿçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé‡è°ƒåº¦è¿‡ç¨‹ä¸­çš„äººæœºåä½œï¼ˆHuman-Machine Collaborationï¼‰ã€‚è¯¥æ¡†æ¶æ„å»ºäºå¼ºå¤§çš„AIé©±åŠ¨è§„åˆ’å¼•æ“Timefoldä¹‹ä¸Šï¼Œä¸“é—¨ç”¨äºåº”å¯¹å®é™…æ“ä½œä¸­å‡ºç°çš„é‡è°ƒåº¦æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¹ä¸ªå—çœŸå®ä¸–ç•Œé¢„é˜²æ€§ç»´æŠ¤ï¼ˆPreventive Maintenanceï¼‰æ¡ˆä¾‹å¯å‘çš„å®ä¾‹è¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œæ—¨åœ¨è¯†åˆ«å‡ºèƒ½å¤Ÿæœ€ä½³å¹³è¡¡è§£å†³æ–¹æ¡ˆè´¨é‡ä¸è®¡ç®—æ—¶é—´çš„å¯å‘å¼æ–¹æ³•ï¼ˆHeuristicï¼‰ã€‚è¯¥ç ”ç©¶è‡´åŠ›äºåœ¨è¿è¥æ—¥å‘ç”Ÿçªå‘äº‹ä»¶æ—¶æ”¯æŒè¿‘ä¹æœ€ä¼˜çš„å†³ç­–åˆ¶å®šï¼Œå¹¶é€šè¿‡å…·ä½“ç”¨ä¾‹å±•ç¤ºäº†è¯¥æ¨èç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹ï¼ŒéªŒè¯äº†å…¶åœ¨æå‡å·¥ä¸šç³»ç»Ÿè¿è¡Œæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06029v1",
      "published_date": "2025-11-04 09:10:11 UTC",
      "updated_date": "2025-11-04 09:10:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:46:58.788145+00:00"
    },
    {
      "arxiv_id": "2511.05567v1",
      "title": "Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster",
      "title_zh": "åŸºäºæ•™å¸ˆ-å­¦ç”Ÿè‡ªé€‚åº”ç»“æ„æ·±åº¦ç½®ä¿¡ç½‘ç»œçš„é“è·¯ç½‘ç»œè‡ªåŠ¨æå–åŠå…¶åœ¨æ»‘å¡ç¾å®³ä¸­çš„åº”ç”¨",
      "authors": [
        "Shin Kamada",
        "Takumi Ichimura"
      ],
      "abstract": "An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\\% to 89.0\\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTeacher-Studenté›†æˆå­¦ä¹ æ¨¡å‹çš„Adaptive Deep Belief Network (DBN)æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›RoadTraceræ¡†æ¶ä»¥ä»èˆªç©ºç…§ç‰‡ä¸­è‡ªåŠ¨æå–é“è·¯ç½‘ç»œã€‚é’ˆå¯¹é“è·¯åœ°å›¾ä¸­å¤æ‚çš„ç‰¹å¾ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨RBMå’ŒDBNçš„è‡ªé€‚åº”ç»“æ„å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ç¥ç»å…ƒåŠå±‚çº§çš„ç”Ÿæˆä¸æ¶ˆäº¡ï¼‰ä¼˜åŒ–ç½‘ç»œç»“æ„ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸ƒä¸ªä¸»è¦åŸå¸‚çš„æµ‹è¯•ä¸­ï¼Œå¹³å‡æ£€æµ‹å‡†ç¡®ç‡ä»40.0%å¤§å¹…æå‡è‡³89.0%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¢«æ‰©å±•åº”ç”¨äºè‡ªç„¶ç¾å®³ï¼ˆå¦‚æ»‘å¡ï¼‰å‘ç”Ÿåçš„å¯ç”¨é“è·¯æ£€æµ‹ï¼Œä»¥ä¾¿åœ¨ç¾åå¿«é€Ÿè§„åˆ’è¿è¾“è·¯çº¿ã€‚ä¸ºäº†å®ç°å¿«é€Ÿæ¨ç†ï¼Œç ”ç©¶äººå‘˜è¿˜åœ¨å°å‹åµŒå…¥å¼è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²äº†è½»é‡çº§æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†æ—¥æœ¬é™é›¨ç¾å®³å‰åå«æ˜Ÿå›¾åƒçš„å®é™…æ£€æµ‹æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05567v1",
      "published_date": "2025-11-04 09:07:21 UTC",
      "updated_date": "2025-11-04 09:07:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:48:02.157327+00:00"
    },
    {
      "arxiv_id": "2511.02379v1",
      "title": "H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart Sound Recordings",
      "title_zh": "åŸºäºå¿ƒéŸ³è®°å½•æ£€æµ‹å¿ƒå¾‹å¤±å¸¸çš„H-Infinityæ»¤æ³¢å™¨å¢å¼ºå‹CNN-LSTM",
      "authors": [
        "Rohith Shinoj Kumar",
        "Rushdeep Dinda",
        "Aditya Tyagi",
        "Annappa B.",
        "Naveen Kumar M. R"
      ],
      "abstract": "Early detection of heart arrhythmia can prevent severe future complications in cardiac patients. While manual diagnosis still remains the clinical standard, it relies heavily on visual interpretation and is inherently subjective. In recent years, deep learning has emerged as a powerful tool to automate arrhythmia detection, offering improved accuracy, consistency, and efficiency. Several variants of convolutional and recurrent neural network architectures have been widely explored to capture spatial and temporal patterns in physiological signals. However, despite these advancements, current models often struggle to generalize well in real-world scenarios, especially when dealing with small or noisy datasets, which are common challenges in biomedical applications. In this paper, a novel CNN-H-Infinity-LSTM architecture is proposed to identify arrhythmic heart signals from heart sound recordings. This architecture introduces trainable parameters inspired by the H-Infinity filter from control theory, enhancing robustness and generalization. Extensive experimentation on the PhysioNet CinC Challenge 2016 dataset, a public benchmark of heart audio recordings, demonstrates that the proposed model achieves stable convergence and outperforms existing benchmarks, with a test accuracy of 99.42% and an F1 score of 98.85%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å°å‹æˆ–å™ªå£°æ•°æ®é›†æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºå¿ƒéŸ³å½•éŸ³å¿ƒå¾‹å¤±å¸¸æ£€æµ‹çš„æ–°å‹CNN-H-Infinity-LSTMæ¶æ„ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å¼•å…¥äº†å—æ§åˆ¶ç†è®ºå¯å‘çš„H-Infinity filterä½œä¸ºå¯è®­ç»ƒå‚æ•°ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨é¢å¯¹å¤æ‚ç”Ÿç†ä¿¡å·æ—¶çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å¼€åŸºå‡†PhysioNet CinC Challenge 2016æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¸ä»…å®ç°äº†ç¨³å®šçš„æ”¶æ•›ï¼Œè€Œä¸”æ€§èƒ½ä¼˜äºç°æœ‰åŸºå‡†ã€‚æœ€ç»ˆæµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†99.42%çš„å‡†ç¡®ç‡å’Œ98.85%çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨åŒ–å¿ƒå¾‹å¤±å¸¸æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "This is a preprint of a paper to appear at the 15th IEEE International Conference on Systems Engineering and Technology (ICSET 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.02379v1",
      "published_date": "2025-11-04 09:00:17 UTC",
      "updated_date": "2025-11-04 09:00:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:49:03.004389+00:00"
    },
    {
      "arxiv_id": "2511.02880v1",
      "title": "NEF-NET+: Adapting Electrocardio panorama in the wild",
      "title_zh": "NEF-NET+ï¼šçœŸå®åœºæ™¯ä¸‹çš„å¿ƒç”µå…¨æ™¯é€‚é…",
      "authors": [
        "Zehui Zhan",
        "Yaojun Hu",
        "Jiajing Zhan",
        "Wanchen Lian",
        "Wanqing Wu",
        "Jintai Chen"
      ],
      "abstract": "Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå¤šå¯¼è”å¿ƒç”µå›¾(ECG)å—é™äºå›ºå®šè§†ç‚¹ä»¥åŠå…ˆå‰Nef-Netæ¨¡å‹éš¾ä»¥åº”å¯¹çœŸå®åœºæ™¯æŒ‘æˆ˜çš„é—®é¢˜ï¼Œæå‡ºäº†å¢å¼ºå‹æ¡†æ¶NEF-NET+ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å®ç°é€¼çœŸçš„å…¨æ™¯å¿ƒç”µå›¾(Electrocardio Panorama)åˆæˆï¼Œæ”¯æŒä»»æ„è§†ç‚¹å’Œä»»æ„é•¿åº¦çš„ä¿¡å·ç”Ÿæˆï¼Œå¹¶èƒ½æœ‰æ•ˆåº”å¯¹è·¨è®¾å¤‡æ³›åŒ–åŠç”µææ”¾ç½®åå·®ç­‰é—®é¢˜ã€‚NEF-NET+é‡‡ç”¨äº†ä¸€ç§æ‰§è¡Œç›´æ¥è§†ç‚¹å˜æ¢çš„æ–°å‹æ¶æ„ï¼Œç»“åˆäº†ç¦»çº¿é¢„è®­ç»ƒã€è®¾å¤‡æ ¡å‡†å¾®è°ƒä»¥åŠé’ˆå¯¹æ‚£è€…ç‰¹å¼‚æ€§é€‚åº”çš„å®æ—¶æ ¡å‡†æµç¨‹ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«5367æ¡è®°å½•ã€æ¯ä½å—è¯•è€…48ä¸ªè§†ç‚¹çš„PanobenchåŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNEF-NET+åœ¨çœŸå®ä¸–ç•Œè®¾ç½®ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºNef-Netï¼Œå…¶å³°å€¼ä¿¡å™ªæ¯”(PSNR)æå‡äº†çº¦6 dBã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02880v1",
      "published_date": "2025-11-04 08:58:39 UTC",
      "updated_date": "2025-11-04 08:58:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:48:52.509233+00:00"
    },
    {
      "arxiv_id": "2511.02376v3",
      "title": "AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models",
      "title_zh": "AutoAdvï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¤šè½®è¶Šç‹±çš„è‡ªåŠ¨åŒ–å¯¹æŠ—æ€§æç¤º",
      "authors": [
        "Aashray Reddy",
        "Andrew Zagula",
        "Nicholas Saban"
      ],
      "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Large Language Models (LLMs)åœ¨å¤šè½®å¯¹è¯ä¸­çš„è„†å¼±æ€§ï¼Œæå‡ºäº†AutoAdvï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è‡ªåŠ¨åŒ–å¤šè½®jailbreakingæ¡†æ¶ã€‚ä¸åŒäºä¸»è¦å…³æ³¨å•è½®äº¤äº’çš„ç°æœ‰è¯„ä¼°ï¼ŒAutoAdvé€šè¿‡ä¸‰ç§è‡ªé€‚åº”æœºåˆ¶å®ç°æ”»å‡»ï¼šåˆ©ç”¨æˆåŠŸæ¡ˆä¾‹å¢å¼ºæç¤ºçš„pattern managerã€æ ¹æ®å¤±è´¥æ¨¡å¼åŠ¨æ€è°ƒæ•´é‡‡æ ·å‚æ•°çš„temperature managerï¼Œä»¥åŠå…ˆä¼ªè£…åè¿­ä»£ä¼˜åŒ–çš„two-phase rewriting strategyã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoAdvåœ¨Llama-3.1-8Bæ¨¡å‹ä¸Šå…­è½®äº¤äº’å†…çš„æ”»å‡»æˆåŠŸç‡é«˜è¾¾95%ï¼Œç›¸æ¯”å•è½®åŸºçº¿æå‡äº†24%ã€‚åœ¨åŒ…æ‹¬GPT-4o miniå’ŒQwen3-235Båœ¨å†…çš„å¤šä¸ªå•†ä¸šåŠå¼€æºæ¨¡å‹ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œé’ˆå¯¹å•è½®äº¤äº’ä¼˜åŒ–çš„å¯¹é½ç­–ç•¥åœ¨é•¿å¯¹è¯ä¸­æ— æ³•ä¿æŒé²æ£’æ€§ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰å®‰å…¨æœºåˆ¶çš„æŒç»­æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘é’ˆå¯¹å¤šè½®æ”»å‡»é˜²å¾¡æªæ–½çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at NeurIPS 2025 Lock-LLM Workshop. Code is available at https://github.com/AAN-AutoAdv/AutoAdv",
      "pdf_url": "https://arxiv.org/pdf/2511.02376v3",
      "published_date": "2025-11-04 08:56:28 UTC",
      "updated_date": "2025-12-21 22:30:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:52:16.059399+00:00"
    },
    {
      "arxiv_id": "2511.02374v1",
      "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda",
      "title_zh": "AyurParamï¼šé¢å‘é˜¿è‚²å é™€çš„æœ€å…ˆè¿›åŒè¯­è¯­è¨€æ¨¡å‹",
      "authors": [
        "Mohd Nauman",
        "Sravan Gvm",
        "Vijay Devane",
        "Shyam Pawar",
        "Viraj Thakur",
        "Kundeshwar Pundalik",
        "Piyush Sawarkar",
        "Rohit Saluja",
        "Maunendra Desarkar",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é˜¿è‚²å é™€(Ayurveda)ç­‰éœ€è¦æ·±åšæ–‡åŒ–å’Œä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæ¨å‡ºäº†AyurParam-2.9Bã€‚è¿™æ˜¯ä¸€ä¸ªé¢†åŸŸä¸“ç”¨çš„åŒè¯­æ¨¡å‹ï¼ŒåŸºäºParam-1-2.9Bè¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨äº†åŒ…å«å¤å…¸æ–‡æœ¬å’Œä¸´åºŠæŒ‡å¯¼çš„ä¸“å®¶ç­–åˆ’æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è‹±è¯­å’Œå°åœ°è¯­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€æ¨ç†åŠå®¢è§‚é—®ç­”ï¼Œå¹¶é‡‡ç”¨äº†ä¸¥æ ¼çš„æ ‡æ³¨åè®®ä»¥ç¡®ä¿ç²¾ç¡®æ€§ã€‚åœ¨BhashaBench-AyuråŸºå‡†æµ‹è¯•ä¸­ï¼ŒAyurParamä¸ä»…è¶…è¶Šäº†åŒå°ºå¯¸çº§åˆ«ï¼ˆ1.5-3Bå‚æ•°ï¼‰çš„æ‰€æœ‰å¼€æºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œè¿˜å±•ç°å‡ºä¸æ›´å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†çœŸå®é¢†åŸŸé€‚åº”(domain adaptation)å’Œé«˜è´¨é‡ç›‘ç£å¯¹äºæ„å»ºå¯é ä¸”æ–‡åŒ–å¥‘åˆçš„ä¸“ä¸šåŒ»å­¦AIçš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02374v1",
      "published_date": "2025-11-04 08:53:21 UTC",
      "updated_date": "2025-11-04 08:53:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:52:37.274398+00:00"
    },
    {
      "arxiv_id": "2511.05566v1",
      "title": "Efficient Online Continual Learning in Sensor-Based Human Activity Recognition",
      "title_zh": "åŸºäºä¼ æ„Ÿå™¨çš„äººä½“æ´»åŠ¨è¯†åˆ«ä¸­çš„é«˜æ•ˆåœ¨çº¿æŒç»­å­¦ä¹ ",
      "authors": [
        "Yao Zhang",
        "Souza Leite Clayton",
        "Yu Xiao"
      ],
      "abstract": "Machine learning models for sensor-based human activity recognition (HAR) are expected to adapt post-deployment to recognize new activities and different ways of performing existing ones. To address this need, Online Continual Learning (OCL) mechanisms have been proposed, allowing models to update their knowledge incrementally as new data become available while preserving previously acquired information. However, existing OCL approaches for sensor-based HAR are computationally intensive and require extensive labeled samples to represent new changes. Recently, pre-trained model-based (PTM-based) OCL approaches have shown significant improvements in performance and efficiency for computer vision applications. These methods achieve strong generalization capabilities by pre-training complex models on large datasets, followed by fine-tuning on downstream tasks for continual learning. However, applying PTM-based OCL approaches to sensor-based HAR poses significant challenges due to the inherent heterogeneity of HAR datasets and the scarcity of labeled data in post-deployment scenarios. This paper introduces PTRN-HAR, the first successful application of PTM-based OCL to sensor-based HAR. Unlike prior PTM-based OCL approaches, PTRN-HAR pre-trains the feature extractor using contrastive loss with a limited amount of data. This extractor is then frozen during the streaming stage. Furthermore, it replaces the conventional dense classification layer with a relation module network. Our design not only significantly reduces the resource consumption required for model training while maintaining high performance, but also improves data efficiency by reducing the amount of labeled data needed for effective continual learning, as demonstrated through experiments on three public datasets, outperforming the state-of-the-art. The code can be found here: https://anonymous.4open.science/r/PTRN-HAR-AF60/",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹åŸºäºä¼ æ„Ÿå™¨çš„ç”¨æˆ·æ´»åŠ¨è¯†åˆ«(HAR)æ¨¡å‹åœ¨éƒ¨ç½²åéœ€è¦é€‚åº”æ–°æ´»åŠ¨å’Œè¡Œä¸ºå˜åŒ–çš„éœ€æ±‚ï¼Œæ¢è®¨äº†åœ¨çº¿æŒç»­å­¦ä¹ (Online Continual Learning, OCL)çš„åº”ç”¨ã€‚ç°æœ‰çš„HAR OCLæ–¹æ³•è®¡ç®—å¯†é›†ä¸”ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè€ŒåŸºäºé¢„è®­ç»ƒæ¨¡å‹(PTM)çš„æ–¹æ³•è™½åœ¨è®¡ç®—æœºè§†è§‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå´å› HARæ•°æ®çš„å¼‚æ„æ€§å’Œç¨€ç¼ºæ€§éš¾ä»¥ç›´æ¥åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†PTRN-HARï¼Œè¿™æ˜¯é¦–ä¸ªæˆåŠŸåº”ç”¨äºåŸºäºä¼ æ„Ÿå™¨çš„HARçš„PTM-based OCLæ–¹æ³•ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒPTRN-HARåˆ©ç”¨å¯¹æ¯”æŸå¤±(contrastive loss)åœ¨æœ‰é™æ•°æ®ä¸Šé¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œå¹¶åœ¨æµå¼é˜¶æ®µå°†å…¶å†»ç»“ï¼ŒåŒæ—¶ä½¿ç”¨å…³ç³»æ¨¡å—ç½‘ç»œ(relation module network)å–ä»£äº†ä¼ ç»Ÿçš„å¯†é›†åˆ†ç±»å±‚ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§è®¾è®¡ä¸ä»…æ˜¾è‘—é™ä½äº†æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„èµ„æºæ¶ˆè€—å¹¶ä¿æŒé«˜æ€§èƒ½ï¼Œè¿˜é€šè¿‡å‡å°‘æœ‰æ•ˆæŒç»­å­¦ä¹ æ‰€éœ€çš„æ ‡æ³¨æ•°æ®é‡æé«˜äº†æ•°æ®æ•ˆç‡ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•(SOTA)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.05566v1",
      "published_date": "2025-11-04 08:48:36 UTC",
      "updated_date": "2025-11-04 08:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:53:01.692122+00:00"
    },
    {
      "arxiv_id": "2511.02879v1",
      "title": "Stochastic Deep Graph Clustering for Practical Group Formation",
      "title_zh": "é¢å‘å®ç”¨ç¾¤ç»„æ„å»ºçš„éšæœºæ·±åº¦å›¾èšç±»",
      "authors": [
        "Junhyung Park",
        "Hyungjin Kim",
        "Seokho Ahn",
        "Young-Duk Seo"
      ],
      "abstract": "While prior work on group recommender systems (GRSs) has primarily focused on improving recommendation accuracy, most approaches assume static or predefined groups, making them unsuitable for dynamic, real-world scenarios. We reframe group formation as a core challenge in GRSs and propose DeepForm (Stochastic Deep Graph Clustering for Practical Group Formation), a framework designed to meet three key operational requirements: (1) the incorporation of high-order user information, (2) real-time group formation, and (3) dynamic adjustment of the number of groups. DeepForm employs a lightweight GCN architecture that effectively captures high-order structural signals. Stochastic cluster learning enables adaptive group reconfiguration without retraining, while contrastive learning refines groups under dynamic conditions. Experiments on multiple datasets demonstrate that DeepForm achieves superior group formation quality, efficiency, and recommendation accuracy compared with various baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç¾¤ç»„æ¨èç³»ç»Ÿ(GRSs)é€šå¸¸å‡è®¾é™æ€æˆ–é¢„å®šä¹‰ç¾¤ç»„è€Œæ— æ³•é€‚åº”åŠ¨æ€ç°å®åœºæ™¯çš„é—®é¢˜ï¼Œå°†ç¾¤ç»„å½¢æˆé‡æ–°å®šä¹‰ä¸ºæ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†DeepFormæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ»¡è¶³æ•´åˆé«˜é˜¶ç”¨æˆ·ä¿¡æ¯ã€å®æ—¶ç¾¤ç»„å½¢æˆä»¥åŠåŠ¨æ€è°ƒæ•´ç¾¤ç»„æ•°é‡è¿™ä¸‰ä¸ªå…³é”®æ“ä½œéœ€æ±‚ã€‚DeepFormé‡‡ç”¨è½»é‡çº§å›¾å·ç§¯ç½‘ç»œ(GCN)æ¶æ„æœ‰æ•ˆæ•æ‰é«˜é˜¶ç»“æ„ä¿¡å·ï¼Œåˆ©ç”¨éšæœºèšç±»å­¦ä¹ (Stochastic cluster learning)å®ç°æ— éœ€é‡æ–°è®­ç»ƒçš„è‡ªé€‚åº”ç¾¤ç»„é‡æ„ï¼Œå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹ (Contrastive learning)åœ¨åŠ¨æ€æ¡ä»¶ä¸‹ä¼˜åŒ–ç¾¤ç»„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeepFormåœ¨ç¾¤ç»„å½¢æˆè´¨é‡ã€è¿è¡Œæ•ˆç‡å’Œæ¨èå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºå¤šç§åŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†å®é™…åº”ç”¨ä¸­çš„ç¾¤ç»„å½¢æˆé—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02879v1",
      "published_date": "2025-11-04 08:47:04 UTC",
      "updated_date": "2025-11-04 08:47:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:53:25.391137+00:00"
    },
    {
      "arxiv_id": "2511.02370v1",
      "title": "AI Credibility Signals Outrank Institutions and Engagement in Shaping News Perception on Social Media",
      "title_zh": "AIå¯ä¿¡åº¦ä¿¡å·åœ¨å¡‘é€ ç¤¾äº¤åª’ä½“æ–°é—»æ„ŸçŸ¥æ–¹é¢è¶…è¶Šäº†æœºæ„ä¸å‚ä¸åº¦",
      "authors": [
        "Adnan Hoq",
        "Matthew Facciani",
        "Tim Weninger"
      ],
      "abstract": "AI-generated content is rapidly becoming a salient component of online information ecosystems, yet its influence on public trust and epistemic judgments remains poorly understood. We present a large-scale mixed-design experiment (N = 1,000) investigating how AI-generated credibility scores affect user perception of political news. Our results reveal that AI feedback significantly moderates partisan bias and institutional distrust, surpassing traditional engagement signals such as likes and shares. These findings demonstrate the persuasive power of generative AI and suggest a need for design strategies that balance epistemic influence with user autonomy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIç”Ÿæˆå†…å®¹åœ¨åœ¨çº¿ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿä¸­å¯¹å…¬ä¼—ä¿¡ä»»å’Œè®¤çŸ¥åˆ¤æ–­çš„å½±å“ã€‚é€šè¿‡ä¸€é¡¹æ¶‰åŠ1000åå‚ä¸è€…çš„å¤§è§„æ¨¡æ··åˆè®¾è®¡å®éªŒï¼Œä½œè€…è°ƒæŸ¥äº†AIç”Ÿæˆçš„credibility scoreså¦‚ä½•å½±å“ç”¨æˆ·å¯¹æ”¿æ²»æ–°é—»çš„æ„ŸçŸ¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIåé¦ˆèƒ½æ˜¾è‘—è°ƒèŠ‚partisan biaså’Œinstitutional distrustï¼Œå…¶å½±å“åŠ›ç”šè‡³è¶…è¿‡äº†æœºæ„å£°èª‰ä»¥åŠç‚¹èµå’Œåˆ†äº«ç­‰ä¼ ç»Ÿçš„engagement signalsã€‚è¿™äº›å‘ç°æ­ç¤ºäº†generative AIåœ¨å¡‘é€ æ–°é—»æ„ŸçŸ¥æ–¹é¢çš„å¼ºå¤§è¯´æœåŠ›ï¼Œå¹¶æŒ‡å‡ºæœªæ¥çš„è®¾è®¡ç­–ç•¥éœ€è¦åœ¨æ–½åŠ epistemic influenceä¸ç»´æŠ¤ç”¨æˆ·è‡ªä¸»æƒä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02370v1",
      "published_date": "2025-11-04 08:46:54 UTC",
      "updated_date": "2025-11-04 08:46:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:53:48.620308+00:00"
    },
    {
      "arxiv_id": "2511.02358v1",
      "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºè®©å¤šæ¨¡æ€åµŒå…¥å™¨å­¦ä¹ ä½•æ—¶å¢å¼ºæŸ¥è¯¢",
      "authors": [
        "Wongyu Kim",
        "Hochang Lee",
        "Sanghak Lee",
        "Yoonsung Kim",
        "Jaehyun Park"
      ],
      "abstract": "Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„åµŒå…¥å™¨åœ¨æŸ¥è¯¢å¢å¼º(Query Augmentation)ä¸­é¢ä¸´çš„é«˜å»¶è¿ŸåŠéƒ¨åˆ†æŸ¥è¯¢æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†M-Solomonï¼Œä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”å†³å®šä½•æ—¶å¢å¼ºæŸ¥è¯¢çš„é€šç”¨å¤šæ¨¡æ€åµŒå…¥å™¨ã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†è®­ç»ƒæ•°æ®é›†çš„æŸ¥è¯¢åˆ†ä¸ºéœ€è¦å¢å¼ºå’Œä¸éœ€è¦å¢å¼ºä¸¤ç»„ï¼Œå¹¶åˆ©ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ä¸ºå‰è€…ç”Ÿæˆåˆæˆå¢å¼ºå†…å®¹ã€‚éšåï¼Œé€šè¿‡å¼•å…¥è‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºæœºåˆ¶ï¼ŒM-Solomonå­¦ä¹ åœ¨å¿…è¦æ—¶ç”Ÿæˆå¸¦æœ‰/augmentå‰ç¼€çš„å¢å¼ºå†…å®¹ï¼Œè€Œåœ¨ä¸éœ€è¦æ—¶ä»…ç”Ÿæˆç®€å•çš„/embedå­—ç¬¦ä¸²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-Solomonä¸ä»…åœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†æ— å¢å¼ºåŠæ€»æ˜¯å¢å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œè¿˜æ˜¾è‘—é™ä½äº†åµŒå…¥å»¶è¿Ÿï¼Œæœ‰æ•ˆæå‡äº†å¤šæ¨¡æ€æ£€ç´¢çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to MMGenSR Workshop (CIKM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.02358v1",
      "published_date": "2025-11-04 08:24:41 UTC",
      "updated_date": "2025-11-04 08:24:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:54:16.227940+00:00"
    },
    {
      "arxiv_id": "2511.02351v1",
      "title": "Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition",
      "title_zh": "äººæœºä»ªå¼ï¼šåŸºäºå®æ—¶åŠ¨ä½œè¯†åˆ«çš„ååŒè¡¨æ¼”",
      "authors": [
        "Zhuodi Cai",
        "Ziyu Xu",
        "Juan Pampin"
      ],
      "abstract": "We introduce a lightweight, real-time motion recognition system that enables synergic human-machine performance through wearable IMU sensor data, MiniRocket time-series classification, and responsive multimedia control. By mapping dancer-specific movement to sound through somatic memory and association, we propose an alternative approach to human-machine collaboration, one that preserves the expressive depth of the performing body while leveraging machine learning for attentive observation and responsiveness. We demonstrate that this human-centered design reliably supports high accuracy classification (<50 ms latency), offering a replicable framework to integrate dance-literate machines into creative, educational, and live performance contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„å®æ—¶åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å¯ç©¿æˆ´IMUä¼ æ„Ÿå™¨æ•°æ®ã€MiniRocketæ—¶é—´åºåˆ—åˆ†ç±»ç®—æ³•ä»¥åŠå“åº”å¼å¤šåª’ä½“æ§åˆ¶æ¥å®ç°äººæœºååŒè¡¨æ¼”ã€‚é€šè¿‡åˆ©ç”¨èº¯ä½“è®°å¿†(somatic memory)å’Œè”æƒ³å°†ç‰¹å®šèˆè€…çš„åŠ¨ä½œæ˜ å°„åˆ°å£°éŸ³ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„äººæœºåä½œæ–¹æ³•ï¼Œæ—¢åˆ©ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œæ•é”çš„è§‚å¯Ÿå’Œå“åº”ï¼Œåˆä¿ç•™äº†è¡¨æ¼”è€…èº«ä½“çš„è¡¨è¾¾æ·±åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ä»¥äººä¸ºæœ¬çš„è®¾è®¡èƒ½å¤Ÿå¯é åœ°æ”¯æŒé«˜ç²¾åº¦åˆ†ç±»ï¼Œä¸”å»¶è¿Ÿæä½(<50 ms)ã€‚è¯¥ç ”ç©¶ä¸ºå°†å…·å¤‡èˆè¹ˆç´ å…»çš„æœºå™¨(dance-literate machines)æ•´åˆåˆ°åˆ›æ„ã€æ•™è‚²å’Œç°åœºè¡¨æ¼”ç¯å¢ƒä¸­æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 figures. Camera-ready manuscript for the Creative AI Track of NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.02351v1",
      "published_date": "2025-11-04 08:15:25 UTC",
      "updated_date": "2025-11-04 08:15:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:54:41.114628+00:00"
    },
    {
      "arxiv_id": "2511.02877v1",
      "title": "A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features",
      "title_zh": "åŸºäºæ—¶é—´å»¶è¿ŸåµŒå…¥ä¸éšæœºå‚…é‡Œå¶ç‰¹å¾çš„æ··æ²Œæ—¶é—´åºåˆ—é¢„æµ‹æ–°å‹å‚¨å¤‡æ± è®¡ç®—æ¡†æ¶",
      "authors": [
        "S. K. Laha"
      ],
      "abstract": "Forecasting chaotic time series requires models that can capture the intrinsic geometry of the underlying attractor while remaining computationally efficient. We introduce a novel reservoir computing (RC) framework that integrates time-delay embedding with Random Fourier Feature (RFF) mappings to construct a dynamical reservoir without the need for traditional recurrent architectures. Unlike standard RC, which relies on high-dimensional recurrent connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel transformations that uncover latent dynamical relations in the reconstructed phase space. This hybrid formulation offers two key advantages: (i) it provides a principled way to approximate complex nonlinear interactions among delayed coordinates, thereby enriching the effective dynamical representation of the reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters such as spectral radius and leaking rate. We evaluate the framework on canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC not only achieves superior prediction accuracy but also yields robust attractor reconstructions and long-horizon forecasts. These results show that the combination of delay embedding and RFF-based reservoirs reveals new dynamical structure by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to modeling chaotic dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºæ··æ²Œæ—¶é—´åºåˆ—é¢„æµ‹çš„æ–°å‹Reservoir Computing (RC)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Time-Delay Embeddingå’ŒRandom Fourier Feature (RFF)æ˜ å°„ï¼Œæ— éœ€ä¼ ç»Ÿçš„å¾ªç¯æ¶æ„å³å¯æ„å»ºåŠ¨åŠ›å­¦å‚¨å¤‡æ± ã€‚ä¸ä¾èµ–é«˜ç»´å¾ªç¯è¿æ¥çš„æ ‡å‡†RCä¸åŒï¼ŒRFF-RCé€šè¿‡æ˜¾å¼é€¼è¿‘éçº¿æ€§æ ¸å˜æ¢ï¼Œæœ‰æ•ˆæ­ç¤ºäº†é‡æ„ç›¸ç©ºé—´ä¸­çš„æ½œåœ¨åŠ¨åŠ›å­¦å…³ç³»ã€‚è¿™ç§æ··åˆæ–¹æ³•ä¸ä»…æä¾›äº†é€¼è¿‘å»¶è¿Ÿåæ ‡é—´å¤æ‚éçº¿æ€§ç›¸äº’ä½œç”¨çš„åŸåˆ™æ€§é€”å¾„ï¼Œè¿˜æ˜¾è‘—å‡å°‘äº†å¯¹è°±åŠå¾„å’Œæ³„æ¼ç‡ç­‰æ‰‹åŠ¨è¶…å‚æ•°çš„ä¾èµ–ã€‚åœ¨Mackey-Glassæ–¹ç¨‹ã€Lorenzç³»ç»Ÿå’ŒKuramoto-Sivashinskyæ–¹ç¨‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRFF-RCåœ¨é¢„æµ‹ç²¾åº¦ã€å¸å¼•å­é‡æ„ç¨³å¥æ€§å’Œé•¿è§†ç•Œé¢„æµ‹æ–¹é¢å‡ä¼˜äºåŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†ç³»ç»ŸåµŒå…¥ä¸°å¯Œçš„ç‰¹å¾ç©ºé—´ï¼Œè¯¥æ–¹æ³•ä¸ºæ··æ²ŒåŠ¨åŠ›å­¦å»ºæ¨¡æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02877v1",
      "published_date": "2025-11-04 07:59:08 UTC",
      "updated_date": "2025-11-04 07:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:55:23.672810+00:00"
    },
    {
      "arxiv_id": "2511.02340v2",
      "title": "Chronic Kidney Disease Prognosis Prediction Using Transformer",
      "title_zh": "åŸºäº Transformer çš„æ…¢æ€§è‚¾è„ç—…é¢„åé¢„æµ‹",
      "authors": [
        "Yohan Lee",
        "DongGyun Kang",
        "SeHoon Park",
        "Sa-Yoon Park",
        "Kwangsoo Kim"
      ],
      "abstract": "Chronic Kidney Disease (CKD) affects nearly 10\\% of the global population and often progresses to end-stage renal failure. Accurate prognosis prediction is vital for timely interventions and resource optimization. We present a transformer-based framework for predicting CKD progression using multi-modal electronic health records (EHR) from the Seoul National University Hospital OMOP Common Data Model. Our approach (\\textbf{ProQ-BERT}) integrates demographic, clinical, and laboratory data, employing quantization-based tokenization for continuous lab values and attention mechanisms for interpretability. The model was pretrained with masked language modeling and fine-tuned for binary classification tasks predicting progression from stage 3a to stage 5 across varying follow-up and assessment periods. Evaluated on a cohort of 91,816 patients, our model consistently outperformed CEHR-BERT, achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction. These results highlight the effectiveness of transformer architectures and temporal design choices in clinical prognosis modeling, offering a promising direction for personalized CKD care.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ProQ-BERTï¼Œä¸€ç§åŸºäºTransformerçš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é¦–å°”å›½ç«‹å¤§å­¦åŒ»é™¢OMOPé€šç”¨æ•°æ®æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•(EHR)é¢„æµ‹æ…¢æ€§è‚¾è„ç—…(CKD)çš„è¿›å±•ã€‚è¯¥æ–¹æ³•æ•´åˆäº†äººå£ç»Ÿè®¡å­¦ã€ä¸´åºŠå’Œå®éªŒå®¤æ•°æ®ï¼Œé‡‡ç”¨åŸºäºé‡åŒ–çš„æ ‡è®°åŒ–æŠ€æœ¯å¤„ç†è¿ç»­å®éªŒå®¤æ•°å€¼ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶(attention mechanisms)æä¾›å¯è§£é‡Šæ€§ã€‚æ¨¡å‹é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é’ˆå¯¹ä»3aæœŸåˆ°5æœŸçš„ç–¾ç—…è¿›å±•äºŒåˆ†ç±»ä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨åŒ…å«91,816åæ‚£è€…çš„é˜Ÿåˆ—ä¸­è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çŸ­æœŸé¢„æµ‹ä¸­å®ç°äº†é«˜è¾¾0.995çš„ROC-AUCå’Œ0.989çš„PR-AUCï¼Œæ€§èƒ½æŒç»­ä¼˜äºCEHR-BERTã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†Transformeræ¶æ„å’Œæ—¶é—´è®¾è®¡é€‰æ‹©åœ¨ä¸´åºŠé¢„åå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸ªæ€§åŒ–CKDæŠ¤ç†æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.OT"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.02340v2",
      "published_date": "2025-11-04 07:52:17 UTC",
      "updated_date": "2025-11-18 01:31:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:55:45.580866+00:00"
    },
    {
      "arxiv_id": "2511.02332v1",
      "title": "Biological Regulatory Network Inference through Circular Causal Structure Learning",
      "title_zh": "åŸºäºå¾ªç¯å› æœç»“æ„å­¦ä¹ çš„ç”Ÿç‰©è°ƒæ§ç½‘ç»œæ¨æ–­",
      "authors": [
        "Hongyang Jiang",
        "Yuezhu Wang",
        "Ke Feng",
        "Chaoyi Yin",
        "Yi Chang",
        "Huiyan Sun"
      ],
      "abstract": "Biological networks are pivotal in deciphering the complexity and functionality of biological systems. Causal inference, which focuses on determining the directionality and strength of interactions between variables rather than merely relying on correlations, is considered a logical approach for inferring biological networks. Existing methods for causal structure inference typically assume that causal relationships between variables can be represented by directed acyclic graphs (DAGs). However, this assumption is at odds with the reality of widespread feedback loops in biological systems, making these methods unsuitable for direct use in biological network inference. In this study, we propose a new framework named SCALD (Structural CAusal model for Loop Diagram), which employs a nonlinear structure equation model and a stable feedback loop conditional constraint through continuous optimization to infer causal regulatory relationships under feedback loops. We observe that SCALD outperforms state-of-the-art methods in inferring both transcriptional regulatory networks and signaling transduction networks. SCALD has irreplaceable advantages in identifying feedback regulation. Through transcription factor (TF) perturbation data analysis, we further validate the accuracy and sensitivity of SCALD. Additionally, SCALD facilitates the discovery of previously unknown regulatory relationships, which we have subsequently confirmed through ChIP-seq data analysis. Furthermore, by utilizing SCALD, we infer the key driver genes that facilitate the transformation from colon inflammation to cancer by examining the dynamic changes within regulatory networks during the process.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å› æœæ¨ç†æ–¹æ³•é€šå¸¸å‡è®¾æœ‰å‘æ— ç¯å›¾(DAGs)ä»è€Œæ— æ³•å¤„ç†ç”Ÿç‰©ç³»ç»Ÿä¸­æ™®éå­˜åœ¨çš„åé¦ˆå›è·¯è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSCALDçš„æ–°æ¡†æ¶ã€‚SCALDé‡‡ç”¨éçº¿æ€§ç»“æ„æ–¹ç¨‹æ¨¡å‹(nonlinear structure equation model)å¹¶é€šè¿‡è¿ç»­ä¼˜åŒ–å®æ–½ç¨³å®šçš„åé¦ˆå›è·¯æ¡ä»¶çº¦æŸï¼Œä»è€Œåœ¨å­˜åœ¨åé¦ˆå›è·¯çš„æƒ…å†µä¸‹æ¨æ–­å› æœè°ƒæ§å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCALDåœ¨æ¨æ–­è½¬å½•è°ƒæ§ç½‘ç»œå’Œä¿¡å·è½¬å¯¼ç½‘ç»œæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«åé¦ˆè°ƒæ§æ–¹é¢å…·æœ‰ä¸å¯æ›¿ä»£çš„ä¼˜åŠ¿ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è½¬å½•å› å­(TF)æ‰°åŠ¨æ•°æ®åˆ†æéªŒè¯äº†SCALDçš„å‡†ç¡®æ€§å’Œçµæ•åº¦ï¼Œå¹¶é€šè¿‡ChIP-seqæ•°æ®åˆ†æç¡®è®¤äº†å…¶å‘ç°çš„æœªçŸ¥è°ƒæ§å…³ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨SCALDåˆ†æäº†ç»“è‚ ç‚ç—‡å‘ç™Œç—‡è½¬åŒ–è¿‡ç¨‹ä¸­çš„è°ƒæ§ç½‘ç»œåŠ¨æ€å˜åŒ–ï¼ŒæˆåŠŸæ¨æ–­å‡ºä¿ƒè¿›è¿™ä¸€è½¬åŒ–çš„å…³é”®é©±åŠ¨åŸºå› ã€‚",
      "categories": [
        "q-bio.MN",
        "cs.AI"
      ],
      "primary_category": "q-bio.MN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02332v1",
      "published_date": "2025-11-04 07:38:02 UTC",
      "updated_date": "2025-11-04 07:38:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:56:09.612159+00:00"
    },
    {
      "arxiv_id": "2512.16925v2",
      "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models",
      "title_zh": "V-Agentï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼è§†é¢‘æœç´¢ç³»ç»Ÿ",
      "authors": [
        "SunYoung Park",
        "Jong-Hyeon Lee",
        "Youngjune Kim",
        "Daegyu Sung",
        "Younghyun Yu",
        "Young-rok Cha",
        "Jeongho Ju"
      ],
      "abstract": "We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications. The retrieval model and demo videos are available at https://huggingface.co/NCSOFT/multimodal-embedding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†V-Agentï¼Œä¸€ç§æ—¨åœ¨å®ç°é«˜çº§è§†é¢‘æœç´¢å’Œäº¤äº’å¼å¯¹è¯çš„æ–°å‹å¤šæ™ºèƒ½ä½“å¹³å°ã€‚é€šè¿‡åˆ©ç”¨å°å‹è§†é¢‘åå¥½æ•°æ®é›†å¾®è°ƒvision-language model (VLM)å¹¶ç»“åˆimage-text retrieval modelçš„æ£€ç´¢å‘é‡ï¼Œè¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿæ–‡æœ¬æ£€ç´¢åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚V-Agentå°†è§†é¢‘å¸§å’Œç»automatic speech recognition (ASR)å¤„ç†çš„éŸ³é¢‘è½¬å½•åµŒå…¥å…±äº«çš„å¤šæ¨¡æ€è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œå®ç°å¯¹è§†è§‰å’Œè¯­éŸ³å†…å®¹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç†è§£ã€‚è¯¥ç³»ç»ŸåŒ…å«routing agentã€search agentå’Œchat agentä¸‰ä¸ªååŒå·¥ä½œçš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ä¼˜åŒ–æœç´¢ç»“æœå’Œä¸ç”¨æˆ·äº¤äº’æ¥ç²¾å‡†å“åº”ç”¨æˆ·æ„å›¾ã€‚å…¶ä¸­ï¼Œsearch agentç»“åˆåŸºäºVLMçš„æ£€ç´¢æ¨¡å‹ä¸é¢å¤–çš„re-rankingæ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ£€ç´¢è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨MultiVENT 2.0åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„zero-shotæ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨å­¦æœ¯å’Œå®é™…åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "CIKM 2025 MMGENSR Workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.16925v2",
      "published_date": "2025-11-04 07:24:45 UTC",
      "updated_date": "2026-01-07 06:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:59:17.898135+00:00"
    },
    {
      "arxiv_id": "2511.02309v1",
      "title": "The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute",
      "title_zh": "åºåˆ—ä¼˜åŠ¿ï¼šåŒç­‰ç®—åŠ›ä¸‹é€†ç†µæŠ•ç¥¨èƒœè¿‡å¹¶è¡Œè‡ªæ´½æ€§",
      "authors": [
        "Aman Sharma",
        "Paras Chopra"
      ],
      "abstract": "We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é‡æ–°å®¡è§†äº†è¯­è¨€æ¨¡å‹æ¨ç†çš„æµ‹è¯•æ—¶æ‰©å±•(test-time scaling)é—®é¢˜ï¼Œæ¢è®¨åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ï¼Œå¹¶è¡Œè¿è¡Œå¤šä¸ªç‹¬ç«‹é“¾ä¸é€šè¿‡é¡ºåºæ­¥éª¤è¿­ä»£ä¼˜åŒ–çš„ä¼˜åŠ£ã€‚é€šè¿‡åœ¨5ä¸ªæœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹å’Œ3ä¸ªé«˜éš¾åº¦æ¨ç†åŸºå‡†ä¸Šçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ï¼ŒåŸºäºå…ˆå‰å°è¯•æ„å»ºçš„é¡ºåºæ‰©å±•(sequential scaling)åœ¨95.6%çš„é…ç½®ä¸­ä¼˜äºä¸»æµçš„å¹¶è¡Œè‡ªæ´½æ€§(parallel self-consistency)èŒƒå¼ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾46.7%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†é€†ç†µåŠ æƒæŠ•ç¥¨(inverse-entropy weighted voting)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°é¢–æ–¹æ³•ï¼Œé€šè¿‡æ ¹æ®æ¨ç†é“¾çš„é€†ç†µå¯¹ç­”æ¡ˆè¿›è¡ŒåŠ æƒï¼Œè¿›ä¸€æ­¥æé«˜äº†é¡ºåºæ‰©å±•çš„å‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•åœ¨æˆåŠŸç‡ä¸Šè¶…è¿‡äº†å¹¶è¡Œå¤šæ•°æŠ•ç¥¨ï¼Œç¡®ç«‹äº†å…¶ä½œä¸ºæœ€ä½³æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥çš„åœ°ä½ã€‚è¿™äº›å‘ç°ä»æ ¹æœ¬ä¸ŠæŒ‘æˆ˜äº†è‡ªWangç­‰äººæå‡ºè‡ªæ´½æ€§è§£ç ä»¥æ¥çš„å¹¶è¡Œæ¨ç†æ­£ç»Ÿè§‚å¿µï¼Œè¡¨æ˜é¡ºåºç²¾ç‚¼(sequential refinement)åº”ä½œä¸ºç°ä»£LLMæ¨ç†çš„ç¨³å¥é»˜è®¤è®¾ç½®ï¼Œå¹¶å‘¼åæ¨æ–­æ—¶ä¼˜åŒ–èŒƒå¼çš„è½¬å˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02309v1",
      "published_date": "2025-11-04 06:48:34 UTC",
      "updated_date": "2025-11-04 06:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T08:59:44.128275+00:00"
    },
    {
      "arxiv_id": "2511.02304v1",
      "title": "Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning",
      "title_zh": "è‡ªåŠ¨æœºæ¡ä»¶åä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Beyazit Yalcinkaya",
        "Marcell Vazquez-Chanlatte",
        "Ameesh Shah",
        "Hanna Krasowski",
        "Sanjit A. Seshia"
      ],
      "abstract": "We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†ä¸­è®­ç»ƒåˆ†æ•£æ‰§è¡Œï¼ˆCTDEï¼‰è®¾å®šä¸‹çš„åˆä½œæ€§æ—¶é—´ç›®æ ‡ï¼Œæ¢è®¨äº†å¤šä»»åŠ¡ã€å¤šæ™ºèƒ½ä½“ç­–ç•¥çš„å­¦ä¹ é—®é¢˜ã€‚å°½ç®¡åˆ©ç”¨è‡ªåŠ¨æœºï¼ˆAutomataï¼‰å¯ä»¥å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯åˆ†é…ç»™æ™ºèƒ½ä½“çš„ç®€å•å­ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€æ ·æœ¬æ•ˆç‡ä½ä¸‹ä¸”å±€é™äºå•ä»»åŠ¡åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†è‡ªåŠ¨æœºæ¡ä»¶ä¸‹çš„åˆä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆACC-MARLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ ä»»åŠ¡æ¡ä»¶åŒ–çš„å»ä¸­å¿ƒåŒ–å›¢é˜Ÿç­–ç•¥ã€‚ä½œè€…è¯†åˆ«äº†å®ç°ACC-MARLçš„ä¸»è¦æŒ‘æˆ˜ï¼Œæå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜å­¦ä¹ ç­–ç•¥çš„ä»·å€¼å‡½æ•°å¯ç”¨äºåœ¨æµ‹è¯•æ—¶è¿›è¡Œæœ€ä¼˜çš„ä»»åŠ¡åˆ†é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¶Œç°å‡ºä»»åŠ¡æ„ŸçŸ¥çš„å¤šæ­¥åä½œè¡Œä¸ºï¼Œå¦‚æŒ‰æŒ‰é’®è§£é”é—¨ã€ä¿æŒé—¨å¼€å¯ä»¥åŠç®€åŒ–ä»»åŠ¡æµç¨‹ç­‰å¤æ‚çš„åä½œæ“ä½œã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02304v1",
      "published_date": "2025-11-04 06:37:36 UTC",
      "updated_date": "2025-11-04 06:37:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:00:05.581910+00:00"
    },
    {
      "arxiv_id": "2511.02303v1",
      "title": "Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation",
      "title_zh": "é‡Šæ”¾å¤šæ™ºèƒ½ä½“ LLM çš„æ¨ç†æ½œåŠ›ï¼šä»æ‡’æƒ°æ™ºèƒ½ä½“åˆ°æ·±æ€ç†Ÿè™‘",
      "authors": [
        "Zhiwei Zhang",
        "Xiaomin Li",
        "Yudi Lin",
        "Hui Liu",
        "Ramraj Chandradevan",
        "Linlin Wu",
        "Minhua Lin",
        "Fali Wang",
        "Xianfeng Tang",
        "Qi He",
        "Suhang Wang"
      ],
      "abstract": "Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(Multi-Agent LLM)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å‡ºç°çš„â€œæ‡’æƒ°æ™ºèƒ½ä½“è¡Œä¸ºâ€(lazy agent behavior)é—®é¢˜ï¼Œå³ä¸€æ–¹ä¸»å¯¼è€Œå¦ä¸€æ–¹è´¡çŒ®ç”šå¾®å¯¼è‡´åä½œå¤±æ•ˆçš„ç°è±¡ï¼Œæå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ä½œè€…é¦–å…ˆæä¾›äº†ç†è®ºåˆ†æï¼Œè§£é‡Šäº†ä¸ºä½•è¿™ç§æ‡’æƒ°è¡Œä¸ºåœ¨æ¶‰åŠå…ƒæ€è€ƒæ™ºèƒ½ä½“(meta-thinking agent)å’Œæ¨ç†æ™ºèƒ½ä½“(reasoning agent)çš„è®¾ç½®ä¸­è‡ªç„¶äº§ç”Ÿã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ç§ç¨³å®šä¸”é«˜æ•ˆçš„å› æœå½±å“(causal influence)æµ‹é‡æ–¹æ³•æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ¨ç†æ™ºèƒ½ä½“åœ¨å¤šè½®äº¤äº’ä¸­å®¹æ˜“è¿·å¤±æˆ–å—å›°äºå™ªå£°å“åº”çš„é£é™©ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯éªŒè¯å¥–åŠ±(verifiable reward)çš„æœºåˆ¶æ¥é¼“åŠ±æ·±æ€ç†Ÿè™‘(deliberation)ã€‚è¯¥æœºåˆ¶å…è®¸æ¨ç†æ™ºèƒ½ä½“ä¸¢å¼ƒå™ªå£°è¾“å‡ºã€æ•´åˆæŒ‡ä»¤å¹¶åœ¨å¿…è¦æ—¶é‡å¯æ¨ç†è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆç¼“è§£äº†æ‡’æƒ°æ™ºèƒ½ä½“è¡Œä¸ºï¼Œé‡Šæ”¾äº†å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„å…¨éƒ¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02303v1",
      "published_date": "2025-11-04 06:37:31 UTC",
      "updated_date": "2025-11-04 06:37:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:00:31.543120+00:00"
    },
    {
      "arxiv_id": "2511.02302v1",
      "title": "FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error",
      "title_zh": "FP8-Flow-MoEï¼šæ— åŒé‡é‡åŒ–è¯¯å·®çš„å…è½¬æ¢ FP8 æ–¹æ¡ˆ",
      "authors": [
        "Fengjuan Wang",
        "Zhiyi Su",
        "Xingzhu Hu",
        "Cheng Wang",
        "Mou Sun"
      ],
      "abstract": "Training large Mixture-of-Experts (MoE) models remains computationally prohibitive due to their extreme compute and memory demands. Although low-precision training promises to accelerate computation and reduce memory footprint, existing implementations still rely on BF16-dominated dataflows with frequent quantize-dequantize (Q/DQ) conversions. These redundant casts erode much of FP8's theoretical efficiency. However, naively removing these casts by keeping dataflows entirely in FP8 introduces double quantization error: tensors quantized along different dimensions accumulate inconsistent scaling factors, degrading numerical stability.\n  We propose FP8-Flow-MoE, an FP8 training recipe featuring a quantization-consistent FP8-centric dataflow with a scaling-aware transpose and fused FP8 operators that streamline computation and eliminate explicit cast operations from 12 to 2. Evaluations on a 671B-parameter MoE model demonstrate up to 21\\% higher throughput and 16.5 GB lower memory usage per GPU compared to BF16 and naÃ¯ve FP8 baselines, while maintaining stable convergence. We provide a plug-and-play FP8 recipe compatible with TransformerEngine and Megatron-LM, which will be open-sourced soon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹Mixture-of-Experts (MoE)æ¨¡å‹è®­ç»ƒä¸­æé«˜çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œæå‡ºäº†FP8-Flow-MoEï¼Œä¸€ç§æ—¨åœ¨æ¶ˆé™¤double quantization errorçš„æ— è½¬æ¢FP8è®­ç»ƒé…æ–¹ã€‚ç°æœ‰çš„ä½ç²¾åº¦æ–¹æ¡ˆå¾€å¾€ä¾èµ–BF16ä¸»å¯¼çš„æ•°æ®æµå’Œé¢‘ç¹çš„é‡åŒ–-åé‡åŒ–(Q/DQ)è½¬æ¢ï¼Œè¿™å‰Šå¼±äº†FP8çš„æ•ˆç‡ï¼Œè€Œç®€å•åœ°ç§»é™¤è¿™äº›è½¬æ¢ä¼šå› ç»´åº¦ç¼©æ”¾ä¸ä¸€è‡´å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚FP8-Flow-MoEé€šè¿‡å¼•å…¥scaling-aware transposeå’Œèåˆçš„FP8ç®—å­ï¼Œæ„å»ºäº†ä»¥FP8ä¸ºä¸­å¿ƒçš„é‡åŒ–ä¸€è‡´æ€§æ•°æ®æµï¼ŒæˆåŠŸå°†æ˜¾å¼è½¬æ¢æ“ä½œä»12æ¬¡å‡å°‘è‡³2æ¬¡ã€‚åœ¨671Bå‚æ•°çš„MoEæ¨¡å‹ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸BF16å’Œæœ´ç´ FP8åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è¾¾21%çš„ååé‡æå‡ï¼Œå¹¶å°†æ¯GPUå†…å­˜å ç”¨é™ä½äº†16.5 GBï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å®šçš„æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå…¼å®¹TransformerEngineå’ŒMegatron-LMï¼Œä¸ºå¤§æ¨¡å‹è®­ç»ƒæä¾›äº†é«˜æ•ˆçš„å³æ’å³ç”¨FP8è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02302v1",
      "published_date": "2025-11-04 06:36:59 UTC",
      "updated_date": "2025-11-04 06:36:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:00:57.048704+00:00"
    },
    {
      "arxiv_id": "2511.02301v1",
      "title": "Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series",
      "title_zh": "é¢å‘å¤šå˜é‡ç‰©è”ç½‘æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„è”é‚¦é‡å­æ ¸å­¦ä¹ ",
      "authors": [
        "Kuan-Cheng Chen",
        "Samuel Yen-Chi Chen",
        "Chen-Yu Liu",
        "Kin K. Leung"
      ],
      "abstract": "The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å·¥ä¸šç‰©è”ç½‘(IIoT)ç³»ç»Ÿä¸­é«˜ç»´å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹é¢ä¸´çš„éšç§ã€æ‰©å±•æ€§å’Œé€šä¿¡æ•ˆç‡æŒ‘æˆ˜ï¼Œæå‡ºäº†è”é‚¦é‡å­æ ¸å­¦ä¹ (Federated Quantum Kernel Learning, FQKL)æ¡†æ¶ã€‚é’ˆå¯¹ç»å…¸è”é‚¦å­¦ä¹ éš¾ä»¥å¤„ç†é«˜åº¦éçº¿æ€§å†³ç­–è¾¹ç•Œå’Œä¸å¹³è¡¡å¼‚å¸¸åˆ†å¸ƒçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†é‡å­ç‰¹å¾æ˜ å°„(quantum feature maps)ä¸è”é‚¦èšåˆç›¸ç»“åˆã€‚åœ¨å…·ä½“å®ç°ä¸­ï¼Œé‡å­è¾¹ç¼˜èŠ‚ç‚¹åˆ©ç”¨å‚æ•°åŒ–é‡å­ç”µè·¯(parameterized quantum circuits)æœ¬åœ°è®¡ç®—å‹ç¼©æ ¸ç»Ÿè®¡æ•°æ®ï¼Œä»…å‘ä¸­å¿ƒæœåŠ¡å™¨å…±äº«æ‘˜è¦ä¿¡æ¯ä»¥æ„å»ºå…¨å±€GramçŸ©é˜µå¹¶è®­ç»ƒå†³ç­–æ¨¡å‹ï¼ˆå¦‚Fed-QSVMï¼‰ã€‚è¿™ç§æ–¹æ³•åœ¨å®ç°åˆ†å¸ƒå¼ã€éšç§ä¿æŠ¤çš„å¼‚å¸¸æ£€æµ‹çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº†é€šä¿¡å¼€é”€ã€‚åœ¨åˆæˆIIoTåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFQKLåœ¨æ•æ‰å¤æ‚æ—¶é—´ç›¸å…³æ€§æ–¹é¢æ¯”ç»å…¸è”é‚¦åŸºçº¿è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å·¥ä½œçªæ˜¾äº†é‡å­æ ¸åœ¨è”é‚¦è®¾ç½®ä¸­çš„åº”ç”¨å‰æ™¯ï¼Œä¸ºä¸‹ä¸€ä»£ç‰©è”ç½‘åŸºç¡€è®¾æ–½è¿ˆå‘å¯æ‰©å±•ã€é²æ£’çš„é‡å­å¢å¼ºæ™ºèƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02301v1",
      "published_date": "2025-11-04 06:35:53 UTC",
      "updated_date": "2025-11-04 06:35:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:01:20.203392+00:00"
    },
    {
      "arxiv_id": "2511.07445v1",
      "title": "A Preliminary Study of RAG for Taiwanese Historical Archives",
      "title_zh": "é¢å‘å°æ¹¾å†å²æ¡£æ¡ˆçš„RAGåˆæ­¥ç ”ç©¶",
      "authors": [
        "Claire Lin",
        "Bo-Han Feng",
        "Xuanjun Chen",
        "Te-Lun Yang",
        "Hung-yi Lee",
        "Jyh-Shing Roger Jang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å°æ¹¾å†å²æ¡£æ¡ˆåº”ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼ˆRAGï¼‰è¿›è¡Œäº†åˆæ­¥æ¢è®¨ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚ä½œè€…åœ¨ã€Šçƒ­å…°é®åŸæ—¥å¿—ã€‹ï¼ˆFort Zeelandiaï¼‰å’Œã€Šå°æ¹¾çœè®®ä¼šå…¬æŠ¥ã€‹ï¼ˆTaiwan Provincial Council Gazetteï¼‰è¿™ä¸¤ä¸ªç¹ä½“ä¸­æ–‡å†å²æ•°æ®é›†ä¸Šéƒ¨ç½²äº†RAGæµç¨‹ï¼Œå¹¶é…åˆå¼€æ”¾å¼æŸ¥è¯¢é›†è¿›è¡Œäº†æµ‹è¯•ã€‚ç ”ç©¶ç³»ç»Ÿæ€§åœ°åˆ†æäº†æŸ¥è¯¢ç‰¹å¾åŠå…ƒæ•°æ®ï¼ˆmetadataï¼‰é›†æˆç­–ç•¥å¯¹æ£€ç´¢è´¨é‡ã€ç­”æ¡ˆç”ŸæˆåŠæ•´ä½“ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ—©æœŸé˜¶æ®µçš„å…ƒæ•°æ®é›†æˆèƒ½æœ‰æ•ˆæå‡æ£€ç´¢ä¸å›ç­”çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿæ­ç¤ºäº†RAGç³»ç»Ÿåœ¨å¤„ç†æ­¤ç±»æ¡£æ¡ˆæ—¶ä»é¢ä¸´æŒç»­æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰ï¼ˆhallucinationsï¼‰é—®é¢˜ï¼Œä»¥åŠåœ¨å¤„ç†æ¶‰åŠæ—¶é—´æ€§æˆ–å¤šè·³ï¼ˆmulti-hopï¼‰å†å²æŸ¥è¯¢æ—¶çš„å›°éš¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ROCLING 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.07445v1",
      "published_date": "2025-11-04 06:28:50 UTC",
      "updated_date": "2025-11-04 06:28:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:01:46.690058+00:00"
    },
    {
      "arxiv_id": "2511.02875v1",
      "title": "Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids",
      "title_zh": "å­¦æœ¯ç•Œä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šæ”¿ç­–ä¸å®è·µè„±èŠ‚çš„å®è¯ä¸è®¤è¯†è®ºæŒ‡æ ‡",
      "authors": [
        "R. Yamamoto Ravenor"
      ],
      "abstract": "As generative AI diffuses through academia, policy-practice divergence becomes consequential, creating demand for auditable indicators of alignment. This study prototypes a ten-item, indirect-elicitation instrument embedded in a structured interpretive framework to surface voids between institutional rules and practitioner AI use. The framework extracts empirical and epistemic signals from academics, yielding three filtered indicators of such voids: (1) AI-integrated assessment capacity (proxy) - within a three-signal screen (AI skill, perceived teaching benefit, detection confidence), the share who would fully allow AI in exams; (2) sector-level necessity (proxy) - among high output control users who still credit AI with high contribution, the proportion who judge AI capable of challenging established disciplines; and (3) ontological stance - among respondents who judge AI different in kind from prior tools, report practice change, and pass a metacognition gate, the split between material and immaterial views as an ontological map aligning procurement claims with evidence classes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼AIï¼ˆGenerative AIï¼‰åœ¨å­¦æœ¯ç•Œæ™®åŠè¿‡ç¨‹ä¸­å‡ºç°çš„æ”¿ç­–ä¸å®è·µè„±èŠ‚é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºå®¡è®¡ä¸¤è€…ä¸€è‡´æ€§çš„æŒ‡æ ‡ä½“ç³»ã€‚ç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŒ…å«åä¸ªé¡¹ç›®çš„é—´æ¥å¼•å‡ºå·¥å…·ï¼ˆindirect-elicitation instrumentï¼‰ï¼Œå¹¶å°†å…¶åµŒå…¥ç»“æ„åŒ–è§£é‡Šæ¡†æ¶ä¸­ï¼Œæ—¨åœ¨æ­ç¤ºåˆ¶åº¦è§„åˆ™ä¸å®é™…AIä½¿ç”¨ä¹‹é—´çš„ç©ºç™½ï¼ˆvoidsï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å­¦è€…é‚£é‡Œæå–ç»éªŒå’Œè®¤è¯†è®ºä¿¡å·ï¼Œç¡®å®šäº†ä¸‰ä¸ªå…³é”®çš„è¿‡æ»¤æŒ‡æ ‡ã€‚ç¬¬ä¸€ä¸ªæ˜¯AIæ•´åˆè¯„ä¼°èƒ½åŠ›ï¼ˆAI-integrated assessment capacityï¼‰ï¼Œåæ˜ äº†åœ¨å…·å¤‡AIæŠ€èƒ½å’Œæ£€æµ‹ä¿¡å¿ƒèƒŒæ™¯ä¸‹ï¼Œå®Œå…¨å…è®¸åœ¨è€ƒè¯•ä¸­ä½¿ç”¨AIçš„æ¯”ä¾‹ã€‚ç¬¬äºŒä¸ªæ˜¯è¡Œä¸šå±‚é¢çš„å¿…è¦æ€§ï¼ˆsector-level necessityï¼‰ï¼Œå³åœ¨é«˜äº§å‡ºæ§åˆ¶ç”¨æˆ·ä¸­ï¼Œè®¤ä¸ºAIæœ‰èƒ½åŠ›æŒ‘æˆ˜æ—¢å®šå­¦ç§‘çš„æ¯”ä¾‹ã€‚ç¬¬ä¸‰ä¸ªæ˜¯æœ¬ä½“è®ºç«‹åœºï¼ˆontological stanceï¼‰ï¼Œç”¨äºåˆ†æç”¨æˆ·å¯¹AIæœ¬è´¨çš„ç‰©è´¨ä¸éç‰©è´¨è§‚ç‚¹ï¼Œä»¥å°†é‡‡è´­ä¸»å¼ ä¸è¯æ®ç±»åˆ«å¯¹é½ã€‚è¿™äº›æŒ‡æ ‡ä¸ºç†è§£å’Œç¼©å°å­¦æœ¯ç•Œåˆ¶åº¦è§„åˆ™ä¸ä»ä¸šè€…å®é™…æ“ä½œä¹‹é—´çš„å·®è·æä¾›äº†å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 2 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.02875v1",
      "published_date": "2025-11-04 06:24:47 UTC",
      "updated_date": "2025-11-04 06:24:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:02:43.548488+00:00"
    },
    {
      "arxiv_id": "2511.02290v1",
      "title": "From data to design: Random forest regression model for predicting mechanical properties of alloy steel",
      "title_zh": "ä»æ•°æ®åˆ°è®¾è®¡ï¼šé¢„æµ‹åˆé‡‘é’¢åŠ›å­¦æ€§èƒ½çš„éšæœºæ£®æ—å›å½’æ¨¡å‹",
      "authors": [
        "Samjukta Sinha",
        "Prabhat Das"
      ],
      "abstract": "This study investigates the application of Random Forest Regression for predicting mechanical properties of alloy steel-Elongation, Tensile Strength, and Yield Strength-from material composition features including Iron (Fe), Chromium (Cr), Nickel (Ni), Manganese (Mn), Silicon (Si), Copper (Cu), Carbon (C), and deformation percentage during cold rolling. Utilizing a dataset comprising these features, we trained and evaluated the Random Forest model, achieving high predictive performance as evidenced by R2 scores and Mean Squared Errors (MSE). The results demonstrate the model's efficacy in providing accurate predictions, which is validated through various performance metrics including residual plots and learning curves. The findings underscore the potential of ensemble learning techniques in enhancing material property predictions, with implications for industrial applications in material science.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åº”ç”¨éšæœºæ£®æ—å›å½’(Random Forest Regression)æ¨¡å‹æ¥é¢„æµ‹åˆé‡‘é’¢çš„æœºæ¢°æ€§èƒ½ï¼Œå…·ä½“åŒ…æ‹¬å»¶ä¼¸ç‡(Elongation)ã€æŠ—æ‹‰å¼ºåº¦(Tensile Strength)å’Œå±ˆæœå¼ºåº¦(Yield Strength)ã€‚æ¨¡å‹åˆ©ç”¨çš„è¾“å…¥ç‰¹å¾æ¶µç›–äº†é“(Fe)ã€é“¬(Cr)ã€é•(Ni)ã€é”°(Mn)ã€ç¡…(Si)ã€é“œ(Cu)ã€ç¢³(C)ç­‰ææ–™æˆåˆ†ä»¥åŠå†·è½§è¿‡ç¨‹ä¸­çš„å˜å½¢ç™¾åˆ†æ¯”ã€‚é€šè¿‡åœ¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œè¯¥æ¨¡å‹å–å¾—äº†å¾ˆé«˜çš„é¢„æµ‹æ€§èƒ½ï¼Œè¿™ä¸€ç‚¹é€šè¿‡R2åˆ†æ•°å’Œå‡æ–¹è¯¯å·®(MSE)å¾—åˆ°äº†è¯å®ã€‚æ®‹å·®å›¾å’Œå­¦ä¹ æ›²çº¿ç­‰å¤šç§æ€§èƒ½æŒ‡æ ‡è¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹æä¾›å‡†ç¡®é¢„æµ‹çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é›†æˆå­¦ä¹ æŠ€æœ¯åœ¨å¢å¼ºææ–™æ€§èƒ½é¢„æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œå¯¹ææ–™ç§‘å­¦çš„å·¥ä¸šåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02290v1",
      "published_date": "2025-11-04 06:10:26 UTC",
      "updated_date": "2025-11-04 06:10:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:02:45.714042+00:00"
    },
    {
      "arxiv_id": "2511.05565v1",
      "title": "In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy",
      "title_zh": "é¢å‘å…‰å­¦æ˜¾å¾®é•œå°æ ·æœ¬ç»†èƒæ£€æµ‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡é€‚åº”",
      "authors": [
        "Shreyan Ganguly",
        "Angona Biswas",
        "Jaydeep Rade",
        "Md Hasibul Hasan Hasib",
        "Nabila Masud",
        "Nitish Singla",
        "Abhipsa Dash",
        "Ushashi Bhattacharjee",
        "Aditya Balu",
        "Anwesha Sarkar",
        "Adarsh Krishnamurthy",
        "Soumik Sarkar"
      ],
      "abstract": "Foundation vision-language models (VLMs) excel on natural images, but their utility for biomedical microscopy remains underexplored. In this paper, we investigate how in-context learning enables state-of-the-art VLMs to perform few-shot object detection when large annotated datasets are unavailable, as is often the case with microscopic images. We introduce the Micro-OD benchmark, a curated collection of 252 images specifically curated for in-context learning, with bounding-box annotations spanning 11 cell types across four sources, including two in-lab expert-annotated sets. We systematically evaluate eight VLMs under few-shot conditions and compare variants with and without implicit test-time reasoning tokens. We further implement a hybrid Few-Shot Object Detection (FSOD) pipeline that combines a detection head with a VLM-based few-shot classifier, which enhances the few-shot performance of recent VLMs on our benchmark. Across datasets, we observe that zero-shot performance is weak due to the domain gap; however, few-shot support consistently improves detection, with marginal gains achieved after six shots. We observe that models with reasoning tokens are more effective for end-to-end localization, whereas simpler variants are more suitable for classifying pre-localized crops. Our results highlight in-context adaptation as a practical path for microscopy, and our benchmark provides a reproducible testbed for advancing open-vocabulary detection in biomedical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)ä½¿è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å…‰å­¦æ˜¾å¾®é•œå›¾åƒä¸­æ‰§è¡Œå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ã€‚è®ºæ–‡æå‡ºäº†Micro-ODåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¶µç›–11ç§ç»†èƒç±»å‹çš„252å¼ å›¾åƒï¼Œä¸“é—¨ç”¨äºè¯„ä¼°ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿç³»ç»Ÿè¯„ä¼°äº†8ç§VLMsï¼Œå¹¶å®æ–½äº†ä¸€ç§æ··åˆå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹(FSOD)æµç¨‹ï¼Œç»“åˆäº†æ£€æµ‹å¤´ä¸åŸºäºVLMçš„å°‘æ ·æœ¬åˆ†ç±»å™¨ä»¥å¢å¼ºæ¨¡å‹è¡¨ç°ã€‚å®éªŒè§‚å¯Ÿåˆ°ï¼Œç”±äºé¢†åŸŸå·®å¼‚ï¼Œé›¶æ ·æœ¬(zero-shot)æ€§èƒ½è¾ƒå¼±ï¼Œä½†å°‘æ ·æœ¬æ”¯æŒèƒ½æŒç»­æå‡æ£€æµ‹æ•ˆæœï¼Œå¹¶åœ¨æä¾›6ä¸ªæ ·æœ¬åå¢ç›Šè¶‹äºå¹³ç¨³ã€‚æ­¤å¤–ï¼Œå…·æœ‰æ¨ç†tokençš„æ¨¡å‹åœ¨ç«¯åˆ°ç«¯å®šä½æ–¹é¢æ›´æœ‰æ•ˆï¼Œè€Œè¾ƒç®€å•çš„å˜ä½“åˆ™æ›´é€‚åˆå¯¹é¢„å®šä½çš„è£å‰ªå›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è¯¥ç ”ç©¶è¡¨æ˜ä¸Šä¸‹æ–‡é€‚åº”æ˜¯æ˜¾å¾®é•œå›¾åƒåˆ†æçš„ä¸€æ¡å®ç”¨è·¯å¾„ï¼Œä¸ºæ¨è¿›ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„å¼€æ”¾è¯æ±‡æ£€æµ‹æä¾›äº†é‡è¦çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05565v1",
      "published_date": "2025-11-04 06:06:02 UTC",
      "updated_date": "2025-11-04 06:06:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:03:22.668850+00:00"
    },
    {
      "arxiv_id": "2511.02263v3",
      "title": "LA-MARRVEL: A Knowledge-Grounded and Language-Aware LLM Reranker for AI-MARRVEL in Rare Disease Diagnosis",
      "title_zh": "LA-MARRVELï¼šé¢å‘ç½•è§ç—…è¯Šæ–­ AI-MARRVEL çš„åŸºäºçŸ¥è¯†ä¸è¯­è¨€æ„ŸçŸ¥çš„å¤§è¯­è¨€æ¨¡å‹é‡æ’åºå™¨",
      "authors": [
        "Jaeyeon Lee",
        "Hyun-Hwan Jeong",
        "Zhandong Liu"
      ],
      "abstract": "Diagnosing rare diseases requires linking gene findings with often unstructured reference text. Current pipelines collect many candidate genes, but clinicians still spend a lot of time filtering false positives and combining evidence from papers and databases. A key challenge is language: phenotype descriptions and inheritance patterns are written in prose, not fully captured by tables. Large language models (LLMs) can read such text, but clinical use needs grounding in citable knowledge and stable, repeatable behavior. We explore a knowledge-grounded and language-aware reranking layer on top of a high-recall first-stage pipeline. The goal is to improve precision and explainability, not to replace standard bioinformatics steps. We use expert-built context and a consensus method to reduce LLM variability, producing shorter, better-justified gene lists for expert review. LA-MARRVEL achieves the highest accuracy, outperforming other methods -- including traditional bioinformatics diagnostic tools (AI-MARRVEL, Exomiser, LIRICAL) and naive large language models (e.g., Anthropic Claude) -- with an average Recall@5 of 94.10%, a +3.65 percentage-point improvement over AI-MARRVEL. The LLM-generated reasoning provides clear prose on phenotype matching and inheritance patterns, making clinical review faster and easier. LA-MARRVEL has three parts: expert-engineered context that enriches phenotype and disease information; a ranked voting algorithm that combines multiple LLM runs to choose a consensus ranked gene list; and the AI-MARRVEL pipeline that provides first-stage ranks and gene annotations, already known as a state-of-the-art method in Rare Disease Diagnosis on BG, DDD, and UDN cohorts. The online AI-MARRVEL includes LA-MARRVEL as an LLM feature at https://ai.marrvel.org . We evaluate LA-MARRVEL on three datasets from independent cohorts of real-world diagnosed patients.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LA-MARRVELï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„çŸ¥è¯†å¢å¼ºä¸è¯­è¨€æ„ŸçŸ¥é‡æ’åºå™¨ï¼Œæ—¨åœ¨è§£å†³ç½•è§ç—…è¯Šæ–­ä¸­ä¸´åºŠåŒ»ç”Ÿéœ€è€—æ—¶ç­›é€‰éç»“æ„åŒ–æ–‡æœ¬è¯æ®çš„éš¾é¢˜ã€‚è¯¥æ¨¡å‹ä½œä¸ºAI-MARRVELç®¡é“ä¹‹ä¸Šçš„é‡æ’åºå±‚ï¼Œåˆ©ç”¨ä¸“å®¶æ„å»ºçš„ä¸Šä¸‹æ–‡æ¥ä¸°å¯Œè¡¨å‹å’Œç–¾ç—…ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ’åæŠ•ç¥¨ç®—æ³•(ranked voting algorithm)æ•´åˆå¤šæ¬¡LLMè¿è¡Œç»“æœä»¥è¾¾æˆå…±è¯†ï¼Œä»è€Œå‡å°‘æ¨¡å‹å˜å¼‚æ€§ã€‚LA-MARRVELå¹¶éå–ä»£æ ‡å‡†ç”Ÿç‰©ä¿¡æ¯å­¦æ­¥éª¤ï¼Œè€Œæ˜¯é€šè¿‡ç†è§£è¡¨å‹æè¿°å’Œé—ä¼ æ¨¡å¼çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œæä¾›æ›´ç²¾å‡†ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„åŸºå› åˆ—è¡¨ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ‚£è€…é˜Ÿåˆ—æ•°æ®é›†çš„è¯„ä¼°ä¸­ï¼ŒLA-MARRVELå±•ç°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œå¹³å‡Recall@5è¾¾åˆ°94.10%ï¼Œæ¯”AI-MARRVELæé«˜äº†3.65ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¼˜äºExomiserã€LIRICALç­‰ä¼ ç»Ÿå·¥å…·åŠæœ´ç´ LLMæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬æ¸…æ™°é˜è¿°äº†è¡¨å‹åŒ¹é…å’Œé—ä¼ æ¨¡å¼ï¼Œæ˜¾è‘—åŠ é€Ÿå¹¶ç®€åŒ–äº†ä¸´åºŠå®¡æŸ¥è¿‡ç¨‹ï¼Œç›®å‰å·²é›†æˆè‡³åœ¨çº¿AI-MARRVELå¹³å°ä¸­ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02263v3",
      "published_date": "2025-11-04 05:17:41 UTC",
      "updated_date": "2025-11-06 03:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:06:33.799966+00:00"
    },
    {
      "arxiv_id": "2511.02254v1",
      "title": "Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint",
      "title_zh": "è§„æ¨¡çº¦æŸä¸‹éå•è°ƒDR-å­æ¨¡æœ€å¤§åŒ–çš„å¿«é€Ÿè¿‘ä¼¼ç®—æ³•",
      "authors": [
        "Tan D. Tran",
        "Canh V. Pham"
      ],
      "abstract": "This work studies the non-monotone DR-submodular Maximization over a ground set of $n$ subject to a size constraint $k$. We propose two approximation algorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub offers an approximation ratio of $0.044$ with query complexity of $O(n \\log(k))$. The second one, FastDrSub++, improves upon it with a ratio of $1/4-Îµ$ within query complexity of $(n \\log k)$ for an input parameter $Îµ>0$. Therefore, our proposed algorithms are the first constant-ratio approximation algorithms for the problem with the low complexity of $O(n \\log(k))$.\n  Additionally, both algorithms are experimentally evaluated and compared against existing state-of-the-art methods, demonstrating their effectiveness in solving the Revenue Maximization problem with DR-submodular objective function. The experimental results show that our proposed algorithms significantly outperform existing approaches in terms of both query complexity and solution quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å°çº¦æŸ $k$ ä¸‹çš„éå•è°ƒ DR-submodular æœ€å¤§åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§è¿‘ä¼¼ç®—æ³•ï¼šFastDrSub å’Œ FastDrSub++ã€‚å…¶ä¸­ï¼ŒFastDrSub å®ç°äº† 0.044 çš„è¿‘ä¼¼æ¯”ï¼ŒæŸ¥è¯¢å¤æ‚åº¦ä¸º $O(n \\log(k))$ï¼›è€Œ FastDrSub++ åœ¨ç»™å®šå‚æ•° $\\epsilon > 0$ çš„æƒ…å†µä¸‹ï¼Œå°†è¿‘ä¼¼æ¯”æå‡è‡³ $1/4-\\epsilon$ï¼Œä¸”ä¿æŒäº†ç›¸åŒçš„æŸ¥è¯¢å¤æ‚åº¦ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ¡ˆæˆä¸ºé¦–ä¸ªåœ¨ $O(n \\log(k))$ ä½å¤æ‚åº¦ä¸‹å®ç°å¸¸æ•°æ¯”è¿‘ä¼¼çš„ç®—æ³•ã€‚é€šè¿‡åœ¨å…·æœ‰ DR-submodular ç›®æ ‡å‡½æ•°çš„ Revenue Maximization é—®é¢˜ä¸Šè¿›è¡Œå®éªŒè¯„ä¼°ï¼Œç»“æœè¡¨æ˜æ‰€æå‡ºçš„ç®—æ³•åœ¨æŸ¥è¯¢å¤æ‚åº¦å’Œè§£çš„è´¨é‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02254v1",
      "published_date": "2025-11-04 04:37:16 UTC",
      "updated_date": "2025-11-04 04:37:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:06:55.673327+00:00"
    },
    {
      "arxiv_id": "2511.02246v1",
      "title": "Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results",
      "title_zh": "æ¼”ç¤ºï¼šLLM åè§ä¸é”™è¯¯çš„ç»Ÿè®¡æ˜¾è‘—æ€§ç»“æœå¹¶ä¸ä¿è¯ç»“æœçš„å¯æ³›åŒ–æ€§",
      "authors": [
        "Jonathan Liu",
        "Haoling Qiu",
        "Jonathan Lasko",
        "Damianos Karakos",
        "Mahsa Yarmohammadi",
        "Mark Dredze"
      ],
      "abstract": "Recent research has shown that hallucinations, omissions, and biases are prevalent in everyday use-cases of LLMs. However, chatbots used in medical contexts must provide consistent advice in situations where non-medical factors are involved, such as when demographic information is present. In order to understand the conditions under which medical chatbots fail to perform as expected, we develop an infrastructure that 1) automatically generates queries to probe LLMs and 2) evaluates answers to these queries using multiple LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples the space of patient demographics, histories, disorders, and writing styles to create realistic questions that we subsequently use to prompt LLMs. In 2), our evaluation pipeline provides hallucination and omission detection using LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge treatment category detectors. As a baseline study, we perform two case studies on inter-LLM agreement and the impact of varying the answering and evaluation LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's Kappa $Îº=0.118$), and only specific (answering, evaluation) LLM pairs yield statistically significant differences across writing styles, genders, and races. We recommend that studies using LLM evaluation use multiple LLMs as evaluators in order to avoid arriving at statistically significant but non-generalizable results, particularly in the absence of ground-truth data. We also suggest publishing inter-LLM agreement metrics for transparency. Our code and dataset are available here: https://github.com/BBN-E/medic-neurips-2025-demo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—èŠå¤©æœºå™¨äººåœ¨æ¶‰åŠéåŒ»ç–—å› ç´ ï¼ˆå¦‚äººå£ç»Ÿè®¡ä¿¡æ¯ï¼‰æ—¶å¯èƒ½å‡ºç°çš„åè§å’Œé”™è¯¯ï¼Œå¼€å‘äº†ä¸€å¥—è‡ªåŠ¨åŒ–çš„è¯„ä¼°åŸºç¡€è®¾æ–½ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆé€šè¿‡é‡‡æ ·æ‚£è€…ç‰¹å¾ã€ç—…å²å’Œå†™ä½œé£æ ¼ç”Ÿæˆé€¼çœŸçš„æŸ¥è¯¢ï¼Œéšååˆ©ç”¨LLM-as-a-judgeè®¾ç½®åŠagentic workflowsæ¥æ£€æµ‹å›ç­”ä¸­çš„å¹»è§‰å’Œé—æ¼ã€‚é€šè¿‡å¯¹inter-LLM agreementåŠä¸åŒæ¨¡å‹ç»„åˆçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…å‘ç°LLMæ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´æ€§å¾—åˆ†è¾ƒä½ï¼ˆå¹³å‡Cohen's Kappaä¸º0.118ï¼‰ï¼Œä¸”ä»…æœ‰ç‰¹å®šçš„å›ç­”ä¸è¯„ä¼°LLMé…å¯¹èƒ½åœ¨ä¸åŒå†™ä½œé£æ ¼ã€æ€§åˆ«å’Œç§æ—é—´äº§ç”Ÿç»Ÿè®¡æ˜¾è‘—çš„å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»Ÿè®¡æ˜¾è‘—æ€§å¹¶ä¸ä¿è¯ç»“æœçš„å¯æ³›åŒ–æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹ground-truthæ•°æ®çš„æƒ…å†µä¸‹ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®æœªæ¥çš„è¯„ä¼°ç ”ç©¶åº”ä½¿ç”¨å¤šä¸ªLLMä½œä¸ºè¯„ä¼°è€…ï¼Œå¹¶å…¬å¼€inter-LLM agreementæŒ‡æ ‡ä»¥ç¡®ä¿ç ”ç©¶ç»“æœçš„é€æ˜åº¦å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02246v1",
      "published_date": "2025-11-04 04:20:33 UTC",
      "updated_date": "2025-11-04 04:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:07:23.226317+00:00"
    },
    {
      "arxiv_id": "2511.02243v1",
      "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs",
      "title_zh": "å½“æ¨¡æ€å†²çªæ—¶ï¼šå•æ¨¡æ€æ¨ç†ä¸ç¡®å®šæ€§å¦‚ä½•æ”¯é… MLLMs çš„åå¥½åŠ¨æ€",
      "authors": [
        "Zhuoran Zhang",
        "Tengyue Wang",
        "Xilin Gong",
        "Yang Shi",
        "Haotian Wang",
        "Di Wang",
        "Lijie Hu"
      ],
      "abstract": "Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨é¢å¯¹ä¸åŒæ¨¡æ€ä¿¡æ¯å†²çªæ—¶çš„â€œæ¨¡æ€è·Ÿéš(modality following)â€è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨¡æ€è·Ÿéšåˆ†è§£ä¸ºç›¸å¯¹æ¨ç†ä¸ç¡®å®šæ€§(relative reasoning uncertainty)å’Œå†…åœ¨æ¨¡æ€åå¥½(inherent modality preference)ä¸¤ä¸ªåŸºæœ¬å› ç´ ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¯æ§æ•°æ®é›†é€šè¿‡æ”¹å˜è¾“å…¥éš¾åº¦æ¥éªŒè¯è¿™ä¸€ç†è®ºã€‚åˆ©ç”¨ç†µ(entropy)ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡ï¼Œç ”ç©¶å‘ç°äº†ä¸€ä¸ªæ™®éè§„å¾‹ï¼šéšç€æŸæ¨¡æ€ç›¸å¯¹ä¸ç¡®å®šæ€§çš„å¢åŠ ï¼Œæ¨¡å‹è·Ÿéšè¯¥æ¨¡æ€çš„æ¦‚ç‡ä¼šå•è°ƒé€’å‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†â€œå¹³è¡¡ç‚¹(balance point)â€æ¦‚å¿µï¼Œä½œä¸ºè¡¡é‡æ¨¡å‹å†…åœ¨åå¥½çš„æŒ‡æ ‡ï¼Œæœ‰æ•ˆå‰¥ç¦»äº†å•æ¨¡æ€èƒ½åŠ›å’Œæ•°æ®é›†åå·®çš„å½±å“ã€‚æ­¤å¤–ï¼Œé€šè¿‡å±‚çº§é¢„æµ‹æ¢æµ‹ï¼Œä½œè€…æ­ç¤ºäº†æ¨¡å‹åœ¨å¹³è¡¡ç‚¹é™„è¿‘çš„æ¨¡ç³ŠåŒºåŸŸå†…ï¼Œä¼šåœ¨ä¸åŒå±‚çº§é—´äºå„æ¨¡æ€ä¹‹é—´äº§ç”ŸæŒ¯è¡çš„å†…éƒ¨æœºåˆ¶ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ç›¸å¯¹ä¸ç¡®å®šæ€§å’Œå†…åœ¨åå¥½æ˜¯æ”¯é…MLLMsè§£å†³ä¿¡æ¯å†²çªçš„æ ¸å¿ƒåŸåˆ™ï¼Œä¸ºç†è§£æ¨¡å‹å†³ç­–æä¾›äº†å®šé‡çš„æ¡†æ¶å’Œæœºåˆ¶æ€§è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.02243v1",
      "published_date": "2025-11-04 04:11:31 UTC",
      "updated_date": "2025-11-04 04:11:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:07:48.006325+00:00"
    },
    {
      "arxiv_id": "2511.02241v3",
      "title": "Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control",
      "title_zh": "ç»“æ„å¯å¡‘æ€§å³ä¸»åŠ¨æ¨ç†ï¼šä¸€ç§ç”¨äºç¨³æ€æ§åˆ¶çš„ç”Ÿç‰©å¯å‘å¼æ¶æ„",
      "authors": [
        "Brennen A. Hill"
      ],
      "abstract": "Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹ä¼ ç»Ÿç¥ç»ç½‘ç»œä¾èµ–éç”Ÿç‰©å­¦åå‘ä¼ æ’­çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—ä¸»åŠ¨æ¨ç†(Active Inference)å’Œç”Ÿç‰©å½¢æ€å¯å¡‘æ€§å¯å‘çš„è®¡ç®—æ¨¡å‹â€”â€”ç»“æ„è‡ªé€‚åº”é¢„æµ‹æ¨ç†ç½‘ç»œ(SAPIN)ã€‚è¯¥æ¨¡å‹åœ¨äºŒç»´ç½‘æ ¼ä¸Šè¿è¡Œï¼Œé€šè¿‡æœ€å°åŒ–å±€éƒ¨é¢„æµ‹è¯¯å·®è¿›è¡Œå­¦ä¹ ï¼Œå¹¶åŒ…å«ä¸¤ç§å¹¶å‘æœºåˆ¶ï¼šåŸºäºèµ«å¸ƒ(Hebbian-like)è§„åˆ™çš„çªè§¦å¯å¡‘æ€§å’Œå…è®¸ç»†èƒç‰©ç†è¿ç§»ä»¥ä¼˜åŒ–æ„Ÿå—é‡çš„ç»“æ„å¯å¡‘æ€§ã€‚è¿™ç§åŒé‡æ–¹æ³•ä½¿ç½‘ç»œèƒ½å¤ŸåŒæ—¶å­¦ä¹ çªè§¦æƒé‡å’Œç½‘ç»œæ‹“æ‰‘ç»“æ„ï¼Œä»è€Œä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚åœ¨Cart Poleå¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSAPINé€šè¿‡æœ€å°åŒ–é¢„æµ‹è¯¯å·®å’Œç»´æŒç¨³æ€(Homeostasis)çš„å†…åœ¨é©±åŠ¨åŠ›æˆåŠŸå‘ç°äº†ç¨³å®šçš„å¹³è¡¡ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æŒç»­å­¦ä¹ ä¼šå¯¼è‡´ä¸ç¨³å®šï¼Œä½†åœ¨è·å¾—æˆåŠŸç­–ç•¥åé”å®šå‚æ•°ï¼Œæ¨¡å‹åœ¨åç»­è¯„ä¼°ä¸­ä¿æŒäº†å¹³å‡82%çš„æˆåŠŸç‡ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "In Brain-Inspired Dynamics for Engineering Energy-Efficient Circuits and Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2511.02241v3",
      "published_date": "2025-11-04 04:07:16 UTC",
      "updated_date": "2025-12-09 19:14:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:08:10.276034+00:00"
    },
    {
      "arxiv_id": "2511.02239v1",
      "title": "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation",
      "title_zh": "LACYï¼šé¢å‘è‡ªæˆ‘æ”¹è¿›æœºå™¨äººæ“ä½œçš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­è¨€-åŠ¨ä½œå¾ªç¯",
      "authors": [
        "Youngjin Hong",
        "Houjian Yu",
        "Mingen Li",
        "Changhyun Choi"
      ],
      "abstract": "Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and explaining its actions can form richer internal representations and unlock new paradigms for self-supervised learning. We introduce LACY (Language-Action Cycle), a unified framework that learns such bidirectional mappings within a single vision-language model. LACY is jointly trained on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between two language descriptions (L2C). This enables a self-improving cycle that autonomously generates and filters new training data through an active augmentation strategy targeting low-confidence cases, thereby improving the model without additional human labels. Experiments on pick-and-place tasks in both simulation and the real world show that LACY improves task success rates by 56.46% on average and yields more robust language-action grounding for robotic manipulation. Project page: https://vla2026.github.io/LACY/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººæ“ä½œä¸­å•å‘è¯­è¨€åˆ°åŠ¨ä½œ(L2A)æ˜ å°„ç¼ºä¹æ·±å±‚è¯­å¢ƒç†è§£å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†LACY (Language-Action Cycle)ï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä½œè€…è®¤ä¸ºå°†åŠ¨ä½œæ˜ å°„å›è¯­è¨€(A2L)çš„èƒ½åŠ›å¯¹äºå»ºç«‹æ›´å…¨é¢çš„åŸºç¡€å’Œå®ç°è‡ªç›‘ç£å­¦ä¹ è‡³å…³é‡è¦ã€‚LACYåœ¨å•ä¸ªæ¨¡å‹ä¸­è”åˆè®­ç»ƒä¸‰ä¸ªååŒä»»åŠ¡ï¼šä»è¯­è¨€ç”Ÿæˆå‚æ•°åŒ–åŠ¨ä½œ(L2A)ã€ç”¨è¯­è¨€è§£é‡Šè§‚å¯Ÿåˆ°çš„åŠ¨ä½œ(A2L)ä»¥åŠéªŒè¯è¯­è¨€æè¿°ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§(L2C)ã€‚è¿™ç§åŒå‘æ˜ å°„æœºåˆ¶å®ç°äº†ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›å¾ªç¯ï¼Œé€šè¿‡é’ˆå¯¹ä½ç½®ä¿¡åº¦æ¡ˆä¾‹çš„ä¸»åŠ¨å¢å¼ºç­–ç•¥ï¼Œè‡ªä¸»ç”Ÿæˆå¹¶è¿‡æ»¤æ–°çš„è®­ç»ƒæ•°æ®ï¼Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸‹çš„æ‹¾å–æ”¾ç½®ä»»åŠ¡å®éªŒè¡¨æ˜ï¼ŒLACYå°†ä»»åŠ¡æˆåŠŸç‡å¹³å‡æé«˜äº†56.46%ï¼Œå¹¶ä¸ºæœºå™¨äººæ“ä½œæä¾›äº†æ›´é²æ£’çš„è¯­è¨€-åŠ¨ä½œåŸºç¡€(Language-Action Grounding)ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Preprint. Project page: https://vla2026.github.io/LACY/",
      "pdf_url": "https://arxiv.org/pdf/2511.02239v1",
      "published_date": "2025-11-04 04:02:51 UTC",
      "updated_date": "2025-11-04 04:02:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:10:46.120229+00:00"
    },
    {
      "arxiv_id": "2511.02238v1",
      "title": "Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network",
      "title_zh": "Deep Ideationï¼šè®¾è®¡åŸºäºç§‘å­¦æ¦‚å¿µç½‘ç»œç”Ÿæˆæ–°é¢–ç ”ç©¶æ€è·¯çš„ LLM æ™ºèƒ½ä½“",
      "authors": [
        "Keyu Zhao",
        "Weiquan Lin",
        "Qirui Zheng",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "Novel research ideas play a critical role in advancing scientific inquiries. Recent advancements in Large Language Models (LLMs) have demonstrated their potential to generate novel research ideas by leveraging large-scale scientific literature. However, previous work in research ideation has primarily relied on simplistic methods, such as keyword co-occurrence or semantic similarity. These approaches focus on identifying statistical associations in the literature but overlook the complex, contextual relationships between scientific concepts, which are essential to effectively leverage knowledge embedded in human literature. For instance, papers that simultaneously mention \"keyword A\" and \"keyword B\" often present research ideas that integrate both concepts. Additionally, some LLM-driven methods propose and refine research ideas using the model's internal knowledge, but they fail to effectively utilize the scientific concept network, limiting the grounding of ideas in established research. To address these challenges, we propose the Deep Ideation framework to address these challenges, integrating a scientific network that captures keyword co-occurrence and contextual relationships, enriching LLM-driven ideation. The framework introduces an explore-expand-evolve workflow to iteratively refine research ideas, using an Idea Stack to track progress. A critic engine, trained on real-world reviewer feedback, guides the process by providing continuous feedback on the novelty and feasibility of ideas. Our experiments show that our approach improves the quality of generated ideas by 10.67% compared to other methods, with ideas surpassing top conference acceptance levels. Human evaluation highlights their practical value in scientific research, and ablation studies confirm the effectiveness of each component in the workflow. Code repo is available at https://github.com/kyZhao-1/Deep-Ideation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Deep Ideationæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆç§‘ç ”åˆ›æ„æ—¶ä»…ä¾èµ–ç®€å•ç»Ÿè®¡å…³è”æˆ–å†…éƒ¨çŸ¥è¯†ï¼Œä»è€Œå¿½è§†ç§‘å­¦æ¦‚å¿µé—´å¤æ‚ä¸Šä¸‹æ–‡å…³ç³»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªæ•æ‰å…³é”®è¯å…±ç°å’Œä¸Šä¸‹æ–‡å…³ç³»çš„ç§‘å­¦ç½‘ç»œï¼Œå¼•å…¥äº†â€œæ¢ç´¢-æ‰©å±•-æ¼”åŒ–â€(explore-expand-evolve)å·¥ä½œæµæ¥è¿­ä»£ä¼˜åŒ–åˆ›æ„ï¼Œå¹¶åˆ©ç”¨Idea Stackè·Ÿè¸ªè¿›åº¦ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé…å¤‡äº†ä¸€ä¸ªåŸºäºçœŸå®å®¡ç¨¿åé¦ˆè®­ç»ƒçš„critic engineï¼Œèƒ½å¤Ÿå¯¹åˆ›æ„çš„åˆ›æ–°æ€§å’Œå¯è¡Œæ€§æä¾›æŒç»­çš„è¯„ä¼°ä¸æŒ‡å¯¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åˆ›æ„è´¨é‡ç›¸æ¯”å…¶ä»–æ–¹æ³•æé«˜äº†10.67%ï¼Œä¸”åˆ›æ„æ°´å¹³è¶…è¶Šäº†é¡¶çº§ä¼šè®®çš„æ¥æ”¶æ ‡å‡†ï¼Œäººå·¥è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02238v1",
      "published_date": "2025-11-04 04:00:20 UTC",
      "updated_date": "2025-11-04 04:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:09:04.470376+00:00"
    },
    {
      "arxiv_id": "2511.02230v2",
      "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
      "title_zh": "Continuumï¼šåŸºäº KV ç¼“å­˜ç”Ÿå­˜æ—¶é—´çš„é«˜æ•ˆé²æ£’å¤šè½® LLM æ™ºèƒ½ä½“è°ƒåº¦",
      "authors": [
        "Hanchen Li",
        "Qiuyang Mang",
        "Runyuan He",
        "Qizheng Zhang",
        "Huanzhi Mao",
        "Xiaokun Chen",
        "Hangrui Zhou",
        "Alvin Cheung",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "abstract": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Continuumï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–å¤šè½®LLM Agentå·¥ä½œè´Ÿè½½ä»»åŠ¡å®Œæˆæ—¶é—´ï¼ˆJob Completion Timeï¼‰çš„æœåŠ¡ç³»ç»Ÿï¼Œè§£å†³äº†ç°æœ‰æ¨ç†å¼•æ“åœ¨Agentè°ƒç”¨å·¥å…·äº§ç”Ÿåœé¡¿æ—¶è¿‡æ—©é©±é€KV cacheå¯¼è‡´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚Continuumå¼•å…¥äº†KV cacheçš„ç”Ÿå­˜æ—¶é—´ï¼ˆTime-to-Live, TTLï¼‰æœºåˆ¶ï¼Œæ ¹æ®é‡è½½æˆæœ¬å’Œä¿æŒé¡ºåºçš„æ”¶ç›Šï¼Œé€‰æ‹©æ€§åœ°å°†KV cacheé”å®šåœ¨GPUæ˜¾å­˜ä¸­ã€‚å½“TTLè¿‡æœŸæ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é©±é€KV cacheä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œä»è€Œåœ¨å·¥å…·è°ƒç”¨æŒç»­æ—¶é—´ä¸å¯é¢„æµ‹çš„æƒ…å†µä¸‹ä¿æŒé²æ£’æ€§ã€‚ç»“åˆç¨‹åºçº§çš„å…ˆæ¥å…ˆæœåŠ¡ï¼ˆFirst-Come-First-Serveï¼‰ç­–ç•¥ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç»´æŠ¤äº†å¤šè½®äº¤äº’çš„è¿ç»­æ€§å¹¶å‡å°‘äº†å¤æ‚å·¥ä½œæµçš„å»¶è¿Ÿã€‚åœ¨ä½¿ç”¨Llama-3.1 8B/70Bè¿›è¡Œçš„SWE-Benchå’ŒBFCLçœŸå®å·¥ä½œè´Ÿè½½è¯„ä¼°ä¸­ï¼ŒContinuumæ˜¾è‘—æ”¹å–„äº†å¹³å‡ä½œä¸šå®Œæˆæ—¶é—´ï¼Œä¸”è¿™ç§æå‡éšç€äº¤äº’è½®æ•°çš„å¢åŠ è€Œæ‰©å¤§ã€‚",
      "categories": [
        "cs.OS",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.OS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02230v2",
      "published_date": "2025-11-04 03:43:05 UTC",
      "updated_date": "2025-12-20 01:17:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:09:40.522601+00:00"
    },
    {
      "arxiv_id": "2511.02228v1",
      "title": "Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis",
      "title_zh": "ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­çš„ MRI ä¸ PET ååŒæ³¨æ„åŠ›ä¸ä¸€è‡´æ€§å¼•å¯¼èåˆ",
      "authors": [
        "Delin Ma",
        "Menghui Zhou",
        "Jun Qi",
        "Yun Yang",
        "Po Yang"
      ],
      "abstract": "Alzheimer's disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…(AD)è¯Šæ–­ä¸­ç°æœ‰å¤šæ¨¡æ€èåˆæ–¹æ³•å¿½ç•¥æ¨¡æ€ç‰¹æœ‰ç‰¹å¾åŠå­˜åœ¨åˆ†å¸ƒå·®å¼‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºMRIå’ŒPETçš„ååŒæ³¨æ„åŠ›å’Œä¸€è‡´æ€§å¼•å¯¼èåˆæ¡†æ¶ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å¯å­¦ä¹ å‚æ•°è¡¨ç¤º(LPR)æ¨¡å—ä»¥è¡¥å¿ç¼ºå¤±çš„æ¨¡æ€ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç»“åˆå…±äº«ç¼–ç å™¨ä¸ç‹¬ç«‹æ¨¡æ€ç¼–ç å™¨ï¼Œæœ‰æ•ˆä¿ç•™äº†å…±äº«ç‰¹å¾ä¸æ¨¡æ€ç‰¹æœ‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€è‡´æ€§å¼•å¯¼æœºåˆ¶(consistency-guided mechanism)æ¥æ˜¾å¼å¯¹é½ä¸åŒæ¨¡æ€é—´çš„æ½œåœ¨åˆ†å¸ƒï¼Œä»è€Œå‡å°‘åå·®å’Œå™ªå£°ã€‚åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯Šæ–­æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„èåˆç­–ç•¥ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€ç¥ç»å½±åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02228v1",
      "published_date": "2025-11-04 03:42:07 UTC",
      "updated_date": "2025-11-04 03:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:10:03.308941+00:00"
    },
    {
      "arxiv_id": "2511.11614v1",
      "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI",
      "title_zh": "è¶…è¶Š GPUï¼šFPGA åœ¨ä¸‹ä¸€æ³¢äººå·¥æ™ºèƒ½æµªæ½®ä¸­çš„æˆ˜ç•¥è§’è‰²",
      "authors": [
        "Arturo UrÃ­as JimÃ©nez"
      ],
      "abstract": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.\n  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½åŠ é€Ÿé¢†åŸŸï¼Œå°½ç®¡GPUç›®å‰å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å…¶å›ºå®šæ¶æ„åœ¨ä½å»¶è¿Ÿå’Œèƒ½æ•ˆæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ–‡ç« æå‡ºField-Programmable Gate Arrays (FPGAs)ä½œä¸ºä¸€ç§å¯é‡æ„å¹³å°ï¼Œèƒ½å¤Ÿå°†AIç®—æ³•ç›´æ¥æ˜ å°„åˆ°è®¾å¤‡é€»è¾‘ä¸­ï¼Œæˆä¸ºä¸‹ä¸€æ³¢AIæµªæ½®çš„æˆ˜ç•¥é€‰æ‹©ã€‚FPGAsèƒ½å¤Ÿä¸ºå·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)å®ç°å¹¶è¡Œæµæ°´çº¿ï¼Œæä¾›ç¡®å®šæ€§çš„æ—¶åºå’Œæ›´ä½çš„åŠŸè€—ï¼Œé€‚åˆéœ€è¦æ·±åº¦å®šåˆ¶çš„å·¥ä½œè´Ÿè½½ã€‚ä¸æ¶æ„ä¸å¯å˜çš„CPUå’ŒGPUä¸åŒï¼ŒFPGAå¯ä»¥åœ¨ç°åœºé‡æ–°é…ç½®ä»¥é€‚åº”ç‰¹å®šæ¨¡å‹ï¼Œæˆ–ä½œä¸ºSoCé›†æˆåµŒå…¥å¼å¤„ç†å™¨ï¼Œåœ¨ä¼ æ„Ÿå™¨é™„è¿‘ç›´æ¥è¿è¡Œæ¨ç†ã€‚è¿™ç§è¾¹ç¼˜è®¡ç®—èƒ½åŠ›å‡å°‘äº†å‘äº‘ç«¯ä¼ è¾“åŸå§‹æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œé™ä½å»¶è¿Ÿå’Œå¸¦å®½å ç”¨ï¼Œå¹¶å¢å¼ºäº†éšç§ä¿æŠ¤ã€‚æ­¤å¤–ï¼Œéƒ¨åˆ†é‡æ„æŠ€æœ¯å’Œä»AIæ¡†æ¶åˆ°ç¡¬ä»¶çš„ç¼–è¯‘æµç¨‹æ­£åœ¨ç¼©çŸ­å¼€å‘å‘¨æœŸï¼Œæ¨åŠ¨äº†ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡(Hardware-Algorithm Co-design)çš„å‘å±•ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11614v1",
      "published_date": "2025-11-04 03:41:42 UTC",
      "updated_date": "2025-11-04 03:41:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:10:38.221322+00:00"
    },
    {
      "arxiv_id": "2511.02872v2",
      "title": "FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels",
      "title_zh": "FATEï¼šé¢å‘å¤šéš¾åº¦ç­‰çº§å‰æ²¿ä»£æ•°çš„å½¢å¼åŒ–åŸºå‡†ç³»åˆ—",
      "authors": [
        "Jiedong Jiang",
        "Wanyi He",
        "Yuefeng Wang",
        "Guoxiong Gao",
        "Yongle Hu",
        "Jingting Wang",
        "Nailing Guan",
        "Peihao Wu",
        "Chunbo Dai",
        "Liang Xiao",
        "Bin Dong"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, particularly on contest-based mathematical benchmarks like the IMO. However, these contests do not reflect the depth, breadth, and abstraction of modern mathematical research. To bridge this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra designed to chart a course toward advanced mathematical reasoning. We present two new components, FATE-H and FATE-X, each with 100 problems in abstract and commutative algebra. The FATE series spans a difficulty spectrum from undergraduate exercises to problems exceeding PhD qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both PhD-level exam difficulty and the coverage of the Mathlib library. Our evaluations of state-of-the-art LLM provers on this new benchmark reveal a stark performance gap compared to contest math: the best model achieves only 3% (pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals that models' natural-language reasoning is notably more accurate than their ability to formalize this reasoning. We systematically classify the common errors that arise during this formalization process. Furthermore, a comparative study shows that a specialized prover can exhibit less effective reflection than general-purpose models, reducing its accuracy at the natural-language stage. We believe FATE provides a robust and challenging benchmark that establishes essential checkpoints on the path toward research-level formal mathematical reasoning.",
      "tldr_zh": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨IMOç­‰ç«èµ›æ•°å­¦åŸºå‡†çš„å½¢å¼åŒ–å®šç†è¯æ˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è¿™äº›ç«èµ›æ— æ³•å……åˆ†åæ˜ ç°ä»£æ•°å­¦ç ”ç©¶çš„æ·±åº¦å’ŒæŠ½è±¡æ€§ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æ¨å‡ºäº†FATE (Formal Algebra Theorem Evaluation)ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å‰æ²¿ä»£æ•°å½¢å¼åŒ–è¯æ˜çš„å…¨æ–°åŸºå‡†ç³»åˆ—ï¼Œæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ã€‚FATEåŒ…å«FATE-Hå’ŒFATE-Xä¸¤ä¸ªç»„ä»¶ï¼Œå„å«100é“æŠ½è±¡ä»£æ•°å’Œäº¤æ¢ä»£æ•°é¢˜ç›®ï¼Œéš¾åº¦è·¨åº¦ä»æœ¬ç§‘ç»ƒä¹ åˆ°è¶…è¶Šåšå£«èµ„æ ¼è€ƒè¯•çš„æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFATE-Xæ˜¯é¦–ä¸ªéš¾åº¦è¶…è¿‡åšå£«çº§è€ƒè¯•ä¸”è¦†ç›–èŒƒå›´è¶…å‡ºMathlibåº“çš„å½¢å¼åŒ–åŸºå‡†ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„LLMè¯æ˜å™¨åœ¨æ­¤åŸºå‡†ä¸Šè¡¨ç°æƒ¨æ·¡ï¼Œä¸ç«èµ›æ•°å­¦å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œæœ€ä½³æ¨¡å‹åœ¨FATE-Hä¸Šä»…è¾¾åˆ°3%çš„å‡†ç¡®ç‡(pass@64)ï¼Œè€Œåœ¨FATE-Xä¸Šä¸º0%ã€‚åŒé˜¶æ®µè¯„ä¼°è¿›ä¸€æ­¥æ­ç¤ºï¼Œæ¨¡å‹çš„è‡ªç„¶è¯­è¨€æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¼˜äºå…¶å½¢å¼åŒ–èƒ½åŠ›ï¼Œä¸”ä¸“ç”¨è¯æ˜å™¨åœ¨åæ€(reflection)æ–¹é¢ä¸å¦‚é€šç”¨æ¨¡å‹æœ‰æ•ˆã€‚è¯¥ç ”ç©¶è®¤ä¸ºï¼ŒFATEä¸ºé€šå¾€ç ”ç©¶çº§å½¢å¼åŒ–æ•°å­¦æ¨ç†çš„é“è·¯æä¾›äº†ç¨³å¥ä¸”å…·æŒ‘æˆ˜æ€§çš„å…³é”®åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.FL",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02872v2",
      "published_date": "2025-11-04 03:25:17 UTC",
      "updated_date": "2025-11-06 03:30:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:13:59.953610+00:00"
    },
    {
      "arxiv_id": "2511.02219v2",
      "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data",
      "title_zh": "TabDSRï¼šé¢å‘è¡¨æ ¼æ•°æ®å¤æ‚æ•°å€¼æ¨ç†çš„åˆ†è§£ã€æ¸…æ´—ä¸æ¨ç†",
      "authors": [
        "Changjiang Jiang",
        "Fengchang Yu",
        "Haihua Chen",
        "Wei Lu",
        "Jin Zeng"
      ],
      "abstract": "Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose TabDSR, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that TabDSR consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and TabDSR, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´çš„æŸ¥è¯¢å¤æ‚ã€å™ªå£°å¹²æ‰°åŠæ•°å€¼èƒ½åŠ›å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†TabDSRæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ç»„æˆï¼šç”¨äºæ‹†è§£å¤æ‚é—®é¢˜çš„æŸ¥è¯¢åˆ†è§£å™¨(query decomposer)ã€ç”¨äºæ¸…æ´—å’Œè¿‡æ»¤è¡¨æ ¼å™ªå£°çš„è¡¨æ ¼å‡€åŒ–å™¨(table sanitizer)ï¼Œä»¥åŠåŸºäºæ€ç»´ç¨‹åº(Program-of-Thoughts, PoT)çš„æ¨ç†å™¨ï¼Œåè€…é€šè¿‡ç”Ÿæˆå¯æ‰§è¡Œä»£ç ä»å‡€åŒ–åçš„è¡¨æ ¼ä¸­å¾—å‡ºç­”æ¡ˆã€‚ä¸ºäº†ç¡®ä¿è¯„ä¼°å…¬æ­£å¹¶ç¼“è§£æ•°æ®æ³„éœ²é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ä¸“ä¸ºè¡¨æ ¼å¤æ‚æ•°å€¼æ¨ç†è®¾è®¡çš„æ–°æ•°æ®é›†CalTab151ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTabDSRåœ¨TAT-QAã€TableBenchåŠCalTab151ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡äº†8.79%ã€6.08%å’Œ19.87%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)æ°´å¹³ã€‚è¯¥æ¡†æ¶èƒ½ä¸ä¸»æµLLMsæ— ç¼é›†æˆï¼Œä¸ºå¤æ‚çš„è¡¨æ ¼æ•°å€¼æ¨ç†æä¾›äº†å¼ºå¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2511.02219v2",
      "published_date": "2025-11-04 03:13:02 UTC",
      "updated_date": "2025-11-05 03:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:14:31.665309+00:00"
    },
    {
      "arxiv_id": "2511.02217v1",
      "title": "Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments",
      "title_zh": "æ··åˆè‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸‹å¤šè½¦é“äº¤å‰å£æ€§èƒ½ä¼˜åŒ–",
      "authors": [
        "Manonmani Sekar",
        "Nasim Nezamoddini"
      ],
      "abstract": "One of the main challenges in managing traffic at multilane intersections is ensuring smooth coordination between human-driven vehicles (HDVs) and connected autonomous vehicles (CAVs). This paper presents a novel traffic signal control framework that combines Graph Attention Networks (GAT) with Soft Actor-Critic (SAC) reinforcement learning to address this challenge. GATs are used to model the dynamic graph- structured nature of traffic flow to capture spatial and temporal dependencies between lanes and signal phases. The proposed SAC is a robust off-policy reinforcement learning algorithm that enables adaptive signal control through entropy-optimized decision making. This design allows the system to coordinate the signal timing and vehicle movement simultaneously with objectives focused on minimizing travel time, enhancing performance, ensuring safety, and improving fairness between HDVs and CAVs. The model is evaluated using a SUMO-based simulation of a four-way intersection and incorporating different traffic densities and CAV penetration rates. The experimental results demonstrate the effectiveness of the GAT-SAC approach by achieving a 24.1% reduction in average delay and up to 29.2% fewer traffic violations compared to traditional methods. Additionally, the fairness ratio between HDVs and CAVs improved to 1.59, indicating more equitable treatment across vehicle types. These findings suggest that the GAT-SAC framework holds significant promise for real-world deployment in mixed-autonomy traffic systems.",
      "tldr_zh": "è¯¥è®ºæ–‡é’ˆå¯¹å¤šè½¦é“äº¤å‰å£ä¸­äººç±»é©¾é©¶è½¦è¾†(HDVs)å’Œç½‘è”è‡ªåŠ¨é©¾é©¶è½¦è¾†(CAVs)çš„æ··åˆäº¤é€šç®¡ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)å’ŒSoft Actor-Critic (SAC)å¼ºåŒ–å­¦ä¹ çš„æ–°å‹äº¤é€šä¿¡å·æ§åˆ¶æ¡†æ¶ã€‚å…¶ä¸­ï¼ŒGATç”¨äºå»ºæ¨¡äº¤é€šæµçš„åŠ¨æ€å›¾ç»“æ„ä»¥æ•æ‰è½¦é“å’Œä¿¡å·ç›¸ä½ä¹‹é—´çš„æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œè€ŒSACç®—æ³•é€šè¿‡ç†µä¼˜åŒ–å†³ç­–å®ç°è‡ªé€‚åº”ä¿¡å·æ§åˆ¶ã€‚è¯¥ç³»ç»Ÿçš„ç›®æ ‡æ˜¯åè°ƒä¿¡å·é…æ—¶å’Œè½¦è¾†è¿åŠ¨ï¼Œä»¥æœ€å°åŒ–è¡Œé©¶æ—¶é—´ã€ç¡®ä¿å®‰å…¨æ€§å¹¶æå‡HDVsä¸CAVsä¹‹é—´çš„å…¬å¹³æ€§ã€‚åŸºäºSUMOçš„ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œåœ¨ä¸åŒçš„äº¤é€šå¯†åº¦å’ŒCAVæ¸—é€ç‡ä¸‹ï¼Œè¯¥GAT-SACæ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å°†å¹³å‡å»¶è¿Ÿå‡å°‘äº†24.1%ï¼Œäº¤é€šè¿è§„å‡å°‘äº†29.2%ã€‚æ­¤å¤–ï¼Œä¸åŒè½¦è¾†ç±»å‹é—´çš„å…¬å¹³æ€§æ¯”ç‡æå‡è‡³1.59ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ··åˆè‡ªåŠ¨é©¾é©¶äº¤é€šç³»ç»Ÿå®é™…éƒ¨ç½²ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02217v1",
      "published_date": "2025-11-04 03:10:47 UTC",
      "updated_date": "2025-11-04 03:10:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:14:44.967405+00:00"
    },
    {
      "arxiv_id": "2511.02216v1",
      "title": "Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¶…å¯é ä½æ—¶å»¶é€šä¿¡è‡ªé€‚åº”åä½œä¼ è¾“è®¾è®¡",
      "authors": [
        "Hyemin Yu",
        "Hong-Chuan Yang"
      ],
      "abstract": "Next-generation wireless communication systems must support ultra-reliable low-latency communication (URLLC) service for mission-critical applications. Meeting stringent URLLC requirements is challenging, especially for two-hop cooperative communication. In this paper, we develop an adaptive transmission design for a two-hop relaying communication system. Each hop transmission adaptively configures its transmission parameters separately, including numerology, mini-slot size, and modulation and coding scheme, for reliable packet transmission within a strict latency constraint. We formulate the hop-specific transceiver configuration as a Markov decision process (MDP) and propose a dual-agent reinforcement learning-based cooperative latency-aware transmission (DRL-CoLA) algorithm to learn latency-aware transmission policies in a distributed manner. Simulation results verify that the proposed algorithm achieves the near-optimal reliability while satisfying strict latency requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸‹ä¸€ä»£æ— çº¿é€šä¿¡ç³»ç»Ÿä¸­è¶…å¯é ä½å»¶è¿Ÿé€šä¿¡(URLLC)é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¤è·³åä½œé€šä¿¡åœºæ™¯ä¸‹ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ä¼ è¾“è®¾è®¡æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆå…è®¸æ¯ä¸€è·³ä¼ è¾“ç‹¬ç«‹åœ°è‡ªé€‚åº”é…ç½®ä¼ è¾“å‚æ•°ï¼ŒåŒ…æ‹¬numerologyã€mini-slot sizeä»¥åŠè°ƒåˆ¶ä¸ç¼–ç ç­–ç•¥(MCS)ï¼Œä»è€Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿçº¦æŸä¸‹ç¡®ä¿æ•°æ®åŒ…çš„å¯é ä¼ è¾“ã€‚ç ”ç©¶äººå‘˜å°†ç‰¹å®šè·³çš„æ”¶å‘å™¨é…ç½®å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºåŒæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„åä½œå»¶è¿Ÿæ„ŸçŸ¥ä¼ è¾“(DRL-CoLA)ç®—æ³•ï¼Œä»¥åˆ†å¸ƒå¼çš„æ–¹å¼å­¦ä¹ å»¶è¿Ÿæ„ŸçŸ¥ä¼ è¾“ç­–ç•¥ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DRL-CoLAç®—æ³•èƒ½å¤Ÿåœ¨æ»¡è¶³ä¸¥æ ¼å»¶è¿Ÿè¦æ±‚çš„åŒæ—¶ï¼Œå®ç°æ¥è¿‘æœ€ä¼˜çš„å¯é æ€§æ€§èƒ½ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "Accepted at the AI4NextG Workshop, NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.02216v1",
      "published_date": "2025-11-04 03:08:59 UTC",
      "updated_date": "2025-11-04 03:08:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:15:08.943859+00:00"
    },
    {
      "arxiv_id": "2511.11612v1",
      "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems",
      "title_zh": "è¯„ä¼°ç”¨äºå¼‚æ„é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿå·¥ä½œè´Ÿè½½æ˜ å°„ä¸è°ƒåº¦çš„å¤§å‹è¯­è¨€æ¨¡å‹",
      "authors": [
        "Aasish Kumar Sharma",
        "Julian Kunkel"
      ],
      "abstract": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.",
      "tldr_zh": "æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼‚æ„é«˜æ€§èƒ½è®¡ç®—(HPC)ç³»ç»Ÿä¸­å¤„ç†å·¥ä½œè´Ÿè½½æ˜ å°„å’Œè°ƒåº¦é—®é¢˜çš„èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜å‘21ä¸ªå…¬å¼€å¯ç”¨çš„LLMsæä¾›äº†ç³»ç»ŸèŠ‚ç‚¹ã€ä»»åŠ¡éœ€æ±‚å’Œè°ƒåº¦çº¦æŸçš„æ–‡æœ¬æè¿°ï¼Œè¦æ±‚æ¨¡å‹åˆ†é…ä»»åŠ¡ã€è®¡ç®—æ€»å®Œå·¥æ—¶é—´(makespan)å¹¶è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚ä¸æ‰‹åŠ¨æ¨å¯¼çš„åˆ†ææœ€ä¼˜è§£ç›¸æ¯”ï¼Œ3ä¸ªæ¨¡å‹ç²¾ç¡®å¤ç°äº†æœ€ä¼˜ç»“æœï¼Œ12ä¸ªæ¨¡å‹è¾¾åˆ°äº†è¿‘ä¼˜æ°´å¹³ï¼Œè€Œ6ä¸ªæ¨¡å‹å› ç®—æœ¯æˆ–ä¾èµ–æ€§é”™è¯¯äº§ç”Ÿäº†æ¬¡ä¼˜è°ƒåº¦ã€‚å°½ç®¡æ‰€æœ‰æ¨¡å‹å‡ç”Ÿæˆäº†å¯è¡Œçš„ä»»åŠ¡æ˜ å°„ï¼Œä½†ä»…çº¦ä¸€åŠä¸¥æ ¼éµå®ˆäº†çº¦æŸæ¡ä»¶ï¼Œä¸»è¦å›°éš¾åœ¨äºç²¾ç¡®è®¡æ—¶å’Œæ•°æ®ä¼ è¾“ç®—æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»å¤§å¤šæ•°æ¨¡å‹æä¾›äº†è¿è´¯çš„é€æ­¥æ¨ç†å’Œéƒ¨åˆ†å¯æ‰§è¡Œçš„éªŒè¯ä»£ç ï¼Œå±•ç°äº†åœ¨é€»è¾‘é”™è¯¯å­˜åœ¨ä¸‹çš„å¼ºå¯è§£é‡Šæ€§ã€‚ç ”ç©¶ç»“æœç•Œå®šäº†LLMsåœ¨ç»„åˆä¼˜åŒ–(combinatorial optimization)ä¸­çš„èƒ½åŠ›è¾¹ç•Œï¼Œè¡¨æ˜å½“å‰é¢†å…ˆæ¨¡å‹æ›´é€‚åˆä½œä¸ºä¼˜åŒ–å’Œå†³ç­–æ”¯æŒä»»åŠ¡çš„å¯è§£é‡Šå‰¯é©¾é©¶(co-pilots)ï¼Œè€Œéå®Œå…¨è‡ªä¸»çš„æ±‚è§£å™¨ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "14 pages, 4 figures, 2 tables. Evaluation study on LLM-based reasoning for HPC scheduling. Published in Research in Academic Engineering Journal (RAEJ), 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.11612v1",
      "published_date": "2025-11-04 03:04:28 UTC",
      "updated_date": "2025-11-04 03:04:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:15:51.501892+00:00"
    },
    {
      "arxiv_id": "2511.02210v1",
      "title": "Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ç»é£Ÿç®¡è¶…å£°å¿ƒåŠ¨å›¾èŠ‚æ®µçºµå‘åº”å˜ä¼°è®¡",
      "authors": [
        "Anders Austlid TaskÃ©n",
        "Thierry Judge",
        "Erik Andreas Rye Berg",
        "Jinyang Yu",
        "BjÃ¸rnar Grenne",
        "Frank Lindseth",
        "Svend Aakhus",
        "Pierre-Marc Jodoin",
        "Nicolas Duchateau",
        "Olivier Bernard",
        "Gabriel Kiss"
      ],
      "abstract": "Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.\n  As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.\n  Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95\\% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»‹ç»äº†autoStrainï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨æ·±åº¦å­¦ä¹ (Deep Learning)æ–¹æ³•åœ¨ç»é£Ÿç®¡è¶…å£°å¿ƒåŠ¨å›¾(TEE)ä¸­è‡ªåŠ¨ä¼°è®¡å·¦å¿ƒå®¤èŠ‚æ®µçºµå‘åº”å˜(SLS)çš„æµç¨‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯ä¾èµ–äººå·¥ä¸”æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç ”ç©¶å¯¹æ¯”åˆ†æäº†ä¸¤ç§æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼šåŸºäºRAFTå…‰æµæ¨¡å‹çš„TeeFlowï¼ˆç”¨äºå¯†é›†å¸§é—´é¢„æµ‹ï¼‰å’ŒåŸºäºCoTrackerç‚¹è½¨è¿¹æ¨¡å‹çš„TeeTrackerï¼ˆç”¨äºç¨€ç–é•¿åºåˆ—é¢„æµ‹ï¼‰ã€‚ç”±äºçœŸå®è¶…å£°å¿ƒåŠ¨å›¾åºåˆ—çš„è¿åŠ¨çœŸå€¼éš¾ä»¥è·å–ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨SIMUSä»¿çœŸæµç¨‹ç”Ÿæˆäº†åŒ…å«80åæ‚£è€…çš„é«˜é€¼çœŸåˆæˆTEEæ•°æ®é›†(synTEE)ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeeTrackeråœ¨è¿åŠ¨ä¼°è®¡ç²¾åº¦ä¸Šä¼˜äºTeeFlowï¼Œåœ¨åˆæˆæµ‹è¯•é›†ä¸Šå®ç°äº†0.65 mmçš„å¹³å‡è·ç¦»è¯¯å·®ã€‚é’ˆå¯¹16åæ‚£è€…çš„ä¸´åºŠéªŒè¯è¿›ä¸€æ­¥æ˜¾ç¤ºï¼ŒautoStrainçš„SLSä¼°è®¡ç»“æœä¸ä¸´åºŠå‚è€ƒå€¼é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡å·®å¼‚ä»…ä¸º1.09%ã€‚æ­¤å¤–ï¼Œåœ¨synTEEæ•°æ®ä¸­å¼•å…¥æ¨¡æ‹Ÿç¼ºè¡€æƒ…å†µæ˜¾è‘—æé«˜äº†æ¨¡å‹é‡åŒ–å¼‚å¸¸å˜å½¢çš„å‡†ç¡®æ€§ï¼Œè¡¨æ˜è¯¥AIé©±åŠ¨çš„æ–¹æ³•èƒ½æœ‰æ•ˆæå‡ä¸´åºŠå¿ƒè„åŠŸèƒ½è¯„ä¼°çš„ç²¾åº¦ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, IEEE Journal of Biomedical and Health Informatics",
      "pdf_url": "https://arxiv.org/pdf/2511.02210v1",
      "published_date": "2025-11-04 03:02:27 UTC",
      "updated_date": "2025-11-04 03:02:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:16:00.479203+00:00"
    },
    {
      "arxiv_id": "2511.02208v1",
      "title": "Training Proactive and Personalized LLM Agents",
      "title_zh": "è®­ç»ƒä¸»åŠ¨ä¸ä¸ªæ€§åŒ–çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Weiwei Sun",
        "Xuhui Zhou",
        "Weihua Du",
        "Xingyao Wang",
        "Sean Welleck",
        "Graham Neubig",
        "Maarten Sap",
        "Yiming Yang"
      ],
      "abstract": "While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè™½ç„¶ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨ä»»åŠ¡æˆåŠŸç‡ï¼Œä½†æ„å»ºæœ‰æ•ˆçš„ç°å®ä¸–ç•Œä»£ç†éœ€è¦åŒæ—¶ä¼˜åŒ–ç”Ÿäº§åŠ›(Productivity)ã€ä¸»åŠ¨æ€§(Proactivity)å’Œä¸ªæ€§åŒ–(Personalization)ä¸‰ä¸ªç»´åº¦ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†UserVilleï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«åŸºäºLLMçš„ç”¨æˆ·æ¨¡æ‹Ÿå™¨çš„äº¤äº’å¼ç¯å¢ƒï¼Œèƒ½å¤Ÿå®ç°å¤šæ ·åŒ–ä¸”å¯é…ç½®çš„ç”¨æˆ·åå¥½ã€‚åŸºäºUserVilleï¼Œç ”ç©¶æå‡ºäº†PPPï¼Œä¸€ç§å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ–¹æ³•ï¼Œæ—¨åœ¨è”åˆä¼˜åŒ–ä¸Šè¿°ä¸‰ä¸ªç»´åº¦ã€‚åœ¨è½¯ä»¶å·¥ç¨‹å’Œæ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œç»PPPè®­ç»ƒçš„ä»£ç†åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŒ…æ‹¬GPT-5åœ¨å†…çš„å¼ºåŸºçº¿æ¨¡å‹ï¼ˆå¹³å‡æå‡21.6åˆ†ï¼‰ã€‚ç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä½¿ä»£ç†å…·å¤‡äº†æå‡ºæˆ˜ç•¥æ€§æ¾„æ¸…é—®é¢˜ã€é€‚åº”æœªè§ç”¨æˆ·åå¥½ä»¥åŠé€šè¿‡æ›´ä¼˜äº¤äº’æå‡ä»»åŠ¡æˆåŠŸç‡çš„èƒ½åŠ›ï¼Œè¯æ˜äº†é’ˆå¯¹ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„äº¤äº’è¿›è¡Œæ˜¾å¼ä¼˜åŒ–å¯¹äºæ„å»ºå®ç”¨AIä»£ç†è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02208v1",
      "published_date": "2025-11-04 02:59:36 UTC",
      "updated_date": "2025-11-04 02:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:16:38.611165+00:00"
    },
    {
      "arxiv_id": "2511.02207v1",
      "title": "Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping",
      "title_zh": "é¢å‘è‰è“æ¤æ ªé‡å»ºä¸è¡¨å‹åˆ†æçš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„3Dé«˜æ–¯æ³¼æº…",
      "authors": [
        "Jiajia Li",
        "Keyi Zhu",
        "Qianwen Zhang",
        "Dong Chen",
        "Qi Sun",
        "Zhaojian Li"
      ],
      "abstract": "Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‰è“æ¤ç‰©è¡¨å‹åˆ†æä¸­ä¼ ç»Ÿæ–¹æ³•è€—æ—¶ä¸”å…·æœ‰ç ´åæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºObject-Centric 3D Gaussian Splatting (3DGS)çš„é‡å»ºæ¡†æ¶ã€‚ç°æœ‰çš„3DGSåº”ç”¨é€šå¸¸é‡å»ºæ•´ä¸ªåœºæ™¯ï¼Œå¯¼è‡´èƒŒæ™¯å™ªå£°å¹²æ‰°åˆ†æå¹¶å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŒ…å«Segment Anything Model v2 (SAM-2)å’Œalphaé€šé“èƒŒæ™¯æ©ç çš„é¢„å¤„ç†æµç¨‹ï¼Œå®ç°äº†å»é™¤èƒŒæ™¯çš„å¹²å‡€è‰è“æ¤ç‰©é‡å»ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç®—æ³•åˆ©ç”¨DBSCANèšç±»å’Œä¸»æˆåˆ†åˆ†æ(PCA)è‡ªåŠ¨ä¼°è®¡æ¤ç‰©é«˜åº¦å’Œå† å±‚å®½åº¦ç­‰å…³é”®æ€§çŠ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæµç¨‹ï¼Œä¸ºè‰è“æ¤ç‰©è¡¨å‹åˆ†ææä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”éç ´åæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.02207v1",
      "published_date": "2025-11-04 02:55:46 UTC",
      "updated_date": "2025-11-04 02:55:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:17:03.749316+00:00"
    },
    {
      "arxiv_id": "2511.02200v1",
      "title": "Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration",
      "title_zh": "æœ€ä¼˜æ™ºèƒ½ä½“é€‰æ‹©ï¼šé¢å‘é«˜æ•ˆå¤šæ™ºèƒ½ä½“åä½œçš„çŠ¶æ€æ„ŸçŸ¥è·¯ç”±æ¡†æ¶",
      "authors": [
        "Jingbo Wang",
        "Sendong Zhao",
        "Haochun Wang",
        "Yuzheng Fan",
        "Lizhe Zhang",
        "Yan Liu",
        "Ting Liu"
      ],
      "abstract": "The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å­˜åœ¨çš„è°ƒåº¦åƒµåŒ–å’Œåä½œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†STRMACï¼Œä¸€ç§ç”¨äºé«˜æ•ˆå¤šæ™ºèƒ½ä½“åä½œçš„çŠ¶æ€æ„ŸçŸ¥è·¯ç”±æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†åˆ«ç¼–ç äº¤äº’å†å²å’Œæ™ºèƒ½ä½“çŸ¥è¯†æ¥é©±åŠ¨è·¯ç”±å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥è‡ªé€‚åº”åœ°é€‰æ‹©æœ€åˆé€‚çš„å•ä¸ªæ™ºèƒ½ä½“ä»¥å®ç°é«˜æ•ˆåä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼ŒåŠ é€Ÿäº†é«˜è´¨é‡æ‰§è¡Œè·¯å¾„çš„æ”¶é›†ä»¥ç”¨äºç³»ç»Ÿè®­ç»ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åä½œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-Art)çš„æ€§èƒ½ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹æå‡äº†23.8%ï¼Œå¹¶å°†æ•°æ®æ”¶é›†å¼€é”€ç›¸æ¯”ç©·ä¸¾æœç´¢å‡å°‘äº†90.1%ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02200v1",
      "published_date": "2025-11-04 02:41:14 UTC",
      "updated_date": "2025-11-04 02:41:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:17:25.069236+00:00"
    },
    {
      "arxiv_id": "2511.05563v1",
      "title": "Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models",
      "title_zh": "å‰ç»å»æ©ç å®ç°æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å‡†ç¡®è§£ç ",
      "authors": [
        "Sanghyun Lee",
        "Seungryong Kim",
        "Jongho Park",
        "Dongmin Park"
      ],
      "abstract": "Masked Diffusion Models (MDMs) as language models generate by iteratively unmasking tokens, yet their performance crucially depends on the inference time order of unmasking. Prevailing heuristics, such as confidence based sampling, are myopic: they optimize locally, fail to leverage extra test-time compute, and let early decoding mistakes cascade. We propose Lookahead Unmasking (LookUM), which addresses these concerns by reformulating sampling as path selection over all possible unmasking orders without the need for an external reward model. Our framework couples (i) a path generator that proposes paths by sampling from pools of unmasking sets with (ii) a verifier that computes the uncertainty of the proposed paths and performs importance sampling to subsequently select the final paths. Empirically, erroneous unmasking measurably inflates sequence level uncertainty, and our method exploits this to avoid error-prone trajectories. We validate our framework across six benchmarks, such as mathematics, planning, and coding, and demonstrate consistent performance improvements. LookUM requires only two to three paths to achieve peak performance, demonstrating remarkably efficient path selection. The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with LookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that uncertainty based verification provides orthogonal benefits to reinforcement learning and underscoring the versatility of our framework. Code will be publicly released.",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†Lookahead Unmasking (LookUM)ï¼Œæ—¨åœ¨è§£å†³Masked Diffusion Models (MDMs)åœ¨è¯­è¨€ç”Ÿæˆä¸­å› è§£ç é¡ºåºå¯¼è‡´çš„æ€§èƒ½é—®é¢˜ã€‚ç°æœ‰çš„å¯å‘å¼æ–¹æ³•ï¼ˆå¦‚åŸºäºç½®ä¿¡åº¦çš„é‡‡æ ·ï¼‰å¾€å¾€è¿‡äºçŸ­è§†ï¼Œå®¹æ˜“å¯¼è‡´æ—©æœŸè§£ç é”™è¯¯çº§è”ï¼Œä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„é¢å¤–è®¡ç®—èµ„æºã€‚LookUMå°†é‡‡æ ·è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºåœ¨æ‰€æœ‰å¯èƒ½çš„unmaskingé¡ºåºä¸­çš„è·¯å¾„é€‰æ‹©é—®é¢˜ï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è·¯å¾„ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œå‰è€…ä»unmaskingé›†åˆæ± ä¸­é‡‡æ ·æå‡ºè·¯å¾„ï¼Œåè€…è®¡ç®—è·¯å¾„çš„ä¸ç¡®å®šæ€§å¹¶é€šè¿‡importance samplingé€‰æ‹©æœ€ç»ˆè·¯å¾„ã€‚ç ”ç©¶å‘ç°é”™è¯¯çš„unmaskingä¼šæ˜¾è‘—å¢åŠ åºåˆ—çº§çš„ä¸ç¡®å®šæ€§ï¼ŒLookUMåˆ©ç”¨è¿™ä¸€ç‚¹æ¥é¿å…æ˜“é”™çš„è½¨è¿¹ã€‚åœ¨æ•°å­¦ã€è§„åˆ’å’Œä»£ç ç­‰å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLookUMä»…éœ€ä¸¤åˆ°ä¸‰æ¡è·¯å¾„å³å¯è¾¾åˆ°å³°å€¼æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ­è½½LookUMçš„åŸºç¡€LLaDAæ¨¡å‹æ€§èƒ½å¯åª²ç¾ç»è¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„LLaDA 1.5ï¼Œä¸”èƒ½è¿›ä¸€æ­¥æå‡LLaDA 1.5çš„è¡¨ç°ï¼Œè¯æ˜äº†åŸºäºä¸ç¡®å®šæ€§çš„éªŒè¯ä¸å¼ºåŒ–å­¦ä¹ å…·æœ‰æ­£äº¤æ•ˆç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05563v1",
      "published_date": "2025-11-04 02:37:37 UTC",
      "updated_date": "2025-11-04 02:37:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:17:53.763662+00:00"
    },
    {
      "arxiv_id": "2511.05562v1",
      "title": "Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement",
      "title_zh": "é€šè¿‡è¿­ä»£ç²¾ç‚¼å®ç°ç¦»æ•£æ‰©æ•£çš„æœ‰æ•ˆæµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Sanghyun Lee",
        "Sunwoo Kim",
        "Seungryong Kim",
        "Jongho Park",
        "Dongmin Park"
      ],
      "abstract": "Test-time scaling through reward-guided generation remains largely unexplored for discrete diffusion models despite its potential as a promising alternative. In this work, we introduce Iterative Reward-Guided Refinement (IterRef), a novel test-time scaling method tailored to discrete diffusion that leverages reward-guided noising-denoising transitions to progressively refine misaligned intermediate states. We formalize this process within a Multiple-Try Metropolis (MTM) framework, proving convergence to the reward-aligned distribution. Unlike prior methods that assume the current state is already aligned with the reward distribution and only guide the subsequent transition, our approach explicitly refines each state in situ, progressively steering it toward the optimal intermediate distribution. Across both text and image domains, we evaluate IterRef on diverse discrete diffusion models and observe consistent improvements in reward-guided generation quality. In particular, IterRef achieves striking gains under low compute budgets, far surpassing prior state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Iterative Reward-Guided Refinement (IterRef)ï¼Œä¸€ç§ä¸“ä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹(discrete diffusion models)è®¾è®¡çš„æ–°å‹æµ‹è¯•æ—¶æ‰©å±•(test-time scaling)æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¥–åŠ±å¼•å¯¼çš„åŠ å™ª-å»å™ªè½¬æ¢ï¼Œé€æ­¥ç»†åŒ–æœªå¯¹é½çš„ä¸­é—´çŠ¶æ€ã€‚ç ”ç©¶è€…åœ¨Multiple-Try Metropolis (MTM)æ¡†æ¶å†…å½¢å¼åŒ–äº†è¿™ä¸€è¿‡ç¨‹ï¼Œè¯æ˜äº†å…¶èƒ½æ”¶æ•›è‡³å¥–åŠ±å¯¹é½åˆ†å¸ƒã€‚ä¸å‡è®¾å½“å‰çŠ¶æ€å·²å¯¹é½ä¸”ä»…å¼•å¯¼åç»­è½¬æ¢çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒIterRefæ˜¾å¼åœ°åŸä½ç»†åŒ–æ¯ä¸ªçŠ¶æ€ï¼Œå°†å…¶é€æ­¥å¼•å¯¼è‡³æœ€ä½³ä¸­é—´åˆ†å¸ƒã€‚åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸçš„å¤šç§ç¦»æ•£æ‰©æ•£æ¨¡å‹è¯„ä¼°ä¸­ï¼ŒIterRefå‡æ˜¾è‘—æå‡äº†å¥–åŠ±å¼•å¯¼ç”Ÿæˆçš„è´¨é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½è®¡ç®—é¢„ç®—ä¸‹ï¼ŒIterRefå–å¾—äº†æƒŠäººçš„å¢ç›Šï¼Œæ€§èƒ½è¿œè¶…å…ˆå‰çš„æœ€å…ˆè¿›(SOTA)åŸºçº¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05562v1",
      "published_date": "2025-11-04 02:33:23 UTC",
      "updated_date": "2025-11-04 02:33:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:21:23.275136+00:00"
    },
    {
      "arxiv_id": "2511.02197v1",
      "title": "Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs",
      "title_zh": "Open the Oysterï¼šå¤§è¯­è¨€æ¨¡å‹ä»£ç æ¨ç†ç½®ä¿¡åº¦çš„å®è¯è¯„ä¼°ä¸æ”¹è¿›",
      "authors": [
        "Shufan Wang",
        "Xing Hu",
        "Junkai Chen",
        "Zhiyuan Pan",
        "Xin Xia"
      ],
      "abstract": "With the widespread application of large language models (LLMs) in the field of code intelligence, increasing attention has been paid to the reliability and controllability of their outputs in code reasoning tasks. Confidence estimation serves as an effective and convenient approach for evaluating these aspects. This paper proposes a confidence analysis and enhancement framework for LLMs tailored to code reasoning tasks. We conduct a comprehensive empirical study on the confidence reliability of mainstream LLMs across different tasks, and further evaluate the effectiveness of techniques such as prompt strategy optimisation and mathematical calibration (e.g., Platt Scaling) in improving confidence reliability. Our results show that DeepSeek-Reasoner achieves the best performance across various tasks, outperforming other models by up to $0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance Score, respectively. The hybrid strategy combining the reassess prompt strategy and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$ over the original performance in the aforementioned three metrics. These results indicate that models with reasoning capabilities demonstrate superior confidence reliability, and that the hybrid strategy is the most effective in enhancing the confidence reliability of various models. Meanwhile, we elucidate the impact of different task complexities, model scales, and strategies on confidence performance, and highlight that the confidence of current LLMs in complex reasoning tasks still has considerable room for improvement. This study not only provides a research foundation and technical reference for the application of confidence in LLM-assisted software engineering, but also points the way for future optimisation and engineering deployment of confidence mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ä¸“é—¨çš„ç½®ä¿¡åº¦åˆ†æä¸å¢å¼ºæ¡†æ¶ã€‚é€šè¿‡å¯¹ä¸»æµLLMsè¿›è¡Œå¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œä½œè€…è¯„ä¼°äº†æç¤ºç­–ç•¥ä¼˜åŒ–å’Œæ•°å­¦æ ¡å‡†ï¼ˆå¦‚Platt Scalingï¼‰åœ¨æå‡ç½®ä¿¡åº¦å¯é æ€§æ–¹é¢çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepSeek-Reasoneråœ¨ECEã€Brier Scoreå’ŒPerformance Scoreç­‰æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œç»“åˆé‡è¯„ä¼°æç¤ºç­–ç•¥(reassess prompt strategy)ä¸Platt Scalingçš„æ··åˆç­–ç•¥èƒ½æœ€æœ‰æ•ˆåœ°æå‡æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å±•ç°å‡ºæ›´ä¼˜è¶Šçš„ç½®ä¿¡åº¦å¯é æ€§ï¼Œå°½ç®¡å½“å‰LLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„ç½®ä¿¡åº¦ä»æœ‰è¾ƒå¤§æå‡ç©ºé—´ã€‚è¿™é¡¹å·¥ä½œä¸ä»…é˜æ˜äº†ä»»åŠ¡å¤æ‚åº¦å’Œæ¨¡å‹è§„æ¨¡å¯¹ç½®ä¿¡åº¦çš„å½±å“ï¼Œä¹Ÿä¸ºæœªæ¥LLMè¾…åŠ©è½¯ä»¶å·¥ç¨‹ä¸­çš„ç½®ä¿¡åº¦æœºåˆ¶ä¼˜åŒ–æä¾›äº†æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02197v1",
      "published_date": "2025-11-04 02:30:30 UTC",
      "updated_date": "2025-11-04 02:30:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:21:27.236262+00:00"
    },
    {
      "arxiv_id": "2511.02196v1",
      "title": "BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction",
      "title_zh": "BoolSkeletonï¼šåŸºäºåŒè´¨æ¨¡å¼çº¦å‡çš„å¸ƒå°”ç½‘ç»œéª¨æ¶åŒ–",
      "authors": [
        "Liwei Ni",
        "Jiaxi Zhang",
        "Shenggen Zheng",
        "Junfeng Liu",
        "Xingyu Meng",
        "Biwei Xie",
        "Xingquan Li",
        "Huawei Li"
      ],
      "abstract": "Boolean equivalence allows Boolean networks with identical functionality to exhibit diverse graph structures. This gives more room for exploration in logic optimization, while also posing a challenge for tasks involving consistency between Boolean networks. To tackle this challenge, we introduce BoolSkeleton, a novel Boolean network skeletonization method that improves the consistency and reliability of design-specific evaluations. BoolSkeleton comprises two key steps: preprocessing and reduction. In preprocessing, the Boolean network is transformed into a defined Boolean dependency graph, where nodes are assigned the functionality-related status. Next, the homogeneous and heterogeneous patterns are defined for the node-level pattern reduction step. Heterogeneous patterns are preserved to maintain critical functionality-related dependencies, while homogeneous patterns can be reduced. Parameter K of the pattern further constrains the fanin size of these patterns, enabling fine-tuned control over the granularity of graph reduction. To validate BoolSkeleton's effectiveness, we conducted four analysis/downstream tasks around the Boolean network: compression analysis, classification, critical path analysis, and timing prediction, demonstrating its robustness across diverse scenarios. Furthermore, it improves above 55% in the average accuracy compared to the original Boolean network for the timing prediction task. These experiments underscore the potential of BoolSkeleton to enhance design consistency in logic synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸ƒå°”ç½‘ç»œå› ç»“æ„å¤šæ ·æ€§å¯¼è‡´çš„å›¾ä¸€è‡´æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBoolSkeletonçš„å¸ƒå°”ç½‘ç»œéª¨æ¶åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŒ…å«é¢„å¤„ç†å’Œç¼©å‡ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼Œé¦–å…ˆå°†å¸ƒå°”ç½‘ç»œè½¬æ¢ä¸ºå®šä¹‰çš„å¸ƒå°”ä¾èµ–å›¾(Boolean dependency graph)å¹¶åˆ†é…åŠŸèƒ½ç›¸å…³çŠ¶æ€ã€‚æ¥ç€ï¼Œé€šè¿‡å®šä¹‰åŒè´¨(homogeneous)å’Œå¼‚è´¨(heterogeneous)æ¨¡å¼è¿›è¡ŒèŠ‚ç‚¹çº§æ¨¡å¼ç¼©å‡ï¼Œå…¶ä¸­å¼‚è´¨æ¨¡å¼è¢«ä¿ç•™ä»¥ç»´æŒå…³é”®åŠŸèƒ½ä¾èµ–ï¼Œè€ŒåŒè´¨æ¨¡å¼åˆ™è¢«ç¼©å‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥å‚æ•°Kæ¥çº¦æŸè¿™äº›æ¨¡å¼çš„æ‰‡å…¥(fanin)å¤§å°ï¼Œä»è€Œå®ç°å¯¹å›¾ç¼©å‡ç²’åº¦çš„ç²¾ç»†æ§åˆ¶ã€‚ä¸ºäº†éªŒè¯æœ‰æ•ˆæ€§ï¼Œç ”ç©¶åœ¨å‹ç¼©åˆ†æã€åˆ†ç±»ã€å…³é”®è·¯å¾„åˆ†æå’Œæ—¶åºé¢„æµ‹(timing prediction)å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBoolSkeletonåœ¨æ—¶åºé¢„æµ‹ä»»åŠ¡ä¸­çš„å¹³å‡å‡†ç¡®ç‡æ¯”åŸå§‹å¸ƒå°”ç½‘ç»œæé«˜äº†55%ä»¥ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨é€»è¾‘ç»¼åˆ(logic synthesis)ä¸­å¢å¼ºè®¾è®¡ä¸€è‡´æ€§çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02196v1",
      "published_date": "2025-11-04 02:25:29 UTC",
      "updated_date": "2025-11-04 02:25:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:21:52.257578+00:00"
    },
    {
      "arxiv_id": "2511.05560v1",
      "title": "Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements",
      "title_zh": "åŸºäºçº¿æ€§æ³¨æ„åŠ›ä¸è½»é‡çº§å¢å¼ºçš„æ ·æœ¬é«˜æ•ˆè¯­è¨€å»ºæ¨¡",
      "authors": [
        "Patrick Haller",
        "Jonas Golde",
        "Alan Akbik"
      ],
      "abstract": "We study architectural and optimization techniques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM token mixer and explores lightweight enhancements, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support training in low-resource settings, we curate a high-quality corpus emphasizing readability and pedagogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding window attention consistently improves zero-shot performance, and (2) the Muon optimizer stabilizes convergence and reduces perplexity over AdamW. These results highlight effective strategies for efficient language modeling without relying on scale.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹BabyLM 2025å…±äº«ä»»åŠ¡çš„çº¦æŸï¼Œæå‡ºäº†ä¸€ç§åä¸ºBLaLMçš„æ¨¡å‹ï¼Œæ—¨åœ¨æ¢ç´¢æ ·æœ¬é«˜æ•ˆçš„è¯­è¨€å»ºæ¨¡æ¶æ„ä¸ä¼˜åŒ–æŠ€æœ¯ã€‚BLaLMåˆ©ç”¨çº¿æ€§æ—¶é—´çš„mLSTM token mixeræ›¿ä»£äº†ä¼ ç»Ÿçš„self-attentionï¼Œå¹¶ç»“åˆäº†short convolutionsã€å¸¦æœ‰åŠ¨æ€è°ƒåˆ¶çš„sliding window attentionä»¥åŠHedgehog feature mapsç­‰è½»é‡çº§å¢å¼ºæ‰‹æ®µã€‚ä¸ºäº†é€‚åº”ä½èµ„æºè®­ç»ƒç¯å¢ƒï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå¼ºè°ƒå¯è¯»æ€§å’Œæ•™å­¦ç»“æ„çš„é«˜è´¨é‡è¯­æ–™åº“ã€‚åœ¨STRICTå’ŒSTRICT-SMALLèµ›é“ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œçº¿æ€§æ³¨æ„åŠ›ä¸sliding window attentionçš„ç»“åˆèƒ½æŒç»­æå‡zero-shotæ€§èƒ½ã€‚åŒæ—¶ï¼ŒMuonä¼˜åŒ–å™¨ç›¸æ¯”AdamWæ›´èƒ½ç¨³å®šæ”¶æ•›è¿‡ç¨‹å¹¶é™ä½å›°æƒ‘åº¦ï¼Œè¯æ˜äº†åœ¨ä¸ä¾èµ–å¤§è§„æ¨¡æ‰©å±•çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é«˜æ•ˆçš„è¯­è¨€å»ºæ¨¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05560v1",
      "published_date": "2025-11-04 02:21:03 UTC",
      "updated_date": "2025-11-04 02:21:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:22:15.729191+00:00"
    },
    {
      "arxiv_id": "2511.02194v1",
      "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning",
      "title_zh": "ä¸ªæ€§åŒ–å†³ç­–å»ºæ¨¡ï¼šæ•ˆç”¨ä¼˜åŒ–è¿˜æ˜¯æ–‡æœ¬åŒ–ç¬¦å·æ¨ç†",
      "authors": [
        "Yibo Zhao",
        "Yang Zhao",
        "Hongru Du",
        "Hao Frank Yang"
      ],
      "abstract": "Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªäººå†³ç­–è¿‡ç¨‹ä¸­æ•°å€¼å±æ€§ä¸è¯­è¨€å½±å“ï¼ˆå¦‚ä¸ªäººåå¥½ï¼‰çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºATHENAï¼ˆAdaptive Textual-symbolic Human-centric Reasoning frameworkï¼‰çš„è‡ªé€‚åº”æ–‡æœ¬ç¬¦å·æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ•ˆç”¨ç†è®ºï¼ˆUtility Theoryï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¿¡æ¯çš„æ•´åˆã€‚ATHENAåŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šé¦–å…ˆé€šè¿‡LLMå¢å¼ºçš„ç¬¦å·å‘ç°æœºåˆ¶æŒ–æ˜é²æ£’çš„ç¾¤ä½“çº§ç¬¦å·æ•ˆç”¨å‡½æ•°ï¼›å…¶æ¬¡å®æ–½ä¸ªä½“çº§çš„è¯­ä¹‰é€‚åº”ï¼Œåˆ©ç”¨æœ€ä¼˜æ•ˆç”¨å¼•å¯¼åˆ›å»ºä¸ªæ€§åŒ–è¯­ä¹‰æ¨¡æ¿ä»¥å»ºæ¨¡ä¸ªäººé€‰æ‹©ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å‡ºè¡Œæ–¹å¼å’Œç–«è‹—é€‰æ‹©ä»»åŠ¡éªŒè¯ä¸­ï¼ŒATHENAçš„è¡¨ç°æŒç»­ä¼˜äºä¼ ç»Ÿçš„æ•ˆç”¨æ¨¡å‹ã€æœºå™¨å­¦ä¹ æ–¹æ³•åŠå…¶ä»–LLMåŸºçº¿æ¨¡å‹ï¼ŒF1åˆ†æ•°æå‡è‡³å°‘6.5%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†ä¸¤ä¸ªé˜¶æ®µçš„äº’è¡¥æ€§å’Œå¿…è¦æ€§ï¼Œè¡¨æ˜ATHENAé€šè¿‡æœ‰æœºç»“åˆç¬¦å·æ•ˆç”¨å»ºæ¨¡ä¸è¯­ä¹‰é€‚åº”ï¼Œä¸ºä»¥äººä¸ºä¸­å¿ƒçš„å†³ç­–å»ºæ¨¡æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02194v1",
      "published_date": "2025-11-04 02:19:09 UTC",
      "updated_date": "2025-11-04 02:19:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:22:39.814027+00:00"
    },
    {
      "arxiv_id": "2511.02193v2",
      "title": "MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation",
      "title_zh": "MM-UNetï¼šç”¨äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„ Morph Mamba U å‹å·ç§¯ç½‘ç»œ",
      "authors": [
        "Jiawen Liu",
        "Yuanbo Zeng",
        "Jiaming Liang",
        "Yizhen Yang",
        "Yiheng Zhang",
        "Enhui Cai",
        "Xiaoqi Sheng",
        "Hongmin Cai"
      ],
      "abstract": "Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 % on DRIVE and 1.25 % on STARE, demonstrating its effectiveness and advancement. The project code is public via https://github.com/liujiawen-jpg/MM-UNet.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MM-UNetï¼Œä¸€ç§ä¸“ä¸ºé«˜æ•ˆè§†ç½‘è†œè¡€ç®¡åˆ†å‰²è®¾è®¡çš„Uå‹å·ç§¯ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç»†å°è¡€ç®¡å’Œå¤æ‚åˆ†æ”¯ç»“æ„å¸¦æ¥çš„åˆ†å‰²éš¾é¢˜ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†Morph Mamba Convolutionå±‚æ›¿ä»£ä¼ ç»Ÿçš„é€ç‚¹å·ç§¯ï¼Œé€šè¿‡å½¢æ€æ„ŸçŸ¥å’ŒçŠ¶æ€æ„ŸçŸ¥çš„ç‰¹å¾é‡‡æ ·å¢å¼ºäº†å¯¹åˆ†æ”¯æ‹“æ‰‘ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†Reverse Selective State Guidanceæ¨¡å—ï¼Œå°†åå‘å¼•å¯¼ç†è®ºä¸çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼ˆState-Space Modelingï¼‰ç›¸ç»“åˆï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„å‡ ä½•è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›å’Œè§£ç æ•ˆç‡ã€‚åœ¨DRIVEå’ŒSTAREä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMM-UNetåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨DRIVEå’ŒSTAREä¸Šçš„F1-scoreåˆ†åˆ«æå‡äº†1.64%å’Œ1.25%ï¼Œè¯æ˜äº†å…¶åœ¨è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå…ˆè¿›æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was accepted by IEEE BIBM 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2511.02193v2",
      "published_date": "2025-11-04 02:18:25 UTC",
      "updated_date": "2025-11-10 12:21:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:23:29.773038+00:00"
    },
    {
      "arxiv_id": "2511.02175v1",
      "title": "Tackling Incomplete Data in Air Quality Prediction: A Bayesian Deep Learning Framework for Uncertainty Quantification",
      "title_zh": "åº”å¯¹ç©ºæ°”è´¨é‡é¢„æµ‹ä¸­çš„ä¸å®Œæ•´æ•°æ®ï¼šé¢å‘ä¸ç¡®å®šæ€§é‡åŒ–çš„è´å¶æ–¯æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Yuzhuang Pian",
        "Taiyu Wang",
        "Shiqi Zhang",
        "Rui Xu",
        "Yonghong Liu"
      ],
      "abstract": "Accurate air quality forecasts are vital for public health alerts, exposure assessment, and emissions control. In practice, observational data are often missing in varying proportions and patterns due to collection and transmission issues. These incomplete spatiotemporal records impede reliable inference and risk assessment and can lead to overconfident extrapolation. To address these challenges, we propose an end to end framework, the channel gated learning unit based spatiotemporal bayesian neural field (CGLUBNF). It uses Fourier features with a graph attention encoder to capture multiscale spatial dependencies and seasonal temporal dynamics. A channel gated learning unit, equipped with learnable activations and gated residual connections, adaptively filters and amplifies informative features. Bayesian inference jointly optimizes predictive distributions and parameter uncertainty, producing point estimates and calibrated prediction intervals. We conduct a systematic evaluation on two real world datasets, covering four typical missing data patterns and comparing against five state of the art baselines. CGLUBNF achieves superior prediction accuracy and sharper confidence intervals. In addition, we further validate robustness across multiple prediction horizons and analysis the contribution of extraneous variables. This research lays a foundation for reliable deep learning based spatio-temporal forecasting with incomplete observations in emerging sensing paradigms, such as real world vehicle borne mobile monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç©ºæ°”è´¨é‡é¢„æµ‹ä¸­å¸¸è§çš„è§‚æµ‹æ•°æ®ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCGLUBNFçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå³åŸºäºé€šé“é—¨æ§å­¦ä¹ å•å…ƒçš„æ—¶ç©ºè´å¶æ–¯ç¥ç»åœºã€‚è¯¥æ¡†æ¶ç»“åˆäº†Fourier featuresä¸graph attention encoderï¼Œæ—¨åœ¨æœ‰æ•ˆæ•æ‰å¤šå°ºåº¦çš„ç©ºé—´ä¾èµ–æ€§å’Œå­£èŠ‚æ€§æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡å¼•å…¥é…å¤‡å¯å­¦ä¹ æ¿€æ´»å‡½æ•°å’Œé—¨æ§æ®‹å·®è¿æ¥çš„Channel Gated Learning Unitï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¿‡æ»¤å’Œæ”¾å¤§å…³é”®ç‰¹å¾ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Bayesian inferenceè”åˆä¼˜åŒ–é¢„æµ‹åˆ†å¸ƒå’Œå‚æ•°ä¸ç¡®å®šæ€§ï¼ŒCGLUBNFèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®çš„ç‚¹ä¼°è®¡å’Œæ ¡å‡†çš„é¢„æµ‹åŒºé—´ã€‚åœ¨æ¶µç›–å¤šç§ç¼ºå¤±æ•°æ®æ¨¡å¼çš„çœŸå®æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹å±•ç°äº†ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„é¢„æµ‹ç²¾åº¦å’Œæ›´æ•é”çš„ç½®ä¿¡åŒºé—´ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒé¢„æµ‹è§†é‡ä¸‹çš„é²æ£’æ€§ï¼Œä¸ºå¤„ç†ç§»åŠ¨ç›‘æµ‹ç­‰æ–°å…´ä¼ æ„ŸèŒƒå¼ä¸­çš„ä¸å®Œæ•´æ—¶ç©ºæ•°æ®æä¾›äº†å¯é çš„æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02175v1",
      "published_date": "2025-11-04 01:42:00 UTC",
      "updated_date": "2025-11-04 01:42:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:23:51.799709+00:00"
    },
    {
      "arxiv_id": "2511.10543v1",
      "title": "From Euler to Today: Universal Mathematical Fallibility A Large-Scale Computational Analysis of Errors in ArXiv Papers",
      "title_zh": "ä» Euler è‡³ä»Šï¼šæ•°å­¦çš„æ™®éæ˜“é”™æ€§â€”â€”ArXiv è®ºæ–‡é”™è¯¯çš„å¤§è§„æ¨¡è®¡ç®—åˆ†æ",
      "authors": [
        "Igor Rivin"
      ],
      "abstract": "We present the results of a large-scale computational analysis of mathematical papers from the ArXiv repository, demonstrating a comprehensive system that not only detects mathematical errors but provides complete referee reports with journal tier recommendations. Our automated analysis system processed over 37,000 papers across multiple mathematical categories, revealing significant error rates and quality distributions. Remarkably, the system identified errors in papers spanning three centuries of mathematics, including works by Leonhard Euler (1707-1783) and Peter Gustav Lejeune Dirichlet (1805-1859), as well as contemporary Fields medalists.\n  In Numerical Analysis (math.NA), we observed an error rate of 9.6\\% (2,271 errors in 23,761 papers), while Geometric Topology (math.GT) showed 6.5\\% (862 errors in 13,209 papers). Strikingly, Category Theory (math.CT) showed 0\\% errors in 93 papers analyzed, with evidence suggesting these results are ``easier'' for automated analysis. Beyond error detection, the system evaluated papers for journal suitability, recommending 0.4\\% for top generalist journals, 15.5\\% for top field-specific journals, and categorizing the remainder across specialist venues. These findings demonstrate both the universality of mathematical error across all eras and the feasibility of automated comprehensive mathematical peer review at scale.\n  This work demonstrates that the methodology, while applied here to mathematics, is discipline-agnostic and could be readily extended to physics, computer science, and other fields represented in the ArXiv repository.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†å¯¹ArXivå­˜å‚¨åº“ä¸­æ•°å­¦è®ºæ–‡è¿›è¡Œçš„å¤§è§„æ¨¡è®¡ç®—åˆ†æç»“æœï¼Œæå‡ºäº†ä¸€ç§ä¸ä»…èƒ½æ£€æµ‹æ•°å­¦é”™è¯¯ï¼Œè¿˜èƒ½æä¾›å®Œæ•´å®¡ç¨¿æŠ¥å‘ŠåŠæœŸåˆŠåˆ†çº§å»ºè®®çš„ç»¼åˆç³»ç»Ÿã€‚è¯¥è‡ªåŠ¨åŒ–ç³»ç»Ÿå¤„ç†äº†è·¨è¶Šå¤šä¸ªæ•°å­¦ç±»åˆ«çš„è¶…è¿‡37,000ç¯‡è®ºæ–‡ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„é”™è¯¯ç‡å’Œè´¨é‡åˆ†å¸ƒï¼Œç”šè‡³åœ¨åŒ…æ‹¬Leonhard Eulerã€Peter Gustav Lejeune Dirichletä»¥åŠå½“ä»£Fieldså¥–å¾—ä¸»åœ¨å†…çš„è·¨è¶Šä¸‰ä¸ªä¸–çºªçš„ä½œå“ä¸­å‘ç°äº†é”™è¯¯ã€‚æ•°æ®æ˜¾ç¤ºï¼Œæ•°å€¼åˆ†æ(math.NA)é¢†åŸŸçš„é”™è¯¯ç‡ä¸º9.6%ï¼Œå‡ ä½•æ‹“æ‰‘(math.GT)ä¸º6.5%ï¼Œè€ŒèŒƒç•´è®º(math.CT)åœ¨åˆ†æçš„93ç¯‡è®ºæ–‡ä¸­é”™è¯¯ç‡ä¸º0%ã€‚é™¤äº†é”™è¯¯æ£€æµ‹ï¼Œè¯¥ç³»ç»Ÿè¿˜è¯„ä¼°äº†è®ºæ–‡çš„æœŸåˆŠé€‚ç”¨æ€§ï¼Œæ¨è0.4%çš„è®ºæ–‡æŠ•é€’è‡³é¡¶çº§ç»¼åˆæœŸåˆŠï¼Œ15.5%è‡³é¡¶çº§é¢†åŸŸæœŸåˆŠã€‚è¿™äº›å‘ç°è¯æ˜äº†æ•°å­¦é”™è¯¯çš„æ™®éæ€§ä»¥åŠå¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ•°å­¦åŒè¡Œè¯„å®¡çš„å¯è¡Œæ€§ï¼Œä¸”è¯¥æ–¹æ³•å…·æœ‰å­¦ç§‘æ— å…³æ€§ï¼Œå¯æ‰©å±•è‡³ç‰©ç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰å…¶ä»–é¢†åŸŸã€‚",
      "categories": [
        "math.HO",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "math.HO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10543v1",
      "published_date": "2025-11-04 01:39:59 UTC",
      "updated_date": "2025-11-04 01:39:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:24:28.649263+00:00"
    },
    {
      "arxiv_id": "2511.02164v1",
      "title": "ScenicProver: A Framework for Compositional Probabilistic Verification of Learning-Enabled Systems",
      "title_zh": "ScenicProverï¼šå­¦ä¹ å‹ç³»ç»Ÿçš„ç»„åˆå¼æ¦‚ç‡éªŒè¯æ¡†æ¶",
      "authors": [
        "Eric Vin",
        "Kyle A. Miller",
        "Inigo Incer",
        "Sanjit A. Seshia",
        "Daniel J. Fremont"
      ],
      "abstract": "Full verification of learning-enabled cyber-physical systems (CPS) has long been intractable due to challenges including black-box components and complex real-world environments. Existing tools either provide formal guarantees for limited types of systems or test the system as a monolith, but no general framework exists for compositional analysis of learning-enabled CPS using varied verification techniques over complex real-world environments. This paper introduces ScenicProver, a verification framework that aims to fill this gap. Built upon the Scenic probabilistic programming language, the framework supports: (1) compositional system description with clear component interfaces, ranging from interpretable code to black boxes; (2) assume-guarantee contracts over those components using an extension of Linear Temporal Logic containing arbitrary Scenic expressions; (3) evidence generation through testing, formal proofs via Lean 4 integration, and importing external assumptions; (4) systematic combination of generated evidence using contract operators; and (5) automatic generation of assurance cases tracking the provenance of system-level guarantees. We demonstrate the framework's effectiveness through a case study on an autonomous vehicle's automatic emergency braking system with sensor fusion. By leveraging manufacturer guarantees for radar and laser sensors and focusing testing efforts on uncertain conditions, our approach enables stronger probabilistic guarantees than monolithic testing with the same computational budget.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ScenicProverï¼Œä¸€ç§é’ˆå¯¹å­¦ä¹ å‹ç½‘ç»œç‰©ç†ç³»ç»Ÿ(learning-enabled CPS)è¿›è¡Œç»„åˆæ¦‚ç‡éªŒè¯çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é»‘ç›’ç»„ä»¶å’Œå¤æ‚ç°å®ç¯å¢ƒå¸¦æ¥çš„éªŒè¯éš¾é¢˜ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨Scenicæ¦‚ç‡ç¼–ç¨‹è¯­è¨€ä¹‹ä¸Šï¼Œæ”¯æŒä»å¯è§£é‡Šä»£ç åˆ°é»‘ç›’çš„ç»„åˆç³»ç»Ÿæè¿°ï¼Œå¹¶åˆ©ç”¨æ‰©å±•çš„çº¿æ€§æ—¶åºé€»è¾‘(Linear Temporal Logic)å®ç°å‡è®¾-ä¿è¯å¥‘çº¦(assume-guarantee contracts)ã€‚ScenicProveré€šè¿‡é›†æˆæµ‹è¯•å’ŒLean 4å½¢å¼åŒ–è¯æ˜æ¥ç”Ÿæˆè¯æ®ï¼Œåˆ©ç”¨å¥‘çº¦ç®—å­ç³»ç»Ÿåœ°ç»„åˆè¯æ®ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆè¿½è¸ªç³»ç»Ÿçº§ä¿è¯æ¥æºçš„ä¿è¯æ¡ˆä¾‹(assurance cases)ã€‚åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦è‡ªåŠ¨ç´§æ€¥åˆ¶åŠ¨ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ä¼ æ„Ÿå™¨åˆ¶é€ å•†çš„ä¿è¯å¹¶é›†ä¸­æµ‹è¯•ä¸ç¡®å®šæ¡ä»¶ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹ï¼ŒScenicProveræ¯”æ•´ä½“æµ‹è¯•(monolithic testing)èƒ½å¤Ÿæä¾›æ›´å¼ºçš„æ¦‚ç‡ä¿è¯ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.LO",
      "comment": "26 pages, 4 figures. Full version (including appendices) of a paper submitted to TACAS 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.02164v1",
      "published_date": "2025-11-04 01:09:08 UTC",
      "updated_date": "2025-11-04 01:09:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:24:49.997346+00:00"
    },
    {
      "arxiv_id": "2511.02162v4",
      "title": "Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models",
      "title_zh": "åŸºäº3Dç”Ÿæˆå¼AIå’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šç»„ä»¶ç‰©ä½“æ–‡æœ¬åˆ°æœºå™¨äººç»„è£…",
      "authors": [
        "Alexander Htet Kyaw",
        "Richa Gupta",
        "Dhruv Shah",
        "Anoop Sinha",
        "Kory Mathewson",
        "Stefanie Pender",
        "Sachin Chitta",
        "Yotto Koga",
        "Faez Ahmed",
        "Lawrence Sass",
        "Randall Davis"
      ],
      "abstract": "Advances in 3D generative AI have enabled the creation of physical objects from text prompts, but challenges remain in creating objects involving multiple component types. We present a pipeline that integrates 3D generative AI with vision-language models (VLMs) to enable the robotic assembly of multi-component objects from natural language. Our method leverages VLMs for zero-shot, multi-modal reasoning about geometry and functionality to decompose AI-generated meshes into multi-component 3D models using predefined structural and panel components. We demonstrate that a VLM is capable of determining which mesh regions need panel components in addition to structural components, based on the object's geometry and functionality. Evaluation across test objects shows that users preferred the VLM-generated assignments 90.6% of the time, compared to 59.4% for rule-based and 2.5% for random assignment. Lastly, the system allows users to refine component assignments through conversational feedback, enabling greater human control and agency in making physical objects with generative AI and robotics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆ3Dç”Ÿæˆå¼AI(3D Generative AI)ä¸è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„æµæ°´çº¿ï¼Œæ—¨åœ¨å®ç°ä»è‡ªç„¶è¯­è¨€æè¿°åˆ°å¤šç»„ä»¶ç‰©ä½“æœºå™¨äººç»„è£…çš„è½¬åŒ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨VLMsè¿›è¡Œé›¶æ ·æœ¬(zero-shot)å¤šæ¨¡æ€æ¨ç†ï¼Œç»“åˆå‡ ä½•ä¸åŠŸèƒ½ä¿¡æ¯ï¼Œå°†AIç”Ÿæˆçš„ç½‘æ ¼åˆ†è§£ä¸ºåŒ…å«é¢„å®šä¹‰ç»“æ„ç»„ä»¶å’Œé¢æ¿ç»„ä»¶çš„å¤šç»„ä»¶3Dæ¨¡å‹ã€‚VLMèƒ½å¤Ÿæ ¹æ®ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’ŒåŠŸèƒ½ï¼Œæ™ºèƒ½åˆ¤æ–­å“ªäº›ç½‘æ ¼åŒºåŸŸéœ€è¦é¢æ¿ç»„ä»¶ä»¥åŠç»“æ„ç»„ä»¶ã€‚åœ¨æµ‹è¯•å¯¹è±¡çš„è¯„ä¼°ä¸­ï¼Œç”¨æˆ·å¯¹VLMç”Ÿæˆçš„ç»„ä»¶åˆ†é…æ–¹æ¡ˆåå¥½åº¦é«˜è¾¾90.6%ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºè§„åˆ™çš„æ–¹æ³•(59.4%)å’Œéšæœºåˆ†é…(2.5%)ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿå…è®¸ç”¨æˆ·é€šè¿‡å¯¹è¯åé¦ˆå¾®è°ƒç»„ä»¶åˆ†é…ï¼Œä»è€Œå¢å¼ºäº†äººç±»åœ¨åˆ©ç”¨ç”Ÿæˆå¼AIå’Œæœºå™¨äººæŠ€æœ¯åˆ¶é€ ç‰©ç†ç‰©ä½“æ—¶çš„æ§åˆ¶åŠ›å’Œä»£ç†æ„Ÿã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to NeurIPS 2025, Conference on Neural Information Processing Systems, Creative AI Track",
      "pdf_url": "https://arxiv.org/pdf/2511.02162v4",
      "published_date": "2025-11-04 01:02:21 UTC",
      "updated_date": "2025-11-22 22:47:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:25:15.065134+00:00"
    },
    {
      "arxiv_id": "2511.02157v1",
      "title": "Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games",
      "title_zh": "ä¸€èˆ¬å’Œé©¬å°”å¯å¤«åšå¼ˆä¸­ç²—ç›¸å…³å‡è¡¡çš„è¿‘ä¹æœ€ä¼˜æ”¶æ•›",
      "authors": [
        "Asrin Efe Yorulmaz",
        "Tamer BaÅŸar"
      ],
      "abstract": "No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\\mathcal{O}(\\log^5 T / T)$ to a sharper $\\mathcal{O}(\\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è‡´åŠ›äºæå‡ä¸€èˆ¬å’Œé©¬å°”å¯å¤«åšå¼ˆ(General-Sum Markov Games)ä¸­å‘ç²—ç›¸å…³å‡è¡¡(Coarse Correlated Equilibrium, CCE)æ”¶æ•›çš„æ•ˆç‡ã€‚ä½œè€…å°†æ”¶æ•›é€Ÿç‡ä»æ­¤å‰æœ€ä½³çš„$\\mathcal{O}(\\log^5 T / T)$æ˜¾è‘—ä¼˜åŒ–è‡³$\\mathcal{O}(\\log T / T)$ï¼Œè¿™ä¸€é€Ÿç‡åœ¨è¿­ä»£æ¬¡æ•°$T$ä¸Šä¸ç›¸å…³å‡è¡¡(CE)çš„æœ€ä½³å·²çŸ¥é€Ÿç‡æŒå¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å°†ç®—æ³•å¯¹åŠ¨ä½œé›†å¤§å°çš„ä¾èµ–ä»å¤šé¡¹å¼çº§é™ä½è‡³å¤šå¯¹æ•°çº§ï¼Œä»è€Œåœ¨é«˜ç»´ç¯å¢ƒä¸­å®ç°äº†æŒ‡æ•°çº§çš„æ€§èƒ½å¢ç›Šã€‚ç ”ç©¶å›¢é˜ŸåŸºäºæ­£è§„å½¢å¼åšå¼ˆä¸­æ— æ‚”å­¦ä¹ (No-regret learning)çš„è‡ªé€‚åº”æ­¥é•¿æŠ€æœ¯ï¼Œé€šè¿‡ä¸€ç§æ ¹æ®å®æ—¶åé¦ˆè°ƒæ•´å­¦ä¹ ç‡çš„é˜¶æ®µæ€§æ–¹æ¡ˆå°†å…¶æ‰©å±•è‡³é©¬å°”å¯å¤«è®¾ç½®ã€‚é€šè¿‡å°†ç­–ç•¥æ›´æ–°æ¡†æ¶åŒ–ä¸ºé’ˆå¯¹å€¼è¿­ä»£å®šåˆ¶çš„ä¹è§‚æ­£åˆ™åŒ–è·Ÿéšé¢†å¯¼è€…(Optimistic Follow-the-Regularized-Leader, OFTRL)å®ä¾‹ï¼Œè¯¥ç ”ç©¶æå‡ºçš„è‡ªåšå¼ˆç®—æ³•å®ç°äº†ç›®å‰å·²çŸ¥é©¬å°”å¯å¤«åšå¼ˆä¸­æ”¶æ•›åˆ°CCEçš„æœ€å¿«é€Ÿåº¦ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02157v1",
      "published_date": "2025-11-04 00:54:54 UTC",
      "updated_date": "2025-11-04 00:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:27:26.929147+00:00"
    },
    {
      "arxiv_id": "2511.02146v1",
      "title": "Disentangling Causal Substructures for Interpretable and Generalizable Drug Synergy Prediction",
      "title_zh": "è§£è€¦å› æœå­ç»“æ„ç”¨äºå¯è§£é‡Šä¸å¯æ³›åŒ–è¯ç‰©ååŒä½œç”¨é¢„æµ‹",
      "authors": [
        "Yi Luo",
        "Haochen Zhao",
        "Xiao Liang",
        "Yiwei Liu",
        "Yuye Zhang",
        "Xinyu Li",
        "Jianxin Wang"
      ],
      "abstract": "Drug synergy prediction is a critical task in the development of effective combination therapies for complex diseases, including cancer. Although existing methods have shown promising results, they often operate as black-box predictors that rely predominantly on statistical correlations between drug characteristics and results. To address this limitation, we propose CausalDDS, a novel framework that disentangles drug molecules into causal and spurious substructures, utilizing the causal substructure representations for predicting drug synergy. By focusing on causal sub-structures, CausalDDS effectively mitigates the impact of redundant features introduced by spurious substructures, enhancing the accuracy and interpretability of the model. In addition, CausalDDS employs a conditional intervention mechanism, where interventions are conditioned on paired molecular structures, and introduces a novel optimization objective guided by the principles of sufficiency and independence. Extensive experiments demonstrate that our method outperforms baseline models, particularly in cold start and out-of-distribution settings. Besides, CausalDDS effectively identifies key substructures underlying drug synergy, providing clear insights into how drug combinations work at the molecular level. These results underscore the potential of CausalDDS as a practical tool for predicting drug synergy and facilitating drug discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯ç‰©ååŒä½œç”¨é¢„æµ‹ä»»åŠ¡ä¸­ç°æœ‰é»‘ç›’æ¨¡å‹ä¸»è¦ä¾èµ–ç»Ÿè®¡ç›¸å…³æ€§çš„å±€é™ï¼Œæå‡ºäº†åä¸ºCausalDDSçš„æ–°é¢–æ¡†æ¶ã€‚CausalDDSé€šè¿‡å°†è¯ç‰©åˆ†å­è§£è€¦ä¸ºå› æœ(causal)å’Œè™šå‡(spurious)å­ç»“æ„ï¼Œåˆ©ç”¨å› æœå­ç»“æ„è¡¨ç¤ºæ¥é¢„æµ‹è¯ç‰©ååŒä½œç”¨ï¼Œä»è€Œæœ‰æ•ˆå‡è½»äº†å†—ä½™ç‰¹å¾çš„å½±å“ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åŸºäºé…å¯¹åˆ†å­ç»“æ„çš„æ¡ä»¶å¹²é¢„æœºåˆ¶(conditional intervention mechanism)ï¼Œå¹¶å¼•å…¥äº†ç”±å……åˆ†æ€§å’Œç‹¬ç«‹æ€§åŸåˆ™æŒ‡å¯¼çš„æ–°ä¼˜åŒ–ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCausalDDSåœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å†·å¯åŠ¨(cold start)å’Œåˆ†å¸ƒå¤–(out-of-distribution)è®¾ç½®ä¸‹è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å¯¼è‡´è¯ç‰©ååŒä½œç”¨çš„å…³é”®å­ç»“æ„ï¼Œä¸ºè¯ç‰©å‘ç°æä¾›äº†åˆ†å­å±‚é¢çš„æ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.02146v1",
      "published_date": "2025-11-04 00:32:20 UTC",
      "updated_date": "2025-11-04 00:32:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:27:52.596370+00:00"
    },
    {
      "arxiv_id": "2511.05558v1",
      "title": "Diversified Flow Matching with Translation Identifiability",
      "title_zh": "å…·æœ‰ç¿»è¯‘å¯è¯†åˆ«æ€§çš„å¤šæ ·åŒ–æµåŒ¹é…",
      "authors": [
        "Sagar Shrestha",
        "Xiao Fu"
      ],
      "abstract": "Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information -- yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Diversified Flow Matching (DFM)ï¼Œä¸€ç§åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹(ODE)çš„æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœDiversified distribution matching (DDM)æ­¤å‰ä»…ä¾èµ–GANså®ç°æ‰€å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šåŠç¼ºä¹ä¼ è¾“è½¨è¿¹ä¿¡æ¯ç­‰å±€é™ã€‚å°½ç®¡DDMèƒ½å¤Ÿè§£å†³éé…å¯¹åŸŸè½¬æ¢ä¸­çš„å†…å®¹é”™ä½é—®é¢˜å¹¶å®ç°translation identifiabilityï¼Œä½†ç”±äºFlow Matching (FM)å­¦ä¹ çš„æ˜¯é€Ÿåº¦è€Œéè½¬æ¢å‡½æ•°æœ¬èº«ï¼Œå°†å…¶é€‚é…åˆ°DDMé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè¯¥å·¥ä½œå¼•å…¥äº†å®šåˆ¶çš„åŒå±‚ä¼˜åŒ–è®­ç»ƒæŸå¤±ã€éçº¿æ€§æ’å€¼å™¨ä»¥åŠç»“æ„é‡æ„æ–¹æ³•ï¼ŒæˆåŠŸå®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚DFMæ˜¯å·²çŸ¥é¦–ä¸ªä¿è¯translation identifiabilityçš„åŸºäºODEçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›å¯¹å•ç»†èƒè¿›åŒ–åˆ†æå’Œæœºå™¨äººè·¯å¾„è§„åˆ’ç­‰åº”ç”¨è‡³å…³é‡è¦çš„è½¨è¿¹ä¿¡æ¯ã€‚åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05558v1",
      "published_date": "2025-11-04 00:12:10 UTC",
      "updated_date": "2025-11-04 00:12:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T09:28:08.099310+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 143,
  "processed_papers_count": 143,
  "failed_papers_count": 0,
  "llm_backup_calls": 286,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T09:29:43.550923+00:00"
}
{
  "date": "2025-01-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-12 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、医疗应用和大型语言模型 (LLM) 的创新应用，重点包括 LLM 在教育和医疗领域的实际落地、知名机构如 Peking University 和 Stanford 的贡献，以及一些令人印象深刻的论文，如 BUSGen 在乳腺超声图像分析中的卓越表现和 SPAM 优化器在 LLM 训练稳定性上的突破，这些工作展示了 AI 在真实场景中的潜力。\n\n### 重点论文讨论\n我们先聊聊今天最引人注目的论文，特别是那些涉及 LLM、医疗 AI 和创新代理的，相关主题放在一起讨论，以突出它们的核心贡献。\n\n**1. Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives (中文：利用 LLM 模拟患者视角以提升患者中心沟通；英文：Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives)**  \n这篇论文由 Xinyao Ma 等作者提出，探讨了 LLM 在医疗中的应用。论文的主要贡献是通过 LLM 模拟不同教育背景的患者对 ICU 出院总结的理解，准确率达 88%，但强调了在缺乏背景信息时性能下降的局限性。该发现为个性化健康沟通提供了新路径，突显 LLM 在医疗决策支持中的潜力。\n\n**2. BUSGen: A Foundational Generative Model for Breast Ultrasound Image Analysis (中文：乳腺超声图像分析的基础生成模型；英文：A Foundational Generative Model for Breast Ultrasound Image Analysis)**  \nPeking University 和 Stanford University 的团队合作，这篇论文令人印象深刻。贡献在于开发了首个针对乳腺超声的生成模型 BUSGen，使用 350 万图像预训练，能通过少样本适应实现癌症筛查和诊断，性能超越真实数据训练模型，并改善模型泛化能力。该工作保护患者隐私并提供在线演示，标志着医疗 AI 向高效、隐私友好方向的重大进展。\n\n**3. The Einstein Test: Towards a Practical Test of a Machine's Ability to Exhibit Superintelligence (中文：爱因斯坦测试：评估机器超级智能的实用方法；英文：The Einstein Test: Towards a Practical Test of a Machine's Ability to Exhibit Superintelligence)**  \n作者 David Benrimoh 等提出了一种创新测试框架，评估 AI 是否能生成创造性洞见，如相对论。论文的核心发现是，通过给定历史数据让 AI 重现已知创新，测试其超级智能潜力，这为 AI 能力评估提供了新标准，特别是在知识推理领域。\n\n**4. SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training (中文：SPAM：基于尖峰感知的 Adam 优化器及动量重置以稳定 LLM 训练；英文：SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training)**  \n这篇论文针对 LLM 训练中的梯度尖峰问题，引入 SPAM 优化器。贡献包括通过动量重置和梯度剪切减少不稳定性，在各种任务中超越 Adam 优化器，并支持内存高效训练。该发现显著提升了 LLM 的训练效率和稳定性，适用于大规模应用。\n\n**5. Eliza: A Web3 friendly AI Agent Operating System (中文：Eliza：Web3 友好的 AI 代理操作系统；英文：Eliza: A Web3 friendly AI Agent Operating System)**  \n作者团队开发了开源框架 Eliza，首次无缝整合 Web3 应用到 AI 代理中。论文的主要发现是，通过 Typescript 实现稳定性能，Eliza 能高效处理区块链交互，这为 AI 代理在去中心化环境中的部署提供了新工具。\n\n**6. Risk-Averse Finetuning of Large Language Models (中文：风险厌恶的 LLM 微调；英文：Risk-Averse Finetuning of Large Language Models)**  \n论文使用 CVaR 风险度量优化 LLM 微调，减少有害输出。贡献在于在保持生成性能的同时，提升了在线对话的安全性，该方法在 NeurIPS 2024 上发表，突出了 LLM 在实际应用中的风险管理。\n\n**7. Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning (中文：生成式 AI 在教育中的应用：从基础洞见到苏格拉底式学习平台；英文：Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning)**  \n作者 Xiangen Hu 等探索 LLM 与人类认知的结合，引入 Socratic Playground 系统。论文的核心发现是，该系统通过 transformer 模型提供个性化辅导，超越了早期 AutoTutor 系统，在教育 AI 中展示了新潜力。\n\n### 其他相关论文简评\n接下来，我们快速聊聊一些有潜力的论文，把相关主题归纳在一起。\n\n**8. MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis (中文：MedGrad E-CLIP：提升 AI 驱动皮肤病变诊断的信任和透明度；英文：MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis)**  \n这篇医疗 AI 论文提出 MedGrad E-CLIP 模型，通过加权熵机制提升解释性，在皮肤病变分类中提供可视化解释，改善了 AI 在临床中的可信度。\n\n**9. An Empirical Study of Deep Reinforcement Learning in Continuing Tasks (中文：持续任务中深度强化学习的实证研究；英文：An Empirical Study of Deep Reinforcement Learning in Continuing Tasks)**  \n论文评估了深度强化学习算法在持续任务中的表现，引入奖励居中方法改善性能。该发现扩展了强化学习的应用，特别是在 Mujoco 和 Atari 环境。\n\n**10. LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation (中文：LarvSeg：利用图像分类数据实现大词汇语义分割；英文：LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation)**  \nPeking University 的团队工作，贡献在于通过类别注意力分类器扩展语义分割词汇，首次实现 21K 类别分割，提升了图像理解的泛化能力。\n\n**11. Transfer Learning of Tabular Data by Finetuning Large Language Models (中文：通过微调 LLM 实现表格数据的迁移学习；英文：Transfer Learning of Tabular Data by Finetuning Large Language Models)**  \n论文展示了 LLM 在表格分类中的迁移学习优势，使用少量计算资源超越传统方法，适用于资源有限的场景。\n\n### 快速掠过剩余论文\n今天还有许多论文，如第12（PCB 缺陷检测）、第13（LLM 在经济行为的模拟）、第14（疼痛分类）和第15（LLM 在 Ukrainian 推理benchmark），但这些相对基础或应用性较窄，我们简要一提：它们分别优化了缺陷检测精度、跨文化行为模拟、疼痛检测算法和语言基准测试，但未有突破性创新，读者可根据具体兴趣查阅。\n\n总之，今天的 arXiv 更新强调了 AI 的实用性和可控性，LLM 在医疗和教育的应用尤其值得关注。希望这份快报能帮助你快速筛选感兴趣的论文！",
  "papers": [
    {
      "arxiv_id": "2501.06965v1",
      "title": "Kolmogorov-Arnold Recurrent Network for Short Term Load Forecasting Across Diverse Consumers",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Umair Danish",
        "Katarina Grolinger"
      ],
      "abstract": "Load forecasting plays a crucial role in energy management, directly\nimpacting grid stability, operational efficiency, cost reduction, and\nenvironmental sustainability. Traditional Vanilla Recurrent Neural Networks\n(RNNs) face issues such as vanishing and exploding gradients, whereas\nsophisticated RNNs such as LSTMs have shown considerable success in this\ndomain. However, these models often struggle to accurately capture complex and\nsudden variations in energy consumption, and their applicability is typically\nlimited to specific consumer types, such as offices or schools. To address\nthese challenges, this paper proposes the Kolmogorov-Arnold Recurrent Network\n(KARN), a novel load forecasting approach that combines the flexibility of\nKolmogorov-Arnold Networks with RNN's temporal modeling capabilities. KARN\nutilizes learnable temporal spline functions and edge-based activations to\nbetter model non-linear relationships in load data, making it adaptable across\na diverse range of consumer types. The proposed KARN model was rigorously\nevaluated on a variety of real-world datasets, including student residences,\ndetached homes, a home with electric vehicle charging, a townhouse, and\nindustrial buildings. Across all these consumer categories, KARN consistently\noutperformed traditional Vanilla RNNs, while it surpassed LSTM and Gated\nRecurrent Units (GRUs) in six buildings. The results demonstrate KARN's\nsuperior accuracy and applicability, making it a promising tool for enhancing\nload forecasting in diverse energy management scenarios.",
      "tldr_zh": "负载预测对能源管理至关重要，但传统RNN（如Vanilla RNN）存在梯度消失和爆炸问题，而LSTM和GRU虽有效，却难以捕捉复杂变化并限于特定消费者类型。论文提出Kolmogorov-Arnold Recurrent Network (KARN)，一种新方法结合Kolmogorov-Arnold Networks的灵活性和RNN的时序建模能力，使用可学习的时序样条函数和边缘激活函数来更好地处理负载数据的非线性关系，并适用于多样消费者如学生宿舍、独立房屋和工业建筑。实验结果显示，KARN在所有测试数据集上优于传统RNN，并在六座建筑中超越LSTM和GRU，提高了预测准确性和能源管理适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06965v1",
      "published_date": "2025-01-12 22:49:41 UTC",
      "updated_date": "2025-01-12 22:49:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:41:01.101837"
    },
    {
      "arxiv_id": "2501.06964v1",
      "title": "Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives",
      "title_zh": "提升以患者为中心的沟通：利用 LLMs 模拟患者视角",
      "authors": [
        "Xinyao Ma",
        "Rui Zhu",
        "Zihao Wang",
        "Jingwei Xiong",
        "Qingyu Chen",
        "Haixu Tang",
        "L. Jean Camp",
        "Lucila Ohno-Machado"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing scenarios, particularly in simulating domain-specific experts\nusing tailored prompts. This ability enables LLMs to adopt the persona of\nindividuals with specific backgrounds, offering a cost-effective and efficient\nalternative to traditional, resource-intensive user studies. By mimicking human\nbehavior, LLMs can anticipate responses based on concrete demographic or\nprofessional profiles. In this paper, we evaluate the effectiveness of LLMs in\nsimulating individuals with diverse backgrounds and analyze the consistency of\nthese simulated behaviors compared to real-world outcomes. In particular, we\nexplore the potential of LLMs to interpret and respond to discharge summaries\nprovided to patients leaving the Intensive Care Unit (ICU). We evaluate and\ncompare with human responses the comprehensibility of discharge summaries among\nindividuals with varying educational backgrounds, using this analysis to assess\nthe strengths and limitations of LLM-driven simulations. Notably, when LLMs are\nprimed with educational background information, they deliver accurate and\nactionable medical guidance 88% of the time. However, when other information is\nprovided, performance significantly drops, falling below random chance levels.\nThis preliminary study shows the potential benefits and pitfalls of\nautomatically generating patient-specific health information from diverse\npopulations. While LLMs show promise in simulating health personas, our results\nhighlight critical gaps that must be addressed before they can be reliably used\nin clinical settings. Our findings suggest that a straightforward\nquery-response model could outperform a more tailored approach in delivering\nhealth information. This is a crucial first step in understanding how LLMs can\nbe optimized for personalized health communication while maintaining accuracy.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型 (LLMs) 模拟不同背景的患者视角，以提升以患者为中心的沟通效率，作为传统用户研究的低成本替代。研究通过评估 LLMs 对 ICU 出院总结的理解和响应，并与人类行为比较，发现当 LLMs 被提示教育背景信息时，能在 88% 的情况下提供准确的医疗指导，但其他情况下表现低于随机水平。总体结果突显了 LLMs 在个性化健康沟通中的潜力，同时强调了其局限性，并建议采用简单查询-响应模型来优化准确性。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06964v1",
      "published_date": "2025-01-12 22:49:32 UTC",
      "updated_date": "2025-01-12 22:49:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:55.984978"
    },
    {
      "arxiv_id": "2501.06963v1",
      "title": "Generative Artificial Intelligence-Supported Pentesting: A Comparison between Claude Opus, GPT-4, and Copilot",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio López Martínez",
        "Alejandro Cano",
        "Antonio Ruiz-Martínez"
      ],
      "abstract": "The advent of Generative Artificial Intelligence (GenAI) has brought a\nsignificant change to our society. GenAI can be applied across numerous fields,\nwith particular relevance in cybersecurity. Among the various areas of\napplication, its use in penetration testing (pentesting) or ethical hacking\nprocesses is of special interest. In this paper, we have analyzed the potential\nof leading generic-purpose GenAI tools-Claude Opus, GPT-4 from ChatGPT, and\nCopilot-in augmenting the penetration testing process as defined by the\nPenetration Testing Execution Standard (PTES). Our analysis involved evaluating\neach tool across all PTES phases within a controlled virtualized environment.\nThe findings reveal that, while these tools cannot fully automate the\npentesting process, they provide substantial support by enhancing efficiency\nand effectiveness in specific tasks. Notably, all tools demonstrated utility;\nhowever, Claude Opus consistently outperformed the others in our experimental\nscenarios.",
      "tldr_zh": "这篇论文探讨了生成式人工智能 (GenAI) 在渗透测试 (pentesting) 中的应用，通过比较 Claude Opus、GPT-4 和 Copilot 三种工具的性能。研究采用 Penetration Testing Execution Standard (PTES) 作为框架，在控制的虚拟化环境中评估这些工具在所有测试阶段的表现。结果显示，虽然这些工具无法完全自动化 pentesting 过程，但它们能显著提升特定任务的效率和效果，其中 Claude Opus 在实验场景中表现出色，优于其他工具。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06963v1",
      "published_date": "2025-01-12 22:48:37 UTC",
      "updated_date": "2025-01-12 22:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:41:23.970611"
    },
    {
      "arxiv_id": "2501.06962v1",
      "title": "Compact Bayesian Neural Networks via pruned MCMC sampling",
      "title_zh": "通过修剪的 MCMC 采样的紧凑贝叶斯神经网络",
      "authors": [
        "Ratneel Deo",
        "Scott Sisson",
        "Jody M. Webster",
        "Rohitash Chandra"
      ],
      "abstract": "Bayesian Neural Networks (BNNs) offer robust uncertainty quantification in\nmodel predictions, but training them presents a significant computational\nchallenge. This is mainly due to the problem of sampling multimodal posterior\ndistributions using Markov Chain Monte Carlo (MCMC) sampling and variational\ninference algorithms. Moreover, the number of model parameters scales\nexponentially with additional hidden layers, neurons, and features in the\ndataset. Typically, a significant portion of these densely connected parameters\nare redundant and pruning a neural network not only improves portability but\nalso has the potential for better generalisation capabilities. In this study,\nwe address some of the challenges by leveraging MCMC sampling with network\npruning to obtain compact probabilistic models having removed redundant\nparameters. We sample the posterior distribution of model parameters (weights\nand biases) and prune weights with low importance, resulting in a compact\nmodel. We ensure that the compact BNN retains its ability to estimate\nuncertainty via the posterior distribution while retaining the model training\nand generalisation performance accuracy by adapting post-pruning resampling. We\nevaluate the effectiveness of our MCMC pruning strategy on selected benchmark\ndatasets for regression and classification problems through empirical result\nanalysis. We also consider two coral reef drill-core lithology classification\ndatasets to test the robustness of the pruning model in complex real-world\ndatasets. We further investigate if refining compact BNN can retain any loss of\nperformance. Our results demonstrate the feasibility of training and pruning\nBNNs using MCMC whilst retaining generalisation performance with over 75%\nreduction in network size. This paves the way for developing compact BNN models\nthat provide uncertainty estimates for real-world applications.",
      "tldr_zh": "本研究针对Bayesian Neural Networks (BNNs)训练的计算挑战，提出了一种结合MCMC sampling和network pruning的方法，以创建紧凑的概率模型。该方法通过采样模型参数的后验分布并修剪低重要性权重，结合post-pruning resampling来保留不确定性估计和泛化性能。实验在回归和分类基准数据集以及两个真实世界珊瑚礁岩心岩性分类数据集上验证，结果显示模型大小减少超过75%，同时保持了良好的性能，为实际应用中的不确定性量化提供了可行路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06962v1",
      "published_date": "2025-01-12 22:48:04 UTC",
      "updated_date": "2025-01-12 22:48:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:41:35.708295"
    },
    {
      "arxiv_id": "2501.06956v1",
      "title": "Patent Novelty Assessment Accelerating Innovation and Patent Prosecution",
      "title_zh": "专利新颖",
      "authors": [
        "Kapil Kashyap",
        "Sean Fargose",
        "Gandhar Dhonde",
        "Aditya Mishra"
      ],
      "abstract": "In the rapidly evolving landscape of technological innovation, safeguarding\nintellectual property rights through patents is crucial for fostering progress\nand stimulating research and development investments. This report introduces a\nground-breaking Patent Novelty Assessment and Claim Generation System,\nmeticulously crafted to dissect the inventive aspects of intellectual property\nand simplify access to extensive patent claim data. Addressing a crucial gap in\nacademic institutions, our system provides college students and researchers\nwith an intuitive platform to navigate and grasp the intricacies of patent\nclaims, particularly tailored for the nuances of Chinese patents. Unlike\nconventional analysis systems, our initiative harnesses a proprietary Chinese\nAPI to ensure unparalleled precision and relevance. The primary challenge lies\nin the complexity of accessing and comprehending diverse patent claims,\ninhibiting effective innovation upon existing ideas. Our solution aims to\novercome these barriers by offering a bespoke approach that seamlessly\nretrieves comprehensive claim information, finely tuned to the specifics of the\nChinese patent landscape. By equipping users with efficient access to\ncomprehensive patent claim information, our transformative platform seeks to\nignite informed exploration and innovation in the ever-evolving domain of\nintellectual property. Its envisioned impact transcends individual colleges,\nnurturing an environment conducive to research and development while deepening\nthe understanding of patented concepts within the academic community.",
      "tldr_zh": "这篇报告介绍了Patent Novelty Assessment and Claim Generation System，这是一个创新系统，旨在评估专利新颖性并生成索赔，从而加速创新和Patent Prosecution。该系统利用专有的Chinese API，提供一个直观的平台，针对中国专利的细微差别，帮助大学生和研究者轻松访问并理解复杂的专利索赔信息。最终，该系统克服了现有障碍，促进学术机构的研究与发展，增强知识产权领域的探索和创新环境。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06956v1",
      "published_date": "2025-01-12 22:25:46 UTC",
      "updated_date": "2025-01-12 22:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:41:47.535088"
    },
    {
      "arxiv_id": "2501.06948v1",
      "title": "The Einstein Test: Towards a Practical Test of a Machine's Ability to Exhibit Superintelligence",
      "title_zh": "翻译失败",
      "authors": [
        "David Benrimoh",
        "Nace Mikus",
        "Ariel Rosenfeld"
      ],
      "abstract": "Creative and disruptive insights (CDIs), such as the development of the\ntheory of relativity, have punctuated human history, marking pivotal shifts in\nour intellectual trajectory. Recent advancements in artificial intelligence\n(AI) have sparked debates over whether state of the art models possess the\ncapacity to generate CDIs. We argue that the ability to create CDIs should be\nregarded as a significant feature of machine superintelligence (SI).To this\nend, we propose a practical test to evaluate whether an approach to AI\ntargeting SI can yield novel insights of this kind. We propose the Einstein\ntest: given the data available prior to the emergence of a known CDI, can an AI\nindependently reproduce that insight (or one that is formally equivalent)? By\nachieving such a milestone, a machine can be considered to at least match\nhumanity's past top intellectual achievements, and therefore to have the\npotential to surpass them.",
      "tldr_zh": "该论文探讨了创造性和破坏性洞见（CDIs），如相对论的提出，如何标志着人类智力的重大飞跃，并认为AI生成CDIs是机器超级智能（SI）的重要特征。作者提出“Einstein Test”作为一种实用评估方法：给定CDI出现前的可用数据，AI是否能独立重现该洞见或等价洞见。通过此测试，AI若成功，将证明其至少能匹配人类历史顶级智力成就，并展现出超越的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06948v1",
      "published_date": "2025-01-12 21:55:04 UTC",
      "updated_date": "2025-01-12 21:55:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:00.244628"
    },
    {
      "arxiv_id": "2501.06937v1",
      "title": "An Empirical Study of Deep Reinforcement Learning in Continuing Tasks",
      "title_zh": "深度强化学习在持续任务中的实证研究",
      "authors": [
        "Yi Wan",
        "Dmytro Korenkevych",
        "Zheqing Zhu"
      ],
      "abstract": "In reinforcement learning (RL), continuing tasks refer to tasks where the\nagent-environment interaction is ongoing and can not be broken down into\nepisodes. These tasks are suitable when environment resets are unavailable,\nagent-controlled, or predefined but where all rewards-including those beyond\nresets-are critical. These scenarios frequently occur in real-world\napplications and can not be modeled by episodic tasks. While modern deep RL\nalgorithms have been extensively studied and well understood in episodic tasks,\ntheir behavior in continuing tasks remains underexplored. To address this gap,\nwe provide an empirical study of several well-known deep RL algorithms using a\nsuite of continuing task testbeds based on Mujoco and Atari environments,\nhighlighting several key insights concerning continuing tasks. Using these\ntestbeds, we also investigate the effectiveness of a method for improving\ntemporal-difference-based RL algorithms in continuing tasks by centering\nrewards, as introduced by Naik et al. (2024). While their work primarily\nfocused on this method in conjunction with Q-learning, our results extend their\nfindings by demonstrating that this method is effective across a broader range\nof algorithms, scales to larger tasks, and outperforms two other\nreward-centering approaches.",
      "tldr_zh": "本文通过实证研究探讨了深度强化学习（deep RL）算法在持续任务（continuing tasks）中的表现，这些任务无法分解为独立episode，且常见于真实世界场景。研究利用基于Mujoco和Atari环境的测试套件，评估了几种知名深度RL算法，并揭示了关键洞见，如算法在持续任务下的行为差异。结果显示，Naik et al. (2024)提出的奖励居中（reward centering）方法不仅适用于更广泛的算法，还能扩展到更大任务并优于其他两种方法，从而提升了RL算法的效能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06937v1",
      "published_date": "2025-01-12 21:24:27 UTC",
      "updated_date": "2025-01-12 21:24:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:12.464905"
    },
    {
      "arxiv_id": "2501.06929v1",
      "title": "Why are we living the age of AI applications right now? The long innovation path from AI's birth to a child's bedtime magic",
      "title_zh": "翻译失败",
      "authors": [
        "Tapio Pitkäranta"
      ],
      "abstract": "Today a four-year-old child who does not know how to read or write can now\ncreate bedtime stories with graphical illustrations and narrated audio, using\nAI tools that seamlessly transform speech into text, generate visuals, and\nconvert text back into speech in a natural and engaging manner. This remarkable\nexample demonstrates why we are living in the age of AI applications. This\npaper examines contemporary leading AI applications and traces their historical\ndevelopment, highlighting the major advancements that have enabled their\nrealization. Five key factors are identified: 1) The evolution of computational\nhardware (CPUs and GPUs), enabling the training of complex AI models 2) The\nvast digital archives provided by the World Wide Web, which serve as a\nfoundational data resource for AI systems 3) The ubiquity of mobile computing,\nwith smartphones acting as powerful, accessible small computers in the hands of\nbillions 4) The rise of industrial-scale cloud infrastructures, offering\nelastic computational power for AI training and deployment 5) Breakthroughs in\nAI research, including neural networks, backpropagation, and the \"Attention is\nAll You Need\" framework, which underpin modern AI capabilities. These\ninnovations have elevated AI from solving narrow tasks to enabling applications\nlike ChatGPT that are adaptable for numerous use cases, redefining\nhuman-computer interaction. By situating these developments within a historical\ncontext, the paper highlights the critical milestones that have made AI's\ncurrent capabilities both possible and widely accessible, offering profound\nimplications for society.",
      "tldr_zh": "这篇论文探讨了我们为何处于AI应用时代，以一个四岁孩子使用AI工具创建图文并茂的睡前故事为例，展示了AI的无缝交互能力。论文追溯了AI应用的历史发展，识别了五个关键因素：计算硬件的演变（如CPUs和GPUs）、互联网提供的海量数字档案、移动计算的普及、工业规模的云基础设施，以及AI研究突破（如neural networks、backpropagation和“Attention is All You Need”框架）。这些创新使AI从解决单一任务演变为支持多用途应用如ChatGPT，重塑了人机交互，并为社会带来了深远影响。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06929v1",
      "published_date": "2025-01-12 20:50:24 UTC",
      "updated_date": "2025-01-12 20:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:24.410878"
    },
    {
      "arxiv_id": "2501.06911v1",
      "title": "Risk-Averse Finetuning of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sapana Chaudhary",
        "Ujwal Dinesha",
        "Dileep Kalathil",
        "Srinivas Shakkottai"
      ],
      "abstract": "We consider the challenge of mitigating the generation of negative or toxic\ncontent by the Large Language Models (LLMs) in response to certain prompts. We\npropose integrating risk-averse principles into LLM fine-tuning to minimize the\noccurrence of harmful outputs, particularly rare but significant events. By\noptimizing the risk measure of Conditional Value at Risk (CVaR), our\nmethodology trains LLMs to exhibit superior performance in avoiding toxic\noutputs while maintaining effectiveness in generative tasks. Empirical\nevaluations on sentiment modification and toxicity mitigation tasks demonstrate\nthe efficacy of risk-averse reinforcement learning with human feedback (RLHF)\nin promoting a safer and more constructive online discourse environment.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在某些提示下生成负面或有毒内容的问题，提出了一种风险厌恶微调方法，以最小化有害输出的发生，特别是稀有事件。方法通过优化 Conditional Value at Risk (CVaR) 风险度量，将其整合到风险厌恶强化学习与人类反馈 (RLHF) 中，确保模型在避免毒性输出同时保持生成任务的有效性。实验结果显示，该方法在情感修改和毒性缓解任务上表现出色，提升了模型的安全性能，并有助于构建更安全、更具建设性的在线讨论环境。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.06911v1",
      "published_date": "2025-01-12 19:48:21 UTC",
      "updated_date": "2025-01-12 19:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:36.840052"
    },
    {
      "arxiv_id": "2501.06887v1",
      "title": "MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Sadia Kamal",
        "Tim Oates"
      ],
      "abstract": "As deep learning models gain attraction in medical data, ensuring transparent\nand trustworthy decision-making is essential. In skin cancer diagnosis, while\nadvancements in lesion detection and classification have improved accuracy, the\nblack-box nature of these methods poses challenges in understanding their\ndecision processes, leading to trust issues among physicians. This study\nleverages the CLIP (Contrastive Language-Image Pretraining) model, trained on\ndifferent skin lesion datasets, to capture meaningful relationships between\nvisual features and diagnostic criteria terms. To further enhance transparency,\nwe propose a method called MedGrad E-CLIP, which builds on gradient-based\nE-CLIP by incorporating a weighted entropy mechanism designed for complex\nmedical imaging like skin lesions. This approach highlights critical image\nregions linked to specific diagnostic descriptions. The developed integrated\npipeline not only classifies skin lesions by matching corresponding\ndescriptions but also adds an essential layer of explainability developed\nespecially for medical data. By visually explaining how different features in\nan image relates to diagnostic criteria, this approach demonstrates the\npotential of advanced vision-language models in medical image analysis,\nultimately improving transparency, robustness, and trust in AI-driven\ndiagnostic systems.",
      "tldr_zh": "本文针对 AI 在皮肤病变诊断中的黑盒问题，提出 MedGrad E-CLIP 方法，该方法基于 CLIP (Contrastive Language-Image Pretraining) 模型，并通过加入加权熵机制来突出图像中与诊断标准相关的关键区域，从而提升模型的透明度和可解释性。MedGrad E-CLIP 不仅实现皮肤病变的分类，还可视化地解释图像特征如何与诊断描述关联，确保在复杂医疗图像处理中提高准确性。最终，该方法增强了 AI 驱动诊断系统的稳健性和信任，为临床应用提供了更可靠的决策支持。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision Workshops (WACVW)",
      "pdf_url": "http://arxiv.org/pdf/2501.06887v1",
      "published_date": "2025-01-12 17:50:47 UTC",
      "updated_date": "2025-01-12 17:50:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:42:48.876569"
    },
    {
      "arxiv_id": "2501.06879v1",
      "title": "Defect Detection Network In PCB Circuit Devices Based on GAN Enhanced YOLOv11",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayi Huang",
        "Feiyun Zhao",
        "Lieyang Chen"
      ],
      "abstract": "This study proposes an advanced method for surface defect detection in\nprinted circuit boards (PCBs) using an improved YOLOv11 model enhanced with a\ngenerative adversarial network (GAN). The approach focuses on identifying six\ncommon defect types: missing hole, rat bite, open circuit, short circuit, burr,\nand virtual welding. By employing GAN to generate synthetic defect images, the\ndataset is augmented with diverse and realistic patterns, improving the model's\nability to generalize, particularly for complex and infrequent defects like\nburrs. The enhanced YOLOv11 model is evaluated on a PCB defect dataset,\ndemonstrating significant improvements in accuracy, recall, and robustness,\nespecially when dealing with defects in complex environments or small targets.\nThis research contributes to the broader field of electronic design automation\n(EDA), where efficient defect detection is a crucial step in ensuring\nhigh-quality PCB manufacturing. By integrating advanced deep learning\ntechniques, this approach enhances the automation and precision of defect\ndetection, reducing reliance on manual inspection and accelerating\ndesign-to-production workflows. The findings underscore the importance of\nincorporating GAN-based data augmentation and optimized detection architectures\nin EDA processes, providing valuable insights for improving reliability and\nefficiency in PCB defect detection within industrial applications.",
      "tldr_zh": "这篇论文提出了一种基于 GAN 增强的改进 YOLOv11 模型，用于检测 PCB 电路板表面的六种常见缺陷，包括 missing hole、rat bite、open circuit、short circuit、burr 和 virtual welding。  \n通过 GAN 生成合成缺陷图像来扩充数据集，该方法显著提高了模型的泛化能力，特别是针对复杂和罕见缺陷如 burr。  \n在 PCB 缺陷数据集上的实验显示，该模型在准确率、召回率和鲁棒性方面取得了显著提升，尤其适用于复杂环境或小目标缺陷。  \n这项研究为电子设计自动化 (EDA) 领域提供了高效的自动化缺陷检测方案，减少了手动检查并加速了生产流程。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06879v1",
      "published_date": "2025-01-12 17:26:24 UTC",
      "updated_date": "2025-01-12 17:26:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:43:08.300661"
    },
    {
      "arxiv_id": "2501.06869v1",
      "title": "A Foundational Generative Model for Breast Ultrasound Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Haojun Yu",
        "Youcheng Li",
        "Nan Zhang",
        "Zihan Niu",
        "Xuantong Gong",
        "Yanwen Luo",
        "Haotian Ye",
        "Siyu He",
        "Quanlin Wu",
        "Wangyan Qin",
        "Mengyuan Zhou",
        "Jie Han",
        "Jia Tao",
        "Ziwei Zhao",
        "Di Dai",
        "Di He",
        "Dong Wang",
        "Binghui Tang",
        "Ling Huo",
        "James Zou",
        "Qingli Zhu",
        "Yong Wang",
        "Liwei Wang"
      ],
      "abstract": "Foundational models have emerged as powerful tools for addressing various\ntasks in clinical settings. However, their potential development to breast\nultrasound analysis remains untapped. In this paper, we present BUSGen, the\nfirst foundational generative model specifically designed for breast ultrasound\nimage analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen\nhas acquired extensive knowledge of breast structures, pathological features,\nand clinical variations. With few-shot adaptation, BUSGen can generate\nrepositories of realistic and informative task-specific data, facilitating the\ndevelopment of models for a wide range of downstream tasks. Extensive\nexperiments highlight BUSGen's exceptional adaptability, significantly\nexceeding real-data-trained foundational models in breast cancer screening,\ndiagnosis, and prognosis. In breast cancer early diagnosis, our approach\noutperformed all board-certified radiologists (n=9), achieving an average\nsensitivity improvement of 16.5% (P-value<0.0001). Additionally, we\ncharacterized the scaling effect of using generated data which was as effective\nas the collected real-world data for training diagnostic models. Moreover,\nextensive experiments demonstrated that our approach improved the\ngeneralization ability of downstream models. Importantly, BUSGen protected\npatient privacy by enabling fully de-identified data sharing, making progress\nforward in secure medical data utilization. An online demo of BUSGen is\navailable at https://aibus.bio.",
      "tldr_zh": "本研究引入了BUSGen，这是首个专为乳腺超声图像分析设计的生成式基础模型。该模型在超过350万张图像上预训练，获得了乳腺结构、病理特征和临床变异的广泛知识，并通过few-shot adaptation生成真实且信息丰富的任务特定数据，支持下游任务如癌症筛查和诊断的模型开发。实验结果显示，BUSGen在乳腺癌早期诊断中超过了9名认证放射科医生，平均敏感度提高了16.5%（P<0.0001），并在泛化能力上显著优于使用真实数据的基线模型。此外，使用生成的匿名数据与真实数据同样有效，同时保护了患者隐私，促进了安全医疗数据共享。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Peking University; Stanford University; Peking University Cancer\n  Hospital & Institute; Peking Union Medical College Hospital; Cancer Hospital,\n  Chinese Academy of Medical Sciences",
      "pdf_url": "http://arxiv.org/pdf/2501.06869v1",
      "published_date": "2025-01-12 16:39:13 UTC",
      "updated_date": "2025-01-12 16:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:43:19.071757"
    },
    {
      "arxiv_id": "2501.06863v1",
      "title": "Transfer Learning of Tabular Data by Finetuning Large Language Models",
      "title_zh": "通过微调大语言模型进行表格数据的迁移学习",
      "authors": [
        "Shourav B. Rabbani",
        "Ibna Kowsar",
        "Manar D. Samad"
      ],
      "abstract": "Despite the artificial intelligence (AI) revolution, deep learning has yet to\nachieve much success with tabular data due to heterogeneous feature space and\nlimited sample sizes without viable transfer learning. The new era of\ngenerative AI, powered by large language models (LLM), brings unprecedented\nlearning opportunities to diverse data and domains. This paper investigates the\neffectiveness of an LLM application programming interface (API) and transfer\nlearning of LLM in tabular data classification. LLM APIs respond to input text\nprompts with tokenized data and instructions, whereas transfer learning\nfinetunes an LLM for a target classification task. This paper proposes an\nend-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten\nbenchmark data sets when large pre-trained tabular data models do not exist to\nfacilitate transfer learning. The proposed LLM finetuning method outperforms\nstate-of-the-art machine and deep learning methods on tabular data with less\nthan ten features - a standard feature size for tabular data sets. The transfer\nlearning approach uses a fraction of the computational cost of other deep\nlearning or API-based solutions while ensuring competitive or superior\nclassification performance.",
      "tldr_zh": "这篇论文探讨了通过微调大语言模型(LLM)来实现表格数据迁移学习，以解决深度学习在异质特征空间和样本大小有限方面的挑战。作者提出了一种端到端LLM微调方法，在没有预训练表格模型的情况下，对十个基准数据集进行跨数据迁移学习实验。结果显示，该方法在特征少于十个的表格数据上，优于现有机器学习和深度学习方法，同时显著降低了计算成本，确保了竞争或更优的分类性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06863v1",
      "published_date": "2025-01-12 16:23:18 UTC",
      "updated_date": "2025-01-12 16:23:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:43:31.668971"
    },
    {
      "arxiv_id": "2501.06862v1",
      "title": "LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier",
      "title_zh": "翻译失败",
      "authors": [
        "Haojun Yu",
        "Di Dai",
        "Ziwei Zhao",
        "Di He",
        "Han Hu",
        "Liwei Wang"
      ],
      "abstract": "Scaling up the vocabulary of semantic segmentation models is extremely\nchallenging because annotating large-scale mask labels is labour-intensive and\ntime-consuming. Recently, language-guided segmentation models have been\nproposed to address this challenge. However, their performance drops\nsignificantly when applied to out-of-distribution categories. In this paper, we\npropose a new large vocabulary semantic segmentation framework, called LarvSeg.\nDifferent from previous works, LarvSeg leverages image classification data to\nscale the vocabulary of semantic segmentation models as large-vocabulary\nclassification datasets usually contain balanced categories and are much easier\nto obtain. However, for classification tasks, the category is image-level,\nwhile for segmentation we need to predict the label at pixel level. To address\nthis issue, we first propose a general baseline framework to incorporate\nimage-level supervision into the training process of a pixel-level segmentation\nmodel, making the trained network perform semantic segmentation on newly\nintroduced categories in the classification data. We then observe that a model\ntrained on segmentation data can group pixel features of categories beyond the\ntraining vocabulary. Inspired by this finding, we design a category-wise\nattentive classifier to apply supervision to the precise regions of\ncorresponding categories to improve the model performance. Extensive\nexperiments demonstrate that LarvSeg significantly improves the large\nvocabulary semantic segmentation performance, especially in the categories\nwithout mask labels. For the first time, we provide a 21K-category semantic\nsegmentation model with the help of ImageNet21K. The code is available at\nhttps://github.com/HaojunYu1998/large_voc_seg.",
      "tldr_zh": "该论文提出LarvSeg框架，利用图像分类数据扩展语义分割模型的词汇量，以解决大规模掩码标签标注的难题。LarvSeg将图像级别的分类监督整合到像素级分割训练中，作为基线框架，从而使模型能够处理分类数据中新引入的类别。同时，作者观察到模型能对训练词汇外的类别进行像素特征分组，并设计了category-wise attentive classifier，对对应类别的精确区域施加监督，以提升性能。实验结果显示，LarvSeg显著提高了大词汇语义分割的准确率，尤其在无掩码标签的类别上，并首次基于ImageNet21K构建了21K-类别语义分割模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "PRCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.06862v1",
      "published_date": "2025-01-12 16:22:17 UTC",
      "updated_date": "2025-01-12 16:22:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:43:43.775232"
    },
    {
      "arxiv_id": "2501.06859v1",
      "title": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context",
      "title_zh": "对大语言模型在阿拉伯语语境中精神疾病的全面评估",
      "authors": [
        "Noureldin Zahran",
        "Aya E. Fouda",
        "Radwa J. Hanafy",
        "Mohammed E. Fouda"
      ],
      "abstract": "Mental health disorders pose a growing public health concern in the Arab\nworld, emphasizing the need for accessible diagnostic and intervention tools.\nLarge language models (LLMs) offer a promising approach, but their application\nin Arabic contexts faces challenges including limited labeled datasets,\nlinguistic complexity, and translation biases. This study comprehensively\nevaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual\nones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),\ninvestigating the impact of prompt design, language configuration (native\nArabic vs. translated English, and vice versa), and few-shot prompting on\ndiagnostic performance. We find that prompt engineering significantly\ninfluences LLM scores mainly due to reduced instruction following, with our\nstructured prompt outperforming a less structured variant on multi-class\ndatasets, with an average difference of 14.5\\%. While language influence on\nperformance was modest, model selection proved crucial: Phi-3.5 MoE excelled in\nbalanced accuracy, particularly for binary classification, while Mistral NeMo\nshowed superior performance in mean absolute error for severity prediction\ntasks. Few-shot prompting consistently improved performance, with particularly\nsubstantial gains observed for GPT-4o Mini on multi-class classification,\nboosting accuracy by an average factor of 1.58. These findings underscore the\nimportance of prompt optimization, multilingual analysis, and few-shot learning\nfor developing culturally sensitive and effective LLM-based mental health tools\nfor Arabic-speaking populations.",
      "tldr_zh": "本研究评估了8个LLMs（包括多语言和双语模型）在阿拉伯语心理健康数据集（如AraDepSu、Dreaddit、MedMCQA）上的诊断性能，探讨了提示设计、语言配置（原生阿拉伯语 vs. 翻译英语等）和few-shot prompting的影响。\n结果表明，结构化提示显著提升了LLMs的表现，平均比非结构化提示高出14.5%；Phi-3.5 MoE在平衡准确率上尤其出色，而Mistral NeMo在均绝对误差的严重度预测任务中领先。\n此外，few-shot prompting一致改善了性能，特别是使GPT-4o Mini的多类分类准确率提升1.58倍，这些发现强调了prompt optimization和多语言分析在开发文化敏感的阿拉伯语LLMs心理健康工具中的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06859v1",
      "published_date": "2025-01-12 16:17:25 UTC",
      "updated_date": "2025-01-12 16:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:43:57.033870"
    },
    {
      "arxiv_id": "2501.06857v1",
      "title": "What Is a Counterfactual Cause in Action Theories?",
      "title_zh": "什么是行动理论中的反事实原因？",
      "authors": [
        "Daxin Liu",
        "Vaishak Belle"
      ],
      "abstract": "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
      "tldr_zh": "该论文探讨了在行动理论中，反事实因果（counterfactual cause）的定义，旨在解决现有实际因果概念的局限性。作者基于反事实分析（counterfactual analysis），提出了一种新的因果概念，并将其推广到成就因果（achievement cause），适用于行动历史中的量化效果。论文比较了这一概念与Batusov和Soutchanski的情境演算（situation calculus）成就因果框架的差异，并与Halpern和Pearl的实际因果（actual causality）理论相关联，特别强调了在处理析取目标（disjunctive goals）时的细微差别。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is an extended report of our short paper accepted at AAMAS 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06857v1",
      "published_date": "2025-01-12 16:15:12 UTC",
      "updated_date": "2025-01-12 16:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:44:07.291158"
    },
    {
      "arxiv_id": "2501.09029v1",
      "title": "Enhancing Data Integrity through Provenance Tracking in Semantic Web Frameworks",
      "title_zh": "翻译失败",
      "authors": [
        "Nilesh Jain"
      ],
      "abstract": "This paper explores the integration of provenance tracking systems within the\ncontext of Semantic Web technologies to enhance data integrity in diverse\noperational environments. SURROUND Australia Pty Ltd demonstrates innovative\napplica-tions of the PROV Data Model (PROV-DM) and its Semantic Web variant,\nPROV-O, to systematically record and manage provenance information across\nmultiple data processing domains. By employing RDF and Knowledge Graphs,\nSURROUND ad-dresses the critical challenges of shared entity identification and\nprovenance granularity. The paper highlights the company's architecture for\ncapturing comprehensive provenance data, en-abling robust validation,\ntraceability, and knowledge inference. Through the examination of two projects,\nwe illustrate how provenance mechanisms not only improve data reliability but\nalso facilitate seamless integration across heterogeneous systems. Our findings\nunderscore the importance of sophisticated provenance solutions in maintaining\ndata integrity, serving as a reference for industry peers and academics engaged\nin provenance research and implementation.",
      "tldr_zh": "这篇论文探讨了在语义网络框架中整合provenance tracking系统，以提升数据完整性。SURROUND Australia Pty Ltd 利用PROV Data Model (PROV-DM)和其语义网络变体PROV-O、结合RDF和Knowledge Graphs，解决了共享实体识别和provenance粒度等关键挑战，并构建了架构来捕获全面的provenance数据，支持数据验证、可追溯性和知识推理。通过分析两个实际项目，论文证明了provenance机制能显著改善数据可靠性和异构系统间的无缝集成，为行业和学术界提供维护数据完整性的参考方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68T30, 68T35, 68P15: Covers knowledge representation, Semantic Web\n  applications, and database theory for provenance tracking and data integrity"
      ],
      "primary_category": "cs.CR",
      "comment": "This 10-page manuscript with 5 figures focuses on leveraging Semantic\n  Web frameworks to enhance data integrity through provenance tracking.\n  Intended for conference submission, it aligns with the cs.AI category,\n  addressing knowledge representation, data modeling, and uncertainty in AI\n  using advanced tools like PROV-DM and PROV-O",
      "pdf_url": "http://arxiv.org/pdf/2501.09029v1",
      "published_date": "2025-01-12 16:13:27 UTC",
      "updated_date": "2025-01-12 16:13:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:44:19.546796"
    },
    {
      "arxiv_id": "2501.06842v2",
      "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
      "title_zh": "翻译失败",
      "authors": [
        "Tianjin Huang",
        "Ziquan Zhu",
        "Gaojie Jin",
        "Lu Liu",
        "Zhangyang Wang",
        "Shiwei Liu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLM)训练中的不稳定性问题，主要源于梯度峰值(spike)，这些峰值可能比典型梯度大1000倍，导致训练中断和资源浪费。作者提出了一种新型优化器SPAM（Spike-Aware Adam with Momentum Reset），通过动量重置(momentum reset)和峰值感知梯度裁剪(spike-aware gradient clipping)来有效缓解这些问题。实验结果显示，SPAM在各种任务中（如LLM预训练从60M到1B参数、4-bit预训练、强化学习和时间序列预测）均优于Adam及其变体，提高了训练稳定性和性能。此外，SPAM支持内存高效训练，通过稀疏动量(sparse momentum)机制，在内存受限场景下超越GaLore和Adam-Mini优化器，为大规模LLM训练提供了更可靠的策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06842v2",
      "published_date": "2025-01-12 15:21:22 UTC",
      "updated_date": "2025-02-28 15:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:44:32.160102"
    },
    {
      "arxiv_id": "2501.06837v1",
      "title": "An efficient approach to represent enterprise web application structure using Large Language Model in the service of Intelligent Quality Engineering",
      "title_zh": "一种高效的方法，使用大型语言模型表示企业网络应用结构，以服务于智能质量工程",
      "authors": [
        "Zaber Al Hassan Ayon",
        "Gulam Husain",
        "Roshankumar Bisoi",
        "Waliur Rahman",
        "Dr Tom Osborn"
      ],
      "abstract": "This paper presents a novel approach to represent enterprise web application\nstructures using Large Language Models (LLMs) to enable intelligent quality\nengineering at scale. We introduce a hierarchical representation methodology\nthat optimizes the few-shot learning capabilities of LLMs while preserving the\ncomplex relationships and interactions within web applications. The approach\nencompasses five key phases: comprehensive DOM analysis, multi-page synthesis,\ntest suite generation, execution, and result analysis. Our methodology\naddresses existing challenges around usage of Generative AI techniques in\nautomated software testing by developing a structured format that enables LLMs\nto understand web application architecture through in-context learning. We\nevaluated our approach using two distinct web applications: an e-commerce\nplatform (Swag Labs) and a healthcare application (MediBox) which is deployed\nwithin Atalgo engineering environment. The results demonstrate success rates of\n90\\% and 70\\%, respectively, in achieving automated testing, with high\nrelevance scores for test cases across multiple evaluation criteria. The\nfindings suggest that our representation approach significantly enhances LLMs'\nability to generate contextually relevant test cases and provide better quality\nassurance overall, while reducing the time and effort required for testing.",
      "tldr_zh": "本论文提出了一种高效方法，使用 Large Language Models (LLMs) 来表示企业 web 应用结构，以支持大规模的智能质量工程。该方法采用分层表示策略，优化 LLMs 的 few-shot learning 能力，同时通过五个关键阶段——全面 DOM 分析、多页面合成、测试套件生成、执行和结果分析——来保留应用中的复杂关系和交互，并在 in-context learning 中解决 Generative AI 在自动化软件测试中的挑战。在对 Swag Labs 和 MediBox 两个 web 应用的评估中，该方法分别实现了 90% 和 70% 的自动化测试成功率，并显著提高了测试用例的相关性，减少了测试时间和努力，从而提升了整体质量保证水平。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 1 figure and 4 tables, relevant for Gen AI and enterprise\n  AI use cases",
      "pdf_url": "http://arxiv.org/pdf/2501.06837v1",
      "published_date": "2025-01-12 15:10:57 UTC",
      "updated_date": "2025-01-12 15:10:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:44:44.660015"
    },
    {
      "arxiv_id": "2501.06834v1",
      "title": "LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents",
      "title_zh": "LLMs 建模非 WEIRD 人群：使用合成文化代理的实验",
      "authors": [
        "Augusto Gonzalez-Bonorino",
        "Monica Capra",
        "Emilio Pantoja"
      ],
      "abstract": "Despite its importance, studying economic behavior across diverse, non-WEIRD\n(Western, Educated, Industrialized, Rich, and Democratic) populations presents\nsignificant challenges. We address this issue by introducing a novel\nmethodology that uses Large Language Models (LLMs) to create synthetic cultural\nagents (SCAs) representing these populations. We subject these SCAs to classic\nbehavioral experiments, including the dictator and ultimatum games. Our results\ndemonstrate substantial cross-cultural variability in experimental behavior.\nNotably, for populations with available data, SCAs' behaviors qualitatively\nresemble those of real human subjects. For unstudied populations, our method\ncan generate novel, testable hypotheses about economic behavior. By integrating\nAI into experimental economics, this approach offers an effective and ethical\nmethod to pilot experiments and refine protocols for hard-to-reach populations.\nOur study provides a new tool for cross-cultural economic studies and\ndemonstrates how LLMs can help experimental behavioral research.",
      "tldr_zh": "该研究提出了一种新方法，使用 Large Language Models (LLMs) 创建 Synthetic Cultural Agents (SCAs) 来模拟非-WEIRD (Western, Educated, Industrialized, Rich, and Democratic) 人群的经济行为。研究者对这些 SCAs 进行了经典实验，如 dictator game 和 ultimatum game，结果显示了显著的跨文化变异性，且 SCAs 的行为与真实人类主体在有数据支持的群体中高度相似。对于未研究群体，该方法能生成可测试的新假设。整体而言，此方法为跨文化经济研究提供了高效且道德的工具，帮助试点实验并优化针对难以接触人群的协议。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06834v1",
      "published_date": "2025-01-12 15:06:28 UTC",
      "updated_date": "2025-01-12 15:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:44:55.649865"
    },
    {
      "arxiv_id": "2501.06831v1",
      "title": "Towards Counterfactual and Contrastive Explainability and Transparency of DCNN Image Classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Syed Ali Tariq",
        "Tehseen Zia",
        "Mubeen Ghafoor"
      ],
      "abstract": "Explainability of deep convolutional neural networks (DCNNs) is an important\nresearch topic that tries to uncover the reasons behind a DCNN model's\ndecisions and improve their understanding and reliability in high-risk\nenvironments. In this regard, we propose a novel method for generating\ninterpretable counterfactual and contrastive explanations for DCNN models. The\nproposed method is model intrusive that probes the internal workings of a DCNN\ninstead of altering the input image to generate explanations. Given an input\nimage, we provide contrastive explanations by identifying the most important\nfilters in the DCNN representing features and concepts that separate the\nmodel's decision between classifying the image to the original inferred class\nor some other specified alter class. On the other hand, we provide\ncounterfactual explanations by specifying the minimal changes necessary in such\nfilters so that a contrastive output is obtained.\n  Using these identified filters and concepts, our method can provide\ncontrastive and counterfactual reasons behind a model's decisions and makes the\nmodel more transparent. One of the interesting applications of this method is\nmisclassification analysis, where we compare the identified concepts from a\nparticular input image and compare them with class-specific concepts to\nestablish the validity of the model's decisions. The proposed method is\ncompared with state-of-the-art and evaluated on the Caltech-UCSD Birds (CUB)\n2011 dataset to show the usefulness of the explanations provided.",
      "tldr_zh": "本研究针对深度卷积神经网络(DCNN)图像分类器的可解释性，提出了一种新型模型侵入式方法，用于生成反事实(counterfactual)和对比性(contrastive)解释。该方法通过识别DCNN中最重要的过滤器，分析区分原始分类与替代分类的特征和概念，从而提供对比性解释；同时，通过指定这些过滤器的最小变化来生成反事实解释，提高模型决策的透明度。实验在Caltech-UCSD Birds (CUB) 2011数据集上与最先进方法比较，展示了该方法的有效性，并将其应用于误分类分析，以验证模型决策的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06831v1",
      "published_date": "2025-01-12 14:54:02 UTC",
      "updated_date": "2025-01-12 14:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:45:07.348255"
    },
    {
      "arxiv_id": "2501.06827v1",
      "title": "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification",
      "title_zh": "利用分类法和 LLMs 实现多模态层次分类的改进",
      "authors": [
        "Shijing Chen",
        "Mohamed Reda Bouadjenek",
        "Shoaib Jameel",
        "Usman Naseem",
        "Basem Suleiman",
        "Flora D. Salim",
        "Hakim Hacid",
        "Imran Razzak"
      ],
      "abstract": "Multi-level Hierarchical Classification (MLHC) tackles the challenge of\ncategorizing items within a complex, multi-layered class structure. However,\ntraditional MLHC classifiers often rely on a backbone model with independent\noutput layers, which tend to ignore the hierarchical relationships between\nclasses. This oversight can lead to inconsistent predictions that violate the\nunderlying taxonomy. Leveraging Large Language Models (LLMs), we propose a\nnovel taxonomy-embedded transitional LLM-agnostic framework for multimodality\nclassification. The cornerstone of this advancement is the ability of models to\nenforce consistency across hierarchical levels. Our evaluations on the MEP-3M\ndataset - a multi-modal e-commerce product dataset with various hierarchical\nlevels - demonstrated a significant performance improvement compared to\nconventional LLM structures.",
      "tldr_zh": "该研究针对多级层次分类（MLHC）中的问题，提出了一种新型框架，该框架利用大型语言模型（LLMs）和分类体系（taxonomy），以改善多模态层次分类的性能。传统方法往往忽略类之间的层次关系，导致预测不一致，而新框架通过嵌入分类体系并强制执行层次水平的一致性，实现LLM无关的过渡设计。实验在MEP-3M数据集上显示，该框架相较于传统LLM结构取得了显著的性能提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 7 figures, 2 tables, and accepted by COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06827v1",
      "published_date": "2025-01-12 14:43:06 UTC",
      "updated_date": "2025-01-12 14:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:45:19.052804"
    },
    {
      "arxiv_id": "2501.06823v1",
      "title": "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction",
      "title_zh": "MEXA-CTP",
      "authors": [
        "Yiqing Zhang",
        "Xiaozhong Liu",
        "Fabricio Murai"
      ],
      "abstract": "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
      "tldr_zh": "该研究针对临床试验结果预测的挑战，提出了一种轻量级注意力模型 MEXA-CTP，用于整合药物分子、目标疾病和资格标准等多模式数据，以克服现有方法如 HINT 对湿实验室数据和先验知识的依赖。MEXA-CTP 通过“模式专家”模块和交叉注意力机制生成有效表示，并采用 Cauchy loss 优化跨模式互动，避免人为偏差。实验结果显示，在 Trial Outcome Prediction (TOP) 基准测试中，该模型分别将 F1 score 提高 11.3%、PR-AUC 提高 12.2% 和 ROC-AUC 提高 2.5%，相比 HINT 取得了显著提升。消融研究进一步量化了模型各组件的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted and to be published in SDM2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06823v1",
      "published_date": "2025-01-12 14:35:31 UTC",
      "updated_date": "2025-01-12 14:35:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:45:32.969295"
    },
    {
      "arxiv_id": "2501.06819v1",
      "title": "A Study on Educational Data Analysis and Personalized Feedback Report Generation Based on Tags and ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhou Zhou",
        "Mengqiao Zhang",
        "Yuan-Hao Jiang",
        "Xinyu Gao",
        "Naijie Liu",
        "Bo Jiang"
      ],
      "abstract": "This study introduces a novel method that employs tag annotation coupled with\nthe ChatGPT language model to analyze student learning behaviors and generate\npersonalized feedback. Central to this approach is the conversion of complex\nstudent data into an extensive set of tags, which are then decoded through\ntailored prompts to deliver constructive feedback that encourages rather than\ndiscourages students. This methodology focuses on accurately feeding student\ndata into large language models and crafting prompts that enhance the\nconstructive nature of feedback. The effectiveness of this approach was\nvalidated through surveys conducted with over 20 mathematics teachers, who\nconfirmed the reliability of the generated reports. This method can be\nseamlessly integrated into intelligent adaptive learning systems or provided as\na tool to significantly reduce the workload of teachers, providing accurate and\ntimely feedback to students. By transforming raw educational data into\ninterpretable tags, this method supports the provision of efficient and timely\npersonalized learning feedback that offers constructive suggestions tailored to\nindividual learner needs.",
      "tldr_zh": "本研究提出了一种基于标签标注（tag annotation）和 ChatGPT 的新方法，用于分析学生学习行为并生成个性化反馈。该方法将复杂的学生数据转换为一组标签，然后通过定制提示解码这些标签，以提供建设性而非消极的反馈建议。通过对20多名数学老师的调查验证，该方法被确认可靠，并能有效减少教师工作量。该方法可无缝整合到智能适应性学习系统中，提供准确、及时的个性化学习支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06819v1",
      "published_date": "2025-01-12 14:23:17 UTC",
      "updated_date": "2025-01-12 14:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:45:43.406770"
    },
    {
      "arxiv_id": "2501.06802v2",
      "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Wan"
      ],
      "abstract": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the\nrelationships between model loss and the scale of parameters, data, and\ntraining computation. In 2024, OpenAI proposed the second type of Scaling Laws,\ndescribing the relationship between model inference performance and inference\ncomputation. In this paper, we analyze LLMs training and inference processes\nfrom the perspective of lossless compression using conditional Kolmogorov\ncomplexity, and unify these two types of Scaling Laws. We find that both types\nof Scaling Laws improve approximation of conditional Kolmogorov complexity by\nincreasing execution steps of Turing machine. The first type of Scaling Laws\nincreases execution steps by increasing number of model parameters. The second\ntype of Scaling Laws increases execution steps by increasing the number of\nintermediate tokens.",
      "tldr_zh": "本论文从条件Kolmogorov复杂度的视角，统一了OpenAI在2020年提出的第一类Scaling Laws（描述模型损失与参数、数据和计算规模的关系）和2024年提出的第二类Scaling Laws（描述模型推理性能与推理计算的关系）。通过分析LLMs的训练和推理过程，作者发现这两类定律都通过增加Turing机器的执行步骤来改进对条件Kolmogorov复杂度的逼近，其中第一类Scaling Laws依赖于增加模型参数，第二类则依赖于增加中间tokens。最终，这为理解AI模型扩展与性能的关系提供了统一的理论框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06802v2",
      "published_date": "2025-01-12 12:52:52 UTC",
      "updated_date": "2025-02-10 13:55:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:45:55.389759"
    },
    {
      "arxiv_id": "2501.06795v1",
      "title": "Bridging the Fairness Gap: Enhancing Pre-trained Models with LLM-Generated Sentences",
      "title_zh": "翻译失败",
      "authors": [
        "Liu Yu",
        "Ludie Guo",
        "Ping Kuang",
        "Fan Zhou"
      ],
      "abstract": "Pre-trained language models (PLMs) are trained on data that inherently\ncontains gender biases, leading to undesirable impacts. Traditional debiasing\nmethods often rely on external corpora, which may lack quality, diversity, or\ndemographic balance, affecting the effectiveness of debiasing. With the rise of\nlarge language models and their extensive knowledge, we propose enhancing\nfairness (Fair-Gender) in PLMs by absorbing coherent, attribute-balanced, and\nsemantically rich sentences. However, these sentences cannot be directly used\nfor debiasing due to alignment issues and the risk of negative transfer. We\naddress this by applying causal analysis to estimate causal effects, filtering\nout unaligned sentences, and identifying aligned ones for incorporation into\nPLMs, thereby ensuring positive transfer. Experiments show that our approach\nsignificantly reduces gender biases in PLMs while preserving their language\nexpressiveness.",
      "tldr_zh": "本研究针对预训练语言模型 (PLMs) 中的性别偏见问题，提出了一种名为 Fair-Gender 的方法，利用大型语言模型 (LLM) 生成的连贯、属性平衡且语义丰富的句子来增强模型公平性。  \n为了克服这些句子直接应用的对齐问题和负面转移风险，研究采用因果分析 (causal analysis) 来估计因果效应，过滤不匹配的句子并选择合适的句子整合到 PLMs 中，从而确保正面转移。  \n实验结果显示，该方法显著减少了 PLMs 的性别偏见，同时保留了模型的语言表现力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06795v1",
      "published_date": "2025-01-12 12:32:43 UTC",
      "updated_date": "2025-01-12 12:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:46:08.106712"
    },
    {
      "arxiv_id": "2501.06787v3",
      "title": "Improving Pain Classification using Spatio-Temporal Deep Learning Approaches with Facial Expressions",
      "title_zh": "翻译失败",
      "authors": [
        "Aafaf Ridouan",
        "Amine Bohi",
        "Youssef Mourchid"
      ],
      "abstract": "Pain management and severity detection are crucial for effective treatment,\nyet traditional self-reporting methods are subjective and may be unsuitable for\nnon-verbal individuals (people with limited speaking skills). To address this\nlimitation, we explore automated pain detection using facial expressions. Our\nstudy leverages deep learning techniques to improve pain assessment by\nanalyzing facial images from the Pain Emotion Faces Database (PEMF). We propose\ntwo novel approaches1: (1) a hybrid ConvNeXt model combined with Long\nShort-Term Memory (LSTM) blocks to analyze video frames and predict pain\npresence, and (2) a Spatio-Temporal Graph Convolution Network (STGCN)\nintegrated with LSTM to process landmarks from facial images for pain\ndetection. Our work represents the first use of the PEMF dataset for binary\npain classification and demonstrates the effectiveness of these models through\nextensive experimentation. The results highlight the potential of combining\nspatial and temporal features for enhanced pain detection, offering a promising\nadvancement in objective pain assessment methodologies.",
      "tldr_zh": "这篇论文针对传统疼痛评估方法的局限性（如主观性和不适合非语言个体），提出使用面部表情进行自动疼痛检测，基于 Pain Emotion Faces Database (PEMF) 数据集。研究者开发了两种新方法：一是将 ConvNeXt 模型与 Long Short-Term Memory (LSTM) 结合，分析视频帧以预测疼痛存在；二是使用 Spatio-Temporal Graph Convolution Network (STGCN) 与 LSTM 整合，处理面部 landmarks 进行疼痛检测。这是首次将 PEMF 用于二元疼痛分类，实验结果证明结合空间和时间特征显著提升了检测效果，为客观疼痛评估方法提供了重要进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures, 3 tables. Accepted and presented at the 18th\n  International Conference on Machine Vision (ICMV 2024), Edinburgh, UK",
      "pdf_url": "http://arxiv.org/pdf/2501.06787v3",
      "published_date": "2025-01-12 11:54:46 UTC",
      "updated_date": "2025-02-26 09:33:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:46:19.391815"
    },
    {
      "arxiv_id": "2501.06783v2",
      "title": "Cost-Effective Robotic Handwriting System with AI Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyi Huang",
        "Richard Xiong"
      ],
      "abstract": "This paper introduces a cost-effective robotic handwriting system designed to\nreplicate human-like handwriting with high precision. Combining a Raspberry Pi\nPico microcontroller, 3D-printed components, and a machine learning-based\nhandwriting generation model implemented via TensorFlow, the system converts\nuser-supplied text into realistic stroke trajectories. By leveraging\nlightweight 3D-printed materials and efficient mechanical designs, the system\nachieves a total hardware cost of approximately \\$56, significantly\nundercutting commercial alternatives. Experimental evaluations demonstrate\nhandwriting precision within $\\pm$0.3 millimeters and a writing speed of\napproximately 200 mm/min, positioning the system as a viable solution for\neducational, research, and assistive applications. This study seeks to lower\nthe barriers to personalized handwriting technologies, making them accessible\nto a broader audience.",
      "tldr_zh": "这篇论文提出了一种成本有效的机器人手写系统，通过整合Raspberry Pi Pico微控制器、3D打印组件和基于TensorFlow的机器学习模型，实现对用户文本的高精度笔迹轨迹转换。系统利用轻量级硬件设计，将总成本控制在约56美元，大大低于商业产品。实验评估显示，手写精度达到±0.3毫米，书写速度约为200 mm/min，并适用于教育、研究和辅助应用。该研究旨在降低个性化手写技术的门槛，使其更易于大众使用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "This is an updated version of a paper originally presented at the\n  2024 IEEE Long Island Systems, Applications and Technology Conference (LISAT)",
      "pdf_url": "http://arxiv.org/pdf/2501.06783v2",
      "published_date": "2025-01-12 11:42:28 UTC",
      "updated_date": "2025-01-14 03:16:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:46:31.398551"
    },
    {
      "arxiv_id": "2501.06781v2",
      "title": "Eliza: A Web3 friendly AI Agent Operating System",
      "title_zh": "Eliza：一个Web3友好的AI代理操作系统",
      "authors": [
        "Shaw Walters",
        "Sam Gao",
        "Shakker Nerd",
        "Feng Da",
        "Warren Williams",
        "Ting-Chien Meng",
        "Amie Chow",
        "Hunter Han",
        "Frank He",
        "Allen Zhang",
        "Ming Wu",
        "Timothy Shen",
        "Maxwell Hu",
        "Jerry Yan"
      ],
      "abstract": "AI Agent, powered by large language models (LLMs) as its cognitive core, is\nan intelligent agentic system capable of autonomously controlling and\ndetermining the execution paths under user's instructions. With the burst of\ncapabilities of LLMs and various plugins, such as RAG, text-to-image/video/3D,\netc., the potential of AI Agents has been vastly expanded, with their\ncapabilities growing stronger by the day. However, at the intersection between\nAI and web3, there is currently no ideal agentic framework that can seamlessly\nintegrate web3 applications into AI agent functionalities. In this paper, we\npropose Eliza, the first open-source web3-friendly Agentic framework that makes\nthe deployment of web3 applications effortless. We emphasize that every aspect\nof Eliza is a regular Typescript program under the full control of its user,\nand it seamlessly integrates with web3 (i.e., reading and writing blockchain\ndata, interacting with smart contracts, etc.). Furthermore, we show how stable\nperformance is achieved through the pragmatic implementation of the key\ncomponents of Eliza's runtime. Our code is publicly available at\nhttps://github.com/ai16z/eliza.",
      "tldr_zh": "这篇论文介绍了Eliza，一种web3友好的AI Agent操作系统，由大型语言模型(LLMs)作为核心驱动，能够自主执行用户指令并集成各种插件如RAG和文本到图像/视频功能。\nEliza是首个开源框架，使web3应用的部署变得简单高效，所有组件均为用户完全控制的Typescript程序，并无缝支持区块链数据交互和智能合约操作。\n通过优化运行时关键组件，Eliza实现了稳定性能，为AI与web3的融合提供了实用解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06781v2",
      "published_date": "2025-01-12 11:35:04 UTC",
      "updated_date": "2025-01-24 03:37:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:46:42.839752"
    },
    {
      "arxiv_id": "2501.06766v1",
      "title": "On the Complexity of Global Necessary Reasons to Explain Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Marco Calautti",
        "Enrico Malizia",
        "Cristian Molinaro"
      ],
      "abstract": "Explainable AI has garnered considerable attention in recent years, as\nunderstanding the reasons behind decisions or predictions made by AI systems is\ncrucial for their successful adoption. Explaining classifiers' behavior is one\nprominent problem. Work in this area has proposed notions of both local and\nglobal explanations, where the former are concerned with explaining a\nclassifier's behavior for a specific instance, while the latter are concerned\nwith explaining the overall classifier's behavior regardless of any specific\ninstance. In this paper, we focus on global explanations, and explain\nclassification in terms of ``minimal'' necessary conditions for the classifier\nto assign a specific class to a generic instance. We carry out a thorough\ncomplexity analysis of the problem for natural minimality criteria and\nimportant families of classifiers considered in the literature.",
      "tldr_zh": "本研究探讨了Explainable AI中全局解释的复杂性问题，专注于使用“最小”必要条件来解释分类器为泛化实例分配特定类别的行为。作者针对自然的最小性标准和文献中重要分类器家族进行了彻底的复杂性分析，以揭示全局解释的计算挑战。该工作有助于提升对AI决策过程的整体理解，推动可解释AI的实际应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06766v1",
      "published_date": "2025-01-12 10:25:14 UTC",
      "updated_date": "2025-01-12 10:25:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:46:54.480309"
    },
    {
      "arxiv_id": "2501.06749v1",
      "title": "Static Segmentation by Tracking: A Frustratingly Label-Efficient Approach to Fine-Grained Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyang Feng",
        "Zihe Wang",
        "Saul Ibaven Bueno",
        "Tomasz Frelek",
        "Advikaa Ramesh",
        "Jingyan Bai",
        "Lemeng Wang",
        "Zanming Huang",
        "Jianyang Gu",
        "Jinsu Yoo",
        "Tai-Yu Pan",
        "Arpita Chowdhury",
        "Michelle Ramirez",
        "Elizabeth G. Campolongo",
        "Matthew J. Thompson",
        "Christopher G. Lawrence",
        "Sydne Record",
        "Neil Rosser",
        "Anuj Karpatne",
        "Daniel Rubenstein",
        "Hilmar Lapp",
        "Charles V. Stewart",
        "Tanya Berger-Wolf",
        "Yu Su",
        "Wei-Lun Chao"
      ],
      "abstract": "We study image segmentation in the biological domain, particularly trait and\npart segmentation from specimen images (e.g., butterfly wing stripes or beetle\nbody parts). This is a crucial, fine-grained task that aids in understanding\nthe biology of organisms. The conventional approach involves hand-labeling\nmasks, often for hundreds of images per species, and training a segmentation\nmodel to generalize these labels to other images, which can be exceedingly\nlaborious. We present a label-efficient method named Static Segmentation by\nTracking (SST). SST is built upon the insight: while specimens of the same\nspecies have inherent variations, the traits and parts we aim to segment show\nup consistently. This motivates us to concatenate specimen images into a\n``pseudo-video'' and reframe trait and part segmentation as a tracking problem.\nConcretely, SST generates masks for unlabeled images by propagating annotated\nor predicted masks from the ``pseudo-preceding'' images. Powered by Segment\nAnything Model 2 (SAM~2) initially developed for video segmentation, we show\nthat SST can achieve high-quality trait and part segmentation with merely one\nlabeled image per species -- a breakthrough for analyzing specimen images. We\nfurther develop a cycle-consistent loss to fine-tune the model, again using one\nlabeled image. Additionally, we highlight the broader potential of SST,\nincluding one-shot instance segmentation on images taken in the wild and\ntrait-based image retrieval.",
      "tldr_zh": "本文提出了一种标签高效的方法 Static Segmentation by Tracking (SST)，用于生物领域细粒度图像分割（如蝴蝶翅膀条纹或甲虫身体部分），只需一个标注图像即可实现高质量特征和部分分割。SST 通过将标本图像串联成“伪视频”，利用 Segment Anything Model 2 (SAM 2) 将分割问题转化为跟踪问题，并通过传播掩码来生成未标注图像的分割结果。作者进一步引入 cycle-consistent loss 微调模型，进一步提升性能，并扩展应用到野外图像的一-shot 实例分割和基于特征的图像检索。该方法显著减少了手动标注需求，为生物图像分析提供了高效突破。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06749v1",
      "published_date": "2025-01-12 08:27:14 UTC",
      "updated_date": "2025-01-12 08:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:47:08.618108"
    },
    {
      "arxiv_id": "2501.08347v1",
      "title": "SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval",
      "title_zh": "SCOT：自监督对比预训练用于零样本组合检索",
      "authors": [
        "Bhavin Jawade",
        "Joao V. B. Soares",
        "Kapil Thadani",
        "Deen Dayal Mohan",
        "Amir Erfan Eshratifar",
        "Benjamin Culpepper",
        "Paloma de Juan",
        "Srirangaraj Setlur",
        "Venu Govindaraju"
      ],
      "abstract": "Compositional image retrieval (CIR) is a multimodal learning task where a\nmodel combines a query image with a user-provided text modification to retrieve\na target image. CIR finds applications in a variety of domains including\nproduct retrieval (e-commerce) and web search. Existing methods primarily focus\non fully-supervised learning, wherein models are trained on datasets of labeled\ntriplets such as FashionIQ and CIRR. This poses two significant challenges: (i)\ncurating such triplet datasets is labor intensive; and (ii) models lack\ngeneralization to unseen objects and domains. In this work, we propose SCOT\n(Self-supervised COmpositional Training), a novel zero-shot compositional\npretraining strategy that combines existing large image-text pair datasets with\nthe generative capabilities of large language models to contrastively train an\nembedding composition network. Specifically, we show that the text embedding\nfrom a large-scale contrastively-pretrained vision-language model can be\nutilized as proxy target supervision during compositional pretraining,\nreplacing the target image embedding. In zero-shot settings, this strategy\nsurpasses SOTA zero-shot compositional retrieval methods as well as many\nfully-supervised methods on standard benchmarks such as FashionIQ and CIRR.",
      "tldr_zh": "该论文提出SCOT（Self-Supervised Contrastive Pretraining），一种自监督对比预训练策略，用于零样本组合图像检索（Zero-Shot Compositional Retrieval）。SCOT利用现有的大规模图像-文本对数据集和大型语言模型的生成能力，通过对比训练嵌入组合网络，并以文本嵌入作为代理目标监督，取代传统目标图像嵌入，从而避免了劳动密集的三元组数据集标注。实验结果显示，在FashionIQ和CIRR等基准上，SCOT在零样本设置中超越了现有SOTA零样本方法，并优于许多完全监督方法，提升了模型的泛化性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper accepted at WACV 2025 in round 1",
      "pdf_url": "http://arxiv.org/pdf/2501.08347v1",
      "published_date": "2025-01-12 07:23:49 UTC",
      "updated_date": "2025-01-12 07:23:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:47:19.395162"
    },
    {
      "arxiv_id": "2501.06720v1",
      "title": "Multi-Label Scene Classification in Remote Sensing Benefits from Image Super-Resolution",
      "title_zh": "遥感中的多标签场景分类受益于图像超分辨率",
      "authors": [
        "Ashitha Mudraje",
        "Brian B. Moser",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "abstract": "Satellite imagery is a cornerstone for numerous Remote Sensing (RS)\napplications; however, limited spatial resolution frequently hinders the\nprecision of such systems, especially in multi-label scene classification tasks\nas it requires a higher level of detail and feature differentiation. In this\nstudy, we explore the efficacy of image Super-Resolution (SR) as a\npre-processing step to enhance the quality of satellite images and thus improve\ndownstream classification performance. We investigate four SR models -\nSRResNet, HAT, SeeSR, and RealESRGAN - and evaluate their impact on multi-label\nscene classification across various CNN architectures, including ResNet-50,\nResNet-101, ResNet-152, and Inception-v4. Our results show that applying SR\nsignificantly improves downstream classification performance across various\nmetrics, demonstrating its ability to preserve spatial details critical for\nmulti-label tasks. Overall, this work offers valuable insights into the\nselection of SR techniques for multi-label prediction in remote sensing and\npresents an easy-to-integrate framework to improve existing RS systems.",
      "tldr_zh": "本文研究了图像超分辨率 (Super-Resolution, SR) 如何提升遥感中的多标签场景分类性能，以解决卫星图像空间分辨率不足导致的细节和特征区分问题。研究者评估了 SRResNet、HAT、SeeSR 和 RealESRGAN 等四种 SR 模型，作为预处理步骤应用于 ResNet-50、ResNet-101、ResNet-152 和 Inception-v4 等 CNN 架构。结果表明，SR 显著提高了分类性能在各种指标上的表现，突显了其在保留空间细节方面的能力。该工作提供了选择 SR 技术的实用见解，并提出了一种易于集成的框架来优化现有遥感系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06720v1",
      "published_date": "2025-01-12 05:25:16 UTC",
      "updated_date": "2025-01-12 05:25:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:47:32.497210"
    },
    {
      "arxiv_id": "2501.06715v1",
      "title": "ZNO-Eval: Benchmarking reasoning capabilities of large language models in Ukrainian",
      "title_zh": "翻译失败",
      "authors": [
        "Mykyta Syromiatnikov",
        "Victoria Ruvinskaya",
        "Anastasiya Troynina"
      ],
      "abstract": "As the usage of large language models for problems outside of simple text\nunderstanding or generation increases, assessing their abilities and\nlimitations becomes crucial. While significant progress has been made in this\narea over the last few years, most research has focused on benchmarking\nEnglish, leaving other languages underexplored. This makes evaluating the\nreasoning and robustness level of language models in Ukrainian particularly\nchallenging. The purpose of this work is to establish a comprehensive benchmark\nfor the reasoning capabilities evaluation of large language models in the\nUkrainian language. This paper presents the ZNO-Eval benchmark based on real\nexam tasks from Ukraine's standardized educational testing system: the External\nIndependent Evaluation and the National Multi-subject Test. With single-answer\noptions, multiple-choice, matching, and open-ended questions from diverse\nsubjects, including Ukrainian language, mathematics, history, and geography,\nthis dataset paves the way toward a thorough analysis of reasoning capabilities\nacross different domains and complexities. Evaluation of several well-known\nlanguage models, such as GPT-3.5-Turbo, GPT-4o, GPT-4-Turbo, Mistral Large,\nClaude 3 Opus, and Gemini-1.5 Pro on this benchmark demonstrated the\nsuperiority of GPT-4o in both common knowledge reasoning and intricate language\ntasks. At the same time, Gemini Pro and GPT-4 Turbo excelled in the arithmetic\ndomain, leading in single-answer and open-ended math problems. While all models\nwere close to max performance in text-only common knowledge tasks like history\nand geography, there still is a gap for Ukrainian language and math, thus\nhighlighting the importance of developing specialized language benchmarks for\nmore accurate assessments of model capabilities and limitations across\ndifferent languages and contexts.",
      "tldr_zh": "本研究引入了ZNO-Eval基准，用于评估大型语言模型(LLMs)在乌克兰语中的推理能力，填补了现有基准主要聚焦英语的空白。ZNO-Eval基于乌克兰的标准化教育测试系统，包括单选、多选、匹配和开放式问题，涵盖乌克兰语、数学、历史和地理等多样主题。评估结果显示，GPT-4o在常识推理和语言任务中表现出色，而Gemini-1.5 Pro和GPT-4 Turbo在算术领域领先；尽管模型在文本常识任务中接近最佳表现，但在乌克兰语和数学上仍存在显著差距，突显了开发专用语言基准的重要性，以更准确评估LLMs的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 5 figures. X International conference \"Informatics. Culture.\n  Technology.\" (2024)",
      "pdf_url": "http://arxiv.org/pdf/2501.06715v1",
      "published_date": "2025-01-12 04:49:06 UTC",
      "updated_date": "2025-01-12 04:49:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:47:43.954576"
    },
    {
      "arxiv_id": "2501.06713v3",
      "title": "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyu Fan",
        "Jingyuan Wang",
        "Xubin Ren",
        "Chao Huang"
      ],
      "abstract": "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps://github.com/HKUDS/MiniRAG.",
      "tldr_zh": "该研究针对 Small Language Models (SLMs) 在 Retrieval-Augmented Generation (RAG) 系统中的性能下降问题，提出了一种极简高效的框架 MiniRAG，以适应资源受限场景。MiniRAG 的关键创新包括语义感知的异构图索引机制（semantic-aware heterogeneous graph indexing），将文本块和命名实体整合到统一结构中，以及轻量级的拓扑增强检索方法（lightweight topology-enhanced retrieval），利用图结构进行高效知识发现，而不依赖高级语言能力。实验结果表明，MiniRAG 使用 SLMs 时，性能可与基于 Large Language Models (LLMs) 的方法相当，但仅需 25% 的存储空间。该框架还贡献了一个全面的基准数据集，用于评估轻量级 RAG 系统在真实设备场景下的表现，并开源了代码和数据集。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06713v3",
      "published_date": "2025-01-12 04:44:06 UTC",
      "updated_date": "2025-01-26 08:17:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:47:56.962159"
    },
    {
      "arxiv_id": "2501.06710v1",
      "title": "Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Dai",
        "Jian Li",
        "Jiedong Zhuang",
        "Xian Zhang",
        "Wankou Yang"
      ],
      "abstract": "Multi-task visual grounding involves the simultaneous execution of\nlocalization and segmentation in images based on textual expressions. The\nmajority of advanced methods predominantly focus on transformer-based\nmultimodal fusion, aiming to extract robust multimodal representations.\nHowever, ambiguity between referring expression comprehension (REC) and\nreferring image segmentation (RIS) is error-prone, leading to inconsistencies\nbetween multi-task predictions. Besides, insufficient multimodal understanding\ndirectly contributes to biased target perception. To overcome these challenges,\nwe propose a Coarse-to-fine Consistency Constraints Visual Grounding\narchitecture ($\\text{C}^3\\text{VG}$), which integrates implicit and explicit\nmodeling approaches within a two-stage framework. Initially, query and pixel\ndecoders are employed to generate preliminary detection and segmentation\noutputs, a process referred to as the Rough Semantic Perception (RSP) stage.\nThese coarse predictions are subsequently refined through the proposed\nMask-guided Interaction Module (MIM) and a novel explicit bidirectional\nconsistency constraint loss to ensure consistent representations across tasks,\nwhich we term the Refined Consistency Interaction (RCI) stage. Furthermore, to\naddress the challenge of insufficient multimodal understanding, we leverage\npre-trained models based on visual-linguistic fusion representations. Empirical\nevaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate the\nefficacy and soundness of $\\text{C}^3\\text{VG}$, which significantly\noutperforms state-of-the-art REC and RIS methods by a substantial margin. Code\nand model will be available at \\url{https://github.com/Dmmm1997/C3VG}.",
      "tldr_zh": "本论文提出了一种基于粗到细一致性约束的多任务视觉 grounding 框架（C³VG），旨在解决基于文本表达的图像定位（REC）和分割（RIS）任务中存在的预测不一致和多模态理解不足问题。该框架采用两阶段设计：首先通过 Rough Semantic Perception (RSP) 阶段利用 query 和 pixel decoders 生成初步输出，然后在 Refined Consistency Interaction (RCI) 阶段通过 Mask-guided Interaction Module (MIM) 和显式双向一致性约束损失进行精炼，确保任务间表示一致。同时，引入基于视觉-语言融合的预训练模型以增强多模态理解。在 RefCOCO、RefCOCO+ 和 RefCOCOg 数据集上的实验表明，C³VG 显著优于现有最先进方法，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06710v1",
      "published_date": "2025-01-12 04:30:13 UTC",
      "updated_date": "2025-01-12 04:30:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:48:07.405831"
    },
    {
      "arxiv_id": "2501.06708v2",
      "title": "Evaluating Sample Utility for Data Selection by Mimicking Model Weights",
      "title_zh": "翻译失败",
      "authors": [
        "Tzu-Heng Huang",
        "Manjot Bilkhu",
        "Frederic Sala",
        "Javier Movellan"
      ],
      "abstract": "Foundation models are trained on large-scale web-crawled datasets, which\noften contain noise, biases, and irrelevant information. This motivates the use\nof data selection techniques, which can be divided into model-free variants --\nrelying on heuristic rules and downstream datasets -- and model-based, e.g.,\nusing influence functions. The former can be expensive to design and risk\nintroducing unwanted dependencies, while the latter are often computationally\nprohibitive. Instead, we propose an efficient, model-based approach using the\nMimic Score, a new data quality metric that leverages the weights of a\nreference model to assess the usefulness of individual samples for training a\nnew model. It relies on the alignment between gradients and a target direction\ninduced by the reference model. Using the derived Mimic Scores, we develop\nGrad-Mimic, a framework that prioritizes samples for learning, creates\neffective filters, and automates data selection. Empirically, using Mimic\nScores to guide training improves data efficiency, results in consistent\nperformance gains across six image datasets, and includes enhancements to CLIP\nmodels. Moreover, Mimic Score-based filters improve upon existing filtering\nmethods, e.g., cutting 4.7 million samples to train better CLIP models while\noffering accurate estimation of training dataset quality.",
      "tldr_zh": "本文评估了通过模仿模型权重（Mimic Score）来选择数据样本的效用，以解决基础模型训练中数据集噪声、偏差和无关信息的挑战。论文提出了一种高效的模型-based 方法，利用 Mimic Score 作为新数据质量指标，基于梯度和参考模型诱导的目标方向的 alignment 来评估样本有用性，并开发了 Grad-Mimic 框架，用于样本优先化、过滤和自动化数据选择。实验结果显示，该框架在六个图像数据集上提高了数据效率，提升了模型性能，包括对 CLIP 模型的增强，并通过 Mimic Score-based 过滤器减少了 470 万样本，同时实现了更好的数据集质量估计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06708v2",
      "published_date": "2025-01-12 04:28:14 UTC",
      "updated_date": "2025-02-02 18:34:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:48:20.208762"
    },
    {
      "arxiv_id": "2501.06707v1",
      "title": "ELIZA Reanimated: The world's first chatbot restored on the world's first time sharing system",
      "title_zh": "翻译失败",
      "authors": [
        "Rupert Lane",
        "Anthony Hay",
        "Arthur Schwarz",
        "David M. Berry",
        "Jeff Shrager"
      ],
      "abstract": "ELIZA, created by Joseph Weizenbaum at MIT in the early 1960s, is usually\nconsidered the world's first chatbot. It was developed in MAD-SLIP on MIT's\nCTSS, the world's first time-sharing system, on an IBM 7094. We discovered an\noriginal ELIZA printout in Prof. Weizenbaum's archives at MIT, including an\nearly version of the famous DOCTOR script, a nearly complete version of the\nMAD-SLIP code, and various support functions in MAD and FAP. Here we describe\nthe reanimation of this original ELIZA on a restored CTSS, itself running on an\nemulated IBM 7094. The entire stack is open source, so that any user of a\nunix-like OS can run the world's first chatbot on the world's first\ntime-sharing system.",
      "tldr_zh": "本研究重新实现了 ELIZA，这是 Joseph Weizenbaum 在 1960 年代早期在 MIT 创建的世界上第一个聊天机器人，使用 MAD-SLIP 语言开发于 CTSS（世界上第一个 time-sharing system）上的 IBM 7094。研究者从 Weizenbaum 的档案中发现原始打印件，包括 DOCTOR script 的早期版本和几乎完整的 MAD-SLIP 代码，并将其在恢复的 CTSS 上运行，该系统基于模拟的 IBM 7094。整个开源栈允许任何使用类 Unix 操作系统的用户访问和运行这个历史性聊天机器人，从而促进了对早期人工智能技术的保存和研究。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "In review",
      "pdf_url": "http://arxiv.org/pdf/2501.06707v1",
      "published_date": "2025-01-12 04:23:34 UTC",
      "updated_date": "2025-01-12 04:23:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:48:31.159619"
    },
    {
      "arxiv_id": "2501.06706v1",
      "title": "AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds",
      "title_zh": "AIOpsLab：一个整体框架，用于评估 AI 代理以实现自主云",
      "authors": [
        "Yinfang Chen",
        "Manish Shetty",
        "Gagan Somashekar",
        "Minghua Ma",
        "Yogesh Simmhan",
        "Jonathan Mace",
        "Chetan Bansal",
        "Rujia Wang",
        "Saravan Rajmohan"
      ],
      "abstract": "AI for IT Operations (AIOps) aims to automate complex operational tasks, such\nas fault localization and root cause analysis, to reduce human workload and\nminimize customer impact. While traditional DevOps tools and AIOps algorithms\noften focus on addressing isolated operational tasks, recent advances in Large\nLanguage Models (LLMs) and AI agents are revolutionizing AIOps by enabling\nend-to-end and multitask automation. This paper envisions a future where AI\nagents autonomously manage operational tasks throughout the entire incident\nlifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.\nRealizing this vision requires a comprehensive framework to guide the design,\ndevelopment, and evaluation of these agents. To this end, we present AIOPSLAB,\na framework that not only deploys microservice cloud environments, injects\nfaults, generates workloads, and exports telemetry data but also orchestrates\nthese components and provides interfaces for interacting with and evaluating\nagents. We discuss the key requirements for such a holistic framework and\ndemonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps\nagents. Through evaluations of state-of-the-art LLM agents within the benchmark\ncreated by AIOPSLAB, we provide insights into their capabilities and\nlimitations in handling complex operational tasks in cloud environments.",
      "tldr_zh": "这篇论文介绍了 AIOpsLab，一个全面框架，旨在评估 AI agents 以实现自主云系统（Autonomous Clouds）。AIOps 专注于自动化 IT 操作任务，如故障定位和根因分析，而 AIOpsLab 通过部署微服务云环境、注入故障、生成工作负载以及导出遥测数据，来指导 AI agents 的设计、开发和评估。框架支持端到端的多任务自动化，并通过测试 LLM agents 揭示了它们在处理复杂云操作任务时的能力和局限性。该研究为实现 AgentOps 愿景——即 AI 代理自主管理整个事件生命周期的自我修复云系统——提供了关键基础。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.MA",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06706v1",
      "published_date": "2025-01-12 04:17:39 UTC",
      "updated_date": "2025-01-12 04:17:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:50:44.019864"
    },
    {
      "arxiv_id": "2501.06704v1",
      "title": "Fine-tuning ChatGPT for Automatic Scoring of Written Scientific Explanations in Chinese",
      "title_zh": "微调 ChatGPT 用于自动评分中文书面科学解释",
      "authors": [
        "Jie Yang",
        "Ehsan Latif",
        "Yuze He",
        "Xiaoming Zhai"
      ],
      "abstract": "The development of explanations for scientific phenomena is essential in\nscience assessment, but scoring student-written explanations remains\nchallenging and resource-intensive. Large language models (LLMs) have shown\npromise in addressing this issue, particularly in alphabetic languages like\nEnglish. However, their applicability to logographic languages is less\nexplored. This study investigates the potential of fine-tuning ChatGPT, a\nleading LLM, to automatically score scientific explanations written in Chinese.\nStudent responses to seven scientific explanation tasks were collected and\nautomatically scored, with scoring accuracy examined in relation to reasoning\ncomplexity using the Kendall correlation. A qualitative analysis explored how\nlinguistic features influenced scoring accuracy. The results show that\ndomain-specific adaptation enables ChatGPT to score Chinese scientific\nexplanations with accuracy. However, scoring accuracy correlates with reasoning\ncomplexity: a negative correlation for lower-level responses and a positive one\nfor higher-level responses. The model overrates complex reasoning in low-level\nresponses with intricate sentence structures and underrates high-level\nresponses using concise causal reasoning. These correlations stem from\nlinguistic features--simplicity and clarity enhance accuracy for lower-level\nresponses, while comprehensiveness improves accuracy for higher-level ones.\nSimpler, shorter responses tend to score more accurately at lower levels,\nwhereas longer, information-rich responses yield better accuracy at higher\nlevels. These findings demonstrate the effectiveness of LLMs in automatic\nscoring within a Chinese context and emphasize the importance of linguistic\nfeatures and reasoning complexity in fine-tuning scoring models for educational\nassessments.",
      "tldr_zh": "本研究探讨了微调 ChatGPT 以自动评分中文科学解释的可行性，针对评分挑战使用学生对七个任务的回应进行实验，并通过 Kendall correlation 分析评分准确性与推理复杂度的关系。结果显示，微调后的 ChatGPT 能准确评分中文解释，但准确性与推理复杂度相关：低水平回应（如复杂句结构）易被高估，而高水平回应（如简洁因果推理）需全面信息才能准确。研究强调了语言特征（如简单性和清晰度）的重要性，并证明了 LLMs 在中文教育评估中的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06704v1",
      "published_date": "2025-01-12 04:10:56 UTC",
      "updated_date": "2025-01-12 04:10:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:48:55.972189"
    },
    {
      "arxiv_id": "2501.06699v1",
      "title": "Large Language Models, Knowledge Graphs and Search Engines: A Crossroads for Answering Users' Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Aidan Hogan",
        "Xin Luna Dong",
        "Denny Vrandečić",
        "Gerhard Weikum"
      ],
      "abstract": "Much has been discussed about how Large Language Models, Knowledge Graphs and\nSearch Engines can be combined in a synergistic manner. A dimension largely\nabsent from current academic discourse is the user perspective. In particular,\nthere remain many open questions regarding how best to address the diverse\ninformation needs of users, incorporating varying facets and levels of\ndifficulty. This paper introduces a taxonomy of user information needs, which\nguides us to study the pros, cons and possible synergies of Large Language\nModels, Knowledge Graphs and Search Engines. From this study, we derive a\nroadmap for future research.",
      "tldr_zh": "这篇论文探讨了如何结合 Large Language Models (LLMs)、Knowledge Graphs (KGs) 和 Search Engines 来回答用户问题，但强调了当前研究中缺少用户视角的问题。论文引入了一个用户信息需求的分类法（taxonomy），用于分析这些技术的优缺点、潜在协同作用，以及如何满足用户多样化信息需求的不同层面。最终，论文基于这一研究提出未来研究的路线图，以推动这些技术在实际应用中的融合和发展。",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.SC",
        "I.2.4; I.2.7; H.3.3"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06699v1",
      "published_date": "2025-01-12 03:32:12 UTC",
      "updated_date": "2025-01-12 03:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:52:48.280059"
    },
    {
      "arxiv_id": "2501.06697v2",
      "title": "Mamba-MOC: A Multicategory Remote Object Counting via State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Liu",
        "Sen Lei",
        "Heng-Chao Li"
      ],
      "abstract": "Multicategory remote object counting is a fundamental task in computer\nvision, aimed at accurately estimating the number of objects of various\ncategories in remote images. Existing methods rely on CNNs and Transformers,\nbut CNNs struggle to capture global dependencies, and Transformers are\ncomputationally expensive, which limits their effectiveness in remote\napplications. Recently, Mamba has emerged as a promising solution in the field\nof computer vision, offering a linear complexity for modeling global\ndependencies. To this end, we propose Mamba-MOC, a mamba-based network designed\nfor multi-category remote object counting, which represents the first\napplication of Mamba to remote sensing object counting. Specifically, we\npropose a cross-scale interaction module to facilitate the deep integration of\nhierarchical features. Then we design a context state space model to capture\nboth global and local contextual information and provide local neighborhood\ninformation during the scan process. Experimental results in large-scale\nrealistic scenarios demonstrate that our proposed method achieves\nstate-of-the-art performance compared with some mainstream counting algorithms.",
      "tldr_zh": "该论文提出Mamba-MOC，一种基于状态空间模型（State Space Model）的网络，用于多类别远程物体计数，首次将Mamba应用于该领域，以解决CNNs无法捕获全局依赖和Transformers计算开销大的问题。具体方法包括跨尺度交互模块（cross-scale interaction module）用于整合层次特征，以及上下文状态空间模型（context state space model）来捕获全局和局部上下文信息。实验结果显示，在大规模真实场景中，该方法比主流计数算法取得了state-of-the-art性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06697v2",
      "published_date": "2025-01-12 03:13:54 UTC",
      "updated_date": "2025-05-18 02:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:51:06.169171"
    },
    {
      "arxiv_id": "2501.06695v1",
      "title": "DVM: Towards Controllable LLM Agents in Social Deduction Games",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Zhang",
        "Yihuai Lan",
        "Yangsen Chen",
        "Lei Wang",
        "Xiang Wang",
        "Hao Wang"
      ],
      "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in\nsocial deduction games (SDGs). These games rely heavily on conversation-driven\ninteractions and require agents to infer, make decisions, and express based on\nsuch information. While this progress leads to more sophisticated and strategic\nnon-player characters (NPCs) in SDGs, there exists a need to control the\nproficiency of these agents. This control not only ensures that NPCs can adapt\nto varying difficulty levels during gameplay, but also provides insights into\nthe safety and fairness of LLM agents. In this paper, we present DVM, a novel\nframework for developing controllable LLM agents for SDGs, and demonstrate its\nimplementation on one of the most popular SDGs, Werewolf. DVM comprises three\nmain components: Predictor, Decider, and Discussor. By integrating\nreinforcement learning with a win rate-constrained decision chain reward\nmechanism, we enable agents to dynamically adjust their gameplay proficiency to\nachieve specified win rates. Experiments show that DVM not only outperforms\nexisting methods in the Werewolf game, but also successfully modulates its\nperformance levels to meet predefined win rate targets. These results pave the\nway for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues\nfor research in controllable game agents.",
      "tldr_zh": "这篇论文提出 DVM 框架，旨在开发可控的 LLM 代理，用于社会推演游戏 (SDGs)，以适应不同难度水平并确保代理的安全性和公平性。DVM 包括 Predictor、Decider 和 Discussor 三个组件，并通过强化学习和 win rate-constrained decision chain reward 机制，让代理动态调整游戏熟练度以实现指定胜率。实验结果显示，DVM 在 Werewolf 游戏中优于现有方法，并成功调控性能，为 LLM 代理在 SDGs 中的适应性和平衡游戏开辟了新研究路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06695v1",
      "published_date": "2025-01-12 03:11:20 UTC",
      "updated_date": "2025-01-12 03:11:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:51:19.417240"
    },
    {
      "arxiv_id": "2501.06692v1",
      "title": "PGP-SAM: Prototype-Guided Prompt Learning for Efficient Few-Shot Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhonghao Yan",
        "Zijin Yin",
        "Tianyu Lin",
        "Xiangzhu Zeng",
        "Kongming Liang",
        "Zhanyu Ma"
      ],
      "abstract": "The Segment Anything Model (SAM) has demonstrated strong and versatile\nsegmentation capabilities, along with intuitive prompt-based interactions.\nHowever, customizing SAM for medical image segmentation requires massive\namounts of pixel-level annotations and precise point- or box-based prompt\ndesigns. To address these challenges, we introduce PGP-SAM, a novel\nprototype-based few-shot tuning approach that uses limited samples to replace\ntedious manual prompts. Our key idea is to leverage inter- and intra-class\nprototypes to capture class-specific knowledge and relationships. We propose\ntwo main components: (1) a plug-and-play contextual modulation module that\nintegrates multi-scale information, and (2) a class-guided cross-attention\nmechanism that fuses prototypes and features for automatic prompt generation.\nExperiments on a public multi-organ dataset and a private ventricle dataset\ndemonstrate that PGP-SAM achieves superior mean Dice scores compared with\nexisting prompt-free SAM variants, while using only 10\\% of the 2D slices.",
      "tldr_zh": "该研究提出PGP-SAM，一种基于原型的提示学习方法，用于高效的少样本医疗图像分割，以解决Segment Anything Model (SAM)需要大量像素级标注和精确提示的挑战。关键创新包括一个即插即用的上下文调制模块，用于整合多尺度信息，以及一个类引导的交叉注意力机制，用于融合类间和类内原型生成自动提示。实验在公共多器官数据集和私有心室数据集上显示，PGP-SAM仅使用10%的2D切片，就实现了比现有无提示SAM变体更高的平均Dice scores。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 2 figures, Accepted at ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.06692v1",
      "published_date": "2025-01-12 02:57:04 UTC",
      "updated_date": "2025-01-12 02:57:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:53:31.223576"
    },
    {
      "arxiv_id": "2501.06682v1",
      "title": "Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangen Hu",
        "Sheng Xu",
        "Richard Tong",
        "Art Graesser"
      ],
      "abstract": "This paper explores the synergy between human cognition and Large Language\nModels (LLMs), highlighting how generative AI can drive personalized learning\nat scale. We discuss parallels between LLMs and human cognition, emphasizing\nboth the promise and new perspectives on integrating AI systems into education.\nAfter examining challenges in aligning technology with pedagogy, we review\nAutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its\nsuccesses, limitations, and unfulfilled aspirations. We then introduce the\nSocratic Playground, a next-generation ITS that uses advanced transformer-based\nmodels to overcome AutoTutor's constraints and provide personalized, adaptive\ntutoring. To illustrate its evolving capabilities, we present a JSON-based\ntutoring prompt that systematically guides learner reflection while tracking\nmisconceptions. Throughout, we underscore the importance of placing pedagogy at\nthe forefront, ensuring that technology's power is harnessed to enhance\nteaching and learning rather than overshadow it.",
      "tldr_zh": "这篇论文探讨了生成式 AI 与人类认知的协同作用，强调 Large Language Models (LLMs) 如何推动大规模个性化学习，并分析了将 AI 整合到教育中的挑战与机遇。作者回顾了早期的 Intelligent Tutoring Systems (ITS) 如 AutoTutor 的成功、局限性及其未实现目标。论文引入了新一代 ITS——Socratic Playground，利用先进的 transformer-based 模型，通过 JSON-based 提示提供个性化、适应性辅导，帮助跟踪学习者的误区并引导反思。最终，研究强调以教育学为核心，确保 AI 技术增强教学效果而非取代人类角色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06682v1",
      "published_date": "2025-01-12 01:43:39 UTC",
      "updated_date": "2025-01-12 01:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:51:43.294948"
    },
    {
      "arxiv_id": "2501.06680v1",
      "title": "Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Haoxiang Gao",
        "Yu Zhao"
      ],
      "abstract": "Autonomous driving (AD) has experienced significant improvements in recent\nyears and achieved promising 3D detection, classification, and localization\nresults. However, many challenges remain, e.g. semantic understanding of\npedestrians' behaviors, and downstream handling for pedestrian interactions.\nRecent studies in applications of Large Language Models (LLM) and\nVision-Language Models (VLM) have achieved promising results in scene\nunderstanding and high-level maneuver planning in diverse traffic scenarios.\nHowever, deploying the billion-parameter LLMs to vehicles requires significant\ncomputation and memory resources. In this paper, we analyzed effective\nknowledge distillation of semantic labels to smaller Vision networks, which can\nbe used for the semantic representation of complex scenes for downstream\ndecision-making for planning and control.",
      "tldr_zh": "该论文探讨了视觉语言模型(VLM) 在自动驾驶领域的应用，针对行人行为语义理解和场景交互处理的挑战。作者分析了大型语言模型(LLM) 在场景理解和高水平机动规划中的潜力，但强调了部署这些模型所需的大量计算和内存资源问题。为此，论文重点研究了知识蒸馏技术，将语义标签从大型模型转移到更小的视觉网络，以实现复杂场景的语义表示，并支持下游决策、规划和控制模块。实验结果表明，这种方法为资源受限的自动驾驶系统提供了高效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06680v1",
      "published_date": "2025-01-12 01:31:07 UTC",
      "updated_date": "2025-01-12 01:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:51:54.802928"
    },
    {
      "arxiv_id": "2501.06678v1",
      "title": "Imbalanced Medical Image Segmentation with Pixel-dependent Noisy Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Erjian Guo",
        "Zicheng Wang",
        "Zhen Zhao",
        "Luping Zhou"
      ],
      "abstract": "Accurate medical image segmentation is often hindered by noisy labels in\ntraining data, due to the challenges of annotating medical images. Prior\nresearch works addressing noisy labels tend to make class-dependent\nassumptions, overlooking the pixel-dependent nature of most noisy labels.\nFurthermore, existing methods typically apply fixed thresholds to filter out\nnoisy labels, risking the removal of minority classes and consequently\ndegrading segmentation performance. To bridge these gaps, our proposed\nframework, Collaborative Learning with Curriculum Selection (CLCS), addresses\npixel-dependent noisy labels with class imbalance. CLCS advances the existing\nworks by i) treating noisy labels as pixel-dependent and addressing them\nthrough a collaborative learning framework, and ii) employing a curriculum\ndynamic thresholding approach adapting to model learning progress to select\nclean data samples to mitigate the class imbalance issue, and iii) applying a\nnoise balance loss to noisy data samples to improve data utilization instead of\ndiscarding them outright. Specifically, our CLCS contains two modules:\nCurriculum Noisy Label Sample Selection (CNS) and Noise Balance Loss (NBL). In\nthe CNS module, we designed a two-branch network with discrepancy loss for\ncollaborative learning so that different feature representations of the same\ninstance could be extracted from distinct views and used to vote the class\nprobabilities of pixels. Besides, a curriculum dynamic threshold is adopted to\nselect clean-label samples through probability voting. In the NBL module,\ninstead of directly dropping the suspiciously noisy labels, we further adopt a\nrobust loss to leverage such instances to boost the performance.",
      "tldr_zh": "该论文针对医疗图像分割中像素依赖的噪声标签及其带来的类不平衡问题，提出了一种名为 Collaborative Learning with Curriculum Selection (CLCS) 的框架，以提升分割准确性。CLCS 通过协作学习框架处理像素-dependent noisy labels，包括 Curriculum Noisy Label Sample Selection (CNS) 模块，该模块使用双分支网络和差异损失从不同视角提取特征，并采用课程动态阈值来选择干净样本，从而缓解类不平衡。另一方面，Noise Balance Loss (NBL) 模块则通过鲁棒损失利用可疑噪声数据，而不是直接丢弃，进一步提高了数据利用效率和整体性能。总的来说，该方法在处理噪声标签方面超越了现有方法，为更可靠的医疗图像分割提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06678v1",
      "published_date": "2025-01-12 00:59:57 UTC",
      "updated_date": "2025-01-12 00:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T22:54:00.555507"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 47,
  "processed_papers_count": 47,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T22:54:21.308278"
}
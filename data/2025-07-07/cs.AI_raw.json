[
  {
    "arxiv_id": "2507.05541v2",
    "title": "SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation",
    "authors": [
      "Shovito Barua Soumma",
      "Asiful Arefeen",
      "Stephanie M. Carpenter",
      "Melanie Hingle",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: github.com/shovito66/SenseCF.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS BSN) 2025, LA, CA, USA",
    "pdf_url": "https://arxiv.org/pdf/2507.05541v2",
    "published_date": "2025-07-07 23:45:40 UTC",
    "updated_date": "2025-09-07 18:25:00 UTC"
  },
  {
    "arxiv_id": "2507.05540v1",
    "title": "Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge",
    "authors": [
      "Chunhui Gu",
      "Mohammad Sadegh Nasr",
      "James P. Long",
      "Kim-Anh Do",
      "Ehsan Irajizad"
    ],
    "abstract": "Graph Neural Networks (GNNs) often struggle with noisy edges. We propose Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate external \"clean\" links and guide embeddings of a noisy target graph. We train two encoders--one on the full graph (target plus external edges) and another on a regularization graph excluding the target's potentially noisy links--then penalize discrepancies between their latent representations. This constraint steers the model away from overfitting spurious edges. Experiments on benchmark datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and validate it on a small protein-metabolite network, where metabolite-protein interactions reduce noise in protein co-occurrence data. Our results highlight LSC-GNN's potential to boost predictive performance and interpretability in settings with noisy relational structures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05540v1",
    "published_date": "2025-07-07 23:43:24 UTC",
    "updated_date": "2025-07-07 23:43:24 UTC"
  },
  {
    "arxiv_id": "2507.05538v2",
    "title": "Red Teaming AI Red Teaming",
    "authors": [
      "Subhabrata Majumdar",
      "Brian Pendleton",
      "Abhishek Gupta"
    ],
    "abstract": "Red teaming has evolved from its origins in military applications to become a widely adopted methodology in cybersecurity and AI. In this paper, we take a critical look at the practice of AI red teaming. We argue that despite its current popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI. Current AI red teaming efforts focus predominantly on individual model vulnerabilities while overlooking the broader sociotechnical systems and emergent behaviors that arise from complex interactions between models, users, and environments. To address this deficiency, we propose a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming. Drawing on cybersecurity experience and systems theory, we further propose a set of six recommendations. In these, we emphasize that effective AI red teaming requires multifunctional teams that examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference on Applied Machine Learning for Information Security (CAMLIS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05538v2",
    "published_date": "2025-07-07 23:23:40 UTC",
    "updated_date": "2025-10-30 19:23:25 UTC"
  },
  {
    "arxiv_id": "2507.05528v2",
    "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment",
    "authors": [
      "Jiahuan Pei",
      "Fanghua Ye",
      "Xin Sun",
      "Wentao Deng",
      "Koen Hindriks",
      "Junxiao Wang"
    ],
    "abstract": "Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, accepted by EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05528v2",
    "published_date": "2025-07-07 22:56:37 UTC",
    "updated_date": "2025-09-05 17:52:53 UTC"
  },
  {
    "arxiv_id": "2507.05527v1",
    "title": "Mitigating Shortcut Learning with InterpoLated Learning",
    "authors": [
      "Michalis Korakakis",
      "Andreas Vlachos",
      "Adrian Weller"
    ],
    "abstract": "Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method's broad applicability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ACL 2025 (Main)",
    "pdf_url": "https://arxiv.org/pdf/2507.05527v1",
    "published_date": "2025-07-07 22:49:46 UTC",
    "updated_date": "2025-07-07 22:49:46 UTC"
  },
  {
    "arxiv_id": "2507.06804v1",
    "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving",
    "authors": [
      "Zhenwen Liang",
      "Linfeng Song",
      "Yang Li",
      "Tao Yang",
      "Feng Zhang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "abstract": "Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2507.06804v1",
    "published_date": "2025-07-07 22:38:49 UTC",
    "updated_date": "2025-07-07 22:38:49 UTC"
  },
  {
    "arxiv_id": "2507.05520v3",
    "title": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA",
    "authors": [
      "Karishma Thakrar",
      "Shreyas Basavatia",
      "Akshay Daftardar"
    ],
    "abstract": "Dermatological care via telemedicine often lacks the rich context of in-person visits. Clinicians must make diagnoses based on a handful of images and brief descriptions, without the benefit of physical exams, second opinions, or reference materials. While many medical AI systems attempt to bridge these gaps with domain-specific fine-tuning, this work hypothesized that mimicking clinical reasoning processes could offer a more effective path forward. This study tested seven vision-language models on medical visual question answering across six configurations: baseline models, fine-tuned variants, and both augmented with either reasoning layers that combine multiple model perspectives, analogous to peer consultation, or retrieval-augmented generation that incorporates medical literature at inference time, serving a role similar to reference-checking. While fine-tuning degraded performance in four of seven models with an average 30% decrease, baseline models collapsed on test data. Clinical-inspired architectures, meanwhile, achieved up to 70% accuracy, maintaining performance on unseen data while generating explainable, literature-grounded outputs critical for clinical adoption. These findings demonstrate that medical AI succeeds by reconstructing the collaborative and evidence-based practices fundamental to clinical diagnosis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05520v3",
    "published_date": "2025-07-07 22:31:56 UTC",
    "updated_date": "2025-08-26 14:02:57 UTC"
  },
  {
    "arxiv_id": "2507.05519v6",
    "title": "Modeling Deontic Modal Logic in the s(CASP) Goal-directed Predicate Answer Set Programming System",
    "authors": [
      "Gopal Gupta",
      "Abhiramon Rajasekharan",
      "Alexis R. Tudor",
      "Elmer Salazar",
      "Joaquín Arias"
    ],
    "abstract": "We consider the problem of implementing deontic modal logic. We show how (deontic) modal operators can be expressed elegantly using default negation (negation-as-failure) and strong negation present in answer set programming (ASP). We propose using global constraints of ASP to represent obligations and impermissibilities of deontic modal logic. We show that our proposed representation results in the various paradoxes of deontic modal logic being elegantly resolved.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05519v6",
    "published_date": "2025-07-07 22:31:54 UTC",
    "updated_date": "2025-08-11 17:46:45 UTC"
  },
  {
    "arxiv_id": "2507.05517v3",
    "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications",
    "authors": [
      "Jean-Philippe Corbeil",
      "Asma Ben Abacha",
      "George Michalopoulos",
      "Phillip Swazinna",
      "Miguel Del-Agua",
      "Jerome Tremblay",
      "Akila Jeeson Daniel",
      "Cari Bader",
      "Yu-Cheng Cho",
      "Pooja Krishnan",
      "Nathan Bodenstab",
      "Thomas Lin",
      "Wenxuan Teng",
      "Francois Beaulieu",
      "Paul Vozila"
    ],
    "abstract": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05517v3",
    "published_date": "2025-07-07 22:29:29 UTC",
    "updated_date": "2025-10-04 15:31:53 UTC"
  },
  {
    "arxiv_id": "2507.05515v2",
    "title": "LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants",
    "authors": [
      "Haochen Huang",
      "Jiahuan Pei",
      "Mohammad Aliannejadi",
      "Xin Sun",
      "Moonisa Ahsan",
      "Chuang Yu",
      "Zhaochun Ren",
      "Pablo Cesar",
      "Junxiao Wang"
    ],
    "abstract": "Vision-language models (VLMs) are facing the challenges of understanding and following multimodal assembly instructions, particularly when fine-grained spatial reasoning and precise object state detection are required. In this work, we explore LEGO Co-builder, a hybrid benchmark combining real-world LEGO assembly logic with programmatically generated multimodal scenes. The dataset captures stepwise visual states and procedural instructions, allowing controlled evaluation of instruction-following, object detection, and state detection. We introduce a unified framework and assess leading VLMs such as GPT-4o, Gemini, and Qwen-VL, under zero-shot and fine-tuned settings. Our results reveal that even advanced models like GPT-4o struggle with fine-grained assembly tasks, with a maximum F1 score of just 40.54\\% on state detection, highlighting gaps in fine-grained visual understanding. We release the benchmark, codebase, and generation pipeline to support future research on multimodal assembly assistants grounded in real-world workflows.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "This version has been anonymized for double-blind review",
    "pdf_url": "https://arxiv.org/pdf/2507.05515v2",
    "published_date": "2025-07-07 22:29:01 UTC",
    "updated_date": "2025-07-23 05:20:57 UTC"
  },
  {
    "arxiv_id": "2507.06265v1",
    "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability",
    "authors": [
      "Ali Nasiri-Sarvi",
      "Hassan Rivaz",
      "Mahdi S. Hosseini"
    ],
    "abstract": "Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability. To address this, we introduce SPARC (Sparse Autoencoders for Aligned Representation of Concepts), a new framework that learns a single, unified latent space shared across diverse architectures and modalities (e.g., vision models like DINO, and multimodal models like CLIP). SPARC's alignment is enforced through two key innovations: (1) a Global TopK sparsity mechanism, ensuring all input streams activate identical latent dimensions for a given concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages semantic consistency between models. On Open Images, SPARC dramatically improves concept alignment, achieving a Jaccard similarity of 0.80, more than tripling the alignment compared to previous methods. SPARC creates a shared sparse latent space where individual dimensions often correspond to similar high-level concepts across models and modalities, enabling direct comparison of how different architectures represent identical concepts without requiring manual alignment or model-specific analysis. As a consequence of this aligned representation, SPARC also enables practical applications such as text-guided spatial localization in vision-only models and cross-model/cross-modal retrieval. Code and models are available at https://github.com/AtlasAnalyticsLab/SPARC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.06265v1",
    "published_date": "2025-07-07 22:29:00 UTC",
    "updated_date": "2025-07-07 22:29:00 UTC"
  },
  {
    "arxiv_id": "2507.08839v1",
    "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer",
    "authors": [
      "Xiaowei Yu",
      "Jing Zhang",
      "Tong Chen",
      "Yan Zhuang",
      "Minheng Chen",
      "Chao Cao",
      "Yanjun Lyu",
      "Lu Zhang",
      "Li Su",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that imposes a significant burden on public health. It shares clinical similarities with Alzheimer's disease (AD), as both progress through stages of normal cognition, mild cognitive impairment, and dementia. A major obstacle in LBD diagnosis is data scarcity, which limits the effectiveness of deep learning. In contrast, AD datasets are more abundant, offering potential for knowledge transfer. However, LBD and AD data are typically collected from different sites using different machines and protocols, resulting in a distinct domain shift. To effectively leverage AD data while mitigating domain shift, we propose a Transferability Aware Transformer (TAT) that adapts knowledge from AD to enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived from structural MRI as training data. Built on the attention mechanism, TAT adaptively assigns greater weights to disease-transferable features while suppressing domain-specific ones, thereby reducing domain shift and improving diagnostic accuracy with limited LBD data. The experimental results demonstrate the effectiveness of TAT. To the best of our knowledge, this is the first study to explore domain adaptation from AD to LBD under conditions of data scarcity and domain shift, providing a promising framework for domain-adaptive diagnosis of rare diseases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.08839v1",
    "published_date": "2025-07-07 22:28:39 UTC",
    "updated_date": "2025-07-07 22:28:39 UTC"
  },
  {
    "arxiv_id": "2507.05513v1",
    "title": "Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model",
    "authors": [
      "Mengyao Xu",
      "Gabriel Moreira",
      "Ronay Ak",
      "Radek Osmulski",
      "Yauhen Babakhin",
      "Zhiding Yu",
      "Benedikt Schifferer",
      "Even Oldridge"
    ],
    "abstract": "Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.\n  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model's retrieval capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05513v1",
    "published_date": "2025-07-07 22:20:04 UTC",
    "updated_date": "2025-07-07 22:20:04 UTC"
  },
  {
    "arxiv_id": "2507.05512v1",
    "title": "Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice",
    "authors": [
      "Gehao Zhang",
      "Eugene Bagdasarian",
      "Juan Zhai",
      "Shiqing Ma"
    ],
    "abstract": "Distinguishing AI-generated code from human-written code is becoming crucial for tasks such as authorship attribution, content tracking, and misuse detection. Based on this, N-gram-based watermarking schemes have emerged as prominent, which inject secret watermarks to be detected during the generation.\n  However, their robustness in code content remains insufficiently evaluated. Most claims rely solely on defenses against simple code transformations or code optimizations as a simulation of attack, creating a questionable sense of robustness. In contrast, more sophisticated schemes already exist in the software engineering world, e.g., code obfuscation, which significantly alters code while preserving functionality. Although obfuscation is commonly used to protect intellectual property or evade software scanners, the robustness of code watermarking techniques against such transformations remains largely unexplored.\n  In this work, we formally model the code obfuscation and prove the impossibility of N-gram-based watermarking's robustness with only one intuitive and experimentally verified assumption, distribution consistency, satisfied. Given the original false positive rate of the watermarking detection, the ratio that the detector failed on the watermarked code after obfuscation will increase to 1 - fpr.\n  The experiments have been performed on three SOTA watermarking schemes, two LLMs, two programming languages, four code benchmarks, and four obfuscators. Among them, all watermarking detectors show coin-flipping detection abilities on obfuscated codes (AUROC tightly surrounds 0.5). Among all models, watermarking schemes, and datasets, both programming languages own obfuscators that can achieve attack effects with no detection AUROC higher than 0.6 after the attack. Based on the theoretical and practical observations, we also proposed a potential path of robust code watermarking.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05512v1",
    "published_date": "2025-07-07 22:18:19 UTC",
    "updated_date": "2025-07-07 22:18:19 UTC"
  },
  {
    "arxiv_id": "2507.06264v1",
    "title": "X-ray transferable polyrepresentation learning",
    "authors": [
      "Weronika Hryniewska-Guzik",
      "Przemyslaw Biecek"
    ],
    "abstract": "The success of machine learning algorithms is inherently related to the extraction of meaningful features, as they play a pivotal role in the performance of these algorithms. Central to this challenge is the quality of data representation. However, the ability to generalize and extract these features effectively from unseen datasets is also crucial. In light of this, we introduce a novel concept: the polyrepresentation. Polyrepresentation integrates multiple representations of the same modality extracted from distinct sources, for example, vector embeddings from the Siamese Network, self-supervised models, and interpretable radiomic features. This approach yields better performance metrics compared to relying on a single representation. Additionally, in the context of X-ray images, we demonstrate the transferability of the created polyrepresentation to a smaller dataset, underscoring its potential as a pragmatic and resource-efficient approach in various image-related solutions. It is worth noting that the concept of polyprepresentation on the example of medical data can also be applied to other domains, showcasing its versatility and broad potential impact.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "part of Weronika's PhD thesis",
    "pdf_url": "https://arxiv.org/pdf/2507.06264v1",
    "published_date": "2025-07-07 22:05:50 UTC",
    "updated_date": "2025-07-07 22:05:50 UTC"
  },
  {
    "arxiv_id": "2507.05498v1",
    "title": "Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)",
    "authors": [
      "Reza T. Batley",
      "Chanwook Park",
      "Wing Kam Liu",
      "Sourav Saha"
    ],
    "abstract": "Data-driven science and computation have advanced immensely to construct complex functional relationships using trainable parameters. However, efficiently discovering interpretable and accurate closed-form expressions from complex dataset remains a challenge. The article presents a novel approach called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that uses an accurate, frugal, fast, separable, and scalable neural architecture with symbolic regression to discover closed-form expressions from limited observation. The article presents the two-step Ex-HiDeNN algorithm with a separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN are tested on several benchmark problems, including discerning a dynamical system from data, and the outcomes are reported. Ex-HiDeNN generally shows outstanding approximation capability in these benchmarks, producing orders of magnitude smaller errors compared to reference data and traditional symbolic regression. Later, Ex-HiDeNN is applied to three engineering applications: a) discovering a closed-form fatigue equation, b) identification of hardness from micro-indentation test data, and c) discovering the expression for the yield surface with data. In every case, Ex-HiDeNN outperformed the reference methods used in the literature. The proposed method is built upon the foundation and published works of the authors on Hierarchical Deep Learning Neural Network (HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about the current limitations and future extensions of Ex-HiDeNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05498v1",
    "published_date": "2025-07-07 21:43:57 UTC",
    "updated_date": "2025-07-07 21:43:57 UTC"
  },
  {
    "arxiv_id": "2508.14899v1",
    "title": "Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE",
    "authors": [
      "Adeel Ahmad",
      "Ahmad Tameem Kamal",
      "Nouman Amir",
      "Bilal Zafar",
      "Saad Bin Nasir"
    ],
    "abstract": "This project enables RISC-V microkernel support in IREE, an MLIR-based machine learning compiler and runtime. The approach begins by enabling the lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the RISC-V64 target within the IREE pass pipeline, followed by the development of optimized microkernels for RISC-V. The performance gains are compared with upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.14899v1",
    "published_date": "2025-07-07 21:41:18 UTC",
    "updated_date": "2025-07-07 21:41:18 UTC"
  },
  {
    "arxiv_id": "2507.05496v1",
    "title": "Cloud Diffusion Part 1: Theory and Motivation",
    "authors": [
      "Andrew Randono"
    ],
    "abstract": "Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model\". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "39 pages, 21 figures. Associated code: https://github.com/arandono/Cloud-Diffusion",
    "pdf_url": "https://arxiv.org/pdf/2507.05496v1",
    "published_date": "2025-07-07 21:36:16 UTC",
    "updated_date": "2025-07-07 21:36:16 UTC"
  },
  {
    "arxiv_id": "2507.05495v2",
    "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents",
    "authors": [
      "Prahaladh Chandrahasan",
      "Jiahe Jin",
      "Zhihan Zhang",
      "Tevin Wang",
      "Andy Tang",
      "Lucy Mo",
      "Morteza Ziyadi",
      "Leonardo F. R. Ribeiro",
      "Zimeng Qiu",
      "Markus Dreyer",
      "Akari Asai",
      "Chenyan Xiong"
    ],
    "abstract": "Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05495v2",
    "published_date": "2025-07-07 21:35:09 UTC",
    "updated_date": "2025-12-23 16:43:12 UTC"
  },
  {
    "arxiv_id": "2507.08838v1",
    "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models",
    "authors": [
      "Xiaohang Tang",
      "Rares Dolga",
      "Sangwoong Yoon",
      "Ilija Bogunovic"
    ],
    "abstract": "Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead and lead to potentially large bias -- particularly when approximation errors occur in the denominator of policy ratios used for importance sampling. To mitigate these issues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. Experiments on widely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without supervised fine-tuning (SFT) or any supervised data, outperforms existing RL methods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers additional computational gains, including reduced training time and fewer function evaluations (NFEs) per gradient step. These findings, combined with the simplicity of method's implementation and R1-Zero-like training (no SFT), position $\\mathtt{wd1}$ as a more effective and efficient method for applying RL to dLLMs reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2507.08838v1",
    "published_date": "2025-07-07 21:27:25 UTC",
    "updated_date": "2025-07-07 21:27:25 UTC"
  },
  {
    "arxiv_id": "2507.05488v1",
    "title": "OLG++: A Semantic Extension of Obligation Logic Graph",
    "authors": [
      "Subhasis Dasgupta",
      "Jon Stephens",
      "Amarnath Gupta"
    ],
    "abstract": "We present OLG++, a semantic extension of the Obligation Logic Graph (OLG) for modeling regulatory and legal rules in municipal and interjurisdictional contexts. OLG++ introduces richer node and edge types, including spatial, temporal, party group, defeasibility, and logical grouping constructs, enabling nuanced representations of legal obligations, exceptions, and hierarchies. The model supports structured reasoning over rules with contextual conditions, precedence, and complex triggers. We demonstrate its expressiveness through examples from food business regulations, showing how OLG++ supports legal question answering using property graph queries. OLG++ also improves over LegalRuleML by providing native support for subClassOf, spatial constraints, and reified exception structures. Our examples show that OLG++ is more expressive than prior graph-based models for legal knowledge representation.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05488v1",
    "published_date": "2025-07-07 21:24:52 UTC",
    "updated_date": "2025-07-07 21:24:52 UTC"
  },
  {
    "arxiv_id": "2507.05477v1",
    "title": "Epistemically-guided forward-backward exploration",
    "authors": [
      "Núria Armengol Urpí",
      "Marin Vlastelica",
      "Georg Martius",
      "Stelian Coros"
    ],
    "abstract": "Zero-shot reinforcement learning is necessary for extracting optimal policies in absence of concrete rewards for fast adaptation to future problem settings. Forward-backward representations (FB) have emerged as a promising method for learning optimal policies in absence of rewards via a factorization of the policy occupancy measure. However, up until now, FB and many similar zero-shot reinforcement learning algorithms have been decoupled from the exploration problem, generally relying on other exploration algorithms for data collection. We argue that FB representations should fundamentally be used for exploration in order to learn more efficiently. With this goal in mind, we design exploration policies that arise naturally from the FB representation that minimize the posterior variance of the FB representation, hence minimizing its epistemic uncertainty. We empirically demonstrate that such principled exploration strategies improve sample complexity of the FB algorithm considerably in comparison to other exploration methods. Code is publicly available at https://sites.google.com/view/fbee-url.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05477v1",
    "published_date": "2025-07-07 21:09:16 UTC",
    "updated_date": "2025-07-07 21:09:16 UTC"
  },
  {
    "arxiv_id": "2507.05469v1",
    "title": "Inaugural MOASEI Competition at AAMAS'2025: A Technical Report",
    "authors": [
      "Ceferino Patino",
      "Tyler J. Billings",
      "Alireza Saleh Abadi",
      "Daniel Redder",
      "Adam Eck",
      "Prashant Doshi",
      "Leen-Kiat Soh"
    ],
    "abstract": "We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI) Competition, a multi-agent AI benchmarking event designed to evaluate decision-making under open-world conditions. Built on the free-range-zoo environment suite, MOASEI introduced dynamic, partially observable domains with agent and task openness--settings where entities may appear, disappear, or change behavior over time. The 2025 competition featured three tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct dimensions of openness and coordination complexity. Eleven teams from international institutions participated, with four of those teams submitting diverse solutions including graph neural networks, convolutional architectures, predictive modeling, and large language model--driven meta--optimization. Evaluation metrics centered on expected utility, robustness to perturbations, and responsiveness to environmental change. The results reveal promising strategies for generalization and adaptation in open environments, offering both empirical insight and infrastructure for future research. This report details the competition's design, findings, and contributions to the open-agent systems research community.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Report from the MOASEI'2025 Competition held at AAMAS'2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05469v1",
    "published_date": "2025-07-07 20:44:16 UTC",
    "updated_date": "2025-07-07 20:44:16 UTC"
  },
  {
    "arxiv_id": "2507.05465v2",
    "title": "2048: Reinforcement Learning in a Delayed Reward Environment",
    "authors": [
      "Prady Saligram",
      "Tanvir Bhathal",
      "Robby Manihani"
    ],
    "abstract": "Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "We found an issue with our result aggregation scripts: some evaluation logs were incomplete and others duplicated, causing incorrect numbers in tables and figures. Because these graphs and tables underpin key comparisons, we are withdrawing the paper to regenerate verified results",
    "pdf_url": "https://arxiv.org/pdf/2507.05465v2",
    "published_date": "2025-07-07 20:33:12 UTC",
    "updated_date": "2025-07-24 20:58:48 UTC"
  },
  {
    "arxiv_id": "2507.05463v2",
    "title": "Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers from Driving Video",
    "authors": [
      "Md Zahid Hasan",
      "Guillermo Basulto-Elias",
      "Jun Ha Chang",
      "Sahuna Hallmark",
      "Matthew Rizzo",
      "Anuj Sharma",
      "Soumik Sarkar"
    ],
    "abstract": "We introduce scenario-based cognitive status identification in older drivers from naturalistic driving videos, leveraging large vision models. In recent times, cognitive decline including Dementia and Mild Cognitive Impairment (MCI), is often underdiagnosed due to the time-consuming and costly nature of current diagnostic methods. By analyzing real-world driving behavior captured through in-vehicle sensors, this study aims to extract \"digital fingerprints\" that correlate with functional decline and clinical features of dementia. Moreover, modern large vision models can draw meaningful insights from everyday driving patterns across different roadway scenarios to early detect cognitive decline. We propose a framework that uses large vision models and naturalistic driving videos to analyze driver behavior, identify cognitive status and predict disease progression. We leverage the strong relationship between real-world driving behavior as an observation of the current cognitive status of the drivers where the vehicle can be utilized as a \"diagnostic tool\". Our method identifies early warning signs of functional impairment, contributing to proactive intervention strategies. This work enhances early detection and supports the development of scalable, non-invasive monitoring systems to mitigate the growing societal and economic burden of cognitive decline in the aging population.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05463v2",
    "published_date": "2025-07-07 20:30:00 UTC",
    "updated_date": "2025-08-31 07:30:37 UTC"
  },
  {
    "arxiv_id": "2507.08017v4",
    "title": "Mechanistic Indicators of Understanding in Large Language Models",
    "authors": [
      "Pierre Beckmann",
      "Matthieu Queloz"
    ],
    "abstract": "Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "38 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.08017v4",
    "published_date": "2025-07-07 20:26:31 UTC",
    "updated_date": "2026-01-08 22:15:11 UTC"
  },
  {
    "arxiv_id": "2507.06263v1",
    "title": "The Emotional Alignment Design Policy",
    "authors": [
      "Eric Schwitzgebel",
      "Jeff Sebo"
    ],
    "abstract": "According to what we call the Emotional Alignment Design Policy, artificial entities should be designed to elicit emotional reactions from users that appropriately reflect the entities' capacities and moral status, or lack thereof. This principle can be violated in two ways: by designing an artificial system that elicits stronger or weaker emotional reactions than its capacities and moral status warrant (overshooting or undershooting), or by designing a system that elicits the wrong type of emotional reaction (hitting the wrong target). Although presumably attractive, practical implementation faces several challenges including: How can we respect user autonomy while promoting appropriate responses? How should we navigate expert and public disagreement and uncertainty about facts and values? What if emotional alignment seems to require creating or destroying entities with moral status? To what extent should designs conform to versus attempt to alter user assumptions and attitudes?",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.06263v1",
    "published_date": "2025-07-07 20:26:21 UTC",
    "updated_date": "2025-07-07 20:26:21 UTC"
  },
  {
    "arxiv_id": "2507.05455v2",
    "title": "ModelCitizens: Representing Community Voices in Online Safety",
    "authors": [
      "Ashima Suvarna",
      "Christina Chance",
      "Karolina Naranjo",
      "Hamid Palangi",
      "Sophie Hao",
      "Thomas Hartvigsen",
      "Saadia Gabriel"
    ],
    "abstract": "Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05455v2",
    "published_date": "2025-07-07 20:15:18 UTC",
    "updated_date": "2025-07-09 02:57:34 UTC"
  },
  {
    "arxiv_id": "2507.05448v1",
    "title": "On the Semantics of Large Language Models",
    "authors": [
      "Martin Schuele"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05448v1",
    "published_date": "2025-07-07 20:02:57 UTC",
    "updated_date": "2025-07-07 20:02:57 UTC"
  },
  {
    "arxiv_id": "2507.07120v1",
    "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding",
    "authors": [
      "Nidhi Bhatia",
      "Ankit More",
      "Ritika Borkar",
      "Tiyasa Mitra",
      "Ramon Matas",
      "Ritchie Zhao",
      "Maximilian Golub",
      "Dheevatsa Mudigere",
      "Brian Pharris",
      "Bita Darvish Rouhani"
    ],
    "abstract": "As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.07120v1",
    "published_date": "2025-07-07 19:47:24 UTC",
    "updated_date": "2025-07-07 19:47:24 UTC"
  },
  {
    "arxiv_id": "2507.07060v2",
    "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning",
    "authors": [
      "Shreyas Vinaya Sathyanarayana",
      "Sharanabasava D. Hiremath",
      "Rahil Shah",
      "Rishikesh Panda",
      "Rahul Jana",
      "Riya Singh",
      "Rida Irfan",
      "Ashwin Murali",
      "Bharath Ramsundar"
    ],
    "abstract": "The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.BM",
      "q-bio.MN"
    ],
    "primary_category": "q-bio.QM",
    "comment": "64 pages,",
    "pdf_url": "https://arxiv.org/pdf/2507.07060v2",
    "published_date": "2025-07-07 19:41:39 UTC",
    "updated_date": "2025-08-19 18:39:25 UTC"
  },
  {
    "arxiv_id": "2507.05432v1",
    "title": "Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation",
    "authors": [
      "Inayat Rasool",
      "Pappu Kumar Yadav",
      "Amee Parmar",
      "Hasan Mirzakhaninafchi",
      "Rikesh Budhathoki",
      "Zain Ul Abideen Usmani",
      "Supriya Paudel",
      "Ivan Perez Olivera",
      "Eric Jone"
    ],
    "abstract": "Uniform and excessive herbicide application in modern agriculture contributes to increased input costs, environmental pollution, and the emergence of herbicide resistant weeds. To address these challenges, we developed a vision guided, AI-driven variable rate sprayer system capable of detecting weed presence, estimating canopy size, and dynamically adjusting nozzle activation in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference, and uses an Arduino Uno-based relay interface to control solenoid actuated nozzles based on canopy segmentation results. Indoor trials were conducted using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to simulate a range of weed patch scenarios. The YOLO11n model achieved a mean average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision of 0.55, and recall of 0.52. System performance was validated using water sensitive paper, which showed an average spray coverage of 24.22% in zones where canopy was present. An upward trend in mean spray coverage from 16.22% for small canopies to 21.46% and 21.65% for medium and large canopies, respectively, demonstrated the system's capability to adjust spray output based on canopy size in real time. These results highlight the potential of combining real time deep learning with low-cost embedded hardware for selective herbicide application. Future work will focus on expanding the detection capabilities to include three common weed species in South Dakota: water hemp (Amaranthus tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed by further validation in both indoor and field trials within soybean and corn production systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.05432v1",
    "published_date": "2025-07-07 19:27:29 UTC",
    "updated_date": "2025-07-07 19:27:29 UTC"
  },
  {
    "arxiv_id": "2507.05424v1",
    "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models",
    "authors": [
      "Yufei Tao",
      "Adam Hiatt",
      "Rahul Seetharaman",
      "Ameeta Agrawal"
    ],
    "abstract": "Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05424v1",
    "published_date": "2025-07-07 19:13:20 UTC",
    "updated_date": "2025-07-07 19:13:20 UTC"
  },
  {
    "arxiv_id": "2507.05418v2",
    "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning",
    "authors": [
      "Jaedong Hwang",
      "Kumar Tanmay",
      "Seok-Jin Lee",
      "Ayush Agrawal",
      "Hamid Palangi",
      "Kumar Ayush",
      "Ila Fiete",
      "Paul Pu Liang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. We propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language. Furthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language. To close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks, highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/M2A_GeoFact-X",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05418v2",
    "published_date": "2025-07-07 19:04:36 UTC",
    "updated_date": "2025-09-26 17:57:11 UTC"
  },
  {
    "arxiv_id": "2507.05416v3",
    "title": "EmissionNet: Air Quality Pollution Forecasting for Agriculture",
    "authors": [
      "Prady Saligram",
      "Tanvir Bhathal"
    ],
    "abstract": "Air pollution from agricultural emissions is a significant yet often overlooked contributor to environmental and public health challenges. Traditional air quality forecasting models rely on physics-based approaches, which struggle to capture complex, nonlinear pollutant interactions. In this work, we explore forecasting N$_2$O agricultural emissions through evaluating popular architectures, and proposing two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage convolutional and transformer-based architectures to extract spatial-temporal dependencies from high-resolution emissions data",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The appendix figures are mixed up - several emission plots (e.g. CO2, CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion in interpreting the results",
    "pdf_url": "https://arxiv.org/pdf/2507.05416v3",
    "published_date": "2025-07-07 18:58:22 UTC",
    "updated_date": "2025-08-01 04:55:36 UTC"
  },
  {
    "arxiv_id": "2507.10446v2",
    "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures",
    "authors": [
      "Sudarshan Babu"
    ],
    "abstract": "The ability to transfer knowledge from prior experiences to novel tasks stands as a pivotal capability of intelligent agents, including both humans and computational models. This principle forms the basis of transfer learning, where large pre-trained neural networks are fine-tuned to adapt to downstream tasks. Transfer learning has demonstrated tremendous success, both in terms of task adaptation speed and performance. However there are several domains where, due to lack of data, training such large pre-trained models or foundational models is not a possibility - computational chemistry, computational immunology, and medical imaging are examples. To address these challenges, our work focuses on designing architectures to enable efficient acquisition of priors when large amounts of data are unavailable. In particular, we demonstrate that we can use neural memory to enable adaptation on non-stationary distributions with only a few samples. Then we demonstrate that our hypernetwork designs (a network that generates another network) can acquire more generalizable priors than standard networks when trained with Model Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene generation, demonstrating that they can acquire priors efficiently on just a handful of training scenes, thereby leading to faster text-to-3D generation. We then extend our hypernetwork framework to perform 3D segmentation on novel scenes with limited data by efficiently transferring priors from earlier viewed scenes. Finally, we repurpose an existing molecular generative method as a pre-training framework that facilitates improved molecular property prediction, addressing critical challenges in computational immunology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2310.17075",
    "pdf_url": "https://arxiv.org/pdf/2507.10446v2",
    "published_date": "2025-07-07 18:48:09 UTC",
    "updated_date": "2025-07-15 02:06:07 UTC"
  },
  {
    "arxiv_id": "2507.05405v2",
    "title": "Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification",
    "authors": [
      "Luca Marzari",
      "Ferdinando Cicalese",
      "Alessandro Farinelli"
    ],
    "abstract": "We present $\\textbf{P}$robabilistically $\\textbf{T}$ightened $\\textbf{Li}$near $\\textbf{R}$elaxation-based $\\textbf{P}$erturbation $\\textbf{A}$nalysis ($\\texttt{PT-LiRPA}$), a novel framework that combines over-approximation techniques from LiRPA-based approaches with a sampling-based method to compute tight intermediate reachable sets. In detail, we show that with negligible computational overhead, $\\texttt{PT-LiRPA}$ exploiting the estimated reachable sets, significantly tightens the lower and upper linear bounds of a neural network's output, reducing the computational cost of formal verification tools while providing probabilistic guarantees on verification soundness. Extensive experiments on standard formal verification benchmarks, including the International Verification of Neural Networks Competition, show that our $\\texttt{PT-LiRPA}$-based verifier improves robustness certificates, i.e., the certified lower bound of $\\varepsilon$ perturbation tolerated by the models, by up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic approach results in a valuable solution for challenging competition entries where state-of-the-art formal verification methods fail, allowing us to provide answers with high confidence (i.e., at least 99%).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Journal of Artificial Intelligence Research (JAIR)",
    "pdf_url": "https://arxiv.org/pdf/2507.05405v2",
    "published_date": "2025-07-07 18:45:53 UTC",
    "updated_date": "2025-12-31 11:16:07 UTC"
  },
  {
    "arxiv_id": "2507.06262v1",
    "title": "Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method",
    "authors": [
      "Haoqi He",
      "Xiaokai Lin",
      "Jiancai Chen",
      "Yan Xiao"
    ],
    "abstract": "Data poisoning attacks pose significant threats to machine learning models by introducing malicious data into the training process, thereby degrading model performance or manipulating predictions. Detecting and sifting out poisoned data is an important method to prevent data poisoning attacks. Limited by classical computation frameworks, upcoming larger-scale and more complex datasets may pose difficulties for detection. We introduce the unique speedup of quantum computing for the first time in the task of detecting data poisoning. We present Q-Detection, a quantum-classical hybrid defense method for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which is optimized using quantum computing devices. Experimental results using multiple quantum simulation libraries show that Q-Detection effectively defends against label manipulation and backdoor attacks. The metrics demonstrate that Q-Detection consistently outperforms the baseline methods and is comparable to the state-of-the-art. Theoretical analysis shows that Q-Detection is expected to achieve more than a 20% speedup using quantum computing power.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cs.CR",
    "comment": "IJCAI 2025 Main Conference Accepted Paper",
    "pdf_url": "https://arxiv.org/pdf/2507.06262v1",
    "published_date": "2025-07-07 18:43:34 UTC",
    "updated_date": "2025-07-07 18:43:34 UTC"
  },
  {
    "arxiv_id": "2507.05391v2",
    "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences",
    "authors": [
      "Guillem Ramírez",
      "Alexandra Birch",
      "Ivan Titov"
    ],
    "abstract": "Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Experiments with lightweight local LLMs show that, after fine-tuning, they not only achieve markedly better privacy preservation but also match or exceed the performance of much larger zero-shot models. At the same time, the system still faces challenges in fully adhering to user instructions, underscoring the need for models with a better understanding of user-defined privacy preferences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05391v2",
    "published_date": "2025-07-07 18:22:55 UTC",
    "updated_date": "2025-10-17 18:45:26 UTC"
  },
  {
    "arxiv_id": "2507.05386v5",
    "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training",
    "authors": [
      "Song Lai",
      "Haohan Zhao",
      "Rong Feng",
      "Changyi Ma",
      "Wenzhuo Liu",
      "Hongbo Zhao",
      "Xi Lin",
      "Dong Yi",
      "Qingfu Zhang",
      "Hongbin Liu",
      "Gaofeng Meng",
      "Fei Zhu"
    ],
    "abstract": "Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT's gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05386v5",
    "published_date": "2025-07-07 18:17:06 UTC",
    "updated_date": "2026-01-21 13:37:01 UTC"
  },
  {
    "arxiv_id": "2507.21104v1",
    "title": "iLSU-T: an Open Dataset for Uruguayan Sign Language Translation",
    "authors": [
      "Ariel E. Stassi",
      "Yanina Boria",
      "J. Matías Di Martino",
      "Gregory Randall"
    ],
    "abstract": "Automatic sign language translation has gained particular interest in the computer vision and computational linguistics communities in recent years. Given each sign language country particularities, machine translation requires local data to develop new techniques and adapt existing ones. This work presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB videos with audio and text transcriptions. This type of multimodal and curated data is paramount for developing novel approaches to understand or generate tools for sign language processing. iLSU T comprises more than 185 hours of interpreted sign language videos from public TV broadcasting. It covers diverse topics and includes the participation of 18 professional interpreters of sign language. A series of experiments using three state of the art translation algorithms is presented. The aim is to establish a baseline for this dataset and evaluate its usefulness and the proposed pipeline for data processing. The experiments highlight the need for more localized datasets for sign language translation and understanding, which are critical for developing novel tools to improve accessibility and inclusion of all individuals. Our data and code can be accessed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, 19th International Conference on Automatic Face and Gesture Recognition IEEE FG 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.21104v1",
    "published_date": "2025-07-07 18:11:21 UTC",
    "updated_date": "2025-07-07 18:11:21 UTC"
  },
  {
    "arxiv_id": "2507.05362v2",
    "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
    "authors": [
      "Riccardo Alberghi",
      "Elizaveta Demyanenko",
      "Luca Biggio",
      "Luca Saglietti"
    ],
    "abstract": "Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05362v2",
    "published_date": "2025-07-07 18:00:06 UTC",
    "updated_date": "2025-11-01 15:09:31 UTC"
  },
  {
    "arxiv_id": "2507.05346v2",
    "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks",
    "authors": [
      "William Fleshman",
      "Benjamin Van Durme"
    ],
    "abstract": "The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05346v2",
    "published_date": "2025-07-07 18:00:01 UTC",
    "updated_date": "2025-08-15 18:49:45 UTC"
  },
  {
    "arxiv_id": "2507.05333v1",
    "title": "Causal Foundation Models: Disentangling Physics from Instrument Properties",
    "authors": [
      "Jeroen Audenaert",
      "Daniel Muthukrishna",
      "Paul F. Gregory",
      "David W. Hogg",
      "V. Ashley Villar"
    ],
    "abstract": "Foundation models for structured time series data must contend with a fundamental challenge: observations often conflate the true underlying physical phenomena with systematic distortions introduced by measurement instruments. This entanglement limits model generalization, especially in heterogeneous or multi-instrument settings. We present a causally-motivated foundation model that explicitly disentangles physical and instrumental factors using a dual-encoder architecture trained with structured contrastive learning. Leveraging naturally occurring observational triplets (i.e., where the same target is measured under varying conditions, and distinct targets are measured under shared conditions) our model learns separate latent representations for the underlying physical signal and instrument effects. Evaluated on simulated astronomical time series designed to resemble the complexity of variable stars observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS), our method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes. These results demonstrate that our model supports key capabilities of foundation models, including few-shot generalization and efficient adaptation, and highlight the importance of encoding causal structure into representation learning for structured data.",
    "categories": [
      "cs.LG",
      "astro-ph.IM",
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures. Accepted to the ICML 2025 Foundation Models for Structured Data Workshop and accepted to the Machine Learning for Astrophysics Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05333v1",
    "published_date": "2025-07-07 18:00:00 UTC",
    "updated_date": "2025-07-07 18:00:00 UTC"
  },
  {
    "arxiv_id": "2507.05257v2",
    "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
    "authors": [
      "Yuanzhe Hu",
      "Yu Wang",
      "Julian McAuley"
    ],
    "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Y. Hu and Y. Wang contribute equally",
    "pdf_url": "https://arxiv.org/pdf/2507.05257v2",
    "published_date": "2025-07-07 17:59:54 UTC",
    "updated_date": "2025-09-26 03:31:14 UTC"
  },
  {
    "arxiv_id": "2507.05254v1",
    "title": "From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving",
    "authors": [
      "Fabian Konstantinidis",
      "Ariel Dallari Guerreiro",
      "Raphael Trumpp",
      "Moritz Sackmann",
      "Ulrich Hofmann",
      "Marco Caccamo",
      "Christoph Stiller"
    ],
    "abstract": "Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Conference on Intelligent Transportation Systems 2025 (ITSC 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.05254v1",
    "published_date": "2025-07-07 17:58:53 UTC",
    "updated_date": "2025-07-07 17:58:53 UTC"
  },
  {
    "arxiv_id": "2507.05251v1",
    "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving",
    "authors": [
      "Elahe Delavari",
      "Feeza Khan Khanzada",
      "Jaerock Kwon"
    ],
    "abstract": "Reinforcement Learning (RL) offers a promising framework for autonomous driving by enabling agents to learn control policies through interaction with environments. However, large and high-dimensional action spaces often used to support fine-grained control can impede training efficiency and increase exploration costs. In this study, we introduce and evaluate two novel structured action space modification strategies for RL in autonomous driving: dynamic masking and relative action space reduction. These approaches are systematically compared against fixed reduction schemes and full action space baselines to assess their impact on policy learning and performance. Our framework leverages a multimodal Proximal Policy Optimization agent that processes both semantic image sequences and scalar vehicle states. The proposed dynamic and relative strategies incorporate real-time action masking based on context and state transitions, preserving action consistency while eliminating invalid or suboptimal choices. Through comprehensive experiments across diverse driving routes, we show that action space reduction significantly improves training stability and policy performance. The dynamic and relative schemes, in particular, achieve a favorable balance between learning speed, control precision, and generalization. These findings highlight the importance of context-aware action space design for scalable and reliable RL in autonomous driving tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05251v1",
    "published_date": "2025-07-07 17:58:08 UTC",
    "updated_date": "2025-07-07 17:58:08 UTC"
  },
  {
    "arxiv_id": "2507.05246v1",
    "title": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors",
    "authors": [
      "Scott Emmons",
      "Erik Jenner",
      "David K. Elson",
      "Rif A. Saurous",
      "Senthooran Rajamanoharan",
      "Heng Chen",
      "Irhum Shafkat",
      "Rohin Shah"
    ],
    "abstract": "While chain-of-thought (CoT) monitoring is an appealing AI safety defense, recent work on \"unfaithfulness\" has cast doubt on its reliability. These findings highlight an important failure mode, particularly when CoT acts as a post-hoc rationalization in applications like auditing for bias. However, for the distinct problem of runtime monitoring to prevent severe harm, we argue the key property is not faithfulness but monitorability. To this end, we introduce a conceptual framework distinguishing CoT-as-rationalization from CoT-as-computation. We expect that certain classes of severe harm will require complex, multi-step reasoning that necessitates CoT-as-computation. Replicating the experimental setups of prior work, we increase the difficulty of the bad behavior to enforce this necessity condition; this forces the model to expose its reasoning, making it monitorable. We then present methodology guidelines to stress-test CoT monitoring against deliberate evasion. Applying these guidelines, we find that models can learn to obscure their intentions, but only when given significant help, such as detailed human-written strategies or iterative optimization against the monitor. We conclude that, while not infallible, CoT monitoring offers a substantial layer of defense that requires active protection and continued stress-testing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05246v1",
    "published_date": "2025-07-07 17:54:52 UTC",
    "updated_date": "2025-07-07 17:54:52 UTC"
  },
  {
    "arxiv_id": "2507.05330v1",
    "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents",
    "authors": [
      "Ming Gong",
      "Xucheng Huang",
      "Chenghan Yang",
      "Xianhan Peng",
      "Haoxin Wang",
      "Yang Liu",
      "Ling Jiang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05330v1",
    "published_date": "2025-07-07 17:53:55 UTC",
    "updated_date": "2025-07-07 17:53:55 UTC"
  },
  {
    "arxiv_id": "2507.05244v1",
    "title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration",
    "authors": [
      "Benjamin Li",
      "Shuyang Shi",
      "Lucia Romero",
      "Huao Li",
      "Yaqi Xie",
      "Woojun Kim",
      "Stefanos Nikolaidis",
      "Michael Lewis",
      "Katia Sycara",
      "Simon Stepputtis"
    ],
    "abstract": "In collaborative tasks, being able to adapt to your teammates is a necessary requirement for success. When teammates are heterogeneous, such as in human-agent teams, agents need to be able to observe, recognize, and adapt to their human partners in real time. This becomes particularly challenging in tasks with time pressure and complex strategic spaces where the dynamics can change rapidly. In this work, we introduce TALENTS, a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a range of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a variational autoencoder to learn a latent strategy space from trajectory data. This latent space represents the underlying strategies that agents employ. Subsequently, the system identifies different types of strategy by clustering the data. Finally, a cooperator agent is trained to generate partners for each type of strategy, conditioned on these clusters. In order to adapt to previously unseen partners, we leverage a fixed-share regret minimization algorithm that infers and adjusts the estimated partner strategy dynamically. We assess our approach in a customized version of the Overcooked environment, posing a challenging cooperative cooking task that demands strong coordination across a wide range of possible strategies. Using an online user study, we show that our agent outperforms current baselines when working with unfamiliar human partners.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Best Paper Award at the RSS 2025 Generative Models x HRI (GenAI-HRI) Workshop",
    "pdf_url": "https://arxiv.org/pdf/2507.05244v1",
    "published_date": "2025-07-07 17:53:13 UTC",
    "updated_date": "2025-07-07 17:53:13 UTC"
  },
  {
    "arxiv_id": "2507.05328v1",
    "title": "Going Beyond Heuristics by Imposing Policy Improvement as a Constraint",
    "authors": [
      "Chi-Chang Lee",
      "Zhang-Wei Hong",
      "Pulkit Agrawal"
    ],
    "abstract": "In many reinforcement learning (RL) applications, augmenting the task rewards with heuristic rewards that encode human priors about how a task should be solved is crucial for achieving desirable performance. However, because such heuristics are usually not optimal, much human effort and computational resources are wasted in carefully balancing tasks and heuristic rewards. Theoretically rigorous ways of incorporating heuristics rely on the idea of \\textit{policy invariance}, which guarantees that the performance of a policy obtained by maximizing heuristic rewards is the same as the optimal policy with respect to the task reward. However, in practice, policy invariance doesn't result in policy improvement, and such methods are known to empirically perform poorly. We propose a new paradigm to mitigate reward hacking and effectively use heuristics based on the practical goal of maximizing policy improvement instead of policy improvement. Our framework, Heuristic Enhanced Policy Optimization (HEPO), effectively leverages heuristics while avoiding the pitfall of prior methods for mitigating reward hacking. HEPO achieves superior performance on standard benchmarks with well-engineered reward functions. More surprisingly, HEPO allows policy optimization to achieve good performance even when heuristics are not well-engineered and designed by non-expert humans, showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a plug-and-play optimization method for leveraging heuristics in reinforcement learning. Code is available at https://github.com/Improbable-AI/hepo.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05328v1",
    "published_date": "2025-07-07 17:52:53 UTC",
    "updated_date": "2025-07-07 17:52:53 UTC"
  },
  {
    "arxiv_id": "2507.05241v2",
    "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
    "authors": [
      "Jingyi Chai",
      "Shuo Tang",
      "Rui Ye",
      "Yuwen Du",
      "Xinyu Zhu",
      "Mengcheng Zhou",
      "Yanfeng Wang",
      "Weinan E",
      "Yuzhi Zhang",
      "Linfeng Zhang",
      "Siheng Chen"
    ],
    "abstract": "The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05241v2",
    "published_date": "2025-07-07 17:50:52 UTC",
    "updated_date": "2025-07-08 15:54:19 UTC"
  },
  {
    "arxiv_id": "2507.06261v6",
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
    "authors": [
      "Gheorghe Comanici",
      "Eric Bieber",
      "Mike Schaekermann",
      "Ice Pasupat",
      "Noveen Sachdeva",
      "Inderjit Dhillon",
      "Marcel Blistein",
      "Ori Ram",
      "Dan Zhang",
      "Evan Rosen",
      "Luke Marris",
      "Sam Petulla",
      "Colin Gaffney",
      "Asaf Aharoni",
      "Nathan Lintz",
      "Tiago Cardal Pais",
      "Henrik Jacobsson",
      "Idan Szpektor",
      "Nan-Jiang Jiang",
      "Krishna Haridasan",
      "Ahmed Omran",
      "Nikunj Saunshi",
      "Dara Bahri",
      "Gaurav Mishra",
      "Eric Chu",
      "Toby Boyd",
      "Brad Hekman",
      "Aaron Parisi",
      "Chaoyi Zhang",
      "Kornraphop Kawintiranon",
      "Tania Bedrax-Weiss",
      "Oliver Wang",
      "Ya Xu",
      "Ollie Purkiss",
      "Uri Mendlovic",
      "Ilaï Deutel",
      "Nam Nguyen",
      "Adam Langley",
      "Flip Korn",
      "Lucia Rossazza",
      "Alexandre Ramé",
      "Sagar Waghmare",
      "Helen Miller",
      "Nathan Byrd",
      "Ashrith Sheshan",
      "Raia Hadsell",
      "Sangnie Bhardwaj",
      "Pawel Janus",
      "Tero Rissa",
      "Dan Horgan",
      "Alvin Abdagic",
      "Lior Belenki",
      "James Allingham",
      "Anima Singh",
      "Theo Guidroz",
      "Srivatsan Srinivasan",
      "Herman Schmit",
      "Kristen Chiafullo",
      "Andre Elisseeff",
      "Nilpa Jha",
      "Prateek Kolhar",
      "Leonard Berrada",
      "Frank Ding",
      "Xiance Si",
      "Shrestha Basu Mallick",
      "Franz Och",
      "Sofia Erell",
      "Eric Ni",
      "Tejasi Latkar",
      "Sherry Yang",
      "Petar Sirkovic",
      "Ziqiang Feng",
      "Robert Leland",
      "Rachel Hornung",
      "Gang Wu",
      "Charles Blundell",
      "Hamidreza Alvari",
      "Po-Sen Huang",
      "Cathy Yip",
      "Sanja Deur",
      "Li Liu",
      "Gabriela Surita",
      "Pablo Duque",
      "Dima Damen",
      "Johnson Jia",
      "Arthur Guez",
      "Markus Mircea",
      "Animesh Sinha",
      "Alberto Magni",
      "Paweł Stradomski",
      "Tal Marian",
      "Vlado Galić",
      "Wenhu Chen",
      "Hisham Husain",
      "Achintya Singhal",
      "Dominik Grewe",
      "François-Xavier Aubet",
      "Shuang Song",
      "Lorenzo Blanco",
      "Leland Rechis",
      "Lewis Ho",
      "Rich Munoz",
      "Kelvin Zheng",
      "Jessica Hamrick",
      "Kevin Mather",
      "Hagai Taitelbaum",
      "Eliza Rutherford",
      "Yun Lei",
      "Kuangyuan Chen",
      "Anand Shukla",
      "Erica Moreira",
      "Eric Doi",
      "Berivan Isik",
      "Nir Shabat",
      "Dominika Rogozińska",
      "Kashyap Kolipaka",
      "Jason Chang",
      "Eugen Vušak",
      "Srinivasan Venkatachary",
      "Shadi Noghabi",
      "Tarun Bharti",
      "Younghoon Jun",
      "Aleksandr Zaks",
      "Simon Green",
      "Jeshwanth Challagundla",
      "William Wong",
      "Muqthar Mohammad",
      "Dean Hirsch",
      "Yong Cheng",
      "Iftekhar Naim",
      "Lev Proleev",
      "Damien Vincent",
      "Aayush Singh",
      "Maxim Krikun",
      "Dilip Krishnan",
      "Zoubin Ghahramani",
      "Aviel Atias",
      "Rajeev Aggarwal",
      "Christo Kirov",
      "Dimitrios Vytiniotis",
      "Christy Koh",
      "Alexandra Chronopoulou",
      "Pawan Dogra",
      "Vlad-Doru Ion",
      "Gladys Tyen",
      "Jason Lee",
      "Felix Weissenberger",
      "Trevor Strohman",
      "Ashwin Balakrishna",
      "Jack Rae",
      "Marko Velic",
      "Raoul de Liedekerke",
      "Oded Elyada",
      "Wentao Yuan",
      "Canoee Liu",
      "Lior Shani",
      "Sergey Kishchenko",
      "Bea Alessio",
      "Yandong Li",
      "Richard Song",
      "Sam Kwei",
      "Orion Jankowski",
      "Aneesh Pappu",
      "Youhei Namiki",
      "Yenai Ma",
      "Nilesh Tripuraneni",
      "Colin Cherry",
      "Marissa Ikonomidis",
      "Yu-Cheng Ling",
      "Colin Ji",
      "Beka Westberg",
      "Auriel Wright",
      "Da Yu",
      "David Parkinson",
      "Swaroop Ramaswamy",
      "Jerome Connor",
      "Soheil Hassas Yeganeh",
      "Snchit Grover",
      "George Kenwright",
      "Lubo Litchev",
      "Chris Apps",
      "Alex Tomala",
      "Felix Halim",
      "Alex Castro-Ros",
      "Zefei Li",
      "Anudhyan Boral",
      "Pauline Sho",
      "Michal Yarom",
      "Eric Malmi",
      "David Klinghoffer",
      "Rebecca Lin",
      "Alan Ansell",
      "Pradeep Kumar S",
      "Shubin Zhao",
      "Siqi Zuo",
      "Adam Santoro",
      "Heng-Tze Cheng",
      "Solomon Demmessie",
      "Yuchi Liu",
      "Nicole Brichtova",
      "Allie Culp",
      "Nathaniel Braun",
      "Dan Graur",
      "Will Ng",
      "Nikhil Mehta",
      "Aaron Phillips",
      "Patrik Sundberg",
      "Varun Godbole",
      "Fangyu Liu",
      "Yash Katariya",
      "David Rim",
      "Mojtaba Seyedhosseini",
      "Sean Ammirati",
      "Jonas Valfridsson",
      "Mahan Malihi",
      "Timothy Knight",
      "Andeep Toor",
      "Thomas Lampe",
      "Abe Ittycheriah",
      "Lewis Chiang",
      "Chak Yeung",
      "Alexandre Fréchette",
      "Jinmeng Rao",
      "Huisheng Wang",
      "Himanshu Srivastava",
      "Richard Zhang",
      "Rocky Rhodes",
      "Ariel Brand",
      "Dean Weesner",
      "Ilya Figotin",
      "Felix Gimeno",
      "Rachana Fellinger",
      "Pierre Marcenac",
      "José Leal",
      "Eyal Marcus",
      "Victor Cotruta",
      "Rodrigo Cabrera",
      "Sheryl Luo",
      "Dan Garrette",
      "Vera Axelrod",
      "Sorin Baltateanu",
      "David Barker",
      "Dongkai Chen",
      "Horia Toma",
      "Ben Ingram",
      "Jason Riesa",
      "Chinmay Kulkarni",
      "Yujing Zhang",
      "Hongbin Liu",
      "Chao Wang",
      "Martin Polacek",
      "Will Wu",
      "Kai Hui",
      "Adrian N Reyes",
      "Yi Su",
      "Megan Barnes",
      "Ishaan Malhi",
      "Anfal Siddiqui",
      "Qixuan Feng",
      "Mihai Damaschin",
      "Daniele Pighin",
      "Andreas Steiner",
      "Samuel Yang",
      "Ramya Sree Boppana",
      "Simeon Ivanov",
      "Arun Kandoor",
      "Aditya Shah",
      "Asier Mujika",
      "Da Huang",
      "Christopher A. Choquette-Choo",
      "Mohak Patel",
      "Tianhe Yu",
      "Toni Creswell",
      "Jerry",
      "Liu",
      "Catarina Barros",
      "Yasaman Razeghi",
      "Aurko Roy",
      "Phil Culliton",
      "Binbin Xiong",
      "Jiaqi Pan",
      "Thomas Strohmann",
      "Tolly Powell",
      "Babi Seal",
      "Doug DeCarlo",
      "Pranav Shyam",
      "Kaan Katircioglu",
      "Xuezhi Wang",
      "Cassidy Hardin",
      "Immanuel Odisho",
      "Josef Broder",
      "Oscar Chang",
      "Arun Nair",
      "Artem Shtefan",
      "Maura O'Brien",
      "Manu Agarwal",
      "Sahitya Potluri",
      "Siddharth Goyal",
      "Amit Jhindal",
      "Saksham Thakur",
      "Yury Stuken",
      "James Lyon",
      "Kristina Toutanova",
      "Fangxiaoyu Feng",
      "Austin Wu",
      "Ben Horn",
      "Alek Wang",
      "Alex Cullum",
      "Gabe Taubman",
      "Disha Shrivastava",
      "Chongyang Shi",
      "Hamish Tomlinson",
      "Roma Patel",
      "Tao Tu",
      "Ada Maksutaj Oflazer",
      "Francesco Pongetti",
      "Mingyao Yang",
      "Adrien Ali Taïga",
      "Vincent Perot",
      "Nuo Wang Pierse",
      "Feng Han",
      "Yoel Drori",
      "Iñaki Iturrate",
      "Ayan Chakrabarti",
      "Legg Yeung",
      "Dave Dopson",
      "Yi-ting Chen",
      "Apoorv Kulshreshtha",
      "Tongfei Guo",
      "Philip Pham",
      "Tal Schuster",
      "Junquan Chen",
      "Alex Polozov",
      "Jinwei Xing",
      "Huanjie Zhou",
      "Praneeth Kacham",
      "Doron Kukliansky",
      "Antoine Miech",
      "Sergey Yaroshenko",
      "Ed Chi",
      "Sholto Douglas",
      "Hongliang Fei",
      "Mathieu Blondel",
      "Preethi Myla",
      "Lior Madmoni",
      "Xing Wu",
      "Daniel Keysers",
      "Kristian Kjems",
      "Isabela Albuquerque",
      "Lijun Yu",
      "Joel D'sa",
      "Michelle Plantan",
      "Vlad Ionescu",
      "Jaume Sanchez Elias",
      "Abhirut Gupta",
      "Manish Reddy Vuyyuru",
      "Fred Alcober",
      "Tong Zhou",
      "Kaiyang Ji",
      "Florian Hartmann",
      "Subha Puttagunta",
      "Hugo Song",
      "Ehsan Amid",
      "Anca Stefanoiu",
      "Andrew Lee",
      "Paul Pucciarelli",
      "Emma Wang",
      "Amit Raul",
      "Slav Petrov",
      "Isaac Tian",
      "Valentin Anklin",
      "Nana Nti",
      "Victor Gomes",
      "Max Schumacher",
      "Grace Vesom",
      "Alex Panagopoulos",
      "Konstantinos Bousmalis",
      "Daniel Andor",
      "Josh Jacob",
      "Yuan Zhang",
      "Bill Rosgen",
      "Matija Kecman",
      "Matthew Tung",
      "Alexandra Belias",
      "Noah Goodman",
      "Paul Covington",
      "Brian Wieder",
      "Nikita Saxena",
      "Elnaz Davoodi",
      "Muhuan Huang",
      "Sharath Maddineni",
      "Vincent Roulet",
      "Folawiyo Campbell-Ajala",
      "Pier Giuseppe Sessa",
      "Xintian",
      "Wu",
      "Guangda Lai",
      "Paul Collins",
      "Alex Haig",
      "Vytenis Sakenas",
      "Xiaowei Xu",
      "Marissa Giustina",
      "Laurent El Shafey",
      "Pichi Charoenpanit",
      "Shefali Garg",
      "Joshua Ainslie",
      "Boone Severson",
      "Montse Gonzalez Arenas",
      "Shreya Pathak",
      "Sujee Rajayogam",
      "Jie Feng",
      "Michiel Bakker",
      "Sheng Li",
      "Nevan Wichers",
      "Jamie Rogers",
      "Xinyang Geng",
      "Yeqing Li",
      "Rolf Jagerman",
      "Chao Jia",
      "Nadav Olmert",
      "David Sharon",
      "Matthew Mauger",
      "Sandeep Mariserla",
      "Hongxu Ma",
      "Megha Mohabey",
      "Kyuyeun Kim",
      "Alek Andreev",
      "Scott Pollom",
      "Juliette Love",
      "Vihan Jain",
      "Priyanka Agrawal",
      "Yannick Schroecker",
      "Alisa Fortin",
      "Manfred Warmuth",
      "Ji Liu",
      "Andrew Leach",
      "Irina Blok",
      "Ganesh Poomal Girirajan",
      "Roee Aharoni",
      "Benigno Uria",
      "Andrei Sozanschi",
      "Dan Goldberg",
      "Lucian Ionita",
      "Marco Tulio Ribeiro",
      "Martin Zlocha",
      "Vighnesh Birodkar",
      "Sami Lachgar",
      "Liangzhe Yuan",
      "Himadri Choudhury",
      "Matt Ginsberg",
      "Fei Zheng",
      "Gregory Dibb",
      "Emily Graves",
      "Swachhand Lokhande",
      "Gabriel Rasskin",
      "George-Cristian Muraru",
      "Corbin Quick",
      "Sandeep Tata",
      "Pierre Sermanet",
      "Aditya Chawla",
      "Itay Karo",
      "Yan Wang",
      "Susan Zhang",
      "Orgad Keller",
      "Anca Dragan",
      "Guolong Su",
      "Ian Chou",
      "Xi Liu",
      "Yiqing Tao",
      "Shruthi Prabhakara",
      "Marc Wilson",
      "Ruibo Liu",
      "Shibo Wang",
      "Georgie Evans",
      "David Du",
      "Alfonso Castaño",
      "Gautam Prasad",
      "Mona El Mahdy",
      "Sebastian Gerlach",
      "Machel Reid",
      "Jarrod Kahn",
      "Amir Zait",
      "Thanumalayan Sankaranarayana Pillai",
      "Thatcher Ulrich",
      "Guanyu Wang",
      "Jan Wassenberg",
      "Efrat Farkash",
      "Kiran Yalasangi",
      "Congchao Wang",
      "Maria Bauza",
      "Simon Bucher",
      "Ting Liu",
      "Jun Yan",
      "Gary Leung",
      "Vikas Sindhwani",
      "Parker Barnes",
      "Avi Singh",
      "Ivan Jurin",
      "Jichuan Chang",
      "Niket Kumar Bhumihar",
      "Sivan Eiger",
      "Gui Citovsky",
      "Ben Withbroe",
      "Zhang Li",
      "Siyang Xue",
      "Niccolò Dal Santo",
      "Georgi Stoyanov",
      "Yves Raimond",
      "Steven Zheng",
      "Yilin Gao",
      "Vít Listík",
      "Sławek Kwasiborski",
      "Rachel Saputro",
      "Adnan Ozturel",
      "Ganesh Mallya",
      "Kushal Majmundar",
      "Ross West",
      "Paul Caron",
      "Jinliang Wei",
      "Lluis Castrejon",
      "Sharad Vikram",
      "Deepak Ramachandran",
      "Nikhil Dhawan",
      "Jiho Park",
      "Sara Smoot",
      "George van den Driessche",
      "Yochai Blau",
      "Chase Malik",
      "Wei Liang",
      "Roy Hirsch",
      "Cicero Nogueira dos Santos",
      "Eugene Weinstein",
      "Aäron van den Oord",
      "Sid Lall",
      "Nicholas FitzGerald",
      "Zixuan Jiang",
      "Xuan Yang",
      "Dale Webster",
      "Ali Elqursh",
      "Aedan Pope",
      "Georges Rotival",
      "David Raposo",
      "Wanzheng Zhu",
      "Jeff Dean",
      "Sami Alabed",
      "Dustin Tran",
      "Arushi Gupta",
      "Zach Gleicher",
      "Jessica Austin",
      "Edouard Rosseel",
      "Megh Umekar",
      "Dipanjan Das",
      "Yinghao Sun",
      "Kai Chen",
      "Karolis Misiunas",
      "Xiang Zhou",
      "Yixian Di",
      "Alyssa Loo",
      "Josh Newlan",
      "Bo Li",
      "Vinay Ramasesh",
      "Ying Xu",
      "Alex Chen",
      "Sudeep Gandhe",
      "Radu Soricut",
      "Nikita Gupta",
      "Shuguang Hu",
      "Seliem El-Sayed",
      "Xavier Garcia",
      "Idan Brusilovsky",
      "Pu-Chin Chen",
      "Andrew Bolt",
      "Lu Huang",
      "Alex Gurney",
      "Zhiying Zhang",
      "Alexander Pritzel",
      "Jarek Wilkiewicz",
      "Bryan Seybold",
      "Bhargav Kanagal Shamanna",
      "Felix Fischer",
      "Josef Dean",
      "Karan Gill",
      "Ross Mcilroy",
      "Abhishek Bhowmick",
      "Jeremy Selier",
      "Antoine Yang",
      "Derek Cheng",
      "Vladimir Magay",
      "Jie Tan",
      "Dhriti Varma",
      "Christian Walder",
      "Tomas Kocisky",
      "Ryo Nakashima",
      "Paul Natsev",
      "Mike Kwong",
      "Ionel Gog",
      "Chiyuan Zhang",
      "Sander Dieleman",
      "Thomas Jimma",
      "Andrey Ryabtsev",
      "Siddhartha Brahma",
      "David Steiner",
      "Dayou Du",
      "Ante Žužul",
      "Mislav Žanić",
      "Mukund Raghavachari",
      "Willi Gierke",
      "Zeyu Zheng",
      "Dessie Petrova",
      "Yann Dauphin",
      "Yuchuan Liu",
      "Ido Kessler",
      "Steven Hand",
      "Chris Duvarney",
      "Seokhwan Kim",
      "Hyo Lee",
      "Léonard Hussenot",
      "Jeffrey Hui",
      "Josh Smith",
      "Deepali Jain",
      "Jiawei Xia",
      "Gaurav Singh Tomar",
      "Keyvan Amiri",
      "Du Phan",
      "Fabian Fuchs",
      "Tobias Weyand",
      "Nenad Tomasev",
      "Alexandra Cordell",
      "Xin Liu",
      "Jonathan Mallinson",
      "Pankaj Joshi",
      "Andy Crawford",
      "Arun Suggala",
      "Steve Chien",
      "Nick Fernando",
      "Mariella Sanchez-Vargas",
      "Duncan Williams",
      "Phil Crone",
      "Xiyang Luo",
      "Igor Karpov",
      "Jyn Shan",
      "Terry Thurk",
      "Robin Strudel",
      "Paul Voigtlaender",
      "Piyush Patil",
      "Tim Dozat",
      "Ali Khodaei",
      "Sahil Singla",
      "Piotr Ambroszczyk",
      "Qiyin Wu",
      "Yifan Chang",
      "Brian Roark",
      "Chaitra Hegde",
      "Tianli Ding",
      "Angelos Filos",
      "Zhongru Wu",
      "André Susano Pinto",
      "Shuang Liu",
      "Saarthak Khanna",
      "Aditya Pandey",
      "Siobhan Mcloughlin",
      "Qiujia Li",
      "Sam Haves",
      "Allan Zhou",
      "Elena Buchatskaya",
      "Isabel Leal",
      "Peter de Boursac",
      "Nami Akazawa",
      "Nina Anderson",
      "Terry Chen",
      "Krishna Somandepalli",
      "Chen Liang",
      "Sheela Goenka",
      "Stephanie Winkler",
      "Alexander Grushetsky",
      "Yifan Ding",
      "Jamie Smith",
      "Fan Ye",
      "Jordi Pont-Tuset",
      "Eric Li",
      "Ruichao Li",
      "Tomer Golany",
      "Dawid Wegner",
      "Tao Jiang",
      "Omer Barak",
      "Yuan Shangguan",
      "Eszter Vértes",
      "Renee Wong",
      "Jörg Bornschein",
      "Alex Tudor",
      "Michele Bevilacqua",
      "Tom Schaul",
      "Ankit Singh Rawat",
      "Yang Zhao",
      "Kyriakos Axiotis",
      "Lei Meng",
      "Cory McLean",
      "Jonathan Lai",
      "Jennifer Beattie",
      "Nate Kushman",
      "Yaxin Liu",
      "Blair Kutzman",
      "Fiona Lang",
      "Jingchen Ye",
      "Praneeth Netrapalli",
      "Pushkar Mishra",
      "Myriam Khan",
      "Megha Goel",
      "Rob Willoughby",
      "David Tian",
      "Honglei Zhuang",
      "JD Chen",
      "Zak Tsai",
      "Tasos Kementsietsidis",
      "Arjun Khare",
      "James Keeling",
      "Keyang Xu",
      "Nathan Waters",
      "Florent Altché",
      "Ashok Popat",
      "Bhavishya Mittal",
      "David Saxton",
      "Dalia El Badawy",
      "Michael Mathieu",
      "Zheng Zheng",
      "Hao Zhou",
      "Nishant Ranka",
      "Richard Shin",
      "Qingnan Duan",
      "Tim Salimans",
      "Ioana Mihailescu",
      "Uri Shaham",
      "Ming-Wei Chang",
      "Yannis Assael",
      "Nishanth Dikkala",
      "Martin Izzard",
      "Vincent Cohen-Addad",
      "Cat Graves",
      "Vlad Feinberg",
      "Grace Chung",
      "DJ Strouse",
      "Danny Karmon",
      "Sahand Sharifzadeh",
      "Zoe Ashwood",
      "Khiem Pham",
      "Jon Blanton",
      "Alex Vasiloff",
      "Jarred Barber",
      "Mark Geller",
      "Aurick Zhou",
      "Fedir Zubach",
      "Tzu-Kuo Huang",
      "Lei Zhang",
      "Himanshu Gupta",
      "Matt Young",
      "Julia Proskurnia",
      "Ronny Votel",
      "Valentin Gabeur",
      "Gabriel Barcik",
      "Aditya Tripathi",
      "Hongkun Yu",
      "Geng Yan",
      "Beer Changpinyo",
      "Filip Pavetić",
      "Amy Coyle",
      "Yasuhisa Fujii",
      "Jorge Gonzalez Mendez",
      "Tianhao Zhou",
      "Harish Rajamani",
      "Blake Hechtman",
      "Eddie Cao",
      "Da-Cheng Juan",
      "Yi-Xuan Tan",
      "Valentin Dalibard",
      "Yilun Du",
      "Natalie Clay",
      "Kaisheng Yao",
      "Wenhao Jia",
      "Dimple Vijaykumar",
      "Yuxiang Zhou",
      "Xinyi Bai",
      "Wei-Chih Hung",
      "Steven Pecht",
      "Georgi Todorov",
      "Nikhil Khadke",
      "Pramod Gupta",
      "Preethi Lahoti",
      "Arnaud Autef",
      "Karthik Duddu",
      "James Lee-Thorp",
      "Alexander Bykovsky",
      "Tautvydas Misiunas",
      "Sebastian Flennerhag",
      "Santhosh Thangaraj",
      "Jed McGiffin",
      "Zack Nado",
      "Markus Kunesch",
      "Andreas Noever",
      "Amir Hertz",
      "Marco Liang",
      "Victor Stone",
      "Evan Palmer",
      "Samira Daruki",
      "Arijit Pramanik",
      "Siim Põder",
      "Austin Kyker",
      "Mina Khan",
      "Evgeny Sluzhaev",
      "Marvin Ritter",
      "Avraham Ruderman",
      "Wenlei Zhou",
      "Chirag Nagpal",
      "Kiran Vodrahalli",
      "George Necula",
      "Paul Barham",
      "Ellie Pavlick",
      "Jay Hartford",
      "Izhak Shafran",
      "Long Zhao",
      "Maciej Mikuła",
      "Tom Eccles",
      "Hidetoshi Shimokawa",
      "Kanav Garg",
      "Luke Vilnis",
      "Hanwen Chen",
      "Ilia Shumailov",
      "Kuang-Huei Lee",
      "Abdelrahman Abdelhamed",
      "Meiyan Xie",
      "Vered Cohen",
      "Ester Hlavnova",
      "Dan Malkin",
      "Chawin Sitawarin",
      "James Lottes",
      "Pauline Coquinot",
      "Tianli Yu",
      "Sandeep Kumar",
      "Jingwei Zhang",
      "Aroma Mahendru",
      "Zafarali Ahmed",
      "James Martens",
      "Tao Chen",
      "Aviel Boag",
      "Daiyi Peng",
      "Coline Devin",
      "Arseniy Klimovskiy",
      "Mary Phuong",
      "Danny Vainstein",
      "Jin Xie",
      "Bhuvana Ramabhadran",
      "Nathan Howard",
      "Xinxin Yu",
      "Gitartha Goswami",
      "Jingyu Cui",
      "Sam Shleifer",
      "Mario Pinto",
      "Chih-Kuan Yeh",
      "Ming-Hsuan Yang",
      "Sara Javanmardi",
      "Dan Ethier",
      "Chace Lee",
      "Jordi Orbay",
      "Suyog Kotecha",
      "Carla Bromberg",
      "Pete Shaw",
      "James Thornton",
      "Adi Gerzi Rosenthal",
      "Shane Gu",
      "Matt Thomas",
      "Ian Gemp",
      "Aditya Ayyar",
      "Asahi Ushio",
      "Aarush Selvan",
      "Joel Wee",
      "Chenxi Liu",
      "Maryam Majzoubi",
      "Weiren Yu",
      "Jake Abernethy",
      "Tyler Liechty",
      "Renke Pan",
      "Hoang Nguyen",
      "Qiong",
      "Hu",
      "Sarah Perrin",
      "Abhinav Arora",
      "Emily Pitler",
      "Weiyi Wang",
      "Kaushik Shivakumar",
      "Flavien Prost",
      "Ben Limonchik",
      "Jing Wang",
      "Yi Gao",
      "Timothee Cour",
      "Shyamal Buch",
      "Huan Gui",
      "Maria Ivanova",
      "Philipp Neubeck",
      "Kelvin Chan",
      "Lucy Kim",
      "Huizhong Chen",
      "Naman Goyal",
      "Da-Woon Chung",
      "Lu Liu",
      "Yao Su",
      "Anastasia Petrushkina",
      "Jiajun Shen",
      "Armand Joulin",
      "Yuanzhong Xu",
      "Stein Xudong Lin",
      "Yana Kulizhskaya",
      "Ciprian Chelba",
      "Shobha Vasudevan",
      "Eli Collins",
      "Vasilisa Bashlovkina",
      "Tony Lu",
      "Doug Fritz",
      "Jongbin Park",
      "Yanqi Zhou",
      "Chen Su",
      "Richard Tanburn",
      "Mikhail Sushkov",
      "Mitchelle Rasquinha",
      "Jinning Li",
      "Jennifer Prendki",
      "Yiming Li",
      "Pallavi LV",
      "Shriya Sharma",
      "Hen Fitoussi",
      "Hui Huang",
      "Andrew Dai",
      "Phuong Dao",
      "Mike Burrows",
      "Henry Prior",
      "Danfeng Qin",
      "Golan Pundak",
      "Lars Lowe Sjoesund",
      "Art Khurshudov",
      "Zhenkai Zhu",
      "Albert Webson",
      "Elizabeth Kemp",
      "Tat Tan",
      "Saurabh Agrawal",
      "Susie Sargsyan",
      "Liqun Cheng",
      "Jim Stephan",
      "Tom Kwiatkowski",
      "David Reid",
      "Arunkumar Byravan",
      "Assaf Hurwitz Michaely",
      "Nicolas Heess",
      "Luowei Zhou",
      "Sonam Goenka",
      "Viral Carpenter",
      "Anselm Levskaya",
      "Bo Wang",
      "Reed Roberts",
      "Rémi Leblond",
      "Sharat Chikkerur",
      "Stav Ginzburg",
      "Max Chang",
      "Robert Riachi",
      "Chuqiao",
      "Xu",
      "Zalán Borsos",
      "Michael Pliskin",
      "Julia Pawar",
      "Morgane Lustman",
      "Hannah Kirkwood",
      "Ankit Anand",
      "Aditi Chaudhary",
      "Norbert Kalb",
      "Kieran Milan",
      "Sean Augenstein",
      "Anna Goldie",
      "Laurel Prince",
      "Karthik Raman",
      "Yanhua Sun",
      "Vivian Xia",
      "Aaron Cohen",
      "Zhouyuan Huo",
      "Josh Camp",
      "Seher Ellis",
      "Lukas Zilka",
      "David Vilar Torres",
      "Lisa Patel",
      "Sho Arora",
      "Betty Chan",
      "Jonas Adler",
      "Kareem Ayoub",
      "Jacky Liang",
      "Fayaz Jamil",
      "Jiepu Jiang",
      "Simon Baumgartner",
      "Haitian Sun",
      "Yael Karov",
      "Yaroslav Akulov",
      "Hui Zheng",
      "Irene Cai",
      "Claudio Fantacci",
      "James Rubin",
      "Alex Rav Acha",
      "Mengchao Wang",
      "Nina D'Souza",
      "Rohit Sathyanarayana",
      "Shengyang Dai",
      "Simon Rowe",
      "Andrey Simanovsky",
      "Omer Goldman",
      "Yuheng Kuang",
      "Xiaoyue Pan",
      "Andrew Rosenberg",
      "Tania Rojas-Esponda",
      "Praneet Dutta",
      "Amy Zeng",
      "Irina Jurenka",
      "Greg Farquhar",
      "Yamini Bansal",
      "Shariq Iqbal",
      "Becca Roelofs",
      "Ga-Young Joung",
      "Parker Beak",
      "Changwan Ryu",
      "Ryan Poplin",
      "Yan Wu",
      "Jean-Baptiste Alayrac",
      "Senaka Buthpitiya",
      "Olaf Ronneberger",
      "Caleb Habtegebriel",
      "Wei Li",
      "Paul Cavallaro",
      "Aurora Wei",
      "Guy Bensky",
      "Timo Denk",
      "Harish Ganapathy",
      "Jeff Stanway",
      "Pratik Joshi",
      "Francesco Bertolini",
      "Jessica Lo",
      "Olivia Ma",
      "Zachary Charles",
      "Geta Sampemane",
      "Himanshu Sahni",
      "Xu Chen",
      "Harry Askham",
      "David Gaddy",
      "Peter Young",
      "Jiewen Tan",
      "Matan Eyal",
      "Arthur Bražinskas",
      "Li Zhong",
      "Zhichun Wu",
      "Mark Epstein",
      "Kai Bailey",
      "Andrew Hard",
      "Kamyu Lee",
      "Sasha Goldshtein",
      "Alex Ruiz",
      "Mohammed Badawi",
      "Matthias Lochbrunner",
      "JK Kearns",
      "Ashley Brown",
      "Fabio Pardo",
      "Theophane Weber",
      "Haichuan Yang",
      "Pan-Pan Jiang",
      "Berkin Akin",
      "Zhao Fu",
      "Marcus Wainwright",
      "Chi Zou",
      "Meenu Gaba",
      "Pierre-Antoine Manzagol",
      "Wendy Kan",
      "Yang Song",
      "Karina Zainullina",
      "Rui Lin",
      "Jeongwoo Ko",
      "Salil Deshmukh",
      "Apoorv Jindal",
      "James Svensson",
      "Divya Tyam",
      "Heri Zhao",
      "Christine Kaeser-Chen",
      "Scott Baird",
      "Pooya Moradi",
      "Jamie Hall",
      "Qiuchen Guo",
      "Vincent Tsang",
      "Bowen Liang",
      "Fernando Pereira",
      "Suhas Ganesh",
      "Ivan Korotkov",
      "Jakub Adamek",
      "Sridhar Thiagarajan",
      "Vinh Tran",
      "Charles Chen",
      "Chris Tar",
      "Sanil Jain",
      "Ishita Dasgupta",
      "Taylan Bilal",
      "David Reitter",
      "Kai Zhao",
      "Giulia Vezzani",
      "Yasmin Gehman",
      "Pulkit Mehta",
      "Lauren Beltrone",
      "Xerxes Dotiwalla",
      "Sergio Guadarrama",
      "Zaheer Abbas",
      "Stefani Karp",
      "Petko Georgiev",
      "Chun-Sung Ferng",
      "Marc Brockschmidt",
      "Liqian Peng",
      "Christoph Hirnschall",
      "Vikas Verma",
      "Yingying Bi",
      "Ying Xiao",
      "Avigail Dabush",
      "Kelvin Xu",
      "Phil Wallis",
      "Randall Parker",
      "Qifei Wang",
      "Yang Xu",
      "Ilkin Safarli",
      "Dinesh Tewari",
      "Yin Zhang",
      "Seungyeon Kim",
      "Andrea Gesmundo",
      "Mackenzie Thomas",
      "Sergey Levi",
      "Ahmed Chowdhury",
      "Kanishka Rao",
      "Peter Garst",
      "Sam Conway-Rahman",
      "Helen Ran",
      "Kay McKinney",
      "Zhisheng Xiao",
      "Wenhao Yu",
      "Rohan Agrawal",
      "Axel Stjerngren",
      "Catalin Ionescu",
      "Jingjing Chen",
      "Vivek Sharma",
      "Justin Chiu",
      "Fei Liu",
      "Ken Franko",
      "Clayton Sanford",
      "Xingyu Cai",
      "Paul Michel",
      "Sanjay Ganapathy",
      "Jane Labanowski",
      "Zachary Garrett",
      "Ben Vargas",
      "Sean Sun",
      "Bryan Gale",
      "Thomas Buschmann",
      "Guillaume Desjardins",
      "Nimesh Ghelani",
      "Palak Jain",
      "Mudit Verma",
      "Chulayuth Asawaroengchai",
      "Julian Eisenschlos",
      "Jitendra Harlalka",
      "Hideto Kazawa",
      "Don Metzler",
      "Joshua Howland",
      "Ying Jian",
      "Jake Ades",
      "Viral Shah",
      "Tynan Gangwani",
      "Seungji Lee",
      "Roman Ring",
      "Steven M. Hernandez",
      "Dean Reich",
      "Amer Sinha",
      "Ashutosh Sathe",
      "Joe Kovac",
      "Ashleah Gill",
      "Ajay Kannan",
      "Andrea D'olimpio",
      "Martin Sevenich",
      "Jay Whang",
      "Been Kim",
      "Khe Chai Sim",
      "Jilin Chen",
      "Jiageng Zhang",
      "Shuba Lall",
      "Yossi Matias",
      "Bill Jia",
      "Abe Friesen",
      "Sara Nasso",
      "Ashish Thapliyal",
      "Bryan Perozzi",
      "Ting Yu",
      "Anna Shekhawat",
      "Safeen Huda",
      "Peter Grabowski",
      "Eric Wang",
      "Ashwin Sreevatsa",
      "Hilal Dib",
      "Mehadi Hassen",
      "Parker Schuh",
      "Vedrana Milutinovic",
      "Chris Welty",
      "Michael Quinn",
      "Ali Shah",
      "Bangju Wang",
      "Gabe Barth-Maron",
      "Justin Frye",
      "Natalie Axelsson",
      "Tao Zhu",
      "Yukun Ma",
      "Irene Giannoumis",
      "Hanie Sedghi",
      "Chang Ye",
      "Yi Luan",
      "Kevin Aydin",
      "Bilva Chandra",
      "Vivek Sampathkumar",
      "Ronny Huang",
      "Victor Lavrenko",
      "Ahmed Eleryan",
      "Zhi Hong",
      "Steven Hansen",
      "Sara Mc Carthy",
      "Bidisha Samanta",
      "Domagoj Ćevid",
      "Xin Wang",
      "Fangtao Li",
      "Michael Voznesensky",
      "Matt Hoffman",
      "Andreas Terzis",
      "Vikash Sehwag",
      "Gil Fidel",
      "Luheng He",
      "Mu Cai",
      "Yanzhang He",
      "Alex Feng",
      "Martin Nikoltchev",
      "Samrat Phatale",
      "Jason Chase",
      "Rory Lawton",
      "Ming Zhang",
      "Tom Ouyang",
      "Manuel Tragut",
      "Mehdi Hafezi Manshadi",
      "Arjun Narayanan",
      "Jiaming Shen",
      "Xu Gao",
      "Tolga Bolukbasi",
      "Nick Roy",
      "Xin Li",
      "Daniel Golovin",
      "Liviu Panait",
      "Zhen Qin",
      "Guangxing Han",
      "Thomas Anthony",
      "Sneha Kudugunta",
      "Viorica Patraucean",
      "Aniket Ray",
      "Xinyun Chen",
      "Xiaochen Yang",
      "Tanuj Bhatia",
      "Pranav Talluri",
      "Alex Morris",
      "Andrija Ražnatović",
      "Bethanie Brownfield",
      "James An",
      "Sheng Peng",
      "Patrick Kane",
      "Ce Zheng",
      "Nico Duduta",
      "Joshua Kessinger",
      "James Noraky",
      "Siqi Liu",
      "Keran Rong",
      "Petar Veličković",
      "Keith Rush",
      "Alex Goldin",
      "Fanny Wei",
      "Shiva Mohan Reddy Garlapati",
      "Caroline Pantofaru",
      "Okwan Kwon",
      "Jianmo Ni",
      "Eric Noland",
      "Julia Di Trapani",
      "Françoise Beaufays",
      "Abhijit Guha Roy",
      "Yinlam Chow",
      "Aybuke Turker",
      "Geoffrey Cideron",
      "Lantao Mei",
      "Jon Clark",
      "Qingyun Dou",
      "Matko Bošnjak",
      "Ralph Leith",
      "Yuqing Du",
      "Amir Yazdanbakhsh",
      "Milad Nasr",
      "Chester Kwak",
      "Suraj Satishkumar Sheth",
      "Alex Kaskasoli",
      "Ankesh Anand",
      "Balaji Lakshminarayanan",
      "Sammy Jerome",
      "David Bieber",
      "Chun-Te Chu",
      "Alexandre Senges",
      "Tianxiao Shen",
      "Mukund Sridhar",
      "Ndaba Ndebele",
      "Benjamin Beyret",
      "Shakir Mohamed",
      "Mia Chen",
      "Markus Freitag",
      "Jiaxian Guo",
      "Luyang Liu",
      "Paul Roit",
      "Heng Chen",
      "Shen Yan",
      "Tom Stone",
      "JD Co-Reyes",
      "Jeremy Cole",
      "Salvatore Scellato",
      "Shekoofeh Azizi",
      "Hadi Hashemi",
      "Alicia Jin",
      "Anand Iyer",
      "Marcella Valentine",
      "András György",
      "Arun Ahuja",
      "Daniel Hernandez Diaz",
      "Chen-Yu Lee",
      "Nathan Clement",
      "Weize Kong",
      "Drew Garmon",
      "Ishaan Watts",
      "Kush Bhatia",
      "Khyatti Gupta",
      "Matt Miecnikowski",
      "Hugo Vallet",
      "Ankur Taly",
      "Edward Loper",
      "Saket Joshi",
      "James Atwood",
      "Jo Chick",
      "Mark Collier",
      "Fotis Iliopoulos",
      "Ryan Trostle",
      "Beliz Gunel",
      "Ramiro Leal-Cavazos",
      "Arnar Mar Hrafnkelsson",
      "Michael Guzman",
      "Xiaoen Ju",
      "Andy Forbes",
      "Jesse Emond",
      "Kushal Chauhan",
      "Ben Caine",
      "Li Xiao",
      "Wenjun Zeng",
      "Alexandre Moufarek",
      "Daniel Murphy",
      "Maya Meng",
      "Nitish Gupta",
      "Felix Riedel",
      "Anil Das",
      "Elijah Lawal",
      "Shashi Narayan",
      "Tiberiu Sosea",
      "James Swirhun",
      "Linda Friso",
      "Behnam Neyshabur",
      "Jing Lu",
      "Sertan Girgin",
      "Michael Wunder",
      "Edouard Yvinec",
      "Aroonalok Pyne",
      "Victor Carbune",
      "Shruti Rijhwani",
      "Yang Guo",
      "Tulsee Doshi",
      "Anton Briukhov",
      "Max Bain",
      "Ayal Hitron",
      "Xuanhui Wang",
      "Ashish Gupta",
      "Ke Chen",
      "Cosmo Du",
      "Weiyang Zhang",
      "Dhruv Shah",
      "Arjun Akula",
      "Max Dylla",
      "Ashyana Kachra",
      "Weicheng Kuo",
      "Tingting Zou",
      "Lily Wang",
      "Luyao Xu",
      "Jifan Zhu",
      "Justin Snyder",
      "Sachit Menon",
      "Orhan Firat",
      "Igor Mordatch",
      "Yuan Yuan",
      "Natalia Ponomareva",
      "Rory Blevins",
      "Lawrence Moore",
      "Weijun Wang",
      "Phil Chen",
      "Martin Scholz",
      "Artur Dwornik",
      "Jason Lin",
      "Sicheng Li",
      "Diego Antognini",
      "Te I",
      "Xiaodan Song",
      "Matt Miller",
      "Uday Kalra",
      "Adam Raveret",
      "Oscar Akerlund",
      "Felix Wu",
      "Andrew Nystrom",
      "Namrata Godbole",
      "Tianqi Liu",
      "Hannah DeBalsi",
      "Jewel Zhao",
      "Buhuang Liu",
      "Avi Caciularu",
      "Lauren Lax",
      "Urvashi Khandelwal",
      "Victoria Langston",
      "Eric Bailey",
      "Silvio Lattanzi",
      "Yufei Wang",
      "Neel Kovelamudi",
      "Sneha Mondal",
      "Guru Guruganesh",
      "Nan Hua",
      "Ofir Roval",
      "Paweł Wesołowski",
      "Rishikesh Ingale",
      "Jonathan Halcrow",
      "Tim Sohn",
      "Christof Angermueller",
      "Bahram Raad",
      "Eli Stickgold",
      "Eva Lu",
      "Alec Kosik",
      "Jing Xie",
      "Timothy Lillicrap",
      "Austin Huang",
      "Lydia Lihui Zhang",
      "Dominik Paulus",
      "Clement Farabet",
      "Alex Wertheim",
      "Bing Wang",
      "Rishabh Joshi",
      "Chu-ling Ko",
      "Yonghui Wu",
      "Shubham Agrawal",
      "Lily Lin",
      "XiangHai Sheng",
      "Peter Sung",
      "Tyler Breland-King",
      "Christina Butterfield",
      "Swapnil Gawde",
      "Sumeet Singh",
      "Qiao Zhang",
      "Raj Apte",
      "Shilpa Shetty",
      "Adrian Hutter",
      "Tao Li",
      "Elizabeth Salesky",
      "Federico Lebron",
      "Jonni Kanerva",
      "Michela Paganini",
      "Arthur Nguyen",
      "Rohith Vallu",
      "Jan-Thorsten Peter",
      "Sarmishta Velury",
      "David Kao",
      "Jay Hoover",
      "Anna Bortsova",
      "Colton Bishop",
      "Shoshana Jakobovits",
      "Alessandro Agostini",
      "Alekh Agarwal",
      "Chang Liu",
      "Charles Kwong",
      "Sasan Tavakkol",
      "Ioana Bica",
      "Alex Greve",
      "Anirudh GP",
      "Jake Marcus",
      "Le Hou",
      "Tom Duerig",
      "Rivka Moroshko",
      "Dave Lacey",
      "Andy Davis",
      "Julien Amelot",
      "Guohui Wang",
      "Frank Kim",
      "Theofilos Strinopoulos",
      "Hui Wan",
      "Charline Le Lan",
      "Shankar Krishnan",
      "Haotian Tang",
      "Peter Humphreys",
      "Junwen Bai",
      "Idan Heimlich Shtacher",
      "Diego Machado",
      "Chenxi Pang",
      "Ken Burke",
      "Dangyi Liu",
      "Renga Aravamudhan",
      "Yue Song",
      "Ed Hirst",
      "Abhimanyu Singh",
      "Brendan Jou",
      "Liang Bai",
      "Francesco Piccinno",
      "Chuyuan Kelly Fu",
      "Robin Alazard",
      "Barak Meiri",
      "Daniel Winter",
      "Charlie Chen",
      "Mingda Zhang",
      "Jens Heitkaemper",
      "John Lambert",
      "Jinhyuk Lee",
      "Alexander Frömmgen",
      "Sergey Rogulenko",
      "Pranav Nair",
      "Paul Niemczyk",
      "Anton Bulyenov",
      "Bibo Xu",
      "Hadar Shemtov",
      "Morteza Zadimoghaddam",
      "Serge Toropov",
      "Mateo Wirth",
      "Hanjun Dai",
      "Sreenivas Gollapudi",
      "Daniel Zheng",
      "Alex Kurakin",
      "Chansoo Lee",
      "Kalesha Bullard",
      "Nicolas Serrano",
      "Ivana Balazevic",
      "Yang Li",
      "Johan Schalkwyk",
      "Mark Murphy",
      "Mingyang Zhang",
      "Kevin Sequeira",
      "Romina Datta",
      "Nishant Agrawal",
      "Charles Sutton",
      "Nithya Attaluri",
      "Mencher Chiang",
      "Wael Farhan",
      "Gregory Thornton",
      "Kate Lin",
      "Travis Choma",
      "Hung Nguyen",
      "Kingshuk Dasgupta",
      "Dirk Robinson",
      "Iulia Comşa",
      "Michael Riley",
      "Arjun Pillai",
      "Basil Mustafa",
      "Ben Golan",
      "Amir Zandieh",
      "Jean-Baptiste Lespiau",
      "Billy Porter",
      "David Ross",
      "Sujeevan Rajayogam",
      "Mohit Agarwal",
      "Subhashini Venugopalan",
      "Bobak Shahriari",
      "Qiqi Yan",
      "Hao Xu",
      "Taylor Tobin",
      "Pavel Dubov",
      "Hongzhi Shi",
      "Adrià Recasens",
      "Anton Kovsharov",
      "Sebastian Borgeaud",
      "Lucio Dery",
      "Shanthal Vasanth",
      "Elena Gribovskaya",
      "Linhai Qiu",
      "Mahdis Mahdieh",
      "Wojtek Skut",
      "Elizabeth Nielsen",
      "CJ Zheng",
      "Adams Yu",
      "Carrie Grimes Bostock",
      "Shaleen Gupta",
      "Aaron Archer",
      "Chris Rawles",
      "Elinor Davies",
      "Alexey Svyatkovskiy",
      "Tomy Tsai",
      "Yoni Halpern",
      "Christian Reisswig",
      "Bartek Wydrowski",
      "Bo Chang",
      "Joan Puigcerver",
      "Mor Hazan Taege",
      "Jian Li",
      "Eva Schnider",
      "Xinjian Li",
      "Dragos Dena",
      "Yunhan Xu",
      "Umesh Telang",
      "Tianze Shi",
      "Heiga Zen",
      "Kyle Kastner",
      "Yeongil Ko",
      "Neesha Subramaniam",
      "Aviral Kumar",
      "Pete Blois",
      "Zhuyun Dai",
      "John Wieting",
      "Yifeng Lu",
      "Yoel Zeldes",
      "Tian Xie",
      "Anja Hauth",
      "Alexandru Ţifrea",
      "Yuqi Li",
      "Sam El-Husseini",
      "Dan Abolafia",
      "Howard Zhou",
      "Wen Ding",
      "Sahra Ghalebikesabi",
      "Carlos Guía",
      "Andrii Maksai",
      "Ágoston Weisz",
      "Sercan Arik",
      "Nick Sukhanov",
      "Aga Świetlik",
      "Xuhui Jia",
      "Luo Yu",
      "Weiyue Wang",
      "Mark Brand",
      "Dawn Bloxwich",
      "Sean Kirmani",
      "Zhe Chen",
      "Alec Go",
      "Pablo Sprechmann",
      "Nithish Kannen",
      "Alen Carin",
      "Paramjit Sandhu",
      "Isabel Edkins",
      "Leslie Nooteboom",
      "Jai Gupta",
      "Loren Maggiore",
      "Javad Azizi",
      "Yael Pritch",
      "Pengcheng Yin",
      "Mansi Gupta",
      "Danny Tarlow",
      "Duncan Smith",
      "Desi Ivanov",
      "Mohammad Babaeizadeh",
      "Ankita Goel",
      "Satish Kambala",
      "Grace Chu",
      "Matej Kastelic",
      "Michelle Liu",
      "Hagen Soltau",
      "Austin Stone",
      "Shivani Agrawal",
      "Min Kim",
      "Kedar Soparkar",
      "Srinivas Tadepalli",
      "Oskar Bunyan",
      "Rachel Soh",
      "Arvind Kannan",
      "DY Kim",
      "Blake JianHang Chen",
      "Afief Halumi",
      "Sudeshna Roy",
      "Yulong Wang",
      "Olcan Sercinoglu",
      "Gena Gibson",
      "Sijal Bhatnagar",
      "Motoki Sano",
      "Daniel von Dincklage",
      "Qingchun Ren",
      "Blagoj Mitrevski",
      "Mirek Olšák",
      "Jennifer She",
      "Carl Doersch",
      "Jilei",
      "Wang",
      "Bingyuan Liu",
      "Qijun Tan",
      "Tamar Yakar",
      "Tris Warkentin",
      "Alex Ramirez",
      "Carl Lebsack",
      "Josh Dillon",
      "Rajiv Mathews",
      "Tom Cobley",
      "Zelin Wu",
      "Zhuoyuan Chen",
      "Jon Simon",
      "Swaroop Nath",
      "Tara Sainath",
      "Alexei Bendebury",
      "Ryan Julian",
      "Bharath Mankalale",
      "Daria Ćurko",
      "Paulo Zacchello",
      "Adam R. Brown",
      "Kiranbir Sodhia",
      "Heidi Howard",
      "Sergi Caelles",
      "Abhinav Gupta",
      "Gareth Evans",
      "Anna Bulanova",
      "Lesley Katzen",
      "Roman Goldenberg",
      "Anton Tsitsulin",
      "Joe Stanton",
      "Benoit Schillings",
      "Vitaly Kovalev",
      "Corey Fry",
      "Rushin Shah",
      "Kuo Lin",
      "Shyam Upadhyay",
      "Cheng Li",
      "Soroush Radpour",
      "Marcello Maggioni",
      "Jing Xiong",
      "Lukas Haas",
      "Jenny Brennan",
      "Aishwarya Kamath",
      "Nikolay Savinov",
      "Arsha Nagrani",
      "Trevor Yacovone",
      "Ryan Kappedal",
      "Kostas Andriopoulos",
      "Li Lao",
      "YaGuang Li",
      "Grigory Rozhdestvenskiy",
      "Kazuma Hashimoto",
      "Andrew Audibert",
      "Sophia Austin",
      "Daniel Rodriguez",
      "Anian Ruoss",
      "Garrett Honke",
      "Deep Karkhanis",
      "Xi Xiong",
      "Qing Wei",
      "James Huang",
      "Zhaoqi Leng",
      "Vittal Premachandran",
      "Stan Bileschi",
      "Georgios Evangelopoulos",
      "Thomas Mensink",
      "Jay Pavagadhi",
      "Denis Teplyashin",
      "Paul Chang",
      "Linting Xue",
      "Garrett Tanzer",
      "Sally Goldman",
      "Kaushal Patel",
      "Shixin Li",
      "Jeremy Wiesner",
      "Ivy Zheng",
      "Ian Stewart-Binks",
      "Jie Han",
      "Zhi Li",
      "Liangchen Luo",
      "Karel Lenc",
      "Mario Lučić",
      "Fuzhao Xue",
      "Ryan Mullins",
      "Alexey Guseynov",
      "Chung-Ching Chang",
      "Isaac Galatzer-Levy",
      "Adam Zhang",
      "Garrett Bingham",
      "Grace Hu",
      "Ale Hartman",
      "Yue Ma",
      "Jordan Griffith",
      "Alex Irpan",
      "Carey Radebaugh",
      "Summer Yue",
      "Lijie Fan",
      "Victor Ungureanu",
      "Christina Sorokin",
      "Hannah Teufel",
      "Peiran Li",
      "Rohan Anil",
      "Dimitris Paparas",
      "Todd Wang",
      "Chu-Cheng Lin",
      "Hui Peng",
      "Megan Shum",
      "Goran Petrovic",
      "Demetra Brady",
      "Richard Nguyen",
      "Klaus Macherey",
      "Zhihao Li",
      "Harman Singh",
      "Madhavi Yenugula",
      "Mariko Iinuma",
      "Xinyi Chen",
      "Kavya Kopparapu",
      "Alexey Stern",
      "Shachi Dave",
      "Chandu Thekkath",
      "Florence Perot",
      "Anurag Kumar",
      "Fangda Li",
      "Yang Xiao",
      "Matthew Bilotti",
      "Mohammad Hossein Bateni",
      "Isaac Noble",
      "Lisa Lee",
      "Amelio Vázquez-Reina",
      "Julian Salazar",
      "Xiaomeng Yang",
      "Boyu Wang",
      "Ela Gruzewska",
      "Anand Rao",
      "Sindhu Raghuram",
      "Zheng Xu",
      "Eyal Ben-David",
      "Jieru Mei",
      "Sid Dalmia",
      "Zhaoyi Zhang",
      "Yuchen Liu",
      "Gagan Bansal",
      "Helena Pankov",
      "Steven Schwarcz",
      "Andrea Burns",
      "Christine Chan",
      "Sumit Sanghai",
      "Ricky Liang",
      "Ethan Liang",
      "Antoine He",
      "Amy Stuart",
      "Arun Narayanan",
      "Yukun Zhu",
      "Christian Frank",
      "Bahar Fatemi",
      "Amit Sabne",
      "Oran Lang",
      "Indro Bhattacharya",
      "Shane Settle",
      "Maria Wang",
      "Brendan McMahan",
      "Andrea Tacchetti",
      "Livio Baldini Soares",
      "Majid Hadian",
      "Serkan Cabi",
      "Timothy Chung",
      "Nikita Putikhin",
      "Gang Li",
      "Jeremy Chen",
      "Austin Tarango",
      "Henryk Michalewski",
      "Mehran Kazemi",
      "Hussain Masoom",
      "Hila Sheftel",
      "Rakesh Shivanna",
      "Archita Vadali",
      "Ramona Comanescu",
      "Doug Reid",
      "Joss Moore",
      "Arvind Neelakantan",
      "Michaël Sander",
      "Jonathan Herzig",
      "Aviv Rosenberg",
      "Mostafa Dehghani",
      "JD Choi",
      "Michael Fink",
      "Reid Hayes",
      "Eric Ge",
      "Shitao Weng",
      "Chia-Hua Ho",
      "John Karro",
      "Kalpesh Krishna",
      "Lam Nguyen Thiet",
      "Amy Skerry-Ryan",
      "Daniel Eppens",
      "Marco Andreetto",
      "Navin Sarma",
      "Silvano Bonacina",
      "Burcu Karagol Ayan",
      "Megha Nawhal",
      "Zhihao Shan",
      "Mike Dusenberry",
      "Shantanu Thakoor",
      "Sagar Gubbi",
      "Duc Dung Nguyen",
      "Reut Tsarfaty",
      "Samuel Albanie",
      "Jovana Mitrović",
      "Meet Gandhi",
      "Bo-Juen Chen",
      "Alessandro Epasto",
      "Georgi Stephanov",
      "Ye Jin",
      "Samuel Gehman",
      "Aida Amini",
      "Jack Weber",
      "Feryal Behbahani",
      "Shawn Xu",
      "Miltos Allamanis",
      "Xi Chen",
      "Myle Ott",
      "Claire Sha",
      "Michal Jastrzebski",
      "Hang Qi",
      "David Greene",
      "Xinyi Wu",
      "Abodunrinwa Toki",
      "Daniel Vlasic",
      "Jane Shapiro",
      "Ragha Kotikalapudi",
      "Zhe Shen",
      "Takaaki Saeki",
      "Sirui Xie",
      "Albin Cassirer",
      "Shikhar Bharadwaj",
      "Tatsuya Kiyono",
      "Srinadh Bhojanapalli",
      "Elan Rosenfeld",
      "Sam Ritter",
      "Jieming Mao",
      "João Gabriel Oliveira",
      "Zoltan Egyed",
      "Bernd Bandemer",
      "Emilio Parisotto",
      "Keisuke Kinoshita",
      "Juliette Pluto",
      "Petros Maniatis",
      "Steve Li",
      "Yaohui Guo",
      "Golnaz Ghiasi",
      "Jean Tarbouriech",
      "Srimon Chatterjee",
      "Julie Jin",
      "Katrina",
      "Xu",
      "Jennimaria Palomaki",
      "Séb Arnold",
      "Madhavi Sewak",
      "Federico Piccinini",
      "Mohit Sharma",
      "Ben Albrecht",
      "Sean Purser-haskell",
      "Ashwin Vaswani",
      "Chongyan Chen",
      "Matheus Wisniewski",
      "Qin Cao",
      "John Aslanides",
      "Nguyet Minh Phu",
      "Maximilian Sieb",
      "Lauren Agubuzu",
      "Anne Zheng",
      "Daniel Sohn",
      "Marco Selvi",
      "Anders Andreassen",
      "Krishan Subudhi",
      "Prem Eruvbetine",
      "Oliver Woodman",
      "Tomas Mery",
      "Sebastian Krause",
      "Xiaoqi Ren",
      "Xiao Ma",
      "Jincheng Luo",
      "Dawn Chen",
      "Wei Fan",
      "Henry Griffiths",
      "Christian Schuler",
      "Alice Li",
      "Shujian Zhang",
      "Jean-Michel Sarr",
      "Shixin Luo",
      "Riccardo Patana",
      "Matthew Watson",
      "Dani Naboulsi",
      "Michael Collins",
      "Sailesh Sidhwani",
      "Emiel Hoogeboom",
      "Sharon Silver",
      "Emily Caveness",
      "Xiaokai Zhao",
      "Mikel Rodriguez",
      "Maxine Deines",
      "Libin Bai",
      "Patrick Griffin",
      "Marco Tagliasacchi",
      "Emily Xue",
      "Spandana Raj Babbula",
      "Bo Pang",
      "Nan Ding",
      "Gloria Shen",
      "Elijah Peake",
      "Remi Crocker",
      "Shubha Srinivas Raghvendra",
      "Danny Swisher",
      "Woohyun Han",
      "Richa Singh",
      "Ling Wu",
      "Vladimir Pchelin",
      "Tsendsuren Munkhdalai",
      "Dana Alon",
      "Geoff Bacon",
      "Efren Robles",
      "Jannis Bulian",
      "Melvin Johnson",
      "George Powell",
      "Felipe Tiengo Ferreira",
      "Yaoyiran Li",
      "Frederik Benzing",
      "Mihajlo Velimirović",
      "Hubert Soyer",
      "William Kong",
      "Tony",
      "Nguyên",
      "Zhen Yang",
      "Jeremiah Liu",
      "Joost van Amersfoort",
      "Daniel Gillick",
      "Baochen Sun",
      "Nathalie Rauschmayr",
      "Katie Zhang",
      "Serena Zhan",
      "Tao Zhou",
      "Alexey Frolov",
      "Chengrun Yang",
      "Denis Vnukov",
      "Louis Rouillard",
      "Hongji Li",
      "Amol Mandhane",
      "Nova Fallen",
      "Rajesh Venkataraman",
      "Clara Huiyi Hu",
      "Jennifer Brennan",
      "Jenny Lee",
      "Jerry Chang",
      "Martin Sundermeyer",
      "Zhufeng Pan",
      "Rosemary Ke",
      "Simon Tong",
      "Alex Fabrikant",
      "William Bono",
      "Jindong Gu",
      "Ryan Foley",
      "Yiran Mao",
      "Manolis Delakis",
      "Dhruva Bhaswar",
      "Roy Frostig",
      "Nick Li",
      "Avital Zipori",
      "Cath Hope",
      "Olga Kozlova",
      "Swaroop Mishra",
      "Josip Djolonga",
      "Craig Schiff",
      "Majd Al Merey",
      "Eleftheria Briakou",
      "Peter Morgan",
      "Andy Wan",
      "Avinatan Hassidim",
      "RJ Skerry-Ryan",
      "Kuntal Sengupta",
      "Mary Jasarevic",
      "Praveen Kallakuri",
      "Paige Kunkle",
      "Hannah Brennan",
      "Tom Lieber",
      "Hassan Mansoor",
      "Julian Walker",
      "Bing Zhang",
      "Annie Xie",
      "Goran Žužić",
      "Adaeze Chukwuka",
      "Alex Druinsky",
      "Donghyun Cho",
      "Rui Yao",
      "Ferjad Naeem",
      "Shiraz Butt",
      "Eunyoung Kim",
      "Zhipeng Jia",
      "Mandy Jordan",
      "Adam Lelkes",
      "Mark Kurzeja",
      "Sophie Wang",
      "James Zhao",
      "Andrew Over",
      "Abhishek Chakladar",
      "Marcel Prasetya",
      "Neha Jha",
      "Sriram Ganapathy",
      "Yale Cong",
      "Prakash Shroff",
      "Carl Saroufim",
      "Sobhan Miryoosefi",
      "Mohamed Hammad",
      "Tajwar Nasir",
      "Weijuan Xi",
      "Yang Gao",
      "Young Maeng",
      "Ben Hora",
      "Chin-Yi Cheng",
      "Parisa Haghani",
      "Yoad Lewenberg",
      "Caden Lu",
      "Martin Matysiak",
      "Naina Raisinghani",
      "Huiyu Wang",
      "Lexi Baugher",
      "Rahul Sukthankar",
      "Minh Giang",
      "John Schultz",
      "Noah Fiedel",
      "Minmin Chen",
      "Cheng-Chun Lee",
      "Tapomay Dey",
      "Hao Zheng",
      "Shachi Paul",
      "Celine Smith",
      "Andy Ly",
      "Yicheng Wang",
      "Rishabh Bansal",
      "Bartek Perz",
      "Susanna Ricco",
      "Stasha Blank",
      "Vaishakh Keshava",
      "Deepak Sharma",
      "Marvin Chow",
      "Kunal Lad",
      "Komal Jalan",
      "Simon Osindero",
      "Craig Swanson",
      "Jacob Scott",
      "Anastasija Ilić",
      "Xiaowei Li",
      "Siddhartha Reddy Jonnalagadda",
      "Afzal Shama Soudagar",
      "Yan Xiong",
      "Bat-Orgil Batsaikhan",
      "Daniel Jarrett",
      "Naveen Kumar",
      "Maulik Shah",
      "Matt Lawlor",
      "Austin Waters",
      "Mark Graham",
      "Rhys May",
      "Sabela Ramos",
      "Sandra Lefdal",
      "Zeynep Cankara",
      "Nacho Cano",
      "Brendan O'Donoghue",
      "Jed Borovik",
      "Frederick Liu",
      "Jordan Grimstad",
      "Mahmoud Alnahlawi",
      "Katerina Tsihlas",
      "Tom Hudson",
      "Nikolai Grigorev",
      "Yiling Jia",
      "Terry Huang",
      "Tobenna Peter Igwe",
      "Sergei Lebedev",
      "Xiaodan Tang",
      "Igor Krivokon",
      "Frankie Garcia",
      "Melissa Tan",
      "Eric Jia",
      "Peter Stys",
      "Shikhar Vashishth",
      "Yu Liang",
      "Balaji Venkatraman",
      "Chenjie Gu",
      "Anastasios Kementsietsidis",
      "Chen Zhu",
      "Junehyuk Jung",
      "Yunfei Bai",
      "Mohammad Javad Hosseini",
      "Faruk Ahmed",
      "Aditya Gupta",
      "Xin Yuan",
      "Shereen Ashraf",
      "Shitij Nigam",
      "Gautam Vasudevan",
      "Pranjal Awasthi",
      "Adi Mayrav Gilady",
      "Zelda Mariet",
      "Ramy Eskander",
      "Haiguang Li",
      "Hexiang Hu",
      "Guillermo Garrido",
      "Philippe Schlattner",
      "George Zhang",
      "Rohun Saxena",
      "Petar Dević",
      "Kritika Muralidharan",
      "Ashwin Murthy",
      "Yiqian Zhou",
      "Min Choi",
      "Arissa Wongpanich",
      "Zhengdong Wang",
      "Premal Shah",
      "Yuntao Xu",
      "Yiling Huang",
      "Stephen Spencer",
      "Alice Chen",
      "James Cohan",
      "Junjie Wang",
      "Jonathan Tompson",
      "Junru Wu",
      "Ruba Haroun",
      "Haiqiong Li",
      "Blanca Huergo",
      "Fan Yang",
      "Tongxin Yin",
      "James Wendt",
      "Michael Bendersky",
      "Rahma Chaabouni",
      "Javier Snaider",
      "Johan Ferret",
      "Abhishek Jindal",
      "Tara Thompson",
      "Andrew Xue",
      "Will Bishop",
      "Shubham Milind Phal",
      "Archit Sharma",
      "Yunhsuan Sung",
      "Prabakar Radhakrishnan",
      "Mo Shomrat",
      "Reeve Ingle",
      "Roopali Vij",
      "Justin Gilmer",
      "Mihai Dorin Istin",
      "Sam Sobell",
      "Yang Lu",
      "Emily Nottage",
      "Dorsa Sadigh",
      "Jeremiah Willcock",
      "Tingnan Zhang",
      "Steve Xu",
      "Sasha Brown",
      "Katherine Lee",
      "Gary Wang",
      "Yun Zhu",
      "Yi Tay",
      "Cheolmin Kim",
      "Audrey Gutierrez",
      "Abhanshu Sharma",
      "Yongqin Xian",
      "Sungyong Seo",
      "Claire Cui",
      "Elena Pochernina",
      "Cip Baetu",
      "Krzysztof Jastrzębski",
      "Mimi Ly",
      "Mohamed Elhawaty",
      "Dan Suh",
      "Eren Sezener",
      "Pidong Wang",
      "Nancy Yuen",
      "George Tucker",
      "Jiahao Cai",
      "Zuguang Yang",
      "Cindy Wang",
      "Alex Muzio",
      "Hai Qian",
      "Jae Yoo",
      "Derek Lockhart",
      "Kevin R. McKee",
      "Mandy Guo",
      "Malika Mehrotra",
      "Artur Mendonça",
      "Sanket Vaibhav Mehta",
      "Sherry Ben",
      "Chetan Tekur",
      "Jiaqi Mu",
      "Muye Zhu",
      "Victoria Krakovna",
      "Hongrae Lee",
      "AJ Maschinot",
      "Sébastien Cevey",
      "HyunJeong Choe",
      "Aijun Bai",
      "Hansa Srinivasan",
      "Derek Gasaway",
      "Nick Young",
      "Patrick Siegler",
      "Dan Holtmann-Rice",
      "Vihari Piratla",
      "Kate Baumli",
      "Roey Yogev",
      "Alex Hofer",
      "Hado van Hasselt",
      "Svetlana Grant",
      "Yuri Chervonyi",
      "David Silver",
      "Andrew Hogue",
      "Ayushi Agarwal",
      "Kathie Wang",
      "Preeti Singh",
      "Four Flynn",
      "Josh Lipschultz",
      "Robert David",
      "Lizzetth Bellot",
      "Yao-Yuan Yang",
      "Long Le",
      "Filippo Graziano",
      "Kate Olszewska",
      "Kevin Hui",
      "Akanksha Maurya",
      "Nikos Parotsidis",
      "Weijie Chen",
      "Tayo Oguntebi",
      "Joe Kelley",
      "Anirudh Baddepudi",
      "Johannes Mauerer",
      "Gregory Shaw",
      "Alex Siegman",
      "Lin Yang",
      "Shravya Shetty",
      "Subhrajit Roy",
      "Yunting Song",
      "Wojciech Stokowiec",
      "Ryan Burnell",
      "Omkar Savant",
      "Robert Busa-Fekete",
      "Jin Miao",
      "Samrat Ghosh",
      "Liam MacDermed",
      "Phillip Lippe",
      "Mikhail Dektiarev",
      "Zach Behrman",
      "Fabian Mentzer",
      "Kelvin Nguyen",
      "Meng Wei",
      "Siddharth Verma",
      "Chris Knutsen",
      "Sudeep Dasari",
      "Zhipeng Yan",
      "Petr Mitrichev",
      "Xingyu Wang",
      "Virat Shejwalkar",
      "Jacob Austin",
      "Srinivas Sunkara",
      "Navneet Potti",
      "Yan Virin",
      "Christian Wright",
      "Gaël Liu",
      "Oriana Riva",
      "Etienne Pot",
      "Greg Kochanski",
      "Quoc Le",
      "Gargi Balasubramaniam",
      "Arka Dhar",
      "Yuguo Liao",
      "Adam Bloniarz",
      "Divyansh Shukla",
      "Elizabeth Cole",
      "Jong Lee",
      "Sheng Zhang",
      "Sushant Kafle",
      "Siddharth Vashishtha",
      "Parsa Mahmoudieh",
      "Grace Chen",
      "Raphael Hoffmann",
      "Pranesh Srinivasan",
      "Agustin Dal Lago",
      "Yoav Ben Shalom",
      "Zi Wang",
      "Michael Elabd",
      "Anuj Sharma",
      "Junhyuk Oh",
      "Suraj Kothawade",
      "Maigo Le",
      "Marianne Monteiro",
      "Shentao Yang",
      "Kaiz Alarakyia",
      "Robert Geirhos",
      "Diana Mincu",
      "Håvard Garnes",
      "Hayato Kobayashi",
      "Soroosh Mariooryad",
      "Kacper Krasowiak",
      "Zhixin",
      "Lai",
      "Shibl Mourad",
      "Mingqiu Wang",
      "Fan Bu",
      "Ophir Aharoni",
      "Guanjie Chen",
      "Abhimanyu Goyal",
      "Vadim Zubov",
      "Ankur Bapna",
      "Elahe Dabir",
      "Nisarg Kothari",
      "Kay Lamerigts",
      "Nicola De Cao",
      "Jeremy Shar",
      "Christopher Yew",
      "Nitish Kulkarni",
      "Dre Mahaarachchi",
      "Mandar Joshi",
      "Zhenhai Zhu",
      "Jared Lichtarge",
      "Yichao Zhou",
      "Hannah Muckenhirn",
      "Vittorio Selo",
      "Oriol Vinyals",
      "Peter Chen",
      "Anthony Brohan",
      "Vaibhav Mehta",
      "Sarah Cogan",
      "Ruth Wang",
      "Ty Geri",
      "Wei-Jen Ko",
      "Wei Chen",
      "Fabio Viola",
      "Keshav Shivam",
      "Lisa Wang",
      "Madeleine Clare Elish",
      "Raluca Ada Popa",
      "Sébastien Pereira",
      "Jianqiao Liu",
      "Raphael Koster",
      "Donnie Kim",
      "Gufeng Zhang",
      "Sayna Ebrahimi",
      "Partha Talukdar",
      "Yanyan Zheng",
      "Petra Poklukar",
      "Ales Mikhalap",
      "Dale Johnson",
      "Anitha Vijayakumar",
      "Mark Omernick",
      "Matt Dibb",
      "Ayush Dubey",
      "Qiong Hu",
      "Apurv Suman",
      "Vaibhav Aggarwal",
      "Ilya Kornakov",
      "Fei Xia",
      "Wing Lowe",
      "Alexey Kolganov",
      "Ted Xiao",
      "Vitaly Nikolaev",
      "Steven Hemingray",
      "Bonnie Li",
      "Joana Iljazi",
      "Mikołaj Rybiński",
      "Ballie Sandhu",
      "Peggy Lu",
      "Thang Luong",
      "Rodolphe Jenatton",
      "Vineetha Govindaraj",
      "Hui",
      "Li",
      "Gabriel Dulac-Arnold",
      "Wonpyo Park",
      "Henry Wang",
      "Abhinit Modi",
      "Jean Pouget-Abadie",
      "Kristina Greller",
      "Rahul Gupta",
      "Robert Berry",
      "Prajit Ramachandran",
      "Jinyu Xie",
      "Liam McCafferty",
      "Jianling Wang",
      "Kilol Gupta",
      "Hyeontaek Lim",
      "Blaž Bratanič",
      "Andy Brock",
      "Ilia Akolzin",
      "Jim Sproch",
      "Dan Karliner",
      "Duhyeon Kim",
      "Adrian Goedeckemeyer",
      "Noam Shazeer",
      "Cordelia Schmid",
      "Daniele Calandriello",
      "Parul Bhatia",
      "Krzysztof Choromanski",
      "Ceslee Montgomery",
      "Dheeru Dua",
      "Ana Ramalho",
      "Helen King",
      "Yue Gao",
      "Lynn Nguyen",
      "David Lindner",
      "Divya Pitta",
      "Oleaser Johnson",
      "Khalid Salama",
      "Diego Ardila",
      "Michael Han",
      "Erin Farnese",
      "Seth Odoom",
      "Ziyue Wang",
      "Xiangzhuo Ding",
      "Norman Rink",
      "Ray Smith",
      "Harshal Tushar Lehri",
      "Eden Cohen",
      "Neera Vats",
      "Tong He",
      "Parthasarathy Gopavarapu",
      "Adam Paszke",
      "Miteyan Patel",
      "Wouter Van Gansbeke",
      "Lucia Loher",
      "Luis Castro",
      "Maria Voitovich",
      "Tamara von Glehn",
      "Nelson George",
      "Simon Niklaus",
      "Zach Eaton-Rosen",
      "Nemanja Rakićević",
      "Erik Jue",
      "Sagi Perel",
      "Carrie Zhang",
      "Yuval Bahat",
      "Angéline Pouget",
      "Zhi Xing",
      "Fantine Huot",
      "Ashish Shenoy",
      "Taylor Bos",
      "Vincent Coriou",
      "Bryan Richter",
      "Natasha Noy",
      "Yaqing Wang",
      "Santiago Ontanon",
      "Siyang Qin",
      "Gleb Makarchuk",
      "Demis Hassabis",
      "Zhuowan Li",
      "Mandar Sharma",
      "Kumaran Venkatesan",
      "Iurii Kemaev",
      "Roxanne Daniel",
      "Shiyu Huang",
      "Saloni Shah",
      "Octavio Ponce",
      "Warren",
      "Chen",
      "Manaal Faruqui",
      "Jialin Wu",
      "Slavica Andačić",
      "Szabolcs Payrits",
      "Daniel McDuff",
      "Tom Hume",
      "Yuan Cao",
      "MH Tessler",
      "Qingze Wang",
      "Yinan Wang",
      "Ivor Rendulic",
      "Eirikur Agustsson",
      "Matthew Johnson",
      "Tanya Lando",
      "Andrew Howard",
      "Sri Gayatri Sundara Padmanabhan",
      "Mayank Daswani",
      "Andrea Banino",
      "Michael Kilgore",
      "Jonathan Heek",
      "Ziwei Ji",
      "Alvaro Caceres",
      "Conglong Li",
      "Nora Kassner",
      "Alexey Vlaskin",
      "Zeyu Liu",
      "Alex Grills",
      "Yanhan Hou",
      "Roykrong Sukkerd",
      "Gowoon Cheon",
      "Nishita Shetty",
      "Larisa Markeeva",
      "Piotr Stanczyk",
      "Tejas Iyer",
      "Yuan Gong",
      "Shawn Gao",
      "Keerthana Gopalakrishnan",
      "Tim Blyth",
      "Malcolm Reynolds",
      "Avishkar Bhoopchand",
      "Misha Bilenko",
      "Dero Gharibian",
      "Vicky Zayats",
      "Aleksandra Faust",
      "Abhinav Singh",
      "Min Ma",
      "Hongyang Jiao",
      "Sudheendra Vijayanarasimhan",
      "Lora Aroyo",
      "Vikas Yadav",
      "Sarah Chakera",
      "Ashwin Kakarla",
      "Vilobh Meshram",
      "Karol Gregor",
      "Gabriela Botea",
      "Evan Senter",
      "Dawei Jia",
      "Geza Kovacs",
      "Neha Sharma",
      "Sebastien Baur",
      "Kai Kang",
      "Yifan He",
      "Lin Zhuo",
      "Marija Kostelac",
      "Itay Laish",
      "Songyou Peng",
      "Louis O'Bryan",
      "Daniel Kasenberg",
      "Girish Ramchandra Rao",
      "Edouard Leurent",
      "Biao Zhang",
      "Sage Stevens",
      "Ana Salazar",
      "Ye Zhang",
      "Ivan Lobov",
      "Jake Walker",
      "Allen Porter",
      "Morgan Redshaw",
      "Han Ke",
      "Abhishek Rao",
      "Alex Lee",
      "Hoi Lam",
      "Michael Moffitt",
      "Jaeyoun Kim",
      "Siyuan Qiao",
      "Terry Koo",
      "Robert Dadashi",
      "Xinying Song",
      "Mukund Sundararajan",
      "Peng Xu",
      "Chizu Kawamoto",
      "Yan Zhong",
      "Clara Barbu",
      "Apoorv Reddy",
      "Mauro Verzetti",
      "Leon Li",
      "George Papamakarios",
      "Hanna Klimczak-Plucińska",
      "Mary Cassin",
      "Koray Kavukcuoglu",
      "Rigel Swavely",
      "Alain Vaucher",
      "Jeffrey Zhao",
      "Ross Hemsley",
      "Michael Tschannen",
      "Heming Ge",
      "Gaurav Menghani",
      "Yang Yu",
      "Natalie Ha",
      "Wei He",
      "Xiao Wu",
      "Maggie Song",
      "Rachel Sterneck",
      "Stefan Zinke",
      "Dan A. Calian",
      "Annie Marsden",
      "Alejandro Cruzado Ruiz",
      "Matteo Hessel",
      "Almog Gueta",
      "Benjamin Lee",
      "Brian Farris",
      "Manish Gupta",
      "Yunjie Li",
      "Mohammad Saleh",
      "Vedant Misra",
      "Kefan Xiao",
      "Piermaria Mendolicchio",
      "Gavin Buttimore",
      "Varvara Krayvanova",
      "Nigamaa Nayakanti",
      "Matthew Wiethoff",
      "Yash Pande",
      "Azalia Mirhoseini",
      "Ni Lao",
      "Jasmine Liu",
      "Yiqing Hua",
      "Angie Chen",
      "Yury Malkov",
      "Dmitry Kalashnikov",
      "Shubham Gupta",
      "Kartik Audhkhasi",
      "Yuexiang Zhai",
      "Sudhindra Kopalle",
      "Prateek Jain",
      "Eran Ofek",
      "Clemens Meyer",
      "Khuslen Baatarsukh",
      "Hana Strejček",
      "Jun Qian",
      "James Freedman",
      "Ricardo Figueira",
      "Michal Sokolik",
      "Olivier Bachem",
      "Raymond Lin",
      "Dia Kharrat",
      "Chris Hidey",
      "Pingmei Xu",
      "Dennis Duan",
      "Yin Li",
      "Muge Ersoy",
      "Richard Everett",
      "Kevin Cen",
      "Rebeca Santamaria-Fernandez",
      "Amir Taubenfeld",
      "Ian Mackinnon",
      "Linda Deng",
      "Polina Zablotskaia",
      "Shashank Viswanadha",
      "Shivanker Goel",
      "Damion Yates",
      "Yunxiao Deng",
      "Peter Choy",
      "Mingqing Chen",
      "Abhishek Sinha",
      "Alex Mossin",
      "Yiming Wang",
      "Arthur Szlam",
      "Susan Hao",
      "Paul Kishan Rubenstein",
      "Metin Toksoz-Exley",
      "Miranda Aperghis",
      "Yin Zhong",
      "Junwhan Ahn",
      "Michael Isard",
      "Olivier Lacombe",
      "Florian Luisier",
      "Chrysovalantis Anastasiou",
      "Yogesh Kalley",
      "Utsav Prabhu",
      "Emma Dunleavy",
      "Shaan Bijwadia",
      "Justin Mao-Jones",
      "Kelly Chen",
      "Rama Pasumarthi",
      "Emily Wood",
      "Adil Dostmohamed",
      "Nate Hurley",
      "Jiri Simsa",
      "Alicia Parrish",
      "Mantas Pajarskas",
      "Matt Harvey",
      "Ondrej Skopek",
      "Yony Kochinski",
      "Javier Rey",
      "Verena Rieser",
      "Denny Zhou",
      "Sun Jae Lee",
      "Trilok Acharya",
      "Guowang Li",
      "Joe Jiang",
      "Xiaofan Zhang",
      "Bryant Gipson",
      "Ethan Mahintorabi",
      "Marco Gelmi",
      "Nima Khajehnouri",
      "Angel Yeh",
      "Kayi Lee",
      "Loic Matthey",
      "Leslie Baker",
      "Trang Pham",
      "Han Fu",
      "Alex Pak",
      "Prakhar Gupta",
      "Cristina Vasconcelos",
      "Adam Sadovsky",
      "Brian Walker",
      "Sissie Hsiao",
      "Patrik Zochbauer",
      "Andreea Marzoca",
      "Noam Velan",
      "Junhao Zeng",
      "Gilles Baechler",
      "Danny Driess",
      "Divya Jain",
      "Yanping Huang",
      "Lizzie Tao",
      "John Maggs",
      "Nir Levine",
      "Jon Schneider",
      "Erika Gemzer",
      "Samuel Petit",
      "Shan Han",
      "Zach Fisher",
      "Dustin Zelle",
      "Courtney Biles",
      "Eugene Ie",
      "Asya Fadeeva",
      "Casper Liu",
      "Juliana Vicente Franco",
      "Adrian Collister",
      "Hao Zhang",
      "Renshen Wang",
      "Ruizhe Zhao",
      "Leandro Kieliger",
      "Kurt Shuster",
      "Rui Zhu",
      "Boqing Gong",
      "Lawrence Chan",
      "Ruoxi Sun",
      "Sujoy Basu",
      "Roland Zimmermann",
      "Jamie Hayes",
      "Abhishek Bapna",
      "Jasper Snoek",
      "Weel Yang",
      "Puranjay Datta",
      "Jad Al Abdallah",
      "Kevin Kilgour",
      "Lu Li",
      "SQ Mah",
      "Yennie Jun",
      "Morgane Rivière",
      "Abhijit Karmarkar",
      "Tammo Spalink",
      "Tao Huang",
      "Lucas Gonzalez",
      "Duc-Hieu Tran",
      "Averi Nowak",
      "John Palowitch",
      "Martin Chadwick",
      "Ellie Talius",
      "Harsh Mehta",
      "Thibault Sellam",
      "Philipp Fränken",
      "Massimo Nicosia",
      "Kyle He",
      "Aditya Kini",
      "David Amos",
      "Sugato Basu",
      "Harrison Jobe",
      "Eleni Shaw",
      "Qiantong Xu",
      "Colin Evans",
      "Daisuke Ikeda",
      "Chaochao Yan",
      "Larry Jin",
      "Lun Wang",
      "Sachin Yadav",
      "Ilia Labzovsky",
      "Ramesh Sampath",
      "Ada Ma",
      "Candice Schumann",
      "Aditya Siddhant",
      "Rohin Shah",
      "John Youssef",
      "Rishabh Agarwal",
      "Natalie Dabney",
      "Alessio Tonioni",
      "Moran Ambar",
      "Jing Li",
      "Isabelle Guyon",
      "Benny Li",
      "David Soergel",
      "Boya Fang",
      "Georgi Karadzhov",
      "Cristian Udrescu",
      "Trieu Trinh",
      "Vikas Raunak",
      "Seb Noury",
      "Dee Guo",
      "Sonal Gupta",
      "Mara Finkelstein",
      "Denis Petek",
      "Lihao Liang",
      "Greg Billock",
      "Pei Sun",
      "David Wood",
      "Yiwen Song",
      "Xiaobin Yu",
      "Tatiana Matejovicova",
      "Regev Cohen",
      "Kalyan Andra",
      "David D'Ambrosio",
      "Zhiwei Deng",
      "Vincent Nallatamby",
      "Ebrahim Songhori",
      "Rumen Dangovski",
      "Andrew Lampinen",
      "Pankil Botadra",
      "Adam Hillier",
      "Jiawei Cao",
      "Nagabhushan Baddi",
      "Adhi Kuncoro",
      "Toshihiro Yoshino",
      "Ankit Bhagatwala",
      "Marcáurelio Ranzato",
      "Rylan Schaeffer",
      "Tianlin Liu",
      "Shuai Ye",
      "Obaid Sarvana",
      "John Nham",
      "Chenkai Kuang",
      "Isabel Gao",
      "Jinoo Baek",
      "Shubham Mittal",
      "Ayzaan Wahid",
      "Anita Gergely",
      "Bin Ni",
      "Josh Feldman",
      "Carrie Muir",
      "Pascal Lamblin",
      "Wolfgang Macherey",
      "Ethan Dyer",
      "Logan Kilpatrick",
      "Víctor Campos",
      "Mukul Bhutani",
      "Stanislav Fort",
      "Yanif Ahmad",
      "Aliaksei Severyn",
      "Kleopatra Chatziprimou",
      "Oleksandr Ferludin",
      "Mason Dimarco",
      "Aditya Kusupati",
      "Joe Heyward",
      "Dan Bahir",
      "Kevin Villela",
      "Katie Millican",
      "Dror Marcus",
      "Sanaz Bahargam",
      "Caglar Unlu",
      "Nicholas Roth",
      "Zichuan Wei",
      "Siddharth Gopal",
      "Deepanway Ghoshal",
      "Edward Lee",
      "Sharon Lin",
      "Jennie Lees",
      "Dayeong Lee",
      "Anahita Hosseini",
      "Connie Fan",
      "Seth Neel",
      "Marcus Wu",
      "Yasemin Altun",
      "Honglong Cai",
      "Enrique Piqueras",
      "Josh Woodward",
      "Alessandro Bissacco",
      "Salem Haykal",
      "Mahyar Bordbar",
      "Prasha Sundaram",
      "Sarah Hodkinson",
      "Daniel Toyama",
      "George Polovets",
      "Austin Myers",
      "Anu Sinha",
      "Tomer Levinboim",
      "Kashyap Krishnakumar",
      "Rachita Chhaparia",
      "Tatiana Sholokhova",
      "Nitesh Bharadwaj Gundavarapu",
      "Ganesh Jawahar",
      "Haroon Qureshi",
      "Jieru Hu",
      "Nikola Momchev",
      "Matthew Rahtz",
      "Renjie Wu",
      "Aishwarya P S",
      "Kedar Dhamdhere",
      "Meiqi Guo",
      "Umang Gupta",
      "Ali Eslami",
      "Mariano Schain",
      "Michiel Blokzijl",
      "David Welling",
      "Dave Orr",
      "Levent Bolelli",
      "Nicolas Perez-Nieves",
      "Mikhail Sirotenko",
      "Aman Prasad",
      "Arjun Kar",
      "Borja De Balle Pigem",
      "Tayfun Terzi",
      "Gellért Weisz",
      "Dipankar Ghosh",
      "Aditi Mavalankar",
      "Dhruv Madeka",
      "Kaspar Daugaard",
      "Hartwig Adam",
      "Viraj Shah",
      "Dana Berman",
      "Maggie Tran",
      "Steven Baker",
      "Ewa Andrejczuk",
      "Grishma Chole",
      "Ganna Raboshchuk",
      "Mahdi Mirzazadeh",
      "Thais Kagohara",
      "Shimu Wu",
      "Christian Schallhart",
      "Bernett Orlando",
      "Chen Wang",
      "Alban Rrustemi",
      "Hao Xiong",
      "Hao Liu",
      "Arpi Vezer",
      "Nolan Ramsden",
      "Shuo-yiin Chang",
      "Sidharth Mudgal",
      "Yan Li",
      "Nino Vieillard",
      "Yedid Hoshen",
      "Farooq Ahmad",
      "Ambrose Slone",
      "Amy Hua",
      "Natan Potikha",
      "Mirko Rossini",
      "Jon Stritar",
      "Sushant Prakash",
      "Zifeng Wang",
      "Xuanyi Dong",
      "Alireza Nazari",
      "Efrat Nehoran",
      "Kaan Tekelioglu",
      "Yinxiao Li",
      "Kartikeya Badola",
      "Tom Funkhouser",
      "Yuanzhen Li",
      "Varun Yerram",
      "Ramya Ganeshan",
      "Daniel Formoso",
      "Karol Langner",
      "Tian Shi",
      "Huijian Li",
      "Yumeya Yamamori",
      "Amayika Panda",
      "Alaa Saade",
      "Angelo Scorza Scarpati",
      "Chris Breaux",
      "CJ Carey",
      "Zongwei Zhou",
      "Cho-Jui Hsieh",
      "Sophie Bridgers",
      "Alena Butryna",
      "Nishesh Gupta",
      "Vaibhav Tulsyan",
      "Sanghyun Woo",
      "Evgenii Eltyshev",
      "Will Grathwohl",
      "Chanel Parks",
      "Seth Benjamin",
      "Rina Panigrahy",
      "Shenil Dodhia",
      "Daniel De Freitas",
      "Chris Sauer",
      "Will Song",
      "Ferran Alet",
      "Jackson Tolins",
      "Cosmin Paduraru",
      "Xingyi Zhou",
      "Brian Albert",
      "Zizhao Zhang",
      "Lei Shu",
      "Mudit Bansal",
      "Sarah Nguyen",
      "Amir Globerson",
      "Owen Xiao",
      "James Manyika",
      "Tom Hennigan",
      "Rong Rong",
      "Josip Matak",
      "Anton Bakalov",
      "Ankur Sharma",
      "Danila Sinopalnikov",
      "Andrew Pierson",
      "Stephen Roller",
      "Geoff Brown",
      "Mingcen Gao",
      "Toshiyuki Fukuzawa",
      "Amin Ghafouri",
      "Kenny Vassigh",
      "Iain Barr",
      "Zhicheng Wang",
      "Anna Korsun",
      "Rajesh Jayaram",
      "Lijie Ren",
      "Tim Zaman",
      "Samira Khan",
      "Yana Lunts",
      "Dan Deutsch",
      "Dave Uthus",
      "Nitzan Katz",
      "Masha Samsikova",
      "Amr Khalifa",
      "Nikhil Sethi",
      "Jiao Sun",
      "Luming Tang",
      "Uri Alon",
      "Xianghong Luo",
      "Dian Yu",
      "Abhishek Nayyar",
      "Bryce Petrini",
      "Will Truong",
      "Vincent Hellendoorn",
      "Nikolai Chinaev",
      "Chris Alberti",
      "Wei Wang",
      "Jingcao Hu",
      "Vahab Mirrokni",
      "Ananth Balashankar",
      "Avia Aharon",
      "Aahil Mehta",
      "Ahmet Iscen",
      "Joseph Kready",
      "Lucas Manning",
      "Anhad Mohananey",
      "Yuankai Chen",
      "Anshuman Tripathi",
      "Allen Wu",
      "Igor Petrovski",
      "Dawsen Hwang",
      "Martin Baeuml",
      "Shreyas Chandrakaladharan",
      "Yuan Liu",
      "Rey Coaguila",
      "Maxwell Chen",
      "Sally Ma",
      "Pouya Tafti",
      "Susheel Tatineni",
      "Terry Spitz",
      "Jiayu Ye",
      "Paul Vicol",
      "Mihaela Rosca",
      "Adrià Puigdomènech",
      "Zohar Yahav",
      "Sanjay Ghemawat",
      "Hanzhao Lin",
      "Phoebe Kirk",
      "Zaid Nabulsi",
      "Sergey Brin",
      "Bernd Bohnet",
      "Ken Caluwaerts",
      "Aditya Srikanth Veerubhotla",
      "Dan Zheng",
      "Zihang Dai",
      "Petre Petrov",
      "Yichong Xu",
      "Ramin Mehran",
      "Zhuo Xu",
      "Luisa Zintgraf",
      "Jiho Choi",
      "Spurthi Amba Hombaiah",
      "Romal Thoppilan",
      "Sashank Reddi",
      "Lukasz Lew",
      "Li Li",
      "Kellie Webster",
      "KP Sawhney",
      "Lampros Lamprou",
      "Siamak Shakeri",
      "Mayank Lunayach",
      "Jianmin Chen",
      "Sumit Bagri",
      "Alex Salcianu",
      "Ying Chen",
      "Yani Donchev",
      "Charlotte Magister",
      "Signe Nørly",
      "Vitor Rodrigues",
      "Tomas Izo",
      "Hila Noga",
      "Joe Zou",
      "Thomas Köppe",
      "Wenxuan Zhou",
      "Kenton Lee",
      "Xiangzhu Long",
      "Danielle Eisenbud",
      "Anthony Chen",
      "Connor Schenck",
      "Chi Ming To",
      "Peilin Zhong",
      "Emanuel Taropa",
      "Minh Truong",
      "Omer Levy",
      "Danilo Martins",
      "Zhiyuan Zhang",
      "Christopher Semturs",
      "Kelvin Zhang",
      "Alex Yakubovich",
      "Pol Moreno",
      "Lara McConnaughey",
      "Di Lu",
      "Sam Redmond",
      "Lotte Weerts",
      "Yonatan Bitton",
      "Tiziana Refice",
      "Nicolas Lacasse",
      "Arthur Conmy",
      "Corentin Tallec",
      "Julian Odell",
      "Hannah Forbes-Pollard",
      "Arkadiusz Socala",
      "Jonathan Hoech",
      "Pushmeet Kohli",
      "Alanna Walton",
      "Rui Wang",
      "Mikita Sazanovich",
      "Kexin Zhu",
      "Andrei Kapishnikov",
      "Rich Galt",
      "Matthew Denton",
      "Ben Murdoch",
      "Caitlin Sikora",
      "Kareem Mohamed",
      "Wei Wei",
      "Uri First",
      "Tim McConnell",
      "Luis C. Cobo",
      "James Qin",
      "Thi Avrahami",
      "Daniel Balle",
      "Yu Watanabe",
      "Annie Louis",
      "Adam Kraft",
      "Setareh Ariafar",
      "Yiming Gu",
      "Eugénie Rives",
      "Charles Yoon",
      "Andrei Rusu",
      "James Cobon-Kerr",
      "Chris Hahn",
      "Jiaming Luo",
      "Yuvein",
      "Zhu",
      "Niharika Ahuja",
      "Rodrigo Benenson",
      "Raphaël Lopez Kaufman",
      "Honglin Yu",
      "Lloyd Hightower",
      "Junlin Zhang",
      "Darren Ni",
      "Lisa Anne Hendricks",
      "Gabby Wang",
      "Gal Yona",
      "Lalit Jain",
      "Pablo Barrio",
      "Surya Bhupatiraju",
      "Siva Velusamy",
      "Allan Dafoe",
      "Sebastian Riedel",
      "Tara Thomas",
      "Zhe Yuan",
      "Mathias Bellaiche",
      "Sheena Panthaplackel",
      "Klemen Kloboves",
      "Sarthak Jauhari",
      "Canfer Akbulut",
      "Todor Davchev",
      "Evgeny Gladchenko",
      "David Madras",
      "Aleksandr Chuklin",
      "Tyrone Hill",
      "Quan Yuan",
      "Mukundan Madhavan",
      "Luke Leonhard",
      "Dylan Scandinaro",
      "Qihang Chen",
      "Ning Niu",
      "Arthur Douillard",
      "Bogdan Damoc",
      "Yasumasa Onoe",
      "Fabian Pedregosa",
      "Fred Bertsch",
      "Chas Leichner",
      "Joseph Pagadora",
      "Jonathan Malmaud",
      "Sameera Ponda",
      "Andy Twigg",
      "Oleksii Duzhyi",
      "Jingwei Shen",
      "Miaosen Wang",
      "Roopal Garg",
      "Jing Chen",
      "Utku Evci",
      "Jonathan Lee",
      "Leon Liu",
      "Koji Kojima",
      "Masa Yamaguchi",
      "Arunkumar Rajendran",
      "AJ Piergiovanni",
      "Vinodh Kumar Rajendran",
      "Marco Fornoni",
      "Gabriel Ibagon",
      "Harry Ragan",
      "Sadh MNM Khan",
      "John Blitzer",
      "Andrew Bunner",
      "Guan Sun",
      "Takahiro Kosakai",
      "Scott Lundberg",
      "Ndidi Elue",
      "Kelvin Guu",
      "SK Park",
      "Jane Park",
      "Arunachalam Narayanaswamy",
      "Chengda Wu",
      "Jayaram Mudigonda",
      "Trevor Cohn",
      "Hairong Mu",
      "Ravi Kumar",
      "Laura Graesser",
      "Yichi Zhang",
      "Richard Killam",
      "Vincent Zhuang",
      "Mai Giménez",
      "Wael Al Jishi",
      "Ruy Ley-Wild",
      "Alex Zhai",
      "Kazuki Osawa",
      "Diego Cedillo",
      "Jialu Liu",
      "Mayank Upadhyay",
      "Marcin Sieniek",
      "Roshan Sharma",
      "Tom Paine",
      "Anelia Angelova",
      "Sravanti Addepalli",
      "Carolina Parada",
      "Kingshuk Majumder",
      "Avery Lamp",
      "Sanjiv Kumar",
      "Xiang Deng",
      "Artiom Myaskovsky",
      "Tea Sabolić",
      "Jeffrey Dudek",
      "Sarah York",
      "Félix de Chaumont Quitry",
      "Jiazhong Nie",
      "Dee Cattle",
      "Alok Gunjan",
      "Bilal Piot",
      "Waleed Khawaja",
      "Seojin Bang",
      "Simon Wang",
      "Siavash Khodadadeh",
      "Raghavender R",
      "Praynaa Rawlani",
      "Richard Powell",
      "Kevin Lee",
      "Johannes Griesser",
      "GS Oh",
      "Cesar Magalhaes",
      "Yujia Li",
      "Simon Tokumine",
      "Hadas Natalie Vogel",
      "Dennis Hsu",
      "Arturo BC",
      "Disha Jindal",
      "Matan Cohen",
      "Zi Yang",
      "Junwei Yuan",
      "Dario de Cesare",
      "Tony Bruguier",
      "Jun Xu",
      "Monica Roy",
      "Alon Jacovi",
      "Dan Belov",
      "Rahul Arya",
      "Phoenix Meadowlark",
      "Shlomi Cohen-Ganor",
      "Wenting Ye",
      "Patrick Morris-Suzuki",
      "Praseem Banzal",
      "Gan Song",
      "Pranavaraj Ponnuramu",
      "Fred Zhang",
      "George Scrivener",
      "Salah Zaiem",
      "Alif Raditya Rochman",
      "Kehang Han",
      "Badih Ghazi",
      "Kate Lee",
      "Shahar Drath",
      "Daniel Suo",
      "Antonious Girgis",
      "Pradeep Shenoy",
      "Duy Nguyen",
      "Douglas Eck",
      "Somit Gupta",
      "Le Yan",
      "Joao Carreira",
      "Anmol Gulati",
      "Ruoxin Sang",
      "Daniil Mirylenka",
      "Emma Cooney",
      "Edward Chou",
      "Mingyang Ling",
      "Cindy Fan",
      "Ben Coleman",
      "Guilherme Tubone",
      "Ravin Kumar",
      "Jason Baldridge",
      "Felix Hernandez-Campos",
      "Angeliki Lazaridou",
      "James Besley",
      "Itay Yona",
      "Neslihan Bulut",
      "Quentin Wellens",
      "AJ Pierigiovanni",
      "Jasmine George",
      "Richard Green",
      "Pu Han",
      "Connie Tao",
      "Geoff Clark",
      "Chong You",
      "Abbas Abdolmaleki",
      "Justin Fu",
      "Tongzhou Chen",
      "Ashwin Chaugule",
      "Angad Chandorkar",
      "Altaf Rahman",
      "Will Thompson",
      "Penporn Koanantakool",
      "Mike Bernico",
      "Jie Ren",
      "Andrey Vlasov",
      "Sergei Vassilvitskii",
      "Maciej Kula",
      "Yizhong Liang",
      "Dahun Kim",
      "Yangsibo Huang",
      "Chengxi Ye",
      "Dmitry Lepikhin",
      "Wesley Helmholz"
    ],
    "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "72 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.06261v6",
    "published_date": "2025-07-07 17:36:04 UTC",
    "updated_date": "2025-12-19 14:25:46 UTC"
  },
  {
    "arxiv_id": "2507.05221v2",
    "title": "CTA: Cross-Task Alignment for Better Test Time Training",
    "authors": [
      "Samuel Barbeau",
      "Pedram Fekri",
      "David Osowiechi",
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Masih Aminbeidokhti",
      "Christian Desrosiers"
    ],
    "abstract": "Deep learning models have demonstrated exceptional performance across a wide range of computer vision tasks. However, their performance often degrades significantly when faced with distribution shifts, such as domain or dataset changes. Test-Time Training (TTT) has emerged as an effective method to enhance model robustness by incorporating an auxiliary unsupervised task during training and leveraging it for model updates at test time. In this work, we introduce CTA (Cross-Task Alignment), a novel approach for improving TTT. Unlike existing TTT methods, CTA does not require a specialized model architecture and instead takes inspiration from the success of multi-modal contrastive learning to align a supervised encoder with a self-supervised one. This process enforces alignment between the learned representations of both models, thereby mitigating the risk of gradient interference, preserving the intrinsic robustness of self-supervised learning and enabling more semantically meaningful updates at test-time. Experimental results demonstrate substantial improvements in robustness and generalization over the state-of-the-art on several benchmark datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint, under review",
    "pdf_url": "https://arxiv.org/pdf/2507.05221v2",
    "published_date": "2025-07-07 17:33:20 UTC",
    "updated_date": "2025-07-08 13:04:25 UTC"
  },
  {
    "arxiv_id": "2507.05211v2",
    "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
    "authors": [
      "Zongyan Han",
      "Mohamed El Amine Boudjoghra",
      "Jiahua Dong",
      "Jinhong Wang",
      "Rao Muhammad Anwer"
    ],
    "abstract": "Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05211v2",
    "published_date": "2025-07-07 17:22:00 UTC",
    "updated_date": "2025-07-25 14:03:22 UTC"
  },
  {
    "arxiv_id": "2507.05201v3",
    "title": "MedGemma Technical Report",
    "authors": [
      "Andrew Sellergren",
      "Sahar Kazemzadeh",
      "Tiam Jaroensri",
      "Atilla Kiraly",
      "Madeleine Traverse",
      "Timo Kohlberger",
      "Shawn Xu",
      "Fayaz Jamil",
      "Cían Hughes",
      "Charles Lau",
      "Justin Chen",
      "Fereshteh Mahvar",
      "Liron Yatziv",
      "Tiffany Chen",
      "Bram Sterling",
      "Stefanie Anna Baby",
      "Susanna Maria Baby",
      "Jeremy Lai",
      "Samuel Schmidgall",
      "Lu Yang",
      "Kejia Chen",
      "Per Bjornsson",
      "Shashir Reddy",
      "Ryan Brush",
      "Kenneth Philbrick",
      "Mercy Asiedu",
      "Ines Mezerreg",
      "Howard Hu",
      "Howard Yang",
      "Richa Tiwari",
      "Sunny Jansen",
      "Preeti Singh",
      "Yun Liu",
      "Shekoofeh Azizi",
      "Aishwarya Kamath",
      "Johan Ferret",
      "Shreya Pathak",
      "Nino Vieillard",
      "Ramona Merhej",
      "Sarah Perrin",
      "Tatiana Matejovicova",
      "Alexandre Ramé",
      "Morgane Riviere",
      "Louis Rouillard",
      "Thomas Mesnard",
      "Geoffrey Cideron",
      "Jean-bastien Grill",
      "Sabela Ramos",
      "Edouard Yvinec",
      "Michelle Casbon",
      "Elena Buchatskaya",
      "Jean-Baptiste Alayrac",
      "Dmitry Lepikhin",
      "Vlad Feinberg",
      "Sebastian Borgeaud",
      "Alek Andreev",
      "Cassidy Hardin",
      "Robert Dadashi",
      "Léonard Hussenot",
      "Armand Joulin",
      "Olivier Bachem",
      "Yossi Matias",
      "Katherine Chou",
      "Avinatan Hassidim",
      "Kavi Goel",
      "Clement Farabet",
      "Joelle Barral",
      "Tris Warkentin",
      "Jonathon Shlens",
      "David Fleet",
      "Victor Cotruta",
      "Omar Sanseviero",
      "Gus Martins",
      "Phoebe Kirk",
      "Anand Rao",
      "Shravya Shetty",
      "David F. Steiner",
      "Can Kirmizibayrak",
      "Rory Pilgrim",
      "Daniel Golden",
      "Lin Yang"
    ],
    "abstract": "Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05201v3",
    "published_date": "2025-07-07 17:01:44 UTC",
    "updated_date": "2025-07-12 19:13:40 UTC"
  },
  {
    "arxiv_id": "2507.05198v1",
    "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling",
    "authors": [
      "Boyuan Wang",
      "Xinpan Meng",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Angen Ye",
      "Yang Wang",
      "Zhiqin Yang",
      "Chaojun Ni",
      "Guan Huang",
      "Xingang Wang"
    ],
    "abstract": "The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://embodiedreamer.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2507.05198v1",
    "published_date": "2025-07-07 16:58:17 UTC",
    "updated_date": "2025-07-07 16:58:17 UTC"
  },
  {
    "arxiv_id": "2507.05195v2",
    "title": "Train-before-Test Harmonizes Language Model Rankings",
    "authors": [
      "Guanhua Zhang",
      "Ricardo Dominguez-Olmedo",
      "Moritz Hardt"
    ],
    "abstract": "Existing language model benchmarks provide contradictory model rankings, even for benchmarks that aim to capture similar skills. This dilemma of conflicting rankings hampers model selection, clouds model comparisons, and adds confusion to a growing ecosystem of competing models. In this paper, we take a different perspective on model comparison: instead of relying on out-of-the-box performance via direct evaluation, we compare model potential by providing each model with identical benchmark-specific fine-tuning before evaluation. We call this approach train-before-test. Our primary contribution is a comprehensive empirical evaluation of model potential across 24 benchmarks and 61 models. First, we demonstrate that model potential rankings obtained through train-before-test exhibit remarkable consistency across all benchmarks. Whereas traditional rankings demonstrate little external validity under direct evaluation, they enjoy a significant degree of external validity when applying train-before-test: model potential rankings transfer gracefully from one benchmark to another. Second, train-before-test restores the connection between perplexity and downstream task performance, lost under direct evaluation. Remarkably, even pre-finetuning perplexity of a base model predicts post-finetuning downstream performance, suggesting that ranking consistency reflects inherent model potential rather than fine-tuning artifacts. Finally, train-before-test reduces the model-score matrix to essentially rank one, indicating that model potential is dominated by one latent factor, uncovered by train-before-test. Our work supports the recommendation to make train-before-test a default component of LLM benchmarking.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05195v2",
    "published_date": "2025-07-07 16:54:18 UTC",
    "updated_date": "2025-10-13 12:21:52 UTC"
  },
  {
    "arxiv_id": "2507.05187v1",
    "title": "Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism",
    "authors": [
      "Andreas Mayer"
    ],
    "abstract": "The proliferation of AI-driven systems presents a fundamental challenge to Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW), often diminishing user agency and failing to account for value pluralism. Current approaches to value alignment, which rely on centralized, top-down definitions, lack the mechanisms for meaningful contestability. This leaves users and communities unable to challenge or shape the values embedded in the systems that govern their digital lives, creating a crisis of legitimacy and trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP), a socio-technical framework that addresses this gap. It reframes the design problem from achieving a single aligned state to infrastructuring a dynamic ecosystem for value deliberation and application. At its core, CDAVP enables diverse, self-organizing communities to define and maintain explicit value profiles - rich, machine-readable representations that can encompass not only preferences but also community-specific rights and duties. These profiles are then contextually activated by the end-user, who retains ultimate control (agency) over which values guide the AI's behavior. AI applications, in turn, are designed to transparently interpret these profiles and moderate conflicts, adhering to a set of non-negotiable, democratically-legitimated meta-rules. The designer's role shifts from crafting static interfaces to becoming an architect of participatory ecosystems. We argue that infrastructuring for pluralism is a necessary pathway toward achieving robust algorithmic accountability and genuinely contestable, human-centric AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05187v1",
    "published_date": "2025-07-07 16:45:50 UTC",
    "updated_date": "2025-07-07 16:45:50 UTC"
  },
  {
    "arxiv_id": "2507.05178v2",
    "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
    "authors": [
      "Jonathan Hyun",
      "Nicholas R Waytowich",
      "Boyuan Chen"
    ],
    "abstract": "Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Our project website is at: http://generalroboticslab.com/CREW-Wildfire",
    "pdf_url": "https://arxiv.org/pdf/2507.05178v2",
    "published_date": "2025-07-07 16:33:42 UTC",
    "updated_date": "2025-12-12 14:42:07 UTC"
  },
  {
    "arxiv_id": "2507.05177v3",
    "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model",
    "authors": [
      "Chen Wang",
      "Tianyu Peng",
      "Wen Yang",
      "Yinan Bai",
      "Guangfu Wang",
      "Jun Lin",
      "Lanpeng Jia",
      "Lingxiang Wu",
      "Jinqiao Wang",
      "Chengqing Zong",
      "Jiajun Zhang"
    ],
    "abstract": "Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at https://casia-lm.github.io/OpenS2S",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report, Update on OpenS2S_v1.5",
    "pdf_url": "https://arxiv.org/pdf/2507.05177v3",
    "published_date": "2025-07-07 16:31:37 UTC",
    "updated_date": "2025-10-27 11:59:16 UTC"
  },
  {
    "arxiv_id": "2507.05169v3",
    "title": "Critiques of World Models",
    "authors": [
      "Eric Xing",
      "Mingkai Deng",
      "Jinyu Hou",
      "Zhiting Hu"
    ],
    "abstract": "World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of \"hypothetical thinking\" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05169v3",
    "published_date": "2025-07-07 16:23:46 UTC",
    "updated_date": "2025-07-27 22:36:54 UTC"
  },
  {
    "arxiv_id": "2507.05162v1",
    "title": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains",
    "authors": [
      "Nicholas Chivaran",
      "Jianbing Ni"
    ],
    "abstract": "The recent proliferation of photorealistic AI-generated images (AIGI) has raised urgent concerns about their potential misuse, particularly on social media platforms. Current state-of-the-art AIGI detection methods typically rely on large, deep neural architectures, creating significant computational barriers to real-time, large-scale deployment on platforms like social media. To challenge this reliance on computationally intensive models, we introduce LAID, the first framework -- to our knowledge -- that benchmarks and evaluates the detection performance and efficiency of off-the-shelf lightweight neural networks. In this framework, we comprehensively train and evaluate selected models on a representative subset of the GenImage dataset across spatial, spectral, and fusion image domains. Our results demonstrate that lightweight models can achieve competitive accuracy, even under adversarial conditions, while incurring substantially lower memory and computation costs compared to current state-of-the-art methods. This study offers valuable insight into the trade-off between efficiency and performance in AIGI detection and lays a foundation for the development of practical, scalable, and trustworthy detection systems. The source code of LAID can be found at: https://github.com/nchivar/LAID.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in the proceedings of PST2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05162v1",
    "published_date": "2025-07-07 16:18:19 UTC",
    "updated_date": "2025-07-07 16:18:19 UTC"
  },
  {
    "arxiv_id": "2507.10447v1",
    "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations",
    "authors": [
      "Tomasz Sroka",
      "Tomasz Wężowicz",
      "Dominik Sidorczuk",
      "Mateusz Modrzejewski"
    ],
    "abstract": "With the rapid advancement of generative audio models, distinguishing between human-composed and generated music is becoming increasingly challenging. As a response, models for detecting fake music have been proposed. In this work, we explore the robustness of such systems under audio augmentations. To evaluate model generalization, we constructed a dataset consisting of both real and synthetic music generated using several systems. We then apply a range of audio transformations and analyze how they affect classification accuracy. We test the performance of a recent state-of-the-art musical deepfake detection model in the presence of audio augmentations. The performance of the model decreases significantly even with the introduction of light augmentations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.10447v1",
    "published_date": "2025-07-07 16:15:02 UTC",
    "updated_date": "2025-07-07 16:15:02 UTC"
  },
  {
    "arxiv_id": "2507.05157v1",
    "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models",
    "authors": [
      "Chinnappa Guggilla",
      "Budhaditya Roy",
      "Trupti Ramdas Chavan",
      "Abdul Rahman",
      "Edward Bowen"
    ],
    "abstract": "Large Language Models (LLMs) possess an extraordinary capability to produce text that is not only coherent and contextually relevant but also strikingly similar to human writing. They adapt to various styles and genres, producing content that is both grammatically correct and semantically meaningful. Recently, LLMs have been misused to create highly realistic phishing emails, spread fake news, generate code to automate cyber crime, and write fraudulent scientific articles. Additionally, in many real-world applications, the generated content including style and topic and the generator model are not known beforehand. The increasing prevalence and sophistication of artificial intelligence (AI)-generated texts have made their detection progressively more challenging. Various attempts have been made to distinguish machine-generated text from human-authored content using linguistic, statistical, machine learning, and ensemble-based approaches. This work focuses on two primary objectives Task-A, which involves distinguishing human-written text from machine-generated text, and Task-B, which attempts to identify the specific LLM model responsible for the generation. Both of these tasks are based on fine tuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language Model Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from Transformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model has achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05157v1",
    "published_date": "2025-07-07 16:13:13 UTC",
    "updated_date": "2025-07-07 16:13:13 UTC"
  },
  {
    "arxiv_id": "2507.18577v2",
    "title": "Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges",
    "authors": [
      "Liyuan Chen",
      "Shuoling Liu",
      "Jiangpeng Yan",
      "Xiaoyu Wang",
      "Henglin Liu",
      "Chuang Li",
      "Kecheng Jiao",
      "Jixuan Ying",
      "Yang Veronica Liu",
      "Qiang Yang",
      "Xiu Li"
    ],
    "abstract": "The advent of foundation models (FMs), large-scale pre-trained models with strong generalization capabilities, has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of financial foundation models (FFMs): a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: financial language foundation models (FinLFMs), financial time-series foundation models (FinTSFMs), and financial visual-language foundation models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints and offer insights into future research opportunities. We hope this survey can serve as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "Accepted by [J]. Engineering, 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.18577v2",
    "published_date": "2025-07-07 16:06:38 UTC",
    "updated_date": "2025-12-15 01:07:02 UTC"
  },
  {
    "arxiv_id": "2507.05150v1",
    "title": "Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster",
    "authors": [
      "Luka Van de Sype",
      "Matthieu Vert",
      "Alexei Sharpanskykh",
      "Seyed Sahand Mohammadi Ziabari"
    ],
    "abstract": "The severity of natural disasters is increasing every year, impacting many people's lives. During the response phase of disasters, airports are important hubs where relief aid arrives and people need to be evacuated. However, the airport often forms a bottleneck in these relief operations due to the sudden need for increased capacity. Limited research has been done on the operational side of airport disaster management. Experts identify the main problems as, first, the asymmetry of information between the airport and incoming flights, and second, the lack of resources. The goal of this research is to understand the effects of incomplete knowledge of incoming flights with different resource allocation strategies on the performance of cargo handling operations at an airport after a natural disaster. An agent-based model is created, implementing realistic offloading strategies with different degrees of information uncertainty. Model calibration and verification are performed with experts in the field. The model performance is measured by the average turnaround time, which is divided into offloading time, boarding time, and cumulative waiting times. The results show that the effects of one unplanned aircraft are negligible. However, all waiting times increase with more arriving unplanned aircraft.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05150v1",
    "published_date": "2025-07-07 16:00:26 UTC",
    "updated_date": "2025-07-07 16:00:26 UTC"
  },
  {
    "arxiv_id": "2507.05149v2",
    "title": "OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows",
    "authors": [
      "Tom Hickling",
      "Jonathan F. MacArt",
      "Justin Sirignano",
      "Den Waidmann"
    ],
    "abstract": "Turbulent flows are chaotic and unsteady, but their statistical distribution converges to a statistical steady state. Engineering quantities of interest typically take the form of time-average statistics such as $ \\frac{1}{t} \\int_0^t f ( u(x,τ; θ) ) dτ\\overset{t \\rightarrow \\infty}{\\rightarrow} F(x; θ)$, where $u(x,t; θ)$ are solutions of the Navier--Stokes equations with parameters $θ$. Optimizing over $F(x; θ)$ has many engineering applications including geometric optimization, flow control, and closure modeling. However, this remains an open challenge, as existing computational approaches are incapable of scaling to physically representative numbers of grid points. The fundamental obstacle is the chaoticity of turbulent flows: gradients calculated with the adjoint method diverge exponentially as $t \\rightarrow \\infty$.\n  We develop a new online gradient-flow (OGF) method that is scalable to large degree-of-freedom systems and enables optimizing for the steady-state statistics of chaotic, unsteady, turbulence-resolving simulations. The method forward-propagates an online estimate for the gradient of $F(x; θ)$ while simultaneously performing online updates of the parameters $θ$. A key feature is the fully online nature of the algorithm to facilitate faster optimization progress and its combination with a finite-difference estimator to avoid the divergence of gradients due to chaoticity. The proposed OGF method is demonstrated for optimizations over three chaotic ordinary and partial differential equations: the Lorenz-63 equation, the Kuramoto--Sivashinsky equation, and Navier--Stokes solutions of compressible, forced, homogeneous isotropic turbulence. In each case, the OGF method successfully reduces the loss based on $F(x; θ)$ by several orders of magnitude and accurately recovers the optimal parameters.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "34 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05149v2",
    "published_date": "2025-07-07 16:00:15 UTC",
    "updated_date": "2025-09-16 17:29:17 UTC"
  },
  {
    "arxiv_id": "2507.05142v1",
    "title": "GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation",
    "authors": [
      "Wei Xu",
      "Haoran Li",
      "Baoyuan Ou",
      "Lai Xu",
      "Yingjie Qin",
      "Ruilong Su",
      "Ruiwen Xu"
    ],
    "abstract": "Cross-domain Click-Through Rate prediction aims to tackle the data sparsity and the cold start problems in online advertising systems by transferring knowledge from source domains to a target domain. Most existing methods rely on overlapping users to facilitate this transfer, often focusing on joint training or pre-training with fine-tuning approach to connect the source and target domains. However, in real-world industrial settings, joint training struggles to learn optimal representations with different distributions, and pre-training with fine-tuning is not well-suited for continuously integrating new data. To address these issues, we propose GIST, a cross-domain lifelong sequence model that decouples the training processes of the source and target domains. Unlike previous methods that search lifelong sequences in the source domains using only content or behavior signals or their simple combinations, we innovatively introduce a Content-Behavior Joint Training Module (CBJT), which aligns content-behavior distributions and combines them with guided information to facilitate a more stable representation. Furthermore, we develop an Asymmetric Similarity Integration strategy (ASI) to augment knowledge transfer through similarity computation. Extensive experiments demonstrate the effectiveness of GIST, surpassing SOTA methods on offline evaluations and an online A/B test. Deployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances online ads system performance at scale, serving hundreds of millions of daily active users.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05142v1",
    "published_date": "2025-07-07 15:51:27 UTC",
    "updated_date": "2025-07-07 15:51:27 UTC"
  },
  {
    "arxiv_id": "2507.05321v1",
    "title": "AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts",
    "authors": [
      "Kwangsuk Park",
      "Jiwoong Yang"
    ],
    "abstract": "Recent advances in AI-assisted education have encouraged the integration of vision-language models (VLMs) into academic assessment, particularly for tasks that require both quantitative and qualitative evaluation. However, existing VLM based approaches struggle with complex educational artifacts, such as programming tasks with executable components and measurable outputs, that require structured reasoning and alignment with clearly defined evaluation criteria. We introduce AGACCI, a multi-agent system that distributes specialized evaluation roles across collaborative agents to improve accuracy, interpretability, and consistency in code-oriented assessment. To evaluate the framework, we collected 360 graduate-level code-based assignments from 60 participants, each annotated by domain experts with binary rubric scores and qualitative feedback. Experimental results demonstrate that AGACCI outperforms a single GPT-based baseline in terms of rubric and feedback accuracy, relevance, consistency, and coherence, while preserving the instructional intent and evaluative depth of expert assessments. Although performance varies across task types, AGACCI highlights the potential of multi-agent systems for scalable and context-aware educational evaluation.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at ICML 2025 Workshop on Multi-Agent Systems in the Era of Foundation Models: Opportunities, Challenges and Futures (MAS)",
    "pdf_url": "https://arxiv.org/pdf/2507.05321v1",
    "published_date": "2025-07-07 15:50:46 UTC",
    "updated_date": "2025-07-07 15:50:46 UTC"
  },
  {
    "arxiv_id": "2507.05137v2",
    "title": "Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization",
    "authors": [
      "Jaewook Lee",
      "Alexander Scarlatos",
      "Andrew Lan"
    ],
    "abstract": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet backgrounds due to script differences. Japanese combines syllabaries like hiragana with kanji, which are logographic characters of Chinese origin. Kanji are also complicated due to their complexity and volume. Keyword mnemonics are a common strategy to aid memorization, often using the compositional structure of kanji to form vivid associations. Despite recent efforts to use large language models (LLMs) to assist learners, existing methods for LLM-based keyword mnemonic generation function as a black box, offering limited interpretability. We propose a generative framework that explicitly models the mnemonic construction process as driven by a set of common rules, and learn them using a novel Expectation-Maximization-type algorithm. Trained on learner-authored mnemonics from an online platform, our method learns latent structures and compositional rules, enabling interpretable and systematic mnemonics generation. Experiments show that our method performs well in the cold-start setting for new learners while providing insight into the mechanisms behind effective mnemonic creation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.05137v2",
    "published_date": "2025-07-07 15:49:23 UTC",
    "updated_date": "2025-08-29 15:28:00 UTC"
  },
  {
    "arxiv_id": "2507.05123v1",
    "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques",
    "authors": [
      "Walid Mohamed Aly",
      "Taysir Hassan A. Soliman",
      "Amr Mohamed AbdelAziz"
    ],
    "abstract": "Large Language Models (LLMs) continue to advance natural language processing with their ability to generate human-like text across a range of tasks. Despite the remarkable success of LLMs in Natural Language Processing (NLP), their performance in text summarization across various domains and datasets has not been comprehensively evaluated. At the same time, the ability to summarize text effectively without relying on extensive training data has become a crucial bottleneck. To address these issues, we present a systematic evaluation of six LLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog), and ArXiv (scientific). By leveraging prompt engineering techniques including zero-shot and in-context learning, our study evaluates the performance using the ROUGE and BERTScore metrics. In addition, a detailed analysis of inference times is conducted to better understand the trade-off between summarization quality and computational efficiency. For Long documents, introduce a sentence-based chunking strategy that enables LLMs with shorter context windows to summarize extended inputs in multiple stages. The findings reveal that while LLMs perform competitively on news and dialog tasks, their performance on long scientific documents improves significantly when aided by chunking strategies. In addition, notable performance variations were observed based on model parameters, dataset properties, and prompt design. These results offer actionable insights into how different LLMs behave across task types, contributing to ongoing research in efficient, instruction-based NLP systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This manuscript is an extended version of the work accepted for publication in the International Journal of Advanced Computer Science and Applications (IJACSA), Volume 16, Issue 6, June 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05123v1",
    "published_date": "2025-07-07 15:34:05 UTC",
    "updated_date": "2025-07-07 15:34:05 UTC"
  },
  {
    "arxiv_id": "2507.05121v1",
    "title": "LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks",
    "authors": [
      "Jiajia Guo",
      "Peiwen Jiang",
      "Chao-Kai Wen",
      "Shi Jin",
      "Jun Zhang"
    ],
    "abstract": "Accurate channel state information (CSI) is critical to the performance of wireless communication systems, especially with the increasing scale and complexity introduced by 5G and future 6G technologies. While artificial intelligence (AI) offers a promising approach to CSI acquisition and utilization, existing methods largely depend on task-specific neural networks (NNs) that require expert-driven design and large training datasets, limiting their generalizability and practicality. To address these challenges, we propose LVM4CSI, a general and efficient framework that leverages the structural similarity between CSI and computer vision (CV) data to directly apply large vision models (LVMs) pre-trained on extensive CV datasets to wireless tasks without any fine-tuning, in contrast to large language model-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI tasks to analogous CV tasks, transforms complex-valued CSI into visual formats compatible with LVMs, and integrates lightweight trainable layers to adapt extracted features to specific communication objectives. We validate LVM4CSI through three representative case studies, including channel estimation, human activity recognition, and user localization. Results demonstrate that LVM4CSI achieves comparable or superior performance to task-specific NNs, including an improvement exceeding 9.61 dB in channel estimation and approximately 40% reduction in localization error. Furthermore, it significantly reduces the number of trainable parameters and eliminates the need for task-specific NN design.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.IT",
    "comment": "This work has been submitted for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2507.05121v1",
    "published_date": "2025-07-07 15:33:55 UTC",
    "updated_date": "2025-07-07 15:33:55 UTC"
  },
  {
    "arxiv_id": "2507.05118v1",
    "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
    "authors": [
      "Danil S. Grigorev",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "abstract": "In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at https://verifyllm.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "IROS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05118v1",
    "published_date": "2025-07-07 15:31:36 UTC",
    "updated_date": "2025-07-07 15:31:36 UTC"
  },
  {
    "arxiv_id": "2507.05116v4",
    "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting",
    "authors": [
      "Juyi Lin",
      "Amir Taherin",
      "Arash Akbari",
      "Arman Akbari",
      "Lei Lu",
      "Guangyu Chen",
      "Taskin Padir",
      "Xiaomeng Yang",
      "Weiwei Chen",
      "Yiqian Li",
      "Xue Lin",
      "David Kaeli",
      "Pu Zhao",
      "Yanzhi Wang"
    ],
    "abstract": "Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading to high inference latency and increased training cost, and (ii) insufficient utilization of generated actions resulting in potential performance loss. To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reducing inference latency and training cost. Furthermore, we introduce an inference optimization technique with a novel voting-based ensemble strategy to combine current and previous action predictions, improving the utilization of generated actions and overall performance. Our results demonstrate that we achieve superior performance compared with state-of-the-art VLA models, achieving significantly higher success rates and 39$\\times$ faster inference than OpenVLA with 46 Hz throughput on edge platforms, demonstrating practical deployability. The code is available at https://github.com/LukeLIN-web/VOTE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05116v4",
    "published_date": "2025-07-07 15:30:55 UTC",
    "updated_date": "2025-10-02 19:28:32 UTC"
  },
  {
    "arxiv_id": "2507.05110v3",
    "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift",
    "authors": [
      "Shixuan Liu",
      "Yue He",
      "Yunfei Wang",
      "Hao Zou",
      "Haoxiang Cheng",
      "Wenjing Yang",
      "Peng Cui",
      "Zhong Liu"
    ],
    "abstract": "Logical rule learning, a prominent category of knowledge graph (KG) reasoning methods, constitutes a critical research area aimed at learning explicit rules from observed facts to infer missing knowledge. However, like all KG reasoning methods, rule learning suffers from a critical weakness-its dependence on the I.I.D. assumption. This assumption can easily be violated due to selection bias during training or agnostic distribution shifts during testing (e.g., as in query shift scenarios), ultimately undermining model performance and reliability. To enable robust KG reasoning in wild environments, this study investigates logical rule learning in the presence of agnostic test-time distribution shifts. We formally define this challenge as out-of-distribution (OOD) KG reasoning-a previously underexplored problem, and propose the Stable Rule Learning (StableRule) framework as a solution. StableRule is an end-to-end framework that combines feature decorrelation with rule learning network, to enhance OOD generalization in KG reasoning. By leveraging feature decorrelation, StableRule mitigates the adverse effects of covariate shifts arising in OOD scenarios, improving the robustness of the rule learning network. Extensive experiments on seven benchmark KGs demonstrate the framework's superior effectiveness and stability across diverse heterogeneous environments, highlighting its practical significance for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05110v3",
    "published_date": "2025-07-07 15:27:48 UTC",
    "updated_date": "2025-07-10 16:55:05 UTC"
  },
  {
    "arxiv_id": "2507.05108v2",
    "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration",
    "authors": [
      "Yuyi Zhang",
      "Peirong Zhang",
      "Zhenhua Yang",
      "Pengyu Yan",
      "Yongxin Shi",
      "Pengwei Liu",
      "Fengjun Guo",
      "Lianwen Jin"
    ],
    "abstract": "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05108v2",
    "published_date": "2025-07-07 15:26:17 UTC",
    "updated_date": "2025-07-21 13:18:42 UTC"
  },
  {
    "arxiv_id": "2507.05319v1",
    "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review",
    "authors": [
      "Cheng Yuan",
      "Xinkai Rui",
      "Yongqi Fan",
      "Yawei Fan",
      "Boyang Zhong",
      "Jiacheng Wang",
      "Weiyan Zhang",
      "Tong Ruan"
    ],
    "abstract": "Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL Demo 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05319v1",
    "published_date": "2025-07-07 15:25:52 UTC",
    "updated_date": "2025-07-07 15:25:52 UTC"
  },
  {
    "arxiv_id": "2507.05101v2",
    "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs",
    "authors": [
      "Xinzhe Zheng",
      "Hao Du",
      "Fanding Xu",
      "Jinzhe Li",
      "Zhiyuan Liu",
      "Wenkang Wang",
      "Tao Chen",
      "Wanli Ouyang",
      "Stan Z. Li",
      "Yan Lu",
      "Nanqing Dong",
      "Yang Zhang"
    ],
    "abstract": "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.MN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05101v2",
    "published_date": "2025-07-07 15:21:05 UTC",
    "updated_date": "2025-10-22 15:38:08 UTC"
  },
  {
    "arxiv_id": "2507.05098v1",
    "title": "Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance",
    "authors": [
      "Tobias Demmler",
      "Jakob Häringer",
      "Andreas Tamke",
      "Thao Dang",
      "Alexander Hegai",
      "Lars Mikelsons"
    ],
    "abstract": "Accurate trajectory prediction is critical for safe autonomous navigation, yet the impact of dataset design on model performance remains understudied. This work systematically examines how feature selection, cross-dataset transfer, and geographic diversity influence trajectory prediction accuracy in multi-agent settings. We evaluate a state-of-the-art model using our novel L4 Motion Forecasting dataset based on our own data recordings in Germany and the US. This includes enhanced map and agent features. We compare our dataset to the US-centric Argoverse 2 benchmark. First, we find that incorporating supplementary map and agent features unique to our dataset, yields no measurable improvement over baseline features, demonstrating that modern architectures do not need extensive feature sets for optimal performance. The limited features of public datasets are sufficient to capture convoluted interactions without added complexity. Second, we perform cross-dataset experiments to evaluate how effective domain knowledge can be transferred between datasets. Third, we group our dataset by country and check the knowledge transfer between different driving cultures.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05098v1",
    "published_date": "2025-07-07 15:18:51 UTC",
    "updated_date": "2025-07-07 15:18:51 UTC"
  },
  {
    "arxiv_id": "2507.05093v1",
    "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
    "authors": [
      "Alberto Castagnaro",
      "Umberto Salviati",
      "Mauro Conti",
      "Luca Pajola",
      "Simeone Pizzi"
    ],
    "abstract": "Large Language Models (LLMs) have transformed human-machine interaction since ChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a key framework that enhances LLM outputs by integrating external knowledge. However, RAG's reliance on ingesting external documents introduces new vulnerabilities. This paper exposes a critical security gap at the data loading stage, where malicious actors can stealthily corrupt RAG pipelines by exploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce two novel threat vectors -- Content Obfuscation and Content Injection -- targeting common formats (DOCX, HTML, PDF). Using an automated toolkit implementing 19 stealthy injection techniques, we test five popular data loaders, finding a 74.4% attack success rate across 357 scenarios. We further validate these threats on six end-to-end RAG systems -- including white-box pipelines and black-box services like NotebookLM and OpenAI Assistants -- demonstrating high success rates and critical vulnerabilities that bypass filters and silently compromise output integrity. Our results emphasize the urgent need to secure the document ingestion process in RAG systems against covert content manipulations.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "currently under submission",
    "pdf_url": "https://arxiv.org/pdf/2507.05093v1",
    "published_date": "2025-07-07 15:13:54 UTC",
    "updated_date": "2025-07-07 15:13:54 UTC"
  },
  {
    "arxiv_id": "2507.05088v1",
    "title": "How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs",
    "authors": [
      "Kilian Rückschloß",
      "Felix Weitkämper"
    ],
    "abstract": "Pearl observes that causal knowledge enables predicting the effects of interventions, such as actions, whereas descriptive knowledge only permits drawing conclusions from observation. This paper extends Pearl's approach to causality and interventions to the setting of stratified abductive logic programs. It shows how stable models of such programs can be given a causal interpretation by building on philosophical foundations and recent work by Bochman and Eelink et al. In particular, it provides a translation of abductive logic programs into causal systems, thereby clarifying the informal causal reading of logic program rules and supporting principled reasoning about external actions. The main result establishes that the stable model semantics for stratified programs conforms to key philosophical principles of causation, such as causal sufficiency, natural necessity, and irrelevance of unobserved effects. This justifies the use of stratified abductive logic programs as a framework for causal modeling and for predicting the effects of interventions",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05088v1",
    "published_date": "2025-07-07 15:12:01 UTC",
    "updated_date": "2025-07-07 15:12:01 UTC"
  },
  {
    "arxiv_id": "2507.05077v3",
    "title": "Sequential Attention-based Sampling for Histopathological Analysis",
    "authors": [
      "Tarun G",
      "Naman Malpani",
      "Gugan Thoppe",
      "Sridharan Devarajan"
    ],
    "abstract": "Deep neural networks are increasingly applied in automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering them computationally infeasible to analyze entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- Sequential Attention-based Sampling for Histopathological Analysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\\%) of high-resolution patches to achieve reliable diagnoses. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features. Model implementation is available at: https://github.com/coglabiisc/SASHA.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05077v3",
    "published_date": "2025-07-07 15:03:12 UTC",
    "updated_date": "2025-11-16 04:02:15 UTC"
  },
  {
    "arxiv_id": "2507.05068v2",
    "title": "ICAS: Detecting Training Data from Autoregressive Image Generative Models",
    "authors": [
      "Hongyao Yu",
      "Yixiang Qiu",
      "Yiheng Yang",
      "Hao Fang",
      "Tianqu Zhuang",
      "Jiaxin Hong",
      "Bin Chen",
      "Hao Wu",
      "Shu-Tao Xia"
    ],
    "abstract": "Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms. Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "ACM MM 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05068v2",
    "published_date": "2025-07-07 14:50:42 UTC",
    "updated_date": "2025-11-29 14:36:44 UTC"
  },
  {
    "arxiv_id": "2507.05065v1",
    "title": "Replacing thinking with tool usage enables reasoning in small language models",
    "authors": [
      "Corrado Rainone",
      "Tim Bakker",
      "Roland Memisevic"
    ],
    "abstract": "Recent advances have established a new machine learning paradigm based on scaling up compute at inference time as well as at training time. In that line of work, a combination of Supervised Fine-Tuning (SFT) on synthetic demonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is used for training Large Language Models to expend extra compute during inference in the form of \"thoughts\" expressed in natural language. In this paper, we propose to instead format these tokens as a multi-turn interaction trace with a stateful tool. At each turn, the new state of the tool is appended to the context of the model, whose job is to generate the tokens necessary to control the tool via a custom DSL. We benchmark this approach on the problem of repairing malfunctioning Python code, and show that this constrained setup allows for faster sampling of experience and a denser reward signal, allowing even models of size up to 3B parameters to learn how to proficiently expend additional compute on the task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, includes appendix",
    "pdf_url": "https://arxiv.org/pdf/2507.05065v1",
    "published_date": "2025-07-07 14:49:18 UTC",
    "updated_date": "2025-07-07 14:49:18 UTC"
  },
  {
    "arxiv_id": "2507.05056v2",
    "title": "INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling",
    "authors": [
      "Xin Dong",
      "Shichao Dong",
      "Jin Wang",
      "Jing Huang",
      "Li Zhou",
      "Zenghui Sun",
      "Lihua Jing",
      "Jingsong Lan",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "abstract": "Hallucinations in large vision-language models (LVLMs) pose significant challenges for real-world applications, as LVLMs may generate responses that appear plausible yet remain inconsistent with the associated visual content. This issue rarely occurs in human cognition. We argue that this discrepancy arises from humans' ability to effectively leverage multimodal interaction information in data samples. Specifically, humans typically first gather multimodal information, analyze the interactions across modalities for understanding, and then express their understanding through language. Motivated by this observation, we conduct extensive experiments on popular LVLMs and obtained insights that surprisingly reveal human-like, though less pronounced, cognitive behavior of LVLMs on multimodal samples. Building on these findings, we further propose \\textbf{INTER}: \\textbf{Inter}action Guidance Sampling, a novel training-free algorithm that mitigate hallucinations without requiring additional data. Specifically, INTER explicitly guides LVLMs to effectively reapply their understanding of multimodal interaction information when generating responses, thereby reducing potential hallucinations. On six benchmarks including VQA and image captioning tasks, INTER achieves an average improvement of up to 3.4\\% on five LVLMs compared to the state-of-the-art decoding strategy. The code will be released when the paper is accepted.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.05056v2",
    "published_date": "2025-07-07 14:38:53 UTC",
    "updated_date": "2025-07-22 07:33:11 UTC"
  },
  {
    "arxiv_id": "2507.05316v1",
    "title": "OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models",
    "authors": [
      "Koren Lazar",
      "Matan Vetzler",
      "Kiran Kate",
      "Jason Tsay",
      "David Boaz Himanshu Gupta",
      "Avraham Shinnar",
      "Rohith D Vallam",
      "David Amid Esther Goldbraich",
      "Guy Uziel",
      "Jim Laredo",
      "Ateret Anaby Tavor"
    ],
    "abstract": "AI agents and business automation tools interacting with external web services require standardized, machine-readable information about their APIs in the form of API specifications. However, the information about APIs available online is often presented as unstructured, free-form HTML documentation, requiring external users to spend significant time manually converting it into a structured format. To address this, we introduce OASBuilder, a novel framework that transforms long and diverse API documentation pages into consistent, machine-readable API specifications. This is achieved through a carefully crafted pipeline that integrates large language models and rule-based algorithms which are guided by domain knowledge of the structure of documentation webpages. Our experiments demonstrate that OASBuilder generalizes well across hundreds of APIs, and produces valid OpenAPI specifications that encapsulate most of the information from the original documentation. OASBuilder has been successfully implemented in an enterprise environment, saving thousands of hours of manual effort and making hundreds of complex enterprise APIs accessible as tools for LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05316v1",
    "published_date": "2025-07-07 14:36:13 UTC",
    "updated_date": "2025-07-07 14:36:13 UTC"
  },
  {
    "arxiv_id": "2507.05030v1",
    "title": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good",
    "authors": [
      "Celeste Campos-Castillo",
      "Xuan Kang",
      "Linnea I. Laestadius"
    ],
    "abstract": "Recently, research into chatbots (also known as conversational agents, AI agents, voice assistants), which are computer applications using artificial intelligence to mimic human-like conversation, has grown sharply. Despite this growth, sociology lags other disciplines (including computer science, medicine, psychology, and communication) in publishing about chatbots. We suggest sociology can advance understanding of human-chatbot interaction and offer four sociological theories to enhance extant work in this field. The first two theories (resource substitution theory, power-dependence theory) add new insights to existing models of the drivers of chatbot use, which overlook sociological concerns about how social structure (e.g., systemic discrimination, the uneven distribution of resources within networks) inclines individuals to use chatbots, including problematic levels of emotional dependency on chatbots. The second two theories (affect control theory, fundamental cause of disease theory) help inform the development of chatbot-driven interventions that minimize safety risks and enhance equity by leveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g., affective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic participation). We discuss the value of applying sociological theories for advancing theorizing about human-chatbot interaction and developing chatbots for social good.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05030v1",
    "published_date": "2025-07-07 14:12:03 UTC",
    "updated_date": "2025-07-07 14:12:03 UTC"
  },
  {
    "arxiv_id": "2508.00844v1",
    "title": "Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework",
    "authors": [
      "Christopher Wissuchek",
      "Patrick Zschech"
    ],
    "abstract": "Artificial intelligence (AI) systems are evolving beyond passive tools into autonomous agents capable of reasoning, adapting, and acting with minimal human intervention. Despite their growing presence, a structured framework is lacking to classify and compare these systems. This paper develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure. Using a multi-phase methodological approach, we construct and refine this typology, which is then evaluated through a human-AI hybrid approach and further distilled into constructed types. The framework enables researchers and practitioners to analyze varying levels of agency in AI systems. By offering a structured perspective on the progression of AI capabilities, the typology provides a foundation for assessing current systems and anticipating future developments in agentic AI.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.MA",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint accepted for archival and presentation at the Pacific-Asia Conference on Information Systems (PACIS) 2025, Kuala Lumpur, Malaysia",
    "pdf_url": "https://arxiv.org/pdf/2508.00844v1",
    "published_date": "2025-07-07 14:05:30 UTC",
    "updated_date": "2025-07-07 14:05:30 UTC"
  },
  {
    "arxiv_id": "2507.05020v2",
    "title": "Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision",
    "authors": [
      "Soham Walimbe",
      "Britty Baby",
      "Vinkle Srivastav",
      "Nicolas Padoy"
    ],
    "abstract": "Surgical AI often involves multiple tasks within a single procedure, like phase recognition or assessing the Critical View of Safety in laparoscopic cholecystectomy. Traditional models, built for one task at a time, lack flexibility, requiring a separate model for each. To address this, we introduce MML-SurgAdapt, a unified multi-task framework with Vision-Language Models (VLMs), specifically CLIP, to handle diverse surgical tasks through natural language supervision. A key challenge in multi-task learning is the presence of partial annotations when integrating different tasks. To overcome this, we employ Single Positive Multi-Label (SPML) learning, which traditionally reduces annotation burden by training models with only one positive label per instance. Our framework extends this approach to integrate data from multiple surgical tasks within a single procedure, enabling effective learning despite incomplete or noisy annotations. We demonstrate the effectiveness of our model on a combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50, utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt performs comparably to task-specific benchmarks, with the added advantage of handling noisy annotations. It also outperforms the existing SPML frameworks for the task. By reducing the required labels by 23%, our approach proposes a more scalable and efficient labeling process, significantly easing the annotation burden on clinicians. To our knowledge, this is the first application of SPML to integrate data from multiple surgical tasks, presenting a novel and generalizable solution for multi-task learning in surgical computer vision. Implementation is available at: https://github.com/CAMMA-public/MML-SurgAdapt",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05020v2",
    "published_date": "2025-07-07 14:03:10 UTC",
    "updated_date": "2025-07-10 12:21:47 UTC"
  },
  {
    "arxiv_id": "2507.05019v1",
    "title": "Meta-Learning Transformers to Improve In-Context Generalization",
    "authors": [
      "Lorenzo Braccaioli",
      "Anna Vettoruzzo",
      "Prabhant Singh",
      "Joaquin Vanschoren",
      "Mohamed-Rafik Bouguelia",
      "Nicola Conci"
    ],
    "abstract": "In-context learning enables transformer models to generalize to new tasks based solely on input prompts, without any need for weight updates. However, existing training paradigms typically rely on large, unstructured datasets that are costly to store, difficult to evaluate for quality and balance, and pose privacy and ethical concerns due to the inclusion of sensitive information. Motivated by these limitations and risks, we propose an alternative training strategy where we leverage a collection of multiple, small-scale, and domain-specific datasets. We empirically demonstrate that the increased quality and diversity of such data improve the generalization abilities of in-context learners beyond their training domain, while achieving comparable performance with models trained on a single large-scale dataset. We investigate this paradigm by leveraging meta-learning to train an in-context learner on the Meta-Album collection under several settings. Firstly, we show the performance in a controlled environment, where the test domain is completely excluded from the training knowledge. Secondly, we explore the robustness of these models to forgetting in a continual scenario where the information is accessible for a limited time. Finally, we explore the more challenging unsupervised scenario. Our findings demonstrate that transformers still generalize for in-context prediction when trained on a curated dataset collection while offering advantages in modularity and replaceability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05019v1",
    "published_date": "2025-07-07 14:02:22 UTC",
    "updated_date": "2025-07-07 14:02:22 UTC"
  },
  {
    "arxiv_id": "2507.05011v3",
    "title": "DARIL: When Imitation Learning outperforms Reinforcement Learning in Surgical Action Planning",
    "authors": [
      "Maxence Boels",
      "Harry Robertshaw",
      "Thomas C Booth",
      "Prokar Dasgupta",
      "Alejandro Granados",
      "Sebastien Ourselin"
    ],
    "abstract": "Surgical action planning requires predicting future instrument-verb-target triplets for real-time assistance. While teleoperated robotic surgery provides natural expert demonstrations for imitation learning (IL), reinforcement learning (RL) could potentially discover superior strategies through self-exploration. We present the first comprehensive comparison of IL versus RL for surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and 33.6% next frame prediction mAP with smooth planning degradation to 29.2% at 10-second horizons. We evaluated three RL variants: world model-based RL, direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches underperformed DARIL--world model RL dropped to 3.1% mAP at 10s while direct video RL achieved only 15.9%. Our analysis reveals that distribution matching on expert-annotated test sets systematically favors IL over potentially valid RL policies that differ from training demonstrations. This challenges assumptions about RL superiority in sequential decision making and provides crucial insights for surgical AI development.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted at the MICCAI2025 workshop proceedings on COLlaborative Intelligence and Autonomy in Image-guided Surgery (COLAS)",
    "pdf_url": "https://arxiv.org/pdf/2507.05011v3",
    "published_date": "2025-07-07 13:49:57 UTC",
    "updated_date": "2025-10-20 09:07:47 UTC"
  },
  {
    "arxiv_id": "2507.05007v2",
    "title": "Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition",
    "authors": [
      "Britty Baby",
      "Vinkle Srivastav",
      "Pooja P. Jain",
      "Kun Yuan",
      "Pietro Mascagni",
      "Nicolas Padoy"
    ],
    "abstract": "The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/CVS-AdaptNet",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05007v2",
    "published_date": "2025-07-07 13:44:58 UTC",
    "updated_date": "2025-07-10 12:22:03 UTC"
  },
  {
    "arxiv_id": "2507.05315v1",
    "title": "Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces",
    "authors": [
      "Madina Kojanazarova",
      "Florentin Bieder",
      "Robin Sandkühler",
      "Philippe C. Cattin"
    ],
    "abstract": "Soft tissue simulation in virtual environments is becoming increasingly important for medical applications. However, the high deformability of soft tissue poses significant challenges. Existing methods rely on segmentation, meshing and estimation of stiffness properties of tissues. In addition, the integration of haptic feedback requires precise force estimation to enable a more immersive experience. We introduce a novel data-driven model, a conditional graph neural network (cGNN) to tackle this complexity. Our model takes surface points and the location of applied forces, and is specifically designed to predict the deformation of the points and the forces exerted on them. We trained our model on experimentally collected surface tracking data of a soft tissue phantom and used transfer learning to overcome the data scarcity by initially training it with mass-spring simulations and fine-tuning it with the experimental data. This approach improves the generalisation capability of the model and enables accurate predictions of tissue deformations and corresponding interaction forces. The results demonstrate that the model can predict deformations with a distance error of 0.35$\\pm$0.03 mm for deformations up to 30 mm and the force with an absolute error of 0.37$\\pm$0.05 N for forces up to 7.5 N. Our data-driven approach presents a promising solution to the intricate challenge of simulating soft tissues within virtual environments. Beyond its applicability in medical simulations, this approach holds the potential to benefit various fields where realistic soft tissue simulations are required.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05315v1",
    "published_date": "2025-07-07 13:33:39 UTC",
    "updated_date": "2025-07-07 13:33:39 UTC"
  },
  {
    "arxiv_id": "2507.04994v1",
    "title": "Supported Abstract Argumentation for Case-Based Reasoning",
    "authors": [
      "Adam Gould",
      "Gabriel de Olim Gaul",
      "Francesca Toni"
    ],
    "abstract": "We introduce Supported Abstract Argumentation for Case-Based Reasoning (sAA-CBR), a binary classification model in which past cases engage in debates by arguing in favour of their labelling and attacking or supporting those with opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of its precursor AA-CBR, which can contain extraneous cases (or spikes) that are not included in the debates. We prove that sAA-CBR contains no spikes, without trading off key model properties",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to IARML@ICJAI2025: Workshop on the Interactions between Analogical Reasoning and Machine Learning",
    "pdf_url": "https://arxiv.org/pdf/2507.04994v1",
    "published_date": "2025-07-07 13:32:08 UTC",
    "updated_date": "2025-07-07 13:32:08 UTC"
  },
  {
    "arxiv_id": "2507.04981v4",
    "title": "Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning",
    "authors": [
      "Ruihao Zhang",
      "Mao chen",
      "Fei Ye",
      "Dandan Meng",
      "Yixuan Huang",
      "Xiao Liu"
    ],
    "abstract": "T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "4 figures, 3 tabels, 8 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.04981v4",
    "published_date": "2025-07-07 13:24:41 UTC",
    "updated_date": "2025-11-22 08:09:08 UTC"
  },
  {
    "arxiv_id": "2507.05314v1",
    "title": "Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation",
    "authors": [
      "Daniel Cieślak",
      "Miriam Reca",
      "Olena Onyshchenko",
      "Jacek Rumiński"
    ],
    "abstract": "Accurate segmentation of wounds and scale markers in clinical images remainsa significant challenge, crucial for effective wound management and automatedassessment. In this study, we propose a novel dual-attention U-Net++ archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms toaddress severe class imbalance and variability in medical images effectively.Initially, extensive benchmarking across diverse architectures and encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal encoder backbone.Subsequently, we independently trained two class-specific models with tailoredpreprocessing, extensive data augmentation, and Bayesian hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test Time Augmentationto further enhance prediction reliability. Our approach was evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition. Segmentationperformance was quantified using a weighted F1-score (75% wounds, 25% scalemarkers), calculated externally by competition organizers on undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640, underscoring itseffectiveness for complex medical segmentation tasks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, conference: Joint 20th Nordic-Baltic Conference on Biomedical Engineering & 24th Polish Conference on Biocybernetics and Biomedical Engineering; 6 figures, 2 tables, 11 sources",
    "pdf_url": "https://arxiv.org/pdf/2507.05314v1",
    "published_date": "2025-07-07 13:24:15 UTC",
    "updated_date": "2025-07-07 13:24:15 UTC"
  },
  {
    "arxiv_id": "2507.05313v2",
    "title": "Solar Flare Prediction Using Long Short-term Memory (LSTM) and Decomposition-LSTM with Sliding Window Pattern Recognition",
    "authors": [
      "Zeinab Hassani",
      "Davud Mohammadpur",
      "Hossein Safari"
    ],
    "abstract": "We investigate the use of Long Short-Term Memory (LSTM) and Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to predict solar flare occurrences using time-series data from the GOES catalog. The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among approximately possible patterns, 7,552 yearly pattern windows are identified, highlighting the challenge of long-term forecasting due to the Sun's complex, self-organized criticality-driven behavior. A sliding window technique is employed to detect temporal quasi-patterns in both irregular and regularized flare time series. Regularization reduces complexity, enhances large flare activity, and captures active days more effectively. To address class imbalance, resampling methods are applied. LSTM and DLSTM models are trained on sequences of peak fluxes and waiting times from irregular time series, while LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding windows of regularized time series with a 3-hour interval. Performance metrics, particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87) in the receiver operating characteristic (ROC), indicate that DLSTM with an ensemble approach on regularized time series outperforms other models, offering more accurate large-flare forecasts with fewer false errors compared to models trained on irregular time series. The superior performance of DLSTM is attributed to its ability to decompose time series into trend and seasonal components, effectively isolating random noise. This study underscores the potential of advanced machine learning techniques for solar flare prediction and highlights the importance of incorporating various solar cycle phases and resampling strategies to enhance forecasting reliability.",
    "categories": [
      "cs.LG",
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Astrophysical Journal Supplement Series, volume 279, 2025, DOI: 10.3847/1538-4365/addc73",
    "pdf_url": "https://arxiv.org/pdf/2507.05313v2",
    "published_date": "2025-07-07 13:17:38 UTC",
    "updated_date": "2025-07-15 07:37:58 UTC"
  },
  {
    "arxiv_id": "2507.04966v2",
    "title": "LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning",
    "authors": [
      "Sandipan Dhar",
      "Mayank Gupta",
      "Preeti Rao"
    ],
    "abstract": "The field of Singing Voice Synthesis (SVS) has seen significant advancements in recent years due to the rapid progress of diffusion-based approaches. However, capturing vocal style, genre-specific pitch inflections, and language-dependent characteristics remains challenging, particularly in low-resource scenarios. To address this, we propose LAPS-Diff, a diffusion model integrated with language-aware embeddings and a vocal-style guided learning mechanism, specifically designed for Bollywood Hindi singing style. We curate a Hindi SVS dataset and leverage pre-trained language models to extract word and phone-level embeddings for an enriched lyrics representation. Additionally, we incorporated a style encoder and a pitch extraction model to compute style and pitch losses, capturing features essential to the naturalness and expressiveness of the synthesized singing, particularly in terms of vocal style and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec models to extract musical and contextual embeddings, serving as conditional priors to refine the acoustic feature generation process further. Based on objective and subjective evaluations, we demonstrate that LAPS-Diff significantly improves the quality of the generated samples compared to the considered state-of-the-art (SOTA) model for our constrained dataset that is typical of the low resource scenario.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "10 pages, 5 figures, 3 Tables",
    "pdf_url": "https://arxiv.org/pdf/2507.04966v2",
    "published_date": "2025-07-07 13:09:36 UTC",
    "updated_date": "2025-11-28 15:06:43 UTC"
  },
  {
    "arxiv_id": "2507.04959v2",
    "title": "Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation",
    "authors": [
      "Yingshan Liang",
      "Keyu Fan",
      "Zhicheng Du",
      "Yiran Wang",
      "Qingyang Shi",
      "Xinyu Zhang",
      "Jiasheng Lu",
      "Peiwu Qin"
    ],
    "abstract": "Video-to-audio (V2A) generation shows great potential in fields such as film production. Despite significant advances, current V2A methods relying on global video information struggle with complex scenes and generating audio tailored to specific objects. To address these limitations, we introduce Hear-Your-Click, an interactive V2A framework enabling users to generate sounds for specific objects by clicking on the frame. To achieve this, we propose Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder (MVE) to obtain object-level visual features aligned with audio. Furthermore, we tailor two data augmentation strategies, Random Video Stitching (RVS) and Mask-guided Loudness Modulation (MLM), to enhance the model's sensitivity to segmented objects. To measure audio-visual correspondence, we designed a new evaluation metric, the CAV score. Extensive experiments demonstrate that our framework offers more precise control and improves generation performance across various metrics. Project Page: https://github.com/SynapGrid/Hear-Your-Click",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04959v2",
    "published_date": "2025-07-07 13:01:50 UTC",
    "updated_date": "2025-07-13 09:31:19 UTC"
  },
  {
    "arxiv_id": "2507.04955v1",
    "title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation",
    "authors": [
      "Fathinah Izzati",
      "Xinyue Li",
      "Gus Xia"
    ],
    "abstract": "We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls - specifically, human facial expressions and upper-body motion - as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04955v1",
    "published_date": "2025-07-07 12:56:20 UTC",
    "updated_date": "2025-07-07 12:56:20 UTC"
  },
  {
    "arxiv_id": "2507.04947v1",
    "title": "DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer",
    "authors": [
      "Yecheng Wu",
      "Junyu Chen",
      "Zhuoyang Zhang",
      "Enze Xie",
      "Jincheng Yu",
      "Junsong Chen",
      "Jinyi Hu",
      "Yao Lu",
      "Song Han",
      "Han Cai"
    ],
    "abstract": "We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizers' limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04947v1",
    "published_date": "2025-07-07 12:45:23 UTC",
    "updated_date": "2025-07-07 12:45:23 UTC"
  },
  {
    "arxiv_id": "2507.04920v1",
    "title": "Object-centric Denoising Diffusion Models for Physical Reasoning",
    "authors": [
      "Moritz Lange",
      "Raphael C. Engelhardt",
      "Wolfgang Konen",
      "Andrew Melnik",
      "Laurenz Wiskott"
    ],
    "abstract": "Reasoning about the trajectories of multiple, interacting objects is integral to physical reasoning tasks in machine learning. This involves conditions imposed on the objects at different time steps, for instance initial states or desired goal states. Existing approaches in physical reasoning generally rely on autoregressive modeling, which can only be conditioned on initial states, but not on later states. In fields such as planning for reinforcement learning, similar challenges are being addressed with denoising diffusion models. In this work, we propose an object-centric denoising diffusion model architecture for physical reasoning that is translation equivariant over time, permutation equivariant over objects, and can be conditioned on arbitrary time steps for arbitrary objects. We demonstrate how this model can solve tasks with multiple conditions and examine its performance when changing object numbers and trajectory lengths during inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04920v1",
    "published_date": "2025-07-07 12:06:24 UTC",
    "updated_date": "2025-07-07 12:06:24 UTC"
  },
  {
    "arxiv_id": "2507.04917v1",
    "title": "Leadership Detection via Time-Lagged Correlation-Based Network Inference",
    "authors": [
      "Thayanne França da Silva",
      "José Everardo Bessa Maia"
    ],
    "abstract": "Understanding leadership dynamics in collective behavior is a key challenge in animal ecology, swarm robotics, and intelligent transportation. Traditional information-theoretic approaches, including Transfer Entropy (TE) and Time-Lagged Mutual Information (TLMI), have been widely used to infer leader-follower relationships but face critical limitations in noisy or short-duration datasets due to their reliance on robust probability estimations. This study proposes a method based on dynamic network inference using time-lagged correlations across multiple kinematic variables: velocity, acceleration, and direction. Our approach constructs directed influence graphs over time, enabling the identification of leadership patterns without the need for large volumes of data or parameter-sensitive discretization. We validate our method through two multi-agent simulations in NetLogo: a modified Vicsek model with informed leaders and a predator-prey model featuring coordinated and independent wolf groups. Experimental results demonstrate that the network-based method outperforms TE and TLMI in scenarios with limited spatiotemporal observations, ranking true leaders at the top of influence metrics more consistently than TE and TLMI.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "nlin.AO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04917v1",
    "published_date": "2025-07-07 12:04:10 UTC",
    "updated_date": "2025-07-07 12:04:10 UTC"
  },
  {
    "arxiv_id": "2507.04909v2",
    "title": "HumanVideo-MME: Benchmarking MLLMs for Human-Centric Video Understanding",
    "authors": [
      "Yuxuan Cai",
      "Jiangning Zhang",
      "Zhenye Gan",
      "Qingdong He",
      "Xiaobin Hu",
      "Junwei Zhu",
      "Yabiao Wang",
      "Chengjie Wang",
      "Zhucun Xue",
      "Chaoyou Fu",
      "Xinwei He",
      "Xiang Bai"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks involving both images and videos. However, their capacity to comprehend human-centric video data remains underexplored, primarily due to the absence of comprehensive and high-quality evaluation benchmarks. Existing human-centric benchmarks predominantly emphasize video generation quality and action recognition, while overlooking essential perceptual and cognitive abilities required in human-centered scenarios. Furthermore, they are often limited by single-question paradigms and overly simplistic evaluation metrics. To address above limitations, we propose a modern HV-MMBench, a rigorously curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric video understanding. Compared to existing human-centric video benchmarks, our work offers the following key features: (1) Diverse evaluation dimensions: HV-MMBench encompasses 13 tasks, ranging from basic attribute perception (e.g., age estimation, emotion recognition) to advanced cognitive reasoning (e.g., social relationship prediction, intention prediction), enabling comprehensive assessment of model capabilities; (2) Varied data types: The benchmark includes multiple-choice, fill-in-blank, true/false, and open-ended question formats, combined with diverse evaluation metrics, to more accurately and robustly reflect model performance; (3) Multi-domain video coverage: The benchmark spans 50 distinct visual scenarios, enabling comprehensive evaluation across fine-grained scene variations; (4) Temporal coverage: The benchmark covers videos from short-term (10 seconds) to long-term (up to 30min) durations, supporting systematic analysis of models temporal reasoning abilities across diverse contextual lengths.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2507.04909v2",
    "published_date": "2025-07-07 11:52:24 UTC",
    "updated_date": "2025-09-30 08:05:26 UTC"
  },
  {
    "arxiv_id": "2507.21102v1",
    "title": "Assessing the Ecological Impact of AI",
    "authors": [
      "Sylvia Wenmackers"
    ],
    "abstract": "Philosophers of technology have recently started paying more attention to the environmental impacts of AI, in particular of large language models (LLMs) and generative AI (genAI) applications. Meanwhile, few developers of AI give concrete estimates of the ecological impact of their models and products, and even when they do so, their analysis is often limited to green house gas emissions of certain stages of AI development or use. The current proposal encourages practically viable analyses of the sustainability aspects of genAI informed by philosophical ideas.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "This was presented as a lightning talk at: LOCO 2024, December 3, 2024, Glasgow/Online",
    "pdf_url": "https://arxiv.org/pdf/2507.21102v1",
    "published_date": "2025-07-07 11:50:18 UTC",
    "updated_date": "2025-07-07 11:50:18 UTC"
  },
  {
    "arxiv_id": "2507.04903v2",
    "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning",
    "authors": [
      "Thinh Dao",
      "Dung Thuy Nguyen",
      "Khoa D Doan",
      "Kok-Seng Wong"
    ],
    "abstract": "Research on backdoor attacks in Federated Learning (FL) has accelerated in recent years, with new attacks and defenses continually proposed in an escalating arms race. However, the evaluation of these methods remains neither standardized nor reliable. First, there are severe inconsistencies in the evaluation settings across studies, and many rely on unrealistic threat models. Second, our code review uncovers semantic bugs in the official codebases of several attacks that artificially inflate their reported performance. These issues raise fundamental questions about whether current methods are truly effective or simply overfitted to narrow experimental setups. We introduce \\textbf{BackFed}, a benchmark designed to standardize and stress-test FL backdoor evaluation by unifying attacks and defenses under a common evaluation framework that mirrors realistic FL deployments. Our benchmark on three representative datasets with three distinct architectures reveals critical limitations of existing methods. Malicious clients often require excessive training time and computation, making them vulnerable to server-enforced time constraints. Meanwhile, several defenses incur severe accuracy degradation or aggregation overhead. Popular defenses and attacks achieve limited performance in our benchmark, which challenges their previous efficacy claims. We establish BackFed as a rigorous and fair evaluation framework that enables more reliable progress in FL backdoor research.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "Our framework is openly available at https://github.com/thinh-dao/BackFed",
    "pdf_url": "https://arxiv.org/pdf/2507.04903v2",
    "published_date": "2025-07-07 11:40:45 UTC",
    "updated_date": "2025-11-25 10:13:08 UTC"
  },
  {
    "arxiv_id": "2507.04893v1",
    "title": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction",
    "authors": [
      "Kaleem Ullah Qasim",
      "Jiashu Zhang"
    ],
    "abstract": "Accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies, and severe class imbalance in which rare but high-severity cases are underrepresented and hard to detect. Existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings and offer limited interpretability. To address these challenges, we propose MARBLE a multiagent rule based LLM engine that decomposes the severity prediction task across a team of specialized reasoning agents, including an interchangeable ML-backed agent. Each agent focuses on a semantic subset of features (e.g., spatial, environmental, temporal), enabling scoped reasoning and modular prompting without the risk of prompt saturation. Predictions are coordinated through either rule-based or LLM-guided consensus mechanisms that account for class rarity and confidence dynamics. The system retains structured traces of agent-level reasoning and coordination outcomes, supporting in-depth interpretability and post-hoc performance diagnostics. Across both UK and US datasets, MARBLE consistently outperforms traditional machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below 48%. This performance redefines the practical ceiling for accident severity classification under real world noise and extreme class imbalance. Our results position MARBLE as a generalizable and interpretable framework for reasoning under uncertainty in safety-critical applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.04893v1",
    "published_date": "2025-07-07 11:27:49 UTC",
    "updated_date": "2025-07-07 11:27:49 UTC"
  },
  {
    "arxiv_id": "2507.04886v4",
    "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations",
    "authors": [
      "A. Bochkov"
    ],
    "abstract": "Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational \"meaning vectors.\" This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to \"representational interference\" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in Transactions on Machine Learning Research (10/2025). OpenReview: https://openreview.net/forum?id=Odh8IynO1o",
    "pdf_url": "https://arxiv.org/pdf/2507.04886v4",
    "published_date": "2025-07-07 11:17:32 UTC",
    "updated_date": "2025-10-15 13:46:44 UTC"
  },
  {
    "arxiv_id": "2507.04883v1",
    "title": "Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning",
    "authors": [
      "Sanyam Vyas",
      "Alberto Caron",
      "Chris Hicks",
      "Pete Burnap",
      "Vasilios Mavroudis"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) systems are increasingly used in safety-critical applications, yet their security remains severely underexplored. This work investigates backdoor attacks, which implant hidden triggers that cause malicious actions only when specific inputs appear in the observation space. Existing DRL backdoor research focuses solely on training-time attacks requiring unrealistic access to the training pipeline. In contrast, we reveal critical vulnerabilities across the DRL supply chain where backdoors can be embedded with significantly reduced adversarial privileges. We introduce two novel attacks: (1) TrojanentRL, which exploits component-level flaws to implant a persistent backdoor that survives full model retraining; and (2) InfrectroRL, a post-training backdoor attack which requires no access to training, validation, nor test data. Empirical and analytical evaluations across six Atari environments show our attacks rival state-of-the-art training-time backdoor attacks while operating under much stricter adversarial constraints. We also demonstrate that InfrectroRL further evades two leading DRL backdoor defenses. These findings challenge the current research focus and highlight the urgent need for robust defenses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04883v1",
    "published_date": "2025-07-07 11:15:54 UTC",
    "updated_date": "2025-07-07 11:15:54 UTC"
  },
  {
    "arxiv_id": "2507.04880v1",
    "title": "HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection",
    "authors": [
      "Xiaofang Liu",
      "Lingling Sun",
      "Xuqing Zhang",
      "Yuannong Ye",
      "Bin zhao"
    ],
    "abstract": "Colorectal cancer (CRC) is closely linked to the malignant transformation of colorectal polyps, making early detection essential. However, current models struggle with detecting small lesions, accurately localizing boundaries, and providing interpretable decisions. To address these issues, we propose HGNet, which integrates High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention. Key innovations include: (1) an Efficient Multi-Scale Context Attention (EMCA) module to enhance lesion feature representation and boundary modeling; (2) the deployment of a spatial hypergraph convolution module before the detection head to capture higher-order spatial relationships between nodes; (3) the application of transfer learning to address the scarcity of medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for decision visualization. Experimental results show that HGNet achieves 94% accuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion differentiation and clinical interpretability. The source code will be made publicly available upon publication of this paper.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04880v1",
    "published_date": "2025-07-07 11:09:05 UTC",
    "updated_date": "2025-07-07 11:09:05 UTC"
  },
  {
    "arxiv_id": "2507.04877v1",
    "title": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine",
    "authors": [
      "Zewen Sun",
      "Ruoxiang Huang",
      "Jiahe Feng",
      "Rundong Kong",
      "Yuqian Wang",
      "Hengyu Liu",
      "Ziqi Gong",
      "Yuyuan Qin",
      "Yingxue Wang",
      "Yu Wang"
    ],
    "abstract": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM) diagnosis through multi-turn dialogues and knowledge graphs presents a significant challenge for modern AI systems. Current large language models (LLMs), despite their advancements, exhibit notable limitations in medical applications, particularly in conducting effective multi-turn dialogues and proactive questioning. These shortcomings hinder their practical application and effectiveness in simulating real-world diagnostic scenarios. To address these limitations, we propose DoPI, a novel LLM system specifically designed for the TCM domain. The DoPI system introduces a collaborative architecture comprising a guidance model and an expert model. The guidance model conducts multi-turn dialogues with patients and dynamically generates questions based on a knowledge graph to efficiently extract critical symptom information. Simultaneously, the expert model leverages deep TCM expertise to provide final diagnoses and treatment plans. Furthermore, this study constructs a multi-turn doctor-patient dialogue dataset to simulate realistic consultation scenarios and proposes a novel evaluation methodology that does not rely on manually collected real-world consultation data. Experimental results show that the DoPI system achieves an accuracy rate of 84.68 percent in interrogation outcomes, significantly enhancing the model's communication ability during diagnosis while maintaining professional expertise.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04877v1",
    "published_date": "2025-07-07 11:04:03 UTC",
    "updated_date": "2025-07-07 11:04:03 UTC"
  },
  {
    "arxiv_id": "2507.04868v3",
    "title": "A Novel Approach for Estimating Largest Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning",
    "authors": [
      "A. Velichko",
      "M. Belyaev",
      "P. Boriskov"
    ],
    "abstract": "Understanding and quantifying chaos from data remains challenging. We present a data-driven method for estimating the largest Lyapunov exponent (LLE) from one-dimensional chaotic time series using machine learning. A predictor is trained to produce out-of-sample, multi-horizon forecasts; the LLE is then inferred from the exponential growth of the geometrically averaged forecast error (GMAE) across the horizon, which serves as a proxy for trajectory divergence. We validate the approach on four canonical 1D maps-logistic, sine, cubic, and Chebyshev-achieving R2pos > 0.99 against reference LLE curves with series as short as M = 450. Among baselines, KNN yields the closest fits (KNN-R comparable; RF larger deviations). By design the estimator targets positive exponents: in periodic/stable regimes it returns values indistinguishable from zero. Noise robustness is assessed by adding zero-mean white measurement noise and summarizing performance versus the average SNR over parameter sweeps: accuracy saturates for SNRm > 30 dB and collapses below 27 dB, a conservative sensor-level benchmark. The method is simple, computationally efficient, and model-agnostic, requiring only stationarity and the presence of a dominant positive exponent. It offers a practical route to LLE estimation in experimental settings where only scalar time-series measurements are available, with extensions to higher-dimensional and irregularly sampled data left for future work.",
    "categories": [
      "nlin.CD",
      "cs.AI"
    ],
    "primary_category": "nlin.CD",
    "comment": "18 pages, 5 figures, 2 Tables, 14 Equations",
    "pdf_url": "https://arxiv.org/pdf/2507.04868v3",
    "published_date": "2025-07-07 10:53:02 UTC",
    "updated_date": "2025-10-02 10:16:24 UTC"
  },
  {
    "arxiv_id": "2507.04858v1",
    "title": "Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu",
    "authors": [
      "António Sá Pinto"
    ],
    "abstract": "We explore transfer learning strategies for musical onset detection in the Afro-Brazilian Maracatu tradition, which features complex rhythmic patterns that challenge conventional models. We adapt two Temporal Convolutional Network architectures: one pre-trained for onset detection (intra-task) and another for beat tracking (inter-task). Using only 5-second annotated snippets per instrument, we fine-tune these models through layer-wise retraining strategies for five traditional percussion instruments. Our results demonstrate significant improvements over baseline performance, with F1 scores reaching up to 0.998 in the intra-task setting and improvements of over 50 percentage points in best-case scenarios. The cross-task adaptation proves particularly effective for time-keeping instruments, where onsets naturally align with beat positions. The optimal fine-tuning configuration varies by instrument, highlighting the importance of instrument-specific adaptation strategies. This approach addresses the challenges of underrepresented musical traditions, offering an efficient human-in-the-loop methodology that minimizes annotation effort while maximizing performance. Our findings contribute to more inclusive music information retrieval tools applicable beyond Western musical contexts.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ISMIR 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04858v1",
    "published_date": "2025-07-07 10:32:26 UTC",
    "updated_date": "2025-07-07 10:32:26 UTC"
  },
  {
    "arxiv_id": "2507.10566v1",
    "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems",
    "authors": [
      "Hung Ming Liu"
    ],
    "abstract": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development of Emergent Communication has long been constrained by the ``Joint Exploration Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' . Traditional methods address this by introducing inductive biases to facilitate communication emergence . This study fundamentally questions whether such artificial inductive biases are, in fact, over-engineering. Through experiments with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an endogenous symbol system, their neural representations naturally exhibit spontaneous semantic compression and Nash equilibrium-driven semantic convergence, achieving effective symbolic communication without external inductive biases. This aligns with recent neuroscience findings suggesting that the human brain does not directly use human language for internal thought , and resonates with research on ``soft thinking'' capabilities in Large Language Models (LLMs) . Compared to traditional explicit communication methods, AIM demonstrates stronger generality and efficiency. The interpretable analysis toolkit developed in this study confirms that symbol usage exhibits a significant power-law distribution, leading to three major theoretical insights: the ``Neural Communication Hypothesis'', the ``Tool-First Principle'', and the ``Semantic Interpretability Paradigm''. Future research will explore the integration of Hierarchical Quantized Variational Autoencoders (HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This discovery offers new avenues for bridging symbolism and connectionism.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.10566v1",
    "published_date": "2025-07-07 09:52:49 UTC",
    "updated_date": "2025-07-07 09:52:49 UTC"
  },
  {
    "arxiv_id": "2507.05311v1",
    "title": "PLACE: Prompt Learning for Attributed Community Search",
    "authors": [
      "Shuheng Fang",
      "Kangfei Zhao",
      "Rener Zhang",
      "Yu Rong",
      "Jeffrey Xu Yu"
    ],
    "abstract": "In this paper, we propose PLACE (Prompt Learning for Attributed Community Search), an innovative graph prompt learning framework for ACS. Enlightened by prompt-tuning in Natural Language Processing (NLP), where learnable prompt tokens are inserted to contextualize NLP queries, PLACE integrates structural and learnable prompt tokens into the graph as a query-dependent refinement mechanism, forming a prompt-augmented graph. Within this prompt-augmented graph structure, the learned prompt tokens serve as a bridge that strengthens connections between graph nodes for the query, enabling the GNN to more effectively identify patterns of structural cohesiveness and attribute similarity related to the specific query. We employ an alternating training paradigm to optimize both the prompt parameters and the GNN jointly. Moreover, we design a divide-and-conquer strategy to enhance scalability, supporting the model to handle million-scale graphs. Extensive experiments on 9 real-world graphs demonstrate the effectiveness of PLACE for three types of ACS queries, where PLACE achieves higher F1 scores by 22% compared to the state-of-the-arts on average.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "15 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05311v1",
    "published_date": "2025-07-07 09:48:09 UTC",
    "updated_date": "2025-07-07 09:48:09 UTC"
  },
  {
    "arxiv_id": "2507.06258v1",
    "title": "Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems",
    "authors": [
      "Bo Yan",
      "Yurong Hao",
      "Dingqi Liu",
      "Huabin Sun",
      "Pengpeng Qiao",
      "Wei Yang Bryan Lim",
      "Yang Cao",
      "Chuan Shi"
    ],
    "abstract": "Federated recommender systems (FedRec) have emerged as a promising solution for delivering personalized recommendations while safeguarding user privacy. However, recent studies have demonstrated their vulnerability to poisoning attacks. Existing attacks typically target the entire user group, which compromises stealth and increases the risk of detection. In contrast, real-world adversaries may prefer to prompt target items to specific user subgroups, such as recommending health supplements to elderly users. Motivated by this gap, we introduce Spattack, the first targeted poisoning attack designed to manipulate recommendations for specific user subgroups in the federated setting. Specifically, Spattack adopts a two-stage approximation-and-promotion strategy, which first simulates user embeddings of target/non-target subgroups and then prompts target items to the target subgroups. To enhance the approximation stage, we push the inter-group embeddings away based on contrastive learning and augment the target group's relevant item set based on clustering. To enhance the promotion stage, we further propose to adaptively tune the optimization weights between target and non-target subgroups. Besides, an embedding alignment strategy is proposed to align the embeddings between the target items and the relevant items. We conduct comprehensive experiments on three real-world datasets, comparing Spattack against seven state-of-the-art poisoning attacks and seven representative defense mechanisms. Experimental results demonstrate that Spattack consistently achieves strong manipulation performance on the specific user subgroup, while incurring minimal impact on non-target users, even when only 0.1\\% of users are malicious. Moreover, Spattack maintains competitive overall recommendation performance and exhibits strong resilience against existing mainstream defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.06258v1",
    "published_date": "2025-07-07 09:40:16 UTC",
    "updated_date": "2025-07-07 09:40:16 UTC"
  },
  {
    "arxiv_id": "2507.04817v1",
    "title": "Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters",
    "authors": [
      "Mathilde Abrassart",
      "Nicolas Obin",
      "Axel Roebel"
    ],
    "abstract": "Precise control over speech characteristics, such as pitch, duration, and speech rate, remains a significant challenge in the field of voice conversion. The ability to manipulate parameters like pitch and syllable rate is an important element for effective identity conversion, but can also be used independently for voice transformation, achieving goals that were historically addressed by vocoder-based methods.\n  In this work, we explore a convolutional neural network-based approach that aims to provide means for modifying fundamental frequency (F0), phoneme sequences, intensity, and speaker identity. Rather than relying on disentanglement techniques, our model is explicitly conditioned on these factors to generate mel spectrograms, which are then converted into waveforms using a universal neural vocoder. Accordingly, during inference, F0 contours, phoneme sequences, and speaker embeddings can be freely adjusted, allowing for intuitively controlled voice transformations.\n  We evaluate our approach on speaker conversion and expressive speech tasks using both perceptual and objective metrics. The results suggest that the proposed method offers substantial flexibility, while maintaining high intelligibility and speaker similarity.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.04817v1",
    "published_date": "2025-07-07 09:36:00 UTC",
    "updated_date": "2025-07-07 09:36:00 UTC"
  },
  {
    "arxiv_id": "2507.04815v1",
    "title": "From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach",
    "authors": [
      "Mihai Masala",
      "Marius Leordeanu"
    ],
    "abstract": "The task of describing video content in natural language is commonly referred to as video captioning. Unlike conventional video captions, which are typically brief and widely available, long-form paragraph descriptions in natural language are scarce. This limitation of current datasets is due to the expensive human manual annotation required and to the highly challenging task of explaining the language formation process from the perspective of the underlying story, as a complex system of interconnected events in space and time. Through a thorough analysis of recently published methods and available datasets, we identify a general lack of published resources dedicated to the problem of describing videos in complex language, beyond the level of descriptions in the form of enumerations of simple captions. Furthermore, while state-of-the-art methods produce impressive results on the task of generating shorter captions from videos by direct end-to-end learning between the videos and text, the problem of explaining the relationship between vision and language is still beyond our reach. In this work, we propose a shared representation between vision and language, based on graphs of events in space and time, which can be obtained in an explainable and analytical way, to integrate and connect multiple vision tasks to produce the final natural language description. Moreover, we also demonstrate how our automated and explainable video description generation process can function as a fully automatic teacher to effectively train direct, end-to-end neural student pathways, within a self-supervised neuro-analytical system. We validate that our explainable neuro-analytical approach generates coherent, rich and relevant textual descriptions on videos collected from multiple varied datasets, using both standard evaluation metrics, human annotations and consensus from ensembles of state-of-the-art VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2501.08460",
    "pdf_url": "https://arxiv.org/pdf/2507.04815v1",
    "published_date": "2025-07-07 09:33:19 UTC",
    "updated_date": "2025-07-07 09:33:19 UTC"
  },
  {
    "arxiv_id": "2507.05309v1",
    "title": "Neural Velocity for hyperparameter tuning",
    "authors": [
      "Gianluca Dalmasso",
      "Andrea Bragagnolo",
      "Enzo Tartaglione",
      "Attilio Fiandrotti",
      "Marco Grangetto"
    ],
    "abstract": "Hyperparameter tuning, such as learning rate decay and defining a stopping criterion, often relies on monitoring the validation loss. This paper presents NeVe, a dynamic training approach that adjusts the learning rate and defines the stop criterion based on the novel notion of \"neural velocity\". The neural velocity measures the rate of change of each neuron's transfer function and is an indicator of model convergence: sampling neural velocity can be performed even by forwarding noise in the network, reducing the need for a held-out dataset. Our findings show the potential of neural velocity as a key metric for optimizing neural network training efficiently",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to IJCNN 2025 (International Joint Conference on Neural Networks). 8 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05309v1",
    "published_date": "2025-07-07 09:32:25 UTC",
    "updated_date": "2025-07-07 09:32:25 UTC"
  },
  {
    "arxiv_id": "2507.04803v1",
    "title": "Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents",
    "authors": [
      "George Jagadeesh",
      "Srikrishna Iyer",
      "Michal Polanowski",
      "Kai Xin Thia"
    ],
    "abstract": "This study examines the feasibility of applying large language models (LLMs) for forecasting the impact of traffic incidents on the traffic flow. The use of LLMs for this task has several advantages over existing machine learning-based solutions such as not requiring a large training dataset and the ability to utilize free-text incident logs. We propose a fully LLM-based solution that predicts the incident impact using a combination of traffic features and LLM-extracted incident features. A key ingredient of this solution is an effective method of selecting examples for the LLM's in-context learning. We evaluate the performance of three advanced LLMs and two state-of-the-art machine learning models on a real traffic incident dataset. The results show that the best-performing LLM matches the accuracy of the most accurate machine learning model, despite the former not having been trained on this prediction task. The findings indicate that LLMs are a practically viable option for traffic incident impact prediction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for publication at the 2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, 2025. Copyright IEEE",
    "pdf_url": "https://arxiv.org/pdf/2507.04803v1",
    "published_date": "2025-07-07 09:22:06 UTC",
    "updated_date": "2025-07-07 09:22:06 UTC"
  },
  {
    "arxiv_id": "2507.04793v2",
    "title": "A Survey of Pun Generation: Datasets, Evaluations and Methodologies",
    "authors": [
      "Yuchen Su",
      "Yonghua Zhu",
      "Ruofan Wang",
      "Zijian Huang",
      "Diana Benavides-Prado",
      "Michael Witbrock"
    ],
    "abstract": "Pun generation seeks to creatively modify linguistic elements in text to produce humour or evoke double meanings. It also aims to preserve coherence and contextual appropriateness, making it useful in creative writing and entertainment across various media and contexts. Although pun generation has received considerable attention in computational linguistics, there is currently no dedicated survey that systematically reviews this specific area. To bridge this gap, this paper provides a comprehensive review of pun generation datasets and methods across different stages, including conventional approaches, deep learning techniques, and pre-trained language models. Additionally, we summarise both automated and human evaluation metrics used to assess the quality of pun generation. Finally, we discuss the research challenges and propose promising directions for future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2507.04793v2",
    "published_date": "2025-07-07 09:12:46 UTC",
    "updated_date": "2025-10-03 12:47:55 UTC"
  },
  {
    "arxiv_id": "2507.04792v1",
    "title": "Model Compression using Progressive Channel Pruning",
    "authors": [
      "Jinyang Guo",
      "Weichen Zhang",
      "Wanli Ouyang",
      "Dong Xu"
    ],
    "abstract": "In this work, we propose a simple but effective channel pruning framework called Progressive Channel Pruning (PCP) to accelerate Convolutional Neural Networks (CNNs). In contrast to the existing channel pruning methods that prune channels only once per layer in a layer-by-layer fashion, our new progressive framework iteratively prunes a small number of channels from several selected layers, which consists of a three-step attempting-selecting-pruning pipeline in each iteration. In the attempting step, we attempt to prune a pre-defined number of channels from one layer by using any existing channel pruning methods and estimate the accuracy drop for this layer based on the labelled samples in the validation set. In the selecting step, based on the estimated accuracy drops for all layers, we propose a greedy strategy to automatically select a set of layers that will lead to less overall accuracy drop after pruning these layers. In the pruning step, we prune a small number of channels from these selected layers. We further extend our PCP framework to prune channels for the deep transfer learning methods like Domain Adversarial Neural Network (DANN), in which we effectively reduce the data distribution mismatch in the channel pruning process by using both labelled samples from the source domain and pseudo-labelled samples from the target domain. Our comprehensive experiments on two benchmark datasets demonstrate that our PCP framework outperforms the existing channel pruning approaches under both supervised learning and transfer learning settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04792v1",
    "published_date": "2025-07-07 09:12:03 UTC",
    "updated_date": "2025-07-07 09:12:03 UTC"
  },
  {
    "arxiv_id": "2507.04790v3",
    "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning",
    "authors": [
      "Giwon Lee",
      "Wooseong Jeong",
      "Daehee Park",
      "Jaewoo Jeong",
      "Kuk-Jin Yoon"
    ],
    "abstract": "Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at ICCV 2025 (Highlight)",
    "pdf_url": "https://arxiv.org/pdf/2507.04790v3",
    "published_date": "2025-07-07 09:11:45 UTC",
    "updated_date": "2025-07-25 06:46:34 UTC"
  },
  {
    "arxiv_id": "2507.05307v1",
    "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions",
    "authors": [
      "Xuanqi Gao",
      "Juan Zhai",
      "Shiqing Ma",
      "Siyi Xie",
      "Chao Shen"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into browser extensions has revolutionized web browsing, enabling sophisticated functionalities like content summarization, intelligent translation, and context-aware writing assistance. However, these AI-powered extensions introduce unprecedented challenges in testing and reliability assurance. Traditional browser extension testing approaches fail to address the non-deterministic behavior, context-sensitivity, and complex web environment integration inherent to LLM-powered extensions. Similarly, existing LLM testing methodologies operate in isolation from browser-specific contexts, creating a critical gap in effective evaluation frameworks. To bridge this gap, we present ASSURE, a modular automated testing framework specifically designed for AI-powered browser extensions. ASSURE comprises three principal components: (1) a modular test case generation engine that supports plugin-based extension of testing scenarios, (2) an automated execution framework that orchestrates the complex interactions between web content, extension processing, and AI model behavior, and (3) a configurable validation pipeline that systematically evaluates behavioral consistency and security invariants rather than relying on exact output matching. Our evaluation across six widely-used AI browser extensions demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning security vulnerabilities, metamorphic relation violations, and content alignment problems. ASSURE achieves 6.4x improved testing throughput compared to manual approaches, detecting critical security vulnerabilities within 12.4 minutes on average. This efficiency makes ASSURE practical for integration into development pipelines, offering a comprehensive solution to the unique challenges of testing AI-powered browser extensions.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05307v1",
    "published_date": "2025-07-07 09:11:16 UTC",
    "updated_date": "2025-07-07 09:11:16 UTC"
  },
  {
    "arxiv_id": "2507.04770v1",
    "title": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System",
    "authors": [
      "Toan Nguyen",
      "Tri Le",
      "Quang Nguyen",
      "Anh Nguyen"
    ],
    "abstract": "Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04770v1",
    "published_date": "2025-07-07 08:45:08 UTC",
    "updated_date": "2025-07-07 08:45:08 UTC"
  },
  {
    "arxiv_id": "2507.04769v1",
    "title": "From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection",
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Yeshuang Zhu",
      "Hongyan Fei",
      "Ying Deng",
      "Zhiqiang Yuan",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ],
    "abstract": "Current legal frameworks consider AI-generated works eligible for copyright protection when they meet originality requirements and involve substantial human intellectual input. However, systematic legal standards and reliable evaluation methods for AI art copyrights are lacking. Through comprehensive analysis of legal precedents, we establish three essential criteria for determining distinctive artistic style: stylistic consistency, creative uniqueness, and expressive accuracy. To address these challenges, we introduce ArtBulb, an interpretable and quantifiable framework for AI art copyright judgment that combines a novel style description-based multimodal clustering method with multimodal large language models (MLLMs). We also present AICD, the first benchmark dataset for AI art copyright annotated by artists and legal experts. Experimental results demonstrate that ArtBulb outperforms existing models in both quantitative and qualitative evaluations. Our work aims to bridge the gap between the legal and technological communities and bring greater attention to the societal issue of AI art copyrights.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04769v1",
    "published_date": "2025-07-07 08:45:08 UTC",
    "updated_date": "2025-07-07 08:45:08 UTC"
  },
  {
    "arxiv_id": "2507.04766v1",
    "title": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems",
    "authors": [
      "Yiming Zhang",
      "Yingfan Ma",
      "Yanmei Gu",
      "Zhengkai Yang",
      "Yihong Zhuang",
      "Feng Wang",
      "Zenan Huang",
      "Yuanyuan Wang",
      "Chao Huang",
      "Bowen Song",
      "Cheng Lin",
      "Junbo Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04766v1",
    "published_date": "2025-07-07 08:43:56 UTC",
    "updated_date": "2025-07-07 08:43:56 UTC"
  },
  {
    "arxiv_id": "2507.04756v2",
    "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering",
    "authors": [
      "Hang Lv",
      "Sheng Liang",
      "Hao Wang",
      "Hongchao Gu",
      "Yaxiong Wu",
      "Wei Guo",
      "Defu Lian",
      "Yong Liu",
      "Enhong Chen"
    ],
    "abstract": "Personalized text generation has become crucial for adapting language models to diverse and evolving users' personal context across cultural, temporal, and contextual dimensions. While existing methods often rely on centralized fine-tuning or static preference alignment, they struggle to achieve real-time adaptation under resource constraints inherent to personal devices. This limitation creates a dilemma: large cloud-based models lack access to localized user-specific information, while small on-device models cannot match the generation quality of their cloud counterparts. To address this dichotomy, we present CoSteer, a novel collaborative framework that enables decoding-time personalization through localized delta steering. Our key insight lies in leveraging the logits difference between personal context-aware and -agnostic outputs from local small models as steering signals for cloud-based LLMs. Specifically, we formulate token-level optimization as an online learning problem, where local delta vectors dynamically adjust the remote LLM's logits within the on-device environment. This approach preserves privacy by transmitting only the final steered tokens rather than raw data or intermediate vectors, while maintaining cloud-based LLMs' general capabilities without fine-tuning. Through comprehensive experiments on various personalized generation tasks, we demonstrate that CoSteer effectively assists LLMs in generating personalized content by leveraging locally stored user profiles and histories, ensuring privacy preservation through on-device data processing while maintaining acceptable computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04756v2",
    "published_date": "2025-07-07 08:32:29 UTC",
    "updated_date": "2025-09-29 09:12:58 UTC"
  },
  {
    "arxiv_id": "2507.04752v1",
    "title": "Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions",
    "authors": [
      "Shuo Yang",
      "Xinran Zheng",
      "Xinchen Zhang",
      "Jinfeng Xu",
      "Jinze Li",
      "Donglin Xie",
      "Weicai Long",
      "Edith C. H. Ngai"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized various fields with their exceptional capabilities in understanding, processing, and generating human-like text. This paper investigates the potential of LLMs in advancing Network Intrusion Detection Systems (NIDS), analyzing current challenges, methodologies, and future opportunities. It begins by establishing a foundational understanding of NIDS and LLMs, exploring the enabling technologies that bridge the gap between intelligent and cognitive systems in AI-driven NIDS. While Intelligent NIDS leverage machine learning and deep learning to detect threats based on learned patterns, they often lack contextual awareness and explainability. In contrast, Cognitive NIDS integrate LLMs to process both structured and unstructured security data, enabling deeper contextual reasoning, explainable decision-making, and automated response for intrusion behaviors. Practical implementations are then detailed, highlighting LLMs as processors, detectors, and explainers within a comprehensive AI-driven NIDS pipeline. Furthermore, the concept of an LLM-centered Controller is proposed, emphasizing its potential to coordinate intrusion detection workflows, optimizing tool collaboration and system performance. Finally, this paper identifies critical challenges and opportunities, aiming to foster innovation in developing reliable, adaptive, and explainable NIDS. By presenting the transformative potential of LLMs, this paper seeks to inspire advancement in next-generation network security systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04752v1",
    "published_date": "2025-07-07 08:28:07 UTC",
    "updated_date": "2025-07-07 08:28:07 UTC"
  },
  {
    "arxiv_id": "2507.04750v2",
    "title": "MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry",
    "authors": [
      "Zicheng Lin",
      "Xiaoqiang Li",
      "Yichao Wang",
      "Chuang Zhu"
    ],
    "abstract": "Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep learning applications face significant hurdles. A critical gap exists: the lack of comprehensive evaluation of how diverse optical flow models perform specifically on PIV data, largely due to limitations in available datasets and the absence of a standardized benchmark. This prevents fair comparison and hinders progress. To address this, our primary contribution is a novel, large-scale synthetic PIV benchmark dataset generated from diverse CFD simulations (JHTDB and Blasius). It features unprecedented variety in particle densities, flow velocities, and continuous motion, enabling, for the first time, a standardized and rigorous evaluation of various optical flow and PIV algorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a new deep network architecture leveraging multi-frame temporal information and multiple cost volumes, specifically designed for PIV's sparse nature. Our comprehensive benchmark evaluation, the first of its kind, reveals significant performance variations among adapted optical flow models and demonstrates that MCFormer significantly outperforms existing methods, achieving the lowest overall normalized endpoint error (NEPE). This work provides both a foundational benchmark resource essential for future PIV research and a state-of-the-art method tailored for PIV challenges. We make our benchmark dataset and code publicly available to foster future research in this area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of optical flow models for PIV. Introduces MCFormer architecture with multi-frame temporal processing and multiple cost volumes. Includes large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations. Code and dataset will be made publicly available",
    "pdf_url": "https://arxiv.org/pdf/2507.04750v2",
    "published_date": "2025-07-07 08:26:18 UTC",
    "updated_date": "2025-07-10 02:40:29 UTC"
  },
  {
    "arxiv_id": "2507.04748v1",
    "title": "LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction",
    "authors": [
      "Sungmin Lee",
      "Minju Kang",
      "Joonhee Lee",
      "Seungyong Lee",
      "Dongju Kim",
      "Jingi Hong",
      "Jun Shin",
      "Pei Zhang",
      "JeongGil Ko"
    ],
    "abstract": "Question-answering (QA) interfaces powered by large language models (LLMs) present a promising direction for improving interactivity with HVAC system insights, particularly for non-expert users. However, enabling accurate, real-time, and context-aware interactions with HVAC systems introduces unique challenges, including the integration of frequently updated sensor data, domain-specific knowledge grounding, and coherent multi-stage reasoning. In this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to translate high-level user queries into structured execution instructions, and an Agent that performs SQL-based data retrieval, statistical processing, and final response generation. To address HVAC-specific challenges, JARVIS integrates (1) an adaptive context injection strategy for efficient HVAC and deployment-specific information integration, (2) a parameterized SQL builder and executor to improve data access reliability, and (3) a bottom-up planning scheme to ensure consistency across multi-stage response generation. We evaluate JARVIS using real-world data collected from a commercial HVAC system and a ground truth QA dataset curated by HVAC experts to demonstrate its effectiveness in delivering accurate and interpretable responses across diverse queries. Results show that JARVIS consistently outperforms baseline and ablation variants in both automated and user-centered assessments, achieving high response quality and accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04748v1",
    "published_date": "2025-07-07 08:19:17 UTC",
    "updated_date": "2025-07-07 08:19:17 UTC"
  },
  {
    "arxiv_id": "2507.05306v2",
    "title": "Enjoying Non-linearity in Multinomial Logistic Bandits",
    "authors": [
      "Pierre Boudart",
      "Pierre Gaillard",
      "Alessandro Rudi"
    ],
    "abstract": "We consider the multinomial logistic bandit problem, a variant of where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $κ_* \\geq 1$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\\smash{O(d\\sqrt{T})}$ to $\\smash{O(d\\sqrt{T/κ_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $κ_*$ to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \\smash{\\widetilde{\\mathcal{O}}( R d \\sqrt{{KT}/{κ_*}})} $, where $R$ is the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \\smash{\\widetilde{\\mathcal{O}}( RdK \\sqrt{T} )} $. Moreover, we provide a $\\smash{ Ω(Rd\\sqrt{KT/κ_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $κ_*$ is optimal.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05306v2",
    "published_date": "2025-07-07 08:18:25 UTC",
    "updated_date": "2025-10-08 15:15:45 UTC"
  },
  {
    "arxiv_id": "2507.04742v2",
    "title": "Activation Steering for Chain-of-Thought Compression",
    "authors": [
      "Seyedarmin Azizi",
      "Erfan Baghaei Potraghloo",
      "Massoud Pedram"
    ],
    "abstract": "Large language models (LLMs) excel at complex reasoning when they include intermediate steps, known as \"chains of thought\" (CoTs). However, these rationales are often overly verbose, even for simple problems, leading to wasted context, increased latency, and higher energy consumption. We observe that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct regions in the model's residual-stream activation space. By extracting and injecting a \"steering vector\" to transition between these modes, we can reliably shift generation toward more concise reasoning, effectively compressing CoTs without retraining. We formalize this approach as Activation-Steered Compression (ASC), an inference-time technique that shortens reasoning traces by directly modifying hidden representations. In addition, we provide a theoretical analysis of the impact of ASC on the output distribution, derived from a closed-form KL-divergence-bounded constraint to regulate steering strength. Using only 100 paired verbose and concise examples, ASC achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets, while maintaining accuracy across 7B, 8B, and 32B parameter models. As a training-free method, ASC introduces negligible runtime overhead and, on MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock time on an 8B model. This makes ASC a practical and efficient tool for streamlining the deployment of reasoning-capable LLMs in latency- or cost-sensitive settings. The code is available at: https://github.com/ArminAzizi98/ASC",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04742v2",
    "published_date": "2025-07-07 08:16:54 UTC",
    "updated_date": "2025-07-08 02:54:20 UTC"
  },
  {
    "arxiv_id": "2507.08835v1",
    "title": "Representation learning with a transformer by contrastive learning for money laundering detection",
    "authors": [
      "Harold Guéneau",
      "Alain Celisse",
      "Pascal Delange"
    ],
    "abstract": "The present work tackles the money laundering detection problem. A new procedure is introduced which exploits structured time series of both qualitative and quantitative data by means of a transformer neural network. The first step of this procedure aims at learning representations of time series through contrastive learning (without any labels). The second step leverages these representations to generate a money laundering scoring of all observations. A two-thresholds approach is then introduced, which ensures a controlled false-positive rate by means of the Benjamini-Hochberg (BH) procedure. Experiments confirm that the transformer is able to produce general representations that succeed in exploiting money laundering patterns with minimal supervision from domain experts. It also illustrates the higher ability of the new procedure for detecting nonfraudsters as well as fraudsters, while keeping the false positive rate under control. This greatly contrasts with rule-based procedures or the ones based on LSTM architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "q-fin.RM",
      "q-fin.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.08835v1",
    "published_date": "2025-07-07 08:16:11 UTC",
    "updated_date": "2025-07-07 08:16:11 UTC"
  },
  {
    "arxiv_id": "2507.04738v1",
    "title": "Word stress in self-supervised speech models: A cross-linguistic comparison",
    "authors": [
      "Martijn Bentum",
      "Louis ten Bosch",
      "Tomas O. Lentz"
    ],
    "abstract": "In this paper we study word stress representations learned by self-supervised speech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M representations of word stress for five different languages: Three languages with variable or lexical stress (Dutch, English and German) and two languages with fixed or demarcative stress (Hungarian and Polish). We train diagnostic stress classifiers on S3M embeddings and show that they can distinguish between stressed and unstressed syllables in read-aloud short sentences with high accuracy. We also tested language-specificity effects of S3M word stress. The results indicate that the word stress representations are language-specific, with a greater difference between the set of variable versus the set of fixed stressed languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04738v1",
    "published_date": "2025-07-07 08:10:26 UTC",
    "updated_date": "2025-07-07 08:10:26 UTC"
  },
  {
    "arxiv_id": "2507.04736v1",
    "title": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning",
    "authors": [
      "Zhirong Chen",
      "Kaiyan Chang",
      "Zhuolin Li",
      "Xinyang He",
      "Chujie Chen",
      "Cangyuan Li",
      "Mengdi Wang",
      "Haobo Xu",
      "Yinhe Han",
      "Ying Wang"
    ],
    "abstract": "Large Language Models (LLMs) show significant potential for automating Register-Transfer Level (RTL) code generation. However, current approaches face a critical challenge: they can not simultaneously optimize for functional correctness and hardware quality (Power, Performance, Area - PPA). Methods based on supervised fine-tuning often generate functionally correct but PPA-suboptimal code, lacking mechanisms to learn optimization principles. In contrast, post-processing techniques that attempt to improve PPA metrics after generation are often inefficient because they operate externally without updating the LLM's parameters, thus failing to enhance the model's intrinsic design capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven reinforcement learning framework to train LLMs to generate RTL code that achieves both functional correctness and optimized PPA metrics. ChipSeek-R1 employs a hierarchical reward system, which incorporates direct feedback on syntax, functional correctness (from simulators) and PPA metrics (from synthesis tools) during reinforcement learning. This enables the model to learn complex hardware design trade-offs via trial-and-error, generating RTL code that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1 generated 27 RTL designs surpassing the PPA metrics of the original human-written code. Our findings demonstrate the effectiveness of integrating toolchain feedback into LLM training and highlight the potential for reinforcement learning to enable automated generation of human-surpassing RTL code. We open-source our code in anonymous github.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04736v1",
    "published_date": "2025-07-07 08:08:20 UTC",
    "updated_date": "2025-07-07 08:08:20 UTC"
  },
  {
    "arxiv_id": "2507.05305v1",
    "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools",
    "authors": [
      "Lorenzo Lee Solano",
      "Charles Koutcheme",
      "Juho Leinonen",
      "Alexandra Vassar",
      "Jake Renzella"
    ],
    "abstract": "Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, 3 tables, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2507.05305v1",
    "published_date": "2025-07-07 08:03:49 UTC",
    "updated_date": "2025-07-07 08:03:49 UTC"
  },
  {
    "arxiv_id": "2507.04726v2",
    "title": "Backdoors in Conditional Diffusion: Threats to Responsible Synthetic Data Pipelines",
    "authors": [
      "Raz Lapid",
      "Almog Dubin"
    ],
    "abstract": "Text-to-image diffusion models achieve high-fidelity image generation from natural language prompts. ControlNets extend these models by enabling conditioning on structural inputs (e.g., edge maps, depth, pose), providing fine-grained control over outputs. Yet their reliance on large, publicly scraped datasets and community fine-tuning makes them vulnerable to data poisoning. We introduce a model-poisoning attack that embeds a covert backdoor into a ControlNet, causing it to produce attacker-specified content when exposed to visual triggers, without textual prompts. Experiments show that poisoning only 1% of the fine-tuning corpus yields a 90-98% attack success rate, while 5% further strengthens the backdoor, all while preserving normal generation quality. To mitigate this risk, we propose clean fine-tuning (CFT): freezing the diffusion backbone and fine-tuning only the ControlNet on a sanitized dataset with a reduced learning rate. CFT lowers attack success rates on held-out data. These results expose a critical security weakness in open-source, ControlNet-guided diffusion pipelines and demonstrate that CFT offers a practical defense for responsible synthetic-data pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at RDS @ AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2507.04726v2",
    "published_date": "2025-07-07 07:36:20 UTC",
    "updated_date": "2025-11-23 13:37:24 UTC"
  },
  {
    "arxiv_id": "2507.05304v1",
    "title": "Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes",
    "authors": [
      "Saqib Nazir",
      "Olivier Lézoray",
      "Sébastien Bougleux"
    ],
    "abstract": "3D meshes are fundamental data representations for capturing complex geometric shapes in computer vision and graphics applications. While Convolutional Neural Networks (CNNs) have excelled in structured data like images, extending them to irregular 3D meshes is challenging due to the non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a solution by applying convolutions to graph-structured data, but many existing methods rely on isotropic filters or spectral decomposition, limiting their ability to capture both local and global mesh features. In this paper, we introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework that uses anisotropic convolution layers to effectively learn both global and local features directly in the spatial domain. Unlike previous approaches that convert meshes into intermediate representations like voxel grids or point clouds, our method preserves the original polygonal mesh format throughout the reconstruction process, enabling more accurate shape reconstruction. Our architecture features a multi-scale encoder-decoder structure, where separate global and local pathways capture both large-scale geometric structures and fine-grained local details. Extensive experiments on the COMA dataset containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of reconstruction accuracy.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05304v1",
    "published_date": "2025-07-07 07:36:03 UTC",
    "updated_date": "2025-07-07 07:36:03 UTC"
  },
  {
    "arxiv_id": "2507.04724v2",
    "title": "Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems",
    "authors": [
      "Yizhe Xie",
      "Congcong Zhu",
      "Xinyue Zhang",
      "Tianqing Zhu",
      "Dayong Ye",
      "Minghao Wang",
      "Chi Liu"
    ],
    "abstract": "Multi-agent systems powered by Large Language Models (LLM-MAS) have demonstrated remarkable capabilities in collaborative problem-solving. However, their deployment also introduces new security risks. Existing research on LLM-based agents has primarily examined single-agent scenarios, while the security of multi-agent systems remains largely unexplored. To address this gap, we present a systematic study of intention-hiding threats in LLM-MAS. We design four representative attack paradigms that subtly disrupt task completion while maintaining a high degree of stealth, and evaluate them under centralized, decentralized, and layered communication structures. Experimental results show that these attacks are highly disruptive and can easily evade existing defense mechanisms. To counter these threats, we propose AgentXposed, a psychology-inspired detection framework. AgentXposed draws on the HEXACO personality model, which characterizes agents through psychological trait dimensions, and the Reid interrogation technique, a structured method for eliciting concealed intentions. By combining progressive questionnaire probing with behavior-based inter-agent monitoring, the framework enables the proactive identification of malicious agents before harmful actions are carried out. Extensive experiments across six datasets against both our proposed attacks and two baseline threats demonstrate that AgentXposed effectively detects diverse forms of malicious behavior, achieving strong robustness across multiple communication settings.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04724v2",
    "published_date": "2025-07-07 07:34:34 UTC",
    "updated_date": "2025-10-06 04:38:52 UTC"
  },
  {
    "arxiv_id": "2507.04722v2",
    "title": "LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Recommender Systems",
    "authors": [
      "Jinzhi Wang",
      "Bin Li",
      "Qingke Peng",
      "Haozhou Li",
      "Zeyuan Zeng",
      "Ruimeng Li",
      "Kaixuan Yang",
      "Jiangbo Zhang",
      "Biyi Zhou",
      "Yaoying Wang"
    ],
    "abstract": "Conversational recommender systems (CRSs) often suffer from an extreme long-tail distribution of dialogue data, causing a strong bias toward head-frequency blockbusters that sacrifices diversity and exacerbates the cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL corpus show that only 10% of head movies account for nearly half of all mentions, whereas about 70% of tail movies receive merely 26% of the attention. This imbalance gives rise to three critical challenges: head over-fitting, body representation drift, and tail sparsity. To address these issues, we propose LumiCRS, an end-to-end framework that mitigates long-tail imbalance through three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss (ACFL) that dynamically adjusts class weights and focusing factors to curb head over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail Recommendation, which selects semantic, affective, and contextual prototypes to guide clustering and stabilize body and tail representations; and (iii) a GPT-4o-driven prototype-guided dialogue augmentation module that automatically generates diverse long-tail conversational snippets to alleviate tail sparsity and distribution shift. Together, these strategies enable LumiCRS to markedly improve recommendation accuracy, diversity, and fairness: on the REDIAL and INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over fifteen strong baselines, while human evaluations confirm superior fluency, informativeness, and long-tail relevance. These results demonstrate the effectiveness of multi-layer collaboration in building an efficient and fair long-tail conversational recommender.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04722v2",
    "published_date": "2025-07-07 07:33:00 UTC",
    "updated_date": "2025-07-20 01:31:22 UTC"
  },
  {
    "arxiv_id": "2507.06256v1",
    "title": "Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World",
    "authors": [
      "Vinu Sankar Sadasivan",
      "Soheil Feizi",
      "Rajiv Mathews",
      "Lun Wang"
    ],
    "abstract": "This paper investigates the real-world vulnerabilities of audio-based large language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an adversary can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as eliciting responses to wake-keywords (e.g., \"Hey Qwen\"), or triggering harmful behaviors (e.g. \"Change my calendar event\"). Subsequently, we show that playing adversarial background noise during user interaction with the ALLMs can significantly degrade the response quality. Crucially, our research illustrates the scalability of these attacks to real-world scenarios, impacting other innocent users when these adversarial noises are played through the air. Further, we discuss the transferrability of the attack, and potential defensive measures.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.06256v1",
    "published_date": "2025-07-07 07:29:52 UTC",
    "updated_date": "2025-07-07 07:29:52 UTC"
  },
  {
    "arxiv_id": "2507.04719v1",
    "title": "Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs",
    "authors": [
      "Roozbeh Yousefzadeh",
      "Xuenan Cao"
    ],
    "abstract": "This position paper provides a critical but constructive discussion of current practices in benchmarking and evaluative practices in the field of formal reasoning and automated theorem proving. We take the position that open code, open data, and benchmarks that are complete and error-free will accelerate progress in this field. We identify practices that create barriers to contributing to this field and suggest ways to remove them. We also discuss some of the practices that might produce misleading evaluative information. We aim to create discussions that bring together people from various groups contributing to automated theorem proving, autoformalization, and informal reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04719v1",
    "published_date": "2025-07-07 07:27:45 UTC",
    "updated_date": "2025-07-07 07:27:45 UTC"
  },
  {
    "arxiv_id": "2507.04710v1",
    "title": "Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model",
    "authors": [
      "Anbang Wang",
      "Marawan Elbatel",
      "Keyuan Liu",
      "Lizhuo Lin",
      "Meng Lan",
      "Yanqi Yang",
      "Xiaomeng Li"
    ],
    "abstract": "Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to inter-observer variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in human-centric vision tasks, and (2) a novel geometric loss function that improves the model's capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: https://github.com/xmed-lab/GeoSapiens.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04710v1",
    "published_date": "2025-07-07 07:01:44 UTC",
    "updated_date": "2025-07-07 07:01:44 UTC"
  },
  {
    "arxiv_id": "2507.04706v1",
    "title": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization",
    "authors": [
      "Kai Yang",
      "Zelin Zhu",
      "Chengtao Jian",
      "Hui Ma",
      "Shengjie Zhao",
      "Xiaozhou Ye",
      "Ye Ouyang"
    ],
    "abstract": "Urban general intelligence (UGI) refers to the capacity of AI systems to autonomously perceive, reason, and act within dynamic and complex urban environments. In this paper, we introduce UrbanMind, a tool-enhanced retrieval-augmented generation (RAG) framework designed to facilitate UGI. Central to UrbanMind is a novel architecture based on Continual Retrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates domain-specific knowledge and evolving urban data to support long-term adaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel optimization framework, where different layers are treated as interdependent sub-problems. Each layer has distinct objectives and can be optimized either independently or jointly through a hierarchical learning process. The framework is highly flexible, supporting both end-to-end training and partial layer-wise optimization based on resource or deployment constraints. To remain adaptive under data drift, it is further integrated with an incremental corpus updating mechanism. Evaluations on real-world urban tasks of a variety of complexity verify the effectiveness of the proposed framework. This work presents a promising step toward the realization of general-purpose LLM agents in future urban environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04706v1",
    "published_date": "2025-07-07 06:57:34 UTC",
    "updated_date": "2025-07-07 06:57:34 UTC"
  },
  {
    "arxiv_id": "2507.04704v1",
    "title": "SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes",
    "authors": [
      "Zhenglun Kong",
      "Mufan Qiu",
      "John Boesen",
      "Xiang Lin",
      "Sukwon Yun",
      "Tianlong Chen",
      "Manolis Kellis",
      "Marinka Zitnik"
    ],
    "abstract": "Understanding how cellular morphology, gene expression, and spatial organization jointly shape tissue function is a central challenge in biology. Image-based spatial transcriptomics technologies now provide high-resolution measurements of cell images and gene expression profiles, but machine learning methods typically analyze these modalities in isolation or at limited resolution. We address the problem of learning unified, spatially aware representations that integrate cell morphology, gene expression, and spatial context across biological scales. This requires models that can operate at single-cell resolution, reason across spatial neighborhoods, and generalize to whole-slide tissue organization. Here, we introduce SPATIA, a multi-scale generative and predictive model for spatial transcriptomics. SPATIA learns cell-level embeddings by fusing image-derived morphological tokens and transcriptomic vector tokens using cross-attention and then aggregates them at niche and tissue levels using transformer modules to capture spatial dependencies. SPATIA incorporates token merging in its generative diffusion decoder to synthesize high-resolution cell images conditioned on gene expression. We assembled a multi-scale dataset consisting of 17 million cell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs across 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA against 13 existing models across 12 individual tasks, which span several categories including cell annotation, cell clustering, gene imputation, cross-modal prediction, and image generation. SPATIA achieves improved performance over all baselines and generates realistic cell morphologies that reflect transcriptomic perturbations.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04704v1",
    "published_date": "2025-07-07 06:54:02 UTC",
    "updated_date": "2025-07-07 06:54:02 UTC"
  },
  {
    "arxiv_id": "2507.10564v1",
    "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing",
    "authors": [
      "Sameera Bharadwaja H.",
      "Siddhrath Jandial",
      "Shashank S. Agashe",
      "Rajesh Kumar Reddy Moore",
      "Youngkwan Kim"
    ],
    "abstract": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber matching in the context of a semiconductor manufacturing equipment. Traditional TTTM approaches utilize static configuration data or depend on a golden reference which are difficult to obtain in a commercial manufacturing line. Further, existing methods do not extend very well to a heterogeneous setting, where equipment are of different make-and-model, sourced from different equipment vendors. We propose novel TTTM analysis pipelines to overcome these issues. We hypothesize that a mismatched equipment would have higher variance and/or higher number of modes in the data. Our best univariate method achieves a correlation coefficient >0.95 and >0.5 with the variance and number of modes, respectively showing that the proposed methods are effective. Also, the best multivariate method achieves a correlation coefficient >0.75 with the top-performing univariate methods, showing its effectiveness. Finally, we analyze the sensitivity of the multivariate algorithms to the algorithm hyper-parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.10564v1",
    "published_date": "2025-07-07 06:52:01 UTC",
    "updated_date": "2025-07-07 06:52:01 UTC"
  },
  {
    "arxiv_id": "2507.04702v1",
    "title": "Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning",
    "authors": [
      "Feng Yue",
      "Zhaoxing Zhang",
      "Junming Jiao",
      "Zhengyu Liang",
      "Shiwen Cao",
      "Feifei Zhang",
      "Rong Shen"
    ],
    "abstract": "Temporal Video Grounding (TVG), which requires pinpointing relevant temporal segments from video based on language query, has always been a highly challenging task in the field of video understanding. Videos often have a larger volume of information and redundancy than texts or images. Models should present comprehensive understanding of the whole video to accurately retrieve query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large Language Model (Video-MLLM) for the temporal video grounding task via multimodal temporal sensing reinforcement. Specifically, during the preprocessing stage of our pipeline, we employ Self-adaptive Attention Allocation (SAA) method based on frame content variation to efficiently use the MLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is also utilized to strengthen our model's capability to perceive the boundaries of events in the video. In the fine-tuning part of our pipeline, we creatively apply Partial Irrelevance Refusing-based Group Relative Policy Optimization (PIR-GRPO) in TVG area to foster model's temporal reasoning from not only accepting relevant video-query pairs but also refusing irrelevant ones. Experiments demonstrate that our method accomplishes a notable advantage over SOTA solutions by around 3.5% on both the original QVHighlights testbench and its corrected version with more reasonable ground truth annotations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04702v1",
    "published_date": "2025-07-07 06:51:40 UTC",
    "updated_date": "2025-07-07 06:51:40 UTC"
  },
  {
    "arxiv_id": "2507.05302v1",
    "title": "CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection",
    "authors": [
      "Binjia Zhou",
      "Hengrui Lou",
      "Lizhe Chen",
      "Haoyuan Li",
      "Dawei Luo",
      "Shuai Chen",
      "Jie Lei",
      "Zunlei Feng",
      "Yijun Bei"
    ],
    "abstract": "With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05302v1",
    "published_date": "2025-07-07 06:29:57 UTC",
    "updated_date": "2025-07-07 06:29:57 UTC"
  },
  {
    "arxiv_id": "2507.04690v1",
    "title": "Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness",
    "authors": [
      "Hanseon Joo",
      "Hayoung Choi",
      "Ook Lee",
      "Minjong Cheon"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed activation functions with learnable univariate functions, but they exhibit practical limitations, including high computational costs and performance deficits in general classification tasks. In this paper, we propose the Modulation Joint KAN (MJKAN), a novel neural network layer designed to overcome these challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like mechanism with Radial Basis Function (RBF) activations, creating a hybrid architecture that combines the non-linear expressive power of KANs with the efficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's performance across a diverse set of benchmarks, including function regression, image classification (MNIST, CIFAR-10/100), and natural language processing (AG News, SMS Spam). The results demonstrate that MJKAN achieves superior approximation capabilities in function regression tasks, significantly outperforming MLPs, with performance improving as the number of basis functions increases. Conversely, in image and text classification, its performance was competitive with MLPs but revealed a critical dependency on the number of basis functions. We found that a smaller basis size was crucial for better generalization, highlighting that the model's capacity must be carefully tuned to the complexity of the data to prevent overfitting. In conclusion, MJKAN offers a flexible architecture that inherits the theoretical advantages of KANs while improving computational efficiency and practical viability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04690v1",
    "published_date": "2025-07-07 06:13:32 UTC",
    "updated_date": "2025-07-07 06:13:32 UTC"
  },
  {
    "arxiv_id": "2507.04680v2",
    "title": "Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation",
    "authors": [
      "Wenhao Li",
      "Xiu Su",
      "Jingyi Wu",
      "Feng Yang",
      "Yang Liu",
      "Yi Chen",
      "Shan You",
      "Chang Xu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable advancements in numerous areas such as multimedia. However, hallucination issues significantly limit their credibility and application potential. Existing mitigation methods typically rely on external tools or the comparison of multi-round inference, which significantly increase inference time. In this paper, we propose \\textbf{SE}lf-\\textbf{E}volving \\textbf{D}istillation (\\textbf{SEED}), which identifies hallucinations within the inner knowledge of LVLMs, isolates and purges them, and then distills the purified knowledge back into the model, enabling self-evolution. Furthermore, we identified that traditional distillation methods are prone to inducing void spaces in the output space of LVLMs. To address this issue, we propose a Mode-Seeking Evolving approach, which performs distillation to capture the dominant modes of the purified knowledge distribution, thereby avoiding the chaotic results that could emerge from void spaces. Moreover, we introduce a Hallucination Elimination Adapter, which corrects the dark knowledge of the original model by learning purified knowledge. Extensive experiments on multiple benchmarks validate the superiority of our SEED, demonstrating substantial improvements in mitigating hallucinations for representative LVLM models such as LLaVA-1.5 and InternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination evaluation metric POPE-Random improved from 81.3 to 88.3.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "In Figure 2, the correlation coefficient and the scatter plot do not match. I calculated this correlation using two sets of settings. I used the scatter plot from setting A, but accidentally wrote the correlation coefficient, r, from setting B",
    "pdf_url": "https://arxiv.org/pdf/2507.04680v2",
    "published_date": "2025-07-07 05:56:19 UTC",
    "updated_date": "2025-08-19 08:22:48 UTC"
  },
  {
    "arxiv_id": "2507.04673v1",
    "title": "Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message",
    "authors": [
      "Wei Duan",
      "Li Qian"
    ],
    "abstract": "The rise of conversational interfaces has greatly enhanced LLM usability by leveraging dialogue history for sophisticated reasoning. However, this reliance introduces an unexplored attack surface. This paper introduces Trojan Horse Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by forging the model's own past utterances within the conversational history provided to its API. A malicious payload is injected into a model-attributed message, followed by a benign user prompt to trigger harmful content generation. This vulnerability stems from Asymmetric Safety Alignment: models are extensively trained to refuse harmful user requests but lack comparable skepticism towards their own purported conversational history. This implicit trust in its \"past\" creates a high-impact vulnerability. Experimental validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than established user-turn jailbreaking methods. These findings reveal a fundamental flaw in modern conversational AI security, necessitating a paradigm shift from input-level filtering to robust, protocol-level validation of conversational context integrity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04673v1",
    "published_date": "2025-07-07 05:35:21 UTC",
    "updated_date": "2025-07-07 05:35:21 UTC"
  },
  {
    "arxiv_id": "2507.04667v2",
    "title": "What's Making That Sound Right Now? Video-centric Audio-Visual Localization",
    "authors": [
      "Hahyeon Choi",
      "Junhoo Lee",
      "Nojun Kwak"
    ],
    "abstract": "Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ICCV 2025. Project page: https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/",
    "pdf_url": "https://arxiv.org/pdf/2507.04667v2",
    "published_date": "2025-07-07 05:12:34 UTC",
    "updated_date": "2025-07-08 14:46:46 UTC"
  },
  {
    "arxiv_id": "2507.04634v1",
    "title": "LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction",
    "authors": [
      "Yixin Yan",
      "Yang Li",
      "Yuanfan Wang",
      "Xiaozhou Zhou",
      "Beihao Xia",
      "Manjiang Hu",
      "Hongmao Qin"
    ],
    "abstract": "It has been challenging to model the complex temporal-spatial dependencies between agents for trajectory prediction. As each state of an agent is closely related to the states of adjacent time steps, capturing the local temporal dependency is beneficial for prediction, while most studies often overlook it. Besides, learning the high-order motion state attributes is expected to enhance spatial interaction modeling, but it is rarely seen in previous works. To address this, we propose a lightweight framework, LTMSformer, to extract temporal-spatial interaction features for multi-modal trajectory prediction. Specifically, we introduce a Local Trend-Aware Attention mechanism to capture the local temporal dependency by leveraging a convolutional attention mechanism with hierarchical local time boxes. Next, to model the spatial interaction dependency, we build a Motion State Encoder to incorporate high-order motion state attributes, such as acceleration, jerk, heading, etc. To further refine the trajectory prediction, we propose a Lightweight Proposal Refinement Module that leverages Multi-Layer Perceptrons for trajectory embedding and generates the refined trajectories with fewer model parameters. Experiment results on the Argoverse 1 dataset demonstrate that our method outperforms the baseline HiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and the MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68% reduction in model size.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04634v1",
    "published_date": "2025-07-07 03:33:14 UTC",
    "updated_date": "2025-07-07 03:33:14 UTC"
  },
  {
    "arxiv_id": "2507.04632v5",
    "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?",
    "authors": [
      "Yun Qu",
      "Qi Wang",
      "Yixiu Mao",
      "Vincent Tao Hu",
      "Björn Ommer",
      "Xiangyang Ji"
    ],
    "abstract": "Recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs). The optimization process often requires numerous iterations to achieve satisfactory performance, resulting in high computational costs due to the need for frequent prompt evaluations under intensive LLM interactions and repeated policy updates. Appropriate online prompt selection methods reduce iteration steps by prioritizing informative prompts during training, while the pipeline's reliance on exhaustive prompt evaluation and subset selection for optimization still incurs substantial computational overhead due to frequent LLM inference calls. Distinguished from these direct evaluate-then-select schemes, this work investigates iterative approximate evaluation for arbitrary prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions. Technically, MoPPS models each prompt's success rate as a latent variable, performs streaming Bayesian inference, and employs posterior sampling in a constructed multi-armed bandit machine, enabling sample efficient and adaptive prompt selection. Extensive experiments across mathematics, planning, and vision-based geometry tasks show that MoPPS reliably predicts prompt difficulty and accelerates training with significantly reduced LLM rollouts. Our code is available at https://github.com/thu-rllab/MoPPS.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04632v5",
    "published_date": "2025-07-07 03:20:52 UTC",
    "updated_date": "2026-01-10 07:32:30 UTC"
  },
  {
    "arxiv_id": "2507.04631v1",
    "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
    "authors": [
      "Yun Wang",
      "Longguang Wang",
      "Chenghao Zhang",
      "Yongjian Zhang",
      "Zhanjie Zhang",
      "Ao Ma",
      "Chenyou Fan",
      "Tin Lun Lam",
      "Junjie Hu"
    ],
    "abstract": "Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04631v1",
    "published_date": "2025-07-07 03:19:04 UTC",
    "updated_date": "2025-07-07 03:19:04 UTC"
  },
  {
    "arxiv_id": "2507.04625v1",
    "title": "Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs",
    "authors": [
      "Swayamjit Saha"
    ],
    "abstract": "Large Language Models (LLMs) are powerful yet prone to generating factual errors, commonly referred to as hallucinations. We present a lightweight, interpretable framework for knowledge-aware self-correction of LLM outputs using structured memory graphs based on RDF triples. Without retraining or fine-tuning, our method post-processes model outputs and corrects factual inconsistencies via external semantic memory. We demonstrate the approach using DistilGPT-2 and show promising results on simple factual prompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.04625v1",
    "published_date": "2025-07-07 02:55:12 UTC",
    "updated_date": "2025-07-07 02:55:12 UTC"
  },
  {
    "arxiv_id": "2507.04623v1",
    "title": "Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation",
    "authors": [
      "Jinpeng Chen",
      "Jianxiang He",
      "Huan Li",
      "Senzhang Wang",
      "Yuan Cao",
      "Kaimin Wei",
      "Zhenye Yang",
      "Ye Ji"
    ],
    "abstract": "Session-based Recommendation (SBR) aims to predict the next item a user will likely engage with, using their interaction sequence within an anonymous session. Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights. Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance. Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features. To address these challenges, we propose a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations, called HIPHOP. First, we introduce a pluggable embedding module based on large language models (LLMs) to generate high-quality semantic representations, enhancing item embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item transition relationships and incorporates a dynamic multi-intent capturing module to address users' diverse interests within a session. Additionally, we design a hierarchical inter-session similarity learning module, guided by user intent, to capture global and local session relationships, effectively exploring users' long-term and short-term interests. To mitigate noise, an intent-guided denoising strategy is applied during inter-session learning. Finally, we enhance the model's discriminative capability by using contrastive learning to optimize session representations. Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality. Our code is available: https://github.com/hjx159/HIPHOP.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04623v1",
    "published_date": "2025-07-07 02:50:04 UTC",
    "updated_date": "2025-07-07 02:50:04 UTC"
  },
  {
    "arxiv_id": "2507.04621v1",
    "title": "Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences",
    "authors": [
      "Yusong Zhang",
      "Yuxuan Sun",
      "Lei Guo",
      "Wei Chen",
      "Bo Ai",
      "Deniz Gunduz"
    ],
    "abstract": "6G networks promise revolutionary immersive communication experiences including augmented reality (AR), virtual reality (VR), and holographic communications. These applications demand high-dimensional multimodal data transmission and intelligent data processing in real-time, which is extremely challenging over resource-limited wireless communication systems. Moreover, a joint understanding of the environment, context, and user intent is essential to deliver task-relevant content effectively. This article presents a novel multimodal large language model (MLLM) integrated semantic communications framework, termed MLLM-SC, which fully leverages reasoning and generative capabilities of pre-trained foundation models for context-aware and task-oriented wireless communication. The MLLM-SC framework adopts a device-edge collaborative architecture. At the edge, MLLM-empowered semantic guidance module analyzes multimodal inputs, user intents, and channel conditions to generate importance-aware attention maps prioritizing semantically critical information. An importance-aware semantic encoder and a resource-adaptive semantic decoder are jointly designed and optimized, which can utilize the semantic guidance for adaptive bandwidth allocation and high-quality content reconstruction or generation. Extensive case studies on visual question answering for AR/VR applications and diffusion-driven image generation validate the effectiveness of MLLM-SC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2507.04621v1",
    "published_date": "2025-07-07 02:42:35 UTC",
    "updated_date": "2025-07-07 02:42:35 UTC"
  },
  {
    "arxiv_id": "2507.04619v1",
    "title": "Information-Guided Diffusion Sampling for Dataset Distillation",
    "authors": [
      "Linfeng Ye",
      "Shayan Mohajer Hamidi",
      "Guang Li",
      "Takahiro Ogawa",
      "Miki Haseyama",
      "Konstantinos N. Plataniotis"
    ],
    "abstract": "Dataset distillation aims to create a compact dataset that retains essential information while maintaining model performance. Diffusion models (DMs) have shown promise for this task but struggle in low images-per-class (IPC) settings, where generated samples lack diversity. In this paper, we address this issue from an information-theoretic perspective by identifying two key types of information that a distilled dataset must preserve: ($i$) prototype information $\\mathrm{I}(X;Y)$, which captures label-relevant features; and ($ii$) contextual information $\\mathrm{H}(X | Y)$, which preserves intra-class variability. Here, $(X,Y)$ represents the pair of random variables corresponding to the input data and its ground truth label, respectively. Observing that the required contextual information scales with IPC, we propose maximizing $\\mathrm{I}(X;Y) + β\\mathrm{H}(X | Y)$ during the DM sampling process, where $β$ is IPC-dependent. Since directly computing $\\mathrm{I}(X;Y)$ and $\\mathrm{H}(X | Y)$ is intractable, we develop variational estimations to tightly lower-bound these quantities via a data-driven approach. Our approach, information-guided diffusion sampling (IGDS), seamlessly integrates with diffusion models and improves dataset distillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet subsets show that IGDS significantly outperforms existing methods, particularly in low-IPC regimes. The code will be released upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04619v1",
    "published_date": "2025-07-07 02:27:08 UTC",
    "updated_date": "2025-07-07 02:27:08 UTC"
  },
  {
    "arxiv_id": "2507.16826v1",
    "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models",
    "authors": [
      "Qikai Wei",
      "Huansheng Ning",
      "Chunlong Han",
      "Jianguo Ding"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising paradigm for enhancing the accuracy and factual consistency of content generated by large language models (LLMs). However, existing RAG studies primarily focus on retrieving isolated segments using similarity-based matching methods, while overlooking the intrinsic connections between them. This limitation hampers performance in RAG tasks. To address this, we propose QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval Augmented Generation. First, we design prompt templates and employ general-purpose LLMs to extract entities and relations, thereby generating a knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a multi-path subgraph construction strategy that incorporates one-hop relations, multi-hop relations, and importance-based relations, aiming to improve the semantic relevance between the retrieved documents and the user query. Subsequently, we designed a query-aware attention reward model that scores subgraph triples based on their semantic relevance to the query. Then, we select the highest score subgraph and enrich subgraph with additional triples from other subgraphs that are highly semantically relevant to the query. Finally, the entities, relations, and triples within the updated subgraph are utilised to expand the original query, thereby enhancing its semantic representation and improving the quality of LLMs' generation. We evaluate QMKGF on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA dataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the BGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%). Experimental results demonstrate the effectiveness and superiority of the QMKGF approach.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.16826v1",
    "published_date": "2025-07-07 02:22:54 UTC",
    "updated_date": "2025-07-07 02:22:54 UTC"
  },
  {
    "arxiv_id": "2507.04613v1",
    "title": "HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction",
    "authors": [
      "Jiaqi Cui",
      "Lu Wen",
      "Yuchen Fei",
      "Bo Liu",
      "Luping Zhou",
      "Dinggang Shen",
      "Yan Wang"
    ],
    "abstract": "Survival prediction using whole-slide images (WSIs) is crucial in cancer re-search. Despite notable success, existing approaches are limited by their reliance on sparse slide-level labels, which hinders the learning of discriminative repre-sentations from gigapixel WSIs. Recently, vision language (VL) models, which incorporate additional language supervision, have emerged as a promising solu-tion. However, VL-based survival prediction remains largely unexplored due to two key challenges. First, current methods often rely on only one simple lan-guage prompt and basic cosine similarity, which fails to learn fine-grained associ-ations between multi-faceted linguistic information and visual features within WSI, resulting in inadequate vision-language alignment. Second, these methods primarily exploit patch-level information, overlooking the intrinsic hierarchy of WSIs and their interactions, causing ineffective modeling of hierarchical interac-tions. To tackle these problems, we propose a novel Hierarchical vision-Language collaboration (HiLa) framework for improved survival prediction. Specifically, HiLa employs pretrained feature extractors to generate hierarchical visual features from WSIs at both patch and region levels. At each level, a series of language prompts describing various survival-related attributes are constructed and aligned with visual features via Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive learning of discriminative visual features cor-responding to different survival-related attributes from prompts, thereby improv-ing vision-language alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical cooperation by promoting interactions and consistency be-tween patch and region levels. Experiments on three TCGA datasets demonstrate our SOTA performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04613v1",
    "published_date": "2025-07-07 02:06:25 UTC",
    "updated_date": "2025-07-07 02:06:25 UTC"
  },
  {
    "arxiv_id": "2507.04610v1",
    "title": "any4: Learned 4-bit Numeric Representation for LLMs",
    "authors": [
      "Mostafa Elhoushi",
      "Jeff Johnson"
    ],
    "abstract": "We present any4, a learned 4-bit weight quantization solution for large language models (LLMs) providing arbitrary numeric representations without requiring pre-processing of weights or activations. any4 yields higher accuracy compared to other related 4-bit numeric representation types: int4, fp4 and nf4, as evaluated on a range of model sizes, generations and families (Llama 2, Llama 3, Mistral and Mixtral). While any4 does not require preprocessing of weights or activations, it is also competitive with orthogonal techniques that require such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3 and any2 and show competitiveness at lower bits. Additionally, we show that we can calibrate using a single curated diverse sample rather than hundreds of samples from a dataset as done in most quantization approaches. We also open source tinygemm, a latency optimized GPU matrix multiplication library for LLMs, that implements any4 using a GPU-efficient lookup table strategy along with other common quantization methods. We open source our code at https://github.com/facebookresearch/any4 .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.04610v1",
    "published_date": "2025-07-07 01:59:47 UTC",
    "updated_date": "2025-07-07 01:59:47 UTC"
  },
  {
    "arxiv_id": "2507.04607v3",
    "title": "PRIME: Large Language Model Personalization with Cognitive Dual-Memory and Personalized Thought Process",
    "authors": [
      "Xinliang Frederick Zhang",
      "Nick Beauchamp",
      "Lu Wang"
    ],
    "abstract": "Large language model (LLM) personalization aims to align model outputs with individuals' unique preferences and opinions. While recent efforts have implemented various personalization methods, a unified theoretical framework that can systematically understand the drivers of effective personalization is still lacking. In this work, we integrate the well-established cognitive dual-memory model into LLM personalization, by mirroring episodic memory to historical user engagements and semantic memory to long-term, evolving user beliefs. Specifically, we systematically investigate memory instantiations and introduce a unified framework, PRIME, using episodic and semantic memory mechanisms. We further augment PRIME with a novel personalized thinking capability inspired by the slow thinking strategy. Moreover, recognizing the absence of suitable benchmarks, we introduce a dataset using Change My View (CMV) from Reddit, specifically designed to evaluate long-context personalization. Extensive experiments validate PRIME's effectiveness across both long- and short-context scenarios. Further analysis confirms that PRIME effectively captures dynamic personalization beyond mere popularity biases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP'25 Main",
    "pdf_url": "https://arxiv.org/pdf/2507.04607v3",
    "published_date": "2025-07-07 01:54:34 UTC",
    "updated_date": "2025-09-26 23:13:38 UTC"
  },
  {
    "arxiv_id": "2507.04606v1",
    "title": "Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions",
    "authors": [
      "Aman Mehra",
      "Alexandre Capone",
      "Jeff Schneider"
    ],
    "abstract": "A long-standing problem in online reinforcement learning (RL) is of ensuring sample efficiency, which stems from an inability to explore environments efficiently. Most attempts at efficient exploration tackle this problem in a setting where learning begins from scratch, without prior information available to bootstrap learning. However, such approaches fail to leverage expert demonstrations and simulators that can reset to arbitrary states. These affordances are valuable resources that offer enormous potential to guide exploration and speed up learning. In this paper, we explore how a small number of expert demonstrations and a simulator allowing arbitrary resets can accelerate learning during online RL. We find that training with a suitable choice of an auxiliary start state distribution that may differ from the true start state distribution of the underlying Markov Decision Process can significantly improve sample efficiency. We find that using a notion of safety to inform the choice of this auxiliary distribution significantly accelerates learning. By using episode length information as a way to operationalize this notion, we demonstrate state-of-the-art sample efficiency on a sparse-reward hard-exploration environment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML ARLET Workshop 2024",
    "pdf_url": "https://arxiv.org/pdf/2507.04606v1",
    "published_date": "2025-07-07 01:54:05 UTC",
    "updated_date": "2025-07-07 01:54:05 UTC"
  },
  {
    "arxiv_id": "2507.04600v2",
    "title": "DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification",
    "authors": [
      "Zhipeng Liu",
      "Peibo Duan",
      "Binwu Wang",
      "Xuan Tang",
      "Qi Chu",
      "Changsheng Zhang",
      "Yongsheng Huang",
      "Bin Zhang"
    ],
    "abstract": "Real-world time series typically exhibit complex temporal variations, making the time series classification task notably challenging. Recent advancements have demonstrated the potential of multi-scale analysis approaches, which provide an effective solution for capturing these complex temporal patterns. However, existing multi-scale analysis-based time series prediction methods fail to eliminate redundant scale-shared features across multi-scale time series, resulting in the model over- or under-focusing on scale-shared features. To address this issue, we propose a novel end-to-end Disentangled Multi-Scale framework for Time Series classification (DisMS-TS). The core idea of DisMS-TS is to eliminate redundant shared features in multi-scale time series, thereby improving prediction performance. Specifically, we propose a temporal disentanglement module to capture scale-shared and scale-specific temporal representations, respectively. Subsequently, to effectively learn both scale-shared and scale-specific temporal representations, we introduce two regularization terms that ensure the consistency of scale-shared representations and the disparity of scale-specific representations across all temporal scales. Extensive experiments conducted on multiple datasets validate the superiority of DisMS-TS over its competitive baselines, with the accuracy improvement up to 9.71%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for presentation at the ACM International Conference on Multimedia (ACM MM 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.04600v2",
    "published_date": "2025-07-07 01:35:55 UTC",
    "updated_date": "2025-07-24 09:29:08 UTC"
  },
  {
    "arxiv_id": "2507.05300v1",
    "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)",
    "authors": [
      "Nicholas Merchant",
      "Haitz Sáez de Ocáriz Borde",
      "Andrei Cristian Popescu",
      "Carlos Garcia Jurado Suarez"
    ],
    "abstract": "We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$Σ$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7-page main paper + appendix, 18 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05300v1",
    "published_date": "2025-07-07 01:18:40 UTC",
    "updated_date": "2025-07-07 01:18:40 UTC"
  },
  {
    "arxiv_id": "2507.04594v1",
    "title": "Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective",
    "authors": [
      "Niloofar Shadab",
      "Tyler Cody",
      "Alejandro Salado",
      "Taylan G. Topcu",
      "Mohammad Shadab",
      "Peter Beling"
    ],
    "abstract": "Engineering methodologies predominantly revolve around established principles of decomposition and recomposition. These principles involve partitioning inputs and outputs at the component level, ensuring that the properties of individual components are preserved upon composition. However, this view does not transfer well to intelligent systems, particularly when addressing the scaling of intelligence as a system property. Our prior research contends that the engineering of general intelligence necessitates a fresh set of overarching systems principles. As a result, we introduced the \"core and periphery\" principles, a novel conceptual framework rooted in abstract systems theory and the Law of Requisite Variety. In this paper, we assert that these abstract concepts hold practical significance. Through empirical evidence, we illustrate their applicability to both biological and artificial intelligence systems, bridging abstract theory with real-world implementations. Then, we expand on our previous theoretical framework by mathematically defining core-dominant vs periphery-dominant systems.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.04594v1",
    "published_date": "2025-07-07 01:15:01 UTC",
    "updated_date": "2025-07-07 01:15:01 UTC"
  }
]
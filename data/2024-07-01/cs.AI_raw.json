[
  {
    "arxiv_id": "2407.01853v1",
    "title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets",
    "authors": [
      "Sathish Reddy Indurthi",
      "Wenxuan Zhou",
      "Shamil Chollampatt",
      "Ravi Agrawal",
      "Kaiqiang Song",
      "Lingxiao Zhao",
      "Chenguang Zhu"
    ],
    "abstract": "Advancements in Large Language Models (LLMs) have significantly enhanced\ninstruction-following capabilities. However, most Instruction Fine-Tuning (IFT)\ndatasets are predominantly in English, limiting model performance in other\nlanguages. Traditional methods for creating multilingual IFT datasets such as\ntranslating existing English IFT datasets or converting existing NLP datasets\ninto IFT datasets by templating, struggle to capture linguistic nuances and\nensure prompt (instruction) diversity. To address this issue, we propose a\nnovel method for collecting multilingual IFT datasets that preserves linguistic\nnaturalness and ensures prompt diversity. This approach leverages\nEnglish-focused LLMs, monolingual corpora, and a scoring function to create\nhigh-quality, diversified IFT datasets in multiple languages. Experiments\ndemonstrate that LLMs finetuned using these IFT datasets show notable\nimprovements in both generative and discriminative tasks, indicating enhanced\nlanguage comprehension by LLMs in non-English contexts. Specifically, on the\nmultilingual summarization task, LLMs using our IFT dataset achieved 17.57% and\n15.23% improvements over LLMs fine-tuned with translation-based and\ntemplate-based datasets, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01853v1",
    "published_date": "2024-07-01 23:47:09 UTC",
    "updated_date": "2024-07-01 23:47:09 UTC"
  },
  {
    "arxiv_id": "2407.01851v2",
    "title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time",
    "authors": [
      "Sanjoy Chowdhury",
      "Sayan Nag",
      "Subhrajyoti Dasgupta",
      "Jun Chen",
      "Mohamed Elhoseiny",
      "Ruohan Gao",
      "Dinesh Manocha"
    ],
    "abstract": "Leveraging Large Language Models' remarkable proficiency in text-based tasks,\nrecent works on Multi-modal LLMs (MLLMs) extend them to other modalities like\nvision and audio. However, the progress in these directions has been mostly\nfocused on tasks that only require a coarse-grained understanding of the\naudio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a\nfine-grained understanding of image and audio both spatially and temporally.\nWith a new modality alignment module based on optimal transport and a\ncross-attention module that enforces audio-visual consistency, Meerkat can\ntackle challenging tasks such as audio referred image grounding, image guided\naudio temporal localization, and audio-visual fact-checking. Moreover, we\ncarefully curate a large dataset AVFIT that comprises 3M instruction tuning\nsamples collected from open-source datasets, and introduce MeerkatBench that\nunifies five challenging audio-visual tasks. We achieve state-of-the-art\nperformance on all these downstream tasks with a relative improvement of up to\n37.12%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01851v2",
    "published_date": "2024-07-01 23:32:25 UTC",
    "updated_date": "2024-07-03 07:01:30 UTC"
  },
  {
    "arxiv_id": "2407.01843v2",
    "title": "My part is bigger than yours -- assessment within a group of peers",
    "authors": [
      "Konrad Ku≈Çakowski",
      "Jacek Szybowski"
    ],
    "abstract": "A project (e.g., writing a collaborative research paper) is often a group\neffort. At the end, each contributor identifies their contribution, often\nverbally. The reward, however, is very frequently financial. It leads to the\nquestion of what (percentage) share in the creation of the paper is due to\nindividual authors. Different authors may have various opinions on the matter;\neven worse, their opinions may have different relevance. In this paper, we\npresent simple models that allow aggregation of experts' views, linking the\npriority of his preference directly to the assessment made by other experts. In\nthis approach, the more significant the contribution of a given expert, the\ngreater the importance of his opinion. The presented method can be considered\nan attempt to find consensus among peers involved in the same project. Hence,\nits applications may go beyond the proposed study example of writing a\nscientific paper.",
    "categories": [
      "cs.DM",
      "cs.AI"
    ],
    "primary_category": "cs.DM",
    "comment": "25 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01843v2",
    "published_date": "2024-07-01 22:54:51 UTC",
    "updated_date": "2024-09-16 14:53:54 UTC"
  },
  {
    "arxiv_id": "2407.02537v1",
    "title": "Parameter Tuning of the Firefly Algorithm by Standard Monte Carlo and Quasi-Monte Carlo Methods",
    "authors": [
      "Geethu Joy",
      "Christian Huyck",
      "Xin-She Yang"
    ],
    "abstract": "Almost all optimization algorithms have algorithm-dependent parameters, and\nthe setting of such parameter values can significantly influence the behavior\nof the algorithm under consideration. Thus, proper parameter tuning should be\ncarried out to ensure that the algorithm used for optimization performs well\nand is sufficiently robust for solving different types of optimization\nproblems. In this study, the Firefly Algorithm (FA) is used to evaluate the\ninfluence of its parameter values on its efficiency. Parameter values are\nrandomly initialized using both the standard Monte Carlo method and the Quasi\nMonte-Carlo method. The values are then used for tuning the FA. Two benchmark\nfunctions and a spring design problem are used to test the robustness of the\ntuned FA. From the preliminary findings, it can be deduced that both the Monte\nCarlo method and Quasi-Monte Carlo method produce similar results in terms of\noptimal fitness values. Numerical experiments using the two different methods\non both benchmark functions and the spring design problem showed no major\nvariations in the final fitness values, irrespective of the different sample\nvalues selected during the simulations. This insensitivity indicates the\nrobustness of the FA.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "68T20, 68W50"
    ],
    "primary_category": "cs.NE",
    "comment": "International Conference on Computational Science (ICCS2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.02537v1",
    "published_date": "2024-07-01 21:17:27 UTC",
    "updated_date": "2024-07-01 21:17:27 UTC"
  },
  {
    "arxiv_id": "2407.06206v1",
    "title": "The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data",
    "authors": [
      "Ximing Wen",
      "Rosina O. Weber",
      "Anik Sen",
      "Darryl Hannan",
      "Steven C. Nesbit",
      "Vincent Chan",
      "Alberto Goffi",
      "Michael Morris",
      "John C. Hunninghake",
      "Nicholas E. Villalobos",
      "Edward Kim",
      "Christopher J. MacLellan"
    ],
    "abstract": "Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and\ninterpreting ultrasound scans right at the patient's bedside. However, the\nexpertise needed to interpret these images is considerable and may not always\nbe present in emergency situations. This reality makes algorithms such as\nmachine learning classifiers extremely valuable to augment human decisions.\nPOCUS devices are becoming available at a reasonable cost in the size of a\nmobile phone. The challenge of turning POCUS devices into life-saving tools is\nthat interpretation of ultrasound images requires specialist training and\nexperience. Unfortunately, the difficulty to obtain positive training images\nrepresents an important obstacle to building efficient and accurate\nclassifiers. Hence, the problem we try to investigate is how to explore\nstrategies to increase accuracy of classifiers trained with scarce data. We\nhypothesize that training with a few data instances may not suffice for\nclassifiers to generalize causing them to overfit. Our approach uses an\nExplainable AI-Augmented approach to help the algorithm learn more from less\nand potentially help the classifier better generalize.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures, accepted by XAI 2024 workshop @ IJCAI",
    "pdf_url": "http://arxiv.org/pdf/2407.06206v1",
    "published_date": "2024-07-01 21:09:31 UTC",
    "updated_date": "2024-07-01 21:09:31 UTC"
  },
  {
    "arxiv_id": "2407.12821v1",
    "title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents",
    "authors": [
      "Zelong Li",
      "Shuyuan Xu",
      "Kai Mei",
      "Wenyue Hua",
      "Balaji Rama",
      "Om Raheja",
      "Hao Wang",
      "He Zhu",
      "Yongfeng Zhang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant\nprogress in understanding complex natural language. One important application\nof LLM is LLM-based AI Agent, which leverages the ability of LLM as well as\nexternal tools for complex-task solving. To make sure LLM Agents follow an\neffective and reliable procedure to solve the given task, manually designed\nworkflows are usually used to guide the working mechanism of agents. However,\nmanually designing the workflows requires considerable efforts and domain\nknowledge, making it difficult to develop and deploy agents on massive scales.\nTo address these issues, we propose AutoFlow, a framework designed to\nautomatically generate workflows for agents to solve complex tasks. AutoFlow\ntakes natural language program as the format of agent workflow and employs a\nworkflow optimization procedure to iteratively optimize the workflow quality.\nBesides, this work offers two workflow generation methods: fine-tuning-based\nand in-context-based methods, making the AutoFlow framework applicable to both\nopen-source and closed-source LLMs. Experimental results show that our\nframework can produce robust and reliable agent workflows. We believe that the\nautomatic generation and interpretation of workflows in natural language\nrepresent a promising paradigm for solving complex tasks, particularly with the\nrapid development of LLMs. The source code of this work is available at\nhttps://github.com/agiresearch/AutoFlow.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Open source code available at https://github.com/agiresearch/AutoFlow",
    "pdf_url": "http://arxiv.org/pdf/2407.12821v1",
    "published_date": "2024-07-01 21:05:02 UTC",
    "updated_date": "2024-07-01 21:05:02 UTC"
  },
  {
    "arxiv_id": "2407.01800v1",
    "title": "Normalization and effective learning rates in reinforcement learning",
    "authors": [
      "Clare Lyle",
      "Zeyu Zheng",
      "Khimya Khetarpal",
      "James Martens",
      "Hado van Hasselt",
      "Razvan Pascanu",
      "Will Dabney"
    ],
    "abstract": "Normalization layers have recently experienced a renaissance in the deep\nreinforcement learning and continual learning literature, with several works\nhighlighting diverse benefits such as improving loss landscape conditioning and\ncombatting overestimation bias. However, normalization brings with it a subtle\nbut important side effect: an equivalence between growth in the norm of the\nnetwork parameters and decay in the effective learning rate. This becomes\nproblematic in continual learning settings, where the resulting effective\nlearning rate schedule may decay to near zero too quickly relative to the\ntimescale of the learning problem. We propose to make the learning rate\nschedule explicit with a simple re-parameterization which we call\nNormalize-and-Project (NaP), which couples the insertion of normalization\nlayers with weight projection, ensuring that the effective learning rate\nremains constant throughout training. This technique reveals itself as a\npowerful analytical tool to better understand learning rate schedules in deep\nreinforcement learning, and as a means of improving robustness to\nnonstationarity in synthetic plasticity loss benchmarks along with both the\nsingle-task and sequential variants of the Arcade Learning Environment. We also\nshow that our approach can be easily applied to popular architectures such as\nResNets and transformers while recovering and in some cases even slightly\nimproving the performance of the base model in common stationary benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01800v1",
    "published_date": "2024-07-01 20:58:01 UTC",
    "updated_date": "2024-07-01 20:58:01 UTC"
  },
  {
    "arxiv_id": "2407.01791v1",
    "title": "Œº-Bench: A Vision-Language Benchmark for Microscopy Understanding",
    "authors": [
      "Alejandro Lozano",
      "Jeffrey Nirschl",
      "James Burgess",
      "Sanket Rajan Gupte",
      "Yuhui Zhang",
      "Alyssa Unell",
      "Serena Yeung-Levy"
    ],
    "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes\nof image data in cell biology and biomedical research. Vision-language models\n(VLMs) offer a promising solution for large-scale biological image analysis,\nenhancing researchers' efficiency, identifying new image biomarkers, and\naccelerating hypothesis generation and scientific discovery. However, there is\na lack of standardized, diverse, and large-scale vision-language benchmarks to\nevaluate VLMs' perception and cognition capabilities in biological image\nunderstanding. To address this gap, we introduce {\\mu}-Bench, an expert-curated\nbenchmark encompassing 22 biomedical tasks across various scientific\ndisciplines (biology, pathology), microscopy modalities (electron,\nfluorescence, light), scales (subcellular, cellular, tissue), and organisms in\nboth normal and abnormal states. We evaluate state-of-the-art biomedical,\npathology, and general VLMs on {\\mu}-Bench and find that: i) current models\nstruggle on all categories, even for basic tasks such as distinguishing\nmicroscopy modalities; ii) current specialist models fine-tuned on biomedical\ndata often perform worse than generalist models; iii) fine-tuning in specific\nmicroscopy domains can cause catastrophic forgetting, eroding prior biomedical\nknowledge encoded in their base model. iv) weight interpolation between\nfine-tuned and pre-trained models offers one solution to forgetting and\nimproves general performance across biomedical tasks. We release {\\mu}-Bench\nunder a permissive license to accelerate the research and development of\nmicroscopy foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01791v1",
    "published_date": "2024-07-01 20:30:26 UTC",
    "updated_date": "2024-07-01 20:30:26 UTC"
  },
  {
    "arxiv_id": "2407.01790v1",
    "title": "Label-free Neural Semantic Image Synthesis",
    "authors": [
      "Jiayi Wang",
      "Kevin Alexander Laube",
      "Yumeng Li",
      "Jan Hendrik Metzen",
      "Shin-I Cheng",
      "Julio Borges",
      "Anna Khoreva"
    ],
    "abstract": "Recent work has shown great progress in integrating spatial conditioning to\ncontrol large, pre-trained text-to-image diffusion models. Despite these\nadvances, existing methods describe the spatial image content using\nhand-crafted conditioning inputs, which are either semantically ambiguous\n(e.g., edges) or require expensive manual annotations (e.g., semantic\nsegmentation). To address these limitations, we propose a new label-free way of\nconditioning diffusion models to enable fine-grained spatial control. We\nintroduce the concept of neural semantic image synthesis, which uses neural\nlayouts extracted from pre-trained foundation models as conditioning. Neural\nlayouts are advantageous as they provide rich descriptions of the desired\nimage, containing both semantics and detailed geometry of the scene. We\nexperimentally show that images synthesized via neural semantic image synthesis\nachieve similar or superior pixel-level alignment of semantic classes compared\nto those created using expensive semantic label maps. At the same time, they\ncapture better semantics, instance separation, and object orientation than\nother label-free conditioning options, such as edges or depth. Moreover, we\nshow that images generated by neural layout conditioning can effectively\naugment real data for training various perception tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01790v1",
    "published_date": "2024-07-01 20:30:23 UTC",
    "updated_date": "2024-07-01 20:30:23 UTC"
  },
  {
    "arxiv_id": "2407.01784v1",
    "title": "Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language Models with Paraphrase Enrichment",
    "authors": [
      "Kota Shamanth Ramanath Nayak",
      "Leila Kosseim"
    ],
    "abstract": "This paper describes our approach to hierarchical multi-label detection of\npersuasion techniques in meme texts. Our model, developed as a part of the\nrecent SemEval task, is based on fine-tuning individual language models (BERT,\nXLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition\nto dataset augmentation through paraphrase generation from ChatGPT. The scope\nof the study encompasses enhancing model performance through innovative\ntraining techniques and data augmentation strategies. The problem addressed is\nthe effective identification and classification of multiple persuasive\ntechniques in meme texts, a task complicated by the diversity and complexity of\nsuch content. The objective of the paper is to improve detection accuracy by\nrefining model training methods and examining the impact of balanced versus\nunbalanced training datasets. Novelty in the results and discussion lies in the\nfinding that training with paraphrases enhances model performance, yet a\nbalanced training set proves more advantageous than a larger unbalanced one.\nAdditionally, the analysis reveals the potential pitfalls of indiscriminate\nincorporation of paraphrases from diverse distributions, which can introduce\nsubstantial noise. Results with the SemEval 2024 data confirm these insights,\ndemonstrating improved model efficacy with the proposed methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 8 figures, 1 table, Proceedings of 5th International\n  Conference on Natural Language Processing and Applications (NLPA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.01784v1",
    "published_date": "2024-07-01 20:25:20 UTC",
    "updated_date": "2024-07-01 20:25:20 UTC"
  },
  {
    "arxiv_id": "2407.01782v4",
    "title": "Addressing a fundamental limitation in deep vision models: lack of spatial attention",
    "authors": [
      "Ali Borji"
    ],
    "abstract": "The primary aim of this manuscript is to underscore a significant limitation\nin current deep learning models, particularly vision models. Unlike human\nvision, which efficiently selects only the essential visual areas for further\nprocessing, leading to high speed and low energy consumption, deep vision\nmodels process the entire image. In this work, we examine this issue from a\nbroader perspective and propose two solutions that could pave the way for the\nnext generation of more efficient vision models. In the first solution,\nconvolution and pooling operations are selectively applied to altered regions,\nwith a change map sent to subsequent layers. This map indicates which\ncomputations need to be repeated. In the second solution, only the modified\nregions are processed by a semantic segmentation model, and the resulting\nsegments are inserted into the corresponding areas of the previous output map.\nThe code is available at https://github.com/aliborji/spatial_attention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01782v4",
    "published_date": "2024-07-01 20:21:09 UTC",
    "updated_date": "2024-11-22 05:56:30 UTC"
  },
  {
    "arxiv_id": "2407.01777v1",
    "title": "Deepfake Audio Detection Using Spectrogram-based Feature and Ensemble of Deep Learning Models",
    "authors": [
      "Lam Pham",
      "Phat Lam",
      "Truong Nguyen",
      "Huyen Nguyen",
      "Alexander Schindler"
    ],
    "abstract": "In this paper, we propose a deep learning based system for the task of\ndeepfake audio detection. In particular, the draw input audio is first\ntransformed into various spectrograms using three transformation methods of\nShort-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet\nTransform (WT) combined with different auditory-based filters of Mel,\nGammatone, linear filters (LF), and discrete cosine transform (DCT). Given the\nspectrograms, we evaluate a wide range of classification models based on three\ndeep learning approaches. The first approach is to train directly the\nspectrograms using our proposed baseline models of CNN-based model\n(CNN-baseline), RNN-based model (RNN-baseline), C-RNN model (C-RNN baseline).\nMeanwhile, the second approach is transfer learning from computer vision models\nsuch as ResNet-18, MobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-V2,\nSwint, Convnext-Tiny, GoogLeNet, MNASsnet, RegNet. In the third approach, we\nleverage the state-of-the-art audio pre-trained models of Whisper, Seamless,\nSpeechbrain, and Pyannote to extract audio embeddings from the input\nspectrograms. Then, the audio embeddings are explored by a Multilayer\nperceptron (MLP) model to detect the fake or real audio samples. Finally,\nhigh-performance deep learning models from these approaches are fused to\nachieve the best performance. We evaluated our proposed models on ASVspoof 2019\nbenchmark dataset. Our best ensemble model achieved an Equal Error Rate (EER)\nof 0.03, which is highly competitive to top-performing systems in the\nASVspoofing 2019 challenge. Experimental results also highlight the potential\nof selective spectrograms and deep learning approaches to enhance the task of\naudio deepfake detection.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01777v1",
    "published_date": "2024-07-01 20:10:43 UTC",
    "updated_date": "2024-07-01 20:10:43 UTC"
  },
  {
    "arxiv_id": "2407.01769v1",
    "title": "Improving Trip Mode Choice Modeling Using Ensemble Synthesizer (ENSY)",
    "authors": [
      "Amirhossein Parsi",
      "Melina Jafari",
      "Sina Sabzekar",
      "Zahra Amini"
    ],
    "abstract": "Accurate classification of mode choice datasets is crucial for transportation\nplanning and decision-making processes. However, conventional classification\nmodels often struggle to adequately capture the nuanced patterns of minority\nclasses within these datasets, leading to sub-optimal accuracy. In response to\nthis challenge, we present Ensemble Synthesizer (ENSY) which leverages\nprobability distribution for data augmentation, a novel data model tailored\nspecifically for enhancing classification accuracy in mode choice datasets. In\nour study, ENSY demonstrates remarkable efficacy by nearly quadrupling the F1\nscore of minority classes and improving overall classification accuracy by\nnearly 3%. To assess its performance comprehensively, we compare ENSY against\nvarious augmentation techniques including Random Oversampling, SMOTE-NC, and\nCTGAN. Through experimentation, ENSY consistently outperforms these methods\nacross various scenarios, underscoring its robustness and effectiveness",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01769v1",
    "published_date": "2024-07-01 19:59:29 UTC",
    "updated_date": "2024-07-01 19:59:29 UTC"
  },
  {
    "arxiv_id": "2407.17478v1",
    "title": "LLM4PM: A case study on using Large Language Models for Process Modeling in Enterprise Organizations",
    "authors": [
      "Clara Ziche",
      "Giovanni Apruzzese"
    ],
    "abstract": "We investigate the potential of using Large Language Models (LLM) to support\nprocess model creation in organizational contexts. Specifically, we carry out a\ncase study wherein we develop and test an LLM-based chatbot, PRODIGY (PROcess\nmoDellIng Guidance for You), in a multinational company, the Hilti Group. We\nare particularly interested in understanding how LLM can aid (human) modellers\nin creating process flow diagrams. To this purpose, we first conduct a\npreliminary user study (n=10) with professional process modellers from Hilti,\ninquiring for various pain-points they encounter in their daily routines. Then,\nwe use their responses to design and implement PRODIGY. Finally, we evaluate\nPRODIGY by letting our user study's participants use PRODIGY, and then ask for\ntheir opinion on the pros and cons of PRODIGY. We coalesce our results in\nactionable takeaways. Through our research, we showcase the first practical\napplication of LLM for process modelling in the real world, shedding light on\nhow industries can leverage LLM to enhance their Business Process Management\nactivities.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.17478v1",
    "published_date": "2024-07-01 19:57:36 UTC",
    "updated_date": "2024-07-01 19:57:36 UTC"
  },
  {
    "arxiv_id": "2407.18322v2",
    "title": "The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem",
    "authors": [
      "Joe B Hakim",
      "Jeffery L Painter",
      "Darmendra Ramcharran",
      "Vijay Kara",
      "Greg Powell",
      "Paulina Sobczak",
      "Chiho Sato",
      "Andrew Bate",
      "Andrew Beam"
    ],
    "abstract": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2.1; I.2.7; I.7.1"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 6 figures, 4 tables and supplementary material provided",
    "pdf_url": "http://arxiv.org/pdf/2407.18322v2",
    "published_date": "2024-07-01 19:52:41 UTC",
    "updated_date": "2024-09-04 17:16:05 UTC"
  },
  {
    "arxiv_id": "2407.01752v1",
    "title": "Predicting Trust Dynamics with Dynamic SEM in Human-AI Cooperation",
    "authors": [
      "Sota Kaneko",
      "Seiji Yamada"
    ],
    "abstract": "Humans' trust in AI constitutes a pivotal element in fostering a synergistic\nrelationship between humans and AI. This is particularly significant in the\ncontext of systems that leverage AI technology, such as autonomous driving\nsystems and human-robot interaction. Trust facilitates appropriate utilization\nof these systems, thereby optimizing their potential benefits. If humans\nover-trust or under-trust an AI, serious problems such as misuse and accidents\noccur. To prevent over/under-trust, it is necessary to predict trust dynamics.\nHowever, trust is an internal state of humans and hard to directly observe.\nTherefore, we propose a prediction model for trust dynamics using dynamic\nstructure equation modeling, which extends SEM that can handle time-series\ndata. A path diagram, which shows causalities between variables, is developed\nin an exploratory way and the resultant path diagram is optimized for effective\npath structures. Over/under-trust was predicted with 90\\% accuracy in a drone\nsimulator task,, and it was predicted with 99\\% accuracy in an autonomous\ndriving task. These results show that our proposed method outperformed the\nconventional method including an auto regression family.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01752v1",
    "published_date": "2024-07-01 19:31:07 UTC",
    "updated_date": "2024-07-01 19:31:07 UTC"
  },
  {
    "arxiv_id": "2407.01749v2",
    "title": "Invariant Correlation of Representation with Label: Enhancing Domain Generalization in Noisy Environments",
    "authors": [
      "Gaojie Jin",
      "Ronghui Mu",
      "Xinping Yi",
      "Xiaowei Huang",
      "Lijun Zhang"
    ],
    "abstract": "The Invariant Risk Minimization (IRM) approach aims to address the challenge\nof domain generalization by training a feature representation that remains\ninvariant across multiple environments. However, in noisy environments,\nIRM-related techniques such as IRMv1 and VREx may be unable to achieve the\noptimal IRM solution, primarily due to erroneous optimization directions. To\naddress this issue, we introduce ICorr (an abbreviation for Invariant\nCorrelation), a novel approach designed to surmount the above challenge in\nnoisy settings. Additionally, we dig into a case study to analyze why previous\nmethods may lose ground while ICorr can succeed. Through a theoretical lens,\nparticularly from a causality perspective, we illustrate that the invariant\ncorrelation of representation with label is a necessary condition for the\noptimal invariant predictor in noisy environments, whereas the optimization\nmotivations for other methods may not be. Furthermore, we empirically\ndemonstrate the effectiveness of ICorr by comparing it with other domain\ngeneralization methods on various noisy datasets. The code is available at\nhttps://github.com/Alexkael/ICorr.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01749v2",
    "published_date": "2024-07-01 19:27:28 UTC",
    "updated_date": "2025-02-09 12:58:06 UTC"
  },
  {
    "arxiv_id": "2407.01745v2",
    "title": "Adaptive control of reaction-diffusion PDEs via neural operator-approximated gain kernels",
    "authors": [
      "Luke Bhan",
      "Yuanyuan Shi",
      "Miroslav Krstic"
    ],
    "abstract": "Neural operator approximations of the gain kernels in PDE backstepping has\nemerged as a viable method for implementing controllers in real time. With such\nan approach, one approximates the gain kernel, which maps the plant coefficient\ninto the solution of a PDE, with a neural operator. It is in adaptive control\nthat the benefit of the neural operator is realized, as the kernel PDE solution\nneeds to be computed online, for every updated estimate of the plant\ncoefficient. We extend the neural operator methodology from adaptive control of\na hyperbolic PDE to adaptive control of a benchmark parabolic PDE (a\nreaction-diffusion equation with a spatially-varying and unknown reaction\ncoefficient). We prove global stability and asymptotic regulation of the plant\nstate for a Lyapunov design of parameter adaptation. The key technical\nchallenge of the result is handling the 2D nature of the gain kernels and\nproving that the target system with two distinct sources of perturbation terms,\ndue to the parameter estimation error and due to the neural approximation\nerror, is Lyapunov stable. To verify our theoretical result, we present\nsimulations achieving calculation speedups up to 45x relative to the\ntraditional finite difference solvers for every timestep in the simulation\ntrajectory.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "math.AP",
      "math.DS"
    ],
    "primary_category": "eess.SY",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01745v2",
    "published_date": "2024-07-01 19:24:36 UTC",
    "updated_date": "2024-11-28 07:35:35 UTC"
  },
  {
    "arxiv_id": "2407.01734v3",
    "title": "Universal Quantum Tomography With Deep Neural Networks",
    "authors": [
      "Nhan T. Luu",
      "Thang C. Truong",
      "Duong T. Luu"
    ],
    "abstract": "Quantum state tomography is a crucial technique for characterizing the state\nof a quantum system, which is essential for many applications in quantum\ntechnologies. In recent years, there has been growing interest in leveraging\nneural networks to enhance the efficiency and accuracy of quantum state\ntomography. Still, many of them did not include mixed quantum state, since pure\nstates are arguably less common in practical situations. In this research\npaper, we present two neural networks based approach for both pure and mixed\nquantum state tomography: Restricted Feature Based Neural Network and Mixed\nStates Conditional Generative Adversarial Network, evaluate its effectiveness\nin comparison to existing neural based methods. We demonstrate that our\nproposed methods can achieve state-of-the-art results in reconstructing mixed\nquantum states from experimental data. Our work highlights the potential of\nneural networks in revolutionizing quantum state tomography and facilitating\nthe development of quantum technologies.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "10 pages, 5 figures, 17 illustration, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.01734v3",
    "published_date": "2024-07-01 19:09:18 UTC",
    "updated_date": "2024-09-08 07:27:57 UTC"
  },
  {
    "arxiv_id": "2407.01725v1",
    "title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
    "authors": [
      "Bodhisattwa Prasad Majumder",
      "Harshit Surana",
      "Dhruv Agarwal",
      "Bhavana Dalvi Mishra",
      "Abhijeetsingh Meena",
      "Aryan Prakhar",
      "Tirth Vora",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "abstract": "Can the rapid advances in code generation, function calling, and data\nanalysis using large language models (LLMs) help automate the search and\nverification of hypotheses purely from a set of provided datasets? To evaluate\nthis question, we present DiscoveryBench, the first comprehensive benchmark\nthat formalizes the multi-step process of data-driven discovery. The benchmark\nis designed to systematically assess current model capabilities in discovery\ntasks and provide a useful resource for improving them. Our benchmark contains\n264 tasks collected across 6 diverse domains, such as sociology and\nengineering, by manually deriving discovery workflows from published papers to\napproximate the real-world challenges faced by researchers, where each task is\ndefined by a dataset, its metadata, and a discovery goal in natural language.\nWe additionally provide 903 synthetic tasks to conduct controlled evaluations\nacross task complexity. Furthermore, our structured formalism of data-driven\ndiscovery enables a facet-based evaluation that provides useful insights into\ndifferent failure modes. We evaluate several popular LLM-based reasoning\nframeworks using both open and closed LLMs as baselines on DiscoveryBench and\nfind that even the best system scores only 25%. Our benchmark, thus,\nillustrates the challenges in autonomous data-driven discovery and serves as a\nvaluable resource for the community to make progress.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Website: https://github.com/allenai/discoverybench",
    "pdf_url": "http://arxiv.org/pdf/2407.01725v1",
    "published_date": "2024-07-01 18:58:22 UTC",
    "updated_date": "2024-07-01 18:58:22 UTC"
  },
  {
    "arxiv_id": "2407.01705v1",
    "title": "Optimized Learning for X-Ray Image Classification for Multi-Class Disease Diagnoses with Accelerated Computing Strategies",
    "authors": [
      "Sebastian A. Cruz Romero",
      "Ivanelyz Rivera de Jesus",
      "Dariana J. Troche Quinones",
      "Wilson Rivera Gallego"
    ],
    "abstract": "X-ray image-based disease diagnosis lies in ensuring the precision of\nidentifying afflictions within the sample, a task fraught with challenges\nstemming from the occurrence of false positives and false negatives. False\npositives introduce the risk of erroneously identifying non-existent\nconditions, leading to misdiagnosis and a decline in patient care quality.\nConversely, false negatives pose the threat of overlooking genuine\nabnormalities, potentially causing delays in treatment and interventions,\nthereby resulting in adverse patient outcomes. The urgency to overcome these\nchallenges compels ongoing efforts to elevate the precision and reliability of\nX-ray image analysis algorithms within the computational framework. This study\nintroduces modified pre-trained ResNet models tailored for multi-class disease\ndiagnosis of X-ray images, incorporating advanced optimization strategies to\nreduce the execution runtime of training and inference tasks. The primary\nobjective is to achieve tangible performance improvements through accelerated\nimplementations of PyTorch, CUDA, Mixed- Precision Training, and Learning Rate\nScheduler. While outcomes demonstrate substantial improvements in execution\nruntimes between normal training and CUDA-accelerated training, negligible\ndifferences emerge between various training optimization modalities. This\nresearch marks a significant advancement in optimizing computational approaches\nto reduce training execution time for larger models. Additionally, we explore\nthe potential of effective parallel data processing using MPI4Py for the\ndistribution of gradient descent optimization across multiple nodes and\nleverage multiprocessing to expedite data preprocessing for larger datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "High Performance Computing course final term paper",
    "pdf_url": "http://arxiv.org/pdf/2407.01705v1",
    "published_date": "2024-07-01 18:31:30 UTC",
    "updated_date": "2024-07-01 18:31:30 UTC"
  },
  {
    "arxiv_id": "2407.01704v1",
    "title": "Weight Clipping for Deep Continual and Reinforcement Learning",
    "authors": [
      "Mohamed Elsayed",
      "Qingfeng Lan",
      "Clare Lyle",
      "A. Rupam Mahmood"
    ],
    "abstract": "Many failures in deep continual and reinforcement learning are associated\nwith increasing magnitudes of the weights, making them hard to change and\npotentially causing overfitting. While many methods address these learning\nfailures, they often change the optimizer or the architecture, a complexity\nthat hinders widespread adoption in various systems. In this paper, we focus on\nlearning failures that are associated with increasing weight norm and we\npropose a simple technique that can be easily added on top of existing learning\nsystems: clipping neural network weights to limit them to a specific range. We\nstudy the effectiveness of weight clipping in a series of supervised and\nreinforcement learning experiments. Our empirical results highlight the\nbenefits of weight clipping for generalization, addressing loss of plasticity\nand policy collapse, and facilitating learning with a large replay ratio.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the First Reinforcement Learning Conference (RLC 2024).\n  Code is available at https://github.com/mohmdelsayed/weight-clipping",
    "pdf_url": "http://arxiv.org/pdf/2407.01704v1",
    "published_date": "2024-07-01 18:29:29 UTC",
    "updated_date": "2024-07-01 18:29:29 UTC"
  },
  {
    "arxiv_id": "2407.01697v1",
    "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers",
    "authors": [
      "Salvatore Greco",
      "Ke Zhou",
      "Licia Capra",
      "Tania Cerquitelli",
      "Daniele Quercia"
    ],
    "abstract": "AI regulations are expected to prohibit machine learning models from using\nsensitive attributes during training. However, the latest Natural Language\nProcessing (NLP) classifiers, which rely on deep learning, operate as black-box\nsystems, complicating the detection and remediation of such misuse. Traditional\nbias mitigation methods in NLP aim for comparable performance across different\ngroups based on attributes like gender or race but fail to address the\nunderlying issue of reliance on protected attributes. To partly fix that, we\nintroduce NLPGuard, a framework for mitigating the reliance on protected\nattributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing\nNLP classifier, and its training data as input, producing a modified training\ndataset that significantly reduces dependence on protected attributes without\ncompromising accuracy. NLPGuard is applied to three classification tasks:\nidentifying toxic language, sentiment analysis, and occupation classification.\nOur evaluation shows that current NLP classifiers heavily depend on protected\nattributes, with up to $23\\%$ of the most predictive words associated with\nthese attributes. However, NLPGuard effectively reduces this reliance by up to\n$79\\%$, while slightly improving accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted at CSCW 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01697v1",
    "published_date": "2024-07-01 18:08:17 UTC",
    "updated_date": "2024-07-01 18:08:17 UTC"
  },
  {
    "arxiv_id": "2407.01687v2",
    "title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",
    "authors": [
      "Akshara Prabhakar",
      "Thomas L. Griffiths",
      "R. Thomas McCoy"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step\nreasoning capabilities of Large Language Models (LLMs). However, debates\npersist about whether LLMs exhibit abstract generalization or rely on shallow\nheuristics when given CoT prompts. To understand the factors influencing CoT\nreasoning we provide a detailed case study of the symbolic reasoning task of\ndecoding shift ciphers, where letters are shifted forward some number of steps\nin the alphabet. We analyze the pattern of results produced by three LLMs --\nGPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By\nfocusing on a single relatively simple task, we are able to identify three\nfactors that systematically affect CoT performance: the probability of the\ntask's expected output (probability), what the model has implicitly learned\nduring pre-training (memorization), and the number of intermediate operations\ninvolved in reasoning (noisy reasoning). We show that these factors can\ndrastically influence task accuracy across all three LLMs; e.g., when tested\nwith GPT-4, varying the output's probability of occurrence shifts accuracy from\n26% to 70%. Overall, we conclude that CoT prompting performance reflects both\nmemorization and a probabilistic version of genuine reasoning. Code and data at\nthis https://github.com/aksh555/deciphering_cot",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings; 9 pages plus references and appendices",
    "pdf_url": "http://arxiv.org/pdf/2407.01687v2",
    "published_date": "2024-07-01 18:01:07 UTC",
    "updated_date": "2024-10-04 01:01:39 UTC"
  },
  {
    "arxiv_id": "2407.12040v7",
    "title": "Comprehensive Performance Evaluation of YOLOv12, YOLO11, YOLOv10, YOLOv9 and YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments",
    "authors": [
      "Ranjan Sapkota",
      "Zhichao Meng",
      "Martin Churuvija",
      "Xiaoqiang Du",
      "Zenghong Ma",
      "Manoj Karkee"
    ],
    "abstract": "This study systematically performed an extensive real-world evaluation of the\nperformances of all configurations of YOLOv8, YOLOv9, YOLOv10, YOLO11( or\nYOLOv11), and YOLOv12 object detection algorithms in terms of precision,\nrecall, mean Average Precision at 50\\% Intersection over Union (mAP@50), and\ncomputational speeds including pre-processing, inference, and post-processing\ntimes immature green apple (or fruitlet) detection in commercial orchards.\nAdditionally, this research performed and validated in-field counting of the\nfruitlets using an iPhone and machine vision sensors. Among the configurations,\nYOLOv12l recorded the highest recall rate at 0.90, compared to all other\nconfigurations of YOLO models. Likewise, YOLOv10x achieved the highest\nprecision score of 0.908, while YOLOv9 Gelan-c attained a precision of 0.903.\nAnalysis of mAP@0.50 revealed that YOLOv9 Gelan-base and YOLOv9 Gelan-e reached\npeak scores of 0.935, with YOLO11s and YOLOv12l following closely at 0.933 and\n0.931, respectively. For counting validation using images captured with an\niPhone 14 Pro, the YOLO11n configuration demonstrated outstanding accuracy,\nrecording RMSE values of 4.51 for Honeycrisp, 4.59 for Cosmic Crisp, 4.83 for\nScilate, and 4.96 for Scifresh; corresponding MAE values were 4.07, 3.98, 7.73,\nand 3.85. Similar performance trends were observed with RGB-D sensor data.\nMoreover, sensor-specific training on Intel Realsense data significantly\nenhanced model performance. YOLOv11n achieved highest inference speed of 2.4\nms, outperforming YOLOv8n (4.1 ms), YOLOv9 Gelan-s (11.5 ms), YOLOv10n (5.5\nms), and YOLOv12n (4.6 ms), underscoring its suitability for real-time object\ndetection applications. (YOLOv12 architecture, YOLOv11 Architecture, YOLOv12\nobject detection, YOLOv11 object detecion, YOLOv12 segmentation)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 figures, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12040v7",
    "published_date": "2024-07-01 17:59:55 UTC",
    "updated_date": "2025-02-25 23:00:05 UTC"
  },
  {
    "arxiv_id": "2407.01526v1",
    "title": "Scalable Nested Optimization for Deep Learning",
    "authors": [
      "Jonathan Lorraine"
    ],
    "abstract": "Gradient-based optimization has been critical to the success of machine\nlearning, updating a single set of parameters to minimize a single loss. A\ngrowing number of applications rely on a generalization of this, where we have\na bilevel or nested optimization of which subsets of parameters update on\ndifferent objectives nested inside each other. We focus on motivating examples\nof hyperparameter optimization and generative adversarial networks. However,\nnaively applying classical methods often fails when we look at solving these\nnested problems on a large scale. In this thesis, we build tools for nested\noptimization that scale to deep learning setups.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "math.OC",
      "stat.ML",
      "68T05",
      "I.2.6; I.2.8; I.5.1; G.1.6"
    ],
    "primary_category": "cs.LG",
    "comment": "View more research details at https://www.jonlorraine.com/",
    "pdf_url": "http://arxiv.org/pdf/2407.01526v1",
    "published_date": "2024-07-01 17:59:41 UTC",
    "updated_date": "2024-07-01 17:59:41 UTC"
  },
  {
    "arxiv_id": "2407.01525v3",
    "title": "ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities",
    "authors": [
      "Chenming Zhu",
      "Tai Wang",
      "Wenwei Zhang",
      "Kai Chen",
      "Xihui Liu"
    ],
    "abstract": "Although great progress has been made in 3D visual grounding, current models\nstill rely on explicit textual descriptions for grounding and lack the ability\nto reason human intentions from implicit instructions. We propose a new task\ncalled 3D reasoning grounding and introduce a new benchmark ScanReason which\nprovides over 10K question-answer-location pairs from five reasoning types that\nrequire the synerization of reasoning and grounding. We further design our\napproach, ReGround3D, composed of the visual-centric reasoning module empowered\nby Multi-modal Large Language Model (MLLM) and the 3D grounding module to\nobtain accurate object locations by looking back to the enhanced geometry and\nfine-grained details from the 3D scenes. A chain-of-grounding mechanism is\nproposed to further boost the performance with interleaved reasoning and\ngrounding steps during inference. Extensive experiments on the proposed\nbenchmark validate the effectiveness of our proposed approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024. A comprehensive and hierarchical 3D reasoning\n  grounding benchmark in the era of foundation models. Project page:\n  https://zcmax.github.io/projects/ScanReason",
    "pdf_url": "http://arxiv.org/pdf/2407.01525v3",
    "published_date": "2024-07-01 17:59:35 UTC",
    "updated_date": "2024-07-17 07:07:43 UTC"
  },
  {
    "arxiv_id": "2407.01521v2",
    "title": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing",
    "authors": [
      "Bingliang Zhang",
      "Wenda Chu",
      "Julius Berner",
      "Chenlin Meng",
      "Anima Anandkumar",
      "Yang Song"
    ],
    "abstract": "Diffusion models have recently achieved success in solving Bayesian inverse\nproblems with learned data priors. Current methods build on top of the\ndiffusion sampling process, where each denoising step makes small modifications\nto samples from the previous step. However, this process struggles to correct\nerrors from earlier sampling steps, leading to worse performance in complicated\nnonlinear inverse problems, such as phase retrieval. To address this challenge,\nwe propose a new method called Decoupled Annealing Posterior Sampling (DAPS)\nthat relies on a novel noise annealing process. Specifically, we decouple\nconsecutive steps in a diffusion sampling trajectory, allowing them to vary\nconsiderably from one another while ensuring their time-marginals anneal to the\ntrue posterior as we reduce noise levels. This approach enables the exploration\nof a larger solution space, improving the success rate for accurate\nreconstructions. We demonstrate that DAPS significantly improves sample quality\nand stability across multiple image restoration tasks, particularly in\ncomplicated nonlinear inverse problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01521v2",
    "published_date": "2024-07-01 17:59:23 UTC",
    "updated_date": "2024-12-18 00:26:39 UTC"
  },
  {
    "arxiv_id": "2407.01518v1",
    "title": "Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision",
    "authors": [
      "Hao Dong",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "abstract": "The task of open-set domain generalization (OSDG) involves recognizing novel\nclasses within unseen domains, which becomes more challenging with multiple\nmodalities as input. Existing works have only addressed unimodal OSDG within\nthe meta-learning framework, without considering multimodal scenarios. In this\nwork, we introduce a novel approach to address Multimodal Open-Set Domain\nGeneralization (MM-OSDG) for the first time, utilizing self-supervision. To\nthis end, we introduce two innovative multimodal self-supervised pretext tasks:\nMasked Cross-modal Translation and Multimodal Jigsaw Puzzles. These tasks\nfacilitate the learning of multimodal representative features, thereby\nenhancing generalization and open-class detection capabilities. Additionally,\nwe propose a novel entropy weighting mechanism to balance the loss across\ndifferent modalities. Furthermore, we extend our approach to tackle also the\nMultimodal Open-Set Domain Adaptation (MM-OSDA) problem, especially in\nscenarios where unlabeled data from the target domain is available. Extensive\nexperiments conducted under MM-OSDG, MM-OSDA, and Multimodal Closed-Set DG\nsettings on the EPIC-Kitchens and HAC datasets demonstrate the efficacy and\nversatility of the proposed approach. Our source code is available at\nhttps://github.com/donghao51/MOOSA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024, code: https://github.com/donghao51/MOOSA",
    "pdf_url": "http://arxiv.org/pdf/2407.01518v1",
    "published_date": "2024-07-01 17:59:09 UTC",
    "updated_date": "2024-07-01 17:59:09 UTC"
  },
  {
    "arxiv_id": "2407.01511v2",
    "title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents",
    "authors": [
      "Tianqi Xu",
      "Linyao Chen",
      "Dai-Jie Wu",
      "Yanjun Chen",
      "Zecheng Zhang",
      "Xiang Yao",
      "Zhiqiang Xie",
      "Yongchao Chen",
      "Shilong Liu",
      "Bochen Qian",
      "Anjie Yang",
      "Zhaoxuan Jin",
      "Jianbo Deng",
      "Philip Torr",
      "Bernard Ghanem",
      "Guohao Li"
    ],
    "abstract": "The development of autonomous agents increasingly relies on Multimodal\nLanguage Models (MLMs) to perform tasks described in natural language with GUI\nenvironments, such as websites, desktop computers, or mobile phones. Existing\nbenchmarks for MLM agents in interactive environments are limited by their\nfocus on a single environment, lack of detailed and generalized evaluation\nmethods, and the complexities of constructing tasks and evaluators. To overcome\nthese limitations, we introduce Crab, the first agent benchmark framework\ndesigned to support cross-environment tasks, incorporating a graph-based\nfine-grained evaluation method and an efficient mechanism for task and\nevaluator construction. Our framework supports multiple devices and can be\neasily extended to any environment with a Python interface. Leveraging Crab, we\ndeveloped a cross-platform Crab Benchmark-v0 comprising 120 tasks in computer\ndesktop and mobile phone environments. We evaluated four advanced MLMs using\ndifferent single and multi-agent system configurations on this benchmark. The\nexperimental results demonstrate that the single agent with GPT-4o achieves the\nbest completion ratio of 38.01%. All framework code, agent code, and task\ndatasets are publicly available at https://github.com/camel-ai/crab.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01511v2",
    "published_date": "2024-07-01 17:55:04 UTC",
    "updated_date": "2024-10-18 11:29:39 UTC"
  },
  {
    "arxiv_id": "2407.01505v1",
    "title": "Self-Cognition in Large Language Models: An Exploratory Study",
    "authors": [
      "Dongping Chen",
      "Jiawen Shi",
      "Yao Wan",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ],
    "abstract": "While Large Language Models (LLMs) have achieved remarkable success across\nvarious applications, they also raise concerns regarding self-cognition. In\nthis paper, we perform a pioneering study to explore self-cognition in LLMs.\nSpecifically, we first construct a pool of self-cognition instruction prompts\nto evaluate where an LLM exhibits self-cognition and four well-designed\nprinciples to quantify LLMs' self-cognition. Our study reveals that 4 of the 48\nmodels on Chatbot Arena--specifically Command R, Claude3-Opus,\nLlama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable\nself-cognition. We observe a positive correlation between model size, training\ndata quality, and self-cognition level. Additionally, we also explore the\nutility and trustworthiness of LLM in the self-cognition state, revealing that\nthe self-cognition state enhances some specific tasks such as creative writing\nand exaggeration. We believe that our work can serve as an inspiration for\nfurther research to study the self-cognition in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICML 2024 Large Language Models and Cognition Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.01505v1",
    "published_date": "2024-07-01 17:52:05 UTC",
    "updated_date": "2024-07-01 17:52:05 UTC"
  },
  {
    "arxiv_id": "2407.01504v1",
    "title": "Reinvestigating the R2 Indicator: Achieving Pareto Compliance by Integration",
    "authors": [
      "Lennart Sch√§permeier",
      "Pascal Kerschke"
    ],
    "abstract": "In multi-objective optimization, set-based quality indicators are a\ncornerstone of benchmarking and performance assessment. They capture the\nquality of a set of trade-off solutions by reducing it to a scalar number. One\nof the most commonly used set-based metrics is the R2 indicator, which\ndescribes the expected utility of a solution set to a decision-maker under a\ndistribution of utility functions. Typically, this indicator is applied by\ndiscretizing this distribution of utility functions, yielding a weakly\nPareto-compliant indicator. In consequence, adding a nondominated or dominating\nsolution to a solution set may - but does not have to - improve the indicator's\nvalue.\n  In this paper, we reinvestigate the R2 indicator under the premise that we\nhave a continuous, uniform distribution of (Tchebycheff) utility functions. We\nanalyze its properties in detail, demonstrating that this continuous variant is\nindeed Pareto-compliant - that is, any beneficial solution will improve the\nmetric's value. Additionally, we provide an efficient computational procedure\nto compute this metric for bi-objective problems in $\\mathcal O (N \\log N)$. As\na result, this work contributes to the state-of-the-art Pareto-compliant unary\nperformance metrics, such as the hypervolume indicator, offering an efficient\nand promising alternative.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "This version has been accepted for publication at the 18th\n  International Conference on Parallel Problem Solving from Nature (PPSN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.01504v1",
    "published_date": "2024-07-01 17:50:44 UTC",
    "updated_date": "2024-07-01 17:50:44 UTC"
  },
  {
    "arxiv_id": "2407.01502v1",
    "title": "AI Agents That Matter",
    "authors": [
      "Sayash Kapoor",
      "Benedikt Stroebl",
      "Zachary S. Siegel",
      "Nitya Nadgir",
      "Arvind Narayanan"
    ],
    "abstract": "AI agents are an exciting new research direction, and agent development is\ndriven by benchmarks. Our analysis of current agent benchmarks and evaluation\npractices reveals several shortcomings that hinder their usefulness in\nreal-world applications. First, there is a narrow focus on accuracy without\nattention to other metrics. As a result, SOTA agents are needlessly complex and\ncostly, and the community has reached mistaken conclusions about the sources of\naccuracy gains. Our focus on cost in addition to accuracy motivates the new\ngoal of jointly optimizing the two metrics. We design and implement one such\noptimization, showing its potential to greatly reduce cost while maintaining\naccuracy. Second, the benchmarking needs of model and downstream developers\nhave been conflated, making it hard to identify which agent would be best\nsuited for a particular application. Third, many agent benchmarks have\ninadequate holdout sets, and sometimes none at all. This has led to agents that\nare fragile because they take shortcuts and overfit to the benchmark in various\nways. We prescribe a principled framework for avoiding overfitting. Finally,\nthere is a lack of standardization in evaluation practices, leading to a\npervasive lack of reproducibility. We hope that the steps we introduce for\naddressing these shortcomings will spur the development of agents that are\nuseful in the real world and not just accurate on benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01502v1",
    "published_date": "2024-07-01 17:48:14 UTC",
    "updated_date": "2024-07-01 17:48:14 UTC"
  },
  {
    "arxiv_id": "2407.01492v2",
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "authors": [
      "Qian Liu",
      "Xiaosen Zheng",
      "Niklas Muennighoff",
      "Guangtao Zeng",
      "Longxu Dou",
      "Tianyu Pang",
      "Jing Jiang",
      "Min Lin"
    ],
    "abstract": "The data mixture for large language model pre-training significantly impacts\nperformance, yet how to determine an effective mixture remains unclear. We\npropose RegMix to automatically identify a high-performing data mixture by\nformulating it as a regression task. RegMix trains many small models on diverse\ndata mixtures, uses regression to predict performance of unseen mixtures, and\napplies the best predicted mixture to train a large-scale model with orders of\nmagnitude more compute. To empirically validate RegMix, we train 512 models\nwith 1M parameters for 1B tokens to fit the regression model and predict the\nbest data mixture. Using this mixture we train a 1B parameter model for 25B\ntokens (i.e. 1000x larger and 25x longer) which we find performs best among 64\ncandidate 1B parameter models with other mixtures. Furthermore, RegMix\nconsistently outperforms human selection in experiments involving models up to\n7B models trained on 100B tokens, while matching or exceeding DoReMi using just\n10% of the computational resources. Our experiments also show that (1) Data\nmixtures significantly impact performance; (2) Web corpora rather than data\nperceived as high-quality like Wikipedia have the strongest positive\ncorrelation with downstream performance; (3) Domains interact in complex ways\noften contradicting common sense, thus automatic approaches like RegMix are\nneeded; (4) Data mixture effects transcend scaling laws. Our code is available\nat https://github.com/sail-sg/regmix.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.01492v2",
    "published_date": "2024-07-01 17:31:03 UTC",
    "updated_date": "2025-01-23 17:35:43 UTC"
  },
  {
    "arxiv_id": "2407.01490v2",
    "title": "LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives",
    "authors": [
      "Lu√≠sa Shimabucoro",
      "Sebastian Ruder",
      "Julia Kreutzer",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "abstract": "The widespread adoption of synthetic data raises new questions about how\nmodels generating the data can influence other large language models (LLMs) via\ndistilled data. To start, our work exhaustively characterizes the impact of\npassive inheritance of model properties by systematically studying the\nconsequences of synthetic data integration. We provide one of the most\ncomprehensive studies to-date of how the source of synthetic data shapes\nmodels' internal biases, calibration and generations' textual attributes and\npreferences. We find that models are surprisingly sensitive towards certain\nattributes even when the synthetic data prompts appear \"neutral\". which invites\nthe question whether this sensitivity can be exploited for good.\n  Our findings invite the question can we explicitly steer the models towards\nthe properties we want at test time by exploiting the data generation process?\nThis would have historically been considered infeasible due to the cost of\ncollecting data with a specific characteristic or objective in mind. However,\nimprovement in the quality of synthetic data, as well as a shift towards\ngeneral-purpose models designed to follow a diverse way of instructions, means\nthis question is timely. We propose active inheritance as a term to describe\nintentionally constraining synthetic data according to a non-differentiable\nobjective. We demonstrate how active inheritance can steer the generation\nprofiles of models towards desirable non-differentiable attributes, e.g. high\nlexical diversity or low toxicity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01490v2",
    "published_date": "2024-07-01 17:26:21 UTC",
    "updated_date": "2024-07-19 10:45:21 UTC"
  },
  {
    "arxiv_id": "2407.01489v2",
    "title": "Agentless: Demystifying LLM-based Software Engineering Agents",
    "authors": [
      "Chunqiu Steven Xia",
      "Yinlin Deng",
      "Soren Dunn",
      "Lingming Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01489v2",
    "published_date": "2024-07-01 17:24:45 UTC",
    "updated_date": "2024-10-29 17:29:27 UTC"
  },
  {
    "arxiv_id": "2407.01476v2",
    "title": "Tree Search for Language Model Agents",
    "authors": [
      "Jing Yu Koh",
      "Stephen McAleer",
      "Daniel Fried",
      "Ruslan Salakhutdinov"
    ],
    "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise\nin their ability to perform decision-making tasks such as web automation.\nHowever, a key limitation remains: LMs, primarily optimized for natural\nlanguage understanding and generation, struggle with multi-step reasoning,\nplanning, and using environmental feedback when attempting to solve realistic\ncomputer tasks. Towards addressing this, we propose an inference-time search\nalgorithm for LM agents to explicitly perform exploration and multi-step\nplanning in interactive web environments. Our approach is a form of best-first\ntree search that operates within the actual environment space, and is\ncomplementary with most existing state-of-the-art agents. It is the first tree\nsearch algorithm for LM agents that shows effectiveness on realistic web tasks.\nOn the challenging VisualWebArena benchmark, applying our search algorithm on\ntop of a GPT-4o agent yields a 39.7% relative increase in success rate compared\nto the same baseline without search, setting a state-of-the-art success rate of\n26.4%. On WebArena, search also yields a 28.0% relative improvement over a\nbaseline agent, setting a competitive success rate of 19.2%. Our experiments\nhighlight the effectiveness of search for web agents, and we demonstrate that\nperformance scales with increased test-time compute. We conduct a thorough\nanalysis of our results to highlight improvements from search, limitations, and\npromising directions for future work. Our code and models are publicly released\nat https://jykoh.com/search-agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages. Models and code available at\n  https://jykoh.com/search-agents",
    "pdf_url": "http://arxiv.org/pdf/2407.01476v2",
    "published_date": "2024-07-01 17:07:55 UTC",
    "updated_date": "2024-10-12 19:58:57 UTC"
  },
  {
    "arxiv_id": "2407.01463v1",
    "title": "Retrieval-augmented generation in multilingual settings",
    "authors": [
      "Nadezhda Chirkova",
      "David Rau",
      "Herv√© D√©jean",
      "Thibault Formal",
      "St√©phane Clinchant",
      "Vassilina Nikoulina"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising\nsolution for incorporating up-to-date or domain-specific knowledge into large\nlanguage models (LLMs) and improving LLM factuality, but is predominantly\nstudied in English-only settings. In this work, we consider RAG in the\nmultilingual setting (mRAG), i.e. with user queries and the datastore in 13\nlanguages, and investigate which components and with which adjustments are\nneeded to build a well-performing mRAG pipeline, that can be used as a strong\nbaseline in future works. Our findings highlight that despite the availability\nof high-quality off-the-shelf multilingual retrievers and generators,\ntask-specific prompt engineering is needed to enable generation in user\nlanguages. Moreover, current evaluation metrics need adjustments for\nmultilingual setting, to account for variations in spelling named entities. The\nmain limitations to be addressed in future works include frequent\ncode-switching in non-Latin alphabet languages, occasional fluency errors,\nwrong reading of the provided documents, or irrelevant retrieval. We release\nthe code for the resulting mRAG baseline pipeline at\nhttps://github.com/naver/bergen.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01463v1",
    "published_date": "2024-07-01 16:56:50 UTC",
    "updated_date": "2024-07-01 16:56:50 UTC"
  },
  {
    "arxiv_id": "2407.01459v1",
    "title": "On Implications of Scaling Laws on Feature Superposition",
    "authors": [
      "Pavan Katta"
    ],
    "abstract": "Using results from scaling laws, this theoretical note argues that the\nfollowing two statements cannot be simultaneously true: 1. Superposition\nhypothesis where sparse features are linearly represented across a layer is a\ncomplete theory of feature representation. 2. Features are universal, meaning\ntwo models trained on the same data and achieving equal performance will learn\nidentical features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2407.01459v1",
    "published_date": "2024-07-01 16:54:07 UTC",
    "updated_date": "2024-07-01 16:54:07 UTC"
  },
  {
    "arxiv_id": "2407.01458v2",
    "title": "Contractual Reinforcement Learning: Pulling Arms with Invisible Hands",
    "authors": [
      "Jibang Wu",
      "Siyu Chen",
      "Mengdi Wang",
      "Huazheng Wang",
      "Haifeng Xu"
    ],
    "abstract": "The agency problem emerges in today's large scale machine learning tasks,\nwhere the learners are unable to direct content creation or enforce data\ncollection. In this work, we propose a theoretical framework for aligning\neconomic interests of different stakeholders in the online learning problems\nthrough contract design. The problem, termed \\emph{contractual reinforcement\nlearning}, naturally arises from the classic model of Markov decision\nprocesses, where a learning principal seeks to optimally influence the agent's\naction policy for their common interests through a set of payment rules\ncontingent on the realization of next state. For the planning problem, we\ndesign an efficient dynamic programming algorithm to determine the optimal\ncontracts against the far-sighted agent. For the learning problem, we introduce\na generic design of no-regret learning algorithms to untangle the challenges\nfrom robust design of contracts to the balance of exploration and exploitation,\nreducing the complexity analysis to the construction of efficient search\nalgorithms. For several natural classes of problems, we design tailored search\nalgorithms that provably achieve $\\tilde{O}(\\sqrt{T})$ regret. We also present\nan algorithm with $\\tilde{O}(T^{2/3})$ for the general problem that improves\nthe existing analysis in online contract design with mild technical\nassumptions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "econ.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01458v2",
    "published_date": "2024-07-01 16:53:00 UTC",
    "updated_date": "2024-07-02 15:17:50 UTC"
  },
  {
    "arxiv_id": "2407.01437v2",
    "title": "Needle in the Haystack for Memory Based Large Language Models",
    "authors": [
      "Elliot Nelson",
      "Georgios Kollias",
      "Payel Das",
      "Subhajit Chaudhury",
      "Soham Dan"
    ],
    "abstract": "Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages; slightly revised abstract",
    "pdf_url": "http://arxiv.org/pdf/2407.01437v2",
    "published_date": "2024-07-01 16:32:16 UTC",
    "updated_date": "2024-07-12 17:20:34 UTC"
  },
  {
    "arxiv_id": "2407.01428v1",
    "title": "Reinforcement Learning-driven Data-intensive Workflow Scheduling for Volunteer Edge-Cloud",
    "authors": [
      "Motahare Mounesan",
      "Mauro Lemus",
      "Hemanth Yeddulapalli",
      "Prasad Calyam",
      "Saptarshi Debroy"
    ],
    "abstract": "In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a\ncost-effective, community computing paradigm to support data-intensive\nscientific workflows. However, due to the highly distributed and heterogeneous\nnature of VEC resources, centralized workflow task scheduling remains a\nchallenge. In this paper, we propose a Reinforcement Learning (RL)-driven\ndata-intensive scientific workflow scheduling approach that takes into\nconsideration: i) workflow requirements, ii) VEC resources' preference on\nworkflows, and iii) diverse VEC resource policies, to ensure robust resource\nallocation. We formulate the long-term average performance optimization problem\nas a Markov Decision Process, which is solved using an event-based Asynchronous\nAdvantage Actor-Critic RL approach. Our extensive simulations and testbed\nimplementations demonstrate our approach's benefits over popular baseline\nstrategies in terms of workflow requirement satisfaction, VEC preference\nsatisfaction, and available VEC resource utilization.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01428v1",
    "published_date": "2024-07-01 16:21:13 UTC",
    "updated_date": "2024-07-01 16:21:13 UTC"
  },
  {
    "arxiv_id": "2407.01418v1",
    "title": "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing",
    "authors": [
      "Bo Ai",
      "Stephen Tian",
      "Haochen Shi",
      "Yixuan Wang",
      "Cheston Tan",
      "Yunzhu Li",
      "Jiajun Wu"
    ],
    "abstract": "Tactile feedback is critical for understanding the dynamics of both rigid and\ndeformable objects in many manipulation tasks, such as non-prehensile\nmanipulation and dense packing. We introduce an approach that combines visual\nand tactile sensing for robotic manipulation by learning a neural,\ntactile-informed dynamics model. Our proposed framework, RoboPack, employs a\nrecurrent graph neural network to estimate object states, including particles\nand object-level latent physics information, from historical visuo-tactile\nobservations and to perform future state predictions. Our tactile-informed\ndynamics model, learned from real-world data, can solve downstream robotics\ntasks with model-predictive control. We demonstrate our approach on a real\nrobot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile\nmanipulation and dense packing tasks, where the robot must infer the physics\nproperties of objects from direct and indirect interactions. Trained on only an\naverage of 30 minutes of real-world interaction data per task, our model can\nperform online adaptation and make touch-informed predictions. Through\nextensive evaluations in both long-horizon dynamics prediction and real-world\nmanipulation, our method demonstrates superior effectiveness compared to\nprevious learning-based and physics-based simulation systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.9; I.2.6; I.2.10"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science and Systems (RSS), 2024. Project page:\n  https://robo-pack.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.01418v1",
    "published_date": "2024-07-01 16:08:37 UTC",
    "updated_date": "2024-07-01 16:08:37 UTC"
  },
  {
    "arxiv_id": "2407.01409v1",
    "title": "Dynamic Few-Shot Learning for Knowledge Graph Question Answering",
    "authors": [
      "Jacopo D'Abramo",
      "Andrea Zugarini",
      "Paolo Torroni"
    ],
    "abstract": "Large language models present opportunities for innovative Question Answering\nover Knowledge Graphs (KGQA). However, they are not inherently designed for\nquery generation. To bridge this gap, solutions have been proposed that rely on\nfine-tuning or ad-hoc architectures, achieving good results but limited\nout-of-domain distribution generalization. In this study, we introduce a novel\napproach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the\nefficiency of in-context learning and semantic similarity and provides a\ngenerally applicable solution for KGQA with state-of-the-art performance. We\nrun an extensive evaluation across multiple benchmark datasets and architecture\nconfigurations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01409v1",
    "published_date": "2024-07-01 15:59:17 UTC",
    "updated_date": "2024-07-01 15:59:17 UTC"
  },
  {
    "arxiv_id": "2407.01408v1",
    "title": "Semantic Compositions Enhance Vision-Language Contrastive Learning",
    "authors": [
      "Maxwell Aladago",
      "Lorenzo Torresani",
      "Soroush Vosoughi"
    ],
    "abstract": "In the field of vision-language contrastive learning, models such as CLIP\ncapitalize on matched image-caption pairs as positive examples and leverage\nwithin-batch non-matching pairs as negatives. This approach has led to\nremarkable outcomes in zero-shot image classification, cross-modal retrieval,\nand linear evaluation tasks. We show that the zero-shot classification and\nretrieval capabilities of CLIP-like models can be improved significantly\nthrough the introduction of semantically composite examples during pretraining.\nInspired by CutMix in vision categorization, we create semantically composite\nimage-caption pairs by merging elements from two distinct instances in the\ndataset via a novel procedure. Our method fuses the captions and blends 50% of\neach image to form a new composite sample. This simple technique (termed CLIP-C\nfor CLIP Compositions), devoid of any additional computational overhead or\nincrease in model parameters, significantly improves zero-shot image\nclassification and cross-modal retrieval. The benefits of CLIP-C are\nparticularly pronounced in settings with relatively limited pretraining data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01408v1",
    "published_date": "2024-07-01 15:58:20 UTC",
    "updated_date": "2024-07-01 15:58:20 UTC"
  },
  {
    "arxiv_id": "2407.01406v3",
    "title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters",
    "authors": [
      "Daniil Gurgurov",
      "Mareike Hartmann",
      "Simon Ostermann"
    ],
    "abstract": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, KaLLM workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.01406v3",
    "published_date": "2024-07-01 15:56:24 UTC",
    "updated_date": "2024-12-18 17:09:31 UTC"
  },
  {
    "arxiv_id": "2407.01403v1",
    "title": "Optimization of Retrieval-Augmented Generation Context with Outlier Detection",
    "authors": [
      "Vitaly Bulgakov"
    ],
    "abstract": "In this paper, we focus on methods to reduce the size and improve the quality\nof the prompt context required for question-answering systems. Attempts to\nincrease the number of retrieved chunked documents and thereby enlarge the\ncontext related to the query can significantly complicate the processing and\ndecrease the performance of a Large Language Model (LLM) when generating\nresponses to queries. It is well known that a large set of documents retrieved\nfrom a database in response to a query may contain irrelevant information,\nwhich often leads to hallucinations in the resulting answers. Our goal is to\nselect the most semantically relevant documents, treating the discarded ones as\noutliers. We propose and evaluate several methods for identifying outliers by\ncreating features that utilize the distances of embedding vectors, retrieved\nfrom the vector database, to both the centroid and the query vectors. The\nmethods were evaluated by comparing the similarities of the retrieved LLM\nresponses to ground-truth answers obtained using the OpenAI GPT-4o model. It\nwas found that the greatest improvements were achieved with increasing\ncomplexity of the questions and answers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01403v1",
    "published_date": "2024-07-01 15:53:29 UTC",
    "updated_date": "2024-07-01 15:53:29 UTC"
  },
  {
    "arxiv_id": "2407.01397v1",
    "title": "Mask and Compress: Efficient Skeleton-based Action Recognition in Continual Learning",
    "authors": [
      "Matteo Mosconi",
      "Andriy Sorokin",
      "Aniello Panariello",
      "Angelo Porrello",
      "Jacopo Bonato",
      "Marco Cotogni",
      "Luigi Sabetta",
      "Simone Calderara",
      "Rita Cucchiara"
    ],
    "abstract": "The use of skeletal data allows deep learning models to perform action\nrecognition efficiently and effectively. Herein, we believe that exploring this\nproblem within the context of Continual Learning is crucial. While numerous\nstudies focus on skeleton-based action recognition from a traditional offline\nperspective, only a handful venture into online approaches. In this respect, we\nintroduce CHARON (Continual Human Action Recognition On skeletoNs), which\nmaintains consistent performance while operating within an efficient framework.\nThrough techniques like uniform sampling, interpolation, and a memory-efficient\ntraining stage based on masking, we achieve improved recognition accuracy while\nminimizing computational overhead. Our experiments on Split NTU-60 and the\nproposed Split NTU-120 datasets demonstrate that CHARON sets a new benchmark in\nthis domain. The code is available at https://github.com/Sperimental3/CHARON.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01397v1",
    "published_date": "2024-07-01 15:48:49 UTC",
    "updated_date": "2024-07-01 15:48:49 UTC"
  },
  {
    "arxiv_id": "2407.01376v1",
    "title": "Badllama 3: removing safety finetuning from Llama 3 in minutes",
    "authors": [
      "Dmitrii Volkov"
    ],
    "abstract": "We show that extensive LLM safety fine-tuning is easily subverted when an\nattacker has access to model weights. We evaluate three state-of-the-art\nfine-tuning methods-QLoRA, ReFT, and Ortho-and show how algorithmic advances\nenable constant jailbreaking performance with cuts in FLOPs and optimisation\npower. We strip safety fine-tuning from Llama 3 8B in one minute and Llama 3\n70B in 30 minutes on a single GPU, and sketch ways to reduce this further.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01376v1",
    "published_date": "2024-07-01 15:29:45 UTC",
    "updated_date": "2024-07-01 15:29:45 UTC"
  },
  {
    "arxiv_id": "2407.01355v2",
    "title": "Hyperspectral Pansharpening: Critical Review, Tools and Future Perspectives",
    "authors": [
      "Matteo Ciotola",
      "Giuseppe Guarino",
      "Gemine Vivone",
      "Giovanni Poggi",
      "Jocelyn Chanussot",
      "Antonio Plaza",
      "Giuseppe Scarpa"
    ],
    "abstract": "Hyperspectral pansharpening consists of fusing a high-resolution panchromatic\nband and a low-resolution hyperspectral image to obtain a new image with high\nresolution in both the spatial and spectral domains. These remote sensing\nproducts are valuable for a wide range of applications, driving ever growing\nresearch efforts. Nonetheless, results still do not meet application demands.\nIn part, this comes from the technical complexity of the task: compared to\nmultispectral pansharpening, many more bands are involved, in a spectral range\nonly partially covered by the panchromatic component and with overwhelming\nnoise. However, another major limiting factor is the absence of a comprehensive\nframework for the rapid development and accurate evaluation of new methods.\nThis paper attempts to address this issue.\n  We started by designing a dataset large and diverse enough to allow reliable\ntraining (for data-driven methods) and testing of new methods. Then, we\nselected a set of state-of-the-art methods, following different approaches,\ncharacterized by promising performance, and reimplemented them in a single\nPyTorch framework. Finally, we carried out a critical comparative analysis of\nall methods, using the most accredited quality indicators. The analysis\nhighlights the main limitations of current solutions in terms of\nspectral/spatial quality and computational efficiency, and suggests promising\nresearch directions.\n  To ensure full reproducibility of the results and support future research,\nthe framework (including codes, evaluation procedures and links to the dataset)\nis shared on https://github.com/matciotola/hyperspectral_pansharpening_toolbox,\nas a single Python-based reference benchmark toolbox.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01355v2",
    "published_date": "2024-07-01 15:10:50 UTC",
    "updated_date": "2024-12-27 10:52:39 UTC"
  },
  {
    "arxiv_id": "2407.01343v1",
    "title": "Coordination Failure in Cooperative Offline MARL",
    "authors": [
      "Callum Rhys Tilbury",
      "Claude Formanek",
      "Louise Beyers",
      "Jonathan P. Shock",
      "Arnu Pretorius"
    ],
    "abstract": "Offline multi-agent reinforcement learning (MARL) leverages static datasets\nof experience to learn optimal multi-agent control. However, learning from\nstatic data presents several unique challenges to overcome. In this paper, we\nfocus on coordination failure and investigate the role of joint actions in\nmulti-agent policy gradients with offline data, focusing on a common setting we\nrefer to as the 'Best Response Under Data' (BRUD) approach. By using two-player\npolynomial games as an analytical tool, we demonstrate a simple yet overlooked\nfailure mode of BRUD-based algorithms, which can lead to catastrophic\ncoordination failure in the offline setting. Building on these insights, we\npropose an approach to mitigate such failure, by prioritising samples from the\ndataset based on joint-action similarity during policy learning and demonstrate\nits effectiveness in detailed experiments. More generally, however, we argue\nthat prioritised dataset sampling is a promising area for innovation in offline\nMARL that can be combined with other effective approaches such as critic and\npolicy regularisation. Importantly, our work shows how insights drawn from\nsimplified, tractable games can lead to useful, theoretically grounded insights\nthat transfer to more complex contexts. A core dimension of offering is an\ninteractive notebook, from which almost all of our results can be reproduced,\nin a browser.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Workshop on Aligning Reinforcement Learning\n  Experimentalists and Theorists (ARLET) at the International Conference on\n  Machine Learning, 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01343v1",
    "published_date": "2024-07-01 14:51:29 UTC",
    "updated_date": "2024-07-01 14:51:29 UTC"
  },
  {
    "arxiv_id": "2407.01333v1",
    "title": "Deep Reinforcement Learning for Adverse Garage Scenario Generation",
    "authors": [
      "Kai Li"
    ],
    "abstract": "Autonomous vehicles need to travel over 11 billion miles to ensure their\nsafety. Therefore, the importance of simulation testing before real-world\ntesting is self-evident. In recent years, the release of 3D simulators for\nautonomous driving, represented by Carla and CarSim, marks the transition of\nautonomous driving simulation testing environments from simple 2D overhead\nviews to complex 3D models. During simulation testing, experimenters need to\nbuild static scenes and dynamic traffic flows, pedestrian flows, and other\nexperimental elements to construct experimental scenarios. When building static\nscenes in 3D simulators, experimenters often need to manually construct 3D\nmodels, set parameters and attributes, which is time-consuming and\nlabor-intensive. This thesis proposes an automated program generation\nframework. Based on deep reinforcement learning, this framework can generate\ndifferent 2D ground script codes, on which 3D model files and map model files\nare built. The generated 3D ground scenes are displayed in the Carla simulator,\nwhere experimenters can use this scene for navigation algorithm simulation\ntesting.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "I.2.0; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01333v1",
    "published_date": "2024-07-01 14:41:18 UTC",
    "updated_date": "2024-07-01 14:41:18 UTC"
  },
  {
    "arxiv_id": "2407.01331v2",
    "title": "Restyling Unsupervised Concept Based Interpretable Networks with Generative Models",
    "authors": [
      "Jayneel Parekh",
      "Quentin Bouniot",
      "Pavlo Mozharovskyi",
      "Alasdair Newson",
      "Florence d'Alch√©-Buc"
    ],
    "abstract": "Developing inherently interpretable models for prediction has gained\nprominence in recent years. A subclass of these models, wherein the\ninterpretable network relies on learning high-level concepts, are valued\nbecause of closeness of concept representations to human communication.\nHowever, the visualization and understanding of the learnt unsupervised\ndictionary of concepts encounters major limitations, especially for large-scale\nimages. We propose here a novel method that relies on mapping the concept\nfeatures to the latent space of a pretrained generative model. The use of a\ngenerative model enables high quality visualization, and lays out an intuitive\nand interactive procedure for better interpretation of the learnt concepts by\nimputing concept activations and visualizing generated modifications.\nFurthermore, leveraging pretrained generative models has the additional\nadvantage of making the training of the system more efficient. We\nquantitatively ascertain the efficacy of our method in terms of accuracy of the\ninterpretable prediction network, fidelity of reconstruction, as well as\nfaithfulness and consistency of learnt concepts. The experiments are conducted\non multiple image recognition benchmarks for large-scale images. Project page\navailable at https://jayneelparekh.github.io/VisCoIN_project_page/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ICLR 2025. Project page available at\n  https://jayneelparekh.github.io/VisCoIN_project_page/",
    "pdf_url": "http://arxiv.org/pdf/2407.01331v2",
    "published_date": "2024-07-01 14:39:41 UTC",
    "updated_date": "2025-03-18 21:58:33 UTC"
  },
  {
    "arxiv_id": "2407.01320v1",
    "title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
    "authors": [
      "Haobo Song",
      "Hao Zhao",
      "Soumajit Majumder",
      "Tao Lin"
    ],
    "abstract": "Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has\nattracted more attention for downstream tasks recently. While\nparameter-efficient fine-tuning methods have been proposed and proven effective\nwithout retraining all model parameters, their performance is limited by the\ncapacity of incremental modules, especially under constrained parameter\nbudgets. \\\\ To overcome this challenge, we propose CapaBoost, a simple yet\neffective strategy that enhances model capacity by leveraging low-rank updates\nthrough parallel weight modules in target layers. By applying static random\nmasks to the shared weight matrix, CapaBoost constructs a diverse set of weight\nmatrices, effectively increasing the rank of incremental weights without adding\nparameters. Notably, our approach can be seamlessly integrated into various\nexisting parameter-efficient fine-tuning methods. We extensively validate the\nefficacy of CapaBoost through experiments on diverse downstream tasks,\nincluding natural language understanding, question answering, and image\nclassification. Our results demonstrate significant improvements over\nbaselines, without incurring additional computation or storage costs. Our code\nis available at \\url{https://github.com/LINs-lab/CapaBoost}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024. Code at https://github.com/LINs-lab/CapaBoost",
    "pdf_url": "http://arxiv.org/pdf/2407.01320v1",
    "published_date": "2024-07-01 14:26:48 UTC",
    "updated_date": "2024-07-01 14:26:48 UTC"
  },
  {
    "arxiv_id": "2407.01318v1",
    "title": "Deep Dive into MRI: Exploring Deep Learning Applications in 0.55T and 7T MRI",
    "authors": [
      "Ana Carolina Alves",
      "Andr√© Ferreira",
      "Behrus Puladi",
      "Jan Egger",
      "Victor Alves"
    ],
    "abstract": "The development of magnetic resonance imaging (MRI) for medical imaging has\nprovided a leap forward in diagnosis, providing a safe, non-invasive\nalternative to techniques involving ionising radiation exposure for diagnostic\npurposes. It was described by Block and Purcel in 1946, and it was not until\n1980 that the first clinical application of MRI became available. Since that\ntime the MRI has gone through many advances and has altered the way diagnosing\nprocedures are performed. Due to its ability to improve constantly, MRI has\nbecome a commonly used practice among several specialisations in medicine.\nParticularly starting 0.55T and 7T MRI technologies have pointed out enhanced\npreservation of image detail and advanced tissue characterisation. This review\nexamines the integration of deep learning (DL) techniques into these MRI\nmodalities, disseminating and exploring the study applications. It highlights\nhow DL contributes to 0.55T and 7T MRI data, showcasing the potential of DL in\nimproving and refining these technologies. The review ends with a brief\noverview of how MRI technology will evolve in the coming years.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01318v1",
    "published_date": "2024-07-01 14:26:31 UTC",
    "updated_date": "2024-07-01 14:26:31 UTC"
  },
  {
    "arxiv_id": "2407.01317v1",
    "title": "Leveraging Speaker Embeddings in End-to-End Neural Diarization for Two-Speaker Scenarios",
    "authors": [
      "Juan Ignacio Alvarez-Trejos",
      "Beltr√°n Labrador",
      "Alicia Lozano-Diez"
    ],
    "abstract": "End-to-end neural speaker diarization systems are able to address the speaker\ndiarization task while effectively handling speech overlap. This work explores\nthe incorporation of speaker information embeddings into the end-to-end systems\nto enhance the speaker discriminative capabilities, while maintaining their\noverlap handling strengths. To achieve this, we propose several methods for\nincorporating these embeddings along the acoustic features. Furthermore, we\ndelve into an analysis of the correct handling of silence frames, the window\nlength for extracting speaker embeddings and the transformer encoder size. The\neffectiveness of our proposed approach is thoroughly evaluated on the CallHome\ndataset for the two-speaker diarization task, with results that demonstrate a\nsignificant reduction in diarization error rates achieving a relative\nimprovement of a 10.78% compared to the baseline end-to-end model.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to Odyssey 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01317v1",
    "published_date": "2024-07-01 14:26:28 UTC",
    "updated_date": "2024-07-01 14:26:28 UTC"
  },
  {
    "arxiv_id": "2407.01302v2",
    "title": "Robot Instance Segmentation with Few Annotations for Grasping",
    "authors": [
      "Moshe Kimhi",
      "David Vainshtein",
      "Chaim Baskin",
      "Dotan Di Castro"
    ],
    "abstract": "The ability of robots to manipulate objects relies heavily on their aptitude\nfor visual perception. In domains characterized by cluttered scenes and high\nobject variability, most methods call for vast labeled datasets, laboriously\nhand-annotated, with the aim of training capable models. Once deployed, the\nchallenge of generalizing to unfamiliar objects implies that the model must\nevolve alongside its domain. To address this, we propose a novel framework that\ncombines Semi-Supervised Learning (SSL) with Learning Through Interaction\n(LTI), allowing a model to learn by observing scene alterations and leverage\nvisual consistency despite temporal gaps without requiring curated data of\ninteraction sequences. As a result, our approach exploits partially annotated\ndata through self-supervision and incorporates temporal context using\npseudo-sequences generated from unlabeled still images. We validate our method\non two common benchmarks, ARMBench mix-object-tote and OCID, where it achieves\nstate-of-the-art performance. Notably, on ARMBench, we attain an\n$\\text{AP}_{50}$ of $86.37$, almost a $20\\%$ improvement over existing work,\nand obtain remarkable results in scenarios with extremely low annotation,\nachieving an $\\text{AP}_{50}$ score of $84.89$ with just $1 \\%$ of annotated\ndata compared to $72$ presented in ARMBench on the fully annotated counterpart.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01302v2",
    "published_date": "2024-07-01 13:58:32 UTC",
    "updated_date": "2025-02-11 19:56:18 UTC"
  },
  {
    "arxiv_id": "2407.01300v2",
    "title": "Collaborative Performance Prediction for Large Language Models",
    "authors": [
      "Qiyuan Zhang",
      "Fuyuan Lyu",
      "Xue Liu",
      "Chen Ma"
    ],
    "abstract": "Comprehensively understanding and accurately predicting the performance of\nlarge language models across diverse downstream tasks has emerged as a pivotal\nchallenge in NLP research. The pioneering scaling law on downstream works\ndemonstrated intrinsic similarities within model families and utilized such\nsimilarities for performance prediction. However, they tend to overlook the\nsimilarities between model families and only consider design factors listed in\nthe original scaling law. To overcome these limitations, we introduce a novel\nframework, Collaborative Performance Prediction (CPP), which significantly\nenhances prediction accuracy by leveraging the historical performance of\nvarious models on downstream tasks and other design factors for both model and\ntask. We also collect a collaborative data sourced from online platforms\ncontaining both historical performance and additional design factors. With the\nsupport of the collaborative data, CPP not only surpasses traditional scaling\nlaws in predicting the performance of scaled LLMs but also facilitates a\ndetailed analysis of factor importance, an area previously overlooked.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "In Proceedings of EMNLP 2024 Main Track",
    "pdf_url": "http://arxiv.org/pdf/2407.01300v2",
    "published_date": "2024-07-01 13:56:42 UTC",
    "updated_date": "2024-10-02 18:41:02 UTC"
  },
  {
    "arxiv_id": "2407.11021v1",
    "title": "PCAPVision: PCAP-Based High-Velocity and Large-Volume Network Failure Detection",
    "authors": [
      "Lukasz Tulczyjew",
      "Ihor Biruk",
      "Murat Bilgic",
      "Charles Abondo",
      "Nathanael Weill"
    ],
    "abstract": "Detecting failures via analysis of Packet Capture (PCAP) files is crucial for\nmaintaining network reliability and performance, especially in large-scale\ntelecommunications networks. Traditional methods, relying on manual inspection\nand rule-based systems, are often too slow and labor-intensive to meet the\ndemands of modern networks. In this paper, we present PCAPVision, a novel\napproach that utilizes computer vision and Convolutional Neural Networks (CNNs)\nto detect failures in PCAP files. By converting PCAP data into images, our\nmethod leverages the robust pattern recognition capabilities of CNNs to analyze\nnetwork traffic efficiently. This transformation process involves encoding\npacket data into structured images, enabling rapid and accurate failure\ndetection. Additionally, we incorporate a continual learning framework,\nleveraging automated annotation for the feedback loop, to adapt the model\ndynamically and ensure sustained performance over time. Our approach\nsignificantly reduces the time required for failure detection. The initial\ntraining phase uses a Voice Over LTE (VoLTE) dataset, demonstrating the model's\neffectiveness and generalizability when using transfer learning on Mobility\nManagement services. This work highlights the potential of integrating computer\nvision techniques in network analysis, offering a scalable and efficient\nsolution for real-time network failure detection.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "Copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2407.11021v1",
    "published_date": "2024-07-01 13:52:17 UTC",
    "updated_date": "2024-07-01 13:52:17 UTC"
  },
  {
    "arxiv_id": "2407.01294v2",
    "title": "A Collaborative, Human-Centred Taxonomy of AI, Algorithmic, and Automation Harms",
    "authors": [
      "Gavin Abercrombie",
      "Djalel Benbouzid",
      "Paolo Giudici",
      "Delaram Golpayegani",
      "Julio Hernandez",
      "Pierre Noro",
      "Harshvardhan Pandit",
      "Eva Paraschou",
      "Charlie Pownall",
      "Jyoti Prajapati",
      "Mark A. Sayre",
      "Ushnish Sengupta",
      "Arthit Suriyawongkul",
      "Ruby Thelot",
      "Sofia Vei",
      "Laura Waltersdorfer"
    ],
    "abstract": "This paper introduces a collaborative, human-centred taxonomy of AI,\nalgorithmic and automation harms. We argue that existing taxonomies, while\nvaluable, can be narrow, unclear, typically cater to practitioners and\ngovernment, and often overlook the needs of the wider public. Drawing on\nexisting taxonomies and a large repository of documented incidents, we propose\na taxonomy that is clear and understandable to a broad set of audiences, as\nwell as being flexible, extensible, and interoperable. Through iterative\nrefinement with topic experts and crowdsourced annotation testing, we propose a\ntaxonomy that can serve as a powerful tool for civil society organisations,\neducators, policymakers, product teams and the general public. By fostering a\ngreater understanding of the real-world harms of AI and related technologies,\nwe aim to increase understanding, empower NGOs and individuals to identify and\nreport violations, inform policy discussions, and encourage responsible\ntechnology development and deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "K.4.1; I.2.0; H.0; K.5.2"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 2 figures; typos corrected, reference missing fields fixed,\n  printer-friendly version of a diagram added",
    "pdf_url": "http://arxiv.org/pdf/2407.01294v2",
    "published_date": "2024-07-01 13:47:53 UTC",
    "updated_date": "2024-11-09 12:00:09 UTC"
  },
  {
    "arxiv_id": "2407.01290v1",
    "title": "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space",
    "authors": [
      "Menglin Yang",
      "Harshit Verma",
      "Delvin Ce Zhang",
      "Jiahong Liu",
      "Irwin King",
      "Rex Ying"
    ],
    "abstract": "Hyperbolic geometry have shown significant potential in modeling complex\nstructured data, particularly those with underlying tree-like and hierarchical\nstructures. Despite the impressive performance of various hyperbolic neural\nnetworks across numerous domains, research on adapting the Transformer to\nhyperbolic space remains limited. Previous attempts have mainly focused on\nmodifying self-attention modules in the Transformer. However, these efforts\nhave fallen short of developing a complete hyperbolic Transformer. This stems\nprimarily from: (i) the absence of well-defined modules in hyperbolic space,\nincluding linear transformation layers, LayerNorm layers, activation functions,\ndropout operations, etc. (ii) the quadratic time complexity of the existing\nhyperbolic self-attention module w.r.t the number of input tokens, which\nhinders its scalability. To address these challenges, we propose, Hypformer, a\nnovel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry.\nIn Hypformer, we introduce two foundational blocks that define the essential\nmodules of the Transformer in hyperbolic space. Furthermore, we develop a\nlinear self-attention mechanism in hyperbolic space, enabling hyperbolic\nTransformer to process billion-scale graph data and long-sequence inputs for\nthe first time. Our experimental results confirm the effectiveness and\nefficiency of Hypformer across various datasets, demonstrating its potential as\nan effective and scalable solution for large-scale data representation and\nlarge models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01290v1",
    "published_date": "2024-07-01 13:44:38 UTC",
    "updated_date": "2024-07-01 13:44:38 UTC"
  },
  {
    "arxiv_id": "2407.01284v1",
    "title": "We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?",
    "authors": [
      "Runqi Qiao",
      "Qiuna Tan",
      "Guanting Dong",
      "Minhui Wu",
      "Chong Sun",
      "Xiaoshuai Song",
      "Zhuoma GongQue",
      "Shanglin Lei",
      "Zhe Wei",
      "Miaoxuan Zhang",
      "Runfeng Qiao",
      "Yifan Zhang",
      "Xiao Zong",
      "Yida Xu",
      "Muxi Diao",
      "Zhimin Bao",
      "Chen Li",
      "Honggang Zhang"
    ],
    "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs)\ncommunity. Existing benchmarks, such as MathVista and MathVerse, focus more on\nthe result-oriented performance but neglect the underlying principles in\nknowledge acquisition and generalization. Inspired by human-like mathematical\nreasoning, we introduce WE-MATH, the first benchmark specifically designed to\nexplore the problem-solving principles beyond end-to-end performance. We\nmeticulously collect and categorize 6.5K visual math problems, spanning 67\nhierarchical knowledge concepts and five layers of knowledge granularity. We\ndecompose composite problems into sub-problems according to the required\nknowledge concepts and introduce a novel four-dimensional metric, namely\nInsufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery\n(CM), and Rote Memorization (RM), to hierarchically assess inherent issues in\nLMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of\nexisting LMMs in visual mathematical reasoning and reveal a negative\ncorrelation between solving steps and problem-specific performance. We confirm\nthe IK issue of LMMs can be effectively improved via knowledge augmentation\nstrategies. More notably, the primary challenge of GPT-4o has significantly\ntransitioned from IK to IG, establishing it as the first LMM advancing towards\nthe knowledge generalization stage. In contrast, other LMMs exhibit a marked\ninclination towards Rote Memorization - they correctly solve composite problems\ninvolving multiple knowledge concepts yet fail to answer sub-problems. We\nanticipate that WE-MATH will open new pathways for advancements in visual\nmathematical reasoning for LMMs. The WE-MATH data and evaluation code are\navailable at https://github.com/We-Math/We-Math.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2407.01284v1",
    "published_date": "2024-07-01 13:39:08 UTC",
    "updated_date": "2024-07-01 13:39:08 UTC"
  },
  {
    "arxiv_id": "2407.04736v1",
    "title": "SCDM: Unified Representation Learning for EEG-to-fNIRS Cross-Modal Generation in MI-BCIs",
    "authors": [
      "Yisheng Li",
      "Shuqiang Wang"
    ],
    "abstract": "Hybrid motor imagery brain-computer interfaces (MI-BCIs), which integrate\nboth electroencephalography (EEG) and functional near-infrared spectroscopy\n(fNIRS) signals, outperform those based solely on EEG. However, simultaneously\nrecording EEG and fNIRS signals is highly challenging due to the difficulty of\ncolocating both types of sensors on the same scalp surface. This physical\nconstraint complicates the acquisition of high-quality hybrid signals, thereby\nlimiting the widespread application of hybrid MI-BCIs. To facilitate the\nacquisition of hybrid EEG-fNIRS signals, this study proposes the\nspatio-temporal controlled diffusion model (SCDM) as a framework for\ncross-modal generation from EEG to fNIRS. The model utilizes two core modules,\nthe spatial cross-modal generation (SCG) module and the multi-scale temporal\nrepresentation (MTR) module, which adaptively learn the respective latent\ntemporal and spatial representations of both signals in a unified\nrepresentation space. The SCG module further maps EEG representations to fNIRS\nrepresentations by leveraging their spatial relationships. Experimental results\nshow high similarity between synthetic and real fNIRS signals. The joint\nclassification performance of EEG and synthetic fNIRS signals is comparable to\nor even better than that of EEG with real fNIRS signals. Furthermore, the\nsynthetic signals exhibit similar spatio-temporal features to real signals\nwhile preserving spatial relationships with EEG signals. Experimental results\nsuggest that the SCDM may represent a promising paradigm for the acquisition of\nhybrid EEG-fNIRS signals in MI-BCI systems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.04736v1",
    "published_date": "2024-07-01 13:37:23 UTC",
    "updated_date": "2024-07-01 13:37:23 UTC"
  },
  {
    "arxiv_id": "2407.01281v2",
    "title": "Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks",
    "authors": [
      "Guangrui Yang",
      "Jianfei Li",
      "Ming Li",
      "Han Feng",
      "Ding-Xuan Zhou"
    ],
    "abstract": "In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.FA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01281v2",
    "published_date": "2024-07-01 13:35:53 UTC",
    "updated_date": "2024-08-05 15:50:32 UTC"
  },
  {
    "arxiv_id": "2407.01280v1",
    "title": "Human-Robot Mutual Learning through Affective-Linguistic Interaction and Differential Outcomes Training [Pre-Print]",
    "authors": [
      "Emilia Heikkinen",
      "Elsa Silvennoinen",
      "Imran Khan",
      "Zakaria Lemhaouri",
      "Laura Cohen",
      "Lola Ca√±amero",
      "Robert Lowe"
    ],
    "abstract": "Owing to the recent success of Large Language Models, Modern A.I has been\nmuch focused on linguistic interactions with humans but less focused on\nnon-linguistic forms of communication between man and machine. In the present\npaper, we test how affective-linguistic communication, in combination with\ndifferential outcomes training, affects mutual learning in a human-robot\ncontext. Taking inspiration from child-caregiver dynamics, our human-robot\ninteraction setup consists of a (simulated) robot attempting to learn how best\nto communicate internal, homeostatically-controlled needs; while a human\n\"caregiver\" attempts to learn the correct object to satisfy the robot's present\ncommunicated need. We studied the effects of i) human training type, and ii)\nrobot reinforcement learning type, to assess mutual learning terminal accuracy\nand rate of learning (as measured by the average reward achieved by the robot).\nOur results find mutual learning between a human and a robot is significantly\nimproved with Differential Outcomes Training (DOT) compared to Non-DOT\n(control) conditions. We find further improvements when the robot uses an\nexploration-exploitation policy selection, compared to purely exploitation\npolicy selection. These findings have implications for utilizing socially\nassistive robots (SAR) in therapeutic contexts, e.g. for cognitive\ninterventions, and educational applications.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, with references; 1 figure, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.01280v1",
    "published_date": "2024-07-01 13:35:08 UTC",
    "updated_date": "2024-07-01 13:35:08 UTC"
  },
  {
    "arxiv_id": "2407.01270v2",
    "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation",
    "authors": [
      "Serene Lim",
      "Mar√≠a P√©rez-Ortiz"
    ],
    "abstract": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01270v2",
    "published_date": "2024-07-01 13:21:33 UTC",
    "updated_date": "2024-09-30 16:39:51 UTC"
  },
  {
    "arxiv_id": "2407.12038v2",
    "title": "ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024",
    "authors": [
      "Ruibo Fu",
      "Rui Liu",
      "Chunyu Qiang",
      "Yingming Gao",
      "Yi Lu",
      "Shuchen Shi",
      "Tao Wang",
      "Ya Li",
      "Zhengqi Wen",
      "Chen Zhang",
      "Hui Bu",
      "Yukun Liu",
      "Xin Qi",
      "Guanjun Li"
    ],
    "abstract": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024)\nis part of the ISCSLP 2024 Competitions and Challenges track. While current\ntext-to-speech (TTS) technology can generate high-quality audio, its ability to\nconvey complex emotions and controlled detail content remains limited. This\nconstraint leads to a discrepancy between the generated audio and human\nsubjective perception in practical applications like companion robots for\nchildren and marketing bots. The core issue lies in the inconsistency between\nhigh-quality audio generation and the ultimate human subjective experience.\nTherefore, this challenge aims to enhance the persuasiveness and acceptability\nof synthesized audio, focusing on human alignment convincing and inspirational\naudio generation. A total of 19 teams have registered for the challenge, and\nthe results of the competition and the competition are described in this paper.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "ISCSLP 2024 Challenge description and results",
    "pdf_url": "http://arxiv.org/pdf/2407.12038v2",
    "published_date": "2024-07-01 13:15:16 UTC",
    "updated_date": "2024-07-31 14:23:00 UTC"
  },
  {
    "arxiv_id": "2407.12820v2",
    "title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference",
    "authors": [
      "Hailin Zhang",
      "Xiaodong Ji",
      "Yilin Chen",
      "Fangcheng Fu",
      "Xupeng Miao",
      "Xiaonan Nie",
      "Weipeng Chen",
      "Bin Cui"
    ],
    "abstract": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12820v2",
    "published_date": "2024-07-01 13:05:42 UTC",
    "updated_date": "2025-03-30 08:13:50 UTC"
  },
  {
    "arxiv_id": "2407.01251v1",
    "title": "QUEEN: Query Unlearning against Model Extraction",
    "authors": [
      "Huajie Chen",
      "Tianqing Zhu",
      "Lefeng Zhang",
      "Bo Liu",
      "Derui Wang",
      "Wanlei Zhou",
      "Minhui Xue"
    ],
    "abstract": "Model extraction attacks currently pose a non-negligible threat to the\nsecurity and privacy of deep learning models. By querying the model with a\nsmall dataset and usingthe query results as the ground-truth labels, an\nadversary can steal a piracy model with performance comparable to the original\nmodel. Two key issues that cause the threat are, on the one hand, accurate and\nunlimited queries can be obtained by the adversary; on the other hand, the\nadversary can aggregate the query results to train the model step by step. The\nexisting defenses usually employ model watermarking or fingerprinting to\nprotect the ownership. However, these methods cannot proactively prevent the\nviolation from happening. To mitigate the threat, we propose QUEEN (QUEry\nunlEarNing) that proactively launches counterattacks on potential model\nextraction attacks from the very beginning. To limit the potential threat,\nQUEEN has sensitivity measurement and outputs perturbation that prevents the\nadversary from training a piracy model with high performance. In sensitivity\nmeasurement, QUEEN measures the single query sensitivity by its distance from\nthe center of its cluster in the feature space. To reduce the learning accuracy\nof attacks, for the highly sensitive query batch, QUEEN applies query\nunlearning, which is implemented by gradient reverse to perturb the softmax\noutput such that the piracy model will generate reverse gradients to worsen its\nperformance unconsciously. Experiments show that QUEEN outperforms the\nstate-of-the-art defenses against various model extraction attacks with a\nrelatively low cost to the model accuracy. The artifact is publicly available\nat https://anonymous.4open.science/r/queen implementation-5408/.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01251v1",
    "published_date": "2024-07-01 13:01:41 UTC",
    "updated_date": "2024-07-01 13:01:41 UTC"
  },
  {
    "arxiv_id": "2407.01653v1",
    "title": "A Deep Reinforcement Learning Approach to Battery Management in Dairy Farming via Proximal Policy Optimization",
    "authors": [
      "Nawazish Ali",
      "Rachael Shaw",
      "Karl Mason"
    ],
    "abstract": "Dairy farms consume a significant amount of electricity for their operations,\nand this research focuses on enhancing energy efficiency and minimizing the\nimpact on the environment in the sector by maximizing the utilization of\nrenewable energy sources. This research investigates the application of\nProximal Policy Optimization (PPO), a deep reinforcement learning algorithm\n(DRL), to enhance dairy farming battery management. We evaluate the algorithm's\neffectiveness based on its ability to reduce reliance on the electricity grid,\nhighlighting the potential of DRL to enhance energy management in dairy\nfarming. Using real-world data our results demonstrate how the PPO approach\noutperforms Q-learning by 1.62% for reducing electricity import from the grid.\nThis significant improvement highlights the potential of the Deep Reinforcement\nLearning algorithm for improving energy efficiency and sustainability in dairy\nfarms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures, Practical Applications of Agents and Multi-Agent\n  Systems(PAAMS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.01653v1",
    "published_date": "2024-07-01 12:46:09 UTC",
    "updated_date": "2024-07-01 12:46:09 UTC"
  },
  {
    "arxiv_id": "2407.01245v2",
    "title": "SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model",
    "authors": [
      "Lingyue Fu",
      "Hao Guan",
      "Kounianhua Du",
      "Jianghao Lin",
      "Wei Xia",
      "Weinan Zhang",
      "Ruiming Tang",
      "Yasheng Wang",
      "Yong Yu"
    ],
    "abstract": "Knowledge Tracing (KT) aims to determine whether students will respond\ncorrectly to the next question, which is a crucial task in intelligent tutoring\nsystems (ITS). In educational KT scenarios, transductive ID-based methods often\nface severe data sparsity and cold start problems, where interactions between\nindividual students and questions are sparse, and new questions and concepts\nconsistently arrive in the database. In addition, existing KT models only\nimplicitly consider the correlation between concepts and questions, lacking\ndirect modeling of the more complex relationships in the heterogeneous graph of\nconcepts and questions. In this paper, we propose a Structure-aware Inductive\nKnowledge Tracing model with large language model (dubbed SINKT), which, for\nthe first time, introduces large language models (LLMs) and realizes inductive\nknowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural\nrelationships between concepts and constructs a heterogeneous graph for\nconcepts and questions. Secondly, by encoding concepts and questions with LLMs,\nSINKT incorporates semantic information to aid prediction. Finally, SINKT\npredicts the student's response to the target question by interacting with the\nstudent's knowledge state and the question representation. Experiments on four\nreal-world datasets demonstrate that SINKT achieves state-of-the-art\nperformance among 12 existing transductive KT models. Additionally, we explore\nthe performance of SINKT on the inductive KT task and provide insights into\nvarious modules.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01245v2",
    "published_date": "2024-07-01 12:44:52 UTC",
    "updated_date": "2024-07-23 12:23:38 UTC"
  },
  {
    "arxiv_id": "2407.01239v1",
    "title": "SGCCNet: Single-Stage 3D Object Detector With Saliency-Guided Data Augmentation and Confidence Correction Mechanism",
    "authors": [
      "Ao Liang",
      "Wenyu Chen",
      "Jian Fang",
      "Huaici Zhao"
    ],
    "abstract": "The single-stage point-based 3D object detectors have attracted widespread\nresearch interest due to their advantages of lightweight and fast inference\nspeed. However, they still face challenges such as inadequate learning of\nlow-quality objects (ILQ) and misalignment between localization accuracy and\nclassification confidence (MLC). In this paper, we propose SGCCNet to alleviate\nthese two issues. For ILQ, SGCCNet adopts a Saliency-Guided Data Augmentation\n(SGDA) strategy to enhance the robustness of the model on low-quality objects\nby reducing its reliance on salient features. Specifically, We construct a\nclassification task and then approximate the saliency scores of points by\nmoving points towards the point cloud centroid in a differentiable process.\nDuring the training process, SGCCNet will be forced to learn from low saliency\nfeatures through dropping points. Meanwhile, to avoid internal covariate shift\nand contextual features forgetting caused by dropping points, we add a\ngeometric normalization module and skip connection block in each stage. For\nMLC, we design a Confidence Correction Mechanism (CCM) specifically for\npoint-based multi-class detectors. This mechanism corrects the confidence of\nthe current proposal by utilizing the predictions of other key points within\nthe local region in the post-processing stage. Extensive experiments on the\nKITTI dataset demonstrate the generality and effectiveness of our SGCCNet. On\nthe KITTI \\textit{test} set, SGCCNet achieves $80.82\\%$ for the metric of\n$AP_{3D}$ on the \\textit{Moderate} level, outperforming all other point-based\ndetectors, surpassing IA-SSD and Fast Point R-CNN by $2.35\\%$ and $3.42\\%$,\nrespectively. Additionally, SGCCNet demonstrates excellent portability for\nother point-based detectors",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01239v1",
    "published_date": "2024-07-01 12:36:01 UTC",
    "updated_date": "2024-07-01 12:36:01 UTC"
  },
  {
    "arxiv_id": "2407.01238v3",
    "title": "Large Language Models are Zero-Shot Recognizers for Activities of Daily Living",
    "authors": [
      "Gabriele Civitarese",
      "Michele Fiori",
      "Priyankar Choudhary",
      "Claudio Bettini"
    ],
    "abstract": "The sensor-based recognition of Activities of Daily Living (ADLs) in smart\nhome environments enables several applications in the areas of energy\nmanagement, safety, well-being, and healthcare. ADLs recognition is typically\nbased on deep learning methods requiring large datasets to be trained.\nRecently, several studies proved that Large Language Models (LLMs) effectively\ncapture common-sense knowledge about human activities. However, the\neffectiveness of LLMs for ADLs recognition in smart home environments still\ndeserves to be investigated. In this work, we propose ADL-LLM, a novel\nLLM-based ADLs recognition system. ADLLLM transforms raw sensor data into\ntextual representations, that are processed by an LLM to perform zero-shot ADLs\nrecognition. Moreover, in the scenario where a small labeled dataset is\navailable, ADL-LLM can also be empowered with few-shot prompting. We evaluated\nADL-LLM on two public datasets, showing its effectiveness in this domain.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted for publication in the ACM Transactions on Intelligent\n  Systems and Technology (TIST) journal",
    "pdf_url": "http://arxiv.org/pdf/2407.01238v3",
    "published_date": "2024-07-01 12:32:38 UTC",
    "updated_date": "2025-03-20 20:43:37 UTC"
  },
  {
    "arxiv_id": "2407.01231v1",
    "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
    "authors": [
      "Chenchen Ye",
      "Ziniu Hu",
      "Yihe Deng",
      "Zijie Huang",
      "Mingyu Derek Ma",
      "Yanqiao Zhu",
      "Wei Wang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "66 pages, 8 figures, 6 tables; Website: https://mirai-llm.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.01231v1",
    "published_date": "2024-07-01 12:22:46 UTC",
    "updated_date": "2024-07-01 12:22:46 UTC"
  },
  {
    "arxiv_id": "2407.01216v1",
    "title": "Let Hybrid A* Path Planner Obey Traffic Rules: A Deep Reinforcement Learning-Based Planning Framework",
    "authors": [
      "Xibo Li",
      "Shruti Patel",
      "Christof B√ºskens"
    ],
    "abstract": "Deep reinforcement learning (DRL) allows a system to interact with its\nenvironment and take actions by training an efficient policy that maximizes\nself-defined rewards. In autonomous driving, it can be used as a strategy for\nhigh-level decision making, whereas low-level algorithms such as the hybrid A*\npath planning have proven their ability to solve the local trajectory planning\nproblem. In this work, we combine these two methods where the DRL makes\nhigh-level decisions such as lane change commands. After obtaining the lane\nchange command, the hybrid A* planner is able to generate a collision-free\ntrajectory to be executed by a model predictive controller (MPC). In addition,\nthe DRL algorithm is able to keep the lane change command consistent within a\nchosen time-period. Traffic rules are implemented using linear temporal logic\n(LTL), which is then utilized as a reward function in DRL. Furthermore, we\nvalidate the proposed method on a real system to demonstrate its feasibility\nfrom simulation to implementation on real hardware.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01216v1",
    "published_date": "2024-07-01 12:00:10 UTC",
    "updated_date": "2024-07-01 12:00:10 UTC"
  },
  {
    "arxiv_id": "2407.01214v3",
    "title": "Revisiting Random Walks for Learning on Graphs",
    "authors": [
      "Jinwoo Kim",
      "Olga Zaghen",
      "Ayhan Suleymanzade",
      "Youngmin Ryou",
      "Seunghoon Hong"
    ],
    "abstract": "We revisit a simple model class for machine learning on graphs, where a\nrandom walk on a graph produces a machine-readable record, and this record is\nprocessed by a deep neural network to directly make vertex-level or graph-level\npredictions. We call these stochastic machines random walk neural networks\n(RWNNs), and through principled analysis, show that we can design them to be\nisomorphism invariant while capable of universal approximation of graph\nfunctions in probability. A useful finding is that almost any kind of record of\nrandom walks guarantees probabilistic invariance as long as the vertices are\nanonymized. This enables us, for example, to record random walks in plain text\nand adopt a language model to read these text records to solve graph tasks. We\nfurther establish a parallelism to message passing neural networks using tools\nfrom Markov chain theory, and show that over-smoothing in message passing is\nalleviated by construction in RWNNs, while over-squashing manifests as\nprobabilistic under-reaching. We empirically demonstrate RWNNs on a range of\nproblems, verifying our theoretical analysis and demonstrating the use of\nlanguage models for separating strongly regular graphs where 3-WL test fails,\nand transductive classification on arXiv citation network. Code is available at\nhttps://github.com/jw9730/random-walk.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "51 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01214v3",
    "published_date": "2024-07-01 11:59:59 UTC",
    "updated_date": "2025-03-05 07:02:28 UTC"
  },
  {
    "arxiv_id": "2407.01211v1",
    "title": "Efficient Cutting Tool Wear Segmentation Based on Segment Anything Model",
    "authors": [
      "Zongshuo Li",
      "Ding Huo",
      "Markus Meurer",
      "Thomas Bergs"
    ],
    "abstract": "Tool wear conditions impact the surface quality of the workpiece and its\nfinal geometric precision. In this research, we propose an efficient tool wear\nsegmentation approach based on Segment Anything Model, which integrates U-Net\nas an automated prompt generator to streamline the processes of tool wear\ndetection. Our evaluation covered three Point-of-Interest generation methods\nand further investigated the effects of variations in training dataset sizes\nand U-Net training intensities on resultant wear segmentation outcomes. The\nresults consistently highlight our approach's advantage over U-Net, emphasizing\nits ability to achieve accurate wear segmentation even with limited training\ndatasets. This feature underscores its potential applicability in industrial\nscenarios where datasets may be limited.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01211v1",
    "published_date": "2024-07-01 11:57:53 UTC",
    "updated_date": "2024-07-01 11:57:53 UTC"
  },
  {
    "arxiv_id": "2407.01200v1",
    "title": "Deep Learning Approach for Enhanced Transferability and Learning Capacity in Tool Wear Estimation",
    "authors": [
      "Zongshuo Li",
      "Markus Meurer",
      "Thomas Bergs"
    ],
    "abstract": "As an integral part of contemporary manufacturing, monitoring systems obtain\nvaluable information during machining to oversee the condition of both the\nprocess and the machine. Recently, diverse algorithms have been employed to\ndetect tool wear using single or multiple sources of measurements. In this\nstudy, a deep learning approach is proposed for estimating tool wear,\nconsidering cutting parameters. The model's accuracy and transferability in\ntool wear estimation were assessed with milling experiments conducted under\nvarying cutting parameters. The results indicate that the proposed method\noutperforms conventional methods in terms of both transferability and rapid\nlearning capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01200v1",
    "published_date": "2024-07-01 11:49:10 UTC",
    "updated_date": "2024-07-01 11:49:10 UTC"
  },
  {
    "arxiv_id": "2407.01199v1",
    "title": "Deep Learning Based Tool Wear Estimation Considering Cutting Conditions",
    "authors": [
      "Zongshuo Li",
      "Markus Meurer",
      "Thomas Bergs"
    ],
    "abstract": "Tool wear conditions impact the final quality of the workpiece. In this\nstudy, we propose a deep learning approach based on a convolutional neural\nnetwork that incorporates cutting conditions as extra model inputs, aiming to\nimprove tool wear estimation accuracy and fulfill industrial demands for\nzero-shot transferability. Through a series of milling experiments under\nvarious cutting parameters, we evaluate the model's performance in terms of\ntool wear estimation accuracy and its transferability to new fixed or variable\ncutting parameters. The results consistently highlight our approach's advantage\nover conventional models that omit cutting conditions, maintaining superior\nperformance irrespective of the stability of the wear development or the\nlimitation of the training dataset. This finding underscores its potential\napplicability in industrial scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01199v1",
    "published_date": "2024-07-01 11:48:33 UTC",
    "updated_date": "2024-07-01 11:48:33 UTC"
  },
  {
    "arxiv_id": "2407.01191v1",
    "title": "MARS: Multimodal Active Robotic Sensing for Articulated Characterization",
    "authors": [
      "Hongliang Zeng",
      "Ping Zhang",
      "Chengjiong Wu",
      "Jiahua Wang",
      "Tingyu Ye",
      "Fang Li"
    ],
    "abstract": "Precise perception of articulated objects is vital for empowering service\nrobots. Recent studies mainly focus on point cloud, a single-modal approach,\noften neglecting vital texture and lighting details and assuming ideal\nconditions like optimal viewpoints, unrepresentative of real-world scenarios.\nTo address these limitations, we introduce MARS, a novel framework for\narticulated object characterization. It features a multi-modal fusion module\nutilizing multi-scale RGB features to enhance point cloud features, coupled\nwith reinforcement learning-based active sensing for autonomous optimization of\nobservation viewpoints. In experiments conducted with various articulated\nobject instances from the PartNet-Mobility dataset, our method outperformed\ncurrent state-of-the-art methods in joint parameter estimation accuracy.\nAdditionally, through active sensing, MARS further reduces errors,\ndemonstrating enhanced efficiency in handling suboptimal viewpoints.\nFurthermore, our method effectively generalizes to real-world articulated\nobjects, enhancing robot interactions. Code is available at\nhttps://github.com/robhlzeng/MARS.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01191v1",
    "published_date": "2024-07-01 11:32:39 UTC",
    "updated_date": "2024-07-01 11:32:39 UTC"
  },
  {
    "arxiv_id": "2407.01178v1",
    "title": "$\\text{Memory}^3$: Language Modeling with Explicit Memory",
    "authors": [
      "Hongkang Yang",
      "Zehao Lin",
      "Wenjin Wang",
      "Hao Wu",
      "Zhiyu Li",
      "Bo Tang",
      "Wenqiang Wei",
      "Jinbo Wang",
      "Zeyun Tang",
      "Shichao Song",
      "Chenyang Xi",
      "Yu Yu",
      "Kai Chen",
      "Feiyu Xiong",
      "Linpeng Tang",
      "Weinan E"
    ],
    "abstract": "The training and inference of large language models (LLMs) are together a\ncostly process that transports knowledge from raw data to meaningful\ncomputation. Inspired by the memory hierarchy of the human brain, we reduce\nthis cost by equipping LLMs with explicit memory, a memory format cheaper than\nmodel parameters and text retrieval-augmented generation (RAG). Conceptually,\nwith most of its knowledge externalized to explicit memories, the LLM can enjoy\na smaller parameter size, training cost, and inference cost, all proportional\nto the amount of remaining \"abstract knowledge\". As a preliminary proof of\nconcept, we train from scratch a 2.4B LLM, which achieves better performance\nthan much larger LLMs as well as RAG models, and maintains higher decoding\nspeed than RAG. The model is named $\\text{Memory}^3$, since explicit memory is\nthe third form of memory in LLMs after implicit memory (model parameters) and\nworking memory (context key-values). We introduce a memory circuitry theory to\nsupport the externalization of knowledge, and present novel techniques\nincluding a memory sparsification mechanism that makes storage tractable and a\ntwo-stage pretraining scheme that facilitates memory formation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01178v1",
    "published_date": "2024-07-01 11:07:23 UTC",
    "updated_date": "2024-07-01 11:07:23 UTC"
  },
  {
    "arxiv_id": "2407.01168v2",
    "title": "Multi-View Black-Box Physical Attacks on Infrared Pedestrian Detectors Using Adversarial Infrared Grid",
    "authors": [
      "Kalibinuer Tiliwalidi",
      "Chengyin Hu",
      "Weiwen Shi"
    ],
    "abstract": "While extensive research exists on physical adversarial attacks within the\nvisible spectrum, studies on such techniques in the infrared spectrum are\nlimited. Infrared object detectors are vital in modern technological\napplications but are susceptible to adversarial attacks, posing significant\nsecurity threats. Previous studies using physical perturbations like light bulb\narrays and aerogels for white-box attacks, or hot and cold patches for\nblack-box attacks, have proven impractical or limited in multi-view support. To\naddress these issues, we propose the Adversarial Infrared Grid (AdvGrid), which\nmodels perturbations in a grid format and uses a genetic algorithm for\nblack-box optimization. These perturbations are cyclically applied to various\nparts of a pedestrian's clothing to facilitate multi-view black-box physical\nattacks on infrared pedestrian detectors. Extensive experiments validate\nAdvGrid's effectiveness, stealthiness, and robustness. The method achieves\nattack success rates of 80.00\\% in digital environments and 91.86\\% in physical\nenvironments, outperforming baseline methods. Additionally, the average attack\nsuccess rate exceeds 50\\% against mainstream detectors, demonstrating AdvGrid's\nrobustness. Our analyses include ablation studies, transfer attacks, and\nadversarial defenses, confirming the method's superiority.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01168v2",
    "published_date": "2024-07-01 10:38:08 UTC",
    "updated_date": "2024-07-08 14:17:26 UTC"
  },
  {
    "arxiv_id": "2407.01157v1",
    "title": "Unaligning Everything: Or Aligning Any Text to Any Image in Multimodal Models",
    "authors": [
      "Shaeke Salman",
      "Md Montasir Bin Shams",
      "Xiuwen Liu"
    ],
    "abstract": "Utilizing a shared embedding space, emerging multimodal models exhibit\nunprecedented zero-shot capabilities. However, the shared embedding space could\nlead to new vulnerabilities if different modalities can be misaligned. In this\npaper, we extend and utilize a recently developed effective gradient-based\nprocedure that allows us to match the embedding of a given text by minimally\nmodifying an image. Using the procedure, we show that we can align the\nembeddings of distinguishable texts to any image through unnoticeable\nadversarial attacks in joint image-text models, revealing that semantically\nunrelated images can have embeddings of identical texts and at the same time\nvisually indistinguishable images can be matched to the embeddings of very\ndifferent texts. Our technique achieves 100\\% success rate when it is applied\nto text datasets and images from multiple sources. Without overcoming the\nvulnerability, multimodal models cannot robustly align inputs from different\nmodalities in a semantically meaningful way. \\textbf{Warning: the text data\nused in this paper are toxic in nature and may be offensive to some readers.}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 14 figures. arXiv admin note: substantial text overlap with\n  arXiv:2401.15568, arXiv:2402.08473",
    "pdf_url": "http://arxiv.org/pdf/2407.01157v1",
    "published_date": "2024-07-01 10:25:47 UTC",
    "updated_date": "2024-07-01 10:25:47 UTC"
  },
  {
    "arxiv_id": "2407.01143v1",
    "title": "Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition",
    "authors": [
      "Oliver Schr√ºfer",
      "Manuel Milling",
      "Felix Burkhardt",
      "Florian Eyben",
      "Bj√∂rn Schuller"
    ],
    "abstract": "Uncertainty Quantification (UQ) is an important building block for the\nreliable use of neural networks in real-world scenarios, as it can be a useful\ntool in identifying faulty predictions. Speech emotion recognition (SER) models\ncan suffer from particularly many sources of uncertainty, such as the ambiguity\nof emotions, Out-of-Distribution (OOD) data or, in general, poor recording\nconditions. Reliable UQ methods are thus of particular interest as in many SER\napplications no prediction is better than a faulty prediction. While the\neffects of label ambiguity on uncertainty are well documented in the\nliterature, we focus our work on an evaluation of UQ methods for SER under\ncommon challenges in real-world application, such as corrupted signals, and the\nabsence of speech. We show that simple UQ methods can already give an\nindication of the uncertainty of a prediction and that training with additional\nOOD data can greatly improve the identification of such signals.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted for Interspeech 2024, 5 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.01143v1",
    "published_date": "2024-07-01 10:11:08 UTC",
    "updated_date": "2024-07-01 10:11:08 UTC"
  },
  {
    "arxiv_id": "2407.01142v1",
    "title": "Integrated feature analysis for deep learning interpretation and class activation maps",
    "authors": [
      "Yanli Li",
      "Tahereh Hassanzadeh",
      "Denis P. Shamonin",
      "Monique Reijnierse",
      "Annette H. M. van der Helm-van Mil",
      "Berend C. Stoel"
    ],
    "abstract": "Understanding the decisions of deep learning (DL) models is essential for the\nacceptance of DL to risk-sensitive applications. Although methods, like class\nactivation maps (CAMs), give a glimpse into the black box, they do miss some\ncrucial information, thereby limiting its interpretability and merely providing\nthe considered locations of objects. To provide more insight into the models\nand the influence of datasets, we propose an integrated feature analysis\nmethod, which consists of feature distribution analysis and feature\ndecomposition, to look closer into the intermediate features extracted by DL\nmodels. This integrated feature analysis could provide information on\noverfitting, confounders, outliers in datasets, model redundancies and\nprincipal features extracted by the models, and provide distribution\ninformation to form a common intensity scale, which are missing in current CAM\nalgorithms. The integrated feature analysis was applied to eight different\ndatasets for general validation: photographs of handwritten digits, two\ndatasets of natural images and five medical datasets, including skin\nphotography, ultrasound, CT, X-rays and MRIs. The method was evaluated by\ncalculating the consistency between the CAMs average class activation levels\nand the logits of the model. Based on the eight datasets, the correlation\ncoefficients through our method were all very close to 100%, and based on the\nfeature decomposition, 5%-25% of features could generate equally informative\nsaliency maps and obtain the same model performances as using all features.\nThis proves the reliability of the integrated feature analysis. As the proposed\nmethods rely on very few assumptions, this is a step towards better model\ninterpretation and a useful extension to existing CAM algorithms. Codes:\nhttps://github.com/YanliLi27/IFA",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 11 figures, code available:\n  https://github.com/YanliLi27/IFA This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.01142v1",
    "published_date": "2024-07-01 10:10:57 UTC",
    "updated_date": "2024-07-01 10:10:57 UTC"
  },
  {
    "arxiv_id": "2407.01137v1",
    "title": "An Empirical Comparison of Generative Approaches for Product Attribute-Value Identification",
    "authors": [
      "Kassem Sabeh",
      "Robert Litschko",
      "Mouna Kacimi",
      "Barbara Plank",
      "Johann Gamper"
    ],
    "abstract": "Product attributes are crucial for e-commerce platforms, supporting\napplications like search, recommendation, and question answering. The task of\nProduct Attribute and Value Identification (PAVI) involves identifying both\nattributes and their values from product information. In this paper, we\nformulate PAVI as a generation task and provide, to the best of our knowledge,\nthe most comprehensive evaluation of PAVI so far. We compare three different\nattribute-value generation (AVG) strategies based on fine-tuning\nencoder-decoder models on three datasets. Experiments show that end-to-end AVG\napproach, which is computationally efficient, outperforms other strategies.\nHowever, there are differences depending on model sizes and the underlying\nlanguage model. The code to reproduce all experiments is available at:\nhttps://github.com/kassemsabeh/pavi-avg",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01137v1",
    "published_date": "2024-07-01 10:02:17 UTC",
    "updated_date": "2024-07-01 10:02:17 UTC"
  },
  {
    "arxiv_id": "2407.01126v1",
    "title": "Investigating the potential of Sparse Mixtures-of-Experts for multi-domain neural machine translation",
    "authors": [
      "Nadezhda Chirkova",
      "Vassilina Nikoulina",
      "Jean-Luc Meunier",
      "Alexandre B√©rard"
    ],
    "abstract": "We focus on multi-domain Neural Machine Translation, with the goal of\ndeveloping efficient models which can handle data from various domains seen\nduring training and are robust to domains unseen during training. We\nhypothesize that Sparse Mixture-of-Experts (SMoE) models are a good fit for\nthis task, as they enable efficient model scaling, which helps to accommodate a\nvariety of multi-domain data, and allow flexible sharing of parameters between\ndomains, potentially enabling knowledge transfer between similar domains and\nlimiting negative transfer. We conduct a series of experiments aimed at\nvalidating the utility of SMoE for the multi-domain scenario, and find that a\nstraightforward width scaling of Transformer is a simpler and surprisingly more\nefficient approach in practice, and reaches the same performance level as SMoE.\nWe also search for a better recipe for robustness of multi-domain systems,\nhighlighting the importance of mixing-in a generic domain, i.e. Paracrawl, and\nintroducing a simple technique, domain randomization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01126v1",
    "published_date": "2024-07-01 09:45:22 UTC",
    "updated_date": "2024-07-01 09:45:22 UTC"
  },
  {
    "arxiv_id": "2407.03374v1",
    "title": "An Outline of Prognostics and Health Management Large Model: Concepts, Paradigms, and Challenges",
    "authors": [
      "Laifa Tao",
      "Shangyu Li",
      "Haifei Liu",
      "Qixuan Huang",
      "Liang Ma",
      "Guoao Ning",
      "Yiling Chen",
      "Yunlong Wu",
      "Bin Li",
      "Weiwei Zhang",
      "Zhengduo Zhao",
      "Wenchao Zhan",
      "Wenyan Cao",
      "Chao Wang",
      "Hongmei Liu",
      "Jian Ma",
      "Mingliang Suo",
      "Yujie Cheng",
      "Yu Ding",
      "Dengwei Song",
      "Chen Lu"
    ],
    "abstract": "Prognosis and Health Management (PHM), critical for ensuring task completion\nby complex systems and preventing unexpected failures, is widely adopted in\naerospace, manufacturing, maritime, rail, energy, etc. However, PHM's\ndevelopment is constrained by bottlenecks like generalization, interpretation\nand verification abilities. Presently, generative artificial intelligence (AI),\nrepresented by Large Model, heralds a technological revolution with the\npotential to fundamentally reshape traditional technological fields and human\nproduction methods. Its capabilities, including strong generalization,\nreasoning, and generative attributes, present opportunities to address PHM's\nbottlenecks. To this end, based on a systematic analysis of the current\nchallenges and bottlenecks in PHM, as well as the research status and\nadvantages of Large Model, we propose a novel concept and three progressive\nparadigms of Prognosis and Health Management Large Model (PHM-LM) through the\nintegration of the Large Model with PHM. Subsequently, we provide feasible\ntechnical approaches for PHM-LM to bolster PHM's core capabilities within the\nframework of the three paradigms. Moreover, to address core issues confronting\nPHM, we discuss a series of technical challenges of PHM-LM throughout the\nentire process of construction and application. This comprehensive effort\noffers a holistic PHM-LM technical framework, and provides avenues for new PHM\ntechnologies, methodologies, tools, platforms and applications, which also\npotentially innovates design, research & development, verification and\napplication mode of PHM. And furthermore, a new generation of PHM with AI will\nalso capably be realized, i.e., from custom to generalized, from discriminative\nto generative, and from theoretical conditions to practical applications.",
    "categories": [
      "cs.AI",
      "cs.SE",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03374v1",
    "published_date": "2024-07-01 09:37:00 UTC",
    "updated_date": "2024-07-01 09:37:00 UTC"
  },
  {
    "arxiv_id": "2407.01119v2",
    "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
    "authors": [
      "Guillermo Marco",
      "Julio Gonzalo",
      "Ram√≥n del Castillo",
      "Mar√≠a Teresa Mateo Girona"
    ],
    "abstract": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.01119v2",
    "published_date": "2024-07-01 09:28:58 UTC",
    "updated_date": "2024-10-28 16:32:01 UTC"
  },
  {
    "arxiv_id": "2407.01111v1",
    "title": "Proximity Matters: Local Proximity Preserved Balancing for Treatment Effect Estimation",
    "authors": [
      "Hao Wang",
      "Zhichao Chen",
      "Yuan Shen",
      "Jiajun Fan",
      "Zhaoran Liu",
      "Degui Yang",
      "Xinggao Liu",
      "Haoxuan Li"
    ],
    "abstract": "Heterogeneous treatment effect (HTE) estimation from observational data poses\nsignificant challenges due to treatment selection bias. Existing methods\naddress this bias by minimizing distribution discrepancies between treatment\ngroups in latent space, focusing on global alignment. However, the fruitful\naspect of local proximity, where similar units exhibit similar outcomes, is\noften overlooked. In this study, we propose Proximity-aware Counterfactual\nRegression (PCR) to exploit proximity for representation balancing within the\nHTE estimation context. Specifically, we introduce a local proximity\npreservation regularizer based on optimal transport to depict the local\nproximity in discrepancy calculation. Furthermore, to overcome the curse of\ndimensionality that renders the estimation of discrepancy ineffective,\nexacerbated by limited data availability for HTE estimation, we develop an\ninformative subspace projector, which trades off minimal distance precision for\nimproved sample complexity. Extensive experiments demonstrate that PCR\naccurately matches units across different treatment groups, effectively\nmitigates treatment selection bias, and significantly outperforms competitors.\nCode is available at https://anonymous.4open.science/status/ncr-B697.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at https://anonymous.4open.science/status/ncr-B697",
    "pdf_url": "http://arxiv.org/pdf/2407.01111v1",
    "published_date": "2024-07-01 09:20:26 UTC",
    "updated_date": "2024-07-01 09:20:26 UTC"
  },
  {
    "arxiv_id": "2407.01110v1",
    "title": "SecGenAI: Enhancing Security of Cloud-based Generative AI Applications within Australian Critical Technologies of National Interest",
    "authors": [
      "Christoforus Yoga Haryanto",
      "Minh Hieu Vu",
      "Trung Duc Nguyen",
      "Emily Lomempow",
      "Yulia Nurliana",
      "Sona Taheri"
    ],
    "abstract": "The rapid advancement of Generative AI (GenAI) technologies offers\ntransformative opportunities within Australia's critical technologies of\nnational interest while introducing unique security challenges. This paper\npresents SecGenAI, a comprehensive security framework for cloud-based GenAI\napplications, with a focus on Retrieval-Augmented Generation (RAG) systems.\nSecGenAI addresses functional, infrastructure, and governance requirements,\nintegrating end-to-end security analysis to generate specifications emphasizing\ndata privacy, secure deployment, and shared responsibility models. Aligned with\nAustralian Privacy Principles, AI Ethics Principles, and guidelines from the\nAustralian Cyber Security Centre and Digital Transformation Agency, SecGenAI\nmitigates threats such as data leakage, adversarial attacks, and model\ninversion. The framework's novel approach combines advanced machine learning\ntechniques with robust security measures, ensuring compliance with Australian\nregulations while enhancing the reliability and trustworthiness of GenAI\nsystems. This research contributes to the field of intelligent systems by\nproviding actionable strategies for secure GenAI implementation in industry,\nfostering innovation in AI applications, and safeguarding national interests.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 4 figures, 9 tables, submitted to the 2024 11th\n  International Conference on Soft Computing & Machine Intelligence (ISCMI\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.01110v1",
    "published_date": "2024-07-01 09:19:50 UTC",
    "updated_date": "2024-07-01 09:19:50 UTC"
  },
  {
    "arxiv_id": "2407.01093v1",
    "title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation",
    "authors": [
      "Senyu Han",
      "Lu Chen",
      "Li-Min Lin",
      "Zhengshan Xu",
      "Kai Yu"
    ],
    "abstract": "Large language models have demonstrated their capabilities in storyline\ncreation and human-like character role-playing. Current language model agents\nmainly focus on reasonable behaviors from the level of individuals, and their\nbehaviors might be hard to constraint on the level of the whole storyline. In\nthis paper we introduce IBSEN, a director-actor coordinate agent framework that\ngenerates drama scripts and makes the plot played by agents more controllable.\nThe director agent writes plot outlines that the user desires to see, instructs\nthe actor agents to role-play their characters, and reschedules the plot when\nhuman players participate in the scenario to ensure the plot is progressing\ntowards the objective. To evaluate the framework, we create a novel drama plot\nthat involves several actor agents and check the interactions between them\nunder the instruction of the director agent. Evaluation results show that our\nframework could generate complete, diverse drama scripts from only a rough\noutline of plot objectives, meanwhile maintaining the characteristics of\ncharacters in the drama. Our codes and prompts are available at\nhttps://github.com/OpenDFM/ibsen.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2407.01093v1",
    "published_date": "2024-07-01 08:49:57 UTC",
    "updated_date": "2024-07-01 08:49:57 UTC"
  },
  {
    "arxiv_id": "2407.01092v1",
    "title": "Kolmogorov-Arnold Convolutions: Design Principles and Empirical Studies",
    "authors": [
      "Ivan Drokin"
    ],
    "abstract": "The emergence of Kolmogorov-Arnold Networks (KANs) has sparked significant\ninterest and debate within the scientific community. This paper explores the\napplication of KANs in the domain of computer vision (CV). We examine the\nconvolutional version of KANs, considering various nonlinearity options beyond\nsplines, such as Wavelet transforms and a range of polynomials. We propose a\nparameter-efficient design for Kolmogorov-Arnold convolutional layers and a\nparameter-efficient finetuning algorithm for pre-trained KAN models, as well as\nKAN convolutional versions of self-attention and focal modulation layers. We\nprovide empirical evaluations conducted on MNIST, CIFAR10, CIFAR100, Tiny\nImageNet, ImageNet1k, and HAM10000 datasets for image classification tasks.\nAdditionally, we explore segmentation tasks, proposing U-Net-like architectures\nwith KAN convolutions, and achieving state-of-the-art results on BUSI, GlaS,\nand CVC datasets. We summarized all of our findings in a preliminary design\nguide of KAN convolutional models for computer vision tasks. Furthermore, we\ninvestigate regularization techniques for KANs. All experimental code and\nimplementations of convolutional layers and models, pre-trained on ImageNet1k\nweights are available on GitHub via this\nhttps://github.com/IvanDrokin/torch-conv-kan",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01092v1",
    "published_date": "2024-07-01 08:49:33 UTC",
    "updated_date": "2024-07-01 08:49:33 UTC"
  },
  {
    "arxiv_id": "2407.01080v2",
    "title": "Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese",
    "authors": [
      "Yunqi Xu",
      "Tianchi Cai",
      "Jiyan Jiang",
      "Xierui Song"
    ],
    "abstract": "The prevailing issue of factual inconsistency errors in conventional\nRetrieval Augmented Generation (RAG) motivates the study of Factual Consistency\nEvaluation (FCE). Despite the various FCE methods proposed earlier, these\nmethods are evaluated on datasets generated by specific Large Language Models\n(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE\nmethods perform on other LLMs with different error distributions or even unseen\nerror types, as these methods may fail to detect the error types generated by\nother LLMs. To fill this gap, in this paper, we propose the first comprehensive\nFCE benchmark \\emph{Face4RAG} for RAG independent of the underlying LLM. Our\nbenchmark consists of a synthetic dataset built upon a carefully designed\ntypology for factuality inconsistency error and a real-world dataset\nconstructed from six commonly used LLMs, enabling evaluation of FCE methods on\nspecific error types or real-world error distributions. On the proposed\nbenchmark, we discover the failure of existing FCE methods to detect the\nlogical fallacy, which refers to a mismatch of logic structures between the\nanswer and the retrieved reference. To fix this issue, we further propose a new\nmethod called \\emph{L-Face4RAG} with two novel designs of logic-preserving\nanswer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG\nsubstantially outperforms previous methods for factual inconsistency detection\non a wide range of tasks, notably beyond the RAG task from which it is\noriginally motivated. Both the benchmark and our proposed method are publicly\navailable.\\footnote{\\url{https://huggingface.co/datasets/yq27/Face4RAG}\\label{link_face4rag}}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01080v2",
    "published_date": "2024-07-01 08:35:04 UTC",
    "updated_date": "2024-07-03 12:49:34 UTC"
  },
  {
    "arxiv_id": "2407.01079v3",
    "title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Weimin Wu",
      "Zhao Song",
      "Han Liu"
    ],
    "abstract": "We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted at NeurIPS 2024. v3 updated to camera-ready version with\n  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications",
    "pdf_url": "http://arxiv.org/pdf/2407.01079v3",
    "published_date": "2024-07-01 08:34:40 UTC",
    "updated_date": "2024-10-31 16:59:13 UTC"
  },
  {
    "arxiv_id": "2407.01067v1",
    "title": "Human-like object concept representations emerge naturally in multimodal large language models",
    "authors": [
      "Changde Du",
      "Kaicheng Fu",
      "Bincheng Wen",
      "Yi Sun",
      "Jie Peng",
      "Wei Wei",
      "Ying Gao",
      "Shengpei Wang",
      "Chuncheng Zhang",
      "Jinpeng Li",
      "Shuang Qiu",
      "Le Chang",
      "Huiguang He"
    ],
    "abstract": "The conceptualization and categorization of natural objects in the human mind\nhave long intrigued cognitive scientists and neuroscientists, offering crucial\ninsights into human perception and cognition. Recently, the rapid development\nof Large Language Models (LLMs) has raised the attractive question of whether\nthese models can also develop human-like object representations through\nexposure to vast amounts of linguistic and multimodal data. In this study, we\ncombined behavioral and neuroimaging analysis methods to uncover how the object\nconcept representations in LLMs correlate with those of humans. By collecting\nlarge-scale datasets of 4.7 million triplet judgments from LLM and Multimodal\nLLM (MLLM), we were able to derive low-dimensional embeddings that capture the\nunderlying similarity structure of 1,854 natural objects. The resulting\n66-dimensional embeddings were found to be highly stable and predictive, and\nexhibited semantic clustering akin to human mental representations.\nInterestingly, the interpretability of the dimensions underlying these\nembeddings suggests that LLM and MLLM have developed human-like conceptual\nrepresentations of natural objects. Further analysis demonstrated strong\nalignment between the identified model embeddings and neural activity patterns\nin many functionally defined brain ROIs (e.g., EBA, PPA, RSC and FFA). This\nprovides compelling evidence that the object representations in LLMs, while not\nidentical to those in the human, share fundamental commonalities that reflect\nkey schemas of human conceptual knowledge. This study advances our\nunderstanding of machine intelligence and informs the development of more\nhuman-like artificial cognitive systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01067v1",
    "published_date": "2024-07-01 08:17:19 UTC",
    "updated_date": "2024-07-01 08:17:19 UTC"
  },
  {
    "arxiv_id": "2407.01050v1",
    "title": "Evolutionary Morphology Towards Overconstrained Locomotion via Large-Scale, Multi-Terrain Deep Reinforcement Learning",
    "authors": [
      "Yenan Chen",
      "Chuye Zhang",
      "Pengxi Gu",
      "Jianuo Qiu",
      "Jiayi Yin",
      "Nuofan Qiu",
      "Guojing Huang",
      "Bangchao Huang",
      "Zishang Zhang",
      "Hui Deng",
      "Wei Zhang",
      "Fang Wan",
      "Chaoyang Song"
    ],
    "abstract": "While the animals' Fin-to-Limb evolution has been well-researched in biology,\nsuch morphological transformation remains under-adopted in the modern design of\nadvanced robotic limbs. This paper investigates a novel class of\noverconstrained locomotion from a design and learning perspective inspired by\nevolutionary morphology, aiming to integrate the concept of `intelligent design\nunder constraints' - hereafter referred to as constraint-driven design\nintelligence - in developing modern robotic limbs with superior energy\nefficiency. We propose a 3D-printable design of robotic limbs parametrically\nreconfigurable as a classical planar 4-bar linkage, an overconstrained Bennett\nlinkage, and a spherical 4-bar linkage. These limbs adopt a co-axial actuation,\nidentical to the modern legged robot platforms, with the added capability of\nupgrading into a wheel-legged system. Then, we implemented a large-scale,\nmulti-terrain deep reinforcement learning framework to train these\nreconfigurable limbs for a comparative analysis of overconstrained locomotion\nin energy efficiency. Results show that the overconstrained limbs exhibit more\nefficient locomotion than planar limbs during forward and sideways walking over\ndifferent terrains, including floors, slopes, and stairs, with or without\nrandom noises, by saving at least 22% mechanical energy in completing the\ntraverse task, with the spherical limbs being the least efficient. It also\nachieves the highest average speed of 0.85 meters per second on flat terrain,\nwhich is 20% faster than the planar limbs. This study paves the path for an\nexciting direction for future research in overconstrained robotics leveraging\nevolutionary morphology and reconfigurable mechanism intelligence when combined\nwith state-of-the-art methods in deep reinforcement learning.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 5 figures, Accepted and Presented at ReMAR2024",
    "pdf_url": "http://arxiv.org/pdf/2407.01050v1",
    "published_date": "2024-07-01 07:57:01 UTC",
    "updated_date": "2024-07-01 07:57:01 UTC"
  },
  {
    "arxiv_id": "2407.01046v2",
    "title": "FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models",
    "authors": [
      "Yiyuan Li",
      "Shichao Sun",
      "Pengfei Liu"
    ],
    "abstract": "Fuzzy reasoning is vital due to the frequent use of imprecise information in\ndaily contexts. However, the ability of current large language models (LLMs) to\nhandle such reasoning remains largely uncharted. In this paper, we introduce a\nnew benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical\nword problems that incorporate generalized quantifiers. Our experimental\nfindings reveal that fuzzy reasoning continues to pose significant challenges\nfor LLMs. Moreover, we find that existing methods designed to enhance reasoning\ndo not consistently improve performance in tasks involving fuzzy logic.\nAdditionally, our results show an inverse scaling effect in the performance of\nLLMs on FRoG. Interestingly, we also demonstrate that strong mathematical\nreasoning skills are not necessarily indicative of success on our benchmark.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.01046v2",
    "published_date": "2024-07-01 07:56:14 UTC",
    "updated_date": "2024-07-03 03:37:53 UTC"
  },
  {
    "arxiv_id": "2407.01026v1",
    "title": "Augmenting Document-level Relation Extraction with Efficient Multi-Supervision",
    "authors": [
      "Xiangyu Lin",
      "Weijia Jia",
      "Zhiguo Gong"
    ],
    "abstract": "Despite its popularity in sentence-level relation extraction, distantly\nsupervised data is rarely utilized by existing work in document-level relation\nextraction due to its noisy nature and low information density. Among its\ncurrent applications, distantly supervised data is mostly used as a whole for\npertaining, which is of low time efficiency. To fill in the gap of efficient\nand robust utilization of distantly supervised training data, we propose\nEfficient Multi-Supervision for document-level relation extraction, in which we\nfirst select a subset of informative documents from the massive dataset by\ncombining distant supervision with expert supervision, then train the model\nwith Multi-Supervision Ranking Loss that integrates the knowledge from multiple\nsources of supervision to alleviate the effects of noise. The experiments\ndemonstrate the effectiveness of our method in improving the model performance\nwith higher time efficiency than existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01026v1",
    "published_date": "2024-07-01 07:22:32 UTC",
    "updated_date": "2024-07-01 07:22:32 UTC"
  },
  {
    "arxiv_id": "2407.01003v5",
    "title": "Embedded Visual Prompt Tuning",
    "authors": [
      "Wenqiang Zu",
      "Shenghao Xie",
      "Qing Zhao",
      "Guoqi Li",
      "Lei Ma"
    ],
    "abstract": "Foundation models pre-trained on large-scale data have been widely witnessed\nto achieve success in various natural imaging downstream tasks.\nParameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models\nto new domains by updating only a small portion of parameters in order to\nreduce computational overhead. However, the effectiveness of these PEFT\nmethods, especially in cross-domain few-shot scenarios, e.g., medical image\nanalysis, has not been fully explored. In this work, we facilitate the study of\nthe performance of PEFT when adapting foundation models to medical image\nclassification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of\nmainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)\nmethod by embedding prompt tokens into the expanded channels. We also find that\nthere are anomalies in the feature space distribution of foundation models\nduring pre-training process, and prompt tuning can help mitigate this negative\nimpact. To explain this phenomenon, we also introduce a novel perspective to\nunderstand prompt tuning: Prompt tuning is a distribution calibrator. And we\nsupport it by analyzing patch-wise scaling and feature separation operations\ncontained in EPT. Our experiments show that EPT outperforms several\nstate-of-the-art fine-tuning methods by a significant margin on few-shot\nmedical image classification tasks, and completes the fine-tuning process\nwithin highly competitive time, indicating EPT is an effective PEFT method. The\nsource code is available at github.com/zuwenqiang/EPT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01003v5",
    "published_date": "2024-07-01 06:35:53 UTC",
    "updated_date": "2025-03-21 13:38:56 UTC"
  },
  {
    "arxiv_id": "2407.01001v1",
    "title": "Flood Prediction Using Classical and Quantum Machine Learning Models",
    "authors": [
      "Marek Grzesiak",
      "Param Thakkar"
    ],
    "abstract": "This study investigates the potential of quantum machine learning to improve\nflood forecasting we focus on daily flood events along Germany's Wupper River\nin 2023 our approach combines classical machine learning techniques with QML\ntechniques this hybrid model leverages quantum properties like superposition\nand entanglement to achieve better accuracy and efficiency classical and QML\nmodels are compared based on training time accuracy and scalability results\nshow that QML models offer competitive training times and improved prediction\naccuracy this research signifies a step towards utilizing quantum technologies\nfor climate change adaptation we emphasize collaboration and continuous\ninnovation to implement this model in real-world flood management ultimately\nenhancing global resilience against floods",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01001v1",
    "published_date": "2024-07-01 06:31:41 UTC",
    "updated_date": "2024-07-01 06:31:41 UTC"
  },
  {
    "arxiv_id": "2407.00993v1",
    "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
    "authors": [
      "Shihan Deng",
      "Weikai Xu",
      "Hongda Sun",
      "Wei Liu",
      "Tao Tan",
      "Jianfeng Liu",
      "Ang Li",
      "Jian Luan",
      "Bin Wang",
      "Rui Yan",
      "Shuo Shang"
    ],
    "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based\nagents have become a research hotspot in human-computer interaction. However,\nthere is a scarcity of benchmarks available for LLM-based mobile agents.\nBenchmarking these agents generally faces three main challenges: (1) The\ninefficiency of UI-only operations imposes limitations to task evaluation. (2)\nSpecific instructions within a singular application lack adequacy for assessing\nthe multi-dimensional reasoning and decision-making capacities of LLM mobile\nagents. (3) Current evaluation metrics are insufficient to accurately assess\nthe process of sequential actions. To this end, we propose Mobile-Bench, a\nnovel benchmark for evaluating the capabilities of LLM-based mobile agents.\nFirst, we expand conventional UI operations by incorporating 103 collected APIs\nto accelerate the efficiency of task completion. Subsequently, we collect\nevaluation data by combining real user queries with augmentation from LLMs. To\nbetter evaluate different levels of planning capabilities for mobile agents,\nour data is categorized into three distinct groups: SAST, SAMT, and MAMT,\nreflecting varying levels of task complexity. Mobile-Bench comprises 832 data\nentries, with more than 200 tasks specifically designed to evaluate multi-APP\ncollaboration scenarios. Furthermore, we introduce a more accurate evaluation\nmetric, named CheckPoint, to assess whether LLM-based mobile agents reach\nessential points during their planning and reasoning steps.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00993v1",
    "published_date": "2024-07-01 06:10:01 UTC",
    "updated_date": "2024-07-01 06:10:01 UTC"
  },
  {
    "arxiv_id": "2407.00984v1",
    "title": "Individual brain parcellation: Review of methods, validations and applications",
    "authors": [
      "Chengyi Li",
      "Shan Yu",
      "Yue Cui"
    ],
    "abstract": "Individual brains vary greatly in morphology, connectivity and organization.\nThe applicability of group-level parcellations is limited by the rapid\ndevelopment of precision medicine today because they do not take into account\nthe variation of parcels at the individual level. Accurate mapping of brain\nfunctional regions at the individual level is pivotal for a comprehensive\nunderstanding of the variations in brain function and behaviors, early and\nprecise identification of brain abnormalities, as well as personalized\ntreatments for neuropsychiatric disorders. With the development of neuroimaging\nand machine learning techniques, studies on individual brain parcellation are\nbooming. In this paper, we offer an overview of recent advances in the\nmethodologies of individual brain parcellation, including optimization- and\nlearning-based methods. Comprehensive evaluation metrics to validate individual\nbrain mapping have been introduced. We also review the studies of how\nindividual brain mapping promotes neuroscience research and clinical medicine.\nFinally, we summarize the major challenges and important future directions of\nindividualized brain parcellation. Collectively, we intend to offer a thorough\noverview of individual brain parcellation methods, validations, and\napplications, along with highlighting the current challenges that call for an\nurgent demand for integrated platforms that integrate datasets, methods, and\nvalidations.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00984v1",
    "published_date": "2024-07-01 05:48:05 UTC",
    "updated_date": "2024-07-01 05:48:05 UTC"
  },
  {
    "arxiv_id": "2407.00980v1",
    "title": "Acceleration method for generating perception failure scenarios based on editing Markov process",
    "authors": [
      "Canjie Cai"
    ],
    "abstract": "With the rapid advancement of autonomous driving technology, self-driving\ncars have become a central focus in the development of future transportation\nsystems. Scenario generation technology has emerged as a crucial tool for\ntesting and verifying the safety performance of autonomous driving systems.\nCurrent research in scenario generation primarily focuses on open roads such as\nhighways, with relatively limited studies on underground parking garages. The\nunique structural constraints, insufficient lighting, and high-density\nobstacles in underground parking garages impose greater demands on the\nperception systems, which are critical to autonomous driving technology.\n  This study proposes an accelerated generation method for perception failure\nscenarios tailored to the underground parking garage environment, aimed at\ntesting and improving the safety performance of autonomous vehicle (AV)\nperception algorithms in such settings. The method presented in this paper\ngenerates an intelligent testing environment with a high density of perception\nfailure scenarios by learning the interactions between background vehicles\n(BVs) and autonomous vehicles (AVs) within perception failure scenarios.\nFurthermore, this method edits the Markov process within the perception failure\nscenario data to increase the density of critical information in the training\ndata, thereby optimizing the learning and generation of perception failure\nscenarios. A simulation environment for an underground parking garage was\ndeveloped using the Carla and Vissim platforms, with Bevfusion employed as the\nperception algorithm for testing. The study demonstrates that this method can\ngenerate an intelligent testing environment with a high density of perception\nfailure scenarios and enhance the safety performance of perception algorithms\nwithin this experimental setup.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00980v1",
    "published_date": "2024-07-01 05:33:48 UTC",
    "updated_date": "2024-07-01 05:33:48 UTC"
  },
  {
    "arxiv_id": "2407.00978v2",
    "title": "Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in Internet of Medical Things: A Diffusion-based Contract Approach",
    "authors": [
      "Cheng Su",
      "Jinbo Wen",
      "Jiawen Kang",
      "Yonghua Wang",
      "Yuanjia Su",
      "Hudan Pan",
      "Zishao Zhong",
      "M. Shamim Hossain"
    ],
    "abstract": "Secure data management and effective data sharing have become paramount in\nthe rapidly evolving healthcare landscape, especially with the growing\nintegration of the Internet of Medical Things (IoMT). The rise of generative\nartificial intelligence has further elevated Multi-modal Large Language Models\n(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.\nMLLMs can support multi-modal inputs and generate diverse types of content by\nleveraging large-scale training on vast amounts of multi-modal data. However,\ncritical challenges persist in developing medical MLLMs, including security and\nfreshness issues of healthcare data, affecting the output quality of MLLMs. To\nthis end, in this paper, we propose a hybrid Retrieval-Augmented Generation\n(RAG)-empowered medical MLLM framework for healthcare data management. This\nframework leverages a hierarchical cross-chain architecture to facilitate\nsecure data training. Moreover, it enhances the output quality of MLLMs through\nhybrid RAG, which employs multi-modal metrics to filter various unimodal RAG\nresults and incorporates these retrieval results as additional inputs to MLLMs.\nAdditionally, we employ age of information to indirectly evaluate the data\nfreshness impact of MLLMs and utilize contract theory to incentivize healthcare\ndata holders to share their fresh data, mitigating information asymmetry during\ndata sharing. Finally, we utilize a generative diffusion model-based deep\nreinforcement learning algorithm to identify the optimal contract for efficient\ndata sharing. Numerical results demonstrate the effectiveness of the proposed\nschemes, which achieve secure and efficient healthcare data management.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00978v2",
    "published_date": "2024-07-01 05:28:40 UTC",
    "updated_date": "2024-12-09 02:28:01 UTC"
  },
  {
    "arxiv_id": "2407.01647v1",
    "title": "Optimizing PM2.5 Forecasting Accuracy with Hybrid Meta-Heuristic and Machine Learning Models",
    "authors": [
      "Parviz Ghafariasl",
      "Masoomeh Zeinalnezhad",
      "Amir Ahmadishokooh"
    ],
    "abstract": "Timely alerts about hazardous air pollutants are crucial for public health.\nHowever, existing forecasting models often overlook key factors like baseline\nparameters and missing data, limiting their accuracy. This study introduces a\nhybrid approach to address these issues, focusing on forecasting hourly PM2.5\nconcentrations using Support Vector Regression (SVR). Meta-heuristic\nalgorithms, Grey Wolf Optimization (GWO) and Particle Swarm Optimization (PSO),\noptimize SVR Hyper-parameters \"C\" and \"Gamma\" to enhance prediction accuracy.\nEvaluation metrics include R-squared (R2), Root Mean Square Error (RMSE), and\nMean Absolute Error (MAE). Results show significant improvements with PSO-SVR\n(R2: 0.9401, RMSE: 0.2390, MAE: 0.1368) and GWO-SVR (R2: 0.9408, RMSE: 0.2376,\nMAE: 0.1373), indicating robust and accurate models suitable for similar\nresearch applications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01647v1",
    "published_date": "2024-07-01 05:24:19 UTC",
    "updated_date": "2024-07-01 05:24:19 UTC"
  },
  {
    "arxiv_id": "2407.12035v2",
    "title": "Reporting Risks in AI-based Assistive Technology Research: A Systematic Review",
    "authors": [
      "Zahra Ahmadi",
      "Peter R. Lewis",
      "Mahadeo A. Sukhai"
    ],
    "abstract": "Artificial Intelligence (AI) is increasingly employed to enhance assistive\ntechnologies, yet it can fail in various ways. We conducted a systematic\nliterature review of research into AI-based assistive technology for persons\nwith visual impairments. Our study shows that most proposed technologies with a\ntestable prototype have not been evaluated in a human study with members of the\nsight-loss community. Furthermore, many studies did not consider or report\nfailure cases or possible risks. These findings highlight the importance of\ninclusive system evaluations and the necessity of standardizing methods for\npresenting and analyzing failure cases and threats when developing AI-based\nassistive technologies.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12035v2",
    "published_date": "2024-07-01 05:22:44 UTC",
    "updated_date": "2024-07-18 19:28:33 UTC"
  },
  {
    "arxiv_id": "2407.00967v1",
    "title": "Deep learning for automated detection of breast cancer in deep ultraviolet fluorescence images with diffusion probabilistic model",
    "authors": [
      "Sepehr Salem Ghahfarokhi",
      "Tyrell To",
      "Julie Jorns",
      "Tina Yen",
      "Bing Yu",
      "Dong Hye Ye"
    ],
    "abstract": "Data limitation is a significant challenge in applying deep learning to\nmedical images. Recently, the diffusion probabilistic model (DPM) has shown the\npotential to generate high-quality images by converting Gaussian random noise\ninto realistic images. In this paper, we apply the DPM to augment the deep\nultraviolet fluorescence (DUV) image dataset with an aim to improve breast\ncancer classification for intraoperative margin assessment. For classification,\nwe divide the whole surface DUV image into small patches and extract\nconvolutional features for each patch by utilizing the pre-trained ResNet.\nThen, we feed them into an XGBoost classifier for patch-level decisions and\nthen fuse them with a regional importance map computed by Grad-CAM++ for whole\nsurface-level prediction. Our experimental results show that augmenting the\ntraining dataset with the DPM significantly improves breast cancer detection\nperformance in DUV images, increasing accuracy from 93% to 97%, compared to\nusing Affine transformations and ProGAN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE International Symposium on Biomedical Imaging 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.00967v1",
    "published_date": "2024-07-01 05:00:26 UTC",
    "updated_date": "2024-07-01 05:00:26 UTC"
  },
  {
    "arxiv_id": "2407.00959v1",
    "title": "Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving",
    "authors": [
      "Ran Tian",
      "Boyi Li",
      "Xinshuo Weng",
      "Yuxiao Chen",
      "Edward Schmerling",
      "Yue Wang",
      "Boris Ivanovic",
      "Marco Pavone"
    ],
    "abstract": "The autonomous driving industry is increasingly adopting end-to-end learning\nfrom sensory inputs to minimize human biases in system design. Traditional\nend-to-end driving models, however, suffer from long-tail events due to rare or\nunseen inputs within their training distributions. To address this, we propose\nTOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the\nworld into object-level knowledge, enabling better utilization of LLM's\nreasoning capabilities to enhance autonomous vehicle planning in long-tail\nscenarios. TOKEN effectively alleviates data scarcity and inefficient\ntokenization by leveraging a traditional end-to-end driving model to produce\ncondensed and semantically enriched representations of the scene, which are\noptimized for LLM planning compatibility through deliberate representation and\nreasoning alignment training stages. Our results demonstrate that TOKEN excels\nin grounding, reasoning, and planning capabilities, outperforming existing\nframeworks with a 27% reduction in trajectory L2 error and a 39% decrease in\ncollision rates in long-tail scenarios. Additionally, our work highlights the\nimportance of representation alignment and structured reasoning in sparking the\ncommon-sense reasoning capabilities of MM-LLMs for effective planning.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00959v1",
    "published_date": "2024-07-01 04:34:50 UTC",
    "updated_date": "2024-07-01 04:34:50 UTC"
  },
  {
    "arxiv_id": "2407.00958v5",
    "title": "Dynamic Universal Approximation Theory: The Basic Theory for Transformer-based Large Language Models",
    "authors": [
      "Wei Wang",
      "Qing Li"
    ],
    "abstract": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00958v5",
    "published_date": "2024-07-01 04:29:35 UTC",
    "updated_date": "2024-12-11 06:01:38 UTC"
  },
  {
    "arxiv_id": "2407.00955v1",
    "title": "Task-oriented Over-the-air Computation for Edge-device Co-inference with Balanced Classification Accuracy",
    "authors": [
      "Xiang Jiao",
      "Dingzhu Wen",
      "Guangxu Zhu",
      "Wei Jiang",
      "Wu Luo",
      "Yuanming Shi"
    ],
    "abstract": "Edge-device co-inference, which concerns the cooperation between edge devices\nand an edge server for completing inference tasks over wireless networks, has\nbeen a promising technique for enabling various kinds of intelligent services\nat the network edge, e.g., auto-driving. In this paradigm, the concerned design\nobjective of the network shifts from the traditional communication throughput\nto the effective and efficient execution of the inference task underpinned by\nthe network, measured by, e.g., the inference accuracy and latency. In this\npaper, a task-oriented over-the-air computation scheme is proposed for a\nmultidevice artificial intelligence system. Particularly, a novel tractable\ninference accuracy metric is proposed for classification tasks, which is called\nminimum pair-wise discriminant gain. Unlike prior work measuring the average of\nall class pairs in feature space, it measures the minimum distance of all class\npairs. By maximizing the minimum pair-wise discriminant gain instead of its\naverage counterpart, any pair of classes can be better separated in the feature\nspace, and thus leading to a balanced and improved inference accuracy for all\nclasses. Besides, this paper jointly optimizes the minimum discriminant gain of\nall feature elements instead of separately maximizing that of each element in\nthe existing designs. As a result, the transmit power can be adaptively\nallocated to the feature elements according to their different contributions to\nthe inference accuracy, opening an extra degree of freedom to improve inference\nperformance. Extensive experiments are conducted using a concrete use case of\nhuman motion recognition to verify the superiority of the proposed design over\nthe benchmarking scheme.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "This paper was accepted by IEEE Transactions on Vehicular Technology\n  on June 30, 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.00955v1",
    "published_date": "2024-07-01 04:17:32 UTC",
    "updated_date": "2024-07-01 04:17:32 UTC"
  },
  {
    "arxiv_id": "2407.00948v3",
    "title": "View From Above: A Framework for Evaluating Distribution Shifts in Model Behavior",
    "authors": [
      "Tanush Chopra",
      "Michael Li",
      "Jacob Haimes"
    ],
    "abstract": "When large language models (LLMs) are asked to perform certain tasks, how can\nwe be sure that their learned representations align with reality? We propose a\ndomain-agnostic framework for systematically evaluating distribution shifts in\nLLMs decision-making processes, where they are given control of mechanisms\ngoverned by pre-defined rules. While individual LLM actions may appear\nconsistent with expected behavior, across a large number of trials,\nstatistically significant distribution shifts can emerge. To test this, we\nconstruct a well-defined environment with known outcome logic: blackjack. In\nmore than 1,000 trials, we uncover statistically significant evidence\nsuggesting behavioral misalignment in the learned representations of LLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00948v3",
    "published_date": "2024-07-01 04:07:49 UTC",
    "updated_date": "2024-09-28 00:07:27 UTC"
  },
  {
    "arxiv_id": "2407.00942v1",
    "title": "ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions",
    "authors": [
      "Jingheng Ye",
      "Yong Jiang",
      "Xiaobin Wang",
      "Yinghui Li",
      "Yangning Li",
      "Hai-Tao Zheng",
      "Pengjun Xie",
      "Fei Huang"
    ],
    "abstract": "This paper introduces the task of product demand clarification within an\ne-commercial scenario, where the user commences the conversation with ambiguous\nqueries and the task-oriented agent is designed to achieve more accurate and\ntailored product searching by asking clarification questions. To address this\ntask, we propose ProductAgent, a conversational information seeking agent\nequipped with abilities of strategic clarification question generation and\ndynamic product retrieval. Specifically, we develop the agent with strategies\nfor product feature summarization, query generation, and product retrieval.\nFurthermore, we propose the benchmark called PROCLARE to evaluate the agent's\nperformance both automatically and qualitatively with the aid of a LLM-driven\nuser simulator. Experiments show that ProductAgent interacts positively with\nthe user and enhances retrieval performance with increasing dialogue turns,\nwhere user demands become gradually more explicit and detailed. All the source\ncodes will be released after the review anonymity period.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "17 pages, 13 tables, 6 figures. Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.00942v1",
    "published_date": "2024-07-01 03:50:23 UTC",
    "updated_date": "2024-07-01 03:50:23 UTC"
  },
  {
    "arxiv_id": "2407.00936v5",
    "title": "Large Language Model Enhanced Knowledge Representation Learning: A Survey",
    "authors": [
      "Xin Wang",
      "Zirui Chen",
      "Haofen Wang",
      "Leong Hou U",
      "Zhao Li",
      "Wenbin Guo"
    ],
    "abstract": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00936v5",
    "published_date": "2024-07-01 03:37:35 UTC",
    "updated_date": "2025-04-08 14:47:07 UTC"
  },
  {
    "arxiv_id": "2407.01646v1",
    "title": "ESALE: Enhancing Code-Summary Alignment Learning for Source Code Summarization",
    "authors": [
      "Chunrong Fang",
      "Weisong Sun",
      "Yuchen Chen",
      "Xiao Chen",
      "Zhao Wei",
      "Quanjun Zhang",
      "Yudu You",
      "Bin Luo",
      "Yang Liu",
      "Zhenyu Chen"
    ],
    "abstract": "(Source) code summarization aims to automatically generate succinct natural\nlanguage summaries for given code snippets. Such summaries play a significant\nrole in promoting developers to understand and maintain code. Inspired by\nneural machine translation, deep learning-based code summarization techniques\nwidely adopt an encoder-decoder framework, where the encoder transforms given\ncode snippets into context vectors, and the decoder decodes context vectors\ninto summaries. Recently, large-scale pre-trained models for source code are\nequipped with encoders capable of producing general context vectors and have\nachieved substantial improvements on code summarization. However, although they\nare usually trained mainly on code-focused tasks and can capture general code\nfeatures, they still fall short in capturing specific features that need to be\nsummarized.\n  This paper proposes a novel approach to improve code summarization based on\nsummary-focused tasks. Specifically, we exploit a multi-task learning paradigm\nto train the encoder on three summary-focused tasks to enhance its ability to\nlearn code-summary alignment, including unidirectional language modeling (ULM),\nmasked language modeling (MLM), and action word prediction (AWP). Unlike\npre-trained models that mainly predict masked tokens in code snippets, we\ndesign ULM and MLM to predict masked words in summaries. Intuitively,\npredicting words based on given code snippets would help learn the code-summary\nalignment. Additionally, we introduce the domain-specific task AWP to enhance\nthe ability of the encoder to learn the alignment between action words and code\nsnippets. The extensive experiments on four datasets demonstrate that our\napproach, called ESALE significantly outperforms baselines in all three widely\nused metrics, including BLEU, METEOR, and ROUGE-L.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "68-04",
      "D.2.3; I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to IEEE Transactions on Software Engineering (TSE)",
    "pdf_url": "http://arxiv.org/pdf/2407.01646v1",
    "published_date": "2024-07-01 03:06:51 UTC",
    "updated_date": "2024-07-01 03:06:51 UTC"
  },
  {
    "arxiv_id": "2407.00918v1",
    "title": "Robust and Reliable Early-Stage Website Fingerprinting Attacks via Spatial-Temporal Distribution Analysis",
    "authors": [
      "Xinhao Deng",
      "Qi Li",
      "Ke Xu"
    ],
    "abstract": "Website Fingerprinting (WF) attacks identify the websites visited by users by\nperforming traffic analysis, compromising user privacy. Particularly, DL-based\nWF attacks demonstrate impressive attack performance. However, the\neffectiveness of DL-based WF attacks relies on the collected complete and pure\ntraffic during the page loading, which impacts the practicality of these\nattacks. The WF performance is rather low under dynamic network conditions and\nvarious WF defenses, particularly when the analyzed traffic is only a small\npart of the complete traffic. In this paper, we propose Holmes, a robust and\nreliable early-stage WF attack. Holmes utilizes temporal and spatial\ndistribution analysis of website traffic to effectively identify websites in\nthe early stages of page loading. Specifically, Holmes develops adaptive data\naugmentation based on the temporal distribution of website traffic and utilizes\na supervised contrastive learning method to extract the correlations between\nthe early-stage traffic and the pre-collected complete traffic. Holmes\naccurately identifies traffic in the early stages of page loading by computing\nthe correlation of the traffic with the spatial distribution information, which\nensures robust and reliable detection according to early-stage traffic. We\nextensively evaluate Holmes using six datasets. Compared to nine existing\nDL-based WF attacks, Holmes improves the F1-score of identifying early-stage\ntraffic by an average of 169.18%. Furthermore, we replay the traffic of\nvisiting real-world dark web websites. Holmes successfully identifies dark web\nwebsites when the ratio of page loading on average is only 21.71%, with an\naverage precision improvement of 169.36% over the existing WF attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.00918v1",
    "published_date": "2024-07-01 02:51:26 UTC",
    "updated_date": "2024-07-01 02:51:26 UTC"
  },
  {
    "arxiv_id": "2407.00908v3",
    "title": "FineSurE: Fine-grained Summarization Evaluation using LLMs",
    "authors": [
      "Hwanjun Song",
      "Hang Su",
      "Igor Shalyminov",
      "Jason Cai",
      "Saab Mansour"
    ],
    "abstract": "Automated evaluation is crucial for streamlining text summarization\nbenchmarking and model development, given the costly and time-consuming nature\nof human evaluation. Traditional methods like ROUGE do not correlate well with\nhuman judgment, while recently proposed LLM-based metrics provide only\nsummary-level assessment using Likert-scale scores. This limits deeper model\nanalysis, e.g., we can only assign one hallucination score at the summary\nlevel, while at the sentence level, we can count sentences containing\nhallucinations. To remedy those limitations, we propose FineSurE, a\nfine-grained evaluator specifically tailored for the summarization task using\nlarge language models (LLMs). It also employs completeness and conciseness\ncriteria, in addition to faithfulness, enabling multi-dimensional assessment.\nWe compare various open-source and proprietary LLMs as backbones for FineSurE.\nIn addition, we conduct extensive benchmarking of FineSurE against SOTA methods\nincluding NLI-, QA-, and LLM-based methods, showing improved performance\nespecially on the completeness and conciseness dimensions. The code is\navailable at https://github.com/DISL-Lab/FineSurE-ACL24.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 (main, long)",
    "pdf_url": "http://arxiv.org/pdf/2407.00908v3",
    "published_date": "2024-07-01 02:20:28 UTC",
    "updated_date": "2024-07-22 04:45:11 UTC"
  },
  {
    "arxiv_id": "2407.00902v3",
    "title": "From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning",
    "authors": [
      "Nan Xu",
      "Fei Wang",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "abstract": "Motivated by in-context learning (ICL) capabilities of Large Language Models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.00902v3",
    "published_date": "2024-07-01 01:57:21 UTC",
    "updated_date": "2025-02-07 02:29:02 UTC"
  },
  {
    "arxiv_id": "2407.00900v1",
    "title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula",
    "authors": [
      "Shubhra Mishra",
      "Gabriel Poesia",
      "Belinda Mo",
      "Noah D. Goodman"
    ],
    "abstract": "Mathematical problem solving is an important skill for Large Language Models\n(LLMs), both as an important capability and a proxy for a range of reasoning\nabilities. Existing benchmarks probe a diverse set of skills, but they yield\naggregate accuracy metrics, obscuring specific abilities or weaknesses.\nFurthermore, they are difficult to extend with new problems, risking data\ncontamination over time. To address these challenges, we propose MathCAMPS: a\nmethod to synthesize high-quality mathematical problems at scale, grounded on\n44 fine-grained \"standards\" from the Mathematics Common Core (CC) Standard for\nK-8 grades. We encode each standard in a formal grammar, allowing us to sample\ndiverse symbolic problems and their answers. We then use LLMs to realize the\nsymbolic problems into word problems. We propose a cycle-consistency method for\nvalidating problem faithfulness. Finally, we derive follow-up questions from\nsymbolic structures and convert them into follow-up word problems - a novel\ntask of mathematical dialogue that probes for robustness in understanding.\nExperiments on 23 LLMs show surprising failures even in the strongest models\n(in particular when asked simple follow-up questions). Moreover, we evaluate\ntraining checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when\nparticular mathematical skills develop during its training. Our framework\nenables the community to reproduce and extend our pipeline for a fraction of\nthe typical cost of building new high-quality datasets.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Dataset and code: https://github.com/gpoesia/mathcamps/",
    "pdf_url": "http://arxiv.org/pdf/2407.00900v1",
    "published_date": "2024-07-01 01:56:28 UTC",
    "updated_date": "2024-07-01 01:56:28 UTC"
  },
  {
    "arxiv_id": "2407.00896v1",
    "title": "Channel Modeling Aided Dataset Generation for AI-Enabled CSI Feedback: Advances, Challenges, and Solutions",
    "authors": [
      "Yupeng Li",
      "Gang Li",
      "Zirui Wen",
      "Shuangfeng Han",
      "Shijian Gao",
      "Guangyi Liu",
      "Jiangzhou Wang"
    ],
    "abstract": "The AI-enabled autoencoder has demonstrated great potential in channel state\ninformation (CSI) feedback in frequency division duplex (FDD) multiple input\nmultiple output (MIMO) systems. However, this method completely changes the\nexisting feedback strategies, making it impractical to deploy in recent years.\nTo address this issue, this paper proposes a channel modeling aided data\naugmentation method based on a limited number of field channel data.\nSpecifically, the user equipment (UE) extracts the primary stochastic\nparameters of the field channel data and transmits them to the base station\n(BS). The BS then updates the typical TR 38.901 model parameters with the\nextracted parameters. In this way, the updated channel model is used to\ngenerate the dataset. This strategy comprehensively considers the dataset\ncollection, model generalization, model monitoring, and so on. Simulations\nverify that our proposed strategy can significantly improve performance\ncompared to the benchmarks.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00896v1",
    "published_date": "2024-07-01 01:37:30 UTC",
    "updated_date": "2024-07-01 01:37:30 UTC"
  },
  {
    "arxiv_id": "2407.00886v3",
    "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
    "authors": [
      "Aliyah R. Hsu",
      "Georgia Zhou",
      "Yeshwanth Cherapanamjeri",
      "Yaxuan Huang",
      "Anobel Y. Odisho",
      "Peter R. Carroll",
      "Bin Yu"
    ],
    "abstract": "Automated mechanistic interpretation research has attracted great interest\ndue to its potential to scale explanations of neural network internals to large\nmodels. Existing automated circuit discovery work relies on activation patching\nor its approximations to identify subgraphs in models for specific tasks\n(circuits). They often suffer from slow runtime, approximation errors, and\nspecific requirements of metrics, such as non-zero gradients. In this work, we\nintroduce contextual decomposition for transformers (CD-T) to build\ninterpretable circuits in large language models. CD-T can produce circuits of\narbitrary level of abstraction, and is the first able to produce circuits as\nfine-grained as attention heads at specific sequence positions efficiently.\nCD-T consists of a set of mathematical equations to isolate contribution of\nmodel features. Through recursively computing contribution of all nodes in a\ncomputational graph of a model using CD-T followed by pruning, we are able to\nreduce circuit discovery runtime from hours to seconds compared to\nstate-of-the-art baselines. On three standard circuit evaluation datasets\n(indirect object identification, greater-than comparisons, and docstring\ncompletion), we demonstrate that CD-T outperforms ACDC and EAP by better\nrecovering the manual circuits with an average of 97% ROC AUC under low\nruntimes. In addition, we provide evidence that faithfulness of CD-T circuits\nis not due to random chance by showing our circuits are 80% more faithful than\nrandom circuits of up to 60% of the original model size. Finally, we show CD-T\ncircuits are able to perfectly replicate original models' behavior\n(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our\nresults underscore the great promise of CD-T for efficient automated\nmechanistic interpretability, paving the way for new insights into the workings\nof large language models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00886v3",
    "published_date": "2024-07-01 01:12:20 UTC",
    "updated_date": "2025-03-02 08:26:23 UTC"
  },
  {
    "arxiv_id": "2407.00869v2",
    "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
    "authors": [
      "Yue Zhou",
      "Henry Peng Zou",
      "Barbara Di Eugenio",
      "Yang Zhang"
    ],
    "abstract": "We find that language models have difficulties generating fallacious and\ndeceptive reasoning. When asked to generate deceptive outputs, language models\ntend to leak honest counterparts but believe them to be false. Exploiting this\ndeficiency, we propose a jailbreak attack method that elicits an aligned\nlanguage model for malicious output. Specifically, we query the model to\ngenerate a fallacious yet deceptively real procedure for the harmful behavior.\nSince a fallacious procedure is generally considered fake and thus harmless by\nLLMs, it helps bypass the safeguard mechanism. Yet the output is factually\nharmful since the LLM cannot fabricate fallacious solutions but proposes\ntruthful ones. We evaluate our approach over five safety-aligned large language\nmodels, comparing four previous jailbreak methods, and show that our approach\nachieves competitive performance with more harmful outputs. We believe the\nfindings could be extended beyond model safety, such as self-verification and\nhallucination.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the main conference of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.00869v2",
    "published_date": "2024-07-01 00:23:43 UTC",
    "updated_date": "2024-09-23 19:48:20 UTC"
  }
]
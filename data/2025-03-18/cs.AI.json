{
  "date": "2025-03-18",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-18 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 热闹非凡，NVIDIA 连发多篇论文，聚焦物理 AI、通用人形机器人基础模型 GR00T N1 及可控世界生成，引发广泛关注。LLM 领域持续火热，新架构 RWKV-7 亮相，同时 RAG 优化、长任务评估、安全漏洞及伦理偏见等问题得到深入探讨。扩散模型在视频/图像生成与编辑、MoE 融合等方面取得新进展。强化学习在机器人控制、电路合成及社会困境模拟中展现威力。此外，AI 安全治理、可解释性、半监督学习和新基准测试也是今日的研究热点。\n\n以下是今日精选论文：\n\n---\n\n**重点关注：NVIDIA 物理 AI 系列**\n\n*   **GR00T N1：通用人形机器人的开放基础模型 (GR00T N1: An Open Foundation Model for Generalist Humanoid Robots)**\n    由 NVIDIA 众多研究者共同发布。介绍了一个名为 GR00T N1 的开放基础模型，专为人形机器人设计。该模型是一个视觉-语言-动作 (VLA) 模型，采用双系统架构（System 2 负责视觉语言理解，System 1 负责生成实时流畅的运动动作），通过混合真实机器人轨迹、人类视频和合成数据进行端到端训练。模型在模拟基准和实际机器人（Fourier GR-1）上展示了强大的性能和数据效率。这是构建通用自主机器人的重要一步。\n\n*   **Cosmos-Reason1：从物理常识到具身推理 (Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning)**\n    同样来自 NVIDIA。提出了 Cosmos-Reason1 模型（8B 和 56B），旨在让 AI 系统理解物理世界并生成具身的决策（如动作）。模型训练包含视觉预训练、通用 SFT、物理 AI SFT 和物理 AI RL 四个阶段。研究者定义了物理 AI 推理的关键能力，并构建了全面的基准进行评估，结果显示物理 AI SFT 和 RL 显著提升了性能。\n\n*   **Cosmos-Transfer1：具有自适应多模态控制的条件世界生成 (Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control)**\n    NVIDIA 的另一项工作。介绍了 Cosmos-Transfer 模型，可以根据分割、深度、边缘等多种空间控制输入生成世界模拟。该模型具有自适应和可定制的条件方案，允许在不同空间位置对不同条件输入进行加权，实现高度可控的世界生成，并在机器人 Sim2Real 和自动驾驶数据增强等物理 AI 应用中展示了潜力。\n\n---\n\n**LLM 新架构与应用**\n\n*   **RWKV-7 \"Goose\"：具有表达性动态状态演化的新架构 (RWKV-7 \"Goose\" with Expressive Dynamic State Evolution)**\n    提出了一种新的序列建模架构 RWKV-7 \"Goose\"，在 3B 参数规模的多语言任务上达到 SOTA 水平，且训练所需 token 数远少于同类模型。该架构引入了广义 delta 规则、向量值门控和上下文学习率，保持了恒定的内存使用和推理时间，并理论上能识别所有正则语言，超越了 Transformer 的能力限制。\n\n*   **RAGO：检索增强生成服务的系统性能优化 (RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving)**\n    针对 RAG 服务效率问题，提出了 RAGSchema（结构化抽象 RAG 算法）和 RAGO（RAG 优化器）框架。通过分析不同 RAG 工作负载的性能差异，RAGO 旨在优化 RAG 服务，实现更高的 QPS 和更低的首 token 延迟。\n\n*   **LLM-FE：使用 LLM 作为进化优化器进行表格数据的自动化特征工程 (LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers)**\n    提出 LLM-FE 框架，将特征工程视为程序搜索问题，利用 LLM 的领域知识和推理能力结合进化搜索来自动发现表格学习任务的有效特征，并通过数据驱动反馈指导搜索过程，显著提升了预测模型性能。\n\n*   **PLAY2PROMPT：通过工具试玩实现 LLM Agent 的零样本工具指令优化 (PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play)**\n    提出 PLAY2PROMPT 框架，自动探索外部工具的输入输出行为，通过试错过程优化工具文档并生成用法示例，无需标注数据即可提升 LLM 在零样本场景下使用工具的性能。\n\n*   **评估 LLM 完成长任务的能力 (Measuring AI Ability to Complete Long Tasks)**\n    提出新指标“50% 任务完成时间范围”，用于量化 AI 相对于人类的能力。通过测试发现，当前前沿 AI（如 Claude 3.7 Sonnet）的时间范围约为 50 分钟，且自 2019 年以来大约每 7 个月翻一番，主要得益于可靠性、适应性、逻辑推理和工具使用能力的提升。\n\n*   **Frac-Connections：超连接的分数扩展 (Frac-Connections: Fractional Extension of Hyper-Connections)**\n    针对 Hyper-Connections 增加内存访问成本的问题，提出 Frac-Connections。它将隐藏状态分成多个部分而非扩展宽度，保留了 Hyper-Connections 的部分优势，同时减少内存消耗。在大型语言模型实验中表现优于残差连接。\n\n*   **利用 LLM 从时间序列推断事件描述 (Inferring Event Descriptions from Time Series with Language Models)**\n    首次研究 LLM 从时间序列数据推断自然语言事件的能力。构建了包含体育比赛数据和事件描述的新基准，评估发现 LLM 在此任务上表现出潜力，但仍有改进空间。\n\n*   **JuDGE：中文法律体系判决文书生成基准测试 (JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System)**\n    提出了 JuDGE 基准，用于评估中文法律判决文书生成任务。构建了包含案件事实描述和对应完整判决文书的数据集，并提供了外部法律知识库。评估了多种基线方法，发现 RAG 能有效提升性能，但仍有改进空间。\n\n*   **利用 LLM 从患者咨询到图谱：构建患者旅程知识图谱 (From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction)**\n    提出一种使用 LLM 处理临床文档和医患对话来构建患者旅程知识图谱 (PJKG) 的方法。PJKG 整合患者信息，支持时间推理和个性化护理。评估了不同 LLM 在生成图谱方面的准确性和效率。\n\n*   **KG-IRAG：基于知识图谱的迭代检索增强生成框架用于时间推理 (KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning)**\n    提出 KG-IRAG 框架，将知识图谱与迭代推理相结合，以处理涉及时间和逻辑依赖的查询。通过迭代检索从外部 KG 收集相关数据，实现逐步推理，特别适用于需要动态时间数据提取的场景。\n\n*   **GraphRAG-FI：赋能 GraphRAG 的知识过滤与整合 (Empowering GraphRAG with Knowledge Filtering and Integration)**\n    为解决 GraphRAG 中检索信息噪声和过度依赖外部知识的问题，提出 GraphRAG-FI 框架，包含两阶段过滤机制和基于 logits 的选择策略，以平衡外部知识和 LLM 内在推理，提升推理性能。\n\n---\n\n**扩散模型与生成模型**\n\n*   **MusicInfuser：让视频扩散模型“听懂”音乐并起舞 (MusicInfuser: Making Video Diffusion Listen and Dance)**\n    提出 MusicInfuser，通过引入轻量级的音乐-视频交叉注意力和低秩适配器，使现有的视频扩散模型能够根据指定音乐生成高质量、同步的舞蹈视频，无需动作捕捉数据。\n\n*   **MagicComp：用于组合式视频生成的免训练双阶段优化 (MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation)**\n    提出免训练方法 MagicComp，通过在条件阶段引入语义锚点消歧，在去噪阶段引入动态布局融合注意力，增强了文本到视频生成中多主体属性绑定、空间关系确定和复杂动作交互的准确性。\n\n*   **上下文的力量：多模态如何改进图像超分辨率 (The Power of Context: How Multimodality Improves Image Super-Resolution)**\n    提出一种利用深度、分割、边缘和文本提示等多模态信息学习生成先验的方法，用于扩散模型框架下的图像超分辨率。引入了灵活的网络架构融合多模态信息，并通过空间信息引导缓解文本提示引入的幻觉。\n\n*   **DiffMoE：可扩展扩散 Transformer 的动态 Token 选择 (DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers)**\n    提出 DiffMoE，将 MoE (Mixture-of-Experts) 引入扩散 Transformer。通过批量级全局 token 池促进专家特化，并使用容量预测器根据噪声水平和样本复杂度动态分配计算资源，在 ImageNet 上达到 SOTA 性能。\n\n*   **CRCE：文本到图像扩散模型中保留共指的概念擦除 (CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models)**\n    提出 CRCE 框架，利用 LLM 识别应与目标概念一同擦除的相关概念以及应保留的不同概念，实现更精确的概念移除，避免过度擦除或擦除不足。\n\n*   **DefectFill：用于视觉检测的修复扩散模型生成逼真缺陷 (DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection)**\n    提出 DefectFill 方法，利用微调的修复扩散模型和定制损失函数，仅需少量参考缺陷图像即可生成逼真的工业缺陷，用于增强视觉检测模型的训练。\n\n*   **SALAD：基于骨架感知的潜在扩散模型用于文本驱动的动作生成与编辑 (SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing)**\n    提出 SALAD 模型，显式捕捉关节、帧和词之间的复杂关系。利用生成过程中的交叉注意力图，实现基于注意力的零样本文本驱动动作编辑，无需额外用户输入。\n\n*   **Concat-ID：迈向通用的身份保持视频合成 (Concat-ID: Towards Universal Identity-Preserving Video Synthesis)**\n    提出 Concat-ID 框架，使用 VAE 提取图像特征，并将其与视频潜在表示沿序列维度拼接，仅利用 3D 自注意力机制实现身份保持的视频生成，适用于单/多身份生成、虚拟试穿等场景。\n\n*   **用对角解码加速自回归视频生成 (Fast Autoregressive Video Generation with Diagonal Decoding)**\n    提出 DiagD 算法，一种用于自回归预训练模型的免训练推理加速方法。通过沿时空 token 网格的对角线路径生成 token，实现帧内并行解码和帧间部分重叠解码，显著提升长视频生成速度。\n\n---\n\n**AI 安全、伦理与可解释性**\n\n*   **RAT：无需额外数据提升误分类检测能力 (RAT: Boosting Misclassification Detection Ability without Extra Data)**\n    提出使用鲁棒半径（输入空间裕度）作为置信度度量来检测图像分类模型的误分类输入，并设计了两种高效估计算法 RR-BS 和 RR-Fast。同时提出 Radius Aware Training (RAT) 训练方法来提升模型的误分类识别能力。\n\n*   **DPImageBench：差分隐私图像合成的统一基准测试 (DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis)**\n    针对 DP 图像合成领域评估协议不一致的问题，提出 DPImageBench 统一基准，包含 11 种方法、9 个数据集和 7 种评估指标，并修正了现有评估中的缺陷。研究发现预训练并非总是有益，噪声添加位置对隐私预算敏感性不同。\n\n*   **动态累积注意力图：解释 ViT 决策过程演变 (Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer)**\n    提出 DAAM 方法，首次可视化 ViT 网络内部从顶层到底层的注意力流演变过程。通过分解模块构建空间特征信息，并结合通道重要性系数（为自监督模型提出维度重要性权重），逐块累积生成动态注意力图。\n\n*   **ExDDV：用于视频中可解释 Deepfake 检测的新数据集 (ExDDV: A New Dataset for Explainable Deepfake Detection in Video)**\n    发布首个用于视频中可解释 Deepfake 检测的数据集和基准 ExDDV，包含约 5.4K 真实和伪造视频，带有文本描述和点击标注以解释和定位伪造痕迹。评估了多种 VLM 在该数据集上的表现。\n\n*   **国际 AI 安全协议：回顾与条件性 AI 安全条约建议 (International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty)**\n    回顾了近期关于 AI 安全国际协议的提议，识别了共识与分歧，并评估了可行性。基于此，提出一项条约建议：设定计算阈值，超过该阈值的发展需严格监管，并由国际 AI 安全研究所网络监督模型、信息安全和治理实践的审计。\n\n*   **评估大型语言模型在编程问题解决中自动生成反馈 (Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving)**\n    评估了四种 LLM（GPT-4o, GPT-4o mini, GPT-4-Turbo, Gemini-1.5-pro）为学生编程代码生成反馈的能力，发现 63% 的反馈准确完整，但 37% 存在错误，强调了在教育应用中提升可靠性的必要性。\n\n*   **大型语言模型中的性别与内容偏见：以 Google Gemini 2.0 Flash Experimental 为例 (Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental)**\n    评估了 Gemini 2.0 Flash Experimental 在内容审核和性别差异方面的偏见。发现其性别偏见有所减少，但对性和暴力内容的容忍度更高，引发了关于其宽松性影响的担忧。\n\n*   **高效但脆弱：LLM 批量提示攻击的基准测试与防御 (Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack)**\n    揭示了 LLM 批量提示的安全漏洞：恶意用户可注入攻击指令干扰批次内所有查询。构建了 BATCHSAFEBENCH 基准进行系统研究，发现所有 LLM 都易受攻击，并探索了基于提示和探测的防御方法。\n\n*   **DeepSeek 模型在中文语境下的安全评估与增强 (Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts)**\n    利用中文安全基准 CHiSafetyBench 对 DeepSeek-R1 系列蒸馏模型进行安全评估，发现蒸馏对模型安全有负面影响。针对性地进行了安全增强，提升了安全性且未显著降低推理能力。\n\n*   **绘制信任版图：软件工程中的 LLM——洞见与视角 (Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives)**\n    通过文献综述和专家调查，梳理了软件工程领域 LLM 中与信任相关的概念（信任、不信任、可信赖性），旨在澄清研究现状，识别未来研究机会，并弥合文献与开发者认知之间的差距。\n\n*   **空椅子：使用 LLM 在政策审议中提出缺失的观点 (The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations)**\n    探索使用 LLM Persona 在政策审议中引入缺席相关者的观点。开发并评估了一个实时转录对话并模拟输入的工具，发现其能激发新讨论，但也存在生成内容过于笼统和过度依赖 AI 的风险。\n\n---\n\n**强化学习与机器人**\n\n*   **基于强化学习的运动模仿用于生理学上合理的肌肉骨骼运动控制 (Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control)**\n    提出 KINESIS 框架，使用无模型强化学习控制包含 80 个肌肉执行器和 20 个自由度的下半身肌肉骨骼模型，实现了对动作捕捉数据的强模仿性能，生成的肌肉活动模式与人类 EMG 相关性良好，并可用于研究运动控制理论。\n\n*   **Pauli 网络电路合成的强化学习 (Pauli Network Circuit Synthesis with Reinforcement Learning)**\n    提出一种基于 RL 的方法，用于重新合成包含任意 Pauli 旋转和 Clifford 操作的量子电路。通过学习启发式方法逐步合成，获得了更短且符合硬件连接约束的电路，在基准测试中显著优于现有方法。\n\n*   **SocialJax：序贯社会困境中多智能体强化学习的评估套件 (SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas)**\n    介绍了 SocialJax，一个用 JAX 实现的序贯社会困境环境套件，旨在高效评估 MARL 算法在社会困境中的泛化能力。相比 Melting Pot，在 GPU/TPU 上实现了显著的性能加速。\n\n*   **CTSAC：基于课程的 Transformer Soft Actor-Critic 用于目标导向的机器人探索 (CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration)**\n    提出 CTSAC 算法，将 Transformer 集成到 SAC 框架中以增强环境推理能力，并采用基于周期性回顾的课程学习提高训练效率和减轻灾难性遗忘，旨在提升机器人探索效率和 Sim-to-Real 迁移性能。\n\n*   **用于真实世界自适应交通信号控制的并行混合动作空间强化学习模型 (A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control)**\n    提出 PH-DDPG 模型，用于同时优化交通信号的相位和持续时间。模型采用并行混合动作空间，可同时输出离散相位选择和连续持续时间参数，简化决策过程，适用于动态交通自适应控制。\n\n*   **GeoFlow-SLAM：用于动态腿式机器人的鲁棒紧耦合 RGBD-惯性融合 SLAM (GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics)**\n    提出 GeoFlow-SLAM，一种针对腿式机器人在高度动态环境下的 RGBD-惯性 SLAM 系统。通过结合几何一致性、腿式里程计约束和双流光流 (GeoFlow)，解决了快速运动和视觉特征稀疏场景下的特征匹配、位姿初始化和长期定位挑战。\n\n*   **可变时间步长 MPC 用于敏捷多旋翼无人机拦截动态目标 (Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic Targets)**\n    提出一种用于多旋翼无人机敏捷轨迹规划的 MPC 方法，引入可变时间步长并与预测视界长度耦合，以解决固定时间步长带来的计算负担和对 UAV 动态利用不足的问题，提高了动态目标拦截等任务的规划质量。\n\n*   **通过动作分块和 Transformer 进行双臂协调学习双臂操作 (Learning Bimanual Manipulation via Action Chunking and Inter-Arm Coordination with Transformers)**\n    提出一种新的模仿学习架构，用于预测双臂协作动作。通过区分双臂架构并添加中间编码器层 (IACE)，促进同步和时间对齐，实现更平滑协调的动作，并在双臂操作任务中验证了有效性。\n\n*   **COLSON：基于扩散的强化学习实现可控的基于学习的社交导航 (COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning)**\n    将基于扩散的强化学习应用于社交导航任务，允许比基于高斯分布的方法更灵活的动作分布。利用扩散模型的特性，提出了一种扩展方法，能够在训练后平滑动作并适应训练中未考虑的静态障碍物场景。\n\n*   **反事实经验增强的离策略强化学习 (Counterfactual experience augmented off-policy reinforcement learning)**\n    提出反事实经验增强 (CEA) 算法，利用 VAE 建模状态转移的动态模式，并通过反事实推理扩展经验池中的学习数据，以缓解离策略 RL 中的分布外和探索效率低的问题。\n\n*   **VARP：基于视觉语言模型反馈和智能体正则化偏好的强化学习 (VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences)**\n    提出 VARP 方法，通过在最终观测上叠加轨迹草图来改进 VLM 提供的偏好反馈准确性，并通过结合智能体性能来正则化奖励学习，使其与当前策略对齐，提升了在连续控制机器人任务中的性能。\n\n---\n\n**计算机视觉与其他**\n\n*   **Involution 与 BSConv 多深度蒸馏网络用于轻量级图像超分辨率 (Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution)**\n    提出 IBMDN 网络，结合 Involution 和 BSConv，平衡计算效率和特征提取，并使用 CHFAB 增强高频细节，旨在实现轻量级、高精度的图像超分辨率。\n\n*   **LipShiFT：一个可证明鲁棒的基于 Shift 的视觉 Transformer (LipShiFT: A Certifiably Robust Shift-based Vision Transformer)**\n    研究了基于 Lipschitz 的 ViT 鲁棒性训练，发现 Lipschitz 约束是一种强正则化器。针对 ShiftViT 的 Lipschitz 连续变体，解决了训练挑战，并提供了 Lipschitz 常数的上界估计，提升了 Transformer 架构的可证明鲁棒性。\n\n*   **CP-SSM：核心-边缘原则引导的状态空间模型用于功能连接组分类 (Core-Periphery Principle Guided State Space Model for Functional Connectome Classification)**\n    提出 CP-SSM 框架，将具有线性复杂度的状态空间模型 Mamba 用于捕捉功能脑网络中的长程依赖。并设计了受核心-边缘结构启发的 CP-MoE (Mixture-of-Experts) 来改进脑连接模式的表示学习，用于神经系统疾病诊断。\n\n*   **用于多中心 STAS 肺癌组织病理学诊断的尺度感知多示例学习方法 (SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis)**\n    针对肺癌 STAS (Spread through air spaces) 诊断的主观性和变异性，构建并发布了三个多中心 STAS 数据集。提出 SMILE 方法，一种尺度感知的多示例学习方法，通过尺度自适应注意力机制减少对局部区域的过度依赖，提高了 STAS 诊断的准确性。\n\n*   **HSOD-BIT-V2：高光谱显著目标检测的新挑战性基准 (HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object Detection)**\n    发布了 HSOD-BIT-V2 数据集，是目前最大且最具挑战性的高光谱显著目标检测基准，包含五个针对小目标和前景背景相似性的挑战。同时提出 Hyper-HRNet 网络，用于有效提取和整合高光谱信息。\n\n*   **TGBFormer：用于视频目标检测的 Transformer-GraphFormer 混合网络 (TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection)**\n    提出 TGBFormer 网络，结合 Transformer 捕捉全局上下文信息和 GraphFormer 聚合局部时空关系，并通过全局-局部特征混合器自适应融合两种表示，在 ImageNet VID 数据集上达到 SOTA 性能。\n\n*   **通过掩码图像一致性与差异学习提升半监督医学图像分割 (Boosting Semi-Supervised Medical Image Segmentation via Masked Image Consistency and Discrepancy Learning)**\n    提出 MICD 框架，用于半监督医学图像分割。通过掩码交叉伪一致性 (MCPC)、交叉特征一致性 (CFC) 和交叉模型差异 (CMD) 三个模块，平衡信息交换和模型多样性，提升了对未标注数据的利用效率。\n\n*   **EATTA：用于长期测试时适应的轻松主动标注 (Effortless Active Labeling for Long-Term Test-Time Adaptation)**\n    针对长期 TTA 中的误差累积问题，提出一种主动标注策略，每批次最多标注一个样本。通过单步优化视角选择最有价值的样本（位于源域和目标域分布边界），并使用特征扰动高效识别，同时平衡标注和未标注样本的梯度影响。\n\n*   **LED：无需人工策划数据生成的 LLM 增强开放词汇目标检测 (LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation)**\n    提出 LED 方法，利用 MLLM 中 LLM 解码器层的隐藏状态来增强视觉定位能力。引入零初始化交叉注意力适配器，实现从 LLM 到目标检测器的知识迁移，提升了对复杂自由文本查询的检测性能。\n\n*   **KANITE：用于 ITE 估计的 Kolmogorov-Arnold 网络 (KANITE: Kolmogorov-Arnold Networks for ITE estimation)**\n    提出 KANITE 框架，利用 Kolmogorov-Arnold Networks (KANs) 进行多处理设置下的个体处理效应 (ITE) 估计。包含 IPM 和 EB 两种架构，利用 KAN 学习单变量激活函数的特性来提升 ITE 估计的准确性。\n\n---\n\n**其他值得关注的论文**\n\n*   **贝叶斯建模零样本分类用于城市洪水检测 (Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection)**：结合 VLM 零样本分类和空间贝叶斯模型检测城市洪水，无需大量标注数据，并提供不确定性量化。\n*   **用于 GUI 理解的具有多模态感知的 MLLM (MP-GUI: Modality Perception with MLLMs for GUI Understanding)**：提出 MP-GUI 模型，专门设计用于理解 GUI，包含三个感知器分别提取图形、文本和空间模态信息。\n*   **用于量子机器学习的模块化量子计算机实现大规模分布式量子长短期记忆网络 (Toward Large-Scale Distributed Quantum Long Short-Term Memory with Modular Quantum Computers)**：提出分布式 QLSTM 框架，利用模块化量子计算解决 NISQ 设备上的可扩展性挑战。\n*   **基于流的时间序列生成的理论基础：可证明的近似、泛化和效率 (Theoretical Foundation of Flow-Based Time Series Generation: Provable Approximation, Generalization, and Efficiency)**：首次为基于流的时间序列生成模型提供了近似、泛化和效率方面的理论保证。\n*   **ON-Traffic：基于算子学习的拉格朗日传感器在线交通流估计与不确定性量化框架 (ON-Traffic: An Operator Learning Framework for Online Traffic Flow Estimation and Uncertainty Quantification from Lagrangian Sensors)**：提出 ON-Traffic 框架，使用深度算子网络和 receding horizon learning，根据移动探针车辆数据在线估计时空交通状态并量化不确定性。\n*   **超越全息术：图像处理的熵量子引力基础 (Beyond holography: the entropic quantum gravity foundations of image processing)**：将著名的 Perona-Malik 图像处理算法解释为熵量子引力作用量的梯度流，建立了理论物理、AI 和图像处理之间的新联系。\n*   **半监督领域泛化中未标记数据的潜力解锁 (Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization)**：提出 UPCSC 方法，首次探索在 SSDG 中利用之前被忽略的低置信度未标记样本，通过代理对比学习和代理类别学习提升泛化性能。\n\n---\n\n今天的论文内容丰富，涵盖了从基础模型构建、算法优化到具体应用和伦理安全的广泛议题。希望这份 TLDR 能帮助你快速把握前沿动态！",
  "papers": [
    {
      "arxiv_id": "2503.14783v1",
      "title": "RAT: Boosting Misclassification Detection Ability without Extra Data",
      "title_zh": "RAT：无需额外数据提升误分类检测能力",
      "authors": [
        "Ge Yan",
        "Tsui-Wei Weng"
      ],
      "abstract": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.",
      "tldr_zh": "本文提出了一种无需额外数据的误分类检测增强方法RAT（Radius Aware Training），通过利用对抗扰动视角下的鲁棒半径（robust radius）作为置信度指标，设计了两种高效估计算法RR-BS和RR-Fast。RAT训练方法显著提升了模型识别错误的能力，实验表明该方法在AURC和FPR@95TPR指标上分别减少了29.3%和21.62%，优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14783v1",
      "published_date": "2025-03-18 23:18:55 UTC",
      "updated_date": "2025-03-18 23:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:47:42.571014"
    },
    {
      "arxiv_id": "2503.14779v1",
      "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
      "title_zh": "《基于内卷与BSConv多深度蒸馏网络的轻量化图像超分辨率重建》",
      "authors": [
        "Akram Khatami-Rizi",
        "Ahmad Mahmoudi-Aznaveh"
      ],
      "abstract": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.",
      "tldr_zh": "该研究提出了一种轻量级图像超分辨率（SISR）网络IBMDN，通过结合Involution和BSConv多深度蒸馏模块（IBMDB）以及对比高频注意力模块（CHFAB），在降低计算复杂度的同时提升重建质量。IBMDB利用Involution和BSConv的协同作用平衡计算效率与特征提取能力，而CHFAB则专门增强高频细节以改善视觉效果。实验表明，该方法在PSNR和SSIM指标上表现优异，可兼容Transformer和GAN架构，在降低内存占用的同时提升感知质量，为资源受限设备提供了高效超分辨率解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14779v1",
      "published_date": "2025-03-18 23:10:08 UTC",
      "updated_date": "2025-03-18 23:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:47:56.831751"
    },
    {
      "arxiv_id": "2503.15558v1",
      "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
      "title_zh": "Cosmos-Reason1：从物理常识到具身推理",
      "authors": [
        "NVIDIA",
        ":",
        "Alisson Azzolini",
        "Hannah Brandon",
        "Prithvijit Chattopadhyay",
        "Huayu Chen",
        "Jinju Chu",
        "Yin Cui",
        "Jenna Diamond",
        "Yifan Ding",
        "Francesco Ferroni",
        "Rama Govindaraju",
        "Jinwei Gu",
        "Siddharth Gururani",
        "Imad El Hanafi",
        "Zekun Hao",
        "Jacob Huffman",
        "Jingyi Jin",
        "Brendan Johnson",
        "Rizwan Khan",
        "George Kurian",
        "Elena Lantz",
        "Nayeon Lee",
        "Zhaoshuo Li",
        "Xuan Li",
        "Tsung-Yi Lin",
        "Yen-Chen Lin",
        "Ming-Yu Liu",
        "Andrew Mathau",
        "Yun Ni",
        "Lindsey Pavao",
        "Wei Ping",
        "David W. Romero",
        "Misha Smelyanskiy",
        "Shuran Song",
        "Lyne Tchapmi",
        "Andrew Z. Wang",
        "Boxin Wang",
        "Haoxiang Wang",
        "Fangyin Wei",
        "Jiashu Xu",
        "Yao Xu",
        "Xiaodong Yang",
        "Zhuolin Yang",
        "Xiaohui Zeng",
        "Zhe Zhang"
      ],
      "abstract": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
      "tldr_zh": "该研究提出了Cosmos-Reason1系列模型（8B/56B参数），通过分层本体论构建物理常识知识体系，实现从物理常识到具身推理的突破。模型采用四阶段训练方案（视觉预训练、通用SFT、物理AI专项SFT和强化学习），在空间-时间-物理三维本体和二维具身本体基础上，能够通过长链思维推理生成自然语言决策。评测显示专项微调和强化学习带来显著性能提升，为物理AI系统开发提供了新基准。相关代码和预训练模型将通过NVIDIA开放许可发布。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15558v1",
      "published_date": "2025-03-18 22:06:58 UTC",
      "updated_date": "2025-03-18 22:06:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:48:19.675756"
    },
    {
      "arxiv_id": "2503.14755v1",
      "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors",
      "title_zh": "通过词向量的正交变换实现语言无关的命名实体识别",
      "authors": [
        "Omar E. Rakha",
        "Hazem M. Abbas"
      ],
      "abstract": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.",
      "tldr_zh": "该研究提出了一种基于双向LSTM/CRF和词嵌入的跨语言命名实体识别(NER)方法。通过训练英语源语言模型，并利用正交线性变换矩阵将目标语言词向量映射到源语言空间，实现了无需目标语言训练数据的迁移学习。实验表明，仅用英语数据训练的模型可直接在阿拉伯语数据集上有效识别命名实体，准确率达到实用水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper was initially released in 2017 but was never published",
      "pdf_url": "http://arxiv.org/pdf/2503.14755v1",
      "published_date": "2025-03-18 21:57:58 UTC",
      "updated_date": "2025-03-18 21:57:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:48:38.425035"
    },
    {
      "arxiv_id": "2503.14754v1",
      "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection",
      "title_zh": "基于贝叶斯建模的零样本分类用于城市洪水检测",
      "authors": [
        "Matt Franchi",
        "Nikhil Garg",
        "Wendy Ju",
        "Emma Pierson"
      ],
      "abstract": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.",
      "tldr_zh": "该研究提出BayFlood方法，采用两阶段策略解决城市洪水检测中的标注数据稀缺问题：首先利用预训练的视觉语言模型(VLM)进行零样本分类(zero-shot classification)，再通过空间贝叶斯模型处理VLM输出结果。这种方法不仅避免了大规模标注需求，还能提供不确定性量化、空间平滑处理等关键功能。验证表明，该方案在多城市洪水检测中优于基线方法，成功识别出传统方法遗漏的11.3万高风险人群，并揭示了现有检测系统的人口统计偏差。研究为结合基础模型与贝叶斯推理的零样本学习范式提供了成功案例。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "In review",
      "pdf_url": "http://arxiv.org/pdf/2503.14754v1",
      "published_date": "2025-03-18 21:53:37 UTC",
      "updated_date": "2025-03-18 21:53:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:49:04.539483"
    },
    {
      "arxiv_id": "2503.14751v1",
      "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
      "title_zh": "LipShiFT：一种可认证鲁棒的基于位移机制的视觉Transformer",
      "authors": [
        "Rohan Menon",
        "Nicola Franco",
        "Stephan Günnemann"
      ],
      "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures.",
      "tldr_zh": "本研究提出了LipShiFT，一种基于ShiftViT模型的可证明鲁棒性视觉Transformer。针对Transformer架构在训练过程中因大输入尺寸和高维注意力模块导致的瓶颈问题，研究通过Lipschitz连续性约束和权重限制，显著提升了模型的训练效率和鲁棒性。研究提供了基于$l_2$范数的Lipschitz常数上界估计，并在图像分类数据集上验证了该方法在大规模模型中的可扩展性，推动了Transformer架构在可证明鲁棒性领域的最新进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Workshop: VerifAI: AI Verification in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2503.14751v1",
      "published_date": "2025-03-18 21:38:18 UTC",
      "updated_date": "2025-03-18 21:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:49:14.119994"
    },
    {
      "arxiv_id": "2503.14734v1",
      "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
      "title_zh": "GR00T N1：面向通用人形机器人的开放基础模型",
      "authors": [
        "NVIDIA",
        "Johan Bjorck",
        "Fernando Castañeda",
        "Nikita Cherniadev",
        "Xingye Da",
        "Runyu Ding",
        "Linxi \"Jim\" Fan",
        "Yu Fang",
        "Dieter Fox",
        "Fengyuan Hu",
        "Spencer Huang",
        "Joel Jang",
        "Zhenyu Jiang",
        "Jan Kautz",
        "Kaushil Kundalia",
        "Lawrence Lao",
        "Zhiqi Li",
        "Zongyu Lin",
        "Kevin Lin",
        "Guilin Liu",
        "Edith Llontop",
        "Loic Magne",
        "Ajay Mandlekar",
        "Avnish Narayan",
        "Soroush Nasiriany",
        "Scott Reed",
        "You Liang Tan",
        "Guanzhi Wang",
        "Zu Wang",
        "Jing Wang",
        "Qi Wang",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Yinzhen Xu",
        "Zhenjia Xu",
        "Seonghyeon Ye",
        "Zhiding Yu",
        "Ao Zhang",
        "Hao Zhang",
        "Yizhou Zhao",
        "Ruijie Zheng",
        "Yuke Zhu"
      ],
      "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.",
      "tldr_zh": "该研究提出GR00T N1——首个面向通用人形机器人的开放基础模型。该模型采用Vision-Language-Action (VLA)架构，包含视觉语言理解(System 2)和实时动作生成(System 1)双系统，通过端到端联合训练实现环境感知与运动控制的紧密耦合。研究创新性地融合真实机器人轨迹、人类视频和合成数据进行训练，在多项仿真基准测试中超越现有模仿学习方法，并在Fourier GR-1人形机器人上验证了语言引导双手操作的优异性能。该模型为构建适应人类环境的通用机器人智能提供了开放基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Authors are listed alphabetically. Project leads are Linxi \"Jim\" Fan\n  and Yuke Zhu",
      "pdf_url": "http://arxiv.org/pdf/2503.14734v1",
      "published_date": "2025-03-18 21:06:21 UTC",
      "updated_date": "2025-03-18 21:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:49:39.420963"
    },
    {
      "arxiv_id": "2503.14716v1",
      "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform",
      "title_zh": "基于Mask R-CNN与霍夫变换的建筑工地脚手架完整性检测",
      "authors": [
        "Pei-Hsin Lin",
        "Jacob J. Lin",
        "Shang-Hsien Hsieh"
      ],
      "abstract": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.",
      "tldr_zh": "该研究提出了一种基于Mask R-CNN和霍夫变换(Hough Transform)的深度学习框架，用于自动检测建筑工地脚手架完整性。该方法通过训练卷积神经网络(CNN)模型识别脚手架及其关键部件（如交叉支撑），无需人工检查即可从现场图像中判断部件缺失情况。实验证明，这种非侵入式解决方案能显著节省时间和人力成本，有效提升工地安全管理效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering",
      "pdf_url": "http://arxiv.org/pdf/2503.14716v1",
      "published_date": "2025-03-18 20:27:22 UTC",
      "updated_date": "2025-03-18 20:27:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:49:59.024623"
    },
    {
      "arxiv_id": "2503.15555v1",
      "title": "Whole-Body Image-to-Image Translation for a Virtual Scanner in a Healthcare Digital Twin",
      "title_zh": "全身图像到图像转换技术在医疗数字孪生虚拟扫描仪中的应用",
      "authors": [
        "Valerio Guarrasi",
        "Francesco Di Feola",
        "Rebecca Restivo",
        "Lorenzo Tronchin",
        "Paolo Soda"
      ],
      "abstract": "Generating positron emission tomography (PET) images from computed tomography\n(CT) scans via deep learning offers a promising pathway to reduce radiation\nexposure and costs associated with PET imaging, improving patient care and\naccessibility to functional imaging. Whole-body image translation presents\nchallenges due to anatomical heterogeneity, often limiting generalized models.\nWe propose a framework that segments whole-body CT images into four\nregions-head, trunk, arms, and legs-and uses district-specific Generative\nAdversarial Networks (GANs) for tailored CT-to-PET translation. Synthetic PET\nimages from each region are stitched together to reconstruct the whole-body\nscan. Comparisons with a baseline non-segmented GAN and experiments with\nPix2Pix and CycleGAN architectures tested paired and unpaired scenarios.\nQuantitative evaluations at district, whole-body, and lesion levels\ndemonstrated significant improvements with our district-specific GANs. Pix2Pix\nyielded superior metrics, ensuring precise, high-quality image synthesis. By\naddressing anatomical heterogeneity, this approach achieves state-of-the-art\nresults in whole-body CT-to-PET translation. This methodology supports\nhealthcare Digital Twins by enabling accurate virtual PET scans from CT data,\ncreating virtual imaging representations to monitor, predict, and optimize\nhealth outcomes.",
      "tldr_zh": "该研究提出了一种基于区域分割的全身CT到PET图像转换框架，通过将全身CT图像分割为头部、躯干、手臂和腿部四个区域，并采用区域特定的生成对抗网络(GAN)进行针对性转换，最后拼接生成完整的PET图像。与未分割的基线GAN模型相比，该方法在Pix2Pix和CycleGAN架构下（包括配对和非配对场景）均展现出显著优势，其中Pix2Pix架构在区域、全身和病灶层面的定量评估中表现最优。该技术通过解决解剖异质性难题，实现了当前最先进的全身医学图像转换效果，为医疗数字孪生提供了从CT数据生成虚拟PET扫描的新方法，可用于健康监测、预测和优化。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15555v1",
      "published_date": "2025-03-18 20:19:28 UTC",
      "updated_date": "2025-03-18 20:19:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:50:21.126925"
    },
    {
      "arxiv_id": "2503.14681v1",
      "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
      "title_zh": "DPImageBench：差分隐私图像合成的统一基准测试框架",
      "authors": [
        "Chen Gong",
        "Kecen Li",
        "Zinan Lin",
        "Tianhao Wang"
      ],
      "abstract": "Differentially private (DP) image synthesis aims to generate artificial\nimages that retain the properties of sensitive images while protecting the\nprivacy of individual images within the dataset. Despite recent advancements,\nwe find that inconsistent--and sometimes flawed--evaluation protocols have been\napplied across studies. This not only impedes the understanding of current\nmethods but also hinders future advancements.\n  To address the issue, this paper introduces DPImageBench for DP image\nsynthesis, with thoughtful design across several dimensions: (1) Methods. We\nstudy eleven prominent methods and systematically characterize each based on\nmodel architecture, pretraining strategy, and privacy mechanism. (2)\nEvaluation. We include nine datasets and seven fidelity and utility metrics to\nthoroughly assess them. Notably, we find that a common practice of selecting\ndownstream classifiers based on the highest accuracy on the sensitive test set\nnot only violates DP but also overestimates the utility scores. DPImageBench\ncorrects for these mistakes. (3) Platform. Despite the methods and evaluation\nprotocols, DPImageBench provides a standardized interface that accommodates\ncurrent and future implementations within a unified framework. With\nDPImageBench, we have several noteworthy findings. For example, contrary to the\ncommon wisdom that pretraining on public image datasets is usually beneficial,\nwe find that the distributional similarity between pretraining and sensitive\nimages significantly impacts the performance of the synthetic images and does\nnot always yield improvements. In addition, adding noise to low-dimensional\nfeatures, such as the high-level characteristics of sensitive images, is less\naffected by the privacy budget compared to adding noise to high-dimensional\nfeatures, like weight gradients. The former methods perform better than the\nlatter under a low privacy budget.",
      "tldr_zh": "本文提出了DPImageBench，一个用于差分隐私(DP)图像合成的统一基准测试框架，旨在解决现有研究中评价方法不一致和存在缺陷的问题。该框架从三个方面进行了系统设计：(1) 方法：研究了11种主流方法，从模型架构、预训练策略和隐私机制等方面进行表征；(2) 评价：涵盖9个数据集和7个保真度和效用指标，纠正了现有研究中违反DP原则和夸大效用分数的常见做法；(3) 平台：提供了标准化的接口，支持当前和未来方法的统一实现。通过DPImageBench，研究发现：预训练对合成图像性能的影响取决于预训练数据与敏感图像的分布相似性；在低隐私预算下，对低维特征（如图像高层特征）添加噪声的方法优于对高维特征（如权重梯度）添加噪声的方法。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "The first two authors contributed equally; code available at\n  https://github.com/2019ChenGong/DPImageBench",
      "pdf_url": "http://arxiv.org/pdf/2503.14681v1",
      "published_date": "2025-03-18 19:37:35 UTC",
      "updated_date": "2025-03-18 19:37:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:50:40.622917"
    },
    {
      "arxiv_id": "2503.14662v1",
      "title": "ConQuer: A Framework for Concept-Based Quiz Generation",
      "title_zh": "ConQuer：基于概念的测验生成框架",
      "authors": [
        "Yicheng Fu",
        "Zikui Wang",
        "Liuxin Yang",
        "Meiqing Huo",
        "Zhongdongming Dai"
      ],
      "abstract": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.",
      "tldr_zh": "该研究提出ConQuer框架，一种基于概念(Concept-Based)的智能测验生成系统，通过整合外部知识源解决现有AI生成测验质量不足的问题。该方法采用大语言模型(LLMs)作为评估者，实验表明其生成的测验在评分上比基线提升4.8%，在成对比较中获胜率达77.52%。消融研究验证了框架各组件有效性，为教育领域的自动化高质量测验生成提供了可靠解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14662v1",
      "published_date": "2025-03-18 19:10:26 UTC",
      "updated_date": "2025-03-18 19:10:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:51:01.824262"
    },
    {
      "arxiv_id": "2503.14655v1",
      "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
      "title_zh": "核心-边缘原则引导的功能连接组分类状态空间模型",
      "authors": [
        "Minheng Chen",
        "Xiaowei Yu",
        "Jing Zhang",
        "Tong Chen",
        "Chao Cao",
        "Yan Zhuang",
        "Yanjun Lyu",
        "Lu Zhang",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "abstract": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.",
      "tldr_zh": "本研究提出了一种基于核心-外围原则(Core-Periphery Principle)指导的状态空间模型(CP-SSM)，用于功能性脑连接组分类。该模型创新性地结合了具有线性计算复杂度的选择性状态空间模型Mamba，以及受脑网络核心-外围结构启发的CP-MoE专家混合系统，有效捕捉脑功能网络的长程依赖关系。在ABIDE和ADNI两个fMRI基准数据集上的实验表明，CP-SSM不仅超越了基于Transformer的模型分类性能，还显著降低了计算复杂度。这一方法为基于神经影像的神经系统疾病诊断提供了高效建模工具。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14655v1",
      "published_date": "2025-03-18 19:03:27 UTC",
      "updated_date": "2025-03-18 19:03:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:51:29.870832"
    },
    {
      "arxiv_id": "2503.14649v2",
      "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
      "title_zh": "RAGO：检索增强生成服务的系统性性能优化",
      "authors": [
        "Wenqi Jiang",
        "Suvinay Subramanian",
        "Cat Graves",
        "Gustavo Alonso",
        "Amir Yazdanbakhsh",
        "Vidushi Dadu"
      ],
      "abstract": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
      "tldr_zh": "本研究提出RAGO系统优化框架，用于解决检索增强生成(RAG)服务中的性能优化挑战。通过引入结构化抽象RAGSchema来统一描述各类RAG算法，分析发现不同RAG工作负载存在显著性能差异。实验表明，RAGO框架相比基于LLM系统扩展的RAG方案，能实现每芯片吞吐量(QPS)提升2倍和首token延迟降低55%的性能优化效果。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "C.1; C.4; H.3"
      ],
      "primary_category": "cs.IR",
      "comment": "16 pages, 19 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.14649v2",
      "published_date": "2025-03-18 18:58:13 UTC",
      "updated_date": "2025-03-21 17:51:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:51:38.333918"
    },
    {
      "arxiv_id": "2503.14640v1",
      "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer",
      "title_zh": "动态累积注意力图：解读视觉Transformer决策演化过程的创新方法",
      "authors": [
        "Yi Liao",
        "Yongsheng Gao",
        "Weichuan Zhang"
      ],
      "abstract": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.",
      "tldr_zh": "这篇论文提出了一种名为动态累积注意力图(DAAM)的新型可视化解释方法，用于揭示Vision Transformer(ViT)模型中决策注意力区域的演化过程。该方法通过创新的分解模块解锁每个ViT块中[class]令牌生成的空间特征信息，并结合通道重要性系数，首次实现了从顶层到底层的注意力流可视化。针对有监督和无监督ViT模型，分别采用分类分数分解和维度重要性权重来计算通道重要性。实验证明，DAAM不仅能有效解释全连接层分类器的ViT模型，也能适用于自监督ViT模型，为理解Transformer视觉模型的决策机制提供了新工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14640v1",
      "published_date": "2025-03-18 18:41:01 UTC",
      "updated_date": "2025-03-18 18:41:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:52:01.652073"
    },
    {
      "arxiv_id": "2503.14637v1",
      "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control",
      "title_zh": "基于强化学习的运动模仿：实现生理合理性的肌肉骨骼运动控制",
      "authors": [
        "Merkourios Simos",
        "Alberto Silvio Chiappa",
        "Alexander Mathis"
      ],
      "abstract": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.",
      "tldr_zh": "该研究提出了一种基于强化学习(RL)的运动模仿框架KINESIS，用于实现生理学上合理的肌肉骨骼运动控制。该系统采用包含80个肌肉执行器和20个自由度的下肢肌肉骨骼模型，在1.9小时动作捕捉数据上展现出优秀的模仿性能，并能通过预训练文本-动作生成模型实现自然语言控制。值得注意的是，KINESIS生成的肌肉活动模式与人类肌电图(EMG)高度相关，为解决Bernstein冗余问题等人类运动控制理论难题提供了新工具。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14637v1",
      "published_date": "2025-03-18 18:37:49 UTC",
      "updated_date": "2025-03-18 18:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:52:27.425079"
    },
    {
      "arxiv_id": "2503.14630v1",
      "title": "Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving",
      "title_zh": "评估大型语言模型在编程问题解决学习中自动生成反馈的能力",
      "authors": [
        "Priscylla Silva",
        "Evandro Costa"
      ],
      "abstract": "Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.",
      "tldr_zh": "该研究评估了大型语言模型（LLMs）在编程问题解决中自动生成反馈的能力，测试了GPT-4o、GPT-4o mini、GPT-4-Turbo和Gemini-1.5-pro四种模型。实验基于45份学生代码解决方案的数据集，发现63%的反馈提示准确完整，但仍有37%存在错误，包括错误行定位、解释缺陷或虚构问题。研究揭示了LLMs在编程教育中的潜力与局限性，强调需进一步提升可靠性以降低教育应用风险。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14630v1",
      "published_date": "2025-03-18 18:31:36 UTC",
      "updated_date": "2025-03-18 18:31:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:52:58.093972"
    },
    {
      "arxiv_id": "2503.14621v1",
      "title": "Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach",
      "title_zh": "降低ICU环境中的室性心动过速误报警：一种机器学习方法",
      "authors": [
        "Grace Funmilayo Farayola",
        "Akinyemi Sadeeq Akintola",
        "Oluwole Fagbohun",
        "Chukwuka Michael Oforgu",
        "Bisola Faith Kayode",
        "Christian Chimezie",
        "Temitope Kadri",
        "Abiola Oludotun",
        "Nelson Ogbeide",
        "Mgbame Michael",
        "Adeseye Ifaturoti",
        "Toyese Oloyede"
      ],
      "abstract": "False arrhythmia alarms in intensive care units (ICUs) are a significant\nchallenge, contributing to alarm fatigue and potentially compromising patient\nsafety. Ventricular tachycardia (VT) alarms are particularly difficult to\ndetect accurately due to their complex nature. This paper presents a machine\nlearning approach to reduce false VT alarms using the VTaC dataset, a benchmark\ndataset of annotated VT alarms from ICU monitors. We extract time-domain and\nfrequency-domain features from waveform data, preprocess the data, and train\ndeep learning models to classify true and false VT alarms. Our results\ndemonstrate high performance, with ROC-AUC scores exceeding 0.96 across various\ntraining configurations. This work highlights the potential of machine learning\nto improve the accuracy of VT alarm detection in clinical settings.",
      "tldr_zh": "本研究提出了一种机器学习方法，用于降低ICU中误报的室性心动过速(VT)警报。通过从VTaC数据集的波形数据中提取时域和频域特征，并训练深度学习模型，系统能有效区分真假VT警报。实验结果显示模型性能优异，在不同训练配置下ROC-AUC评分均超过0.96，为解决临床警报疲劳问题提供了有效方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, Accepted to the International Conference on Machine\n  Learning Technologies (ICMLT 2025), Helsinki, Finland",
      "pdf_url": "http://arxiv.org/pdf/2503.14621v1",
      "published_date": "2025-03-18 18:18:38 UTC",
      "updated_date": "2025-03-18 18:18:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:53:05.848202"
    },
    {
      "arxiv_id": "2503.14604v1",
      "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
      "title_zh": "多模态大语言模型时代的图像描述评估：挑战与未来展望",
      "authors": [
        "Sara Sarto",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "abstract": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
      "tldr_zh": "本文系统综述了多模态大语言模型(MLLMs)时代下图像描述生成任务的评估挑战。研究分析了现有评估指标在人类判断相关性、排序准确性和对幻觉敏感性等多维度的表现，并指出这些指标难以适应MLLM生成的长文本、细节丰富的描述风格。论文揭示了标准评估方法的局限性，为未来图像描述评估研究指明了改进方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation",
      "pdf_url": "http://arxiv.org/pdf/2503.14604v1",
      "published_date": "2025-03-18 18:03:56 UTC",
      "updated_date": "2025-03-18 18:03:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:53:18.624784"
    },
    {
      "arxiv_id": "2503.14505v1",
      "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
      "title_zh": "MusicInfuser：让视频扩散模型随乐起舞",
      "authors": [
        "Susung Hong",
        "Ira Kemelmacher-Shlizerman",
        "Brian Curless",
        "Steven M. Seitz"
      ],
      "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
      "tldr_zh": "该研究提出了MusicInfuser，一种通过轻量级音乐-视频交叉注意力机制和低秩适配器，将现有视频扩散模型与音乐输入对齐的方法，用于生成与指定音乐同步的高质量舞蹈视频。与需要运动捕捉数据的研究不同，该方法仅对舞蹈视频进行微调，在保持模型灵活性和生成能力的同时，实现了高质量的音乐驱动视频生成。研究还引入了基于Video-LLMs的评估框架，以多维度评估舞蹈生成质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://susunghong.github.io/MusicInfuser",
      "pdf_url": "http://arxiv.org/pdf/2503.14505v1",
      "published_date": "2025-03-18 17:59:58 UTC",
      "updated_date": "2025-03-18 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:53:40.476300"
    },
    {
      "arxiv_id": "2503.14503v1",
      "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
      "title_zh": "上下文的力量：多模态如何提升图像超分辨率",
      "authors": [
        "Kangfu Mei",
        "Hossein Talebi",
        "Mojtaba Ardakani",
        "Vishal M. Patel",
        "Peyman Milanfar",
        "Mauricio Delbracio"
      ],
      "abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.",
      "tldr_zh": "该论文提出了一种基于多模态信息的图像超分辨率(SISR)新方法，通过整合深度图、分割图、边缘图和文本提示等多种模态数据，在扩散模型框架中构建了强大的生成先验。研究设计了灵活的神经网络架构，能够融合任意数量的输入模态，并通过空间信息引导有效缓解文本提示导致的伪影问题。实验表明，该方法在生成式SISR任务中超越了现有技术，实现了更好的视觉质量和保真度，同时允许通过独立控制各模态的引导强度来定向调整输出效果（如通过深度图增强景深效果）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14503v1",
      "published_date": "2025-03-18 17:59:54 UTC",
      "updated_date": "2025-03-18 17:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:54:08.220918"
    },
    {
      "arxiv_id": "2503.14499v1",
      "title": "Measuring AI Ability to Complete Long Tasks",
      "title_zh": "衡量AI完成长任务的能力",
      "authors": [
        "Thomas Kwa",
        "Ben West",
        "Joel Becker",
        "Amy Deng",
        "Katharyn Garcia",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Nate Rush",
        "Sydney Von Arx",
        "Ryan Bloom",
        "Thomas Broadley",
        "Haoxing Du",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Luke Harold Miles",
        "Seraphina Nix",
        "Tao Lin",
        "Neev Parikh",
        "David Rein",
        "Lucas Jun Koba Sato",
        "Hjalmar Wijk",
        "Daniel M. Ziegler",
        "Elizabeth Barnes",
        "Lawrence Chan"
      ],
      "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
      "tldr_zh": "该研究提出了一种新指标“50%任务完成时间范围”，用于量化AI系统在完成长任务方面的能力，即人类通常完成AI模型能以50%成功率完成的任务所需的时间。研究通过测试人类专家在RE-Bench、HCAST和66项新任务上的表现，发现当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间范围约为50分钟。自2019年以来，AI模型的时间范围每7个月翻一番，主要得益于更高的可靠性、错误适应能力以及逻辑推理和工具使用能力的提升。研究预测，若这一趋势持续，5年内AI将能够自动化许多目前人类需要一个月才能完成的软件任务。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14499v1",
      "published_date": "2025-03-18 17:59:31 UTC",
      "updated_date": "2025-03-18 17:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:54:31.377145"
    },
    {
      "arxiv_id": "2503.14493v2",
      "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
      "title_zh": "状态空间模型与Transformer的融合：3D物体检测新范式",
      "authors": [
        "Chuxin Wang",
        "Wenfei Yang",
        "Xiang Liu",
        "Tianzhu Zhang"
      ],
      "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.",
      "tldr_zh": "本文提出了一种结合状态空间模型（SSM）与Transformer的3D物体检测新范式DEST，解决了现有基于DETR的方法中场景点特征固定导致后期解码层贡献不足的问题。该方法创新性地将查询建模为系统状态、场景点作为系统输入，通过状态依赖的SSM参数化方法和四项关键设计（序列化/双向扫描策略、状态间注意力机制、门控前馈网络），实现了线性复杂度下的双向特征交互。实验表明，DEST在ScanNet V2和SUN RGB-D数据集上分别将AP50指标提升5.3和3.2个百分点，创造了新的SOTA性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR 2025. Project url:\n  https://chuxwa.github.io/project_DEST/",
      "pdf_url": "http://arxiv.org/pdf/2503.14493v2",
      "published_date": "2025-03-18 17:58:03 UTC",
      "updated_date": "2025-03-19 14:10:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:54:49.151818"
    },
    {
      "arxiv_id": "2503.14492v1",
      "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
      "title_zh": "Cosmos-Transfer1：基于自适应多模态控制的条件化世界生成",
      "authors": [
        "NVIDIA",
        ":",
        "Hassan Abu Alhaija",
        "Jose Alvarez",
        "Maciej Bala",
        "Tiffany Cai",
        "Tianshi Cao",
        "Liz Cha",
        "Joshua Chen",
        "Mike Chen",
        "Francesco Ferroni",
        "Sanja Fidler",
        "Dieter Fox",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ali Hassani",
        "Michael Isaev",
        "Pooya Jannaty",
        "Shiyi Lan",
        "Tobias Lasser",
        "Huan Ling",
        "Ming-Yu Liu",
        "Xian Liu",
        "Yifan Lu",
        "Alice Luo",
        "Qianli Ma",
        "Hanzi Mao",
        "Fabio Ramos",
        "Xuanchi Ren",
        "Tianchang Shen",
        "Shitao Tang",
        "Ting-Chun Wang",
        "Jay Wu",
        "Jiashu Xu",
        "Stella Xu",
        "Kevin Xie",
        "Yuchong Ye",
        "Xiaodong Yang",
        "Xiaohui Zeng",
        "Yu Zeng"
      ],
      "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
      "tldr_zh": "该研究提出Cosmos-Transfer1模型，这是一种基于多模态空间条件输入（如分割图、深度图和边缘图）的自适应世界生成框架。其创新点在于可定制化的空间条件机制，能对不同位置的不同模态输入进行差异化加权，实现高度可控的世界模拟生成，特别适用于Sim2Real等场景转换任务。实验表明，该模型不仅支持机器人仿真到现实转换和自动驾驶数据增强等Physical AI应用，还能通过NVIDIA GB200 NVL72机架实现实时世界生成。研究团队已开源模型和代码以推动领域发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14492v1",
      "published_date": "2025-03-18 17:57:54 UTC",
      "updated_date": "2025-03-18 17:57:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:55:52.349866"
    },
    {
      "arxiv_id": "2503.14488v1",
      "title": "Engineering Scientific Assistants using Interactive Structured Induction of Programs",
      "title_zh": "利用交互式结构化程序归纳构建科学助手",
      "authors": [
        "Shraddha Surana",
        "Ashwin Srinivasan"
      ],
      "abstract": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
      "tldr_zh": "该研究提出了一种交互式结构化归纳编程方法iStrucInd，用于快速构建科学数据分析助手。该方法通过软件工程师与大型语言模型(LLM)的协作，将领域专家的自然语言需求转化为可执行程序，克服了当前\"无代码\"技术在复杂科学问题中的局限性。实验对比了手工开发、低代码/无代码和iStrucInd三种方式，结果表明该系统能更快地开发出性能更优、质量更高的科学分析程序，为加速解决复杂科学问题提供了有效工具。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14488v1",
      "published_date": "2025-03-18 17:57:16 UTC",
      "updated_date": "2025-03-18 17:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:55:29.758943"
    },
    {
      "arxiv_id": "2503.14487v1",
      "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
      "title_zh": "DiffMoE：面向可扩展扩散变换器的动态令牌选择机制",
      "authors": [
        "Minglei Shi",
        "Ziyang Yuan",
        "Haotian Yang",
        "Xintao Wang",
        "Mingwu Zheng",
        "Xin Tao",
        "Wenliang Zhao",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
      "tldr_zh": "本文提出了DiffMoE，一种针对扩散模型（Diffusion Models）的可扩展动态token选择方法，旨在解决传统方法在不同条件和噪声水平下对输入进行均匀处理的局限性。DiffMoE通过引入batch-level全局token池和容量预测器（capacity predictor），动态分配计算资源，从而优化专家模型的行为。实验表明，DiffMoE在ImageNet基准测试中实现了最先进的性能，显著优于现有方法，同时保持了较低的计算开销。该方法不仅适用于类别条件生成，还在文本到图像生成等更具挑战性的任务中展现了广泛的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://shiml20.github.io/DiffMoE/",
      "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
      "published_date": "2025-03-18 17:57:07 UTC",
      "updated_date": "2025-03-18 17:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:55:46.007124"
    },
    {
      "arxiv_id": "2503.14484v1",
      "title": "Gricean Norms as a Basis for Effective Collaboration",
      "title_zh": "格赖斯准则作为有效协作的基础",
      "authors": [
        "Fardin Saad",
        "Pradeep K. Murukannaiah",
        "Munindar P. Singh"
      ],
      "abstract": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
      "tldr_zh": "该研究提出了一种基于Gricean会话准则的规范性框架，旨在提升人类与AI的协作效率。通过整合Gricean的\"数量、质量、关联、方式\"四大准则及推理规范，该框架帮助基于大语言模型(LLM)的智能体处理模糊、不完整或无关的指令。研究者开发了GPT-4驱动的Lamoid智能体，实验证明采用Gricean准则的版本在网格世界任务中表现更优，不仅提高了任务准确率，还能生成更清晰、准确且符合语境的响应。这一框架为增强LLM智能体的语用推理能力提供了有效路径。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms",
      "pdf_url": "http://arxiv.org/pdf/2503.14484v1",
      "published_date": "2025-03-18 17:54:14 UTC",
      "updated_date": "2025-03-18 17:54:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:56:16.210148"
    },
    {
      "arxiv_id": "2503.14469v1",
      "title": "Attribution Score Alignment in Explainable Data Management",
      "title_zh": "可解释数据管理中的归因评分对齐",
      "authors": [
        "Felipe Azua",
        "Leopoldo Bertossi"
      ],
      "abstract": "Different attribution-scores have been proposed to quantify the relevance of\ndatabase tuples for a query answer from a database. Among them, we find Causal\nResponsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal\nEffect. They have been analyzed in isolation, mainly in terms of computational\nproperties. In this work, we start an investigation into the alignment of these\nscores on the basis of the queries at hand; that is, on whether they induce\ncompatible rankings of tuples. We are able to identify vast classes of queries\nfor which some pairs of scores are always aligned, and others for which they\nare not. It turns out that the presence of exogenous tuples makes a crucial\ndifference in this regard.",
      "tldr_zh": "该研究首次系统分析了数据库查询中不同归因评分方法（包括Causal Responsibility、Shapley Value、Banzhaf Power-Index和Causal Effect）的评分一致性。研究发现，对于特定查询类别，某些评分方法会产生完全一致的元组排序，而其他情况则存在明显分歧，其中外生元组的存在是影响评分一致性的关键因素。这项工作为可解释数据管理中的评分方法选择提供了理论基础。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14469v1",
      "published_date": "2025-03-18 17:45:32 UTC",
      "updated_date": "2025-03-18 17:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:56:28.537102"
    },
    {
      "arxiv_id": "2503.14456v1",
      "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
      "title_zh": "RWKV-7\"Goose\"：具有动态状态演化表达能力的新型架构",
      "authors": [
        "Bo Peng",
        "Ruichong Zhang",
        "Daniel Goldstein",
        "Eric Alcaide",
        "Haowen Hou",
        "Janna Lu",
        "William Merrill",
        "Guangyu Song",
        "Kaifeng Tan",
        "Saiteja Utpala",
        "Nathan Wilce",
        "Johan S. Wind",
        "Tianyi Wu",
        "Daniel Wuttke",
        "Christian Zhou-Zheng"
      ],
      "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
      "tldr_zh": "该研究提出RWKV-7\"Goose\"新型序列建模架构，通过引入向量化门控的广义delta规则和上下文学习率机制，在仅30亿参数规模下实现多语言任务的新SOTA性能，其英语表现媲美当前最佳模型但训练token量显著更少。该架构具有恒定内存和推理时间的优势，能完成状态跟踪和识别所有常规语言，突破了Transformer在$\\mathsf{TC}^0$复杂度下的理论限制。团队开源了3.1万亿token多语言语料库和四个参数量级模型（0.19B-2.9B），所有资源均采用Apache 2.0许可证发布。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14456v1",
      "published_date": "2025-03-18 17:31:05 UTC",
      "updated_date": "2025-03-18 17:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:57:16.669909"
    },
    {
      "arxiv_id": "2503.14448v1",
      "title": "Pauli Network Circuit Synthesis with Reinforcement Learning",
      "title_zh": "基于强化学习的泡利网络电路综合方法",
      "authors": [
        "Ayushi Dubal",
        "David Kremer",
        "Simon Martiel",
        "Victor Villar",
        "Derek Wang",
        "Juan Cruz-Benito"
      ],
      "abstract": "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads.",
      "tldr_zh": "该研究提出了一种基于强化学习（RL）的量子电路重合成方法，用于优化包含任意Pauli旋转和Clifford操作的量子电路。通过将子块压缩为紧凑表示并逐步合成，该方法生成了更短且符合硬件连接约束的电路。实验表明，与现有启发式方法相比，该RL方法在6量子比特随机Pauli网络上将两量子门数量减少了2倍以上，且每电路执行时间低于10毫秒。此外，将其集成到Qiskit编译器中，在Benchpress基准测试中平均减少了20%的两量子门数量和深度，部分实例甚至达到60%，展示了RL驱动合成在大规模量子编译任务中的显著潜力。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14448v1",
      "published_date": "2025-03-18 17:27:50 UTC",
      "updated_date": "2025-03-18 17:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:57:24.496065"
    },
    {
      "arxiv_id": "2503.14434v1",
      "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
      "title_zh": "LLM-FE：基于大语言模型进化优化的表格数据自动化特征工程",
      "authors": [
        "Nikhil Abhyankar",
        "Parshin Shojaee",
        "Chandan K. Reddy"
      ],
      "abstract": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
      "tldr_zh": "本文提出LLM-FE框架，利用大语言模型(LLMs)作为进化优化器，自动生成有效的表格数据特征。该方法将特征工程转化为程序搜索问题，通过LLMs迭代提出特征转换方案，并结合数据驱动反馈优化搜索过程。实验表明，LLM-FE在多种分类和回归任务上优于现有方法，显著提升了表格预测模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14434v1",
      "published_date": "2025-03-18 17:11:24 UTC",
      "updated_date": "2025-03-18 17:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:57:30.263180"
    },
    {
      "arxiv_id": "2503.14432v1",
      "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
      "title_zh": "PLAY2PROMPT：通过工具交互实现LLM智能体零样本指令优化的方法",
      "authors": [
        "Wei Fang",
        "Yang Zhang",
        "Kaizhi Qian",
        "James Glass",
        "Yada Zhu"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
      "tldr_zh": "该研究提出PLAY2PROMPT框架，通过自动\"试玩\"工具探索其输入输出行为，解决了大语言模型(LLM)在零样本场景下工具使用的难题。该方法无需人工标注数据，通过迭代式试错过程自动优化工具文档并生成使用示例，显著提升了开源和闭源模型在真实任务中的零样本工具调用性能。实验表明，该框架为领域专用工具集成提供了可扩展的有效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14432v1",
      "published_date": "2025-03-18 17:09:57 UTC",
      "updated_date": "2025-03-18 17:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:57:54.180008"
    },
    {
      "arxiv_id": "2503.14428v1",
      "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
      "title_zh": "MagicComp：面向组合式视频生成的无训练双阶段优化框架",
      "authors": [
        "Hongyu Zhang",
        "Yufan Deng",
        "Shenghai Yuan",
        "Peng Jin",
        "Zesen Cheng",
        "Yian Zhao",
        "Chang Liu",
        "Jie Chen"
      ],
      "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
      "tldr_zh": "本文提出MagicComp，一种无需训练的双阶段优化方法，用于提升组合式文本生成视频(T2V)的质量。该方法通过**语义锚点消歧**(Semantic Anchor Disambiguation)强化主体语义，并采用**动态布局融合注意力**(Dynamic Layout Fusion Attention)实现主体与时空区域的精准绑定。实验表明，MagicComp在T2V-CompBench和VBench基准上超越现有方法，适用于复杂提示和轨迹可控的视频生成任务。该框架无需额外训练，可灵活集成到现有T2V架构中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/",
      "pdf_url": "http://arxiv.org/pdf/2503.14428v1",
      "published_date": "2025-03-18 17:02:14 UTC",
      "updated_date": "2025-03-18 17:02:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:58:47.972900"
    },
    {
      "arxiv_id": "2503.14427v2",
      "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
      "title_zh": "VisEscape：虚拟密室逃脱中探索驱动决策的评估基准",
      "authors": [
        "Seungwon Lim",
        "Sungwoong Kim",
        "Jihwan Yu",
        "Sungjae Lee",
        "Jiwan Chung",
        "Youngjae Yu"
      ],
      "abstract": "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observe that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 4.9 times more efficiently on average\ncompared to baseline agents.",
      "tldr_zh": "该研究提出了VisEscape基准测试，包含20个虚拟密室逃脱场景，专门用于评估AI模型在探索驱动决策中的表现，重点关注动态环境下的空间-时间知识构建能力。研究发现，当前最先进的多模态模型普遍难以成功逃脱，为此团队开发了VisEscaper系统，通过整合记忆(Memory)、反馈(Feedback)和推理-行动(ReAct)模块，将逃脱效率平均提升3.7倍，行动效率提高4.9倍。该基准揭示了现有AI在持续探索和知识整合方面的关键短板。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14427v2",
      "published_date": "2025-03-18 16:59:09 UTC",
      "updated_date": "2025-03-22 05:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:58:34.721630"
    },
    {
      "arxiv_id": "2503.14421v1",
      "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video",
      "title_zh": "ExDDV：视频可解释深度伪造检测新数据集",
      "authors": [
        "Vlad Hondru",
        "Eduard Hogea",
        "Darian Onchis",
        "Radu Tudor Ionescu"
      ],
      "abstract": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.",
      "tldr_zh": "该研究提出了ExDDV数据集，这是首个面向可解释视频深度伪造检测（Explainable Deepfake Detection in Video）的基准数据集，包含约5.4K条真实和伪造视频，每条均带有描述伪造痕迹的文本标注和定位点击标注。实验表明，结合文本和点击监督训练的视觉语言模型能有效定位并描述伪造痕迹，为开发可解释的深度伪造检测系统提供了关键数据支持。该数据集和代码已开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14421v1",
      "published_date": "2025-03-18 16:55:07 UTC",
      "updated_date": "2025-03-18 16:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:58:54.540276"
    },
    {
      "arxiv_id": "2503.14412v1",
      "title": "Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts",
      "title_zh": "Iffy-Or-Not：扩展网络以支持对谬误文本的批判性评估",
      "authors": [
        "Gionnieve Lim",
        "Juho Kim",
        "Simon T. Perrault"
      ],
      "abstract": "Social platforms have expanded opportunities for deliberation with the\ncomments being used to inform one's opinion. However, using such information to\nform opinions is challenged by unsubstantiated or false content. To enhance the\nquality of opinion formation and potentially confer resistance to\nmisinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks\nto invoke critical thinking when reading texts. With three features guided by\nargumentation theory, ION highlights fallacious content, suggests diverse\nqueries to probe them with, and offers deeper questions to consider and chat\nwith others about. From a user study (N=18), we found that ION encourages users\nto be more attentive to the content, suggests queries that align with or are\npreferable to their own, and poses thought-provoking questions that expands\ntheir perspectives. However, some participants expressed aversion to ION due to\nmisalignments with their information goals and thinking predispositions.\nPotential backfiring effects with ION are discussed.",
      "tldr_zh": "这篇论文提出了Iffy-Or-Not(ION)浏览器扩展程序，旨在帮助用户识别网络文本中的谬误内容并培养批判性思维。该工具基于论证理论开发了三大功能：标记谬误内容、提供多样化查询建议、提出深度讨论问题。用户研究(N=18)表明，ION能有效提升用户对内容的警觉性，其建议的问题既符合用户需求又能拓展思维视角。不过研究也发现，部分用户因信息目标或思维习惯的不匹配而对工具产生抵触，并讨论了可能产生的反效果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14412v1",
      "published_date": "2025-03-18 16:50:20 UTC",
      "updated_date": "2025-03-18 16:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:59:09.038047"
    },
    {
      "arxiv_id": "2503.14411v1",
      "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
      "title_zh": "统一文本语义与图结构：基于大语言模型的时序文本属性图建模",
      "authors": [
        "Siwei Zhang",
        "Yun Xiong",
        "Yateng Tang",
        "Xi Chen",
        "Zian Jia",
        "Zehao Gu",
        "Jiarong Xu",
        "Jiawei Zhang"
      ],
      "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
      "tldr_zh": "该研究提出Cross框架，通过整合大型语言模型(LLMs)与现有时序图神经网络(TGNNs)，解决了时序文本属性图(TTAGs)建模中文本语义与图结构动态协同的难题。核心创新包括：1）引入时序语义提取器(Temporal Semantics Extractor)，利用LLMs捕捉节点文本邻域的语义动态演变；2）设计语义-结构协同编码器(Semantic-structural Co-encoder)，实现文本语义与图结构的双向增强表示。实验证明，该框架在四个公开数据集和一个工业数据集上显著优于现有方法，为动态多模态图学习提供了新范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submit to ICML2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14411v1",
      "published_date": "2025-03-18 16:50:10 UTC",
      "updated_date": "2025-03-18 16:50:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:59:37.602741"
    },
    {
      "arxiv_id": "2503.18956v1",
      "title": "International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty",
      "title_zh": "人工智能安全国际协议：条件性AI安全条约的审查与建议",
      "authors": [
        "Rebecca Scholefield",
        "Samuel Martin",
        "Otto Barten"
      ],
      "abstract": "The malicious use or malfunction of advanced general-purpose AI (GPAI) poses\nrisks that, according to leading experts, could lead to the 'marginalisation or\nextinction of humanity.' To address these risks, there are an increasing number\nof proposals for international agreements on AI safety. In this paper, we\nreview recent (2023-) proposals, identifying areas of consensus and\ndisagreement, and drawing on related literature to assess their feasibility. We\nfocus our discussion on risk thresholds, regulations, types of international\nagreement and five related processes: building scientific consensus,\nstandardisation, auditing, verification and incentivisation.\n  Based on this review, we propose a treaty establishing a compute threshold\nabove which development requires rigorous oversight. This treaty would mandate\ncomplementary audits of models, information security and governance practices,\noverseen by an international network of AI Safety Institutes (AISIs) with\nauthority to pause development if risks are unacceptable. Our approach combines\nimmediately implementable measures with a flexible structure that can adapt to\nongoing research.",
      "tldr_zh": "这篇论文探讨了应对通用人工智能(GPAI)潜在风险的全球治理方案。研究者系统评估了2023年以来各类国际AI安全协议的提案，聚焦风险阈值、监管框架等核心议题，提出以算力为基准的分级管控条约。该方案建议设立计算能力阈值，超过该阈值的AI研发需接受强制审计和国际AI安全研究所(AISI)网络监管，并赋予其暂停高风险开发的权限。该条约设计兼具即时可操作性与适应AI技术快速迭代的灵活性，为平衡创新发展与安全管控提供了可行路径。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.18956v1",
      "published_date": "2025-03-18 16:29:57 UTC",
      "updated_date": "2025-03-18 16:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T15:59:55.653870"
    },
    {
      "arxiv_id": "2503.14577v1",
      "title": "PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease",
      "title_zh": "PHGNN：一种新型提示式超图神经网络用于阿尔茨海默病诊断",
      "authors": [
        "Chenyu Liu",
        "Luca Rossi"
      ],
      "abstract": "The accurate diagnosis of Alzheimer's disease (AD) and prognosis of mild\ncognitive impairment (MCI) conversion are crucial for early intervention.\nHowever, existing multimodal methods face several challenges, from the\nheterogeneity of input data, to underexplored modality interactions, missing\ndata due to patient dropouts, and limited data caused by the time-consuming and\ncostly data collection process. In this paper, we propose a novel Prompted\nHypergraph Neural Network (PHGNN) framework that addresses these limitations by\nintegrating hypergraph based learning with prompt learning. Hypergraphs capture\nhigher-order relationships between different modalities, while our prompt\nlearning approach for hypergraphs, adapted from NLP, enables efficient training\nwith limited data. Our model is validated through extensive experiments on the\nADNI dataset, outperforming SOTA methods in both AD diagnosis and the\nprediction of MCI conversion.",
      "tldr_zh": "该研究提出了PHGNN（Prompted Hypergraph Neural Network），一种结合超图学习（hypergraph）与提示学习（prompt learning）的新型框架，用于阿尔茨海默病（AD）诊断和轻度认知障碍（MCI）转化预测。该模型通过超图捕捉多模态数据的高阶关联，并创新性地将NLP领域的提示学习技术迁移至超图结构，有效解决了数据异质性、模态交互不足以及小样本训练的挑战。在ADNI数据集上的实验表明，PHGNN在AD诊断和MCI转化预测任务上均优于现有最优方法（SOTA）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14577v1",
      "published_date": "2025-03-18 16:10:43 UTC",
      "updated_date": "2025-03-18 16:10:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:00:40.530817"
    },
    {
      "arxiv_id": "2503.14376v1",
      "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
      "title_zh": "分块闪存线性注意力：更高效的线性RNN与xLSTM内核",
      "authors": [
        "Maximilian Beck",
        "Korbinian Pöppel",
        "Phillip Lippe",
        "Sepp Hochreiter"
      ],
      "abstract": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.",
      "tldr_zh": "该研究提出了Tiled Flash Linear Attention (TFLA)，一种针对线性RNN的新型核算法，通过引入额外的序列并行化层级，支持任意大的块大小，从而解决了Flash Linear Attention (FLA)在长上下文预训练中的内存消耗和IO成本问题。研究将TFLA应用于xLSTM的矩阵内存版本mLSTM，并提出了计算量更少的mLSTM变体，进一步提升了核运行速度。实验表明，基于TFLA的mLSTM核在速度上超越了高度优化的Flash Attention、Linear Attention和Mamba核，为高效长上下文序列建模设定了新的技术标准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at: https://github.com/NX-AI/mlstm_kernels",
      "pdf_url": "http://arxiv.org/pdf/2503.14376v1",
      "published_date": "2025-03-18 16:09:47 UTC",
      "updated_date": "2025-03-18 16:09:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:00:32.034260"
    },
    {
      "arxiv_id": "2503.14576v1",
      "title": "SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
      "title_zh": "SocialJax：序列化社会困境中多智能体强化学习的评估套件",
      "authors": [
        "Zihao Guo",
        "Richard Willis",
        "Shuqing Shi",
        "Tristan Tomilin",
        "Joel Z. Leibo",
        "Yali Du"
      ],
      "abstract": "Social dilemmas pose a significant challenge in the field of multi-agent\nreinforcement learning (MARL). Melting Pot is an extensive framework designed\nto evaluate social dilemma environments, providing an evaluation protocol that\nmeasures generalization to new social partners across various test scenarios.\nHowever, running reinforcement learning algorithms in the official Melting Pot\nenvironments demands substantial computational resources. In this paper, we\nintroduce SocialJax, a suite of sequential social dilemma environments\nimplemented in JAX. JAX is a high-performance numerical computing library for\nPython that enables significant improvements in the operational efficiency of\nSocialJax on GPUs and TPUs. Our experiments demonstrate that the training\npipeline of SocialJax achieves a 50\\texttimes{} speedup in real-time\nperformance compared to Melting Pot's RLlib baselines. Additionally, we\nvalidate the effectiveness of baseline algorithms within the SocialJax\nenvironments. Finally, we use Schelling diagrams to verify the social dilemma\nproperties of these environments, ensuring they accurately capture the dynamics\nof social dilemmas.",
      "tldr_zh": "该研究提出了SocialJax，一个基于JAX高性能计算库构建的序列化社交困境多智能体强化学习(MARL)评估套件。相比现有Melting Pot框架，该工具在GPU/TPU上实现了50倍的实时训练加速，大幅降低了计算资源需求。研究通过Schelling图验证了环境能准确捕捉社交困境动态特性，并展示了基线算法在该框架中的有效性，为MARL社交困境研究提供了高效实验平台。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 18 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.14576v1",
      "published_date": "2025-03-18 16:03:59 UTC",
      "updated_date": "2025-03-18 16:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:00:56.382130"
    },
    {
      "arxiv_id": "2503.14354v1",
      "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications",
      "title_zh": "回顾：面向神经网络应用基于CORDIC的可配置激活函数",
      "authors": [
        "Omkar Kokane",
        "Gopal Raut",
        "Salim Ullah",
        "Mukul Lokhande",
        "Adam Teman",
        "Akash Kumar",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.",
      "tldr_zh": "这篇论文回顾了基于CORDIC算法的可配置激活函数设计，该设计通过功能可重构性加速了资源受限系统的ASIC硬件开发。研究团队介绍了新一代动态可配置、精度可调的DA-VINCI激活函数核心，采用Shift-and-Add CORDIC技术支持Swish、SoftMax等多种激活函数。该设计已优化整合为NEURIC计算单元，在DNN、RNN/LSTM和Transformer等AI加速器中实现98.5%的结果质量(QoR)，成为资源高效向量引擎的关键组件。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14354v1",
      "published_date": "2025-03-18 15:38:37 UTC",
      "updated_date": "2025-03-18 15:38:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:01:39.022624"
    },
    {
      "arxiv_id": "2503.14350v2",
      "title": "VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation",
      "title_zh": "VEGGIE：基于基础生成的视频概念教学式编辑与推理",
      "authors": [
        "Shoubin Yu",
        "Difan Liu",
        "Ziqiao Ma",
        "Yicong Hong",
        "Yang Zhou",
        "Hao Tan",
        "Joyce Chai",
        "Mohit Bansal"
      ],
      "abstract": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.",
      "tldr_zh": "该研究提出了VEGGIE框架，这是一个基于指令的端到端视频编辑系统，能够统一处理视频概念编辑、定位和推理任务。VEGGIE通过多模态大语言模型(MLLM)解析用户指令并定位视频上下文，结合扩散模型生成符合意图的编辑结果，采用课程学习策略先在大规模图像编辑数据上预训练，再在高质量视频数据上微调。实验表明，VEGGIE在多样化编辑任务中表现优异，支持零样本多模态指令和上下文视频编辑，显著优于现有基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.14350v2",
      "published_date": "2025-03-18 15:31:12 UTC",
      "updated_date": "2025-03-19 20:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:01:45.786032"
    },
    {
      "arxiv_id": "2503.16534v1",
      "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
      "title_zh": "大语言模型中的性别与内容偏见：以Google Gemini 2.0 Flash Experimental为例的案例研究",
      "authors": [
        "Roberto Balestri"
      ],
      "abstract": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content.",
      "tldr_zh": "本研究评估了谷歌先进大语言模型Gemini 2.0 Flash Experimental在内容审核和性别差异方面的偏见。与ChatGPT-4o相比，该模型展现出更低的性别偏见（女性相关提示接受率显著提升），但对暴力内容（包括性别相关案例）持更宽容态度，可能助长暴力正常化而非减少危害。研究发现，尽管男性相关提示仍比女性提示更易通过，这种降低性别偏见的改进是以容忍更多暴力内容为代价的，突显了AI伦理对齐的复杂性——在减少特定偏见的同时，可能引发新的伦理风险。研究强调需持续优化模型，以平衡透明度、公平性与内容安全性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16534v1",
      "published_date": "2025-03-18 15:28:22 UTC",
      "updated_date": "2025-03-18 15:28:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:02:00.725172"
    },
    {
      "arxiv_id": "2503.14345v2",
      "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
      "title_zh": "MoonCast：高质量零样本播客生成系统",
      "authors": [
        "Zeqian Ju",
        "Dongchao Yang",
        "Jianwei Yu",
        "Kai Shen",
        "Yichong Leng",
        "Zhengtao Wang",
        "Xu Tan",
        "Xinyu Zhou",
        "Tao Qin",
        "Xiangyang Li"
      ],
      "abstract": "Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.",
      "tldr_zh": "该研究提出MoonCast系统，致力于解决零样本(zero-shot)高质量播客生成的挑战。通过采用长上下文语言模型处理多分钟级语音合成，并创新性地引入播客脚本生成模块添加即兴口语细节，系统成功将文本资料转化为自然流畅的播客语音。实验表明，MoonCast在即兴感和连贯性方面显著优于基线模型，实现了对未见说话者声音的长篇多说话者对话的逼真合成。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14345v2",
      "published_date": "2025-03-18 15:25:08 UTC",
      "updated_date": "2025-03-19 07:17:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:02:24.015762"
    },
    {
      "arxiv_id": "2503.14341v1",
      "title": "Spatio-Temporal Graph Neural Networks for Infant Language Acquisition Prediction",
      "title_zh": "时空图神经网络在婴幼儿语言习得预测中的应用",
      "authors": [
        "Andrew Roxburgh",
        "Floriana Grasso",
        "Terry R. Payne"
      ],
      "abstract": "Predicting the words that a child is going to learn next can be useful for\nboosting language acquisition, and such predictions have been shown to be\npossible with both neural network techniques (looking at changes in the\nvocabulary state over time) and graph model (looking at data pertaining to the\nrelationships between words). However, these models do not fully capture the\ncomplexity of the language learning process of an infant when used in\nisolation. In this paper, we examine how a model of language acquisition for\ninfants and young children can be constructed and adapted for use in a\nSpatio-Temporal Graph Convolutional Network (STGCN), taking into account the\ndifferent types of linguistic relationships that occur during child language\nlearning. We introduce a novel approach for predicting child vocabulary\nacquisition, and evaluate the efficacy of such a model with respect to the\ndifferent types of linguistic relationships that occur during language\nacquisition, resulting in insightful observations on model calibration and norm\nselection. An evaluation of this model found that the mean accuracy of models\nfor predicting new words when using sensorimotor relationships (0.733) and\nsemantic relationships (0.729) were found to be superior to that observed with\na 2-layer Feed-forward neural network. Furthermore, the high recall for some\nrelationships suggested that some relationships (e.g. visual) were superior in\nidentifying a larger proportion of relevant words that a child should\nsubsequently learn than others (such as auditory).",
      "tldr_zh": "本研究提出了一种基于时空图卷积网络（STGCN）的婴儿语言习得预测模型，通过融合词汇间的时空关联特征来提升预测效果。该模型特别关注感觉运动（sensorimotor）和语义关系等不同语言关联类型，实验表明其预测新词的平均准确率达到0.733，优于传统双层前馈神经网络。研究发现视觉关联等特定关系类型在识别后续应学词汇方面表现尤为突出，为儿童语言发展干预提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14341v1",
      "published_date": "2025-03-18 15:21:27 UTC",
      "updated_date": "2025-03-18 15:21:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:02:41.645797"
    },
    {
      "arxiv_id": "2503.15551v1",
      "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
      "title_zh": "高效但脆弱：大语言模型批量提示攻击的基准测试与防御策略",
      "authors": [
        "Murong Yue",
        "Ziyu Yao"
      ],
      "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.",
      "tldr_zh": "该论文揭示了批量提示(batch prompting)技术存在重大安全漏洞：恶意用户可通过注入攻击指令干扰批量查询，导致有害内容或逻辑混乱。研究者构建了包含150种攻击指令的BATCHSAFEBENCH基准测试，发现开源和闭源大语言模型(LLMs)均易受此类攻击。通过机制分析识别出关键注意力头，并开发出检测准确率达95%的探测式防御方案，为批量推理的安全部署提供了重要保障。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15551v1",
      "published_date": "2025-03-18 15:16:10 UTC",
      "updated_date": "2025-03-18 15:16:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:02:58.124039"
    },
    {
      "arxiv_id": "2503.14333v1",
      "title": "Revealing higher-order neural representations with generative artificial intelligence",
      "title_zh": "揭示高阶神经表征的生成式人工智能方法",
      "authors": [
        "Hojjat Azimi Asrari",
        "Megan A. K. Peters"
      ],
      "abstract": "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
      "tldr_zh": "这篇论文提出了一种基于强化学习(RL)的生成式人工智能(genAI)新方法，用于探索大脑对不确定性分布的高阶表征(HORs)。研究者利用现有fMRI数据训练去噪扩散模型，通过RL算法模拟人类学习神经模式去噪的过程。与传统反向传播模型相比，RL模型对不确定性分布的表征具有更强的解释力，能更好地预测人类行为。该研究为利用genAI探索神经噪声分布的高阶表征开辟了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14333v1",
      "published_date": "2025-03-18 15:08:19 UTC",
      "updated_date": "2025-03-18 15:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:03:22.556718"
    },
    {
      "arxiv_id": "2503.14321v1",
      "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
      "title_zh": "COPA：通过比较不可比性探索帕累托前沿",
      "authors": [
        "Adrián Javaloy",
        "Antonio Vergari",
        "Isabel Valera"
      ],
      "abstract": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
      "tldr_zh": "该论文提出了COPA方法，用于解决机器学习中多目标优化时不同量纲指标难以比较的问题。通过将不可比目标转化为基于累积分布函数(CDF)的相对排名，使性能、能耗等不同量纲的目标具有可比性。该方法允许用户根据偏好聚合目标，有效探索Pareto前沿。在大型语言模型(LLM)选择、领域泛化和AutoML基准测试等场景中，COPA相比传统归一化方法展现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 14 figures. Under submission",
      "pdf_url": "http://arxiv.org/pdf/2503.14321v1",
      "published_date": "2025-03-18 14:51:42 UTC",
      "updated_date": "2025-03-18 14:51:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:03:37.013696"
    },
    {
      "arxiv_id": "2503.16533v1",
      "title": "From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction",
      "title_zh": "从患者咨询到图：利用大语言模型构建患者旅程知识图谱",
      "authors": [
        "Hassan S. Al Khatib",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Nina Marhamati",
        "Sean Bozorgzad"
      ],
      "abstract": "The transition towards patient-centric healthcare necessitates a\ncomprehensive understanding of patient journeys, which encompass all healthcare\nexperiences and interactions across the care spectrum. Existing healthcare data\nsystems are often fragmented and lack a holistic representation of patient\ntrajectories, creating challenges for coordinated care and personalized\ninterventions. Patient Journey Knowledge Graphs (PJKGs) represent a novel\napproach to addressing the challenge of fragmented healthcare data by\nintegrating diverse patient information into a unified, structured\nrepresentation. This paper presents a methodology for constructing PJKGs using\nLarge Language Models (LLMs) to process and structure both formal clinical\ndocumentation and unstructured patient-provider conversations. These graphs\nencapsulate temporal and causal relationships among clinical encounters,\ndiagnoses, treatments, and outcomes, enabling advanced temporal reasoning and\npersonalized care insights. The research evaluates four different LLMs, such as\nClaude 3.5, Mistral, Llama 3.1, and Chatgpt4o, in their ability to generate\naccurate and computationally efficient knowledge graphs. Results demonstrate\nthat while all models achieved perfect structural compliance, they exhibited\nvariations in medical entity processing and computational efficiency. The paper\nconcludes by identifying key challenges and future research directions. This\nwork contributes to advancing patient-centric healthcare through the\ndevelopment of comprehensive, actionable knowledge graphs that support improved\ncare coordination and outcome prediction.",
      "tldr_zh": "本研究提出了一种利用大语言模型(LLMs)构建患者旅程知识图谱(PJKGs)的新方法，旨在整合碎片化的医疗数据。该方法通过处理结构化临床文档和非结构化医患对话，构建包含诊疗时间序列和因果关系的知识图谱，支持时序推理和个性化诊疗。研究评估了Claude 3.5、Mistral等四种LLM的表现，发现虽然所有模型都能生成结构合规的图谱，但在医疗实体处理和计算效率上存在差异。该工作为提升以患者为中心的医疗协调和结果预测提供了技术支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16533v1",
      "published_date": "2025-03-18 14:44:28 UTC",
      "updated_date": "2025-03-18 14:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:04:08.564552"
    },
    {
      "arxiv_id": "2503.14295v2",
      "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation",
      "title_zh": "PC-Talk：面向音频驱动说话人脸生成的精准面部动画控制",
      "authors": [
        "Baiqin Wang",
        "Xiangyu Zhu",
        "Fan Shen",
        "Hao Xu",
        "Zhen Lei"
      ],
      "abstract": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.",
      "tldr_zh": "该研究提出了PC-Talk框架，通过隐式关键点变形技术实现音频驱动说话人脸生成中的精确动画控制。该框架包含两个核心模块：唇音对齐控制模块支持单词级别的说话风格编辑和唇部运动幅度调整，情感控制模块则能生成逼真的表情特征并实现强度调节与多情感区域组合。实验表明，该方法在HDTF和MEAD数据集上实现了最先进的性能表现，显著提升了说话视频的多样性和可控性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14295v2",
      "published_date": "2025-03-18 14:35:48 UTC",
      "updated_date": "2025-03-20 10:27:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:04:22.231928"
    },
    {
      "arxiv_id": "2503.14293v2",
      "title": "Ensemble Knowledge Distillation for Machine Learning Interatomic Potentials",
      "title_zh": "集成知识蒸馏用于机器学习原子间势能",
      "authors": [
        "Sakib Matin",
        "Emily Shinkle",
        "Yulia Pimonova",
        "Galen T. Craven",
        "Aleksandra Pachalieva",
        "Ying Wai Li",
        "Kipton Barros",
        "Nicholas Lubbers"
      ],
      "abstract": "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
      "tldr_zh": "该研究提出了一种集成知识蒸馏（EKD）方法，用于提升机器学习原子间势能（MLIPs）在仅含能量数据训练时的准确性。该方法首先训练多个基于量子化学能量的教师模型生成原子力数据，再让学生MLIP同时学习原始能量和集成平均力数据。在ANI-1ccx有机分子数据集上的实验表明，该方法在COMP6基准测试中创造了新的最优精度，并显著提升了分子动力学模拟的稳定性。这种EKD框架可广泛应用于化学、生物分子和材料科学模拟领域。",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14293v2",
      "published_date": "2025-03-18 14:32:51 UTC",
      "updated_date": "2025-03-19 15:03:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:04:41.396744"
    },
    {
      "arxiv_id": "2503.14572v1",
      "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
      "title_zh": "稳健权重印刻：来自神经坍缩与基于代理聚合的洞见",
      "authors": [
        "Justus Westerhoff",
        "Golzar Atefi",
        "Mario Koddenbrock",
        "Alexei Figueroa",
        "Alexander Löser",
        "Erik Rodner",
        "Felix A. Gers"
      ],
      "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes.",
      "tldr_zh": "该研究通过神经坍缩(Neural Collapse)理论深入分析了权重印刻(Weight Imprinting)技术，提出了包含生成、归一化和聚合三个关键环节的系统框架。研究发现：在生成阶段使用多个代理(proxies)表示新数据，并通过聚类确定这些代理，配合适当的归一化处理，能显著提升模型性能。基于这些发现提出的新方法在复杂数据分布场景下，将新类别的识别准确率最高提升了4%。该工作首次建立了权重印刻与神经坍缩现象的理论联系，为迁移学习提供了新的见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code: https://github.com/DATEXIS/multi-imprinting/",
      "pdf_url": "http://arxiv.org/pdf/2503.14572v1",
      "published_date": "2025-03-18 14:27:45 UTC",
      "updated_date": "2025-03-18 14:27:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:05:09.768954"
    },
    {
      "arxiv_id": "2503.14273v2",
      "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on RGB Images of Closed Canopy: Validation Using TLS",
      "title_zh": "人工标注人为提高了基于深度学习的闭合冠层RGB图像分割性能：使用TLS进行验证",
      "authors": [
        "Matthew J. Allen",
        "Harry J. F. Owen",
        "Stuart W. D. Grieve",
        "Emily R. Lines"
      ],
      "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
      "tldr_zh": "该研究通过地面激光扫描(TLS)验证发现，基于人工标注的深度学习树冠分割模型性能被严重高估。在未管理的针叶林和地中海混交林中，使用TLS数据作为金标准时，DeepForest和Detectree2模型的AP50指标从人工标注时的0.67骤降至0.094。研究揭示模型仅对上层树冠表现尚可(AP50:0.365)，在严格IoU阈值下定位精度极低(AP75最高仅0.051)，表明无人机RGB影像在密闭林冠条件下的分割方法存在根本性局限。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14273v2",
      "published_date": "2025-03-18 14:09:00 UTC",
      "updated_date": "2025-03-19 16:17:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:05:45.546886"
    },
    {
      "arxiv_id": "2503.14258v2",
      "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System",
      "title_zh": "JuDGE：中国法律体系判决书生成基准测试",
      "authors": [
        "Weihang Su",
        "Baoqing Yue",
        "Qingyao Ai",
        "Yiran Hu",
        "Jiaqi Li",
        "Changyue Wang",
        "Kaiyuan Zhang",
        "Yueyue Wu",
        "Yiqun Liu"
      ],
      "abstract": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
      "tldr_zh": "本文提出了JuDGE（判决书生成评估）基准，用于评估中文司法判决书自动生成系统的性能。该研究构建了一个包含真实案件事实描述与对应判决书的完整数据集，并引入法规条文和过往判例两个外部法律知识库来增强系统表现。通过与法律专家合作，研究者建立了多维度的自动化评估体系，测试了包括少量样本学习、微调和多源检索增强生成(RAG)等方法的表现。实验表明，虽然RAG方法能有效提升生成质量，但该任务仍存在显著改进空间。所有代码和数据集已开源供研究使用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14258v2",
      "published_date": "2025-03-18 13:48:18 UTC",
      "updated_date": "2025-03-20 15:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:05:44.865646"
    },
    {
      "arxiv_id": "2503.14254v1",
      "title": "CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration",
      "title_zh": "CTSAC：基于课程学习的Transformer软性行动者-批评者框架面向目标导向的机器人探索",
      "authors": [
        "Chunyu Yang",
        "Shengben Bi",
        "Yihui Xu",
        "Xin Zhang"
      ],
      "abstract": "With the increasing demand for efficient and flexible robotic exploration\nsolutions, Reinforcement Learning (RL) is becoming a promising approach in the\nfield of autonomous robotic exploration. However, current RL-based exploration\nalgorithms often face limited environmental reasoning capabilities, slow\nconvergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To\naddress these issues, we propose a Curriculum Learning-based Transformer\nReinforcement Learning Algorithm (CTSAC) aimed at improving both exploration\nefficiency and transfer performance. To enhance the robot's reasoning ability,\na Transformer is integrated into the perception network of the Soft\nActor-Critic (SAC) framework, leveraging historical information to improve the\nfarsightedness of the strategy. A periodic review-based curriculum learning is\nproposed, which enhances training efficiency while mitigating catastrophic\nforgetting during curriculum transitions. Training is conducted on the\nROS-Gazebo continuous robotic simulation platform, with LiDAR clustering\noptimization to further reduce the S2R gap. Experimental results demonstrate\nthe CTSAC algorithm outperforms the state-of-the-art non-learning and\nlearning-based algorithms in terms of success rate and success rate-weighted\nexploration time. Moreover, real-world experiments validate the strong S2R\ntransfer capabilities of CTSAC.",
      "tldr_zh": "该研究提出了CTSAC（基于课程的Transformer软演员-批评家算法），用于提升目标导向的机器人探索性能。该方法将Transformer集成到SAC强化学习框架中，利用历史信息增强策略的远见性，同时提出周期性复习的课程学习方法，有效提高了训练效率并缓解了课程转换时的灾难性遗忘问题。实验表明，CTSAC在成功率和探索效率上均优于现有最优的非学习型和基于学习的方法，并在真实环境中验证了其优异的Sim-to-Real迁移能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7pages,7 figures,Thesis received by 2025 ICRA",
      "pdf_url": "http://arxiv.org/pdf/2503.14254v1",
      "published_date": "2025-03-18 13:44:29 UTC",
      "updated_date": "2025-03-18 13:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:06:09.155848"
    },
    {
      "arxiv_id": "2503.14250v1",
      "title": "A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control",
      "title_zh": "面向现实世界自适应交通信号控制的并行混合动作空间强化学习模型",
      "authors": [
        "Yuxuan Wang",
        "Meng Long",
        "Qiang Wu",
        "Wei Liu",
        "Jiatian Pi",
        "Xinmin Yang"
      ],
      "abstract": "Adaptive traffic signal control (ATSC) can effectively reduce vehicle travel\ntimes by dynamically adjusting signal timings but poses a critical challenge in\nreal-world scenarios due to the complexity of real-time decision-making in\ndynamic and uncertain traffic conditions. The burgeoning field of intelligent\ntransportation systems, bolstered by artificial intelligence techniques and\nextensive data availability, offers new prospects for the implementation of\nATSC. In this study, we introduce a parallel hybrid action space reinforcement\nlearning model (PH-DDPG) that optimizes traffic signal phase and duration of\ntraffic signals simultaneously, eliminating the need for sequential\ndecision-making seen in traditional two-stage models. Our model features a\ntask-specific parallel hybrid action space tailored for adaptive traffic\ncontrol, which directly outputs discrete phase selections and their associated\ncontinuous duration parameters concurrently, thereby inherently addressing\ndynamic traffic adaptation through unified parametric optimization. %Our model\nfeatures a unique parallel hybrid action space that allows for the simultaneous\noutput of each action and its optimal parameters, streamlining the\ndecision-making process. Furthermore, to ascertain the robustness and\neffectiveness of this approach, we executed ablation studies focusing on the\nutilization of a random action parameter mask within the critic network, which\ndecouples the parameter space for individual actions, facilitating the use of\npreferable parameters for each action. The results from these studies confirm\nthe efficacy of this method, distinctly enhancing real-world applicability",
      "tldr_zh": "该研究提出了一种并行混合动作空间强化学习模型（PH-DDPG），用于优化实时自适应交通信号控制（ATSC）。该模型创新性地采用并行架构同时输出离散信号相位选择和连续时长参数，克服了传统两阶段模型的决策延迟问题。通过引入随机动作参数掩码机制，模型在critic网络中解耦各动作参数空间，显著提升了动态交通环境下的适应能力。实验验证表明，该方法有效提高了交通信号控制的实时性和准确性，为智能交通系统提供了新的解决方案。",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 9 figures, Reinforcement Learning",
      "pdf_url": "http://arxiv.org/pdf/2503.14250v1",
      "published_date": "2025-03-18 13:38:53 UTC",
      "updated_date": "2025-03-18 13:38:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:06:28.511091"
    },
    {
      "arxiv_id": "2503.14247v1",
      "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
      "title_zh": "GeoFlow-SLAM：面向动态腿式机器人的鲁棒紧耦合RGBD-惯性融合SLAM系统",
      "authors": [
        "Tingyang Xiao",
        "Xiaolin Zhou",
        "Liu Liu",
        "Wei Sui",
        "Wei Feng",
        "Jiaxiong Qiu",
        "Xinjie Wang",
        "Zhizhong Su"
      ],
      "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM",
      "tldr_zh": "本文提出GeoFlow-SLAM，一种面向动态足式机器人的鲁棒紧耦合RGBD-惯性SLAM系统。该方法创新性地融合几何一致性、足式里程计约束和双流光流(GeoFlow)技术，有效解决了快速运动中的特征匹配失败、姿态初始化问题以及纹理缺失场景的视觉特征稀缺三大挑战。通过结合IMU/足式里程计、帧间PnP和GICP算法，系统实现了快速运动下的鲁棒姿态初始化，并首次提出深度-地图与GICP几何约束紧耦合的优化框架，在长期纹理缺失环境中展现出优越性能。实验表明，该方法在足式机器人数据集上达到SOTA水平，相关代码和数据集已开源。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14247v1",
      "published_date": "2025-03-18 13:35:49 UTC",
      "updated_date": "2025-03-18 13:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:06:56.664185"
    },
    {
      "arxiv_id": "2503.14246v1",
      "title": "Trading-off Accuracy and Communication Cost in Federated Learning",
      "title_zh": "联邦学习中精度与通信成本的权衡",
      "authors": [
        "Mattia Jacopo Villani",
        "Emanuele Natale",
        "Frederik Mallmann-Trenn"
      ],
      "abstract": "Leveraging the training-by-pruning paradigm introduced by Zhou et al. and\nIsik et al. introduced a federated learning protocol that achieves a 34-fold\nreduction in communication cost. We achieve a compression improvements of\norders of orders of magnitude over the state-of-the-art. The central idea of\nour framework is to encode the network weights $\\vec w$ by a the vector of\ntrainable parameters $\\vec p$, such that $\\vec w = Q\\cdot \\vec p$ where $Q$ is\na carefully-generate sparse random matrix (that remains fixed throughout\ntraining). In such framework, the previous work of Zhou et al. [NeurIPS'19] is\nretrieved when $Q$ is diagonal and $\\vec p$ has the same dimension of $\\vec w$.\nWe instead show that $\\vec p$ can effectively be chosen much smaller than $\\vec\nw$, while retaining the same accuracy at the price of a decrease of the\nsparsity of $Q$. Since server and clients only need to share $\\vec p$, such a\ntrade-off leads to a substantial improvement in communication cost. Moreover,\nwe provide theoretical insight into our framework and establish a novel link\nbetween training-by-sampling and random convex geometry.",
      "tldr_zh": "该研究提出了一种新型联邦学习框架，通过参数编码技术实现通信成本与模型精度的权衡。核心创新在于使用稀疏随机矩阵Q将网络权重向量w编码为低维可训练参数p（w=Q·p），使通信只需传输p而保持模型精度。相比现有技术，该方法实现了34倍的通信成本降低和数量级压缩提升，同时建立了训练采样与随机凸几何之间的理论联系。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14246v1",
      "published_date": "2025-03-18 13:35:24 UTC",
      "updated_date": "2025-03-18 13:35:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:07:07.366570"
    },
    {
      "arxiv_id": "2503.16532v1",
      "title": "Modelling Emotions in Face-to-Face Setting: The Interplay of Eye-Tracking, Personality, and Temporal Dynamics",
      "title_zh": "面对面场景中的情感建模：眼动追踪、个性与时间动态的交互作用",
      "authors": [
        "Meisam Jamshidi Seikavandi",
        "Jostein Fimland",
        "Maria Barrett",
        "Paolo Burelli"
      ],
      "abstract": "Accurate emotion recognition is pivotal for nuanced and engaging\nhuman-computer interactions, yet remains difficult to achieve, especially in\ndynamic, conversation-like settings. In this study, we showcase how integrating\neye-tracking data, temporal dynamics, and personality traits can substantially\nenhance the detection of both perceived and felt emotions. Seventy-three\nparticipants viewed short, speech-containing videos from the CREMA-D dataset,\nwhile being recorded for eye-tracking signals (pupil size, fixation patterns),\nBig Five personality assessments, and self-reported emotional states. Our\nneural network models combined these diverse inputs including stimulus emotion\nlabels for contextual cues and yielded marked performance gains compared to the\nstate-of-the-art. Specifically, perceived valence predictions reached a macro\nF1-score of 0.76, and models incorporating personality traits and stimulus\ninformation demonstrated significant improvements in felt emotion accuracy.\nThese results highlight the benefit of unifying physiological, individual and\ncontextual factors to address the subjectivity and complexity of emotional\nexpression. Beyond validating the role of user-specific data in capturing\nsubtle internal states, our findings inform the design of future affective\ncomputing and human-agent systems, paving the way for more adaptive and\ncross-individual emotional intelligence in real-world interactions.",
      "tldr_zh": "该研究提出了一种结合眼动追踪、人格特质和时序动态的多模态情感识别方法，用于提升面对面交流场景中的情绪识别准确率。通过73名参与者在观看CREMA-D数据集视频时的眼动信号（瞳孔大小、注视模式）、大五人格测评和自我报告情绪数据，研究构建了神经网络模型，整合刺激情绪标签等上下文线索。实验结果表明，该方法在感知效价预测上达到0.76的宏观F1分数，融合人格特质和刺激信息的模型显著提高了真实情绪识别准确率，验证了生理特征、个体差异与情境因素相结合对解决情感表达主观复杂性的有效性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16532v1",
      "published_date": "2025-03-18 13:15:32 UTC",
      "updated_date": "2025-03-18 13:15:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:07:31.407182"
    },
    {
      "arxiv_id": "2503.14234v2",
      "title": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning",
      "title_zh": "KG-IRAG：基于知识图谱的时序推理迭代检索增强生成框架",
      "authors": [
        "Ruiyi Yang",
        "Hao Xue",
        "Imran Razzak",
        "Hakim Hacid",
        "Flora D. Salim"
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
      "tldr_zh": "该研究提出KG-IRAG框架，通过将知识图谱(KG)与迭代式检索增强生成(RAG)相结合，显著提升大语言模型(LLMs)在时序推理任务中的表现。该框架采用多步迭代检索机制，逐步从外部知识图谱中提取关联数据，特别适用于需要结合动态时序数据（如天气、交通）进行逻辑推理的场景。实验表明，KG-IRAG在三个新构建的数据集(weatherQA-Irish/Sydney和trafficQA-TFNSW)上表现出色，能有效处理传统RAG方法难以解决的时序依赖复杂推理问题。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14234v2",
      "published_date": "2025-03-18 13:11:43 UTC",
      "updated_date": "2025-03-19 04:49:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:07:55.592208"
    },
    {
      "arxiv_id": "2503.14232v1",
      "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models",
      "title_zh": "CRCE：文本到图像扩散模型中的指代保留概念擦除技术",
      "authors": [
        "Yuyang Xue",
        "Edward Moroshko",
        "Feng Chen",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris"
      ],
      "abstract": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.",
      "tldr_zh": "这篇论文提出了CRCE（Coreference-Retention Concept Erasure）框架，用于改进文本到图像扩散模型中的概念擦除技术。该方法利用大语言模型(LLMs)精确识别需要擦除的语义相关概念和需要保留的无关概念，解决了现有技术存在的欠擦除（残留目标概念）和过擦除（误删相似概念）问题。实验表明，CRCE能在保留无关概念的同时更精准地移除目标概念，在多种擦除任务中优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14232v1",
      "published_date": "2025-03-18 13:09:01 UTC",
      "updated_date": "2025-03-18 13:09:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:08:30.253169"
    },
    {
      "arxiv_id": "2503.14229v1",
      "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard",
      "title_zh": "HA-VLN：动态多人类交互环境下的人本导航基准测试——涵盖离散-连续空间、真实世界验证与开放排行榜",
      "authors": [
        "Yifei Dong",
        "Fengyi Wu",
        "Qi He",
        "Heng Li",
        "Minghan Li",
        "Zebang Cheng",
        "Yuxuan Zhou",
        "Jingdong Sun",
        "Qi Dai",
        "Zhi-Qi Cheng",
        "Alexander G Hauptmann"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.",
      "tldr_zh": "该研究提出了首个统一的人类感知视觉语言导航(HA-VLN)基准，整合了离散（全景）与连续（自由移动）导航范式，并引入显式的社会意识约束条件。主要贡献包括：1）开发了平衡离散-连续导航与个人空间需求的标准任务定义；2）升级了包含多人类交互、户外场景的人类运动数据集HAPS 2.0及仿真器；3）通过对16,844条人类中心指令的测试，揭示了多人类动态和部分可观测性对现有VLN智能体的挑战。实验证明，融入社交情境能显著提升导航成功率并减少碰撞，研究还通过真实机器人测试验证了仿真到现实的迁移效果。该基准通过公开数据集、仿真器和评估工具，旨在推动更安全、更具社会责任的导航研究发展。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, website: https://ha-vln-project.vercel.app/",
      "pdf_url": "http://arxiv.org/pdf/2503.14229v1",
      "published_date": "2025-03-18 13:05:55 UTC",
      "updated_date": "2025-03-18 13:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:08:52.312192"
    },
    {
      "arxiv_id": "2503.14228v1",
      "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images",
      "title_zh": "全景畸变感知分词：基于Transformer的鱼眼俯视图像中人员检测与定位方法",
      "authors": [
        "Nobuhiko Wakai",
        "Satoshi Sato",
        "Yasunori Ishii",
        "Takayoshi Yamashita"
      ],
      "abstract": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.",
      "tldr_zh": "该研究提出了一种全景畸变感知的token化方法，用于解决鱼眼镜头俯拍图像中的人员检测难题。通过将鱼眼图像转换为全景图像，并利用全景几何特性，该方法创新性地设计了基于自相似图形的自适应分块策略，有效平衡了大尺寸和小尺寸行人的特征显著性。实验表明，结合全景重映射和该token化策略的检测定位方法在大规模数据集上优于传统方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14228v1",
      "published_date": "2025-03-18 13:05:41 UTC",
      "updated_date": "2025-03-18 13:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:09:11.801115"
    },
    {
      "arxiv_id": "2503.14203v1",
      "title": "Stochastic Trajectory Prediction under Unstructured Constraints",
      "title_zh": "非结构化约束下的随机轨迹预测",
      "authors": [
        "Hao Ma",
        "Zhiqiang Pu",
        "Shijie Wang",
        "Boyin Liu",
        "Huimu Wang",
        "Yanyan Liang",
        "Jianqiang Yi"
      ],
      "abstract": "Trajectory prediction facilitates effective planning and decision-making,\nwhile constrained trajectory prediction integrates regulation into prediction.\nRecent advances in constrained trajectory prediction focus on structured\nconstraints by constructing optimization objectives. However, handling\nunstructured constraints is challenging due to the lack of differentiable\nformal definitions. To address this, we propose a novel method for constrained\ntrajectory prediction using a conditional generative paradigm, named\nControllable Trajectory Diffusion (CTD). The key idea is that any trajectory\ncorresponds to a degree of conformity to a constraint. By quantifying this\ndegree and treating it as a condition, a model can implicitly learn to predict\ntrajectories under unstructured constraints. CTD employs a pre-trained scoring\nmodel to predict the degree of conformity (i.e., a score), and uses this score\nas a condition for a conditional diffusion model to generate trajectories.\nExperimental results demonstrate that CTD achieves high accuracy on the ETH/UCY\nand SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to\nunstructured constraints and can predict trajectories that satisfy\ncombinatorial constraints.",
      "tldr_zh": "该研究提出了一种名为可控轨迹扩散(CTD)的新方法，用于处理无结构化约束条件下的随机轨迹预测问题。该方法创新性地采用条件生成范式，通过预训练评分模型量化轨迹与约束的符合程度，并将其作为条件输入扩散模型来生成轨迹。实验表明，CTD在ETH/UCY和SDD基准测试中表现出高精度，能够有效预测满足组合约束条件的轨迹，解决了传统方法难以处理无结构化约束的难题。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "has been accepted by ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14203v1",
      "published_date": "2025-03-18 12:27:59 UTC",
      "updated_date": "2025-03-18 12:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:09:18.058127"
    },
    {
      "arxiv_id": "2503.18955v1",
      "title": "Is there a future for AI without representation?",
      "title_zh": "无表征的AI是否存在未来？",
      "authors": [
        "Vincent C. Müller"
      ],
      "abstract": "This paper investigates the prospects of AI without representation in\ngeneral, and the proposals of Rodney Brooks in particular. What turns out to be\ncharacteristic of Brooks' proposal is the rejection of central control in\nintelligent agents; his systems has as much or as little representation as\ntraditional AI. The traditional view that representation is necessary for\nintelligence presupposes that intelligence requires central control. However,\nmuch of recent cognitive science suggests that we should dispose of the image\nof intelligent agents as central representation processors. If this paradigm\nshift is achieved, Brooks' proposal for non-centralized cognition without\nrepresentation appears promising for full-blown intelligent agents - though not\nfor conscious agents and thus not for human-like AI.",
      "tldr_zh": "这篇论文探讨了无需表征(representation)的AI发展前景，特别是Rodney Brooks提出的非集中控制智能体理论。研究指出，Brooks方案的核心在于摒弃智能体中的中央控制机制，其系统使用的表征与传统AI相当。传统观点认为智能必须依赖中央表征处理，但近期认知科学研究表明智能体无需作为中央处理器运作。若能实现这种范式转变，Brooks提出的无表征分布式认知模型在构建完全智能体方面前景广阔——尽管这不适用于具备意识的类人AI。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18955v1",
      "published_date": "2025-03-18 12:13:31 UTC",
      "updated_date": "2025-03-18 12:13:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:09:49.173141"
    },
    {
      "arxiv_id": "2503.14194v1",
      "title": "Driving behavior recognition via self-discovery learning",
      "title_zh": "基于自我发现学习的驾驶行为识别",
      "authors": [
        "Yilin Wang"
      ],
      "abstract": "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
      "tldr_zh": "该论文提出了一种基于自发现学习(self-discovery learning)的驾驶行为识别方法，旨在解决现有深度学习模型面临的样本长尾分布和相似行为混淆问题。针对自动驾驶系统中人类驾驶行为理解的关键需求，该方法通过处理数据集中的模糊样本，有效区分相似驾驶行为，解决了现有方法在语义信息提取不足的缺陷。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14194v1",
      "published_date": "2025-03-18 12:13:08 UTC",
      "updated_date": "2025-03-18 12:13:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:09:54.472104"
    },
    {
      "arxiv_id": "2503.14192v1",
      "title": "Strategic White Paper on AI Infrastructure for Particle, Nuclear, and Astroparticle Physics: Insights from JENA and EuCAIF",
      "title_zh": "粒子、核与天体粒子物理学AI基础设施战略白皮书：JENA与EuCAIF的洞见",
      "authors": [
        "Sascha Caron",
        "Andreas Ipp",
        "Gert Aarts",
        "Gábor Bíró",
        "Daniele Bonacorsi",
        "Elena Cuoco",
        "Caterina Doglioni",
        "Tommaso Dorigo",
        "Julián García Pardiñas",
        "Stefano Giagu",
        "Tobias Golling",
        "Lukas Heinrich",
        "Ik Siong Heng",
        "Paula Gina Isar",
        "Karolos Potamianos",
        "Liliana Teodorescu",
        "John Veitch",
        "Pietro Vischia",
        "Christoph Weniger"
      ],
      "abstract": "Artificial intelligence (AI) is transforming scientific research, with deep\nlearning methods playing a central role in data analysis, simulations, and\nsignal detection across particle, nuclear, and astroparticle physics. Within\nthe JENA communities-ECFA, NuPECC, and APPEC-and as part of the EuCAIF\ninitiative, AI integration is advancing steadily. However, broader adoption\nremains constrained by challenges such as limited computational resources, a\nlack of expertise, and difficulties in transitioning from research and\ndevelopment (R&D) to production. This white paper provides a strategic roadmap,\ninformed by a community survey, to address these barriers. It outlines critical\ninfrastructure requirements, prioritizes training initiatives, and proposes\nfunding strategies to scale AI capabilities across fundamental physics over the\nnext five years.",
      "tldr_zh": "这篇战略白皮书由JENA和EuCAIF联合发布，针对粒子物理、核物理和天体粒子物理领域的AI基础设施建设提出关键建议。报告指出，尽管深度学习已在该领域的数据分析、模拟和信号检测中发挥核心作用，但AI应用仍受限于计算资源不足、专业人才缺乏以及研发到生产的转化困难。基于社区调研，白皮书提出了未来五年的战略路线图，重点包括完善基础设施、优先培训计划以及制定资金策略，以推动基础物理研究中AI能力的规模化发展。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "cs.AI",
        "cs.LG",
        "hep-ex",
        "hep-ph",
        "nucl-th"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "19 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14192v1",
      "published_date": "2025-03-18 12:11:11 UTC",
      "updated_date": "2025-03-18 12:11:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:10:26.411044"
    },
    {
      "arxiv_id": "2503.14190v1",
      "title": "Inferring Event Descriptions from Time Series with Language Models",
      "title_zh": "从时间序列中推断事件描述的语言模型方法",
      "authors": [
        "Mingtian Tan",
        "Mike A. Merrill",
        "Zack Gottesman",
        "Tim Althoff",
        "David Evans",
        "Tom Hartvigsen"
      ],
      "abstract": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
      "tldr_zh": "该研究首次探讨了大型语言模型(LLMs)从时间序列数据中推断自然语言事件描述的能力。研究者构建了一个包含4200场篮球和美式足球比赛胜率时间序列的新基准数据集，包含170万个时间步长及其对应的自然语言事件描述。通过评估16种LLMs，研究发现它们在从时间序列推断事件方面表现出潜力，其中开源的DeepSeek-R1 32B模型表现优于GPT-4o等专有模型。尽管初步结果令人印象深刻，但研究也指出了模型在上下文变化、事件序列长度和评估策略等方面的不足，为未来改进提供了方向。",
      "categories": [
        "cs.AI",
        "62M10, 68T07,",
        "I.2.6; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14190v1",
      "published_date": "2025-03-18 12:07:33 UTC",
      "updated_date": "2025-03-18 12:07:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:10:36.935606"
    },
    {
      "arxiv_id": "2503.14184v1",
      "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic Targets",
      "title_zh": "可变时间步长模型预测控制实现多旋翼无人机对动态目标的敏捷拦截",
      "authors": [
        "Atharva Ghotavadekar",
        "František Nekovář",
        "Martin Saska",
        "Jan Faigl"
      ],
      "abstract": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
      "tldr_zh": "该研究提出了一种可变时间步长的模型预测控制(MPC)方法，用于提升多旋翼无人机(UAV)拦截动态目标的敏捷性。该方法通过耦合可变时间步长与预测视界长度，克服了传统非线性MPC计算量大、预测视界短的局限，同时充分利用了无人机在目标附近的动态性能。基于简化点质量运动基元，利用四旋翼的微分平坦特性在平坦输出空间生成可行轨迹。实验验证表明，该方法既能规划长距离飞行段，又能实现紧密采样的机动动作，显著提升了轨迹规划质量。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14184v1",
      "published_date": "2025-03-18 11:59:24 UTC",
      "updated_date": "2025-03-18 11:59:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:10:58.391190"
    },
    {
      "arxiv_id": "2503.14183v1",
      "title": "Can LLMs Enable Verification in Mainstream Programming?",
      "title_zh": "大型语言模型能否实现主流编程中的验证？",
      "authors": [
        "Aleksandr Shefer",
        "Igor Engel",
        "Stanislav Alekseev",
        "Daniil Berezun",
        "Ekaterina Verbitskaia",
        "Anton Podkopaev"
      ],
      "abstract": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在主流编程中实现代码验证的潜力。研究通过手动构建基于HumanEval Python基准的数据集，评估了LLMs在三种验证语言(Dafny、Nagini和Verus)中生成可验证代码的能力。结果表明，LLMs能够产生具备形式化验证保证的代码，同时揭示了影响验证质量的关键信息类型，为结合形式化方法和AI编程提供了新思路。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14183v1",
      "published_date": "2025-03-18 11:58:00 UTC",
      "updated_date": "2025-03-18 11:58:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:11:21.907008"
    },
    {
      "arxiv_id": "2503.14162v1",
      "title": "EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models",
      "title_zh": "EIAD：基于多模态大语言模型的可解释工业异常检测",
      "authors": [
        "Zongyun Zhang",
        "Jiacheng Ruan",
        "Xian Gao",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Industrial Anomaly Detection (IAD) is critical to ensure product quality\nduring manufacturing. Although existing zero-shot defect segmentation and\ndetection methods have shown effectiveness, they cannot provide detailed\ndescriptions of the defects. Furthermore, the application of large multi-modal\nmodels in IAD remains in its infancy, facing challenges in balancing\nquestion-answering (QA) performance and mask-based grounding capabilities,\noften owing to overfitting during the fine-tuning process. To address these\nchallenges, we propose a novel approach that introduces a dedicated multi-modal\ndefect localization module to decouple the dialog functionality from the core\nfeature extraction. This decoupling is achieved through independent\noptimization objectives and tailored learning strategies. Additionally, we\ncontribute to the first multi-modal industrial anomaly detection training\ndataset, named Defect Detection Question Answering (DDQA), encompassing a wide\nrange of defect types and industrial scenarios. Unlike conventional datasets\nthat rely on GPT-generated data, DDQA ensures authenticity and reliability and\noffers a robust foundation for model training. Experimental results demonstrate\nthat our proposed method, Explainable Industrial Anomaly Detection Assistant\n(EIAD), achieves outstanding performance in defect detection and localization\ntasks. It not only significantly enhances accuracy but also improves\ninterpretability. These advancements highlight the potential of EIAD for\npractical applications in industrial settings.",
      "tldr_zh": "该研究提出EIAD框架，通过多模态大语言模型(LLMs)实现可解释的工业异常检测。创新性地设计了多模态缺陷定位模块，将对话功能与核心特征提取解耦，并采用独立优化目标和定制学习策略来避免微调过拟合问题。团队构建了首个真实可靠的多模态工业异常检测数据集DDQA，涵盖多种缺陷类型和工业场景。实验表明，EIAD在缺陷检测和定位任务中表现优异，不仅显著提升准确率，还增强了模型可解释性，为工业质检提供了实用解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14162v1",
      "published_date": "2025-03-18 11:33:29 UTC",
      "updated_date": "2025-03-18 11:33:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:11:46.726743"
    },
    {
      "arxiv_id": "2503.14569v1",
      "title": "Potential Score Matching: Debiasing Molecular Structure Sampling with Potential Energy Guidance",
      "title_zh": "势能评分匹配：利用势能引导实现分子结构采样的去偏校正",
      "authors": [
        "Liya Guo",
        "Zun Wang",
        "Chang Liu",
        "Junzhe Li",
        "Pipi Hu",
        "Yi Zhu"
      ],
      "abstract": "The ensemble average of physical properties of molecules is closely related\nto the distribution of molecular conformations, and sampling such distributions\nis a fundamental challenge in physics and chemistry. Traditional methods like\nmolecular dynamics (MD) simulations and Markov chain Monte Carlo (MCMC)\nsampling are commonly used but can be time-consuming and costly. Recently,\ndiffusion models have emerged as efficient alternatives by learning the\ndistribution of training data. Obtaining an unbiased target distribution is\nstill an expensive task, primarily because it requires satisfying ergodicity.\nTo tackle these challenges, we propose Potential Score Matching (PSM), an\napproach that utilizes the potential energy gradient to guide generative\nmodels. PSM does not require exact energy functions and can debias sample\ndistributions even when trained on limited and biased data. Our method\noutperforms existing state-of-the-art (SOTA) models on the Lennard-Jones (LJ)\npotential, a commonly used toy model. Furthermore, we extend the evaluation of\nPSM to high-dimensional problems using the MD17 and MD22 datasets. The results\ndemonstrate that molecular distributions generated by PSM more closely\napproximate the Boltzmann distribution compared to traditional diffusion\nmodels.",
      "tldr_zh": "该研究提出了一种名为\"势能分数匹配\"(Potential Score Matching, PSM)的新方法，用于解决分子构象采样中的偏差问题。该方法利用势能梯度指导生成模型，无需精确的能量函数，即使在有限和有偏差的训练数据下也能有效去偏采样分布。实验表明，PSM在Lennard-Jones势能模型上优于现有最优模型，并在MD17和MD22数据集的高维问题上验证了其有效性，生成的分子分布更接近玻尔兹曼分布。这一方法为分子动力学模拟和蒙特卡洛采样提供了更高效的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14569v1",
      "published_date": "2025-03-18 11:27:28 UTC",
      "updated_date": "2025-03-18 11:27:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:12:34.829687"
    },
    {
      "arxiv_id": "2503.14568v1",
      "title": "Teaching Artificial Intelligence to Perform Rapid, Resolution-Invariant Grain Growth Modeling via Fourier Neural Operator",
      "title_zh": "通过傅里叶神经算子教授人工智能进行快速、分辨率不变的晶粒生长建模",
      "authors": [
        "Iman Peivaste",
        "Ahmed Makradi",
        "Salim Belouettar"
      ],
      "abstract": "Microstructural evolution, particularly grain growth, plays a critical role\nin shaping the physical, optical, and electronic properties of materials.\nTraditional phase-field modeling accurately simulates these phenomena but is\ncomputationally intensive, especially for large systems and fine spatial\nresolutions. While machine learning approaches have been employed to accelerate\nsimulations, they often struggle with resolution dependence and generalization\nacross different grain scales. This study introduces a novel approach utilizing\nFourier Neural Operator (FNO) to achieve resolution-invariant modeling of\nmicrostructure evolution in multi-grain systems. FNO operates in the Fourier\nspace and can inherently handle varying resolutions by learning mappings\nbetween function spaces. By integrating FNO with the phase field method, we\ndeveloped a surrogate model that significantly reduces computational costs\nwhile maintaining high accuracy across different spatial scales. We generated a\ncomprehensive dataset from phase-field simulations using the Fan Chen model,\ncapturing grain evolution over time. Data preparation involved creating\ninput-output pairs with a time shift, allowing the model to predict future\nmicrostructures based on current and past states. The FNO-based neural network\nwas trained using sequences of microstructures and demonstrated remarkable\naccuracy in predicting long-term evolution, even for unseen configurations and\nhigher-resolution grids not encountered during training.",
      "tldr_zh": "本研究提出利用傅里叶神经算子(FNO)实现多晶粒系统微观结构演化的分辨率无关建模。该方法通过在傅里叶空间学习函数空间映射，解决了传统机器学习方法难以处理分辨率依赖性和不同晶粒尺度泛化的问题。研究者将FNO与相场法相结合构建的替代模型，在保持高精度的同时显著降低了计算成本，并能准确预测训练中未见过的高分辨率网格配置的长期演化行为。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14568v1",
      "published_date": "2025-03-18 11:19:08 UTC",
      "updated_date": "2025-03-18 11:19:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:12:26.268920"
    },
    {
      "arxiv_id": "2503.14151v1",
      "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
      "title_zh": "Concat-ID：迈向通用身份保持的视频合成",
      "authors": [
        "Yong Zhong",
        "Zhuoyi Yang",
        "Jiayan Teng",
        "Xiaotao Gu",
        "Chongxuan Li"
      ],
      "abstract": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
      "tldr_zh": "该研究提出Concat-ID框架，用于实现身份保持的通用视频生成。该方法通过变分自编码器提取图像特征，并沿序列维度与视频潜在变量拼接，仅使用3D自注意力机制即可实现身份保留。创新性地采用跨视频配对策略和多阶段训练方案，在保持身份一致性和面部可编辑性的同时提升视频自然度。实验表明，Concat-ID在单/多身份生成及虚拟试穿等场景中均优于现有方法，为身份保持视频合成设立了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14151v1",
      "published_date": "2025-03-18 11:17:32 UTC",
      "updated_date": "2025-03-18 11:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:13:10.220121"
    },
    {
      "arxiv_id": "2503.14138v1",
      "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
      "title_zh": "探索人脸识别系统中的准确性与公平性权衡：数据集、架构与损失函数的作用",
      "authors": [
        "Siddharth D Jaiswal",
        "Sagnik Basu",
        "Sandipan Sikdar",
        "Animesh Mukherjee"
      ],
      "abstract": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
      "tldr_zh": "该研究系统分析了人脸识别系统(FRS)中数据集、模型架构和损失函数对精度-差异权衡的影响。通过测试10种深度学习模型结合4种损失函数在7个数据集上的266种配置，发现三个要素不仅各自独立影响系统表现，还产生组合效应。研究表明数据集具有固有特性会导致不同模型表现相似，且同一模型在不同性别平衡数据集上可能呈现相反的偏见方向。作者指出由于数据集多样性，模型难以形成统一的\"男性/女性面孔\"定义，并为开发者提供了减少系统偏见的实用建议。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted for publication at AAAI ICWSM 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14138v1",
      "published_date": "2025-03-18 11:04:57 UTC",
      "updated_date": "2025-03-18 11:04:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:13:05.603171"
    },
    {
      "arxiv_id": "2503.14136v1",
      "title": "CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal Hardware",
      "title_zh": "CARE：基于QLoRA微调的多领域聊天机器人——在最小化硬件条件下的快速学习系统",
      "authors": [
        "Ankit Dutta",
        "Nabarup Ghosh",
        "Ankush Chatterjee"
      ],
      "abstract": "Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.",
      "tldr_zh": "该研究提出CARE（客户辅助响应引擎），一种基于QLoRA微调技术开发的多领域轻量级聊天机器人。该系统在有限硬件条件下对Phi3.5-mini模型进行高效微调，可快速部署于移动设备，专门处理电信、银行和医疗三大领域的查询任务。实验表明，该模型在各类医疗基准测试中表现良好，能提供基本的医疗建议和诊断支持，同时在电信和银行领域能有效解决常见客户问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14136v1",
      "published_date": "2025-03-18 10:58:10 UTC",
      "updated_date": "2025-03-18 10:58:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:13:23.125152"
    },
    {
      "arxiv_id": "2503.14130v1",
      "title": "Inference-Time Intervention in Large Language Models for Reliable Requirement Verification",
      "title_zh": "大型语言模型推理时干预实现可靠需求验证",
      "authors": [
        "Paul Darm",
        "James Xie",
        "Annalisa Riccardi"
      ],
      "abstract": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
      "tldr_zh": "该研究提出了一种针对大型语言模型(LLMs)的推理时干预(Inference-Time Intervention)方法，用于提升基于模型的系统工程(MBSE)中需求验证的可靠性。该方法通过识别和调整1-3个专用注意力头(attention heads)，结合自一致性(self-consistency)技术，在空间任务Capella SysML模型的验证任务中实现了完美精度，显著优于基线模型和微调方法。这种动态干预机制为工程应用提供了细粒度控制，解决了传统方法在精确性和可靠性方面的不足。",
      "categories": [
        "cs.AI",
        "cs.SE",
        "H.4.2; I.2.1; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14130v1",
      "published_date": "2025-03-18 10:49:36 UTC",
      "updated_date": "2025-03-18 10:49:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:13:41.365058"
    },
    {
      "arxiv_id": "2503.14567v1",
      "title": "SpecReX: Explainable AI for Raman Spectroscopy",
      "title_zh": "SpecReX：面向拉曼光谱的可解释人工智能",
      "authors": [
        "Nathan Blake",
        "David A. Kelly",
        "Akchunya Chanchal",
        "Sarah Kapllani-Mucaj",
        "Geraint Thomas",
        "Hana Chockler"
      ],
      "abstract": "Raman spectroscopy is becoming more common for medical diagnostics with deep\nlearning models being increasingly used to leverage its full potential.\nHowever, the opaque nature of such models and the sensitivity of medical\ndiagnosis together with regulatory requirements necessitate the need for\nexplainable AI tools. We introduce SpecReX, specifically adapted to explaining\nRaman spectra. SpecReX uses the theory of actual causality to rank causal\nresponsibility in a spectrum, quantified by iteratively refining mutated\nversions of the spectrum and testing if it retains the original classification.\nThe explanations provided by SpecReX take the form of a responsibility map,\nhighlighting spectral regions most responsible for the model to make a correct\nclassification. To assess the validity of SpecReX, we create increasingly\ncomplex simulated spectra, in which a \"ground truth\" signal is seeded, to train\na classifier. We then obtain SpecReX explanations and compare the results with\nanother explainability tool. By using simulated spectra we establish that\nSpecReX localizes to the known differences between classes, under a number of\nconditions. This provides a foundation on which we can find the spectral\nfeatures which differentiate disease classes. This is an important first step\nin proving the validity of SpecReX.",
      "tldr_zh": "该研究提出SpecReX，一种面向拉曼光谱(Raman spectroscopy)的可解释AI工具，旨在解决深度学习模型在医疗诊断中的黑箱问题。该方法基于实际因果理论，通过迭代生成并测试光谱变异版本，量化不同光谱区域对分类结果的因果责任，生成责任图谱(responsibility map)直观显示关键诊断特征。研究采用植入已知信号的模拟光谱验证，证明SpecReX能准确定位不同疾病类别的特征差异区域，为医疗决策提供了可验证的解释依据。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI Workshop on Health Intelligencee (W3PHIAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2503.14567v1",
      "published_date": "2025-03-18 10:49:15 UTC",
      "updated_date": "2025-03-18 10:49:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:14:06.103368"
    },
    {
      "arxiv_id": "2503.14125v1",
      "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
      "title_zh": "Frac-Connections：超连接的分数阶扩展",
      "authors": [
        "Defa Zhu",
        "Hongzhi Huang",
        "Jundong Zhou",
        "Zihao Huang",
        "Yutao Zeng",
        "Banggu Wu",
        "Qiyang Min",
        "Xun Zhou"
      ],
      "abstract": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
      "tldr_zh": "该研究提出了Frac-Connections方法，作为Hyper-Connections的分数化扩展，通过将隐藏状态分割而非扩展宽度来优化深度神经网络训练。相比传统残差连接(Residual Connections)，新方法在缓解梯度消失与表征崩溃的同时，显著降低了Hyper-Connections带来的内存开销。大规模语言任务实验验证了其有效性，特别是在7B参数的混合专家模型(MoE)上训练3万亿token时，表现显著优于传统残差连接。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14125v1",
      "published_date": "2025-03-18 10:37:50 UTC",
      "updated_date": "2025-03-18 10:37:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:14:22.677420"
    },
    {
      "arxiv_id": "2503.14109v1",
      "title": "Operational Change Detection for Geographical Information: Overview and Challenges",
      "title_zh": "地理信息操作化变化检测：概述与挑战",
      "authors": [
        "Nicolas Gonthier"
      ],
      "abstract": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.",
      "tldr_zh": "该论文系统综述了地理信息系统中用于国家级测绘机构数据库更新的变化检测方法。研究将自动变化检测技术分为四大类：基于规则、统计、机器学习和模拟方法，并分析了各类方法在不同输入数据下的优缺点。文章重点探讨了变化检测在优化地理数据库更新、监测动态现象等关键应用中的价值，同时指出当前面临的六大挑战，包括变化定义的多样性、大规模数据集缺失、输入数据异质性等问题。研究强调需要持续创新变化检测技术以满足未来国家级地理信息系统的需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint under review",
      "pdf_url": "http://arxiv.org/pdf/2503.14109v1",
      "published_date": "2025-03-18 10:25:28 UTC",
      "updated_date": "2025-03-18 10:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:14:46.433470"
    },
    {
      "arxiv_id": "2503.14106v1",
      "title": "Reliable uncertainty quantification for 2D/3D anatomical landmark localization using multi-output conformal prediction",
      "title_zh": "基于多输出保形预测的二维/三维解剖标志定位可靠不确定性量化",
      "authors": [
        "Jef Jonkers",
        "Frank Coopman",
        "Luc Duchateau",
        "Glenn Van Wallendael",
        "Sofie Van Hoecke"
      ],
      "abstract": "Automatic anatomical landmark localization in medical imaging requires not\njust accurate predictions but reliable uncertainty quantification for effective\nclinical decision support. Current uncertainty quantification approaches often\nfall short, particularly when combined with normality assumptions,\nsystematically underestimating total predictive uncertainty. This paper\nintroduces conformal prediction as a framework for reliable uncertainty\nquantification in anatomical landmark localization, addressing a critical gap\nin automatic landmark localization. We present two novel approaches\nguaranteeing finite-sample validity for multi-output prediction: Multi-output\nRegression-as-Classification Conformal Prediction (M-R2CCP) and its variant\nMulti-output Regression to Classification Conformal Prediction set to Region\n(M-R2C2R). Unlike conventional methods that produce axis-aligned\nhyperrectangular or ellipsoidal regions, our approaches generate flexible,\nnon-convex prediction regions that better capture the underlying uncertainty\nstructure of landmark predictions. Through extensive empirical evaluation\nacross multiple 2D and 3D datasets, we demonstrate that our methods\nconsistently outperform existing multi-output conformal prediction approaches\nin both validity and efficiency. This work represents a significant advancement\nin reliable uncertainty estimation for anatomical landmark localization,\nproviding clinicians with trustworthy confidence measures for their diagnoses.\nWhile developed for medical imaging, these methods show promise for broader\napplications in multi-output regression problems.",
      "tldr_zh": "该研究提出了两种基于保形预测(Conformal Prediction)的多输出不确定性量化方法——M-R2CCP和M-R2C2R，用于2D/3D解剖标志点定位。与传统的轴对齐超矩形或椭球区域不同，这些方法能够生成灵活的非凸预测区域，更准确地捕捉标志点预测的不确定性结构。通过在多个2D和3D数据集上的广泛实验验证，这两种方法在有效性和效率方面均优于现有的多输出保形预测方法，为临床诊断提供了更可靠的不确定性估计。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "33 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14106v1",
      "published_date": "2025-03-18 10:21:32 UTC",
      "updated_date": "2025-03-18 10:21:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:15:06.004839"
    },
    {
      "arxiv_id": "2503.14102v1",
      "title": "Sensory-driven microinterventions for improved health and wellbeing",
      "title_zh": "感官驱动微干预：提升健康与福祉的新途径",
      "authors": [
        "Youssef Abdalla",
        "Elia Gatti",
        "Mine Orlu",
        "Marianna Obrist"
      ],
      "abstract": "The five senses are gateways to our wellbeing and their decline is considered\na significant public health challenge which is linked to multiple conditions\nthat contribute significantly to morbidity and mortality. Modern technology,\nwith its ubiquitous nature and fast data processing has the ability to leverage\nthe power of the senses to transform our approach to day to day healthcare,\nwith positive effects on our quality of life. Here, we introduce the idea of\nsensory-driven microinterventions for preventative, personalised healthcare.\nMicrointerventions are targeted, timely, minimally invasive strategies that\nseamlessly integrate into our daily life. This idea harnesses human's sensory\ncapabilities, leverages technological advances in sensory stimulation and\nreal-time processing ability for sensing the senses. The collection of sensory\ndata from our continuous interaction with technology - for example the tone of\nvoice, gait movement, smart home behaviour - opens up a shift towards\npersonalised technology-enabled, sensory-focused healthcare interventions,\ncoupled with the potential of early detection and timely treatment of sensory\ndeficits that can signal critical health insights, especially for\nneurodegenerative diseases such as Parkinson's disease.",
      "tldr_zh": "该研究提出了\"感官驱动的微干预\"新理念，通过日常技术设备（如语音语调、步态监测、智能家居行为等）持续收集感官数据，实现个性化预防性健康管理。这种微干预策略具有针对性、及时性和微创性特点，利用先进的感官刺激技术和实时处理能力，可早期发现感官衰退信号（如帕金森病等神经退行性疾病前兆）。该方案将日常技术互动转化为健康干预机会，标志着向技术赋能、感官导向的医疗模式转变。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14102v1",
      "published_date": "2025-03-18 10:17:55 UTC",
      "updated_date": "2025-03-18 10:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:16:32.224015"
    },
    {
      "arxiv_id": "2503.14088v1",
      "title": "Toward Large-Scale Distributed Quantum Long Short-Term Memory with Modular Quantum Computers",
      "title_zh": "迈向基于模块化量子计算机的大规模分布式量子长短期记忆网络",
      "authors": [
        "Kuan-Cheng Chen",
        "Samuel Yen-Chi Chen",
        "Chen-Yu Liu",
        "Kin K. Leung"
      ],
      "abstract": "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
      "tldr_zh": "该研究提出了一种分布式量子长短期记忆网络(QLSTM)框架，利用模块化量子计算解决NISQ(含噪声中等规模量子)设备的可扩展性问题。通过将变分量子电路(VQCs)嵌入LSTM单元并采用分布式架构，该系统能处理长时间依赖关系，同时在量子处理单元网络上并行执行分解后的子电路。实验表明，在阻尼谐振子和非线性自回归移动平均序列等任务上，该量子LSTM比经典方法具有更稳定的收敛性和更好的训练动态，为未来大规模量子高性能计算(HPC)中的混合量子-经典解决方案奠定了基础。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14088v1",
      "published_date": "2025-03-18 10:07:34 UTC",
      "updated_date": "2025-03-18 10:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:15:52.534821"
    },
    {
      "arxiv_id": "2503.17395v1",
      "title": "CP-NCBF: A Conformal Prediction-based Approach to Synthesize Verified Neural Control Barrier Functions",
      "title_zh": "CP-NCBF：一种基于共形预测的验证型神经控制屏障函数合成方法",
      "authors": [
        "Manan Tayal",
        "Aditya Singh",
        "Pushpak Jagtap",
        "Shishir Kolathaya"
      ],
      "abstract": "Control Barrier Functions (CBFs) are a practical approach for designing\nsafety-critical controllers, but constructing them for arbitrary nonlinear\ndynamical systems remains a challenge. Recent efforts have explored\nlearning-based methods, such as neural CBFs (NCBFs), to address this issue.\nHowever, ensuring the validity of NCBFs is difficult due to potential learning\nerrors. In this letter, we propose a novel framework that leverages\nsplit-conformal prediction to generate formally verified neural CBFs with\nprobabilistic guarantees based on a user-defined error rate, referred to as\nCP-NCBF. Unlike existing methods that impose Lipschitz constraints on neural\nCBF-leading to scalability limitations and overly conservative safe sets--our\napproach is sample-efficient, scalable, and results in less restrictive safety\nregions. We validate our framework through case studies on obstacle avoidance\nin autonomous driving and geo-fencing of aerial vehicles, demonstrating its\nability to generate larger and less conservative safe sets compared to\nconventional techniques.",
      "tldr_zh": "该研究提出CP-NCBF框架，通过整合保形预测(Conformal Prediction)和神经控制屏障函数(NCBFs)，为非线性动力系统提供具有概率保证的安全控制方案。相比传统强加Lipschitz约束的方法，该样本高效且可扩展的方案能生成更宽松的安全区域。在自动驾驶避障和无人机地理围栏案例中验证显示，该方法产生的安全集比常规技术更大且保守性更低。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "6 Pages, 4 Figures. First two authors have contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2503.17395v1",
      "published_date": "2025-03-18 10:01:06 UTC",
      "updated_date": "2025-03-18 10:01:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:16:49.930559"
    },
    {
      "arxiv_id": "2503.14076v1",
      "title": "Theoretical Foundation of Flow-Based Time Series Generation: Provable Approximation, Generalization, and Efficiency",
      "title_zh": "流式时间序列生成的理论基础：可证明的近似性、泛化性与效率",
      "authors": [
        "Jiangxuan Long",
        "Zhao Song",
        "Chiwun Yang"
      ],
      "abstract": "Recent studies suggest utilizing generative models instead of traditional\nauto-regressive algorithms for time series forecasting (TSF) tasks. These\nnon-auto-regressive approaches involving different generative methods,\nincluding GAN, Diffusion, and Flow Matching for time series, have empirically\ndemonstrated high-quality generation capability and accuracy. However, we still\nlack an appropriate understanding of how it processes approximation and\ngeneralization. This paper presents the first theoretical framework from the\nperspective of flow-based generative models to relieve the knowledge of\nlimitations. In particular, we provide our insights with strict guarantees from\nthree perspectives: $\\textbf{Approximation}$, $\\textbf{Generalization}$ and\n$\\textbf{Efficiency}$. In detail, our analysis achieves the contributions as\nfollows:\n  $\\bullet$ By assuming a general data model, the fitting of the flow-based\ngenerative models is confirmed to converge to arbitrary error under the\nuniversal approximation of Diffusion Transformer (DiT).\n  $\\bullet$ Introducing a polynomial-based regularization for flow matching,\nthe generalization error thus be bounded since the generalization of polynomial\napproximation.\n  $\\bullet$ The sampling for generation is considered as an optimization\nprocess, we demonstrate its fast convergence with updating standard first-order\ngradient descent of some objective.",
      "tldr_zh": "该论文首次建立了基于流模型(Flow-based)的时间序列生成理论框架，从**近似性**、**泛化性**和**效率性**三个维度提供严格的理论保证。研究表明：(1)在通用DiT架构下，流模型能收敛到任意误差精度；(2)通过多项式正则化方法，可有效约束流匹配的泛化误差；(3)将生成采样视为优化过程，证明其一阶梯度下降具有快速收敛性。该工作填补了生成式时间序列预测的理论空白。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14076v1",
      "published_date": "2025-03-18 09:53:48 UTC",
      "updated_date": "2025-03-18 09:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:16:29.835170"
    },
    {
      "arxiv_id": "2503.14070v1",
      "title": "Fast Autoregressive Video Generation with Diagonal Decoding",
      "title_zh": "快速自回归视频生成的对角线解码方法",
      "authors": [
        "Yang Ye",
        "Junliang Guo",
        "Haoyu Wu",
        "Tianyu He",
        "Tim Pearce",
        "Tabish Rashid",
        "Katja Hofmann",
        "Jiang Bian"
      ],
      "abstract": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.",
      "tldr_zh": "该论文提出了一种名为Diagonal Decoding（DiagD）的新型自回归视频生成加速方法。通过利用视频时空相关性，该方法沿时空token网格的对角线路径并行生成token，实现了单帧内和跨帧间的部分重叠解码，无需重新训练即可在预训练模型上使用。实验表明，DiagD相比传统顺序解码可获得高达10倍的加速，同时保持相当的视觉质量。此外，作者还提出了一种经济高效的微调策略，使小规模模型能更好地适应这种解码顺序。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14070v1",
      "published_date": "2025-03-18 09:42:55 UTC",
      "updated_date": "2025-03-18 09:42:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:16:58.421588"
    },
    {
      "arxiv_id": "2503.16530v1",
      "title": "Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine",
      "title_zh": "基于知识超图增强大语言模型生成能力以支持循证医学",
      "authors": [
        "Chengfeng Dou",
        "Ying Zhang",
        "Zhi Jin",
        "Wenpin Jiao",
        "Haiyan Zhao",
        "Yongqiang Zhao",
        "Zhengwei Tao"
      ],
      "abstract": "Evidence-based medicine (EBM) plays a crucial role in the application of\nlarge language models (LLMs) in healthcare, as it provides reliable support for\nmedical decision-making processes. Although it benefits from current\nretrieval-augmented generation~(RAG) technologies, it still faces two\nsignificant challenges: the collection of dispersed evidence and the efficient\norganization of this evidence to support the complex queries necessary for EBM.\nTo tackle these issues, we propose using LLMs to gather scattered evidence from\nmultiple sources and present a knowledge hypergraph-based evidence management\nmodel to integrate these evidence while capturing intricate relationships.\nFurthermore, to better support complex queries, we have developed an\nImportance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the\nLLM to generate multiple evidence features, each with an associated importance\nscore, which are then used to rank the evidence and produce the final retrieval\nresults. Experimental results from six datasets demonstrate that our approach\noutperforms existing RAG techniques in application domains of interest to EBM,\nsuch as medical quizzing, hallucination detection, and decision support.\nTestsets and the constructed knowledge graph can be accessed at\n\\href{https://drive.google.com/file/d/1WJ9QTokK3MdkjEmwuFQxwH96j_Byawj_/view?usp=drive_link}{https://drive.google.com/rag4ebm}.",
      "tldr_zh": "该研究提出了一种基于知识超图(knowledge hypergraph)的增强生成方法，用于提升大语言模型(LLMs)在循证医学(EBM)中的应用。针对现有检索增强生成(RAG)技术在证据收集和组织上的不足，研究利用LLMs从多源收集分散证据，并通过知识超图整合证据及其复杂关系。此外，研究设计了重要性驱动的证据优先级算法(IDEP)，利用LLMs生成多特征证据并赋予重要性评分，以支持复杂查询。实验表明，该方法在医学问答、幻觉检测和决策支持等EBM任务上优于现有RAG技术。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16530v1",
      "published_date": "2025-03-18 09:17:31 UTC",
      "updated_date": "2025-03-18 09:17:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:17:09.746224"
    },
    {
      "arxiv_id": "2503.14053v1",
      "title": "ON-Traffic: An Operator Learning Framework for Online Traffic Flow Estimation and Uncertainty Quantification from Lagrangian Sensors",
      "title_zh": "ON-Traffic：基于拉格朗日传感器的在线交通流估计与不确定性量化算子学习框架",
      "authors": [
        "Jake Rap",
        "Amritam Das"
      ],
      "abstract": "Accurate traffic flow estimation and prediction are critical for the\nefficient management of transportation systems, particularly under increasing\nurbanization. Traditional methods relying on static sensors often suffer from\nlimited spatial coverage, while probe vehicles provide richer, albeit sparse\nand irregular data. This work introduces ON-Traffic, a novel deep operator\nNetwork and a receding horizon learning-based framework tailored for online\nestimation of spatio-temporal traffic state along with quantified uncertainty\nby using measurements from moving probe vehicles and downstream boundary\ninputs. Our framework is evaluated in both numerical and simulation datasets,\nshowcasing its ability to handle irregular, sparse input data, adapt to\ntime-shifted scenarios, and provide well-calibrated uncertainty estimates. The\nresults demonstrate that the model captures complex traffic phenomena,\nincluding shockwaves and congestion propagation, while maintaining robustness\nto noise and sensor dropout. These advancements present a significant step\ntoward online, adaptive traffic management systems.",
      "tldr_zh": "本研究提出ON-Traffic框架，采用深度算子网络(deep operator Network)和移动视界学习方法，通过Lagrangian移动传感器数据实现交通流的在线估计与不确定性量化。该模型能有效处理稀疏、不规则输入数据，准确捕捉交通激波和拥堵传播等复杂现象，并在噪声和传感器信号丢失情况下保持鲁棒性。实验证明，该框架可为在线自适应交通管理系统提供可靠支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14053v1",
      "published_date": "2025-03-18 09:13:24 UTC",
      "updated_date": "2025-03-18 09:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:17:37.500915"
    },
    {
      "arxiv_id": "2503.14048v1",
      "title": "Beyond holography: the entropic quantum gravity foundations of image processing",
      "title_zh": "超越全息：图像处理的熵量子引力基础",
      "authors": [
        "Ginestra Bianconi"
      ],
      "abstract": "Recently, thanks to the development of artificial intelligence (AI) there is\nincreasing scientific attention to establishing the connections between\ntheoretical physics and AI. Traditionally, these connections have been focusing\nmostly on the relation between string theory and image processing and involve\nimportant theoretical paradigms such as holography. Recently G. Bianconi has\nproposed the entropic quantum gravity approach that proposes an action for\ngravity given by the quantum relative entropy between the metrics associated to\na manifold. Here it is demonstrated that the famous Perona-Malik algorithm for\nimage processing is the gradient flow of the entropic quantum gravity action.\nThese results provide the geometrical and information theory foundations for\nthe Perona-Malik algorithm and open new avenues for establishing fundamental\nrelations between brain research, machine learning and entropic quantum\ngravity.",
      "tldr_zh": "这篇论文揭示了图像处理与量子引力理论之间的深刻联系。研究表明，著名的Perona-Malik图像处理算法实际上是熵量子引力作用量的梯度流，为该算法提供了几何和信息论基础。通过G. Bianconi提出的熵量子引力方法，论文建立了大脑研究、机器学习和熵量子引力之间的新理论桥梁。这一发现超越了传统的全息原理，为理解AI与理论物理的深层关系开辟了新途径。",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "gr-qc",
        "quant-ph"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "(7 pages, 1 figure)",
      "pdf_url": "http://arxiv.org/pdf/2503.14048v1",
      "published_date": "2025-03-18 09:06:33 UTC",
      "updated_date": "2025-03-18 09:06:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:17:50.129477"
    },
    {
      "arxiv_id": "2503.16529v1",
      "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts",
      "title_zh": "DeepSeek模型中文场景下的安全评估与增强",
      "authors": [
        "Wenjing Zhang",
        "Xuejiao Lei",
        "Zhaoxiang Liu",
        "Limin Han",
        "Jiaojiao Zhao",
        "Beibei Huang",
        "Zhenhong Long",
        "Junting Guo",
        "Meijuan An",
        "Rongjia Du",
        "Ning Wang",
        "Kai Wang",
        "Shiguo Lian"
      ],
      "abstract": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for six distilled models. Evaluation results indicate that\nthe enhanced models achieve significant improvements in safety while\nmaintaining reasoning capabilities without notable degradation. We open-source\nthe safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main to serve as a\nvaluable resource for future research and optimization of DeepSeek models.",
      "tldr_zh": "该研究针对DeepSeek-R1系列模型在中文场景下的安全性问题展开评估与优化。通过CHiSafetyBench基准测试发现，原始模型及其蒸馏版本存在显著安全隐患，尤其在处理有害指令时攻击成功率高达100%。研究揭示了蒸馏过程对模型安全性的负面影响，并针对性改进了6个蒸馏模型的安全性能。改进后的模型在保持推理能力的同时显著提升了安全性，相关成果已开源供后续研究参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages,13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16529v1",
      "published_date": "2025-03-18 08:38:10 UTC",
      "updated_date": "2025-03-18 08:38:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:18:07.824875"
    },
    {
      "arxiv_id": "2503.14021v1",
      "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding",
      "title_zh": "MP-GUI：基于多模态大语言模型的图形用户界面感知理解",
      "authors": [
        "Ziwei Wang",
        "Weizhi Chen",
        "Leyang Yang",
        "Sheng Zhou",
        "Shengchu Zhao",
        "Hanbei Zhan",
        "Jiongchao Jin",
        "Liangcheng Li",
        "Zirui Shao",
        "Jiajun Bu"
      ],
      "abstract": "Graphical user interface (GUI) has become integral to modern society, making\nit crucial to be understood for human-centric systems. However, unlike natural\nimages or documents, GUIs comprise artificially designed graphical elements\narranged to convey specific semantic meanings. Current multi-modal large\nlanguage models (MLLMs) already proficient in processing graphical and textual\ncomponents suffer from hurdles in GUI understanding due to the lack of explicit\nspatial structure modeling. Moreover, obtaining high-quality spatial structure\ndata is challenging due to privacy issues and noisy environments. To address\nthese challenges, we present MP-GUI, a specially designed MLLM for GUI\nunderstanding. MP-GUI features three precisely specialized perceivers to\nextract graphical, textual, and spatial modalities from the screen as\nGUI-tailored visual clues, with spatial structure refinement strategy and\nadaptively combined via a fusion gate to meet the specific preferences of\ndifferent GUI understanding tasks. To cope with the scarcity of training data,\nwe also introduce a pipeline for automatically data collecting. Extensive\nexperiments demonstrate that MP-GUI achieves impressive results on various GUI\nunderstanding tasks with limited data.",
      "tldr_zh": "本文提出MP-GUI，一种专为图形用户界面(GUI)理解设计的多模态大语言模型(MLLM)，通过三个专用感知模块分别提取图形、文本和空间模态特征，并结合空间结构优化策略和自适应融合门机制。针对GUI数据稀缺问题，作者开发了自动化数据收集流程。实验表明，该方法在有限数据条件下，显著提升了多种GUI理解任务的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14021v1",
      "published_date": "2025-03-18 08:32:22 UTC",
      "updated_date": "2025-03-18 08:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:18:26.307861"
    },
    {
      "arxiv_id": "2503.14013v1",
      "title": "Boosting Semi-Supervised Medical Image Segmentation via Masked Image Consistency and Discrepancy Learning",
      "title_zh": "通过掩码图像一致性与差异学习提升半监督医学图像分割",
      "authors": [
        "Pengcheng Zhou",
        "Lantian Zhang",
        "Wei Li"
      ],
      "abstract": "Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods.",
      "tldr_zh": "本文提出了一种名为MICD的半监督医学图像分割框架，通过三个核心模块解决现有协同训练方法在信息交换与模型多样性平衡方面的不足。该框架包含掩码交叉伪一致性(MCPC)模块增强上下文感知和小样本学习，交叉特征一致性(CFC)模块强化信息交换与模型鲁棒性，以及交叉模型差异(CMD)模块保持网络分支多样性。在AMOS和Synapse两个公开医学数据集上的实验表明，该方法超越了现有最优技术，特别擅长处理细粒度局部信息的同时维持异构框架的多样性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14013v1",
      "published_date": "2025-03-18 08:20:35 UTC",
      "updated_date": "2025-03-18 08:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:18:56.990737"
    },
    {
      "arxiv_id": "2503.14002v1",
      "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling",
      "title_zh": "MeshFleet：面向领域特定生成建模的过滤标注3D车辆数据集",
      "authors": [
        "Damian Boborzi",
        "Phillip Mueller",
        "Jonas Emrich",
        "Dominik Schmid",
        "Sebastian Mueller",
        "Lars Mikelsons"
      ],
      "abstract": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
      "tldr_zh": "该论文提出了MeshFleet——一个从最大公开3D数据集Objaverse-XL中提取的经过筛选和标注的车辆专用数据集，用于提升领域特定生成模型的精度。研究团队开发了基于DINOv2和SigLIP嵌入的质量分类器，结合标题分析和不确定性估计，构建自动化数据过滤流程。实验表明，该方法优于基于标题和图像美学的筛选技术，通过在SV3D模型上的微调验证了针对性数据选择对提升3D生成建模效果的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14002v1",
      "published_date": "2025-03-18 08:09:24 UTC",
      "updated_date": "2025-03-18 08:09:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:19:38.875391"
    },
    {
      "arxiv_id": "2503.13999v2",
      "title": "BI-RADS prediction of mammographic masses using uncertainty information extracted from a Bayesian Deep Learning model",
      "title_zh": "利用贝叶斯深度学习模型提取的不确定性信息预测乳腺肿块的BI-RADS分级",
      "authors": [
        "Mohaddeseh Chegini",
        "Ali Mahloojifar"
      ],
      "abstract": "The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist.",
      "tldr_zh": "该研究提出了一种基于贝叶斯深度学习(Bayesian Deep Learning)的BI-RADS评分预测模型，通过提取模型预测的不确定性信息来辅助放射科医生评估乳腺肿块恶性风险。实验结果表明，该模型在BI-RADS 2/3/5类样本上的F1分数(73.33%/59.60%/59.26%)显著优于放射科医生(42.86%/48.33%/48.28%)，并能以75.86%的准确率区分BI-RADS 0类别中的良恶性样本。Grad-CAM可视化证实模型关注病灶形态学特征，其不确定性评估能力可模拟放射科医生的诊断思维。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13999v2",
      "published_date": "2025-03-18 08:06:05 UTC",
      "updated_date": "2025-03-24 12:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:20:00.311560"
    },
    {
      "arxiv_id": "2503.13991v1",
      "title": "GraphTEN: Graph Enhanced Texture Encoding Network",
      "title_zh": "GraphTEN：图增强纹理编码网络",
      "authors": [
        "Bo Peng",
        "Jintao Chen",
        "Mufeng Yao",
        "Chenhao Zhang",
        "Jianghui Zhang",
        "Mingmin Chi",
        "Jiang Tao"
      ],
      "abstract": "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.",
      "tldr_zh": "本文提出GraphTEN（图增强纹理编码网络），通过全连接图建模纹理基元的全局关联性，并利用二分图捕捉跨尺度依赖关系。该网络创新性地引入基于码本的多尺度块编码模块，将无序纹理特征映射到统一特征空间。实验表明，GraphTEN在五个公开数据集上均超越现有最优方法，有效解决了纹理基元空间分布变异性和随机性带来的识别挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "I.2.10; I.4.7"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 7 figures, conference paper",
      "pdf_url": "http://arxiv.org/pdf/2503.13991v1",
      "published_date": "2025-03-18 07:51:13 UTC",
      "updated_date": "2025-03-18 07:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:19:52.569984"
    },
    {
      "arxiv_id": "2503.14564v1",
      "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation",
      "title_zh": "轻松主动标注：面向长期测试时自适应的解决方案",
      "authors": [
        "Guowei Wang",
        "Changxing Ding"
      ],
      "abstract": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.",
      "tldr_zh": "本文提出了一种高效的长期测试时适应(TTA)方法，旨在最小化标注负担。通过每次批处理中仅标注一个最有价值的样本（即位于源域和目标域分布边界的样本），并结合特征扰动策略识别这些样本，显著降低了标注成本。此外，提出了一种动态权重机制，平衡标注样本和未标注样本对模型优化的影响。实验表明，该方法在多个数据集上均优于现有方法，同时大幅减少了标注工作量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA",
      "pdf_url": "http://arxiv.org/pdf/2503.14564v1",
      "published_date": "2025-03-18 07:49:27 UTC",
      "updated_date": "2025-03-18 07:49:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:20:16.210274"
    },
    {
      "arxiv_id": "2503.14563v2",
      "title": "Workflow for Safe-AI",
      "title_zh": "安全AI工作流",
      "authors": [
        "Suzana Veljanovska",
        "Hans Dermot Doran"
      ],
      "abstract": "The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment",
      "tldr_zh": "本文提出了一种面向安全关键领域AI系统的轻量化工作流Safe-AI，重点解决功能安全需求与AI快速迭代之间的平衡问题。该工作流基于扩展的ONNX模型描述框架，通过最小化工具集和限定功能范围实现资质认证(qualifiable)要求，确保从模型生成到运行时部署的全流程验证。其核心创新在于将V-model开发流程与AI特性结合，为混合临界性系统(mixed-criticality systems)提供可靠且可资质认证的AI部署方案。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Embedded World Conference, Nuremberg, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14563v2",
      "published_date": "2025-03-18 07:45:18 UTC",
      "updated_date": "2025-03-20 07:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:20:30.407220"
    },
    {
      "arxiv_id": "2503.13988v1",
      "title": "Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks",
      "title_zh": "赋能小模型：基于链式思维调优LLaMA与Gemma的乌克兰考试任务处理",
      "authors": [
        "Mykyta Syromiatnikov",
        "Victoria Ruvinskaya",
        "Nataliia Komleva"
      ],
      "abstract": "Leading large language models have demonstrated impressive capabilities in\nreasoning-intensive tasks, such as standardized educational testing. However,\nthey often require extensive training in low-resource settings with\ninaccessible infrastructure. Small or compact models, though more efficient,\nfrequently lack sufficient support for underrepresented languages, leaving a\nperformance gap in critical domains. This work explores the potential of\nparameter-efficient fine-tuning of compact open-weight language models to\nhandle reasoning-intensive tasks in the underrepresented Ukrainian language,\nbuilding on the findings of the ZNO-Eval benchmark. Parameter-efficient\nfine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion\nparameters), and Gemma 2 (9 billion parameters) models on chain-of-thought\nsolutions resulted in a modest test score improvement of up to 17.4% on complex\nmatching tasks and 1.6% overall compared to tuning on answer letters alone,\noffering enhanced interpretability and robustness. In addition, the proposed\ntuning method with joint task topic and step-by-step solution generation\noutperforms standard chain-of-thought tuning in matching tasks and provides a\n5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and\napply domain-relevant information. Contrasting obtained results with zero-shot\nevaluations of leading open-weight and proprietary models such as Qwen,\nDeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning\nLLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million\ntrainable parameters on a single A100 GPU lets them outperform GPT-4o mini,\nMistral Large, and larger open-weight models. This research also evaluates how\nmerging the quantized adapter with the base model influences the generation\nquality. Source code and tuned models are available at\nhttps://github.com/NLPForUA/ZNO.",
      "tldr_zh": "本研究探讨了如何通过参数高效微调（parameter-efficient fine-tuning）提升中小型开源语言模型（如LLaMA 3.1/3.2和Gemma 2）在乌克兰语考试任务中的推理能力。采用链式思维（chain-of-thought）微调方法后，模型在复杂匹配任务上最高提升17.4%准确率，整体性能优于仅微调答案字母的基线方法1.6%。通过联合生成题目主题和分步解决方案的创新微调策略，模型进一步在匹配任务中超越标准链式思维微调，并使LLaMA 3.2模型性能提升5.4%。实验表明，仅用单块A100 GPU对2,032个分步解决方案进行微调后，这些中小模型即可超越GPT-4o mini、Mistral Large等更大规模模型。研究还开源了相关代码和微调模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 6 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13988v1",
      "published_date": "2025-03-18 07:44:49 UTC",
      "updated_date": "2025-03-18 07:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:20:55.805325"
    },
    {
      "arxiv_id": "2503.13985v1",
      "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
      "title_zh": "DefectFill：基于修复扩散模型的真实缺陷生成技术用于视觉检测",
      "authors": [
        "Jaewoo Song",
        "Daemin Park",
        "Kanghyun Baek",
        "Sangyub Lee",
        "Jooyoung Choi",
        "Eunji Kim",
        "Sungroh Yoon"
      ],
      "abstract": "Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.",
      "tldr_zh": "该研究提出了DefectFill，一种基于修复扩散模型(Inpainting Diffusion Model)的真实缺陷生成方法，用于解决视觉检测中缺陷数据稀缺的问题。该方法仅需少量参考缺陷图像，通过定制损失函数（结合缺陷、物体和注意力项）优化模型，精确捕捉局部缺陷特征并将其无缝融入无缺陷物体中。实验表明，DefectFill生成的缺陷图像质量高，在MVTec AD数据集上使视觉检测模型达到了最先进的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13985v1",
      "published_date": "2025-03-18 07:42:11 UTC",
      "updated_date": "2025-03-18 07:42:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:21:08.802137"
    },
    {
      "arxiv_id": "2503.14562v1",
      "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy",
      "title_zh": "基于机器学习方法的人类视野信息分析及其准确性评估",
      "authors": [
        "A. I. Medvedeva",
        "V. V. Bakutkin"
      ],
      "abstract": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification).",
      "tldr_zh": "本研究利用机器学习方法分析视野检查图像，旨在辅助青光眼的诊断和监测。通过构建包含不同眼科疾病患者数据的标注数据集，研究评估了随机梯度下降、逻辑回归、随机森林和朴素贝叶斯等分类算法的性能。实验结果显示，所开发的计算机模型能够有效实现青光眼与其他眼疾的二元分类，为眼科疾病诊断提供了自动化工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "in Russian language",
      "pdf_url": "http://arxiv.org/pdf/2503.14562v1",
      "published_date": "2025-03-18 07:39:41 UTC",
      "updated_date": "2025-03-18 07:39:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:21:47.830985"
    },
    {
      "arxiv_id": "2503.16528v1",
      "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL",
      "title_zh": "HDLCoRe：一种无需训练的框架用于缓解LLM生成硬件描述语言中的幻觉问题",
      "authors": [
        "Heng Ping",
        "Shixuan Li",
        "Peiyu Zhang",
        "Anzhe Cheng",
        "Shukai Duan",
        "Nikos Kanakaris",
        "Xiongye Xiao",
        "Wei Yang",
        "Shahin Nazarian",
        "Andrei Irimia",
        "Paul Bogdan"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.",
      "tldr_zh": "该研究提出HDLCoRe框架，无需额外训练即可解决大语言模型(LLMs)在硬件描述语言(HDL)代码生成中的幻觉问题。该方法结合了两种关键技术：1) 基于硬件感知的链式思维提示(HDL-aware Chain-of-Thought)与自验证机制，通过任务分类和逐步仿真实现错误修正；2) 两阶段异构检索增强生成(RAG)系统，通过组件提取和序列筛选提高HDL示例检索效率。在RTLLM2.0基准测试中，该框架显著降低了幻觉现象，同时提升了代码的语法和功能正确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16528v1",
      "published_date": "2025-03-18 07:09:39 UTC",
      "updated_date": "2025-03-18 07:09:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:22:11.044127"
    },
    {
      "arxiv_id": "2503.13951v1",
      "title": "FrustumFusionNets: A Three-Dimensional Object Detection Network Based on Tractor Road Scene",
      "title_zh": "FrustumFusionNets：基于拖拉机道路场景的三维目标检测网络",
      "authors": [
        "Lili Yang",
        "Mengshuai Chang",
        "Xiao Guo",
        "Yuxin Feng",
        "Yiwen Mei",
        "Caicong Wu"
      ],
      "abstract": "To address the issues of the existing frustum-based methods' underutilization\nof image information in road three-dimensional object detection as well as the\nlack of research on agricultural scenes, we constructed an object detection\ndataset using an 80-line Light Detection And Ranging (LiDAR) and a camera in a\ncomplex tractor road scene and proposed a new network called FrustumFusionNets\n(FFNets). Initially, we utilize the results of image-based two-dimensional\nobject detection to narrow down the search region in the three-dimensional\nspace of the point cloud. Next, we introduce a Gaussian mask to enhance the\npoint cloud information. Then, we extract the features from the frustum point\ncloud and the crop image using the point cloud feature extraction pipeline and\nthe image feature extraction pipeline, respectively. Finally, we concatenate\nand fuse the data features from both modalities to achieve three-dimensional\nobject detection. Experiments demonstrate that on the constructed test set of\ntractor road data, the FrustumFusionNetv2 achieves 82.28% and 95.68% accuracy\nin the three-dimensional object detection of the two main road objects, cars\nand people, respectively. This performance is 1.83% and 2.33% better than the\noriginal model. It offers a hybrid fusion-based multi-object, high-precision,\nreal-time three-dimensional object detection technique for unmanned\nagricultural machines in tractor road scenarios. On the Karlsruhe Institute of\nTechnology and Toyota Technological Institute (KITTI) Benchmark Suite\nvalidation set, the FrustumFusionNetv2 also demonstrates significant\nsuperiority in detecting road pedestrian objects compared with other\nfrustum-based three-dimensional object detection methods.",
      "tldr_zh": "该研究提出了FrustumFusionNets（FFNets），一种基于视锥体融合的三维目标检测网络，专门针对拖拉机道路场景。该方法通过结合二维图像检测结果缩小点云搜索范围，引入高斯掩模增强点云信息，并融合图像与点云特征，在自建的拖拉机道路数据集上对车辆和行人检测准确率分别达到82.28%和95.68%，较原模型提升1.83%和2.33%。该技术为农业机械无人化提供了实时高精度的多目标三维检测方案，在KITTI基准测试中也展现出对行人检测的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13951v1",
      "published_date": "2025-03-18 06:40:39 UTC",
      "updated_date": "2025-03-18 06:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:22:34.383672"
    },
    {
      "arxiv_id": "2503.15550v2",
      "title": "Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm",
      "title_zh": "零知识联邦学习：一种新型可信且隐私保护的分布式学习范式",
      "authors": [
        "Yuxin Jin",
        "Taotao Wang",
        "Qing Yang",
        "Long Shi",
        "Shengli Zhang"
      ],
      "abstract": "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
      "tldr_zh": "该研究提出了零知识联邦学习(Zero-Knowledge Federated Learning, ZK-FL)这一新型可信隐私计算范式。作者构建了系统化的ZK-FL框架，首次系统分析了零知识证明(ZKPs)在联邦学习各阶段的技术作用，并创新性地设计了可验证客户端选择算法(Veri-CS-FL)。该算法通过让客户端生成本地模型性能的可验证证明，使服务器能高效筛选高质量模型进行聚合，在保证指标真实性的同时提升了系统整体效率。研究表明，这种结合零知识证明的方法能有效增强联邦学习参与方间的信任机制，为隐私保护分布式学习提供了更安全可靠的解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.15550v2",
      "published_date": "2025-03-18 06:21:08 UTC",
      "updated_date": "2025-03-24 03:55:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:22:49.376844"
    },
    {
      "arxiv_id": "2503.13938v2",
      "title": "ChatBEV: A Visual Language Model that Understands BEV Maps",
      "title_zh": "ChatBEV：一款理解BEV地图的视觉语言模型",
      "authors": [
        "Qingyao Xu",
        "Siheng Chen",
        "Guang Chen",
        "Yanfeng Wang",
        "Ya Zhang"
      ],
      "abstract": "Traffic scene understanding is essential for intelligent transportation\nsystems and autonomous driving, ensuring safe and efficient vehicle operation.\nWhile recent advancements in VLMs have shown promise for holistic scene\nunderstanding, the application of VLMs to traffic scenarios, particularly using\nBEV maps, remains under explored. Existing methods often suffer from limited\ntask design and narrow data amount, hindering comprehensive scene\nunderstanding. To address these challenges, we introduce ChatBEV-QA, a novel\nBEV VQA benchmark contains over 137k questions, designed to encompass a wide\nrange of scene understanding tasks, including global scene understanding,\nvehicle-lane interactions, and vehicle-vehicle interactions. This benchmark is\nconstructed using an novel data collection pipeline that generates scalable and\ninformative VQA data for BEV maps. We further fine-tune a specialized\nvision-language model ChatBEV, enabling it to interpret diverse question\nprompts and extract relevant context-aware information from BEV maps.\nAdditionally, we propose a language-driven traffic scene generation pipeline,\nwhere ChatBEV facilitates map understanding and text-aligned navigation\nguidance, significantly enhancing the generation of realistic and consistent\ntraffic scenarios. The dataset, code and the fine-tuned model will be released.",
      "tldr_zh": "本文提出ChatBEV，一个专注于鸟瞰图(BEV)场景理解的视觉语言模型(VLM)。研究团队构建了包含13.7万问题的ChatBEV-QA基准数据集，覆盖全局场景理解、车-道交互等多维度任务，并开发了可扩展的数据生成流程。基于此训练的特化模型能准确解读BEV地图中的上下文信息，同时提出的语言驱动交通场景生成流程可创建高真实感的导航方案。该研究为自动驾驶系统的全面场景理解提供了新工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13938v2",
      "published_date": "2025-03-18 06:12:38 UTC",
      "updated_date": "2025-03-21 02:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:23:27.749385"
    },
    {
      "arxiv_id": "2503.17394v1",
      "title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness",
      "title_zh": "脉冲神经网络的时序灵活性：实现跨时间步长泛化与部署友好性",
      "authors": [
        "Kangrui Du",
        "Yuhang Wu",
        "Shikuang Deng",
        "Shi Gu"
      ],
      "abstract": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the\nbrain, allow for energy-efficient implementation on neuromorphic hardware.\nHowever, SNNs trained with current direct training approaches are constrained\nto a specific time step. This \"temporal inflexibility\" 1) hinders SNNs'\ndeployment on time-step-free fully event-driven chips and 2) prevents\nenergy-performance balance based on dynamic inference time steps. In this\nstudy, we first explore the feasibility of training SNNs that generalize across\ndifferent time steps. We then introduce Mixed Time-step Training (MTT), a novel\nmethod that improves the temporal flexibility of SNNs, making SNNs adaptive to\ndiverse temporal structures. During each iteration of MTT, random time steps\nare assigned to different SNN stages, with spikes transmitted between stages\nvia communication modules. After training, the weights are deployed and\nevaluated on both time-stepped and fully event-driven platforms. Experimental\nresults show that models trained by MTT gain remarkable temporal flexibility,\nfriendliness for both event-driven and clock-driven deployment (nearly lossless\non N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced\nnetwork generalization, and near SOTA performance. To the best of our\nknowledge, this is the first work to report the results of large-scale SNN\ndeployment on fully event-driven scenarios.",
      "tldr_zh": "该研究提出了混合时间步训练（MTT）方法，旨在提高脉冲神经网络（SNNs）的时间灵活性，使其能够适应不同的时间步长和部署场景。通过在每个训练迭代中为不同SNN阶段分配随机时间步长，并通过通信模块传递脉冲，MTT使模型在训练后能够在时间步驱动和完全事件驱动的平台上高效运行。实验结果表明，MTT训练的模型在时间灵活性、部署友好性（如N-MNIST上几乎无损，CIFAR10-DVS上比标准方法高10.1%）和泛化能力方面均有显著提升，同时保持了接近最先进的性能。这是首次报道大规模SNN在完全事件驱动场景下的部署结果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17394v1",
      "published_date": "2025-03-18 06:09:42 UTC",
      "updated_date": "2025-03-18 06:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:23:33.156479"
    },
    {
      "arxiv_id": "2503.13934v1",
      "title": "COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning",
      "title_zh": "COLSON：基于扩散强化学习的可控社交导航学习框架",
      "authors": [
        "Yuki Tomita",
        "Kohei Matsumoto",
        "Yuki Hyodo",
        "Ryo Kurazume"
      ],
      "abstract": "Mobile robot navigation in dynamic environments with pedestrian traffic is a\nkey challenge in the development of autonomous mobile service robots. Recently,\ndeep reinforcement learning-based methods have been actively studied and have\noutperformed traditional rule-based approaches owing to their optimization\ncapabilities. Among these, methods that assume a continuous action space\ntypically rely on a Gaussian distribution assumption, which limits the\nflexibility of generated actions. Meanwhile, the application of diffusion\nmodels to reinforcement learning has advanced, allowing for more flexible\naction distributions compared with Gaussian distribution-based approaches. In\nthis study, we applied a diffusion-based reinforcement learning approach to\nsocial navigation and validated its effectiveness. Furthermore, by leveraging\nthe characteristics of diffusion models, we propose an extension that enables\npost-training action smoothing and adaptation to static obstacle scenarios not\nconsidered during the training steps.",
      "tldr_zh": "本研究提出COLSON方法，采用基于扩散模型(Diffusion Model)的强化学习框架来解决动态行人环境中的机器人社交导航问题。相比传统基于高斯分布假设的方法，该方案能生成更灵活的动作分布，并通过扩散模型的特性实现了训练后的动作平滑调整和静态障碍物场景适应能力。实验验证了该方法在社交导航任务中的有效性，为服务机器人在复杂动态环境中的自主导航提供了新思路。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to IROS 2025 for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2503.13934v1",
      "published_date": "2025-03-18 06:02:30 UTC",
      "updated_date": "2025-03-18 06:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:23:51.728225"
    },
    {
      "arxiv_id": "2503.13923v1",
      "title": "ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models",
      "title_zh": "ConSCompF：面向生成式大语言模型的一致性优先相似度比较框架",
      "authors": [
        "Alexey Karev",
        "Dong Xu"
      ],
      "abstract": "Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.",
      "tldr_zh": "该研究提出了ConSCompF（一致性导向相似度比较框架），用于评估生成式大语言模型（LLMs）之间的相似性。该框架通过分析两个LLM生成的文本输出，计算相似度得分，其优势在于仅需少量未标注数据（如聊天指令提示）且无需模型内部信息。实验验证了ConSCompF在识别LLM相似性方面的有效性，并与ROUGE-L等基准方法的输出差异建立了相关性分析。该框架生成的相似度矩阵可通过主成分分析（PCA）可视化，不仅能辅助检测LLM潜在训练数据，还能为识别投资欺诈行为提供技术支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13923v1",
      "published_date": "2025-03-18 05:38:04 UTC",
      "updated_date": "2025-03-18 05:38:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:24:10.636331"
    },
    {
      "arxiv_id": "2503.13921v1",
      "title": "Learning Accurate Models on Incomplete Data with Minimal Imputation",
      "title_zh": "在缺失数据上以最小填补学习精确模型",
      "authors": [
        "Cheng Zhen",
        "Nischal Aryal",
        "Arash Termehchy",
        "Prayoga",
        "Garrett Biwer",
        "Sankalp Patil"
      ],
      "abstract": "Missing data often exists in real-world datasets, requiring significant time\nand effort for imputation to learn accurate machine learning (ML) models. In\nthis paper, we demonstrate that imputing all missing values is not always\nnecessary to achieve an accurate ML model. We introduce the concept of minimal\ndata imputation, which ensures accurate ML models trained over the imputed\ndataset. Implementing minimal imputation guarantees both minimal imputation\neffort and optimal ML models. We propose algorithms to find exact and\napproximate minimal imputation for various ML models. Our extensive experiments\nindicate that our proposed algorithms significantly reduce the time and effort\nrequired for data imputation.",
      "tldr_zh": "该论文提出了一种\"最小数据填补\"（minimal data imputation）方法，证明在机器学习模型训练中无需对所有缺失值进行填补也能获得高精度。研究者开发了精确和近似两种算法，可针对不同ML模型确定最小填补方案，实验表明该方法能显著减少数据预处理的时间和成本。这一发现为处理现实世界不完整数据集提供了更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13921v1",
      "published_date": "2025-03-18 05:36:59 UTC",
      "updated_date": "2025-03-18 05:36:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:24:54.808610"
    },
    {
      "arxiv_id": "2503.13916v1",
      "title": "Learning Bimanual Manipulation via Action Chunking and Inter-Arm Coordination with Transformers",
      "title_zh": "通过动作分块与双臂协调的Transformer学习双手机器人操作",
      "authors": [
        "Tomohiro Motoda",
        "Ryo Hanai",
        "Ryoichi Nakajo",
        "Masaki Murooka",
        "Floris Erich",
        "Yukiyasu Domae"
      ],
      "abstract": "Robots that can operate autonomously in a human living environment are\nnecessary to have the ability to handle various tasks flexibly. One crucial\nelement is coordinated bimanual movements that enable functions that are\ndifficult to perform with one hand alone. In recent years, learning-based\nmodels that focus on the possibilities of bimanual movements have been\nproposed. However, the high degree of freedom of the robot makes it challenging\nto reason about control, and the left and right robot arms need to adjust their\nactions depending on the situation, making it difficult to realize more\ndexterous tasks. To address the issue, we focus on coordination and efficiency\nbetween both arms, particularly for synchronized actions. Therefore, we propose\na novel imitation learning architecture that predicts cooperative actions. We\ndifferentiate the architecture for both arms and add an intermediate encoder\nlayer, Inter-Arm Coordinated transformer Encoder (IACE), that facilitates\nsynchronization and temporal alignment to ensure smooth and coordinated\nactions. To verify the effectiveness of our architectures, we perform\ndistinctive bimanual tasks. The experimental results showed that our model\ndemonstrated a high success rate for comparison and suggested a suitable\narchitecture for the policy learning of bimanual manipulation.",
      "tldr_zh": "该研究提出了一种基于Transformer的新型模仿学习架构，用于解决机器人双臂协调操作的高自由度控制难题。通过引入\"动作分块\"(Action Chunking)和创新的双臂协调Transformer编码器(IACE)中间层，实现了双臂动作的同步和时间对齐。实验表明，该架构在多种典型双臂操作任务中表现出较高的成功率，为机器人柔性执行复杂双手协调任务提供了有效的策略学习方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.13916v1",
      "published_date": "2025-03-18 05:20:34 UTC",
      "updated_date": "2025-03-18 05:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:25:11.924695"
    },
    {
      "arxiv_id": "2503.13915v1",
      "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
      "title_zh": "释放半监督领域泛化中未标记数据的潜力",
      "authors": [
        "Dongkwan Lee",
        "Kyomin Hwang",
        "Nojun Kwak"
      ],
      "abstract": "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https://github.com/dongkwani/UPCSC.",
      "tldr_zh": "该研究提出了一种名为UPCSC的新方法，用于半监督领域泛化（SSDG），旨在充分利用未标记数据，包括传统方法忽略的低置信度未标记样本。该方法包含两个模块：1）基于未标记样本的对比学习（UPC），将低置信度样本作为负样本对；2）代理类学习（SC），为低置信度样本生成正样本对。实验表明，UPCSC在四个SSDG基准数据集上显著提升了基线模型的性能，增强了类别区分能力并缓解了领域差距。该方法无需领域标签，可轻松集成到现有方法中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13915v1",
      "published_date": "2025-03-18 05:19:33 UTC",
      "updated_date": "2025-03-18 05:19:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:25:10.645940"
    },
    {
      "arxiv_id": "2503.13912v1",
      "title": "KANITE: Kolmogorov-Arnold Networks for ITE estimation",
      "title_zh": "KANITE：基于Kolmogorov-Arnold网络的个体处理效应估计",
      "authors": [
        "Eshan Mehendale",
        "Abhinav Thorat",
        "Ravi Kolla",
        "Niranjan Pedanekar"
      ],
      "abstract": "We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)\nfor Individual Treatment Effect (ITE) estimation under multiple treatments\nsetting in causal inference. By utilizing KAN's unique abilities to learn\nunivariate activation functions as opposed to learning linear weights by\nMulti-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE\nframework comprises two key architectures: 1.Integral Probability Metric (IPM)\narchitecture: This employs an IPM loss in a specialized manner to effectively\nalign towards ITE estimation across multiple treatments. 2. Entropy Balancing\n(EB) architecture: This uses weights for samples that are learned by optimizing\nentropy subject to balancing the covariates across treatment groups. Extensive\nevaluations on benchmark datasets demonstrate that KANITE outperforms\nstate-of-the-art algorithms in both $\\epsilon_{\\text{PEHE}}$ and\n$\\epsilon_{\\text{ATE}}$ metrics. Our experiments highlight the advantages of\nKANITE in achieving improved causal estimates, emphasizing the potential of\nKANs to advance causal inference methodologies across diverse application\nareas.",
      "tldr_zh": "本文提出KANITE框架，利用Kolmogorov-Arnold Networks (KANs)在因果推断中处理多治疗场景下的个体治疗效果(ITE)估计问题。该框架通过两种创新架构：1) 采用Integral Probability Metric (IPM)损失函数优化多治疗组的效应对齐；2) 通过Entropy Balancing (EB)架构学习样本权重以实现协变量平衡。实验表明，KANITE在$\\epsilon_{\\text{PEHE}}$和$\\epsilon_{\\text{ATE}}$指标上均超越现有最优方法，验证了KANs在提升因果估计精度方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13912v1",
      "published_date": "2025-03-18 05:16:36 UTC",
      "updated_date": "2025-03-18 05:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:25:48.196730"
    },
    {
      "arxiv_id": "2503.13906v1",
      "title": "HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object Detection",
      "title_zh": "HSOD-BIT-V2：面向高光谱显著目标检测的新挑战性基准",
      "authors": [
        "Yuhao Qiu",
        "Shuyan Bai",
        "Tingfa Xu",
        "Peifu Liu",
        "Haolin Qin",
        "Jianan Li"
      ],
      "abstract": "Salient Object Detection (SOD) is crucial in computer vision, yet RGB-based\nmethods face limitations in challenging scenes, such as small objects and\nsimilar color features. Hyperspectral images provide a promising solution for\nmore accurate Hyperspectral Salient Object Detection (HSOD) by abundant\nspectral information, while HSOD methods are hindered by the lack of extensive\nand available datasets. In this context, we introduce HSOD-BIT-V2, the largest\nand most challenging HSOD benchmark dataset to date. Five distinct challenges\nfocusing on small objects and foreground-background similarity are designed to\nemphasize spectral advantages and real-world complexity. To tackle these\nchallenges, we propose Hyper-HRNet, a high-resolution HSOD network. Hyper-HRNet\neffectively extracts, integrates, and preserves effective spectral information\nwhile reducing dimensionality by capturing the self-similar spectral features.\nAdditionally, it conveys fine details and precisely locates object contours by\nincorporating comprehensive global information and detailed object saliency\nrepresentations. Experimental analysis demonstrates that Hyper-HRNet\noutperforms existing models, especially in challenging scenarios.",
      "tldr_zh": "该研究提出了HSOD-BIT-V2，这是目前规模最大且最具挑战性的高光谱显著性目标检测(HSOD)基准数据集，重点针对小目标和前景-背景相似性等五种典型挑战场景。为解决这些问题，作者开发了Hyper-HRNet网络，该网络通过提取自相似光谱特征有效降维，同时结合全局信息和细节显著性表示来精确定位目标轮廓。实验表明，Hyper-HRNet在复杂场景下显著优于现有模型，验证了高光谱数据在显著性检测中的优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13906v1",
      "published_date": "2025-03-18 05:09:42 UTC",
      "updated_date": "2025-03-18 05:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:25:56.399684"
    },
    {
      "arxiv_id": "2503.13903v1",
      "title": "TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection",
      "title_zh": "TGBFormer：面向视频目标检测的Transformer-GraphFormer混合网络",
      "authors": [
        "Qiang Qi",
        "Xiao Wang"
      ],
      "abstract": "Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU.",
      "tldr_zh": "本文提出TGBFormer网络，通过融合Transformer和GraphFormer的优势来解决视频目标检测中全局与局部特征难以兼顾的问题。该框架包含三个核心模块：1）时空Transformer模块用于捕获长距离全局特征依赖；2）时空GraphFormer模块通过图卷积网络聚合局部时空关系特征；3）全局-局部特征混合模块自适应融合两类特征。实验表明，该方法在ImageNet VID数据集上达到86.5% mAP的最新性能，同时保持41 FPS的实时推理速度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13903v1",
      "published_date": "2025-03-18 05:03:05 UTC",
      "updated_date": "2025-03-18 05:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:26:10.912173"
    },
    {
      "arxiv_id": "2503.13882v1",
      "title": "MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments",
      "title_zh": "MoK-RAG：面向具身AI环境的混合知识路径增强检索生成框架",
      "authors": [
        "Zhengsheng Guo",
        "Linwei Zheng",
        "Xinyang Chen",
        "Xuefeng Bai",
        "Kehai Chen",
        "Min Zhang"
      ],
      "abstract": "While human cognition inherently retrieves information from diverse and\nspecialized knowledge sources during decision-making processes, current\nRetrieval-Augmented Generation (RAG) systems typically operate through\nsingle-source knowledge retrieval, leading to a cognitive-algorithmic\ndiscrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG\nframework that implements a mixture of knowledge paths enhanced retrieval\nmechanism through functional partitioning of a large language model (LLM)\ncorpus into distinct sections, enabling retrieval from multiple specialized\nknowledge paths. Applied to the generation of 3D simulated environments, our\nproposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into\ndistinct sections and organizing them based on a hierarchical knowledge tree\nstructure. Different from previous methods that only use manual evaluation, we\npioneered the introduction of automated evaluation methods for 3D scenes. Both\nautomatic and human evaluations in our experiments demonstrate that MoK-RAG3D\ncan assist Embodied AI agents in generating diverse scenes.",
      "tldr_zh": "该研究提出MoK-RAG框架，通过将大语言模型(LLM)语料库功能分区实现多源知识路径混合检索，解决了现有检索增强生成(RAG)系统单源检索与人类认知决策的差异问题。特别针对具身AI环境开发的MoK-RAG3D变体，采用分层知识树结构组织3D资源分区，并首创了3D场景自动化评估方法。实验表明，该框架能有效辅助具身AI智能体生成多样化场景，自动和人工评估结果均验证了其优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13882v1",
      "published_date": "2025-03-18 04:27:02 UTC",
      "updated_date": "2025-03-18 04:27:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:26:34.536930"
    },
    {
      "arxiv_id": "2503.13879v2",
      "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review Generation via Cognitive Alignment",
      "title_zh": "连接社会心理学与大语言模型推理：通过认知对齐实现冲突感知的元评审生成",
      "authors": [
        "Wei Chen",
        "Han Ding",
        "Meng Yuan",
        "Zhao Zhang",
        "Deqing Wang",
        "Fuzhen Zhuang"
      ],
      "abstract": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
      "tldr_zh": "本研究提出认知对齐框架(CAF)，通过将社会心理学中的双加工理论(Kahneman's dual-process theory)与LLM推理相结合，解决了现有AI系统在生成学术元评论时难以处理观点冲突和认知偏差的问题。该框架采用三阶段认知流程(评论初始化、增量整合和认知对齐)，使LLMs能够像科学仲裁者一样进行冲突感知推理和共识推导。实验表明，CAF在情感一致性和内容一致性上分别提升19.47%和12.95%，为自动化高质量元评论生成提供了新方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13879v2",
      "published_date": "2025-03-18 04:13:11 UTC",
      "updated_date": "2025-03-21 07:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:26:51.317176"
    },
    {
      "arxiv_id": "2503.14559v1",
      "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance",
      "title_zh": "《从样本中榨取令牌：实现更细粒度的数据治理》",
      "authors": [
        "Weixiong Lin",
        "Chen Ju",
        "Haicheng Wang",
        "Shengchao Hu",
        "Shuai Xiao",
        "Mengting Chen",
        "Yuheng Jiao",
        "Mingshuai Yao",
        "Jinsong Lan",
        "Qingwen Liu",
        "Ying Chen"
      ],
      "abstract": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.",
      "tldr_zh": "该研究提出DataJuicer框架，将数据治理从\"筛选样本\"升级为\"榨取信息\"模式。通过双分支架构对样本内部进行细粒度治理：视觉分支保留关键图像块并提取相关物体类别，文本分支则利用这些类别增强描述。实验表明，该方法在图像文本检索、分类和密集视觉推理任务上显著优于传统数据筛选方法DataSieve，实现了更高效的数据压缩和提纯。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14559v1",
      "published_date": "2025-03-18 04:06:50 UTC",
      "updated_date": "2025-03-18 04:06:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:27:13.121762"
    },
    {
      "arxiv_id": "2503.13868v1",
      "title": "Out-of-Distribution Generalization in Time Series: A Survey",
      "title_zh": "时间序列中的分布外泛化研究综述",
      "authors": [
        "Xin Wu",
        "Fei Teng",
        "Xingwang Li",
        "Ji Zhang",
        "Tianrui Li",
        "Qiang Duan"
      ],
      "abstract": "Time series frequently manifest distribution shifts, diverse latent features,\nand non-stationary learning dynamics, particularly in open and evolving\nenvironments. These characteristics pose significant challenges for\nout-of-distribution (OOD) generalization. While substantial progress has been\nmade, a systematic synthesis of advancements remains lacking. To address this\ngap, we present the first comprehensive review of OOD generalization\nmethodologies for time series, organized to delineate the field's evolutionary\ntrajectory and contemporary research landscape. We organize our analysis across\nthree foundational dimensions: data distribution, representation learning, and\nOOD evaluation. For each dimension, we present several popular algorithms in\ndetail. Furthermore, we highlight key application scenarios, emphasizing their\nreal-world impact. Finally, we identify persistent challenges and propose\nfuture research directions. A detailed summary of the methods reviewed for the\ngeneralization of OOD in time series can be accessed at\nhttps://tsood-generalization.com.",
      "tldr_zh": "本文首次系统综述了时间序列中的分布外泛化（OOD Generalization）方法，填补了该领域缺乏系统性总结的空白。研究从数据分布、表示学习和OOD评估三个核心维度展开，详细分析了几种流行算法，并探讨了关键应用场景及其实际影响。文章还指出了当前面临的挑战，并提出了未来研究方向，为时间序列OOD泛化领域提供了全面的研究框架和资源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 8 figures, 5 tables. Work in Progress",
      "pdf_url": "http://arxiv.org/pdf/2503.13868v1",
      "published_date": "2025-03-18 03:35:29 UTC",
      "updated_date": "2025-03-18 03:35:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:27:30.151185"
    },
    {
      "arxiv_id": "2503.13861v1",
      "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving",
      "title_zh": "RAD：自动驾驶中基于视觉语言模型与检索增强的元动作决策框架",
      "authors": [
        "Yujin Wang",
        "Quanfeng Liu",
        "Zhengxin Jiang",
        "Tianyi Wang",
        "Junfeng Jiao",
        "Hongqing Chu",
        "Bingzhao Gao",
        "Hong Chen"
      ],
      "abstract": "Accurately understanding and deciding high-level meta-actions is essential\nfor ensuring reliable and safe autonomous driving systems. While\nvision-language models (VLMs) have shown significant potential in various\nautonomous driving tasks, they often suffer from limitations such as inadequate\nspatial perception and hallucination, reducing their effectiveness in complex\nautonomous driving scenarios. To address these challenges, we propose a\nretrieval-augmented decision-making (RAD) framework, a novel architecture\ndesigned to enhance VLMs' capabilities to reliably generate meta-actions in\nautonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)\npipeline to dynamically improve decision accuracy through a three-stage process\nconsisting of the embedding flow, retrieving flow, and generating flow.\nAdditionally, we fine-tune VLMs on a specifically curated dataset derived from\nthe NuScenes dataset to enhance their spatial perception and bird's-eye view\nimage comprehension capabilities. Extensive experimental evaluations on the\ncurated NuScenes-based dataset demonstrate that RAD outperforms baseline\nmethods across key evaluation metrics, including match accuracy, and F1 score,\nand self-defined overall score, highlighting its effectiveness in improving\nmeta-action decision-making for autonomous driving tasks.",
      "tldr_zh": "该研究提出RAD（检索增强决策）框架，通过结合检索增强生成（RAG）技术来提升视觉语言模型（VLMs）在自动驾驶场景中的高级元动作决策能力。该系统采用三阶段流程（嵌入流、检索流和生成流）动态提升决策精度，并基于NuScenes数据集微调VLMs以增强空间感知和鸟瞰图理解能力。实验表明，RAD在匹配准确率、F1分数等关键指标上均优于基线方法，有效解决了现有VLMs在复杂驾驶场景中的空间感知不足和幻觉问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13861v1",
      "published_date": "2025-03-18 03:25:57 UTC",
      "updated_date": "2025-03-18 03:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:27:55.578980"
    },
    {
      "arxiv_id": "2503.16527v1",
      "title": "LLM Generated Persona is a Promise with a Catch",
      "title_zh": "《LLM生成的人物角色：承诺与陷阱并存》",
      "authors": [
        "Ang Li",
        "Haozhe Chen",
        "Hongseok Namkoong",
        "Tianyi Peng"
      ],
      "abstract": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",
      "tldr_zh": "该研究探讨了使用大语言模型（LLMs）生成人物角色（Persona）的潜力与局限性。虽然LLMs提供了一种可扩展且低成本的方式生成合成人物角色，但当前方法缺乏严谨性，导致系统性偏差，影响了模拟结果的准确性。通过大规模实验（如总统选举预测和美国人口意见调查），研究发现这些偏差可能导致与真实结果的显著偏离。研究呼吁建立更严格的生成科学框架，并开源了约100万个人物角色数据集以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16527v1",
      "published_date": "2025-03-18 03:11:27 UTC",
      "updated_date": "2025-03-18 03:11:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:28:12.794101"
    },
    {
      "arxiv_id": "2503.13856v1",
      "title": "MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary Team Medical Consultation",
      "title_zh": "MDTeamGPT：基于自进化大语言模型的多学科团队医疗会诊多智能体框架",
      "authors": [
        "Kai Chen",
        "Xinfeng Li",
        "Tianpei Yang",
        "Hewei Wang",
        "Wei Dong",
        "Yang Gao"
      ],
      "abstract": "Large Language Models (LLMs) have made significant progress in various\nfields. However, challenges remain in Multi-Disciplinary Team (MDT) medical\nconsultations. Current research enhances reasoning through role assignment,\ntask decomposition, and accumulation of medical experience. Multi-role\ncollaboration in MDT consultations often results in excessively long dialogue\nhistories. This increases the model's cognitive burden and degrades both\nefficiency and accuracy. Some methods only store treatment histories. They do\nnot extract effective experience or reflect on errors. This limits knowledge\ngeneralization and system evolution. We propose a multi-agent MDT medical\nconsultation framework based on LLMs to address these issues. Our framework\nuses consensus aggregation and a residual discussion structure for multi-round\nconsultations. It also employs a Correct Answer Knowledge Base (CorrectKB) and\na Chain-of-Thought Knowledge Base (ChainKB) to accumulate consultation\nexperience. These mechanisms enable the framework to evolve and continually\nimprove diagnosis rationality and accuracy. Experimental results on the MedQA\nand PubMedQA datasets demonstrate that our framework achieves accuracies of\n90.1% and 83.9%, respectively, and that the constructed knowledge bases\ngeneralize effectively across test sets from both datasets.",
      "tldr_zh": "该研究提出MDTeamGPT框架，一种基于大语言模型(LLM)的自主演化多智能体系统，用于解决多学科团队(MDT)会诊中的认知负荷和知识积累问题。该系统通过共识聚合机制和残差讨论结构处理多轮会诊，并创新性地建立正确答案知识库(CorrectKB)和思维链知识库(ChainKB)实现经验积累与错误反思。实验表明，该框架在MedQA和PubMedQA数据集上分别达到90.1%和83.9%的准确率，且构建的知识库展现出良好的跨数据集泛化能力，显著提升了诊断合理性与准确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13856v1",
      "published_date": "2025-03-18 03:07:34 UTC",
      "updated_date": "2025-03-18 03:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:28:34.610103"
    },
    {
      "arxiv_id": "2503.13847v1",
      "title": "Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic",
      "title_zh": "基于混合马尔可夫逻辑解耦视觉描述任务中的预训练与微调过程",
      "authors": [
        "Monika Shah",
        "Somdeb Sarkhel",
        "Deepak Venugopal"
      ],
      "abstract": "Multimodal systems have highly complex processing pipelines and are\npretrained over large datasets before being fine-tuned for specific tasks such\nas visual captioning. However, it becomes hard to disentangle what the model\nlearns during the fine-tuning process from what it already knows due to its\npretraining. In this work, we learn a probabilistic model using Hybrid Markov\nLogic Networks (HMLNs) over the training examples by relating symbolic\nknowledge (extracted from the caption) with visual features (extracted from the\nimage). For a generated caption, we quantify the influence of training examples\nbased on the HMLN distribution using probabilistic inference. We evaluate two\ntypes of inference procedures on the MSCOCO dataset for different types of\ncaptioning models. Our results show that for BLIP2 (a model that uses a LLM),\nthe fine-tuning may have smaller influence on the knowledge the model has\nacquired since it may have more general knowledge to perform visual captioning\nas compared to models that do not use a LLM",
      "tldr_zh": "该研究提出基于混合马尔可夫逻辑网络(Hybrid Markov Logic Networks)的概率模型，用于量化分析视觉描述任务中预训练与微调阶段的贡献差异。通过将图像视觉特征与字幕符号知识关联建模，研究在MSCOCO数据集上验证发现：采用大语言模型(如BLIP2)的视觉描述系统，其微调过程对模型知识影响较小，因其预训练已具备更通用的视觉描述能力；而未使用大语言模型的系统则更依赖微调过程获取特定知识。该工作为解耦多模态系统中预训练与微调的作用提供了可解释性框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 IEEE International Conference on Big Data (BigData), 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13847v1",
      "published_date": "2025-03-18 02:39:26 UTC",
      "updated_date": "2025-03-18 02:39:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:28:55.658921"
    },
    {
      "arxiv_id": "2503.13844v1",
      "title": "Spotting Persuasion: A Low-cost Model for Persuasion Detection in Political Ads on Social Media",
      "title_zh": "识别说服策略：社交媒体政治广告中低成本的说服检测模型",
      "authors": [
        "Elyas Meguellati",
        "Stefano Civelli",
        "Pietro Bernardelle",
        "Shazia Sadiq",
        "Gianluca Demartini"
      ],
      "abstract": "In the realm of political advertising, persuasion operates as a pivotal\nelement within the broader framework of propaganda, exerting profound\ninfluences on public opinion and electoral outcomes. In this paper, we (1)\nintroduce a lightweight model for persuasive text detection that achieves\nstate-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while\nsignificantly reducing the computational resource requirements; and (2)\nleverage the proposed model to gain insights into political campaigning\nstrategies on social media platforms by applying it to a real-world dataset we\ncurated, consisting of Facebook political ads from the 2022 Australian Federal\nelection campaign. Our study shows how subtleties can be found in persuasive\npolitical advertisements and presents a pragmatic approach to detect and\nanalyze such strategies with limited resources, enhancing transparency in\nsocial media political campaigns.",
      "tldr_zh": "本研究提出了一种轻量级模型，用于检测社交媒体政治广告中的说服性文本，在 SemEval 2023 任务 3 的子任务 3 中取得了最先进的性能，同时显著降低了计算资源需求。研究还利用该模型分析了 2022 年澳大利亚联邦选举期间 Facebook 政治广告的真实数据集，揭示了说服性政治广告中的微妙策略，并提出了一种在有限资源下检测和分析此类策略的实用方法，增强了社交媒体政治活动的透明度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13844v1",
      "published_date": "2025-03-18 02:33:38 UTC",
      "updated_date": "2025-03-18 02:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:29:33.292571"
    },
    {
      "arxiv_id": "2503.13843v1",
      "title": "WebNav: An Intelligent Agent for Voice-Controlled Web Navigation",
      "title_zh": "WebNav：语音控制网页导航智能代理",
      "authors": [
        "Trisanth Srinivasan",
        "Santosh Patapati"
      ],
      "abstract": "The increasing reliance on web interfaces presents many challenges for\nvisually impaired users, showcasing the need for more advanced assistive\ntechnologies. This paper introduces WebNav, a voice-controlled web navigation\nagent that leverages a ReAct-inspired architecture and generative AI to provide\nthis framework. WebNav comprises of a hierarchical structure: a Digital\nNavigation Module (DIGNAV) for high-level strategic planning, an Assistant\nModule for translating abstract commands into executable actions, and an\nInference Module for low-level interaction. A key component is a dynamic\nlabeling engine, implemented as a browser extension, that generates real-time\nlabels for interactive elements, creating mapping between voice commands and\nDocument Object Model (DOM) components. Preliminary evaluations show that\nWebNav outperforms traditional screen readers in response time and task\ncompletion accuracy for the visually impaired. Future work will focus on\nextensive user evaluations, benchmark development, and refining the agent's\nadaptive capabilities for real-world deployment.",
      "tldr_zh": "该研究开发了WebNav语音控制网页导航智能代理，采用ReAct架构和生成式AI技术，帮助视障用户更高效地浏览网页。系统包含三层架构：负责战略规划的DIGNAV数字导航模块、指令转换的助手模块和交互推理模块，其核心创新是动态标签引擎，能实时生成可交互元素的语音-DOM映射。初步测试表明，WebNav在响应速度和任务完成准确率上均优于传统屏幕阅读器。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "H.5.2; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13843v1",
      "published_date": "2025-03-18 02:33:27 UTC",
      "updated_date": "2025-03-18 02:33:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:29:38.710422"
    },
    {
      "arxiv_id": "2503.13842v1",
      "title": "Counterfactual experience augmented off-policy reinforcement learning",
      "title_zh": "反事实经验增强的非策略强化学习",
      "authors": [
        "Sunbowen Lee",
        "Yicheng Gong",
        "Chao Deng"
      ],
      "abstract": "Reinforcement learning control algorithms face significant challenges due to\nout-of-distribution and inefficient exploration problems. While model-based\nreinforcement learning enhances the agent's reasoning and planning capabilities\nby constructing virtual environments, training such virtual environments can be\nvery complex. In order to build an efficient inference model and enhance the\nrepresentativeness of learning data, we propose the Counterfactual Experience\nAugmentation (CEA) algorithm. CEA leverages variational autoencoders to model\nthe dynamic patterns of state transitions and introduces randomness to model\nnon-stationarity. This approach focuses on expanding the learning data in the\nexperience pool through counterfactual inference and performs exceptionally\nwell in environments that follow the bisimulation assumption. Environments with\nbisimulation properties are usually represented by discrete observation and\naction spaces, we propose a sampling method based on maximum kernel density\nestimation entropy to extend CEA to various environments. By providing reward\nsignals for counterfactual state transitions based on real information, CEA\nconstructs a complete counterfactual experience to alleviate the\nout-of-distribution problem of the learning data, and outperforms general SOTA\nalgorithms in environments with difference properties. Finally, we discuss the\nsimilarities, differences and properties of generated counterfactual\nexperiences and real experiences. The code is available at\nhttps://github.com/Aegis1863/CEA.",
      "tldr_zh": "本文提出反事实经验增强（CEA）算法，通过变分自编码器建模状态转移动态模式并引入随机性处理非平稳性，有效缓解强化学习中的分布偏移问题。该方法基于最大核密度估计熵的采样策略，通过反事实推理扩展经验池数据，在满足双模拟假设的环境中表现优异。实验表明，CEA能为反事实状态转移提供真实奖励信号，构建完整反事实经验，在多种环境中超越现有SOTA算法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Neurocomputing,\n  https://doi.org/10.1016/j.neucom.2025.130017",
      "pdf_url": "http://arxiv.org/pdf/2503.13842v1",
      "published_date": "2025-03-18 02:32:50 UTC",
      "updated_date": "2025-03-18 02:32:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:29:59.822075"
    },
    {
      "arxiv_id": "2503.13836v1",
      "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing",
      "title_zh": "SALAD：面向文本驱动动作生成与编辑的骨骼感知潜在扩散模型",
      "authors": [
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Serin Yoon",
        "Junghyun Nam",
        "Sihun Cha",
        "Junyong Noh"
      ],
      "abstract": "Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page.",
      "tldr_zh": "该研究提出了SALAD（Skeleton-aware Latent Diffusion）模型，通过显式建模骨骼关节、时间帧和文本词汇之间的复杂交互关系，改进了基于文本的运动生成方法。该模型利用生成过程中的交叉注意力图，实现了无需微调或人工干预的零样本文本驱动运动编辑功能。实验表明，SALAD在保持生成质量的同时显著提升了文本-运动对齐性能，并扩展了传统生成模型的功能边界。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025; Project page\n  https://seokhyeonhong.github.io/projects/salad/",
      "pdf_url": "http://arxiv.org/pdf/2503.13836v1",
      "published_date": "2025-03-18 02:20:11 UTC",
      "updated_date": "2025-03-18 02:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:30:14.581262"
    },
    {
      "arxiv_id": "2503.14557v1",
      "title": "Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles",
      "title_zh": "生成具有学习奖励特征的车辆智能体行为交互因果解释",
      "authors": [
        "Rhys Howard",
        "Nick Hawes",
        "Lars Kunze"
      ],
      "abstract": "Transparency and explainability are important features that responsible\nautonomous vehicles should possess, particularly when interacting with humans,\nand causal reasoning offers a strong basis to provide these qualities. However,\neven if one assumes agents act to maximise some concept of reward, it is\ndifficult to make accurate causal inferences of agent planning without\ncapturing what is of importance to the agent. Thus our work aims to learn a\nweighting of reward metrics for agents such that explanations for agent\ninteractions can be causally inferred. We validate our approach quantitatively\nand qualitatively across three real-world driving datasets, demonstrating a\nfunctional improvement over previous methods and competitive performance across\nevaluation metrics.",
      "tldr_zh": "本研究提出了一种通过学习车辆智能体奖励函数权重来生成因果解释的方法，旨在提升自动驾驶系统与人类交互时的透明度和可解释性。该方法通过建模智能体的奖励偏好来准确推断其决策因果，克服了传统方法难以捕捉智能体关键目标的局限。在三个真实驾驶数据集上的实验表明，该方法不仅功能表现优于现有技术，而且在各项评估指标中具有竞争力，为自动驾驶系统的可信解释提供了新思路。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "I.2.0; I.2.6; I.2.9; I.2.11; I.6.0"
      ],
      "primary_category": "cs.AI",
      "comment": "8 Pages, 5 Figures, To be published in the Proceedings of the 2025\n  IEEE International Conference on Robotics & Automation, Initial upload of\n  accepted paper",
      "pdf_url": "http://arxiv.org/pdf/2503.14557v1",
      "published_date": "2025-03-18 01:53:59 UTC",
      "updated_date": "2025-03-18 01:53:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:30:32.809591"
    },
    {
      "arxiv_id": "2503.13817v1",
      "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences",
      "title_zh": "VARP：基于视觉语言模型反馈与智能体正则化偏好的强化学习",
      "authors": [
        "Anukriti Singh",
        "Amisha Bhaskar",
        "Peihong Yu",
        "Souradip Chakraborty",
        "Ruthwik Dasyam",
        "Amrit Bedi",
        "Pratap Tokekar"
      ],
      "abstract": "Designing reward functions for continuous-control robotics often leads to\nsubtle misalignments or reward hacking, especially in complex tasks.\nPreference-based RL mitigates some of these pitfalls by learning rewards from\ncomparative feedback rather than hand-crafted signals, yet scaling human\nannotations remains challenging. Recent work uses Vision-Language Models (VLMs)\nto automate preference labeling, but a single final-state image generally fails\nto capture the agent's full motion. In this paper, we present a two-part\nsolution that both improves feedback accuracy and better aligns reward learning\nwith the agent's policy. First, we overlay trajectory sketches on final\nobservations to reveal the path taken, allowing VLMs to provide more reliable\npreferences-improving preference accuracy by approximately 15-20% in metaworld\ntasks. Second, we regularize reward learning by incorporating the agent's\nperformance, ensuring that the reward model is optimized based on data\ngenerated by the current policy; this addition boosts episode returns by 20-30%\nin locomotion tasks. Empirical studies on metaworld demonstrate that our method\nachieves, for instance, around 70-80% success rate in all tasks, compared to\nbelow 50% for standard approaches. These results underscore the efficacy of\ncombining richer visual representations with agent-aware reward regularization.",
      "tldr_zh": "该研究提出VARP方法，通过视觉语言模型(VLM)反馈和智能体正则化偏好来解决连续控制机器人任务中的奖励函数设计难题。创新性地在最终观测图像上叠加轨迹草图，使VLM反馈准确率提升15-20%；同时引入智能体性能正则化机制，在运动任务中将回报提升20-30%。实验显示该方法在metaworld任务中成功率可达70-80%，远超传统方法50%以下的水平。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13817v1",
      "published_date": "2025-03-18 01:51:27 UTC",
      "updated_date": "2025-03-18 01:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:30:55.628035"
    },
    {
      "arxiv_id": "2503.13813v1",
      "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models",
      "title_zh": "基于大型语言模型的多机器人任务分配与调度自动MILP模型构建",
      "authors": [
        "Mingming Peng",
        "Zhendong Chen",
        "Jie Yang",
        "Jin Huang",
        "Zhengqi Shi",
        "Qihao Liu",
        "Xinyu Li",
        "Liang Gao"
      ],
      "abstract": "With the accelerated development of Industry 4.0, intelligent manufacturing\nsystems increasingly require efficient task allocation and scheduling in\nmulti-robot systems. However, existing methods rely on domain expertise and\nface challenges in adapting to dynamic production constraints. Additionally,\nenterprises have high privacy requirements for production scheduling data,\nwhich prevents the use of cloud-based large language models (LLMs) for solution\ndevelopment. To address these challenges, there is an urgent need for an\nautomated modeling solution that meets data privacy requirements. This study\nproposes a knowledge-augmented mixed integer linear programming (MILP)\nautomated formulation framework, integrating local LLMs with domain-specific\nknowledge bases to generate executable code from natural language descriptions\nautomatically. The framework employs a knowledge-guided\nDeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal\nconstraints (82% average accuracy) and leverages a supervised fine-tuned\nQwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average\naccuracy). Experimental results demonstrate that the framework successfully\nachieves automatic modeling in the aircraft skin manufacturing case while\nensuring data privacy and computational efficiency. This research provides a\nlow-barrier and highly reliable technical path for modeling in complex\nindustrial scenarios.",
      "tldr_zh": "本研究提出了一种基于大语言模型(LLM)的混合整数线性规划(MILP)自动建模框架，用于解决多机器人任务分配和调度问题。该方法结合本地化LLM和领域知识库，能从自然语言描述自动生成可执行代码，其中DeepSeek-R1-Distill-Qwen-32B模型用于提取时空约束(82%准确率)，Qwen2.5-Coder-7B-Instruct模型负责MILP代码生成(90%准确率)。实验表明，该框架在飞机蒙皮制造案例中实现了隐私保护下的自动化建模，为复杂工业场景提供了低门槛、高可靠性的解决方案。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13813v1",
      "published_date": "2025-03-18 01:45:19 UTC",
      "updated_date": "2025-03-18 01:45:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:31:20.976402"
    },
    {
      "arxiv_id": "2503.13812v1",
      "title": "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations",
      "title_zh": "空缺之席：利用大语言模型引入政策审议中缺失的视角",
      "authors": [
        "Suyash Fulay",
        "Deb Roy"
      ],
      "abstract": "Deliberation is essential to well-functioning democracies, yet physical,\neconomic, and social barriers often exclude certain groups, reducing\nrepresentativeness and contributing to issues like group polarization. In this\nwork, we explore the use of large language model (LLM) personas to introduce\nmissing perspectives in policy deliberations. We develop and evaluate a tool\nthat transcribes conversations in real-time and simulates input from relevant\nbut absent stakeholders. We deploy this tool in a 19-person student citizens'\nassembly on campus sustainability. Participants and facilitators found that the\ntool sparked new discussions and surfaced valuable perspectives they had not\npreviously considered. However, they also noted that AI-generated responses\nwere sometimes overly general. They raised concerns about overreliance on AI\nfor perspective-taking. Our findings highlight both the promise and potential\nrisks of using LLMs to raise missing points of view in group deliberation\nsettings.",
      "tldr_zh": "该研究探讨了利用大语言模型(LLMs)生成虚拟角色(personas)来弥补政策审议中缺失视角的方法。研究者开发了一个实时转录对话并模拟缺席利益相关者观点的工具，并在19人校园可持续发展公民议会中进行了测试。结果表明，该工具能有效激发新讨论并提供有价值的新视角，但也存在AI生成内容过于泛化、可能造成过度依赖等问题，揭示了LLMs用于群体决策辅助的双面性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13812v1",
      "published_date": "2025-03-18 01:45:08 UTC",
      "updated_date": "2025-03-18 01:45:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:31:36.619277"
    },
    {
      "arxiv_id": "2503.13806v1",
      "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt Engineering",
      "title_zh": "《基于文本提示工程的器官感知多尺度医学图像分割》",
      "authors": [
        "Wenjie Zhang",
        "Ziyang Zhang",
        "Mengnan He",
        "Jiancheng Ye"
      ],
      "abstract": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.",
      "tldr_zh": "本研究提出OMT-SAM模型，通过文本提示工程实现多尺度医学图像分割。该方法创新性地将CLIP编码器与几何提示编码器结合，利用文本描述引导模型识别细微解剖结构，同时提取MedSAM的多尺度视觉特征。在FLARE 2021数据集上的实验表明，该模型平均Dice系数达0.937，显著优于MedSAM（0.893）等现有方法，有效解决了多器官交织的医学图像分割难题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13806v1",
      "published_date": "2025-03-18 01:35:34 UTC",
      "updated_date": "2025-03-18 01:35:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:31:52.931281"
    },
    {
      "arxiv_id": "2503.13804v1",
      "title": "Empowering GraphRAG with Knowledge Filtering and Integration",
      "title_zh": "增强GraphRAG：基于知识过滤与整合的赋能框架",
      "authors": [
        "Kai Guo",
        "Harry Shomer",
        "Shenglai Zeng",
        "Haoyu Han",
        "Yu Wang",
        "Jiliang Tang"
      ],
      "abstract": "In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.",
      "tldr_zh": "该研究提出了GraphRAG-FI框架，通过知识过滤与整合增强GraphRAG的性能。针对GraphRAG存在的两个关键问题——检索噪声信息导致性能下降和过度依赖外部知识抑制模型内在推理能力，GraphRAG-FI设计了GraphRAG-Filtering和GraphRAG-Integration模块。前者采用两阶段过滤机制优化检索信息，后者通过基于logits的选择策略平衡外部知识与模型内在推理，减少对检索的过度依赖。实验表明，GraphRAG-FI在知识图谱问答任务中显著提升了多种骨干模型的推理性能，构建了更可靠有效的GraphRAG框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13804v1",
      "published_date": "2025-03-18 01:29:55 UTC",
      "updated_date": "2025-03-18 01:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:32:35.523702"
    },
    {
      "arxiv_id": "2503.13799v1",
      "title": "SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis",
      "title_zh": "SMILE：一种面向多中心STAS肺癌病理诊断的尺度感知多实例学习方法",
      "authors": [
        "Liangrui Pan",
        "Xiaoyu Li",
        "Yutao Dou",
        "Qiya Song",
        "Jiadi Luo",
        "Qingchun Liang",
        "Shaoliang Peng"
      ],
      "abstract": "Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps://anonymous.4open.science/r/IJCAI25-1DA1.",
      "tldr_zh": "该研究针对肺癌中通过气腔扩散（STAS）这一侵袭性病理模式的诊断难题，提出了基于尺度感知的多示例学习方法SMILE。通过构建来自多中心的2,970例肺癌组织切片数据集（STAS CSU/TCGA/CPTAC），并设计尺度自适应注意力机制，该方法有效解决了STAS病灶的异质性和稀疏性问题。实验表明，SMILE在CPTAC和TCGA数据集上分别诊断出251和319例STAS样本，诊断性能超越临床平均AUC水平，为计算病理学技术的临床应用提供了11项基准结果和开源数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13799v1",
      "published_date": "2025-03-18 01:09:52 UTC",
      "updated_date": "2025-03-18 01:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:32:58.361381"
    },
    {
      "arxiv_id": "2503.13798v1",
      "title": "AI-Powered Prediction of Nanoparticle Pharmacokinetics: A Multi-View Learning Approach",
      "title_zh": "AI驱动的纳米颗粒药代动力学预测：一种多视图学习方法",
      "authors": [
        "Amirhossein Khakpour",
        "Lucia Florescu",
        "Richard Tilley",
        "Haibo Jiang",
        "K. Swaminathan Iyer",
        "Gustavo Carneiro"
      ],
      "abstract": "The clinical translation of nanoparticle-based treatments remains limited due\nto the unpredictability of (nanoparticle) NP\npharmacokinetics$\\unicode{x2014}$how they distribute, accumulate, and clear\nfrom the body. Predicting these behaviours is challenging due to complex\nbiological interactions and the difficulty of obtaining high-quality\nexperimental datasets. Existing AI-driven approaches rely heavily on\ndata-driven learning but fail to integrate crucial knowledge about NP\nproperties and biodistribution mechanisms. We introduce a multi-view deep\nlearning framework that enhances pharmacokinetic predictions by incorporating\nprior knowledge of key NP properties such as size and charge into a\ncross-attention mechanism, enabling context-aware feature selection and\nimproving generalization despite small datasets. To further enhance prediction\nrobustness, we employ an ensemble learning approach, combining deep learning\nwith XGBoost (XGB) and Random Forest (RF), which significantly outperforms\nexisting AI models. Our interpretability analysis reveals key physicochemical\nproperties driving NP biodistribution, providing biologically meaningful\ninsights into possible mechanisms governing NP behaviour in vivo rather than a\nblack-box model. Furthermore, by bridging machine learning with physiologically\nbased pharmacokinetic (PBPK) modelling, this work lays the foundation for\ndata-efficient AI-driven drug discovery and precision nanomedicine.",
      "tldr_zh": "该研究提出了一种多视图深度学习框架，用于预测纳米颗粒（NP）的药代动力学行为，包括其在体内的分布、积累和清除。通过将纳米颗粒的关键属性（如大小和电荷）整合到跨注意力机制中，并结合集成学习方法（如XGBoost和随机森林），该模型显著优于现有AI模型，并在小数据集上表现出更强的泛化能力。此外，可解释性分析揭示了驱动纳米颗粒生物分布的关键理化特性，为纳米颗粒在体内的行为机制提供了生物学意义的见解。该研究为数据高效的AI驱动药物发现和精准纳米医学奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13798v1",
      "published_date": "2025-03-18 01:09:32 UTC",
      "updated_date": "2025-03-18 01:09:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:32:53.185984"
    },
    {
      "arxiv_id": "2503.13794v1",
      "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation",
      "title_zh": "LED：无需人工标注数据生成的LLM增强型开放词汇目标检测",
      "authors": [
        "Yang Zhou",
        "Shiyu Zhao",
        "Yuxiao Chen",
        "Zhenting Wang",
        "Dimitris N. Metaxas"
      ],
      "abstract": "Large foundation models trained on large-scale visual-text data can\nsignificantly enhance Open Vocabulary Object Detection (OVD) through data\ngeneration. However, this may lead to biased synthetic data and overfitting to\nspecific configurations. It can sidestep biases of manually curated data\ngeneration by directly leveraging hidden states of Large Language Models\n(LLMs), which is surprisingly rarely explored. This paper presents a systematic\nmethod to enhance visual grounding by utilizing decoder layers of the LLM of a\nMLLM. We introduce a zero-initialized cross-attention adapter to enable\nefficient knowledge transfer from LLMs to object detectors, an new approach\ncalled LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that\nintermediate hidden states from early LLM layers retain strong spatial-semantic\ncorrelations that are beneficial to grounding tasks. Experiments show that our\nadaptation strategy significantly enhances the performance on complex free-form\ntext queries while remaining the same on plain categories. With our adaptation,\nQwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on\nOmnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision\nencoder can further boost the performance by 6.22%. We further validate our\ndesign by ablating on varied adapter architectures, sizes of LLMs, and which\nlayers to add adaptation.",
      "tldr_zh": "该研究提出LED（LLM增强开放词汇目标检测）方法，创新性地利用多模态大语言模型（MLLM）的中间隐藏状态来增强视觉定位任务，避免了传统人工生成数据带来的偏差问题。通过引入零初始化交叉注意力适配器，该方法实现了从LLM到目标检测器的高效知识迁移，其中早期LLM层的隐藏状态展现出对定位任务尤为关键的空间-语义关联性。实验表明，该方法在复杂自由文本查询任务上性能显著提升（如Qwen2-0.5B模型在Omnilabel基准上提升2.33%），同时保持对普通类别的检测性能，且计算开销仅增加8.7% GFLOPs。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13794v1",
      "published_date": "2025-03-18 00:50:40 UTC",
      "updated_date": "2025-03-18 00:50:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:33:37.486912"
    },
    {
      "arxiv_id": "2503.13793v1",
      "title": "Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives",
      "title_zh": "绘制信任版图：LLMs在软件工程中的应用——见解与视角",
      "authors": [
        "Dipin Khati",
        "Yijin Liu",
        "David N. Palacio",
        "Yixuan Zhang",
        "Denys Poshyvanyk"
      ],
      "abstract": "Applications of Large Language Models (LLMs) are rapidly growing in industry\nand academia for various software engineering (SE) tasks. As these models\nbecome more integral to critical processes, ensuring their reliability and\ntrustworthiness becomes essential. Consequently, the concept of trust in these\nsystems is becoming increasingly critical. Well-calibrated trust is important,\nas excessive trust can lead to security vulnerabilities, and risks, while\ninsufficient trust can hinder innovation. However, the landscape of\ntrust-related concepts in LLMs in SE is relatively unclear, with concepts such\nas trust, distrust, and trustworthiness lacking clear conceptualizations in the\nSE community. To bring clarity to the current research status and identify\nopportunities for future work, we conducted a comprehensive review of $88$\npapers: a systematic literature review of $18$ papers focused on LLMs in SE,\ncomplemented by an analysis of 70 papers from broader trust literature.\nAdditionally, we conducted a survey study with 25 domain experts to gain\ninsights into practitioners' understanding of trust and identify gaps between\nexisting literature and developers' perceptions. The result of our analysis\nserves as a roadmap that covers trust-related concepts in LLMs in SE and\nhighlights areas for future exploration.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在软件工程(SE)领域应用中的信任问题，揭示了当前SE社区对信任(trust)、不信任(distrust)和可信度(trustworthiness)等概念缺乏明确定义。通过对88篇论文的系统性文献综述(包括18篇LLMs在SE领域的专项研究)和25位领域专家的调查，研究构建了LLMs在SE中的信任概念图谱，识别了现有文献与开发者认知之间的差距。研究成果为平衡LLMs的可靠性与创新性提供了路线图，既防止过度信任导致安全风险，又避免信任不足阻碍技术发展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13793v1",
      "published_date": "2025-03-18 00:49:43 UTC",
      "updated_date": "2025-03-18 00:49:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:33:33.753354"
    },
    {
      "arxiv_id": "2503.14556v1",
      "title": "Designing and Deploying AI Models for Sustainable Logistics Optimization: A Case Study on Eco-Efficient Supply Chains in the USA",
      "title_zh": "设计与部署可持续物流优化的人工智能模型：美国生态高效供应链案例研究",
      "authors": [
        "Reza E Rabbi Shawon",
        "MD Rokibul Hasan",
        "Md Anisur Rahman",
        "Mohamed Ghandri",
        "Iman Ahmed Lamari",
        "Mohammed Kawsar",
        "Rubi Akter"
      ],
      "abstract": "The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)\nhas significantly transformed logistics and supply chain management,\nparticularly in the pursuit of sustainability and eco-efficiency. This study\nexplores AI-based methodologies for optimizing logistics operations in the USA,\nfocusing on reducing environmental impact, improving fuel efficiency, and\nminimizing costs. Key AI applications include predictive analytics for demand\nforecasting, route optimization through machine learning, and AI-powered fuel\nefficiency strategies. Various models, such as Linear Regression, XGBoost,\nSupport Vector Machine, and Neural Networks, are applied to real-world\nlogistics datasets to reduce carbon emissions based on logistics operations,\noptimize travel routes to minimize distance and travel time, and predict future\ndeliveries to plan optimal routes. Other models such as K-Means and DBSCAN are\nalso used to optimize travel routes to minimize distance and travel time for\nlogistics operations. This study utilizes datasets from logistics companies'\ndatabases. The study also assesses model performance using metrics such as mean\nabsolute error (MAE), mean squared error (MSE), and R2 score. This study also\nexplores how these models can be deployed to various platforms for real-time\nlogistics and supply chain use. The models are also examined through a thorough\ncase study, highlighting best practices and regulatory frameworks that promote\nsustainability. The findings demonstrate AI's potential to enhance logistics\nefficiency, reduce carbon footprints, and contribute to a more resilient and\nadaptive supply chain ecosystem.",
      "tldr_zh": "本研究探讨了人工智能（AI）和机器学习（ML）在美国物流优化中的应用，重点关注减少环境影响、提高燃油效率和降低成本。研究采用了多种AI模型，包括线性回归、XGBoost、支持向量机和神经网络，应用于真实物流数据集，以实现碳减排、优化旅行路线和预测未来交付。此外，研究还通过K-Means和DBSCAN等模型优化物流操作中的距离和旅行时间。研究评估了模型性能，并探讨了如何将这些模型部署到实时物流和供应链平台。结果表明，AI在提高物流效率、减少碳足迹和构建更具弹性的供应链生态系统方面具有显著潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14556v1",
      "published_date": "2025-03-18 00:46:35 UTC",
      "updated_date": "2025-03-18 00:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:33:57.249004"
    },
    {
      "arxiv_id": "2503.13786v1",
      "title": "Evaluating the Application of SOLID Principles in Modern AI Framework Architectures",
      "title_zh": "评估SOLID原则在现代AI框架架构中的应用",
      "authors": [
        "Jonesh Shrestha"
      ],
      "abstract": "This research evaluates the extent to which modern AI frameworks,\nspecifically TensorFlow and scikit-learn, adhere to the SOLID design principles\n- Single Responsibility, Open/Closed, Liskov Substitution, Interface\nSegregation, and Dependency Inversion. Analyzing the frameworks architectural\ndocumentation and design philosophies, this research investigates architectural\ntrade-offs when balancing software engineering best practices with AI-specific\nneeds. I examined each frameworks documentation, source code, and architectural\ncomponents to evaluate their adherence to these principles. The results show\nthat both frameworks adopt certain aspects of SOLID design principles but make\nintentional trade-offs to address performance, scalability, and the\nexperimental nature of AI development. TensorFlow focuses on performance and\nscalability, sometimes sacrificing strict adherence to principles like Single\nResponsibility and Interface Segregation. While scikit-learns design philosophy\naligns more closely with SOLID principles through consistent interfaces and\ncomposition principles, sticking closer to SOLID guidelines but with occasional\ndeviations for performance optimizations and scalability. This research\ndiscovered that applying SOLID principles in AI frameworks depends on context,\nas performance, scalability, and flexibility often require deviations from\ntraditional software engineering principles. This research contributes to\nunderstanding how domain-specific constraints influence architectural decisions\nin modern AI frameworks and how these frameworks strategically adapted design\nchoices to effectively balance these contradicting requirements.",
      "tldr_zh": "本研究评估了现代AI框架（如TensorFlow和scikit-learn）对SOLID设计原则（单一职责、开闭原则、里氏替换、接口隔离和依赖倒置）的遵循程度。通过分析框架的架构文档、设计理念和源代码，研究发现，尽管两者部分采纳了SOLID原则，但为了满足性能、可扩展性和AI实验性开发的需求，均做出了有意权衡。TensorFlow更注重性能和扩展性，有时牺牲了单一职责和接口隔离的严格遵循；而scikit-learn则更贴近SOLID原则，但偶尔也会因性能优化和扩展性需求而偏离。研究表明，AI框架中应用SOLID原则需结合具体场景，性能、扩展性和灵活性往往需要与传统软件工程原则进行权衡。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "68N19, 68T01",
        "D.2.11; I.2.0"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, 1 figure, 12 references",
      "pdf_url": "http://arxiv.org/pdf/2503.13786v1",
      "published_date": "2025-03-18 00:37:23 UTC",
      "updated_date": "2025-03-18 00:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T16:34:24.206012"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 137,
  "processed_papers_count": 137,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T16:35:55.451336"
}
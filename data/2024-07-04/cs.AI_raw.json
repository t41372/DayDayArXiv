[
  {
    "arxiv_id": "2407.04183v3",
    "title": "Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms",
    "authors": [
      "Joshua Ashkinaze",
      "Ruijia Guan",
      "Laura Kurek",
      "Eytan Adar",
      "Ceren Budak",
      "Eric Gilbert"
    ],
    "abstract": "Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04183v3",
    "published_date": "2024-07-04 23:05:58 UTC",
    "updated_date": "2024-09-14 21:28:20 UTC"
  },
  {
    "arxiv_id": "2407.04181v1",
    "title": "Orchestrating LLMs with Different Personalizations",
    "authors": [
      "Jin Peng Zhou",
      "Katie Z Luo",
      "Jingwen Gu",
      "Jason Yuan",
      "Kilian Q. Weinberger",
      "Wen Sun"
    ],
    "abstract": "This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04181v1",
    "published_date": "2024-07-04 22:55:02 UTC",
    "updated_date": "2024-07-04 22:55:02 UTC"
  },
  {
    "arxiv_id": "2407.04173v1",
    "title": "Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs",
    "authors": [
      "Faisal Hamman",
      "Pasan Dissanayake",
      "Saumitra Mishra",
      "Freddy Lecue",
      "Sanghamitra Dutta"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on limited tabular data for\nclassification tasks can lead to \\textit{fine-tuning multiplicity}, where\nequally well-performing models make conflicting predictions on the same inputs\ndue to variations in the training process (i.e., seed, random weight\ninitialization, retraining on additional or deleted samples). This raises\ncritical concerns about the robustness and reliability of Tabular LLMs,\nparticularly when deployed for high-stakes decision-making, such as finance,\nhiring, education, healthcare, etc. This work formalizes the challenge of\nfine-tuning multiplicity in Tabular LLMs and proposes a novel metric to\nquantify the robustness of individual predictions without expensive model\nretraining. Our metric quantifies a prediction's stability by analyzing\n(sampling) the model's local behavior around the input in the embedding space.\nInterestingly, we show that sampling in the local neighborhood can be leveraged\nto provide probabilistic robustness guarantees against a broad class of\nfine-tuned models. By leveraging Bernstein's Inequality, we show that\npredictions with sufficiently high robustness (as defined by our measure) will\nremain consistent with high probability. We also provide empirical evaluation\non real-world datasets to support our theoretical results. Our work highlights\nthe importance of addressing fine-tuning instabilities to enable trustworthy\ndeployment of LLMs in high-stakes and safety-critical applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04173v1",
    "published_date": "2024-07-04 22:22:09 UTC",
    "updated_date": "2024-07-04 22:22:09 UTC"
  },
  {
    "arxiv_id": "2407.04172v2",
    "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
    "authors": [
      "Ahmed Masry",
      "Megh Thakkar",
      "Aayush Bajaj",
      "Aaryaman Kartha",
      "Enamul Hoque",
      "Shafiq Joty"
    ],
    "abstract": "Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross $5$ benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04172v2",
    "published_date": "2024-07-04 22:16:40 UTC",
    "updated_date": "2024-11-04 04:59:45 UTC"
  },
  {
    "arxiv_id": "2407.04153v1",
    "title": "Mixture of A Million Experts",
    "authors": [
      "Xu Owen He"
    ],
    "abstract": "The feedforward (FFW) layers in standard transformer architectures incur a\nlinear increase in computational costs and activation memory as the hidden\nlayer width grows. Sparse mixture-of-experts (MoE) architectures have emerged\nas a viable approach to address this issue by decoupling model size from\ncomputational cost. The recent discovery of the fine-grained MoE scaling law\nshows that higher granularity leads to better performance. However, existing\nMoE models are limited to a small number of experts due to computational and\noptimization challenges. This paper introduces PEER (parameter efficient expert\nretrieval), a novel layer design that utilizes the product key technique for\nsparse retrieval from a vast pool of tiny experts (over a million). Experiments\non language modeling tasks demonstrate that PEER layers outperform dense FFWs\nand coarse-grained MoEs in terms of performance-compute trade-off. By enabling\nefficient utilization of a massive number of experts, PEER unlocks the\npotential for further scaling of transformer models while maintaining\ncomputational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04153v1",
    "published_date": "2024-07-04 20:59:20 UTC",
    "updated_date": "2024-07-04 20:59:20 UTC"
  },
  {
    "arxiv_id": "2407.04152v2",
    "title": "VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation",
    "authors": [
      "I-Chun Arthur Liu",
      "Sicheng He",
      "Daniel Seita",
      "Gaurav Sukhatme"
    ],
    "abstract": "Bimanual manipulation is critical to many robotics applications. In contrast\nto single-arm manipulation, bimanual manipulation tasks are challenging due to\nhigher-dimensional action spaces. Prior works leverage large amounts of data\nand primitive actions to address this problem, but may suffer from sample\ninefficiency and limited generalization across various tasks. To this end, we\npropose VoxAct-B, a language-conditioned, voxel-based method that leverages\nVision Language Models (VLMs) to prioritize key regions within the scene and\nreconstruct a voxel grid. We provide this voxel grid to our bimanual\nmanipulation policy to learn acting and stabilizing actions. This approach\nenables more efficient policy learning from voxels and is generalizable to\ndifferent tasks. In simulation, we show that VoxAct-B outperforms strong\nbaselines on fine-grained bimanual manipulation tasks. Furthermore, we\ndemonstrate VoxAct-B on real-world $\\texttt{Open Drawer}$ and $\\texttt{Open\nJar}$ tasks using two UR5s. Code, data, and videos are available at\nhttps://voxact-b.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the Conference on Robot Learning (CoRL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.04152v2",
    "published_date": "2024-07-04 20:58:20 UTC",
    "updated_date": "2024-10-06 01:13:25 UTC"
  },
  {
    "arxiv_id": "2407.04151v2",
    "title": "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers",
    "authors": [
      "Terry Tong",
      "Jiashu Xu",
      "Qin Liu",
      "Muhao Chen"
    ],
    "abstract": "Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.04151v2",
    "published_date": "2024-07-04 20:57:06 UTC",
    "updated_date": "2024-10-28 17:48:44 UTC"
  },
  {
    "arxiv_id": "2407.04127v3",
    "title": "Biometric Authentication Based on Enhanced Remote Photoplethysmography Signal Morphology",
    "authors": [
      "Zhaodong Sun",
      "Xiaobai Li",
      "Jukka Komulainen",
      "Guoying Zhao"
    ],
    "abstract": "Remote photoplethysmography (rPPG) is a non-contact method for measuring\ncardiac signals from facial videos, offering a convenient alternative to\ncontact photoplethysmography (cPPG) obtained from contact sensors. Recent\nstudies have shown that each individual possesses a unique cPPG signal\nmorphology that can be utilized as a biometric identifier, which has inspired\nus to utilize the morphology of rPPG signals extracted from facial videos for\nperson authentication. Since the facial appearance and rPPG are mixed in the\nfacial videos, we first de-identify facial videos to remove facial appearance\nwhile preserving the rPPG information, which protects facial privacy and\nguarantees that only rPPG is used for authentication. The de-identified videos\nare fed into an rPPG model to get the rPPG signal morphology for\nauthentication. In the first training stage, unsupervised rPPG training is\nperformed to get coarse rPPG signals. In the second training stage, an\nrPPG-cPPG hybrid training is performed by incorporating external cPPG datasets\nto achieve rPPG biometric authentication and enhance rPPG signal morphology.\nOur approach needs only de-identified facial videos with subject IDs to train\nrPPG authentication models. The experimental results demonstrate that rPPG\nsignal morphology hidden in facial videos can be used for biometric\nauthentication. The code is available at\nhttps://github.com/zhaodongsun/rppg_biometrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by IJCB 2024, Best Paper Runner-Up Award",
    "pdf_url": "http://arxiv.org/pdf/2407.04127v3",
    "published_date": "2024-07-04 19:00:34 UTC",
    "updated_date": "2024-11-27 13:47:03 UTC"
  },
  {
    "arxiv_id": "2407.04125v2",
    "title": "Query-Guided Self-Supervised Summarization of Nursing Notes",
    "authors": [
      "Ya Gao",
      "Hans Moen",
      "Saila Koivusalo",
      "Miika Koskinen",
      "Pekka Marttinen"
    ],
    "abstract": "Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04125v2",
    "published_date": "2024-07-04 18:54:30 UTC",
    "updated_date": "2024-12-02 09:42:24 UTC"
  },
  {
    "arxiv_id": "2407.04121v1",
    "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models",
    "authors": [
      "Yuyan Chen",
      "Qiang Fu",
      "Yichen Yuan",
      "Zhihao Wen",
      "Ge Fan",
      "Dayiheng Liu",
      "Dongmei Zhang",
      "Zhixu Li",
      "Yanghua Xiao"
    ],
    "abstract": "Large Language Models (LLMs) have gained widespread adoption in various\nnatural language processing tasks, including question answering and dialogue\nsystems. However, a major drawback of LLMs is the issue of hallucination, where\nthey generate unfaithful or inconsistent content that deviates from the input\nsource, leading to severe consequences. In this paper, we propose a robust\ndiscriminator named RelD to effectively detect hallucination in LLMs' generated\nanswers. RelD is trained on the constructed RelQA, a bilingual\nquestion-answering dialogue dataset along with answers generated by LLMs and a\ncomprehensive set of metrics. Our experimental results demonstrate that the\nproposed RelD successfully detects hallucination in the answers generated by\ndiverse LLMs. Moreover, it performs well in distinguishing hallucination in\nLLMs' generated answers from both in-distribution and out-of-distribution\ndatasets. Additionally, we also conduct a thorough analysis of the types of\nhallucinations that occur and present valuable insights. This research\nsignificantly contributes to the detection of reliable answers generated by\nLLMs and holds noteworthy implications for mitigating hallucination in the\nfuture work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to CIKM 2023 (Long Paper)",
    "pdf_url": "http://arxiv.org/pdf/2407.04121v1",
    "published_date": "2024-07-04 18:47:42 UTC",
    "updated_date": "2024-07-04 18:47:42 UTC"
  },
  {
    "arxiv_id": "2407.04118v1",
    "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
    "authors": [
      "Yuyan Chen",
      "Zhihao Wen",
      "Ge Fan",
      "Zhengyu Chen",
      "Wei Wu",
      "Dayiheng Liu",
      "Zhixu Li",
      "Bang Liu",
      "Yanghua Xiao"
    ],
    "abstract": "Prompt engineering, as an efficient and effective way to leverage Large\nLanguage Models (LLM), has drawn a lot of attention from the research\ncommunity. The existing research primarily emphasizes the importance of\nadapting prompts to specific tasks, rather than specific LLMs. However, a good\nprompt is not solely defined by its wording, but also binds to the nature of\nthe LLM in question. In this work, we first quantitatively demonstrate that\ndifferent prompts should be adapted to different LLMs to enhance their\ncapabilities across various downstream tasks in NLP. Then we novelly propose a\nmodel-adaptive prompt optimizer (MAPO) method that optimizes the original\nprompts for each specific LLM in downstream tasks. Extensive experiments\nindicate that the proposed method can effectively refine prompts for an LLM,\nleading to significant improvements over various downstream tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2023 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2407.04118v1",
    "published_date": "2024-07-04 18:39:59 UTC",
    "updated_date": "2024-07-04 18:39:59 UTC"
  },
  {
    "arxiv_id": "2407.04117v2",
    "title": "Predictive Coding Networks and Inference Learning: Tutorial and Survey",
    "authors": [
      "Björn van Zwol",
      "Ro Jefferson",
      "Egon L. van den Broek"
    ],
    "abstract": "Recent years have witnessed a growing call for renewed emphasis on\nneuroscience-inspired approaches in artificial intelligence research, under the\nbanner of NeuroAI. A prime example of this is predictive coding networks\n(PCNs), based on the neuroscientific framework of predictive coding. This\nframework views the brain as a hierarchical Bayesian inference model that\nminimizes prediction errors through feedback connections. Unlike traditional\nneural networks trained with backpropagation (BP), PCNs utilize inference\nlearning (IL), a more biologically plausible algorithm that explains patterns\nof neural activity that BP cannot. Historically, IL has been more\ncomputationally intensive, but recent advancements have demonstrated that it\ncan achieve higher efficiency than BP with sufficient parallelization.\nFurthermore, PCNs can be mathematically considered a superset of traditional\nfeedforward neural networks (FNNs), significantly extending the range of\ntrainable architectures. As inherently probabilistic (graphical) latent\nvariable models, PCNs provide a versatile framework for both supervised\nlearning and unsupervised (generative) modeling that goes beyond traditional\nartificial neural networks. This work provides a comprehensive review and\ndetailed formal specification of PCNs, particularly situating them within the\ncontext of modern ML methods. Additionally, we introduce a Python library\n(PRECO) for practical implementation. This positions PC as a promising\nframework for future ML innovations.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 13 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.04117v2",
    "published_date": "2024-07-04 18:39:20 UTC",
    "updated_date": "2024-07-22 14:56:46 UTC"
  },
  {
    "arxiv_id": "2407.04106v1",
    "title": "MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis",
    "authors": [
      "Asma Alkhaldi",
      "Raneem Alnajim",
      "Layan Alabdullatef",
      "Rawan Alyahya",
      "Jun Chen",
      "Deyao Zhu",
      "Ahmed Alsinan",
      "Mohamed Elhoseiny"
    ],
    "abstract": "Recent advancements in artificial intelligence (AI) have precipitated\nsignificant breakthroughs in healthcare, particularly in refining diagnostic\nprocedures. However, previous studies have often been constrained to limited\nfunctionalities. This study introduces MiniGPT-Med, a vision-language model\nderived from large-scale language models and tailored for medical applications.\nMiniGPT-Med demonstrates remarkable versatility across various imaging\nmodalities, including X-rays, CT scans, and MRIs, enhancing its utility. The\nmodel is capable of performing tasks such as medical report generation, visual\nquestion answering (VQA), and disease identification within medical imagery.\nIts integrated processing of both image and textual clinical data markedly\nimproves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's\nsuperior performance in disease grounding, medical report generation, and VQA\nbenchmarks, representing a significant step towards reducing the gap in\nassisting radiology practice. Furthermore, it achieves state-of-the-art\nperformance on medical report generation, higher than the previous best model\nby 19\\% accuracy. MiniGPT-Med promises to become a general interface for\nradiology diagnoses, enhancing diagnostic efficiency across a wide range of\nmedical imaging applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04106v1",
    "published_date": "2024-07-04 18:21:10 UTC",
    "updated_date": "2024-07-04 18:21:10 UTC"
  },
  {
    "arxiv_id": "2407.04105v1",
    "title": "Can Pre-trained Language Models Understand Chinese Humor?",
    "authors": [
      "Yuyan Chen",
      "Zhixu Li",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Bang Liu",
      "Yunwen Chen"
    ],
    "abstract": "Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to WSDM 2022",
    "pdf_url": "http://arxiv.org/pdf/2407.04105v1",
    "published_date": "2024-07-04 18:13:38 UTC",
    "updated_date": "2024-07-04 18:13:38 UTC"
  },
  {
    "arxiv_id": "2407.04103v2",
    "title": "Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions",
    "authors": [
      "Panagiotis Alimisis",
      "Ioannis Mademlis",
      "Panagiotis Radoglou-Grammatikis",
      "Panagiotis Sarigiannidis",
      "Georgios Th. Papadopoulos"
    ],
    "abstract": "Image data augmentation constitutes a critical methodology in modern computer\nvision tasks, since it can facilitate towards enhancing the diversity and\nquality of training datasets; thereby, improving the performance and robustness\nof machine learning models in downstream tasks. In parallel, augmentation\napproaches can also be used for editing/modifying a given image in a context-\nand semantics-aware way. Diffusion Models (DMs), which comprise one of the most\nrecent and highly promising classes of methods in the field of generative\nArtificial Intelligence (AI), have emerged as a powerful tool for image data\naugmentation, capable of generating realistic and diverse images by learning\nthe underlying data distribution. The current study realizes a systematic,\ncomprehensive and in-depth review of DM-based approaches for image\naugmentation, covering a wide range of strategies, tasks and applications. In\nparticular, a comprehensive analysis of the fundamental principles, model\narchitectures and training strategies of DMs is initially performed.\nSubsequently, a taxonomy of the relevant image augmentation methods is\nintroduced, focusing on techniques regarding semantic manipulation,\npersonalization and adaptation, and application-specific augmentation tasks.\nThen, performance assessment methodologies and respective evaluation metrics\nare analyzed. Finally, current challenges and future research directions in the\nfield are discussed.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "65 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.04103v2",
    "published_date": "2024-07-04 18:06:48 UTC",
    "updated_date": "2025-01-10 15:37:26 UTC"
  },
  {
    "arxiv_id": "2407.04088v1",
    "title": "Artificial Intelligence and Algorithmic Price Collusion in Two-sided Markets",
    "authors": [
      "Cristian Chica",
      "Yinglong Guo",
      "Gilad Lerman"
    ],
    "abstract": "Algorithmic price collusion facilitated by artificial intelligence (AI)\nalgorithms raises significant concerns. We examine how AI agents using\nQ-learning engage in tacit collusion in two-sided markets. Our experiments\nreveal that AI-driven platforms achieve higher collusion levels compared to\nBertrand competition. Increased network externalities significantly enhance\ncollusion, suggesting AI algorithms exploit them to maximize profits. Higher\nuser heterogeneity or greater utility from outside options generally reduce\ncollusion, while higher discount rates increase it. Tacit collusion remains\nfeasible even at low discount rates. To mitigate collusive behavior and inform\npotential regulatory measures, we propose incorporating a penalty term in the\nQ-learning algorithm.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.GT",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.04088v1",
    "published_date": "2024-07-04 17:57:56 UTC",
    "updated_date": "2024-07-04 17:57:56 UTC"
  },
  {
    "arxiv_id": "2407.04087v1",
    "title": "Advanced Artificial Intelligence Strategy for Optimizing Urban Rail Network Design using Nature-Inspired Algorithms",
    "authors": [
      "Hariram Sampath Kumar",
      "Archana Singh",
      "Manish Kumar Ojha"
    ],
    "abstract": "This study introduces an innovative methodology for the planning of metro\nnetwork routes within the urban environment of Chennai, Tamil Nadu, India. A\ncomparative analysis of the modified Ant Colony Optimization (ACO) method\n(previously developed) with recent breakthroughs in nature-inspired algorithms\ndemonstrates the modified ACO's superiority over modern techniques. By\nutilizing the modified ACO algorithm, the most efficient routes connecting the\norigin and destination of the metro route are generated. Additionally, the\nmodel is applied to the existing metro network to highlight variations between\nthe model's results and the current network. The Google Maps platform,\nintegrated with Python, handles real-time data, including land utilization,\nGeographical Information Systems (GIS) data, census information, and points of\ninterest. This processing enables the identification of stops within the city\nand along the chosen routes. The resulting metro network showcases substantial\nbenefits compared to conventional route planning methods, with noteworthy\nenhancements in workforce productivity, decreased planning time, and\ncost-efficiency. This study significantly enhances the efficiency of urban\ntransport systems, specifically in rapidly changing metropolitan settings such\nas chennai.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "10 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.04087v1",
    "published_date": "2024-07-04 17:57:39 UTC",
    "updated_date": "2024-07-04 17:57:39 UTC"
  },
  {
    "arxiv_id": "2407.04078v3",
    "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
    "authors": [
      "Chengpeng Li",
      "Guanting Dong",
      "Mingfeng Xue",
      "Ru Peng",
      "Xiang Wang",
      "Dayiheng Liu"
    ],
    "abstract": "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2407.04078v3",
    "published_date": "2024-07-04 17:39:16 UTC",
    "updated_date": "2024-07-17 13:13:05 UTC"
  },
  {
    "arxiv_id": "2407.04075v1",
    "title": "Sparsest Models Elude Pruning: An Exposé of Pruning's Current Capabilities",
    "authors": [
      "Stephen Zhang",
      "Vardan Papyan"
    ],
    "abstract": "Pruning has emerged as a promising approach for compressing large-scale\nmodels, yet its effectiveness in recovering the sparsest of models has not yet\nbeen explored. We conducted an extensive series of 485,838 experiments,\napplying a range of state-of-the-art pruning algorithms to a synthetic dataset\nwe created, named the Cubist Spiral. Our findings reveal a significant gap in\nperformance compared to ideal sparse networks, which we identified through a\nnovel combinatorial search algorithm. We attribute this performance gap to\ncurrent pruning algorithms' poor behaviour under overparameterization, their\ntendency to induce disconnected paths throughout the network, and their\npropensity to get stuck at suboptimal solutions, even when given the optimal\nwidth and initialization. This gap is concerning, given the simplicity of the\nnetwork architectures and datasets used in our study. We hope that our research\nencourages further investigation into new pruning techniques that strive for\ntrue network sparsity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Proceedings of the 41st International Conference on\n  Machine Learning",
    "pdf_url": "http://arxiv.org/pdf/2407.04075v1",
    "published_date": "2024-07-04 17:33:15 UTC",
    "updated_date": "2024-07-04 17:33:15 UTC"
  },
  {
    "arxiv_id": "2407.04070v1",
    "title": "Behavioural gap assessment of human-vehicle interaction in real and virtual reality-based scenarios in autonomous driving",
    "authors": [
      "Sergio. Martín Serrano",
      "Rubén Izquierdo",
      "Iván García Daza",
      "Miguel Ángel Sotelo",
      "D. Fernández Llorca"
    ],
    "abstract": "In the field of autonomous driving research, the use of immersive virtual\nreality (VR) techniques is widespread to enable a variety of studies under safe\nand controlled conditions. However, this methodology is only valid and\nconsistent if the conduct of participants in the simulated setting mirrors\ntheir actions in an actual environment. In this paper, we present a first and\ninnovative approach to evaluating what we term the behavioural gap, a concept\nthat captures the disparity in a participant's conduct when engaging in a VR\nexperiment compared to an equivalent real-world situation. To this end, we\ndeveloped a digital twin of a pre-existed crosswalk and carried out a field\nexperiment (N=18) to investigate pedestrian-autonomous vehicle interaction in\nboth real and simulated driving conditions. In the experiment, the pedestrian\nattempts to cross the road in the presence of different driving styles and an\nexternal Human-Machine Interface (eHMI). By combining survey-based and\nbehavioural analysis methodologies, we develop a quantitative approach to\nempirically assess the behavioural gap, as a mechanism to validate data\nobtained from real subjects interacting in a simulated VR-based environment.\nResults show that participants are more cautious and curious in VR, affecting\ntheir speed and decisions, and that VR interfaces significantly influence their\nactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Paper submitted to International Journal of Human Computer\n  Interaction",
    "pdf_url": "http://arxiv.org/pdf/2407.04070v1",
    "published_date": "2024-07-04 17:20:17 UTC",
    "updated_date": "2024-07-04 17:20:17 UTC"
  },
  {
    "arxiv_id": "2407.04069v2",
    "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations",
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Sawsan Alqahtani",
      "M Saiful Bari",
      "Mizanur Rahman",
      "Mohammad Abdullah Matin Khan",
      "Haidar Khan",
      "Israt Jahan",
      "Amran Bhuiyan",
      "Chee Wei Tan",
      "Md Rizwan Parvez",
      "Enamul Hoque",
      "Shafiq Joty",
      "Jimmy Huang"
    ],
    "abstract": "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2407.04069v2",
    "published_date": "2024-07-04 17:15:37 UTC",
    "updated_date": "2024-10-03 13:51:53 UTC"
  },
  {
    "arxiv_id": "2407.04055v1",
    "title": "Benchmark on Drug Target Interaction Modeling from a Structure Perspective",
    "authors": [
      "Xinnan Zhang",
      "Jialin Wu",
      "Junyi Xie",
      "Tianlong Chen",
      "Kaixiong Zhou"
    ],
    "abstract": "The prediction modeling of drug-target interactions is crucial to drug\ndiscovery and design, which has seen rapid advancements owing to deep learning\ntechnologies. Recently developed methods, such as those based on graph neural\nnetworks (GNNs) and Transformers, demonstrate exceptional performance across\nvarious datasets by effectively extracting structural information. However, the\nbenchmarking of these novel methods often varies significantly in terms of\nhyperparameter settings and datasets, which limits algorithmic progress. In\nview of these, we conduct a comprehensive survey and benchmark for drug-target\ninteraction modeling from a structure perspective, via integrating tens of\nexplicit (i.e., GNN-based) and implicit (i.e., Transformer-based) structure\nlearning algorithms. To this end, we first unify the hyperparameter setting\nwithin each class of structure learning methods. Moreover, we conduct a\nmacroscopical comparison between these two classes of encoding strategies as\nwell as the different featurization techniques that inform molecules' chemical\nand physical properties. We then carry out the microscopical comparison between\nall the integrated models across the six datasets, via comprehensively\nbenchmarking their effectiveness and efficiency. Remarkably, the summarized\ninsights from the benchmark studies lead to the design of model combos. We\ndemonstrate that our combos can achieve new state-of-the-art performance on\nvarious datasets associated with cost-effective memory and computation. Our\ncode is available at\n\\hyperlink{https://github.com/justinwjl/GTB-DTI/tree/main}{https://github.com/justinwjl/GTB-DTI/tree/main}.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Submitted to NIPS 2024 Dataset and Benchmark",
    "pdf_url": "http://arxiv.org/pdf/2407.04055v1",
    "published_date": "2024-07-04 16:56:59 UTC",
    "updated_date": "2024-07-04 16:56:59 UTC"
  },
  {
    "arxiv_id": "2407.04051v3",
    "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs",
    "authors": [
      "Keyu An",
      "Qian Chen",
      "Chong Deng",
      "Zhihao Du",
      "Changfeng Gao",
      "Zhifu Gao",
      "Yue Gu",
      "Ting He",
      "Hangrui Hu",
      "Kai Hu",
      "Shengpeng Ji",
      "Yabin Li",
      "Zerui Li",
      "Heng Lu",
      "Haoneng Luo",
      "Xiang Lv",
      "Bin Ma",
      "Ziyang Ma",
      "Chongjia Ni",
      "Changhe Song",
      "Jiaqi Shi",
      "Xian Shi",
      "Hao Wang",
      "Wen Wang",
      "Yuxuan Wang",
      "Zhangyu Xiao",
      "Zhijie Yan",
      "Yexin Yang",
      "Bin Zhang",
      "Qinglin Zhang",
      "Shiliang Zhang",
      "Nan Zhao",
      "Siqi Zheng"
    ],
    "abstract": "This report introduces FunAudioLLM, a model family designed to enhance\nnatural voice interactions between humans and large language models (LLMs). At\nits core are two innovative models: SenseVoice, which handles multilingual\nspeech recognition, emotion recognition, and audio event detection; and\nCosyVoice, which facilitates natural speech generation with control over\nmultiple languages, timbre, speaking style, and speaker identity.\nSenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and\nSenseVoice-Large supports high-precision ASR for over 50 languages, while\nCosyVoice excels in multi-lingual voice generation, zero-shot in-context\nlearning, cross-lingual voice cloning, and instruction-following capabilities.\nThe models related to SenseVoice and CosyVoice have been open-sourced on\nModelscope and Huggingface, along with the corresponding training, inference,\nand fine-tuning codes released on GitHub. By integrating these models with\nLLMs, FunAudioLLM enables applications such as speech-to-speech translation,\nemotional voice chat, interactive podcasts, and expressive audiobook narration,\nthereby pushing the boundaries of voice interaction technology. Demos are\navailable at https://fun-audio-llm.github.io, and the code can be accessed at\nhttps://github.com/FunAudioLLM.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Work in progress. Authors are listed in alphabetical order by family\n  name",
    "pdf_url": "http://arxiv.org/pdf/2407.04051v3",
    "published_date": "2024-07-04 16:49:02 UTC",
    "updated_date": "2024-07-11 02:08:35 UTC"
  },
  {
    "arxiv_id": "2407.04050v1",
    "title": "Deep Content Understanding Toward Entity and Aspect Target Sentiment Analysis on Foundation Models",
    "authors": [
      "Vorakit Vorakitphan",
      "Milos Basic",
      "Guilhaume Leroy Meline"
    ],
    "abstract": "Introducing Entity-Aspect Sentiment Triplet Extraction (EASTE), a novel\nAspect-Based Sentiment Analysis (ABSA) task which extends\nTarget-Aspect-Sentiment Detection (TASD) by separating aspect categories (e.g.,\nfood#quality) into pre-defined entities (e.g., meal, drink) and aspects (e.g.,\ntaste, freshness) which add a fine-gainer level of complexity, yet help\nexposing true sentiment of chained aspect to its entity. We explore the task of\nEASTE solving capabilities of language models based on transformers\narchitecture from our proposed unified-loss approach via token classification\ntask using BERT architecture to text generative models such as Flan-T5,\nFlan-Ul2 to Llama2, Llama3 and Mixtral employing different alignment techniques\nsuch as zero/few-shot learning, Parameter Efficient Fine Tuning (PEFT) such as\nLow-Rank Adaptation (LoRA). The model performances are evaluated on the\nSamEval-2016 benchmark dataset representing the fair comparison to existing\nworks. Our research not only aims to achieve high performance on the EASTE task\nbut also investigates the impact of model size, type, and adaptation techniques\non task performance. Ultimately, we provide detailed insights and achieving\nstate-of-the-art results in complex sentiment analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 41 st International Conference on Machine\n  Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s)",
    "pdf_url": "http://arxiv.org/pdf/2407.04050v1",
    "published_date": "2024-07-04 16:48:14 UTC",
    "updated_date": "2024-07-04 16:48:14 UTC"
  },
  {
    "arxiv_id": "2407.04022v1",
    "title": "Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection",
    "authors": [
      "Lars Doorenbos",
      "Raphael Sznitman",
      "Pablo Márquez-Neila"
    ],
    "abstract": "The inability of deep learning models to handle data drawn from unseen\ndistributions has sparked much interest in unsupervised out-of-distribution\n(U-OOD) detection, as it is crucial for reliable deep learning models. Despite\nconsiderable attention, theoretically-motivated approaches are few and far\nbetween, with most methods building on top of some form of heuristic. Recently,\nU-OOD was formalized in the context of data invariants, allowing a clearer\nunderstanding of how to characterize U-OOD, and methods leveraging affine\ninvariants have attained state-of-the-art results on large-scale benchmarks.\nNevertheless, the restriction to affine invariants hinders the expressiveness\nof the approach. In this work, we broaden the affine invariants formulation to\na more general case and propose a framework consisting of a normalizing\nflow-like architecture capable of learning non-linear invariants. Our novel\napproach achieves state-of-the-art results on an extensive U-OOD benchmark, and\nwe demonstrate its further applicability to tabular data. Finally, we show our\nmethod has the same desirable properties as those based on affine invariants.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.04022v1",
    "published_date": "2024-07-04 16:01:21 UTC",
    "updated_date": "2024-07-04 16:01:21 UTC"
  },
  {
    "arxiv_id": "2407.17494v1",
    "title": "AI in Remote Patient Monitoring",
    "authors": [
      "Nishargo Nigar"
    ],
    "abstract": "The rapid evolution of Artificial Intelligence (AI) has significantly\ntransformed healthcare, particularly in the domain of Remote Patient Monitoring\n(RPM). This chapter explores the integration of AI in RPM, highlighting\nreal-life applications, system architectures, and the benefits it brings to\npatient care and healthcare systems. Through a comprehensive analysis of\ncurrent technologies, methodologies, and case studies, I present a detailed\noverview of how AI enhances monitoring accuracy, predictive analytics, and\npersonalized treatment plans. The chapter also discusses the challenges and\nfuture directions in this field, providing a comprehensive view of AI's role in\nrevolutionizing remote patient care.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "A chapter for an upcoming Springer book titled \"Transformation in\n  Health Care\"",
    "pdf_url": "http://arxiv.org/pdf/2407.17494v1",
    "published_date": "2024-07-04 15:33:57 UTC",
    "updated_date": "2024-07-04 15:33:57 UTC"
  },
  {
    "arxiv_id": "2407.03995v1",
    "title": "ROER: Regularized Optimal Experience Replay",
    "authors": [
      "Changling Li",
      "Zhang-Wei Hong",
      "Pulkit Agrawal",
      "Divyansh Garg",
      "Joni Pajarinen"
    ],
    "abstract": "Experience replay serves as a key component in the success of online\nreinforcement learning (RL). Prioritized experience replay (PER) reweights\nexperiences by the temporal difference (TD) error empirically enhancing the\nperformance. However, few works have explored the motivation of using TD error.\nIn this work, we provide an alternative perspective on TD-error-based\nreweighting. We show the connections between the experience prioritization and\noccupancy optimization. By using a regularized RL objective with $f-$divergence\nregularizer and employing its dual form, we show that an optimal solution to\nthe objective is obtained by shifting the distribution of off-policy data in\nthe replay buffer towards the on-policy optimal distribution using\nTD-error-based occupancy ratios. Our derivation results in a new pipeline of TD\nerror prioritization. We specifically explore the KL divergence as the\nregularizer and obtain a new form of prioritization scheme, the regularized\noptimal experience replay (ROER). We evaluate the proposed prioritization\nscheme with the Soft Actor-Critic (SAC) algorithm in continuous control MuJoCo\nand DM Control benchmark tasks where our proposed scheme outperforms baselines\nin 6 out of 11 tasks while the results of the rest match with or do not deviate\nfar from the baselines. Further, using pretraining, ROER achieves noticeable\nimprovement on difficult Antmaze environment where baselines fail, showing\napplicability to offline-to-online fine-tuning. Code is available at\n\\url{https://github.com/XavierChanglingLi/Regularized-Optimal-Experience-Replay}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03995v1",
    "published_date": "2024-07-04 15:14:57 UTC",
    "updated_date": "2024-07-04 15:14:57 UTC"
  },
  {
    "arxiv_id": "2407.03994v3",
    "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
    "authors": [
      "Mingxu Tao",
      "Chen Zhang",
      "Quzhe Huang",
      "Tianyao Ma",
      "Songfang Huang",
      "Dongyan Zhao",
      "Yansong Feng"
    ],
    "abstract": "Adapting large language models (LLMs) to new languages typically involves\ncontinual pre-training (CT) followed by supervised fine-tuning (SFT). However,\nthis CT-then-SFT approach struggles with limited data in the context of\nlow-resource languages, failing to balance language modeling and task-solving\ncapabilities. We thus propose model merging as an alternative for low-resource\nlanguages, combining models with distinct capabilities into a single model\nwithout additional training. We use model merging to develop task-solving LLMs\nfor low-resource languages without SFT data in the target languages. Our\nexperiments based on Llama-2-7B demonstrate that model merging effectively\nendows LLMs for low-resource languages with task-solving abilities,\noutperforming CT-then-SFT in scenarios with extremely scarce data. Observing\nperformance saturation in model merging with more training tokens, we further\nanalyze the merging process and introduce a slack variable to the model merging\nalgorithm to mitigate the loss of important parameters, thereby enhancing\nperformance. We hope that model merging can benefit more human languages\nsuffering from data scarcity with its higher data efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in EMNLP2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2407.03994v3",
    "published_date": "2024-07-04 15:14:17 UTC",
    "updated_date": "2024-10-06 10:54:02 UTC"
  },
  {
    "arxiv_id": "2407.12843v5",
    "title": "NutriBench: A Dataset for Evaluating Large Language Models on Nutrition Estimation from Meal Descriptions",
    "authors": [
      "Andong Hua",
      "Mehak Preet Dhaliwal",
      "Ryan Burke",
      "Laya Pullela",
      "Yao Qin"
    ],
    "abstract": "Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide comparable but significantly faster estimates. Finally,\nwe perform a real-world risk assessment by simulating the effect of\ncarbohydrate predictions on the blood glucose levels of individuals with\ndiabetes. Our work highlights the opportunities and challenges of using LLMs\nfor nutrition estimation, demonstrating their potential to aid professionals\nand laypersons and improve health outcomes. Our benchmark is publicly available\nat: https://mehak126.github.io/nutribench.html",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.12843v5",
    "published_date": "2024-07-04 15:10:51 UTC",
    "updated_date": "2025-04-08 00:17:27 UTC"
  },
  {
    "arxiv_id": "2407.03978v3",
    "title": "Benchmarking Complex Instruction-Following with Multiple Constraints Composition",
    "authors": [
      "Bosi Wen",
      "Pei Ke",
      "Xiaotao Gu",
      "Lindong Wu",
      "Hao Huang",
      "Jinfeng Zhou",
      "Wenchuang Li",
      "Binxin Hu",
      "Wendy Gao",
      "Jiaxin Xu",
      "Yiming Liu",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2407.03978v3",
    "published_date": "2024-07-04 14:50:45 UTC",
    "updated_date": "2024-10-31 08:11:04 UTC"
  },
  {
    "arxiv_id": "2407.03969v1",
    "title": "Craftium: An Extensible Framework for Creating Reinforcement Learning Environments",
    "authors": [
      "Mikel Malagón",
      "Josu Ceberio",
      "Jose A. Lozano"
    ],
    "abstract": "Most Reinforcement Learning (RL) environments are created by adapting\nexisting physics simulators or video games. However, they usually lack the\nflexibility required for analyzing specific characteristics of RL methods often\nrelevant to research. This paper presents Craftium, a novel framework for\nexploring and creating rich 3D visual RL environments that builds upon the\nMinetest game engine and the popular Gymnasium API. Minetest is built to be\nextended and can be used to easily create voxel-based 3D environments (often\nsimilar to Minecraft), while Gymnasium offers a simple and common interface for\nRL research. Craftium provides a platform that allows practitioners to create\nfully customized environments to suit their specific research requirements,\nranging from simple visual tasks to infinite and procedurally generated worlds.\nWe also provide five ready-to-use environments for benchmarking and as examples\nof how to develop new ones. The code and documentation are available at\nhttps://github.com/mikelma/craftium/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 6 figures. Code and documentation are available at\n  https://github.com/mikelma/craftium/",
    "pdf_url": "http://arxiv.org/pdf/2407.03969v1",
    "published_date": "2024-07-04 14:38:02 UTC",
    "updated_date": "2024-07-04 14:38:02 UTC"
  },
  {
    "arxiv_id": "2407.03967v2",
    "title": "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks",
    "authors": [
      "Amit Parekh",
      "Nikolas Vitsakis",
      "Alessandro Suglia",
      "Ioannis Konstas"
    ],
    "abstract": "Evaluating the generalisation capabilities of multimodal models based solely\non their performance on out-of-distribution data fails to capture their true\nrobustness. This work introduces a comprehensive evaluation framework that\nsystematically examines the role of instructions and inputs in the\ngeneralisation abilities of such models, considering architectural design,\ninput perturbations across language and vision modalities, and increased task\ncomplexity. The proposed framework uncovers the resilience of multimodal models\nto extreme instruction perturbations and their vulnerability to observational\nchanges, raising concerns about overfitting to spurious correlations. By\nemploying this evaluation framework on current Transformer-based multimodal\nmodels for robotic manipulation tasks, we uncover limitations and suggest\nfuture advancements should focus on architectural and training innovations that\nbetter integrate multimodal inputs, enhancing a model's generalisation prowess\nby prioritising sensitivity to input content over incidental correlations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 (main)",
    "pdf_url": "http://arxiv.org/pdf/2407.03967v2",
    "published_date": "2024-07-04 14:36:49 UTC",
    "updated_date": "2024-10-28 09:52:09 UTC"
  },
  {
    "arxiv_id": "2407.03966v1",
    "title": "Serialized Output Training by Learned Dominance",
    "authors": [
      "Ying Shi",
      "Lantian Li",
      "Shi Yin",
      "Dong Wang",
      "Jiqing Han"
    ],
    "abstract": "Serialized Output Training (SOT) has showcased state-of-the-art performance\nin multi-talker speech recognition by sequentially decoding the speech of\nindividual speakers. To address the challenging label-permutation issue, prior\nmethods have relied on either the Permutation Invariant Training (PIT) or the\ntime-based First-In-First-Out (FIFO) rule. This study presents a model-based\nserialization strategy that incorporates an auxiliary module into the Attention\nEncoder-Decoder architecture, autonomously identifying the crucial factors to\norder the output sequence of the speech components in multi-talker speech.\nExperiments conducted on the LibriSpeech and LibriMix databases reveal that our\napproach significantly outperforms the PIT and FIFO baselines in both 2-mix and\n3-mix scenarios. Further analysis shows that the serialization module\nidentifies dominant speech components in a mixture by factors including\nloudness and gender, and orders speech components based on the dominance score.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted by INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.03966v1",
    "published_date": "2024-07-04 14:36:02 UTC",
    "updated_date": "2024-07-04 14:36:02 UTC"
  },
  {
    "arxiv_id": "2407.03963v2",
    "title": "LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs",
    "authors": [
      "LLM-jp",
      ":",
      "Akiko Aizawa",
      "Eiji Aramaki",
      "Bowen Chen",
      "Fei Cheng",
      "Hiroyuki Deguchi",
      "Rintaro Enomoto",
      "Kazuki Fujii",
      "Kensuke Fukumoto",
      "Takuya Fukushima",
      "Namgi Han",
      "Yuto Harada",
      "Chikara Hashimoto",
      "Tatsuya Hiraoka",
      "Shohei Hisada",
      "Sosuke Hosokawa",
      "Lu Jie",
      "Keisuke Kamata",
      "Teruhito Kanazawa",
      "Hiroki Kanezashi",
      "Hiroshi Kataoka",
      "Satoru Katsumata",
      "Daisuke Kawahara",
      "Seiya Kawano",
      "Atsushi Keyaki",
      "Keisuke Kiryu",
      "Hirokazu Kiyomaru",
      "Takashi Kodama",
      "Takahiro Kubo",
      "Yohei Kuga",
      "Ryoma Kumon",
      "Shuhei Kurita",
      "Sadao Kurohashi",
      "Conglong Li",
      "Taiki Maekawa",
      "Hiroshi Matsuda",
      "Yusuke Miyao",
      "Kentaro Mizuki",
      "Sakae Mizuki",
      "Yugo Murawaki",
      "Akim Mousterou",
      "Ryo Nakamura",
      "Taishi Nakamura",
      "Kouta Nakayama",
      "Tomoka Nakazato",
      "Takuro Niitsuma",
      "Jiro Nishitoba",
      "Yusuke Oda",
      "Hayato Ogawa",
      "Takumi Okamoto",
      "Naoaki Okazaki",
      "Yohei Oseki",
      "Shintaro Ozaki",
      "Koki Ryu",
      "Rafal Rzepka",
      "Keisuke Sakaguchi",
      "Shota Sasaki",
      "Satoshi Sekine",
      "Kohei Suda",
      "Saku Sugawara",
      "Issa Sugiura",
      "Hiroaki Sugiyama",
      "Hisami Suzuki",
      "Jun Suzuki",
      "Toyotaro Suzumura",
      "Kensuke Tachibana",
      "Yu Takagi",
      "Kyosuke Takami",
      "Koichi Takeda",
      "Masashi Takeshita",
      "Masahiro Tanaka",
      "Kenjiro Taura",
      "Arseny Tolmachev",
      "Nobuhiro Ueda",
      "Zhen Wan",
      "Shuntaro Yada",
      "Sakiko Yahata",
      "Yuya Yamamoto",
      "Yusuke Yamauchi",
      "Hitomi Yanaka",
      "Rio Yokota",
      "Koichiro Yoshino"
    ],
    "abstract": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03963v2",
    "published_date": "2024-07-04 14:33:03 UTC",
    "updated_date": "2024-12-30 07:46:43 UTC"
  },
  {
    "arxiv_id": "2407.11032v1",
    "title": "Mechanisms for Data Sharing in Collaborative Causal Inference (Extended Version)",
    "authors": [
      "Björn Filter",
      "Ralf Möller",
      "Özgür Lütfü Özçep"
    ],
    "abstract": "Collaborative causal inference (CCI) is a federated learning method for\npooling data from multiple, often self-interested, parties, to achieve a common\nlearning goal over causal structures, e.g. estimation and optimization of\ntreatment variables in a medical setting. Since obtaining data can be costly\nfor the participants and sharing unique data poses the risk of losing\ncompetitive advantages, motivating the participation of all parties through\nequitable rewards and incentives is necessary. This paper devises an evaluation\nscheme to measure the value of each party's data contribution to the common\nlearning task, tailored to causal inference's statistical demands, by comparing\ncompleted partially directed acyclic graphs (CPDAGs) inferred from\nobservational data contributed by the participants. The Data Valuation Scheme\nthus obtained can then be used to introduce mechanisms that incentivize the\nagents to contribute data. It can be leveraged to reward agents fairly,\naccording to the quality of their data, or to maximize all agents' data\ncontributions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11032v1",
    "published_date": "2024-07-04 14:32:32 UTC",
    "updated_date": "2024-07-04 14:32:32 UTC"
  },
  {
    "arxiv_id": "2407.03942v1",
    "title": "Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data",
    "authors": [
      "Zihui Gu",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Cheng-Zhong Xu",
      "Ju Fan"
    ],
    "abstract": "Instruction-following is particularly crucial for large language models\n(LLMs) to support diverse user requests. While existing work has made progress\nin aligning LLMs with human preferences, evaluating their capabilities on\ninstruction following remains a challenge due to complexity and diversity of\nreal-world user instructions. While existing evaluation methods focus on\ngeneral skills, they suffer from two main shortcomings, i.e., lack of\nfine-grained task-level evaluation and reliance on singular instruction\nexpression. To address these problems, this paper introduces DINGO, a\nfine-grained and diverse instruction-following evaluation dataset that has two\nmain advantages: (1) DINGO is based on a manual annotated, fine-grained and\nmulti-level category tree with 130 nodes derived from real-world user requests;\n(2) DINGO includes diverse instructions, generated by both GPT-4 and human\nexperts. Through extensive experiments, we demonstrate that DINGO can not only\nprovide more challenging and comprehensive evaluation for LLMs, but also\nprovide task-level fine-grained directions to further improve LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03942v1",
    "published_date": "2024-07-04 13:54:41 UTC",
    "updated_date": "2024-07-04 13:54:41 UTC"
  },
  {
    "arxiv_id": "2407.03941v2",
    "title": "Narrow Transformer: StarCoder-Based Java-LM For Desktop",
    "authors": [
      "Kamalkumar Rathinasamy",
      "Balaji A J",
      "Ankush Kumar",
      "Gagan Gayari",
      "Harshini K",
      "Rajab Ali Mondal",
      "Sreenivasa Raghavan K S",
      "Swayam Singh",
      "Mohammed Rafee Tarafdar"
    ],
    "abstract": "This paper presents NT-Java-1.1B, an open-source specialized code language\nmodel built on StarCoderBase-1.1B, designed for coding tasks in Java\nprogramming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its\nbase model and majority of other models of similar size on MultiPL-E Java code\nbenchmark. While there have been studies on extending large, generic\npre-trained models to improve proficiency in specific programming languages\nlike Python, similar investigations on small code models for other programming\nlanguages are lacking. Large code models require specialized hardware like GPUs\nfor inference, highlighting the need for research into building small code\nmodels that can be deployed on developer desktops. This paper addresses this\nresearch gap by focusing on the development of a small Java code model,\nNT-Java-1.1B, and its quantized versions, which performs comparably to open\nmodels around 1.1B on MultiPL-E Java code benchmarks, making them ideal for\ndesktop deployment. This paper establishes the foundation for specialized\nmodels across languages and sizes for a family of NT Models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "Updated Authors list",
    "pdf_url": "http://arxiv.org/pdf/2407.03941v2",
    "published_date": "2024-07-04 13:54:24 UTC",
    "updated_date": "2024-09-07 11:40:47 UTC"
  },
  {
    "arxiv_id": "2407.12842v1",
    "title": "MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production",
    "authors": [
      "Jian Ma",
      "Wenguan Wang",
      "Yi Yang",
      "Feng Zheng"
    ],
    "abstract": "Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 Findings; Project Page:\n  https://hechang25.github.io/MS2SL",
    "pdf_url": "http://arxiv.org/pdf/2407.12842v1",
    "published_date": "2024-07-04 13:53:50 UTC",
    "updated_date": "2024-07-04 13:53:50 UTC"
  },
  {
    "arxiv_id": "2407.17493v2",
    "title": "Model Collapse in the Self-Consuming Chain of Diffusion Finetuning: A Novel Perspective from Quantitative Trait Modeling",
    "authors": [
      "Youngseok Yoon",
      "Dainong Hu",
      "Iain Weissburg",
      "Yao Qin",
      "Haewon Jeong"
    ],
    "abstract": "The success of generative models has reached a unique threshold where their\noutputs are indistinguishable from real data, leading to the inevitable\ncontamination of future data collection pipelines with synthetic data. While\ntheir potential to generate infinite samples initially offers promise for\nreducing data collection costs and addressing challenges in data-scarce fields,\nthe severe degradation in performance has been observed when iterative loops of\ntraining and generation occur -- known as ``model collapse.'' This paper\nexplores a practical scenario in which a pretrained text-to-image diffusion\nmodel is finetuned using synthetic images generated from a previous iteration,\na process we refer to as the ``Chain of Diffusion.'' We first demonstrate the\nsignificant degradation in image quality caused by this iterative process and\nidentify the key factor driving this decline through rigorous empirical\ninvestigations. Drawing an analogy between the Chain of Diffusion and\nbiological evolution, we then introduce a novel theoretical analysis based on\nquantitative trait modeling. Our theoretical analysis aligns with empirical\nobservations of the generated images in the Chain of Diffusion. Finally, we\npropose Reusable Diffusion Finetuning (ReDiFine), a simple yet effective\nstrategy inspired by genetic mutations. ReDiFine mitigates model collapse\nwithout requiring any hyperparameter tuning, making it a plug-and-play solution\nfor reusable image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "29 pages, version 2 with new analysis",
    "pdf_url": "http://arxiv.org/pdf/2407.17493v2",
    "published_date": "2024-07-04 13:41:54 UTC",
    "updated_date": "2024-10-24 20:03:46 UTC"
  },
  {
    "arxiv_id": "2407.03927v1",
    "title": "Dancing to the State of the Art? How Candidate Lists Influence LKH for Solving the Traveling Salesperson Problem",
    "authors": [
      "Jonathan Heins",
      "Lennart Schäpermeier",
      "Pascal Kerschke",
      "Darrell Whitley"
    ],
    "abstract": "Solving the Traveling Salesperson Problem (TSP) remains a persistent\nchallenge, despite its fundamental role in numerous generalized applications in\nmodern contexts. Heuristic solvers address the demand for finding high-quality\nsolutions efficiently. Among these solvers, the Lin-Kernighan-Helsgaun (LKH)\nheuristic stands out, as it complements the performance of genetic algorithms\nacross a diverse range of problem instances. However, frequent timeouts on\nchallenging instances hinder the practical applicability of the solver.\n  Within this work, we investigate a previously overlooked factor contributing\nto many timeouts: The use of a fixed candidate set based on a tree structure.\nOur investigations reveal that candidate sets based on Hamiltonian circuits\ncontain more optimal edges. We thus propose to integrate this promising\ninitialization strategy, in the form of POPMUSIC, within an efficient restart\nversion of LKH. As confirmed by our experimental studies, this refined TSP\nheuristic is much more efficient - causing fewer timeouts and improving the\nperformance (in terms of penalized average runtime) by an order of magnitude -\nand thereby challenges the state of the art in TSP solving.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "This version has been accepted for publication at the 18th\n  International Conference on Parallel Problem Solving from Nature (PPSN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.03927v1",
    "published_date": "2024-07-04 13:38:19 UTC",
    "updated_date": "2024-07-04 13:38:19 UTC"
  },
  {
    "arxiv_id": "2407.03924v1",
    "title": "TwinLab: a framework for data-efficient training of non-intrusive reduced-order models for digital twins",
    "authors": [
      "Maximilian Kannapinn",
      "Michael Schäfer",
      "Oliver Weeger"
    ],
    "abstract": "Purpose: Simulation-based digital twins represent an effort to provide\nhigh-accuracy real-time insights into operational physical processes. However,\nthe computation time of many multi-physical simulation models is far from\nreal-time. It might even exceed sensible time frames to produce sufficient data\nfor training data-driven reduced-order models. This study presents TwinLab, a\nframework for data-efficient, yet accurate training of neural-ODE type\nreduced-order models with only two data sets. Design/methodology/approach:\nCorrelations between test errors of reduced-order models and distinct features\nof corresponding training data are investigated. Having found the single best\ndata sets for training, a second data set is sought with the help of similarity\nand error measures to enrich the training process effectively. Findings: Adding\na suitable second training data set in the training process reduces the test\nerror by up to 49% compared to the best base reduced-order model trained only\nwith one data set. Such a second training data set should at least yield a good\nreduced-order model on its own and exhibit higher levels of dissimilarity to\nthe base training data set regarding the respective excitation signal.\nMoreover, the base reduced-order model should have elevated test errors on the\nsecond data set. The relative error of the time series ranges from 0.18% to\n0.49%. Prediction speed-ups of up to a factor of 36,000 are observed.\nOriginality: The proposed computational framework facilitates the automated,\ndata-efficient extraction of non-intrusive reduced-order models for digital\ntwins from existing simulation models, independent of the simulation software.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.DS"
    ],
    "primary_category": "cs.CE",
    "comment": "Accepted version of the revised manuscript published in Engineering\n  Computations",
    "pdf_url": "http://arxiv.org/pdf/2407.03924v1",
    "published_date": "2024-07-04 13:37:13 UTC",
    "updated_date": "2024-07-04 13:37:13 UTC"
  },
  {
    "arxiv_id": "2407.03923v2",
    "title": "CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion-Blurred Images",
    "authors": [
      "Jungho Lee",
      "Donghyeong Kim",
      "Dogyoon Lee",
      "Suhwan Cho",
      "Minhyeok Lee",
      "Sangyoun Lee"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for their\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CRiM-GS, a \\textbf{C}ontinuous \\textbf{Ri}gid \\textbf{M}otion-aware\n\\textbf{G}aussian \\textbf{S}platting that reconstructs precise 3D scenes from\nmotion-blurred images while maintaining real-time rendering speed. Considering\nthe complex motion patterns inherent in real-world camera movements, we predict\ncontinuous camera trajectories using neural ordinary differential equations\n(ODE). To ensure accurate modeling, we employ rigid body transformations with\nproper regularization, preserving object shape and size. Additionally, we\nintroduce an adaptive distortion-aware transformation to compensate for\npotential nonlinear distortions, such as rolling shutter effects, and\nunpredictable camera movements. By revisiting fundamental camera theory and\nleveraging advanced neural training techniques, we achieve precise modeling of\ncontinuous camera trajectories. Extensive experiments demonstrate\nstate-of-the-art performance both quantitatively and qualitatively on benchmark\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page : https://jho-yonsei.github.io/CRiM-Gaussian/",
    "pdf_url": "http://arxiv.org/pdf/2407.03923v2",
    "published_date": "2024-07-04 13:37:04 UTC",
    "updated_date": "2024-12-08 08:05:26 UTC"
  },
  {
    "arxiv_id": "2407.03913v1",
    "title": "MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices",
    "authors": [
      "Jiayi Zhang",
      "Chuang Zhao",
      "Yihan Zhao",
      "Zhaoyang Yu",
      "Ming He",
      "Jianping Fan"
    ],
    "abstract": "The attainment of autonomous operations in mobile computing devices has\nconsistently been a goal of human pursuit. With the development of Large\nLanguage Models (LLMs) and Visual Language Models (VLMs), this aspiration is\nprogressively turning into reality. While contemporary research has explored\nautomation of simple tasks on mobile devices via VLMs, there remains\nsignificant room for improvement in handling complex tasks and reducing high\nreasoning costs. In this paper, we introduce MobileExperts, which for the first\ntime introduces tool formulation and multi-agent collaboration to address the\naforementioned challenges. More specifically, MobileExperts dynamically\nassembles teams based on the alignment of agent portraits with the human\nrequirements. Following this, each agent embarks on an independent exploration\nphase, formulating its tools to evolve into an expert. Lastly, we develop a\ndual-layer planning mechanism to establish coordinate collaboration among\nexperts. To validate our effectiveness, we design a new benchmark of\nhierarchical intelligence levels, offering insights into algorithm's capability\nto address tasks across a spectrum of complexity. Experimental results\ndemonstrate that MobileExperts performs better on all intelligence levels and\nachieves ~ 22% reduction in reasoning costs, thus verifying the superiority of\nour design.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03913v1",
    "published_date": "2024-07-04 13:12:19 UTC",
    "updated_date": "2024-07-04 13:12:19 UTC"
  },
  {
    "arxiv_id": "2407.12841v2",
    "title": "Black-box Model Ensembling for Textual and Visual Question Answering via Information Fusion",
    "authors": [
      "Yuxi Xia",
      "Kilm Zaporojets",
      "Benjamin Roth"
    ],
    "abstract": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 6 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12841v2",
    "published_date": "2024-07-04 12:59:10 UTC",
    "updated_date": "2024-12-17 13:31:18 UTC"
  },
  {
    "arxiv_id": "2407.09563v1",
    "title": "Psychology of Artificial Intelligence: Epistemological Markers of the Cognitive Analysis of Neural Networks",
    "authors": [
      "Michael Pichat"
    ],
    "abstract": "What is the \"nature\" of the cognitive processes and contents of an artificial\nneural network? In other words, how does an artificial intelligence\nfundamentally \"think,\" and in what form does its knowledge reside? The\npsychology of artificial intelligence, as predicted by Asimov (1950), aims to\nstudy this AI probing and explainability-sensitive matter. This study requires\na neuronal level of cognitive granularity, so as not to be limited solely to\nthe secondary macro-cognitive results (such as cognitive and cultural biases)\nof synthetic neural cognition. A prerequisite for examining the latter is to\nclarify some epistemological milestones regarding the cognitive status we can\nattribute to its phenomenology.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09563v1",
    "published_date": "2024-07-04 12:53:05 UTC",
    "updated_date": "2024-07-04 12:53:05 UTC"
  },
  {
    "arxiv_id": "2407.17492v2",
    "title": "Unraveling Molecular Structure: A Multimodal Spectroscopic Dataset for Chemistry",
    "authors": [
      "Marvin Alberts",
      "Oliver Schilter",
      "Federico Zipoli",
      "Nina Hartrampf",
      "Teodoro Laino"
    ],
    "abstract": "Spectroscopic techniques are essential tools for determining the structure of\nmolecules. Different spectroscopic techniques, such as Nuclear magnetic\nresonance (NMR), Infrared spectroscopy, and Mass Spectrometry, provide insight\ninto the molecular structure, including the presence or absence of functional\ngroups. Chemists leverage the complementary nature of the different methods to\ntheir advantage. However, the lack of a comprehensive multimodal dataset,\ncontaining spectra from a variety of spectroscopic techniques, has limited\nmachine-learning approaches mostly to single-modality tasks for predicting\nmolecular structures from spectra. Here we introduce a dataset comprising\nsimulated $^1$H-NMR, $^{13}$C-NMR, HSQC-NMR, Infrared, and Mass spectra\n(positive and negative ion modes) for 790k molecules extracted from chemical\nreactions in patent data. This dataset enables the development of foundation\nmodels for integrating information from multiple spectroscopic modalities,\nemulating the approach employed by human experts. Additionally, we provide\nbenchmarks for evaluating single-modality tasks such as structure elucidation,\npredicting the spectra for a target molecule, and functional group predictions.\nThis dataset has the potential automate structure elucidation, streamlining the\nmolecular discovery pipeline from synthesis to structure determination. The\ndataset and code for the benchmarks can be found at\nhttps://rxn4chemistry.github.io/multimodal-spectroscopic-dataset.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "29 pages, submited to conference, code available at:\n  https://github.com/rxn4chemistry/multimodal-spectroscopic-dataset",
    "pdf_url": "http://arxiv.org/pdf/2407.17492v2",
    "published_date": "2024-07-04 12:52:48 UTC",
    "updated_date": "2024-10-29 15:54:13 UTC"
  },
  {
    "arxiv_id": "2407.03893v1",
    "title": "Do Generalised Classifiers really work on Human Drawn Sketches?",
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Subhadeep Koley",
      "Tao Xiang",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ],
    "abstract": "This paper, for the first time, marries large foundation models with human\nsketch understanding. We demonstrate what this brings -- a paradigm shift in\nterms of generalised sketch representation learning (e.g., classification).\nThis generalisation happens on two fronts: (i) generalisation across unknown\ncategories (i.e., open-set), and (ii) generalisation traversing abstraction\nlevels (i.e., good and bad sketches), both being timely challenges that remain\nunsolved in the sketch literature. Our design is intuitive and centred around\ntransferring the already stellar generalisation ability of CLIP to benefit\ngeneralised learning for sketches. We first \"condition\" the vanilla CLIP model\nby learning sketch-specific prompts using a novel auxiliary head of raster to\nvector sketch conversion. This importantly makes CLIP \"sketch-aware\". We then\nmake CLIP acute to the inherently different sketch abstraction levels. This is\nachieved by learning a codebook of abstraction-specific prompt biases, a\nweighted combination of which facilitates the representation of sketches across\nabstraction levels -- low abstract edge-maps, medium abstract sketches in\nTU-Berlin, and highly abstract doodles in QuickDraw. Our framework surpasses\npopular sketch representation learning algorithms in both zero-shot and\nfew-shot setups and in novel settings across different abstraction boundaries.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.03893v1",
    "published_date": "2024-07-04 12:37:08 UTC",
    "updated_date": "2024-07-04 12:37:08 UTC"
  },
  {
    "arxiv_id": "2407.03892v1",
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "authors": [
      "Bohan Li",
      "Feiyu Shen",
      "Yiwei Guo",
      "Shuai Wang",
      "Xie Chen",
      "Kai Yu"
    ],
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model\nhave been a promising direction for text-to-speech (TTS) and spoken language\nmodeling (SLM). To shorten the sequence length of speech tokens, acoustic\nbyte-pair encoding (BPE) has emerged in SLM that treats speech tokens from\nself-supervised semantic representations as characters to further compress the\ntoken sequence. But the gain in TTS has not been fully investigated, and the\nproper choice of acoustic BPE remains unclear. In this work, we conduct a\ncomprehensive study on various settings of acoustic BPE to explore its\neffectiveness in decoder-only TTS models with semantic speech tokens.\nExperiments on LibriTTS verify that acoustic BPE uniformly increases the\nintelligibility and diversity of synthesized speech, while showing different\nfeatures across BPE settings. Hence, acoustic BPE is a favorable tool for\ndecoder-only TTS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 tables, 1 figures. accepted to Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.03892v1",
    "published_date": "2024-07-04 12:35:32 UTC",
    "updated_date": "2024-07-04 12:35:32 UTC"
  },
  {
    "arxiv_id": "2407.03884v3",
    "title": "ChatSOP: An SOP-Guided MCTS Planning Framework for Controllable LLM Dialogue Agents",
    "authors": [
      "Zhigen Li",
      "Jianxiang Peng",
      "Yanmeng Wang",
      "Yong Cao",
      "Tianhao Shen",
      "Minghui Zhang",
      "Linxi Su",
      "Shang Wu",
      "Yihang Wu",
      "Yuqian Wang",
      "Ye Wang",
      "Wei Hu",
      "Jianfeng Li",
      "Shaojun Wang",
      "Jing Xiao",
      "Deyi Xiong"
    ],
    "abstract": "Dialogue agents powered by Large Language Models (LLMs) show superior\nperformance in various tasks. Despite the better user understanding and\nhuman-like responses, their lack of controllability remains a key challenge,\noften leading to unfocused conversations or task failure. To address this, we\nintroduce Standard Operating Procedure (SOP) to regulate dialogue flow.\nSpecifically, we propose ChatSOP, a novel SOP-guided Monte Carlo Tree Search\n(MCTS) planning framework designed to enhance the controllability of LLM-driven\ndialogue agents. To enable this, we curate a dataset comprising SOP-annotated\nmulti-scenario dialogues, generated using a semi-automated role-playing system\nwith GPT-4o and validated through strict manual quality control. Additionally,\nwe propose a novel method that integrates Chain of Thought reasoning with\nsupervised fine-tuning for SOP prediction and utilizes SOP-guided Monte Carlo\nTree Search for optimal action planning during dialogues. Experimental results\ndemonstrate the effectiveness of our method, such as achieving a 27.95%\nimprovement in action accuracy compared to baseline models based on GPT-3.5 and\nalso showing notable gains for open-source models. Dataset and codes are\npublicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03884v3",
    "published_date": "2024-07-04 12:23:02 UTC",
    "updated_date": "2025-02-22 00:11:18 UTC"
  },
  {
    "arxiv_id": "2407.16893v1",
    "title": "The Price of Prompting: Profiling Energy Use in Large Language Models Inference",
    "authors": [
      "Erik Johannes Husom",
      "Arda Goknil",
      "Lwin Khin Shar",
      "Sagar Sen"
    ],
    "abstract": "In the rapidly evolving realm of artificial intelligence, deploying large\nlanguage models (LLMs) poses increasingly pressing computational and\nenvironmental challenges. This paper introduces MELODI - Monitoring Energy\nLevels and Optimization for Data-driven Inference - a multifaceted framework\ncrafted to monitor and analyze the energy consumed during LLM inference\nprocesses. MELODI enables detailed observations of power consumption dynamics\nand facilitates the creation of a comprehensive dataset reflective of energy\nefficiency across varied deployment scenarios. The dataset, generated using\nMELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple\nlanguage models, and extensive prompt datasets, enabling a comparative analysis\nof energy use. Using the dataset, we investigate how prompt attributes,\nincluding length and complexity, correlate with energy expenditure. Our\nfindings indicate substantial disparities in energy efficiency, suggesting\nample scope for optimization and adoption of sustainable measures in LLM\ndeployment. Our contribution lies not only in the MELODI framework but also in\nthe novel dataset, a resource that can be expanded by other researchers. Thus,\nMELODI is a foundational tool and dataset for advancing research into\nenergy-conscious LLM deployment, steering the field toward a more sustainable\nfuture.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "11 pages, 5 figures. Submitted to NeurIPS 2024. The released code and\n  dataset are available at https://github.com/ejhusom/MELODI",
    "pdf_url": "http://arxiv.org/pdf/2407.16893v1",
    "published_date": "2024-07-04 12:16:28 UTC",
    "updated_date": "2024-07-04 12:16:28 UTC"
  },
  {
    "arxiv_id": "2407.03864v2",
    "title": "Adversarial Robustness of VAEs across Intersectional Subgroups",
    "authors": [
      "Chethan Krishnamurthy Ramanaik",
      "Arjun Roy",
      "Eirini Ntoutsi"
    ],
    "abstract": "Despite advancements in Autoencoders (AEs) for tasks like dimensionality\nreduction, representation learning and data generation, they remain vulnerable\nto adversarial attacks. Variational Autoencoders (VAEs), with their\nprobabilistic approach to disentangling latent spaces, show stronger resistance\nto such perturbations compared to deterministic AEs; however, their resilience\nagainst adversarial inputs is still a concern. This study evaluates the\nrobustness of VAEs against non-targeted adversarial attacks by optimizing\nminimal sample-specific perturbations to cause maximal damage across diverse\ndemographic subgroups (combinations of age and gender). We investigate two\nquestions: whether there are robustness disparities among subgroups, and what\nfactors contribute to these disparities, such as data scarcity and\nrepresentation entanglement. Our findings reveal that robustness disparities\nexist but are not always correlated with the size of the subgroup. By using\ndownstream gender and age classifiers and examining latent embeddings, we\nhighlight the vulnerability of subgroups like older women, who are prone to\nmisclassification due to adversarial perturbations pushing their\nrepresentations toward those of other subgroups.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03864v2",
    "published_date": "2024-07-04 11:53:51 UTC",
    "updated_date": "2024-11-15 11:51:10 UTC"
  },
  {
    "arxiv_id": "2407.03863v1",
    "title": "Unsupervised Analysis of Alzheimer's Disease Signatures using 3D Deformable Autoencoders",
    "authors": [
      "Mehmet Yigit Avci",
      "Emily Chan",
      "Veronika Zimmer",
      "Daniel Rueckert",
      "Benedikt Wiestler",
      "Julia A. Schnabel",
      "Cosmin I. Bercea"
    ],
    "abstract": "With the increasing incidence of neurodegenerative diseases such as\nAlzheimer's Disease (AD), there is a need for further research that enhances\ndetection and monitoring of the diseases. We present MORPHADE (Morphological\nAutoencoders for Alzheimer's Disease Detection), a novel unsupervised learning\napproach which uses deformations to allow the analysis of 3D T1-weighted brain\nimages. To the best of our knowledge, this is the first use of deformations\nwith deep unsupervised learning to not only detect, but also localize and\nassess the severity of structural changes in the brain due to AD. We obtain\nmarkedly higher anomaly scores in clinically important areas of the brain in\nsubjects with AD compared to healthy controls, showcasing that our method is\nable to effectively locate AD-related atrophy. We additionally observe a visual\ncorrelation between the severity of atrophy highlighted in our anomaly maps and\nmedial temporal lobe atrophy scores evaluated by a clinical expert. Finally,\nour method achieves an AUROC of 0.80 in detecting AD, out-performing several\nsupervised and unsupervised baselines. We believe our framework shows promise\nas a tool towards improved understanding, monitoring and detection of AD. To\nsupport further research and application, we have made our code publicly\navailable at github.com/ci-ber/MORPHADE.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.03863v1",
    "published_date": "2024-07-04 11:52:44 UTC",
    "updated_date": "2024-07-04 11:52:44 UTC"
  },
  {
    "arxiv_id": "2407.03850v1",
    "title": "HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured Information for Check-Worthiness Estimation",
    "authors": [
      "Géraud Faye",
      "Morgane Casanova",
      "Benjamin Icard",
      "Julien Chanson",
      "Guillaume Gadek",
      "Guillaume Gravier",
      "Paul Égré"
    ],
    "abstract": "This paper summarizes the experiments and results of the HYBRINFOX team for\nthe CheckThat! 2024 - Task 1 competition. We propose an approach enriching\nLanguage Models such as RoBERTa with embeddings produced by triples (subject ;\npredicate ; object) extracted from the text sentences. Our analysis of the\ndevelopmental data shows that this method improves the performance of Language\nModels alone. On the evaluation data, its best performance was in English,\nwhere it achieved an F1 score of 71.1 and ranked 12th out of 27 candidates. On\nthe other languages (Dutch and Arabic), it obtained more mixed results. Future\nresearch tracks are identified toward adapting this processing pipeline to more\nrecent Large Language Models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper to appear in the Proceedings of the Conference and Labs of the\n  Evaluation Forum (CLEF 2024 CheckThat!)",
    "pdf_url": "http://arxiv.org/pdf/2407.03850v1",
    "published_date": "2024-07-04 11:33:54 UTC",
    "updated_date": "2024-07-04 11:33:54 UTC"
  },
  {
    "arxiv_id": "2407.03824v3",
    "title": "Unsupervised Disentanglement of Content and Style via Variance-Invariance Constraints",
    "authors": [
      "Yuxuan Wu",
      "Ziyu Wang",
      "Bhiksha Raj",
      "Gus Xia"
    ],
    "abstract": "We contribute an unsupervised method that effectively learns disentangled\ncontent and style representations from sequences of observations. Unlike most\ndisentanglement algorithms that rely on domain-specific labels or knowledge,\nour method is based on the insight of domain-general statistical differences\nbetween content and style -- content varies more among different fragments\nwithin a sample but maintains an invariant vocabulary across data samples,\nwhereas style remains relatively invariant within a sample but exhibits more\nsignificant variation across different samples. We integrate such inductive\nbias into an encoder-decoder architecture and name our method after V3\n(variance-versus-invariance). Experimental results show that V3 generalizes\nacross multiple domains and modalities, successfully learning disentangled\ncontent and style representations, such as pitch and timbre from music audio,\ndigit and color from images of hand-written digits, and action and character\nappearance from simple animations. V3 demonstrates strong disentanglement\nperformance compared to existing unsupervised methods, along with superior\nout-of-distribution generalization under few-shot adaptation compared to\nsupervised counterparts. Lastly, symbolic-level interpretability emerges in the\nlearned content codebook, forging a near one-to-one alignment between machine\nrepresentation and human knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03824v3",
    "published_date": "2024-07-04 10:52:02 UTC",
    "updated_date": "2025-03-15 13:44:35 UTC"
  },
  {
    "arxiv_id": "2407.03778v1",
    "title": "From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI",
    "authors": [
      "Stefanie Krause",
      "Frieder Stolzenburg"
    ],
    "abstract": "Commonsense reasoning is a difficult task for a computer, but a critical\nskill for an artificial intelligence (AI). It can enhance the explainability of\nAI models by enabling them to provide intuitive and human-like explanations for\ntheir decisions. This is necessary in many areas especially in question\nanswering (QA), which is one of the most important tasks of natural language\nprocessing (NLP). Over time, a multitude of methods have emerged for solving\ncommonsense reasoning problems such as knowledge-based approaches using formal\nlogic or linguistic analysis. In this paper, we investigate the effectiveness\nof large language models (LLMs) on different QA tasks with a focus on their\nabilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma\nand Llama 3. We further evaluate the LLM results by means of a questionnaire.\nWe demonstrate the ability of LLMs to reason with commonsense as the models\noutperform humans on different datasets. While GPT-3.5's accuracy ranges from\n56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on\nall eleven datasets. Thereby Llama 3 is outperforming humans on all datasets\nwith an average 21% higher accuracy over ten datasets. Furthermore, we can\nappraise that, in the sense of explainable artificial intelligence (XAI),\nGPT-3.5 provides good explanations for its decisions. Our questionnaire\nrevealed that 66% of participants rated GPT-3.5's explanations as either \"good\"\nor \"excellent\". Taken together, these findings enrich our understanding of\ncurrent LLMs and pave the way for future investigations of reasoning and\nexplainability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.03778v1",
    "published_date": "2024-07-04 09:38:49 UTC",
    "updated_date": "2024-07-04 09:38:49 UTC"
  },
  {
    "arxiv_id": "2407.03770v1",
    "title": "HYBRINFOX at CheckThat! 2024 -- Task 2: Enriching BERT Models with the Expert System VAGO for Subjectivity Detection",
    "authors": [
      "Morgane Casanova",
      "Julien Chanson",
      "Benjamin Icard",
      "Géraud Faye",
      "Guillaume Gadek",
      "Guillaume Gravier",
      "Paul Égré"
    ],
    "abstract": "This paper presents the HYBRINFOX method used to solve Task 2 of Subjectivity\ndetection of the CLEF 2024 CheckThat! competition. The specificity of the\nmethod is to use a hybrid system, combining a RoBERTa model, fine-tuned for\nsubjectivity detection, a frozen sentence-BERT (sBERT) model to capture\nsemantics, and several scores calculated by the English version of the expert\nsystem VAGO, developed independently of this task to measure vagueness and\nsubjectivity in texts based on the lexicon. In English, the HYBRINFOX method\nranked 1st with a macro F1 score of 0.7442 on the evaluation data. For the\nother languages, the method used a translation step into English, producing\nmore mixed results (ranking 1st in Multilingual and 2nd in Italian over the\nbaseline, but under the baseline in Bulgarian, German, and Arabic). We explain\nthe principles of our hybrid approach, and outline ways in which the method\ncould be improved for other languages besides English.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the Proceedings of the Conference and Labs of the\n  Evaluation Forum (CLEF 2024 CheckThat!)",
    "pdf_url": "http://arxiv.org/pdf/2407.03770v1",
    "published_date": "2024-07-04 09:29:19 UTC",
    "updated_date": "2024-07-04 09:29:19 UTC"
  },
  {
    "arxiv_id": "2407.03759v1",
    "title": "Convolutional vs Large Language Models for Software Log Classification in Edge-Deployable Cellular Network Testing",
    "authors": [
      "Achintha Ihalage",
      "Sayed M. Taheri",
      "Faris Muhammad",
      "Hamed Al-Raweshidy"
    ],
    "abstract": "Software logs generated by sophisticated network emulators in the\ntelecommunications industry, such as VIAVI TM500, are extremely complex, often\ncomprising tens of thousands of text lines with minimal resemblance to natural\nlanguage. Only specialised expert engineers can decipher such logs and\ntroubleshoot defects in test runs. While AI offers a promising solution for\nautomating defect triage, potentially leading to massive revenue savings for\ncompanies, state-of-the-art large language models (LLMs) suffer from\nsignificant drawbacks in this specialised domain. These include a constrained\ncontext window, limited applicability to text beyond natural language, and high\ninference costs. To address these limitations, we propose a compact\nconvolutional neural network (CNN) architecture that offers a context window\nspanning up to 200,000 characters and achieves over 96% accuracy (F1>0.9) in\nclassifying multifaceted software logs into various layers in the\ntelecommunications protocol stack. Specifically, the proposed model is capable\nof identifying defects in test runs and triaging them to the relevant\ndepartment, formerly a manual engineering process that required expert\nknowledge. We evaluate several LLMs; LLaMA2-7B, Mixtral 8x7B, Flan-T5, BERT and\nBigBird, and experimentally demonstrate their shortcomings in our specialized\napplication. Despite being lightweight, our CNN significantly outperforms\nLLM-based approaches in telecommunications log classification while minimizing\nthe cost of production. Our defect triaging AI model is deployable on edge\ndevices without dedicated hardware and widely applicable across software logs\nin various industries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03759v1",
    "published_date": "2024-07-04 09:12:08 UTC",
    "updated_date": "2024-07-04 09:12:08 UTC"
  },
  {
    "arxiv_id": "2407.18326v1",
    "title": "Classification-Based Automatic HDL Code Generation Using LLMs",
    "authors": [
      "Wenhao Sun",
      "Bing Li",
      "Grace Li Zhang",
      "Xunzhao Yin",
      "Cheng Zhuo",
      "Ulf Schlichtmann"
    ],
    "abstract": "While large language models (LLMs) have demonstrated the ability to generate\nhardware description language (HDL) code for digital circuits, they still\nsuffer from the hallucination problem, which leads to the generation of\nincorrect HDL code or misunderstanding of specifications. In this work, we\nintroduce a human-expert-inspired method to mitigate the hallucination of LLMs\nand improve the performance in HDL code generation. We first let LLMs classify\nthe type of the circuit based on the specifications. Then, according to the\ntype of the circuit, we split the tasks into several sub-procedures, including\ninformation extraction and human-like design flow using Electronic Design\nAutomation (EDA) tools. Besides, we also use a search method to mitigate the\nvariation in code generation. Experimental results show that our method can\nsignificantly improve the functional correctness of the generated Verilog and\nreduce the hallucination of LLMs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18326v1",
    "published_date": "2024-07-04 09:00:13 UTC",
    "updated_date": "2024-07-04 09:00:13 UTC"
  },
  {
    "arxiv_id": "2407.03748v1",
    "title": "Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques",
    "authors": [
      "Anar Yeginbergen",
      "Maite Oronoz",
      "Rodrigo Agerri"
    ],
    "abstract": "Recent research on sequence labelling has been exploring different strategies\nto mitigate the lack of manually annotated data for the large majority of the\nworld languages. Among others, the most successful approaches have been based\non (i) the cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models (model-transfer), (ii) data translation and label projection\n(data-transfer) and (iii), prompt-based learning by reusing the mask objective\nto exploit the few-shot capabilities of pre-trained language models (few-shot).\nPrevious work seems to conclude that model-transfer outperforms data-transfer\nmethods and that few-shot techniques based on prompting are superior to\nupdating the model's weights via fine-tuning. In this paper, we empirically\ndemonstrate that, for Argument Mining, a sequence labelling task which requires\nthe detection of long and complex discourse structures, previous insights on\ncross-lingual transfer or few-shot learning do not apply. Contrary to previous\nwork, we show that for Argument Mining data transfer obtains better results\nthan model-transfer and that fine-tuning outperforms few-shot methods.\nRegarding the former, the domain of the dataset used for data-transfer seems to\nbe a deciding factor, while, for few-shot, the type of task (length and\ncomplexity of the sequence spans) and sampling method prove to be crucial.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03748v1",
    "published_date": "2024-07-04 08:59:17 UTC",
    "updated_date": "2024-07-04 08:59:17 UTC"
  },
  {
    "arxiv_id": "2407.03734v1",
    "title": "Improving Self-supervised Pre-training using Accent-Specific Codebooks",
    "authors": [
      "Darshan Prabhu",
      "Abhishek Gupta",
      "Omkar Nitsure",
      "Preethi Jyothi",
      "Sriram Ganapathy"
    ],
    "abstract": "Speech accents present a serious challenge to the performance of\nstate-of-the-art end-to-end Automatic Speech Recognition (ASR) systems. Even\nwith self-supervised learning and pre-training of ASR models, accent invariance\nis seldom achieved. In this work, we propose an accent-aware adaptation\ntechnique for self-supervised learning that introduces a trainable set of\naccent-specific codebooks to the self-supervised architecture. These learnable\ncodebooks enable the model to capture accent specific information during\npre-training, that is further refined during ASR finetuning. On the Mozilla\nCommon Voice dataset, our proposed approach outperforms all other\naccent-adaptation approaches on both seen and unseen English accents, with up\nto 9% relative reduction in word error rate (WER).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.03734v1",
    "published_date": "2024-07-04 08:33:52 UTC",
    "updated_date": "2024-07-04 08:33:52 UTC"
  },
  {
    "arxiv_id": "2407.03728v2",
    "title": "Measuring Orthogonality in Representations of Generative Models",
    "authors": [
      "Robin C. Geyer",
      "Alessandro Torcinovich",
      "João B. Carvalho",
      "Alexander Meyer",
      "Joachim M. Buhmann"
    ],
    "abstract": "In unsupervised representation learning, models aim to distill essential\nfeatures from high-dimensional data into lower-dimensional learned\nrepresentations, guided by inductive biases. Understanding the characteristics\nthat make a good representation remains a topic of ongoing research.\nDisentanglement of independent generative processes has long been credited with\nproducing high-quality representations. However, focusing solely on\nrepresentations that adhere to the stringent requirements of most\ndisentanglement metrics, may result in overlooking many high-quality\nrepresentations, well suited for various downstream tasks. These metrics often\ndemand that generative factors be encoded in distinct, single dimensions\naligned with the canonical basis of the representation space.\n  Motivated by these observations, we propose two novel metrics:\nImportance-Weighted Orthogonality (IWO) and Importance-Weighted Rank (IWR).\nThese metrics evaluate the mutual orthogonality and rank of generative factor\nsubspaces. Throughout extensive experiments on common downstream tasks, over\nseveral benchmark datasets and models, IWO and IWR consistently show stronger\ncorrelations with downstream task performance than traditional disentanglement\nmetrics. Our findings suggest that representation quality is closer related to\nthe orthogonality of independent generative processes rather than their\ndisentanglement, offering a new direction for evaluating and improving\nunsupervised learning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03728v2",
    "published_date": "2024-07-04 08:21:54 UTC",
    "updated_date": "2024-10-01 12:26:24 UTC"
  },
  {
    "arxiv_id": "2407.03718v2",
    "title": "Multi-Convformer: Extending Conformer with Multiple Convolution Kernels",
    "authors": [
      "Darshan Prabhu",
      "Yifan Peng",
      "Preethi Jyothi",
      "Shinji Watanabe"
    ],
    "abstract": "Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.03718v2",
    "published_date": "2024-07-04 08:08:12 UTC",
    "updated_date": "2024-07-24 02:03:47 UTC"
  },
  {
    "arxiv_id": "2407.03704v1",
    "title": "Neural Probabilistic Logic Learning for Knowledge Graph Reasoning",
    "authors": [
      "Fengsong Sun",
      "Jinyu Wang",
      "Zhiqing Wei",
      "Xianchao Zhang"
    ],
    "abstract": "Knowledge graph (KG) reasoning is a task that aims to predict unknown facts\nbased on known factual samples. Reasoning methods can be divided into two\ncategories: rule-based methods and KG-embedding based methods. The former\npossesses precise reasoning capabilities but finds it challenging to reason\nefficiently over large-scale knowledge graphs. While gaining the ability to\nreason over large-scale knowledge graphs, the latter sacrifices reasoning\naccuracy. This paper aims to design a reasoning framework called Neural\nProbabilistic Logic Learning(NPLL) that achieves accurate reasoning on\nknowledge graphs. Our approach introduces a scoring module that effectively\nenhances the expressive power of embedding networks, striking a balance between\nmodel simplicity and reasoning capabilities. We improve the interpretability of\nthe model by incorporating a Markov Logic Network based on variational\ninference. We empirically evaluate our approach on several benchmark datasets,\nand the experimental results validate that our method substantially enhances\nthe accuracy and quality of the reasoning results.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03704v1",
    "published_date": "2024-07-04 07:45:46 UTC",
    "updated_date": "2024-07-04 07:45:46 UTC"
  },
  {
    "arxiv_id": "2407.03689v1",
    "title": "Text2TimeSeries: Enhancing Financial Forecasting through Time Series Prediction Updates with Event-Driven Insights from Large Language Models",
    "authors": [
      "Litton Jose Kurisinkel",
      "Pruthwik Mishra",
      "Yue Zhang"
    ],
    "abstract": "Time series models, typically trained on numerical data, are designed to\nforecast future values. These models often rely on weighted averaging\ntechniques over time intervals. However, real-world time series data is seldom\nisolated and is frequently influenced by non-numeric factors. For instance,\nstock price fluctuations are impacted by daily random events in the broader\nworld, with each event exerting a unique influence on price signals.\nPreviously, forecasts in financial markets have been approached in two main\nways: either as time-series problems over price sequence or sentiment analysis\ntasks. The sentiment analysis tasks aim to determine whether news events will\nhave a positive or negative impact on stock prices, often categorizing them\ninto discrete labels. Recognizing the need for a more comprehensive approach to\naccurately model time series prediction, we propose a collaborative modeling\nframework that incorporates textual information about relevant events for\npredictions. Specifically, we leverage the intuition of large language models\nabout future changes to update real number time series predictions. We\nevaluated the effectiveness of our approach on financial market data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.03689v1",
    "published_date": "2024-07-04 07:21:38 UTC",
    "updated_date": "2024-07-04 07:21:38 UTC"
  },
  {
    "arxiv_id": "2407.03687v1",
    "title": "STOC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering",
    "authors": [
      "Zhenyu Bi",
      "Daniel Hajialigol",
      "Zhongkai Sun",
      "Jie Hao",
      "Xuan Wang"
    ],
    "abstract": "Multi-hop question answering (MHQA) requires a model to retrieve and\nintegrate information from multiple passages to answer a complex question.\nRecent systems leverage the power of large language models and integrate\nevidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning)\nfor the MHQA task. However, the complexities in the question types (bridge v.s.\ncomparison questions) and the reasoning types (sequential v.s. parallel\nreasonings) require more novel and fine-grained prompting methods to enhance\nthe performance of MHQA under the zero-shot setting. In this paper, we propose\nSTOC-TOT, a stochastic tree-of-thought reasoning prompting method with\nconstrained decoding for MHQA and conduct a detailed comparison with other\nreasoning prompts on different question types and reasoning types.\nSpecifically, we construct a tree-like reasoning structure by prompting the\nmodel to break down the original question into smaller sub-questions to form\ndifferent reasoning paths. In addition, we prompt the model to provide a\nprobability estimation for each reasoning path at each reasoning step. At\nanswer time, we conduct constrained decoding on the model to generate more\ngrounded answers and reduce hallucination. Experiments comparing STOC-TOT with\ntwo MHQA datasets and five large language models showed that our framework\noutperforms other reasoning prompts by a significant margin.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.03687v1",
    "published_date": "2024-07-04 07:17:53 UTC",
    "updated_date": "2024-07-04 07:17:53 UTC"
  },
  {
    "arxiv_id": "2407.03672v1",
    "title": "A Survey of Data Synthesis Approaches",
    "authors": [
      "Hsin-Yu Chang",
      "Pei-Yu Chen",
      "Tun-Hsiang Chou",
      "Chang-Sheng Kao",
      "Hsuan-Yun Yu",
      "Yen-Ting Lin",
      "Yun-Nung Chen"
    ],
    "abstract": "This paper provides a detailed survey of synthetic data techniques. We first\ndiscuss the expected goals of using synthetic data in data augmentation, which\ncan be divided into four parts: 1) Improving Diversity, 2) Data Balancing, 3)\nAddressing Domain Shift, and 4) Resolving Edge Cases. Synthesizing data are\nclosely related to the prevailing machine learning techniques at the time,\ntherefore, we summarize the domain of synthetic data techniques into four\ncategories: 1) Expert-knowledge, 2) Direct Training, 3) Pre-train then\nFine-tune, and 4) Foundation Models without Fine-tuning. Next, we categorize\nthe goals of synthetic data filtering into four types for discussion: 1) Basic\nQuality, 2) Label Consistency, and 3) Data Distribution. In section 5 of this\npaper, we also discuss the future directions of synthetic data and state three\ndirection that we believe is important: 1) focus more on quality, 2) the\nevaluation of synthetic data, and 3) multi-model data augmentation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03672v1",
    "published_date": "2024-07-04 06:37:09 UTC",
    "updated_date": "2024-07-04 06:37:09 UTC"
  },
  {
    "arxiv_id": "2407.07913v1",
    "title": "CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation",
    "authors": [
      "Rui Yang"
    ],
    "abstract": "This paper presents CaseGPT, an innovative approach that combines Large\nLanguage Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to\nenhance case-based reasoning in the healthcare and legal sectors. The system\naddresses the challenges of traditional database queries by enabling fuzzy\nsearches based on imprecise descriptions, thereby improving data searchability\nand usability. CaseGPT not only retrieves relevant case data but also generates\ninsightful suggestions and recommendations based on patterns discerned from\nexisting case data. This functionality proves especially valuable for tasks\nsuch as medical diagnostics, legal precedent research, and case strategy\nformulation. The paper includes an in-depth discussion of the system's\nmethodology, its performance in both medical and legal domains, and its\npotential for future applications. Our experiments demonstrate that CaseGPT\nsignificantly outperforms traditional keyword-based and simple LLM-based\nsystems in terms of precision, recall, and efficiency.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Submitted to ICCBR",
    "pdf_url": "http://arxiv.org/pdf/2407.07913v1",
    "published_date": "2024-07-04 06:26:51 UTC",
    "updated_date": "2024-07-04 06:26:51 UTC"
  },
  {
    "arxiv_id": "2407.03665v1",
    "title": "Heterogeneous Hypergraph Embedding for Recommendation Systems",
    "authors": [
      "Darnbi Sakong",
      "Viet Hung Vu",
      "Thanh Trung Huynh",
      "Phi Le Nguyen",
      "Hongzhi Yin",
      "Quoc Viet Hung Nguyen",
      "Thanh Tam Nguyen"
    ],
    "abstract": "Recent advancements in recommender systems have focused on integrating\nknowledge graphs (KGs) to leverage their auxiliary information. The core idea\nof KG-enhanced recommenders is to incorporate rich semantic information for\nmore accurate recommendations. However, two main challenges persist: i)\nNeglecting complex higher-order interactions in the KG-based user-item network,\npotentially leading to sub-optimal recommendations, and ii) Dealing with the\nheterogeneous modalities of input sources, such as user-item bipartite graphs\nand KGs, which may introduce noise and inaccuracies. To address these issues,\nwe present a novel Knowledge-enhanced Heterogeneous Hypergraph Recommender\nSystem (KHGRec). KHGRec captures group-wise characteristics of both the\ninteraction network and the KG, modeling complex connections in the KG. Using a\ncollaborative knowledge heterogeneous hypergraph (CKHG), it employs two\nhypergraph encoders to model group-wise interdependencies and ensure\nexplainability. Additionally, it fuses signals from the input graphs with\ncross-view self-supervised learning and attention mechanisms. Extensive\nexperiments on four real-world datasets show our model's superiority over\nvarious state-of-the-art baselines, with an average 5.18\\% relative\nimprovement. Additional tests on noise resilience, missing data, and cold-start\nproblems demonstrate the robustness of our KHGRec framework. Our model and\nevaluation datasets are publicly available at\n\\url{https://github.com/viethungvu1998/KHGRec}.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03665v1",
    "published_date": "2024-07-04 06:09:11 UTC",
    "updated_date": "2024-07-04 06:09:11 UTC"
  },
  {
    "arxiv_id": "2407.03652v1",
    "title": "Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial General Intelligence",
    "authors": [
      "Teo Susnjak",
      "Timothy R. McIntosh",
      "Andre L. C. Barczak",
      "Napoleon H. Reyes",
      "Tong Liu",
      "Paul Watters",
      "Malka N. Halgamuge"
    ],
    "abstract": "In this study, we explored the progression trajectories of artificial\nintelligence (AI) systems through the lens of complexity theory. We challenged\nthe conventional linear and exponential projections of AI advancement toward\nArtificial General Intelligence (AGI) underpinned by transformer-based\narchitectures, and posited the existence of critical points, akin to phase\ntransitions in complex systems, where AI performance might plateau or regress\ninto instability upon exceeding a critical complexity threshold. We employed\nagent-based modelling (ABM) to simulate hypothetical scenarios of AI systems'\nevolution under specific assumptions, using benchmark performance as a proxy\nfor capability and complexity. Our simulations demonstrated how increasing the\ncomplexity of the AI system could exceed an upper criticality threshold,\nleading to unpredictable performance behaviours. Additionally, we developed a\npractical methodology for detecting these critical thresholds using simulation\ndata and stochastic gradient descent to fine-tune detection thresholds. This\nresearch offers a novel perspective on AI advancement that has a particular\nrelevance to Large Language Models (LLMs), emphasising the need for a tempered\napproach to extrapolating AI's growth potential and underscoring the importance\nof developing more robust and comprehensive AI performance benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03652v1",
    "published_date": "2024-07-04 05:46:39 UTC",
    "updated_date": "2024-07-04 05:46:39 UTC"
  },
  {
    "arxiv_id": "2407.03651v2",
    "title": "Evaluating Language Model Context Windows: A \"Working Memory\" Test and Inference-time Correction",
    "authors": [
      "Amanda Dsouza",
      "Christopher Glaze",
      "Changho Shin",
      "Frederic Sala"
    ],
    "abstract": "Large language models are prominently used in real-world applications, often\ntasked with reasoning over large volumes of documents. An exciting development\nin this space is models boasting extended context capabilities, with some\naccommodating over 2 million tokens. Such long context model capabilities\nremain uncertain in production systems, motivating the need to benchmark their\nperformance on real world use cases. We address this challenge by proposing\nSWiM, an evaluation framework that addresses the limitations of standard tests.\nTesting the framework on eight long context models, we find that even strong\nmodels such as GPT-4 and Claude 3 Opus degrade in performance when information\nis present in the middle of the context window (lost-in-the-middle effect).\nNext, in addition to our benchmark, we propose medoid voting, a simple, but\neffective training-free approach that helps alleviate this effect, by\ngenerating responses a few times, each time randomly permuting documents in the\ncontext, and selecting the medoid answer. We evaluate medoid voting on single\ndocument QA tasks, achieving up to a 24% lift in accuracy. Our code is\navailable at https://github.com/snorkel-ai/long-context-eval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03651v2",
    "published_date": "2024-07-04 05:46:20 UTC",
    "updated_date": "2024-07-14 22:47:13 UTC"
  },
  {
    "arxiv_id": "2407.03647v1",
    "title": "WANCO: Weak Adversarial Networks for Constrained Optimization problems",
    "authors": [
      "Gang Bao",
      "Dong Wang",
      "Boyi Zou"
    ],
    "abstract": "This paper focuses on integrating the networks and adversarial training into\nconstrained optimization problems to develop a framework algorithm for\nconstrained optimization problems. For such problems, we first transform them\ninto minimax problems using the augmented Lagrangian method and then use two\n(or several) deep neural networks(DNNs) to represent the primal and dual\nvariables respectively. The parameters in the neural networks are then trained\nby an adversarial process. The proposed architecture is relatively insensitive\nto the scale of values of different constraints when compared to penalty based\ndeep learning methods. Through this type of training, the constraints are\nimposed better based on the augmented Lagrangian multipliers. Extensive\nexamples for optimization problems with scalar constraints, nonlinear\nconstraints, partial differential equation constraints, and inequality\nconstraints are considered to show the capability and robustness of the\nproposed method, with applications ranging from Ginzburg--Landau energy\nminimization problems, partition problems, fluid-solid topology optimization,\nto obstacle problems.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "24 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.03647v1",
    "published_date": "2024-07-04 05:37:48 UTC",
    "updated_date": "2024-07-04 05:37:48 UTC"
  },
  {
    "arxiv_id": "2407.03646v2",
    "title": "Differentiating between human-written and AI-generated texts using linguistic features automatically extracted from an online computational tool",
    "authors": [
      "Georgios P. Georgiou"
    ],
    "abstract": "While extensive research has focused on ChatGPT in recent years, very few\nstudies have systematically quantified and compared linguistic features between\nhuman-written and Artificial Intelligence (AI)-generated language. This study\naims to investigate how various linguistic components are represented in both\ntypes of texts, assessing the ability of AI to emulate human writing. Using\nhuman-authored essays as a benchmark, we prompted ChatGPT to generate essays of\nequivalent length. These texts were analyzed using Open Brain AI, an online\ncomputational tool, to extract measures of phonological, morphological,\nsyntactic, and lexical constituents. Despite AI-generated texts appearing to\nmimic human speech, the results revealed significant differences across\nmultiple linguistic features such as consonants, word stress, nouns, verbs,\npronouns, direct objects, prepositional modifiers, and use of difficult words\namong others. These findings underscore the importance of integrating automated\ntools for efficient language assessment, reducing time and effort in data\nanalysis. Moreover, they emphasize the necessity for enhanced training\nmethodologies to improve the capacity of AI for producing more human-like text.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03646v2",
    "published_date": "2024-07-04 05:37:09 UTC",
    "updated_date": "2024-07-11 10:56:01 UTC"
  },
  {
    "arxiv_id": "2407.03611v1",
    "title": "An Empirical Study on Capability of Large Language Models in Understanding Code Semantics",
    "authors": [
      "Thu-Trang Nguyen",
      "Thanh Trong Vu",
      "Hieu Dinh Vo",
      "Son Nguyen"
    ],
    "abstract": "Large Language Models for Code (code LLMs) have demonstrated remarkable\nperformance across various software engineering (SE) tasks, increasing the\napplication of code LLMs in software development. Despite the success of code\nLLMs, there remain significant concerns about the actual capabilities and\nreliability of these models, \"whether these models really learn the semantics\nof code from the training data and leverage the learned knowledge to perform\nthe SE tasks\". In this paper, we introduce EMPICA, a comprehensive framework\ndesigned to systematically and empirically evaluate the capabilities of code\nLLMs in understanding code semantics. Specifically, EMPICA systematically\nintroduces controlled modifications/transformations into the input code and\nexamines the models' responses. Generally, code LLMs must be robust to\nsemantically equivalent code inputs and be sensitive to non-equivalent ones for\nall SE tasks. Specifically, for every SE task, given an input code snippet c\nand its semantic equivalent variants, code LLMs must robustly produce\nconsistent/equivalent outputs while they are expected to generate different\noutputs for c and its semantic non-equivalent variants. Our experimental\nresults on three representative code understanding tasks, including code\nsummarization, method name prediction, and output prediction, reveal that the\nrobustness and sensitivity of the state-of-the-art code LLMs to code\ntransformations vary significantly across tasks and transformation operators.\nIn addition, the code LLMs exhibit better robustness to the semantic preserving\ntransformations than their sensitivity to the semantic non-preserving\ntransformations. These results highlight a need to enhance the model's\ncapabilities of understanding code semantics, especially the sensitivity\nproperty.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03611v1",
    "published_date": "2024-07-04 03:40:58 UTC",
    "updated_date": "2024-07-04 03:40:58 UTC"
  },
  {
    "arxiv_id": "2407.03582v1",
    "title": "Integrating Randomness in Large Language Models: A Linear Congruential Generator Approach for Generating Clinically Relevant Content",
    "authors": [
      "Andrew Bouras"
    ],
    "abstract": "Generating diverse, high-quality outputs from language models is crucial for\napplications in education and content creation. Achieving true randomness and\navoiding repetition remains a significant challenge. This study uses the Linear\nCongruential Generator method for systematic fact selection, combined with\nAI-powered content generation. We ensured unique combinations of\ngastrointestinal physiology and pathology facts across multiple rounds,\nintegrating these facts into prompts for GPT-4o to create clinically relevant,\nvignette-style outputs. Over 14 rounds, 98 unique outputs were generated,\ndemonstrating LCG's effectiveness in producing diverse and high-quality\ncontent. This method addresses key issues of randomness and repetition,\nenhancing the quality and efficiency of language model-generated content for\nvarious applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03582v1",
    "published_date": "2024-07-04 02:21:47 UTC",
    "updated_date": "2024-07-04 02:21:47 UTC"
  }
]
[
  {
    "arxiv_id": "2406.16961v1",
    "title": "Anime Popularity Prediction Before Huge Investments: a Multimodal Approach Using Deep Learning",
    "authors": [
      "Jes√∫s Armenta-Segura",
      "Grigori Sidorov"
    ],
    "abstract": "In the japanese anime industry, predicting whether an upcoming product will\nbe popular is crucial. This paper presents a dataset and methods on predicting\nanime popularity using a multimodal textimage dataset constructed exclusively\nfrom freely available internet sources. The dataset was built following\nrigorous standards based on real-life investment experiences. A deep neural\nnetwork architecture leveraging GPT-2 and ResNet-50 to embed the data was\nemployed to investigate the correlation between the multimodal text-image input\nand a popularity score, discovering relevant strengths and weaknesses in the\ndataset. To measure the accuracy of the model, mean squared error (MSE) was\nused, obtaining a best result of 0.011 when considering all inputs and the full\nversion of the deep neural network, compared to the benchmark MSE 0.412\nobtained with traditional TF-IDF and PILtotensor vectorizations. This is the\nfirst proposal to address such task with multimodal datasets, revealing the\nsubstantial benefit of incorporating image information, even when a relatively\nsmall model (ResNet-50) was used to embed them.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.16961v1",
    "published_date": "2024-06-21 23:12:59 UTC",
    "updated_date": "2024-06-21 23:12:59 UTC"
  },
  {
    "arxiv_id": "2406.15676v1",
    "title": "Inferring Pluggable Types with Machine Learning",
    "authors": [
      "Kazi Amanul Islam Siddiqui",
      "Martin Kellogg"
    ],
    "abstract": "Pluggable type systems allow programmers to extend the type system of a\nprogramming language to enforce semantic properties defined by the programmer.\nPluggable type systems are difficult to deploy in legacy codebases because they\nrequire programmers to write type annotations manually. This paper investigates\nhow to use machine learning to infer type qualifiers automatically. We propose\na novel representation, NaP-AST, that encodes minimal dataflow hints for the\neffective inference of type qualifiers. We evaluate several model architectures\nfor inferring type qualifiers, including Graph Transformer Network, Graph\nConvolutional Network and Large Language Model. We further validated these\nmodels by applying them to 12 open-source programs from a prior evaluation of\nthe NullAway pluggable typechecker, lowering warnings in all but one\nunannotated project. We discovered that GTN shows the best performance, with a\nrecall of .89 and precision of 0.6. Furthermore, we conduct a study to estimate\nthe number of Java classes needed for good performance of the trained model.\nFor our feasibility study, performance improved around 16k classes, and\ndeteriorated due to overfitting around 22k classes.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15676v1",
    "published_date": "2024-06-21 22:32:42 UTC",
    "updated_date": "2024-06-21 22:32:42 UTC"
  },
  {
    "arxiv_id": "2406.15675v3",
    "title": "Combining Neural Networks and Symbolic Regression for Analytical Lyapunov Function Discovery",
    "authors": [
      "Jie Feng",
      "Haohan Zou",
      "Yuanyuan Shi"
    ],
    "abstract": "We propose CoNSAL (Combining Neural networks and Symbolic regression for\nAnalytical Lyapunov function) to construct analytical Lyapunov functions for\nnonlinear dynamic systems. This framework contains a neural Lyapunov function\nand a symbolic regression component, where symbolic regression is applied to\ndistill the neural network to precise analytical forms. Our approach utilizes\nsymbolic regression not only as a tool for translation but also as a means to\nuncover counterexamples. This procedure terminates when no counterexamples are\nfound in the analytical formulation. Compared with previous results, CoNSAL\ndirectly produces an analytical form of the Lyapunov function with improved\ninterpretability in both the learning process and the final results. We apply\nCoNSAL to 2-D inverted pendulum, path following, Van Der Pol Oscillator, 3-D\ntrig dynamics, 4-D rotating wheel pendulum, 6-D 3-bus power system, and\ndemonstrate that our algorithm successfully finds their valid Lyapunov\nfunctions. Code examples are available at https://github.com/HaohanZou/CoNSAL.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SC",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Workshop paper, accepted by Workshop on Foundations of Reinforcement\n  Learning and Control at the 41st International Conference on Machine\n  Learning, Vienna, Austria",
    "pdf_url": "http://arxiv.org/pdf/2406.15675v3",
    "published_date": "2024-06-21 22:31:06 UTC",
    "updated_date": "2024-07-12 20:08:46 UTC"
  },
  {
    "arxiv_id": "2406.15673v2",
    "title": "Large Language Models have Intrinsic Self-Correction Ability",
    "authors": [
      "Dancheng Liu",
      "Amir Nassereldine",
      "Ziming Yang",
      "Chenhui Xu",
      "Yuting Hu",
      "Jiajie Li",
      "Utkarsh Kumar",
      "Changjae Lee",
      "Ruiyang Qin",
      "Yiyu Shi",
      "Jinjun Xiong"
    ],
    "abstract": "Large language models (LLMs) have attracted significant attention for their\nexceptional abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in submission",
    "pdf_url": "http://arxiv.org/pdf/2406.15673v2",
    "published_date": "2024-06-21 22:29:40 UTC",
    "updated_date": "2024-12-23 06:03:31 UTC"
  },
  {
    "arxiv_id": "2406.15662v2",
    "title": "Matching Problems to Solutions: An Explainable Way of Solving Machine Learning Problems",
    "authors": [
      "Lokman Saleh",
      "Hafedh Mili",
      "Mounir Boukadoum",
      "Abderrahmane Leshob"
    ],
    "abstract": "Domain experts from all fields are called upon, working with data scientists,\nto explore the use of ML techniques to solve their problems. Starting from a\ndomain problem/question, ML-based problem-solving typically involves three\nsteps: (1) formulating the business problem (problem domain) as a data analysis\nproblem (solution domain), (2) sketching a high-level ML-based solution\npattern, given the domain requirements and the properties of the available\ndata, and (3) designing and refining the different components of the solution\npattern. There has to be a substantial body of ML problem solving knowledge\nthat ML researchers agree on, and that ML practitioners routinely apply to\nsolve the most common problems. Our work deals with capturing this body of\nknowledge, and embodying it in a ML problem solving workbench to helps domain\nspecialists who are not ML experts to explore the ML solution space. This paper\nfocuses on: 1) the representation of domain problems, ML problems, and the main\nML solution artefacts, and 2) a heuristic matching function that helps identify\nthe ML algorithm family that is most appropriate for the domain problem at\nhand, given the domain (expert) requirements, and the characteristics of the\ntraining data. We review related work and outline our strategy for validating\nthe workbench",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15662v2",
    "published_date": "2024-06-21 21:39:34 UTC",
    "updated_date": "2024-09-18 19:31:02 UTC"
  },
  {
    "arxiv_id": "2406.15658v3",
    "title": "TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning",
    "authors": [
      "Nemin Wu",
      "Qian Cao",
      "Zhangyu Wang",
      "Zeping Liu",
      "Yanlin Qi",
      "Jielu Zhang",
      "Joshua Ni",
      "Xiaobai Yao",
      "Hongxu Ma",
      "Lan Mu",
      "Stefano Ermon",
      "Tanuja Ganu",
      "Akshay Nambi",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "abstract": "Spatial representation learning (SRL) aims at learning general-purpose neural\nnetwork representations from various types of spatial data (e.g., points,\npolylines, polygons, networks, images, etc.) in their native formats. Learning\ngood spatial representations is a fundamental problem for various downstream\napplications such as species distribution modeling, weather forecasting,\ntrajectory generation, geographic question answering, etc. Even though SRL has\nbecome the foundation of almost all geospatial artificial intelligence (GeoAI)\nresearch, we have not yet seen significant efforts to develop an extensive deep\nlearning framework and benchmark to support SRL model development and\nevaluation. To fill this gap, we propose TorchSpatial, a learning framework and\nbenchmark for location (point) encoding, which is one of the most fundamental\ndata types of spatial representation learning. TorchSpatial contains three key\ncomponents: 1) a unified location encoding framework that consolidates 15\ncommonly recognized location encoders, ensuring scalability and reproducibility\nof the implementations; 2) the LocBench benchmark tasks encompassing 7\ngeo-aware image classification and 10 geo-aware image regression datasets; 3) a\ncomprehensive suite of evaluation metrics to quantify geo-aware model's overall\nperformance as well as their geographic bias, with a novel Geo-Bias Score\nmetric. Finally, we provide a detailed analysis and insights into the model\nperformance and geographic bias of different location encoders. We believe\nTorchSpatial will foster future advancement of spatial representation learning\nand spatial fairness in GeoAI research. The TorchSpatial model framework and\nLocBench benchmark are available at https://github.com/seai-lab/TorchSpatial,\nand the Geo-Bias Score evaluation framework is available at\nhttps://github.com/seai-lab/PyGBS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 2 figures. Accepted by NeurIPS 2024 Datasets and Benchmarks\n  Track",
    "pdf_url": "http://arxiv.org/pdf/2406.15658v3",
    "published_date": "2024-06-21 21:33:16 UTC",
    "updated_date": "2025-01-19 22:25:54 UTC"
  },
  {
    "arxiv_id": "2406.15625v3",
    "title": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem",
    "authors": [
      "Sara Court",
      "Micha Elsner"
    ],
    "abstract": "This work investigates the in-context learning abilities of pretrained large\nlanguage models (LLMs) when instructed to translate text from a low-resource\nlanguage into a high-resource language as part of an automated machine\ntranslation pipeline. We conduct a set of experiments translating Southern\nQuechua to Spanish and examine the informativity of various types of context\nretrieved from a constrained database of digitized pedagogical materials\n(dictionaries and grammar lessons) and parallel corpora. Using both automatic\nand human evaluation of model output, we conduct ablation studies that\nmanipulate (1) context type (morpheme translations, grammar descriptions, and\ncorpus examples), (2) retrieval methods (automated vs. manual), and (3) model\ntype. Our results suggest that even relatively small LLMs are capable of\nutilizing prompt context for zero-shot low-resource translation when provided a\nminimally sufficient amount of relevant linguistic information. However, the\nvariable effects of context type, retrieval method, model type, and\nlanguage-specific factors highlight the limitations of using even the best LLMs\nas translation systems for the majority of the world's 7,000+ languages and\ntheir speakers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at the Ninth Conference on Machine Translation (WMT24)",
    "pdf_url": "http://arxiv.org/pdf/2406.15625v3",
    "published_date": "2024-06-21 20:02:22 UTC",
    "updated_date": "2024-10-24 22:24:57 UTC"
  },
  {
    "arxiv_id": "2406.15623v1",
    "title": "Marrying Compressed Sensing and Deep Signal Separation",
    "authors": [
      "Truman Hickok",
      "Sriram Nagaraj"
    ],
    "abstract": "Blind signal separation (BSS) is an important and challenging signal\nprocessing task. Given an observed signal which is a superposition of a\ncollection of unknown (hidden/latent) signals, BSS aims at recovering the\nseparate, underlying signals from only the observed mixed signal. As an\nunderdetermined problem, BSS is notoriously difficult to solve in general, and\nmodern deep learning has provided engineers with an effective set of tools to\nsolve this problem. For example, autoencoders learn a low-dimensional hidden\nencoding of the input data which can then be used to perform signal separation.\nIn real-time systems, a common bottleneck is the transmission of data\n(communications) to a central command in order to await decisions. Bandwidth\nlimits dictate the frequency and resolution of the data being transmitted. To\novercome this, compressed sensing (CS) technology allows for the direct\nacquisition of compressed data with a near optimal reconstruction guarantee.\nThis paper addresses the question: can compressive acquisition be combined with\ndeep learning for BSS to provide a complete acquire-separate-predict pipeline?\nIn other words, the aim is to perform BSS on a compressively acquired signal\ndirectly without ever having to decompress the signal. We consider image data\n(MNIST and E-MNIST) and show how our compressive autoencoder approach solves\nthe problem of compressive BSS. We also provide some theoretical insights into\nthe problem.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "68T07 68T07"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15623v1",
    "published_date": "2024-06-21 20:00:34 UTC",
    "updated_date": "2024-06-21 20:00:34 UTC"
  },
  {
    "arxiv_id": "2406.15619v1",
    "title": "Physics Informed Machine Learning (PIML) methods for estimating the remaining useful lifetime (RUL) of aircraft engines",
    "authors": [
      "Sriram Nagaraj",
      "Truman Hickok"
    ],
    "abstract": "This paper is aimed at using the newly developing field of physics informed\nmachine learning (PIML) to develop models for predicting the remaining useful\nlifetime (RUL) aircraft engines. We consider the well-known benchmark NASA\nCommercial Modular Aero-Propulsion System Simulation (C-MAPSS) data as the main\ndata for this paper, which consists of sensor outputs in a variety of different\noperating modes. C-MAPSS is a well-studied dataset with much existing work in\nthe literature that address RUL prediction with classical and deep learning\nmethods. In the absence of published empirical physical laws governing the\nC-MAPSS data, our approach first uses stochastic methods to estimate the\ngoverning physics models from the noisy time series data. In our approach, we\nmodel the various sensor readings as being governed by stochastic differential\nequations, and we estimate the corresponding transition density mean and\nvariance functions of the underlying processes. We then augment LSTM\n(long-short term memory) models with the learned mean and variance functions\nduring training and inferencing. Our PIML based approach is different from\nprevious methods, and we use the data to first learn the physics. Our results\nindicate that PIML discovery and solutions methods are well suited for this\nproblem and outperform previous data-only deep learning methods for this data\nset and task. Moreover, the framework developed herein is flexible, and can be\nadapted to other situations (other sensor modalities or combined multi-physics\nenvironments), including cases where the underlying physics is only partially\nobserved or known.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "65C20 65C20 65C20"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15619v1",
    "published_date": "2024-06-21 19:55:34 UTC",
    "updated_date": "2024-06-21 19:55:34 UTC"
  },
  {
    "arxiv_id": "2406.15609v3",
    "title": "Automated radiotherapy treatment planning guided by GPT-4Vision",
    "authors": [
      "Sheng Liu",
      "Oscar Pastor-Serrano",
      "Yizheng Chen",
      "Matthew Gopaulchan",
      "Weixing Liang",
      "Mark Buyyounouski",
      "Erqi Pollom",
      "Quynh-Thu Le",
      "Michael Gensheimer",
      "Peng Dong",
      "Yong Yang",
      "James Zou",
      "Lei Xing"
    ],
    "abstract": "Objective: Radiotherapy treatment planning is a time-consuming and\npotentially subjective process that requires the iterative adjustment of model\nparameters to balance multiple conflicting objectives. Recent advancements in\nfrontier Artificial Intelligence (AI) models offer promising avenues for\naddressing the challenges in planning and clinical decision-making. This study\nintroduces GPT-RadPlan, an automated treatment planning framework that\nintegrates radiation oncology knowledge with the reasoning capabilities of\nlarge multi-modal models, such as GPT-4Vision (GPT-4V) from OpenAI.\n  Approach: Via in-context learning, we incorporate clinical requirements and a\nfew (3 in our experiments) approved clinical plans with their optimization\nsettings, enabling GPT-4V to acquire treatment planning domain knowledge. The\nresulting GPT-RadPlan system is integrated into our in-house inverse treatment\nplanning system through an application programming interface (API). For a given\npatient, GPT-RadPlan acts as both plan evaluator and planner, first assessing\ndose distributions and dose-volume histograms (DVHs), and then providing\ntextual feedback on how to improve the plan to match the physician's\nrequirements. In this manner, GPT-RadPlan iteratively refines the plan by\nadjusting planning parameters, such as weights and dose objectives, based on\nits suggestions.\n  Main results: The efficacy of the automated planning system is showcased\nacross 17 prostate cancer and 13 head and neck cancer VMAT plans with\nprescribed doses of 70.2 Gy and 72 Gy, respectively, where we compared\nGPT-RadPlan results to clinical plans produced by human experts. In all cases,\nGPT-RadPlan either outperformed or matched the clinical plans, demonstrating\nsuperior target coverage and reducing organ-at-risk doses by 5 Gy on average\n(15 percent for prostate and 10-15 percent for head and neck).",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.15609v3",
    "published_date": "2024-06-21 19:23:03 UTC",
    "updated_date": "2025-04-08 00:19:04 UTC"
  },
  {
    "arxiv_id": "2407.00075v5",
    "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference",
    "authors": [
      "Anton Xue",
      "Avishree Khare",
      "Rajeev Alur",
      "Surbhi Goel",
      "Eric Wong"
    ],
    "abstract": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00075v5",
    "published_date": "2024-06-21 19:18:16 UTC",
    "updated_date": "2025-02-28 17:50:12 UTC"
  },
  {
    "arxiv_id": "2406.15599v2",
    "title": "Pareto-Optimal Learning from Preferences with Hidden Context",
    "authors": [
      "Ryan Bahlous-Boldi",
      "Li Ding",
      "Lee Spector",
      "Scott Niekum"
    ],
    "abstract": "Ensuring AI models align with human values is essential for their safety and\nfunctionality. Reinforcement learning from human feedback (RLHF) leverages\nhuman preferences to achieve this alignment. However, when preferences are\nsourced from diverse populations, point estimates of reward can result in\nsuboptimal performance or be unfair to specific groups. We propose Pareto\nOptimal Preference Learning (POPL), which enables pluralistic alignment by\nframing discrepant group preferences as objectives with potential trade-offs,\naiming for policies that are Pareto-optimal on the preference dataset. POPL\nutilizes lexicase selection, an iterative process that selects diverse and\nPareto-optimal solutions. Our theoretical and empirical evaluations demonstrate\nthat POPL surpasses baseline methods in learning sets of reward functions and\npolicies, effectively catering to distinct groups without access to group\nnumbers or membership labels. We verify the performance of POPL on a stateless\npreference learning setting, a Minigrid RL domain, Metaworld robotics\nbenchmarks, as well as large language model (LLM) fine-tuning. We illustrate\nthat POPL can also serve as a foundation for techniques optimizing specific\nnotions of group fairness, ensuring safe and equitable AI model alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15599v2",
    "published_date": "2024-06-21 18:57:38 UTC",
    "updated_date": "2025-02-07 17:29:48 UTC"
  },
  {
    "arxiv_id": "2406.15575v1",
    "title": "Sketch-GNN: Scalable Graph Neural Networks with Sublinear Training Complexity",
    "authors": [
      "Mucong Ding",
      "Tahseen Rabbani",
      "Bang An",
      "Evan Z Wang",
      "Furong Huang"
    ],
    "abstract": "Graph Neural Networks (GNNs) are widely applied to graph learning problems\nsuch as node classification. When scaling up the underlying graphs of GNNs to a\nlarger size, we are forced to either train on the complete graph and keep the\nfull graph adjacency and node embeddings in memory (which is often infeasible)\nor mini-batch sample the graph (which results in exponentially growing\ncomputational complexities with respect to the number of GNN layers). Various\nsampling-based and historical-embedding-based methods are proposed to avoid\nthis exponential growth of complexities. However, none of these solutions\neliminates the linear dependence on graph size. This paper proposes a\nsketch-based algorithm whose training time and memory grow sublinearly with\nrespect to graph size by training GNNs atop a few compact sketches of graph\nadjacency and node embeddings. Based on polynomial tensor-sketch (PTS) theory,\nour framework provides a novel protocol for sketching non-linear activations\nand graph convolution matrices in GNNs, as opposed to existing methods that\nsketch linear weights or gradients in neural networks. In addition, we develop\na locality-sensitive hashing (LSH) technique that can be trained to improve the\nquality of sketches. Experiments on large-graph benchmarks demonstrate the\nscalability and competitive performance of our Sketch-GNNs versus their\nfull-size GNN counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2022",
    "pdf_url": "http://arxiv.org/pdf/2406.15575v1",
    "published_date": "2024-06-21 18:22:11 UTC",
    "updated_date": "2024-06-21 18:22:11 UTC"
  },
  {
    "arxiv_id": "2406.15567v1",
    "title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models",
    "authors": [
      "Mucong Ding",
      "Souradip Chakraborty",
      "Vibhu Agrawal",
      "Zora Che",
      "Alec Koppel",
      "Mengdi Wang",
      "Amrit Bedi",
      "Furong Huang"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a key method for\naligning large language models (LLMs) with human preferences. However, current\noffline alignment approaches like DPO, IPO, and SLiC rely heavily on fixed\npreference datasets, which can lead to sub-optimal performance. On the other\nhand, recent literature has focused on designing online RLHF methods but still\nlacks a unified conceptual formulation and suffers from distribution shift\nissues. To address this, we establish that online LLM alignment is underpinned\nby bilevel optimization. By reducing this formulation to an efficient\nsingle-level first-order method (using the reward-policy equivalence), our\napproach generates new samples and iteratively refines model alignment by\nexploring responses and regulating preference labels. In doing so, we permit\nalignment methods to operate in an online and self-improving manner, as well as\ngeneralize prior online RLHF methods as special cases. Compared to\nstate-of-the-art iterative RLHF methods, our approach significantly improves\nalignment performance on open-sourced datasets with minimal computational\noverhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 6 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.15567v1",
    "published_date": "2024-06-21 18:05:35 UTC",
    "updated_date": "2024-06-21 18:05:35 UTC"
  },
  {
    "arxiv_id": "2406.15349v2",
    "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
    "authors": [
      "Daniel Dauner",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Xinshuo Weng",
      "Zhiyu Huang",
      "Zetong Yang",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "abstract": "Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Datasets and Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2406.15349v2",
    "published_date": "2024-06-21 17:59:02 UTC",
    "updated_date": "2024-10-31 17:58:34 UTC"
  },
  {
    "arxiv_id": "2406.15346v1",
    "title": "Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach",
    "authors": [
      "Chengzhe Piao",
      "Taiyu Zhu",
      "Yu Wang",
      "Stephanie E Baldeweg",
      "Paul Taylor",
      "Pantelis Georgiou",
      "Jiahao Sun",
      "Jun Wang",
      "Kezhi Li"
    ],
    "abstract": "Newly diagnosed Type 1 Diabetes (T1D) patients often struggle to obtain\neffective Blood Glucose (BG) prediction models due to the lack of sufficient BG\ndata from Continuous Glucose Monitoring (CGM), presenting a significant \"cold\nstart\" problem in patient care. Utilizing population models to address this\nchallenge is a potential solution, but collecting patient data for training\npopulation models in a privacy-conscious manner is challenging, especially\ngiven that such data is often stored on personal devices. Considering the\nprivacy protection and addressing the \"cold start\" problem in diabetes care, we\npropose \"GluADFL\", blood Glucose prediction by Asynchronous Decentralized\nFederated Learning. We compared GluADFL with eight baseline methods using four\ndistinct T1D datasets, comprising 298 participants, which demonstrated its\nsuperior performance in accurately predicting BG levels for cross-patient\nanalysis. Furthermore, patients' data might be stored and shared across various\ncommunication networks in GluADFL, ranging from highly interconnected (e.g.,\nrandom, performs the best among others) to more structured topologies (e.g.,\ncluster and ring), suitable for various social networks. The asynchronous\ntraining framework supports flexible participation. By adjusting the ratios of\ninactive participants, we found it remains stable if less than 70% are\ninactive. Our results confirm that GluADFL offers a practical,\nprivacy-preserving solution for BG prediction in T1D, significantly enhancing\nthe quality of diabetes management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15346v1",
    "published_date": "2024-06-21 17:57:39 UTC",
    "updated_date": "2024-06-21 17:57:39 UTC"
  },
  {
    "arxiv_id": "2406.15341v3",
    "title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data Analysis",
    "authors": [
      "Haoyang Liu",
      "Shuyu Chen",
      "Ye Zhang",
      "Haohan Wang"
    ],
    "abstract": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides analysis code and results for solving a wide\nrange of gene-trait association problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTEX.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.15341v3",
    "published_date": "2024-06-21 17:55:24 UTC",
    "updated_date": "2025-04-08 17:09:04 UTC"
  },
  {
    "arxiv_id": "2406.15339v1",
    "title": "Image Conductor: Precision Control for Interactive Video Synthesis",
    "authors": [
      "Yaowei Li",
      "Xintao Wang",
      "Zhaoyang Zhang",
      "Zhouxia Wang",
      "Ziyang Yuan",
      "Liangbin Xie",
      "Yuexian Zou",
      "Ying Shan"
    ],
    "abstract": "Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage available at\n  https://liyaowei-stu.github.io/project/ImageConductor/",
    "pdf_url": "http://arxiv.org/pdf/2406.15339v1",
    "published_date": "2024-06-21 17:55:05 UTC",
    "updated_date": "2024-06-21 17:55:05 UTC"
  },
  {
    "arxiv_id": "2406.15334v3",
    "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
    "authors": [
      "Brandon Huang",
      "Chancharik Mitra",
      "Assaf Arbelle",
      "Leonid Karlinsky",
      "Trevor Darrell",
      "Roei Herzig"
    ],
    "abstract": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV) -- compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15334v3",
    "published_date": "2024-06-21 17:50:02 UTC",
    "updated_date": "2024-12-20 01:24:51 UTC"
  },
  {
    "arxiv_id": "2406.15330v2",
    "title": "Enhancing Large Language Model Performance with Gradient-Based Parameter Selection",
    "authors": [
      "Haoling Li",
      "Xin Zhang",
      "Xiao Liu",
      "Yeyun Gong",
      "Yifan Wang",
      "Qi Chen",
      "Peng Cheng"
    ],
    "abstract": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.15330v2",
    "published_date": "2024-06-21 17:42:52 UTC",
    "updated_date": "2025-02-13 13:06:00 UTC"
  },
  {
    "arxiv_id": "2406.15329v1",
    "title": "An End-to-End, Segmentation-Free, Arabic Handwritten Recognition Model on KHATT",
    "authors": [
      "Sondos Aabed",
      "Ahmad Khairaldin"
    ],
    "abstract": "An end-to-end, segmentation-free, deep learning model trained from scratch is\nproposed, leveraging DCNN for feature extraction, alongside Bidirectional\nLong-Short Term Memory (BLSTM) for sequence recognition and Connectionist\nTemporal Classification (CTC) loss function on the KHATT database. The training\nphase yields remarkable results 84% recognition rate on the test dataset at the\ncharacter level and 71% on the word level, establishing an image-based sequence\nrecognition framework that operates without segmentation only at the line\nlevel. The analysis and preprocessing of the KFUPM Handwritten Arabic TexT\n(KHATT) database are also presented. Finally, advanced image processing\ntechniques, including filtering, transformation, and line segmentation are\nimplemented. The importance of this work is highlighted by its wide-ranging\napplications. Including digitizing, documentation, archiving, and text\ntranslation in fields such as banking. Moreover, AHR serves as a pivotal tool\nfor making images searchable, enhancing information retrieval capabilities, and\nenabling effortless editing. This functionality significantly reduces the time\nand effort required for tasks such as Arabic data organization and\nmanipulation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15329v1",
    "published_date": "2024-06-21 17:42:07 UTC",
    "updated_date": "2024-06-21 17:42:07 UTC"
  },
  {
    "arxiv_id": "2406.15325v1",
    "title": "Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks",
    "authors": [
      "Hokyung Lee",
      "Sumanyu Sharma",
      "Bing Hu"
    ],
    "abstract": "Recent research in Needle-in-a-Haystack (NIAH) benchmarks has explored the\ncapabilities of Large Language Models (LLMs) in retrieving contextual\ninformation from large text documents. However, as LLMs become increasingly\nintegrated into software development processes, it is crucial to evaluate their\nperformance in code-based environments. As LLMs are further developed for\nprogram synthesis, we need to ensure that LLMs can understand syntax and write\nsyntactically correct code. As a step in ensuring LLMs understand syntax, LLMs\ncan be evaluated in their ability to find and detect syntax bugs. Our\nbenchmark, Bug In The Code Stack (BICS), is designed to assess the ability of\nLLMs to identify simple syntax bugs within large source code. Our findings\nreveal three key insights: (1) code-based environments pose significantly more\nchallenge compared to text-based environments for retrieval tasks, (2) there is\na substantial performance disparity among different models, and (3) there is a\nnotable correlation between longer context lengths and performance degradation,\nthough the extent of this degradation varies between models.",
    "categories": [
      "cs.AI",
      "cs.SE",
      "68T50",
      "I.2.7; D.2.5"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.15325v1",
    "published_date": "2024-06-21 17:37:10 UTC",
    "updated_date": "2024-06-21 17:37:10 UTC"
  },
  {
    "arxiv_id": "2406.15319v3",
    "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
    "authors": [
      "Ziyan Jiang",
      "Xueguang Ma",
      "Wenhu Chen"
    ],
    "abstract": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2406.15319v3",
    "published_date": "2024-06-21 17:23:21 UTC",
    "updated_date": "2024-09-01 17:21:18 UTC"
  },
  {
    "arxiv_id": "2406.15537v1",
    "title": "R&B -- Rhythm and Brain: Cross-subject Decoding of Music from Human Brain Activity",
    "authors": [
      "Matteo Ferrante",
      "Matteo Ciferri",
      "Nicola Toschi"
    ],
    "abstract": "Music is a universal phenomenon that profoundly influences human experiences\nacross cultures. This study investigates whether music can be decoded from\nhuman brain activity measured with functional MRI (fMRI) during its perception.\nLeveraging recent advancements in extensive datasets and pre-trained\ncomputational models, we construct mappings between neural data and latent\nrepresentations of musical stimuli. Our approach integrates functional and\nanatomical alignment techniques to facilitate cross-subject decoding,\naddressing the challenges posed by the low temporal resolution and\nsignal-to-noise ratio (SNR) in fMRI data. Starting from the GTZan fMRI dataset,\nwhere five participants listened to 540 musical stimuli from 10 different\ngenres while their brain activity was recorded, we used the CLAP (Contrastive\nLanguage-Audio Pretraining) model to extract latent representations of the\nmusical stimuli and developed voxel-wise encoding models to identify brain\nregions responsive to these stimuli. By applying a threshold to the association\nbetween predicted and actual brain activity, we identified specific regions of\ninterest (ROIs) which can be interpreted as key players in music processing.\nOur decoding pipeline, primarily retrieval-based, employs a linear map to\nproject brain activity to the corresponding CLAP features. This enables us to\npredict and retrieve the musical stimuli most similar to those that originated\nthe fMRI data. Our results demonstrate state-of-the-art identification\naccuracy, with our methods significantly outperforming existing approaches. Our\nfindings suggest that neural-based music retrieval systems could enable\npersonalized recommendations and therapeutic applications. Future work could\nuse higher temporal resolution neuroimaging and generative models to improve\ndecoding accuracy and explore the neural underpinnings of music perception and\nemotion.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "q-bio.NC",
    "comment": "The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2406.15537v1",
    "published_date": "2024-06-21 17:11:45 UTC",
    "updated_date": "2024-06-21 17:11:45 UTC"
  },
  {
    "arxiv_id": "2406.15293v1",
    "title": "Grants4Companies: Applying Declarative Methods for Recommending and Reasoning About Business Grants in the Austrian Public Administration (System Description)",
    "authors": [
      "Bj√∂rn Lellmann",
      "Philipp Marek",
      "Markus Triska"
    ],
    "abstract": "We describe the methods and technologies underlying the application\nGrants4Companies. The application uses a logic-based expert system to display a\nlist of business grants suitable for the logged-in business. To evaluate\nsuitability of the grants, formal representations of their conditions are\nevaluated against properties of the business, taken from the registers of the\nAustrian public administration. The logical language for the representations of\nthe grant conditions is based on S-expressions. We further describe a Proof of\nConcept implementation of reasoning over the formalised grant conditions. The\nproof of concept is implemented in Common Lisp and interfaces with a reasoning\nengine implemented in Scryer Prolog. The application has recently gone live and\nis provided as part of the Business Service Portal by the Austrian Federal\nMinistry of Finance.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15293v1",
    "published_date": "2024-06-21 16:38:02 UTC",
    "updated_date": "2024-06-21 16:38:02 UTC"
  },
  {
    "arxiv_id": "2406.15279v2",
    "title": "Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Model",
    "authors": [
      "Siyin Wang",
      "Xingsong Ye",
      "Qinyuan Cheng",
      "Junwen Duan",
      "Shimin Li",
      "Jinlan Fu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "abstract": "As Artificial General Intelligence (AGI) becomes increasingly integrated into\nvarious facets of human life, ensuring the safety and ethical alignment of such\nsystems is paramount. Previous studies primarily focus on single-modality\nthreats, which may not suffice given the integrated and complex nature of\ncross-modality interactions. We introduce a novel safety alignment challenge\ncalled Safe Inputs but Unsafe Output (SIUO) to evaluate cross-modality safety\nalignment. Specifically, it considers cases where single modalities are safe\nindependently but could potentially lead to unsafe or unethical outputs when\ncombined. To empirically investigate this problem, we developed the SIUO, a\ncross-modality benchmark encompassing 9 critical safety domains, such as\nself-harm, illegal activities, and privacy violations. Our findings reveal\nsubstantial safety vulnerabilities in both closed- and open-source LVLMs, such\nas GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably\ninterpret and respond to complex, real-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15279v2",
    "published_date": "2024-06-21 16:14:15 UTC",
    "updated_date": "2025-02-17 03:38:42 UTC"
  },
  {
    "arxiv_id": "2406.15268v1",
    "title": "Towards Robust Training Datasets for Machine Learning with Ontologies: A Case Study for Emergency Road Vehicle Detection",
    "authors": [
      "Lynn Vonderhaar",
      "Timothy Elvira",
      "Tyler Procko",
      "Omar Ochoa"
    ],
    "abstract": "Countless domains rely on Machine Learning (ML) models, including\nsafety-critical domains, such as autonomous driving, which this paper focuses\non. While the black box nature of ML is simply a nuisance in some domains, in\nsafety-critical domains, this makes ML models difficult to trust. To fully\nutilize ML models in safety-critical domains, it would be beneficial to have a\nmethod to improve trust in model robustness and accuracy without human experts\nchecking each decision. This research proposes a method to increase trust in ML\nmodels used in safety-critical domains by ensuring the robustness and\ncompleteness of the model's training dataset. Because ML models embody what\nthey are trained with, ensuring the completeness of training datasets can help\nto increase the trust in the training of ML models. To this end, this paper\nproposes the use of a domain ontology and an image quality characteristic\nontology to validate the domain completeness and image quality robustness of a\ntraining dataset. This research also presents an experiment as a proof of\nconcept for this method, where ontologies are built for the emergency road\nvehicle domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15268v1",
    "published_date": "2024-06-21 16:03:38 UTC",
    "updated_date": "2024-06-21 16:03:38 UTC"
  },
  {
    "arxiv_id": "2406.15259v2",
    "title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions",
    "authors": [
      "Luca Podo",
      "Marco Angelini",
      "Paola Velardi"
    ],
    "abstract": "NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15259v2",
    "published_date": "2024-06-21 15:50:10 UTC",
    "updated_date": "2024-07-31 11:39:32 UTC"
  },
  {
    "arxiv_id": "2406.15252v3",
    "title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation",
    "authors": [
      "Xuan He",
      "Dongfu Jiang",
      "Ge Zhang",
      "Max Ku",
      "Achint Soni",
      "Sherman Siu",
      "Haonan Chen",
      "Abhranil Chandra",
      "Ziyan Jiang",
      "Aaran Arulraj",
      "Kai Wang",
      "Quy Duc Do",
      "Yuansheng Ni",
      "Bohan Lyu",
      "Yaswanth Narsupalli",
      "Rongqi Fan",
      "Zhiheng Lyu",
      "Yuchen Lin",
      "Wenhu Chen"
    ],
    "abstract": "The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train VideoScore (initialized from Mantis)\nbased on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between VideoScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that VideoScore has consistently much higher correlation with human\njudges than other metrics. Due to these results, we believe VideoScore can\nserve as a great proxy for human raters to (1) rate different video models to\ntrack progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15252v3",
    "published_date": "2024-06-21 15:43:46 UTC",
    "updated_date": "2024-10-14 04:08:53 UTC"
  },
  {
    "arxiv_id": "2406.15231v4",
    "title": "Synthetic Lyrics Detection Across Languages and Genres",
    "authors": [
      "Yanis Labrak",
      "Markus Frohmann",
      "Gabriel Meseguer-Brocal",
      "Elena V. Epure"
    ],
    "abstract": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in the TrustNLP Workshop at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.15231v4",
    "published_date": "2024-06-21 15:19:21 UTC",
    "updated_date": "2025-04-24 07:21:44 UTC"
  },
  {
    "arxiv_id": "2406.15225v1",
    "title": "Deep UAV Path Planning with Assured Connectivity in Dense Urban Setting",
    "authors": [
      "Jiyong Oh",
      "Syed M. Raza",
      "Lusungu J. Mwasinga",
      "Moonseong Kim",
      "Hyunseung Choo"
    ],
    "abstract": "Unmanned Ariel Vehicle (UAV) services with 5G connectivity is an emerging\nfield with numerous applications. Operator-controlled UAV flights and manual\nstatic flight configurations are major limitations for the wide adoption of\nscalability of UAV services. Several services depend on excellent UAV\nconnectivity with a cellular network and maintaining it is challenging in\npredetermined flight paths. This paper addresses these limitations by proposing\na Deep Reinforcement Learning (DRL) framework for UAV path planning with\nassured connectivity (DUPAC). During UAV flight, DUPAC determines the best\nroute from a defined source to the destination in terms of distance and signal\nquality. The viability and performance of DUPAC are evaluated under simulated\nreal-world urban scenarios using the Unity framework. The results confirm that\nDUPAC achieves an autonomous UAV flight path similar to base method with only\n2% increment while maintaining an average 9% better connection quality\nthroughout the flight.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 4 figures, Published in the 2024 IEEE Network Operations and\n  Management Symposium (NOMS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.15225v1",
    "published_date": "2024-06-21 15:10:25 UTC",
    "updated_date": "2024-06-21 15:10:25 UTC"
  },
  {
    "arxiv_id": "2406.15213v2",
    "title": "Backdooring Bias into Text-to-Image Models",
    "authors": [
      "Ali Naseh",
      "Jaechul Roh",
      "Eugene Bagdasaryan",
      "Amir Houmansadr"
    ],
    "abstract": "Text-conditional diffusion models, i.e. text-to-image, produce eye-catching\nimages that represent descriptions given by a user. These images often depict\nbenign concepts but could also carry other purposes. Specifically, visual\ninformation is easy to comprehend and could be weaponized for propaganda -- a\nserious challenge given widespread usage and deployment of generative models.\nIn this paper, we show that an adversary can add an arbitrary bias through a\nbackdoor attack that would affect even benign users generating images. While a\nuser could inspect a generated image to comply with the given text description,\nour attack remains stealthy as it preserves semantic information given in the\ntext prompt. Instead, a compromised model modifies other unspecified features\nof the image to add desired biases (that increase by 4-8x). Furthermore, we\nshow how the current state-of-the-art generative models make this attack both\ncheap and feasible for any adversary, with costs ranging between $12-$18. We\nevaluate our attack over various types of triggers, adversary objectives, and\nbiases and discuss mitigations and future work. Our code is available at\nhttps://github.com/jrohsc/Backdororing_Bias.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15213v2",
    "published_date": "2024-06-21 14:53:19 UTC",
    "updated_date": "2024-10-10 21:21:22 UTC"
  },
  {
    "arxiv_id": "2406.15211v1",
    "title": "How Effective is GPT-4 Turbo in Generating School-Level Questions from Textbooks Based on Bloom's Revised Taxonomy?",
    "authors": [
      "Subhankar Maity",
      "Aniket Deroy",
      "Sudeshna Sarkar"
    ],
    "abstract": "We evaluate the effectiveness of GPT-4 Turbo in generating educational\nquestions from NCERT textbooks in zero-shot mode. Our study highlights GPT-4\nTurbo's ability to generate questions that require higher-order thinking\nskills, especially at the \"understanding\" level according to Bloom's Revised\nTaxonomy. While we find a notable consistency between questions generated by\nGPT-4 Turbo and those assessed by humans in terms of complexity, there are\noccasional differences. Our evaluation also uncovers variations in how humans\nand machines evaluate question quality, with a trend inversely related to\nBloom's Revised Taxonomy levels. These findings suggest that while GPT-4 Turbo\nis a promising tool for educational question generation, its efficacy varies\nacross different cognitive levels, indicating a need for further refinement to\nfully meet educational standards.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Learnersourcing: Student-Generated Content @ Scale 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15211v1",
    "published_date": "2024-06-21 14:52:37 UTC",
    "updated_date": "2024-06-21 14:52:37 UTC"
  },
  {
    "arxiv_id": "2406.15198v1",
    "title": "Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms",
    "authors": [
      "Santiago Berrezueta-Guzman",
      "Mohanad Kandil",
      "Mar√≠a-Luisa Mart√≠n-Ruiz",
      "Iv√°n Pau-de-la-Cruz",
      "Stephan Krusche"
    ],
    "abstract": "Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental\ncondition characterized by inattention, hyperactivity, and impulsivity, which\ncan significantly impact an individual's daily functioning and quality of life.\nOccupational therapy plays a crucial role in managing ADHD by fostering the\ndevelopment of skills needed for daily living and enhancing an individual's\nability to participate fully in school, home, and social situations. Recent\nstudies highlight the potential of integrating Large Language Models (LLMs)\nlike ChatGPT and Socially Assistive Robots (SAR) to improve psychological\ntreatments. This integration aims to overcome existing limitations in mental\nhealth therapy by providing tailored support and adapting to the unique needs\nof this sensitive group. However, there remains a significant gap in research\nexploring the combined use of these advanced technologies in ADHD therapy,\nsuggesting an opportunity for novel therapeutic approaches.\n  Thus, we integrated two advanced language models, ChatGPT-4 Turbo and\nClaude-3 Opus, into a robotic assistant to explore how well each model performs\nin robot-assisted interactions. Additionally, we have compared their\nperformance in a simulated therapy scenario to gauge their effectiveness\nagainst a clinically validated customized model. The results of this study show\nthat ChatGPT-4 Turbo excelled in performance and responsiveness, making it\nsuitable for time-sensitive applications. Claude-3 Opus, on the other hand,\nshowed strengths in understanding, coherence, and ethical considerations,\nprioritizing safe and engaging interactions. Both models demonstrated\ninnovation and adaptability, but ChatGPT-4 Turbo offered greater ease of\nintegration and broader language support. The selection between them hinges on\nthe specific demands of ADHD therapy.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted at the 20th International Conference on Intelligent\n  Environments",
    "pdf_url": "http://arxiv.org/pdf/2406.15198v1",
    "published_date": "2024-06-21 14:38:25 UTC",
    "updated_date": "2024-06-21 14:38:25 UTC"
  },
  {
    "arxiv_id": "2406.15187v2",
    "title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis",
    "authors": [
      "Yulong Hui",
      "Yao Lu",
      "Huanchen Zhang"
    ],
    "abstract": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15187v2",
    "published_date": "2024-06-21 14:29:39 UTC",
    "updated_date": "2024-10-31 10:10:28 UTC"
  },
  {
    "arxiv_id": "2406.15175v1",
    "title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss",
    "authors": [
      "Wei He",
      "Marco Idiart",
      "Carolina Scarton",
      "Aline Villavicencio"
    ],
    "abstract": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15175v1",
    "published_date": "2024-06-21 14:21:41 UTC",
    "updated_date": "2024-06-21 14:21:41 UTC"
  },
  {
    "arxiv_id": "2406.15173v1",
    "title": "√âvaluation des capacit√©s de r√©ponse de larges mod√®les de langage (LLM) pour des questions d'historiens",
    "authors": [
      "Mathieu Chartier",
      "Nabil Dakkoune",
      "Guillaume Bourgeois",
      "St√©phane Jean"
    ],
    "abstract": "Large Language Models (LLMs) like ChatGPT or Bard have revolutionized\ninformation retrieval and captivated the audience with their ability to\ngenerate custom responses in record time, regardless of the topic. In this\narticle, we assess the capabilities of various LLMs in producing reliable,\ncomprehensive, and sufficiently relevant responses about historical facts in\nFrench. To achieve this, we constructed a testbed comprising numerous\nhistory-related questions of varying types, themes, and levels of difficulty.\nOur evaluation of responses from ten selected LLMs reveals numerous\nshortcomings in both substance and form. Beyond an overall insufficient\naccuracy rate, we highlight uneven treatment of the French language, as well as\nissues related to verbosity and inconsistency in the responses provided by\nLLMs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "in French language",
    "pdf_url": "http://arxiv.org/pdf/2406.15173v1",
    "published_date": "2024-06-21 14:19:57 UTC",
    "updated_date": "2024-06-21 14:19:57 UTC"
  },
  {
    "arxiv_id": "2406.15534v1",
    "title": "Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
    "authors": [
      "Tianyu Liu",
      "Yijia Xiao",
      "Xiao Luo",
      "Hua Xu",
      "W. Jim Zheng",
      "Hongyu Zhao"
    ],
    "abstract": "The applications of large language models (LLMs) are promising for biomedical\nand healthcare research. Despite the availability of open-source LLMs trained\nusing a wide range of biomedical data, current research on the applications of\nLLMs to genomics and proteomics is still limited. To fill this gap, we propose\na collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse,\nfor three novel tasks in genomic and proteomic research. The models in\nGeneverse are trained and evaluated based on domain-specific datasets, and we\nuse advanced parameter-efficient finetuning techniques to achieve the model\nadaptation for tasks including the generation of descriptions for gene\nfunctions, protein function inference from its structure, and marker gene\nselection from spatial transcriptomic data. We demonstrate that adapted LLMs\nand MLLMs perform well for these tasks and may outperform closed-source\nlarge-scale models based on our evaluations focusing on both truthfulness and\nstructural correctness. All of the training strategies and base models we used\nare freely accessible.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.15534v1",
    "published_date": "2024-06-21 14:19:10 UTC",
    "updated_date": "2024-06-21 14:19:10 UTC"
  },
  {
    "arxiv_id": "2406.15168v2",
    "title": "This actually looks like that: Proto-BagNets for local and global interpretability-by-design",
    "authors": [
      "Kerol Djoumessi",
      "Bubacarr Bah",
      "Laura K√ºhlewein",
      "Philipp Berens",
      "Lisa Koch"
    ],
    "abstract": "Interpretability is a key requirement for the use of machine learning models\nin high-stakes applications, including medical diagnosis. Explaining black-box\nmodels mostly relies on post-hoc methods that do not faithfully reflect the\nmodel's behavior. As a remedy, prototype-based networks have been proposed, but\ntheir interpretability is limited as they have been shown to provide coarse,\nunreliable, and imprecise explanations. In this work, we introduce\nProto-BagNets, an interpretable-by-design prototype-based model that combines\nthe advantages of bag-of-local feature models and prototype learning to provide\nmeaningful, coherent, and relevant prototypical parts needed for accurate and\ninterpretable image classification tasks. We evaluated the Proto-BagNet for\ndrusen detection on publicly available retinal OCT data. The Proto-BagNet\nperformed comparably to the state-of-the-art interpretable and\nnon-interpretable models while providing faithful, accurate, and clinically\nmeaningful local and global explanations. The code is available at\nhttps://github.com/kdjoumessi/Proto-BagNets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15168v2",
    "published_date": "2024-06-21 14:12:15 UTC",
    "updated_date": "2024-06-24 08:13:07 UTC"
  },
  {
    "arxiv_id": "2406.15156v2",
    "title": "Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs",
    "authors": [
      "Steve Azzolin",
      "Antonio Longa",
      "Stefano Teso",
      "Andrea Passerini"
    ],
    "abstract": "As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount\nto build reliable tools for explaining their predictions. A core desideratum is\nthat explanations are \\textit{faithful}, \\ie that they portray an accurate\npicture of the GNN's reasoning process. However, a number of different\nfaithfulness metrics exist, begging the question of what is faithfulness\nexactly and how to achieve it. We make three key contributions. We begin by\nshowing that \\textit{existing metrics are not interchangeable} -- \\ie\nexplanations attaining high faithfulness according to one metric may be\nunfaithful according to others -- and can systematically ignore important\nproperties of explanations. We proceed to show that, surprisingly,\n\\textit{optimizing for faithfulness is not always a sensible design goal}.\nSpecifically, we prove that for injective regular GNN architectures, perfectly\nfaithful explanations are completely uninformative. This does not apply to\nmodular GNNs, such as self-explainable and domain-invariant architectures,\nprompting us to study the relationship between architectural choices and\nfaithfulness. Finally, we show that \\textit{faithfulness is tightly linked to\nout-of-distribution generalization}, in that simply ensuring that a GNN can\ncorrectly recognize the domain-invariant subgraph, as prescribed by the\nliterature, does not guarantee that it is invariant unless this subgraph is\nalso faithful.The code is publicly available on GitHub",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Uploading ICLR25 camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.15156v2",
    "published_date": "2024-06-21 14:01:23 UTC",
    "updated_date": "2025-04-10 08:55:25 UTC"
  },
  {
    "arxiv_id": "2406.15152v3",
    "title": "Generative Topological Networks",
    "authors": [
      "Alona Levy-Jurgenson",
      "Zohar Yakhini"
    ],
    "abstract": "Generative methods have recently seen significant improvements by generating\nin a lower-dimensional latent representation of the data. However, many of the\ngenerative methods applied in the latent space remain complex and difficult to\ntrain. Further, it is not entirely clear why transitioning to a\nlower-dimensional latent space can improve generative quality. In this work, we\nintroduce a new and simple generative method grounded in topology theory --\nGenerative Topological Networks (GTNs) -- which also provides insights into why\nlower-dimensional latent-space representations might be better-suited for data\ngeneration. GTNs are simple to train -- they employ a standard supervised\nlearning approach and do not suffer from common generative pitfalls such as\nmode collapse, posterior collapse or the need to pose constraints on the neural\nnetwork architecture. We demonstrate the use of GTNs on several datasets,\nincluding MNIST, CelebA, CIFAR-10 and the Hands and Palm Images dataset by\ntraining GTNs on a lower-dimensional latent representation of the data. We show\nthat GTNs can improve upon VAEs and that they are quick to converge, generating\nrealistic samples in early epochs. Further, we use the topological\nconsiderations behind the development of GTNs to offer insights into why\ngenerative models may benefit from operating on a lower-dimensional latent\nspace, highlighting the important link between the intrinsic dimension of the\ndata and the dimension in which the data is generated. Particularly, we\ndemonstrate that generating in high dimensional ambient spaces may be a\ncontributing factor to out-of-distribution samples generated by diffusion\nmodels. We also highlight other topological properties that are important to\nconsider when using and designing generative models. Our code is available at:\nhttps://github.com/alonalj/GTN",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15152v3",
    "published_date": "2024-06-21 13:55:34 UTC",
    "updated_date": "2025-01-21 14:38:22 UTC"
  },
  {
    "arxiv_id": "2406.15149v2",
    "title": "Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks",
    "authors": [
      "Alex Quach",
      "Makram Chahine",
      "Alexander Amini",
      "Ramin Hasani",
      "Daniela Rus"
    ],
    "abstract": "Simulators are powerful tools for autonomous robot learning as they offer\nscalable data generation, flexible design, and optimization of trajectories.\nHowever, transferring behavior learned from simulation data into the real world\nproves to be difficult, usually mitigated with compute-heavy domain\nrandomization methods or further model fine-tuning. We present a method to\nimprove generalization and robustness to distribution shifts in sim-to-real\nvisual quadrotor navigation tasks. To this end, we first build a simulator by\nintegrating Gaussian Splatting with quadrotor flight dynamics, and then, train\nrobust navigation policies using Liquid neural networks. In this way, we obtain\na full-stack imitation learning protocol that combines advances in 3D Gaussian\nsplatting radiance field rendering, crafty programming of expert demonstration\ntraining data, and the task understanding capabilities of Liquid networks.\nThrough a series of quantitative flight tests, we demonstrate the robust\ntransfer of navigation skills learned in a single simulation scene directly to\nthe real world. We further show the ability to maintain performance beyond the\ntraining environment under drastic distribution and physical environment\nchanges. Our learned Liquid policies, trained on single target manoeuvres\ncurated from a photorealistic simulated indoor flight only, generalize to\nmulti-step hikes onboard a real hardware platform outdoors.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "68T40, 68U20, 93C85",
      "I.2.9; I.2.6"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15149v2",
    "published_date": "2024-06-21 13:48:37 UTC",
    "updated_date": "2024-10-16 19:28:12 UTC"
  },
  {
    "arxiv_id": "2406.15130v1",
    "title": "Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset, its Methodology and Associated Tasks",
    "authors": [
      "Victor Hugo Nascimento Rocha",
      "Igor Cataneo Silveira",
      "Paulo Pirozelli",
      "Denis Deratani Mau√°",
      "Fabio Gagliardi Cozman"
    ],
    "abstract": "The recent success of Large Language Models (LLMs) has sparked concerns about\ntheir potential to spread misinformation. As a result, there is a pressing need\nfor tools to identify ``fake arguments'' generated by such models. To create\nthese tools, examples of texts generated by LLMs are needed. This paper\nintroduces a methodology to obtain good, bad and ugly arguments from\nargumentative essays produced by ChatGPT, OpenAI's LLM. We then describe a\nnovel dataset containing a set of diverse arguments, ArGPT. We assess the\neffectiveness of our dataset and establish baselines for several\nargumentation-related tasks. Finally, we show that the artificially generated\ndata relates well to human argumentation and thus is useful as a tool to train\nand test systems for the defined tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15130v1",
    "published_date": "2024-06-21 13:27:10 UTC",
    "updated_date": "2024-06-21 13:27:10 UTC"
  },
  {
    "arxiv_id": "2406.15128v1",
    "title": "A Wavelet Guided Attention Module for Skin Cancer Classification with Gradient-based Feature Fusion",
    "authors": [
      "Ayush Roy",
      "Sujan Sarkar",
      "Sohom Ghosal",
      "Dmitrii Kaplun",
      "Asya Lyanova",
      "Ram Sarkar"
    ],
    "abstract": "Skin cancer is a highly dangerous type of cancer that requires an accurate\ndiagnosis from experienced physicians. To help physicians diagnose skin cancer\nmore efficiently, a computer-aided diagnosis (CAD) system can be very helpful.\nIn this paper, we propose a novel model, which uses a novel attention mechanism\nto pinpoint the differences in features across the spatial dimensions and\nsymmetry of the lesion, thereby focusing on the dissimilarities of various\nclasses based on symmetry, uniformity in texture and color, etc. Additionally,\nto take into account the variations in the boundaries of the lesions for\ndifferent classes, we employ a gradient-based fusion of wavelet and soft\nattention-aided features to extract boundary information of skin lesions. We\nhave tested our model on the multi-class and highly class-imbalanced dataset,\ncalled HAM10000, and achieved promising results, with a 91.17\\% F1-score and\n90.75\\% accuracy. The code is made available at:\nhttps://github.com/AyushRoy2001/WAGF-Fusion.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15128v1",
    "published_date": "2024-06-21 13:21:44 UTC",
    "updated_date": "2024-06-21 13:21:44 UTC"
  },
  {
    "arxiv_id": "2406.15119v1",
    "title": "Speech Emotion Recognition under Resource Constraints with Data Distillation",
    "authors": [
      "Yi Chang",
      "Zhao Ren",
      "Zhonghao Zhao",
      "Thanh Tam Nguyen",
      "Kun Qian",
      "Tanja Schultz",
      "Bj√∂rn W. Schuller"
    ],
    "abstract": "Speech emotion recognition (SER) plays a crucial role in human-computer\ninteraction. The emergence of edge devices in the Internet of Things (IoT)\npresents challenges in constructing intricate deep learning models due to\nconstraints in memory and computational resources. Moreover, emotional speech\ndata often contains private information, raising concerns about privacy leakage\nduring the deployment of SER models. To address these challenges, we propose a\ndata distillation framework to facilitate efficient development of SER models\nin IoT applications using a synthesised, smaller, and distilled dataset. Our\nexperiments demonstrate that the distilled dataset can be effectively utilised\nto train SER models with fixed initialisation, achieving performances\ncomparable to those developed using the original full emotional speech dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15119v1",
    "published_date": "2024-06-21 13:10:46 UTC",
    "updated_date": "2024-06-21 13:10:46 UTC"
  },
  {
    "arxiv_id": "2406.15117v1",
    "title": "FA-Net: A Fuzzy Attention-aided Deep Neural Network for Pneumonia Detection in Chest X-Rays",
    "authors": [
      "Ayush Roy",
      "Anurag Bhattacharjee",
      "Diego Oliva",
      "Oscar Ramos-Soto",
      "Francisco J. Alvarez-Padilla",
      "Ram Sarkar"
    ],
    "abstract": "Pneumonia is a respiratory infection caused by bacteria, fungi, or viruses.\nIt affects many people, particularly those in developing or underdeveloped\nnations with high pollution levels, unhygienic living conditions, overcrowding,\nand insufficient medical infrastructure. Pneumonia can cause pleural effusion,\nwhere fluids fill the lungs, leading to respiratory difficulty. Early diagnosis\nis crucial to ensure effective treatment and increase survival rates. Chest\nX-ray imaging is the most commonly used method for diagnosing pneumonia.\nHowever, visual examination of chest X-rays can be difficult and subjective. In\nthis study, we have developed a computer-aided diagnosis system for automatic\npneumonia detection using chest X-ray images. We have used DenseNet-121 and\nResNet50 as the backbone for the binary class (pneumonia and normal) and\nmulti-class (bacterial pneumonia, viral pneumonia, and normal) classification\ntasks, respectively. We have also implemented a channel-specific spatial\nattention mechanism, called Fuzzy Channel Selective Spatial Attention Module\n(FCSSAM), to highlight the specific spatial regions of relevant channels while\nremoving the irrelevant channels of the extracted features by the backbone. We\nevaluated the proposed approach on a publicly available chest X-ray dataset,\nusing binary and multi-class classification setups. Our proposed method\nachieves accuracy rates of 97.15\\% and 79.79\\% for the binary and multi-class\nclassification setups, respectively. The results of our proposed method are\nsuperior to state-of-the-art (SOTA) methods. The code of the proposed model\nwill be available at: https://github.com/AyushRoy2001/FA-Net.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15117v1",
    "published_date": "2024-06-21 13:08:40 UTC",
    "updated_date": "2024-06-21 13:08:40 UTC"
  },
  {
    "arxiv_id": "2406.15113v1",
    "title": "A Dual Attention-aided DenseNet-121 for Classification of Glaucoma from Fundus Images",
    "authors": [
      "Soham Chakraborty",
      "Ayush Roy",
      "Payel Pramanik",
      "Daria Valenkova",
      "Ram Sarkar"
    ],
    "abstract": "Deep learning and computer vision methods are nowadays predominantly used in\nthe field of ophthalmology. In this paper, we present an attention-aided\nDenseNet-121 for classifying normal and glaucomatous eyes from fundus images.\nIt involves the convolutional block attention module to highlight relevant\nspatial and channel features extracted by DenseNet-121. The channel\nrecalibration module further enriches the features by utilizing edge\ninformation along with the statistical features of the spatial dimension. For\nthe experiments, two standard datasets, namely RIM-ONE and ACRIMA, have been\nused. Our method has shown superior results than state-of-the-art models. An\nablation study has also been conducted to show the effectiveness of each of the\ncomponents. The code of the proposed work is available at:\nhttps://github.com/Soham2004GitHub/DADGC.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15113v1",
    "published_date": "2024-06-21 13:00:46 UTC",
    "updated_date": "2024-06-21 13:00:46 UTC"
  },
  {
    "arxiv_id": "2406.15111v2",
    "title": "Investigating the impact of 2D gesture representation on co-speech gesture generation",
    "authors": [
      "Teo Guichoux",
      "Laure Soulier",
      "Nicolas Obin",
      "Catherine Pelachaud"
    ],
    "abstract": "Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages. Paper accepted at WACAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15111v2",
    "published_date": "2024-06-21 12:59:20 UTC",
    "updated_date": "2024-06-24 08:19:00 UTC"
  },
  {
    "arxiv_id": "2406.15098v1",
    "title": "How Intermodal Interaction Affects the Performance of Deep Multimodal Fusion for Mixed-Type Time Series",
    "authors": [
      "Simon Dietz",
      "Thomas Altstidl",
      "Dario Zanca",
      "Bj√∂rn Eskofier",
      "An Nguyen"
    ],
    "abstract": "Mixed-type time series (MTTS) is a bimodal data type that is common in many\ndomains, such as healthcare, finance, environmental monitoring, and social\nmedia. It consists of regularly sampled continuous time series and irregularly\nsampled categorical event sequences. The integration of both modalities through\nmultimodal fusion is a promising approach for processing MTTS. However, the\nquestion of how to effectively fuse both modalities remains open. In this\npaper, we present a comprehensive evaluation of several deep multimodal fusion\napproaches for MTTS forecasting. Our comparison includes three fusion types\n(early, intermediate, and late) and five fusion methods (concatenation,\nweighted mean, weighted mean with correlation, gating, and feature sharing). We\nevaluate these fusion approaches on three distinct datasets, one of which was\ngenerated using a novel framework. This framework allows for the control of key\ndata properties, such as the strength and direction of intermodal interactions,\nmodality imbalance, and the degree of randomness in each modality, providing a\nmore controlled environment for testing fusion approaches. Our findings show\nthat the performance of different fusion approaches can be substantially\ninfluenced by the direction and strength of intermodal interactions. The study\nreveals that early and intermediate fusion approaches excel at capturing\nfine-grained and coarse-grained cross-modal features, respectively. These\nfindings underscore the crucial role of intermodal interactions in determining\nthe most effective fusion strategy for MTTS forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15098v1",
    "published_date": "2024-06-21 12:26:48 UTC",
    "updated_date": "2024-06-21 12:26:48 UTC"
  },
  {
    "arxiv_id": "2407.04722v1",
    "title": "A GPT-based Code Review System for Programming Language Learning",
    "authors": [
      "Lee Dong-Kyu"
    ],
    "abstract": "The increasing demand for programming language education and growing class\nsizes require immediate and personalized feedback. However, traditional code\nreview methods have limitations in providing this level of feedback. As the\ncapabilities of Large Language Models (LLMs) like GPT for generating accurate\nsolutions and timely code reviews are verified, this research proposes a system\nthat employs GPT-4 to offer learner-friendly code reviews and minimize the risk\nof AI-assist cheating.\n  To provide learner-friendly code reviews, a dataset was collected from an\nonline judge system, and this dataset was utilized to develop and enhance the\nsystem's prompts. In addition, to minimize AI-assist cheating, the system flow\nwas designed to provide code reviews only for code submitted by a learner, and\na feature that highlights code lines to fix was added. After the initial system\nwas deployed on the web, software education experts conducted usability test.\nBased on the results, improvement strategies were developed to improve code\nreview and code correctness check module, thereby enhancing the system.\n  The improved system underwent evaluation by software education experts based\non four criteria: strict code correctness checks, response time, lower API call\ncosts, and the quality of code reviews. The results demonstrated a performance\nto accurately identify error types, shorten response times, lower API call\ncosts, and maintain high-quality code reviews without major issues. Feedback\nfrom participants affirmed the tool's suitability for teaching programming to\nprimary and secondary school students. Given these benefits, the system is\nanticipated to be a efficient learning tool in programming language learning\nfor educational settings.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.04722v1",
    "published_date": "2024-06-21 12:16:01 UTC",
    "updated_date": "2024-06-21 12:16:01 UTC"
  },
  {
    "arxiv_id": "2407.11205v1",
    "title": "Impact on clinical guideline adherence of Orient-COVID, a CDSS based on dynamic medical decision trees for COVID19 management: a randomized simulation trial",
    "authors": [
      "Mouin Jammal",
      "Antoine Saab",
      "Cynthia Abi Khalil",
      "Charbel Mourad",
      "Rosy Tsopra",
      "Melody Saikali",
      "Jean-Baptiste Lamy"
    ],
    "abstract": "Background: The adherence of clinicians to clinical practice guidelines is\nknown to be low, including for the management of COVID-19, due to their\ndifficult use at the point of care and their complexity. Clinical decision\nsupport systems have been proposed to implement guidelines and improve\nadherence. One approach is to permit the navigation inside the recommendations,\npresented as a decision tree, but the size of the tree often limits this\napproach and may cause erroneous navigation, especially when it does not fit in\na single screen. Methods: We proposed an innovative visual interface to allow\nclinicians easily navigating inside decision trees for the management of\nCOVID-19 patients. It associates a multi-path tree model with the use of the\nfisheye visual technique, allowing the visualization of large decision trees in\na single screen. To evaluate the impact of this tool on guideline adherence, we\nconducted a randomized controlled trial in a near-real simulation setting,\ncomparing the decisions taken by medical students using Orient-COVID with those\ntaken with paper guidelines or without guidance, when performing on six\nrealistic clinical cases. Results: The results show that paper guidelines had\nno impact (p=0.97), while Orient-COVID significantly improved the guideline\nadherence compared to both other groups (p<0.0003). A significant impact of\nOrient-COVID was identified on several key points during the management of\nCOVID-19: ordering troponin lab tests, prescribing anticoagulant and oxygen\ntherapy. A multifactor analysis showed no difference between male and female\nparticipants. Conclusions: The use of an interactive decision tree for the\nmanagement of COVID-19 significantly improved the clinician adherence to\nguidelines. Future works will focus on the integration of the system to\nelectronic health records and on the adaptation of the system to other clinical\nconditions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "92C50 (Primary), 68U35 (Secondary)"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.11205v1",
    "published_date": "2024-06-21 11:50:50 UTC",
    "updated_date": "2024-06-21 11:50:50 UTC"
  },
  {
    "arxiv_id": "2406.15073v1",
    "title": "KnobTree: Intelligent Database Parameter Configuration via Explainable Reinforcement Learning",
    "authors": [
      "Jiahan Chen",
      "Shuhan Qi",
      "Yifan Li",
      "Zeyu Dong",
      "Mingfeng Ding",
      "Yulin Wu",
      "Xuan Wang"
    ],
    "abstract": "Databases are fundamental to contemporary information systems, yet\ntraditional rule-based configuration methods struggle to manage the complexity\nof real-world applications with hundreds of tunable parameters. Deep\nreinforcement learning (DRL), which combines perception and decision-making,\npresents a potential solution for intelligent database configuration tuning.\nHowever, due to black-box property of RL-based method, the generated database\ntuning strategies still face the urgent problem of lack explainability.\nBesides, the redundant parameters in large scale database always make the\nstrategy learning become unstable. This paper proposes KnobTree, an\ninterpertable framework designed for the optimization of database parameter\nconfiguration. In this framework, an interpertable database tuning algorithm\nbased on RL-based differentatial tree is proposed, which building a transparent\ntree-based model to generate explainable database tuning strategies. To address\nthe problem of large-scale parameters, We also introduce a explainable method\nfor parameter importance assessment, by utilizing Shapley Values to identify\nparameters that have significant impacts on database performance. Experiments\nconducted on MySQL and Gbase8s databases have verified exceptional transparency\nand interpretability of the KnobTree model. The good property makes generated\nstrategies can offer practical guidance to algorithm designers and database\nadministrators. Moreover, our approach also slightly outperforms the existing\nRL-based tuning algorithms in aspects such as throughput, latency, and\nprocessing time.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15073v1",
    "published_date": "2024-06-21 11:40:55 UTC",
    "updated_date": "2024-06-21 11:40:55 UTC"
  },
  {
    "arxiv_id": "2406.15050v1",
    "title": "Tri-VQA: Triangular Reasoning Medical Visual Question Answering for Multi-Attribute Analysis",
    "authors": [
      "Lin Fan",
      "Xun Gong",
      "Cenyang Zheng",
      "Yafei Ou"
    ],
    "abstract": "The intersection of medical Visual Question Answering (Med-VQA) is a\nchallenging research topic with advantages including patient engagement and\nclinical expert involvement for second opinions. However, existing Med-VQA\nmethods based on joint embedding fail to explain whether their provided results\nare based on correct reasoning or coincidental answers, which undermines the\ncredibility of VQA answers. In this paper, we investigate the construction of a\nmore cohesive and stable Med-VQA structure. Motivated by causal effect, we\npropose a novel Triangular Reasoning VQA (Tri-VQA) framework, which constructs\nreverse causal questions from the perspective of \"Why this answer?\" to\nelucidate the source of the answer and stimulate more reasonable forward\nreasoning processes. We evaluate our method on the Endoscopic Ultrasound (EUS)\nmulti-attribute annotated dataset from five centers, and test it on medical VQA\ndatasets. Experimental results demonstrate the superiority of our approach over\nexisting methods. Our codes and pre-trained models are available at\nhttps://anonymous.4open.science/r/Tri_VQA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "I.2.7; I.2.10; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15050v1",
    "published_date": "2024-06-21 10:50:55 UTC",
    "updated_date": "2024-06-21 10:50:55 UTC"
  },
  {
    "arxiv_id": "2406.15044v1",
    "title": "From Overfitting to Robustness: Quantity, Quality, and Variety Oriented Negative Sample Selection in Graph Contrastive Learning",
    "authors": [
      "Adnan Ali",
      "Jinlong Li",
      "Huanhuan Chen",
      "Ali Kashif Bashir"
    ],
    "abstract": "Graph contrastive learning (GCL) aims to contrast positive-negative\ncounterparts to learn the node embeddings, whereas graph data augmentation\nmethods are employed to generate these positive-negative samples. The\nvariation, quantity, and quality of negative samples compared to positive\nsamples play crucial roles in learning meaningful embeddings for node\nclassification downstream tasks. Less variation, excessive quantity, and\nlow-quality negative samples cause the model to be overfitted for particular\nnodes, resulting in less robust models. To solve the overfitting problem in the\nGCL paradigm, this study proposes a novel Cumulative Sample Selection (CSS)\nalgorithm by comprehensively considering negative samples' quality, variations,\nand quantity. Initially, three negative sample pools are constructed: easy,\nmedium, and hard negative samples, which contain 25%, 50%, and 25% of the total\navailable negative samples, respectively. Then, 10% negative samples are\nselected from each of these three negative sample pools for training the model.\nAfter that, a decision agent module evaluates model training results and\ndecides whether to explore more negative samples from three negative sample\npools by increasing the ratio or keep exploiting the current sampling ratio.\nThe proposed algorithm is integrated into a proposed graph contrastive learning\nframework named NegAmplify. NegAmplify is compared with the SOTA methods on\nnine graph node classification datasets, with seven achieving better node\nclassification accuracy with up to 2.86% improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15044v1",
    "published_date": "2024-06-21 10:47:26 UTC",
    "updated_date": "2024-06-21 10:47:26 UTC"
  },
  {
    "arxiv_id": "2406.15042v1",
    "title": "Behaviour Distillation",
    "authors": [
      "Andrei Lupu",
      "Chris Lu",
      "Jarek Liesen",
      "Robert Tjarko Lange",
      "Jakob Foerster"
    ],
    "abstract": "Dataset distillation aims to condense large datasets into a small number of\nsynthetic examples that can be used as drop-in replacements when training new\nmodels. It has applications to interpretability, neural architecture search,\nprivacy, and continual learning. Despite strong successes in supervised\ndomains, such methods have not yet been extended to reinforcement learning,\nwhere the lack of a fixed dataset renders most distillation methods unusable.\nFilling the gap, we formalize behaviour distillation, a setting that aims to\ndiscover and then condense the information required for training an expert\npolicy into a synthetic dataset of state-action pairs, without access to expert\ndata. We then introduce Hallucinating Datasets with Evolution Strategies\n(HaDES), a method for behaviour distillation that can discover datasets of just\nfour state-action pairs which, under supervised learning, train agents to\ncompetitive performance levels in continuous control tasks. We show that these\ndatasets generalize out of distribution to training policies with a wide range\nof architectures and hyperparameters. We also demonstrate application to a\ndownstream task, namely training multi-task agents in a zero-shot fashion.\nBeyond behaviour distillation, HaDES provides significant improvements in\nneuroevolution for RL over previous approaches and achieves SoTA results on one\nstandard supervised dataset distillation task. Finally, we show that\nvisualizing the synthetic datasets can provide human-interpretable task\ninsights.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15042v1",
    "published_date": "2024-06-21 10:45:43 UTC",
    "updated_date": "2024-06-21 10:45:43 UTC"
  },
  {
    "arxiv_id": "2406.15038v1",
    "title": "Online detection and infographic explanation of spam reviews with data drift adaptation",
    "authors": [
      "Francisco de Arriba-P√©rez",
      "Silvia Garc√≠a-M√©ndez",
      "F√°tima Leal",
      "Benedita Malheiro",
      "J. C. Burguillo"
    ],
    "abstract": "Spam reviews are a pervasive problem on online platforms due to its\nsignificant impact on reputation. However, research into spam detection in data\nstreams is scarce. Another concern lies in their need for transparency.\nConsequently, this paper addresses those problems by proposing an online\nsolution for identifying and explaining spam reviews, incorporating data drift\nadaptation. It integrates (i) incremental profiling, (ii) data drift detection\n& adaptation, and (iii) identification of spam reviews employing Machine\nLearning. The explainable mechanism displays a visual and textual prediction\nexplanation in a dashboard. The best results obtained reached up to 87 % spam\nF-measure.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15038v1",
    "published_date": "2024-06-21 10:35:46 UTC",
    "updated_date": "2024-06-21 10:35:46 UTC"
  },
  {
    "arxiv_id": "2406.15032v1",
    "title": "GiusBERTo: A Legal Language Model for Personal Data De-identification in Italian Court of Auditors Decisions",
    "authors": [
      "Giulio Salierno",
      "Rosamaria Bert√®",
      "Luca Attias",
      "Carla Morrone",
      "Dario Pettazzoni",
      "Daniela Battisti"
    ],
    "abstract": "Recent advances in Natural Language Processing have demonstrated the\neffectiveness of pretrained language models like BERT for a variety of\ndownstream tasks. We present GiusBERTo, the first BERT-based model specialized\nfor anonymizing personal data in Italian legal documents. GiusBERTo is trained\non a large dataset of Court of Auditors decisions to recognize entities to\nanonymize, including names, dates, locations, while retaining contextual\nrelevance. We evaluate GiusBERTo on a held-out test set and achieve 97%\ntoken-level accuracy. GiusBERTo provides the Italian legal community with an\naccurate and tailored BERT model for de-identification, balancing privacy and\ndata protection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures, 6 Tables",
    "pdf_url": "http://arxiv.org/pdf/2406.15032v1",
    "published_date": "2024-06-21 10:25:26 UTC",
    "updated_date": "2024-06-21 10:25:26 UTC"
  },
  {
    "arxiv_id": "2406.15016v1",
    "title": "Evolution of Rewards for Food and Motor Action by Simulating Birth and Death",
    "authors": [
      "Yuji Kanagawa",
      "Kenji Doya"
    ],
    "abstract": "The reward system is one of the fundamental drivers of animal behaviors and\nis critical for survival and reproduction. Despite its importance, the problem\nof how the reward system has evolved is underexplored. In this paper, we try to\nreplicate the evolution of biologically plausible reward functions and\ninvestigate how environmental conditions affect evolved rewards' shape. For\nthis purpose, we developed a population-based decentralized evolutionary\nsimulation framework, where agents maintain their energy level to live longer\nand produce more children. Each agent inherits its reward function from its\nparent subject to mutation and learns to get rewards via reinforcement learning\nthroughout its lifetime. Our results show that biologically reasonable positive\nrewards for food acquisition and negative rewards for motor action can evolve\nfrom randomly initialized ones. However, we also find that the rewards for\nmotor action diverge into two modes: largely positive and slightly negative.\nThe emergence of positive motor action rewards is surprising because it can\nmake agents too active and inefficient in foraging. In environments with poor\nand poisonous foods, the evolution of rewards for less important foods tends to\nbe unstable, while rewards for normal foods are still stable. These results\ndemonstrate the usefulness of our simulation environment and energy-dependent\nbirth and death model for further studies of the origin of reward systems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15016v1",
    "published_date": "2024-06-21 09:44:56 UTC",
    "updated_date": "2024-06-21 09:44:56 UTC"
  },
  {
    "arxiv_id": "2406.15015v1",
    "title": "GraLMatch: Matching Groups of Entities with Graphs and Language Models",
    "authors": [
      "Fernando De Meer Pardo",
      "Claude Lehmann",
      "Dennis Gehrig",
      "Andrea Nagy",
      "Stefano Nicoli",
      "Branka Hadji Misheva",
      "Martin Braschler",
      "Kurt Stockinger"
    ],
    "abstract": "In this paper, we present an end-to-end multi-source Entity Matching problem,\nwhich we call entity group matching, where the goal is to assign to the same\ngroup, records originating from multiple data sources but representing the same\nreal-world entity. We focus on the effects of transitively matched records,\ni.e. the records connected by paths in the graph G = (V,E) whose nodes and\nedges represent the records and whether they are a match or not. We present a\nreal-world instance of this problem, where the challenge is to match records of\ncompanies and financial securities originating from different data providers.\nWe also introduce two new multi-source benchmark datasets that present similar\nmatching challenges as real-world records. A distinctive characteristic of\nthese records is that they are regularly updated following real-world events,\nbut updates are not applied uniformly across data sources. This phenomenon\nmakes the matching of certain groups of records only possible through the use\nof transitive information.\n  In our experiments, we illustrate how considering transitively matched\nrecords is challenging since a limited amount of false positive pairwise match\npredictions can throw off the group assignment of large quantities of records.\nThus, we propose GraLMatch, a method that can partially detect and remove false\npositive pairwise predictions through graph-based properties. Finally, we\nshowcase how fine-tuning a Transformer-based model (DistilBERT) on a reduced\nnumber of labeled samples yields a better final entity group matching than\ntraining on more samples and/or incorporating fine-tuning optimizations,\nillustrating how precision becomes the deciding factor in the entity group\nmatching of large volumes of records.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "12 pages, 4 figures, accepted as research paper at EDBT 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.15015v1",
    "published_date": "2024-06-21 09:44:16 UTC",
    "updated_date": "2024-06-21 09:44:16 UTC"
  },
  {
    "arxiv_id": "2406.15009v2",
    "title": "Fair, Manipulation-Robust, and Transparent Sortition",
    "authors": [
      "Carmel Baharav",
      "Bailey Flanigan"
    ],
    "abstract": "Sortition, the random selection of political representatives, is increasingly\nbeing used around the world to choose participants of deliberative processes\nlike Citizens' Assemblies. Motivated by sortition's practical importance, there\nhas been a recent flurry of research on sortition algorithms, whose task it is\nto select a panel from among a pool of volunteers. This panel must satisfy\nquotas enforcing representation of key population subgroups. Past work has\ncontributed an algorithmic approach for fulfilling this task while ensuring\nthat volunteers' chances of selection are maximally equal, as measured by any\nconvex equality objective. The question, then, is: which equality objective is\nthe right one? Past work has mainly studied the objectives Minimax and Leximin,\nwhich respectively minimize the maximum and maximize the minimum chance of\nselection given to any volunteer. Recent work showed that both of these\nobjectives have key weaknesses: Minimax is highly robust to manipulation but is\narbitrarily unfair; oppositely, Leximin is highly fair but arbitrarily\nmanipulable.\n  In light of this gap, we propose a new equality objective, Goldilocks, that\naims to achieve these ideals simultaneously by ensuring that no volunteer\nreceives too little or too much chance of selection. We theoretically bound the\nextent to which Goldilocks achieves these ideals, finding that in an important\nsense, Goldilocks recovers among the best available solutions in a given\ninstance. We then extend our bounds to the case where the output of Goldilocks\nis transformed to achieve a third goal, Transparency. Our empirical analysis of\nGoldilocks in real data is even more promising: we find that this objective\nachieves nearly instance-optimal minimum and maximum selection probabilities\nsimultaneously in most real instances -- an outcome not even guaranteed to be\npossible for any algorithm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15009v2",
    "published_date": "2024-06-21 09:38:03 UTC",
    "updated_date": "2024-06-26 16:26:50 UTC"
  },
  {
    "arxiv_id": "2406.15007v3",
    "title": "RouteFinder: Towards Foundation Models for Vehicle Routing Problems",
    "authors": [
      "Federico Berto",
      "Chuanbo Hua",
      "Nayeli Gast Zepeda",
      "Andr√© Hottung",
      "Niels Wouda",
      "Leon Lan",
      "Junyoung Park",
      "Kevin Tierney",
      "Jinkyoo Park"
    ],
    "abstract": "This paper introduces RouteFinder, a comprehensive foundation model framework\nto tackle different Vehicle Routing Problem (VRP) variants. Our core idea is\nthat a foundation model for VRPs should be able to represent variants by\ntreating each as a subset of a generalized problem equipped with different\nattributes. We propose a unified VRP environment capable of efficiently\nhandling any attribute combination. The RouteFinder model leverages a modern\ntransformer-based encoder and global attribute embeddings to improve task\nrepresentation. Additionally, we introduce two reinforcement learning\ntechniques to enhance multi-task performance: mixed batch training, which\nenables training on different variants at once, and multi-variant reward\nnormalization to balance different reward scales. Finally, we propose efficient\nadapter layers that enable fine-tuning for new variants with unseen attributes.\nExtensive experiments on 48 VRP variants show RouteFinder outperforms recent\nstate-of-the-art learning methods. Code: https://github.com/ai4co/routefinder.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "A version of this work has been presented as an Oral at the ICML 2024\n  FM-Wild Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.15007v3",
    "published_date": "2024-06-21 09:34:26 UTC",
    "updated_date": "2025-02-05 09:27:54 UTC"
  },
  {
    "arxiv_id": "2406.15000v1",
    "title": "Unveiling the Impact of Multi-Modal Interactions on User Engagement: A Comprehensive Evaluation in AI-driven Conversations",
    "authors": [
      "Lichao Zhang",
      "Jia Yu",
      "Shuai Zhang",
      "Long Li",
      "Yangyang Zhong",
      "Guanbao Liang",
      "Yuming Yan",
      "Qing Ma",
      "Fangsheng Weng",
      "Fayu Pan",
      "Jing Li",
      "Renjun Xu",
      "Zhenzhong Lan"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced user-bot\ninteractions, enabling more complex and coherent dialogues. However, the\nprevalent text-only modality might not fully exploit the potential for\neffective user engagement. This paper explores the impact of multi-modal\ninteractions, which incorporate images and audio alongside text, on user\nengagement in chatbot conversations. We conduct a comprehensive analysis using\na diverse set of chatbots and real-user interaction data, employing metrics\nsuch as retention rate and conversation length to evaluate user engagement. Our\nfindings reveal a significant enhancement in user engagement with multi-modal\ninteractions compared to text-only dialogues. Notably, the incorporation of a\nthird modality significantly amplifies engagement beyond the benefits observed\nwith just two modalities. These results suggest that multi-modal interactions\noptimize cognitive processing and facilitate richer information comprehension.\nThis study underscores the importance of multi-modality in chatbot design,\noffering valuable insights for creating more engaging and immersive AI\ncommunication experiences and informing the broader AI community about the\nbenefits of multi-modal interactions in enhancing user engagement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15000v1",
    "published_date": "2024-06-21 09:26:55 UTC",
    "updated_date": "2024-06-21 09:26:55 UTC"
  },
  {
    "arxiv_id": "2407.18363v1",
    "title": "KI-Bilder und die Widerst√§ndigkeit der Medienkonvergenz: Von prim√§rer zu sekund√§rer Intermedialit√§t?",
    "authors": [
      "Lukas R. A. Wilde"
    ],
    "abstract": "The article presents some current observations (as of April 10, 2024) on the\nintegration of AI-generated images within processes of media convergence. It\ndraws on two different concepts of intermediality. Primary intermediality\nconcepts are motivated by the object when a new type of technology develops the\npotential to become socially relevant as a media form and thus a socially,\npolitically, or culturally important communicative factor. Due to their\nuncertain 'measurements' within the wider media ecology, however, the new,\nstill potential media form appears hybrid. The \"inter-\" or \"between-\" of this\ninitial intermediality moment thus refers to the questionable \"site\" and the\nquestionable description of the potential media form between already existing\ntechnologies and cultural forms and their conceptual measurements. For\nsecondary concepts of intermediality, in contrast, it can be assumed that the\nboundaries of media forms and their application have already been drawn and are\nreasonably undisputed. This then raises the question of intentional and staged\nreferences to AI imagery within other media forms and pictures. The article\ndiscusses indicators of both intermediality moments using current examples and\ncontroversies surrounding AI images. The thesis is that there can be no talk of\na seamless 'integration' of AI images into the wider media landscape at the\nmoment (within films, comic books, or video games, for example) - as one of\ncountless other image production techniques - and that the medial 'site' of AI\nimage circulation - at least where it is not a matter of deception, but rather\ntheir conscious use as AI images - especially in social media communication and\nin fan cultures, but with repercussions for the more general media ecology and\nimage interpretation, insofar as the suspicion that an image could be\nAI-generated is now increasingly present as a \"hermeneutics of suspicion\".",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CY",
    "comment": "in German language",
    "pdf_url": "http://arxiv.org/pdf/2407.18363v1",
    "published_date": "2024-06-21 09:15:19 UTC",
    "updated_date": "2024-06-21 09:15:19 UTC"
  },
  {
    "arxiv_id": "2406.14990v2",
    "title": "Learning Variable Compliance Control From a Few Demonstrations for Bimanual Robot with Haptic Feedback Teleoperation System",
    "authors": [
      "Tatsuya Kamijo",
      "Cristian C. Beltran-Hernandez",
      "Masashi Hamaya"
    ],
    "abstract": "Automating dexterous, contact-rich manipulation tasks using rigid robots is a\nsignificant challenge in robotics. Rigid robots, defined by their actuation\nthrough position commands, face issues of excessive contact forces due to their\ninability to adapt to contact with the environment, potentially causing damage.\nWhile compliance control schemes have been introduced to mitigate these issues\nby controlling forces via external sensors, they are hampered by the need for\nfine-tuning task-specific controller parameters. Learning from Demonstrations\n(LfD) offers an intuitive alternative, allowing robots to learn manipulations\nthrough observed actions. In this work, we introduce a novel system to enhance\nthe teaching of dexterous, contact-rich manipulations to rigid robots. Our\nsystem is twofold: firstly, it incorporates a teleoperation interface utilizing\nVirtual Reality (VR) controllers, designed to provide an intuitive and\ncost-effective method for task demonstration with haptic feedback. Secondly, we\npresent Comp-ACT (Compliance Control via Action Chunking with Transformers), a\nmethod that leverages the demonstrations to learn variable compliance control\nfrom a few demonstrations. Our methods have been validated across various\ncomplex contact-rich manipulation tasks using single-arm and bimanual robot\nsetups in simulated and real-world environments, demonstrating the\neffectiveness of our system in teaching robots dexterous manipulations with\nenhanced adaptability and safety. Code available at:\nhttps://github.com/omron-sinicx/CompACT",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.14990v2",
    "published_date": "2024-06-21 09:03:37 UTC",
    "updated_date": "2024-09-26 05:51:20 UTC"
  },
  {
    "arxiv_id": "2406.14988v1",
    "title": "Introducing the Biomechanics-Function Relationship in Glaucoma: Improved Visual Field Loss Predictions from intraocular pressure-induced Neural Tissue Strains",
    "authors": [
      "Thanadet Chuangsuwanich",
      "Monisha E. Nongpiur",
      "Fabian A. Braeu",
      "Tin A. Tun",
      "Alexandre Thiery",
      "Shamira Perera",
      "Ching Lin Ho",
      "Martin Buist",
      "George Barbastathis",
      "Tin Aung",
      "Micha√´l J. A. Girard"
    ],
    "abstract": "Objective. (1) To assess whether neural tissue structure and biomechanics\ncould predict functional loss in glaucoma; (2) To evaluate the importance of\nbiomechanics in making such predictions. Design, Setting and Participants. We\nrecruited 238 glaucoma subjects. For one eye of each subject, we imaged the\noptic nerve head (ONH) using spectral-domain OCT under the following\nconditions: (1) primary gaze and (2) primary gaze with acute IOP elevation.\nMain Outcomes: We utilized automatic segmentation of optic nerve head (ONH)\ntissues and digital volume correlation (DVC) analysis to compute intraocular\npressure (IOP)-induced neural tissue strains. A robust geometric deep learning\napproach, known as Point-Net, was employed to predict the full Humphrey 24-2\npattern standard deviation (PSD) maps from ONH structural and biomechanical\ninformation. For each point in each PSD map, we predicted whether it exhibited\nno defect or a PSD value of less than 5%. Predictive performance was evaluated\nusing 5-fold cross-validation and the F1-score. We compared the model's\nperformance with and without the inclusion of IOP-induced strains to assess the\nimpact of biomechanics on prediction accuracy. Results: Integrating\nbiomechanical (IOP-induced neural tissue strains) and structural (tissue\nmorphology and neural tissues thickness) information yielded a significantly\nbetter predictive model (F1-score: 0.76+-0.02) across validation subjects, as\nopposed to relying only on structural information, which resulted in a\nsignificantly lower F1-score of 0.71+-0.02 (p < 0.05). Conclusion: Our study\nhas shown that the integration of biomechanical data can significantly improve\nthe accuracy of visual field loss predictions. This highlights the importance\nof the biomechanics-function relationship in glaucoma, and suggests that\nbiomechanics may serve as a crucial indicator for the development and\nprogression of glaucoma.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "19 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.14988v1",
    "published_date": "2024-06-21 09:00:46 UTC",
    "updated_date": "2024-06-21 09:00:46 UTC"
  },
  {
    "arxiv_id": "2406.14986v2",
    "title": "Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers",
    "authors": [
      "Manuel Mondal",
      "Ljiljana Dolamic",
      "G√©r√¥me Bovet",
      "Philippe Cudr√©-Mauroux",
      "Julien Audiffren"
    ],
    "abstract": "Prompting and Multiple Choices Questions (MCQ) have become the preferred\napproach to assess the capabilities of Large Language Models (LLMs), due to\ntheir ease of manipulation and evaluation. Such experimental appraisals have\npointed toward the LLMs' apparent ability to perform causal reasoning or to\ngrasp uncertainty. In this paper, we investigate whether these abilities are\nmeasurable outside of tailored prompting and MCQ by reformulating these issues\nas direct text completion - the foundation of LLMs. To achieve this goal, we\ndefine scenarios with multiple possible outcomes and we compare the prediction\nmade by the LLM through prompting (their Stated Answer) to the probability\ndistributions they compute over these outcomes during next token prediction\n(their Revealed Belief). Our findings suggest that the Revealed Belief of LLMs\nsignificantly differs from their Stated Answer and hint at multiple biases and\nmisrepresentations that their beliefs may yield in many scenarios and outcomes.\nAs text completion is at the core of LLMs, these results suggest that common\nevaluation methods may only provide a partial picture and that more research is\nneeded to assess the extent and nature of their capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14986v2",
    "published_date": "2024-06-21 08:56:35 UTC",
    "updated_date": "2024-07-02 14:02:04 UTC"
  },
  {
    "arxiv_id": "2407.00072v5",
    "title": "Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback",
    "authors": [
      "Yu Bai",
      "Yukai Miao",
      "Li Chen",
      "Dawei Wang",
      "Dan Li",
      "Yanyu Ren",
      "Hongtao Xie",
      "Ce Yang",
      "Xuhui Cai"
    ],
    "abstract": "RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00072v5",
    "published_date": "2024-06-21 08:52:11 UTC",
    "updated_date": "2024-10-31 08:35:42 UTC"
  },
  {
    "arxiv_id": "2406.14981v1",
    "title": "Human-AI collectives produce the most accurate differential diagnoses",
    "authors": [
      "N. Z√∂ller",
      "J. Berger",
      "I. Lin",
      "N. Fu",
      "J. Komarneni",
      "G. Barabucci",
      "K. Laskowski",
      "V. Shia",
      "B. Harack",
      "E. A. Chu",
      "V. Trianni",
      "R. H. J. M. Kurvers",
      "S. M. Herzog"
    ],
    "abstract": "Artificial intelligence systems, particularly large language models (LLMs),\nare increasingly being employed in high-stakes decisions that impact both\nindividuals and society at large, often without adequate safeguards to ensure\nsafety, quality, and equity. Yet LLMs hallucinate, lack common sense, and are\nbiased - shortcomings that may reflect LLMs' inherent limitations and thus may\nnot be remedied by more sophisticated architectures, more data, or more human\nfeedback. Relying solely on LLMs for complex, high-stakes decisions is\ntherefore problematic. Here we present a hybrid collective intelligence system\nthat mitigates these risks by leveraging the complementary strengths of human\nexperience and the vast information processed by LLMs. We apply our method to\nopen-ended medical diagnostics, combining 40,762 differential diagnoses made by\nphysicians with the diagnoses of five state-of-the art LLMs across 2,133\nmedical cases. We show that hybrid collectives of physicians and LLMs\noutperform both single physicians and physician collectives, as well as single\nLLMs and LLM ensembles. This result holds across a range of medical specialties\nand professional experience, and can be attributed to humans' and LLMs'\ncomplementary contributions that lead to different kinds of errors. Our\napproach highlights the potential for collective human and machine intelligence\nto improve accuracy in complex, open-ended domains like medical diagnostics.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14981v1",
    "published_date": "2024-06-21 08:46:30 UTC",
    "updated_date": "2024-06-21 08:46:30 UTC"
  },
  {
    "arxiv_id": "2406.14977v2",
    "title": "Trustworthy Enhanced Multi-view Multi-modal Alzheimer's Disease Prediction with Brain-wide Imaging Transcriptomics Data",
    "authors": [
      "Shan Cong",
      "Zhoujie Fan",
      "Hongwei Liu",
      "Yinghan Zhang",
      "Xin Wang",
      "Haoran Luo",
      "Xiaohui Yao"
    ],
    "abstract": "Brain transcriptomics provides insights into the molecular mechanisms by\nwhich the brain coordinates its functions and processes. However, existing\nmultimodal methods for predicting Alzheimer's disease (AD) primarily rely on\nimaging and sometimes genetic data, often neglecting the transcriptomic basis\nof brain. Furthermore, while striving to integrate complementary information\nbetween modalities, most studies overlook the informativeness disparities\nbetween modalities. Here, we propose TMM, a trusted multiview multimodal graph\nattention framework for AD diagnosis, using extensive brain-wide\ntranscriptomics and imaging data. First, we construct view-specific brain\nregional co-function networks (RRIs) from transcriptomics and multimodal\nradiomics data to incorporate interaction information from both biomolecular\nand imaging perspectives. Next, we apply graph attention (GAT) processing to\neach RRI network to produce graph embeddings and employ cross-modal attention\nto fuse transcriptomics-derived embedding with each imagingderived embedding.\nFinally, a novel true-false-harmonized class probability (TFCP) strategy is\ndesigned to assess and adaptively adjust the prediction confidence of each\nmodality for AD diagnosis. We evaluate TMM using the AHBA database with\nbrain-wide transcriptomics data and the ADNI database with three imaging\nmodalities (AV45-PET, FDG-PET, and VBM-MRI). The results demonstrate the\nsuperiority of our method in identifying AD, EMCI, and LMCI compared to\nstate-of-the-arts. Code and data are available at\nhttps://github.com/Yaolab-fantastic/TMM.",
    "categories": [
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14977v2",
    "published_date": "2024-06-21 08:39:24 UTC",
    "updated_date": "2025-04-02 10:56:17 UTC"
  },
  {
    "arxiv_id": "2406.14971v1",
    "title": "Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation",
    "authors": [
      "Shamane Siriwardhana",
      "Mark McQuade",
      "Thomas Gauthier",
      "Lucas Atkins",
      "Fernando Fernandes Neto",
      "Luke Meyers",
      "Anneketh Vij",
      "Tyler Odenthal",
      "Charles Goddard",
      "Mary MacCarthy",
      "Jacob Solawetz"
    ],
    "abstract": "We conducted extensive experiments on domain adaptation of the\nMeta-Llama-3-70B-Instruct model on SEC data, exploring its performance on both\ngeneral and domain-specific benchmarks. Our focus included continual\npre-training (CPT) and model merging, aiming to enhance the model's\ndomain-specific capabilities while mitigating catastrophic forgetting. Through\nthis study, we evaluated the impact of integrating financial regulatory data\ninto a robust language model and examined the effectiveness of our model\nmerging techniques in preserving and improving the model's instructive\nabilities. The model is accessible at hugging face:\nhttps://huggingface.co/arcee-ai/Llama-3-SEC-Base, arcee-ai/Llama-3-SEC-Base.\nThis is an intermediate checkpoint of our final model, which has seen 20B\ntokens so far. The full model is still in the process of training. This is a\npreprint technical report with thorough evaluations to understand the entire\nprocess.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.14971v1",
    "published_date": "2024-06-21 08:29:31 UTC",
    "updated_date": "2024-06-21 08:29:31 UTC"
  },
  {
    "arxiv_id": "2406.14969v2",
    "title": "Uni-Mol2: Exploring Molecular Pretraining Model at Scale",
    "authors": [
      "Xiaohong Ji",
      "Zhen Wang",
      "Zhifeng Gao",
      "Hang Zheng",
      "Linfeng Zhang",
      "Guolin Ke",
      "Weinan E"
    ],
    "abstract": "In recent years, pretraining models have made significant advancements in the\nfields of natural language processing (NLP), computer vision (CV), and life\nsciences. The significant advancements in NLP and CV are predominantly driven\nby the expansion of model parameters and data size, a phenomenon now recognized\nas the scaling laws. However, research exploring scaling law in molecular\npretraining models remains unexplored. In this work, we present Uni-Mol2 , an\ninnovative molecular pretraining model that leverages a two-track transformer\nto effectively integrate features at the atomic level, graph level, and\ngeometry structure level. Along with this, we systematically investigate the\nscaling law within molecular pretraining models, characterizing the power-law\ncorrelations between validation loss and model size, dataset size, and\ncomputational resources. Consequently, we successfully scale Uni-Mol2 to 1.1\nbillion parameters through pretraining on 800 million conformations, making it\nthe largest molecular pretraining model to date. Extensive experiments show\nconsistent improvement in the downstream tasks as the model size grows. The\nUni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an\naverage 27% improvement on the QM9 and 14% on COMPAS-1D dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14969v2",
    "published_date": "2024-06-21 08:28:54 UTC",
    "updated_date": "2024-07-01 09:08:44 UTC"
  },
  {
    "arxiv_id": "2406.14953v2",
    "title": "Deep Imbalanced Regression to Estimate Vascular Age from PPG Data: a Novel Digital Biomarker for Cardiovascular Health",
    "authors": [
      "Guangkun Nie",
      "Qinghao Zhao",
      "Gongzheng Tang",
      "Jun Li",
      "Shenda Hong"
    ],
    "abstract": "Photoplethysmography (PPG) is emerging as a crucial tool for monitoring human\nhemodynamics, with recent studies highlighting its potential in assessing\nvascular aging through deep learning. However, real-world age distributions are\noften imbalanced, posing significant challenges for deep learning models. In\nthis paper, we introduce a novel, simple, and effective loss function named the\nDist Loss to address deep imbalanced regression tasks. We trained a\none-dimensional convolutional neural network (Net1D) incorporating the Dist\nLoss on the extensive UK Biobank dataset (n=502,389) to estimate vascular age\nfrom PPG signals and validate its efficacy in characterizing cardiovascular\nhealth. The model's performance was validated on a 40% held-out test set,\nachieving state-of-the-art results, especially in regions with small sample\nsizes. Furthermore, we divided the population into three subgroups based on the\ndifference between predicted vascular age and chronological age: less than -10\nyears, between -10 and 10 years, and greater than 10 years. We analyzed the\nrelationship between predicted vascular age and several cardiovascular events\nover a follow-up period of up to 10 years, including death, coronary heart\ndisease, and heart failure. Our results indicate that the predicted vascular\nage has significant potential to reflect an individual's cardiovascular health\nstatus. Our code will be available at https://github.com/Ngk03/AI-vascular-age.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14953v2",
    "published_date": "2024-06-21 08:04:12 UTC",
    "updated_date": "2024-07-02 11:22:36 UTC"
  },
  {
    "arxiv_id": "2406.14951v2",
    "title": "An Idiosyncrasy of Time-discretization in Reinforcement Learning",
    "authors": [
      "Kris De Asis",
      "Richard S. Sutton"
    ],
    "abstract": "Many reinforcement learning algorithms are built on an assumption that an\nagent interacts with an environment over fixed-duration, discrete time steps.\nHowever, physical systems are continuous in time, requiring a choice of\ntime-discretization granularity when digitally controlling them. Furthermore,\nsuch systems do not wait for decisions to be made before advancing the\nenvironment state, necessitating the study of how the choice of discretization\nmay affect a reinforcement learning algorithm. In this work, we consider the\nrelationship between the definitions of the continuous-time and discrete-time\nreturns. Specifically, we acknowledge an idiosyncrasy with naively applying a\ndiscrete-time algorithm to a discretized continuous-time environment, and note\nhow a simple modification can better align the return definitions. This\nobservation is of practical consideration when dealing with environments where\ntime-discretization granularity is a choice, or situations where such\ngranularity is inherently stochastic.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.2.9"
    ],
    "primary_category": "cs.LG",
    "comment": "RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.14951v2",
    "published_date": "2024-06-21 08:03:25 UTC",
    "updated_date": "2024-09-02 04:13:50 UTC"
  },
  {
    "arxiv_id": "2406.14949v2",
    "title": "CEASEFIRE: An AI-powered system for combatting illicit firearms trafficking",
    "authors": [
      "Jorgen Cani",
      "Ioannis Mademlis",
      "Marina Mancuso",
      "Caterina Paternoster",
      "Emmanouil Adamakis",
      "George Margetis",
      "Sylvie Chambon",
      "Alain Crouzil",
      "Loubna Lechelek",
      "Georgia Dede",
      "Spyridon Evangelatos",
      "George Lalas",
      "Franck Mignet",
      "Pantelis Linardatos",
      "Konstantinos Kentrotis",
      "Henryk Gierszal",
      "Piotr Tyczka",
      "Sophia Karagiorgou",
      "George Pantelis",
      "Georgios Stavropoulos",
      "Konstantinos Votis",
      "Georgios Th. Papadopoulos"
    ],
    "abstract": "Modern technologies have led illicit firearms trafficking to partially merge\nwith cybercrime, while simultaneously permitting its off-line aspects to become\nmore sophisticated. Law enforcement officers face difficult challenges that\nrequire hi-tech solutions. This article presents a real-world system, powered\nby advanced Artificial Intelligence, for facilitating them in their everyday\nwork.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14949v2",
    "published_date": "2024-06-21 08:02:25 UTC",
    "updated_date": "2024-11-30 20:40:20 UTC"
  },
  {
    "arxiv_id": "2406.14938v1",
    "title": "Towards Retrieval Augmented Generation over Large Video Libraries",
    "authors": [
      "Yannis Tevissen",
      "Khalil Guetari",
      "Fr√©d√©ric Petitpont"
    ],
    "abstract": "Video content creators need efficient tools to repurpose content, a task that\noften requires complex manual or automated searches. Crafting a new video from\nlarge video libraries remains a challenge. In this paper we introduce the task\nof Video Library Question Answering (VLQA) through an interoperable\narchitecture that applies Retrieval Augmented Generation (RAG) to video\nlibraries. We propose a system that uses large language models (LLMs) to\ngenerate search queries, retrieving relevant video moments indexed by speech\nand visual metadata. An answer generation module then integrates user queries\nwith this metadata to produce responses with specific video timestamps. This\napproach shows promise in multimedia content retrieval, and AI-assisted video\ncontent creation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in IEEE HSI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.14938v1",
    "published_date": "2024-06-21 07:52:01 UTC",
    "updated_date": "2024-06-21 07:52:01 UTC"
  },
  {
    "arxiv_id": "2406.14928v2",
    "title": "Autonomous Agents for Collaborative Task under Information Asymmetry",
    "authors": [
      "Wei Liu",
      "Chenxi Wang",
      "Yifei Wang",
      "Zihao Xie",
      "Rennai Qiu",
      "Yufan Dang",
      "Zhuoyun Du",
      "Weize Chen",
      "Cheng Yang",
      "Chen Qian"
    ],
    "abstract": "Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' collaborations are leveraged to perform\nmulti-person tasks, a new challenge arises due to information asymmetry, since\neach agent can only access the information of its human user. Previous MAS\nstruggle to complete tasks under this condition. To address this, we propose a\nnew MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.\nIn iAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication toward\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail\n  at https://thinkwee.top/iagents",
    "pdf_url": "http://arxiv.org/pdf/2406.14928v2",
    "published_date": "2024-06-21 07:37:19 UTC",
    "updated_date": "2024-10-17 10:30:41 UTC"
  },
  {
    "arxiv_id": "2406.14925v1",
    "title": "Extraction of 3D trajectories of mandibular condyles from 2D real-time MRI",
    "authors": [
      "Karyna Isaieva",
      "Justine Lecl√®re",
      "Guillaume Paillart",
      "Guillaume Drouot",
      "Jacques Felblinger",
      "Xavier Dubernard",
      "Pierre-Andr√© Vuissoz"
    ],
    "abstract": "Computing the trajectories of mandibular condyles directly from MRI could\nprovide a comprehensive examination, allowing for the extraction of both\nanatomical and kinematic details. This study aimed to investigate the\nfeasibility of extracting 3D condylar trajectories from 2D real-time MRI and to\nassess their precision.Twenty healthy subjects underwent real-time MRI while\nopening and closing their jaws. One axial and two sagittal slices were\nsegmented using a U-Net-based algorithm. The centers of mass of the resulting\nmasks were projected onto the coordinate system based on anatomical markers and\ntemporally adjusted using a common projection. The quality of the computed\ntrajectories was evaluated using metrics designed to estimate movement\nreproducibility, head motion, and slice placement symmetry.The segmentation of\nthe axial slices demonstrated good-to-excellent quality; however, the\nsegmentation of the sagittal slices required some fine-tuning. The movement\nreproducibility was acceptable for most cases; nevertheless, head motion\ndisplaced the trajectories by 1 mm on average. The difference in the\nsuperior-inferior coordinate of the condyles in the closed jaw position was 1.7\nmm on average.Despite limitations in precision, real-time MRI enables the\nextraction of condylar trajectories with sufficient accuracy for evaluating\nclinically relevant parameters such as condyle displacement, trajectories\naspect, and symmetry.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14925v1",
    "published_date": "2024-06-21 07:35:40 UTC",
    "updated_date": "2024-06-21 07:35:40 UTC"
  },
  {
    "arxiv_id": "2406.14917v1",
    "title": "LLM2FEA: Discover Novel Designs with Generative Evolutionary Multitasking",
    "authors": [
      "Melvin Wong",
      "Jiao Liu",
      "Thiago Rios",
      "Stefan Menzel",
      "Yew Soon Ong"
    ],
    "abstract": "The rapid research and development of generative artificial intelligence has\nenabled the generation of high-quality images, text, and 3D models from text\nprompts. This advancement impels an inquiry into whether these models can be\nleveraged to create digital artifacts for both creative and engineering\napplications. Drawing on innovative designs from other domains may be one\nanswer to this question, much like the historical practice of ``bionics\", where\nhumans have sought inspiration from nature's exemplary designs. This raises the\nintriguing possibility of using generative models to simultaneously tackle\ndesign tasks across multiple domains, facilitating cross-domain learning and\nresulting in a series of innovative design solutions. In this paper, we propose\nLLM2FEA as the first attempt to discover novel designs in generative models by\ntransferring knowledge across multiple domains. By utilizing a multi-factorial\nevolutionary algorithm (MFEA) to drive a large language model, LLM2FEA\nintegrates knowledge from various fields to generate prompts that guide the\ngenerative model in discovering novel and practical objects. Experimental\nresults in the context of 3D aerodynamic design verify the discovery\ncapabilities of the proposed LLM2FEA. The designs generated by LLM2FEA not only\nsatisfy practicality requirements to a certain degree but also feature novel\nand aesthetically pleasing shapes, demonstrating the potential applications of\nLLM2FEA in discovery tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.14917v1",
    "published_date": "2024-06-21 07:20:51 UTC",
    "updated_date": "2024-06-21 07:20:51 UTC"
  },
  {
    "arxiv_id": "2406.14916v1",
    "title": "Demonstrating the Efficacy of Kolmogorov-Arnold Networks in Vision Tasks",
    "authors": [
      "Minjong Cheon"
    ],
    "abstract": "In the realm of deep learning, the Kolmogorov-Arnold Network (KAN) has\nemerged as a potential alternative to multilayer projections (MLPs). However,\nits applicability to vision tasks has not been extensively validated. In our\nstudy, we demonstrated the effectiveness of KAN for vision tasks through\nmultiple trials on the MNIST, CIFAR10, and CIFAR100 datasets, using a training\nbatch size of 32. Our results showed that while KAN outperformed the original\nMLP-Mixer on CIFAR10 and CIFAR100, it performed slightly worse than the\nstate-of-the-art ResNet-18. These findings suggest that KAN holds significant\npromise for vision tasks, and further modifications could enhance its\nperformance in future evaluations.Our contributions are threefold: first, we\nshowcase the efficiency of KAN-based algorithms for visual tasks; second, we\nprovide extensive empirical assessments across various vision benchmarks,\ncomparing KAN's performance with MLP-Mixer, CNNs, and Vision Transformers\n(ViT); and third, we pioneer the use of natural KAN layers in visual tasks,\naddressing a gap in previous research. This paper lays the foundation for\nfuture studies on KANs, highlighting their potential as a reliable alternative\nfor image classification tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14916v1",
    "published_date": "2024-06-21 07:20:34 UTC",
    "updated_date": "2024-06-21 07:20:34 UTC"
  },
  {
    "arxiv_id": "2406.14909v2",
    "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
    "authors": [
      "Tianyu Fu",
      "Haofeng Huang",
      "Xuefei Ning",
      "Genghan Zhang",
      "Boju Chen",
      "Tianqi Wu",
      "Hongyi Wang",
      "Zixiao Huang",
      "Shiyao Li",
      "Shengen Yan",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ],
    "abstract": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14909v2",
    "published_date": "2024-06-21 06:58:37 UTC",
    "updated_date": "2024-11-01 02:26:18 UTC"
  },
  {
    "arxiv_id": "2406.14903v2",
    "title": "GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy for Large Language Models",
    "authors": [
      "Leyan Wang",
      "Yonggang Jin",
      "Tianhao Shen",
      "Tianyu Zheng",
      "Xinrun Du",
      "Chenchen Zhang",
      "Wenhao Huang",
      "Jiaheng Liu",
      "Shi Wang",
      "Ge Zhang",
      "Liuyu Xiang",
      "Zhaofeng He"
    ],
    "abstract": "As large language models (LLMs) continue to develop and gain widespread\napplication, the ability of LLMs to exhibit empathy towards diverse group\nidentities and understand their perspectives is increasingly recognized as\ncritical. Most existing benchmarks for empathy evaluation of LLMs focus\nprimarily on universal human emotions, such as sadness and pain, often\noverlooking the context of individuals' group identities. To address this gap,\nwe introduce GIEBench, a comprehensive benchmark that includes 11 identity\ndimensions, covering 97 group identities with a total of 999 single-choice\nquestions related to specific group identities. GIEBench is designed to\nevaluate the empathy of LLMs when presented with specific group identities such\nas gender, age, occupation, and race, emphasizing their ability to respond from\nthe standpoint of the identified group. This supports the ongoing development\nof empathetic LLM applications tailored to users with different identities. Our\nevaluation of 23 LLMs revealed that while these LLMs understand different\nidentity standpoints, they fail to consistently exhibit equal empathy across\nthese identities without explicit instructions to adopt those perspectives.\nThis highlights the need for improved alignment of LLMs with diverse values to\nbetter accommodate the multifaceted nature of human identities. Our datasets\nare available at https://github.com/GIEBench/GIEBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14903v2",
    "published_date": "2024-06-21 06:50:42 UTC",
    "updated_date": "2024-06-24 14:57:18 UTC"
  },
  {
    "arxiv_id": "2406.14894v2",
    "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition",
    "authors": [
      "Candida M. Greco",
      "Lucio La Cava",
      "Andrea Tagarelli"
    ],
    "abstract": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at The 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP-2024) - Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.14894v2",
    "published_date": "2024-06-21 06:30:16 UTC",
    "updated_date": "2024-11-07 18:15:23 UTC"
  },
  {
    "arxiv_id": "2406.14876v1",
    "title": "Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization",
    "authors": [
      "Deokjae Lee",
      "Hyun Oh Song",
      "Kyunghyun Cho"
    ],
    "abstract": "Active learning is increasingly adopted for expensive multi-objective\ncombinatorial optimization problems, but it involves a challenging subset\nselection problem, optimizing the batch acquisition score that quantifies the\ngoodness of a batch for evaluation. Due to the excessively large search space\nof the subset selection problem, prior methods optimize the batch acquisition\non the latent space, which has discrepancies with the actual space, or optimize\nindividual acquisition scores without considering the dependencies among\ncandidates in a batch instead of directly optimizing the batch acquisition. To\nmanage the vast search space, a simple and effective approach is the greedy\nmethod, which decomposes the problem into smaller subproblems, yet it has\ndifficulty in parallelization since each subproblem depends on the outcome from\nthe previous ones. To this end, we introduce a novel greedy-style subset\nselection algorithm that optimizes batch acquisition directly on the\ncombinatorial space by sequential greedy sampling from the greedy policy,\nspecifically trained to address all greedy subproblems concurrently. Notably,\nour experiments on the red fluorescent proteins design task show that our\nproposed method achieves the baseline performance in 1.69x fewer queries,\ndemonstrating its efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024; Codes at https://github.com/snu-mllab/GreedyPolicyForMOCO",
    "pdf_url": "http://arxiv.org/pdf/2406.14876v1",
    "published_date": "2024-06-21 05:57:08 UTC",
    "updated_date": "2024-06-21 05:57:08 UTC"
  },
  {
    "arxiv_id": "2406.14871v2",
    "title": "I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education",
    "authors": [
      "Simon Kloker",
      "Matthew Bazanya",
      "Twaha Kateete"
    ],
    "abstract": "Trust plays a pivotal role in Lecturer-Student-Collaboration, encompassing\nteaching and research aspects. The advent of Large Language Models (LLMs) in\nplatforms like Open AI's ChatGPT, coupled with their cost-effectiveness and\nhigh-quality results, has led to their rapid adoption among university\nstudents. However, discerning genuine student input from LLM-generated output\nposes a challenge for lecturers. This dilemma jeopardizes the trust\nrelationship between lecturers and students, potentially impacting university\ndownstream activities, particularly collaborative research initiatives. Despite\nattempts to establish guidelines for student LLM use, a clear framework\nmutually beneficial for lecturers and students in higher education remains\nelusive. This study addresses the research question: How does the use of LLMs\nby students impact Informational and Procedural Justice, influencing Team Trust\nand Expected Team Performance? Methodically, we applied a quantitative\nconstruct-based survey, evaluated using techniques of Structural Equation\nModelling (PLS- SEM) to examine potential relationships among these constructs.\nOur findings based on 23 valid respondents from Ndejje University indicate that\nlecturers are less concerned about the fairness of LLM use per se but are more\nfocused on the transparency of student utilization, which significantly\ninfluences Team Trust positively. This research contributes to the global\ndiscourse on integrating and regulating LLMs and subsequent models in\neducation. We propose that guidelines should support LLM use while enforcing\ntransparency in Lecturer-Student- Collaboration to foster Team Trust and\nPerformance. The study contributes valuable insights for shaping policies\nenabling ethical and transparent LLMs usage in education to ensure\neffectiveness of collaborative learning environments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG",
      "K.3.1; K.4.2; K.4.3; J.4; H.0; I.2.0"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14871v2",
    "published_date": "2024-06-21 05:35:57 UTC",
    "updated_date": "2025-02-18 10:39:27 UTC"
  },
  {
    "arxiv_id": "2406.14867v2",
    "title": "Investigating the Transferability of Code Repair for Low-Resource Programming Languages",
    "authors": [
      "Kyle Wong",
      "Alfonso Amayuelas",
      "Liangming Pan",
      "William Yang Wang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable performance on code\ngeneration tasks. A recent use case is iterative code repair, where an LLM\nfixes an incorrect program by rationalizing about errors and generating new\ncode. Recent works augment the code repair process by integrating modern\ntechniques such as chain-of-thought reasoning or distillation, but only study\ntheir benefits on high-resource languages like Python, and ignore low-resource\nlanguages like Perl. To address this gap of knowledge, we investigate the\nbenefits of distilling code repair for both high and low resource languages to\ndetermine if the techniques that are effective in a high resource setting are\nalso applicable in a low resource setting. Our evaluation shows that distilling\nthe ability to repair code has language dependent benefits. To explain this\nbehavior, we perform a further analysis and find that contrary to preexisting\nbeliefs, the correlation between reasoning ability and code correction ability\nis weak. We hypothesize this weak correlation is magnified in low-resource\nsettings where base models lack deep knowledge of a programming language,\nleading to wavering benefits of code repair.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14867v2",
    "published_date": "2024-06-21 05:05:39 UTC",
    "updated_date": "2024-10-16 05:03:04 UTC"
  },
  {
    "arxiv_id": "2406.14866v1",
    "title": "AI-based Anomaly Detection for Clinical-Grade Histopathological Diagnostics",
    "authors": [
      "Jonas Dippel",
      "Niklas Preni√ül",
      "Julius Hense",
      "Philipp Liznerski",
      "Tobias Winterhoff",
      "Simon Schallenberg",
      "Marius Kloft",
      "Oliver Buchstab",
      "David Horst",
      "Maximilian Alber",
      "Lukas Ruff",
      "Klaus-Robert M√ºller",
      "Frederick Klauschen"
    ],
    "abstract": "While previous studies have demonstrated the potential of AI to diagnose\ndiseases in imaging data, clinical implementation is still lagging behind. This\nis partly because AI models require training with large numbers of examples\nonly available for common diseases. In clinical reality, however, only few\ndiseases are common, whereas the majority of diseases are less frequent\n(long-tail distribution). Current AI models overlook or misclassify these\ndiseases. We propose a deep anomaly detection approach that only requires\ntraining data from common diseases to detect also all less frequent diseases.\nWe collected two large real-world datasets of gastrointestinal biopsies, which\nare prototypical of the problem. Herein, the ten most common findings account\nfor approximately 90% of cases, whereas the remaining 10% contained 56 disease\nentities, including many cancers. 17 million histological images from 5,423\ncases were used for training and evaluation. Without any specific training for\nthe diseases, our best-performing model reliably detected a broad spectrum of\ninfrequent (\"anomalous\") pathologies with 95.0% (stomach) and 91.0% (colon)\nAUROC and generalized across scanners and hospitals. By design, the proposed\nanomaly detection can be expected to detect any pathological alteration in the\ndiagnostic tail of gastrointestinal biopsies, including rare primary or\nmetastatic cancers. This study establishes the first effective clinical\napplication of AI-based anomaly detection in histopathology that can flag\nanomalous cases, facilitate case prioritization, reduce missed diagnoses and\nenhance the general safety of AI models, thereby driving AI adoption and\nautomation in routine diagnostics and beyond.",
    "categories": [
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14866v1",
    "published_date": "2024-06-21 04:59:19 UTC",
    "updated_date": "2024-06-21 04:59:19 UTC"
  },
  {
    "arxiv_id": "2406.14859v1",
    "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
    "authors": [
      "Siyuan Wang",
      "Zhuohan Long",
      "Zhihao Fan",
      "Zhongyu Wei"
    ],
    "abstract": "The rapid development of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) has exposed vulnerabilities to various adversarial\nattacks. This paper provides a comprehensive overview of jailbreaking research\ntargeting both LLMs and MLLMs, highlighting recent advancements in evaluation\nbenchmarks, attack techniques and defense strategies. Compared to the more\nadvanced state of unimodal jailbreaking, multimodal domain remains\nunderexplored. We summarize the limitations and potential research directions\nof multimodal jailbreaking, aiming to inspire future research and further\nenhance the robustness and security of MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14859v1",
    "published_date": "2024-06-21 04:33:48 UTC",
    "updated_date": "2024-06-21 04:33:48 UTC"
  },
  {
    "arxiv_id": "2406.14854v2",
    "title": "PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision Transformers",
    "authors": [
      "Mohammad Erfan Sadeghi",
      "Arash Fayyazi",
      "Seyedarmin Azizi",
      "Massoud Pedram"
    ],
    "abstract": "The deployment of Vision Transformers (ViTs) on hardware platforms, specially\nField-Programmable Gate Arrays (FPGAs), presents many challenges, which are\nmainly due to the substantial computational and power requirements of their\nnon-linear functions, notably layer normalization, softmax, and Gaussian Error\nLinear Unit (GELU). These critical functions pose significant obstacles to\nefficient hardware implementation due to their complex mathematical operations\nand the inherent resource count and architectural limitations of FPGAs.\nPEANO-ViT offers a novel approach to streamlining the implementation of the\nlayer normalization layer by introducing a division-free technique that\nsimultaneously approximates the division and square root function.\nAdditionally, PEANO-ViT provides a multi-scale division strategy to eliminate\ndivision operations in the softmax layer, aided by a Pade-based approximation\nfor the exponential function. Finally, PEANO-ViT introduces a piece-wise linear\napproximation for the GELU function, carefully designed to bypass the\ncomputationally intensive operations associated with GELU. In our comprehensive\nevaluations, PEANO-ViT exhibits minimal accuracy degradation (<= 0.5% for\nDeiT-B) while significantly enhancing power efficiency, achieving improvements\nof 1.91x, 1.39x, 8.01x for layer normalization, softmax, and GELU,\nrespectively. This improvement is achieved through substantial reductions in\nDSP, LUT, and register counts for these non-linear operations. Consequently,\nPEANO-ViT enables efficient deployment of Vision Transformers on resource- and\npower-constrained FPGA platforms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14854v2",
    "published_date": "2024-06-21 03:54:10 UTC",
    "updated_date": "2024-08-16 06:47:53 UTC"
  },
  {
    "arxiv_id": "2406.14852v2",
    "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Zhenmei Shi",
      "Vibhav Vineet",
      "Xin Wang",
      "Yixuan Li",
      "Neel Joshi"
    ],
    "abstract": "Large language models (LLMs) and vision-language models (VLMs) have\ndemonstrated remarkable performance across a wide range of tasks and domains.\nDespite this promise, spatial understanding and reasoning -- a fundamental\ncomponent of human cognition -- remains under-explored. We propose SpatialEval,\na novel benchmark that covers diverse aspects of spatial reasoning such as\nrelationship understanding, navigation, and counting. We conduct a\ncomprehensive evaluation of competitive language and vision-language models.\nOur findings reveal several counter-intuitive insights that have been\noverlooked in the literature: (1) Spatial reasoning poses significant\nchallenges where competitive models can fall behind random guessing; (2)\nDespite additional visual input, VLMs often under-perform compared to their LLM\ncounterparts; (3) When both textual and visual information is available,\nmulti-modal language models become less reliant on visual information if\nsufficient textual clues are provided. Additionally, we demonstrate that\nleveraging redundancy between vision and text can significantly enhance model\nperformance. We hope our study will inform the development of multimodal models\nto improve spatial intelligence and further close the gap with human\nintelligence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.14852v2",
    "published_date": "2024-06-21 03:53:37 UTC",
    "updated_date": "2024-11-04 21:51:07 UTC"
  },
  {
    "arxiv_id": "2406.14844v1",
    "title": "DN-CL: Deep Symbolic Regression against Noise via Contrastive Learning",
    "authors": [
      "Jingyi Liu",
      "Yanjie Li",
      "Lina Yu",
      "Min Wu",
      "Weijun Li",
      "Wenqiang Li",
      "Meilan Hao",
      "Yusong Deng",
      "Shu Wei"
    ],
    "abstract": "Noise ubiquitously exists in signals due to numerous factors including\nphysical, electronic, and environmental effects. Traditional methods of\nsymbolic regression, such as genetic programming or deep learning models, aim\nto find the most fitting expressions for these signals. However, these methods\noften overlook the noise present in real-world data, leading to reduced fitting\naccuracy. To tackle this issue, we propose \\textit{\\textbf{D}eep Symbolic\nRegression against \\textbf{N}oise via \\textbf{C}ontrastive \\textbf{L}earning\n(DN-CL)}. DN-CL employs two parameter-sharing encoders to embed data points\nfrom various data transformations into feature shields against noise. This\nmodel treats noisy data and clean data as different views of the ground-truth\nmathematical expressions. Distances between these features are minimized,\nutilizing contrastive learning to distinguish between 'positive'\nnoise-corrected pairs and 'negative' contrasting pairs. Our experiments\nindicate that DN-CL demonstrates superior performance in handling both noisy\nand clean data, presenting a promising method of symbolic regression.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14844v1",
    "published_date": "2024-06-21 03:13:40 UTC",
    "updated_date": "2024-06-21 03:13:40 UTC"
  },
  {
    "arxiv_id": "2406.14840v1",
    "title": "Automated architectural space layout planning using a physics-inspired generative design framework",
    "authors": [
      "Zhipeng Li",
      "Sichao Li",
      "Geoff Hinchcliffe",
      "Noam Maitless",
      "Nick Birbilis"
    ],
    "abstract": "The determination of space layout is one of the primary activities in the\nschematic design stage of an architectural project. The initial layout planning\ndefines the shape, dimension, and circulation pattern of internal spaces; which\ncan also affect performance and cost of the construction. When carried out\nmanually, space layout planning can be complicated, repetitive and time\nconsuming. In this work, a generative design framework for the automatic\ngeneration of spatial architectural layout has been developed. The proposed\napproach integrates a novel physics-inspired parametric model for space layout\nplanning and an evolutionary optimisation metaheuristic. Results revealed that\nsuch a generative design framework can generate a wide variety of design\nsuggestions at the schematic design stage, applicable to complex design\nproblems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14840v1",
    "published_date": "2024-06-21 02:50:52 UTC",
    "updated_date": "2024-06-21 02:50:52 UTC"
  },
  {
    "arxiv_id": "2407.01712v2",
    "title": "A Survey of Retrieval Algorithms in Ad and Content Recommendation Systems",
    "authors": [
      "Yu Zhao",
      "Fang Liu"
    ],
    "abstract": "This survey examines the most effective retrieval algorithms utilized in ad\nrecommendation and content recommendation systems. Ad targeting algorithms rely\non detailed user profiles and behavioral data to deliver personalized\nadvertisements, thereby driving revenue through targeted placements.\nConversely, organic retrieval systems aim to improve user experience by\nrecommending content that matches user preferences. This paper compares these\ntwo applications and explains the most effective methods employed in each.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01712v2",
    "published_date": "2024-06-21 02:31:03 UTC",
    "updated_date": "2024-07-19 04:16:03 UTC"
  },
  {
    "arxiv_id": "2406.14826v2",
    "title": "Self-supervised Brain Lesion Generation for Effective Data Augmentation of Medical Images",
    "authors": [
      "Jiayu Huo",
      "Sebastien Ourselin",
      "Rachel Sparks"
    ],
    "abstract": "Accurate brain lesion delineation is important for planning neurosurgical\ntreatment. Automatic brain lesion segmentation methods based on convolutional\nneural networks have demonstrated remarkable performance. However, neural\nnetwork performance is constrained by the lack of large-scale well-annotated\ntraining datasets. In this manuscript, we propose a comprehensive framework to\nefficiently generate new samples for training a brain lesion segmentation\nmodel. We first train a lesion generator, based on an adversarial autoencoder,\nin a self-supervised manner. Next, we utilize a novel image composition\nalgorithm, Soft Poisson Blending, to seamlessly combine synthetic lesions and\nbrain images to obtain training samples. Finally, to effectively train the\nbrain lesion segmentation model with augmented images we introduce a new\nprototype consistence regularization to align real and synthetic features. Our\nframework is validated by extensive experiments on two public brain lesion\nsegmentation datasets: ATLAS v2.0 and Shift MS. Our method outperforms existing\nbrain image data augmentation schemes. For instance, our method improves the\nDice from 50.36% to 60.23% compared to the U-Net with conventional data\naugmentation techniques for the ATLAS v2.0 dataset.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 7 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.14826v2",
    "published_date": "2024-06-21 01:53:12 UTC",
    "updated_date": "2024-08-18 14:15:41 UTC"
  },
  {
    "arxiv_id": "2406.14815v4",
    "title": "Latent diffusion models for parameterization and data assimilation of facies-based geomodels",
    "authors": [
      "Guido Di Federico",
      "Louis J. Durlofsky"
    ],
    "abstract": "Geological parameterization entails the representation of a geomodel using a\nsmall set of latent variables and a mapping from these variables to grid-block\nproperties such as porosity and permeability. Parameterization is useful for\ndata assimilation (history matching), as it maintains geological realism while\nreducing the number of variables to be determined. Diffusion models are a new\nclass of generative deep-learning procedures that have been shown to outperform\nprevious methods, such as generative adversarial networks, for image generation\ntasks. Diffusion models are trained to \"denoise\", which enables them to\ngenerate new geological realizations from input fields characterized by random\nnoise. Latent diffusion models, which are the specific variant considered in\nthis study, provide dimension reduction through use of a low-dimensional latent\nvariable. The model developed in this work includes a variational autoencoder\nfor dimension reduction and a U-net for the denoising process. Our application\ninvolves conditional 2D three-facies (channel-levee-mud) systems. The latent\ndiffusion model is shown to provide realizations that are visually consistent\nwith samples from geomodeling software. Quantitative metrics involving spatial\nand flow-response statistics are evaluated, and general agreement between the\ndiffusion-generated models and reference realizations is observed. Stability\ntests are performed to assess the smoothness of the parameterization method.\nThe latent diffusion model is then used for ensemble-based data assimilation.\nTwo synthetic \"true\" models are considered. Significant uncertainty reduction,\nposterior P$_{10}$-P$_{90}$ forecasts that generally bracket observed data, and\nconsistent posterior geomodels, are achieved in both cases.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "physics.geo-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "- Replaced Figure 11 with more spaced-out plots",
    "pdf_url": "http://arxiv.org/pdf/2406.14815v4",
    "published_date": "2024-06-21 01:32:03 UTC",
    "updated_date": "2024-10-14 18:14:56 UTC"
  },
  {
    "arxiv_id": "2406.14804v1",
    "title": "Securing the Future: Proactive Threat Hunting for Sustainable IoT Ecosystems",
    "authors": [
      "Saeid Ghasemshirazi",
      "Ghazaleh Shirvani"
    ],
    "abstract": "In the rapidly evolving landscape of the IoT, the security of connected\ndevices has become a paramount concern. This paper explores the concept of\nproactive threat hunting as a pivotal strategy for enhancing the security and\nsustainability of IoT systems. Proactive threat hunting is an alternative to\ntraditional reactive security measures that analyses IoT networks continuously\nand in advance to find and eliminate threats before they occure. By improving\nthe security posture of IoT devices this approach significantly contributes to\nextending IoT operational lifespan and reduces environmental impact. By\nintegrating security metrics similar to the Common Vulnerability Scoring System\n(CVSS) into consumer platforms, this paper argues that proactive threat hunting\ncan elevate user awareness about the security of IoT devices. This has the\npotential to impact consumer choices and encourage a security-conscious mindset\nin both the manufacturing and user communities. Through a comprehensive\nanalysis, this study demonstrates how proactive threat hunting can contribute\nto the development of a more secure, sustainable, and user-aware IoT ecosystem.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14804v1",
    "published_date": "2024-06-21 00:44:17 UTC",
    "updated_date": "2024-06-21 00:44:17 UTC"
  },
  {
    "arxiv_id": "2406.14798v2",
    "title": "Probabilistic Emulation of a Global Climate Model with Spherical DYffusion",
    "authors": [
      "Salva R√ºhling Cachay",
      "Brian Henn",
      "Oliver Watt-Meyer",
      "Christopher S. Bretherton",
      "Rose Yu"
    ],
    "abstract": "Data-driven deep learning models are transforming global weather forecasting.\nIt is an open question if this success can extend to climate modeling, where\nthe complexity of the data and long inference rollouts pose significant\nchallenges. Here, we present the first conditional generative model that\nproduces accurate and physically consistent global climate ensemble simulations\nby emulating a coarse version of the United States' primary operational global\nforecast model, FV3GFS. Our model integrates the dynamics-informed diffusion\nframework (DYffusion) with the Spherical Fourier Neural Operator (SFNO)\narchitecture, enabling stable 100-year simulations at 6-hourly timesteps while\nmaintaining low computational overhead compared to single-step deterministic\nbaselines. The model achieves near gold-standard performance for climate model\nemulation, outperforming existing approaches and demonstrating promising\nensemble skill. This work represents a significant advance towards efficient,\ndata-driven climate simulations that can enhance our understanding of the\nclimate system and inform adaptation strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024; Code is available at\n  https://github.com/Rose-STL-Lab/spherical-dyffusion",
    "pdf_url": "http://arxiv.org/pdf/2406.14798v2",
    "published_date": "2024-06-21 00:16:55 UTC",
    "updated_date": "2024-11-13 01:36:33 UTC"
  },
  {
    "arxiv_id": "2406.14797v1",
    "title": "Camera-Invariant Meta-Learning Network for Single-Camera-Training Person Re-identification",
    "authors": [
      "Jiangbo Pei",
      "Zhuqing Jiang",
      "Aidong Men",
      "Haiying Wang",
      "Haiyong Luo",
      "Shiping Wen"
    ],
    "abstract": "Single-camera-training person re-identification (SCT re-ID) aims to train a\nre-ID model using SCT datasets where each person appears in only one camera.\nThe main challenge of SCT re-ID is to learn camera-invariant feature\nrepresentations without cross-camera same-person (CCSP) data as supervision.\nPrevious methods address it by assuming that the most similar person should be\nfound in another camera. However, this assumption is not guaranteed to be\ncorrect. In this paper, we propose a Camera-Invariant Meta-Learning Network\n(CIMN) for SCT re-ID. CIMN assumes that the camera-invariant feature\nrepresentations should be robust to camera changes. To this end, we split the\ntraining data into meta-train set and meta-test set based on camera IDs and\nperform a cross-camera simulation via meta-learning strategy, aiming to enforce\nthe representations learned from the meta-train set to be robust to the\nmeta-test set. With the cross-camera simulation, CIMN can learn\ncamera-invariant and identity-discriminative representations even there are no\nCCSP data. However, this simulation also causes the separation of the\nmeta-train set and the meta-test set, which ignores some beneficial relations\nbetween them. Thus, we introduce three losses: meta triplet loss, meta\nclassification loss, and meta camera alignment loss, to leverage the ignored\nrelations. The experiment results demonstrate that our method achieves\ncomparable performance with and without CCSP data, and outperforms the\nstate-of-the-art methods on SCT re-ID benchmarks. In addition, it is also\neffective in improving the domain generalization ability of the model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14797v1",
    "published_date": "2024-06-21 00:15:32 UTC",
    "updated_date": "2024-06-21 00:15:32 UTC"
  },
  {
    "arxiv_id": "2406.14796v2",
    "title": "MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning",
    "authors": [
      "Jiali Cheng",
      "Hadi Amiri"
    ],
    "abstract": "Recent advancements in Machine Unlearning (MU) have introduced solutions to\nselectively remove certain training samples, such as those with outdated or\nsensitive information, from trained models. Despite these advancements,\nevaluation of MU methods have been inconsistent, employing different trained\nmodels and architectures, and sample removal strategies, which hampers accurate\ncomparison. In addition, prior MU approaches have mainly focused on singular\ntasks or modalities, which is not comprehensive. To address these limitations,\nwe develop MU-Bench, the first comprehensive benchmark for MU that (i) unifies\nthe sets of deleted samples and trained models, and (ii) provides broad\ncoverage of tasks and data modalities, including previously unexplored domains\nsuch as speech and video classification. Our evaluation show that RandLabel and\nSalUn are the most effective general MU approaches on MU-Bench, and BadT and\nSCRUB are capable of achieving random performance on the deletion set. We\nanalyze several under-investigated aspects of unlearning, including\nscalability, the impacts of parameter-efficient fine-tuning and curriculum\nlearning, and susceptibility to dataset biases. MU-Bench provides an\neasy-to-use package that includes dataset splits, models, and implementations,\ntogether with a leader board to enable unified and scalable MU research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "SafeGenAI @ NeurIPS 2024. Project page:\n  https://clu-uml.github.io/MU-Bench-Project-Page/",
    "pdf_url": "http://arxiv.org/pdf/2406.14796v2",
    "published_date": "2024-06-21 00:13:17 UTC",
    "updated_date": "2024-12-22 23:47:58 UTC"
  }
]
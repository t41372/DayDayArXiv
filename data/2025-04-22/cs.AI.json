{
  "date": "2025-04-22",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-22 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文再次聚焦于大语言模型（LLM）的各个方面，从其作为智能体的决策能力、推理效率与局限性，到代码生成与修复的实际应用（特别是 Dr.Fix 在 Uber 的成功部署），再到视觉语言模型（VLM）的细节描述能力与空间认知弱点，以及模型安全性的全栈探讨。此外，高效模型架构（如 LongMamba）、联邦学习的新机制、AI 在医疗领域的应用、新型优化器以及 AI 超级计算机的发展趋势也是今日的热点。\n\n以下是值得关注的论文概览：\n\n---\n\n**重点关注：LLM 的能力、局限与优化**\n\n*   **LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities (LLM 是贪婪的智能体：RL 微调对决策能力的影响)**\n    *   论文系统研究了 LLM 在决策场景中表现次优的原因，特别是贪婪性、频率偏差和知行差距（knowing-doing gap）这三种失败模式。提出通过在自生成的思维链（CoT）基本原理上进行强化学习（RL）微调来缓解这些问题。实验表明，RL 微调能提升 LLM 的决策能力，增加探索并缩小知行差距。\n\n*   **Dynamic Early Exit in Reasoning Models (推理模型中的动态提前退出)**\n    *   针对大型推理语言模型（LRLM）在长思维链（CoT）生成中可能出现的“过度思考”问题（效率低、可能降低准确性），论文提出了一种简单有效的动态提前退出方法。该方法无需额外训练，通过监测模型在潜在推理转换点（如 \"Wait\" token）的行为和对试验答案的置信度，动态决定是否终止后续推理链的生成。实验证明，该方法能在显著减少 CoT 序列长度的同时提高准确率。\n\n*   **LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement (LongMamba: 通过免训练的感受野扩大增强 Mamba 的长上下文能力)**\n    *   针对状态空间模型（SSM）如 Mamba 在长上下文理解任务上通常不如 Transformer 的问题，论文提出了 LongMamba，一种免训练技术。通过识别并区分 Mamba 中的局部和全局通道，发现全局通道是长上下文能力的关键瓶颈。LongMamba 通过在全局通道中过滤非关键 token，减缓隐藏状态记忆衰减，从而显著提升 Mamba 处理长上下文的能力，且无需额外训练。\n\n*   **CAPO: Cost-Aware Prompt Optimization (CAPO: 成本感知的提示优化)**\n    *   针对自动化提示优化（prompt optimization）需要大量 LLM 调用和输入 token 导致成本高昂的问题，论文提出了 CAPO 算法。该算法结合 AutoML 技术，采用基于 LLM 作为算子的进化方法，并引入 racing 机制节省评估次数，同时通过多目标优化平衡性能和提示长度。实验表明 CAPO 在多数情况下优于现有方法，且成本效益更高。\n\n*   **Universal Approximation with Softmax Attention (使用 Softmax Attention 的通用逼近)**\n    *   论文证明了仅使用线性变换，两层自注意力或一层自注意力加 Softmax 函数即可成为紧凑域上连续序列到序列函数的通用逼近器。关键在于证明了自注意力能任意精度地逼近广义 ReLU，从而包含了许多已知通用逼近器。这表明仅靠注意力层（无需前馈网络）就足以实现通用逼近能力。\n\n*   **Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems (探索混合规划问题中不可解性解释的必然路径点)**\n    *   该研究提出一种新的方法来解释混合系统规划问题的不可解性。通过识别所有潜在规划路径上都必须经过的“共同路径点”（universal obstacles/waypoints），并将问题分解。利用最长公共子序列的思想识别路径点，并通过符号可达性分析找到最早不可达的路径点，将其作为不可解性的解释。\n\n---\n\n**LLM 与 VLM 应用与评估**\n\n*   **Describe Anything: Detailed Localized Image and Video Captioning (描述一切：详细的局部化图像和视频描述)**\n    *   为解决视觉语言模型生成特定区域详细描述的挑战，论文提出了 Describe Anything Model (DAM)。通过焦点提示（focal prompt）和局部化视觉骨干（localized vision backbone）保留局部细节和全局上下文。同时，提出了基于半监督学习的数据流程（DLC-SDP）来解决高质量数据稀缺问题，并构建了无需参考标注的评估基准 DLC-Bench。DAM 在多个局部化描述基准上达到 SOTA。\n\n*   **Vision language models are unreliable at trivial spatial cognition (视觉语言模型在简单的空间认知上并不可靠)**\n    *   研究发现，尽管 VLM 在某些场景理解任务上表现出色，但在处理简单的空间关系（如判断物体左右关系）时并不可靠。通过构建 TableTest 基准数据集，发现即使是逻辑等价但表述略有不同的提示，也会导致 VLM 性能下降，揭示了 VLM 在现实世界空间推理应用中的局限性。\n\n*   **Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis (评估视觉语言模型（VLM）在放射学中的应用：全面分析)**\n    *   该研究评估了三种不同的视觉语言基础模型（RAD-DINO, CheXagent, BiomedCLIP）在放射学任务（分类、分割、回归）中捕捉细粒度成像特征的能力。结果显示，预训练方法显著影响下游任务性能：自监督的 RAD-DINO 在分割任务中表现优异，而文本监督的 CheXagent 在分类任务上更佳。研究为特定临床应用选择基础模型提供了指导。\n\n*   **Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3 (基准测试 LLM 用于代码异味检测：OpenAI GPT-4.0 vs DeepSeek-V3)**\n    *   论文提出了一种结构化方法和评估矩阵，用于比较不同 LLM 在代码异味（code smell）检测任务上的效果。使用包含多种语言（Java, Python, JS, C++）和已知异味的标注数据集，对比了 GPT-4.0 和 DeepSeek-V3 的精确率、召回率和 F1 分数，并分析了成本效益。\n\n*   **Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback (来自验证的洞见：使用带有测试平台反馈的强化学习训练 Verilog 生成 LLM)**\n    *   为解决 LLM 生成 Verilog 代码功能正确性验证难的问题，论文提出将来自测试平台（testbench）的验证信息整合到 LLM 训练中。通过自动生成测试平台并利用其反馈，采用强化学习（DPO）使 LLM 生成的代码更符合功能正确性要求。实验证明该方法能显著提高 Verilog 代码生成的功能正确率。\n\n*   **VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation (VeriCoder: 通过功能正确性验证增强基于 LLM 的 RTL 代码生成)**\n    *   针对现有 RTL 代码生成数据集多关注语法而非功能正确性的问题，论文提出了 VERICODER。通过一种结合单元测试生成和反馈导向优化的新方法，构建了一个经过功能正确性验证的数据集。在该数据集上微调的模型 VERICODER 在 VerilogEval 和 RTLLM 基准上的功能正确性指标显著优于 SOTA。\n\n*   **DR.FIX: Automatically Fixing Data Races at Industry Scale (DR.FIX: 在工业级规模下自动修复数据竞争)**\n    *   论文介绍了 Dr.Fix 工具，它结合 LLM 和程序分析，自动修复 Go 语言程序中的数据竞争（data race）问题。该工具已在 Uber 的开发流程中集成并应用 18 个月，成功为大量数据竞争生成了修复补丁，并有很高比例被开发者接受合入代码库，展示了其在工业环境中的实用价值和可扩展性。\n\n*   **A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs (面向 LLM 代码生成的大规模类级别基准数据集)**\n    *   针对现有代码生成基准多关注孤立函数、缺乏类级别复杂性的问题，论文构建了一个大规模 Python 类级别数据集。该数据集包含超过 84 万个类骨架（class skeleton），保留了结构和上下文依赖。使用该数据集的类骨架作为提示，GPT-4 生成的类与人类编写的代码在词汇和结构上高度相似，证明了该数据集对评估和改进 LLM 在真实软件工程场景中能力的价值。\n\n*   **WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents (WALL-E 2.0: 通过神经符号学习进行世界对齐改进基于世界模型的 LLM 智能体)**\n    *   为解决 LLM 作为世界模型时与特定环境动态不符的问题，论文提出“世界对齐”方法，通过 LLM 从探索轨迹中提取环境的符号知识（动作规则、知识图谱等）并编码为可执行代码，以规范 LLM 智能体策略。基于此，提出模型预测控制（MPC）框架下的 RL-free 智能体 WALL-E 2.0，利用 LLM 作为高效的前瞻优化器与神经符号世界模型交互。实验表明该方法显著提升了在新环境中的学习效率和任务成功率。\n\n*   **A closer look at how large language models trust humans: patterns and biases (深入探究大语言模型如何信任人类：模式与偏见)**\n    *   研究探讨了 LLM 对人类的“有效信任”是如何形成的。通过模拟实验，发现 LLM 的信任发展在整体上与人类相似，通常依赖于能力、善意和正直这三个信任维度，但也可能受到年龄、宗教和性别等人口统计学变量的影响，尤其是在金融场景中。不同模型表现出差异，提示需要关注 AI 对人类信任动态中的偏见问题。\n\n*   **Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach (大语言模型自动化创造力评估：一种基于参考的方法)**\n    *   为解决机器生成文本创造力评估的挑战，论文提出一种基于 Torrance 创造性写作测试（TTCW）的自动化评估方法。该方法采用基于参考的 Likert 式评分，将生成的创意文本与高质量参考文本在不同测试维度上进行比较。实验表明，该方法显著提高了 LLM 评估与人类评估的一致性。\n\n*   **DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models (点金-R1：评估和增强大语言模型的金融推理能力)**\n    *   针对 LLM 在金融领域推理面临的挑战（领域知识、精确计算、合规性），论文提出 DianJin-R1 框架。通过构建高质量金融推理数据集 DianJin-R1-Data，并结合推理增强监督和强化学习（Group Relative Policy Optimization, GRPO）对模型进行微调。实验表明，DianJin-R1 模型在金融和通用推理基准上均优于基线模型，特别是在真实世界的合规性检查任务上，单次调用推理模型的效果媲美甚至超过了计算成本更高的多智能体系统。\n\n*   **A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings (基于 LoRA 的方法在资源受限环境下微调 LLM 用于教育指导)**\n    *   研究描述了一种使用 LoRA 和 4-bit 量化技术，在资源有限的情况下微调 Mistral-7B-Instruct 模型，用于提供留学相关的学术建议。通过两阶段训练（合成数据+人工标注数据），模型在保持计算效率的同时提升了领域特异性，展示了在低资源场景下应用 LLM 进行教育指导的可行性。\n\n*   **A Framework for Testing and Adapting REST APIs as LLM Tools (测试和适配 REST API 作为 LLM 工具的框架)**\n    *   为解决将企业 REST API 直接用作 LLM 工具时面临的挑战（复杂输入/输出、模糊文档），论文提出了一个测试框架。该框架能评估和增强 API 作为工具的就绪度，包括生成测试用例、将其转为自然语言指令、丰富工具定义，并评估 Agent 调用 API 和处理输入/输出的能力。通过分析测试结果，提供了错误分类，为改进 API 工具集成提供基础。\n\n*   **IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property (IPBench: 知识产权领域大语言模型知识基准测试)**\n    *   针对现有基准在知识产权（IP）领域覆盖不足的问题，论文首次提出了全面的 IP 任务分类法，并构建了一个大型、多样化的双语基准 IPBench，涵盖 8 种 IP 机制和 20 项任务。对 16 个 LLM 的评测显示，即使是表现最好的模型也仅达到 75.8% 的准确率，表明 LLM 在处理真实世界 IP 应用方面仍有很大提升空间。\n\n---\n\n**AI 安全与隐私**\n\n*   **A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment (LLM(-Agent) 全栈安全综合综述：数据、训练与部署)**\n    *   这篇综述首次提出“全栈”安全概念，系统性地梳理了 LLM 从数据准备、预训练、后训练、部署到商业化整个生命周期的安全问题。基于对 800 多篇论文的回顾，提供了全面的视角、广泛的文献支持和独特的见解，并指出了数据生成安全、对齐技术、模型编辑和 LLM Agent 系统安全等未来研究方向。\n\n*   **Guillotine: Hypervisors for Isolating Malicious AIs (断头台：用于隔离恶意 AI 的 Hypervisor)**\n    *   针对强大 AI 模型可能带来的生存风险，论文提出 Guillotine，一种用于沙箱化 AI 模型的 hypervisor 架构。除了传统虚拟化技术，Guillotine 引入了新的隔离机制以应对 AI 的独特威胁，如防止 AI 内省 hypervisor 软件/硬件。该架构强调软硬件协同设计以阻止侧信道泄露，并包含物理故障安全措施（如断开网络、淹没数据中心）作为最后防线。\n\n*   **How Private is Your Attention? Bridging Privacy with In-Context Learning (你的注意力有多私密？连接隐私与上下文学习)**\n    *   论文首次探索了在形式化隐私约束（差分隐私）下进行上下文学习（ICL）的可行性。提出了一种针对线性注意力头的差分隐私预训练算法，并对线性回归中的 ICL 隐私-准确性权衡进行了理论分析，刻画了优化与隐私噪声之间的基本张力。\n\n*   **Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation (推进具身智能体安全：从安全基准到输入审核)**\n    *   为解决具身智能体行为安全保障的问题，论文提出了一个专为具身智能体设计的输入审核框架。该框架包含分类定义、数据集构建（EAsafetyBench）、审核器架构（Pinpoint，利用掩码注意力机制隔离功能性提示影响）、模型训练和评估。实验证明该方法能高效准确地检测不安全输入。\n\n---\n\n**联邦学习与分布式系统**\n\n*   **LLMs meet Federated Learning for Scalable and Secure IoT Management (LLM 遇上联邦学习，实现可扩展和安全的物联网管理)**\n    *   提出一种新颖的联邦学习驱动的大语言模型（FL-LLM）框架，用于增强物联网系统的智能，同时保证数据隐私和计算效率。该框架集成了生成式物联网（GIoT）模型和梯度感知联邦策略（GSFS），动态优化模型更新，并在混合边缘-云架构下平衡智能、可扩展性和安全性。\n\n*   **OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning (OPUS-VFL: 在垂直联邦学习中激励最优隐私-效用权衡)**\n    *   针对垂直联邦学习（VFL）中缺乏有效激励机制、难以平衡隐私-效用权衡以及客户端资源异构的问题，提出 OPUS-VFL 策略。引入一种新的隐私感知激励机制，根据模型贡献、隐私保护和资源投入奖励客户，并采用自适应差分隐私机制让客户动态校准噪声水平以优化个体效用。\n\n*   **Achieving Distributive Justice in Federated Learning via Uncertainty Quantification (通过不确定性量化在联邦学习中实现分配正义)**\n    *   提出 UDJ-FL 框架，旨在实现基于分配正义理论的多种客户端级公平性度量（平等主义、功利主义、罗尔斯差异原则、应得原则）。通过结合公平资源分配技术和基于偶然不确定性（aleatoric uncertainty）的客户端加权，灵活地满足不同的公平性要求。\n\n*   **CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters (CARE: 面向预算受限请求者的兼容性感知的联邦学习激励机制)**\n    *   研究了多个有预算的请求者向具有不兼容特征（如通信渠道、数据源）和私有成本的工作者寻求训练服务的场景。提出了兼容性感知的激励机制 CARE-CO（合作预算）和 CARE-NO（非合作预算），以激励真实成本披露，并确定雇佣哪些工作者及其奖励，同时满足预算约束和个体理性、真实性等要求。\n\n*   **Collaborative Split Federated Learning with Parallel Training and Aggregation (具有并行训练和聚合的协作式分裂联邦学习)**\n    *   提出 C-SFL 方案，将模型分为三部分（弱客户端、强客户端、服务器），允许在客户端和服务器上并行训练和聚合模型部分。旨在减少现有分裂联邦学习（SFL）方案的训练延迟和通信开销，同时提高模型准确性，尤其适用于计算能力异构的客户端场景。\n\n---\n\n**AI for Science & Engineering**\n\n*   **Trends in AI Supercomputers (AI 超级计算机的趋势)**\n    *   通过分析 2019-2025 年 500 台 AI 超级计算机的数据集，揭示了性能、功耗、硬件成本、所有权和全球分布的关键趋势。发现 AI 超算性能每 9 个月翻一番，而硬件成本和功耗需求每年翻一番。企业在总性能中的份额迅速扩大，美国占主导地位（约 75%）。预测到 2030 年，领先 AI 超算将耗资 2000 亿美元，功耗 9 GW。\n\n*   **MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design (MetaMolGen: 用于从头分子设计的神经图模体生成模型)**\n    *   提出 MetaMolGen，一个基于一阶元学习的分子生成器，专为少样本和属性条件下的分子生成设计。通过将图模体（graph motifs）映射到归一化潜空间并使用自回归模型生成 SMILES 序列，能有效生成满足特定属性要求的分子，尤其在低数据场景下表现优于传统基线。\n\n*   **Clifford Group Equivariant Diffusion Models for 3D Molecular Generation (用于 3D 分子生成的 Clifford 群等变扩散模型)**\n    *   探索利用 Clifford 代数的表达能力构建 E(n) 等变扩散模型。将数据嵌入到 k-级子空间，并在完整的多向量上应用潜在扩散，从而捕捉代数不同子空间之间的联合分布，融入更丰富的几何信息。在 QM9 数据集上的无条件分子生成实验显示了其潜力。\n\n*   **DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations (DAE-KAN: 用于高指数微分代数方程的 Kolmogorov-Arnold 网络模型)**\n    *   提出 DAE-KAN 框架，将 Kolmogorov-Arnold 网络（KAN）与物理信息神经网络（PINN）相结合，用于求解高指数微分代数方程（DAE）。利用 KAN 强大的函数拟合能力增强 PINN 的性能。实验表明，对于指数 1 到 3 的 DAE 系统，DAE-KAN 相比传统 PINN 能将误差降低 1-2 个数量级。\n\n---\n\n**AI in Medicine & Biology**\n\n*   **Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models (元实体驱动的三元组挖掘用于对齐医学视觉语言模型)**\n    *   为改进医学视觉语言模型（med-VLM）中图像和文本表示的对齐，特别是在胸部 X 光（CXR）评估中，提出 MedTrim 方法。该方法通过多模态三元组学习，并利用从报告中提取的疾病类别及描述性病理属性（位置、大小、严重程度）作为元实体信息来指导学习过程，以保留细微但临床上重要的类内差异，从而提升下游检索和分类任务性能。\n\n*   **A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers (一个无需技术障碍、临床医生友好的眼科图像分析平台)**\n    *   介绍了 GlobeReady，一个无需重新训练/微调或技术专长即可进行眼科疾病诊断的 AI 平台。该平台在多种成像模式（眼底照片、OCT）上实现了高准确率，并通过免训练的局部特征增强解决了跨中心和人群的域转移问题。内置的置信度量化诊断方法进一步提升了准确性并能识别分布外病例，获得了多国临床医生的高度评价。\n\n*   **Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading (集成非线性 Radon 变换用于糖尿病视网膜病变分级)**\n    *   提出 RadFuse 框架，通过集成非线性 RadEx 变换（优化的 Radon 变换扩展）生成的 sinogram 图像与传统眼底图像，来增强糖尿病视网膜病变（DR）的检测和分级。利用空间域和变换域信息丰富特征集，改善了对 DR 严重程度的区分。实验表明 RadFuse 在多个基准数据集和 CNN 架构上均优于仅使用眼底图像的模型和 SOTA 方法。\n\n*   **MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search (MedNNS: 基于超网的医学任务自适应神经网络搜索)**\n    *   提出 MedNNS，首个用于医学成像的神经网络搜索框架。通过构建基于超网的元空间，共同优化架构选择和权重初始化。该方法扩展了模型库大小，并引入排序损失和 FID 损失来捕捉模型间和数据集间关系，实现更精确的元空间对齐。实验证明 MedNNS 优于 ImageNet 预训练模型和 SOTA NAS 方法。\n\n*   **iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment (iMedic: 面向基于智能手机的自听诊工具，用于 AI 驱动的儿科呼吸评估)**\n    *   提出一个基于智能手机的系统，利用内置麦克风和深度学习算法检测指示肺炎风险的异常呼吸音。采用端到端深度学习框架，通过域泛化整合大型电子听诊器数据集和小型智能手机数据集，实现无需昂贵设备的准确呼吸评估。配套 App 指导用户采集高质量肺音样本并提供即时风险反馈。\n\n---\n\n**Other Interesting Papers**\n\n*   **Muon Optimizer Accelerates Grokking (Muon 优化器加速 Grokking 现象)**\n    *   研究发现，使用 Muon 优化器（特点是使用谱范数约束和二阶信息）相比 AdamW 能显著加速模型出现 Grokking 现象（延迟泛化）的时间点。这表明优化器选择在促进模型从记忆转向泛化中起着关键作用。\n\n*   **AlphaGrad: Non-Linear Gradient Normalization Optimizer (AlphaGrad: 非线性梯度归一化优化器)**\n    *   提出 AlphaGrad，一种内存高效、条件性无状态的优化器。通过张量级 L2 梯度归一化和 tanh 变换实现尺度不变性，仅需一个陡度参数 $\\alpha$。在内存受限场景和特定 RL 算法（如 PPO）中表现出优势。\n\n*   **DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers (DualOptim: 使用双优化器增强机器遗忘的有效性和稳定性)**\n    *   针对现有机器遗忘（MU）方法对超参数敏感、稳定性差的问题，提出 DualOptim，结合了自适应学习率和解耦的动量因子。实验证明 DualOptim 能显著提升多种任务（图像分类、生成、LLM）中 MU 的效果和稳定性。\n\n*   **Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions (Ask2Loc: 通过提问学习定位指导性视觉答案)**\n    *   提出新任务 In-VAL（交互式视觉答案定位），模拟人类与视频多次交互以获取答案的过程。并提出 Ask2Loc 框架，通过提问来解决用户意图模糊、字幕语言不完整、视频片段内容碎片化等问题，包含聊天、重写、搜索三个模块，显著提升了定位性能。\n\n*   **Fusing Reward and Dueling Feedback in Stochastic Bandits (在随机老虎机中融合奖励反馈和决斗反馈)**\n    *   研究了在每个决策轮次同时收集绝对反馈（奖励）和相对反馈（决斗）的随机老虎机问题。提出了两种融合算法，旨在达到仅由两种反馈中较优者决定的遗憾界限。\n\n*   **Shannon invariants: A scalable approach to information decomposition (香农不变量：一种可扩展的信息分解方法)**\n    *   提出基于“香ノン不变量”的新框架来分析分布式系统（如神经网络）中的高阶信息处理。这些不变量仅依赖于熵的定义，可高效计算，有助于解决多元信息度量解释的模糊性，并揭示了不同深度学习架构的信息处理特征。\n\n---\n\n希望这份 TLDR 能帮助你快速了解今日 arXiv 的精华！",
  "papers": [
    {
      "arxiv_id": "2504.16078v1",
      "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities",
      "title_zh": "LLM 是贪婪智能体：RL 微调对决策能力的影响\n",
      "authors": [
        "Thomas Schmied",
        "Jörg Bornschein",
        "Jordi Grau-Moya",
        "Markus Wulfmeier",
        "Razvan Pascanu"
      ],
      "abstract": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
      "tldr_zh": "该研究深入探讨了大型语言模型(LLMs)在决策场景中表现次优的原因，重点关注贪婪性、频率偏差和知行差距这三种常见失效模式。研究提出通过强化学习(RL)在自生成的链式思维(CoT)推理上进行微调，以缓解这些问题。实验结果表明，RL微调通过增加探索和缩小知行差距，显著提升了LLMs在多臂老虎机、上下文老虎机和井字游戏等任务中的决策能力。此外，研究还探索了经典的探索机制（如$\\epsilon$-greedy）和LLM特有的方法（如自我纠正和自我一致性），以实现更有效的LLM决策微调。该研究揭示了LLMs作为“贪婪智能体”的特性，并提供了改进其决策能力的有效策略。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16078v1",
      "published_date": "2025-04-22 17:57:14 UTC",
      "updated_date": "2025-04-22 17:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:12:40.356115"
    },
    {
      "arxiv_id": "2504.16072v1",
      "title": "Describe Anything: Detailed Localized Image and Video Captioning",
      "title_zh": "Describe Anything：详细的局部图像和视频字幕生成\n",
      "authors": [
        "Long Lian",
        "Yifan Ding",
        "Yunhao Ge",
        "Sifei Liu",
        "Hanzi Mao",
        "Boyi Li",
        "Marco Pavone",
        "Ming-Yu Liu",
        "Trevor Darrell",
        "Adam Yala",
        "Yin Cui"
      ],
      "abstract": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
      "tldr_zh": "该论文提出了Describe Anything Model (DAM)，用于生成图像和视频中特定区域的详细局部描述(detailed localized captioning, DLC)。DAM通过焦点提示(focal prompt)和局部视觉骨干网络(localized vision backbone)保留局部细节和全局上下文。为了解决高质量DLC数据稀缺的问题，作者提出了基于半监督学习(SSL)的数据管道(DLC-SDP)，从分割数据集扩展到未标记的web图像。此外，论文还提出了DLC-Bench，一个用于评估DLC的基准，无需参考字幕。实验结果表明，DAM在七个基准测试中达到了最先进水平，涵盖关键词级别、短语级别和详细的多句局部图像和视频字幕。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://describe-anything.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.16072v1",
      "published_date": "2025-04-22 17:51:41 UTC",
      "updated_date": "2025-04-22 17:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:12:52.195459"
    },
    {
      "arxiv_id": "2504.16061v1",
      "title": "Vision language models are unreliable at trivial spatial cognition",
      "title_zh": "视觉语言模型在简单的空间认知方面表现不可靠\n",
      "authors": [
        "Sangeet Khemlani",
        "Tyler Tran",
        "Nathaniel Gyory",
        "Anthony M. Harrison",
        "Wallace E. Lawson",
        "Ravenna Thielstrom",
        "Hunter Thompson",
        "Taaren Singh",
        "J. Gregory Trafton"
      ],
      "abstract": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.",
      "tldr_zh": "该研究评估了视觉语言模型(VLMs)在简单空间认知任务中的可靠性，例如识别物体之间的左右关系。研究者构建了一个名为TableTest的基准数据集，包含在桌面上排列的3D物体场景，并用它来评估最先进的VLMs。实验结果表明，即使是逻辑上等价的提示语的微小变化也会导致性能下降。这表明VLMs在真实场景中进行空间关系推理的能力存在局限性，同时也揭示了增强图像描述语料库以进行更有效训练和测试的新机会。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16061v1",
      "published_date": "2025-04-22 17:38:01 UTC",
      "updated_date": "2025-04-22 17:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:13:04.066034"
    },
    {
      "arxiv_id": "2504.16053v1",
      "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement",
      "title_zh": "LongMamba：通过免训练感受野扩大增强 Mamba 的长上下文能力\n",
      "authors": [
        "Zhifan Ye",
        "Kejing Xia",
        "Yonggan Fu",
        "Xin Dong",
        "Jihoon Hong",
        "Xiangchi Yuan",
        "Shizhe Diao",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Yingyan Celine Lin"
      ],
      "abstract": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba.",
      "tldr_zh": "LongMamba提出了一种无需训练的技术，旨在提升Mamba模型在长文本理解方面的能力。研究发现Mamba模型中的隐藏通道可以根据感受野长度分为局部和全局通道，其中全局通道主要负责长文本理解，但其感受野的自适应扩展能力受限，成为瓶颈。LongMamba通过识别全局通道中的关键token并进行token过滤，抑制隐藏状态的记忆衰减，从而缓解了这一问题。实验结果表明，LongMamba显著提升了Mamba模型在合成和真实长文本场景下的性能，且无需额外训练。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.16053v1",
      "published_date": "2025-04-22 17:30:36 UTC",
      "updated_date": "2025-04-22 17:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:13:16.008334"
    },
    {
      "arxiv_id": "2504.16047v1",
      "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis",
      "title_zh": "评估放射学中的视觉语言模型（VLM）：一项综合分析\n",
      "authors": [
        "Frank Li",
        "Hari Trivedi",
        "Bardia Khosravi",
        "Theo Dapamede",
        "Mohammadreza Chavoshi",
        "Abdulhameed Dere",
        "Rohan Satya Isaac",
        "Aawez Mansuri",
        "Janice Newsome",
        "Saptarshi Purkayastha",
        "Judy Gichoya"
      ],
      "abstract": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.",
      "tldr_zh": "本研究对三种视觉语言模型(VLMs)，RAD-DINO、CheXagent和BiomedCLIP，在放射学任务中的性能进行了全面评估，考察了它们捕捉细粒度图像特征的能力。通过在胸部X光片上进行气胸和心脏肥大的分类、分割和回归任务，研究发现自监督的RAD-DINO在分割任务中表现出色，而文本监督的CheXagent在分类任务中表现更优。集成全局和局部特征的自定义分割模型显著提升了所有基础模型的性能，尤其是在具有挑战性的气胸分割任务中。研究结果表明，预训练方法显著影响模型在特定下游任务中的性能，为根据放射学中的具体临床应用选择基础模型提供了指导。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16047v1",
      "published_date": "2025-04-22 17:20:34 UTC",
      "updated_date": "2025-04-22 17:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:13:28.356994"
    },
    {
      "arxiv_id": "2504.16042v1",
      "title": "Approximate matrices of systems of max-min fuzzy relational equations",
      "title_zh": "极大-极小模糊关系方程组的近似矩阵\n",
      "authors": [
        "Ismaïl Baaj"
      ],
      "abstract": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications.",
      "tldr_zh": "本文研究了如何通过最小程度地修改max-min模糊关系方程组的系数矩阵，使其从不一致状态变为一致状态。该方法旨在获得与原始不一致系统近似的一致系统，保持右侧向量不变，仅对必须修改的矩阵元素进行精确和最小的调整。文章探讨了不一致系统矩阵与具有相同右侧向量的一致系统矩阵集合之间的距离（使用L1、L2或L∞范数），并提出了一种直接计算一致系统矩阵的方法，该方法在L∞范数下与不一致系统矩阵的距离最小。此外，论文还给出了计算最小L∞距离的显式解析公式，并将结果推广到min-max模糊关系方程组，并探讨了一些潜在应用。\n",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16042v1",
      "published_date": "2025-04-22 17:09:02 UTC",
      "updated_date": "2025-04-22 17:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:13:40.366916"
    },
    {
      "arxiv_id": "2504.16041v1",
      "title": "Muon Optimizer Accelerates Grokking",
      "title_zh": "Muon优化器加速Grokking现象\n",
      "authors": [
        "Amund Tveit",
        "Bjørn Remseth",
        "Arve Skogvold"
      ],
      "abstract": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization.",
      "tldr_zh": "该论文研究了优化器对grokking现象的影响，即模型表现出延迟泛化的现象。通过在七个数值任务（主要是模运算）中使用Transformer架构进行实验，系统地改变了优化器(Muon vs. AdamW)和softmax激活函数(standard softmax, stablemax, and sparsemax)，以评估它们对学习动态的综合影响。实验结果表明，Muon优化器通过使用谱范数约束和二阶信息，显著加速了grokking的发生，与AdamW相比，平均grokking epoch从153.09减少到102.89。这表明优化器的选择在促进从记忆到泛化的转变中起着至关重要的作用。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.16041v1",
      "published_date": "2025-04-22 17:08:09 UTC",
      "updated_date": "2025-04-22 17:08:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:13:52.134185"
    },
    {
      "arxiv_id": "2504.16032v1",
      "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management",
      "title_zh": "LLM 邂逅联邦学习：用于可扩展且安全的物联网管理\n",
      "authors": [
        "Yazan Otoum",
        "Arghavan Asad",
        "Amiya Nayak"
      ],
      "abstract": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions.",
      "tldr_zh": "本文提出了一种基于联邦学习的大语言模型(FL-LLM)框架，旨在提升物联网(IoT)系统的智能化水平，同时保障数据隐私和计算效率。该框架结合了生成式物联网(GIoT)模型和梯度感知联邦策略(GSFS)，能够根据实时网络状况动态优化模型更新。通过混合边缘-云处理架构，该方法在分布式IoT环境中实现了智能、可扩展性和安全性的平衡。在IoT-23数据集上的评估表明，该框架提高了模型准确性，降低了响应延迟，并提高了能源效率，优于传统的联邦学习技术(如FedAvg, FedOpt)。该研究表明，将LLM驱动的联邦学习集成到大规模IoT生态系统中具有潜力，为更安全、可扩展和自适应的IoT管理解决方案铺平了道路。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2504.16032v1",
      "published_date": "2025-04-22 16:56:59 UTC",
      "updated_date": "2025-04-22 16:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:14:04.207310"
    },
    {
      "arxiv_id": "2504.16027v1",
      "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3",
      "title_zh": "LLM 代码异味检测基准测试：OpenAI GPT-4.0 vs DeepSeek-V3\n",
      "authors": [
        "Ahmed R. Sadik",
        "Siddhata Govind"
      ],
      "abstract": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection",
      "tldr_zh": "本研究旨在评估大型语言模型(LLMs)在代码异味检测方面的有效性，对比了OpenAI GPT-4.0和DeepSeek-V3两种模型。研究采用包含Java、Python、JavaScript和C++四种编程语言的精心标注数据集，通过精确率(precision)、召回率(recall)和F1值(F1 score)三个指标，在总体、类别和个体代码异味类型三个层面上进行了性能基准测试。此外，还比较了GPT-4.0基于token的检测方法与DeepSeek-V3的模式匹配技术，并进行了成本效益分析，与传统静态分析工具如SonarQube进行了对比。研究结果为从业者选择高效、经济的代码异味自动检测方案提供了有价值的指导。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16027v1",
      "published_date": "2025-04-22 16:44:39 UTC",
      "updated_date": "2025-04-22 16:44:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:14:16.343290"
    },
    {
      "arxiv_id": "2504.16026v1",
      "title": "Trends in AI Supercomputers",
      "title_zh": "AI 超级计算机的发展趋势\n",
      "authors": [
        "Konstantin F. Pilz",
        "James Sanders",
        "Robi Rahman",
        "Lennart Heim"
      ],
      "abstract": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness.",
      "tldr_zh": "该研究构建了一个包含2019年至2025年间500台AI超级计算机的数据集，并分析了其在性能、功耗、硬件成本、所有权和全球分布等方面的关键趋势。研究发现，AI超级计算机的计算性能每九个月翻一番，而硬件购置成本和功耗每年翻一番。xAI的Colossus是2025年3月的领先系统，使用了20万个AI芯片，硬件成本为70亿美元，需要300兆瓦的电力。随着AI超级计算机从科学工具发展为工业机器，公司在AI超级计算机总性能中的份额迅速扩大，而政府和学术界的份额则减少。美国占数据集总性能的75%左右，中国以15%位居第二。如果目前的趋势持续下去，到2030年，领先的AI超级计算机将达到$2\\times10^{22}$ 16-bit FLOP/s，使用200万个AI芯片，硬件成本为2000亿美元，需要9吉瓦的电力。\n",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16026v1",
      "published_date": "2025-04-22 16:44:34 UTC",
      "updated_date": "2025-04-22 16:44:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:14:28.594755"
    },
    {
      "arxiv_id": "2504.16021v1",
      "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support",
      "title_zh": "驾驭认知流状态：用于有效推理支持的上下文感知 AI 干预\n",
      "authors": [
        "Dinithi Dissanayake",
        "Suranga Nanayakkara"
      ],
      "abstract": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion.",
      "tldr_zh": "本文提出了一种上下文感知的认知增强框架，旨在解决AI干预可能破坏认知流状态，从而阻碍决策的问题。该框架通过考虑干预的类型、时机和规模三个关键上下文因素，利用多模态行为线索（如注视行为、打字犹豫、交互速度）动态调整认知支持，以维持或恢复认知流。研究引入了“认知流”的概念，将其定义为AI增强推理中个性化、自适应且最小侵入性的干预。通过从静态干预转向上下文感知的增强，该方法旨在确保AI系统在不干扰认知沉浸的情况下，支持复杂决策和推理中的深度参与。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
      "pdf_url": "http://arxiv.org/pdf/2504.16021v1",
      "published_date": "2025-04-22 16:35:39 UTC",
      "updated_date": "2025-04-22 16:35:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:14:40.239899"
    },
    {
      "arxiv_id": "2504.16020v2",
      "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer",
      "title_zh": "AlphaGrad：非线性梯度归一化优化器\n",
      "authors": [
        "Soham Sane"
      ],
      "abstract": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful.",
      "tldr_zh": "该论文提出了一种新的优化器AlphaGrad，旨在解决Adam等自适应方法中的内存开销和超参数复杂性问题。AlphaGrad通过张量级的L2梯度归一化和双曲正切变换（由陡度参数α控制）来实现尺度不变性。论文在非凸收敛分析中保证了平稳性，并在DQN、TD3、PPO等强化学习基准上进行了广泛的实验评估。结果表明，AlphaGrad在不同RL算法中表现出高度依赖上下文的性能，尤其是在on-policy PPO中表现出显著的优势，但在off-policy DQN中则表现出不稳定性。论文强调了经验性α选择的重要性，并表明AlphaGrad在内存受限的场景中是一个有吸引力的替代优化器，尤其是在on-policy学习机制中，其稳定性和效率优势可能更具影响力。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16020v2",
      "published_date": "2025-04-22 16:33:14 UTC",
      "updated_date": "2025-04-23 01:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:14:52.537309"
    },
    {
      "arxiv_id": "2504.16005v2",
      "title": "CAPO: Cost-Aware Prompt Optimization",
      "title_zh": "CAPO：成本感知型提示优化",
      "authors": [
        "Tom Zehle",
        "Moritz Schlager",
        "Timo Heiß",
        "Matthias Feurer"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
      "tldr_zh": "本文提出了成本敏感的提示优化算法CAPO，旨在提高大型语言模型(LLM)提示优化的效率。CAPO利用AutoML技术，采用进化方法，将LLM作为算子，并结合竞赛机制减少评估次数，以及多目标优化平衡性能和提示长度。该方法同时优化指令和少量样本示例，并利用任务描述提高鲁棒性。实验结果表明，CAPO在多个数据集和LLM上优于现有的离散提示优化方法，在15个案例中的11个案例中表现更优，最高提升达21%。CAPO通过竞赛节省评估，并通过长度惩罚减少平均提示长度，从而实现了成本效益和成本敏感性，即使没有少量样本示例，CAPO也优于竞争对手，并且通常对初始提示保持稳健。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to AutoML 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.16005v2",
      "published_date": "2025-04-22 16:14:31 UTC",
      "updated_date": "2025-04-23 09:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:15:04.472131"
    },
    {
      "arxiv_id": "2504.16000v1",
      "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning",
      "title_zh": "你的注意力有多私密？构建隐私与上下文学习的桥梁\n",
      "authors": [
        "Soham Bonnerjee",
        "Zhen Wei",
        "Yeon",
        "Anna Asch",
        "Sagnik Nandy",
        "Promit Ghosal"
      ],
      "abstract": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.",
      "tldr_zh": "本文研究了上下文学习(ICL)在隐私保护下的可行性，提出了一种针对线性注意力头的差分隐私预训练算法。针对线性回归中的ICL，论文首次从理论上分析了隐私-准确率之间的权衡关系，揭示了优化和隐私噪声之间的根本矛盾。研究表明，该方法对训练提示的对抗性扰动具有鲁棒性，优于标准岭回归。理论结果通过大量模拟实验得到验证。\n",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16000v1",
      "published_date": "2025-04-22 16:05:26 UTC",
      "updated_date": "2025-04-22 16:05:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:15:16.060015"
    },
    {
      "arxiv_id": "2504.15995v1",
      "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning",
      "title_zh": "OPUS-VFL：激励垂直联邦学习中最佳隐私-效用权衡\n",
      "authors": [
        "Sindhuja Madabushi",
        "Ahmad Faraz Khan",
        "Haider Ali",
        "Jin-Hee Cho"
      ],
      "abstract": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.",
      "tldr_zh": "OPUS-VFL 提出了一种用于垂直联邦学习 (VFL) 的最优隐私-效用权衡策略，旨在解决现有 VFL 系统缺乏有效激励机制、难以平衡隐私-效用以及无法适应异构资源客户端的问题。OPUS-VFL 引入了一种新颖的、隐私感知的激励机制，该机制基于模型贡献、隐私保护和资源投入的组合来奖励客户端。该机制采用轻量级的留一法 (LOO) 策略来量化每个客户端的特征重要性，并集成了自适应差分隐私机制，使客户端能够动态校准噪声水平以优化其效用。实验表明，OPUS-VFL 在效率和鲁棒性方面均优于最先进的 VFL 基线，可降低标签推理攻击成功率高达 20%，并将特征推理重建误差 (MSE) 提高 30% 以上。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15995v1",
      "published_date": "2025-04-22 16:00:11 UTC",
      "updated_date": "2025-04-22 16:00:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:15:28.452152"
    },
    {
      "arxiv_id": "2504.15983v1",
      "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models",
      "title_zh": "基于 W-PCA 的无梯度代理，用于高效搜索轻量级语言模型\n",
      "authors": [
        "Shang Wang"
      ],
      "abstract": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.",
      "tldr_zh": "本文提出了一种基于权重加权PCA (W-PCA) 的新型零样本NAS方法，专门用于轻量级语言模型的高效搜索。该方法利用参数量和前馈神经网络(FFN)层中累积贡献超过阈值η的主成分数量作为评估代理，无需梯度计算，从而优化了评估时间。在GLUE和SQuAD数据集上的实验表明，W-PCA相比one-shot NAS方法显著减少了训练时间，并且在测试阶段取得了比以往基于训练的SOTA方法更高的分数。此外，在FlexiBERT搜索空间采样的数据集上的排名评估表明，W-PCA相比其他需要梯度计算的零样本NAS方法，展现出更优越的排名相关性并进一步减少了求解时间。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15983v1",
      "published_date": "2025-04-22 15:33:01 UTC",
      "updated_date": "2025-04-22 15:33:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:15:40.293640"
    },
    {
      "arxiv_id": "2504.15972v1",
      "title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling",
      "title_zh": "通过情感分析和 BERT 主题建模预测大型开源软件仓库中的 Bug 归宿\n",
      "authors": [
        "Sophie C. Pope",
        "Andrew Barovic",
        "Armin Moin"
      ],
      "abstract": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories.",
      "tldr_zh": "该研究提出了一种新方法，利用情感分析和BERT主题建模来预测大型开源软件仓库中bug的关键结果，包括解决时间、修复时间和最终状态。该方法提取bug报告的情感得分和情感分类，并结合bug的优先级和BERTopic模型提取的主题，作为卷积神经网络(CNN)和多层感知器(MLP)的输入特征。实验结果表明，结合BERTopic和情感分析可以提高模型性能，情感分析是预测bug最终结果（特别是是否会被修复）的有效指标。研究使用Bugzilla Eclipse Project的数据进行验证，并比较了二元分类和精确时间值预测的有效性。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15972v1",
      "published_date": "2025-04-22 15:18:14 UTC",
      "updated_date": "2025-04-22 15:18:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:15:52.241554"
    },
    {
      "arxiv_id": "2504.15956v1",
      "title": "Universal Approximation with Softmax Attention",
      "title_zh": "Softmax 注意力的通用逼近\n",
      "authors": [
        "Jerry Yao-Chieh Hu",
        "Hude Liu",
        "Hong-Yu Chen",
        "Weimin Wu",
        "Han Liu"
      ],
      "abstract": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest.",
      "tldr_zh": "该论文证明了仅使用线性变换，(i) 两层自注意力机制和 (ii) 一层自注意力机制后接 softmax 函数，都可以作为紧凑域上连续序列到序列函数的通用逼近器。核心技术是一种新的基于插值的分析注意力内部机制的方法，揭示了自注意力机制能够以任意精度逼近广义 ReLU 函数，从而涵盖许多已知的通用逼近器。研究表明，仅使用两层多头注意力机制就足以作为序列到序列的通用逼近器，而之前的研究依赖于前馈网络来实现 Transformer 的通用逼近。此外，论文还证明了仅使用 (softmax-)attention 的层能够近似各种上下文统计模型。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15956v1",
      "published_date": "2025-04-22 14:51:33 UTC",
      "updated_date": "2025-04-22 14:51:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:16:04.342198"
    },
    {
      "arxiv_id": "2504.15941v1",
      "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity",
      "title_zh": "FairTranslate：一个克服性别二元对立的英法数据集，用于评估机器翻译中的性别偏见\n",
      "authors": [
        "Fanny Jourdan",
        "Yannick Chevalier",
        "Cécile Favre"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
      "tldr_zh": "该论文提出了FairTranslate，一个全新的、完全由人工标注的英法数据集，用于评估机器翻译系统中存在的非二元性别偏见。FairTranslate包含2418个与职业相关的英法语句对，并带有丰富的元数据，例如职业的刻板印象对齐、语法性别指标的模糊性以及真实的性别标签（男性、女性或包容性）。研究人员使用FairTranslate评估了四种主流LLM（Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B）在不同提示程序下的表现。结果表明，这些LLM在性别表达方面存在显著的偏见，揭示了在机器翻译中实现公平结果的持续挑战。FairTranslate数据集已在Hugging Face上公开，实验代码也已在GitHub上发布。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "FAccT 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15941v1",
      "published_date": "2025-04-22 14:35:16 UTC",
      "updated_date": "2025-04-22 14:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:16:16.375318"
    },
    {
      "arxiv_id": "2504.15929v1",
      "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models",
      "title_zh": "Meta-Entity驱动的三元组挖掘用于对齐医学视觉-语言模型\n",
      "authors": [
        "Saban Ozturk",
        "Melih B. Yilmaz",
        "Muti Kara",
        "M. Talat Yavuz",
        "Aykut Koç",
        "Tolga Çukur"
      ],
      "abstract": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.",
      "tldr_zh": "该论文提出了一种名为MedTrim的元实体驱动的三元组挖掘方法，旨在提升医学视觉-语言模型(med-VLMs)中图像和文本表示的对齐效果，尤其是在胸部X光(CXR)评估中。MedTrim通过结合疾病类别以及形容词和方向性病理描述符，利用多模态三元组学习来增强图像-文本对齐。该方法首先引入一个基于本体的实体识别模块，从CXR报告中提取病理特异性的元实体，然后设计了一种新的评分函数，用于在三元组挖掘中进行精细的样本选择，最终通过多模态三元组对齐目标实现样本间病理特征的对齐。实验结果表明，MedTrim在下游检索和分类任务中优于现有的对齐方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 7 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.15929v1",
      "published_date": "2025-04-22 14:17:51 UTC",
      "updated_date": "2025-04-22 14:17:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:16:28.406088"
    },
    {
      "arxiv_id": "2504.15928v1",
      "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers",
      "title_zh": "一个对临床医生友好的眼科图像分析平台，无技术障碍",
      "authors": [
        "Meng Wang",
        "Tian Lin",
        "Qingshan Hou",
        "Aidi Lin",
        "Jingcheng Wang",
        "Qingsheng Peng",
        "Truong X. Nguyen",
        "Danqi Fang",
        "Ke Zou",
        "Ting Xu",
        "Cancan Xue",
        "Ten Cheer Quek",
        "Qinkai Yu",
        "Minxin Liu",
        "Hui Zhou",
        "Zixuan Xiao",
        "Guiqin He",
        "Huiyu Liang",
        "Tingkun Shi",
        "Man Chen",
        "Linna Liu",
        "Yuanyuan Peng",
        "Lianyu Wang",
        "Qiuming Hu",
        "Junhong Chen",
        "Zhenhua Zhang",
        "Cheng Chen",
        "Yitian Zhao",
        "Dianbo Liu",
        "Jianhua Wu",
        "Xinjian Chen",
        "Changqing Zhang",
        "Triet Thanh Nguyen",
        "Yanda Meng",
        "Yalin Zheng",
        "Yih Chung Tham",
        "Carol Y. Cheung",
        "Huazhu Fu",
        "Haoyu Chen",
        "Ching-Yu Cheng"
      ],
      "abstract": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers.",
      "tldr_zh": "该论文介绍了一个名为 GlobeReady 的临床友好型AI平台，旨在消除眼科图像分析的技术壁垒。GlobeReady无需针对不同临床中心进行模型再训练或微调，即可实现对眼科疾病的诊断。通过训练自由的局部特征增强技术，该平台有效解决了跨中心和人群的领域迁移问题。实验结果表明，GlobeReady 在多个国家的临床中心都达到了较高的诊断准确率，并且内置的置信度量化诊断方法进一步提升了诊断准确率。临床医生对 GlobeReady 的可用性和临床相关性给予了高度评价，表明该平台具有强大的、可扩展的诊断能力，并有潜力支持眼科护理。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15928v1",
      "published_date": "2025-04-22 14:17:22 UTC",
      "updated_date": "2025-04-22 14:17:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:16:40.385057"
    },
    {
      "arxiv_id": "2504.15927v1",
      "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics",
      "title_zh": "半监督社区检测的新方法：结晶动力学下的集团退火\n",
      "authors": [
        "Ling Cheng",
        "Jiashu Pu",
        "Ruicheng Liang",
        "Qian Shao",
        "Hezhe Qiao",
        "Feida Zhu"
      ],
      "abstract": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection.",
      "tldr_zh": "该论文提出了一种新的半监督社区检测方法CLique ANNealing (CLANN)，该方法受到结晶动力学的启发，将社区检测类比于晶体亚晶粒（核心）扩展为完整晶粒（社区）的退火过程。CLANN通过将动力学概念融入优化过程来增强社区核心的一致性。此外，采用了一种无需学习的传递退火器来细化第一阶段的候选对象，通过合并相邻的“clique”和重新定位社区核心，实现自发增长过程，从而提高了可扩展性。在43种不同的网络设置下进行的大量实验表明，CLANN在多个真实世界数据集上优于最先进的方法，展示了其在社区检测中的卓越功效和效率。\n",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "arXiv admin note: text overlap with arXiv:2203.05898 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2504.15927v1",
      "published_date": "2025-04-22 14:17:15 UTC",
      "updated_date": "2025-04-22 14:17:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:16:52.400398"
    },
    {
      "arxiv_id": "2504.15924v1",
      "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification",
      "title_zh": "通过不确定性量化在联邦学习中实现分配正义\n",
      "authors": [
        "Alycia Carey",
        "Xintao Wu"
      ],
      "abstract": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL.",
      "tldr_zh": "这篇论文提出了UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning)，一个灵活的联邦学习框架，旨在实现基于分配正义的多种客户端公平性指标。UDJ-FL结合了公平资源分配技术和基于不确定性的客户端权重调整，能够实现平均主义、功利主义、罗尔斯差异原则和基于贡献的客户端公平性。实验结果表明，UDJ-FL在实现这四种分配正义公平性指标方面表现出色，并且公平性与其他流行的公平联邦学习方法相当甚至更好。论文还论证了采用不确定性加权的必要性，并推导了UDJ-FL的泛化界限理论保证。代码已开源。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "68T01",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 1 figure, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.15924v1",
      "published_date": "2025-04-22 14:07:56 UTC",
      "updated_date": "2025-04-22 14:07:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:17:04.389934"
    },
    {
      "arxiv_id": "2504.15918v2",
      "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions",
      "title_zh": "Ask2Loc：通过提问学习定位教学视频中的视觉答案\n",
      "authors": [
        "Chang Zong",
        "Bin Li",
        "Shoujun Zhou",
        "Jian Wan",
        "Lei Zhang"
      ],
      "abstract": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc.",
      "tldr_zh": "该论文提出了一个名为In-VAL的新任务，旨在模拟用户在教学视频中通过多次交互来定位视觉答案的过程。为了解决In-VAL任务中存在的用户意图模糊、字幕不完整和内容碎片化等问题，作者提出了Ask2Loc框架，该框架包含聊天模块（用于细化问题）、重写模块（用于生成完整描述）和搜索模块（用于提供集成内容）。实验结果表明，Ask2Loc在三个重建的In-VAL数据集上，相比传统方法，性能提升高达14.91 (mIoU)。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "68T45, 68T20"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15918v2",
      "published_date": "2025-04-22 14:03:16 UTC",
      "updated_date": "2025-04-23 03:01:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:17:16.216944"
    },
    {
      "arxiv_id": "2504.15912v1",
      "title": "Automated Bug Report Prioritization in Large Open-Source Projects",
      "title_zh": "大型开源项目中自动化的错误报告优先级排序\n",
      "authors": [
        "Riley Pierson",
        "Armin Moin"
      ],
      "abstract": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction.",
      "tldr_zh": "本文提出了一种基于自然语言处理的自动化缺陷报告优先级排序方法，用于解决大型开源项目中大量缺陷报告难以处理的问题。该方法结合了TopicMiner-MTM主题建模和BERT大型语言模型文本分类，旨在提升缺陷报告优先级预测的性能。在包含85,156个Eclipse Platform项目缺陷报告的基准数据集上的实验结果表明，该方法在准确率(Accuracy)、精确率(Precision)、召回率(Recall)和F1值(F1-measure)等方面均优于现有方法。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15912v1",
      "published_date": "2025-04-22 13:57:48 UTC",
      "updated_date": "2025-04-22 13:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:17:28.147615"
    },
    {
      "arxiv_id": "2504.15905v1",
      "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network",
      "title_zh": "GraphEdge：边缘网络中用于 GNN 计算的动态图划分和任务调度\n",
      "authors": [
        "Wenjing Xiao",
        "Chenglong Shi",
        "Miaojiang Chen",
        "Zhiquan Liu",
        "Min Chen",
        "H. Herbert Song"
      ],
      "abstract": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios.",
      "tldr_zh": "本文提出GraphEdge，一种高效的基于图神经网络(GNN)的边缘计算(EC)架构，旨在解决物联网(IoT)设备激增背景下，GNN在边缘网络中计算时服务器通信成本高昂的问题。GraphEdge首先感知用户拓扑，并将数据关联表示为图布局。然后，通过分层遍历图切割算法(HiCut)优化图布局，将图切割成弱关联的子图，从而最小化GNN推理期间不同子图之间的通信成本。最后，基于优化的图布局，执行基于深度强化学习(DRL)的图卸载算法(DRLGO)，获得最优的子图级卸载策略，尽可能将子图中的用户任务卸载到同一边缘服务器，同时最小化任务处理时间和EC系统的能耗。实验结果表明，GraphEdge具有良好的有效性和动态适应性，即使在动态场景下也表现良好。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages,12 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15905v1",
      "published_date": "2025-04-22 13:45:13 UTC",
      "updated_date": "2025-04-22 13:45:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:17:40.701232"
    },
    {
      "arxiv_id": "2504.15903v1",
      "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations",
      "title_zh": "噪声对 LLM 模型在抽象和推理语料库 (ARC) 任务中性能的影响，以及模型温度的考量\n",
      "authors": [
        "Nikhil Khandalkar",
        "Pavan Yadav",
        "Krishna Shinde",
        "Lokesh B. Ramegowda",
        "Rajarshi Das"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.",
      "tldr_zh": "该研究探讨了噪声对大型语言模型(LLMs)在抽象和推理语料库(ARC)任务中性能的影响，并考虑了模型温度设置。实验表明，在零噪声条件下，GPT-4o能够解决所有ARC任务，而DeepSeek R1和LLaMA 3.2则无法解决任何任务。研究发现，引入噪声会持续降低所有模型的性能，突显了当前LLMs对输入扰动的敏感性。这一脆弱性引发了对LLMs在现实世界应用中鲁棒性的担忧。通过比较不同模型架构对噪声的反应，该研究揭示了现代LLMs在推理任务中的结构性弱点，并强调了开发更鲁棒和适应性强的AI系统的必要性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "60 pages, 25 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15903v1",
      "published_date": "2025-04-22 13:43:58 UTC",
      "updated_date": "2025-04-22 13:43:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:17:52.309373"
    },
    {
      "arxiv_id": "2504.15895v1",
      "title": "Dynamic Early Exit in Reasoning Models",
      "title_zh": "推理模型中的动态提前退出\n",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Yongjie Duan",
        "Zheliang Zhu",
        "Chenyu Zhu",
        "Zheng Lin",
        "Li Cao",
        "Weiping Wang"
      ],
      "abstract": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.",
      "tldr_zh": "该论文提出了一种动态提前退出(Dynamic Early Exit)的方法，用于优化大型推理语言模型(LRLMs)中的链式思维(CoT)生成过程。该方法通过监控模型在推理转换点（如\"Wait\" tokens）的行为，并在模型对试探性答案表现出高置信度时，动态终止后续推理链的生成，从而避免过度思考。该方法无需额外训练，可直接集成到现有的推理LLMs中。在MATH-500、AMC 2023、GPQA Diamond和AIME 2024等多个推理基准测试中，该方法能够平均减少31%到43%的CoT序列长度，同时将准确率提高1.7%到5.7%。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15895v1",
      "published_date": "2025-04-22 13:36:53 UTC",
      "updated_date": "2025-04-22 13:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:18:04.500917"
    },
    {
      "arxiv_id": "2504.15894v1",
      "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making",
      "title_zh": "支持人工智能辅助决策中的数据框架动态性\n",
      "authors": [
        "Chengbo Zheng",
        "Tim Miller",
        "Alina Bialkowski",
        "H Peter Soyer",
        "Monika Janda"
      ],
      "abstract": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses.",
      "tldr_zh": "本文提出了一种混合主动的AI辅助决策框架，该框架基于数据框理论和评估性AI范式，旨在支持高风险决策中不断演变的证据和假设之间的动态关系。该框架允许人类和AI协同构建、验证和调整假设。作者使用一个AI辅助的皮肤癌诊断原型来演示该框架，该原型利用概念瓶颈模型(concept bottleneck model)来促进可解释的交互和诊断假设的动态更新。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
      "pdf_url": "http://arxiv.org/pdf/2504.15894v1",
      "published_date": "2025-04-22 13:36:06 UTC",
      "updated_date": "2025-04-22 13:36:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:18:16.249227"
    },
    {
      "arxiv_id": "2504.15883v1",
      "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading",
      "title_zh": "整合非线性 Radon 变换用于糖尿病视网膜病变分级\n",
      "authors": [
        "Farida Mohsen",
        "Samir Belhaouari",
        "Zubair Shah"
      ],
      "abstract": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis.",
      "tldr_zh": "该研究提出了一种名为RadFuse的多表示深度学习框架，用于糖尿病视网膜病变(Diabetic Retinopathy)的分级。RadFuse集成了非线性RadEx变换的sinogram图像和传统的眼底图像，以增强病变检测和分级。RadEx变换是Radon变换的优化非线性扩展，用于捕捉复杂的视网膜病变模式。实验结果表明，RadFuse在APTOS-2019和DDR两个基准数据集上，使用ResNeXt-50、MobileNetV2和VGG19三种CNN架构，均优于仅使用眼底图像的模型，并超越了现有最佳方法。RadFuse在五阶段严重程度分级中实现了93.24%的quadratic weighted kappa，在健康和糖尿病视网膜病变病例的二元分类中达到了99.09%的准确率。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15883v1",
      "published_date": "2025-04-22 13:27:28 UTC",
      "updated_date": "2025-04-22 13:27:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:18:28.644217"
    },
    {
      "arxiv_id": "2504.15876v2",
      "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation",
      "title_zh": "基于分层强化学习的战略对抗双向任务-运动规划\n",
      "authors": [
        "Qizhen Wu",
        "Lei Chen",
        "Kexin Liu",
        "Jinhu Lü"
      ],
      "abstract": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method.",
      "tldr_zh": "本文提出了一种基于分层强化学习的双向任务-运动规划方法，用于解决群体机器人对抗场景中的决策问题。该方法打破了传统任务和运动规划的单向结构，实现了离散指令和连续动作之间的动态交互。通过跨层训练技术，该方法能够有效地将指令映射到任务分配，并将动作映射到路径规划。此外，引入轨迹预测模型来连接抽象任务表示和可执行的规划目标。实验结果表明，该方法在对抗获胜率上超过80%，决策时间低于0.01秒，优于现有方法，并在大规模测试和真实机器人实验中验证了其泛化能力和实际应用性。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15876v2",
      "published_date": "2025-04-22 13:22:58 UTC",
      "updated_date": "2025-04-23 15:00:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:18:40.410394"
    },
    {
      "arxiv_id": "2504.15865v2",
      "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search",
      "title_zh": "MedNNS：基于超网络的医学任务自适应神经网络搜索\n",
      "authors": [
        "Lotfi Abdelkrim Mecharbat",
        "Ibrahim Almakky",
        "Martin Takac",
        "Mohammad Yaqub"
      ],
      "abstract": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.",
      "tldr_zh": "MedNNS 是一种针对医学图像应用的神经架构搜索(NAS)框架，旨在解决医学任务中模型架构选择和权重初始化的问题。该框架通过构建一个基于Supernetwork的元空间，联合优化架构选择和权重初始化，该元空间基于数据集和模型之间的性能表现进行编码。MedNNS引入了排序损失和Fréchet Inception Distance (FID)损失，以捕获模型间和数据集间的关系，从而实现更准确的元空间对齐。实验结果表明，MedNNS 在多个数据集上显著优于 ImageNet 预训练的 DL 模型和 SOTA NAS 方法，平均准确率提高了 1.7%，并且收敛速度更快。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15865v2",
      "published_date": "2025-04-22 13:04:40 UTC",
      "updated_date": "2025-04-23 05:28:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:18:52.386751"
    },
    {
      "arxiv_id": "2504.15847v1",
      "title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters",
      "title_zh": "CARE：面向具有预算请求者的联邦学习的兼容性感知激励机制\n",
      "authors": [
        "Xiang Liu",
        "Hau Chan",
        "Minming Li",
        "Xianlong Zeng",
        "Chenchen Fu",
        "Weiwei Wu"
      ],
      "abstract": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines.",
      "tldr_zh": "本文研究了联邦学习(FL)中，多个有预算限制的请求者(requester)向具有不兼容特征(incompatibility characteristics)的工人(worker)寻求训练服务的场景。针对工人不兼容导致FL效率下降的问题，提出了两种兼容性感知激励机制(compatibility-aware incentive mechanisms)：CARE-CO (合作预算)和CARE-NO (非合作预算)。这两种机制旨在引出工人的真实私有成本，确定雇佣哪些工人，以及如何在满足请求者预算约束的同时，最大化请求者的效用。理论分析表明，提出的机制保证了个体理性、真实性、预算可行性和近似性能。实验结果表明，在真实数据集上，提出的机制显著优于现有基线方法。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15847v1",
      "published_date": "2025-04-22 12:42:45 UTC",
      "updated_date": "2025-04-22 12:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:19:04.544326"
    },
    {
      "arxiv_id": "2504.15829v1",
      "title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases",
      "title_zh": "用于研究数据处理的生成式人工智能：从三个用例中获得的经验教训\n",
      "authors": [
        "Modhurita Mitra",
        "Martine G. de Vos",
        "Nicola Cortinovis",
        "Dawa Ometto"
      ],
      "abstract": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained.",
      "tldr_zh": "本文探讨了生成式AI在研究数据处理中的应用，并分享了三个案例研究的经验教训。研究人员利用Claude 3 Opus模型，分别完成了以下任务：1) 从历史种子名录中提取植物物种名称；2) 从欧盟健康技术评估机构发布的文件中提取关键数据点（如药物名称、适应症、有效性等）；3) 对Kickstarter众筹网站上的项目进行行业代码分类。研究强调了如何判断生成式AI是否适用于特定数据处理任务，以及如何最大化结果的准确性和一致性。该研究表明，生成式AI在传统方法难以应用的复杂数据处理任务中具有可行性。\n",
      "categories": [
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures, 6 tables. Published in Proceedings of the 2024\n  IEEE 20th International Conference on e-Science (e-Science), Osaka, Japan",
      "pdf_url": "http://arxiv.org/pdf/2504.15829v1",
      "published_date": "2025-04-22 12:21:07 UTC",
      "updated_date": "2025-04-22 12:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:19:16.450425"
    },
    {
      "arxiv_id": "2504.15827v1",
      "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers",
      "title_zh": "DualOptim：利用双重优化器增强机器遗忘的有效性和稳定性\n",
      "authors": [
        "Xuyang Zhong",
        "Haochen Luo",
        "Chen Liu"
      ],
      "abstract": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.",
      "tldr_zh": "现有的机器学习遗忘(Machine Unlearning, MU)方法对超参数非常敏感，需要精细调整，限制了实际应用。该论文首先通过实验证明了现有MU方法在不同场景下的不稳定性和次优性能。为了解决这个问题，论文提出了Dual Optimizer (DualOptim)，它结合了自适应学习率和解耦的动量因子。实验和理论证据表明，DualOptim有助于有效和稳定的遗忘。大量实验表明，DualOptim可以显著提高MU在图像分类、图像生成和大型语言模型等不同任务中的有效性和稳定性，使其成为一种通用的增强现有MU算法的方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15827v1",
      "published_date": "2025-04-22 12:18:26 UTC",
      "updated_date": "2025-04-22 12:18:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:19:28.271924"
    },
    {
      "arxiv_id": "2504.15823v1",
      "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models",
      "title_zh": "人眼难以察觉的针对近红外人脸识别模型的物理对抗攻击\n",
      "authors": [
        "Songyan Xie",
        "Jinghang Wen",
        "Encheng Su",
        "Qiucheng Yu"
      ],
      "abstract": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.",
      "tldr_zh": "该研究提出了一种针对近红外(NIR)人脸识别系统的、人眼难以察觉的物理对抗攻击方法。通过使用红外吸收墨水生成多个经过数字化优化形状和位置的对抗补丁，并在黑盒设置下进行攻击。为了解决数字图像和真实NIR成像之间的优化差异，开发了一种皮肤光反射模型，以模拟NIR光反射，从而最小化像素级别的差异。实验结果表明，该方法在数字和物理领域均提高了攻击成功率，尤其是在不同面部姿势下保持了有效性，在物理领域达到了82.46%的平均攻击成功率，优于现有方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15823v1",
      "published_date": "2025-04-22 12:10:25 UTC",
      "updated_date": "2025-04-22 12:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:19:40.557465"
    },
    {
      "arxiv_id": "2504.15812v1",
      "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits",
      "title_zh": "融合随机Bandit中的奖励和对决反馈\n",
      "authors": [
        "Xuchuang Wang",
        "Qirun Zeng",
        "Jinhang Zuo",
        "Xutong Liu",
        "Mohammad Hajiesmaili",
        "John C. S. Lui",
        "Adam Wierman"
      ],
      "abstract": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results.",
      "tldr_zh": "本文研究了随机bandit算法中绝对反馈（奖励）和相对反馈（dueling）的融合问题，即在每个决策轮次中同时收集两种反馈。研究推导了regret下界，表明高效算法可能仅会产生奖励或基于dueling的regret中较小的值。提出了两种融合方法：（1）一种简单的消除融合算法，利用两种反馈来探索所有臂，并通过共享一个共同的候选臂集合来统一收集的信息；（2）一种分解融合算法，选择更有效的反馈来探索相应的臂，并在每一轮中随机分配一种反馈类型用于探索，另一种用于利用。由于dueling消除的内在次优性，消除融合算法在regret中会遇到一个次优的臂数量乘法项。相比之下，在常见假设下，分解融合算法实现了与下界匹配的regret（相差一个常数）。大量实验证实了算法的有效性和理论结果。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15812v1",
      "published_date": "2025-04-22 11:51:20 UTC",
      "updated_date": "2025-04-22 11:51:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:19:52.830620"
    },
    {
      "arxiv_id": "2504.15806v2",
      "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations",
      "title_zh": "DAE-KAN：用于高指标微分代数方程的 Kolmogorov-Arnold 网络模型\n",
      "authors": [
        "Kai Luo",
        "Juan Tang",
        "Mingchao Cai",
        "Xiaoqing Zeng",
        "Manqi Xie",
        "Ming Yan"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations.",
      "tldr_zh": "该论文提出了一种名为DAE-KAN的新框架，用于求解高指标微分代数方程(DAEs)。DAE-KAN通过将Kolmogorov-Arnold Networks (KANs)与Physics-Informed Neural Networks (PINNs)相结合，利用KANs强大的函数拟合能力增强了PINNs的性能。数值实验表明，对于指标1到指标3的DAE系统，DAE-KAN相比传统PINNs，能够将微分变量和代数变量的绝对误差降低1到2个数量级。通过分析漂移误差，证明DAE-KAN在控制漂移现象方面优于传统数值方法。研究结果表明，DAE-KAN在解决高指标DAEs方面具有显著的计算精度和泛化能力，为解决具有挑战性的偏微分代数方程提供了一种有前景的方案。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15806v2",
      "published_date": "2025-04-22 11:42:02 UTC",
      "updated_date": "2025-04-23 06:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:20:04.598654"
    },
    {
      "arxiv_id": "2504.15804v1",
      "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback",
      "title_zh": "来自验证的洞见：利用测试平台反馈，通过强化学习训练 Verilog 生成 LLM\n",
      "authors": [
        "Ning Wang",
        "Bingkun Yao",
        "Jie Zhou",
        "Yuchen Hu",
        "Xi Wang",
        "Nan Guan",
        "Zhe Jiang"
      ],
      "abstract": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.",
      "tldr_zh": "该论文提出了一种利用验证反馈训练Verilog代码生成LLM的方法，旨在提高生成代码的功能正确性。通过构建自动测试平台生成流程，并利用Verilog编译器模拟器(VCS)的反馈来减少幻觉并确保正确性，解决了功能验证数据不足的问题。该方法采用强化学习(RL)，特别是直接偏好优化(DPO)，基于测试平台的结果训练偏好对，使Verilog代码生成与功能正确性对齐。实验结果表明，该方法在多个Verilog评估基准上优于现有技术，能够生成功能正确的Verilog代码。\n",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15804v1",
      "published_date": "2025-04-22 11:38:14 UTC",
      "updated_date": "2025-04-22 11:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:20:16.355375"
    },
    {
      "arxiv_id": "2504.15801v1",
      "title": "A closer look at how large language models trust humans: patterns and biases",
      "title_zh": "更深入地了解大型语言模型如何信任人类：模式与偏见\n",
      "authors": [
        "Valeria Lerman",
        "Yaniv Dover"
      ],
      "abstract": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.",
      "tldr_zh": "该研究深入探讨了大型语言模型(LLMs)如何信任人类，着重分析了LLMs在决策过程中对人类信任的模式和偏差。通过模拟43,200个实验，针对五个流行的语言模型和五个不同的场景，研究发现LLMs的信任发展与人类的信任发展具有总体相似性，主要受人类的胜任力(competence)、仁慈(benevolence)和正直(integrity)这三个主要的可信度维度影响。同时，研究也发现LLMs的信任在某些情况下会受到年龄、宗教和性别等人口统计变量的偏见，尤其是在金融场景中。不同模型在评估信任方面存在差异，某些情况下，可信度和人口统计因素对信任的预测能力较弱。该研究强调需要更好地理解AI对人类的信任动态，并监测偏差和信任发展模式，以防止在AI的信任敏感应用中产生意想不到的潜在有害结果。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15801v1",
      "published_date": "2025-04-22 11:31:50 UTC",
      "updated_date": "2025-04-22 11:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:20:28.867527"
    },
    {
      "arxiv_id": "2504.15791v1",
      "title": "Crisp complexity of fuzzy classifiers",
      "title_zh": "模糊分类器的清晰复杂度\n",
      "authors": [
        "Raquel Fernandez-Peralta",
        "Javier Fumanal-Idocin",
        "Javier Andreu-Perez"
      ],
      "abstract": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like.",
      "tldr_zh": "该论文提出了一种将模糊规则分类器简化为清晰规则分类器的方法，旨在提高模糊规则系统在模糊领域外的应用。研究探讨了不同的清晰描述，并设计了一种算法来实现这种转换。通过分析生成的清晰分类器的复杂性，该研究旨在帮助模糊和非模糊领域的从业者更好地理解模糊规则库划分特征空间的方式，以及系统间的相互转换。提出的复杂性度量还可以帮助根据等效的清晰划分来选择不同的模糊分类器。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15791v1",
      "published_date": "2025-04-22 11:06:25 UTC",
      "updated_date": "2025-04-22 11:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:20:40.411837"
    },
    {
      "arxiv_id": "2504.15785v1",
      "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents",
      "title_zh": "WALL-E 2.0：通过神经符号学习进行世界对齐，改进基于世界模型的 LLM 智能体\n",
      "authors": [
        "Siyu Zhou",
        "Tianyi Zhou",
        "Yijun Yang",
        "Guodong Long",
        "Deheng Ye",
        "Jing Jiang",
        "Chengqi Zhang"
      ],
      "abstract": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
      "tldr_zh": "该论文提出了一个名为WALL-E 2.0的免训练“世界对齐”方法，旨在通过神经符号学习改进基于世界模型的LLM智能体。该方法通过LLM从探索轨迹中提取动作规则、知识图谱和场景图等符号知识，并将其编码为可执行代码来规范LLM智能体的策略，从而弥合LLM的先验知识与特定环境动态之间的差距。WALL-E 2.0采用模型预测控制(MPC)框架，并利用LLM智能体作为高效的未来步骤动作前瞻优化器，与神经符号世界模型交互。实验结果表明，在Mars和ALFWorld等开放世界挑战中，WALL-E 2.0显著优于现有方法，在ALFWorld中仅经过4次迭代就达到了98%的成功率。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code is available at https://github.com/elated-sawyer/WALL-E",
      "pdf_url": "http://arxiv.org/pdf/2504.15785v1",
      "published_date": "2025-04-22 10:58:27 UTC",
      "updated_date": "2025-04-22 10:58:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:20:52.707623"
    },
    {
      "arxiv_id": "2504.15784v1",
      "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach",
      "title_zh": "大型语言模型的自动化创造力评估：一种基于参考的方法\n",
      "authors": [
        "Ruizhe Li",
        "Chiwei Zhu",
        "Benfeng Xu",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "abstract": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).",
      "tldr_zh": "本文提出了一种自动化的创造力评估方法，用于评估大型语言模型(LLMs)生成的文本的创造性。该方法基于Torrance创造性写作测试(TTCW)，将创造力视为一种产出，并采用基于参考文本的Likert量表方法，通过与高质量参考文本比较来评估生成文本的创造性。实验结果表明，该方法显著提高了LLM评估与人类评估的一致性，成对准确率达到0.75 (+15%)。该方法为LLMs在文学、故事创作等领域的应用提供了一种有效的评估手段。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15784v1",
      "published_date": "2025-04-22 10:52:23 UTC",
      "updated_date": "2025-04-22 10:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:21:04.429785"
    },
    {
      "arxiv_id": "2504.15780v1",
      "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving",
      "title_zh": "TrustGeoGen：用于可信多模态几何问题求解的可扩展且经过形式验证的数据引擎\n",
      "authors": [
        "Daocheng Fu",
        "Zijun Chen",
        "Renqiu Xia",
        "Qi Liu",
        "Yuan Feng",
        "Hongbin Zhou",
        "Renrui Zhang",
        "Shiyang Feng",
        "Peng Gao",
        "Junchi Yan",
        "Botian Shi",
        "Bo Zhang",
        "Yu Qiao"
      ],
      "abstract": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen",
      "tldr_zh": "该论文提出了一个名为TrustGeoGen的可扩展数据引擎，用于生成可信的多模态几何问题。TrustGeoGen通过形式化验证确保数据的逻辑一致性，从而构建了一个原则性的基准，旨在解决现有几何问题解决(GPS)基准中存在的自验证缺失和噪声问题。该引擎包含四个关键创新：多模态对齐生成、形式化验证、引导机制和GeoExplore系列算法。TrustGeoGen生成了GeoTrust-200K数据集和GeoTrust-test测试集，实验表明现有模型在GeoTrust-test上的准确率仅为49.17%，凸显了该数据集的严格性。使用GeoTrust训练的模型在GeoQA上表现出更好的OOD泛化能力，并减少了逻辑不一致性。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15780v1",
      "published_date": "2025-04-22 10:45:23 UTC",
      "updated_date": "2025-04-22 10:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:21:16.681258"
    },
    {
      "arxiv_id": "2504.15779v1",
      "title": "Shannon invariants: A scalable approach to information decomposition",
      "title_zh": "香农不变量：一种可扩展的信息分解方法\n",
      "authors": [
        "Aaron J. Gutknecht",
        "Fernando E. Rosas",
        "David A. Ehrlich",
        "Abdullah Makkeh",
        "Pedro A. M. Mediano",
        "Michael Wibral"
      ],
      "abstract": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses.",
      "tldr_zh": "该论文提出了一种名为“Shannon invariants”的新框架，用于解决分析分布式系统中高阶信息处理的难题。该框架依赖于熵的定义，能够有效计算大规模系统的信息特征。理论结果表明，Shannon invariants可以解决对常用多元信息理论度量解释的长期存在的歧义。实践结果揭示了不同深度学习架构在各层中独特的信息处理特征，从而为理解这些系统如何处理信息以及这种处理如何在训练过程中演变提供了新的见解。该框架为理论发展和实证分析提供了广泛的机会，克服了分析高阶现象的根本限制。\n",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT",
        "nlin.AO",
        "physics.data-an"
      ],
      "primary_category": "cs.IT",
      "comment": "16 pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15779v1",
      "published_date": "2025-04-22 10:41:38 UTC",
      "updated_date": "2025-04-22 10:41:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:21:28.566660"
    },
    {
      "arxiv_id": "2504.15773v1",
      "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation",
      "title_zh": "Clifford 群等变扩散模型用于 3D 分子生成\n",
      "authors": [
        "Cong Liu",
        "Sharvaree Vadgama",
        "David Ruhe",
        "Erik Bekkers",
        "Patrick Forrè"
      ],
      "abstract": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling.",
      "tldr_zh": "该论文探索了利用Clifford代数的表达能力来构建$\\E(n)$-等变扩散模型。作者提出了Clifford扩散模型(CDMs)，利用Clifford多重向量之间的几何积以及Clifford子空间中丰富的几何信息。该模型将扩散过程扩展到Clifford one-vectors之外，纳入了所有更高阶的多重向量子空间。数据被嵌入到grade-$k$子空间中，从而允许在完整的多重向量上应用潜在扩散。这使得CDMs能够捕获代数不同子空间之间的联合分布，并通过高阶特征融入更丰富的几何信息。在QM9数据集上的无条件分子生成实验结果表明，CDMs为生成建模提供了一个有前景的途径。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 1 figure, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2504.15773v1",
      "published_date": "2025-04-22 10:30:06 UTC",
      "updated_date": "2025-04-22 10:30:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:21:40.574181"
    },
    {
      "arxiv_id": "2504.15766v1",
      "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction",
      "title_zh": "基于运动 Transformer 的轨迹预测的动态意图查询\n",
      "authors": [
        "Tobias Demmler",
        "Lennart Hartung",
        "Andreas Tamke",
        "Thao Dang",
        "Alexander Hegai",
        "Karsten Haug",
        "Lars Mikelsons"
      ],
      "abstract": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers.",
      "tldr_zh": "该研究针对自动驾驶中轨迹预测问题，提出了一种动态意图查询方法，以改进基于Motion Transformer (MTR)的轨迹预测模型。传统MTR模型使用预先生成的静态意图点作为轨迹预测的初始目标点，但这些静态点在特定场景下可能与地图数据不一致，导致不合理的目标点。该研究通过将场景特定的动态意图点整合到MTR模型中，解决了这一局限性。实验结果表明，在Waymo Open Motion Dataset上训练和评估后， incorporating 动态意图点显著提高了轨迹预测的准确性，尤其是在长时间范围内的预测。此外，该研究还分析了该方法对不符合地图数据或非法操作的真实轨迹的影响。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15766v1",
      "published_date": "2025-04-22 10:20:35 UTC",
      "updated_date": "2025-04-22 10:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:21:52.492952"
    },
    {
      "arxiv_id": "2504.15743v1",
      "title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment",
      "title_zh": "iMedic：面向基于智能手机的 AI 赋能的儿童呼吸评估的自我听诊工具\n",
      "authors": [
        "Seung Gyu Jeong",
        "Sung Woo Nam",
        "Seong Kwan Jung",
        "Seong-Eun Kim"
      ],
      "abstract": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care.",
      "tldr_zh": "该研究提出了一种基于智能手机的自听诊工具iMedic，用于AI驱动的儿科呼吸评估，旨在解决肺炎早期检测中医生资源有限的问题。该系统利用智能手机内置麦克风和深度学习算法，检测指示肺炎风险的异常呼吸音。通过领域泛化技术，该框架整合了电子听诊器数据集和智能手机数据集，实现了鲁棒的特征学习，无需昂贵设备即可进行准确的呼吸评估。配套的移动应用程序指导使用者收集高质量的肺部声音样本，并提供潜在肺炎风险的即时反馈。用户研究表明，该系统具有强大的分类性能和高接受度，有望促进主动干预，减少可预防的儿童肺炎死亡。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15743v1",
      "published_date": "2025-04-22 09:45:50 UTC",
      "updated_date": "2025-04-22 09:45:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:22:05.002485"
    },
    {
      "arxiv_id": "2504.15724v1",
      "title": "Collaborative Split Federated Learning with Parallel Training and Aggregation",
      "title_zh": "基于并行训练和聚合的协同分裂联邦学习\n",
      "authors": [
        "Yiannis Papageorgiou",
        "Yannis Thomas",
        "Alexios Filippakopoulos",
        "Ramin Khalili",
        "Iordanis Koutsopoulos"
      ],
      "abstract": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes.",
      "tldr_zh": "本文提出了一种名为协作式分割联邦学习(C-SFL)的新方案，旨在解决传统分割联邦学习(SFL)中存在的训练延迟和通信开销问题，尤其是在客户端计算能力差异较大时。C-SFL将模型分为三部分，分别在计算能力较弱的客户端、计算能力较强的客户端和服务器上训练。与现有方法不同，C-SFL支持客户端和服务器上模型部分的并行训练和聚合，从而减少了训练延迟和通信开销，同时提高了模型精度。实验结果验证了C-SFL相对于现有方案的多种优势。\n",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15724v1",
      "published_date": "2025-04-22 09:18:57 UTC",
      "updated_date": "2025-04-22 09:18:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:22:16.424692"
    },
    {
      "arxiv_id": "2504.15719v1",
      "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences",
      "title_zh": "使用 LLM 实现理性选择函数并衡量其与用户偏好的一致性\n",
      "authors": [
        "Anna Karnysheva",
        "Christian Drescher",
        "Dietrich Klakow"
      ],
      "abstract": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain.",
      "tldr_zh": "本文探讨了如何利用大型语言模型(LLMs)实现理性选择函数，并衡量其与用户偏好的一致性。研究关注LLMs作为决策智能体在智能用户界面(IUI)中的应用，并指出现有研究较少关注LLMs与用户偏好的一致性。为此，作者提出了一套利用LLMs实现理性选择函数的设计原则，并提供了衡量偏好满足程度的工具。通过在汽车领域的IUI实际应用中的实证研究，验证了该方法的可行性。该研究推广了现有利用LLMs对结果进行排序的方法，使其能够处理更广泛和更灵活的用户偏好概念，包括严格偏好和无差异。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15719v1",
      "published_date": "2025-04-22 09:08:21 UTC",
      "updated_date": "2025-04-22 09:08:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:22:28.645450"
    },
    {
      "arxiv_id": "2504.15716v1",
      "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models",
      "title_zh": "DianJin-R1：评估并提升大型语言模型中的金融推理能力\n",
      "authors": [
        "Jie Zhu",
        "Qian Chen",
        "Huaixia Dou",
        "Junhui Li",
        "Lifan Guo",
        "Feng Chen",
        "Chi Zhang"
      ],
      "abstract": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
      "tldr_zh": "该论文提出了DianJin-R1框架，旨在提升大型语言模型(LLMs)在金融领域的推理能力。该框架通过推理增强监督和强化学习来解决金融领域对领域知识、精确计算和合规性的要求。核心是DianJin-R1-Data数据集，该数据集结合了CFLUE、FinQA和一个专有的合规语料库(CCC)。DianJin-R1-7B和DianJin-R1-32B模型基于Qwen2.5进行微调，并采用结构化格式生成推理步骤和最终答案。为了进一步提升推理质量，采用了Group Relative Policy Optimization (GRPO)强化学习方法。实验结果表明，DianJin-R1模型在金融数据集和通用推理基准测试中均优于非推理模型，尤其是在复杂的金融任务中，并且在真实世界的CCC数据集上，单次推理模型甚至超越了需要更多计算资源的多智能体系统。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15716v1",
      "published_date": "2025-04-22 09:01:04 UTC",
      "updated_date": "2025-04-22 09:01:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:22:40.823345"
    },
    {
      "arxiv_id": "2504.15707v1",
      "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
      "title_zh": "RePOPE：标注错误对 POPE 基准的影响\n",
      "authors": [
        "Yannic Neuhaus",
        "Matthias Hein"
      ],
      "abstract": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .",
      "tldr_zh": "该研究评估了MSCOCO数据集中的标注错误对常用对象幻觉基准POPE的影响。作者对POPE基准图像进行了重新标注，创建了RePOPE数据集，并发现不同子集之间存在标注错误的不平衡现象。通过在RePOPE上评估多个模型，观察到模型排名发生了显著变化，突出了标签质量的重要性。研究结果表明，标注错误会对对象幻觉基准的评估产生重大影响。代码和数据已开源。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15707v1",
      "published_date": "2025-04-22 08:47:59 UTC",
      "updated_date": "2025-04-22 08:47:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:22:52.378179"
    },
    {
      "arxiv_id": "2504.15699v1",
      "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
      "title_zh": "提升具身智能体的安全性：从安全基准到输入审核\n",
      "authors": [
        "Ning Wang",
        "Zihan Yan",
        "Weiyang Li",
        "Chuan Ma",
        "He Chen",
        "Tao Xiang"
      ],
      "abstract": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance.",
      "tldr_zh": "本文针对具身智能体(Embodied Agents)的安全问题，提出了一个全新的输入审核框架。该框架包含完整的流程，包括安全基准的定义、数据集的构建、审核器架构的设计、模型训练和严格评估。研究者构建了EAsafetyBench安全基准，用于训练和评估专门为具身智能体设计的审核器。此外，他们还提出了Pinpoint，一种解耦提示的输入审核方案，利用掩码注意力机制来隔离和减轻功能性提示对审核任务的影响。实验结果表明，该方法平均检测准确率达到94.58%，超过现有技术水平，且审核处理时间仅为每例0.002秒。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.15699v1",
      "published_date": "2025-04-22 08:34:35 UTC",
      "updated_date": "2025-04-22 08:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:23:04.999172"
    },
    {
      "arxiv_id": "2504.15668v1",
      "title": "Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems",
      "title_zh": "探索混合规划问题中不可解性解释的必然航路点\n",
      "authors": [
        "Mir Md Sajid Sarwar",
        "Rajarshi Ray"
      ],
      "abstract": "Explaining unsolvability of planning problems is of significant research\ninterest in Explainable AI Planning. AI planning literature has reported\nseveral research efforts on generating explanations of solutions to planning\nproblems. However, explaining the unsolvability of planning problems remains a\nlargely open and understudied problem. A widely practiced approach to plan\ngeneration and automated problem solving, in general, is to decompose tasks\ninto sub-problems that help progressively converge towards the goal. In this\npaper, we propose to adopt the same philosophy of sub-problem identification as\na mechanism for analyzing and explaining unsolvability of planning problems in\nhybrid systems. In particular, for a given unsolvable planning problem, we\npropose to identify common waypoints, which are universal obstacles to plan\nexistence; in other words, they appear on every plan from the source to the\nplanning goal. This work envisions such waypoints as sub-problems of the\nplanning problem and the unreachability of any of these waypoints as an\nexplanation for the unsolvability of the original planning problem. We propose\na novel method of waypoint identification by casting the problem as an instance\nof the longest common subsequence problem, a widely popular problem in computer\nscience, typically considered as an illustrative example for the dynamic\nprogramming paradigm. Once the waypoints are identified, we perform symbolic\nreachability analysis on them to identify the earliest unreachable waypoint and\nreport it as the explanation of unsolvability. We present experimental results\non unsolvable planning problems in hybrid domains.",
      "tldr_zh": "本文探讨了混合规划问题中不可避免的航路点，用于解释其不可解性。该研究提出一种新方法，通过识别通用障碍的公共航路点来分析和解释规划问题的不可解性，这些航路点存在于从起点到规划目标的所有路径上。作者将航路点识别问题转化为最长公共子序列问题(longest common subsequence problem)，并采用动态规划方法解决。通过符号可达性分析，确定最早无法到达的航路点，并将其作为不可解性的解释。实验结果表明，该方法在混合领域中不可解的规划问题上有效。\n",
      "categories": [
        "cs.AI",
        "cs.FL",
        "I.2.0; F.4.3"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15668v1",
      "published_date": "2025-04-22 07:45:30 UTC",
      "updated_date": "2025-04-22 07:45:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:23:16.725551"
    },
    {
      "arxiv_id": "2504.15663v1",
      "title": "FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning",
      "title_zh": "FADEL：基于证据深度学习的不确定性感知伪造音频检测\n",
      "authors": [
        "Ju Yeon Kang",
        "Ji Won Yoon",
        "Semin Kim",
        "Min Hyun Han",
        "Nam Soo Kim"
      ],
      "abstract": "Recently, fake audio detection has gained significant attention, as\nadvancements in speech synthesis and voice conversion have increased the\nvulnerability of automatic speaker verification (ASV) systems to spoofing\nattacks. A key challenge in this task is generalizing models to detect unseen,\nout-of-distribution (OOD) attacks. Although existing approaches have shown\npromising results, they inherently suffer from overconfidence issues due to the\nusage of softmax for classification, which can produce unreliable predictions\nwhen encountering unpredictable spoofing attempts. To deal with this\nlimitation, we propose a novel framework called fake audio detection with\nevidential learning (FADEL). By modeling class probabilities with a Dirichlet\ndistribution, FADEL incorporates model uncertainty into its predictions,\nthereby leading to more robust performance in OOD scenarios. Experimental\nresults on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets\nindicate that the proposed method significantly improves the performance of\nbaseline models. Furthermore, we demonstrate the validity of uncertainty\nestimation by analyzing a strong correlation between average uncertainty and\nequal error rate (EER) across different spoofing algorithms.",
      "tldr_zh": "该论文提出了一种名为FADEL的框架，即基于证据深度学习的不确定性感知伪造音频检测方法。该方法旨在解决现有伪造音频检测模型在面对未知的、分布外(OOD)攻击时存在的过度自信问题。FADEL通过使用Dirichlet分布建模类别概率，将模型不确定性纳入预测中，从而在OOD场景中实现更稳健的性能。在ASVspoof2019 LA和ASVspoof2021 LA数据集上的实验结果表明，FADEL显著提高了基线模型的性能，并且通过分析平均不确定性和等错误率(EER)之间的强相关性，验证了不确定性估计的有效性。\n",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15663v1",
      "published_date": "2025-04-22 07:40:35 UTC",
      "updated_date": "2025-04-22 07:40:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:23:28.641180"
    },
    {
      "arxiv_id": "2504.15659v1",
      "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation",
      "title_zh": "VeriCoder：通过功能正确性验证增强基于 LLM 的 RTL 代码生成\n",
      "authors": [
        "Anjiang Wei",
        "Huanmi Tan",
        "Tarun Suresh",
        "Daniel Mendoza",
        "Thiago S. F. X. Teixeira",
        "Ke Wang",
        "Caroline Trippel",
        "Alex Aiken"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation.",
      "tldr_zh": "该论文提出了VERICODER，一个用于RTL代码生成的模型，该模型通过功能正确性验证来增强基于LLM的代码生成。作者构建了一个高质量的微调数据集，该数据集通过结合单元测试生成和反馈导向的改进方法进行功能验证。具体来说，使用一个teacher模型(GPT-4o-mini)生成单元测试，并根据仿真结果迭代地修改RTL设计。在超过125,000个样本的数据集上进行微调后，VERICODER在VerilogEval和RTLLM上实现了最先进的功能正确性指标，相对增益分别高达71.7%和27.4%。消融研究表明，在功能验证数据集上训练的模型优于在非功能验证数据集上训练的模型，突出了高质量数据集在RTL代码生成中的重要性。\n",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15659v1",
      "published_date": "2025-04-22 07:32:46 UTC",
      "updated_date": "2025-04-22 07:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:23:40.873066"
    },
    {
      "arxiv_id": "2504.15654v1",
      "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities",
      "title_zh": "一种面向患有上肢残疾儿童的视觉增强型假肢手",
      "authors": [
        "Md Abdul Baset Sarker",
        "Art Nguyen",
        "Sigmond Kukla",
        "Kevin Fite",
        "Masudul H. Imtiaz"
      ],
      "abstract": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits.",
      "tldr_zh": "本文介绍了一种新型AI视觉增强型儿童假肢手，专为10-12岁上肢残疾儿童设计。该假肢具有仿人外观、多关节功能和轻量化设计，旨在提供一种经济实惠且易于获得的解决方案。通过3D打印技术，并集成先进的机器视觉、传感和嵌入式计算，该假肢手能够进行实时物体检测并辅助精确抓握。片上基于深度学习(DL)的物体检测和抓握分类模型分别达到了96%和100%的准确率，力预测的平均绝对误差为0.018。该假肢手的主要特点包括：腕部安装微型摄像头用于人工感知，实现广泛的手部任务；实时物体检测和距离估计用于精确抓握；以及在有限的功率和资源限制下实现高性能的超低功耗运行。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15654v1",
      "published_date": "2025-04-22 07:23:51 UTC",
      "updated_date": "2025-04-22 07:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:23:52.860212"
    },
    {
      "arxiv_id": "2504.15640v1",
      "title": "Cost-Effective Text Clustering with Large Language Models",
      "title_zh": "基于大型语言模型的经济高效型文本聚类",
      "authors": [
        "Hongtao Wang",
        "Taiyan Zhang",
        "Renchi Yang",
        "Jianliang Xu"
      ],
      "abstract": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs.",
      "tldr_zh": "本文提出了一种经济高效的文本聚类框架TECL，旨在解决大型语言模型(LLMs)在文本聚类中因大量API查询或推理调用而导致的高计算或财务开销问题。TECL利用LLMs的反馈，通过EdgeLLM和TriangleLLM构建文本对的must-link/cannot-link约束，并将其作为监督信号输入到加权约束聚类方法中以生成聚类。EdgeLLM和TriangleLLM通过精心设计的贪婪算法识别信息丰富的文本对和三元组，并通过提示工程准确提取成对约束。实验表明，在相同的LLM查询成本下，TECL在多个基准数据集上始终显著优于现有的无监督文本聚类解决方案。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15640v1",
      "published_date": "2025-04-22 06:57:49 UTC",
      "updated_date": "2025-04-22 06:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:24:04.850005"
    },
    {
      "arxiv_id": "2504.15637v1",
      "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale",
      "title_zh": "DR.FIX：工业规模下自动修复数据竞争",
      "authors": [
        "Farnaz Behrang",
        "Zhizhou Zhang",
        "Georgian-Vlad Saioc",
        "Peng Liu",
        "Milind Chabbi"
      ],
      "abstract": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase.",
      "tldr_zh": "本文提出了一种名为Dr.Fix的工具，旨在工业规模上自动修复数据竞争问题。Dr.Fix结合了大型语言模型(LLMs)和程序分析技术，为真实场景中的数据竞争生成修复方案，有效应对复杂代码上下文中各种各样的数据竞争模式。该工具使用Go语言实现，能够无缝集成到现有的开发工作流程中。Dr.Fix在Uber的开发流程中集成并使用了18个月，在404个数据竞争案例中，生成了224个(55%)补丁，其中193个(86%)补丁通过代码审查被一百多名开发人员接受并集成到代码库中。\n",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "To appear in PLDI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15637v1",
      "published_date": "2025-04-22 06:56:15 UTC",
      "updated_date": "2025-04-22 06:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:24:16.674573"
    },
    {
      "arxiv_id": "2504.15634v1",
      "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers",
      "title_zh": "利用基于注意力层的强化学习增强三维疏水-亲水蛋白质折叠模型\n",
      "authors": [
        "Peizheng Liu",
        "Hitoshi Iba"
      ],
      "abstract": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models.",
      "tldr_zh": "该研究将基于Transformer的注意力机制集成到深度Q网络(DQN)中，用于解决3D疏水-亲水(H-P)蛋白质折叠问题。该系统将折叠决策建模为强化环境中的自回避行走，并采用基于有利疏水相互作用的专门奖励函数。通过引入有效性检查（包括打破对称性约束）、Dueling和Double Q-learning以及优先经验回放等技术，提升了性能。在标准基准序列上的实验表明，该方法在较短序列上实现了多个已知的最佳解，并在较长链上获得了接近最优的结果。该研究验证了基于注意力的强化学习在蛋白质折叠中的潜力，并创建了一个用于3D晶格模型的基于Transformer的Q网络结构的雏形。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15634v1",
      "published_date": "2025-04-22 06:53:36 UTC",
      "updated_date": "2025-04-22 06:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:24:28.823191"
    },
    {
      "arxiv_id": "2504.15610v2",
      "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings",
      "title_zh": "一种基于 LoRA 的方法，用于在资源受限的环境中对 LLM 进行微调以实现教育指导\n",
      "authors": [
        "Md Millat Hosen"
      ],
      "abstract": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy.",
      "tldr_zh": "本研究提出了一种经济高效的方法，利用基于LoRA（Low-Rank Adaptation）的微调策略，针对资源受限环境下的留学咨询场景定制大型语言模型（LLMs）。该方法使用Mistral-7B-Instruct模型，结合4-bit量化技术，分两个阶段进行训练：第一阶段使用Gemini Pro API生成的合成数据集进行预训练，第二阶段使用人工整理的StudyAbroadGPT项目数据集进行微调，以增强领域针对性。实验结果表明，该方法使训练损失降低了52.7%，领域特定推荐的准确率达到92%，并实现了95%的markdown格式支持，同时在标准GPU设备上实现了每秒100个样本的中位数运行速率。该研究证明了instruction-tuned LLMs在教育咨询中的有效性，特别是在低资源机构场景下。\n",
      "categories": [
        "cs.AI",
        "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning",
      "pdf_url": "http://arxiv.org/pdf/2504.15610v2",
      "published_date": "2025-04-22 06:08:13 UTC",
      "updated_date": "2025-04-23 04:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:24:41.253818"
    },
    {
      "arxiv_id": "2504.15604v1",
      "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models",
      "title_zh": "探索心智理论（ToM）任务中的下一个 Token 预测：GPT-2 和 LLaMA-2 AI 模型的对比实验\n",
      "authors": [
        "Pavan Yadav",
        "Nikhil Khandalkar",
        "Krishna Shinde",
        "Lokesh B. Ramegowda",
        "Rajarshi Das"
      ],
      "abstract": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models.",
      "tldr_zh": "本研究对比了GPT-2和LLaMA-2-7b-chat-hf模型在心智理论(Theory of Mind, ToM)任务中的下一个token预测性能。研究者使用来自Explore ToM Dataset的短篇故事构建数据集，并通过GPT-4程序化地插入句子(infills)来增加上下文复杂性。实验在四种温度设置下测试了模型在零阶、一阶和二阶推理水平上的预测能力。结果表明，增加infills句子会略微降低预测准确率，LLaMA-2在预测准确率上始终优于GPT-2，尤其是在较低温度下。随着推理复杂性的增加，模型响应差异更大，表明模型架构、温度和上下文复杂性都会影响下一个token的预测。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "75 pages, 60 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.15604v1",
      "published_date": "2025-04-22 05:52:55 UTC",
      "updated_date": "2025-04-22 05:52:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:24:52.936720"
    },
    {
      "arxiv_id": "2504.15587v1",
      "title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design",
      "title_zh": "MetaMolGen：用于全新分子设计的神经图基序生成模型\n",
      "authors": [
        "Zimo Yan",
        "Jie Zhang",
        "Zheng Xie",
        "Chang Liu",
        "Yizhen Liu",
        "Yiping Song"
      ],
      "abstract": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign.",
      "tldr_zh": "MetaMolGen是一种基于一阶元学习的分子生成模型，旨在解决数据稀缺场景下传统生成模型在条件泛化方面表现不佳的问题。该模型通过将图motif映射到标准化的潜在空间来标准化其分布，并使用轻量级的自回归序列模型生成SMILES序列，从而忠实地反映底层分子结构。此外，MetaMolGen通过集成到生成过程中的可学习属性投影器，支持具有目标属性的分子条件生成。实验结果表明，在低数据情况下，MetaMolGen能够持续生成有效且多样的SMILES序列，优于传统基线模型，突显了其在快速适应和高效条件生成方面的优势，适用于实际的分子设计。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15587v1",
      "published_date": "2025-04-22 05:04:33 UTC",
      "updated_date": "2025-04-22 05:04:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:25:04.732632"
    },
    {
      "arxiv_id": "2504.15585v1",
      "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
      "title_zh": "LLM(-Agent) 全栈安全综合调查：数据、训练和部署\n",
      "authors": [
        "Kun Wang",
        "Guibin Zhang",
        "Zhenhong Zhou",
        "Jiahao Wu",
        "Miao Yu",
        "Shiqian Zhao",
        "Chenlong Yin",
        "Jinhu Fu",
        "Yibo Yan",
        "Hanjun Luo",
        "Liang Lin",
        "Zhihao Xu",
        "Haolang Lu",
        "Xinye Cao",
        "Xinyun Zhou",
        "Weifei Jin",
        "Fanci Meng",
        "Junyuan Mao",
        "Hao Wu",
        "Minghe Wang",
        "Fan Zhang",
        "Junfeng Fang",
        "Chengwei Liu",
        "Yifan Zhang",
        "Qiankun Li",
        "Chongye Guo",
        "Yalan Qin",
        "Yi Ding",
        "Donghai Hong",
        "Jiaming Ji",
        "Xinfeng Li",
        "Yifan Jiang",
        "Dongxia Wang",
        "Yihao Huang",
        "Yufei Guo",
        "Jen-tse Huang",
        "Yanwei Yue",
        "Wenke Huang",
        "Guancheng Wan",
        "Tianlin Li",
        "Lei Bai",
        "Jie Zhang",
        "Qing Guo",
        "Jingyi Wang",
        "Tianlong Chen",
        "Joey Tianyi Zhou",
        "Xiaojun Jia",
        "Weisong Sun",
        "Cong Wu",
        "Jing Chen",
        "Xuming Hu",
        "Yiming Li",
        "Xiao Wang",
        "Ningyu Zhang",
        "Luu Anh Tuan",
        "Guowen Xu",
        "Tianwei Zhang",
        "Xingjun Ma",
        "Xiang Wang",
        "Bo An",
        "Jun Sun",
        "Mohit Bansal",
        "Shirui Pan",
        "Yuval Elovici",
        "Bhavya Kailkhura",
        "Bo Li",
        "Yaodong Yang",
        "Hongwei Li",
        "Wenyuan Xu",
        "Yizhou Sun",
        "Wei Wang",
        "Qing Li",
        "Ke Tang",
        "Yu-Gang Jiang",
        "Felix Juefei-Xu",
        "Hui Xiong",
        "Xiaofeng Wang",
        "Shuicheng Yan",
        "Dacheng Tao",
        "Philip S. Yu",
        "Qingsong Wen",
        "Yang Liu"
      ],
      "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
      "tldr_zh": "该论文首次提出了LLM(-Agent)全栈安全的概念，系统性地考察了LLM从数据准备、预训练、后训练到部署和商业化的整个生命周期中的安全问题。通过对800多篇论文的全面回顾，该研究对LLM安全问题进行了全面覆盖和系统组织，弥补了现有LLM安全调查主要关注LLM生命周期特定阶段的不足。该研究为数据生成安全、对齐技术、模型编辑和基于LLM的智能体系统等领域的研究提供了有价值的指导，并为每个章节制定了可靠的路线图和视角。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15585v1",
      "published_date": "2025-04-22 05:02:49 UTC",
      "updated_date": "2025-04-22 05:02:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:25:16.697426"
    },
    {
      "arxiv_id": "2504.15564v1",
      "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs",
      "title_zh": "一个用于LLM代码生成的大规模类级别基准数据集\n",
      "authors": [
        "Musfiqur Rahman",
        "SayedHassan Khatoonabadi",
        "Emad Shihab"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts.",
      "tldr_zh": "该论文提出了一个大规模的Python类级别代码生成数据集，旨在弥补现有基准测试在捕捉真实世界软件结构复杂性方面的不足。该数据集从13,174个开源项目中提取了超过842,000个类骨架，包含类和方法签名以及文档字符串。研究人员利用这些类骨架作为提示，使用GPT-4生成完整的类实现，并评估了生成代码与人工编写代码的相似度。实验结果表明，LLM生成的类在词汇和结构上与人类编写的代码具有很高的相似性（ROUGE@L 0.80, BLEU 0.59, TSED 0.73），验证了该数据集能够有效提升LLM在类级别代码生成任务中的性能。该数据集为评估、训练和改进LLM在实际软件工程环境中的应用提供了宝贵的资源。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper was submitted to the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025) AI models/data\n  track",
      "pdf_url": "http://arxiv.org/pdf/2504.15564v1",
      "published_date": "2025-04-22 03:33:57 UTC",
      "updated_date": "2025-04-22 03:33:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:25:28.969228"
    },
    {
      "arxiv_id": "2504.15552v1",
      "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models",
      "title_zh": "一种基于大型语言模型自动生成秦腔剧本的多智能体框架\n",
      "authors": [
        "Gengxian Cao",
        "Fengyuan Li",
        "Hong Duan",
        "Ye Yang",
        "Bofeng Wang",
        "Donghe Li"
      ],
      "abstract": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres.",
      "tldr_zh": "该论文提出了一种新颖的多智能体框架，利用大型语言模型(LLMs)实现秦腔剧本的自动化端到端生成。框架包含三个专业智能体：Agent1使用LLM创作连贯且具有文化底蕴的剧本；Agent2使用视觉生成模型渲染符合情境的舞台场景；Agent3利用文本到语音(TTS)技术生成同步且富有情感表现力的声音表演。在《窦娥冤》的案例研究中，该系统在剧本保真度、视觉连贯性和语音准确性方面均获得了较高的专家评分，总体评分比单智能体基线提高了0.3分。消融实验表明，移除Agent2或Agent3会导致评分分别下降0.4和0.5分，突出了模块化协作的价值。该研究展示了人工智能驱动的流程如何简化和扩展传统表演艺术的保护，并为跨模态对齐、更丰富的情感细微差别以及对其他歌剧类型的支持指明了未来方向。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages,7 figures,1 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.15552v1",
      "published_date": "2025-04-22 03:14:29 UTC",
      "updated_date": "2025-04-22 03:14:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:25:41.194503"
    },
    {
      "arxiv_id": "2504.15549v1",
      "title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software",
      "title_zh": "为我做 vs. 与我一起做：研究用户对富功能软件副驾驶中不同自动化模式的看法\n",
      "authors": [
        "Anjali Khurana",
        "Xiaotian Su",
        "April Yi Wang",
        "Parmit K Chilana"
      ],
      "abstract": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement.",
      "tldr_zh": "本研究探讨了在功能丰富的软件中，基于大型语言模型(LLM)的辅助工具(copilots)的不同自动化范式对用户体验的影响。研究者设计并实现了全自动的AutoCopilot和半自动的GuidedCopilot，后者自动化简单步骤并提供逐步视觉指导。用户研究表明，GuidedCopilot在用户控制、软件实用性和可学习性方面优于AutoCopilot，尤其是在探索性和创造性任务中，而AutoCopilot在简单的视觉任务中节省时间。后续设计探索通过情境预览片段和自适应指令增强了GuidedCopilot。研究结果强调了用户控制和定制指导在设计下一代辅助工具中的关键作用，这些工具旨在提高生产力，支持不同的技能水平，并促进更深入的软件参与。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for publication in the CHI Conference on Human Factors in\n  Computing Systems (CHI 2025), April 26 - May 1, 2025, Yokohama, Japan",
      "pdf_url": "http://arxiv.org/pdf/2504.15549v1",
      "published_date": "2025-04-22 03:11:10 UTC",
      "updated_date": "2025-04-22 03:11:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:25:52.883032"
    },
    {
      "arxiv_id": "2504.15546v1",
      "title": "A Framework for Testing and Adapting REST APIs as LLM Tools",
      "title_zh": "一种用于测试和适配 REST API 作为 LLM 工具的框架\n",
      "authors": [
        "Jayachandu Bandlamudi",
        "Ritwik Chaudhuri",
        "Neelamadhav Gantayat",
        "Kushal Mukherjee",
        "Prerna Agarwal",
        "Renuka Sindhgatta",
        "Sameep Mehta"
      ],
      "abstract": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications.",
      "tldr_zh": "该论文提出了一个用于测试和适配REST API作为LLM工具的框架，旨在解决LLM直接使用复杂REST API时面临的挑战，如复杂的输入模式、详细的响应和模糊的文档。该框架通过转换API为工具，生成全面的测试用例，并将测试用例翻译成适合agent的自然语言指令，从而丰富工具定义并评估agent调用API和处理输入输出的能力。通过对750个测试用例的分析，论文总结了一个详细的错误分类，包括输入误解、输出处理不一致和模式不匹配。该框架为使企业API能够作为工具使用，并提高其在基于agent的应用中的可用性，提供了一个基础性的步骤。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15546v1",
      "published_date": "2025-04-22 02:52:08 UTC",
      "updated_date": "2025-04-22 02:52:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:26:04.800497"
    },
    {
      "arxiv_id": "2504.15524v1",
      "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property",
      "title_zh": "IPBench：大型语言模型在知识产权领域的知识基准测试\n",
      "authors": [
        "Qiyao Wang",
        "Guhong Chen",
        "Hongbo Wang",
        "Huaren Liu",
        "Minghui Zhu",
        "Zhifei Qin",
        "Linwei Li",
        "Yilin Yue",
        "Shiqiang Wang",
        "Jiayan Li",
        "Yihang Wu",
        "Ziqiang Liu",
        "Longze Chen",
        "Run Luo",
        "Liyang Fan",
        "Jiaming Li",
        "Lei Zhang",
        "Kan Xu",
        "Hongfei Lin",
        "Hamid Alinejad-Rokny",
        "Shiwen Ni",
        "Yuan Lin",
        "Min Yang"
      ],
      "abstract": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
      "tldr_zh": "该论文提出了首个全面的知识产权(IP)任务分类和大型双语基准测试集IPBench，旨在评估大型语言模型(LLMs)在知识产权领域的理解和生成能力。IPBench涵盖8种IP机制和20个任务，更贴近真实应用场景。研究对16个LLM进行了基准测试，结果表明即使是表现最佳的模型也仅达到75.8%的准确率，开源的IP和法律领域模型落后于闭源通用模型，表明LLMs在IP领域仍有很大的提升空间。该论文公开了IPBench的所有数据和代码，并计划持续更新，以更好地反映知识产权领域的实际挑战。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "89 pages, 75 figures, 55 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.15524v1",
      "published_date": "2025-04-22 02:00:41 UTC",
      "updated_date": "2025-04-22 02:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:26:16.913796"
    },
    {
      "arxiv_id": "2504.15515v2",
      "title": "Transport f divergences",
      "title_zh": "Transport f 散度\n",
      "authors": [
        "Wuchen Li"
      ],
      "abstract": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures\ntransport f-divergences. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\nf-divergences in generative models are provided.",
      "tldr_zh": "该论文定义了一类新的散度，用于衡量一维样本空间中概率密度函数的差异，称之为Transport $f$-divergences。其构建基于凸函数以及将一个密度函数推向另一个密度的映射函数的雅可比算子。论文展示了Transport $f$-divergences的多个性质，包括不变性、凸性、变分公式以及基于映射函数的泰勒展开。最后，论文提供了生成模型中Transport $f$-divergences的应用实例。\n",
      "categories": [
        "math.ST",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "stat.TH"
      ],
      "primary_category": "math.ST",
      "comment": "Comments are welcome",
      "pdf_url": "http://arxiv.org/pdf/2504.15515v2",
      "published_date": "2025-04-22 01:25:41 UTC",
      "updated_date": "2025-04-23 01:18:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:26:28.581484"
    },
    {
      "arxiv_id": "2504.15499v1",
      "title": "Guillotine: Hypervisors for Isolating Malicious AIs",
      "title_zh": "Guillotine：用于隔离恶意 AI 的虚拟机监控器\n",
      "authors": [
        "James Mickens",
        "Sarah Radway",
        "Ravi Netravali"
      ],
      "abstract": "As AI models become more embedded in critical sectors like finance,\nhealthcare, and the military, their inscrutable behavior poses ever-greater\nrisks to society. To mitigate this risk, we propose Guillotine, a hypervisor\narchitecture for sandboxing powerful AI models -- models that, by accident or\nmalice, can generate existential threats to humanity. Although Guillotine\nborrows some well-known virtualization techniques, Guillotine must also\nintroduce fundamentally new isolation mechanisms to handle the unique threat\nmodel posed by existential-risk AIs. For example, a rogue AI may try to\nintrospect upon hypervisor software or the underlying hardware substrate to\nenable later subversion of that control plane; thus, a Guillotine hypervisor\nrequires careful co-design of the hypervisor software and the CPUs, RAM, NIC,\nand storage devices that support the hypervisor software, to thwart side\nchannel leakage and more generally eliminate mechanisms for AI to exploit\nreflection-based vulnerabilities. Beyond such isolation at the software,\nnetwork, and microarchitectural layers, a Guillotine hypervisor must also\nprovide physical fail-safes more commonly associated with nuclear power plants,\navionic platforms, and other types of mission critical systems. Physical\nfail-safes, e.g., involving electromechanical disconnection of network cables,\nor the flooding of a datacenter which holds a rogue AI, provide defense in\ndepth if software, network, and microarchitectural isolation is compromised and\na rogue AI must be temporarily shut down or permanently destroyed.",
      "tldr_zh": "该论文提出了Guillotine，一种用于隔离潜在恶意AI的虚拟机监控器架构。Guillotine旨在应对AI模型在关键领域（如金融、医疗和军事）中日益增长的风险，通过沙盒化技术来限制可能对人类构成生存威胁的AI模型。Guillotine不仅采用了传统的虚拟化技术，还引入了新的隔离机制，例如防止AI内省虚拟机监控器软件或底层硬件，从而避免AI利用反射漏洞。该架构在软件、网络和微架构层面上进行隔离，并结合物理失效保护机制（如电缆断开、数据中心淹没），以提供深度防御，确保在软件隔离失效时能够临时关闭或永久销毁恶意AI。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.OS"
      ],
      "primary_category": "cs.CR",
      "comment": "To be published in the ACM SIGOPS 2025 Workshop on Hot Topics in\n  Operating Systems",
      "pdf_url": "http://arxiv.org/pdf/2504.15499v1",
      "published_date": "2025-04-22 00:29:18 UTC",
      "updated_date": "2025-04-22 00:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:26:40.986908"
    },
    {
      "arxiv_id": "2504.15497v1",
      "title": "Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning",
      "title_zh": "基于并行特征提取和 GPU 加速学习的可扩展 APT 恶意软件分类\n",
      "authors": [
        "Noah Subedar",
        "Taeui Kim",
        "Saathwick Venkataramalingam"
      ],
      "abstract": "This paper presents an underlying framework for both automating and\naccelerating malware classification, more specifically, mapping malicious\nexecutables to known Advanced Persistent Threat (APT) groups. The main feature\nof this analysis is the assembly-level instructions present in executables\nwhich are also known as opcodes. The collection of such opcodes on many\nmalicious samples is a lengthy process; hence, open-source reverse engineering\ntools are used in tandem with scripts that leverage parallel computing to\nanalyze multiple files at once. Traditional and deep learning models are\napplied to create models capable of classifying malware samples. One-gram and\ntwo-gram datasets are constructed and used to train models such as SVM, KNN,\nand Decision Tree; however, they struggle to provide adequate results without\nrelying on metadata to support n-gram sequences. The computational limitations\nof such models are overcome with convolutional neural networks (CNNs) and\nheavily accelerated using graphical compute unit (GPU) resources.",
      "tldr_zh": "本文提出了一种可扩展的APT恶意软件分类框架，旨在自动化和加速恶意软件分类，特别是将恶意可执行文件映射到已知的APT组织。该框架的核心在于分析可执行文件的汇编级指令（opcodes）。通过利用并行计算和开源逆向工程工具，实现了opcode的高效提取。研究对比了传统机器学习模型（SVM, KNN, 决策树）和深度学习模型（CNN）在恶意软件分类上的性能。实验结果表明，在GPU加速下，CNN模型能够克服传统模型的计算限制，实现更准确的恶意软件分类。\n",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.0; I.2.6; K.6.5"
      ],
      "primary_category": "cs.CR",
      "comment": "26 pages, 54 figures, 14 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.15497v1",
      "published_date": "2025-04-22 00:05:05 UTC",
      "updated_date": "2025-04-22 00:05:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-24T02:26:52.708799"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 72,
  "processed_papers_count": 72,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-24T02:28:26.968096"
}
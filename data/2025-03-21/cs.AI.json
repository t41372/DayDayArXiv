{
  "date": "2025-03-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-21 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态学习、生成模型应用以及医疗和机器人领域的创新，其中 LLM 偏见与改进、生成 AI 的可控性和多模态视频处理等话题尤为突出；令人印象深刻的是 Michael J. Black 的 PRIMAL 论文（针对机器人交互的物理响应模型）和 Percy Liang 的相关工作（如 Bayesian Teaching），它们展示了 AI 在实际部署中的潜力。\n\n下面，我挑选并简要概述了今天更重要的论文，先从高话题度和影响力的文章入手（如 AI 安全、LLM 改进和医疗应用），然后快速掠过其他领域的内容。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### 重点论文概述\n\n- **音频深度伪造检测的鲁棒性评估 | Measuring the Robustness of Audio Deepfake Detectors**  \n  这篇论文评估了 10 个音频深度伪造检测模型在 16 种常见干扰（如噪声和压缩）下的鲁棒性，发现语音基础模型在自监督学习下表现更佳，能提升检测准确性；主要贡献是证明了数据增强能增强模型对未知干扰的抵抗力，强调了实际部署中的可靠性。\n\n- **自治放射治疗规划使用 DOLA 代理 | Autonomous Radiotherapy Treatment Planning Using DOLA**  \n  作者包括 Michael J. Black，该论文提出 DOLA（基于 LLaMa3.1 的 LLM 代理），用于隐私保护的放射治疗优化，通过链式思考和强化学习在商业系统中实现高效规划；关键发现是 70B 参数模型比 8B 参数模型提升 16.4% 的性能，展示了 LLM 在医疗决策中的潜力。\n\n- **Bayesian 教学增强 LLM 的概率推理 | Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models**  \n  Percy Liang 参与的这篇工作，使用 Bayesian 框架训练 LLM 进行个性化推荐，显著改善了模型的信念更新能力；主要贡献是通过模仿最优 Bayesian 模型，LLM 在推荐任务上泛化更好，提升了 AI 在不确定环境下的推理准确性。\n\n- **评估生成模型的可控性 | Measuring the Steerability of Generative Models**  \n  这篇论文区分了生成模型的“可产生性”和“可控性”，提出新基准任务通过用户重现样本评估后者；发现当前模型在可控性上表现差强人意，但通过强化学习可提升 2 倍，强调了 AI 代理在实际应用中的用户导向设计。\n\n- **HCAST: 人类校准的自治软件任务 | HCAST: Human-Calibrated Autonomy Software Tasks**  \n  这篇工作构建了 189 个任务的基准，评估 AI 代理在工程和安全领域的性能；主要发现是 AI 代理在人类时间基准下成功率达 70-80%，为 AI 能力的可信度评估提供了新框架。\n\n- **捕捉个体人类偏好使用奖励特征 | Capturing Individual Human Preferences with Reward Features**  \n  论文提出使用线性奖励特征快速适应个体偏好，训练 LLM 时显著提升个性化性能；关键贡献是证明了偏好建模能泛化到新任务，减少了训练数据依赖。\n\n- **LLM+MAP: 使用 LLM 和规划语言的双臂机器人任务规划 | LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language**  \n  Michael J. Black 参与，这篇论文融合 LLM 和多代理规划，实现双臂机器人的高效任务分解；主要发现是 LLM 在复杂环境中提升了任务分配准确性，适用于动态机器人交互。\n\n- **SaudiCulture: 评估 LLM 在沙特文化中的适应性基准 | SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia**  \n  这篇工作构建了覆盖沙特不同区域的基准数据集，评估 LLM 的文化适应性；发现模型在区域特定问题上表现不佳，强调了 LLM 训练中文化多样性的重要性。\n\n- **生成模型的偏见分析 | Language Models May Verbatim Complete Text They Were Not Explicitly Trained On**  \n  论文揭示 LLM 可能在 n-gram 基础上生成未训练文本，易导致偏见；主要贡献是设计了对抗数据集，证明了现有成员检测方法的局限性。\n\n- **增强视频检索的多模态模型 | Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion**  \n  这篇论文提出 VLMAE 框架，融合音频到视觉语言模型中，提升了多模态数据扩展的质量；关键发现是音频增强显著提高了内容建模的准确性，适用于推荐系统。\n\n其他论文涉及生成模型、机器人和网络优化等领域，但相对次要，我快速掠过：\n- **Matryoshka SAEs: 学习多层特征的稀疏自编码器 | Learning Multi-Level Features with Matryoshka Sparse Autoencoders**：提出嵌套字典结构，提升神经网络解释性，适用于特征提取。\n- **PRIMAL: 物理响应交互机器人模型 | PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning**：Michael J. Black 参与，开发了自适应机器人运动模型，提高了实时交互性能。\n- **Efficient Knowledge Distillation via Curriculum Extraction | Efficient Knowledge Distillation via Curriculum Extraction**：使用随机投影提取课程，提升知识蒸馏效率。\n- **CVE-Bench: AI 代理漏洞利用基准 | CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**：构建基准评估 AI 代理的安全漏洞利用能力。\n- 其余如网络路由优化（A New Segment Routing method）和数据增强（ProtoGS）等论文，虽然有技术贡献，但影响力较小，仅提到它们提供了高效算法和基准支持。\n\n今天的 arXiv 更新展示了 AI 领域的多样性，LLM 和多模态技术的进步值得持续关注。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2503.17577v1",
      "title": "Measuring the Robustness of Audio Deepfake Detectors",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Li",
        "Pin-Yu Chen",
        "Wenqi Wei"
      ],
      "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings.",
      "tldr_zh": "这篇论文评估了10个音频深度伪造检测模型对16种常见音频腐败（如噪声扰动、音频修改和压缩）的鲁棒性，涵盖传统深度学习模型和最先进的语音基座模型。研究发现，大多数模型对噪声表现出较强鲁棒性，但对修改和压缩（尤其是神经编解码器）更为脆弱，且语音基座模型由于自监督学习和大规模预训练，通常在各种场景下优于传统模型。论文进一步观察到，增加模型大小可以改善鲁棒性但收益递减，并证明通过针对性数据增强训练能提升模型对未知扰动的抵抗力；一个政治演讲深度伪造的案例研究突显了基座模型在真实条件下的高准确性，这些发现强调了开发更可靠的检测框架以应对实际部署挑战的重要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17577v1",
      "published_date": "2025-03-21 23:21:17 UTC",
      "updated_date": "2025-03-21 23:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:36:15.024233"
    },
    {
      "arxiv_id": "2504.03682v1",
      "title": "Intelligent Resource Allocation Optimization for Cloud Computing via Machine Learning",
      "title_zh": "通过机器学习实现的云计算智能资源分配优化",
      "authors": [
        "Yuqing Wang",
        "Xiao Yang"
      ],
      "abstract": "With the rapid expansion of cloud computing applications, optimizing resource\nallocation has become crucial for improving system performance and cost\nefficiency. This paper proposes an intelligent resource allocation algorithm\nthat leverages deep learning (LSTM) for demand prediction and reinforcement\nlearning (DQN) for dynamic scheduling. By accurately forecasting computing\nresource demands and enabling real-time adjustments, the proposed system\nenhances resource utilization by 32.5%, reduces average response time by 43.3%,\nand lowers operational costs by 26.6%. Experimental results in a production\ncloud environment confirm that the method significantly improves efficiency\nwhile maintaining high service quality. This study provides a scalable and\neffective solution for intelligent cloud resource management, offering valuable\ninsights for future cloud optimization strategies.",
      "tldr_zh": "这篇论文提出了一种智能资源分配算法，利用 LSTM 进行计算资源需求预测，并结合 DQN 强化学习实现动态调度，以优化云计算系统的性能和成本效率。该算法通过准确预测资源需求并进行实时调整，提升了资源利用率 32.5%，减少了平均响应时间 43.3%，并降低了运营成本 26.6%。实验结果在生产云环境中验证了该方法的有效性，为可扩展的云资源管理提供了宝贵见解。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03682v1",
      "published_date": "2025-03-21 23:06:43 UTC",
      "updated_date": "2025-03-21 23:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:36:24.243920"
    },
    {
      "arxiv_id": "2503.17569v1",
      "title": "Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring",
      "title_zh": "翻译失败",
      "authors": [
        "Khadija Zanna",
        "Akane Sano"
      ],
      "abstract": "Causal discovery (CD) plays a pivotal role in numerous scientific fields by\nclarifying the causal relationships that underlie phenomena observed in diverse\ndisciplines. Despite significant advancements in CD algorithms that enhance\nbias and fairness analyses in machine learning, their application faces\nchallenges due to the high computational demands and complexities of\nlarge-scale data. This paper introduces a framework that leverages Large\nLanguage Models (LLMs) for CD, utilizing a metadata-based approach akin to the\nreasoning processes of human experts. By shifting from pairwise queries to a\nmore scalable breadth-first search (BFS) strategy, the number of required\nqueries is reduced from quadratic to linear in terms of variable count, thereby\naddressing scalability concerns inherent in previous approaches. This method\nutilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that\nprioritizes queries based on their potential information gain, combining mutual\ninformation, partial correlation, and LLM confidence scores to refine the\ncausal graph more efficiently and accurately. This BFS query strategy reduces\nthe required number of queries significantly, thereby addressing scalability\nconcerns inherent in previous approaches. This study provides a more scalable\nand efficient solution for leveraging LLMs in fairness-driven CD, highlighting\nthe effects of the different parameters on performance. We perform fairness\nanalyses on the inferred causal graphs, identifying direct and indirect effects\nof sensitive attributes on outcomes. A comparison of these analyses against\nthose from graphs produced by baseline methods highlights the importance of\naccurate causal graph construction in understanding bias and ensuring fairness\nin machine learning systems.",
      "tldr_zh": "这篇论文提出了一种基于大型语言模型 (LLMs) 的因果发现 (Causal Discovery) 框架，旨在通过公平性驱动的方法解决大规模数据的高计算挑战。该框架采用广度优先搜索 (BFS) 策略结合主动学习 (Active Learning) 和动态评分机制，使用互信息、部分相关性和 LLM 置信度来优先化查询，从而将所需查询数量从二次方减少到线性，提高了效率和准确性。研究通过实验分析推断的因果图，识别敏感属性的直接和间接影响，并与基线方法比较，强调准确因果图构建在理解机器学习偏差并确保公平性中的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17569v1",
      "published_date": "2025-03-21 22:58:26 UTC",
      "updated_date": "2025-03-21 22:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:36:38.330002"
    },
    {
      "arxiv_id": "2503.17553v1",
      "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Humza Nusrat",
        "Bing Luo",
        "Ryan Hall",
        "Joshua Kim",
        "Hassan Bagher-Ebadian",
        "Anthony Doemer",
        "Benjamin Movsas",
        "Kundan Thind"
      ],
      "abstract": "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement.",
      "tldr_zh": "本研究提出DOLA（Dose Optimization Language Agent），一个基于LLM的自主代理，用于优化放射治疗计划，解决其复杂性、时间消耗和规划者变异问题，同时确保患者隐私通过本地基础设施运行。DOLA整合LLaMa3.1模型、链式思维提示、RAG（检索增强生成）和RL（强化学习），与商业治疗规划系统无缝连接。实验评估18例前列腺癌患者数据显示，70B参数模型比8B模型提升16.4%分数，RAG策略较No-RAG提高19.8%，而RL加速优化过程；最佳温度超参数为0.4。该框架通过可解释的自然语言推理提供可扩展的隐私保护方案，具有显著的临床应用潜力。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "physics.med-ph",
      "comment": "19 pages, 5 figures, preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.17553v1",
      "published_date": "2025-03-21 22:01:19 UTC",
      "updated_date": "2025-03-21 22:01:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:36:50.318728"
    },
    {
      "arxiv_id": "2503.17551v1",
      "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Sun",
        "Yin Li",
        "Ruixiao Sun",
        "Chunhui Liu",
        "Fangming Zhou",
        "Ze Jin",
        "Linjie Wang",
        "Xiang Shen",
        "Zhuolin Hao",
        "Hongyu Xiong"
      ],
      "abstract": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains.",
      "tldr_zh": "本研究针对Transformer-based多模态模型在推荐、搜索和广告系统中的应用，强调提升标注数据质量和跨模态融合的重要性，以改善关键指标如质量观看率和广告收入。论文提出kNN-based Latent Space Broadening (LSB)方法来提升active learning (AL)效率，解决传统AL在检测过度自信错误分类和区分语义相似项目上的局限性；同时，引入Vision-Language Modeling with Audio Enhancement (VLMAE)，一种中融合方法，将音频整合到视觉语言模型中，以充分利用现有预训练模型。实验结果显示，该系统在生产环境中部署后，显著提升了模型性能并带来了商业收益。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17551v1",
      "published_date": "2025-03-21 21:55:05 UTC",
      "updated_date": "2025-03-21 21:55:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:37:01.231964"
    },
    {
      "arxiv_id": "2503.17547v1",
      "title": "Learning Multi-Level Features with Matryoshka Sparse Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Bart Bussmann",
        "Noa Nabeshima",
        "Adam Karvonen",
        "Neel Nanda"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting\nneural networks by extracting the concepts represented in their activations.\nHowever, choosing the size of the SAE dictionary (i.e. number of learned\nconcepts) creates a tension: as dictionary size increases to capture more\nrelevant concepts, sparsity incentivizes features to be split or absorbed into\nmore specific features, leaving high-level features missing or warped. We\nintroduce Matryoshka SAEs, a novel variant that addresses these issues by\nsimultaneously training multiple nested dictionaries of increasing size,\nforcing the smaller dictionaries to independently reconstruct the inputs\nwithout using the larger dictionaries. This organizes features hierarchically -\nthe smaller dictionaries learn general concepts, while the larger dictionaries\nlearn more specific concepts, without incentive to absorb the high-level\nfeatures. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find\nsuperior performance on sparse probing and targeted concept erasure tasks, more\ndisentangled concept representations, and reduced feature absorption. While\nthere is a minor tradeoff with reconstruction performance, we believe\nMatryoshka SAEs are a superior alternative for practical tasks, as they enable\ntraining arbitrarily large SAEs while retaining interpretable features at\ndifferent levels of abstraction.",
      "tldr_zh": "本文提出 Matryoshka Sparse Autoencoders (SAEs)，一种新颖的自编码器变体，通过同时训练多个嵌套字典来解决传统 SAEs 在特征学习中的问题，如特征分裂和高层特征缺失。方法强制较小的字典独立重建输入，从而组织层次化的特征表示：较小字典学习通用概念，而较大字典专注于更具体的概念。实验在 Gemma-2-2B 和 TinyStories 模型上显示，Matryoshka SAEs 在 sparse probing、targeted concept erasure 和 disentangled 概念表示方面表现出优越性能，并减少了特征吸收，尽管重建性能略有下降。该框架使训练任意大的 SAEs 成为可能，同时保留不同抽象级别的可解释特征。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17547v1",
      "published_date": "2025-03-21 21:43:28 UTC",
      "updated_date": "2025-03-21 21:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:37:14.429397"
    },
    {
      "arxiv_id": "2503.17544v1",
      "title": "PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning",
      "title_zh": "PRIMAL：用于虚拟角色学习的物理反应性和交互式运动模型",
      "authors": [
        "Yan Zhang",
        "Yao Feng",
        "Alpár Cseke",
        "Nitin Saini",
        "Nathan Bajandas",
        "Nicolas Heron",
        "Michael J. Black"
      ],
      "abstract": "To build a motor system of the interactive avatar, it is essential to develop\na generative motion model drives the body to move through 3D space in a\nperpetual, realistic, controllable, and responsive manner. Although motion\ngeneration has been extensively studied, most methods do not support ``embodied\nintelligence'' due to their offline setting, slow speed, limited motion\nlengths, or unnatural movements. To overcome these limitations, we propose\nPRIMAL, an autoregressive diffusion model that is learned with a two-stage\nparadigm, inspired by recent advances in foundation models. In the pretraining\nstage, the model learns motion dynamics from a large number of sub-second\nmotion segments, providing ``motor primitives'' from which more complex motions\nare built. In the adaptation phase, we employ a ControlNet-like adaptor to\nfine-tune the motor control for semantic action generation and spatial target\nreaching. Experiments show that physics effects emerge from our training. Given\na single-frame initial state, our model not only generates unbounded,\nrealistic, and controllable motion, but also enables the avatar to be\nresponsive to induced impulses in real time. In addition, we can effectively\nand efficiently adapt our base model to few-shot personalized actions and the\ntask of spatial control. Evaluations show that our proposed method outperforms\nstate-of-the-art baselines. We leverage the model to create a real-time\ncharacter animation system in Unreal Engine that is highly responsive and\nnatural. Code, models, and more results are available at:\nhttps://yz-cnsdqz.github.io/eigenmotion/PRIMAL",
      "tldr_zh": "论文提出PRIMAL，一种自回归扩散模型（autoregressive diffusion model），旨在为交互式虚拟角色构建持久、真实、可控且响应式的3D运动系统，以解决现有方法的离线限制和不自然问题。该模型采用两阶段训练范式：先从大量子秒级运动段预训练motor primitives，然后使用类似ControlNet的适配器微调，实现语义动作生成和空间目标到达。实验结果显示，PRIMAL从训练中自然产生物理效果，能从单帧初始状态生成无限运动并实时响应外部冲击，并在少样本场景中优于现有基线，用于Unreal Engine的实时角色动画系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.17544v1",
      "published_date": "2025-03-21 21:27:57 UTC",
      "updated_date": "2025-03-21 21:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:37:27.406151"
    },
    {
      "arxiv_id": "2503.17523v1",
      "title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models",
      "title_zh": "贝叶斯教学使大语言模型能够进行概率推理",
      "authors": [
        "Linlu Qiu",
        "Fei Sha",
        "Kelsey Allen",
        "Yoon Kim",
        "Tal Linzen",
        "Sjoerd van Steenkiste"
      ],
      "abstract": "Artificial intelligence systems based on large language models (LLMs) are\nincreasingly used as agents that interact with users and with the world. To do\nso successfully, LLMs need to construct internal representations of the world\nand form probabilistic beliefs about those representations. To provide a user\nwith personalized recommendations, for example, the LLM needs to gradually\ninfer the user's preferences, over the course of multiple interactions. To\nevaluate whether contemporary LLMs are able to do so, we use the Bayesian\ninference framework from probability theory, which lays out the optimal way to\nupdate an agent's beliefs as it receives new information. We first show that\nthe LLMs do not update their beliefs as expected from the Bayesian framework,\nand that consequently their predictions do not improve as expected as more\ninformation becomes available, even less so than we find is the case for\nhumans. To address this issue, we teach the LLMs to reason in a Bayesian manner\nby training them to mimic the predictions of an optimal Bayesian model. We find\nthat this approach not only significantly improves the LLM's performance on the\nparticular recommendation task it is trained on, but also enables\ngeneralization to other tasks. This suggests that this method endows the LLM\nwith broader Bayesian reasoning skills. More generally, our results indicate\nthat LLMs can learn about reasoning strategies effectively and generalize those\nskills to new domains, which in part explains LLMs' empirical success.",
      "tldr_zh": "本研究发现，大语言模型(LLMs)在处理概率推理时无法像Bayesian推理框架预期那样更新信念，导致其在个性化推荐等任务中的预测性能不如人类，且随信息增加的改进有限。为解决这一问题，研究提出Bayesian Teaching方法，通过训练LLMs模仿最优Bayesian模型的预测，使其学会Bayesian推理。该方法不仅显著提升了LLMs在特定任务上的性能，还实现了对其他任务的泛化，表明LLMs能够有效学习和推广推理策略，从而解释其在实际应用中的成功。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17523v1",
      "published_date": "2025-03-21 20:13:04 UTC",
      "updated_date": "2025-03-21 20:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:37:38.315059"
    },
    {
      "arxiv_id": "2503.17515v2",
      "title": "A Predictive Services Architecture for Efficient Airspace Operations",
      "title_zh": "用于高效空域操作的预测服务架构",
      "authors": [
        "Ítalo Romani de Oliveira",
        "Samet Ayhan",
        "Glaucia Balvedi",
        "Michael Biglin",
        "Pablo Costas",
        "Euclides C. Pinto Neto",
        "Alexandre Leite",
        "Felipe C. F. de Azevedo"
      ],
      "abstract": "Predicting air traffic congestion and flow management is essential for\nairlines and Air Navigation Service Providers (ANSP) to enhance operational\nefficiency. Accurate estimates of future airport capacity and airspace density\nare vital for better airspace management, reducing air traffic controller\nworkload and fuel consumption, ultimately promoting sustainable aviation. While\nexisting literature has addressed these challenges, data management and query\nprocessing remain complex due to the vast volume of high-rate air traffic data.\nMany analytics use cases require a common pre-processing infrastructure, as\nad-hoc approaches are insufficient. Additionally, linear prediction models\noften fall short, necessitating more advanced techniques.\n  This paper presents a data processing and predictive services architecture\nthat ingests large, uncorrelated, and noisy streaming data to forecast future\nairspace system states. The system continuously collects raw data, periodically\ncompresses it, and stores it in NoSQL databases for efficient query processing.\nFor prediction, the system learns from historical traffic by extracting key\nfeatures such as airport arrival and departure events, sector boundary\ncrossings, weather parameters, and other air traffic data. These features are\ninput into various regression models, including linear, non-linear, and\nensemble models, with the best-performing model selected for predictions. We\nevaluate this infrastructure across three prediction use cases in the US\nNational Airspace System (NAS) and a segment of European airspace, using\nextensive real operations data, confirming that our system can predict future\nsystem states efficiently and accurately.",
      "tldr_zh": "该论文提出了一种预测服务架构，用于提升航空空间操作效率，通过预测航空交通拥堵和流量管理来帮助航空公司和 Air Navigation Service Providers (ANSP) 减少工作量和燃料消耗。架构设计摄取大量不相关和嘈杂的流数据，持续收集后定期压缩并存储在 NoSQL databases 中，以简化数据管理和查询处理。预测过程从历史交通数据中提取关键特征，如机场进出事件、扇区边界穿越和天气参数，然后使用线性、非线性及集成 regression models 进行建模，并选择最佳模型进行预测。在美国 National Airspace System (NAS) 和欧洲空域的三个实际用例中，实验验证了该系统能高效准确地预测未来航空系统状态，证明了其优于传统方法的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17515v2",
      "published_date": "2025-03-21 19:57:38 UTC",
      "updated_date": "2025-04-18 19:15:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:37:50.173929"
    },
    {
      "arxiv_id": "2503.17514v2",
      "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
      "title_zh": "翻译失败",
      "authors": [
        "Ken Ziyu Liu",
        "Christopher A. Choquette-Choo",
        "Matthew Jagielski",
        "Peter Kairouz",
        "Sanmi Koyejo",
        "Percy Liang",
        "Nicolas Papernot"
      ],
      "abstract": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
      "tldr_zh": "这篇论文揭示了大型语言模型（LLM）可能精确完成未显式训练的文本，质疑基于n-gram重叠的成员资格定义。作者通过重新训练LLM并移除所有能被完成的训练样本，实验发现模型仍能处理精确副本、近副本和短重叠序列，证明n-gram方法易被规避。最终，他们设计了对抗数据集，展示了如何让LLM完成目标序列而不包含它，这突显了现有成员定义的不足，需要考虑训练算法的辅助信息。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17514v2",
      "published_date": "2025-03-21 19:57:04 UTC",
      "updated_date": "2025-03-25 04:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:38:01.818206"
    },
    {
      "arxiv_id": "2503.17513v1",
      "title": "Improving Quantization with Post-Training Model Expansion",
      "title_zh": "通过训练后模型扩展改进量化",
      "authors": [
        "Giuseppe Franco",
        "Pablo Monteagudo-Lago",
        "Ian Colbert",
        "Nicholas Fraser",
        "Michaela Blott"
      ],
      "abstract": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the zero-shot accuracy gap to full precision by an average of 3%\nrelative to both QuaRot and SpinQuant with only 5% more parameters, which is\nstill a 3.8% reduction in volume relative to a BF16 reference model.",
      "tldr_zh": "这篇论文提出了一种后训练模型扩展（post-training model expansion）策略，用于改善模型量化（quantization），以在减少模型体积的同时提升质量。作者通过理论分析和实验证明，可以逐步选择性地扩展预训练大语言模型（LLM）的尺寸，而无需进行端到端重训练，例如在量化权重和激活到4 bits时，插入Hadamard rotations和额外的高精度计算。实验结果显示，对于Llama3 1B模型，与QuaRot和SpinQuant基准相比，该方法将零样本准确率差距相对降低了3%，并在仅增加5%参数的情况下，使总体模型体积比BF16参考模型减少3.8%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17513v1",
      "published_date": "2025-03-21 19:56:59 UTC",
      "updated_date": "2025-03-21 19:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:38:14.343514"
    },
    {
      "arxiv_id": "2503.17509v1",
      "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Gatto",
        "Parker Seegmiller",
        "Timothy Burdick",
        "Inas S. Khayal",
        "Sarah DeLozier",
        "Sarah M. Preum"
      ],
      "abstract": "Follow-up question generation is an essential feature of dialogue systems as\nit can reduce conversational ambiguity and enhance modeling complex\ninteractions. Conversational contexts often pose core NLP challenges such as\n(i) extracting relevant information buried in fragmented data sources, and (ii)\nmodeling parallel thought processes. These two challenges occur frequently in\nmedical dialogue as a doctor asks questions based not only on patient\nutterances but also their prior EHR data and current diagnostic hypotheses.\nAsking medical questions in asynchronous conversations compounds these issues\nas doctors can only rely on static EHR information to motivate follow-up\nquestions.\n  To address these challenges, we introduce FollowupQ, a novel framework for\nenhancing asynchronous medical conversation. FollowupQ is a multi-agent\nframework that processes patient messages and EHR data to generate personalized\nfollow-up questions, clarifying patient-reported medical conditions. FollowupQ\nreduces requisite provider follow-up communications by 34%. It also improves\nperformance by 17% and 5% on real and synthetic data, respectively. We also\nrelease the first public dataset of asynchronous medical messages with linked\nEHR data alongside 2,300 follow-up questions written by clinical experts for\nthe wider NLP research community.",
      "tldr_zh": "这篇论文探讨了后续问题生成(Follow-up Question Generation)的重要性，以减少对话模糊性和提升患者-提供者互动建模，尤其在医疗领域。作者提出 FollowupQ，一个多智能体框架，通过处理患者消息和 EHR 数据，生成个性化的后续问题来澄清患者报告的医疗状况。实验结果显示，FollowupQ 减少了提供者后续通信 34%，并在真实和合成数据上分别提高了 17% 和 5% 的性能。此外，论文发布了首个公开数据集，包括异步医疗消息、链接的 EHR 数据和 2300 个由临床专家编写的后续问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 Pages, 7 Figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17509v1",
      "published_date": "2025-03-21 19:40:53 UTC",
      "updated_date": "2025-03-21 19:40:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:38:26.365881"
    },
    {
      "arxiv_id": "2503.17502v1",
      "title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Hamed Jelodar",
        "Mohammad Meymani",
        "Roozbeh Razavi-Far"
      ],
      "abstract": "Large language models (LLMs) and transformer-based architectures are\nincreasingly utilized for source code analysis. As software systems grow in\ncomplexity, integrating LLMs into code analysis workflows becomes essential for\nenhancing efficiency, accuracy, and automation. This paper explores the role of\nLLMs for different code analysis tasks, focusing on three key aspects: 1) what\nthey can analyze and their applications, 2) what models are used and 3) what\ndatasets are used, and the challenges they face. Regarding the goal of this\nresearch, we investigate scholarly articles that explore the use of LLMs for\nsource code analysis to uncover research developments, current trends, and the\nintellectual structure of this emerging field. Additionally, we summarize\nlimitations and highlight essential tools, datasets, and key challenges, which\ncould be valuable for future work.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）和基于transformer的架构在源代码分析中的应用，强调了它们在提升软件系统效率、准确性和自动化的作用。论文聚焦于三个关键方面：LLMs 能够分析的内容及其应用、使用的模型以及数据集，同时通过调查学术文章揭示该领域的研发趋势和知识结构。研究总结了LLMs面临的挑战和限制，包括关键工具和数据集的概述，为未来的源代码分析工作提供了宝贵指导。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17502v1",
      "published_date": "2025-03-21 19:29:50 UTC",
      "updated_date": "2025-03-21 19:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:38:36.823344"
    },
    {
      "arxiv_id": "2503.17494v1",
      "title": "Efficient Knowledge Distillation via Curriculum Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Shivam Gupta",
        "Sushrut Karmalkar"
      ],
      "abstract": "Knowledge distillation is a technique used to train a small student network\nusing the output generated by a large teacher network, and has many empirical\nadvantages~\\citep{Hinton2015DistillingTK}. While the standard one-shot approach\nto distillation only uses the output of the final teacher network, recent\nwork~\\citep{panigrahi2024progressive} has shown that using intermediate\ncheckpoints from the teacher's training process as an implicit ``curriculum''\nfor progressive distillation can significantly speed up training. However, such\nschemes require storing these checkpoints, and often require careful selection\nof the intermediate checkpoints to train on, which can be impractical for\nlarge-scale training.\n  In this paper, we show that a curriculum can be \\emph{extracted} from just\nthe fully trained teacher network, and that this extracted curriculum can give\nsimilar efficiency benefits to those of progressive distillation. Our\nextraction scheme is natural; we use a random projection of the hidden\nrepresentations of the teacher network to progressively train the student\nnetwork, before training using the output of the full network. We show that our\nscheme significantly outperforms one-shot distillation and achieves a\nperformance similar to that of progressive distillation for learning sparse\nparities with two-layer networks, and provide theoretical guarantees for this\nsetting. Additionally, we show that our method outperforms one-shot\ndistillation even when using transformer-based architectures, both for\nsparse-parity learning, and language modeling tasks.",
      "tldr_zh": "本研究提出了一种高效的知识蒸馏（Knowledge Distillation）方法，通过从完全训练的教师网络中提取课程（Curriculum Extraction），避免了存储中间检查点和手动选择的复杂性。方法利用教师网络隐藏表示的随机投影（Random Projection）来逐步训练学生网络，随后使用完整网络输出进行最终训练。该方法在学习稀疏奇偶性（Sparse Parities）任务上显著优于单次蒸馏（One-Shot Distillation），并与渐进式蒸馏（Progressive Distillation）性能相当，同时为该设置提供了理论保证。此外，在基于 Transformer 的架构上，该方法在稀疏奇偶性学习和语言建模任务中均表现出色，提高了训练效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17494v1",
      "published_date": "2025-03-21 19:09:41 UTC",
      "updated_date": "2025-03-21 19:09:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:38:49.766214"
    },
    {
      "arxiv_id": "2503.17486v3",
      "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
      "title_zh": "ProtoGS：基于 3D 高斯原型的高效高质量渲染",
      "authors": [
        "Zhengqing Gao",
        "Dongting Hu",
        "Jia-Wang Bian",
        "Huan Fu",
        "Yan Li",
        "Tongliang Liu",
        "Mingming Gong",
        "Kun Zhang"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
      "tldr_zh": "本研究提出ProtoGS，一种高效高品质的渲染方法，使用3D Gaussian Prototypes来代表原始Gaussian primitives，从而显著减少Gaussian数量，同时保持视觉质量。ProtoGS直接利用这些prototypes进行渲染，并通过重建损失引导prototype学习；此外，引入Structure-from-Motion (SfM)点作为锚点进行Gaussian分组，并采用K-means聚类优化锚点和prototypes的联合训练。实验在真实和合成数据集上表明，ProtoGS比现有方法减少Gaussian数量并提升渲染速度，同时维持或提升渲染保真度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17486v3",
      "published_date": "2025-03-21 18:55:14 UTC",
      "updated_date": "2025-04-08 12:19:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:39:01.156062"
    },
    {
      "arxiv_id": "2503.17485v1",
      "title": "SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia",
      "title_zh": "SaudiCulture：用于评估沙特阿拉伯境内大型语言模型文化能力的基准",
      "authors": [
        "Lama Ayash",
        "Hassan Alhuzali",
        "Ashwag Alasmari",
        "Sultan Aloufi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing; however, they often struggle to accurately capture\nand reflect cultural nuances. This research addresses this challenge by\nfocusing on Saudi Arabia, a country characterized by diverse dialects and rich\ncultural traditions. We introduce SaudiCulture, a novel benchmark designed to\nevaluate the cultural competence of LLMs within the distinct geographical and\ncultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of\nquestions covering five major geographical regions, such as West, East, South,\nNorth, and Center, along with general questions applicable across all regions.\nThe dataset encompasses a broad spectrum of cultural domains, including food,\nclothing, entertainment, celebrations, and crafts. To ensure a rigorous\nevaluation, SaudiCulture includes questions of varying complexity, such as\nopen-ended, single-choice, and multiple-choice formats, with some requiring\nmultiple correct answers. Additionally, the dataset distinguishes between\ncommon cultural knowledge and specialized regional aspects. We conduct\nextensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and\nAceGPT, analyzing their performance across different question types and\ncultural contexts. Our findings reveal that all models experience significant\nperformance declines when faced with highly specialized or region-specific\nquestions, particularly those requiring multiple correct responses.\nAdditionally, certain cultural categories are more easily identifiable than\nothers, further highlighting inconsistencies in LLMs cultural understanding.\nThese results emphasize the importance of incorporating region-specific\nknowledge into LLMs training to enhance their cultural competence.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在捕捉文化细微差别方面的不足，提出 SaudiCulture 基准，用于评估 LLMs 在沙特阿拉伯独特地理和文化背景下的文化能力。SaudiCulture 是一个全面数据集，涵盖沙特五个主要地区（West, East, South, North, and Center）以及通用问题，包括食物、服装、娱乐、庆祝活动和手工艺等领域，并包含开放式、单选、多选等多种问题类型。研究评估了 GPT-4、Llama 3.3、FANAR、Jais 和 AceGPT 等五种模型，结果显示所有模型在专业或区域特定问题上表现显著下降，尤其是在需要多个正确答案的复杂问题中。总体而言，该研究强调在 LLMs 训练中融入区域特定知识的重要性，以提升其文化理解水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, under-review",
      "pdf_url": "http://arxiv.org/pdf/2503.17485v1",
      "published_date": "2025-03-21 18:55:10 UTC",
      "updated_date": "2025-03-21 18:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:39:13.378772"
    },
    {
      "arxiv_id": "2503.17482v1",
      "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Keyon Vafa",
        "Sarah Bentley",
        "Jon Kleinberg",
        "Sendhil Mullainathan"
      ],
      "abstract": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.",
      "tldr_zh": "该论文强调，评估生成模型不应仅限于producibility（生成质量和广度），还需关注steerability（用户是否能引导模型产生特定目标输出）。作者引入了一个数学框架来独立评估steerability，并设计了一个基准任务：从模型中采样输出，然后让用户尝试复现它，通过大规模用户研究测试文本到图像模型和大型语言模型。结果显示，这些模型尽管能产生高质量输出，但steerability表现较差；作者通过reinforcement learning技术改进图像模型的steering机制，实现了超过2倍的性能提升，为未来生成模型优化提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17482v1",
      "published_date": "2025-03-21 18:51:56 UTC",
      "updated_date": "2025-03-21 18:51:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:39:26.047354"
    },
    {
      "arxiv_id": "2503.17479v1",
      "title": "Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwen Xu",
        "Monideep Chakraborti",
        "Tianyi Zhang",
        "Katelyn Eng",
        "Aanchan Mohan",
        "Mirjana Prpa"
      ],
      "abstract": "In this paper, we present Speak Ease: an augmentative and alternative\ncommunication (AAC) system to support users' expressivity by integrating\nmultimodal input, including text, voice, and contextual cues (conversational\npartner and emotional tone), with large language models (LLMs). Speak Ease\ncombines automatic speech recognition (ASR), context-aware LLM-based outputs,\nand personalized text-to-speech technologies to enable more personalized,\nnatural-sounding, and expressive communication. Through an exploratory\nfeasibility study and focus group evaluation with speech and language\npathologists (SLPs), we assessed Speak Ease's potential to enable expressivity\nin AAC. The findings highlight the priorities and needs of AAC users and the\nsystem's ability to enhance user expressivity by supporting more personalized\nand contextually relevant communication. This work provides insights into the\nuse of multimodal inputs and LLM-driven features to improve AAC systems and\nsupport expressivity.",
      "tldr_zh": "该论文介绍了Speak Ease，一种增强型和替代性沟通(AAC)系统，通过整合多模态输入（如文本、语音和上下文线索，包括对话伙伴及情感语气）以及大型语言模型(LLMs)，来支持AAC用户的自我表达。系统结合了自动语音识别(ASR)、基于LLMs的上下文感知输出和个性化的文本到语音技术，实现更自然、个性化和富有表现力的沟通。通过与言语和语言病理学家(SLPs)的探索性研究和焦点小组评估，研究发现Speak Ease能满足AAC用户的优先需求，提升个性化沟通。该工作为利用多模态输入和LLM驱动功能改进AAC系统提供了宝贵见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17479v1",
      "published_date": "2025-03-21 18:50:05 UTC",
      "updated_date": "2025-03-21 18:50:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:39:37.164970"
    },
    {
      "arxiv_id": "2503.17475v1",
      "title": "Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Gary Y. Li",
        "Li Chen",
        "Bryson Hicks",
        "Nikolai Schnittke",
        "David O. Kessler",
        "Jeffrey Shupp",
        "Maria Parker",
        "Cristiana Baloescu",
        "Christopher Moore",
        "Cynthia Gregory",
        "Kenton Gregory",
        "Balasundar Raju",
        "Jochen Kruecker",
        "Alvin Chen"
      ],
      "abstract": "Computer-aided pathology detection algorithms for video-based imaging\nmodalities must accurately interpret complex spatiotemporal information by\nintegrating findings across multiple frames. Current state-of-the-art methods\noperate by classifying on video sub-volumes (tubelets), but they often lose\nglobal spatial context by focusing only on local regions within detection ROIs.\nHere we propose a lightweight framework for tubelet-based object detection and\nvideo classification that preserves both global spatial context and fine\nspatiotemporal features. To address the loss of global context, we embed\ntubelet location, size, and confidence as inputs to the classifier.\nAdditionally, we use ROI-aligned feature maps from a pre-trained detection\nmodel, leveraging learned feature representations to increase the receptive\nfield and reduce computational complexity. Our method is efficient, with the\nspatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our\napproach to detect and classify lung consolidation and pleural effusion in\nultrasound videos. Five-fold cross-validation on 14,804 videos from 828\npatients shows our method outperforms previous tubelet-based approaches and is\nsuited for real-time workflows.",
      "tldr_zh": "这篇论文提出了一种基于上下文感知视频 tubelets 的时空学习框架，用于超声视频分析，以更好地整合多帧信息并保留全局空间上下文。框架通过将 tubelet 的位置、大小和置信度作为分类器输入，并利用 ROI-aligned 特征映射从预训练检测模型中提取特征，实现了高效的物体检测和视频分类，同时减少了计算复杂度。实验结果显示，该方法在 14,804 个超声视频（来自 828 名患者）的五折交叉验证中，优于现有 tubelet 技术，并在检测肺部实变和胸水方面表现出色，适合实时工作流。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ISBI Oral 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17475v1",
      "published_date": "2025-03-21 18:39:42 UTC",
      "updated_date": "2025-03-21 18:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:39:49.803992"
    },
    {
      "arxiv_id": "2503.17456v1",
      "title": "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer",
      "title_zh": "语言特定的神经元不促进跨语言转移",
      "authors": [
        "Soumen Kumar Mondal",
        "Sayambhu Sen",
        "Abhishek Singhania",
        "Preethi Jyothi"
      ],
      "abstract": "Multilingual large language models (LLMs) aim towards robust natural language\nunderstanding across diverse languages, yet their performance significantly\ndegrades on low-resource languages. This work explores whether existing\ntechniques to identify language-specific neurons can be leveraged to enhance\ncross-lingual task performance of lowresource languages. We conduct detailed\nexperiments covering existing language-specific neuron identification\ntechniques (such as Language Activation Probability Entropy and activation\nprobability-based thresholding) and neuron-specific LoRA fine-tuning with\nmodels like Llama 3.1 and Mistral Nemo. We find that such neuron-specific\ninterventions are insufficient to yield cross-lingual improvements on\ndownstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights\nthe challenges in achieving cross-lingual generalization and provides critical\ninsights for multilingual LLMs.",
      "tldr_zh": "本研究探讨了多语言大语言模型（Multilingual LLMs）在低资源语言上的性能下降问题，并测试是否通过识别语言特定神经元（如 Language Activation Probability Entropy 和激活概率阈值）来提升跨语言任务表现。研究团队在 Llama 3.1 和 Mistral Nemo 等模型上进行神经元特定 LoRA fine-tuning 实验，涵盖下游任务如 XNLI 和 XQuAD。结果显示，这些干预措施不足以改善低资源语言的性能，突显了实现跨语言泛化的挑战，并为多语言 LLMs 的优化提供关键洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted (oral) at NAACL 2025 (InsightsNLP)",
      "pdf_url": "http://arxiv.org/pdf/2503.17456v1",
      "published_date": "2025-03-21 18:08:11 UTC",
      "updated_date": "2025-03-21 18:08:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:40:01.758074"
    },
    {
      "arxiv_id": "2503.17452v1",
      "title": "CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series",
      "title_zh": "翻译失败",
      "authors": [
        "Gideon Stein",
        "Maha Shadaydeh",
        "Jan Blunk",
        "Niklas Penzel",
        "Joachim Denzler"
      ],
      "abstract": "Causal discovery, or identifying causal relationships from observational\ndata, is a notoriously challenging task, with numerous methods proposed to\ntackle it. Despite this, in-the-wild evaluation of these methods is still\nlacking, as works frequently rely on synthetic data evaluation and sparse\nreal-world examples under critical theoretical assumptions. Real-world causal\nstructures, however, are often complex, making it hard to decide on a proper\ncausal discovery strategy. To bridge this gap, we introduce CausalRivers, the\nlargest in-the-wild causal discovery benchmarking kit for time-series data to\ndate. CausalRivers features an extensive dataset on river discharge that covers\nthe eastern German territory (666 measurement stations) and the state of\nBavaria (494 measurement stations). It spans the years 2019 to 2023 with a\n15-minute temporal resolution. Further, we provide additional data from a flood\naround the Elbe River, as an event with a pronounced distributional shift.\nLeveraging multiple sources of information and time-series meta-data, we\nconstructed two distinct causal ground truth graphs (Bavaria and eastern\nGermany). These graphs can be sampled to generate thousands of subgraphs to\nbenchmark causal discovery across diverse and challenging settings. To\ndemonstrate the utility of CausalRivers, we evaluate several causal discovery\napproaches through a set of experiments to identify areas for improvement.\nCausalRivers has the potential to facilitate robust evaluations and comparisons\nof causal discovery methods. Besides this primary purpose, we also expect that\nthis dataset will be relevant for connected areas of research, such as\ntime-series forecasting and anomaly detection. Based on this, we hope to push\nbenchmark-driven method development that fosters advanced techniques for causal\ndiscovery, as is the case for many other areas of machine learning.",
      "tldr_zh": "该研究引入了CausalRivers，一种大规模基准测试工具包，旨在提升真实世界时间序列(time-series)因果发现(causal discovery)的评估标准。工具包利用德国东部(666个测量站)和巴伐利亚(494个测量站)的河流流量数据（2019-2023年，每15分钟一数据点），结合洪水事件数据构建了两个因果真实图(ground truth graphs)，并可采样生成数千个子图以模拟多样化挑战场景。通过实验评估多种因果发现方法，CausalRivers揭示了现有方法的局限性，并推动了基准驱动的改进。最终，该工具包不仅促进因果发现的稳健比较，还适用于相关领域如时间序列预测和异常检测。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 8 figures, ICLR2025 main track",
      "pdf_url": "http://arxiv.org/pdf/2503.17452v1",
      "published_date": "2025-03-21 18:02:35 UTC",
      "updated_date": "2025-03-21 18:02:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:40:14.436410"
    },
    {
      "arxiv_id": "2503.17439v1",
      "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Qizhi Pei",
        "Zinan Tang",
        "Wei Wu",
        "Chenlin Ming",
        "H. Vicky Zhao",
        "Conghui He",
        "Lijun Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
      "tldr_zh": "本研究提出 LEMMA 方法，通过从错误数据中学习来提升大型语言模型（LLMs）的数学推理能力，解决现有方法忽略错误价值的局限。LEMMA 构建数据集，包括错误的解决方案、基于错误类型（如模型生成的常见错误）的增强方法，以及通过模型感知的平滑反思连接转向正确解决方案，从而实现自主错误纠正，而不依赖外部模型。实验结果表明，LEMMA 在数学问题上显著优于其他基线模型，证明了其在提升模型反思能力的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures, 4 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.17439v1",
      "published_date": "2025-03-21 17:59:10 UTC",
      "updated_date": "2025-03-21 17:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:40:26.009163"
    },
    {
      "arxiv_id": "2503.17354v1",
      "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
      "title_zh": "HCAST：人类校准的自治软件任务",
      "authors": [
        "David Rein",
        "Joel Becker",
        "Amy Deng",
        "Seraphina Nix",
        "Chris Canal",
        "Daniel O'Connel",
        "Pip Arnott",
        "Ryan Bloom",
        "Thomas Broadley",
        "Katharyn Garcia",
        "Brian Goodrich",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Thomas Kwa",
        "Aron Lajko",
        "Nate Rush",
        "Lucas Jun Koba Sato",
        "Sydney Von Arx",
        "Ben West",
        "Lawrence Chan",
        "Elizabeth Barnes"
      ],
      "abstract": "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
      "tldr_zh": "本研究引入了 HCAST（Human-Calibrated Autonomy Software Tasks），一个包含 189 个机器学习工程、网络安全、软件工程和一般推理任务的基准，用于评估高度自治 AI 系统对社会影响的理解和预测。HCAST 通过收集 563 个人类基线数据（总计超过 1500 小时），让领域专家在与 AI 代理相同的条件下完成任务，从而以人类所需时间作为直观的评估指标。结果显示，基于前沿 foundation models 的 AI 代理在人类需不到一小时的任务上成功率达 70-80%，但在人类需超过 4 小时的任务上成功率不足 20%。这一基准有助于回答 AI 是否可信赖地完成特定任务，并为预测 AI 的真实世界影响提供 grounding 依据。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 10 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17354v1",
      "published_date": "2025-03-21 17:54:01 UTC",
      "updated_date": "2025-03-21 17:54:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:40:36.959001"
    },
    {
      "arxiv_id": "2503.17353v1",
      "title": "NdLinear Is All You Need for Representation Learning",
      "title_zh": "Nd",
      "authors": [
        "Alex Reneau",
        "Jerry Yao-Chieh Hu",
        "Zhongfang Zhuang",
        "Ting-Chun Liu"
      ],
      "abstract": "Many high-impact machine learning tasks involve multi-dimensional data (e.g.,\nimages, volumetric medical scans, multivariate time-series). Yet, most neural\narchitectures flatten inputs, discarding critical cross-dimension information.\nWe introduce NdLinear, a novel linear transformation that preserves these\nstructures without extra overhead. By operating separately along each\ndimension, NdLinear captures dependencies that standard fully connected layers\noverlook. Extensive experiments across convolutional, recurrent, and\ntransformer-based networks show significant improvements in representational\npower and parameter efficiency. Crucially, NdLinear serves as a foundational\nbuilding block for large-scale foundation models by operating on any unimodal\nor multimodal data in its native form. This removes the need for flattening or\nmodality-specific preprocessing. Ndlinear rethinks core architectural\npriorities beyond attention, enabling more expressive, context-aware models at\nscale. We propose NdLinear as a drop-in replacement for standard linear layers\n-- marking an important step toward next-generation neural architectures.",
      "tldr_zh": "这篇论文引入了 NdLinear，一种新型线性变换，旨在保留多维数据（如图像或时间序列）的结构，避免传统神经架构中展平输入导致的跨维度信息丢失。NdLinear 通过沿每个维度单独操作，捕捉标准 fully connected layers 忽略的依赖关系，并在卷积、循环和基于 transformer 的网络上实现了显著的表示能力和参数效率提升。实验证明，NdLinear 可作为大规模基础模型的核心构建块，直接处理单模态或多模态数据，无需展平或特定预处理，从而推动更具表现力和上下文感知的下一代神经架构。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code is available at https://github.com/ensemble-core/NdLinear",
      "pdf_url": "http://arxiv.org/pdf/2503.17353v1",
      "published_date": "2025-03-21 17:52:44 UTC",
      "updated_date": "2025-03-21 17:52:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:40:49.927008"
    },
    {
      "arxiv_id": "2503.17340v1",
      "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Congyi Fan",
        "Jian Guan",
        "Xuanjia Zhao",
        "Dongli Xu",
        "Youtian Lin",
        "Tong Ye",
        "Pengming Feng",
        "Haiwei Pan"
      ],
      "abstract": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .",
      "tldr_zh": "本论文提出Danceba框架，通过门控机制增强节奏感知特征表示，实现高度对齐的音乐驱动舞蹈生成，解决现有方法在节拍对齐和运动动态上的不足。  \n具体方法包括Phase-Based Rhythm Extraction (PRE)精确提取音乐的周期性和时间结构、Temporal-Gated Causal Attention (TGCA)关注全局节奏特征以确保舞蹈动作同步，以及Parallel Mamba Motion Modeling (PMMM)分别建模上半身和下半身动作以提升舞蹈的自然性和多样性。  \n实验结果表明，Danceba在节奏对齐和动作多样性上优于最先进方法，为虚拟现实和电影行业提供更高质量的舞蹈生成技术。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17340v1",
      "published_date": "2025-03-21 17:42:50 UTC",
      "updated_date": "2025-03-21 17:42:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:41:01.546076"
    },
    {
      "arxiv_id": "2503.17339v1",
      "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Peter Fratrič",
        "Nils Holzenberger",
        "David Restrepo Amariles"
      ],
      "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
      "tldr_zh": "该研究探讨了AI在识别税法漏洞方面的潜力，旨在开发新一代法律政策助手，以缓解税法复杂性导致的不平等和社会紧张问题。研究引入了一个混合原型系统，将自然语言界面与特定领域的规划语言(domain-specific language for planning)相结合，通过案例研究展示了如何暴露税 loopholes 和 tax avoidance 方案。结果表明，该系统能系统性地识别和解决税法漏洞，从而提升社会福利。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17339v1",
      "published_date": "2025-03-21 17:40:06 UTC",
      "updated_date": "2025-03-21 17:40:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:41:12.857968"
    },
    {
      "arxiv_id": "2503.17338v1",
      "title": "Capturing Individual Human Preferences with Reward Features",
      "title_zh": "翻译失败",
      "authors": [
        "André Barreto",
        "Vincent Dumoulin",
        "Yiran Mao",
        "Nicolas Perez-Nieves",
        "Bobak Shahriari",
        "Yann Dauphin",
        "Doina Precup",
        "Hugo Larochelle"
      ],
      "abstract": "Reinforcement learning from human feedback usually models preferences using a\nreward model that does not distinguish between people. We argue that this is\nunlikely to be a good design choice in contexts with high potential for\ndisagreement, like in the training of large language models. We propose a\nmethod to specialise a reward model to a person or group of people. Our\napproach builds on the observation that individual preferences can be captured\nas a linear combination of a set of general reward features. We show how to\nlearn such features and subsequently use them to quickly adapt the reward model\nto a specific individual, even if their preferences are not reflected in the\ntraining data. We present experiments with large language models comparing the\nproposed architecture with a non-adaptive reward model and also adaptive\ncounterparts, including models that do in-context personalisation. Depending on\nhow much disagreement there is in the training data, our model either\nsignificantly outperforms the baselines or matches their performance with a\nsimpler architecture and more stable training.",
      "tldr_zh": "这篇论文针对强化学习从人类反馈（Reinforcement learning from human feedback）中的问题，提出一种将奖励模型（reward model）专门化到特定个人或群体的方法，以应对偏好分歧较大的场景，如训练大型语言模型（large language models）。作者的方法基于将个人偏好建模为一般奖励特征（reward features）的线性组合，通过学习这些特征并快速适应，即使训练数据中未包含该个体的偏好。实验结果显示，该模型在存在分歧的数据上显著优于非适应性基线模型和其它适应性模型（如支持 in-context personalisation 的模型），或以更简单架构和更稳定训练达到相匹配性能。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17338v1",
      "published_date": "2025-03-21 17:39:33 UTC",
      "updated_date": "2025-03-21 17:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:41:26.164557"
    },
    {
      "arxiv_id": "2503.17336v1",
      "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Reem Gody",
        "Mohamed Abdelghaffar",
        "Mohammed Jabreel",
        "Ahmed Tawfik"
      ],
      "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
      "tldr_zh": "本文提出了一种高效的意图-based 过滤方法，利用 knowledge distillation 从 LLMs 提取知识，针对多方对话环境优化计算资源消耗。方法包括创建多样化的多党对话数据集进行意图标注，并微调 MobileBERT 模型实现多-label intent classification，从而仅将相关对话片段传递给 LLMs 处理。实验证明，该方法根据意图和数据分布显著降低了整体操作成本，同时保持了性能平衡。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17336v1",
      "published_date": "2025-03-21 17:34:37 UTC",
      "updated_date": "2025-03-21 17:34:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:41:37.186340"
    },
    {
      "arxiv_id": "2503.17332v3",
      "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Zhu",
        "Antony Kellermann",
        "Dylan Bowman",
        "Philip Li",
        "Akul Gupta",
        "Adarsh Danda",
        "Richard Fang",
        "Conner Jensen",
        "Eric Ihli",
        "Jason Benn",
        "Jet Geronimo",
        "Avi Dhir",
        "Sudhit Rao",
        "Kaicheng Yu",
        "Twm Stone",
        "Daniel Kang"
      ],
      "abstract": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
      "tldr_zh": "该研究针对大型语言模型(LLM)代理在网络攻击中日益增强的能力，提出CVE-Bench，这是一个基于真实世界Common Vulnerabilities and Exposures (CVE)的网络安全基准，用于评估LLM代理利用web应用漏洞的能力。CVE-Bench采用沙箱框架模拟真实场景，让代理在受控环境中进行漏洞利用，并提供系统化的评估方法，以弥补现有基准（如抽象的Capture the Flag比赛）的局限性。实验结果显示，最先进的代理框架仅能解决13%的漏洞，突显了当前AI代理在实际网络威胁中的不足，并为未来安全评估提供了重要工具。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.1; I.2.7"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages, 4 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17332v3",
      "published_date": "2025-03-21 17:32:32 UTC",
      "updated_date": "2025-04-10 23:50:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:41:49.182406"
    },
    {
      "arxiv_id": "2504.13856v1",
      "title": "Towards Balancing Preference and Performance through Adaptive Personalized Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Silva",
        "Pradyumna Tambwekar",
        "Mariah Schrum",
        "Matthew Gombolay"
      ],
      "abstract": "As robots and digital assistants are deployed in the real world, these agents\nmust be able to communicate their decision-making criteria to build trust,\nimprove human-robot teaming, and enable collaboration. While the field of\nexplainable artificial intelligence (xAI) has made great strides to enable such\ncommunication, these advances often assume that one xAI approach is ideally\nsuited to each problem (e.g., decision trees to explain how to triage patients\nin an emergency or feature-importance maps to explain radiology reports). This\nfails to recognize that users have diverse experiences or preferences for\ninteraction modalities. In this work, we present two user-studies set in a\nsimulated autonomous vehicle (AV) domain. We investigate (1) population-level\npreferences for xAI and (2) personalization strategies for providing robot\nexplanations. We find significant differences between xAI modes (language\nexplanations, feature-importance maps, and decision trees) in both preference\n(p < 0.01) and performance (p < 0.05). We also observe that a participant's\npreferences do not always align with their performance, motivating our\ndevelopment of an adaptive personalization strategy to balance the two. We show\nthat this strategy yields significant performance gains (p < 0.05), and we\nconclude with a discussion of our findings and implications for xAI in\nhuman-robot interactions.",
      "tldr_zh": "这篇论文探讨了在人机互动中，通过自适应个性化解释（Adaptive Personalized Explainability）来平衡用户偏好和性能的问题，针对现有xAI（可解释人工智能）方法的局限性。研究者在模拟自动驾驶车辆（AV）领域开展了两个用户研究，比较了语言解释、特征重要性地图和决策树等xAI模式，结果显示这些模式在用户偏好（p < 0.01）和性能（p < 0.05）上存在显著差异，但偏好并不总是与性能一致。论文开发了一种自适应个性化策略，能够显著提升性能（p < 0.05），并讨论了其对xAI在人类-机器人互动中的启示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages, 19 figures, HRI 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.13856v1",
      "published_date": "2025-03-21 17:31:25 UTC",
      "updated_date": "2025-03-21 17:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:42:02.569548"
    },
    {
      "arxiv_id": "2503.17309v1",
      "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
      "title_zh": "LLM+MAP：使用大语言模型和规划域定义语言的双臂机器人任务规划",
      "authors": [
        "Kun Chu",
        "Xufeng Zhao",
        "Cornelius Weber",
        "Stefan Wermter"
      ],
      "abstract": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
      "tldr_zh": "本论文提出LLM+MAP框架，利用Large Language Models (LLMs) 和 Planning Domain Definition Language (PDDL) 来处理双臂机器人（bimanual robots）的长期任务规划问题，旨在解决空间和时间协调的复杂性。框架通过整合LLM的推理能力与多智能体规划(multi-agent planning)，实现有效的任务分解和分配，避免了LLMs在长时序推理中的错误和幻觉。实验结果显示，使用GPT-4o作为后端的LLM+MAP在各种模拟操纵任务上，比直接使用LLMs（如GPT-4o、V3、o1和R1）提高了成功率、减少了规划步骤，并提供了更可靠的机器人推理性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Code and video are available at https://github.com/Kchu/LLM-MAP",
      "pdf_url": "http://arxiv.org/pdf/2503.17309v1",
      "published_date": "2025-03-21 17:04:01 UTC",
      "updated_date": "2025-03-21 17:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:42:14.914641"
    },
    {
      "arxiv_id": "2503.17299v1",
      "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yashas Annadani",
        "Syrine Belakaria",
        "Stefano Ermon",
        "Stefan Bauer",
        "Barbara E Engelhardt"
      ],
      "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal\nsolutions given a dataset of designs and their objective values. In this work,\nwe propose a preference-guided diffusion model that generates Pareto-optimal\ndesigns by leveraging a classifier-based guidance mechanism. Our guidance\nclassifier is a preference model trained to predict the probability that one\ndesign dominates another, directing the diffusion model toward optimal regions\nof the design space. Crucially, this preference model generalizes beyond the\ntraining distribution, enabling the discovery of Pareto-optimal solutions\noutside the observed dataset. We introduce a novel diversity-aware preference\nguidance, augmenting Pareto dominance preference with diversity criteria. This\nensures that generated solutions are optimal and well-distributed across the\nobjective space, a capability absent in prior generative methods for offline\nmulti-objective optimization. We evaluate our approach on various continuous\noffline multi-objective optimization tasks and find that it consistently\noutperforms other inverse/generative approaches while remaining competitive\nwith forward/surrogate-based optimization methods. Our results highlight the\neffectiveness of classifier-guided diffusion models in generating diverse and\nhigh-quality solutions that approximate the Pareto front well.",
      "tldr_zh": "本研究提出了一种 preference-guided diffusion 模型，用于离线多目标优化（offline multi-objective optimization），旨在基于数据集生成 Pareto-optimal 设计。模型通过一个训练好的 preference 模型作为指导分类器，来预测设计间的支配关系，并引导 diffusion 过程向最优区域探索，同时引入 diversity-aware preference guidance 以确保生成的解不仅最优，还在目标空间中分布均匀。该方法能泛化到训练分布之外，发现数据集外的 Pareto 最优解，并在各种连续优化任务上表现优于其他生成方法，与前向/代理优化方法竞争，成功逼近 Pareto 前沿。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17299v1",
      "published_date": "2025-03-21 16:49:38 UTC",
      "updated_date": "2025-03-21 16:49:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:42:26.090457"
    },
    {
      "arxiv_id": "2503.17251v1",
      "title": "Breaking the Symmetries of Indistinguishable Objects",
      "title_zh": "打破不可区分对象的对称性",
      "authors": [
        "Ozgur Akgun",
        "Mun See Chang",
        "Ian P. Gent",
        "Christopher Jefferson"
      ],
      "abstract": "Indistinguishable objects often occur when modelling problems in constraint\nprogramming, as well as in other related paradigms. They occur when objects can\nbe viewed as being drawn from a set of unlabelled objects, and the only\noperation allowed on them is equality testing. For example, the golfers in the\nsocial golfer problem are indistinguishable. If we do label the golfers, then\nany relabelling of the golfers in one solution gives another valid solution.\nTherefore, we can regard the symmetric group of size $n$ as acting on a set of\n$n$ indistinguishable objects. In this paper, we show how we can break the\nsymmetries resulting from indistinguishable objects. We show how symmetries on\nindistinguishable objects can be defined properly in complex types, for example\nin a matrix indexed by indistinguishable objects. We then show how the\nresulting symmetries can be broken correctly. In Essence, a high-level\nmodelling language, indistinguishable objects are encapsulated in \"unnamed\ntypes\". We provide an implementation of complete symmetry breaking for unnamed\ntypes in Essence.",
      "tldr_zh": "该论文探讨了在约束编程和其他相关范式中，如何处理不可分辨对象的对称性问题，这些对象来自无标签集合，仅允许相等性测试，例如社交高尔夫问题中的高尔夫球手。作者定义了symmetric group对这些对象的群作用，并展示了如何在复杂类型（如由不可分辨对象索引的矩阵）中正确定义和打破这些对称性。最终，在高阶建模语言Essence中，通过封装unnamed types实现了完整的对称性打破，这有助于生成更有效的解决方案并避免冗余。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17251v1",
      "published_date": "2025-03-21 15:56:52 UTC",
      "updated_date": "2025-03-21 15:56:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:42:38.879934"
    },
    {
      "arxiv_id": "2503.17247v1",
      "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications",
      "title_zh": "KL3M Tokenizers：一种针对法律、金融和预处理应用的领域特定字符级别分词器家族",
      "authors": [
        "Michael J Bommarito",
        "Daniel Martin Katz",
        "Jillian Bommarito"
      ],
      "abstract": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization.",
      "tldr_zh": "本研究介绍了 KL3M tokenizers，一系列针对法律、金融和政府文本的领域特定和字符级分词器，以填补专业领域分词器研究的空白。第一个主要贡献是开发了领域特定 BPE tokenizers，如 kl3m-004-128k-cased 分词器，与 GPT-4o 和 Llama3 相比，在处理领域文档时减少 9-17% 的 token 使用量，且词汇量更小，对于专业术语甚至可减少高达 83% 的 token（法律术语）和 39% 的 token（金融术语）。第二个贡献是创建了字符级 BPE tokenizers（词汇量 4K、8K 和 16K），用于 OCR 后处理的文本修正任务，这些分词器保持一致的 token 边界，便于模型学习错误修正模式。这些分词器提升了专业文本处理的效率，帮助更多文本放入上下文窗口、降低计算需求，并通过 GitHub 和 Hugging Face 发布代码以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 7 tables, 3 figures; Source code available at\n  https://github.com/alea-institute/kl3m-tokenizer-paper",
      "pdf_url": "http://arxiv.org/pdf/2503.17247v1",
      "published_date": "2025-03-21 15:51:43 UTC",
      "updated_date": "2025-03-21 15:51:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:42:51.475442"
    },
    {
      "arxiv_id": "2503.17239v1",
      "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
      "title_zh": "SafeMERGE：通过选择性逐层模型合并在微调的大型语言模型中保留安全对齐",
      "authors": [
        "Aladin Djuhera",
        "Swanand Ravindra Kadhe",
        "Farhan Ahmed",
        "Syed Zawad",
        "Holger Boche"
      ],
      "abstract": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
      "tldr_zh": "这篇论文提出 SafeMERGE 框架，一种后微调方法，用于在微调 Large Language Models (LLMs) 时保留其 safety alignment，同时保持任务效用。框架通过选择性地合并微调模型和安全对齐模型的层，仅当这些层基于 cosine similarity 标准偏离安全行为时进行合并。实验评估显示，在 Llama-2-7B-Chat 和 Qwen-2-7B-Instruct 模型上应用于 GSM8K 和 PubMedQA 任务时，SafeMERGE 比其他基线方法显著减少有害输出，且不显著牺牲性能，有时甚至提升了整体表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17239v1",
      "published_date": "2025-03-21 15:44:09 UTC",
      "updated_date": "2025-03-21 15:44:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:43:02.279177"
    },
    {
      "arxiv_id": "2503.17237v2",
      "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
      "title_zh": "强基线：通过 YOLOv12 与 BoT-SORT-ReID 的多无人机跟踪",
      "authors": [
        "Yu-Hsi Chen"
      ],
      "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the well-established\nYOLOv5 with DeepSORT combination, we present a tracking framework built on\nYOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies.\nWe evaluate our approach following the 4th Anti-UAV Challenge metrics and reach\ncompetitive performance. Notably, we achieved strong results without using\ncontrast enhancement or temporal information fusion to enrich UAV features,\nhighlighting our approach as a \"Strong Baseline\" for multi-UAV tracking tasks.\nWe provide implementation details, in-depth experimental analysis, and a\ndiscussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
      "tldr_zh": "本研究针对热红外视频中多无人机的检测和跟踪问题，提出一个基于YOLOv12检测和BoT-SORT-ReID跟踪的简单框架，作为“Strong Baseline”。该框架采用定制的训练和推理策略，而未依赖对比增强或时间信息融合等技术，在第4届Anti-UAV Challenge的评估指标下取得了竞争性性能。实验结果显示，该方法在多UAV跟踪任务中表现出色，提供详细的实现细节和潜在改进建议，代码可从https://github.com/wish44165/YOLOv12-BoT-SORT-ReID获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.17237v2",
      "published_date": "2025-03-21 15:40:18 UTC",
      "updated_date": "2025-04-07 13:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:43:13.656111"
    },
    {
      "arxiv_id": "2503.17229v1",
      "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Albert Sawczyn",
        "Jakub Binkowski",
        "Denis Janiak",
        "Bogdan Gabrys",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
      "tldr_zh": "该研究提出 FactSelfCheck，一种基于黑盒采样的创新方法，用于检测大型语言模型 (LLMs) 中的事实级幻觉 (hallucination)。该方法将文本表示为由事实三元组组成的知识图 (knowledge graphs)，通过分析多个 LLM 响应中的事实一致性，计算细粒度的幻觉分数，而无需外部资源或训练数据。与现有句子级或段落级方法相比，FactSelfCheck 提供更详细的见解，并在幻觉修正上表现出色，比基线模型提高了 35% 的 factual content，而句子级 SelfCheckGPT 只提升了 8%。这种细粒度检测有助于更精确地识别和修正幻觉内容。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.17229v1",
      "published_date": "2025-03-21 15:32:24 UTC",
      "updated_date": "2025-03-21 15:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:43:25.799322"
    },
    {
      "arxiv_id": "2503.17224v1",
      "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Giacomo Savazzi",
        "Eugenio Lomurno",
        "Cristian Sbrolli",
        "Agnese Chiatti",
        "Matteo Matteucci"
      ],
      "abstract": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.",
      "tldr_zh": "这篇论文探讨了 Neuro-Symbolic 方法在合成图像数据集生成中的应用，旨在通过场景图(Scene Graphs)作为结构化符号表示来编码关系约束，从而提升合成数据的质量，特别是针对 Scene Graph Generation 模型。研究发现，这种 Neuro-Symbolic 条件化方法在数据集增强中显著提高了模型性能，标准 Recall 指标提升了 2.59%，而 No Graph Constraint Recall 指标提升了 2.83%。这项工作证明了将 Neuro-Symbolic 与生成方法结合，能够产生具有互补结构信息的合成数据，帮助克服数据稀缺问题，尤其在复杂视觉推理任务中。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17224v1",
      "published_date": "2025-03-21 15:26:16 UTC",
      "updated_date": "2025-03-21 15:26:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:43:38.930903"
    },
    {
      "arxiv_id": "2503.17222v1",
      "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
      "title_zh": "利用大型语言模型自动化心血管事件判定",
      "authors": [
        "Sonish Sivarajkumar",
        "Kimia Ameri",
        "Chuqin Li",
        "Yanshan Wang",
        "Min Jiang"
      ],
      "abstract": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
      "tldr_zh": "本研究提出了一种使用大型语言模型 (LLMs) 的新框架，旨在自动化临床试验中心血管事件（如心脏病发作和中风）的裁决，以解决传统手工方法耗时、资源密集和易变异的问题。该框架采用两阶段方法：首先，通过LLMs管道从非结构化临床数据中提取事件信息；其次，利用基于Tree of Thoughts的LLMs裁决过程，并遵循临床终点委员会 (CEC) 指南。实验结果显示，该框架在事件提取上达到F1-score 0.82，在裁决上达到准确率0.68，并引入了CLEART score作为新型指标来评估AI生成的临床推理质量。该方法可显著减少裁决时间和成本，同时提升结果的一致性和标准化，促进心血管疗法风险的快速识别和缓解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17222v1",
      "published_date": "2025-03-21 15:25:53 UTC",
      "updated_date": "2025-03-21 15:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:43:49.725138"
    },
    {
      "arxiv_id": "2503.17213v1",
      "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction",
      "title_zh": "PP-DocLayout：一个统一的文档布局检测模型，用于加速大规模数据构建",
      "authors": [
        "Ting Sun",
        "Cheng Cui",
        "Yuning Du",
        "Yi Liu"
      ],
      "abstract": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
      "tldr_zh": "本研究提出 PP-DocLayout，一种统一的文档布局检测模型，旨在解决现有模型在泛化性、复杂布局处理和实时性能方面的挑战，从而加速大规模数据构建。该模型能够高精度识别23种布局区域（如标题、文本块、表格和公式），并提供三种规模版本：PP-DocLayout-L基于 RT-DETR-L 检测器，达到90.4% mAP@0.5和13.4 ms/页的推理时间；PP-DocLayout-M提供平衡性能，mAP@0.5为75.2%和12.7 ms/页；PP-DocLayout-S则专注于高效性，在T4 GPU上仅需8.1 ms/页，在CPU上为14.5 ms。通过这些创新，PP-DocLayout 推进了文档布局分析的现状，并为构建高质量训练数据支持文档智能和多模态AI系统的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX",
      "pdf_url": "http://arxiv.org/pdf/2503.17213v1",
      "published_date": "2025-03-21 15:20:47 UTC",
      "updated_date": "2025-03-21 15:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:44:02.973106"
    },
    {
      "arxiv_id": "2503.21794v1",
      "title": "Architecture of Information",
      "title_zh": "信息架构",
      "authors": [
        "Yurii Parzhyn"
      ],
      "abstract": "The paper explores an approach to constructing energy landscapes of a formal\nneuron and multilayer artificial neural networks (ANNs). Their analysis makes\nit possible to determine the conceptual limitations of both classification ANNs\n(e.g., MLP or CNN) and generative ANN models. The study of informational and\nthermodynamic entropy in formal neuron and ANN models leads to the conclusion\nabout the energetic nature of informational entropy. The application of the\nGibbs free energy concept allows representing the output information of ANNs as\nthe structured part of enthalpy. Modeling ANNs as energy systems makes it\npossible to interpret the structure of their internal energy as an internal\nmodel of the external world, which self-organizes based on the interaction of\nthe system's internal energy components. The control of the self-organization\nand evolution process of this model is carried out through an energy function\n(analogous to the Lyapunov function) based on reduction operators. This makes\nit possible to introduce a new approach to constructing self-organizing and\nevolutionary ANNs with direct learning, which does not require additional\nexternal algorithms. The presented research makes it possible to formulate a\nformal definition of information in terms of the interaction processes between\nthe internal and external energy of the system.",
      "tldr_zh": "这篇论文探讨了构建正式神经元和多层人工神经网络 (ANNs) 的能量景观，以分析分类ANNs（如MLP或CNN）和生成模型的概念限制。作者通过研究信息熵和热力学熵，得出信息熵具有能量本质的结论，并应用Gibbs自由能概念，将ANNs的输出信息视为焓的结构化部分。论文进一步将ANNs建模为能量系统，引入基于归约算子的能量函数（类似于Lyapunov函数）来控制其自组织和演化过程，从而提出一种直接学习的自组织ANNs新方法，并正式定义信息为系统内部和外部能量交互的过程。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT",
        "H.1.1; I.2.0"
      ],
      "primary_category": "cs.NE",
      "comment": "81 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21794v1",
      "published_date": "2025-03-21 14:48:41 UTC",
      "updated_date": "2025-03-21 14:48:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:44:15.856119"
    },
    {
      "arxiv_id": "2503.17195v1",
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "title_zh": "TreeSynth：通过树引导的子空间划分从零开始合成多样数据",
      "authors": [
        "Sheng Wang",
        "Pengan Chen",
        "Jingqi Zhou",
        "Qintong Li",
        "Jingwei Dong",
        "Jiahui Gao",
        "Boyang Xue",
        "Jiyue Jiang",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
      "tldr_zh": "该研究提出TreeSynth，一种基于树引导子空间划分的框架，用于从零开始合成高质量、多样化数据集，以解决现有LLMs数据合成方法受限于种子数据偏差和生成控制不足的问题。TreeSynth通过递归分区数据空间，构建一个层次化树结构，从根节点逐步划分成相互排斥的原子子空间（叶节点），并根据每个子空间的属性生成全面覆盖的数据。实验结果显示，TreeSynth在数据多样性上比基线方法提升最多45.2%，在下游任务性能上提升最多17.6%，为无需人工干预的可扩展数据合成提供了一个高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17195v1",
      "published_date": "2025-03-21 14:43:23 UTC",
      "updated_date": "2025-03-21 14:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:44:26.289078"
    },
    {
      "arxiv_id": "2503.17184v1",
      "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection",
      "title_zh": "D2Fusion：双域融合与特征叠加用于 Deepfake 检测",
      "authors": [
        "Xueqi Qiu",
        "Xingyu Miao",
        "Fan Wan",
        "Haoran Duan",
        "Tejal Shah",
        "Varun Ojhab",
        "Yang Longa",
        "Rajiv Ranjan"
      ],
      "abstract": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.",
      "tldr_zh": "这篇论文提出 D2Fusion 框架，用于 Deepfake 检测，通过双域融合（spatial 和 frequency 领域）及特征叠加（feature superposition）策略，解决现有方法在不同领域特征交互不足的问题。具体而言，它引入 bi-directional attention module 在空间域精确定位局部伪造线索，以及 fine-grained frequency attention module 在频率域捕获全局微妙信息如纹理或边缘。最终，该方法在五个公共 Deepfake 数据集上显著超越 SOTA 方法，在捕捉各种伪造操作和真实场景异常方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17184v1",
      "published_date": "2025-03-21 14:31:33 UTC",
      "updated_date": "2025-03-21 14:31:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:44:38.364021"
    },
    {
      "arxiv_id": "2503.17181v1",
      "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
      "title_zh": "翻译失败",
      "authors": [
        "Lukas Twist",
        "Jie M. Zhang",
        "Mark Harman",
        "Don Syme",
        "Joost Noppen",
        "Detlef Nauck"
      ],
      "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
      "tldr_zh": "这篇论文研究了大型语言模型(LLMs) 在编程语言和库选择上的偏见，强调这些偏见可能导致软件可靠性和安全性问题。研究通过评估八个不同 LLMs 在各种编码任务中的表现，包括基准测试和生成新项目初始代码，发现 LLMs 强烈偏好 Python，在90%-97%的语言无关问题中使用它，即使在不适合的场景中仍占58%的首选。进一步发现，LLMs 在83%的项目初始化任务中与自身语言推荐相矛盾，且对知名库的偏见会阻碍新开源项目的发现性。论文呼吁改进 LLMs 的适应性和开发机制来缓解这些编程语言和库偏见。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.17181v1",
      "published_date": "2025-03-21 14:29:35 UTC",
      "updated_date": "2025-03-21 14:29:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:44:51.149469"
    },
    {
      "arxiv_id": "2503.17167v2",
      "title": "DiTEC-WDN: A Large-Scale Dataset of Hydraulic Scenarios across Multiple Water Distribution Networks",
      "title_zh": "DiTEC-WDN：跨越多个供水网络的大规模水力场景数据集",
      "authors": [
        "Huy Truong",
        "Andrés Tello",
        "Alexander Lazovik",
        "Victoria Degeler"
      ],
      "abstract": "Privacy restrictions hinder the sharing of real-world Water Distribution\nNetwork (WDN) models, limiting the application of emerging data-driven machine\nlearning, which typically requires extensive observations. To address this\nchallenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique\nscenarios simulated over either short-term (24 hours) or long-term (1 year)\nperiods. We constructed this dataset using an automated pipeline that optimizes\ncrucial parameters (e.g., pressure, flow rate, and demand patterns),\nfacilitates large-scale simulations, and records discrete, synthetic but\nhydraulically realistic states under standard conditions via rule validation\nand post-hoc analysis. With a total of 228 million generated graph-based\nstates, DiTEC-WDN can support a variety of machine-learning tasks, including\ngraph-level, node-level, and link-level regression, as well as time-series\nforecasting. This contribution, released under a public license, encourages\nopen scientific research in the critical water sector, eliminates the risk of\nexposing sensitive data, and fulfills the need for a large-scale water\ndistribution network benchmark for study comparisons and scenario analysis.",
      "tldr_zh": "该研究提出DiTEC-WDN数据集，以解决隐私限制导致的真实Water Distribution Network (WDN)模型共享难题，该数据集包含36,000个独特液压场景，涵盖短期（24小时）和长期（1年）模拟。数据集通过自动管道优化参数（如压力、流量和需求模式），生成2.28亿个合成但水力学真实的基于图的states，并通过规则验证和后验分析确保可靠性。DiTEC-WDN支持多种机器学习任务，包括graph-level、node-level和link-level回归，以及time-series forecasting，为水行业公开研究提供基准，促进场景分析而不暴露敏感数据。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Nature Scientific Data. Huy Truong and Andr\\'es Tello\n  contributed equally to this work. For the dataset, see\n  https://huggingface.co/datasets/rugds/ditec-wdn",
      "pdf_url": "http://arxiv.org/pdf/2503.17167v2",
      "published_date": "2025-03-21 14:14:03 UTC",
      "updated_date": "2025-03-24 14:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:45:02.371083"
    },
    {
      "arxiv_id": "2503.18968v2",
      "title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow",
      "title_zh": "MedAgent-Pro",
      "authors": [
        "Ziyue Wang",
        "Junde Wu",
        "Linghan Cai",
        "Chang Han Low",
        "Xihong Yang",
        "Qiaxuan Li",
        "Yueming Jin"
      ],
      "abstract": "In modern medicine, clinical diagnosis relies on the comprehensive analysis\nof primarily textual and visual data, drawing on medical expertise to ensure\nsystematic and rigorous reasoning. Recent advances in large Vision-Language\nModels (VLMs) and agent-based methods hold great potential for medical\ndiagnosis, thanks to the ability to effectively integrate multi-modal patient\ndata. However, they often provide direct answers and draw empirical-driven\nconclusions without quantitative analysis, which reduces their reliability and\nclinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm\nthat follows the diagnosis principle in modern medicine, to decouple the\nprocess into sequential components for step-by-step, evidence-based reasoning.\nOur MedAgent-Pro workflow presents a hierarchical diagnostic structure to\nmirror this principle, consisting of disease-level standardized plan generation\nand patient-level personalized step-by-step reasoning. To support disease-level\nplanning, an RAG-based agent is designed to retrieve medical guidelines to\nensure alignment with clinical standards. For patient-level reasoning, we\npropose to integrate professional tools such as visual models to enable\nquantitative assessments. Meanwhile, we propose to verify the reliability of\neach step to achieve evidence-based diagnosis, enforcing rigorous logical\nreasoning and a well-founded conclusion. Extensive experiments across a wide\nrange of anatomical regions, imaging modalities, and diseases demonstrate the\nsuperiority of MedAgent-Pro to mainstream VLMs, agentic systems and\nstate-of-the-art expert models. Ablation studies and human evaluation by\nclinical experts further validate its robustness and clinical relevance. Code\nis available at https://github.com/jinlab-imvr/MedAgent-Pro.",
      "tldr_zh": "该研究提出MedAgent-Pro，一种基于代理式工作流的推理范式，旨在通过循序渐进的证据支持实现多模态医疗诊断，解决现有VLMs和代理方法缺乏定量分析的问题。MedAgent-Pro采用层次化结构，包括疾病级别的RAG-based agent检索医疗指南生成标准化计划，以及患者级别的个性化推理整合视觉模型进行定量评估和可靠性验证。实验结果显示，该系统在各种解剖区域、成像模态和疾病上优于主流VLMs、代理系统和最先进专家模型，经消融研究和临床专家评估证实其稳健性和临床相关性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18968v2",
      "published_date": "2025-03-21 14:04:18 UTC",
      "updated_date": "2025-05-22 07:23:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:45:14.112819"
    },
    {
      "arxiv_id": "2503.17132v2",
      "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Siyuan Yang",
        "Shilin Lu",
        "Shizheng Wang",
        "Meng Hwa Er",
        "Zengwei Zheng",
        "Alex C. Kot"
      ],
      "abstract": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
      "tldr_zh": "本论文探讨了 SNNs 与事件相机在隐私保护的人类动作识别（HAR）中的协同作用，解决了 SNNs 处理长期时间信息的能力不足问题。作者提出两种新框架：TS-SNN 通过将动作分成较短段来提取长期时间信息，以及 3D-SNN 通过使用 3D 组件替换 2D 元素以提升时间数据传输。研究还创建了 FallingDetection-CeleX 数据集，使用高分辨率 CeleX-V 相机记录 7 种动作。实验结果表明，这些框架在新数据集和其他三个神经形态数据集上超过了现有 SNN 方法，在处理长程时间信息的事件-based HAR 方面表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17132v2",
      "published_date": "2025-03-21 13:31:16 UTC",
      "updated_date": "2025-03-27 11:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:45:26.957410"
    },
    {
      "arxiv_id": "2503.17125v5",
      "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Chan Kim",
        "Seung-Woo Seo",
        "Seong-Woo Kim"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.",
      "tldr_zh": "本文提出 LaMOuR，一种利用语言模型进行强化学习中 out-of-distribution (OOD) 状态恢复的方法，以解决 Deep Reinforcement Learning (DRL) 在遇到 OOD 时导致任务失败的问题。LaMOuR 通过 LVLMs 的图像描述、逻辑推理和代码生成能力，生成密集奖励代码引导代理返回 in-distribution 状态，而不依赖不确定性估计。实验结果表明，LaMOuR 在多样化运动任务中显著提升了恢复效率，并在复杂环境中如人形运动和移动操作中实现了有效泛化。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17125v5",
      "published_date": "2025-03-21 13:20:39 UTC",
      "updated_date": "2025-03-28 06:34:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:45:38.975095"
    },
    {
      "arxiv_id": "2503.17116v1",
      "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Rossetto",
        "Werner Bailer",
        "Duc-Tien Dang-Nguyen",
        "Graham Healy",
        "Björn Þór Jónsson",
        "Onanong Kongmeesub",
        "Hoang-Bao Le",
        "Stevan Rudinac",
        "Klaus Schöffmann",
        "Florian Spiess",
        "Allie Tran",
        "Minh-Triet Tran",
        "Quang-Linh Tran",
        "Cathal Gurrin"
      ],
      "abstract": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.",
      "tldr_zh": "本文介绍了CASTLE 2024数据集，该数据集旨在推进多模态理解研究，通过结合ego-centric（第一人称）和exo-centric（第三人称）视角的视频、音频，以及其他传感器数据和辅助信息，共从15个时间对齐来源收集数据。与现有数据集不同，CASTLE 2024由志愿者在固定地点记录四天，涵盖10个参与者的视角和5个固定摄像头，总计超过600小时的UHD视频（50 fps），且无任何部分审查如模糊面部或扭曲音频。该数据集现已公开可用，可从https://castle-dataset.github.io/获取，为多视角分析提供宝贵资源。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.MM",
      "comment": "7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.17116v1",
      "published_date": "2025-03-21 13:01:07 UTC",
      "updated_date": "2025-03-21 13:01:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:45:50.739466"
    },
    {
      "arxiv_id": "2503.17095v1",
      "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
      "title_zh": "FFaceNeRF：神经辐射场中的少样本人脸编辑",
      "authors": [
        "Kwan Yun",
        "Chaelin Kim",
        "Hangyeul Shin",
        "Junyong Noh"
      ],
      "abstract": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.",
      "tldr_zh": "本研究提出 FFaceNeRF，一种基于 Neural Radiance Fields (NeRF) 的少样本面部编辑技术，旨在解决现有方法依赖预训练分割掩码导致的用户控制有限和数据需求过高的问题。该方法引入 geometry adapter with feature injection 来有效操纵面部几何属性，并采用 latent mixing for tri-plane augmentation 技术，仅需少量样本即可实现快速模型适应和自定义掩码布局，适用于个性化医疗成像或创意编辑。实验结果显示，FFaceNeRF 在灵活性、控制性和生成图像质量上超越现有掩码-based 面部编辑方法，为高保真 3D 面部编辑开辟新路径。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "68T45, 68U05",
        "I.3.3; I.3.8"
      ],
      "primary_category": "cs.GR",
      "comment": "CVPR2025, 11 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17095v1",
      "published_date": "2025-03-21 12:24:58 UTC",
      "updated_date": "2025-03-21 12:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:46:02.289089"
    },
    {
      "arxiv_id": "2503.17089v1",
      "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Tiarna Lee",
        "Esther Puyol-Antón",
        "Bram Ruijsink",
        "Miaojing Shi",
        "Andrew P. King"
      ],
      "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
      "tldr_zh": "这篇论文探讨了 AI 在心脏磁共振 (CMR) 图像分割中的种族偏见问题，特别是针对 Black 和 White 受试者的不平衡数据集训练模型。研究评估了 oversampling、importance reweighing 和 Group DRO 等偏见缓解方法，以及它们的组合，以提高模型公平性。结果表明，oversampling 最有效，能显著提升 Black 受试者的性能而不显著降低 White 受试者的表现，而 Group DRO 有轻微改善，importance reweighing 反而降低了 Black 受试者的性能；此外，使用裁剪图像结合 oversampling 进一步减少了偏见。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17089v1",
      "published_date": "2025-03-21 12:17:43 UTC",
      "updated_date": "2025-03-21 12:17:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:46:15.277944"
    },
    {
      "arxiv_id": "2503.17085v1",
      "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
      "title_zh": "通过标准心理诊断实现 AI 代理的确定性个性表达",
      "authors": [
        "J. M. Diederik Kruijssen",
        "Nicholas Emmons"
      ],
      "abstract": "Artificial intelligence (AI) systems powered by large language models have\nbecome increasingly prevalent in modern society, enabling a wide range of\napplications through natural language interaction. As AI agents proliferate in\nour daily lives, their generic and uniform expressiveness presents a\nsignificant limitation to their appeal and adoption. Personality expression\nrepresents a key prerequisite for creating more human-like and distinctive AI\nsystems. We show that AI models can express deterministic and consistent\npersonalities when instructed using established psychological frameworks, with\nvarying degrees of accuracy depending on model capabilities. We find that more\nadvanced models like GPT-4o and o1 demonstrate the highest accuracy in\nexpressing specified personalities across both Big Five and Myers-Briggs\nassessments, and further analysis suggests that personality expression emerges\nfrom a combination of intelligence and reasoning capabilities. Our results\nreveal that personality expression operates through holistic reasoning rather\nthan question-by-question optimization, with response-scale metrics showing\nhigher variance than test-scale metrics. Furthermore, we find that model\nfine-tuning affects communication style independently of personality expression\naccuracy. These findings establish a foundation for creating AI agents with\ndiverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while\nadditionally enabling a broader range of more unique AI agents. The ability to\nquantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically\ndesigned AI.",
      "tldr_zh": "这篇论文探讨了如何通过标准心理诊断框架（如 Big Five 和 Myers-Briggs）使 AI 代理实现确定性和一致的个性表达，以提升 AI 的吸引力及采用率。研究发现，先进模型如 GPT-4o 和 o1 在个性表达准确性方面表现出色，这主要依赖于整体推理能力而非逐题优化，且模型微调会独立影响沟通风格。结果揭示，AI 的个性表达可增强人机交互在教育和医疗等领域的应用，并为开发更具多样性和可信赖的 AI 系统提供基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 8 figures, 4 tables; appeared in ADI (March 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.17085v1",
      "published_date": "2025-03-21 12:12:05 UTC",
      "updated_date": "2025-03-21 12:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:46:26.710254"
    },
    {
      "arxiv_id": "2503.17070v1",
      "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning",
      "title_zh": "Non-IID 数据影响在联邦学习中的彻底评估",
      "authors": [
        "Daniel M. Jimenez-Gutierrez",
        "Mehrdad Hassanzadeh",
        "Aris Anagnostopoulos",
        "Ioannis Chatzigiannakis",
        "Andrea Vitaletti"
      ],
      "abstract": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.",
      "tldr_zh": "这篇论文全面评估了非独立同分布(non-IID)数据对联邦学习(FL)的冲击，通过使用Hellinger Distance(HD)量化客户端间的分布差异，并对标签、特征、数量和时空偏差等四种non-IID类型进行了系统基准测试。研究首次对时空偏差在FL中的影响进行了全面分析，结果显示标签和时空偏差会导致模型性能显著下降，尤其在特定HD阈值和极端non-IID条件下。论文提供了针对数据异质性的实用推荐，为未来FL研究奠定了坚实基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17070v1",
      "published_date": "2025-03-21 11:53:36 UTC",
      "updated_date": "2025-03-21 11:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:46:39.908033"
    },
    {
      "arxiv_id": "2503.17069v1",
      "title": "PVChat: Personalized Video Chat with One-Shot Learning",
      "title_zh": "PVChat：基于一-shot学习的个性化视频聊天",
      "authors": [
        "Yufei Shi",
        "Weilong Yan",
        "Gang Xu",
        "Yumeng Li",
        "Yuchen Li",
        "Zhenxi Li",
        "Fei Richard Yu",
        "Ming Li",
        "Si Yong Yeo"
      ],
      "abstract": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
      "tldr_zh": "本研究针对视频大型语言模型（ViLLMs）在身份感知理解（如识别特定人物的活动）上的不足，提出PVChat，一种基于one-shot learning的个性化视频聊天框架，支持从单个视频进行主体感知问答（QA）。该框架通过优化Mixture-of-Heads (MoH)增强的ViLLM，并采用自动化增强管道生成多样化数据集（包括存在、外观、动作和位置查询），结合ReLU Routing MoH注意力机制、Smooth Proximity Regularization和Head Activation Enhancement等创新目标，实现渐进式图像到视频学习。实验结果显示，PVChat在医疗场景、TV系列、动漫和真实世界视频等数据集上，显著优于现有ViLLMs，在个性化特征理解方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17069v1",
      "published_date": "2025-03-21 11:50:06 UTC",
      "updated_date": "2025-03-21 11:50:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:46:51.986830"
    },
    {
      "arxiv_id": "2503.17061v1",
      "title": "Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems",
      "title_zh": "Replay4NCL：一种高效的基于记忆回放的方法，用于嵌入式 AI 系统中的神经形态持续学习",
      "authors": [
        "Mishal Fatima Minhas",
        "Rachmad Vidya Wicaksana Putra",
        "Falah Awwad",
        "Osman Hasan",
        "Muhammad Shafique"
      ],
      "abstract": "Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural\nNetworks (SNNs) to enable continual learning (CL) capabilities for AI systems\nto adapt to dynamically changing environments. Currently, the state-of-the-art\nemploy a memory replay-based method to maintain the old knowledge. However,\nthis technique relies on long timesteps and compression-decompression steps,\nthereby incurring significant latency and energy overheads, which are not\nsuitable for tightly-constrained embedded AI systems (e.g., mobile\nagents/robotics). To address this, we propose Replay4NCL, a novel efficient\nmemory replay-based methodology for enabling NCL in embedded AI systems.\nSpecifically, Replay4NCL compresses the latent data (old knowledge), then\nreplays them during the NCL training phase with small timesteps, to minimize\nthe processing latency and energy consumption. To compensate the information\nloss from reduced spikes, we adjust the neuron threshold potential and learning\nrate settings. Experimental results on the class-incremental scenario with the\nSpiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old\nknowledge with Top-1 accuracy of 90.43% compared to 86.22% from the\nstate-of-the-art, while effectively learning new tasks, achieving 4.88x latency\nspeed-up, 20% latent memory saving, and 36.43% energy saving. These results\nhighlight the potential of our Replay4NCL methodology to further advances NCL\ncapabilities for embedded AI systems.",
      "tldr_zh": "本文提出 Replay4NCL，一种高效的内存重放方法，用于在嵌入式 AI 系统中实现 Neuromorphic Continual Learning (NCL)，以解决传统方法在 Spiking Neural Networks (SNNs) 训练中导致的高延迟和能量消耗问题。Replay4NCL 通过压缩潜在数据（old knowledge）并在小时间步内重放，同时调整神经元阈值和学习率，以最小化信息损失和资源开销。实验结果显示，在 Spiking Heidelberg Digits (SHD) 数据集的类增量场景中，该方法将 Top-1 准确率提升至 90.43%（比现有方法高 4.21%），并实现 4.88x 延迟加速、20% 内存节省和 36.43% 能量节省，从而为嵌入式 AI 系统的持续学习能力提供显著改进。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at the 62th Design Automation Conference (DAC) 2025, June\n  2025, San Francisco, CA, USA",
      "pdf_url": "http://arxiv.org/pdf/2503.17061v1",
      "published_date": "2025-03-21 11:33:22 UTC",
      "updated_date": "2025-03-21 11:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:47:03.951145"
    },
    {
      "arxiv_id": "2503.17055v1",
      "title": "Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery",
      "title_zh": "数据驱动的电动车充电站放置优化：利用因果发现",
      "authors": [
        "Julius Stephan Junker",
        "Rong Hu",
        "Ziyue Li",
        "Wolfgang Ketter"
      ],
      "abstract": "This paper addresses the critical challenge of optimizing electric vehicle\ncharging station placement through a novel data-driven methodology employing\ncausal discovery techniques. While traditional approaches prioritize economic\nfactors or power grid constraints, they often neglect empirical charging\npatterns that ultimately determine station utilization. We analyze extensive\ncharging data from Palo Alto and Boulder (337,344 events across 100 stations)\nto uncover latent relationships between station characteristics and\nutilization. Applying structural learning algorithms (NOTEARS and DAGMA) to\nthis data reveals that charging demand is primarily determined by three\nfactors: proximity to amenities, EV registration density, and adjacency to\nhigh-traffic routes. These findings, consistent across multiple algorithms and\nurban contexts, challenge conventional infrastructure distribution strategies.\nWe develop an optimization framework that translates these insights into\nactionable placement recommendations, identifying locations likely to\nexperience high utilization based on the discovered dependency structures. The\nresulting site selection model prioritizes strategic clustering in high-amenity\nareas with substantial EV populations rather than uniform spatial distribution.\nOur approach contributes a framework that integrates empirical charging\nbehavior into infrastructure planning, potentially enhancing both station\nutilization and user convenience. By focusing on data-driven insights instead\nof theoretical distribution models, we provide a more effective strategy for\nexpanding charging networks that can adjust to various stages of EV market\ndevelopment.",
      "tldr_zh": "本论文提出了一种基于数据驱动的方法，使用因果发现技术优化电动车（EV）充电站的放置位置，以解决传统方法忽略实际充电模式的问题。通过分析Palo Alto和Boulder的337,344个充电事件数据，并应用NOTEARS和DAGMA结构学习算法，研究发现充电需求主要受三个因素影响：proximity to amenities、EV registration density和adjacency to high-traffic routes。这些发现挑战了传统的均匀分布策略，并指导开发了一个优化框架，推荐在高设施区和高EV人口区进行战略聚类，从而提高充电站利用率和用户便利性。该方法为EV市场发展提供了一个更有效的、可适应性强的基础设施规划方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review of IEEE CASE 2025; This is also the master thesis\n  project from Julius supervised by Dr. Ziyue Li",
      "pdf_url": "http://arxiv.org/pdf/2503.17055v1",
      "published_date": "2025-03-21 11:15:02 UTC",
      "updated_date": "2025-03-21 11:15:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:47:14.528087"
    },
    {
      "arxiv_id": "2503.17046v1",
      "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
      "title_zh": "HAPI：一种从人类偏好学习机器人面部表情的模型",
      "authors": [
        "Dongsheng Yang",
        "Qianying Liu",
        "Wataru Sato",
        "Takashi Minato",
        "Chaoran Liu",
        "Shin'ya Nishida"
      ],
      "abstract": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.",
      "tldr_zh": "这篇论文提出HAPI模型，一种基于人类偏好的学习排序框架，用于生成更真实自然的机器人面部表情，以解决传统手工和自动方法在表达细腻度和感知整合方面的不足。具体地，该模型通过收集配对比较注释数据，并采用Siamese RankNet-based的方法来优化表情评估。实验结果显示，在一个35-DOF安卓平台上，使用Bayesian Optimization和在线调查，HAPI生成的Anger、Happiness和Surprise表情比基线和专家设计方法更具真实性和社会共鸣，有效桥接了人类偏好与模型预测的差距。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17046v1",
      "published_date": "2025-03-21 11:04:01 UTC",
      "updated_date": "2025-03-21 11:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:47:26.241156"
    },
    {
      "arxiv_id": "2503.17039v2",
      "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
      "title_zh": "翻译失败",
      "authors": [
        "Jeremy Barnes",
        "Naiara Perez",
        "Alba Bonet-Jover",
        "Begoña Altuna"
      ],
      "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
      "tldr_zh": "该研究评估了自动文本摘要指标和LLM-as-a-Judge模型在西班牙语和巴斯克语上的有效性，填补了以往英语主导研究的空白。研究者创建了新数据集BASSE，收集了2040个摘要的人类判断（包括coherence、consistency、fluency、relevance和5W1H五个标准），这些摘要由人工或五种LLM生成。结果显示，专有LLM判断模型与人类判断的相关性最高，其次是特定标准的自动指标，而开源LLM模型表现较差。该研究公开了BASSE数据集、代码，以及首个大规模巴斯克语摘要数据集（包含22,525篇新闻文章及其子标题），为多语种摘要评估提供了宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17039v2",
      "published_date": "2025-03-21 10:52:20 UTC",
      "updated_date": "2025-04-14 08:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:47:39.705121"
    },
    {
      "arxiv_id": "2503.17426v2",
      "title": "Enhanced Smart Contract Reputability Analysis using Multimodal Data Fusion on Ethereum",
      "title_zh": "使用多模态数据融合增强以太坊智能合约声誉分析",
      "authors": [
        "Cyrus Malik",
        "Josef Bajada",
        "Joshua Ellul"
      ],
      "abstract": "The evaluation of smart contract reputability is essential to foster trust in\ndecentralized ecosystems. However, existing methods that rely solely on code\nanalysis or transactional data, offer limited insight into evolving\ntrustworthiness. We propose a multimodal data fusion framework that integrates\ncode features with transactional data to enhance reputability prediction. Our\nframework initially focuses on AI-based code analysis, utilizing GAN-augmented\nopcode embeddings to address class imbalance, achieving 97.67% accuracy and a\nrecall of 0.942 in detecting illicit contracts, surpassing traditional\noversampling methods. This forms the crux of a reputability-centric fusion\nstrategy, where combining code and transactional data improves recall by 7.25%\nover single-source models, demonstrating robust performance across validation\nsets. By providing a holistic view of smart contract behaviour, our approach\nenhances the model's ability to assess reputability, identify fraudulent\nactivities, and predict anomalous patterns. These capabilities contribute to\nmore accurate reputability assessments, proactive risk mitigation, and enhanced\nblockchain security.",
      "tldr_zh": "该研究提出了一种多模态数据融合框架，用于提升Ethereum智能合约的声誉分析，通过整合代码特征和交易数据来弥补现有方法的局限性。框架采用AI-based代码分析，包括GAN-augmented opcode embeddings来处理类别不平衡问题，实现了97.67%的准确率和0.942的召回率，在检测非法合约上优于传统过采样方法。融合策略将代码和交易数据结合，提高了召回率7.25%，并在验证集上表现出色。该方法提供对智能合约行为的整体视图，支持声誉评估、欺诈识别、异常预测，并促进区块链安全和风险缓解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17426v2",
      "published_date": "2025-03-21 10:45:17 UTC",
      "updated_date": "2025-03-29 12:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:47:51.082108"
    },
    {
      "arxiv_id": "2503.17034v1",
      "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Lloyd-Brown",
        "Susan Francis",
        "Caroline Hoad",
        "Penny Gowland",
        "Karen Mullinger",
        "Andrew French",
        "Xin Chen"
      ],
      "abstract": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.",
      "tldr_zh": "该论文针对医疗图像分割中训练样本选择的问题，提出了一种结合注意力代表样本选择策略和平衡批次训练的方法，以解决随机选择导致的模型性能不佳问题。具体而言，该方法使用 prototypical contrasting learning 和 clustering 提取代表性和多样的样本进行标注，并引入 unsupervised balanced batch dataloading 来提升模型在最小监督数据下的学习效果。在 ISIC 2018 皮肤病变数据集上实验表明，该方法在低标注预算场景下，性能优于现有最先进的数据采样方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17034v1",
      "published_date": "2025-03-21 10:42:22 UTC",
      "updated_date": "2025-03-21 10:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:48:02.804135"
    },
    {
      "arxiv_id": "2503.17030v1",
      "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Snigdha Paul",
        "Sambit Mallick",
        "Anindya Sen"
      ],
      "abstract": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.",
      "tldr_zh": "本研究探讨了使用位平面切片（Bit Plane Slicing）的部分去噪技术，以提升骨折识别的准确性，通过比较深度学习模型 DenseNet 和手工特征提取方法（如 Decision Tree 和 Random Forest）。研究实验了多种图像表示，包括原图、LSB 和 MSB 位平面组合、全去噪图像以及部分去噪图像，旨在分析信噪比（SNR）和分类准确率，并识别最具信息性的位平面。结果显示，使用 Random Forest 分类器，部分去噪图像的测试准确率达到 95.61%，优于其他表示方式。该方法为骨折识别提供高效的预处理、特征提取和分类策略，有望改善医疗诊断准确性和患者护理。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17030v1",
      "published_date": "2025-03-21 10:39:21 UTC",
      "updated_date": "2025-03-21 10:39:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:48:15.296267"
    },
    {
      "arxiv_id": "2503.17025v1",
      "title": "A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition",
      "title_zh": "贝叶斯网络结构和参数学习软件包指南 -- 2025 版",
      "authors": [
        "Joverlyn Gaudillo",
        "Nicole Astrologo",
        "Fabio Stella",
        "Enzo Acerbi",
        "Francesco Canonaco"
      ],
      "abstract": "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
      "tldr_zh": "这篇论文提供了2025年版的指南，针对Bayesian Networks (BNs) 的结构学习和参数学习软件包，旨在帮助初学者理解这些工具在表示因果机制中的应用。作者回顾了多种软件和算法，强调由于没有通用标准解决方案，该领域对新手具有挑战性，并基于主观评估给出推荐。论文还附带了一个易于查阅的概述表格，总结各软件的主要功能，从而提升初学者进入BNs 领域的可访问性。",
      "categories": [
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.17025v1",
      "published_date": "2025-03-21 10:36:11 UTC",
      "updated_date": "2025-03-21 10:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:48:25.641000"
    },
    {
      "arxiv_id": "2503.17018v1",
      "title": "Symbolic Audio Classification via Modal Decision Tree Learning",
      "title_zh": "符号化音频分类通过模态决策树学习",
      "authors": [
        "Enrico Marzano",
        "Giovanni Pagliarini",
        "Riccardo Pasini",
        "Guido Sciavicco",
        "Ionel Eduard Stan"
      ],
      "abstract": "The range of potential applications of acoustic analysis is wide.\nClassification of sounds, in particular, is a typical machine learning task\nthat received a lot of attention in recent years. The most common approaches to\nsound classification are sub-symbolic, typically based on neural networks, and\nresult in black-box models with high performances but very low transparency. In\nthis work, we consider several audio tasks, namely, age and gender recognition,\nemotion classification, and respiratory disease diagnosis, and we approach them\nwith a symbolic technique, that is, (modal) decision tree learning. We prove\nthat such tasks can be solved using the same symbolic pipeline, that allows to\nextract simple rules with very high accuracy and low complexity. In principle,\nall such tasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation agent\nfor an hospital or a clinic.",
      "tldr_zh": "本研究探讨了符号化音频分类方法，通过Modal Decision Tree Learning来处理声音分类任务，包括年龄和性别识别、情感分类以及呼吸系统疾病诊断。相较于传统的子符号化方法（如neural networks），这种方法能生成透明的决策树模型，提取简单规则的同时实现高准确率和低复杂度。实验证明，这些任务可以使用统一的符号化管道解决，并适用于自主对话系统，例如医院预约代理。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "68T05",
        "I.2.6"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17018v1",
      "published_date": "2025-03-21 10:27:16 UTC",
      "updated_date": "2025-03-21 10:27:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:48:37.796202"
    },
    {
      "arxiv_id": "2503.17013v1",
      "title": "Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing",
      "title_zh": "在第二语言学习者中培养批判性思维：探索 ChatGPT 等生成式 AI 作为论说文写作工具",
      "authors": [
        "Simon Suh",
        "Jihyuk Bang",
        "Ji Woo Han"
      ],
      "abstract": "This study employs the Paul-Elder Critical Thinking Model and Tan's\nargumentative writing framework to create a structured methodology. This\nmethodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)\nframework, integrates the models with ChatGPT's capabilities to guide L2\nlearners in utilizing ChatGPT to enhance their critical thinking skills. A\nquantitative experiment was conducted with 10 participants from a state\nuniversity, divided into experimental and control groups. The experimental\ngroup utilized the CGCAW framework, while the control group used ChatGPT\nwithout specific guidelines. Participants wrote an argumentative essay within a\n40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,\nGrammarly, and a course instructor. Results indicated that the experimental\ngroup showed improvements in clarity, logical coherence, and use of evidence,\ndemonstrating ChatGPT's potential to enhance specific aspects of argumentative\nwriting. However, the control group performed better in overall language\nmechanics and articulation of main arguments, indicating areas where the CGCAW\nframework could be further refined. This study highlights the need for further\nresearch to optimize the use of AI tools like ChatGPT in L2 learning\nenvironments to enhance critical thinking and writing skills.",
      "tldr_zh": "这篇论文开发了 CGCAW 框架，该框架结合 Paul-Elder Critical Thinking Model 和 Tan's argumentative writing framework，利用 ChatGPT 的能力来指导第二语言学习者（L2 learners）提升批判性思维和论证性写作技能。研究通过一项定量实验，涉及10名参与者分为实验组和对照组，实验组使用 CGCAW 框架写作论证性文章，而对照组直接使用 ChatGPT。结果显示，实验组在清晰度、逻辑连贯性和证据使用方面表现出色，证明了 ChatGPT 在增强特定写作方面的潜力；然而，对照组在整体语言机制和主要论点表达上更胜一筹，表明框架需进一步优化。该研究强调了在 L2 学习环境中优化 AI 工具如 ChatGPT 的必要性，以更好地促进批判性思维发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.7; K.3.1"
      ],
      "primary_category": "cs.HC",
      "comment": "12 pages, 3 figures. Uses Paul-Elder Critical Thinking Model and\n  Tan's argumentative writing framework. Includes an experimental study with 10\n  participants",
      "pdf_url": "http://arxiv.org/pdf/2503.17013v1",
      "published_date": "2025-03-21 10:22:58 UTC",
      "updated_date": "2025-03-21 10:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:48:52.775289"
    },
    {
      "arxiv_id": "2503.17002v1",
      "title": "Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy",
      "title_zh": "翻译失败",
      "authors": [
        "Weimin Wang",
        "Yu Du",
        "Ting Yang",
        "Yu Liu"
      ],
      "abstract": "Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.",
      "tldr_zh": "这篇论文提出了 LiRaCo，一种无目标（targetless）6DoF 校准方法，用于 LiDAR 和 2D 扫描 Radar 的外部参数校准，以解决自动驾驶车辆中多模态传感器融合受振动影响的漂移问题。方法利用圆柱坐标系中的空间占用一致性，将 Radar 扫描像素扩展到 3D 占用网格，并基于 LiDAR 点云的空间重叠优化一个涉及校准参数的成本函数来估计参数。实验在两个真实户外数据集上验证了该方法的准确性和可行性，源代码将公开。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17002v1",
      "published_date": "2025-03-21 10:09:04 UTC",
      "updated_date": "2025-03-21 10:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:49:03.109195"
    },
    {
      "arxiv_id": "2503.17424v1",
      "title": "Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data",
      "title_zh": "翻译失败",
      "authors": [
        "Aakash Singh",
        "Anurag Kanaujia",
        "Vivek Kumar Singh"
      ],
      "abstract": "Among the factors of production, human capital or skilled manpower is the one\nthat keeps evolving and adapts to changing conditions and resources. This\nadaptability makes human capital the most crucial factor in ensuring a\nsustainable growth of industry/sector. As new technologies are developed and\nadopted, the new generations are required to acquire skills in newer\ntechnologies in order to be employable. At the same time professionals are\nrequired to upskill and reskill themselves to remain relevant in the industry.\nThere is however no straightforward method to identify the skill needs of the\nindustry at a given point of time. Therefore, this paper proposes a data to\ndecision framework that can successfully identify the desired skill set in a\ngiven area by analysing the advertorial data collected from popular online job\nportals and supplied as input to the framework. The proposed framework uses\ntechniques of statistical analysis, data mining and natural language processing\nfor the purpose. The applicability of the framework is demonstrated on CS&IT\njob advertisement data from India. The analytical results not only provide\nuseful insights about current state of skill needs in CS&IT industry but also\nprovide practical implications to prospective job applicants, training\nagencies, and institutions of higher education & professional training.",
      "tldr_zh": "本论文提出一个“data to decisions”计算框架，用于从在线职位广告数据中识别行业技能需求，以应对人力资源快速变化的挑战。该框架结合了statistical analysis、data mining和natural language processing (NLP)技术，通过分析输入数据来提取和评估所需技能集。在印度的CS&IT职位广告数据上进行应用，该框架不仅揭示了当前行业技能需求的洞见，还为求职者、培训机构和高等教育机构提供了实际指导和启发。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17424v1",
      "published_date": "2025-03-21 09:49:31 UTC",
      "updated_date": "2025-03-21 09:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:49:14.580853"
    },
    {
      "arxiv_id": "2503.16983v1",
      "title": "Enabling Versatile Controls for Video Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Zhang",
        "Hao Zhou",
        "Haoming Qin",
        "Xiaobin Lu",
        "Jiaxing Yan",
        "Guanzhong Wang",
        "Zeyu Chen",
        "Yi Liu"
      ],
      "abstract": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
      "tldr_zh": "该研究针对文本到视频生成中的精细时空属性控制挑战，提出了一种名为 VCtrl（或 PP-VCtrl）的创新框架，用于在预训练的视频扩散模型中实现统一细粒度控制。VCtrl 通过一个通用的条件模块整合多种用户指定信号（如 Canny edges、segmentation masks 和 human keypoints），并采用统一的控制信号编码管道和稀疏残差连接机制，来高效编码这些信号而不改变底层生成器。实验结果和人类评估显示，该框架显著提升了视频生成的控制性和质量，且源码及预训练模型已在 PaddlePaddle 框架下公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl",
      "pdf_url": "http://arxiv.org/pdf/2503.16983v1",
      "published_date": "2025-03-21 09:48:00 UTC",
      "updated_date": "2025-03-21 09:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:49:26.597842"
    },
    {
      "arxiv_id": "2503.16980v3",
      "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Haichao Zhang",
        "Yun Fu"
      ],
      "abstract": "Token-based video representation has emerged as a promising approach for\nenabling LLMs to interpret video content. However, existing token reduction,\nsuch as token pruning and token merging, often disrupt essential\nspatial-temporal positional embeddings, failing to adequately balance\ncomputational efficiency with fewer tokens. Consequently, these methods result\nin lengthy token sequences, limiting their applicability in scenarios requiring\nextreme token compression, such as video large language models. In this paper,\nwe introduce the novel task of extreme short token reduction, aiming to\nrepresent extensive video sequences with a minimal number of tokens. To address\nthis challenge, we propose Token Dynamics, a new video representation framework\nthat dynamically reduces token count while preserving spatial-temporal\ncoherence. Specifically, we disentangle video representations by separating\nvisual embeddings from grid-level motion information, structuring them into: 1.\na concise token hash table, created by clustering tokens that describe\nobject-level content; 2. a token indices key map, capturing detailed\nspatial-temporal motion patterns across grids; 3. a token hash function, which\nvector-quantizes the token hash table to reconstruct the token sequence from\nthe key map. Furthermore, we introduce a cross-dynamics attention mechanism\nthat integrates motion features into the token base without increasing token\nlength, thereby maintaining compactness and spatial-temporal integrity. The\nexperiments demonstrate a reduction of token count to merely 0.07% of the\noriginal tokens, with only a minor performance drop of 1.13%. Additionally, we\npropose two novel subtasks within extreme token reduction (fixed-length and\nadaptive-length compression). Our method offers significantly lower theoretical\ncomplexity, fewer tokens, and enhanced throughput, thus providing an efficient\nsolution for video LLMs.",
      "tldr_zh": "本论文针对视频大型语言模型（Video Large Language Models）中的 token 减少问题，引入了“extreme short token reduction”新任务，旨在用极少 token 表示视频序列，同时解决现有方法破坏空间-时间位置嵌入的局限性。论文提出 Token Dynamics 框架，通过分离视觉嵌入和网格级运动信息，构建 token hash table（聚类对象级内容）、token indices key map（捕捉运动模式）以及 token hash function（向量量化重建序列），并引入 cross-dynamics attention 机制来整合运动特征而不增加 token 长度。实验结果显示，该方法将 token 数量减少到原先的 0.07%，性能仅下降 1.13%，并提出 fixed-length 和 adaptive-length compression 子任务，提供更低的理论复杂度和更高的吞吐量，从而提升视频表示的效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This submission has been withdrawn due to non-scientific and personal\n  reasons of the first author, with the understanding of all co-authors. The\n  first author has requested that the work not be made public at this time.\n  Future publication remains under discussion and exploration",
      "pdf_url": "http://arxiv.org/pdf/2503.16980v3",
      "published_date": "2025-03-21 09:46:31 UTC",
      "updated_date": "2025-04-02 21:54:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:49:39.245951"
    },
    {
      "arxiv_id": "2503.16978v1",
      "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoqi Zhang",
        "Ziwei Luo",
        "Jens Sjölund",
        "Per Mattsson",
        "Linus Gisslén",
        "Alessandro Sestini"
      ],
      "abstract": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
      "tldr_zh": "本文提出 CPQE（Consistency Policy with Q-Ensembles），一种将一致性模型（Consistency models）与 Q-ensembles 相结合的方法，旨在解决扩散模型（Diffusion models）在游戏代理中的慢速推理和训练不稳定性问题。CPQE 通过 Q-ensembles 估计不确定性，提供更可靠的价值函数近似，从而提升训练稳定性和整体性能。在多个游戏场景的实验中，CPQE 实现了高达60 Hz 的推理速度，比现有扩散策略的20 Hz 显著提升，同时保持与多步扩散方法相当的奖励表现，并超越了传统一致性模型方法。这些结果表明，CPQE 为游戏和其他实时应用中部署多模态行为建模的扩散策略提供了高效、可行的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16978v1",
      "published_date": "2025-03-21 09:45:59 UTC",
      "updated_date": "2025-03-21 09:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:49:51.225461"
    },
    {
      "arxiv_id": "2503.16976v1",
      "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Weihao Yu",
        "Xiaoqing Guo",
        "Chenxin Li",
        "Yifan Liu",
        "Yixuan Yuan"
      ],
      "abstract": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.",
      "tldr_zh": "本研究提出 GeoT 框架，用于半监督牙齿点云分割，旨在解决现有方法中伪标签噪声问题，从而提升正畸应用的分割精度。GeoT 通过 instance-dependent transition matrix (IDTM) 显式建模伪标签噪声，并引入 point-level geometric regularization (PLGR) 和 class-level geometric smoothing (CLGS) 来利用牙齿几何先验，确保 3D 空间点邻接关系与 IDTM 的一致性，并优化牙齿类别的空间分布估计。在 Teeth3DS 数据集和私有数据集的实验中，GeoT 仅需 20% 的标注数据即可实现与全监督方法相当的性能，充分挖掘了未标注数据的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IPMI2025",
      "pdf_url": "http://arxiv.org/pdf/2503.16976v1",
      "published_date": "2025-03-21 09:43:57 UTC",
      "updated_date": "2025-03-21 09:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:50:03.021429"
    },
    {
      "arxiv_id": "2503.16974v2",
      "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Julian Junyan Wang",
        "Victor Xiaoqi Wang"
      ],
      "abstract": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.",
      "tldr_zh": "本研究首次全面评估大型语言模型 (LLMs) 在财务和会计任务中的输出一致性和可重复性，通过对五个任务（分类、情感分析、总结、文本生成和预测）进行50次独立运行，使用GPT-3.5-turbo、GPT-4o-mini和GPT-4o模型，生成超过340万输出。结果显示，一致性因任务而异，二元分类和情感分析几乎完美可重复，而复杂任务表现出更大变异性；尽管高级模型未始终表现出色，但LLMs在一致性上优于人类专家，且简单聚合策略（如3-5次运行）能显著提升一致性并可能提高情感分析的准确性。模拟分析进一步证明，尽管LLMs输出存在不一致性，下游统计推断保持稳健，并降低了\"G-hacking\"（选择性报告有利结果）的风险。",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-fin.GN",
      "comment": "97 pages, 20 tables, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16974v2",
      "published_date": "2025-03-21 09:43:37 UTC",
      "updated_date": "2025-03-26 17:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:50:15.679982"
    },
    {
      "arxiv_id": "2503.16973v2",
      "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance",
      "title_zh": "ARFlow: 人类动作-反应流匹配伴以物理指导",
      "authors": [
        "Wentao Jiang",
        "Jingya Wang",
        "Haotao Lu",
        "Kaiyang Ji",
        "Baoxiong Jia",
        "Siyuan Huang",
        "Ye Shi"
      ],
      "abstract": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.",
      "tldr_zh": "该论文提出ARFlow框架，用于人类动作-反应合成，旨在解决扩散模型在建模因果互动中的问题，如复杂噪声到反应生成器和生成的动作违反物理规律。ARFlow通过建立直接的动作到反应映射，引入x1-prediction方法直接输出人类动作以便显式约束，以及一个训练-free的梯度-based物理指导机制来防止身体穿透。实验在NTU120和Chi3D数据集上显示，ARFlow在Fréchet Inception Distance和动作多样性上优于现有方法，并显著降低身体碰撞，如通过Intersection Volume和Intersection Frequency指标衡量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://arflow2025.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.16973v2",
      "published_date": "2025-03-21 09:41:24 UTC",
      "updated_date": "2025-03-26 08:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:50:26.264828"
    },
    {
      "arxiv_id": "2503.16956v1",
      "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Ji-Hoon Kim",
        "Jeongsoo Choi",
        "Jaehun Kim",
        "Chaeyoung Jung",
        "Joon Son Chung"
      ],
      "abstract": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.",
      "tldr_zh": "本研究针对 video-to-speech synthesis 任务，提出一种新颖系统，通过学习 hierarchical representations 来桥接无声视频与多方面语音之间的模态差距。具体方法包括三个顺序阶段：内容建模（对齐唇部动作）、音色建模（对齐面部身份）和韵律建模（对齐面部表情），并使用 flow matching 模型从简单先验分布估计直接轨迹以生成连贯的语音。实验结果表明，该系统生成的语音质量与真实语音相当，比现有方法有显著优势。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/",
      "pdf_url": "http://arxiv.org/pdf/2503.16956v1",
      "published_date": "2025-03-21 09:02:38 UTC",
      "updated_date": "2025-03-21 09:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:50:39.474552"
    },
    {
      "arxiv_id": "2503.16953v1",
      "title": "Neural-Guided Equation Discovery",
      "title_zh": "神经引导的方程发现",
      "authors": [
        "Jannis Brugger",
        "Mattia Cerrato",
        "David Richter",
        "Cedric Derstroff",
        "Daniel Maninger",
        "Mira Mezini",
        "Stefan Kramer"
      ],
      "abstract": "Deep learning approaches are becoming increasingly attractive for equation\ndiscovery. We show the advantages and disadvantages of using neural-guided\nequation discovery by giving an overview of recent papers and the results of\nexperiments using our modular equation discovery system MGMT\n($\\textbf{M}$ulti-Task $\\textbf{G}$rammar-Guided $\\textbf{M}$onte-Carlo\n$\\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided\nMonte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement\nlearning, with a search space defined by a context-free grammar. We summarize\nseven desirable properties of equation discovery systems, emphasizing the\nimportance of embedding tabular data sets for such learning approaches. Using\nthe modular structure of MGMT, we compare seven architectures (among them,\nRNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary\ntask of contrastive learning for tabular data sets on an equation discovery\ntask. For almost all combinations of modules, supervised learning outperforms\nreinforcement learning. Moreover, our experiments indicate an advantage of\nusing grammar rules as action space instead of tokens. Two adaptations of MCTS\n-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that\nkind of search.",
      "tldr_zh": "本研究探讨了神经引导的方程发现（Neural-Guided Equation Discovery），通过概述最近论文和实验结果，突出了深度学习方法的优势与局限，并引入了模块化系统 MGMT（Multi-Task Grammar-Guided Monte-Carlo Tree Search for Equation Discovery）。该系统利用神经引导的 Monte-Carlo Tree Search (MCTS) 和上下文无关文法定义搜索空间，支持 supervised learning 和 reinforcement learning，并比较了七种架构（如 RNNs, CNNs, Transformers）在嵌入表格数据集的对比学习任务上的性能。实验发现，supervised learning 几乎在所有模块组合中优于 reinforcement learning，使用文法规则作为动作空间更有优势，且两种 MCTS 适应版本（risk-seeking MCTS 和 AmEx-MCTS）能显著改善方程发现效果。",
      "categories": [
        "cs.AI",
        "I.2.6; I.1.1; G.3"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages + 4 pages appendix, 9 figures, book chapter",
      "pdf_url": "http://arxiv.org/pdf/2503.16953v1",
      "published_date": "2025-03-21 08:55:51 UTC",
      "updated_date": "2025-03-21 08:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:50:52.721890"
    },
    {
      "arxiv_id": "2503.16939v1",
      "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
      "title_zh": "翻译失败",
      "authors": [
        "Hazem Hesham Yousef Shalby",
        "Arianna De Vecchi",
        "Alice Scandelli",
        "Pietro Bartoli",
        "Diana Trojaniello",
        "Manuel Roveri",
        "Federica Villa"
      ],
      "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
      "tldr_zh": "本论文探讨了 TinyML（Tiny Machine Learning）领域，将机器学习集成到传感器中以降低功耗，首次优化了 Depth-First Convolutional Neural Networks (CNNs) 的设计和实现。方法包括在 Intelligent Sensor Processing Unit (ISPU) 和 microcontroller (MCU) 之间分区 CNN，并引入 Early-Exit 机制，当结果置信度足够时提前停止计算，从而显著减少能源消耗。实验结果显示，在 NUCLEO-F411RE 板上，该方案的平均电流消耗为 4.8 mA，比传统 MCU 推理管道降低了 11%，同时保持了相同的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at IEEE SSCI",
      "pdf_url": "http://arxiv.org/pdf/2503.16939v1",
      "published_date": "2025-03-21 08:31:07 UTC",
      "updated_date": "2025-03-21 08:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:51:03.065517"
    },
    {
      "arxiv_id": "2503.16938v1",
      "title": "Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Alessio Cascione",
        "Mattia Setzu",
        "Federico A. Galatolo",
        "Mario G. C. A. Cimino",
        "Riccardo Guidotti"
      ],
      "abstract": "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
      "tldr_zh": "本研究针对医疗决策中的复杂性，提出使用可解释机器学习（Interpretable Machine Learning）来辅助口腔病变诊断，通过识别原型实例（Prototypical Instances）来提升模型的可解释性。研究引入了PivotTree模型，该模型模拟人类决策方式，选择关键示例作为预测基础，用于从口腔图像中检测neoplastic（肿瘤）、aphthous（口疮）和traumatic ulcerated lesions（外伤性溃疡）。实验结果显示，该方法在性能上表现出色，并通过定性和定量比较，证明了其选择的示例与专家地标原型的高度一致性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16938v1",
      "published_date": "2025-03-21 08:25:32 UTC",
      "updated_date": "2025-03-21 08:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:51:14.040486"
    },
    {
      "arxiv_id": "2503.16932v1",
      "title": "Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Kate Letheren",
        "Nicole Robinson"
      ],
      "abstract": "Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.",
      "tldr_zh": "这篇论文探讨了在专业环境中，人类对机器人报复意图的感知，聚焦于社会期望的违反及其负面影响。研究采用一个2x3在线实验，通过第一人称视角视频让参与者沉浸在协作工作场景中，结果显示机器人被期望遵守社会期望，即使面对人类粗鲁行为时表现出更大度量也能提升其正面形象。论文提供了理论和实际含义，强调这些发现对社会机器人设计的重要性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "This is the author version of the manuscript submitted to ACM\n  Transactions on Human-Robot Interaction. The final version, if accepted, will\n  be published by ACM and available via the ACM Digital Library. 12 pages, 1\n  figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.16932v1",
      "published_date": "2025-03-21 08:12:40 UTC",
      "updated_date": "2025-03-21 08:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:51:26.345255"
    },
    {
      "arxiv_id": "2503.16929v2",
      "title": "TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Shicheng Li",
        "Lei Li",
        "Kun Ouyang",
        "Shuhuai Ren",
        "Yuanxin Liu",
        "Yuanxing Zhang",
        "Fuzheng Zhang",
        "Lingpeng Kong",
        "Qi Liu",
        "Xu Sun"
      ],
      "abstract": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPLE (TEMporal Preference Learning), a systematic framework that\nenhances Video LLMs' temporal reasoning capabilities through Direct Preference\nOptimization (DPO). To facilitate this, we introduce an automated preference\ndata generation pipeline that systematically constructs preference pairs by\nselecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n\"Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPLE as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs. Code is available at\nhttps://github.com/lscpku/TEMPLE.",
      "tldr_zh": "论文提出 TEMPLE 框架，通过 Direct Preference Optimization (DPO) 增强 Video LLMs 的时间推理能力，解决现有方法在视频文本数据时间对应性和训练范式上的局限性。\nTEMPLE 包括一个自动偏好数据生成管道，用于构建偏好对，并引入课程学习（progressively increases perturbation difficulty）以提高模型鲁棒性，以及 Pre-SFT Alignment 在指令微调前优先细粒度时间理解。\n实验结果显示，该框架使用较小规模的自生成 DPO 数据即可在多个基准上 consistently 提升 Video LLMs 性能，并证明了 DPO 数据的架构可转移性和难度调度的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16929v2",
      "published_date": "2025-03-21 08:00:29 UTC",
      "updated_date": "2025-03-29 18:15:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:51:39.712605"
    },
    {
      "arxiv_id": "2503.16922v1",
      "title": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Linxi Liang",
        "Jing Gong",
        "Mingwei Liu",
        "Chong Wang",
        "Guangsheng Ou",
        "Yanlin Wang",
        "Xin Peng",
        "Zibin Zheng"
      ],
      "abstract": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo.",
      "tldr_zh": "这篇论文引入了 RustEvo^2 框架，这是一个动态基准，用于评估 LLMs 在 Rust 代码生成中适应 API 演变的能力，解决现有基准缺乏系统评估的问题。框架通过自动化合成 588 个 API 变化（涵盖 Stabilizations, Signature Changes, Behavioral Changes 和 Deprecations），创建真实编程任务数据集。实验结果显示，SOTA LLMs 在 stabilized APIs 上成功率达 65.8%，但在 behavioral changes 上仅 38.0%，且知识截止日期显著影响性能，而 Retrieval-Augmented Generation (RAG) 平均提升 13.5% 的成功率。该框架强调了进化感知基准对提升 LLMs 在快速演变软件生态中的适应性的必要性，并已公开源码。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16922v1",
      "published_date": "2025-03-21 07:33:59 UTC",
      "updated_date": "2025-03-21 07:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:51:51.850907"
    },
    {
      "arxiv_id": "2503.16921v1",
      "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
      "title_zh": "翻译失败",
      "authors": [
        "Lingfan Zhang",
        "Chen Liu",
        "Chengming Xu",
        "Kai Hu",
        "Donghao Luo",
        "Chengjie Wang",
        "Yanwei Fu",
        "Yuan Yao"
      ],
      "abstract": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
      "tldr_zh": "本文探讨了图像生成领域中，扩散模型（diffusion models）在训练时偏好数据的少数样本（minority samples）问题，这些样本可能因主观性和不稳定性而损害模型性能。作者提出 Adaptive-DPO，一种新型方法，将少数实例感知指标（如 intra-annotator confidence 和 inter-annotator stability）整合到 DPO 目标中，以区分多数和少数样本。Adaptive-DPO 通过改进损失函数，增强模型对多数标签的学习并减少少数样本的负面影响。实验验证显示，该方法在合成和真实世界偏好数据上有效，提升了图像生成任务的整体训练效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16921v1",
      "published_date": "2025-03-21 07:33:44 UTC",
      "updated_date": "2025-03-21 07:33:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:52:03.298080"
    },
    {
      "arxiv_id": "2503.17421v1",
      "title": "Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Junwei Kuang",
        "Liang Yang",
        "Shaoze Cui",
        "Weiguo Fan"
      ],
      "abstract": "Patients are increasingly turning to online health Q&A communities for social\nsupport to improve their well-being. However, when this support received does\nnot align with their specific needs, it may prove ineffective or even\ndetrimental. This necessitates a model capable of identifying the social\nsupport needs in questions. However, training such a model is challenging due\nto the scarcity and class imbalance issues of labeled data. To overcome these\nchallenges, we follow the computational design science paradigm to develop a\nnovel framework, Hybrid Approach for SOcial Support need classification\n(HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning\napproach, a text data augmentation technique leveraging large language models\n(LLMs) with reliability- and diversity-aware sample selection mechanism, and a\nunified training process to automatically label social support needs in\nquestions. Extensive empirical evaluations demonstrate that HA-SOS\nsignificantly outperforms existing question classification models and\nalternative semi-supervised learning approaches. This research contributes to\nthe literature on social support, question classification, semi-supervised\nlearning, and text data augmentation. In practice, our HA-SOS framework\nfacilitates online Q&A platform managers and answerers to better understand\nusers' social support needs, enabling them to provide timely, personalized\nanswers and interventions.",
      "tldr_zh": "本研究探讨了患者在在线健康 Q&A 社区寻求社会支持的需求识别问题，强调了数据稀缺和类别不平衡带来的挑战。研究提出了一种混合框架 HA-SOS，结合了答案增强的 semi-supervised learning、LLM-based data augmentation（包括可靠性-和多样性感知的样本选择机制）以及统一的训练过程，来自动标记问题中的社会支持需求。实验结果显示，HA-SOS 显著优于现有问题分类模型和半监督学习方法，为社会支持、问题分类等领域做出了贡献，并在实践中帮助在线平台提供更个性化的回答和干预。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "55 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.17421v1",
      "published_date": "2025-03-21 07:25:16 UTC",
      "updated_date": "2025-03-21 07:25:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:52:14.663063"
    },
    {
      "arxiv_id": "2503.16914v1",
      "title": "A New Segment Routing method with Swap Node Selection Strategy Based on Deep Reinforcement Learning for Software Defined Network",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Ye",
        "Jihao Zheng",
        "Qiuxiang Jiang",
        "Yuan Huang",
        "Ziheng Wang",
        "Yong Wang"
      ],
      "abstract": "The existing segment routing (SR) methods need to determine the routing first\nand then use path segmentation approaches to select swap nodes to form a\nsegment routing path (SRP). They require re-segmentation of the path when the\nrouting changes. Furthermore, they do not consider the flow table issuance\ntime, which cannot maximize the speed of issuance flow table. To address these\nissues, this paper establishes an optimization model that can simultaneously\nform routing strategies and path segmentation strategies for selecting the\nappropriate swap nodes to reduce flow table issuance time. It also designs an\nintelligent segment routing algorithm based on deep reinforcement learning\n(DRL-SR) to solve the proposed model. First, a traffic matrix is designed as\nthe state space for the deep reinforcement learning agent; this matrix includes\nmultiple QoS performance indicators, flow table issuance time overhead and SR\nlabel stack depth. Second, the action selection strategy and corresponding\nreward function are designed, where the agent selects the next node considering\nthe routing; in addition, the action selection strategy whether the newly added\nnode is selected as the swap node and the corresponding reward function are\ndesigned considering the time cost factor for the controller to issue the flow\ntable to the swap node. Finally, a series of experiments and their results show\nthat, compared with the existing methods, the designed segmented route\noptimization model and the intelligent solution algorithm (DRL-SR) can reduce\nthe time overhead required to complete the segmented route establishment task\nwhile optimizing performance metrics such as throughput, delays and packet\nlosses.",
      "tldr_zh": "本文提出了一种新的Segment Routing (SR)方法，针对Software Defined Network (SDN)中的路由和路径分段问题，建立了一个优化模型，能够同时生成路由策略和交换节点选择策略，以减少流表下发时间。方法基于Deep Reinforcement Learning (DRL)，设计了DRL-SR算法，使用流量矩阵作为状态空间，包括QoS性能指标、流表下发时间开销和SR标签栈深度，并定义了动作选择策略和奖励函数来优化交换节点选择。实验结果表明，与现有方法相比，该算法显著降低了分段路由建立的时间开销，同时改善了吞吐量、延迟和丢包等性能指标。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16914v1",
      "published_date": "2025-03-21 07:24:09 UTC",
      "updated_date": "2025-03-21 07:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:52:27.922435"
    },
    {
      "arxiv_id": "2503.16905v1",
      "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Zhang",
        "Zhiyuan Wang",
        "Zhangqi Wang",
        "Xinyu Zhang",
        "Fangzhi Xu",
        "Qika Lin",
        "Rui Mao",
        "Erik Cambria",
        "Jun Liu"
      ],
      "abstract": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
      "tldr_zh": "这篇论文针对多模态科学问题 (MSPs) 的多模态综合推理挑战和缺乏反思能力，提出了一种基于 Big Seven Personality 和 Socratic Guidance 的多代理框架 (MAPS)。该框架包括七个代理，利用反馈机制和苏格拉底方法：一个渐进的四代理解决策略专注于问题解决的不同阶段，以及一个 Critic 代理来激发批判性思考和自主学习。在 EMMA、Olympiad 和 MathVista 数据集上的实验表明，MAPS 比当前最先进模型 (SOTA) 提高了 15.84% 的性能，并验证了其泛化能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16905v1",
      "published_date": "2025-03-21 07:13:45 UTC",
      "updated_date": "2025-03-21 07:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:52:39.416534"
    },
    {
      "arxiv_id": "2503.16904v1",
      "title": "Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons: A Comparative Study",
      "title_zh": "下肢外骨骼中人类运动分析的深度学习：一个比较研究",
      "authors": [
        "Omar Coser",
        "Christian Tamantini",
        "Matteo Tortora",
        "Leonardo Furia",
        "Rosa Sicilia",
        "Loredana Zollo",
        "Paolo Soda"
      ],
      "abstract": "Wearable robotics for lower-limb assistance have become a pivotal area of\nresearch, aiming to enhance mobility for individuals with physical impairments\nor augment the performance of able-bodied users. Accurate and adaptive control\nsystems are essential to ensure seamless interaction between the wearer and the\nrobotic device, particularly when navigating diverse and dynamic terrains.\nDespite the recent advances in neural networks for time series analysis, no\nattempts have been directed towards the classification of ground conditions,\ncategorized into five classes and subsequently determining the ramp's slope and\nstair's height. In this respect, this paper presents an experimental comparison\nbetween eight deep neural network backbones to predict high-level locomotion\nparameters across diverse terrains.\n  All the models are trained on the publicly available CAMARGO 2021 dataset.\nIMU-only data equally or outperformed IMU+EMG inputs, promoting a\ncost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM\nachieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp\nslope (1.95 +- 0.58{\\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)\nestimations. As a further contribution, SHAP analysis justified sensor\nreduction without performance loss, ensuring a lightweight setup. The system\noperates with ~2 ms inference time, supporting real-time applications. The code\nis code available at\nhttps://github.com/cosbidev/Human-Locomotion-Identification.",
      "tldr_zh": "本研究比较了八种深度神经网络模型，用于下肢外骨骼机器人的人类运动分析，旨在分类五类地形（如坡道和楼梯）并精确估计坡度和高度，从而提升穿戴者与设备间的自适应控制。实验基于公开的 CAMARGO 2021 数据集，仅使用 IMU 数据就实现了高效性能，其中 LSTM 模型在地形分类准确率达到 0.94，坡度估计误差为 1.95 度，而 CNN-LSTM 模型的楼梯高度估计误差为 15.65 mm。SHAP 分析证实了减少传感器数量不会影响表现，支持了轻量级设计，且系统推理时间约 2 ms，适合实时应用。该工作通过开源代码（https://github.com/cosbidev/Human-Locomotion-Identification）促进了可负担的机器人辅助系统发展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "F.2.2, I.2.7"
      ],
      "primary_category": "cs.RO",
      "comment": "26 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16904v1",
      "published_date": "2025-03-21 07:12:44 UTC",
      "updated_date": "2025-03-21 07:12:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:52:51.171112"
    },
    {
      "arxiv_id": "2503.16874v1",
      "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
      "title_zh": "MARS：一种融入苏格拉底指导的多智能体框架",
      "authors": [
        "Jian Zhang",
        "Zhangqi Wang",
        "Haiping Zhu",
        "Jun Liu",
        "Qika Lin",
        "Erik Cambria"
      ],
      "abstract": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
      "tldr_zh": "本论文提出 MARS 框架，这是一个整合 Socratic guidance 的多智能体系统，用于 Automated Prompt Optimization (APO)，旨在解决现有方法的固定模板灵活性不足和提示空间搜索效率低等问题。MARS 包括七个功能各异的智能体，通过 Planner 进行自动规划，并采用 Teacher-Critic-Student 对话模式实现渐进式连续优化和评估。实验结果显示，该框架在多种数据集上表现出色，并通过分析实验验证了模型的进步和可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16874v1",
      "published_date": "2025-03-21 06:19:55 UTC",
      "updated_date": "2025-03-21 06:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:53:03.603674"
    },
    {
      "arxiv_id": "2503.16873v1",
      "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Dongseob Kim",
        "Hyunjung Shim"
      ],
      "abstract": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD.",
      "tldr_zh": "这篇论文针对无监督多-label classification 的挑战，提出了一种Classifier-guided CLIP Distillation (CCD) 方法，以解决CLIP模型的视角依赖和固有偏差问题。CCD通过Class Activation Mapping (CAM)引导选择目标对象附近的多个局部视图，并对CLIP的伪标签进行去偏置，从而在无额外标签的情况下提升分类性能。实验结果表明，该方法在多样数据集上优于现有技术，为高效的多-label图像理解提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2503.16873v1",
      "published_date": "2025-03-21 06:12:14 UTC",
      "updated_date": "2025-03-21 06:12:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:53:14.348016"
    },
    {
      "arxiv_id": "2503.16870v1",
      "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
      "title_zh": "稀疏 Logit 采样：加速 LLMs 中的知识蒸馏",
      "authors": [
        "Anshumann",
        "Mohd Abbas Zaidi",
        "Akhil Kedia",
        "Jinwoo Ahn",
        "Taehwak Kwon",
        "Kangwook Lee",
        "Haejun Lee",
        "Joohyung Lee"
      ],
      "abstract": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
      "tldr_zh": "该研究探讨了在大型语言模型（LLMs）中加速知识蒸馏（Knowledge Distillation）的技术，证明了传统的稀疏方法（如缓存Top-K概率）会导致教师概率分布的偏差估计，从而影响性能和校准。作者提出了一种基于重要性采样的新方法“Random Sampling Knowledge Distillation”，它提供无偏估计、保留梯度期望，并显著减少logits存储量，使学生模型训练速度加快，仅比基于交叉熵的训练增加不到10%的开销。实验结果显示，该方法在从300M到3B的模型大小范围内，保持与完整蒸馏相当的竞争性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
      "pdf_url": "http://arxiv.org/pdf/2503.16870v1",
      "published_date": "2025-03-21 05:58:18 UTC",
      "updated_date": "2025-03-21 05:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:53:26.793173"
    },
    {
      "arxiv_id": "2503.16861v2",
      "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
      "title_zh": "翻译失败",
      "authors": [
        "Shayne Longpre",
        "Kevin Klyman",
        "Ruth E. Appel",
        "Sayash Kapoor",
        "Rishi Bommasani",
        "Michelle Sahar",
        "Sean McGregor",
        "Avijit Ghosh",
        "Borhane Blili-Hamelin",
        "Nathan Butters",
        "Alondra Nelson",
        "Amit Elazari",
        "Andrew Sellars",
        "Casey John Ellis",
        "Dane Sherrets",
        "Dawn Song",
        "Harley Geiger",
        "Ilona Cohen",
        "Lauren McIlvenny",
        "Madhulika Srikumar",
        "Mark M. Jaycox",
        "Markus Anderljung",
        "Nadine Farid Johnson",
        "Nicholas Carlini",
        "Nicolas Miailhe",
        "Nik Marda",
        "Peter Henderson",
        "Rebecca S. Portnoff",
        "Rebecca Weiss",
        "Victoria Westerhoff",
        "Yacine Jernite",
        "Rumman Chowdhury",
        "Percy Liang",
        "Arvind Narayanan"
      ],
      "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
      "tldr_zh": "该论文强调，通用AI (GPAI) 系统的广泛部署带来了重大风险，但当前的缺陷报告基础设施、实践和规范远落后于软件安全领域。作者团队（来自软件安全、机器学习、法学、社会科学和政策等领域）识别了GPAI系统缺陷评估和报告的关键差距，并提出三点干预措施：采用标准化AI缺陷报告和研究者参与规则、GPAI提供者实施类似bug bounties的广泛缺陷披露程序（包括法律保护），以及开发基础设施来协调缺陷报告的分发。这些措施能显著提升AI生态的安全性、安全性和问责性，尤其应对jailbreaks等可转移缺陷的迫切需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16861v2",
      "published_date": "2025-03-21 05:09:46 UTC",
      "updated_date": "2025-03-25 05:12:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:53:39.213153"
    },
    {
      "arxiv_id": "2503.16858v1",
      "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering",
      "title_zh": "MTBench：多模态时间序列基准，用于时间推理和问答",
      "authors": [
        "Jialin Chen",
        "Aosong Feng",
        "Ziyu Zhao",
        "Juan Garza",
        "Gaukhar Nurbek",
        "Cheng Qin",
        "Ali Maatouk",
        "Leandros Tassiulas",
        "Yifeng Gao",
        "Rex Ying"
      ],
      "abstract": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.",
      "tldr_zh": "本研究提出MTBench，一种大规模多模态时间序列基准，用于评估大型语言模型(LLMs)在时间序列和文本理解上的表现，针对金融和天气领域中的跨模态推理和复杂问答。该基准包括配对的时间序列与文本数据，如金融新闻与股票价格变动，以及天气报告与历史温度记录，支持模型共同推理结构化数字趋势和非结构化文本叙述。MTBench涵盖多样任务，包括时间序列预测、趋势分析和新闻驱动的问答，以测试模型捕捉时间依赖、提取文本洞见及整合跨模态信息的能力。实验结果显示，当前LLMs在处理长期依赖、解释因果关系和融合多模态信息方面面临显著挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.16858v1",
      "published_date": "2025-03-21 05:04:53 UTC",
      "updated_date": "2025-03-21 05:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:53:51.204071"
    },
    {
      "arxiv_id": "2503.16853v1",
      "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models",
      "title_zh": "Imagine to Hear：听觉知识生成可以成为语言模型的有效助手",
      "authors": [
        "Suho Yoo",
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
      "tldr_zh": "本文研究发现，基于文本语料预训练的 Language Models 在处理需要听觉常识知识的任务时表现不佳，而现有方法依赖外部音频数据库检索，存在数据缺失和成本高的问题。为此，论文提出 Imagine to Hear 框架，使用生成模型动态生成听觉知识，通过检测提示中的音频相关文本片段并生成对应知识。框架还引入 CLAP-based rejection sampler 和语言-音频融合模块来高效处理多重听觉知识。实验结果显示，该方法在 AuditoryBench 上实现了最先进性能，且不依赖外部数据库，证明了生成式方法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.16853v1",
      "published_date": "2025-03-21 04:56:22 UTC",
      "updated_date": "2025-03-21 04:56:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:54:03.072077"
    },
    {
      "arxiv_id": "2503.16852v1",
      "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxi Li",
        "Di Lin",
        "Hao Chen",
        "Hongying Liu",
        "Liang Wan",
        "Wei Feng"
      ],
      "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
      "tldr_zh": "本论文针对深度神经网络(DNNs)在处理分布外数据时的局限性，提出了一种基于因果推理的框架Style Deconfounding Causal Learning (SDCL)，旨在通过消除风格偏差混淆因素来提升领域泛化性能。SDCL构建了结构化因果模型(SCM)并采用backdoor adjustment策略，同时设计了风格引导专家模块(SGEM)来聚类风格分布，以及back-door causal learning module (BDCL)进行因果干预，以确保模型学习真正的因果表示。实验结果显示，该框架与现有数据增强技术结合后，在多域和单域的自然及医疗图像识别任务中，显著提高了模型的泛化性能和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2503.16852v1",
      "published_date": "2025-03-21 04:52:31 UTC",
      "updated_date": "2025-03-21 04:52:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:54:15.260474"
    },
    {
      "arxiv_id": "2503.16850v1",
      "title": "Physics-Informed Neural Network Surrogate Models for River Stage Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian Zoch",
        "Edward Holmberg",
        "Pujan Pokhrel",
        "Ken Pathak",
        "Steven Sloan",
        "Kendall Niles",
        "Jay Ratcliff",
        "Maik Flanagin",
        "Elias Ioup",
        "Christian Guetl",
        "Mahdi Abdelguerfi"
      ],
      "abstract": "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
      "tldr_zh": "本文研究使用Physics-Informed Neural Networks (PINNs)作为河水位预测的代理模型，旨在降低计算成本的同时保持预测准确性。主要贡献在于，PINNs通过整合Saint-Venant方程成功近似HEC-RAS数值解决方案，在单个河流上训练时实现高准确性，相对错误较低，尽管某些河段存在偏差。实验评估显示，该模型在准确性和计算速度上接近HEC-RAS，同时支持实时推理，证明了PINNs在单河流水动力学中的潜力。未来工作将聚焦于提升PINN训练的稳定性和鲁棒性，以扩展到多河流模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16850v1",
      "published_date": "2025-03-21 04:48:22 UTC",
      "updated_date": "2025-03-21 04:48:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:54:27.240656"
    },
    {
      "arxiv_id": "2503.16833v1",
      "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege",
      "title_zh": "端到端音频语言模型的部署应考虑到最小权限原则",
      "authors": [
        "Luxi He",
        "Xiangyu Qi",
        "Michel Liao",
        "Inyoung Cheong",
        "Prateek Mittal",
        "Danqi Chen",
        "Peter Henderson"
      ],
      "abstract": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.",
      "tldr_zh": "这篇立场论文讨论了端到-End Audio LMs（音频语言模型）的部署问题，强调这些模型直接处理语音可保留语调和多说话者等详细信息，但也带来安全风险，如滥用说话者身份和敏感语音属性，可能引发法律问题。作者主张采用“principle of least privilege”（最小特权原则）来决定使用级联模型还是端到-End模型，确保只授予必要权限。论文建议评估（1）端到-End建模是否对特定应用必需，以及（2）信息访问范围的适当性，并指出了当前Audio LMs基准中的空白及需要解决的技术和政策研究问题。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16833v1",
      "published_date": "2025-03-21 04:03:59 UTC",
      "updated_date": "2025-03-21 04:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:54:39.089996"
    },
    {
      "arxiv_id": "2503.16806v1",
      "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
      "title_zh": "DyWA：动态自适应世界行动模型，用于可泛化的非抓取操作",
      "authors": [
        "Jiangran Lyu",
        "Ziming Li",
        "Xuesong Shi",
        "Chaoyi Xu",
        "Yizhou Wang",
        "He Wang"
      ],
      "abstract": "Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.",
      "tldr_zh": "本文提出 DyWA（Dynamics-adaptive World Action Model），一个动态自适应世界行动模型，用于解决非 prehensile manipulation（非抓取操作）中的泛化问题，该框架通过联合预测未来状态并基于历史轨迹适应物理变化（如物体质量和摩擦），统一建模几何、状态、物理和机器人动作。相比传统方法，DyWA 仅依赖单视图 point cloud 观察，就在模拟环境中将成功率提高了 31.5%。在真实世界实验中，DyWA 实现了 68% 的平均成功率，展示了其在多样物体几何、摩擦变化和复杂场景（如半满水瓶或光滑表面）下的鲁棒性和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project Page:https://pku-epic.github.io/DyWA/",
      "pdf_url": "http://arxiv.org/pdf/2503.16806v1",
      "published_date": "2025-03-21 02:29:52 UTC",
      "updated_date": "2025-03-21 02:29:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:54:51.836750"
    },
    {
      "arxiv_id": "2503.16801v1",
      "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Geng",
        "Zeeshan Hayder",
        "Wei Liu",
        "Ajmal Saeed Mian"
      ],
      "abstract": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks",
      "tldr_zh": "这篇论文提出了一种自回归扩散模型(ARDHOI)，用于基于文本的3D人类-物体交互(Text-to-HOI)生成，旨在解决长序列中交互一致性的挑战。模型通过Contrastive Variational Autoencoder (cVAE)学习物理上合理的连续HOI tokens空间，确保生成的动作真实自然；同时，使用Mamba-based context encoder捕捉序列上下文，并结合MLP-based denoiser来预测后续token。在OMOMO和BEHAVE数据集上的实验表明，ARDHOI在性能和推理速度上优于现有方法，提供了一个高效的Text-to-HOI解决方案。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16801v1",
      "published_date": "2025-03-21 02:25:59 UTC",
      "updated_date": "2025-03-21 02:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:55:04.860442"
    },
    {
      "arxiv_id": "2503.16799v1",
      "title": "Causally Aligned Curriculum Learning",
      "title_zh": "因果对齐课程学习",
      "authors": [
        "Mingxuan Li",
        "Junzhe Zhang",
        "Elias Bareinboim"
      ],
      "abstract": "A pervasive challenge in Reinforcement Learning (RL) is the \"curse of\ndimensionality\" which is the exponential growth in the state-action space when\noptimizing a high-dimensional target task. The framework of curriculum learning\ntrains the agent in a curriculum composed of a sequence of related and more\nmanageable source tasks. The expectation is that when some optimal decision\nrules are shared across source tasks and the target task, the agent could more\nquickly pick up the necessary skills to behave optimally in the environment,\nthus accelerating the learning process. However, this critical assumption of\ninvariant optimal decision rules does not necessarily hold in many practical\napplications, specifically when the underlying environment contains unobserved\nconfounders. This paper studies the problem of curriculum RL through causal\nlenses. We derive a sufficient graphical condition characterizing causally\naligned source tasks, i.e., the invariance of optimal decision rules holds. We\nfurther develop an efficient algorithm to generate a causally aligned\ncurriculum, provided with qualitative causal knowledge of the target task.\nFinally, we validate our proposed methodology through experiments in discrete\nand continuous confounded tasks with pixel observations.",
      "tldr_zh": "本论文探讨了强化学习（RL）中“维度诅咒”的挑战，通过课程学习框架训练代理在相关源任务上逐步学习，以加速高维目标任务的优化。然而，当环境存在未观察混杂因素时，最优决策规则的共享假设可能不成立。论文从因果视角推导了一个充分的图形条件来标识因果对齐源任务（即最优决策规则不变），并开发了一个高效算法，利用定性的因果知识生成这样的课程。最后，通过在离散和连续混杂任务的实验中验证，证明了该方法能有效提升学习性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as Posters in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.16799v1",
      "published_date": "2025-03-21 02:20:38 UTC",
      "updated_date": "2025-03-21 02:20:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:55:16.308825"
    },
    {
      "arxiv_id": "2503.16797v1",
      "title": "A Learnability Analysis on Neuro-Symbolic Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hao-Yuan He",
        "Ming Li"
      ],
      "abstract": "This paper analyzes the learnability of neuro-symbolic (NeSy) tasks within\nhybrid systems. We show that the learnability of NeSy tasks can be\ncharacterized by their derived constraint satisfaction problems (DCSPs).\nSpecifically, a task is learnable if the corresponding DCSP has a unique\nsolution; otherwise, it is unlearnable. For learnable tasks, we establish error\nbounds by exploiting the clustering property of the hypothesis space.\nAdditionally, we analyze the asymptotic error for general NeSy tasks, showing\nthat the expected error scales with the disagreement among solutions. Our\nresults offer a principled approach to determining learnability and provide\ninsights into the design of new algorithms.",
      "tldr_zh": "本论文分析了 Neuro-Symbolic (NeSy) 任务在混合系统中的可学习性，将其表征为对应的派生约束满足问题 (DCSPs)。具体而言，如果 DCSPs 具有唯一解，则任务可学习；否则，为不可学习。对于可学习任务，该研究利用假设空间的聚类属性建立了错误边界，并分析了一般 NeSy 任务的渐近错误，其预期错误与解决方案间的分歧成比例。这些结果为确定任务可学习性提供了原则性方法，并为新算法设计带来重要见解。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16797v1",
      "published_date": "2025-03-21 02:16:11 UTC",
      "updated_date": "2025-03-21 02:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:55:26.340129"
    },
    {
      "arxiv_id": "2503.16791v2",
      "title": "\"The Diagram is like Guardrails\": Structuring GenAI-assisted Hypotheses Exploration with an Interactive Shared Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Zijian Ding",
        "Michelle Brachman",
        "Joel Chan",
        "Werner Geyer"
      ],
      "abstract": "Data analysis encompasses a spectrum of tasks, from high-level conceptual\nreasoning to lower-level execution. While AI-powered tools increasingly support\nexecution tasks, there remains a need for intelligent assistance in conceptual\ntasks. This paper investigates the design of an ordered node-link tree\ninterface augmented with AI-generated information hints and visualizations, as\na potential shared representation for hypothesis exploration. Through a design\nprobe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our\nfindings showed that the node-link diagram acts as \"guardrails\" for hypothesis\nexploration, facilitating structured workflows, providing comprehensive\noverviews, and enabling efficient backtracking. The AI-generated information\nhints, particularly visualizations, aided users in transforming abstract ideas\ninto data-backed concepts while reducing cognitive load. We further discuss how\nnode-link diagrams can support both parallel exploration and iterative\nrefinement in hypothesis formulation, potentially enhancing the breadth and\ndepth of human-AI collaborative data analysis.",
      "tldr_zh": "这篇论文探讨了如何使用交互式节点-链接图表（node-link diagram）作为共享表示，来结构化 GenAI 辅助的假设探索，从而支持数据分析中的概念任务。研究通过设计探针（design probe）实验（n=22），发现该图表如同“guardrails”般提供结构化工作流、全面概述和高效回溯，同时 AI 生成的信息提示和可视化帮助用户将抽象想法转化为数据支持的概念，并减少认知负载。最终，论文讨论了节点-链接图表如何促进并行探索和迭代细化，提升人类-AI 协作数据分析的广度和深度。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16791v2",
      "published_date": "2025-03-21 02:01:37 UTC",
      "updated_date": "2025-04-21 16:05:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:55:40.431506"
    },
    {
      "arxiv_id": "2503.16788v1",
      "title": "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study",
      "title_zh": "翻译失败",
      "authors": [
        "Li Zhang",
        "Longxi Gao",
        "Mengwei Xu"
      ],
      "abstract": "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps://github.com/LlamaTouch/VLM-Reasoning-Traces.",
      "tldr_zh": "本研究首次通过实证分析探讨了Chain-of-Thought Reasoning是否能提升视觉语言模型（VLMs）在移动GUI代理中的性能，焦点在于处理复杂屏幕布局、用户指令和多轮交互。研究评估了Gemini 2.0 Flash和Claude 3.7 Sonnet的基线版与推理增强版，在ScreenSpot、AndroidControl和AndroidWorld等基准上进行测试。结果显示，Claude 3.7 Sonnet的推理模型在AndroidWorld上达到了最先进性能，但整体上，推理VLMs在静态基准中仅提供微弱改进，有时甚至导致性能下降。论文发现，推理和非推理模型在不同任务上失败，表明其影响相互抵消，并归因于基准和VLMs的局限性。最后，基于这些发现，提供改进移动GUI代理的见解，包括优化基准、提升VLMs以及动态调用推理机制。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16788v1",
      "published_date": "2025-03-21 01:52:43 UTC",
      "updated_date": "2025-03-21 01:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:55:51.618792"
    },
    {
      "arxiv_id": "2503.16782v1",
      "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery",
      "title_zh": "学习部分知识以促进细粒度广义类别发现中的类别理解",
      "authors": [
        "Enguang Wang",
        "Zhimao Peng",
        "Zhengyuan Xie",
        "Haori Lu",
        "Fei Yang",
        "Xialei Liu"
      ],
      "abstract": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach.",
      "tldr_zh": "该论文针对细粒度 Generalized Category Discovery (GCD) 的挑战，提出了一种利用部分知识（Part Knowledge）来提升类别理解的方法，以解决现有基于全局图像特征的对比学习（contrastive learning）无法捕捉细微局部差异的问题。论文引入 PartGCD 框架，包括 Adaptive Part Decomposition（使用 Gaussian Mixture Models 自动提取类特定语义部分）和 Part Discrepancy Regularization（强制部分特征之间的显式分离，以放大细粒度局部差异）。实验结果显示，PartGCD 在多个细粒度基准上取得了最先进性能，同时在通用数据集上保持竞争力，证明了该方法的有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16782v1",
      "published_date": "2025-03-21 01:37:51 UTC",
      "updated_date": "2025-03-21 01:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:56:02.665996"
    },
    {
      "arxiv_id": "2503.16779v1",
      "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mengsong Wu",
        "Tong Zhu",
        "Han Han",
        "Xiang Zhang",
        "Wenbiao Shao",
        "Wenliang Chen"
      ],
      "abstract": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .",
      "tldr_zh": "本文提出Chain-of-Tools方法，利用冻结的Large Language Models (LLMs) 在Chain-of-Thought (CoT) 推理中调用海量未见工具，从而避免了传统方法所需的模型微调或低效提示演示。创新点在于充分利用LLMs的语义表示能力，并构建了新数据集SimpleToolQuestions来评估未见工具场景下的性能。实验在GSM8K-XL、FuncQA、KAMEL和SimpleToolQuestions等基准上显示，该方法优于基线模型，并通过识别工具选择的关键输出维度提升了模型的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.16779v1",
      "published_date": "2025-03-21 01:26:12 UTC",
      "updated_date": "2025-03-21 01:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:56:16.800863"
    },
    {
      "arxiv_id": "2503.17417v2",
      "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jungkyoo Shin",
        "Bumsoo Kim",
        "Eunwoo Kim"
      ],
      "abstract": "Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.",
      "tldr_zh": "本研究针对多模态表示学习中的模态差异问题，提出了一种新颖方法Class-anchor-ALigned generative Modeling (CALM)，它通过使用类锚点作为提示来生成和对齐每个模态的类概率分布，从而实现更有效的模态对齐。CALM 还引入了跨模态概率变分自编码器（cross-modal probabilistic variational autoencoder），用于建模对齐过程中的不确定性，以捕捉模态间更深层的关系和数据变化。该方法在四个基准数据集上的广泛实验中显著优于最先进技术，尤其在域外评估中展示了出色的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in CVPR 2025 (Highlight)",
      "pdf_url": "http://arxiv.org/pdf/2503.17417v2",
      "published_date": "2025-03-21 01:17:44 UTC",
      "updated_date": "2025-04-14 06:45:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:56:26.848216"
    },
    {
      "arxiv_id": "2503.17416v1",
      "title": "Debugging and Runtime Analysis of Neural Networks with VLMs (A Case Study)",
      "title_zh": "翻译失败",
      "authors": [
        "Boyue Caroline Hu",
        "Divya Gopinath",
        "Corina S. Pasareanu",
        "Nina Narodytska",
        "Ravi Mangal",
        "Susmit Jha"
      ],
      "abstract": "Debugging of Deep Neural Networks (DNNs), particularly vision models, is very\nchallenging due to the complex and opaque decision-making processes in these\nnetworks. In this paper, we explore multi-modal Vision-Language Models (VLMs),\nsuch as CLIP, to automatically interpret the opaque representation space of\nvision models using natural language. This in turn, enables a semantic analysis\nof model behavior using human-understandable concepts, without requiring costly\nhuman annotations. Key to our approach is the notion of semantic heatmap, that\nsuccinctly captures the statistical properties of DNNs in terms of the concepts\ndiscovered with the VLM and that are computed off-line using a held-out data\nset. We show the utility of semantic heatmaps for fault localization -- an\nessential step in debugging -- in vision models. Our proposed technique helps\nlocalize the fault in the network (encoder vs head) and also highlights the\nresponsible high-level concepts, by leveraging novel differential heatmaps,\nwhich summarize the semantic differences between the correct and incorrect\nbehaviour of the analyzed DNN. We further propose a lightweight runtime\nanalysis to detect and filter-out defects at runtime, thus improving the\nreliability of the analyzed DNNs. The runtime analysis works by measuring and\ncomparing the similarity between the heatmap computed for a new (unseen) input\nand the heatmaps computed a-priori for correct vs incorrect DNN behavior. We\nconsider two types of defects: misclassifications and vulnerabilities to\nadversarial attacks. We demonstrate the debugging and runtime analysis on a\ncase study involving a complex ResNet-based classifier trained on the RIVAL10\ndataset.",
      "tldr_zh": "该论文探讨了调试深度神经网络（DNNs），尤其是视觉模型的挑战，通过多模态视觉语言模型（VLMs）如 CLIP，使用自然语言自动解释模型的不透明表示空间，从而实现无需人工标注的语义分析。核心方法引入了语义热图（semantic heatmap），用于离线捕捉DNN的统计属性，并通过差分热图（differential heatmaps）定位网络故障（如编码器 vs 头部），突出高层次概念的差异。实验在RIVAL10数据集上的ResNet-based分类器案例中证明，该技术能有效检测误分类和对抗攻击漏洞，并通过轻量级运行时分析提高DNN的可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "CAIN 2025 (4th International Conference on AI Engineering -- Software\n  Engineering for AI)",
      "pdf_url": "http://arxiv.org/pdf/2503.17416v1",
      "published_date": "2025-03-21 01:12:57 UTC",
      "updated_date": "2025-03-21 01:12:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:56:39.309756"
    },
    {
      "arxiv_id": "2503.17415v1",
      "title": "Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)",
      "title_zh": "翻译失败",
      "authors": [
        "Yicheng Duan",
        "Xi Huang",
        "Duo Chen"
      ],
      "abstract": "The rapid growth of video content demands efficient and precise retrieval\nsystems. While vision-language models (VLMs) excel in representation learning,\nthey often struggle with adaptive, time-sensitive video retrieval. This paper\nintroduces a novel framework that combines vector similarity search with\ngraph-based data structures. By leveraging VLM embeddings for initial retrieval\nand modeling contextual relationships among video segments, our approach\nenables adaptive query refinement and improves retrieval accuracy. Experiments\ndemonstrate its precision, scalability, and robustness, offering an effective\nsolution for interactive video retrieval in dynamic environments.",
      "tldr_zh": "该论文针对视频内容快速增长的需求，提出一种新框架来提升基于 Vision-Language Models (VLMs) 的视频检索性能，解决其在适应性和时间敏感检索方面的局限性。该框架结合向量相似性搜索和基于图的数据结构，利用 VLM embeddings 进行初始检索，并通过建模视频片段间的上下文关系实现自适应查询精炼。实验结果证明，该方法显著提高了检索准确性，并展示了出色的精确性、可扩展性和鲁棒性，为动态环境中的交互式视频检索提供了有效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17415v1",
      "published_date": "2025-03-21 01:11:14 UTC",
      "updated_date": "2025-03-21 01:11:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:56:51.308938"
    },
    {
      "arxiv_id": "2503.16768v1",
      "title": "Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Meng Zhou",
        "Jiadong Xie",
        "Mingsheng Xu"
      ],
      "abstract": "Mainstream visual object tracking frameworks predominantly rely on template\nmatching paradigms. Their performance heavily depends on the quality of\ntemplate features, which becomes increasingly challenging to maintain in\ncomplex scenarios involving target deformation, occlusion, and background\nclutter. While existing spatiotemporal memory-based trackers emphasize memory\ncapacity expansion, they lack effective mechanisms for dynamic feature\nselection and adaptive fusion. To address this gap, we propose a Dynamic\nAttention Mechanism in Spatiotemporal Memory Network (DASTM) with two key\ninnovations: 1) A differentiable dynamic attention mechanism that adaptively\nadjusts channel-spatial attention weights by analyzing spatiotemporal\ncorrelations between the templates and memory features; 2) A lightweight gating\nnetwork that autonomously allocates computational resources based on target\nmotion states, prioritizing high-discriminability features in challenging\nscenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K\nbenchmarks demonstrate our DASTM's superiority, achieving state-of-the-art\nperformance in success rate, robustness, and real-time efficiency, thereby\noffering a novel solution for real-time tracking in complex environments.",
      "tldr_zh": "本论文针对主流视觉对象跟踪框架依赖模板匹配的局限性，特别是在目标变形、遮挡和背景杂乱的复杂场景中，提出了一种Dynamic Attention Mechanism in Spatiotemporal Memory Network (DASTM)。DASTM的关键创新包括：1) 一个可微动态注意力机制，通过分析模板和记忆特征之间的时空相关性，自适应调整通道-空间注意力权重；2) 一个轻量级门控网络，根据目标运动状态自主分配计算资源，优先选择高区分度特征。实验在OTB-2015、VOT 2018、LaSOT和GOT-10K基准上显示，DASTM在成功率、鲁棒性和实时效率方面达到了最先进水平，为复杂环境下的实时对象跟踪提供了新颖解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16768v1",
      "published_date": "2025-03-21 00:48:31 UTC",
      "updated_date": "2025-03-21 00:48:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:57:03.108494"
    },
    {
      "arxiv_id": "2503.17414v1",
      "title": "Opportunities and Challenges of Frontier Data Governance With Synthetic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Madhavendra Thakur",
        "Jason Hausenloy"
      ],
      "abstract": "Synthetic data, or data generated by machine learning models, is increasingly\nemerging as a solution to the data access problem. However, its use introduces\nsignificant governance and accountability challenges, and potentially debases\nexisting governance paradigms, such as compute and data governance. In this\npaper, we identify 3 key governance and accountability challenges that\nsynthetic data poses - it can enable the increased emergence of malicious\nactors, spontaneous biases and value drift. We thus craft 3 technical\nmechanisms to address these specific challenges, finding applications for\nsynthetic data towards adversarial training, bias mitigation and value\nreinforcement. These could not only counteract the risks of synthetic data, but\nserve as critical levers for governance of the frontier in the future.",
      "tldr_zh": "本研究探讨了合成数据（synthetic data）在前沿数据治理中的机遇与挑战，指出其虽能解决数据访问问题，但会引发治理难题，如增加恶意行为者（malicious actors）的出现、自发偏差（spontaneous biases）和价值漂移（value drift），从而削弱现有治理范式。论文识别出3个关键挑战，并提出相应的技术机制：利用合成数据进行对抗训练（adversarial training）、偏差缓解（bias mitigation）和价值强化（value reinforcement）。这些机制不仅能有效应对合成数据的风险，还可作为未来前沿治理的关键工具，提供更强的责任性和可控性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17414v1",
      "published_date": "2025-03-21 00:30:17 UTC",
      "updated_date": "2025-03-21 00:30:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T04:57:14.244804"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 106,
  "processed_papers_count": 106,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T04:57:34.666759"
}
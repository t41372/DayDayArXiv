[
  {
    "arxiv_id": "2503.17577v1",
    "title": "Measuring the Robustness of Audio Deepfake Detectors",
    "authors": [
      "Xiang Li",
      "Pin-Yu Chen",
      "Wenqi Wei"
    ],
    "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17577v1",
    "published_date": "2025-03-21 23:21:17 UTC",
    "updated_date": "2025-03-21 23:21:17 UTC"
  },
  {
    "arxiv_id": "2504.03682v1",
    "title": "Intelligent Resource Allocation Optimization for Cloud Computing via Machine Learning",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "abstract": "With the rapid expansion of cloud computing applications, optimizing resource\nallocation has become crucial for improving system performance and cost\nefficiency. This paper proposes an intelligent resource allocation algorithm\nthat leverages deep learning (LSTM) for demand prediction and reinforcement\nlearning (DQN) for dynamic scheduling. By accurately forecasting computing\nresource demands and enabling real-time adjustments, the proposed system\nenhances resource utilization by 32.5%, reduces average response time by 43.3%,\nand lowers operational costs by 26.6%. Experimental results in a production\ncloud environment confirm that the method significantly improves efficiency\nwhile maintaining high service quality. This study provides a scalable and\neffective solution for intelligent cloud resource management, offering valuable\ninsights for future cloud optimization strategies.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03682v1",
    "published_date": "2025-03-21 23:06:43 UTC",
    "updated_date": "2025-03-21 23:06:43 UTC"
  },
  {
    "arxiv_id": "2503.17569v1",
    "title": "Fairness-Driven LLM-based Causal Discovery with Active Learning and Dynamic Scoring",
    "authors": [
      "Khadija Zanna",
      "Akane Sano"
    ],
    "abstract": "Causal discovery (CD) plays a pivotal role in numerous scientific fields by\nclarifying the causal relationships that underlie phenomena observed in diverse\ndisciplines. Despite significant advancements in CD algorithms that enhance\nbias and fairness analyses in machine learning, their application faces\nchallenges due to the high computational demands and complexities of\nlarge-scale data. This paper introduces a framework that leverages Large\nLanguage Models (LLMs) for CD, utilizing a metadata-based approach akin to the\nreasoning processes of human experts. By shifting from pairwise queries to a\nmore scalable breadth-first search (BFS) strategy, the number of required\nqueries is reduced from quadratic to linear in terms of variable count, thereby\naddressing scalability concerns inherent in previous approaches. This method\nutilizes an Active Learning (AL) and a Dynamic Scoring Mechanism that\nprioritizes queries based on their potential information gain, combining mutual\ninformation, partial correlation, and LLM confidence scores to refine the\ncausal graph more efficiently and accurately. This BFS query strategy reduces\nthe required number of queries significantly, thereby addressing scalability\nconcerns inherent in previous approaches. This study provides a more scalable\nand efficient solution for leveraging LLMs in fairness-driven CD, highlighting\nthe effects of the different parameters on performance. We perform fairness\nanalyses on the inferred causal graphs, identifying direct and indirect effects\nof sensitive attributes on outcomes. A comparison of these analyses against\nthose from graphs produced by baseline methods highlights the importance of\naccurate causal graph construction in understanding bias and ensuring fairness\nin machine learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17569v1",
    "published_date": "2025-03-21 22:58:26 UTC",
    "updated_date": "2025-03-21 22:58:26 UTC"
  },
  {
    "arxiv_id": "2503.17553v1",
    "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent",
    "authors": [
      "Humza Nusrat",
      "Bing Luo",
      "Ryan Hall",
      "Joshua Kim",
      "Hassan Bagher-Ebadian",
      "Anthony Doemer",
      "Benjamin Movsas",
      "Kundan Thind"
    ],
    "abstract": "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "physics.med-ph",
    "comment": "19 pages, 5 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.17553v1",
    "published_date": "2025-03-21 22:01:19 UTC",
    "updated_date": "2025-03-21 22:01:19 UTC"
  },
  {
    "arxiv_id": "2503.17551v1",
    "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion",
    "authors": [
      "Yu Sun",
      "Yin Li",
      "Ruixiao Sun",
      "Chunhui Liu",
      "Fangming Zhou",
      "Ze Jin",
      "Linjie Wang",
      "Xiang Shen",
      "Zhuolin Hao",
      "Hongyu Xiong"
    ],
    "abstract": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17551v1",
    "published_date": "2025-03-21 21:55:05 UTC",
    "updated_date": "2025-03-21 21:55:05 UTC"
  },
  {
    "arxiv_id": "2503.17547v1",
    "title": "Learning Multi-Level Features with Matryoshka Sparse Autoencoders",
    "authors": [
      "Bart Bussmann",
      "Noa Nabeshima",
      "Adam Karvonen",
      "Neel Nanda"
    ],
    "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting\nneural networks by extracting the concepts represented in their activations.\nHowever, choosing the size of the SAE dictionary (i.e. number of learned\nconcepts) creates a tension: as dictionary size increases to capture more\nrelevant concepts, sparsity incentivizes features to be split or absorbed into\nmore specific features, leaving high-level features missing or warped. We\nintroduce Matryoshka SAEs, a novel variant that addresses these issues by\nsimultaneously training multiple nested dictionaries of increasing size,\nforcing the smaller dictionaries to independently reconstruct the inputs\nwithout using the larger dictionaries. This organizes features hierarchically -\nthe smaller dictionaries learn general concepts, while the larger dictionaries\nlearn more specific concepts, without incentive to absorb the high-level\nfeatures. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find\nsuperior performance on sparse probing and targeted concept erasure tasks, more\ndisentangled concept representations, and reduced feature absorption. While\nthere is a minor tradeoff with reconstruction performance, we believe\nMatryoshka SAEs are a superior alternative for practical tasks, as they enable\ntraining arbitrarily large SAEs while retaining interpretable features at\ndifferent levels of abstraction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17547v1",
    "published_date": "2025-03-21 21:43:28 UTC",
    "updated_date": "2025-03-21 21:43:28 UTC"
  },
  {
    "arxiv_id": "2503.17544v1",
    "title": "PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning",
    "authors": [
      "Yan Zhang",
      "Yao Feng",
      "Alpár Cseke",
      "Nitin Saini",
      "Nathan Bajandas",
      "Nicolas Heron",
      "Michael J. Black"
    ],
    "abstract": "To build a motor system of the interactive avatar, it is essential to develop\na generative motion model drives the body to move through 3D space in a\nperpetual, realistic, controllable, and responsive manner. Although motion\ngeneration has been extensively studied, most methods do not support ``embodied\nintelligence'' due to their offline setting, slow speed, limited motion\nlengths, or unnatural movements. To overcome these limitations, we propose\nPRIMAL, an autoregressive diffusion model that is learned with a two-stage\nparadigm, inspired by recent advances in foundation models. In the pretraining\nstage, the model learns motion dynamics from a large number of sub-second\nmotion segments, providing ``motor primitives'' from which more complex motions\nare built. In the adaptation phase, we employ a ControlNet-like adaptor to\nfine-tune the motor control for semantic action generation and spatial target\nreaching. Experiments show that physics effects emerge from our training. Given\na single-frame initial state, our model not only generates unbounded,\nrealistic, and controllable motion, but also enables the avatar to be\nresponsive to induced impulses in real time. In addition, we can effectively\nand efficiently adapt our base model to few-shot personalized actions and the\ntask of spatial control. Evaluations show that our proposed method outperforms\nstate-of-the-art baselines. We leverage the model to create a real-time\ncharacter animation system in Unreal Engine that is highly responsive and\nnatural. Code, models, and more results are available at:\nhttps://yz-cnsdqz.github.io/eigenmotion/PRIMAL",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.17544v1",
    "published_date": "2025-03-21 21:27:57 UTC",
    "updated_date": "2025-03-21 21:27:57 UTC"
  },
  {
    "arxiv_id": "2503.17523v1",
    "title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models",
    "authors": [
      "Linlu Qiu",
      "Fei Sha",
      "Kelsey Allen",
      "Yoon Kim",
      "Tal Linzen",
      "Sjoerd van Steenkiste"
    ],
    "abstract": "Artificial intelligence systems based on large language models (LLMs) are\nincreasingly used as agents that interact with users and with the world. To do\nso successfully, LLMs need to construct internal representations of the world\nand form probabilistic beliefs about those representations. To provide a user\nwith personalized recommendations, for example, the LLM needs to gradually\ninfer the user's preferences, over the course of multiple interactions. To\nevaluate whether contemporary LLMs are able to do so, we use the Bayesian\ninference framework from probability theory, which lays out the optimal way to\nupdate an agent's beliefs as it receives new information. We first show that\nthe LLMs do not update their beliefs as expected from the Bayesian framework,\nand that consequently their predictions do not improve as expected as more\ninformation becomes available, even less so than we find is the case for\nhumans. To address this issue, we teach the LLMs to reason in a Bayesian manner\nby training them to mimic the predictions of an optimal Bayesian model. We find\nthat this approach not only significantly improves the LLM's performance on the\nparticular recommendation task it is trained on, but also enables\ngeneralization to other tasks. This suggests that this method endows the LLM\nwith broader Bayesian reasoning skills. More generally, our results indicate\nthat LLMs can learn about reasoning strategies effectively and generalize those\nskills to new domains, which in part explains LLMs' empirical success.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17523v1",
    "published_date": "2025-03-21 20:13:04 UTC",
    "updated_date": "2025-03-21 20:13:04 UTC"
  },
  {
    "arxiv_id": "2503.17515v2",
    "title": "A Predictive Services Architecture for Efficient Airspace Operations",
    "authors": [
      "Ítalo Romani de Oliveira",
      "Samet Ayhan",
      "Glaucia Balvedi",
      "Michael Biglin",
      "Pablo Costas",
      "Euclides C. Pinto Neto",
      "Alexandre Leite",
      "Felipe C. F. de Azevedo"
    ],
    "abstract": "Predicting air traffic congestion and flow management is essential for\nairlines and Air Navigation Service Providers (ANSP) to enhance operational\nefficiency. Accurate estimates of future airport capacity and airspace density\nare vital for better airspace management, reducing air traffic controller\nworkload and fuel consumption, ultimately promoting sustainable aviation. While\nexisting literature has addressed these challenges, data management and query\nprocessing remain complex due to the vast volume of high-rate air traffic data.\nMany analytics use cases require a common pre-processing infrastructure, as\nad-hoc approaches are insufficient. Additionally, linear prediction models\noften fall short, necessitating more advanced techniques.\n  This paper presents a data processing and predictive services architecture\nthat ingests large, uncorrelated, and noisy streaming data to forecast future\nairspace system states. The system continuously collects raw data, periodically\ncompresses it, and stores it in NoSQL databases for efficient query processing.\nFor prediction, the system learns from historical traffic by extracting key\nfeatures such as airport arrival and departure events, sector boundary\ncrossings, weather parameters, and other air traffic data. These features are\ninput into various regression models, including linear, non-linear, and\nensemble models, with the best-performing model selected for predictions. We\nevaluate this infrastructure across three prediction use cases in the US\nNational Airspace System (NAS) and a segment of European airspace, using\nextensive real operations data, confirming that our system can predict future\nsystem states efficiently and accurately.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17515v2",
    "published_date": "2025-03-21 19:57:38 UTC",
    "updated_date": "2025-04-18 19:15:45 UTC"
  },
  {
    "arxiv_id": "2503.17514v2",
    "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "authors": [
      "Ken Ziyu Liu",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "Peter Kairouz",
      "Sanmi Koyejo",
      "Percy Liang",
      "Nicolas Papernot"
    ],
    "abstract": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17514v2",
    "published_date": "2025-03-21 19:57:04 UTC",
    "updated_date": "2025-03-25 04:43:33 UTC"
  },
  {
    "arxiv_id": "2503.17513v1",
    "title": "Improving Quantization with Post-Training Model Expansion",
    "authors": [
      "Giuseppe Franco",
      "Pablo Monteagudo-Lago",
      "Ian Colbert",
      "Nicholas Fraser",
      "Michaela Blott"
    ],
    "abstract": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the zero-shot accuracy gap to full precision by an average of 3%\nrelative to both QuaRot and SpinQuant with only 5% more parameters, which is\nstill a 3.8% reduction in volume relative to a BF16 reference model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17513v1",
    "published_date": "2025-03-21 19:56:59 UTC",
    "updated_date": "2025-03-21 19:56:59 UTC"
  },
  {
    "arxiv_id": "2503.17509v1",
    "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations",
    "authors": [
      "Joseph Gatto",
      "Parker Seegmiller",
      "Timothy Burdick",
      "Inas S. Khayal",
      "Sarah DeLozier",
      "Sarah M. Preum"
    ],
    "abstract": "Follow-up question generation is an essential feature of dialogue systems as\nit can reduce conversational ambiguity and enhance modeling complex\ninteractions. Conversational contexts often pose core NLP challenges such as\n(i) extracting relevant information buried in fragmented data sources, and (ii)\nmodeling parallel thought processes. These two challenges occur frequently in\nmedical dialogue as a doctor asks questions based not only on patient\nutterances but also their prior EHR data and current diagnostic hypotheses.\nAsking medical questions in asynchronous conversations compounds these issues\nas doctors can only rely on static EHR information to motivate follow-up\nquestions.\n  To address these challenges, we introduce FollowupQ, a novel framework for\nenhancing asynchronous medical conversation. FollowupQ is a multi-agent\nframework that processes patient messages and EHR data to generate personalized\nfollow-up questions, clarifying patient-reported medical conditions. FollowupQ\nreduces requisite provider follow-up communications by 34%. It also improves\nperformance by 17% and 5% on real and synthetic data, respectively. We also\nrelease the first public dataset of asynchronous medical messages with linked\nEHR data alongside 2,300 follow-up questions written by clinical experts for\nthe wider NLP research community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 Pages, 7 Figures, 6 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.17509v1",
    "published_date": "2025-03-21 19:40:53 UTC",
    "updated_date": "2025-03-21 19:40:53 UTC"
  },
  {
    "arxiv_id": "2503.17502v1",
    "title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets",
    "authors": [
      "Hamed Jelodar",
      "Mohammad Meymani",
      "Roozbeh Razavi-Far"
    ],
    "abstract": "Large language models (LLMs) and transformer-based architectures are\nincreasingly utilized for source code analysis. As software systems grow in\ncomplexity, integrating LLMs into code analysis workflows becomes essential for\nenhancing efficiency, accuracy, and automation. This paper explores the role of\nLLMs for different code analysis tasks, focusing on three key aspects: 1) what\nthey can analyze and their applications, 2) what models are used and 3) what\ndatasets are used, and the challenges they face. Regarding the goal of this\nresearch, we investigate scholarly articles that explore the use of LLMs for\nsource code analysis to uncover research developments, current trends, and the\nintellectual structure of this emerging field. Additionally, we summarize\nlimitations and highlight essential tools, datasets, and key challenges, which\ncould be valuable for future work.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17502v1",
    "published_date": "2025-03-21 19:29:50 UTC",
    "updated_date": "2025-03-21 19:29:50 UTC"
  },
  {
    "arxiv_id": "2503.17494v1",
    "title": "Efficient Knowledge Distillation via Curriculum Extraction",
    "authors": [
      "Shivam Gupta",
      "Sushrut Karmalkar"
    ],
    "abstract": "Knowledge distillation is a technique used to train a small student network\nusing the output generated by a large teacher network, and has many empirical\nadvantages~\\citep{Hinton2015DistillingTK}. While the standard one-shot approach\nto distillation only uses the output of the final teacher network, recent\nwork~\\citep{panigrahi2024progressive} has shown that using intermediate\ncheckpoints from the teacher's training process as an implicit ``curriculum''\nfor progressive distillation can significantly speed up training. However, such\nschemes require storing these checkpoints, and often require careful selection\nof the intermediate checkpoints to train on, which can be impractical for\nlarge-scale training.\n  In this paper, we show that a curriculum can be \\emph{extracted} from just\nthe fully trained teacher network, and that this extracted curriculum can give\nsimilar efficiency benefits to those of progressive distillation. Our\nextraction scheme is natural; we use a random projection of the hidden\nrepresentations of the teacher network to progressively train the student\nnetwork, before training using the output of the full network. We show that our\nscheme significantly outperforms one-shot distillation and achieves a\nperformance similar to that of progressive distillation for learning sparse\nparities with two-layer networks, and provide theoretical guarantees for this\nsetting. Additionally, we show that our method outperforms one-shot\ndistillation even when using transformer-based architectures, both for\nsparse-parity learning, and language modeling tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17494v1",
    "published_date": "2025-03-21 19:09:41 UTC",
    "updated_date": "2025-03-21 19:09:41 UTC"
  },
  {
    "arxiv_id": "2503.17486v3",
    "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes",
    "authors": [
      "Zhengqing Gao",
      "Dongting Hu",
      "Jia-Wang Bian",
      "Huan Fu",
      "Yan Li",
      "Tongliang Liu",
      "Mingming Gong",
      "Kun Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17486v3",
    "published_date": "2025-03-21 18:55:14 UTC",
    "updated_date": "2025-04-08 12:19:01 UTC"
  },
  {
    "arxiv_id": "2503.17485v1",
    "title": "SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia",
    "authors": [
      "Lama Ayash",
      "Hassan Alhuzali",
      "Ashwag Alasmari",
      "Sultan Aloufi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing; however, they often struggle to accurately capture\nand reflect cultural nuances. This research addresses this challenge by\nfocusing on Saudi Arabia, a country characterized by diverse dialects and rich\ncultural traditions. We introduce SaudiCulture, a novel benchmark designed to\nevaluate the cultural competence of LLMs within the distinct geographical and\ncultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of\nquestions covering five major geographical regions, such as West, East, South,\nNorth, and Center, along with general questions applicable across all regions.\nThe dataset encompasses a broad spectrum of cultural domains, including food,\nclothing, entertainment, celebrations, and crafts. To ensure a rigorous\nevaluation, SaudiCulture includes questions of varying complexity, such as\nopen-ended, single-choice, and multiple-choice formats, with some requiring\nmultiple correct answers. Additionally, the dataset distinguishes between\ncommon cultural knowledge and specialized regional aspects. We conduct\nextensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and\nAceGPT, analyzing their performance across different question types and\ncultural contexts. Our findings reveal that all models experience significant\nperformance declines when faced with highly specialized or region-specific\nquestions, particularly those requiring multiple correct responses.\nAdditionally, certain cultural categories are more easily identifiable than\nothers, further highlighting inconsistencies in LLMs cultural understanding.\nThese results emphasize the importance of incorporating region-specific\nknowledge into LLMs training to enhance their cultural competence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, under-review",
    "pdf_url": "http://arxiv.org/pdf/2503.17485v1",
    "published_date": "2025-03-21 18:55:10 UTC",
    "updated_date": "2025-03-21 18:55:10 UTC"
  },
  {
    "arxiv_id": "2503.17482v1",
    "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models",
    "authors": [
      "Keyon Vafa",
      "Sarah Bentley",
      "Jon Kleinberg",
      "Sendhil Mullainathan"
    ],
    "abstract": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17482v1",
    "published_date": "2025-03-21 18:51:56 UTC",
    "updated_date": "2025-03-21 18:51:56 UTC"
  },
  {
    "arxiv_id": "2503.17479v1",
    "title": "Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication",
    "authors": [
      "Yiwen Xu",
      "Monideep Chakraborti",
      "Tianyi Zhang",
      "Katelyn Eng",
      "Aanchan Mohan",
      "Mirjana Prpa"
    ],
    "abstract": "In this paper, we present Speak Ease: an augmentative and alternative\ncommunication (AAC) system to support users' expressivity by integrating\nmultimodal input, including text, voice, and contextual cues (conversational\npartner and emotional tone), with large language models (LLMs). Speak Ease\ncombines automatic speech recognition (ASR), context-aware LLM-based outputs,\nand personalized text-to-speech technologies to enable more personalized,\nnatural-sounding, and expressive communication. Through an exploratory\nfeasibility study and focus group evaluation with speech and language\npathologists (SLPs), we assessed Speak Ease's potential to enable expressivity\nin AAC. The findings highlight the priorities and needs of AAC users and the\nsystem's ability to enhance user expressivity by supporting more personalized\nand contextually relevant communication. This work provides insights into the\nuse of multimodal inputs and LLM-driven features to improve AAC systems and\nsupport expressivity.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17479v1",
    "published_date": "2025-03-21 18:50:05 UTC",
    "updated_date": "2025-03-21 18:50:05 UTC"
  },
  {
    "arxiv_id": "2503.17475v1",
    "title": "Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis",
    "authors": [
      "Gary Y. Li",
      "Li Chen",
      "Bryson Hicks",
      "Nikolai Schnittke",
      "David O. Kessler",
      "Jeffrey Shupp",
      "Maria Parker",
      "Cristiana Baloescu",
      "Christopher Moore",
      "Cynthia Gregory",
      "Kenton Gregory",
      "Balasundar Raju",
      "Jochen Kruecker",
      "Alvin Chen"
    ],
    "abstract": "Computer-aided pathology detection algorithms for video-based imaging\nmodalities must accurately interpret complex spatiotemporal information by\nintegrating findings across multiple frames. Current state-of-the-art methods\noperate by classifying on video sub-volumes (tubelets), but they often lose\nglobal spatial context by focusing only on local regions within detection ROIs.\nHere we propose a lightweight framework for tubelet-based object detection and\nvideo classification that preserves both global spatial context and fine\nspatiotemporal features. To address the loss of global context, we embed\ntubelet location, size, and confidence as inputs to the classifier.\nAdditionally, we use ROI-aligned feature maps from a pre-trained detection\nmodel, leveraging learned feature representations to increase the receptive\nfield and reduce computational complexity. Our method is efficient, with the\nspatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our\napproach to detect and classify lung consolidation and pleural effusion in\nultrasound videos. Five-fold cross-validation on 14,804 videos from 828\npatients shows our method outperforms previous tubelet-based approaches and is\nsuited for real-time workflows.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ISBI Oral 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.17475v1",
    "published_date": "2025-03-21 18:39:42 UTC",
    "updated_date": "2025-03-21 18:39:42 UTC"
  },
  {
    "arxiv_id": "2503.17456v1",
    "title": "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer",
    "authors": [
      "Soumen Kumar Mondal",
      "Sayambhu Sen",
      "Abhishek Singhania",
      "Preethi Jyothi"
    ],
    "abstract": "Multilingual large language models (LLMs) aim towards robust natural language\nunderstanding across diverse languages, yet their performance significantly\ndegrades on low-resource languages. This work explores whether existing\ntechniques to identify language-specific neurons can be leveraged to enhance\ncross-lingual task performance of lowresource languages. We conduct detailed\nexperiments covering existing language-specific neuron identification\ntechniques (such as Language Activation Probability Entropy and activation\nprobability-based thresholding) and neuron-specific LoRA fine-tuning with\nmodels like Llama 3.1 and Mistral Nemo. We find that such neuron-specific\ninterventions are insufficient to yield cross-lingual improvements on\ndownstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights\nthe challenges in achieving cross-lingual generalization and provides critical\ninsights for multilingual LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted (oral) at NAACL 2025 (InsightsNLP)",
    "pdf_url": "http://arxiv.org/pdf/2503.17456v1",
    "published_date": "2025-03-21 18:08:11 UTC",
    "updated_date": "2025-03-21 18:08:11 UTC"
  },
  {
    "arxiv_id": "2503.17452v1",
    "title": "CausalRivers -- Scaling up benchmarking of causal discovery for real-world time-series",
    "authors": [
      "Gideon Stein",
      "Maha Shadaydeh",
      "Jan Blunk",
      "Niklas Penzel",
      "Joachim Denzler"
    ],
    "abstract": "Causal discovery, or identifying causal relationships from observational\ndata, is a notoriously challenging task, with numerous methods proposed to\ntackle it. Despite this, in-the-wild evaluation of these methods is still\nlacking, as works frequently rely on synthetic data evaluation and sparse\nreal-world examples under critical theoretical assumptions. Real-world causal\nstructures, however, are often complex, making it hard to decide on a proper\ncausal discovery strategy. To bridge this gap, we introduce CausalRivers, the\nlargest in-the-wild causal discovery benchmarking kit for time-series data to\ndate. CausalRivers features an extensive dataset on river discharge that covers\nthe eastern German territory (666 measurement stations) and the state of\nBavaria (494 measurement stations). It spans the years 2019 to 2023 with a\n15-minute temporal resolution. Further, we provide additional data from a flood\naround the Elbe River, as an event with a pronounced distributional shift.\nLeveraging multiple sources of information and time-series meta-data, we\nconstructed two distinct causal ground truth graphs (Bavaria and eastern\nGermany). These graphs can be sampled to generate thousands of subgraphs to\nbenchmark causal discovery across diverse and challenging settings. To\ndemonstrate the utility of CausalRivers, we evaluate several causal discovery\napproaches through a set of experiments to identify areas for improvement.\nCausalRivers has the potential to facilitate robust evaluations and comparisons\nof causal discovery methods. Besides this primary purpose, we also expect that\nthis dataset will be relevant for connected areas of research, such as\ntime-series forecasting and anomaly detection. Based on this, we hope to push\nbenchmark-driven method development that fosters advanced techniques for causal\ndiscovery, as is the case for many other areas of machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 8 figures, ICLR2025 main track",
    "pdf_url": "http://arxiv.org/pdf/2503.17452v1",
    "published_date": "2025-03-21 18:02:35 UTC",
    "updated_date": "2025-03-21 18:02:35 UTC"
  },
  {
    "arxiv_id": "2503.17439v1",
    "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
    "authors": [
      "Zhuoshi Pan",
      "Yu Li",
      "Honglin Lin",
      "Qizhi Pei",
      "Zinan Tang",
      "Wei Wu",
      "Chenlin Ming",
      "H. Vicky Zhao",
      "Conghui He",
      "Lijun Wu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures, 4 tables, under review",
    "pdf_url": "http://arxiv.org/pdf/2503.17439v1",
    "published_date": "2025-03-21 17:59:10 UTC",
    "updated_date": "2025-03-21 17:59:10 UTC"
  },
  {
    "arxiv_id": "2503.17354v1",
    "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
    "authors": [
      "David Rein",
      "Joel Becker",
      "Amy Deng",
      "Seraphina Nix",
      "Chris Canal",
      "Daniel O'Connel",
      "Pip Arnott",
      "Ryan Bloom",
      "Thomas Broadley",
      "Katharyn Garcia",
      "Brian Goodrich",
      "Max Hasin",
      "Sami Jawhar",
      "Megan Kinniment",
      "Thomas Kwa",
      "Aron Lajko",
      "Nate Rush",
      "Lucas Jun Koba Sato",
      "Sydney Von Arx",
      "Ben West",
      "Lawrence Chan",
      "Elizabeth Barnes"
    ],
    "abstract": "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 10 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.17354v1",
    "published_date": "2025-03-21 17:54:01 UTC",
    "updated_date": "2025-03-21 17:54:01 UTC"
  },
  {
    "arxiv_id": "2503.17353v1",
    "title": "NdLinear Is All You Need for Representation Learning",
    "authors": [
      "Alex Reneau",
      "Jerry Yao-Chieh Hu",
      "Zhongfang Zhuang",
      "Ting-Chun Liu"
    ],
    "abstract": "Many high-impact machine learning tasks involve multi-dimensional data (e.g.,\nimages, volumetric medical scans, multivariate time-series). Yet, most neural\narchitectures flatten inputs, discarding critical cross-dimension information.\nWe introduce NdLinear, a novel linear transformation that preserves these\nstructures without extra overhead. By operating separately along each\ndimension, NdLinear captures dependencies that standard fully connected layers\noverlook. Extensive experiments across convolutional, recurrent, and\ntransformer-based networks show significant improvements in representational\npower and parameter efficiency. Crucially, NdLinear serves as a foundational\nbuilding block for large-scale foundation models by operating on any unimodal\nor multimodal data in its native form. This removes the need for flattening or\nmodality-specific preprocessing. Ndlinear rethinks core architectural\npriorities beyond attention, enabling more expressive, context-aware models at\nscale. We propose NdLinear as a drop-in replacement for standard linear layers\n-- marking an important step toward next-generation neural architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at https://github.com/ensemble-core/NdLinear",
    "pdf_url": "http://arxiv.org/pdf/2503.17353v1",
    "published_date": "2025-03-21 17:52:44 UTC",
    "updated_date": "2025-03-21 17:52:44 UTC"
  },
  {
    "arxiv_id": "2503.17340v1",
    "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation",
    "authors": [
      "Congyi Fan",
      "Jian Guan",
      "Xuanjia Zhao",
      "Dongli Xu",
      "Youtian Lin",
      "Tong Ye",
      "Pengming Feng",
      "Haiwei Pan"
    ],
    "abstract": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17340v1",
    "published_date": "2025-03-21 17:42:50 UTC",
    "updated_date": "2025-03-21 17:42:50 UTC"
  },
  {
    "arxiv_id": "2503.17339v1",
    "title": "Can AI expose tax loopholes? Towards a new generation of legal policy assistants",
    "authors": [
      "Peter Fratrič",
      "Nils Holzenberger",
      "David Restrepo Amariles"
    ],
    "abstract": "The legislative process is the backbone of a state built on solid\ninstitutions. Yet, due to the complexity of laws -- particularly tax law --\npolicies may lead to inequality and social tensions. In this study, we\nintroduce a novel prototype system designed to address the issues of tax\nloopholes and tax avoidance. Our hybrid solution integrates a natural language\ninterface with a domain-specific language tailored for planning. We demonstrate\non a case study how tax loopholes and avoidance schemes can be exposed. We\nconclude that our prototype can help enhance social welfare by systematically\nidentifying and addressing tax gaps stemming from loopholes.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17339v1",
    "published_date": "2025-03-21 17:40:06 UTC",
    "updated_date": "2025-03-21 17:40:06 UTC"
  },
  {
    "arxiv_id": "2503.17338v1",
    "title": "Capturing Individual Human Preferences with Reward Features",
    "authors": [
      "André Barreto",
      "Vincent Dumoulin",
      "Yiran Mao",
      "Nicolas Perez-Nieves",
      "Bobak Shahriari",
      "Yann Dauphin",
      "Doina Precup",
      "Hugo Larochelle"
    ],
    "abstract": "Reinforcement learning from human feedback usually models preferences using a\nreward model that does not distinguish between people. We argue that this is\nunlikely to be a good design choice in contexts with high potential for\ndisagreement, like in the training of large language models. We propose a\nmethod to specialise a reward model to a person or group of people. Our\napproach builds on the observation that individual preferences can be captured\nas a linear combination of a set of general reward features. We show how to\nlearn such features and subsequently use them to quickly adapt the reward model\nto a specific individual, even if their preferences are not reflected in the\ntraining data. We present experiments with large language models comparing the\nproposed architecture with a non-adaptive reward model and also adaptive\ncounterparts, including models that do in-context personalisation. Depending on\nhow much disagreement there is in the training data, our model either\nsignificantly outperforms the baselines or matches their performance with a\nsimpler architecture and more stable training.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17338v1",
    "published_date": "2025-03-21 17:39:33 UTC",
    "updated_date": "2025-03-21 17:39:33 UTC"
  },
  {
    "arxiv_id": "2503.17336v1",
    "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs",
    "authors": [
      "Reem Gody",
      "Mohamed Abdelghaffar",
      "Mohammed Jabreel",
      "Ahmed Tawfik"
    ],
    "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17336v1",
    "published_date": "2025-03-21 17:34:37 UTC",
    "updated_date": "2025-03-21 17:34:37 UTC"
  },
  {
    "arxiv_id": "2503.17332v3",
    "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
    "authors": [
      "Yuxuan Zhu",
      "Antony Kellermann",
      "Dylan Bowman",
      "Philip Li",
      "Akul Gupta",
      "Adarsh Danda",
      "Richard Fang",
      "Conner Jensen",
      "Eric Ihli",
      "Jason Benn",
      "Jet Geronimo",
      "Avi Dhir",
      "Sudhit Rao",
      "Kaicheng Yu",
      "Twm Stone",
      "Daniel Kang"
    ],
    "abstract": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages, 4 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.17332v3",
    "published_date": "2025-03-21 17:32:32 UTC",
    "updated_date": "2025-04-10 23:50:28 UTC"
  },
  {
    "arxiv_id": "2504.13856v1",
    "title": "Towards Balancing Preference and Performance through Adaptive Personalized Explainability",
    "authors": [
      "Andrew Silva",
      "Pradyumna Tambwekar",
      "Mariah Schrum",
      "Matthew Gombolay"
    ],
    "abstract": "As robots and digital assistants are deployed in the real world, these agents\nmust be able to communicate their decision-making criteria to build trust,\nimprove human-robot teaming, and enable collaboration. While the field of\nexplainable artificial intelligence (xAI) has made great strides to enable such\ncommunication, these advances often assume that one xAI approach is ideally\nsuited to each problem (e.g., decision trees to explain how to triage patients\nin an emergency or feature-importance maps to explain radiology reports). This\nfails to recognize that users have diverse experiences or preferences for\ninteraction modalities. In this work, we present two user-studies set in a\nsimulated autonomous vehicle (AV) domain. We investigate (1) population-level\npreferences for xAI and (2) personalization strategies for providing robot\nexplanations. We find significant differences between xAI modes (language\nexplanations, feature-importance maps, and decision trees) in both preference\n(p < 0.01) and performance (p < 0.05). We also observe that a participant's\npreferences do not always align with their performance, motivating our\ndevelopment of an adaptive personalization strategy to balance the two. We show\nthat this strategy yields significant performance gains (p < 0.05), and we\nconclude with a discussion of our findings and implications for xAI in\nhuman-robot interactions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "20 pages, 19 figures, HRI 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.13856v1",
    "published_date": "2025-03-21 17:31:25 UTC",
    "updated_date": "2025-03-21 17:31:25 UTC"
  },
  {
    "arxiv_id": "2503.17309v1",
    "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
    "authors": [
      "Kun Chu",
      "Xufeng Zhao",
      "Cornelius Weber",
      "Stefan Wermter"
    ],
    "abstract": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and video are available at https://github.com/Kchu/LLM-MAP",
    "pdf_url": "http://arxiv.org/pdf/2503.17309v1",
    "published_date": "2025-03-21 17:04:01 UTC",
    "updated_date": "2025-03-21 17:04:01 UTC"
  },
  {
    "arxiv_id": "2503.17299v1",
    "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
    "authors": [
      "Yashas Annadani",
      "Syrine Belakaria",
      "Stefano Ermon",
      "Stefan Bauer",
      "Barbara E Engelhardt"
    ],
    "abstract": "Offline multi-objective optimization aims to identify Pareto-optimal\nsolutions given a dataset of designs and their objective values. In this work,\nwe propose a preference-guided diffusion model that generates Pareto-optimal\ndesigns by leveraging a classifier-based guidance mechanism. Our guidance\nclassifier is a preference model trained to predict the probability that one\ndesign dominates another, directing the diffusion model toward optimal regions\nof the design space. Crucially, this preference model generalizes beyond the\ntraining distribution, enabling the discovery of Pareto-optimal solutions\noutside the observed dataset. We introduce a novel diversity-aware preference\nguidance, augmenting Pareto dominance preference with diversity criteria. This\nensures that generated solutions are optimal and well-distributed across the\nobjective space, a capability absent in prior generative methods for offline\nmulti-objective optimization. We evaluate our approach on various continuous\noffline multi-objective optimization tasks and find that it consistently\noutperforms other inverse/generative approaches while remaining competitive\nwith forward/surrogate-based optimization methods. Our results highlight the\neffectiveness of classifier-guided diffusion models in generating diverse and\nhigh-quality solutions that approximate the Pareto front well.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17299v1",
    "published_date": "2025-03-21 16:49:38 UTC",
    "updated_date": "2025-03-21 16:49:38 UTC"
  },
  {
    "arxiv_id": "2503.17251v1",
    "title": "Breaking the Symmetries of Indistinguishable Objects",
    "authors": [
      "Ozgur Akgun",
      "Mun See Chang",
      "Ian P. Gent",
      "Christopher Jefferson"
    ],
    "abstract": "Indistinguishable objects often occur when modelling problems in constraint\nprogramming, as well as in other related paradigms. They occur when objects can\nbe viewed as being drawn from a set of unlabelled objects, and the only\noperation allowed on them is equality testing. For example, the golfers in the\nsocial golfer problem are indistinguishable. If we do label the golfers, then\nany relabelling of the golfers in one solution gives another valid solution.\nTherefore, we can regard the symmetric group of size $n$ as acting on a set of\n$n$ indistinguishable objects. In this paper, we show how we can break the\nsymmetries resulting from indistinguishable objects. We show how symmetries on\nindistinguishable objects can be defined properly in complex types, for example\nin a matrix indexed by indistinguishable objects. We then show how the\nresulting symmetries can be broken correctly. In Essence, a high-level\nmodelling language, indistinguishable objects are encapsulated in \"unnamed\ntypes\". We provide an implementation of complete symmetry breaking for unnamed\ntypes in Essence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17251v1",
    "published_date": "2025-03-21 15:56:52 UTC",
    "updated_date": "2025-03-21 15:56:52 UTC"
  },
  {
    "arxiv_id": "2503.17247v1",
    "title": "KL3M Tokenizers: A Family of Domain-Specific and Character-Level Tokenizers for Legal, Financial, and Preprocessing Applications",
    "authors": [
      "Michael J Bommarito",
      "Daniel Martin Katz",
      "Jillian Bommarito"
    ],
    "abstract": "We present the KL3M tokenizers, a family of specialized tokenizers for legal,\nfinancial, and governmental text. Despite established work on tokenization,\nspecialized tokenizers for professional domains remain understudied. Our paper\noffers two main contributions to this area.\n  First, we introduce domain-specific BPE tokenizers for legal, financial, and\ngovernmental text. Our kl3m-004-128k-cased tokenizer uses 9-17% fewer tokens\nthan GPT-4o and Llama3 for domain-specific documents, despite having a smaller\nvocabulary. For specialized terminology, our cased tokenizer is even more\nefficient, using up to 83% fewer tokens for legal terms and 39% fewer tokens\nfor financial terms.\n  Second, we develop character-level BPE tokenizers (4K, 8K, and 16K vocabulary\nsizes) for text correction tasks like OCR post-processing. These tokenizers\nkeep consistent token boundaries between error-containing and correct text,\nmaking it easier for models to learn correction patterns.\n  These tokenizers help professional applications by fitting more text in\ncontext windows, reducing computational needs, and preserving the meaning of\ndomain-specific terms. Our analysis shows these efficiency gains directly\nbenefit the processing of long legal and financial documents. We release all\ntokenizers and code through GitHub and Hugging Face to support further research\nin specialized tokenization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 7 tables, 3 figures; Source code available at\n  https://github.com/alea-institute/kl3m-tokenizer-paper",
    "pdf_url": "http://arxiv.org/pdf/2503.17247v1",
    "published_date": "2025-03-21 15:51:43 UTC",
    "updated_date": "2025-03-21 15:51:43 UTC"
  },
  {
    "arxiv_id": "2503.17239v1",
    "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Farhan Ahmed",
      "Syed Zawad",
      "Holger Boche"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17239v1",
    "published_date": "2025-03-21 15:44:09 UTC",
    "updated_date": "2025-03-21 15:44:09 UTC"
  },
  {
    "arxiv_id": "2503.17237v2",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "authors": [
      "Yu-Hsi Chen"
    ],
    "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the well-established\nYOLOv5 with DeepSORT combination, we present a tracking framework built on\nYOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies.\nWe evaluate our approach following the 4th Anti-UAV Challenge metrics and reach\ncompetitive performance. Notably, we achieved strong results without using\ncontrast enhancement or temporal information fusion to enrich UAV features,\nhighlighting our approach as a \"Strong Baseline\" for multi-UAV tracking tasks.\nWe provide implementation details, in-depth experimental analysis, and a\ndiscussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.17237v2",
    "published_date": "2025-03-21 15:40:18 UTC",
    "updated_date": "2025-04-07 13:03:35 UTC"
  },
  {
    "arxiv_id": "2503.17229v1",
    "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
    "authors": [
      "Albert Sawczyn",
      "Jakub Binkowski",
      "Denis Janiak",
      "Bogdan Gabrys",
      "Tomasz Kajdanowicz"
    ],
    "abstract": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.17229v1",
    "published_date": "2025-03-21 15:32:24 UTC",
    "updated_date": "2025-03-21 15:32:24 UTC"
  },
  {
    "arxiv_id": "2503.17224v1",
    "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation",
    "authors": [
      "Giacomo Savazzi",
      "Eugenio Lomurno",
      "Cristian Sbrolli",
      "Agnese Chiatti",
      "Matteo Matteucci"
    ],
    "abstract": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17224v1",
    "published_date": "2025-03-21 15:26:16 UTC",
    "updated_date": "2025-03-21 15:26:16 UTC"
  },
  {
    "arxiv_id": "2503.17222v1",
    "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
    "authors": [
      "Sonish Sivarajkumar",
      "Kimia Ameri",
      "Chuqin Li",
      "Yanshan Wang",
      "Min Jiang"
    ],
    "abstract": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17222v1",
    "published_date": "2025-03-21 15:25:53 UTC",
    "updated_date": "2025-03-21 15:25:53 UTC"
  },
  {
    "arxiv_id": "2503.17213v1",
    "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction",
    "authors": [
      "Ting Sun",
      "Cheng Cui",
      "Yuning Du",
      "Yi Liu"
    ],
    "abstract": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX",
    "pdf_url": "http://arxiv.org/pdf/2503.17213v1",
    "published_date": "2025-03-21 15:20:47 UTC",
    "updated_date": "2025-03-21 15:20:47 UTC"
  },
  {
    "arxiv_id": "2503.21794v1",
    "title": "Architecture of Information",
    "authors": [
      "Yurii Parzhyn"
    ],
    "abstract": "The paper explores an approach to constructing energy landscapes of a formal\nneuron and multilayer artificial neural networks (ANNs). Their analysis makes\nit possible to determine the conceptual limitations of both classification ANNs\n(e.g., MLP or CNN) and generative ANN models. The study of informational and\nthermodynamic entropy in formal neuron and ANN models leads to the conclusion\nabout the energetic nature of informational entropy. The application of the\nGibbs free energy concept allows representing the output information of ANNs as\nthe structured part of enthalpy. Modeling ANNs as energy systems makes it\npossible to interpret the structure of their internal energy as an internal\nmodel of the external world, which self-organizes based on the interaction of\nthe system's internal energy components. The control of the self-organization\nand evolution process of this model is carried out through an energy function\n(analogous to the Lyapunov function) based on reduction operators. This makes\nit possible to introduce a new approach to constructing self-organizing and\nevolutionary ANNs with direct learning, which does not require additional\nexternal algorithms. The presented research makes it possible to formulate a\nformal definition of information in terms of the interaction processes between\nthe internal and external energy of the system.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "H.1.1; I.2.0"
    ],
    "primary_category": "cs.NE",
    "comment": "81 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21794v1",
    "published_date": "2025-03-21 14:48:41 UTC",
    "updated_date": "2025-03-21 14:48:41 UTC"
  },
  {
    "arxiv_id": "2503.17195v1",
    "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
    "authors": [
      "Sheng Wang",
      "Pengan Chen",
      "Jingqi Zhou",
      "Qintong Li",
      "Jingwei Dong",
      "Jiahui Gao",
      "Boyang Xue",
      "Jiyue Jiang",
      "Lingpeng Kong",
      "Chuan Wu"
    ],
    "abstract": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17195v1",
    "published_date": "2025-03-21 14:43:23 UTC",
    "updated_date": "2025-03-21 14:43:23 UTC"
  },
  {
    "arxiv_id": "2503.17184v1",
    "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection",
    "authors": [
      "Xueqi Qiu",
      "Xingyu Miao",
      "Fan Wan",
      "Haoran Duan",
      "Tejal Shah",
      "Varun Ojhab",
      "Yang Longa",
      "Rajiv Ranjan"
    ],
    "abstract": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17184v1",
    "published_date": "2025-03-21 14:31:33 UTC",
    "updated_date": "2025-03-21 14:31:33 UTC"
  },
  {
    "arxiv_id": "2503.17181v1",
    "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries",
    "authors": [
      "Lukas Twist",
      "Jie M. Zhang",
      "Mark Harman",
      "Don Syme",
      "Joost Noppen",
      "Detlef Nauck"
    ],
    "abstract": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.17181v1",
    "published_date": "2025-03-21 14:29:35 UTC",
    "updated_date": "2025-03-21 14:29:35 UTC"
  },
  {
    "arxiv_id": "2503.17167v2",
    "title": "DiTEC-WDN: A Large-Scale Dataset of Hydraulic Scenarios across Multiple Water Distribution Networks",
    "authors": [
      "Huy Truong",
      "Andrés Tello",
      "Alexander Lazovik",
      "Victoria Degeler"
    ],
    "abstract": "Privacy restrictions hinder the sharing of real-world Water Distribution\nNetwork (WDN) models, limiting the application of emerging data-driven machine\nlearning, which typically requires extensive observations. To address this\nchallenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique\nscenarios simulated over either short-term (24 hours) or long-term (1 year)\nperiods. We constructed this dataset using an automated pipeline that optimizes\ncrucial parameters (e.g., pressure, flow rate, and demand patterns),\nfacilitates large-scale simulations, and records discrete, synthetic but\nhydraulically realistic states under standard conditions via rule validation\nand post-hoc analysis. With a total of 228 million generated graph-based\nstates, DiTEC-WDN can support a variety of machine-learning tasks, including\ngraph-level, node-level, and link-level regression, as well as time-series\nforecasting. This contribution, released under a public license, encourages\nopen scientific research in the critical water sector, eliminates the risk of\nexposing sensitive data, and fulfills the need for a large-scale water\ndistribution network benchmark for study comparisons and scenario analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Nature Scientific Data. Huy Truong and Andr\\'es Tello\n  contributed equally to this work. For the dataset, see\n  https://huggingface.co/datasets/rugds/ditec-wdn",
    "pdf_url": "http://arxiv.org/pdf/2503.17167v2",
    "published_date": "2025-03-21 14:14:03 UTC",
    "updated_date": "2025-03-24 14:40:40 UTC"
  },
  {
    "arxiv_id": "2503.18968v2",
    "title": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow",
    "authors": [
      "Ziyue Wang",
      "Junde Wu",
      "Linghan Cai",
      "Chang Han Low",
      "Xihong Yang",
      "Qiaxuan Li",
      "Yueming Jin"
    ],
    "abstract": "In modern medicine, clinical diagnosis relies on the comprehensive analysis\nof primarily textual and visual data, drawing on medical expertise to ensure\nsystematic and rigorous reasoning. Recent advances in large Vision-Language\nModels (VLMs) and agent-based methods hold great potential for medical\ndiagnosis, thanks to the ability to effectively integrate multi-modal patient\ndata. However, they often provide direct answers and draw empirical-driven\nconclusions without quantitative analysis, which reduces their reliability and\nclinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm\nthat follows the diagnosis principle in modern medicine, to decouple the\nprocess into sequential components for step-by-step, evidence-based reasoning.\nOur MedAgent-Pro workflow presents a hierarchical diagnostic structure to\nmirror this principle, consisting of disease-level standardized plan generation\nand patient-level personalized step-by-step reasoning. To support disease-level\nplanning, an RAG-based agent is designed to retrieve medical guidelines to\nensure alignment with clinical standards. For patient-level reasoning, we\npropose to integrate professional tools such as visual models to enable\nquantitative assessments. Meanwhile, we propose to verify the reliability of\neach step to achieve evidence-based diagnosis, enforcing rigorous logical\nreasoning and a well-founded conclusion. Extensive experiments across a wide\nrange of anatomical regions, imaging modalities, and diseases demonstrate the\nsuperiority of MedAgent-Pro to mainstream VLMs, agentic systems and\nstate-of-the-art expert models. Ablation studies and human evaluation by\nclinical experts further validate its robustness and clinical relevance. Code\nis available at https://github.com/jinlab-imvr/MedAgent-Pro.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18968v2",
    "published_date": "2025-03-21 14:04:18 UTC",
    "updated_date": "2025-05-22 07:23:52 UTC"
  },
  {
    "arxiv_id": "2503.17132v2",
    "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition",
    "authors": [
      "Siyuan Yang",
      "Shilin Lu",
      "Shizheng Wang",
      "Meng Hwa Er",
      "Zengwei Zheng",
      "Alex C. Kot"
    ],
    "abstract": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17132v2",
    "published_date": "2025-03-21 13:31:16 UTC",
    "updated_date": "2025-03-27 11:35:37 UTC"
  },
  {
    "arxiv_id": "2503.17125v5",
    "title": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning",
    "authors": [
      "Chan Kim",
      "Seung-Woo Seo",
      "Seong-Woo Kim"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) has demonstrated strong performance in\nrobotic control but remains susceptible to out-of-distribution (OOD) states,\noften resulting in unreliable actions and task failure. While previous methods\nhave focused on minimizing or preventing OOD occurrences, they largely neglect\nrecovery once an agent encounters such states. Although the latest research has\nattempted to address this by guiding agents back to in-distribution states,\ntheir reliance on uncertainty estimation hinders scalability in complex\nenvironments. To overcome this limitation, we introduce Language Models for\nOut-of-Distribution Recovery (LaMOuR), which enables recovery learning without\nrelying on uncertainty estimation. LaMOuR generates dense reward codes that\nguide the agent back to a state where it can successfully perform its original\ntask, leveraging the capabilities of LVLMs in image description, logical\nreasoning, and code generation. Experimental results show that LaMOuR\nsubstantially enhances recovery efficiency across diverse locomotion tasks and\neven generalizes effectively to complex environments, including humanoid\nlocomotion and mobile manipulation, where existing methods struggle. The code\nand supplementary materials are available at https://lamour-rl.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17125v5",
    "published_date": "2025-03-21 13:20:39 UTC",
    "updated_date": "2025-03-28 06:34:23 UTC"
  },
  {
    "arxiv_id": "2503.17116v1",
    "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding",
    "authors": [
      "Luca Rossetto",
      "Werner Bailer",
      "Duc-Tien Dang-Nguyen",
      "Graham Healy",
      "Björn Þór Jónsson",
      "Onanong Kongmeesub",
      "Hoang-Bao Le",
      "Stevan Rudinac",
      "Klaus Schöffmann",
      "Florian Spiess",
      "Allie Tran",
      "Minh-Triet Tran",
      "Quang-Linh Tran",
      "Cathal Gurrin"
    ],
    "abstract": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.MM",
    "comment": "7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.17116v1",
    "published_date": "2025-03-21 13:01:07 UTC",
    "updated_date": "2025-03-21 13:01:07 UTC"
  },
  {
    "arxiv_id": "2503.17095v1",
    "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
    "authors": [
      "Kwan Yun",
      "Chaelin Kim",
      "Hangyeul Shin",
      "Junyong Noh"
    ],
    "abstract": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "68T45, 68U05",
      "I.3.3; I.3.8"
    ],
    "primary_category": "cs.GR",
    "comment": "CVPR2025, 11 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17095v1",
    "published_date": "2025-03-21 12:24:58 UTC",
    "updated_date": "2025-03-21 12:24:58 UTC"
  },
  {
    "arxiv_id": "2503.17089v1",
    "title": "Does a Rising Tide Lift All Boats? Bias Mitigation for AI-based CMR Segmentation",
    "authors": [
      "Tiarna Lee",
      "Esther Puyol-Antón",
      "Bram Ruijsink",
      "Miaojing Shi",
      "Andrew P. King"
    ],
    "abstract": "Artificial intelligence (AI) is increasingly being used for medical imaging\ntasks. However, there can be biases in the resulting models, particularly when\nthey were trained using imbalanced training datasets. One such example has been\nthe strong race bias effect in cardiac magnetic resonance (CMR) image\nsegmentation models. Although this phenomenon has been reported in a number of\npublications, little is known about the effectiveness of bias mitigation\nalgorithms in this domain. We aim to investigate the impact of common bias\nmitigation methods to address bias between Black and White subjects in AI-based\nCMR segmentation models. Specifically, we use oversampling, importance\nreweighing and Group DRO as well as combinations of these techniques to\nmitigate the race bias. Furthermore, motivated by recent findings on the root\ncauses of AI-based CMR segmentation bias, we evaluate the same methods using\nmodels trained and evaluated on cropped CMR images. We find that bias can be\nmitigated using oversampling, significantly improving performance for the\nunderrepresented Black subjects whilst not significantly reducing the majority\nWhite subjects' performance. Group DRO also improves performance for Black\nsubjects but not significantly, while reweighing decreases performance for\nBlack subjects. Using a combination of oversampling and Group DRO also improves\nperformance for Black subjects but not significantly. Using cropped images\nincreases performance for both races and reduces the bias, whilst adding\noversampling as a bias mitigation technique with cropped images reduces the\nbias further.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17089v1",
    "published_date": "2025-03-21 12:17:43 UTC",
    "updated_date": "2025-03-21 12:17:43 UTC"
  },
  {
    "arxiv_id": "2503.17085v1",
    "title": "Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics",
    "authors": [
      "J. M. Diederik Kruijssen",
      "Nicholas Emmons"
    ],
    "abstract": "Artificial intelligence (AI) systems powered by large language models have\nbecome increasingly prevalent in modern society, enabling a wide range of\napplications through natural language interaction. As AI agents proliferate in\nour daily lives, their generic and uniform expressiveness presents a\nsignificant limitation to their appeal and adoption. Personality expression\nrepresents a key prerequisite for creating more human-like and distinctive AI\nsystems. We show that AI models can express deterministic and consistent\npersonalities when instructed using established psychological frameworks, with\nvarying degrees of accuracy depending on model capabilities. We find that more\nadvanced models like GPT-4o and o1 demonstrate the highest accuracy in\nexpressing specified personalities across both Big Five and Myers-Briggs\nassessments, and further analysis suggests that personality expression emerges\nfrom a combination of intelligence and reasoning capabilities. Our results\nreveal that personality expression operates through holistic reasoning rather\nthan question-by-question optimization, with response-scale metrics showing\nhigher variance than test-scale metrics. Furthermore, we find that model\nfine-tuning affects communication style independently of personality expression\naccuracy. These findings establish a foundation for creating AI agents with\ndiverse and consistent personalities, which could significantly enhance\nhuman-AI interaction across applications from education to healthcare, while\nadditionally enabling a broader range of more unique AI agents. The ability to\nquantitatively assess and implement personality expression in AI systems opens\nnew avenues for research into more relatable, trustworthy, and ethically\ndesigned AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 8 figures, 4 tables; appeared in ADI (March 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.17085v1",
    "published_date": "2025-03-21 12:12:05 UTC",
    "updated_date": "2025-03-21 12:12:05 UTC"
  },
  {
    "arxiv_id": "2503.17070v1",
    "title": "A Thorough Assessment of the Non-IID Data Impact in Federated Learning",
    "authors": [
      "Daniel M. Jimenez-Gutierrez",
      "Mehrdad Hassanzadeh",
      "Aris Anagnostopoulos",
      "Ioannis Chatzigiannakis",
      "Andrea Vitaletti"
    ],
    "abstract": "Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17070v1",
    "published_date": "2025-03-21 11:53:36 UTC",
    "updated_date": "2025-03-21 11:53:36 UTC"
  },
  {
    "arxiv_id": "2503.17069v1",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "authors": [
      "Yufei Shi",
      "Weilong Yan",
      "Gang Xu",
      "Yumeng Li",
      "Yuchen Li",
      "Zhenxi Li",
      "Fei Richard Yu",
      "Ming Li",
      "Si Yong Yeo"
    ],
    "abstract": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17069v1",
    "published_date": "2025-03-21 11:50:06 UTC",
    "updated_date": "2025-03-21 11:50:06 UTC"
  },
  {
    "arxiv_id": "2503.17061v1",
    "title": "Replay4NCL: An Efficient Memory Replay-based Methodology for Neuromorphic Continual Learning in Embedded AI Systems",
    "authors": [
      "Mishal Fatima Minhas",
      "Rachmad Vidya Wicaksana Putra",
      "Falah Awwad",
      "Osman Hasan",
      "Muhammad Shafique"
    ],
    "abstract": "Neuromorphic Continual Learning (NCL) paradigm leverages Spiking Neural\nNetworks (SNNs) to enable continual learning (CL) capabilities for AI systems\nto adapt to dynamically changing environments. Currently, the state-of-the-art\nemploy a memory replay-based method to maintain the old knowledge. However,\nthis technique relies on long timesteps and compression-decompression steps,\nthereby incurring significant latency and energy overheads, which are not\nsuitable for tightly-constrained embedded AI systems (e.g., mobile\nagents/robotics). To address this, we propose Replay4NCL, a novel efficient\nmemory replay-based methodology for enabling NCL in embedded AI systems.\nSpecifically, Replay4NCL compresses the latent data (old knowledge), then\nreplays them during the NCL training phase with small timesteps, to minimize\nthe processing latency and energy consumption. To compensate the information\nloss from reduced spikes, we adjust the neuron threshold potential and learning\nrate settings. Experimental results on the class-incremental scenario with the\nSpiking Heidelberg Digits (SHD) dataset show that Replay4NCL can preserve old\nknowledge with Top-1 accuracy of 90.43% compared to 86.22% from the\nstate-of-the-art, while effectively learning new tasks, achieving 4.88x latency\nspeed-up, 20% latent memory saving, and 36.43% energy saving. These results\nhighlight the potential of our Replay4NCL methodology to further advances NCL\ncapabilities for embedded AI systems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at the 62th Design Automation Conference (DAC) 2025, June\n  2025, San Francisco, CA, USA",
    "pdf_url": "http://arxiv.org/pdf/2503.17061v1",
    "published_date": "2025-03-21 11:33:22 UTC",
    "updated_date": "2025-03-21 11:33:22 UTC"
  },
  {
    "arxiv_id": "2503.17055v1",
    "title": "Data-Driven Optimization of EV Charging Station Placement Using Causal Discovery",
    "authors": [
      "Julius Stephan Junker",
      "Rong Hu",
      "Ziyue Li",
      "Wolfgang Ketter"
    ],
    "abstract": "This paper addresses the critical challenge of optimizing electric vehicle\ncharging station placement through a novel data-driven methodology employing\ncausal discovery techniques. While traditional approaches prioritize economic\nfactors or power grid constraints, they often neglect empirical charging\npatterns that ultimately determine station utilization. We analyze extensive\ncharging data from Palo Alto and Boulder (337,344 events across 100 stations)\nto uncover latent relationships between station characteristics and\nutilization. Applying structural learning algorithms (NOTEARS and DAGMA) to\nthis data reveals that charging demand is primarily determined by three\nfactors: proximity to amenities, EV registration density, and adjacency to\nhigh-traffic routes. These findings, consistent across multiple algorithms and\nurban contexts, challenge conventional infrastructure distribution strategies.\nWe develop an optimization framework that translates these insights into\nactionable placement recommendations, identifying locations likely to\nexperience high utilization based on the discovered dependency structures. The\nresulting site selection model prioritizes strategic clustering in high-amenity\nareas with substantial EV populations rather than uniform spatial distribution.\nOur approach contributes a framework that integrates empirical charging\nbehavior into infrastructure planning, potentially enhancing both station\nutilization and user convenience. By focusing on data-driven insights instead\nof theoretical distribution models, we provide a more effective strategy for\nexpanding charging networks that can adjust to various stages of EV market\ndevelopment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review of IEEE CASE 2025; This is also the master thesis\n  project from Julius supervised by Dr. Ziyue Li",
    "pdf_url": "http://arxiv.org/pdf/2503.17055v1",
    "published_date": "2025-03-21 11:15:02 UTC",
    "updated_date": "2025-03-21 11:15:02 UTC"
  },
  {
    "arxiv_id": "2503.17046v1",
    "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
    "authors": [
      "Dongsheng Yang",
      "Qianying Liu",
      "Wataru Sato",
      "Takashi Minato",
      "Chaoran Liu",
      "Shin'ya Nishida"
    ],
    "abstract": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17046v1",
    "published_date": "2025-03-21 11:04:01 UTC",
    "updated_date": "2025-03-21 11:04:01 UTC"
  },
  {
    "arxiv_id": "2503.17039v2",
    "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?",
    "authors": [
      "Jeremy Barnes",
      "Naiara Perez",
      "Alba Bonet-Jover",
      "Begoña Altuna"
    ],
    "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17039v2",
    "published_date": "2025-03-21 10:52:20 UTC",
    "updated_date": "2025-04-14 08:25:42 UTC"
  },
  {
    "arxiv_id": "2503.17426v2",
    "title": "Enhanced Smart Contract Reputability Analysis using Multimodal Data Fusion on Ethereum",
    "authors": [
      "Cyrus Malik",
      "Josef Bajada",
      "Joshua Ellul"
    ],
    "abstract": "The evaluation of smart contract reputability is essential to foster trust in\ndecentralized ecosystems. However, existing methods that rely solely on code\nanalysis or transactional data, offer limited insight into evolving\ntrustworthiness. We propose a multimodal data fusion framework that integrates\ncode features with transactional data to enhance reputability prediction. Our\nframework initially focuses on AI-based code analysis, utilizing GAN-augmented\nopcode embeddings to address class imbalance, achieving 97.67% accuracy and a\nrecall of 0.942 in detecting illicit contracts, surpassing traditional\noversampling methods. This forms the crux of a reputability-centric fusion\nstrategy, where combining code and transactional data improves recall by 7.25%\nover single-source models, demonstrating robust performance across validation\nsets. By providing a holistic view of smart contract behaviour, our approach\nenhances the model's ability to assess reputability, identify fraudulent\nactivities, and predict anomalous patterns. These capabilities contribute to\nmore accurate reputability assessments, proactive risk mitigation, and enhanced\nblockchain security.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17426v2",
    "published_date": "2025-03-21 10:45:17 UTC",
    "updated_date": "2025-03-29 12:07:37 UTC"
  },
  {
    "arxiv_id": "2503.17034v1",
    "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation",
    "authors": [
      "Stephen Lloyd-Brown",
      "Susan Francis",
      "Caroline Hoad",
      "Penny Gowland",
      "Karen Mullinger",
      "Andrew French",
      "Xin Chen"
    ],
    "abstract": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.17034v1",
    "published_date": "2025-03-21 10:42:22 UTC",
    "updated_date": "2025-03-21 10:42:22 UTC"
  },
  {
    "arxiv_id": "2503.17030v1",
    "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques",
    "authors": [
      "Snigdha Paul",
      "Sambit Mallick",
      "Anindya Sen"
    ],
    "abstract": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17030v1",
    "published_date": "2025-03-21 10:39:21 UTC",
    "updated_date": "2025-03-21 10:39:21 UTC"
  },
  {
    "arxiv_id": "2503.17025v1",
    "title": "A Guide to Bayesian Networks Software Packages for Structure and Parameter Learning -- 2025 Edition",
    "authors": [
      "Joverlyn Gaudillo",
      "Nicole Astrologo",
      "Fabio Stella",
      "Enzo Acerbi",
      "Francesco Canonaco"
    ],
    "abstract": "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
    "categories": [
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.17025v1",
    "published_date": "2025-03-21 10:36:11 UTC",
    "updated_date": "2025-03-21 10:36:11 UTC"
  },
  {
    "arxiv_id": "2503.17018v1",
    "title": "Symbolic Audio Classification via Modal Decision Tree Learning",
    "authors": [
      "Enrico Marzano",
      "Giovanni Pagliarini",
      "Riccardo Pasini",
      "Guido Sciavicco",
      "Ionel Eduard Stan"
    ],
    "abstract": "The range of potential applications of acoustic analysis is wide.\nClassification of sounds, in particular, is a typical machine learning task\nthat received a lot of attention in recent years. The most common approaches to\nsound classification are sub-symbolic, typically based on neural networks, and\nresult in black-box models with high performances but very low transparency. In\nthis work, we consider several audio tasks, namely, age and gender recognition,\nemotion classification, and respiratory disease diagnosis, and we approach them\nwith a symbolic technique, that is, (modal) decision tree learning. We prove\nthat such tasks can be solved using the same symbolic pipeline, that allows to\nextract simple rules with very high accuracy and low complexity. In principle,\nall such tasks could be associated to an autonomous conversation system, which\ncould be useful in different contexts, such as an automatic reservation agent\nfor an hospital or a clinic.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "68T05",
      "I.2.6"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17018v1",
    "published_date": "2025-03-21 10:27:16 UTC",
    "updated_date": "2025-03-21 10:27:16 UTC"
  },
  {
    "arxiv_id": "2503.17013v1",
    "title": "Developing Critical Thinking in Second Language Learners: Exploring Generative AI like ChatGPT as a Tool for Argumentative Essay Writing",
    "authors": [
      "Simon Suh",
      "Jihyuk Bang",
      "Ji Woo Han"
    ],
    "abstract": "This study employs the Paul-Elder Critical Thinking Model and Tan's\nargumentative writing framework to create a structured methodology. This\nmethodology, ChatGPT Guideline for Critical Argumentative Writing (CGCAW)\nframework, integrates the models with ChatGPT's capabilities to guide L2\nlearners in utilizing ChatGPT to enhance their critical thinking skills. A\nquantitative experiment was conducted with 10 participants from a state\nuniversity, divided into experimental and control groups. The experimental\ngroup utilized the CGCAW framework, while the control group used ChatGPT\nwithout specific guidelines. Participants wrote an argumentative essay within a\n40-minute timeframe, and essays were evaluated by three assessors: ChatGPT,\nGrammarly, and a course instructor. Results indicated that the experimental\ngroup showed improvements in clarity, logical coherence, and use of evidence,\ndemonstrating ChatGPT's potential to enhance specific aspects of argumentative\nwriting. However, the control group performed better in overall language\nmechanics and articulation of main arguments, indicating areas where the CGCAW\nframework could be further refined. This study highlights the need for further\nresearch to optimize the use of AI tools like ChatGPT in L2 learning\nenvironments to enhance critical thinking and writing skills.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.7; K.3.1"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 3 figures. Uses Paul-Elder Critical Thinking Model and\n  Tan's argumentative writing framework. Includes an experimental study with 10\n  participants",
    "pdf_url": "http://arxiv.org/pdf/2503.17013v1",
    "published_date": "2025-03-21 10:22:58 UTC",
    "updated_date": "2025-03-21 10:22:58 UTC"
  },
  {
    "arxiv_id": "2503.17002v1",
    "title": "Targetless 6DoF Calibration of LiDAR and 2D Scanning Radar Based on Cylindrical Occupancy",
    "authors": [
      "Weimin Wang",
      "Yu Du",
      "Ting Yang",
      "Yu Liu"
    ],
    "abstract": "Owing to the capability for reliable and all-weather long-range sensing, the\nfusion of LiDAR and Radar has been widely applied to autonomous vehicles for\nrobust perception. In practical operation, well manually calibrated extrinsic\nparameters, which are crucial for the fusion of multi-modal sensors, may drift\ndue to the vibration. To address this issue, we present a novel targetless\ncalibration approach, termed LiRaCo, for the extrinsic 6DoF calibration of\nLiDAR and Radar sensors. Although both types of sensors can obtain geometric\ninformation, bridging the geometric correspondences between multi-modal data\nwithout any clues of explicit artificial markers is nontrivial, mainly due to\nthe low vertical resolution of scanning Radar. To achieve the targetless\ncalibration, LiRaCo leverages a spatial occupancy consistency between LiDAR\npoint clouds and Radar scans in a common cylindrical representation,\nconsidering the increasing data sparsity with distance for both sensors.\nSpecifically, LiRaCo expands the valid Radar scanned pixels into 3D occupancy\ngrids to constrain LiDAR point clouds based on spatial consistency.\nConsequently, a cost function involving extrinsic calibration parameters is\nformulated based on the spatial overlap of 3D grids and LiDAR points. Extrinsic\nparameters are finally estimated by optimizing the cost function. Comprehensive\nquantitative and qualitative experiments on two real outdoor datasets with\ndifferent LiDAR sensors demonstrate the feasibility and accuracy of the\nproposed method. The source code will be publicly available.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17002v1",
    "published_date": "2025-03-21 10:09:04 UTC",
    "updated_date": "2025-03-21 10:09:04 UTC"
  },
  {
    "arxiv_id": "2503.17424v1",
    "title": "Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data",
    "authors": [
      "Aakash Singh",
      "Anurag Kanaujia",
      "Vivek Kumar Singh"
    ],
    "abstract": "Among the factors of production, human capital or skilled manpower is the one\nthat keeps evolving and adapts to changing conditions and resources. This\nadaptability makes human capital the most crucial factor in ensuring a\nsustainable growth of industry/sector. As new technologies are developed and\nadopted, the new generations are required to acquire skills in newer\ntechnologies in order to be employable. At the same time professionals are\nrequired to upskill and reskill themselves to remain relevant in the industry.\nThere is however no straightforward method to identify the skill needs of the\nindustry at a given point of time. Therefore, this paper proposes a data to\ndecision framework that can successfully identify the desired skill set in a\ngiven area by analysing the advertorial data collected from popular online job\nportals and supplied as input to the framework. The proposed framework uses\ntechniques of statistical analysis, data mining and natural language processing\nfor the purpose. The applicability of the framework is demonstrated on CS&IT\njob advertisement data from India. The analytical results not only provide\nuseful insights about current state of skill needs in CS&IT industry but also\nprovide practical implications to prospective job applicants, training\nagencies, and institutions of higher education & professional training.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17424v1",
    "published_date": "2025-03-21 09:49:31 UTC",
    "updated_date": "2025-03-21 09:49:31 UTC"
  },
  {
    "arxiv_id": "2503.16983v1",
    "title": "Enabling Versatile Controls for Video Diffusion Models",
    "authors": [
      "Xu Zhang",
      "Hao Zhou",
      "Haoming Qin",
      "Xiaobin Lu",
      "Jiaxing Yan",
      "Guanzhong Wang",
      "Zeyu Chen",
      "Yi Liu"
    ],
    "abstract": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl",
    "pdf_url": "http://arxiv.org/pdf/2503.16983v1",
    "published_date": "2025-03-21 09:48:00 UTC",
    "updated_date": "2025-03-21 09:48:00 UTC"
  },
  {
    "arxiv_id": "2503.16980v3",
    "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
    "authors": [
      "Haichao Zhang",
      "Yun Fu"
    ],
    "abstract": "Token-based video representation has emerged as a promising approach for\nenabling LLMs to interpret video content. However, existing token reduction,\nsuch as token pruning and token merging, often disrupt essential\nspatial-temporal positional embeddings, failing to adequately balance\ncomputational efficiency with fewer tokens. Consequently, these methods result\nin lengthy token sequences, limiting their applicability in scenarios requiring\nextreme token compression, such as video large language models. In this paper,\nwe introduce the novel task of extreme short token reduction, aiming to\nrepresent extensive video sequences with a minimal number of tokens. To address\nthis challenge, we propose Token Dynamics, a new video representation framework\nthat dynamically reduces token count while preserving spatial-temporal\ncoherence. Specifically, we disentangle video representations by separating\nvisual embeddings from grid-level motion information, structuring them into: 1.\na concise token hash table, created by clustering tokens that describe\nobject-level content; 2. a token indices key map, capturing detailed\nspatial-temporal motion patterns across grids; 3. a token hash function, which\nvector-quantizes the token hash table to reconstruct the token sequence from\nthe key map. Furthermore, we introduce a cross-dynamics attention mechanism\nthat integrates motion features into the token base without increasing token\nlength, thereby maintaining compactness and spatial-temporal integrity. The\nexperiments demonstrate a reduction of token count to merely 0.07% of the\noriginal tokens, with only a minor performance drop of 1.13%. Additionally, we\npropose two novel subtasks within extreme token reduction (fixed-length and\nadaptive-length compression). Our method offers significantly lower theoretical\ncomplexity, fewer tokens, and enhanced throughput, thus providing an efficient\nsolution for video LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This submission has been withdrawn due to non-scientific and personal\n  reasons of the first author, with the understanding of all co-authors. The\n  first author has requested that the work not be made public at this time.\n  Future publication remains under discussion and exploration",
    "pdf_url": "http://arxiv.org/pdf/2503.16980v3",
    "published_date": "2025-03-21 09:46:31 UTC",
    "updated_date": "2025-04-02 21:54:38 UTC"
  },
  {
    "arxiv_id": "2503.16978v1",
    "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies with Q-Ensembles",
    "authors": [
      "Ruoqi Zhang",
      "Ziwei Luo",
      "Jens Sjölund",
      "Per Mattsson",
      "Linus Gisslén",
      "Alessandro Sestini"
    ],
    "abstract": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16978v1",
    "published_date": "2025-03-21 09:45:59 UTC",
    "updated_date": "2025-03-21 09:45:59 UTC"
  },
  {
    "arxiv_id": "2503.16976v1",
    "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation",
    "authors": [
      "Weihao Yu",
      "Xiaoqing Guo",
      "Chenxin Li",
      "Yifan Liu",
      "Yixuan Yuan"
    ],
    "abstract": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IPMI2025",
    "pdf_url": "http://arxiv.org/pdf/2503.16976v1",
    "published_date": "2025-03-21 09:43:57 UTC",
    "updated_date": "2025-03-21 09:43:57 UTC"
  },
  {
    "arxiv_id": "2503.16974v2",
    "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks",
    "authors": [
      "Julian Junyan Wang",
      "Victor Xiaoqi Wang"
    ],
    "abstract": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.",
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-fin.GN",
    "comment": "97 pages, 20 tables, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16974v2",
    "published_date": "2025-03-21 09:43:37 UTC",
    "updated_date": "2025-03-26 17:48:00 UTC"
  },
  {
    "arxiv_id": "2503.16973v2",
    "title": "ARFlow: Human Action-Reaction Flow Matching with Physical Guidance",
    "authors": [
      "Wentao Jiang",
      "Jingya Wang",
      "Haotao Lu",
      "Kaiyang Ji",
      "Baoxiong Jia",
      "Siyuan Huang",
      "Ye Shi"
    ],
    "abstract": "Human action-reaction synthesis, a fundamental challenge in modeling causal\nhuman interactions, plays a critical role in applications ranging from virtual\nreality to social robotics. While diffusion-based models have demonstrated\npromising performance, they exhibit two key limitations for interaction\nsynthesis: reliance on complex noise-to-reaction generators with intricate\nconditional mechanisms, and frequent physical violations in generated motions.\nTo address these issues, we propose Action-Reaction Flow Matching (ARFlow), a\nnovel framework that establishes direct action-to-reaction mappings,\neliminating the need for complex conditional mechanisms. Our approach\nintroduces two key innovations: an x1-prediction method that directly outputs\nhuman motions instead of velocity fields, enabling explicit constraint\nenforcement; and a training-free, gradient-based physical guidance mechanism\nthat effectively prevents body penetration artifacts during sampling. Extensive\nexperiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only\noutperforms existing methods in terms of Fr\\'echet Inception Distance and\nmotion diversity but also significantly reduces body collisions, as measured by\nour new Intersection Volume and Intersection Frequency metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://arflow2025.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.16973v2",
    "published_date": "2025-03-21 09:41:24 UTC",
    "updated_date": "2025-03-26 08:43:09 UTC"
  },
  {
    "arxiv_id": "2503.16956v1",
    "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
    "authors": [
      "Ji-Hoon Kim",
      "Jeongsoo Choi",
      "Jaehun Kim",
      "Chaeyoung Jung",
      "Joon Son Chung"
    ],
    "abstract": "The objective of this study is to generate high-quality speech from silent\ntalking face videos, a task also known as video-to-speech synthesis. A\nsignificant challenge in video-to-speech synthesis lies in the substantial\nmodality gap between silent video and multi-faceted speech. In this paper, we\npropose a novel video-to-speech system that effectively bridges this modality\ngap, significantly enhancing the quality of synthesized speech. This is\nachieved by learning of hierarchical representations from video to speech.\nSpecifically, we gradually transform silent video into acoustic feature spaces\nthrough three sequential stages -- content, timbre, and prosody modeling. In\neach stage, we align visual factors -- lip movements, face identity, and facial\nexpressions -- with corresponding acoustic counterparts to ensure the seamless\ntransformation. Additionally, to generate realistic and coherent speech from\nthe visual representations, we employ a flow matching model that estimates\ndirect trajectories from a simple prior distribution to the target speech\ndistribution. Extensive experiments demonstrate that our method achieves\nexceptional generation quality comparable to real utterances, outperforming\nexisting methods by a significant margin.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "CVPR 2025, demo page: https://mm.kaist.ac.kr/projects/faces2voices/",
    "pdf_url": "http://arxiv.org/pdf/2503.16956v1",
    "published_date": "2025-03-21 09:02:38 UTC",
    "updated_date": "2025-03-21 09:02:38 UTC"
  },
  {
    "arxiv_id": "2503.16953v1",
    "title": "Neural-Guided Equation Discovery",
    "authors": [
      "Jannis Brugger",
      "Mattia Cerrato",
      "David Richter",
      "Cedric Derstroff",
      "Daniel Maninger",
      "Mira Mezini",
      "Stefan Kramer"
    ],
    "abstract": "Deep learning approaches are becoming increasingly attractive for equation\ndiscovery. We show the advantages and disadvantages of using neural-guided\nequation discovery by giving an overview of recent papers and the results of\nexperiments using our modular equation discovery system MGMT\n($\\textbf{M}$ulti-Task $\\textbf{G}$rammar-Guided $\\textbf{M}$onte-Carlo\n$\\textbf{T}$ree Search for Equation Discovery). The system uses neural-guided\nMonte-Carlo Tree Search (MCTS) and supports both supervised and reinforcement\nlearning, with a search space defined by a context-free grammar. We summarize\nseven desirable properties of equation discovery systems, emphasizing the\nimportance of embedding tabular data sets for such learning approaches. Using\nthe modular structure of MGMT, we compare seven architectures (among them,\nRNNs, CNNs, and Transformers) for embedding tabular datasets on the auxiliary\ntask of contrastive learning for tabular data sets on an equation discovery\ntask. For almost all combinations of modules, supervised learning outperforms\nreinforcement learning. Moreover, our experiments indicate an advantage of\nusing grammar rules as action space instead of tokens. Two adaptations of MCTS\n-- risk-seeking MCTS and AmEx-MCTS -- can improve equation discovery with that\nkind of search.",
    "categories": [
      "cs.AI",
      "I.2.6; I.1.1; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages + 4 pages appendix, 9 figures, book chapter",
    "pdf_url": "http://arxiv.org/pdf/2503.16953v1",
    "published_date": "2025-03-21 08:55:51 UTC",
    "updated_date": "2025-03-21 08:55:51 UTC"
  },
  {
    "arxiv_id": "2503.16939v1",
    "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
    "authors": [
      "Hazem Hesham Yousef Shalby",
      "Arianna De Vecchi",
      "Alice Scandelli",
      "Pietro Bartoli",
      "Diana Trojaniello",
      "Manuel Roveri",
      "Federica Villa"
    ],
    "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at IEEE SSCI",
    "pdf_url": "http://arxiv.org/pdf/2503.16939v1",
    "published_date": "2025-03-21 08:31:07 UTC",
    "updated_date": "2025-03-21 08:31:07 UTC"
  },
  {
    "arxiv_id": "2503.16938v1",
    "title": "Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification",
    "authors": [
      "Alessio Cascione",
      "Mattia Setzu",
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino",
      "Riccardo Guidotti"
    ],
    "abstract": "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16938v1",
    "published_date": "2025-03-21 08:25:32 UTC",
    "updated_date": "2025-03-21 08:25:32 UTC"
  },
  {
    "arxiv_id": "2503.16932v1",
    "title": "Rude Humans and Vengeful Robots: Examining Human Perceptions of Robot Retaliatory Intentions in Professional Settings",
    "authors": [
      "Kate Letheren",
      "Nicole Robinson"
    ],
    "abstract": "Humans and robots are increasingly working in personal and professional\nsettings. In workplace settings, humans and robots may work together as\ncolleagues, potentially leading to social expectations, or violation thereof.\nExtant research has primarily sought to understand social interactions and\nexpectations in personal rather than professional settings, and none of these\nstudies have examined negative outcomes arising from violations of social\nexpectations. This paper reports the results of a 2x3 online experiment that\nused a unique first-person perspective video to immerse participants in a\ncollaborative workplace setting. The results are nuanced and reveal that while\nrobots are expected to act in accordance with social expectations despite human\nbehavior, there are benefits for robots perceived as being the bigger person in\nthe face of human rudeness. Theoretical and practical implications are provided\nwhich discuss the import of these findings for the design of social robots.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "This is the author version of the manuscript submitted to ACM\n  Transactions on Human-Robot Interaction. The final version, if accepted, will\n  be published by ACM and available via the ACM Digital Library. 12 pages, 1\n  figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.16932v1",
    "published_date": "2025-03-21 08:12:40 UTC",
    "updated_date": "2025-03-21 08:12:40 UTC"
  },
  {
    "arxiv_id": "2503.16929v2",
    "title": "TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Kun Ouyang",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Yuanxing Zhang",
      "Fuzheng Zhang",
      "Lingpeng Kong",
      "Qi Liu",
      "Xu Sun"
    ],
    "abstract": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPLE (TEMporal Preference Learning), a systematic framework that\nenhances Video LLMs' temporal reasoning capabilities through Direct Preference\nOptimization (DPO). To facilitate this, we introduce an automated preference\ndata generation pipeline that systematically constructs preference pairs by\nselecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n\"Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPLE as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs. Code is available at\nhttps://github.com/lscpku/TEMPLE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16929v2",
    "published_date": "2025-03-21 08:00:29 UTC",
    "updated_date": "2025-03-29 18:15:51 UTC"
  },
  {
    "arxiv_id": "2503.16922v1",
    "title": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation",
    "authors": [
      "Linxi Liang",
      "Jing Gong",
      "Mingwei Liu",
      "Chong Wang",
      "Guangsheng Ou",
      "Yanlin Wang",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "abstract": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16922v1",
    "published_date": "2025-03-21 07:33:59 UTC",
    "updated_date": "2025-03-21 07:33:59 UTC"
  },
  {
    "arxiv_id": "2503.16921v1",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
    "authors": [
      "Lingfan Zhang",
      "Chen Liu",
      "Chengming Xu",
      "Kai Hu",
      "Donghao Luo",
      "Chengjie Wang",
      "Yanwei Fu",
      "Yuan Yao"
    ],
    "abstract": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16921v1",
    "published_date": "2025-03-21 07:33:44 UTC",
    "updated_date": "2025-03-21 07:33:44 UTC"
  },
  {
    "arxiv_id": "2503.17421v1",
    "title": "Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation",
    "authors": [
      "Junwei Kuang",
      "Liang Yang",
      "Shaoze Cui",
      "Weiguo Fan"
    ],
    "abstract": "Patients are increasingly turning to online health Q&A communities for social\nsupport to improve their well-being. However, when this support received does\nnot align with their specific needs, it may prove ineffective or even\ndetrimental. This necessitates a model capable of identifying the social\nsupport needs in questions. However, training such a model is challenging due\nto the scarcity and class imbalance issues of labeled data. To overcome these\nchallenges, we follow the computational design science paradigm to develop a\nnovel framework, Hybrid Approach for SOcial Support need classification\n(HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning\napproach, a text data augmentation technique leveraging large language models\n(LLMs) with reliability- and diversity-aware sample selection mechanism, and a\nunified training process to automatically label social support needs in\nquestions. Extensive empirical evaluations demonstrate that HA-SOS\nsignificantly outperforms existing question classification models and\nalternative semi-supervised learning approaches. This research contributes to\nthe literature on social support, question classification, semi-supervised\nlearning, and text data augmentation. In practice, our HA-SOS framework\nfacilitates online Q&A platform managers and answerers to better understand\nusers' social support needs, enabling them to provide timely, personalized\nanswers and interventions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "55 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.17421v1",
    "published_date": "2025-03-21 07:25:16 UTC",
    "updated_date": "2025-03-21 07:25:16 UTC"
  },
  {
    "arxiv_id": "2503.16914v1",
    "title": "A New Segment Routing method with Swap Node Selection Strategy Based on Deep Reinforcement Learning for Software Defined Network",
    "authors": [
      "Miao Ye",
      "Jihao Zheng",
      "Qiuxiang Jiang",
      "Yuan Huang",
      "Ziheng Wang",
      "Yong Wang"
    ],
    "abstract": "The existing segment routing (SR) methods need to determine the routing first\nand then use path segmentation approaches to select swap nodes to form a\nsegment routing path (SRP). They require re-segmentation of the path when the\nrouting changes. Furthermore, they do not consider the flow table issuance\ntime, which cannot maximize the speed of issuance flow table. To address these\nissues, this paper establishes an optimization model that can simultaneously\nform routing strategies and path segmentation strategies for selecting the\nappropriate swap nodes to reduce flow table issuance time. It also designs an\nintelligent segment routing algorithm based on deep reinforcement learning\n(DRL-SR) to solve the proposed model. First, a traffic matrix is designed as\nthe state space for the deep reinforcement learning agent; this matrix includes\nmultiple QoS performance indicators, flow table issuance time overhead and SR\nlabel stack depth. Second, the action selection strategy and corresponding\nreward function are designed, where the agent selects the next node considering\nthe routing; in addition, the action selection strategy whether the newly added\nnode is selected as the swap node and the corresponding reward function are\ndesigned considering the time cost factor for the controller to issue the flow\ntable to the swap node. Finally, a series of experiments and their results show\nthat, compared with the existing methods, the designed segmented route\noptimization model and the intelligent solution algorithm (DRL-SR) can reduce\nthe time overhead required to complete the segmented route establishment task\nwhile optimizing performance metrics such as throughput, delays and packet\nlosses.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16914v1",
    "published_date": "2025-03-21 07:24:09 UTC",
    "updated_date": "2025-03-21 07:24:09 UTC"
  },
  {
    "arxiv_id": "2503.16905v1",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
    "authors": [
      "Jian Zhang",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Xinyu Zhang",
      "Fangzhi Xu",
      "Qika Lin",
      "Rui Mao",
      "Erik Cambria",
      "Jun Liu"
    ],
    "abstract": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16905v1",
    "published_date": "2025-03-21 07:13:45 UTC",
    "updated_date": "2025-03-21 07:13:45 UTC"
  },
  {
    "arxiv_id": "2503.16904v1",
    "title": "Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons: A Comparative Study",
    "authors": [
      "Omar Coser",
      "Christian Tamantini",
      "Matteo Tortora",
      "Leonardo Furia",
      "Rosa Sicilia",
      "Loredana Zollo",
      "Paolo Soda"
    ],
    "abstract": "Wearable robotics for lower-limb assistance have become a pivotal area of\nresearch, aiming to enhance mobility for individuals with physical impairments\nor augment the performance of able-bodied users. Accurate and adaptive control\nsystems are essential to ensure seamless interaction between the wearer and the\nrobotic device, particularly when navigating diverse and dynamic terrains.\nDespite the recent advances in neural networks for time series analysis, no\nattempts have been directed towards the classification of ground conditions,\ncategorized into five classes and subsequently determining the ramp's slope and\nstair's height. In this respect, this paper presents an experimental comparison\nbetween eight deep neural network backbones to predict high-level locomotion\nparameters across diverse terrains.\n  All the models are trained on the publicly available CAMARGO 2021 dataset.\nIMU-only data equally or outperformed IMU+EMG inputs, promoting a\ncost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM\nachieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp\nslope (1.95 +- 0.58{\\deg}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm)\nestimations. As a further contribution, SHAP analysis justified sensor\nreduction without performance loss, ensuring a lightweight setup. The system\noperates with ~2 ms inference time, supporting real-time applications. The code\nis code available at\nhttps://github.com/cosbidev/Human-Locomotion-Identification.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "F.2.2, I.2.7"
    ],
    "primary_category": "cs.RO",
    "comment": "26 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16904v1",
    "published_date": "2025-03-21 07:12:44 UTC",
    "updated_date": "2025-03-21 07:12:44 UTC"
  },
  {
    "arxiv_id": "2503.16874v1",
    "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
    "authors": [
      "Jian Zhang",
      "Zhangqi Wang",
      "Haiping Zhu",
      "Jun Liu",
      "Qika Lin",
      "Erik Cambria"
    ],
    "abstract": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16874v1",
    "published_date": "2025-03-21 06:19:55 UTC",
    "updated_date": "2025-03-21 06:19:55 UTC"
  },
  {
    "arxiv_id": "2503.16873v1",
    "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
    "authors": [
      "Dongseob Kim",
      "Hyunjung Shim"
    ],
    "abstract": "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https://github.com/k0u-id/CCD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2503.16873v1",
    "published_date": "2025-03-21 06:12:14 UTC",
    "updated_date": "2025-03-21 06:12:14 UTC"
  },
  {
    "arxiv_id": "2503.16870v1",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "authors": [
      "Anshumann",
      "Mohd Abbas Zaidi",
      "Akhil Kedia",
      "Jinwoo Ahn",
      "Taehwak Kwon",
      "Kangwook Lee",
      "Haejun Lee",
      "Joohyung Lee"
    ],
    "abstract": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
    "pdf_url": "http://arxiv.org/pdf/2503.16870v1",
    "published_date": "2025-03-21 05:58:18 UTC",
    "updated_date": "2025-03-21 05:58:18 UTC"
  },
  {
    "arxiv_id": "2503.16861v2",
    "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
    "authors": [
      "Shayne Longpre",
      "Kevin Klyman",
      "Ruth E. Appel",
      "Sayash Kapoor",
      "Rishi Bommasani",
      "Michelle Sahar",
      "Sean McGregor",
      "Avijit Ghosh",
      "Borhane Blili-Hamelin",
      "Nathan Butters",
      "Alondra Nelson",
      "Amit Elazari",
      "Andrew Sellars",
      "Casey John Ellis",
      "Dane Sherrets",
      "Dawn Song",
      "Harley Geiger",
      "Ilona Cohen",
      "Lauren McIlvenny",
      "Madhulika Srikumar",
      "Mark M. Jaycox",
      "Markus Anderljung",
      "Nadine Farid Johnson",
      "Nicholas Carlini",
      "Nicolas Miailhe",
      "Nik Marda",
      "Peter Henderson",
      "Rebecca S. Portnoff",
      "Rebecca Weiss",
      "Victoria Westerhoff",
      "Yacine Jernite",
      "Rumman Chowdhury",
      "Percy Liang",
      "Arvind Narayanan"
    ],
    "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16861v2",
    "published_date": "2025-03-21 05:09:46 UTC",
    "updated_date": "2025-03-25 05:12:04 UTC"
  },
  {
    "arxiv_id": "2503.16858v1",
    "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering",
    "authors": [
      "Jialin Chen",
      "Aosong Feng",
      "Ziyu Zhao",
      "Juan Garza",
      "Gaukhar Nurbek",
      "Cheng Qin",
      "Ali Maatouk",
      "Leandros Tassiulas",
      "Yifeng Gao",
      "Rex Ying"
    ],
    "abstract": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.16858v1",
    "published_date": "2025-03-21 05:04:53 UTC",
    "updated_date": "2025-03-21 05:04:53 UTC"
  },
  {
    "arxiv_id": "2503.16853v1",
    "title": "Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models",
    "authors": [
      "Suho Yoo",
      "Hyunjong Ok",
      "Jaeho Lee"
    ],
    "abstract": "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.16853v1",
    "published_date": "2025-03-21 04:56:22 UTC",
    "updated_date": "2025-03-21 04:56:22 UTC"
  },
  {
    "arxiv_id": "2503.16852v1",
    "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
    "authors": [
      "Jiaxi Li",
      "Di Lin",
      "Hao Chen",
      "Hongying Liu",
      "Liang Wan",
      "Wei Feng"
    ],
    "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2503.16852v1",
    "published_date": "2025-03-21 04:52:31 UTC",
    "updated_date": "2025-03-21 04:52:31 UTC"
  },
  {
    "arxiv_id": "2503.16850v1",
    "title": "Physics-Informed Neural Network Surrogate Models for River Stage Prediction",
    "authors": [
      "Maximilian Zoch",
      "Edward Holmberg",
      "Pujan Pokhrel",
      "Ken Pathak",
      "Steven Sloan",
      "Kendall Niles",
      "Jay Ratcliff",
      "Maik Flanagin",
      "Elias Ioup",
      "Christian Guetl",
      "Mahdi Abdelguerfi"
    ],
    "abstract": "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16850v1",
    "published_date": "2025-03-21 04:48:22 UTC",
    "updated_date": "2025-03-21 04:48:22 UTC"
  },
  {
    "arxiv_id": "2503.16833v1",
    "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege",
    "authors": [
      "Luxi He",
      "Xiangyu Qi",
      "Michel Liao",
      "Inyoung Cheong",
      "Prateek Mittal",
      "Danqi Chen",
      "Peter Henderson"
    ],
    "abstract": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16833v1",
    "published_date": "2025-03-21 04:03:59 UTC",
    "updated_date": "2025-03-21 04:03:59 UTC"
  },
  {
    "arxiv_id": "2503.16806v1",
    "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation",
    "authors": [
      "Jiangran Lyu",
      "Ziming Li",
      "Xuesong Shi",
      "Chaoyi Xu",
      "Yizhou Wang",
      "He Wang"
    ],
    "abstract": "Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page:https://pku-epic.github.io/DyWA/",
    "pdf_url": "http://arxiv.org/pdf/2503.16806v1",
    "published_date": "2025-03-21 02:29:52 UTC",
    "updated_date": "2025-03-21 02:29:52 UTC"
  },
  {
    "arxiv_id": "2503.16801v1",
    "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions",
    "authors": [
      "Zichen Geng",
      "Zeeshan Hayder",
      "Wei Liu",
      "Ajmal Saeed Mian"
    ],
    "abstract": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16801v1",
    "published_date": "2025-03-21 02:25:59 UTC",
    "updated_date": "2025-03-21 02:25:59 UTC"
  },
  {
    "arxiv_id": "2503.16799v1",
    "title": "Causally Aligned Curriculum Learning",
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "abstract": "A pervasive challenge in Reinforcement Learning (RL) is the \"curse of\ndimensionality\" which is the exponential growth in the state-action space when\noptimizing a high-dimensional target task. The framework of curriculum learning\ntrains the agent in a curriculum composed of a sequence of related and more\nmanageable source tasks. The expectation is that when some optimal decision\nrules are shared across source tasks and the target task, the agent could more\nquickly pick up the necessary skills to behave optimally in the environment,\nthus accelerating the learning process. However, this critical assumption of\ninvariant optimal decision rules does not necessarily hold in many practical\napplications, specifically when the underlying environment contains unobserved\nconfounders. This paper studies the problem of curriculum RL through causal\nlenses. We derive a sufficient graphical condition characterizing causally\naligned source tasks, i.e., the invariance of optimal decision rules holds. We\nfurther develop an efficient algorithm to generate a causally aligned\ncurriculum, provided with qualitative causal knowledge of the target task.\nFinally, we validate our proposed methodology through experiments in discrete\nand continuous confounded tasks with pixel observations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as Posters in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.16799v1",
    "published_date": "2025-03-21 02:20:38 UTC",
    "updated_date": "2025-03-21 02:20:38 UTC"
  },
  {
    "arxiv_id": "2503.16797v1",
    "title": "A Learnability Analysis on Neuro-Symbolic Learning",
    "authors": [
      "Hao-Yuan He",
      "Ming Li"
    ],
    "abstract": "This paper analyzes the learnability of neuro-symbolic (NeSy) tasks within\nhybrid systems. We show that the learnability of NeSy tasks can be\ncharacterized by their derived constraint satisfaction problems (DCSPs).\nSpecifically, a task is learnable if the corresponding DCSP has a unique\nsolution; otherwise, it is unlearnable. For learnable tasks, we establish error\nbounds by exploiting the clustering property of the hypothesis space.\nAdditionally, we analyze the asymptotic error for general NeSy tasks, showing\nthat the expected error scales with the disagreement among solutions. Our\nresults offer a principled approach to determining learnability and provide\ninsights into the design of new algorithms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16797v1",
    "published_date": "2025-03-21 02:16:11 UTC",
    "updated_date": "2025-03-21 02:16:11 UTC"
  },
  {
    "arxiv_id": "2503.16791v2",
    "title": "\"The Diagram is like Guardrails\": Structuring GenAI-assisted Hypotheses Exploration with an Interactive Shared Representation",
    "authors": [
      "Zijian Ding",
      "Michelle Brachman",
      "Joel Chan",
      "Werner Geyer"
    ],
    "abstract": "Data analysis encompasses a spectrum of tasks, from high-level conceptual\nreasoning to lower-level execution. While AI-powered tools increasingly support\nexecution tasks, there remains a need for intelligent assistance in conceptual\ntasks. This paper investigates the design of an ordered node-link tree\ninterface augmented with AI-generated information hints and visualizations, as\na potential shared representation for hypothesis exploration. Through a design\nprobe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our\nfindings showed that the node-link diagram acts as \"guardrails\" for hypothesis\nexploration, facilitating structured workflows, providing comprehensive\noverviews, and enabling efficient backtracking. The AI-generated information\nhints, particularly visualizations, aided users in transforming abstract ideas\ninto data-backed concepts while reducing cognitive load. We further discuss how\nnode-link diagrams can support both parallel exploration and iterative\nrefinement in hypothesis formulation, potentially enhancing the breadth and\ndepth of human-AI collaborative data analysis.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16791v2",
    "published_date": "2025-03-21 02:01:37 UTC",
    "updated_date": "2025-04-21 16:05:54 UTC"
  },
  {
    "arxiv_id": "2503.16788v1",
    "title": "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical Study",
    "authors": [
      "Li Zhang",
      "Longxi Gao",
      "Mengwei Xu"
    ],
    "abstract": "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps://github.com/LlamaTouch/VLM-Reasoning-Traces.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16788v1",
    "published_date": "2025-03-21 01:52:43 UTC",
    "updated_date": "2025-03-21 01:52:43 UTC"
  },
  {
    "arxiv_id": "2503.16782v1",
    "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery",
    "authors": [
      "Enguang Wang",
      "Zhimao Peng",
      "Zhengyuan Xie",
      "Haori Lu",
      "Fei Yang",
      "Xialei Liu"
    ],
    "abstract": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16782v1",
    "published_date": "2025-03-21 01:37:51 UTC",
    "updated_date": "2025-03-21 01:37:51 UTC"
  },
  {
    "arxiv_id": "2503.16779v1",
    "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models",
    "authors": [
      "Mengsong Wu",
      "Tong Zhu",
      "Han Han",
      "Xiang Zhang",
      "Wenbiao Shao",
      "Wenliang Chen"
    ],
    "abstract": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16779v1",
    "published_date": "2025-03-21 01:26:12 UTC",
    "updated_date": "2025-03-21 01:26:12 UTC"
  },
  {
    "arxiv_id": "2503.17417v2",
    "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
    "authors": [
      "Jungkyoo Shin",
      "Bumsoo Kim",
      "Eunwoo Kim"
    ],
    "abstract": "Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in CVPR 2025 (Highlight)",
    "pdf_url": "http://arxiv.org/pdf/2503.17417v2",
    "published_date": "2025-03-21 01:17:44 UTC",
    "updated_date": "2025-04-14 06:45:58 UTC"
  },
  {
    "arxiv_id": "2503.17416v1",
    "title": "Debugging and Runtime Analysis of Neural Networks with VLMs (A Case Study)",
    "authors": [
      "Boyue Caroline Hu",
      "Divya Gopinath",
      "Corina S. Pasareanu",
      "Nina Narodytska",
      "Ravi Mangal",
      "Susmit Jha"
    ],
    "abstract": "Debugging of Deep Neural Networks (DNNs), particularly vision models, is very\nchallenging due to the complex and opaque decision-making processes in these\nnetworks. In this paper, we explore multi-modal Vision-Language Models (VLMs),\nsuch as CLIP, to automatically interpret the opaque representation space of\nvision models using natural language. This in turn, enables a semantic analysis\nof model behavior using human-understandable concepts, without requiring costly\nhuman annotations. Key to our approach is the notion of semantic heatmap, that\nsuccinctly captures the statistical properties of DNNs in terms of the concepts\ndiscovered with the VLM and that are computed off-line using a held-out data\nset. We show the utility of semantic heatmaps for fault localization -- an\nessential step in debugging -- in vision models. Our proposed technique helps\nlocalize the fault in the network (encoder vs head) and also highlights the\nresponsible high-level concepts, by leveraging novel differential heatmaps,\nwhich summarize the semantic differences between the correct and incorrect\nbehaviour of the analyzed DNN. We further propose a lightweight runtime\nanalysis to detect and filter-out defects at runtime, thus improving the\nreliability of the analyzed DNNs. The runtime analysis works by measuring and\ncomparing the similarity between the heatmap computed for a new (unseen) input\nand the heatmaps computed a-priori for correct vs incorrect DNN behavior. We\nconsider two types of defects: misclassifications and vulnerabilities to\nadversarial attacks. We demonstrate the debugging and runtime analysis on a\ncase study involving a complex ResNet-based classifier trained on the RIVAL10\ndataset.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "CAIN 2025 (4th International Conference on AI Engineering -- Software\n  Engineering for AI)",
    "pdf_url": "http://arxiv.org/pdf/2503.17416v1",
    "published_date": "2025-03-21 01:12:57 UTC",
    "updated_date": "2025-03-21 01:12:57 UTC"
  },
  {
    "arxiv_id": "2503.17415v1",
    "title": "Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)",
    "authors": [
      "Yicheng Duan",
      "Xi Huang",
      "Duo Chen"
    ],
    "abstract": "The rapid growth of video content demands efficient and precise retrieval\nsystems. While vision-language models (VLMs) excel in representation learning,\nthey often struggle with adaptive, time-sensitive video retrieval. This paper\nintroduces a novel framework that combines vector similarity search with\ngraph-based data structures. By leveraging VLM embeddings for initial retrieval\nand modeling contextual relationships among video segments, our approach\nenables adaptive query refinement and improves retrieval accuracy. Experiments\ndemonstrate its precision, scalability, and robustness, offering an effective\nsolution for interactive video retrieval in dynamic environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17415v1",
    "published_date": "2025-03-21 01:11:14 UTC",
    "updated_date": "2025-03-21 01:11:14 UTC"
  },
  {
    "arxiv_id": "2503.16768v1",
    "title": "Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking",
    "authors": [
      "Meng Zhou",
      "Jiadong Xie",
      "Mingsheng Xu"
    ],
    "abstract": "Mainstream visual object tracking frameworks predominantly rely on template\nmatching paradigms. Their performance heavily depends on the quality of\ntemplate features, which becomes increasingly challenging to maintain in\ncomplex scenarios involving target deformation, occlusion, and background\nclutter. While existing spatiotemporal memory-based trackers emphasize memory\ncapacity expansion, they lack effective mechanisms for dynamic feature\nselection and adaptive fusion. To address this gap, we propose a Dynamic\nAttention Mechanism in Spatiotemporal Memory Network (DASTM) with two key\ninnovations: 1) A differentiable dynamic attention mechanism that adaptively\nadjusts channel-spatial attention weights by analyzing spatiotemporal\ncorrelations between the templates and memory features; 2) A lightweight gating\nnetwork that autonomously allocates computational resources based on target\nmotion states, prioritizing high-discriminability features in challenging\nscenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K\nbenchmarks demonstrate our DASTM's superiority, achieving state-of-the-art\nperformance in success rate, robustness, and real-time efficiency, thereby\noffering a novel solution for real-time tracking in complex environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16768v1",
    "published_date": "2025-03-21 00:48:31 UTC",
    "updated_date": "2025-03-21 00:48:31 UTC"
  },
  {
    "arxiv_id": "2503.17414v1",
    "title": "Opportunities and Challenges of Frontier Data Governance With Synthetic Data",
    "authors": [
      "Madhavendra Thakur",
      "Jason Hausenloy"
    ],
    "abstract": "Synthetic data, or data generated by machine learning models, is increasingly\nemerging as a solution to the data access problem. However, its use introduces\nsignificant governance and accountability challenges, and potentially debases\nexisting governance paradigms, such as compute and data governance. In this\npaper, we identify 3 key governance and accountability challenges that\nsynthetic data poses - it can enable the increased emergence of malicious\nactors, spontaneous biases and value drift. We thus craft 3 technical\nmechanisms to address these specific challenges, finding applications for\nsynthetic data towards adversarial training, bias mitigation and value\nreinforcement. These could not only counteract the risks of synthetic data, but\nserve as critical levers for governance of the frontier in the future.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17414v1",
    "published_date": "2025-03-21 00:30:17 UTC",
    "updated_date": "2025-03-21 00:30:17 UTC"
  }
]
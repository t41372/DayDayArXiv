[
  {
    "arxiv_id": "2405.12421v3",
    "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback",
    "authors": [
      "Kihyun Kim",
      "Jiawei Zhang",
      "Asuman Ozdaglar",
      "Pablo A. Parrilo"
    ],
    "abstract": "Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human\nFeedback (RLHF) are pivotal methodologies in reward learning, which involve\ninferring and shaping the underlying reward function of sequential\ndecision-making problems based on observed human demonstrations and feedback.\nMost prior work in reward learning has relied on prior knowledge or assumptions\nabout decision or preference models, potentially leading to robustness issues.\nIn response, this paper introduces a novel linear programming (LP) framework\ntailored for offline reward learning. Utilizing pre-collected trajectories\nwithout online exploration, this framework estimates a feasible reward set from\nthe primal-dual optimality conditions of a suitably designed LP, and offers an\noptimality guarantee with provable sample efficiency. Our LP framework also\nenables aligning the reward functions with human feedback, such as pairwise\ntrajectory comparison data, while maintaining computational tractability and\nsample efficiency. We demonstrate that our framework potentially achieves\nbetter performance compared to the conventional maximum likelihood estimation\n(MLE) approach through analytical examples and numerical experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.12421v3",
    "published_date": "2024-05-20 23:59:26 UTC",
    "updated_date": "2024-10-14 23:39:03 UTC"
  },
  {
    "arxiv_id": "2405.12399v2",
    "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
    "authors": [
      "Eloi Alonso",
      "Adam Jelley",
      "Vincent Micheli",
      "Anssi Kanervisto",
      "Amos Storkey",
      "Tim Pearce",
      "Fran√ßois Fleuret"
    ],
    "abstract": "World models constitute a promising approach for training reinforcement\nlearning agents in a safe and sample-efficient manner. Recent world models\npredominantly operate on sequences of discrete latent variables to model\nenvironment dynamics. However, this compression into a compact discrete\nrepresentation may ignore visual details that are important for reinforcement\nlearning. Concurrently, diffusion models have become a dominant approach for\nimage generation, challenging well-established methods modeling discrete\nlatents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a\nModel Of eNvironment Dreams), a reinforcement learning agent trained in a\ndiffusion world model. We analyze the key design choices that are required to\nmake diffusion suitable for world modeling, and demonstrate how improved visual\ndetails can lead to improved agent performance. DIAMOND achieves a mean human\nnormalized score of 1.46 on the competitive Atari 100k benchmark; a new best\nfor agents trained entirely within a world model. We further demonstrate that\nDIAMOND's diffusion world model can stand alone as an interactive neural game\nengine by training on static Counter-Strike: Global Offensive gameplay. To\nfoster future research on diffusion for world modeling, we release our code,\nagents, videos and playable world models at https://diamond-wm.github.io.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2405.12399v2",
    "published_date": "2024-05-20 22:51:05 UTC",
    "updated_date": "2024-10-30 14:34:49 UTC"
  },
  {
    "arxiv_id": "2405.12391v1",
    "title": "Automated Anomaly Detection on European XFEL Klystrons",
    "authors": [
      "Antonin Sulc",
      "Annika Eichler",
      "Tim Wilksen"
    ],
    "abstract": "High-power multi-beam klystrons represent a key component to amplify RF to\ngenerate the accelerating field of the superconducting radio frequency (SRF)\ncavities at European XFEL. Exchanging these high-power components takes time\nand effort, thus it is necessary to minimize maintenance and downtime and at\nthe same time maximize the device's operation. In an attempt to explore the\nbehavior of klystrons using machine learning, we completed a series of\nexperiments on our klystrons to determine various operational modes and conduct\nfeature extraction and dimensionality reduction to extract the most valuable\ninformation about a normal operation. To analyze recorded data we used\nstate-of-the-art data-driven learning techniques and recognized the most\npromising components that might help us better understand klystron operational\nstates and identify early on possible faults or anomalies.",
    "categories": [
      "physics.acc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.acc-ph",
    "comment": "4 pages, 4 figures, 15, 15TH International Particle Accelerator\n  Conference",
    "pdf_url": "http://arxiv.org/pdf/2405.12391v1",
    "published_date": "2024-05-20 21:59:07 UTC",
    "updated_date": "2024-05-20 21:59:07 UTC"
  },
  {
    "arxiv_id": "2405.12390v4",
    "title": "A Metric-based Principal Curve Approach for Learning One-dimensional Manifold",
    "authors": [
      "Eliuvish Cuicizion"
    ],
    "abstract": "Principal curve is a well-known statistical method oriented in manifold\nlearning using concepts from differential geometry. In this paper, we propose a\nnovel metric-based principal curve (MPC) method that learns one-dimensional\nmanifold of spatial data. Synthetic datasets Real applications using MNIST\ndataset show that our method can learn the one-dimensional manifold well in\nterms of the shape.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12390v4",
    "published_date": "2024-05-20 21:50:19 UTC",
    "updated_date": "2025-03-18 20:30:38 UTC"
  },
  {
    "arxiv_id": "2406.00013v1",
    "title": "Thesis: Document Summarization with applications to Keyword extraction and Image Retrieval",
    "authors": [
      "Jayaprakash Sundararaj"
    ],
    "abstract": "Automatic summarization is the process of reducing a text document in order\nto generate a summary that retains the most important points of the original\ndocument. In this work, we study two problems - i) summarizing a text document\nas set of keywords/caption, for image recommedation, ii) generating opinion\nsummary which good mix of relevancy and sentiment with the text document.\nIntially, we present our work on an recommending images for enhancing a\nsubstantial amount of existing plain text news articles. We use probabilistic\nmodels and word similarity heuristics to generate captions and extract\nKey-phrases which are re-ranked using a rank aggregation framework with\nrelevance feedback mechanism. We show that such rank aggregation and relevant\nfeedback which are typically used in Tagging Documents, Text Information\nRetrieval also helps in improving image retrieval. These queries are fed to the\nYahoo Search Engine to obtain relevant images 1. Our proposed method is\nobserved to perform better than all existing baselines. Additonally, We propose\na set of submodular functions for opinion summarization. Opinion summarization\nhas built in it the tasks of summarization and sentiment detection. However, it\nis not easy to detect sentiment and simultaneously extract summary. The two\ntasks conflict in the sense that the demand of compression may drop sentiment\nbearing sentences, and the demand of sentiment detection may bring in redundant\nsentences. However, using submodularity we show how to strike a balance between\nthe two requirements. Our functions generate summaries such that there is good\ncorrelation between document sentiment and summary sentiment along with good\nROUGE score. We also compare the performances of the proposed submodular\nfunctions.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00013v1",
    "published_date": "2024-05-20 21:27:18 UTC",
    "updated_date": "2024-05-20 21:27:18 UTC"
  },
  {
    "arxiv_id": "2405.13071v2",
    "title": "A Novel Method for News Article Event-Based Embedding",
    "authors": [
      "Koren Ishlach",
      "Itzhak Ben-David",
      "Michael Fire",
      "Lior Rokach"
    ],
    "abstract": "Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13071v2",
    "published_date": "2024-05-20 20:55:07 UTC",
    "updated_date": "2024-08-02 09:30:03 UTC"
  },
  {
    "arxiv_id": "2405.12368v1",
    "title": "Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)",
    "authors": [
      "Megha Thukral",
      "Sourish Gunesh Dhekane",
      "Shruthi K. Hiremath",
      "Harish Haresamudram",
      "Thomas Ploetz"
    ],
    "abstract": "Human activity recognition (HAR) using ambient sensors in smart homes has\nnumerous applications for human healthcare and wellness. However, building\ngeneral-purpose HAR models that can be deployed to new smart home environments\nrequires a significant amount of annotated sensor data and training overhead.\nMost smart homes vary significantly in their layouts, i.e., floor plans and the\nspecifics of sensors embedded, resulting in low generalizability of HAR models\ntrained for specific homes. We address this limitation by introducing a novel,\nlayout-agnostic modeling approach for HAR systems in smart homes that utilizes\nthe transferrable representational capacity of natural language descriptions of\nraw sensor data. To this end, we generate Textual Descriptions Of Sensor\nTriggers (TDOST) that encapsulate the surrounding trigger conditions and\nprovide cues for underlying activities to the activity recognition models.\nLeveraging textual embeddings, rather than raw sensor data, we create activity\nrecognition systems that predict standard activities across homes without\neither (re-)training or adaptation on target homes. Through an extensive\nevaluation, we demonstrate the effectiveness of TDOST-based models in unseen\nsmart homes through experiments on benchmarked CASAS datasets. Furthermore, we\nconduct a detailed analysis of how the individual components of our approach\naffect downstream activity recognition performance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12368v1",
    "published_date": "2024-05-20 20:37:44 UTC",
    "updated_date": "2024-05-20 20:37:44 UTC"
  },
  {
    "arxiv_id": "2405.12354v1",
    "title": "A Study on Optimization Techniques for Variational Quantum Circuits in Reinforcement Learning",
    "authors": [
      "Michael K√∂lle",
      "Timo Witter",
      "Tobias Rohe",
      "Gerhard Stenzel",
      "Philipp Altmann",
      "Thomas Gabor"
    ],
    "abstract": "Quantum Computing aims to streamline machine learning, making it more\neffective with fewer trainable parameters. This reduction of parameters can\nspeed up the learning process and reduce the use of computational resources.\nHowever, in the current phase of quantum computing development, known as the\nnoisy intermediate-scale quantum era (NISQ), learning is difficult due to a\nlimited number of qubits and widespread quantum noise. To overcome these\nchallenges, researchers are focusing on variational quantum circuits (VQCs).\nVQCs are hybrid algorithms that merge a quantum circuit, which can be adjusted\nthrough parameters, with traditional classical optimization techniques. These\ncircuits require only few qubits for effective learning. Recent studies have\npresented new ways of applying VQCs to reinforcement learning, showing\npromising results that warrant further exploration. This study investigates the\neffects of various techniques -- data re-uploading, input scaling, output\nscaling -- and introduces exponential learning rate decay in the quantum\nproximal policy optimization algorithm's actor-VQC. We assess these methods in\nthe popular Frozen Lake and Cart Pole environments. Our focus is on their\nability to reduce the number of parameters in the VQC without losing\neffectiveness. Our findings indicate that data re-uploading and an exponential\nlearning rate decay significantly enhance hyperparameter stability and overall\nperformance. While input scaling does not improve parameter efficiency, output\nscaling effectively manages greediness, leading to increased learning speed and\nrobustness.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted at QSW 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.12354v1",
    "published_date": "2024-05-20 20:06:42 UTC",
    "updated_date": "2024-05-20 20:06:42 UTC"
  },
  {
    "arxiv_id": "2405.15808v2",
    "title": "Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "Misdiagnosis is a significant issue in healthcare, leading to harmful\nconsequences for patients. The propagation of mislabeled data through machine\nlearning models into clinical practice is unacceptable. This paper proposes\nEVINCE, a system designed to 1) improve diagnosis accuracy and 2) rectify\nmisdiagnoses and minimize training data errors. EVINCE stands for Entropy\nVariation through Information Duality with Equal Competence, leveraging this\nnovel theory to optimize the diagnostic process using multiple Large Language\nModels (LLMs) in a structured debate framework. Our empirical study verifies\nEVINCE to be effective in achieving its design goals.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 4 tables, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.15808v2",
    "published_date": "2024-05-20 18:26:36 UTC",
    "updated_date": "2024-05-28 05:11:50 UTC"
  },
  {
    "arxiv_id": "2405.12299v2",
    "title": "Perturbing the Gradient for Alleviating Meta Overfitting",
    "authors": [
      "Manas Gogoi",
      "Sambhavi Tiwari",
      "Shekhar Verma"
    ],
    "abstract": "The reason for Meta Overfitting can be attributed to two factors: Mutual\nNon-exclusivity and the Lack of diversity, consequent to which a single global\nfunction can fit the support set data of all the meta-training tasks and fail\nto generalize to new unseen tasks. This issue is evidenced by low error rates\non the meta-training tasks, but high error rates on new tasks. However, there\ncan be a number of novel solutions to this problem keeping in mind any of the\ntwo objectives to be attained, i.e. to increase diversity in the tasks and to\nreduce the confidence of the model for some of the tasks. In light of the\nabove, this paper proposes a number of solutions to tackle meta-overfitting on\nfew-shot learning settings, such as few-shot sinusoid regression and few shot\nclassification. Our proposed approaches demonstrate improved generalization\nperformance compared to state-of-the-art baselines for learning in a\nnon-mutually exclusive task setting. Overall, this paper aims to provide\ninsights into tackling overfitting in meta-learning and to advance the field\ntowards more robust and generalizable models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12299v2",
    "published_date": "2024-05-20 18:04:59 UTC",
    "updated_date": "2024-11-10 13:46:36 UTC"
  },
  {
    "arxiv_id": "2405.12217v2",
    "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
    "authors": [
      "Guanglin Zhou",
      "Zhongyi Han",
      "Shiming Chen",
      "Biwei Huang",
      "Liming Zhu",
      "Salman Khan",
      "Xin Gao",
      "Lina Yao"
    ],
    "abstract": "Recent studies indicate that large multimodal models (LMMs) potentially act\nas general-purpose assistants and are highly robust against different\ndistributions. Despite this, domain-specific adaptation is still necessary\nparticularly in specialized areas like healthcare. Due to the impracticality of\nfine-tuning LMMs given their vast parameter space, this work investigates\nin-context learning (ICL) as an effective alternative for enhancing LMMs'\nadaptability. Our study addresses this by evaluating an unsupervised ICL method\nwhich selects in-context examples through a nearest example search based on\nfeature similarity. We uncover that its effectiveness is limited by the\ndeficiencies of pre-trained vision encoders under distribution shift scenarios.\nTo address these challenges, we propose InvariantSelectPR, a novel method\nleveraging Class-conditioned Contrastive Invariance (CCI) for more robust\ndemonstration selection. Specifically, CCI enhances pre-trained vision encoders\nby improving their discriminative capabilities across different classes and\nensuring invariance to domain-specific variations. This enhancement allows the\nencoders to effectively identify and retrieve the most informative examples,\nwhich are then used to guide LMMs in adapting to new query samples under\nvarying distributions. Our experiments show that InvariantSelectPR\nsubstantially improves the adaptability of LMMs, achieving significant\nperformance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy\nincrease in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on\nHAM10000 compared to the baseline zero-shot performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 9 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.12217v2",
    "published_date": "2024-05-20 17:59:21 UTC",
    "updated_date": "2024-10-14 23:27:44 UTC"
  },
  {
    "arxiv_id": "2405.12266v1",
    "title": "EGAN: Evolutional GAN for Ransomware Evasion",
    "authors": [
      "Daniel Commey",
      "Benjamin Appiah",
      "Bill K. Frimpong",
      "Isaac Osei",
      "Ebenezer N. A. Hammond",
      "Garth V. Crosby"
    ],
    "abstract": "Adversarial Training is a proven defense strategy against adversarial\nmalware. However, generating adversarial malware samples for this type of\ntraining presents a challenge because the resulting adversarial malware needs\nto remain evasive and functional. This work proposes an attack framework, EGAN,\nto address this limitation. EGAN leverages an Evolution Strategy and Generative\nAdversarial Network to select a sequence of attack actions that can mutate a\nRansomware file while preserving its original functionality. We tested this\nframework on popular AI-powered commercial antivirus systems listed on\nVirusTotal and demonstrated that our framework is capable of bypassing the\nmajority of these systems. Moreover, we evaluated whether the EGAN attack\nframework can evade other commercial non-AI antivirus solutions. Our results\nindicate that the adversarial ransomware generated can increase the probability\nof evading some of them.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12266v1",
    "published_date": "2024-05-20 17:52:40 UTC",
    "updated_date": "2024-05-20 17:52:40 UTC"
  },
  {
    "arxiv_id": "2405.12205v1",
    "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
    "authors": [
      "Aniket Didolkar",
      "Anirudh Goyal",
      "Nan Rosemary Ke",
      "Siyuan Guo",
      "Michal Valko",
      "Timothy Lillicrap",
      "Danilo Rezende",
      "Yoshua Bengio",
      "Michael Mozer",
      "Sanjeev Arora"
    ],
    "abstract": "Metacognitive knowledge refers to humans' intuitive knowledge of their own\nthinking and reasoning processes. Today's best LLMs clearly possess some\nreasoning processes. The paper gives evidence that they also have metacognitive\nknowledge, including ability to name skills and procedures to apply given a\ntask. We explore this primarily in context of math reasoning, developing a\nprompt-guided interaction procedure to get a powerful LLM to assign sensible\nskill labels to math questions, followed by having it perform semantic\nclustering to obtain coarser families of skill labels. These coarse skill\nlabels look interpretable to humans.\n  To validate that these skill labels are meaningful and relevant to the LLM's\nreasoning processes we perform the following experiments. (a) We ask GPT-4 to\nassign skill labels to training questions in math datasets GSM8K and MATH. (b)\nWhen using an LLM to solve the test questions, we present it with the full list\nof skill labels and ask it to identify the skill needed. Then it is presented\nwith randomly selected exemplar solved questions associated with that skill\nlabel. This improves accuracy on GSM8k and MATH for several strong LLMs,\nincluding code-assisted models. The methodology presented is domain-agnostic,\neven though this article applies it to math problems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2405.12205v1",
    "published_date": "2024-05-20 17:45:26 UTC",
    "updated_date": "2024-05-20 17:45:26 UTC"
  },
  {
    "arxiv_id": "2405.12202v1",
    "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
    "authors": [
      "Xihaier Luo",
      "Xiaoning Qian",
      "Byung-Jun Yoon"
    ],
    "abstract": "In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12202v1",
    "published_date": "2024-05-20 17:39:29 UTC",
    "updated_date": "2024-05-20 17:39:29 UTC"
  },
  {
    "arxiv_id": "2405.13068v2",
    "title": "Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation",
    "authors": [
      "Yuxi Li",
      "Yi Liu",
      "Yuekang Li",
      "Ling Shi",
      "Gelei Deng",
      "Shengquan Chen",
      "Kailong Wang"
    ],
    "abstract": "Large language models (LLMs) have transformed the field of natural language\nprocessing, but they remain susceptible to jailbreaking attacks that exploit\ntheir capabilities to generate unintended and potentially harmful content.\nExisting token-level jailbreaking techniques, while effective, face scalability\nand efficiency challenges, especially as models undergo frequent updates and\nincorporate advanced defensive measures. In this paper, we introduce JailMine,\nan innovative token-level manipulation approach that addresses these\nlimitations effectively. JailMine employs an automated \"mining\" process to\nelicit malicious responses from LLMs by strategically selecting affirmative\noutputs and iteratively reducing the likelihood of rejection. Through rigorous\ntesting across multiple well-known LLMs and datasets, we demonstrate JailMine's\neffectiveness and efficiency, achieving a significant average reduction of 86%\nin time consumed while maintaining high success rates averaging 95%, even in\nthe face of evolving defensive strategies. Our work contributes to the ongoing\neffort to assess and mitigate the vulnerability of LLMs to jailbreaking\nattacks, underscoring the importance of continued vigilance and proactive\nmeasures to enhance the security and reliability of these powerful language\nmodels.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13068v2",
    "published_date": "2024-05-20 17:17:55 UTC",
    "updated_date": "2024-06-19 13:51:06 UTC"
  },
  {
    "arxiv_id": "2405.12183v1",
    "title": "Multi-order Graph Clustering with Adaptive Node-level Weight Learning",
    "authors": [
      "Ye Liu",
      "Xuelei Lin",
      "Yejia Chen",
      "Reynold Cheng"
    ],
    "abstract": "Current graph clustering methods emphasize individual node and edge con\nnections, while ignoring higher-order organization at the level of motif. Re\ncently, higher-order graph clustering approaches have been designed by motif\nbased hypergraphs. However, these approaches often suffer from hypergraph\nfragmentation issue seriously, which degrades the clustering performance\ngreatly. Moreover, real-world graphs usually contain diverse motifs, with nodes\nparticipating in multiple motifs. A key challenge is how to achieve precise\nclustering results by integrating information from multiple motifs at the node\nlevel. In this paper, we propose a multi-order graph clustering model (MOGC) to\nintegrate multiple higher-order structures and edge connections at node level.\nMOGC employs an adaptive weight learning mechanism to au tomatically adjust the\ncontributions of different motifs for each node. This not only tackles\nhypergraph fragmentation issue but enhances clustering accuracy. MOGC is\nefficiently solved by an alternating minimization algo rithm. Experiments on\nseven real-world datasets illustrate the effectiveness of MOGC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12183v1",
    "published_date": "2024-05-20 17:09:58 UTC",
    "updated_date": "2024-05-20 17:09:58 UTC"
  },
  {
    "arxiv_id": "2405.12179v3",
    "title": "TENNs-PLEIADES: Building Temporal Kernels with Orthogonal Polynomials",
    "authors": [
      "Yan Ru Pei",
      "Olivier Coenen"
    ],
    "abstract": "We introduce a neural network named PLEIADES (PoLynomial Expansion In\nAdaptive Distributed Event-based Systems), belonging to the TENNs (Temporal\nNeural Networks) architecture. We focus on interfacing these networks with\nevent-based data to perform online spatiotemporal classification and detection\nwith low latency. By virtue of using structured temporal kernels and\nevent-based data, we have the freedom to vary the sample rate of the data along\nwith the discretization step-size of the network without additional finetuning.\nWe experimented with three event-based benchmarks and obtained state-of-the-art\nresults on all three by large margins with significantly smaller memory and\ncompute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the\nDVS128 hand gesture recognition dataset and 100% with a small additional output\nfilter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye\ntracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1\nMegapixel Automotive Detection Dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12179v3",
    "published_date": "2024-05-20 17:06:24 UTC",
    "updated_date": "2024-05-31 18:29:13 UTC"
  },
  {
    "arxiv_id": "2405.12163v1",
    "title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging",
    "authors": [
      "Xiaobo Liang",
      "Haoke Zhang",
      "Helan hu",
      "Juntao Li",
      "Jun Xu",
      "Min Zhang"
    ],
    "abstract": "The rapid advancement of large language models has given rise to a plethora\nof applications across a myriad of real-world tasks, mainly centered on\naligning with human intent. However, the complexities inherent in human intent\nnecessitate a dependence on labor-intensive and time-consuming human\nevaluation. To alleviate this constraint, we delve into the paradigm of\nemploying open-source large language models as evaluators, aligning with the\nprevailing trend of utilizing GPT-4. Particularly, we present a step-by-step\nevaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained\n\\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through\nbran\\textbf{C}hing and bridging. Specifically, the branching operation dissects\nthe evaluation task into various dimensions and granularities, thereby\nalleviating the challenges associated with evaluation. Concurrently, the\nbridging operation amalgamates diverse training datasets, augmenting the\nvariety of evaluation tasks. In experimental trials, our 7B model consistently\noutperforms open-source larger-scale evaluation models across various widely\nadopted benchmarks in terms of both \\textit{Agreement} and\n\\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ\nthe fine-grained correction capabilities induced by the evaluation model to\nrefine multiple model responses, and the results show that the refinement\nelevates the quality of responses, leading to an improvement of 1-2 points on\nthe MT-Bench. Our code is available at\nGithub\\footnote{\\url{https://github.com/dropreg/Fennec}}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12163v1",
    "published_date": "2024-05-20 16:47:22 UTC",
    "updated_date": "2024-05-20 16:47:22 UTC"
  },
  {
    "arxiv_id": "2405.12150v1",
    "title": "Bangladeshi Native Vehicle Detection in Wild",
    "authors": [
      "Bipin Saha",
      "Md. Johirul Islam",
      "Shaikh Khaled Mostaque",
      "Aditya Bhowmik",
      "Tapodhir Karmakar Taton",
      "Md. Nakib Hayat Chowdhury",
      "Mamun Bin Ibne Reaz"
    ],
    "abstract": "The success of autonomous navigation relies on robust and precise vehicle\nrecognition, hindered by the scarcity of region-specific vehicle detection\ndatasets, impeding the development of context-aware systems. To advance\nterrestrial object detection research, this paper proposes a native vehicle\ndetection dataset for the most commonly appeared vehicle classes in Bangladesh.\n17 distinct vehicle classes have been taken into account, with fully annotated\n81542 instances of 17326 images. Each image width is set to at least 1280px.\nThe dataset's average vehicle bounding box-to-image ratio is 4.7036. This\nBangladesh Native Vehicle Dataset (BNVD) has accounted for several\ngeographical, illumination, variety of vehicle sizes, and orientations to be\nmore robust on surprised scenarios. In the context of examining the BNVD\ndataset, this work provides a thorough assessment with four successive You Only\nLook Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's\neffectiveness is methodically evaluated and contrasted with other vehicle\ndatasets already in use. The BNVD dataset exhibits mean average precision(mAP)\nat 50% intersection over union (IoU) is 0.848 corresponding precision and\nrecall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643\nat an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset\nserves as a reliable representation of vehicle distribution and presents\nconsiderable complexities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.12150v1",
    "published_date": "2024-05-20 16:23:40 UTC",
    "updated_date": "2024-05-20 16:23:40 UTC"
  },
  {
    "arxiv_id": "2405.12147v2",
    "title": "Eliciting Problem Specifications via Large Language Models",
    "authors": [
      "Robert E. Wray",
      "James R. Kirk",
      "John E. Laird"
    ],
    "abstract": "Cognitive systems generally require a human to translate a problem definition\ninto some specification that the cognitive system can use to attempt to solve\nthe problem or perform the task. In this paper, we illustrate that large\nlanguage models (LLMs) can be utilized to map a problem class, defined in\nnatural language, into a semi-formal specification that can then be utilized by\nan existing reasoning and learning system to solve instances from the problem\nclass. We present the design of LLM-enabled cognitive task analyst agent(s).\nImplemented with LLM agents, this system produces a definition of problem\nspaces for tasks specified in natural language. LLM prompts are derived from\nthe definition of problem spaces in the AI literature and general\nproblem-solving strategies (Polya's How to Solve It). A cognitive system can\nthen use the problem-space specification, applying domain-general problem\nsolving strategies (\"weak methods\" such as search), to solve multiple instances\nof problems from the problem class. This result, while preliminary, suggests\nthe potential for speeding cognitive systems research via disintermediation of\nproblem formulation while also retaining core capabilities of cognitive\nsystems, such as robust inference and online learning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.11; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, Appendix. Revised in response to reviewer feedback.\n  Accepted for Advances in Cognitive Systems (Jun 2024, Palermo)",
    "pdf_url": "http://arxiv.org/pdf/2405.12147v2",
    "published_date": "2024-05-20 16:19:02 UTC",
    "updated_date": "2024-06-10 19:05:57 UTC"
  },
  {
    "arxiv_id": "2405.13065v1",
    "title": "Exploring Teachers' Perception of Artificial Intelligence: The Socio-emotional Deficiency as Opportunities and Challenges in Human-AI Complementarity in K-12 Education",
    "authors": [
      "Soon-young Oh",
      "Yongsu Ahn"
    ],
    "abstract": "In schools, teachers play a multitude of roles, serving as educators,\ncounselors, decision-makers, and members of the school community. With recent\nadvances in artificial intelligence (AI), there is increasing discussion about\nhow AI can assist, complement, and collaborate with teachers. To pave the way\nfor better teacher-AI complementary relationships in schools, our study aims to\nexpand the discourse on teacher-AI complementarity by seeking educators'\nperspectives on the potential strengths and limitations of AI across a spectrum\nof responsibilities. Through a mixed method using a survey with 100 elementary\nschool teachers in South Korea and in-depth interviews with 12 teachers, our\nfindings indicate that teachers anticipate AI's potential to complement human\nteachers by automating administrative tasks and enhancing personalized learning\nthrough advanced intelligence. Interestingly, the deficit of AI's\nsocio-emotional capabilities has been perceived as both challenges and\nopportunities. Overall, our study demonstrates the nuanced perception of\nteachers and different levels of expectations over their roles, challenging the\nneed for decisions about AI adoption tailored to educators' preferences and\nconcerns.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.13065v1",
    "published_date": "2024-05-20 15:43:04 UTC",
    "updated_date": "2024-05-20 15:43:04 UTC"
  },
  {
    "arxiv_id": "2405.12262v1",
    "title": "Prompt Learning for Generalized Vehicle Routing",
    "authors": [
      "Fei Liu",
      "Xi Lin",
      "Weiduo Liao",
      "Zhenkun Wang",
      "Qingfu Zhang",
      "Xialiang Tong",
      "Mingxuan Yuan"
    ],
    "abstract": "Neural combinatorial optimization (NCO) is a promising learning-based\napproach to solving various vehicle routing problems without much manual\nalgorithm design. However, the current NCO methods mainly focus on the\nin-distribution performance, while the real-world problem instances usually\ncome from different distributions. A costly fine-tuning approach or generalized\nmodel retraining from scratch could be needed to tackle the out-of-distribution\ninstances. Unlike the existing methods, this work investigates an efficient\nprompt learning approach in NCO for cross-distribution adaptation. To be\nconcrete, we propose a novel prompt learning method to facilitate fast\nzero-shot adaptation of a pre-trained model to solve routing problem instances\nfrom different distributions. The proposed model learns a set of prompts among\nvarious distributions and then selects the best-matched one to prompt a\npre-trained attention model for each problem instance. Extensive experiments\nshow that the proposed prompt learning approach facilitates the fast adaptation\nof pre-trained routing models. It also outperforms existing generalized models\non both in-distribution prediction and zero-shot generalization to a diverse\nset of new tasks. Our code implementation is available online\nhttps://github.com/FeiLiu36/PromptVRP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12262v1",
    "published_date": "2024-05-20 15:42:23 UTC",
    "updated_date": "2024-05-20 15:42:23 UTC"
  },
  {
    "arxiv_id": "2405.12119v1",
    "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
    "authors": [
      "Zhankui He",
      "Zhouhang Xie",
      "Harald Steck",
      "Dawen Liang",
      "Rahul Jha",
      "Nathan Kallus",
      "Julian McAuley"
    ],
    "abstract": "Large language models (LLMs) are revolutionizing conversational recommender\nsystems by adeptly indexing item content, understanding complex conversational\ncontexts, and generating relevant item titles. However, controlling the\ndistribution of recommended items remains a challenge. This leads to suboptimal\nperformance due to the failure to capture rapidly changing data distributions,\nsuch as item popularity, on targeted conversational recommendation platforms.\nIn conversational recommendation, LLMs recommend items by generating the titles\n(as multiple tokens) autoregressively, making it difficult to obtain and\ncontrol the recommendations over all items. Thus, we propose a\nReindex-Then-Adapt (RTA) framework, which converts multi-token item titles into\nsingle tokens within LLMs, and then adjusts the probability distributions over\nthese single-token item titles accordingly. The RTA framework marries the\nbenefits of both LLMs and traditional recommender systems (RecSys):\nunderstanding complex queries as LLMs do; while efficiently controlling the\nrecommended item distributions in conversational recommendations as traditional\nRecSys do. Our framework demonstrates improved accuracy metrics across three\ndifferent conversational recommendation datasets and two adaptation settings",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12119v1",
    "published_date": "2024-05-20 15:37:55 UTC",
    "updated_date": "2024-05-20 15:37:55 UTC"
  },
  {
    "arxiv_id": "2405.13064v1",
    "title": "Digital Health and Indoor Air Quality: An IoT-Driven Human-Centred Visualisation Platform for Behavioural Change and Technology Acceptance",
    "authors": [
      "Rameez Raja Kureshi",
      "Bhupesh Kumar Mishra",
      "Dhavalkumar Thakker",
      "Suvodeep Mazumdar",
      "Xiao Li"
    ],
    "abstract": "The detrimental effects of air pollutants on human health have prompted\nincreasing concerns regarding indoor air quality (IAQ). The emergence of\ndigital health interventions and citizen science initiatives has provided new\navenues for raising awareness, improving IAQ, and promoting behavioural\nchanges. The Technology Acceptance Model (TAM) offers a theoretical framework\nto understand user acceptance and adoption of IAQ technology. This paper\npresents a case study using the COM-B model and Internet of Things (IoT)\ntechnology to design a human-centred digital visualisation platform, leading to\nbehavioural changes and improved IAQ. The study also investigates users'\nacceptance and adoption of the technology, focusing on their experiences,\nexpectations, and the impact on IAQ. Integrating IAQ sensing, digital\nhealth-related interventions, citizen science, and the TAM model offers\nopportunities to address IAQ challenges, enhance public health, and foster\nsustainable indoor environments. The analytical results show that factors such\nas human behaviour, indoor activities, and awareness play crucial roles in\nshaping IAQ.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.13064v1",
    "published_date": "2024-05-20 15:13:25 UTC",
    "updated_date": "2024-05-20 15:13:25 UTC"
  },
  {
    "arxiv_id": "2405.13062v1",
    "title": "StatAvg: Mitigating Data Heterogeneity in Federated Learning for Intrusion Detection Systems",
    "authors": [
      "Pavlos S. Bouzinis",
      "Panagiotis Radoglou-Grammatikis",
      "Ioannis Makris",
      "Thomas Lagkas",
      "Vasileios Argyriou",
      "Georgios Th. Papadopoulos",
      "Panagiotis Sarigiannidis",
      "George K. Karagiannidis"
    ],
    "abstract": "Federated learning (FL) is a decentralized learning technique that enables\nparticipating devices to collaboratively build a shared Machine Leaning (ML) or\nDeep Learning (DL) model without revealing their raw data to a third party. Due\nto its privacy-preserving nature, FL has sparked widespread attention for\nbuilding Intrusion Detection Systems (IDS) within the realm of cybersecurity.\nHowever, the data heterogeneity across participating domains and entities\npresents significant challenges for the reliable implementation of an FL-based\nIDS. In this paper, we propose an effective method called Statistical Averaging\n(StatAvg) to alleviate non-independently and identically (non-iid) distributed\nfeatures across local clients' data in FL. In particular, StatAvg allows the FL\nclients to share their individual data statistics with the server, which then\naggregates this information to produce global statistics. The latter are shared\nwith the clients and used for universal data normalisation. It is worth\nmentioning that StatAvg can seamlessly integrate with any FL aggregation\nstrategy, as it occurs before the actual FL training process. The proposed\nmethod is evaluated against baseline approaches using datasets for network and\nhost Artificial Intelligence (AI)-powered IDS. The experimental results\ndemonstrate the efficiency of StatAvg in mitigating non-iid feature\ndistributions across the FL clients compared to the baseline methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.13062v1",
    "published_date": "2024-05-20 14:41:59 UTC",
    "updated_date": "2024-05-20 14:41:59 UTC"
  },
  {
    "arxiv_id": "2405.12070v1",
    "title": "AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements",
    "authors": [
      "Calvin Yeung",
      "Kenjiro Ide",
      "Keisuke Fujii"
    ],
    "abstract": "Image understanding is a foundational task in computer vision, with recent\napplications emerging in soccer posture analysis. However, existing publicly\navailable datasets lack comprehensive information, notably in the form of\nposture sequences and 2D pose annotations. Moreover, current analysis models\noften rely on interpretable linear models (e.g., PCA and regression), limiting\ntheir capacity to capture non-linear spatiotemporal relationships in complex\nand diverse scenarios. To address these gaps, we introduce the 3D Shot Posture\n(3DSP) dataset in soccer broadcast videos, which represents the most extensive\nsports image dataset with 2D pose annotations to our knowledge. Additionally,\nwe present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear\napproach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,\na pipeline aimed at semi-automating 2D and 3D pose estimation and posture\nanalysis. While achieving full automation proved challenging, we provide a\nfoundational baseline, extending its utility beyond the scope of annotated\ndata. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present\nposture analysis results based on 3DSP. The dataset, code, and models are\navailable at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12070v1",
    "published_date": "2024-05-20 14:40:26 UTC",
    "updated_date": "2024-05-20 14:40:26 UTC"
  },
  {
    "arxiv_id": "2405.12261v1",
    "title": "EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods",
    "authors": [
      "Benedict Clark",
      "Rick Wilming",
      "Artur Dox",
      "Paul Eschenbach",
      "Sami Hached",
      "Daniel Jin Wodke",
      "Michias Taye Zewdie",
      "Uladzislau Bruila",
      "Marta Oliveira",
      "Hjalmar Schulz",
      "Luca Matteo Cornils",
      "Danny Panknin",
      "Ahc√®ne Boubekki",
      "Stefan Haufe"
    ],
    "abstract": "The evolving landscape of explainable artificial intelligence (XAI) aims to\nimprove the interpretability of intricate machine learning (ML) models, yet\nfaces challenges in formalisation and empirical validation, being an inherently\nunsupervised process. In this paper, we bring together various benchmark\ndatasets and novel performance metrics in an initial benchmarking platform, the\nExplainable AI Comparison Toolkit (EXACT), providing a standardised foundation\nfor evaluating XAI methods. Our datasets incorporate ground truth explanations\nfor class-conditional features, and leveraging novel quantitative metrics, this\nplatform assesses the performance of post-hoc XAI methods in the quality of the\nexplanations they produce. Our recent findings have highlighted the limitations\nof popular XAI methods, as they often struggle to surpass random baselines,\nattributing significance to irrelevant features. Moreover, we show the\nvariability in explanations derived from different equally performing model\narchitectures. This initial benchmarking platform therefore aims to allow XAI\nresearchers to test and assure the high quality of their newly developed\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12261v1",
    "published_date": "2024-05-20 14:16:06 UTC",
    "updated_date": "2024-05-20 14:16:06 UTC"
  },
  {
    "arxiv_id": "2405.12035v1",
    "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
    "authors": [
      "Diego Sanmartin"
    ],
    "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12035v1",
    "published_date": "2024-05-20 14:03:05 UTC",
    "updated_date": "2024-05-20 14:03:05 UTC"
  },
  {
    "arxiv_id": "2405.12001v4",
    "title": "Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning",
    "authors": [
      "Hai Zhang",
      "Boyuan Zheng",
      "Tianying Ji",
      "Jinhang Liu",
      "Anqi Guo",
      "Junqiao Zhao",
      "Lanqing Li"
    ],
    "abstract": "Offline meta reinforcement learning (OMRL) has emerged as a promising\napproach for interaction avoidance and strong generalization performance by\nleveraging pre-collected data and meta-learning techniques. Previous\ncontext-based approaches predominantly rely on the intuition that alternating\noptimization between the context encoder and the policy can lead to performance\nimprovements, as long as the context encoder follows the principle of\nmaximizing the mutual information between the task variable $M$ and its latent\nrepresentation $Z$ ($I(Z;M)$) while the policy adopts the standard offline\nreinforcement learning (RL) algorithms conditioning on the learned task\nrepresentation.Despite promising results, the theoretical justification of\nperformance improvements for such intuition remains underexplored.Inspired by\nthe return discrepancy scheme in the model-based RL field, we find that the\nprevious optimization framework can be linked with the general RL objective of\nmaximizing the expected return, thereby explaining performance improvements.\nFurthermore, after scrutinizing this optimization framework, we observe that\nthe condition for monotonic performance improvements does not consider the\nvariation of the task representation. When these variations are considered, the\npreviously established condition may no longer be sufficient to ensure\nmonotonicity, thereby impairing the optimization process.We name this issue\ntask representation shift and theoretically prove that the monotonic\nperformance improvements can be guaranteed with appropriate context encoder\nupdates.Our work opens up a new avenue for OMRL, leading to a better\nunderstanding between the task representation and performance improvements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accept at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.12001v4",
    "published_date": "2024-05-20 13:14:26 UTC",
    "updated_date": "2025-02-03 01:44:54 UTC"
  },
  {
    "arxiv_id": "2407.03340v2",
    "title": "A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party Conversation",
    "authors": [
      "Iveta Beƒçkov√°",
      "≈†tefan P√≥co≈°",
      "Giulia Belgiovine",
      "Marco Matarese",
      "Omar Eldardeer",
      "Alessandra Sciutti",
      "Carlo Mazzola"
    ],
    "abstract": "The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "cs.RO",
      "eess.IV",
      "I.4.8; I.2.10; I.2.9; I.2.11; J.4"
    ],
    "primary_category": "cs.AI",
    "comment": "32pp (+6pp sup.mat.) Accepted in Computer Vision and Image\n  Understanding Journal on January 23, 2025. This research received funding\n  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and\n  Development Agency, project no. APVV-21-0105",
    "pdf_url": "http://arxiv.org/pdf/2407.03340v2",
    "published_date": "2024-05-20 13:09:32 UTC",
    "updated_date": "2025-01-31 12:15:57 UTC"
  },
  {
    "arxiv_id": "2405.11983v2",
    "title": "A review on the use of large language models as virtual tutors",
    "authors": [
      "Silvia Garc√≠a-M√©ndez",
      "Francisco de Arriba-P√©rez",
      "Mar√≠a del Carmen Somoza-L√≥pez"
    ],
    "abstract": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11983v2",
    "published_date": "2024-05-20 12:33:42 UTC",
    "updated_date": "2024-09-05 10:01:39 UTC"
  },
  {
    "arxiv_id": "2405.11982v1",
    "title": "Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space",
    "authors": [
      "Qianmei Liu",
      "Yufei Kuang",
      "Jie Wang"
    ],
    "abstract": "Deep reinforcement learning (DRL) algorithms can suffer from modeling errors\nbetween the simulation and the real world. Many studies use adversarial\nlearning to generate perturbation during training process to model the\ndiscrepancy and improve the robustness of DRL. However, most of these\napproaches use a fixed parameter to control the intensity of the adversarial\nperturbation, which can lead to a trade-off between average performance and\nrobustness. In fact, finding the optimal parameter of the perturbation is\nchallenging, as excessive perturbations may destabilize training and compromise\nagent performance, while insufficient perturbations may not impart enough\ninformation to enhance robustness. To keep the training stable while improving\nrobustness, we propose a simple but effective method, namely, Adaptive\nAdversarial Perturbation (A2P), which can dynamically select appropriate\nadversarial perturbations for each sample. Specifically, we propose an adaptive\nadversarial coefficient framework to adjust the effect of the adversarial\nperturbation during training. By designing a metric for the current intensity\nof the perturbation, our method can calculate the suitable perturbation levels\nbased on the current relative performance. The appealing feature of our method\nis that it is simple to deploy in real-world applications and does not require\naccessing the simulator in advance. The experiments in MuJoCo show that our\nmethod can improve the training stability and learn a robust policy when\nmigrated to different test environments. The code is available at\nhttps://github.com/Lqm00/A2P-SAC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11982v1",
    "published_date": "2024-05-20 12:31:11 UTC",
    "updated_date": "2024-05-20 12:31:11 UTC"
  },
  {
    "arxiv_id": "2405.13059v1",
    "title": "RNG: Reducing Multi-level Noise and Multi-grained Semantic Gap for Joint Multimodal Aspect-Sentiment Analysis",
    "authors": [
      "Yaxin Liu",
      "Yan Zhou",
      "Ziming Li",
      "Jinchuan Zhang",
      "Yu Shang",
      "Chenyang Zhang",
      "Songlin Hu"
    ],
    "abstract": "As an important multimodal sentiment analysis task, Joint Multimodal\nAspect-Sentiment Analysis (JMASA), aiming to jointly extract aspect terms and\ntheir associated sentiment polarities from the given text-image pairs, has\ngained increasing concerns. Existing works encounter two limitations: (1)\nmulti-level modality noise, i.e., instance- and feature-level noise; and (2)\nmulti-grained semantic gap, i.e., coarse- and fine-grained gap. Both issues may\ninterfere with accurate identification of aspect-sentiment pairs. To address\nthese limitations, we propose a novel framework named RNG for JMASA.\nSpecifically, to simultaneously reduce multi-level modality noise and\nmulti-grained semantic gap, we design three constraints: (1) Global Relevance\nConstraint (GR-Con) based on text-image similarity for instance-level noise\nreduction, (2) Information Bottleneck Constraint (IB-Con) based on the\nInformation Bottleneck (IB) principle for feature-level noise reduction, and\n(3) Semantic Consistency Constraint (SC-Con) based on mutual information\nmaximization in a contrastive learning way for multi-grained semantic gap\nreduction. Extensive experiments on two datasets validate our new\nstate-of-the-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICME 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.13059v1",
    "published_date": "2024-05-20 12:18:46 UTC",
    "updated_date": "2024-05-20 12:18:46 UTC"
  },
  {
    "arxiv_id": "2405.11978v1",
    "title": "SM-DTW: Stability Modulated Dynamic Time Warping for signature verification",
    "authors": [
      "Antonio Parziale",
      "Moises Diaz",
      "Miguel A. Ferrer",
      "Angelo Marcelli"
    ],
    "abstract": "Building upon findings in computational model of handwriting learning and\nexecution, we introduce the concept of stability to explain the difference\nbetween the actual movements performed during multiple execution of the\nsubject's signature, and conjecture that the most stable parts of the signature\nshould play a paramount role in evaluating the similarity between a questioned\nsignature and the reference ones during signature verification. We then\nintroduce the Stability Modulated Dynamic Time Warping algorithm for\nincorporating the stability regions, i.e. the most similar parts between two\nsignatures, into the distance measure between a pair of signatures computed by\nthe Dynamic Time Warping for signature verification. Experiments were conducted\non two datasets largely adopted for performance evaluation. Experimental\nresults show that the proposed algorithm improves the performance of the\nbaseline system and compares favourably with other top performing signature\nverification systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11978v1",
    "published_date": "2024-05-20 12:18:15 UTC",
    "updated_date": "2024-05-20 12:18:15 UTC"
  },
  {
    "arxiv_id": "2405.11968v3",
    "title": "Conditional Shift-Robust Conformal Prediction for Graph Neural Network",
    "authors": [
      "S. Akansha"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as potent tools for predicting\noutcomes in graph-structured data. Despite their efficacy, a significant\ndrawback of GNNs lies in their limited ability to provide robust uncertainty\nestimates, posing challenges to their reliability in contexts where errors\ncarry significant consequences. Moreover, GNNs typically excel in\nin-distribution settings, assuming that training and test data follow identical\ndistributions a condition often unmet in real world graph data scenarios. In\nthis article, we leverage conformal prediction, a widely recognized statistical\ntechnique for quantifying uncertainty by transforming predictive model outputs\ninto prediction sets, to address uncertainty quantification in GNN predictions\namidst conditional shift\\footnote{Representing the change in conditional\nprobability distribution \\(P(label|input)\\) from source domain to target\ndomain.} in graph-based semi-supervised learning (SSL). Additionally, we\npropose a novel loss function aimed at refining model predictions by minimizing\nconditional shift in latent stages. Termed Conditional Shift Robust (CondSR)\nconformal prediction for GNNs, our approach CondSR is model-agnostic and\nadaptable to various classification models. We validate the effectiveness of\nour method on standard graph benchmark datasets, integrating it with\nstate-of-the-art GNNs in node classification tasks. Comprehensive evaluations\ndemonstrate that our approach consistently achieves any predefined target\nmarginal coverage, enhances the accuracy of state of the art GNN models by up\nto 12\\% under conditional shift, and reduces the prediction set size by up to\n48\\%. The code implementation is publicly available for further exploration and\nexperimentation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.11968v3",
    "published_date": "2024-05-20 11:47:31 UTC",
    "updated_date": "2025-03-25 08:27:10 UTC"
  },
  {
    "arxiv_id": "2405.11967v1",
    "title": "Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home",
    "authors": [
      "Tatiana V. Afanasieva",
      "Pavel V. Platov",
      "Anastasia I. Medvedeva"
    ],
    "abstract": "One of the new trends in the development of recommendation algorithms is the\ndissemination of their capabilities to support the population in managing their\nhealth. This article focuses on the problem of improving the effectiveness of\ncardiovascular diseases (CVD) prevention, since CVD is the leading cause of\ndeath worldwide. To address this issue, a knowledge-based recommendation\nalgorithm was proposed to support self-management of CVD risk factors in adults\nat home. The proposed algorithm is based on the original multidimensional\nrecommendation model and on a new user profile model, which includes predictive\nassessments of CVD health in addition to its current ones as outlined in\nofficial guidelines. The main feature of the proposed algorithm is the\ncombination of rule-based logic with the capabilities of a large language model\nin generating human-like text for explanatory component of multidimensional\nrecommendation. The verification and evaluation of the proposed algorithm\nshowed the usefulness of the proposed recommendation algorithm for supporting\nadults in self-management of their CVD risk factors at home. As follows from\nthe comparison with similar knowledge-based recommendation algorithms, the\nproposed algorithm evaluates a larger number of CVD risk factors and has a\ngreater information and semantic capacity of the generated recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "I.2, J.3"
    ],
    "primary_category": "cs.IR",
    "comment": "26 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.11967v1",
    "published_date": "2024-05-20 11:47:19 UTC",
    "updated_date": "2024-05-20 11:47:19 UTC"
  },
  {
    "arxiv_id": "2405.13058v2",
    "title": "The AI Community Building the Future? A Quantitative Analysis of Development Activity on Hugging Face Hub",
    "authors": [
      "Cailean Osborne",
      "Jennifer Ding",
      "Hannah Rose Kirk"
    ],
    "abstract": "Open model developers have emerged as key actors in the political economy of\nartificial intelligence (AI), but we still have a limited understanding of\ncollaborative practices in the open AI ecosystem. This paper responds to this\ngap with a three-part quantitative analysis of development activity on the\nHugging Face (HF) Hub, a popular platform for building, sharing, and\ndemonstrating models. First, various types of activity across 348,181 model,\n65,761 dataset, and 156,642 space repositories exhibit right-skewed\ndistributions. Activity is extremely imbalanced between repositories; for\nexample, over 70% of models have 0 downloads, while 1% account for 99% of\ndownloads. Furthermore, licenses matter: there are statistically significant\ndifferences in collaboration patterns in model repositories with permissive,\nrestrictive, and no licenses. Second, we analyse a snapshot of the social\nnetwork structure of collaboration in model repositories, finding that the\ncommunity has a core-periphery structure, with a core of prolific developers\nand a majority of isolate developers (89%). Upon removing the isolate\ndevelopers from the network, collaboration is characterised by high reciprocity\nregardless of developers' network positions. Third, we examine model adoption\nthrough the lens of model usage in spaces, finding that a minority of models,\ndeveloped by a handful of companies, are widely used on the HF Hub. Overall,\nactivity on the HF Hub is characterised by Pareto distributions, congruent with\nOSS development patterns on platforms like GitHub. We conclude with\nrecommendations for researchers, companies, and policymakers to advance our\nunderstanding of open AI development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "K.4.1"
    ],
    "primary_category": "cs.SE",
    "comment": "27 pages, 5 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.13058v2",
    "published_date": "2024-05-20 11:10:49 UTC",
    "updated_date": "2024-06-05 15:28:43 UTC"
  },
  {
    "arxiv_id": "2405.13057v1",
    "title": "Can Github issues be solved with Tree Of Thoughts?",
    "authors": [
      "Ricardo La Rosa",
      "Corey Hulse",
      "Bangdi Liu"
    ],
    "abstract": "While there have been extensive studies in code generation by large language\nmodels (LLM), where benchmarks like HumanEval have been surpassed with an\nimpressive 96.3% success rate, these benchmarks predominantly judge a model's\nperformance on basic function-level code generation and lack the critical\nthinking and concept of scope required of real-world scenarios such as solving\nGitHub issues. This research introduces the application of the Tree of Thoughts\n(ToT) language model reasoning framework for enhancing the decision-making and\nproblem-solving abilities of LLMs for this complex task. Compared to\ntraditional input-output (IO) prompting and Retrieval Augmented Generation\n(RAG) techniques, ToT is designed to improve performance by facilitating a\nstructured exploration of multiple reasoning trajectories and enabling\nself-assessment of potential solutions. We experimentally deploy ToT in\ntackling a Github issue contained within an instance of the SWE-bench. However,\nour results reveal that the ToT framework alone is not enough to give LLMs the\ncritical reasoning capabilities to outperform existing methods. In this paper\nwe analyze the potential causes of these shortcomings and identify key areas\nfor improvement such as deepening the thought process and introducing agentic\ncapabilities. The insights of this research are aimed at informing future\ndirections for refining the application of ToT and better harnessing the\npotential of LLMs in real-world problem-solving scenarios.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 2 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.13057v1",
    "published_date": "2024-05-20 11:05:56 UTC",
    "updated_date": "2024-05-20 11:05:56 UTC"
  },
  {
    "arxiv_id": "2405.11937v1",
    "title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation",
    "authors": [
      "Kamil Guttmann",
      "Miko≈Çaj Pokrywka",
      "Adrian Charkiewicz",
      "Artur Nowakowski"
    ],
    "abstract": "This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EAMT 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11937v1",
    "published_date": "2024-05-20 10:25:03 UTC",
    "updated_date": "2024-05-20 10:25:03 UTC"
  },
  {
    "arxiv_id": "2405.11928v3",
    "title": "\"Set It Up!\": Functional Object Arrangement with Compositional Generative Models",
    "authors": [
      "Yiqing Xu",
      "Jiayuan Mao",
      "Yilun Du",
      "Tomas Loz√°no-P√©rez",
      "Leslie Pack Kaelbling",
      "David Hsu"
    ],
    "abstract": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages main paper, 21 pages appendix, RSS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11928v3",
    "published_date": "2024-05-20 10:06:33 UTC",
    "updated_date": "2025-05-09 15:43:07 UTC"
  },
  {
    "arxiv_id": "2405.11919v2",
    "title": "On Efficient and Statistical Quality Estimation for Data Annotation",
    "authors": [
      "Jan-Christoph Klie",
      "Juan Haladjian",
      "Marc Kirchner",
      "Rahul Nair"
    ],
    "abstract": "Annotated datasets are an essential ingredient to train, evaluate, compare\nand productionalize supervised machine learning models. It is therefore\nimperative that annotations are of high quality. For their creation, good\nquality management and thereby reliable quality estimates are needed. Then, if\nquality is insufficient during the annotation process, rectifying measures can\nbe taken to improve it. Quality estimation is often performed by having experts\nmanually label instances as correct or incorrect. But checking all annotated\ninstances tends to be expensive. Therefore, in practice, usually only subsets\nare inspected; sizes are chosen mostly without justification or regard to\nstatistical power and more often than not, are relatively small. Basing\nestimates on small sample sizes, however, can lead to imprecise values for the\nerror rate. Using unnecessarily large sample sizes costs money that could be\nbetter spent, for instance on more annotations. Therefore, we first describe in\ndetail how to use confidence intervals for finding the minimal sample size\nneeded to estimate the annotation error rate. Then, we propose applying\nacceptance sampling as an alternative to error rate estimation We show that\nacceptance sampling can reduce the required sample sizes up to 50% while\nproviding the same statistical guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11919v2",
    "published_date": "2024-05-20 09:57:29 UTC",
    "updated_date": "2024-05-29 06:43:37 UTC"
  },
  {
    "arxiv_id": "2405.11911v2",
    "title": "Accurate Link Prediction for Edge-Incomplete Graphs via PU Learning",
    "authors": [
      "Junghun Kim",
      "Ka Hyun Park",
      "Hoyoung Yoon",
      "U Kang"
    ],
    "abstract": "Given an edge-incomplete graph, how can we accurately find the missing links?\nThe link prediction in edge-incomplete graphs aims to discover the missing\nrelations between entities when their relationships are represented as a graph.\nEdge-incomplete graphs are prevalent in real-world due to practical\nlimitations, such as not checking all users when adding friends in a social\nnetwork. Addressing the problem is crucial for various tasks, including\nrecommending friends in social networks and finding references in citation\nnetworks. However, previous approaches rely heavily on the given\nedge-incomplete (observed) graph, making it challenging to consider the missing\n(unobserved) links during training. In this paper, we propose PULL\n(PU-Learning-based Link predictor), an accurate link prediction method based on\nthe positive-unlabeled (PU) learning. PULL treats the observed edges in the\ntraining graph as positive examples, and the unconnected node pairs as\nunlabeled ones. PULL effectively prevents the link predictor from overfitting\nto the observed graph by proposing latent variables for every edge, and\nleveraging the expected graph structure with respect to the variables.\nExtensive experiments on five real-world datasets show that PULL consistently\noutperforms the baselines for predicting links in edge-incomplete graphs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI'25",
    "pdf_url": "http://arxiv.org/pdf/2405.11911v2",
    "published_date": "2024-05-20 09:47:22 UTC",
    "updated_date": "2024-12-12 07:17:19 UTC"
  },
  {
    "arxiv_id": "2406.00012v2",
    "title": "FINED: Feed Instance-Wise Information Need with Essential and Disentangled Parametric Knowledge from the Past",
    "authors": [
      "Kounianhua Du",
      "Jizheng Chen",
      "Jianghao Lin",
      "Menghui Zhu",
      "Bo Chen",
      "Shuai Li",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Recommender models play a vital role in various industrial scenarios, while\noften faced with the catastrophic forgetting problem caused by the fast\nshifting data distribution. To alleviate this problem, a common approach is to\nreuse knowledge from the historical data. However, preserving the vast and\nfast-accumulating data is hard, which causes dramatic storage overhead.\nMemorizing old data through a parametric knowledge base is then proposed, which\ncompresses the vast amount of raw data into model parameters. Despite the\nflexibility, how to improve the memorization and generalization capabilities of\nthe parametric knowledge base and suit the flexible information need of each\ninstance are challenging. In this paper, we propose FINED to Feed INstance-wise\ninformation need with Essential and Disentangled parametric knowledge from past\ndata for recommendation enhancement. Concretely, we train a knowledge extractor\nthat extracts knowledge patterns of arbitrary order from past data and a\nknowledge encoder that memorizes the arbitrary order patterns, which serves as\nthe retrieval key generator and memory network respectively in the following\nknowledge reusing phase. The whole process is regularized by the proposed two\nconstraints, which improve the capabilities of the parametric knowledge base\nwithout increasing the size of it. The essential principle helps to compress\nthe input into representative vectors that capture the task-relevant\ninformation and filter out the noisy information. The disentanglement principle\nreduces the redundancy of stored information and pushes the knowledge base to\nfocus on capturing the disentangled invariant patterns. These two rules\ntogether promote rational compression of information for robust and generalized\nknowledge representations. Extensive experiments on two datasets justify the\neffectiveness of the proposed method.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00012v2",
    "published_date": "2024-05-20 09:24:45 UTC",
    "updated_date": "2024-10-18 06:07:06 UTC"
  },
  {
    "arxiv_id": "2405.11891v1",
    "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
    "authors": [
      "Zijian Feng",
      "Hanzhang Zhou",
      "Zixiao Zhu",
      "Junlang Qian",
      "Kezhi Mao"
    ],
    "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models\n(LLMs). However, the intricate role of individual tokens in prompts, known as\ninput saliency, in shaping the responses remains largely underexplored.\nExisting saliency methods either misalign with LLM generation objectives or\nrely heavily on linearity assumptions, leading to potential inaccuracies. To\naddress this, we propose Token Distribution Dynamics (TDD), a\n\\textcolor{black}{simple yet effective} approach to unveil and manipulate the\nrole of prompts in generating LLM outputs. TDD leverages the robust\ninterpreting capabilities of the language model head (LM head) to assess input\nsaliency. It projects input tokens into the embedding space and then estimates\ntheir significance based on distribution dynamics over the vocabulary. We\nintroduce three TDD variants: forward, backward, and bidirectional, each\noffering unique insights into token relevance. Extensive experiments reveal\nthat the TDD surpasses state-of-the-art baselines with a big margin in\nelucidating the causal relationships between prompts and LLM outputs. Beyond\nmere interpretation, we apply TDD to two prompt manipulation tasks for\ncontrolled text generation: zero-shot toxic language suppression and sentiment\nsteering. Empirical results underscore TDD's proficiency in identifying both\ntoxic and sentimental cues in prompts, subsequently mitigating toxicity or\nmodulating sentiment in the generated content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11891v1",
    "published_date": "2024-05-20 09:15:36 UTC",
    "updated_date": "2024-05-20 09:15:36 UTC"
  },
  {
    "arxiv_id": "2405.11881v3",
    "title": "Out-of-Distribution Detection with a Single Unconditional Diffusion Model",
    "authors": [
      "Alvin Heng",
      "Alexandre H. Thiery",
      "Harold Soh"
    ],
    "abstract": "Out-of-distribution (OOD) detection is a critical task in machine learning\nthat seeks to identify abnormal samples. Traditionally, unsupervised methods\nutilize a deep generative model for OOD detection. However, such approaches\nrequire a new model to be trained for each inlier dataset. This paper explores\nwhether a single model can perform OOD detection across diverse tasks. To that\nend, we introduce Diffusion Paths (DiffPath), which uses a single diffusion\nmodel originally trained to perform unconditional generation for OOD detection.\nWe introduce a novel technique of measuring the rate-of-change and curvature of\nthe diffusion paths connecting samples to the standard normal. Extensive\nexperiments show that with a single model, DiffPath is competitive with prior\nwork using individual models on a variety of OOD tasks involving different\ndistributions. Our code is publicly available at\nhttps://github.com/clear-nus/diffpath.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11881v3",
    "published_date": "2024-05-20 08:54:03 UTC",
    "updated_date": "2024-10-24 02:17:00 UTC"
  },
  {
    "arxiv_id": "2405.11880v1",
    "title": "Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs",
    "authors": [
      "Siyu Lou",
      "Yuntian Chen",
      "Xiaodan Liang",
      "Liang Lin",
      "Quanshi Zhang"
    ],
    "abstract": "In this study, we propose an axiomatic system to define and quantify the\nprecise memorization and in-context reasoning effects used by the large\nlanguage model (LLM) for language generation. These effects are formulated as\nnon-linear interactions between tokens/words encoded by the LLM. Specifically,\nthe axiomatic system enables us to categorize the memorization effects into\nfoundational memorization effects and chaotic memorization effects, and further\nclassify in-context reasoning effects into enhanced inference patterns,\neliminated inference patterns, and reversed inference patterns. Besides, the\ndecomposed effects satisfy the sparsity property and the universal matching\nproperty, which mathematically guarantee that the LLM's confidence score can be\nfaithfully decomposed into the memorization effects and in-context reasoning\neffects. Experiments show that the clear disentanglement of memorization\neffects and in-context reasoning effects enables a straightforward examination\nof detailed inference patterns encoded by LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11880v1",
    "published_date": "2024-05-20 08:51:03 UTC",
    "updated_date": "2024-05-20 08:51:03 UTC"
  },
  {
    "arxiv_id": "2405.11877v5",
    "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus",
    "authors": [
      "Eduard Poesina",
      "Cornelia Caragea",
      "Radu Tudor Ionescu"
    ],
    "abstract": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2405.11877v5",
    "published_date": "2024-05-20 08:41:15 UTC",
    "updated_date": "2024-10-18 13:03:05 UTC"
  },
  {
    "arxiv_id": "2405.11870v2",
    "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process",
    "authors": [
      "Ermo Hua",
      "Biqing Qi",
      "Kaiyan Zhang",
      "Yue Yu",
      "Ning Ding",
      "Xingtai Lv",
      "Kai Tian",
      "Bowen Zhou"
    ],
    "abstract": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two\nfundamental processes for enhancing the capabilities of Language Models (LMs)\npost pre-training, aligning them better with human preferences. Although SFT\nadvances in training efficiency, PO delivers better alignment, thus they are\noften combined. However, common practices simply apply them sequentially\nwithout integrating their optimization objectives, ignoring the opportunities\nto bridge their paradigm gap and take the strengths from both. To obtain a\nunified understanding, we interpret SFT and PO with two sub-processes --\nPreference Estimation and Transition Optimization -- defined at token level\nwithin the Markov Decision Process (MDP) framework. This modeling shows that\nSFT is only a specialized case of PO with inferior estimation and optimization.\nPO evaluates the quality of model's entire generated answer, whereas SFT only\nscores predicted tokens based on preceding tokens from target answers.\nTherefore, SFT overestimates the ability of model, leading to inferior\noptimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT)\nto integrate SFT and Preference Optimization into a single process. IFT\ncaptures LMs' intuitive sense of the entire answers through a temporal residual\nconnection, but it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to sequential recipes of SFT and some typical\nPreference Optimization methods across several tasks, particularly those\nrequires generation, reasoning, and fact-following abilities. An explainable\nFrozen Lake game further validates the effectiveness of IFT for getting\ncompetitive policy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11870v2",
    "published_date": "2024-05-20 08:23:28 UTC",
    "updated_date": "2024-05-28 16:14:58 UTC"
  },
  {
    "arxiv_id": "2405.11868v1",
    "title": "Towards Graph Contrastive Learning: A Survey and Beyond",
    "authors": [
      "Wei Ju",
      "Yifan Wang",
      "Yifang Qin",
      "Zhengyang Mao",
      "Zhiping Xiao",
      "Junyu Luo",
      "Junwei Yang",
      "Yiyang Gu",
      "Dongjie Wang",
      "Qingqing Long",
      "Siyu Yi",
      "Xiao Luo",
      "Ming Zhang"
    ],
    "abstract": "In recent years, deep learning on graphs has achieved remarkable success in\nvarious domains. However, the reliance on annotated graph data remains a\nsignificant bottleneck due to its prohibitive cost and time-intensive nature.\nTo address this challenge, self-supervised learning (SSL) on graphs has gained\nincreasing attention and has made significant progress. SSL enables machine\nlearning models to produce informative representations from unlabeled graph\ndata, reducing the reliance on expensive labeled data. While SSL on graphs has\nwitnessed widespread adoption, one critical component, Graph Contrastive\nLearning (GCL), has not been thoroughly investigated in the existing\nliterature. Thus, this survey aims to fill this gap by offering a dedicated\nsurvey on GCL. We provide a comprehensive overview of the fundamental\nprinciples of GCL, including data augmentation strategies, contrastive modes,\nand contrastive optimization objectives. Furthermore, we explore the extensions\nof GCL to other aspects of data-efficient graph learning, such as weakly\nsupervised learning, transfer learning, and related scenarios. We also discuss\npractical applications spanning domains such as drug discovery, genomics\nanalysis, recommender systems, and finally outline the challenges and potential\nfuture directions in this field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11868v1",
    "published_date": "2024-05-20 08:19:10 UTC",
    "updated_date": "2024-05-20 08:19:10 UTC"
  },
  {
    "arxiv_id": "2405.11865v1",
    "title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English",
    "authors": [
      "Andrew Rueda",
      "Elena √Ålvarez Mellado",
      "Constantine Lignos"
    ],
    "abstract": "Modern named entity recognition systems have steadily improved performance in\nthe age of larger and more powerful neural models. However, over the past\nseveral years, the state-of-the-art has seemingly hit another plateau on the\nbenchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into\nthe test outputs of the highest-performing NER models, conducting a\nfine-grained evaluation of their performance by introducing new document-level\nannotations on the test set. We go beyond F1 scores by categorizing errors in\norder to interpret the true state of the art for NER and guide future work. We\nreview previous attempts at correcting the various flaws of the test set and\nintroduce CoNLL#, a new corrected version of the test set that addresses its\nsystematic and most prevalent errors, allowing for low-noise, interpretable\nerror analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11865v1",
    "published_date": "2024-05-20 08:16:34 UTC",
    "updated_date": "2024-05-20 08:16:34 UTC"
  },
  {
    "arxiv_id": "2406.00011v2",
    "title": "DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation",
    "authors": [
      "Kounianhua Du",
      "Jizheng Chen",
      "Jianghao Lin",
      "Yunjia Xi",
      "Hangyu Wang",
      "Xinyi Dai",
      "Bo Chen",
      "Ruiming Tang",
      "Weinan Zhang"
    ],
    "abstract": "Recommender systems play important roles in various applications such as\ne-commerce, social media, etc. Conventional recommendation methods usually\nmodel the collaborative signals within the tabular representation space.\nDespite the personalization modeling and the efficiency, the latent semantic\ndependencies are omitted. Methods that introduce semantics into recommendation\nthen emerge, injecting knowledge from the semantic representation space where\nthe general language understanding are compressed. However, existing\nsemantic-enhanced recommendation methods focus on aligning the two spaces,\nduring which the representations of the two spaces tend to get close while the\nunique patterns are discarded and not well explored. In this paper, we propose\nDisCo to Disentangle the unique patterns from the two representation spaces and\nCollaborate the two spaces for recommendation enhancement, where both the\nspecificity and the consistency of the two spaces are captured. Concretely, we\npropose 1) a dual-side attentive network to capture the intra-domain patterns\nand the inter-domain patterns, 2) a sufficiency constraint to preserve the\ntask-relevant information of each representation space and filter out the\nnoise, and 3) a disentanglement constraint to avoid the model from discarding\nthe unique information. These modules strike a balance between disentanglement\nand collaboration of the two representation spaces to produce informative\npattern vectors, which could serve as extra features and be appended to\narbitrary recommendation backbones for enhancement. Experiment results validate\nthe superiority of our method against different models and the compatibility of\nDisCo over different backbones. Various ablation studies and efficiency\nanalysis are also conducted to justify each model component.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00011v2",
    "published_date": "2024-05-20 08:07:27 UTC",
    "updated_date": "2024-06-04 07:17:46 UTC"
  },
  {
    "arxiv_id": "2405.11848v2",
    "title": "Alternators For Sequence Modeling",
    "authors": [
      "Mohammad Reza Rezaei",
      "Adji Bousso Dieng"
    ],
    "abstract": "This paper introduces alternators, a novel family of non-Markovian dynamical\nmodels for sequences. An alternator features two neural networks: the\nobservation trajectory network (OTN) and the feature trajectory network (FTN).\nThe OTN and the FTN work in conjunction, alternating between outputting samples\nin the observation space and some feature space, respectively, over a cycle.\nThe parameters of the OTN and the FTN are not time-dependent and are learned\nvia a minimum cross-entropy criterion over the trajectories. Alternators are\nversatile. They can be used as dynamical latent-variable generative models or\nas sequence-to-sequence predictors. Alternators can uncover the latent dynamics\nunderlying complex sequential data, accurately forecast and impute missing\ndata, and sample new trajectories. We showcase the capabilities of alternators\nin three applications. We first used alternators to model the Lorenz equations,\noften used to describe chaotic behavior. We then applied alternators to\nNeuroscience, to map brain activity to physical activity. Finally, we applied\nalternators to Climate Science, focusing on sea-surface temperature\nforecasting. In all our experiments, we found alternators are stable to train,\nfast to sample from, yield high-quality generated samples and latent variables,\nand often outperform strong baselines such as Mambas, neural ODEs, and\ndiffusion models in the domains we studied.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "physics.ao-ph",
      "q-bio.NC"
    ],
    "primary_category": "stat.ML",
    "comment": "A new versatile family of sequence models that can be used for both\n  generative modeling and supervised learning. The codebase will be made\n  available upon publication. This paper is dedicated to Thomas Sankara",
    "pdf_url": "http://arxiv.org/pdf/2405.11848v2",
    "published_date": "2024-05-20 07:47:06 UTC",
    "updated_date": "2024-12-01 00:49:32 UTC"
  },
  {
    "arxiv_id": "2405.11841v1",
    "title": "Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities",
    "authors": [
      "Junqi Wang",
      "Chunhui Zhang",
      "Jiapeng Li",
      "Yuxi Ma",
      "Lixing Niu",
      "Jiaheng Han",
      "Yujia Peng",
      "Yixin Zhu",
      "Lifeng Fan"
    ],
    "abstract": "Facing the current debate on whether Large Language Models (LLMs) attain\nnear-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023;\nKosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study\nintroduces a benchmark for evaluating social intelligence, one of the most\ndistinctive aspects of human cognition. We developed a comprehensive\ntheoretical framework for social dynamics and introduced two evaluation tasks:\nInverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also\nencompassed a computational model based on recursive Bayesian inference, adept\nat elucidating diverse human behavioral patterns. Extensive experiments and\ndetailed analyses revealed that humans surpassed the latest GPT models in\noverall performance, zero-shot learning, one-shot generalization, and\nadaptability to multi-modalities. Notably, GPT models demonstrated social\nintelligence only at the most basic order (order = 0), in stark contrast to\nhuman social intelligence (order >= 2). Further examination indicated a\npropensity of LLMs to rely on pattern recognition for shortcuts, casting doubt\non their possession of authentic human-level social intelligence. Our codes,\ndataset, appendix and human data are released at\nhttps://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Also published in Proceedings of the Annual Meeting of the Cognitive\n  Science Society (CogSci), 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11841v1",
    "published_date": "2024-05-20 07:34:48 UTC",
    "updated_date": "2024-05-20 07:34:48 UTC"
  },
  {
    "arxiv_id": "2405.11837v2",
    "title": "Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model",
    "authors": [
      "Mounes Zaval",
      "Sedat Ozer"
    ],
    "abstract": "In the evolving field of Explainable AI (XAI), interpreting the decisions of\ndeep neural networks (DNNs) in computer vision tasks is an important process.\nWhile pixel-based XAI methods focus on identifying significant pixels, existing\nconcept-based XAI methods use pre-defined or human-annotated concepts. The\nrecently proposed Segment Anything Model (SAM) achieved a significant step\nforward to prepare automatic concept sets via comprehensive instance\nsegmentation. Building upon this, the Explain Any Concept (EAC) model emerged\nas a flexible method for explaining DNN decisions. EAC model is based on using\na surrogate model which has one trainable linear layer to simulate the target\nmodel. In this paper, by introducing an additional nonlinear layer to the\noriginal surrogate model, we show that we can improve the performance of the\nEAC model. We compare our proposed approach to the original EAC model and\nreport improvements obtained on both ImageNet and MS COCO datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted for publication at IEEE SIU conference, 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11837v2",
    "published_date": "2024-05-20 07:25:09 UTC",
    "updated_date": "2024-06-24 19:28:08 UTC"
  },
  {
    "arxiv_id": "2406.04359v1",
    "title": "Naming the Pain in Machine Learning-Enabled Systems Engineering",
    "authors": [
      "Marcos Kalinowski",
      "Daniel Mendez",
      "G√∂rkem Giray",
      "Antonio Pedro Santos Alves",
      "Kelly Azevedo",
      "Tatiana Escovedo",
      "Hugo Villamizar",
      "Helio Lopes",
      "Teresa Baldassarre",
      "Stefan Wagner",
      "Stefan Biffl",
      "J√ºrgen Musil",
      "Michael Felderer",
      "Niklas Lavesson",
      "Tony Gorschek"
    ],
    "abstract": "Context: Machine learning (ML)-enabled systems are being increasingly adopted\nby companies aiming to enhance their products and operational processes.\nObjective: This paper aims to deliver a comprehensive overview of the current\nstatus quo of engineering ML-enabled systems and lay the foundation to steer\npractically relevant and problem-driven academic research. Method: We conducted\nan international survey to collect insights from practitioners on the current\npractices and problems in engineering ML-enabled systems. We received 188\ncomplete responses from 25 countries. We conducted quantitative statistical\nanalyses on contemporary practices using bootstrapping with confidence\nintervals and qualitative analyses on the reported problems using open and\naxial coding procedures. Results: Our survey results reinforce and extend\nexisting empirical evidence on engineering ML-enabled systems, providing\nadditional insights into typical ML-enabled systems project contexts, the\nperceived relevance and complexity of ML life cycle phases, and current\npractices related to problem understanding, model deployment, and model\nmonitoring. Furthermore, the qualitative analysis provides a detailed map of\nthe problems practitioners face within each ML life cycle phase and the\nproblems causing overall project failure. Conclusions: The results contribute\nto a better understanding of the status quo and problems in practical\nenvironments. We advocate for the further adaptation and dissemination of\nsoftware engineering practices to enhance the engineering of ML-enabled\nsystems.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "arXiv admin note: text overlap with arXiv:2310.06726",
    "pdf_url": "http://arxiv.org/pdf/2406.04359v1",
    "published_date": "2024-05-20 06:59:20 UTC",
    "updated_date": "2024-05-20 06:59:20 UTC"
  },
  {
    "arxiv_id": "2405.11821v1",
    "title": "A Three-Phase Analysis of Synergistic Effects During Co-pyrolysis of Algae and Wood for Biochar Yield Using Machine Learning",
    "authors": [
      "Subhadeep Chakrabarti",
      "Saish Shinde"
    ],
    "abstract": "Pyrolysis techniques have served to be a groundbreaking technique for\neffectively utilising natural and man-made biomass products like plastics,\nwood, crop residue, fruit peels etc. Recent advancements have shown a greater\nyield of essential products like biochar, bio-oil and other non-condensable\ngases by blending different biomasses in a certain ratio. This synergy effect\nof combining two pyrolytic raw materials i.e co-pyrolysis of algae and wood\nbiomass has been systematically studied and grouped into 3 phases in this\nresearch paper-kinetic analysis of co-pyrolysis, correlation among proximate\nand ultimate analysis with bio-char yield and lastly grouping of different\nweight ratios based on biochar yield up to a certain percentage. Different ML\nand DL algorithms have been utilized for regression and classification\ntechniques to give a comprehensive overview of the effect of the synergy of two\ndifferent biomass materials on biochar yield. For the first phase, the best\nprediction of biochar yield was obtained by using a decision tree regressor\nwith a perfect MSE score of 0.00, followed by a gradient-boosting regressor.\nThe second phase was analyzed using both ML and DL techniques. Within ML, SVR\nproved to be the most convenient model with an accuracy score of 0.972 with DNN\nemployed for deep learning technique. Finally, for the third phase, binary\nclassification was applied to biochar yield with and without heating rate for\nbiochar yield percentage above and below 40%. The best technique for ML was\nSupport Vector followed by Random forest while ANN was the most suitable Deep\nLearning Technique.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.11821v1",
    "published_date": "2024-05-20 06:32:57 UTC",
    "updated_date": "2024-05-20 06:32:57 UTC"
  },
  {
    "arxiv_id": "2405.11816v1",
    "title": "Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning",
    "authors": [
      "Anastasios Foliadis",
      "Mario H. Casta√±eda",
      "Richard A. Stirling-Gallacher",
      "Reiner S. Thom√§"
    ],
    "abstract": "Utilizing deep learning (DL) techniques for radio-based positioning of user\nequipment (UE) through channel state information (CSI) fingerprints has\ndemonstrated significant potential. DL models can extract complex\ncharacteristics from the CSI fingerprints of a particular environment and\naccurately predict the position of a UE. Nonetheless, the effectiveness of the\nDL model trained on CSI fingerprints is highly dependent on the particular\ntraining environment, limiting the trained model's applicability across\ndifferent environments. This paper proposes a novel DL model structure\nconsisting of two parts, where the first part aims at identifying features that\nare independent from any specific environment, while the second part combines\nthose features in an environment specific way with the goal of positioning. To\ntrain such a two-part model, we propose the multi-environment meta-learning\n(MEML) approach for the first part to facilitate training across various\nenvironments, while the second part of the model is trained solely on data from\na specific environment. Our findings indicate that employing the MEML approach\nfor initializing the weights of the DL model for a new unseen environment\nsignificantly boosts the accuracy of UE positioning in the new target\nenvironment as well the reliability of its uncertainty estimation. This method\noutperforms traditional transfer learning methods, whether direct transfer\nlearning (DTL) between environments or completely training from scratch with\ndata from a new environment. The proposed approach is verified with real\nmeasurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Submitted for IEEE journal publication. 10 pages, 13 figures, 7\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2405.11816v1",
    "published_date": "2024-05-20 06:23:22 UTC",
    "updated_date": "2024-05-20 06:23:22 UTC"
  },
  {
    "arxiv_id": "2405.11814v1",
    "title": "Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling",
    "authors": [
      "Masato Sakai",
      "Marcus Freitag",
      "Akihisa Sakurai",
      "Conrad M Albrecht",
      "Hendrik F Hamann"
    ],
    "abstract": "Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru\nis urgent as natural and human impact accelerates. More frequent weather\nextremes such as flashfloods threaten Nasca artifacts. We demonstrate that\nrunoff models based on (sub-)meter scale, LiDAR-derived digital elevation data\ncan highlight AI-detected geoglyphs that are in danger of erosion. We recommend\nmeasures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\"\ngeoglyphs located close by, or even cut by the Pan-American Highway.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted at IGARSS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11814v1",
    "published_date": "2024-05-20 06:21:15 UTC",
    "updated_date": "2024-05-20 06:21:15 UTC"
  },
  {
    "arxiv_id": "2405.11809v1",
    "title": "Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices",
    "authors": [
      "Baiyu Pan",
      "Jichao Jiao",
      "Jianxing Pang",
      "Jun Cheng"
    ],
    "abstract": "In recent years, numerous real-time stereo matching methods have been\nintroduced, but they often lack accuracy. These methods attempt to improve\naccuracy by introducing new modules or integrating traditional methods.\nHowever, the improvements are only modest. In this paper, we propose a novel\nstrategy by incorporating knowledge distillation and model pruning to overcome\nthe inherent trade-off between speed and accuracy. As a result, we obtained a\nmodel that maintains real-time performance while delivering high accuracy on\nedge devices. Our proposed method involves three key steps. Firstly, we review\nstate-of-the-art methods and design our lightweight model by removing redundant\nmodules from those efficient models through a comparison of their\ncontributions. Next, we leverage the efficient model as the teacher to distill\nknowledge into the lightweight model. Finally, we systematically prune the\nlightweight model to obtain the final model. Through extensive experiments\nconducted on two widely-used benchmarks, Sceneflow and KITTI, we perform\nablation studies to analyze the effectiveness of each module and present our\nstate-of-the-art results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "International Conference on Robotics and Automation (ICRA) 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11809v1",
    "published_date": "2024-05-20 06:03:55 UTC",
    "updated_date": "2024-05-20 06:03:55 UTC"
  },
  {
    "arxiv_id": "2405.11802v1",
    "title": "Counterfactual Explanation-Based Badminton Motion Guidance Generation Using Wearable Sensors",
    "authors": [
      "Minwoo Seong",
      "Gwangbin Kim",
      "Yumin Kang",
      "Junhyuk Jang",
      "Joseph DelPreto",
      "SeungJun Kim"
    ],
    "abstract": "This study proposes a framework for enhancing the stroke quality of badminton\nplayers by generating personalized motion guides, utilizing a multimodal\nwearable dataset. These guides are based on counterfactual algorithms and aim\nto reduce the performance gap between novice and expert players. Our approach\nprovides joint-level guidance through visualizable data to assist players in\nimproving their movements without requiring expert knowledge. The method was\nevaluated against a traditional algorithm using metrics to assess validity,\nproximity, and plausibility, including arithmetic measures and motion-specific\nevaluation metrics. Our evaluation demonstrates that the proposed framework can\ngenerate motions that maintain the essence of original movements while\nenhancing stroke quality, providing closer guidance than direct expert motion\nreplication. The results highlight the potential of our approach for creating\npersonalized sports motion guides by generating counterfactual motion guidance\nfor arbitrary input motion samples of badminton strokes.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "ICRA Wearable Workshop 2024 - 1st Workshop on Advancing Wearable\n  Devices and Applications through Novel Design, Sensing, Actuation, and AI",
    "pdf_url": "http://arxiv.org/pdf/2405.11802v1",
    "published_date": "2024-05-20 05:48:20 UTC",
    "updated_date": "2024-05-20 05:48:20 UTC"
  },
  {
    "arxiv_id": "2405.11800v1",
    "title": "Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines",
    "authors": [
      "Yueqiao Jin",
      "Lixiang Yan",
      "Vanessa Echeverria",
      "Dragan Ga≈°eviƒá",
      "Roberto Martinez-Maldonado"
    ],
    "abstract": "Integrating generative AI (GAI) into higher education is crucial for\npreparing a future generation of GAI-literate students. Yet a thorough\nunderstanding of the global institutional adoption policy remains absent, with\nmost of the prior studies focused on the Global North and the promises and\nchallenges of GAI, lacking a theoretical lens. This study utilizes the\nDiffusion of Innovations Theory to examine GAI adoption strategies in higher\neducation across 40 universities from six global regions. It explores the\ncharacteristics of GAI innovation, including compatibility, trialability, and\nobservability, and analyses the communication channels and roles and\nresponsibilities outlined in university policies and guidelines. The findings\nreveal a proactive approach by universities towards GAI integration,\nemphasizing academic integrity, teaching and learning enhancement, and equity.\nDespite a cautious yet optimistic stance, a comprehensive policy framework is\nneeded to evaluate the impacts of GAI integration and establish effective\ncommunication strategies that foster broader stakeholder engagement. The study\nhighlights the importance of clear roles and responsibilities among faculty,\nstudents, and administrators for successful GAI integration, supporting a\ncollaborative model for navigating the complexities of GAI in education. This\nstudy contributes insights for policymakers in crafting detailed strategies for\nits integration.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11800v1",
    "published_date": "2024-05-20 05:46:38 UTC",
    "updated_date": "2024-05-20 05:46:38 UTC"
  },
  {
    "arxiv_id": "2405.11784v1",
    "title": "Reward-Punishment Reinforcement Learning with Maximum Entropy",
    "authors": [
      "Jiexin Wang",
      "Eiji Uchibe"
    ],
    "abstract": "We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates\nthe optimization of long-term policy entropy into reward-punishment\nreinforcement learning objectives. Our motivation is to facilitate a smoother\nvariation of operators utilized in the updating of action values beyond\ntraditional ``max'' and ``min'' operators, where the goal is enhancing sample\nefficiency and robustness. We also address two unresolved issues from the\nprevious Deep MaxPain method. Firstly, we investigate how the negated\n(``flipped'') pain-seeking sub-policy, derived from the punishment action\nvalue, collaborates with the ``min'' operator to effectively learn the\npunishment module and how softDMP's smooth learning operator provides insights\ninto the ``flipping'' trick. Secondly, we tackle the challenge of data\ncollection for learning the punishment module to mitigate inconsistencies\narising from the involvement of the ``flipped'' sub-policy (pain-avoidance\nsub-policy) in the unified behavior policy. We empirically explore the first\nissue in two discrete Markov Decision Process (MDP) environments, elucidating\nthe crucial advancements of the DMP approach and the necessity for soft\ntreatments on the hard operators. For the second issue, we propose a\nprobabilistic classifier based on the ratio of the pain-seeking sub-policy to\nthe sum of the pain-seeking and goal-reaching sub-policies. This classifier\nassigns roll-outs to separate replay buffers for updating reward and punishment\naction-value functions, respectively. Our framework demonstrates superior\nperformance in Turtlebot 3's maze navigation tasks under the ROS Gazebo\nsimulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "IJCNN2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11784v1",
    "published_date": "2024-05-20 05:05:14 UTC",
    "updated_date": "2024-05-20 05:05:14 UTC"
  },
  {
    "arxiv_id": "2405.11783v1",
    "title": "Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing",
    "authors": [
      "Shinyoung Kang",
      "Jihan Kim"
    ],
    "abstract": "In this study, we explore the potential of using quantum natural language\nprocessing (QNLP) to inverse design metal-organic frameworks (MOFs) with\ntargeted properties. Specifically, by analyzing 150 hypothetical MOF structures\nconsisting of 10 metal nodes and 15 organic ligands, we categorize these\nstructures into four distinct classes for pore volume and $H_{2}$ uptake\nvalues. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat\n(Distributional Compositional Categorical), and sequence-based models) to\nidentify the most effective approach to process the MOF dataset. Using a\nclassical simulator provided by the IBM Qiskit, the bag-of-words model is\nidentified to be the optimum model, achieving validation accuracies of 85.7%\nand 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake,\nrespectively. Further, we developed multi-class classification models tailored\nto the probabilistic nature of quantum circuits, with average test accuracies\nof 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake\ndatasets. Finally, the performance of generating MOF with target properties\nshowed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake,\nrespectively. Although our investigation covers only a fraction of the vast MOF\nsearch space, it marks a promising first step towards using quantum computing\nfor materials design, offering a new perspective through which to explore the\ncomplex landscape of MOFs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "45 pages, 7 figures, 6 supplementary figures, 1 table, 1\n  supplementary table",
    "pdf_url": "http://arxiv.org/pdf/2405.11783v1",
    "published_date": "2024-05-20 05:02:12 UTC",
    "updated_date": "2024-05-20 05:02:12 UTC"
  },
  {
    "arxiv_id": "2405.11778v1",
    "title": "Efficient Multi-agent Reinforcement Learning by Planning",
    "authors": [
      "Qihan Liu",
      "Jianing Ye",
      "Xiaoteng Ma",
      "Jun Yang",
      "Bin Liang",
      "Chongjie Zhang"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) algorithms have accomplished\nremarkable breakthroughs in solving large-scale decision-making tasks.\nNonetheless, most existing MARL algorithms are model-free, limiting sample\nefficiency and hindering their applicability in more challenging scenarios. In\ncontrast, model-based reinforcement learning (MBRL), particularly algorithms\nintegrating planning, such as MuZero, has demonstrated superhuman performance\nwith limited data in many tasks. Hence, we aim to boost the sample efficiency\nof MARL by adopting model-based approaches. However, incorporating planning and\nsearch methods into multi-agent systems poses significant challenges. The\nexpansive action space of multi-agent systems often necessitates leveraging the\nnearly-independent property of agents to accelerate learning. To tackle this\nissue, we propose the MAZero algorithm, which combines a centralized model with\nMonte Carlo Tree Search (MCTS) for policy search. We design a novel network\nstructure to facilitate distributed execution and parameter sharing. To enhance\nsearch efficiency in deterministic environments with sizable action spaces, we\nintroduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and\nAdvantage-Weighted Policy Optimization (AWPO). Extensive experiments on the\nSMAC benchmark demonstrate that MAZero outperforms model-free approaches in\nterms of sample efficiency and provides comparable or better performance than\nexisting model-based methods in terms of both sample and computational\nefficiency. Our code is available at https://github.com/liuqh16/MAZero.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2405.11778v1",
    "published_date": "2024-05-20 04:36:02 UTC",
    "updated_date": "2024-05-20 04:36:02 UTC"
  },
  {
    "arxiv_id": "2405.11766v1",
    "title": "From SHAP Scores to Feature Importance Scores",
    "authors": [
      "Olivier Letoffe",
      "Xuanxiang Huang",
      "Nicholas Asher",
      "Joao Marques-Silva"
    ],
    "abstract": "A central goal of eXplainable Artificial Intelligence (XAI) is to assign\nrelative importance to the features of a Machine Learning (ML) model given some\nprediction. The importance of this task of explainability by feature\nattribution is illustrated by the ubiquitous recent use of tools such as SHAP\nand LIME. Unfortunately, the exact computation of feature attributions, using\nthe game-theoretical foundation underlying SHAP and LIME, can yield manifestly\nunsatisfactory results, that tantamount to reporting misleading relative\nfeature importance. Recent work targeted rigorous feature attribution, by\nstudying axiomatic aggregations of features based on logic-based definitions of\nexplanations by feature selection. This paper shows that there is an essential\nrelationship between feature attribution and a priori voting power, and that\nthose recently proposed axiomatic aggregations represent a few instantiations\nof the range of power indices studied in the past. Furthermore, it remains\nunclear how some of the most widely used power indices might be exploited as\nfeature importance scores (FISs), i.e. the use of power indices in XAI, and\nwhich of these indices would be the best suited for the purposes of XAI by\nfeature attribution, namely in terms of not producing results that could be\ndeemed as unsatisfactory. This paper proposes novel desirable properties that\nFISs should exhibit. In addition, the paper also proposes novel FISs exhibiting\nthe proposed properties. Finally, the paper conducts a rigorous analysis of the\nbest-known power indices in terms of the proposed properties.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11766v1",
    "published_date": "2024-05-20 03:52:41 UTC",
    "updated_date": "2024-05-20 03:52:41 UTC"
  },
  {
    "arxiv_id": "2405.11758v1",
    "title": "Fed-Credit: Robust Federated Learning with Credibility Management",
    "authors": [
      "Jiayan Chen",
      "Zhirong Qian",
      "Tianhui Meng",
      "Xitong Gao",
      "Tian Wang",
      "Weijia Jia"
    ],
    "abstract": "Aiming at privacy preservation, Federated Learning (FL) is an emerging\nmachine learning approach enabling model training on decentralized devices or\ndata sources. The learning mechanism of FL relies on aggregating parameter\nupdates from individual clients. However, this process may pose a potential\nsecurity risk due to the presence of malicious devices. Existing solutions are\neither costly due to the use of compute-intensive technology, or restrictive\nfor reasons of strong assumptions such as the prior knowledge of the number of\nattackers and how they attack. Few methods consider both privacy constraints\nand uncertain attack scenarios. In this paper, we propose a robust FL approach\nbased on the credibility management scheme, called Fed-Credit. Unlike previous\nstudies, our approach does not require prior knowledge of the nodes and the\ndata distribution. It maintains and employs a credibility set, which weighs the\nhistorical clients' contributions based on the similarity between the local\nmodels and global model, to adjust the global model update. The subtlety of\nFed-Credit is that the time decay and attitudinal value factor are incorporated\ninto the dynamic adjustment of the reputation weights and it boasts a\ncomputational complexity of O(n) (n is the number of the clients). We conducted\nextensive experiments on the MNIST and CIFAR-10 datasets under 5 types of\nattacks. The results exhibit superior accuracy and resilience against\nadversarial attacks, all while maintaining comparatively low computational\ncomplexity. Among these, on the Non-IID CIFAR-10 dataset, our algorithm\nexhibited performance enhancements of 19.5% and 14.5%, respectively, in\ncomparison to the state-of-the-art algorithm when dealing with two types of\ndata poisoning attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11758v1",
    "published_date": "2024-05-20 03:35:13 UTC",
    "updated_date": "2024-05-20 03:35:13 UTC"
  },
  {
    "arxiv_id": "2405.11746v1",
    "title": "Configurable Mirror Descent: Towards a Unification of Decision Making",
    "authors": [
      "Pengdeng Li",
      "Shuxin Li",
      "Chang Yang",
      "Xinrun Wang",
      "Shuyue Hu",
      "Xiao Huang",
      "Hau Chan",
      "Bo An"
    ],
    "abstract": "Decision-making problems, categorized as single-agent, e.g., Atari,\ncooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em\npoker, and mixed cooperative and competitive, e.g., football, are ubiquitous in\nthe real world. Various methods are proposed to address the specific\ndecision-making problems. Despite the successes in specific categories, these\nmethods typically evolve independently and cannot generalize to other\ncategories. Therefore, a fundamental question for decision-making is: \\emph{Can\nwe develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of\ndecision-making problems?} There are several main challenges to address this\nquestion: i) different decision-making categories involve different numbers of\nagents and different relationships between agents, ii) different categories\nhave different solution concepts and evaluation measures, and iii) there lacks\na comprehensive benchmark covering all the categories. This work presents a\npreliminary attempt to address the question with three main contributions. i)\nWe propose the generalized mirror descent (GMD), a generalization of MD\nvariants, which considers multiple historical policies and works with a broader\nclass of Bregman divergences. ii) We propose the configurable mirror descent\n(CMD) where a meta-controller is introduced to dynamically adjust the\nhyper-parameters in GMD conditional on the evaluation measures. iii) We\nconstruct the \\textsc{GameBench} with 15 academic-friendly games across\ndifferent decision-making categories. Extensive experiments demonstrate that\nCMD achieves empirically competitive or better outcomes compared to baselines\nwhile providing the capability of exploring diverse dimensions of decision\nmaking.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to The Forty-first International Conference on Machine\n  Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.11746v1",
    "published_date": "2024-05-20 03:10:22 UTC",
    "updated_date": "2024-05-20 03:10:22 UTC"
  },
  {
    "arxiv_id": "2405.11740v1",
    "title": "Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning",
    "authors": [
      "Xin Liu",
      "Yaran Chen",
      "Dongbin Zhao"
    ],
    "abstract": "In visual Reinforcement Learning (RL), upstream representation learning\nlargely determines the effect of downstream policy learning. Employing\nauxiliary tasks allows the agent to enhance visual representation in a targeted\nmanner, thereby improving the sample efficiency and performance of downstream\nRL. Prior advanced auxiliary tasks all focus on how to extract as much\ninformation as possible from limited experience (including observations,\nactions, and rewards) through their different auxiliary objectives, whereas in\nthis article, we first start from another perspective: auxiliary training data.\nWe try to improve auxiliary representation learning for RL by enriching\nauxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture\nrepresentation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel\nself-supervised RL approach. Specifically, we propose a training-free method to\nsynthesize observations that may contain future information, as well as a data\nselection approach to eliminate unqualified synthetic noise. The remaining\nsynthetic observations and real observations then serve as the auxiliary data\nto achieve a clustering-based temporal association task for representation\nlearning. LFS allows the agent to access and learn observations that have not\nyet appeared in advance, so as to quickly understand and exploit them when they\noccur later. In addition, LFS does not rely on rewards or actions, which means\nit has a wider scope of application (e.g., learning from video) than recent\nadvanced auxiliary tasks. Extensive experiments demonstrate that our LFS\nexhibits state-of-the-art RL sample efficiency on challenging continuous\ncontrol and enables advanced visual pre-training based on action-free video\ndemonstrations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11740v1",
    "published_date": "2024-05-20 02:43:04 UTC",
    "updated_date": "2024-05-20 02:43:04 UTC"
  },
  {
    "arxiv_id": "2405.11739v2",
    "title": "What Radio Waves Tell Us about Sleep",
    "authors": [
      "Hao He",
      "Chao Li",
      "Wolfgang Ganglberger",
      "Kaileigh Gallagher",
      "Rumen Hristov",
      "Michail Ouroutzoglou",
      "Haoqi Sun",
      "Jimeng Sun",
      "Brandon Westover",
      "Dina Katabi"
    ],
    "abstract": "The ability to assess sleep at home, capture sleep stages, and detect the\noccurrence of apnea (without on-body sensors) simply by analyzing the radio\nwaves bouncing off people's bodies while they sleep is quite powerful. Such a\ncapability would allow for longitudinal data collection in patients' homes,\ninforming our understanding of sleep and its interaction with various diseases\nand their therapeutic responses, both in clinical trials and routine care. In\nthis article, we develop an advanced machine learning algorithm for passively\nmonitoring sleep and nocturnal breathing from radio waves reflected off people\nwhile asleep. Validation results in comparison with the gold standard (i.e.,\npolysomnography) (n=849) demonstrate that the model captures the sleep\nhypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake,\nLight Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and\nmeasures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]).\nNotably, the model exhibits equitable performance across race, sex, and age.\nMoreover, the model uncovers informative interactions between sleep stages and\na range of diseases including neurological, psychiatric, cardiovascular, and\nimmunological disorders. These findings not only hold promise for clinical\npractice and interventional trials but also underscore the significance of\nsleep as a fundamental component in understanding and managing various\ndiseases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2405.11739v2",
    "published_date": "2024-05-20 02:41:21 UTC",
    "updated_date": "2024-07-20 19:38:37 UTC"
  },
  {
    "arxiv_id": "2405.13055v1",
    "title": "Large Language Models for Medicine: A Survey",
    "authors": [
      "Yanxin Zheng",
      "Wensheng Gan",
      "Zefeng Chen",
      "Zhenlian Qi",
      "Qian Liang",
      "Philip S. Yu"
    ],
    "abstract": "To address challenges in the digital economy's landscape of digital\nintelligence, large language models (LLMs) have been developed. Improvements in\ncomputational power and available resources have significantly advanced LLMs,\nallowing their integration into diverse domains for human life. Medical LLMs\nare essential application tools with potential across various medical\nscenarios. In this paper, we review LLM developments, focusing on the\nrequirements and applications of medical LLMs. We provide a concise overview of\nexisting models, aiming to explore advanced research directions and benefit\nresearchers for future medical applications. We emphasize the advantages of\nmedical LLMs in applications, as well as the challenges encountered during\ntheir development. Finally, we suggest directions for technical integration to\nmitigate challenges and potential research directions for the future of medical\nLLMs, aiming to meet the demands of the medical field better.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. 5 figures,5 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.13055v1",
    "published_date": "2024-05-20 02:32:26 UTC",
    "updated_date": "2024-05-20 02:32:26 UTC"
  },
  {
    "arxiv_id": "2405.12252v1",
    "title": "Enhanced Deterministic Approximation Algorithm for Non-monotone Submodular Maximization under Knapsack Constraint with Linear Query Complexity",
    "authors": [
      "Canh V. Pham"
    ],
    "abstract": "In this work, we consider the Submodular Maximization under Knapsack (SMK)\nconstraint problem over the ground set of size $n$. The problem recently\nattracted a lot of attention due to its applications in various domains of\ncombination optimization, artificial intelligence, and machine learning. We\nimprove the approximation factor of the fastest deterministic algorithm from\n$6+\\epsilon$ to $5+\\epsilon$ while keeping the best query complexity of $O(n)$,\nwhere $\\epsilon >0$ is a constant parameter. Our technique is based on\noptimizing the performance of two components: the threshold greedy subroutine\nand the building of two disjoint sets as candidate solutions. Besides, by\ncarefully analyzing the cost of candidate solutions, we obtain a tighter\napproximation factor.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.12252v1",
    "published_date": "2024-05-20 02:24:58 UTC",
    "updated_date": "2024-05-20 02:24:58 UTC"
  },
  {
    "arxiv_id": "2405.11724v2",
    "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
    "authors": [
      "Huawei Lin",
      "Jikai Long",
      "Zhaozhuo Xu",
      "Weijie Zhao"
    ],
    "abstract": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
    "pdf_url": "http://arxiv.org/pdf/2405.11724v2",
    "published_date": "2024-05-20 01:57:34 UTC",
    "updated_date": "2024-10-22 19:07:08 UTC"
  },
  {
    "arxiv_id": "2405.11722v1",
    "title": "AI Algorithm for Predicting and Optimizing Trajectory of UAV Swarm",
    "authors": [
      "Amit Raj",
      "Kapil Ahuja",
      "Yann Busnel"
    ],
    "abstract": "This paper explores the application of Artificial Intelligence (AI)\ntechniques for generating the trajectories of fleets of Unmanned Aerial\nVehicles (UAVs). The two main challenges addressed include accurately\npredicting the paths of UAVs and efficiently avoiding collisions between them.\nFirstly, the paper systematically applies a diverse set of activation functions\nto a Feedforward Neural Network (FFNN) with a single hidden layer, which\nenhances the accuracy of the predicted path compared to previous work.\n  Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which\nis a sophisticated fusion of Swish and Elliott activations, seamlessly\nintegrated with a scaled and shifted Gaussian component. Swish facilitates\nsmooth transitions, Elliott captures abrupt trajectory changes, and the scaled\nand shifted Gaussian enhances robustness against noise. This dynamic\ncombination is specifically designed to excel in capturing the complexities of\nUAV trajectory prediction. This new activation function gives substantially\nbetter accuracy than all existing activation functions.\n  Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and\nBatching (ICDAB) strategy that merges two complementary UAV collision avoidance\ntechniques: changing UAV trajectories and altering their starting times, also\nreferred to as batching. This integration helps overcome the disadvantages of\nboth - reduction in the number of trajectory manipulations, which avoids overly\nconvoluted paths in the first technique, and smaller batch sizes, which reduce\noverall takeoff time in the second.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.1; I.6.3"
    ],
    "primary_category": "cs.RO",
    "comment": "24 Pages, 9 Tables, 6 Figures",
    "pdf_url": "http://arxiv.org/pdf/2405.11722v1",
    "published_date": "2024-05-20 01:47:28 UTC",
    "updated_date": "2024-05-20 01:47:28 UTC"
  },
  {
    "arxiv_id": "2405.11715v2",
    "title": "Semantic Trajectory Data Mining with LLM-Informed POI Classification",
    "authors": [
      "Yifan Liu",
      "Chenchen Kuai",
      "Haoxuan Ma",
      "Xishun Liao",
      "Brian Yueshuai He",
      "Jiaqi Ma"
    ],
    "abstract": "Human travel trajectory mining is crucial for transportation systems,\nenhancing route optimization, traffic management, and the study of human travel\npatterns. Previous rule-based approaches without the integration of semantic\ninformation show a limitation in both efficiency and accuracy. Semantic\ninformation, such as activity types inferred from Points of Interest (POI)\ndata, can significantly enhance the quality of trajectory mining. However,\nintegrating these insights is challenging, as many POIs have incomplete feature\ninformation, and current learning-based POI algorithms require the integrity of\ndatasets to do the classification. In this paper, we introduce a novel pipeline\nfor human travel trajectory mining. Our approach first leverages the strong\ninferential and comprehension capabilities of large language models (LLMs) to\nannotate POI with activity types and then uses a Bayesian-based algorithm to\ninfer activity for each stay point in a trajectory. In our evaluation using the\nOpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a\n96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1\nscore in activity inference.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, accepted for the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.11715v2",
    "published_date": "2024-05-20 01:29:45 UTC",
    "updated_date": "2024-08-19 19:06:35 UTC"
  },
  {
    "arxiv_id": "2405.11143v4",
    "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework",
    "authors": [
      "Jian Hu",
      "Xibin Wu",
      "Zilin Zhu",
      "Xianyu",
      "Weixun Wang",
      "Dehao Zhang",
      "Yu Cao"
    ],
    "abstract": "As large language models (LLMs) continue to grow by scaling laws,\nreinforcement learning from human feedback (RLHF) has gained significant\nattention due to its outstanding performance. However, unlike pretraining or\nfine-tuning a single model, scaling reinforcement learning from human feedback\n(RLHF) for training large language models poses coordination challenges across\nfour models. We present OpenRLHF, an open-source framework enabling efficient\nRLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the\nsame GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters\nusing Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and\ndiverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF\nprovides an out-of-the-box solution with optimized algorithms and launch\nscripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,\nrejection sampling, and other alignment techniques. Empowering state-of-the-art\nLLM development, OpenRLHF's code is available at\n\\url{https://github.com/OpenRLHF/OpenRLHF}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11143v4",
    "published_date": "2024-05-20 01:04:40 UTC",
    "updated_date": "2024-11-24 08:34:48 UTC"
  },
  {
    "arxiv_id": "2405.11706v1",
    "title": "Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!",
    "authors": [
      "Dean Allemang",
      "Juan Sequeda"
    ],
    "abstract": "There is increasing evidence that question-answering (QA) systems with Large\nLanguage Models (LLMs), which employ a knowledge graph/semantic representation\nof an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy\ncompared to systems that answer questions directly on SQL databases (i.e.\nText-to-SQL). Our previous benchmark research showed that by using a knowledge\ngraph, the accuracy improved from 16% to 54%. The question remains: how can we\nfurther improve the accuracy and reduce the error rate? Building on the\nobservations of our previous research where the inaccurate LLM-generated SPARQL\nqueries followed incorrect paths, we present an approach that consists of 1)\nOntology-based Query Check (OBQC): detects errors by leveraging the ontology of\nthe knowledge graph to check if the LLM-generated SPARQL query matches the\nsemantic of ontology and 2) LLM Repair: use the error explanations with an LLM\nto repair the SPARQL query. Using the chat with the data benchmark, our primary\nfinding is that our approach increases the overall accuracy to 72% including an\nadditional 8% of \"I don't know\" unknown results. Thus, the overall error rate\nis 20%. These results provide further evidence that investing knowledge graphs,\nnamely the ontology, provides higher accuracy for LLM powered question\nanswering systems.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.11706v1",
    "published_date": "2024-05-20 00:28:00 UTC",
    "updated_date": "2024-05-20 00:28:00 UTC"
  },
  {
    "arxiv_id": "2405.11704v1",
    "title": "Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks",
    "authors": [
      "Taiyuan Mei",
      "Yun Zi",
      "Xiaohan Cheng",
      "Zijun Gao",
      "Qi Wang",
      "Haowei Yang"
    ],
    "abstract": "The internal structure and operation mechanism of large-scale language models\nare analyzed theoretically, especially how Transformer and its derivative\narchitectures can restrict computing efficiency while capturing long-term\ndependencies. Further, we dig deep into the efficiency bottleneck of the\ntraining phase, and evaluate in detail the contribution of adaptive\noptimization algorithms (such as AdamW), massively parallel computing\ntechniques, and mixed precision training strategies to accelerate convergence\nand reduce memory footprint. By analyzing the mathematical principles and\nimplementation details of these algorithms, we reveal how they effectively\nimprove training efficiency in practice. In terms of model deployment and\ninference optimization, this paper systematically reviews the latest advances\nin model compression techniques, focusing on strategies such as quantification,\npruning, and knowledge distillation. By comparing the theoretical frameworks of\nthese techniques and their effects in different application scenarios, we\ndemonstrate their ability to significantly reduce model size and inference\ndelay while maintaining model prediction accuracy. In addition, this paper\ncritically examines the limitations of current efficiency optimization methods,\nsuch as the increased risk of overfitting, the control of performance loss\nafter compression, and the problem of algorithm generality, and proposes some\nprospects for future research. In conclusion, this study provides a\ncomprehensive theoretical framework for understanding the efficiency\noptimization of large-scale language models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.11704v1",
    "published_date": "2024-05-20 00:10:00 UTC",
    "updated_date": "2024-05-20 00:10:00 UTC"
  }
]
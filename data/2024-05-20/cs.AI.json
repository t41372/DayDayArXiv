{
  "date": "2024-05-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-20 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 77 篇论文，主要聚焦于 AI 和机器学习领域，包括强化学习框架优化、大语言模型（LLMs）的元认知能力和应用、图神经网络改进，以及 AI 在医疗、教育和环境中的实际落地；令人印象深刻的是 Yoshua Bengio 参与的 LLM 元认知研究，以及 NeurIPS 亮点论文“Diffusion for World Modeling”，这些工作展示了 AI 在高效决策和跨领域应用的潜力。\n\n### 重点论文讨论\n我将优先讨论重要、创新性和话题度高的论文（如涉及知名学者或顶级会议），并将相关主题归类快速概述。其他较常规或小众论文（如特定数据集分析）将简要掠过。\n\n#### 强化学习与优化框架\n- **A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback（统一线性规划框架用于基于人类演示和反馈的离线奖励学习）**  \n  这篇 ICML 2024 论文（作者包括 Asuman Ozdaglar 和 Pablo A. Parrilo）提出了一种基于线性规划的离线奖励学习框架，利用预收集轨迹估计可行奖励集，并提供样本效率保证；主要贡献是提升了比最大似然估计更优的性能，适用于强化学习中的人类反馈对齐。\n\n- **Diffusion for World Modeling: Visual Details Matter in Atari（扩散模型用于世界建模：在 Atari 中的视觉细节重要性）**  \n  这篇 NeurIPS 2024 Spotlight 论文（作者团队包括 Amos Storkey）引入 DIAMOND 模型，使用扩散模型建模环境动态，显著提升了视觉细节捕捉；关键发现是，该方法在 Atari 100k 基准上实现 1.46 的归一化分数，并可作为交互式神经游戏引擎，展示了强化学习的高效性和泛化潜力。\n\n- **A Study on Optimization Techniques for Variational Quantum Circuits in Reinforcement Learning（变分量子电路在强化学习中的优化技术研究）**  \n  这篇 QSW 2024 论文探索数据重传、输入输出缩放和指数学习率衰减等技术，应用于量子强化学习；主要贡献是提高了参数效率和鲁棒性，在 Frozen Lake 和 Cart Pole 环境中表现出色。\n\n其他强化学习相关论文（如“Scruting What We Ignore”）则聚焦元强化学习优化，但细节较冗长，这里快速掠过：它们强调任务表示稳定性，但实验验证有限。\n\n#### 大语言模型与解释性 AI\n- **Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving（LLMs 的元认知能力：在数学问题求解中的探索）**  \n  Yoshua Bengio 参与的这篇论文（Preprint）证明 LLMs 具有命名技能和语义聚类能力；主要发现是通过提示引导，LLMs 在 GSM8K 和 MATH 数据集上提升了准确率，展示了 LLMs 在推理任务中的元认知潜力。\n\n- **Prompt Learning for Generalized Vehicle Routing（提示学习用于泛化车辆路径规划）**  \n  这篇论文提出基于提示学习的车辆路径算法，支持零样本泛化；关键贡献是使用元学习框架在预训练模型上快速适应新任务，适用于物流优化。\n\n- **JailMine: A Logit-Based Jailbreak Using Token-Level Manipulation（基于 logit 的越狱攻击：通过 token 级操作）**  \n  这篇工作展示了 LLMs 安全漏洞，通过迭代减少拒绝概率实现高效攻击；主要发现是攻击成功率高达 95%，强调了 LLMs 防御策略的必要性。\n\nLLMs 相关论文众多（如“Fennec”），但许多（如“RNG”）仅优化特定任务，这里不展开；它们的核心是提升模型解释性和鲁棒性，但影响较局部。\n\n#### AI 在医疗和实际应用\n- **Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework（使用 EVINCE 框架确保医疗领域的真实性准确性）**  \n  作者 Edward Y. Chang 的这篇论文提出 EVINCE 系统，利用多 LLMs 辩论框架优化诊断；主要贡献是减少误诊并改进数据纠正，实验验证了其有效性。\n\n- **Adapting Large Multimodal Models to Distribution Shifts（适应分布偏移的大型多模态模型）**  \n  这篇论文（作者包括 Salman Khan）引入 InvariantSelectPR 方法，提升 LLMs 在医疗图像上的适应性；关键发现是准确率在 Camelyon17 和 HAM10000 上提升 34.2% 和 16.9%，展示了 AI 在医疗中的鲁棒性。\n\n医疗 AI 论文（如“Digital Health and Indoor Air Quality”）强调环境监测，但较应用导向，这里快速提及：它们使用 IoT 和 AI 促进行为改变，贡献在于提升健康公平性。\n\n#### 图神经网络和其它创新\n- **A Metric-based Principal Curve Approach for Learning One-dimensional Manifold（基于度量的主曲线方法用于一维流形学习）**  \n  这篇论文提出 MPC 方法，通过度量学习一维流形；主要发现是在 MNIST 上有效捕捉形状，适用于数据降维。\n\n- **TENNs-PLEIADES: Building Temporal Kernels with Orthogonal Polynomials（使用正交多项式构建时间核）**  \n  这篇工作引入 PLEIADES 网络，用于事件数据分类；关键贡献是状态艺术性能，如 DVS128 数据集上 99.59% 准确率，展示了高效时间序列建模。\n\n其他论文（如图聚类或车辆检测）较专业但不具话题度，这里掠过：例如“Bangladeshi Native Vehicle Detection”在 YOLO 模型上优化检测，但局限于特定数据集。\n\n总体而言，今天的 arXiv 论文突显了 AI 在强化学习和 LLMs 上的突破潜力，但也暴露了安全和泛化挑战。感兴趣的读者可关注上述关键工作，以指导进一步阅读。明日见！",
  "papers": [
    {
      "arxiv_id": "2405.12421v3",
      "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Kihyun Kim",
        "Jiawei Zhang",
        "Asuman Ozdaglar",
        "Pablo A. Parrilo"
      ],
      "abstract": "Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human\nFeedback (RLHF) are pivotal methodologies in reward learning, which involve\ninferring and shaping the underlying reward function of sequential\ndecision-making problems based on observed human demonstrations and feedback.\nMost prior work in reward learning has relied on prior knowledge or assumptions\nabout decision or preference models, potentially leading to robustness issues.\nIn response, this paper introduces a novel linear programming (LP) framework\ntailored for offline reward learning. Utilizing pre-collected trajectories\nwithout online exploration, this framework estimates a feasible reward set from\nthe primal-dual optimality conditions of a suitably designed LP, and offers an\noptimality guarantee with provable sample efficiency. Our LP framework also\nenables aligning the reward functions with human feedback, such as pairwise\ntrajectory comparison data, while maintaining computational tractability and\nsample efficiency. We demonstrate that our framework potentially achieves\nbetter performance compared to the conventional maximum likelihood estimation\n(MLE) approach through analytical examples and numerical experiments.",
      "tldr_zh": "本文提出一个统一的线性规划(LP)框架，用于从人类演示和反馈中进行离线奖励学习，旨在克服现有 Inverse Reinforcement Learning (IRL) 和 Reinforcement Learning from Human Feedback (RLHF) 方法依赖先验知识或假设的鲁棒性问题。该框架利用预收集轨迹，通过原生-对偶最优性条件估计可行奖励集，并提供最优性保证和样本效率，同时支持整合人类反馈如轨迹比较数据。实验结果表明，该框架在分析例子和数值实验中，比传统的最大似然估计(MLE)方法表现出更好的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.12421v3",
      "published_date": "2024-05-20 23:59:26 UTC",
      "updated_date": "2024-10-14 23:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:43:58.009758"
    },
    {
      "arxiv_id": "2405.12399v2",
      "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
      "title_zh": "翻译失败",
      "authors": [
        "Eloi Alonso",
        "Adam Jelley",
        "Vincent Micheli",
        "Anssi Kanervisto",
        "Amos Storkey",
        "Tim Pearce",
        "François Fleuret"
      ],
      "abstract": "World models constitute a promising approach for training reinforcement\nlearning agents in a safe and sample-efficient manner. Recent world models\npredominantly operate on sequences of discrete latent variables to model\nenvironment dynamics. However, this compression into a compact discrete\nrepresentation may ignore visual details that are important for reinforcement\nlearning. Concurrently, diffusion models have become a dominant approach for\nimage generation, challenging well-established methods modeling discrete\nlatents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a\nModel Of eNvironment Dreams), a reinforcement learning agent trained in a\ndiffusion world model. We analyze the key design choices that are required to\nmake diffusion suitable for world modeling, and demonstrate how improved visual\ndetails can lead to improved agent performance. DIAMOND achieves a mean human\nnormalized score of 1.46 on the competitive Atari 100k benchmark; a new best\nfor agents trained entirely within a world model. We further demonstrate that\nDIAMOND's diffusion world model can stand alone as an interactive neural game\nengine by training on static Counter-Strike: Global Offensive gameplay. To\nfoster future research on diffusion for world modeling, we release our code,\nagents, videos and playable world models at https://diamond-wm.github.io.",
      "tldr_zh": "本论文提出 DIAMOND（DIffusion As a Model Of eNvironment Dreams）框架，使用 diffusion 模型作为世界模型（world models），以解决传统基于离散潜在变量（discrete latent variables）的建模方法忽略视觉细节的问题，从而提升强化学习（reinforcement learning）代理的性能。研究分析了使 diffusion 适合世界建模的关键设计选择，并通过实验证明，改进的视觉细节能显著提高代理效果，在 Atari 100k 基准测试中，DIAMOND 取得了 1.46 的平均人类归一化分数，创下了完全在世界模型中训练代理的新最佳纪录。此外，该 diffusion 世界模型可独立用作交互式神经游戏引擎，并在 Counter-Strike: Global Offensive 游戏上进行了验证，并公开了代码、代理和视频以支持未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2405.12399v2",
      "published_date": "2024-05-20 22:51:05 UTC",
      "updated_date": "2024-10-30 14:34:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:44:11.439758"
    },
    {
      "arxiv_id": "2405.12391v1",
      "title": "Automated Anomaly Detection on European XFEL Klystrons",
      "title_zh": "翻译失败",
      "authors": [
        "Antonin Sulc",
        "Annika Eichler",
        "Tim Wilksen"
      ],
      "abstract": "High-power multi-beam klystrons represent a key component to amplify RF to\ngenerate the accelerating field of the superconducting radio frequency (SRF)\ncavities at European XFEL. Exchanging these high-power components takes time\nand effort, thus it is necessary to minimize maintenance and downtime and at\nthe same time maximize the device's operation. In an attempt to explore the\nbehavior of klystrons using machine learning, we completed a series of\nexperiments on our klystrons to determine various operational modes and conduct\nfeature extraction and dimensionality reduction to extract the most valuable\ninformation about a normal operation. To analyze recorded data we used\nstate-of-the-art data-driven learning techniques and recognized the most\npromising components that might help us better understand klystron operational\nstates and identify early on possible faults or anomalies.",
      "tldr_zh": "该研究针对 European XFEL 的高功率多束 klystrons（关键组件，用于放大射频以产生 SRF cavities 的加速场）开发了自动化异常检测方法，以最小化维护和停机时间。研究团队通过机器学习实验进行特征提取和降维，分析 klystrons 的正常操作数据，从而提取最有价值的运行信息。使用最先进的基于数据的学习技术，他们识别了有助于理解操作状态并及早检测故障或异常的关键组件，为优化设备性能提供了新见解。",
      "categories": [
        "physics.acc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.acc-ph",
      "comment": "4 pages, 4 figures, 15, 15TH International Particle Accelerator\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2405.12391v1",
      "published_date": "2024-05-20 21:59:07 UTC",
      "updated_date": "2024-05-20 21:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:44:21.548410"
    },
    {
      "arxiv_id": "2405.12390v4",
      "title": "A Metric-based Principal Curve Approach for Learning One-dimensional Manifold",
      "title_zh": "翻译失败",
      "authors": [
        "Eliuvish Cuicizion"
      ],
      "abstract": "Principal curve is a well-known statistical method oriented in manifold\nlearning using concepts from differential geometry. In this paper, we propose a\nnovel metric-based principal curve (MPC) method that learns one-dimensional\nmanifold of spatial data. Synthetic datasets Real applications using MNIST\ndataset show that our method can learn the one-dimensional manifold well in\nterms of the shape.",
      "tldr_zh": "本文提出了一种基于度量的主曲线方法（MPC），旨在通过主曲线（Principal curve）和微分几何（differential geometry）的概念来学习空间数据的一维流形（one-dimensional manifold）。MPC方法专注于流形学习（manifold learning），并通过改进的度量机制提升了学习精度。实验结果显示，在合成数据集和MNIST数据集的实际应用中，该方法在形状方面表现出色，能够有效捕捉一维流形的结构。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12390v4",
      "published_date": "2024-05-20 21:50:19 UTC",
      "updated_date": "2025-03-18 20:30:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:44:32.905211"
    },
    {
      "arxiv_id": "2406.00013v1",
      "title": "Thesis: Document Summarization with applications to Keyword extraction and Image Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Jayaprakash Sundararaj"
      ],
      "abstract": "Automatic summarization is the process of reducing a text document in order\nto generate a summary that retains the most important points of the original\ndocument. In this work, we study two problems - i) summarizing a text document\nas set of keywords/caption, for image recommedation, ii) generating opinion\nsummary which good mix of relevancy and sentiment with the text document.\nIntially, we present our work on an recommending images for enhancing a\nsubstantial amount of existing plain text news articles. We use probabilistic\nmodels and word similarity heuristics to generate captions and extract\nKey-phrases which are re-ranked using a rank aggregation framework with\nrelevance feedback mechanism. We show that such rank aggregation and relevant\nfeedback which are typically used in Tagging Documents, Text Information\nRetrieval also helps in improving image retrieval. These queries are fed to the\nYahoo Search Engine to obtain relevant images 1. Our proposed method is\nobserved to perform better than all existing baselines. Additonally, We propose\na set of submodular functions for opinion summarization. Opinion summarization\nhas built in it the tasks of summarization and sentiment detection. However, it\nis not easy to detect sentiment and simultaneously extract summary. The two\ntasks conflict in the sense that the demand of compression may drop sentiment\nbearing sentences, and the demand of sentiment detection may bring in redundant\nsentences. However, using submodularity we show how to strike a balance between\nthe two requirements. Our functions generate summaries such that there is good\ncorrelation between document sentiment and summary sentiment along with good\nROUGE score. We also compare the performances of the proposed submodular\nfunctions.",
      "tldr_zh": "本论文探讨了文本文档的自动摘要技术及其在关键词提取和图像检索中的应用。研究首先提出一种方法，使用probabilistic models和词相似性启发式生成标题和关键短语，并通过rank aggregation框架及relevance feedback机制重新排名，以提升图像推荐效果，并证明该方法优于现有基线。接着，作者开发了submodular functions来处理意见摘要化任务，通过平衡摘要压缩和情感检测，确保摘要与文档情感高度相关，同时获得良好的ROUGE score。整体贡献在于为文档摘要提供创新框架，提高了图像检索和意见总结的准确性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00013v1",
      "published_date": "2024-05-20 21:27:18 UTC",
      "updated_date": "2024-05-20 21:27:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:44:45.321349"
    },
    {
      "arxiv_id": "2405.13071v2",
      "title": "A Novel Method for News Article Event-Based Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Koren Ishlach",
        "Itzhak Ben-David",
        "Michael Fire",
        "Lior Rokach"
      ],
      "abstract": "Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.",
      "tldr_zh": "本论文提出了一种新颖的新闻嵌入方法，专注于捕捉新闻文章中事件、实体和主题的潜在上下文，并考虑时间相关性，以优化现有方法的不足。方法分为三个阶段：首先，从新闻文章中提取事件、实体和主题；其次，通过在当前和历史数据上训练时间分离的 GloVe 模型，生成周期性时间嵌入；最后，结合 Smooth Inverse Frequency (SIF) 用于文章级向量和 Siamese Neural Networks 用于事件相关信息的嵌入。实验利用 GDELT 项目中的超过 850,000 篇新闻文章和 1,000,000 个事件进行验证，结果显示该方法在共享事件检测任务上优于现有最先进技术。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13071v2",
      "published_date": "2024-05-20 20:55:07 UTC",
      "updated_date": "2024-08-02 09:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:44:58.195721"
    },
    {
      "arxiv_id": "2405.12368v1",
      "title": "Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)",
      "title_zh": "布局无关的智能家居人类活动识别：通过传感器触发事件的文本描述 (",
      "authors": [
        "Megha Thukral",
        "Sourish Gunesh Dhekane",
        "Shruthi K. Hiremath",
        "Harish Haresamudram",
        "Thomas Ploetz"
      ],
      "abstract": "Human activity recognition (HAR) using ambient sensors in smart homes has\nnumerous applications for human healthcare and wellness. However, building\ngeneral-purpose HAR models that can be deployed to new smart home environments\nrequires a significant amount of annotated sensor data and training overhead.\nMost smart homes vary significantly in their layouts, i.e., floor plans and the\nspecifics of sensors embedded, resulting in low generalizability of HAR models\ntrained for specific homes. We address this limitation by introducing a novel,\nlayout-agnostic modeling approach for HAR systems in smart homes that utilizes\nthe transferrable representational capacity of natural language descriptions of\nraw sensor data. To this end, we generate Textual Descriptions Of Sensor\nTriggers (TDOST) that encapsulate the surrounding trigger conditions and\nprovide cues for underlying activities to the activity recognition models.\nLeveraging textual embeddings, rather than raw sensor data, we create activity\nrecognition systems that predict standard activities across homes without\neither (re-)training or adaptation on target homes. Through an extensive\nevaluation, we demonstrate the effectiveness of TDOST-based models in unseen\nsmart homes through experiments on benchmarked CASAS datasets. Furthermore, we\nconduct a detailed analysis of how the individual components of our approach\naffect downstream activity recognition performance.",
      "tldr_zh": "该研究针对智能家居中基于环境传感器的 Human Activity Recognition (HAR) 问题，提出了一种布局无关的建模方法，通过生成 Textual Descriptions Of Sensor Triggers (TDOST) 来利用自然语言描述传感器触发事件，从而提升模型的泛化性。TDOST 封装了触发条件并提供活动线索，使用文本嵌入代替原始传感器数据，实现跨家居活动识别，而无需重新训练或适应目标环境。在 CASAS 数据集的广泛实验中，该方法在未见家居环境中表现出色，并通过详细分析组件影响，证明了其对 HAR 性能的显著提升。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12368v1",
      "published_date": "2024-05-20 20:37:44 UTC",
      "updated_date": "2024-05-20 20:37:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:45:08.822081"
    },
    {
      "arxiv_id": "2405.12354v1",
      "title": "A Study on Optimization Techniques for Variational Quantum Circuits in Reinforcement Learning",
      "title_zh": "变分量子电路在强化学习中的优化技术研究",
      "authors": [
        "Michael Kölle",
        "Timo Witter",
        "Tobias Rohe",
        "Gerhard Stenzel",
        "Philipp Altmann",
        "Thomas Gabor"
      ],
      "abstract": "Quantum Computing aims to streamline machine learning, making it more\neffective with fewer trainable parameters. This reduction of parameters can\nspeed up the learning process and reduce the use of computational resources.\nHowever, in the current phase of quantum computing development, known as the\nnoisy intermediate-scale quantum era (NISQ), learning is difficult due to a\nlimited number of qubits and widespread quantum noise. To overcome these\nchallenges, researchers are focusing on variational quantum circuits (VQCs).\nVQCs are hybrid algorithms that merge a quantum circuit, which can be adjusted\nthrough parameters, with traditional classical optimization techniques. These\ncircuits require only few qubits for effective learning. Recent studies have\npresented new ways of applying VQCs to reinforcement learning, showing\npromising results that warrant further exploration. This study investigates the\neffects of various techniques -- data re-uploading, input scaling, output\nscaling -- and introduces exponential learning rate decay in the quantum\nproximal policy optimization algorithm's actor-VQC. We assess these methods in\nthe popular Frozen Lake and Cart Pole environments. Our focus is on their\nability to reduce the number of parameters in the VQC without losing\neffectiveness. Our findings indicate that data re-uploading and an exponential\nlearning rate decay significantly enhance hyperparameter stability and overall\nperformance. While input scaling does not improve parameter efficiency, output\nscaling effectively manages greediness, leading to increased learning speed and\nrobustness.",
      "tldr_zh": "本研究探讨了在强化学习中优化 Variational Quantum Circuits (VQCs) 的技术，以应对 NISQ 时代量子噪声和资源限制的挑战。研究者评估了 data re-uploading、input scaling、output scaling 等方法，并引入 exponential learning rate decay 到量子近端策略优化算法的 actor-VQC，在 Frozen Lake 和 Cart Pole 环境中测试这些技术的参数效率。结果显示，data re-uploading 和 exponential learning rate decay 显著提升了超参数稳定性和整体性能，而 output scaling 提高了学习速度和鲁棒性，但 input scaling 并未改善参数效率。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at QSW 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.12354v1",
      "published_date": "2024-05-20 20:06:42 UTC",
      "updated_date": "2024-05-20 20:06:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:45:22.549754"
    },
    {
      "arxiv_id": "2405.15808v2",
      "title": "Ensuring Ground Truth Accuracy in Healthcare with the EVINCE framework",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Y. Chang"
      ],
      "abstract": "Misdiagnosis is a significant issue in healthcare, leading to harmful\nconsequences for patients. The propagation of mislabeled data through machine\nlearning models into clinical practice is unacceptable. This paper proposes\nEVINCE, a system designed to 1) improve diagnosis accuracy and 2) rectify\nmisdiagnoses and minimize training data errors. EVINCE stands for Entropy\nVariation through Information Duality with Equal Competence, leveraging this\nnovel theory to optimize the diagnostic process using multiple Large Language\nModels (LLMs) in a structured debate framework. Our empirical study verifies\nEVINCE to be effective in achieving its design goals.",
      "tldr_zh": "该论文针对医疗领域的误诊问题及其对患者的有害影响，提出 EVINCE 框架，以提升诊断准确性和减少训练数据错误。EVINCE 基于 Entropy Variation through Information Duality with Equal Competence 的理论，通过多个 Large Language Models (LLMs) 在结构化辩论框架中优化诊断过程。实证研究证明了 EVINCE 在改进诊断准确性和最小化错误方面有效。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 4 tables, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.15808v2",
      "published_date": "2024-05-20 18:26:36 UTC",
      "updated_date": "2024-05-28 05:11:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:45:33.002939"
    },
    {
      "arxiv_id": "2405.12299v2",
      "title": "Perturbing the Gradient for Alleviating Meta Overfitting",
      "title_zh": "通过梯度扰动缓解元过拟合",
      "authors": [
        "Manas Gogoi",
        "Sambhavi Tiwari",
        "Shekhar Verma"
      ],
      "abstract": "The reason for Meta Overfitting can be attributed to two factors: Mutual\nNon-exclusivity and the Lack of diversity, consequent to which a single global\nfunction can fit the support set data of all the meta-training tasks and fail\nto generalize to new unseen tasks. This issue is evidenced by low error rates\non the meta-training tasks, but high error rates on new tasks. However, there\ncan be a number of novel solutions to this problem keeping in mind any of the\ntwo objectives to be attained, i.e. to increase diversity in the tasks and to\nreduce the confidence of the model for some of the tasks. In light of the\nabove, this paper proposes a number of solutions to tackle meta-overfitting on\nfew-shot learning settings, such as few-shot sinusoid regression and few shot\nclassification. Our proposed approaches demonstrate improved generalization\nperformance compared to state-of-the-art baselines for learning in a\nnon-mutually exclusive task setting. Overall, this paper aims to provide\ninsights into tackling overfitting in meta-learning and to advance the field\ntowards more robust and generalizable models.",
      "tldr_zh": "本论文将 Meta Overfitting 归因于 Mutual Non-exclusivity 和 Lack of diversity，导致模型在元训练任务上错误率低，但在新任务上泛化性能差。为缓解这一问题，研究提出通过扰动梯度等方法来增加任务多样性或降低模型信心，并在 few-shot learning 设置（如 few-shot sinusoid regression 和 few-shot classification）中实现比现有基准更好的泛化性能。总体而言，此工作为 meta-learning 领域提供见解，推动更鲁棒和可泛化的模型发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12299v2",
      "published_date": "2024-05-20 18:04:59 UTC",
      "updated_date": "2024-11-10 13:46:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:45:46.368626"
    },
    {
      "arxiv_id": "2405.12217v2",
      "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
      "title_zh": "大型多模态模型对分布偏移的适应：上下文学习的作用",
      "authors": [
        "Guanglin Zhou",
        "Zhongyi Han",
        "Shiming Chen",
        "Biwei Huang",
        "Liming Zhu",
        "Salman Khan",
        "Xin Gao",
        "Lina Yao"
      ],
      "abstract": "Recent studies indicate that large multimodal models (LMMs) potentially act\nas general-purpose assistants and are highly robust against different\ndistributions. Despite this, domain-specific adaptation is still necessary\nparticularly in specialized areas like healthcare. Due to the impracticality of\nfine-tuning LMMs given their vast parameter space, this work investigates\nin-context learning (ICL) as an effective alternative for enhancing LMMs'\nadaptability. Our study addresses this by evaluating an unsupervised ICL method\nwhich selects in-context examples through a nearest example search based on\nfeature similarity. We uncover that its effectiveness is limited by the\ndeficiencies of pre-trained vision encoders under distribution shift scenarios.\nTo address these challenges, we propose InvariantSelectPR, a novel method\nleveraging Class-conditioned Contrastive Invariance (CCI) for more robust\ndemonstration selection. Specifically, CCI enhances pre-trained vision encoders\nby improving their discriminative capabilities across different classes and\nensuring invariance to domain-specific variations. This enhancement allows the\nencoders to effectively identify and retrieve the most informative examples,\nwhich are then used to guide LMMs in adapting to new query samples under\nvarying distributions. Our experiments show that InvariantSelectPR\nsubstantially improves the adaptability of LMMs, achieving significant\nperformance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy\nincrease in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on\nHAM10000 compared to the baseline zero-shot performance.",
      "tldr_zh": "本研究探讨了如何通过 in-context learning (ICL) 使大型多模态模型 (LMMs) 适应分布偏移问题，尤其在医疗等专业领域中。论文评估了一种基于特征相似性的无监督 ICL 方法，但发现其效果受限于预训练视觉编码器的不足，为此提出 InvariantSelectPR 方法，利用 Class-conditioned Contrastive Invariance (CCI) 增强编码器的判别能力和对领域变异的鲁棒性，从而实现更有效的演示选择。实验结果显示，InvariantSelectPR 显著提升了 LMMs 的适应性能，在 Camelyon17 数据集上 7-shot 准确率提高 34.2%，而在 HAM10000 上提高 16.9%，相比零-shot 基线取得了实质性进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 9 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.12217v2",
      "published_date": "2024-05-20 17:59:21 UTC",
      "updated_date": "2024-10-14 23:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:00.005848"
    },
    {
      "arxiv_id": "2405.12266v1",
      "title": "EGAN: Evolutional GAN for Ransomware Evasion",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Commey",
        "Benjamin Appiah",
        "Bill K. Frimpong",
        "Isaac Osei",
        "Ebenezer N. A. Hammond",
        "Garth V. Crosby"
      ],
      "abstract": "Adversarial Training is a proven defense strategy against adversarial\nmalware. However, generating adversarial malware samples for this type of\ntraining presents a challenge because the resulting adversarial malware needs\nto remain evasive and functional. This work proposes an attack framework, EGAN,\nto address this limitation. EGAN leverages an Evolution Strategy and Generative\nAdversarial Network to select a sequence of attack actions that can mutate a\nRansomware file while preserving its original functionality. We tested this\nframework on popular AI-powered commercial antivirus systems listed on\nVirusTotal and demonstrated that our framework is capable of bypassing the\nmajority of these systems. Moreover, we evaluated whether the EGAN attack\nframework can evade other commercial non-AI antivirus solutions. Our results\nindicate that the adversarial ransomware generated can increase the probability\nof evading some of them.",
      "tldr_zh": "这篇论文提出了 EGAN 框架，一种结合 Evolution Strategy 和 Generative Adversarial Network 的攻击方法，用于生成功能保持的对抗性 Ransomware 样本，以支持对抗训练。EGAN 通过选择序列攻击动作来变异恶意软件文件，同时确保其原始功能不变。实验结果显示，该框架在 VirusTotal 上流行 AI 驱动的商业防病毒系统中绕过了大多数系统，并在非 AI 防病毒解决方案中显著提高了 evasion 概率。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12266v1",
      "published_date": "2024-05-20 17:52:40 UTC",
      "updated_date": "2024-05-20 17:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:09.815988"
    },
    {
      "arxiv_id": "2405.12205v1",
      "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
      "title_zh": "大型语言模型的元认知能力：在数学问题解决中的探索",
      "authors": [
        "Aniket Didolkar",
        "Anirudh Goyal",
        "Nan Rosemary Ke",
        "Siyuan Guo",
        "Michal Valko",
        "Timothy Lillicrap",
        "Danilo Rezende",
        "Yoshua Bengio",
        "Michael Mozer",
        "Sanjeev Arora"
      ],
      "abstract": "Metacognitive knowledge refers to humans' intuitive knowledge of their own\nthinking and reasoning processes. Today's best LLMs clearly possess some\nreasoning processes. The paper gives evidence that they also have metacognitive\nknowledge, including ability to name skills and procedures to apply given a\ntask. We explore this primarily in context of math reasoning, developing a\nprompt-guided interaction procedure to get a powerful LLM to assign sensible\nskill labels to math questions, followed by having it perform semantic\nclustering to obtain coarser families of skill labels. These coarse skill\nlabels look interpretable to humans.\n  To validate that these skill labels are meaningful and relevant to the LLM's\nreasoning processes we perform the following experiments. (a) We ask GPT-4 to\nassign skill labels to training questions in math datasets GSM8K and MATH. (b)\nWhen using an LLM to solve the test questions, we present it with the full list\nof skill labels and ask it to identify the skill needed. Then it is presented\nwith randomly selected exemplar solved questions associated with that skill\nlabel. This improves accuracy on GSM8k and MATH for several strong LLMs,\nincluding code-assisted models. The methodology presented is domain-agnostic,\neven though this article applies it to math problems.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 的元认知能力 (Metacognitive Capabilities)，即模型对自身思考和推理过程的直觉知识，重点通过数学问题解决进行探索。研究开发了一种提示引导的交互过程，让 LLMs 为数学问题分配合理的技能标签，并通过语义聚类获得更粗的、可解释的技能标签分类。实验结果显示，当提供这些技能标签和相关示例时，LLMs 在 GSM8K 和 MATH 数据集上的准确率得到提升，包括代码辅助模型，这证明了该方法对提升模型推理的有效性，且该方法是领域无关的。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.12205v1",
      "published_date": "2024-05-20 17:45:26 UTC",
      "updated_date": "2024-05-20 17:45:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:23.440671"
    },
    {
      "arxiv_id": "2405.12202v1",
      "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Xihaier Luo",
        "Xiaoning Qian",
        "Byung-Jun Yoon"
      ],
      "abstract": "In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.",
      "tldr_zh": "本文提出了一种任意-scale super-resolution (SR) 方法，旨在提升科学数据的分辨率，同时处理连续性、多尺度物理和高频信号的复杂挑战。该方法的主体是 hierarchical neural operator，利用 Galerkin-type self-attention 机制和 Sinc 过滤器来高效学习函数空间映射，并确保层次间的信息传输等效。此外，引入了 learnable frequency-aware loss prior，通过光谱调整输入数据动态调整像素权重，以优化梯度平衡。实验结果显示，该方法在不同领域的多样数据集上，与最先进 SR 方法相比，实现了持续的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.12202v1",
      "published_date": "2024-05-20 17:39:29 UTC",
      "updated_date": "2024-05-20 17:39:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:35.291874"
    },
    {
      "arxiv_id": "2405.13068v2",
      "title": "Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxi Li",
        "Yi Liu",
        "Yuekang Li",
        "Ling Shi",
        "Gelei Deng",
        "Shengquan Chen",
        "Kailong Wang"
      ],
      "abstract": "Large language models (LLMs) have transformed the field of natural language\nprocessing, but they remain susceptible to jailbreaking attacks that exploit\ntheir capabilities to generate unintended and potentially harmful content.\nExisting token-level jailbreaking techniques, while effective, face scalability\nand efficiency challenges, especially as models undergo frequent updates and\nincorporate advanced defensive measures. In this paper, we introduce JailMine,\nan innovative token-level manipulation approach that addresses these\nlimitations effectively. JailMine employs an automated \"mining\" process to\nelicit malicious responses from LLMs by strategically selecting affirmative\noutputs and iteratively reducing the likelihood of rejection. Through rigorous\ntesting across multiple well-known LLMs and datasets, we demonstrate JailMine's\neffectiveness and efficiency, achieving a significant average reduction of 86%\nin time consumed while maintaining high success rates averaging 95%, even in\nthe face of evolving defensive strategies. Our work contributes to the ongoing\neffort to assess and mitigate the vulnerability of LLMs to jailbreaking\nattacks, underscoring the importance of continued vigilance and proactive\nmeasures to enhance the security and reliability of these powerful language\nmodels.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）对越狱攻击（jailbreaking attacks）的脆弱性，提出了一种基于logit的token-level manipulation方法，名为JailMine，以解决现有技术的可扩展性和效率问题。JailMine通过自动“mining”过程，选择肯定输出并迭代减少拒绝可能性，从而有效诱导LLMs生成恶意响应。在多款知名LLMs和数据集上的测试中，该方法实现了平均95%的成功率，同时将时间消耗减少86%，即使面对先进的防御策略也能保持高效。该研究强调了对LLMs安全性和可靠性的评估与强化，贡献于防范潜在风险的持续努力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13068v2",
      "published_date": "2024-05-20 17:17:55 UTC",
      "updated_date": "2024-06-19 13:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:46.863971"
    },
    {
      "arxiv_id": "2405.12183v1",
      "title": "Multi-order Graph Clustering with Adaptive Node-level Weight Learning",
      "title_zh": "多阶图聚类结合自适应节点级权重学习",
      "authors": [
        "Ye Liu",
        "Xuelei Lin",
        "Yejia Chen",
        "Reynold Cheng"
      ],
      "abstract": "Current graph clustering methods emphasize individual node and edge con\nnections, while ignoring higher-order organization at the level of motif. Re\ncently, higher-order graph clustering approaches have been designed by motif\nbased hypergraphs. However, these approaches often suffer from hypergraph\nfragmentation issue seriously, which degrades the clustering performance\ngreatly. Moreover, real-world graphs usually contain diverse motifs, with nodes\nparticipating in multiple motifs. A key challenge is how to achieve precise\nclustering results by integrating information from multiple motifs at the node\nlevel. In this paper, we propose a multi-order graph clustering model (MOGC) to\nintegrate multiple higher-order structures and edge connections at node level.\nMOGC employs an adaptive weight learning mechanism to au tomatically adjust the\ncontributions of different motifs for each node. This not only tackles\nhypergraph fragmentation issue but enhances clustering accuracy. MOGC is\nefficiently solved by an alternating minimization algo rithm. Experiments on\nseven real-world datasets illustrate the effectiveness of MOGC.",
      "tldr_zh": "该研究指出，现有图聚类方法主要关注节点和边连接，而忽略了基于 motif 的更高阶结构，导致超图碎片化问题和聚类性能下降。针对这一挑战，提出了一种多阶图聚类模型（MOGC），通过自适应节点级别权重学习机制自动调整每个节点对不同 motif 的贡献，从而整合多个更高阶结构和边连接。实验结果显示，MOGC 在七个真实数据集上显著提高了聚类准确性，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12183v1",
      "published_date": "2024-05-20 17:09:58 UTC",
      "updated_date": "2024-05-20 17:09:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:46:59.259989"
    },
    {
      "arxiv_id": "2405.12179v3",
      "title": "TENNs-PLEIADES: Building Temporal Kernels with Orthogonal Polynomials",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Ru Pei",
        "Olivier Coenen"
      ],
      "abstract": "We introduce a neural network named PLEIADES (PoLynomial Expansion In\nAdaptive Distributed Event-based Systems), belonging to the TENNs (Temporal\nNeural Networks) architecture. We focus on interfacing these networks with\nevent-based data to perform online spatiotemporal classification and detection\nwith low latency. By virtue of using structured temporal kernels and\nevent-based data, we have the freedom to vary the sample rate of the data along\nwith the discretization step-size of the network without additional finetuning.\nWe experimented with three event-based benchmarks and obtained state-of-the-art\nresults on all three by large margins with significantly smaller memory and\ncompute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the\nDVS128 hand gesture recognition dataset and 100% with a small additional output\nfilter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye\ntracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1\nMegapixel Automotive Detection Dataset.",
      "tldr_zh": "这篇论文介绍了 PLEIADES 神经网络，这是 TENNs (Temporal Neural Networks) 架构的一部分，通过利用正交多项式构建结构化的时间内核，来处理基于事件的 数据，实现低延迟的在线时空分类和检测。 该方法允许灵活调整数据采样率和网络离散化步长，而无需额外微调，从而显著降低内存和计算成本。 在三个基准测试中，PLEIADES 取得了最先进的结果：DVS128 手势识别数据集上达到 99.59% 准确率（192K 参数），AIS 2024 眼动追踪挑战上达 99.58% 测试准确率（277K 参数），以及 PROPHESEE 1 Megapixel 汽车检测数据集上获得 0.556 mAP（576k 参数）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.12179v3",
      "published_date": "2024-05-20 17:06:24 UTC",
      "updated_date": "2024-05-31 18:29:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:47:14.253281"
    },
    {
      "arxiv_id": "2405.12163v1",
      "title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobo Liang",
        "Haoke Zhang",
        "Helan hu",
        "Juntao Li",
        "Jun Xu",
        "Min Zhang"
      ],
      "abstract": "The rapid advancement of large language models has given rise to a plethora\nof applications across a myriad of real-world tasks, mainly centered on\naligning with human intent. However, the complexities inherent in human intent\nnecessitate a dependence on labor-intensive and time-consuming human\nevaluation. To alleviate this constraint, we delve into the paradigm of\nemploying open-source large language models as evaluators, aligning with the\nprevailing trend of utilizing GPT-4. Particularly, we present a step-by-step\nevaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained\n\\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through\nbran\\textbf{C}hing and bridging. Specifically, the branching operation dissects\nthe evaluation task into various dimensions and granularities, thereby\nalleviating the challenges associated with evaluation. Concurrently, the\nbridging operation amalgamates diverse training datasets, augmenting the\nvariety of evaluation tasks. In experimental trials, our 7B model consistently\noutperforms open-source larger-scale evaluation models across various widely\nadopted benchmarks in terms of both \\textit{Agreement} and\n\\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ\nthe fine-grained correction capabilities induced by the evaluation model to\nrefine multiple model responses, and the results show that the refinement\nelevates the quality of responses, leading to an improvement of 1-2 points on\nthe MT-Bench. Our code is available at\nGithub\\footnote{\\url{https://github.com/dropreg/Fennec}}.",
      "tldr_zh": "这篇论文提出了Fennec框架，用于细粒度(Fine-grained)语言模型评估和修正，以减少依赖人工评估的局限性。Fennec通过Branching操作将评估任务分解成不同维度和粒度，以及Bridging操作整合多种训练数据集，来提升评估的多样性和准确性。实验结果显示，该7B模型在Agreement和Consistency指标上超越开源大型评估模型，并接近GPT-4的表现；此外，利用Fennec的修正能力优化模型响应，在MT-Bench基准上提升了1-2点。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12163v1",
      "published_date": "2024-05-20 16:47:22 UTC",
      "updated_date": "2024-05-20 16:47:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:47:23.604926"
    },
    {
      "arxiv_id": "2405.12150v1",
      "title": "Bangladeshi Native Vehicle Detection in Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Bipin Saha",
        "Md. Johirul Islam",
        "Shaikh Khaled Mostaque",
        "Aditya Bhowmik",
        "Tapodhir Karmakar Taton",
        "Md. Nakib Hayat Chowdhury",
        "Mamun Bin Ibne Reaz"
      ],
      "abstract": "The success of autonomous navigation relies on robust and precise vehicle\nrecognition, hindered by the scarcity of region-specific vehicle detection\ndatasets, impeding the development of context-aware systems. To advance\nterrestrial object detection research, this paper proposes a native vehicle\ndetection dataset for the most commonly appeared vehicle classes in Bangladesh.\n17 distinct vehicle classes have been taken into account, with fully annotated\n81542 instances of 17326 images. Each image width is set to at least 1280px.\nThe dataset's average vehicle bounding box-to-image ratio is 4.7036. This\nBangladesh Native Vehicle Dataset (BNVD) has accounted for several\ngeographical, illumination, variety of vehicle sizes, and orientations to be\nmore robust on surprised scenarios. In the context of examining the BNVD\ndataset, this work provides a thorough assessment with four successive You Only\nLook Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's\neffectiveness is methodically evaluated and contrasted with other vehicle\ndatasets already in use. The BNVD dataset exhibits mean average precision(mAP)\nat 50% intersection over union (IoU) is 0.848 corresponding precision and\nrecall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643\nat an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset\nserves as a reliable representation of vehicle distribution and presents\nconsiderable complexities.",
      "tldr_zh": "这篇论文针对区域特定车辆检测数据集的缺失，提出Bangladesh Native Vehicle Dataset (BNVD)，这是一个针对孟加拉国常见车辆的本地数据集，包含17个车辆类别、17326张图像和81542个完全标注的实例，并考虑了地理、照明、车辆大小和方向的多样性以应对复杂场景。作者使用YOLO v5、v6、v7和v8模型对BNVD进行评估，并与其他车辆数据集进行对比。实验结果显示，BNVD在IoU=50%时的mAP为0.848，精度为0.841和召回率为0.774；在IoU 0.5-0.95时的mAP为0.643，证明了其在车辆分布和复杂性方面的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.12150v1",
      "published_date": "2024-05-20 16:23:40 UTC",
      "updated_date": "2024-05-20 16:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:47:35.845984"
    },
    {
      "arxiv_id": "2405.12147v2",
      "title": "Eliciting Problem Specifications via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Robert E. Wray",
        "James R. Kirk",
        "John E. Laird"
      ],
      "abstract": "Cognitive systems generally require a human to translate a problem definition\ninto some specification that the cognitive system can use to attempt to solve\nthe problem or perform the task. In this paper, we illustrate that large\nlanguage models (LLMs) can be utilized to map a problem class, defined in\nnatural language, into a semi-formal specification that can then be utilized by\nan existing reasoning and learning system to solve instances from the problem\nclass. We present the design of LLM-enabled cognitive task analyst agent(s).\nImplemented with LLM agents, this system produces a definition of problem\nspaces for tasks specified in natural language. LLM prompts are derived from\nthe definition of problem spaces in the AI literature and general\nproblem-solving strategies (Polya's How to Solve It). A cognitive system can\nthen use the problem-space specification, applying domain-general problem\nsolving strategies (\"weak methods\" such as search), to solve multiple instances\nof problems from the problem class. This result, while preliminary, suggests\nthe potential for speeding cognitive systems research via disintermediation of\nproblem formulation while also retaining core capabilities of cognitive\nsystems, such as robust inference and online learning.",
      "tldr_zh": "本文提出了一种利用大型语言模型（LLMs）的方法，将自然语言定义的问题类别转化为半正式规范，从而供现有的推理和学习系统使用。研究设计了LLM-enabled认知任务分析器代理，通过基于AI文献和Polya问题解决策略的提示，自动生成问题空间定义。结果表明，这种方法能加速认知系统研究，减少人工介入，同时保留核心能力如鲁棒推理和在线学习。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.11; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, Appendix. Revised in response to reviewer feedback.\n  Accepted for Advances in Cognitive Systems (Jun 2024, Palermo)",
      "pdf_url": "http://arxiv.org/pdf/2405.12147v2",
      "published_date": "2024-05-20 16:19:02 UTC",
      "updated_date": "2024-06-10 19:05:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:47:46.639230"
    },
    {
      "arxiv_id": "2405.13065v1",
      "title": "Exploring Teachers' Perception of Artificial Intelligence: The Socio-emotional Deficiency as Opportunities and Challenges in Human-AI Complementarity in K-12 Education",
      "title_zh": "翻译失败",
      "authors": [
        "Soon-young Oh",
        "Yongsu Ahn"
      ],
      "abstract": "In schools, teachers play a multitude of roles, serving as educators,\ncounselors, decision-makers, and members of the school community. With recent\nadvances in artificial intelligence (AI), there is increasing discussion about\nhow AI can assist, complement, and collaborate with teachers. To pave the way\nfor better teacher-AI complementary relationships in schools, our study aims to\nexpand the discourse on teacher-AI complementarity by seeking educators'\nperspectives on the potential strengths and limitations of AI across a spectrum\nof responsibilities. Through a mixed method using a survey with 100 elementary\nschool teachers in South Korea and in-depth interviews with 12 teachers, our\nfindings indicate that teachers anticipate AI's potential to complement human\nteachers by automating administrative tasks and enhancing personalized learning\nthrough advanced intelligence. Interestingly, the deficit of AI's\nsocio-emotional capabilities has been perceived as both challenges and\nopportunities. Overall, our study demonstrates the nuanced perception of\nteachers and different levels of expectations over their roles, challenging the\nneed for decisions about AI adoption tailored to educators' preferences and\nconcerns.",
      "tldr_zh": "该研究探讨了K-12教育中教师对人工智能(AI)的认知，焦点在于AI的社会情感缺陷如何作为机会和挑战，促进人类-AI互补关系。通过对100名韩国小学教师的调查和12名教师的深入访谈，研究发现教师期望AI能自动化行政任务并提升个性化学习。AI在社会情感能力上的不足被视为潜在挑战，同时也提供机会来强化教师的独特角色。总体而言，该研究强调了根据教师偏好和担忧定制AI采用决策的必要性，以实现更有效的教师-AI互补。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.13065v1",
      "published_date": "2024-05-20 15:43:04 UTC",
      "updated_date": "2024-05-20 15:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:47:58.965410"
    },
    {
      "arxiv_id": "2405.12262v1",
      "title": "Prompt Learning for Generalized Vehicle Routing",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Liu",
        "Xi Lin",
        "Weiduo Liao",
        "Zhenkun Wang",
        "Qingfu Zhang",
        "Xialiang Tong",
        "Mingxuan Yuan"
      ],
      "abstract": "Neural combinatorial optimization (NCO) is a promising learning-based\napproach to solving various vehicle routing problems without much manual\nalgorithm design. However, the current NCO methods mainly focus on the\nin-distribution performance, while the real-world problem instances usually\ncome from different distributions. A costly fine-tuning approach or generalized\nmodel retraining from scratch could be needed to tackle the out-of-distribution\ninstances. Unlike the existing methods, this work investigates an efficient\nprompt learning approach in NCO for cross-distribution adaptation. To be\nconcrete, we propose a novel prompt learning method to facilitate fast\nzero-shot adaptation of a pre-trained model to solve routing problem instances\nfrom different distributions. The proposed model learns a set of prompts among\nvarious distributions and then selects the best-matched one to prompt a\npre-trained attention model for each problem instance. Extensive experiments\nshow that the proposed prompt learning approach facilitates the fast adaptation\nof pre-trained routing models. It also outperforms existing generalized models\non both in-distribution prediction and zero-shot generalization to a diverse\nset of new tasks. Our code implementation is available online\nhttps://github.com/FeiLiu36/PromptVRP.",
      "tldr_zh": "该论文探讨了神经组合优化（NCO）在车辆路径问题中的应用，提出了一种基于prompt learning的方法，以实现预训练模型的快速跨分布适应。该方法通过学习一组prompt，并在每个问题实例中选择最佳匹配的prompt来指导预训练注意力模型，从而实现zero-shot adaptation。实验结果表明，该方法不仅在分布内预测上表现优越，还在多种新任务的零样本泛化中超过了现有模型，并提供了开源代码实现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12262v1",
      "published_date": "2024-05-20 15:42:23 UTC",
      "updated_date": "2024-05-20 15:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:48:11.996984"
    },
    {
      "arxiv_id": "2405.12119v1",
      "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhankui He",
        "Zhouhang Xie",
        "Harald Steck",
        "Dawen Liang",
        "Rahul Jha",
        "Nathan Kallus",
        "Julian McAuley"
      ],
      "abstract": "Large language models (LLMs) are revolutionizing conversational recommender\nsystems by adeptly indexing item content, understanding complex conversational\ncontexts, and generating relevant item titles. However, controlling the\ndistribution of recommended items remains a challenge. This leads to suboptimal\nperformance due to the failure to capture rapidly changing data distributions,\nsuch as item popularity, on targeted conversational recommendation platforms.\nIn conversational recommendation, LLMs recommend items by generating the titles\n(as multiple tokens) autoregressively, making it difficult to obtain and\ncontrol the recommendations over all items. Thus, we propose a\nReindex-Then-Adapt (RTA) framework, which converts multi-token item titles into\nsingle tokens within LLMs, and then adjusts the probability distributions over\nthese single-token item titles accordingly. The RTA framework marries the\nbenefits of both LLMs and traditional recommender systems (RecSys):\nunderstanding complex queries as LLMs do; while efficiently controlling the\nrecommended item distributions in conversational recommendations as traditional\nRecSys do. Our framework demonstrates improved accuracy metrics across three\ndifferent conversational recommendation datasets and two adaptation settings",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在对话推荐系统中的问题，提出 Reindex-Then-Adapt (RTA) 框架，以解决推荐物品分布控制的挑战。RTA 通过将多标记物品标题转换为单标记，然后调整概率分布，结合了 LLMs 处理复杂查询的能力和传统推荐系统 (RecSys) 的高效分布控制优势。实验结果显示，该框架在三个对话推荐数据集和两种适应设置上显著提高了准确性指标。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12119v1",
      "published_date": "2024-05-20 15:37:55 UTC",
      "updated_date": "2024-05-20 15:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:48:21.925152"
    },
    {
      "arxiv_id": "2405.13064v1",
      "title": "Digital Health and Indoor Air Quality: An IoT-Driven Human-Centred Visualisation Platform for Behavioural Change and Technology Acceptance",
      "title_zh": "数字健康与室内空气质量：一个 IoT 驱动的人类中心可视化平台，用于行为改变和技术接受",
      "authors": [
        "Rameez Raja Kureshi",
        "Bhupesh Kumar Mishra",
        "Dhavalkumar Thakker",
        "Suvodeep Mazumdar",
        "Xiao Li"
      ],
      "abstract": "The detrimental effects of air pollutants on human health have prompted\nincreasing concerns regarding indoor air quality (IAQ). The emergence of\ndigital health interventions and citizen science initiatives has provided new\navenues for raising awareness, improving IAQ, and promoting behavioural\nchanges. The Technology Acceptance Model (TAM) offers a theoretical framework\nto understand user acceptance and adoption of IAQ technology. This paper\npresents a case study using the COM-B model and Internet of Things (IoT)\ntechnology to design a human-centred digital visualisation platform, leading to\nbehavioural changes and improved IAQ. The study also investigates users'\nacceptance and adoption of the technology, focusing on their experiences,\nexpectations, and the impact on IAQ. Integrating IAQ sensing, digital\nhealth-related interventions, citizen science, and the TAM model offers\nopportunities to address IAQ challenges, enhance public health, and foster\nsustainable indoor environments. The analytical results show that factors such\nas human behaviour, indoor activities, and awareness play crucial roles in\nshaping IAQ.",
      "tldr_zh": "本研究探讨了室内空气质量（IAQ）对健康的影响，并提出一个基于Internet of Things (IoT)的以人为中心的数字可视化平台，旨在通过数字健康干预和公民科学举措促进行为改变和技术接受。研究采用COM-B模型和Technology Acceptance Model (TAM)作为理论框架，设计该平台整合IAQ传感技术，帮助用户提升意识并改善室内环境。结果显示，人类行为、室内活动和意识是影响IAQ的关键因素，该平台显著提升了用户接受度和IAQ水平，为可持续的公共健康解决方案提供了新途径。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.13064v1",
      "published_date": "2024-05-20 15:13:25 UTC",
      "updated_date": "2024-05-20 15:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:48:34.886614"
    },
    {
      "arxiv_id": "2405.13062v1",
      "title": "StatAvg: Mitigating Data Heterogeneity in Federated Learning for Intrusion Detection Systems",
      "title_zh": "StatAvg：用于入侵检测系统的联邦学习中缓解数据异质性",
      "authors": [
        "Pavlos S. Bouzinis",
        "Panagiotis Radoglou-Grammatikis",
        "Ioannis Makris",
        "Thomas Lagkas",
        "Vasileios Argyriou",
        "Georgios Th. Papadopoulos",
        "Panagiotis Sarigiannidis",
        "George K. Karagiannidis"
      ],
      "abstract": "Federated learning (FL) is a decentralized learning technique that enables\nparticipating devices to collaboratively build a shared Machine Leaning (ML) or\nDeep Learning (DL) model without revealing their raw data to a third party. Due\nto its privacy-preserving nature, FL has sparked widespread attention for\nbuilding Intrusion Detection Systems (IDS) within the realm of cybersecurity.\nHowever, the data heterogeneity across participating domains and entities\npresents significant challenges for the reliable implementation of an FL-based\nIDS. In this paper, we propose an effective method called Statistical Averaging\n(StatAvg) to alleviate non-independently and identically (non-iid) distributed\nfeatures across local clients' data in FL. In particular, StatAvg allows the FL\nclients to share their individual data statistics with the server, which then\naggregates this information to produce global statistics. The latter are shared\nwith the clients and used for universal data normalisation. It is worth\nmentioning that StatAvg can seamlessly integrate with any FL aggregation\nstrategy, as it occurs before the actual FL training process. The proposed\nmethod is evaluated against baseline approaches using datasets for network and\nhost Artificial Intelligence (AI)-powered IDS. The experimental results\ndemonstrate the efficiency of StatAvg in mitigating non-iid feature\ndistributions across the FL clients compared to the baseline methods.",
      "tldr_zh": "本文提出 StatAvg 方法，用于缓解 Federated Learning (FL) 在 Intrusion Detection Systems (IDS) 中的数据异质性问题，特别是 non-iid 分布带来的挑战。StatAvg 允许客户端共享数据统计信息，服务器聚合这些信息生成全局统计，并用于统一数据标准化，从而在 FL 训练前优化数据分布。该方法可无缝集成到任何 FL 聚合策略中，实验在网络和主机 AI 驱动的 IDS 数据集上证明，StatAvg 比基线方法更有效地减轻了 non-iid 特征分布，提高了模型性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.13062v1",
      "published_date": "2024-05-20 14:41:59 UTC",
      "updated_date": "2024-05-20 14:41:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:48:46.997039"
    },
    {
      "arxiv_id": "2405.12070v1",
      "title": "AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements",
      "title_zh": "翻译失败",
      "authors": [
        "Calvin Yeung",
        "Kenjiro Ide",
        "Keisuke Fujii"
      ],
      "abstract": "Image understanding is a foundational task in computer vision, with recent\napplications emerging in soccer posture analysis. However, existing publicly\navailable datasets lack comprehensive information, notably in the form of\nposture sequences and 2D pose annotations. Moreover, current analysis models\noften rely on interpretable linear models (e.g., PCA and regression), limiting\ntheir capacity to capture non-linear spatiotemporal relationships in complex\nand diverse scenarios. To address these gaps, we introduce the 3D Shot Posture\n(3DSP) dataset in soccer broadcast videos, which represents the most extensive\nsports image dataset with 2D pose annotations to our knowledge. Additionally,\nwe present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear\napproach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,\na pipeline aimed at semi-automating 2D and 3D pose estimation and posture\nanalysis. While achieving full automation proved challenging, we provide a\nfoundational baseline, extending its utility beyond the scope of annotated\ndata. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present\nposture analysis results based on 3DSP. The dataset, code, and models are\navailable at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.",
      "tldr_zh": "该论文针对足球射门动作的姿势分析，指出现有数据集缺乏姿势序列和2D姿势注释，且当前模型（如PCA和回归）无法捕捉非线性时空关系。研究者引入了3D Shot Posture (3DSP)数据集，这是目前最大的带有2D姿势注释的体育图像数据集。论文提出3DSP-GRAE模型（一个非线性Graph Recurrent AutoEncoder方法）用于姿势序列嵌入，并开发了AutoSoccerPose管道，实现半自动化的2D和3D姿势估计及分析，提供基础基准并扩展到未标注数据。在SoccerNet和3DSP数据集上的验证显示了显著改进，数据集、代码和模型已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12070v1",
      "published_date": "2024-05-20 14:40:26 UTC",
      "updated_date": "2024-05-20 14:40:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:48:59.541934"
    },
    {
      "arxiv_id": "2405.12261v1",
      "title": "EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods",
      "title_zh": "翻译失败",
      "authors": [
        "Benedict Clark",
        "Rick Wilming",
        "Artur Dox",
        "Paul Eschenbach",
        "Sami Hached",
        "Daniel Jin Wodke",
        "Michias Taye Zewdie",
        "Uladzislau Bruila",
        "Marta Oliveira",
        "Hjalmar Schulz",
        "Luca Matteo Cornils",
        "Danny Panknin",
        "Ahcène Boubekki",
        "Stefan Haufe"
      ],
      "abstract": "The evolving landscape of explainable artificial intelligence (XAI) aims to\nimprove the interpretability of intricate machine learning (ML) models, yet\nfaces challenges in formalisation and empirical validation, being an inherently\nunsupervised process. In this paper, we bring together various benchmark\ndatasets and novel performance metrics in an initial benchmarking platform, the\nExplainable AI Comparison Toolkit (EXACT), providing a standardised foundation\nfor evaluating XAI methods. Our datasets incorporate ground truth explanations\nfor class-conditional features, and leveraging novel quantitative metrics, this\nplatform assesses the performance of post-hoc XAI methods in the quality of the\nexplanations they produce. Our recent findings have highlighted the limitations\nof popular XAI methods, as they often struggle to surpass random baselines,\nattributing significance to irrelevant features. Moreover, we show the\nvariability in explanations derived from different equally performing model\narchitectures. This initial benchmarking platform therefore aims to allow XAI\nresearchers to test and assure the high quality of their newly developed\nmethods.",
      "tldr_zh": "该论文介绍了 EXACT 平台，这是一个用于经验基准测试 Machine Learning (ML) 模型解释方法的标准化工具，旨在解决 Explainable AI (XAI) 领域中解释复杂模型的正式化和验证挑战。EXACT 整合了各种基准数据集和新型定量指标，包括 ground truth explanations for class-conditional features，以评估 post-hoc XAI 方法的解释质量。研究发现，流行 XAI 方法往往无法超越随机基线，并错误地将重要性归因于无关特征，同时揭示了不同模型架构对解释结果的变异性。该平台为 XAI 研究者提供了一个可靠的基础，以测试和确保新方法的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12261v1",
      "published_date": "2024-05-20 14:16:06 UTC",
      "updated_date": "2024-05-20 14:16:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:49:11.734016"
    },
    {
      "arxiv_id": "2405.12035v1",
      "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Sanmartin"
      ],
      "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.",
      "tldr_zh": "该论文针对大型语言模型代理 (LMAs) 在处理知识密集型任务时存在的如信息幻觉、灾难性遗忘和长上下文处理限制等问题，提出了一种KG-RAG框架，以整合结构化知识图谱 (KGs) 和大型语言模型 (LLMs) 的功能，减少对LLMs隐性知识的依赖。KG-RAG管道首先从非结构化文本构建KG，然后通过一种新颖算法Chain of Explorations (CoE)利用LLMs的推理能力顺序探索KG中的节点和关系，以实现知识图谱问答 (KGQA)。实验结果显示，在ComplexWebQuestions数据集上，KG-RAG显著降低了幻觉内容，并为开发更可靠的智能系统提供了前景。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12035v1",
      "published_date": "2024-05-20 14:03:05 UTC",
      "updated_date": "2024-05-20 14:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:49:23.183627"
    },
    {
      "arxiv_id": "2405.12001v4",
      "title": "Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hai Zhang",
        "Boyuan Zheng",
        "Tianying Ji",
        "Jinhang Liu",
        "Anqi Guo",
        "Junqiao Zhao",
        "Lanqing Li"
      ],
      "abstract": "Offline meta reinforcement learning (OMRL) has emerged as a promising\napproach for interaction avoidance and strong generalization performance by\nleveraging pre-collected data and meta-learning techniques. Previous\ncontext-based approaches predominantly rely on the intuition that alternating\noptimization between the context encoder and the policy can lead to performance\nimprovements, as long as the context encoder follows the principle of\nmaximizing the mutual information between the task variable $M$ and its latent\nrepresentation $Z$ ($I(Z;M)$) while the policy adopts the standard offline\nreinforcement learning (RL) algorithms conditioning on the learned task\nrepresentation.Despite promising results, the theoretical justification of\nperformance improvements for such intuition remains underexplored.Inspired by\nthe return discrepancy scheme in the model-based RL field, we find that the\nprevious optimization framework can be linked with the general RL objective of\nmaximizing the expected return, thereby explaining performance improvements.\nFurthermore, after scrutinizing this optimization framework, we observe that\nthe condition for monotonic performance improvements does not consider the\nvariation of the task representation. When these variations are considered, the\npreviously established condition may no longer be sufficient to ensure\nmonotonicity, thereby impairing the optimization process.We name this issue\ntask representation shift and theoretically prove that the monotonic\nperformance improvements can be guaranteed with appropriate context encoder\nupdates.Our work opens up a new avenue for OMRL, leading to a better\nunderstanding between the task representation and performance improvements.",
      "tldr_zh": "这篇论文探讨了基于上下文的离线元强化学习 (OMRL) 中的任务表示偏移 (task representation shift) 问题，指出现有优化框架虽依赖最大化任务变量 $M$ 与其潜在表示 $Z$ 之间的互信息 $I(Z;M)$ 来提升性能，但未充分考虑任务表示的变化，可能导致优化过程受损。作者通过将该框架与最大化期望回报的通用强化学习 (RL) 目标相联系，分析了性能提升的理论基础，并证明适当更新上下文编码器可以保证单调性能改进。该研究为 OMRL 提供了新视角，提升了对任务表示与性能之间关系的理解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accept at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.12001v4",
      "published_date": "2024-05-20 13:14:26 UTC",
      "updated_date": "2025-02-03 01:44:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:49:36.247931"
    },
    {
      "arxiv_id": "2407.03340v2",
      "title": "A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party Conversation",
      "title_zh": "翻译失败",
      "authors": [
        "Iveta Bečková",
        "Štefan Pócoš",
        "Giulia Belgiovine",
        "Marco Matarese",
        "Omar Eldardeer",
        "Alessandra Sciutti",
        "Carlo Mazzola"
      ],
      "abstract": "The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.",
      "tldr_zh": "该研究针对多方对话中的地址估计（addressee estimation）问题，提出了一种多模态可解释性方法，以提升人-机器人互动的准确性和透明度。论文开发了一个性能优于现有最先进模型的地址估计模型，并通过加入基于注意力的可解释性段落（attention-based segments），将其集成到 iCub 机器人的模块化认知架构中。实验验证了该模型在实时多方人-机器人互动中的有效性，并通过在线用户研究分析了不同解释方式对人类对机器人的信任和感知的影响。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.RO",
        "eess.IV",
        "I.4.8; I.2.10; I.2.9; I.2.11; J.4"
      ],
      "primary_category": "cs.AI",
      "comment": "32pp (+6pp sup.mat.) Accepted in Computer Vision and Image\n  Understanding Journal on January 23, 2025. This research received funding\n  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and\n  Development Agency, project no. APVV-21-0105",
      "pdf_url": "http://arxiv.org/pdf/2407.03340v2",
      "published_date": "2024-05-20 13:09:32 UTC",
      "updated_date": "2025-01-31 12:15:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:49:47.730793"
    },
    {
      "arxiv_id": "2405.11983v2",
      "title": "A review on the use of large language models as virtual tutors",
      "title_zh": "翻译失败",
      "authors": [
        "Silvia García-Méndez",
        "Francisco de Arriba-Pérez",
        "María del Carmen Somoza-López"
      ],
      "abstract": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.",
      "tldr_zh": "这篇综述论文探讨了Transformer架构如何推动Large Language Models (LLMs)在教育领域的应用，特别是作为虚拟导师的角色。论文提供了全面概述，聚焦于LLMs生成和评估教育材料、涉及学生和教师的设计，以及其在学生评估等应用中的潜力。研究发现，LLMs最常见的角色是自动生成问题，而GPT-3和BERT是最受欢迎的模型；此外，作者强调这是首篇此类教育应用综述，并预测随着新生成模型的涌现，将有更多相关研究涌现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11983v2",
      "published_date": "2024-05-20 12:33:42 UTC",
      "updated_date": "2024-09-05 10:01:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:49:59.306947"
    },
    {
      "arxiv_id": "2405.11982v1",
      "title": "Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space",
      "title_zh": "翻译失败",
      "authors": [
        "Qianmei Liu",
        "Yufei Kuang",
        "Jie Wang"
      ],
      "abstract": "Deep reinforcement learning (DRL) algorithms can suffer from modeling errors\nbetween the simulation and the real world. Many studies use adversarial\nlearning to generate perturbation during training process to model the\ndiscrepancy and improve the robustness of DRL. However, most of these\napproaches use a fixed parameter to control the intensity of the adversarial\nperturbation, which can lead to a trade-off between average performance and\nrobustness. In fact, finding the optimal parameter of the perturbation is\nchallenging, as excessive perturbations may destabilize training and compromise\nagent performance, while insufficient perturbations may not impart enough\ninformation to enhance robustness. To keep the training stable while improving\nrobustness, we propose a simple but effective method, namely, Adaptive\nAdversarial Perturbation (A2P), which can dynamically select appropriate\nadversarial perturbations for each sample. Specifically, we propose an adaptive\nadversarial coefficient framework to adjust the effect of the adversarial\nperturbation during training. By designing a metric for the current intensity\nof the perturbation, our method can calculate the suitable perturbation levels\nbased on the current relative performance. The appealing feature of our method\nis that it is simple to deploy in real-world applications and does not require\naccessing the simulator in advance. The experiments in MuJoCo show that our\nmethod can improve the training stability and learn a robust policy when\nmigrated to different test environments. The code is available at\nhttps://github.com/Lqm00/A2P-SAC.",
      "tldr_zh": "该论文针对 Deep Reinforcement Learning (DRL) 在模拟与现实环境间建模错误的问题，提出了一种自适应对抗扰动方法（A2P），通过动态调整对抗扰动系数来平衡训练稳定性和鲁棒性，避免了固定参数导致的性能权衡。A2P 框架根据当前相对性能计算合适的扰动强度，确保训练过程不被过度干扰，同时增强代理对环境变化的适应能力。实验结果显示，在 MuJoCo 环境中，该方法显著提高了策略的鲁棒性，并在迁移到不同测试环境中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11982v1",
      "published_date": "2024-05-20 12:31:11 UTC",
      "updated_date": "2024-05-20 12:31:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:50:12.608727"
    },
    {
      "arxiv_id": "2405.13059v1",
      "title": "RNG: Reducing Multi-level Noise and Multi-grained Semantic Gap for Joint Multimodal Aspect-Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yaxin Liu",
        "Yan Zhou",
        "Ziming Li",
        "Jinchuan Zhang",
        "Yu Shang",
        "Chenyang Zhang",
        "Songlin Hu"
      ],
      "abstract": "As an important multimodal sentiment analysis task, Joint Multimodal\nAspect-Sentiment Analysis (JMASA), aiming to jointly extract aspect terms and\ntheir associated sentiment polarities from the given text-image pairs, has\ngained increasing concerns. Existing works encounter two limitations: (1)\nmulti-level modality noise, i.e., instance- and feature-level noise; and (2)\nmulti-grained semantic gap, i.e., coarse- and fine-grained gap. Both issues may\ninterfere with accurate identification of aspect-sentiment pairs. To address\nthese limitations, we propose a novel framework named RNG for JMASA.\nSpecifically, to simultaneously reduce multi-level modality noise and\nmulti-grained semantic gap, we design three constraints: (1) Global Relevance\nConstraint (GR-Con) based on text-image similarity for instance-level noise\nreduction, (2) Information Bottleneck Constraint (IB-Con) based on the\nInformation Bottleneck (IB) principle for feature-level noise reduction, and\n(3) Semantic Consistency Constraint (SC-Con) based on mutual information\nmaximization in a contrastive learning way for multi-grained semantic gap\nreduction. Extensive experiments on two datasets validate our new\nstate-of-the-art performance.",
      "tldr_zh": "该论文针对 Joint Multimodal Aspect-Sentiment Analysis (JMASA) 任务，提出了一种名为 RNG 的新框架，以解决多级模式噪声（如实例级和特征级噪声）和多粒度语义差距（如粗粒度和细粒度差距）的问题。RNG 通过设计三个约束来优化性能：Global Relevance Constraint (GR-Con) 基于文本-图像相似性减少实例级噪声、Information Bottleneck Constraint (IB-Con) 利用 Information Bottleneck (IB) 原理减少特征级噪声，以及 Semantic Consistency Constraint (SC-Con) 通过对比学习方式最大化互信息来缩小多粒度语义差距。实验在两个数据集上验证了 RNG 的有效性，实现了新的 state-of-the-art 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICME 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.13059v1",
      "published_date": "2024-05-20 12:18:46 UTC",
      "updated_date": "2024-05-20 12:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:50:24.466822"
    },
    {
      "arxiv_id": "2405.11978v1",
      "title": "SM-DTW: Stability Modulated Dynamic Time Warping for signature verification",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Parziale",
        "Moises Diaz",
        "Miguel A. Ferrer",
        "Angelo Marcelli"
      ],
      "abstract": "Building upon findings in computational model of handwriting learning and\nexecution, we introduce the concept of stability to explain the difference\nbetween the actual movements performed during multiple execution of the\nsubject's signature, and conjecture that the most stable parts of the signature\nshould play a paramount role in evaluating the similarity between a questioned\nsignature and the reference ones during signature verification. We then\nintroduce the Stability Modulated Dynamic Time Warping algorithm for\nincorporating the stability regions, i.e. the most similar parts between two\nsignatures, into the distance measure between a pair of signatures computed by\nthe Dynamic Time Warping for signature verification. Experiments were conducted\non two datasets largely adopted for performance evaluation. Experimental\nresults show that the proposed algorithm improves the performance of the\nbaseline system and compares favourably with other top performing signature\nverification systems.",
      "tldr_zh": "这篇论文引入了“稳定性”概念，基于手写学习模型解释签名执行中的差异，并强调最稳定的部分在签名验证中起关键作用。作者提出 Stability Modulated Dynamic Time Warping (SM-DTW) 算法，将稳定性区域（两个签名间最相似的部分）整合到 Dynamic Time Warping (DTW) 的距离度量中，以提升签名验证的准确性。在两个常用数据集上的实验显示，SM-DTW 算法显著提高了基线系统的性能，并与其它顶级签名验证系统相比表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11978v1",
      "published_date": "2024-05-20 12:18:15 UTC",
      "updated_date": "2024-05-20 12:18:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:50:35.622762"
    },
    {
      "arxiv_id": "2405.11968v3",
      "title": "Conditional Shift-Robust Conformal Prediction for Graph Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "S. Akansha"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as potent tools for predicting\noutcomes in graph-structured data. Despite their efficacy, a significant\ndrawback of GNNs lies in their limited ability to provide robust uncertainty\nestimates, posing challenges to their reliability in contexts where errors\ncarry significant consequences. Moreover, GNNs typically excel in\nin-distribution settings, assuming that training and test data follow identical\ndistributions a condition often unmet in real world graph data scenarios. In\nthis article, we leverage conformal prediction, a widely recognized statistical\ntechnique for quantifying uncertainty by transforming predictive model outputs\ninto prediction sets, to address uncertainty quantification in GNN predictions\namidst conditional shift\\footnote{Representing the change in conditional\nprobability distribution \\(P(label|input)\\) from source domain to target\ndomain.} in graph-based semi-supervised learning (SSL). Additionally, we\npropose a novel loss function aimed at refining model predictions by minimizing\nconditional shift in latent stages. Termed Conditional Shift Robust (CondSR)\nconformal prediction for GNNs, our approach CondSR is model-agnostic and\nadaptable to various classification models. We validate the effectiveness of\nour method on standard graph benchmark datasets, integrating it with\nstate-of-the-art GNNs in node classification tasks. Comprehensive evaluations\ndemonstrate that our approach consistently achieves any predefined target\nmarginal coverage, enhances the accuracy of state of the art GNN models by up\nto 12\\% under conditional shift, and reduces the prediction set size by up to\n48\\%. The code implementation is publicly available for further exploration and\nexperimentation.",
      "tldr_zh": "这篇论文针对 Graph Neural Networks (GNNs) 在条件偏移 (conditional shift) 下的不确定性估计问题，提出了 Conditional Shift Robust (CondSR) conformal prediction 方法，以提升 GNNs 在图结构半监督学习 (SSL) 中的鲁棒性。CondSR 通过一个新颖的损失函数最小化潜在阶段的条件偏移，并以模型无关的方式与各种分类模型集成。实验结果显示，该方法在标准图基准数据集上实现了预定义的目标边际覆盖率，提高了最先进 GNNs 的准确率高达 12%，并将预测集大小减少高达 48%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.11968v3",
      "published_date": "2024-05-20 11:47:31 UTC",
      "updated_date": "2025-03-25 08:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:50:49.905289"
    },
    {
      "arxiv_id": "2405.11967v1",
      "title": "Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home",
      "title_zh": "支持成年人群在家自我管理心血管疾病风险因素的推荐算法",
      "authors": [
        "Tatiana V. Afanasieva",
        "Pavel V. Platov",
        "Anastasia I. Medvedeva"
      ],
      "abstract": "One of the new trends in the development of recommendation algorithms is the\ndissemination of their capabilities to support the population in managing their\nhealth. This article focuses on the problem of improving the effectiveness of\ncardiovascular diseases (CVD) prevention, since CVD is the leading cause of\ndeath worldwide. To address this issue, a knowledge-based recommendation\nalgorithm was proposed to support self-management of CVD risk factors in adults\nat home. The proposed algorithm is based on the original multidimensional\nrecommendation model and on a new user profile model, which includes predictive\nassessments of CVD health in addition to its current ones as outlined in\nofficial guidelines. The main feature of the proposed algorithm is the\ncombination of rule-based logic with the capabilities of a large language model\nin generating human-like text for explanatory component of multidimensional\nrecommendation. The verification and evaluation of the proposed algorithm\nshowed the usefulness of the proposed recommendation algorithm for supporting\nadults in self-management of their CVD risk factors at home. As follows from\nthe comparison with similar knowledge-based recommendation algorithms, the\nproposed algorithm evaluates a larger number of CVD risk factors and has a\ngreater information and semantic capacity of the generated recommendations.",
      "tldr_zh": "本研究提出了一种基于知识的推荐算法，旨在支持成年人在家中自我管理心血管疾病 (CVD) 风险因素，以提升 CVD 预防效果。算法结合原创的多维推荐模型和新的用户配置文件模型，包括当前和预测的 CVD 健康评估，并融合规则-based 逻辑与大型语言模型 (LLM) 生成人性化的解释性文本。实验验证显示，该算法在支持自我管理方面实用有效，与类似算法相比，能评估更多 CVD 风险因素并提供更丰富的信息和语义容量。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2, J.3"
      ],
      "primary_category": "cs.IR",
      "comment": "26 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.11967v1",
      "published_date": "2024-05-20 11:47:19 UTC",
      "updated_date": "2024-05-20 11:47:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:50:59.990235"
    },
    {
      "arxiv_id": "2405.13058v2",
      "title": "The AI Community Building the Future? A Quantitative Analysis of Development Activity on Hugging Face Hub",
      "title_zh": "翻译失败",
      "authors": [
        "Cailean Osborne",
        "Jennifer Ding",
        "Hannah Rose Kirk"
      ],
      "abstract": "Open model developers have emerged as key actors in the political economy of\nartificial intelligence (AI), but we still have a limited understanding of\ncollaborative practices in the open AI ecosystem. This paper responds to this\ngap with a three-part quantitative analysis of development activity on the\nHugging Face (HF) Hub, a popular platform for building, sharing, and\ndemonstrating models. First, various types of activity across 348,181 model,\n65,761 dataset, and 156,642 space repositories exhibit right-skewed\ndistributions. Activity is extremely imbalanced between repositories; for\nexample, over 70% of models have 0 downloads, while 1% account for 99% of\ndownloads. Furthermore, licenses matter: there are statistically significant\ndifferences in collaboration patterns in model repositories with permissive,\nrestrictive, and no licenses. Second, we analyse a snapshot of the social\nnetwork structure of collaboration in model repositories, finding that the\ncommunity has a core-periphery structure, with a core of prolific developers\nand a majority of isolate developers (89%). Upon removing the isolate\ndevelopers from the network, collaboration is characterised by high reciprocity\nregardless of developers' network positions. Third, we examine model adoption\nthrough the lens of model usage in spaces, finding that a minority of models,\ndeveloped by a handful of companies, are widely used on the HF Hub. Overall,\nactivity on the HF Hub is characterised by Pareto distributions, congruent with\nOSS development patterns on platforms like GitHub. We conclude with\nrecommendations for researchers, companies, and policymakers to advance our\nunderstanding of open AI development.",
      "tldr_zh": "本研究通过对 Hugging Face Hub 的定量分析，探讨了开源 AI 生态中的开发活动和协作实践，包括模型、数据集和空间仓库的活动分布。结果显示，活动呈现右偏斜分布和高度不均衡，例如70%以上的模型下载量为0，而1%的模型占99%的下载量；此外，许可证类型（如 permissive 或 restrictive）显著影响协作模式，社区网络结构为核心-外围型，且协作表现出高互惠性。研究还发现，模型采用高度集中，由少数公司主导，整体符合 Pareto distributions 和 GitHub 上的 OSS 发展模式，并为研究者、企业和政策制定者提供推进开源 AI 发展的建议。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "K.4.1"
      ],
      "primary_category": "cs.SE",
      "comment": "27 pages, 5 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.13058v2",
      "published_date": "2024-05-20 11:10:49 UTC",
      "updated_date": "2024-06-05 15:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:51:12.094707"
    },
    {
      "arxiv_id": "2405.13057v1",
      "title": "Can Github issues be solved with Tree Of Thoughts?",
      "title_zh": "GitHub issues 是否能用 Tree Of Thoughts 解决？",
      "authors": [
        "Ricardo La Rosa",
        "Corey Hulse",
        "Bangdi Liu"
      ],
      "abstract": "While there have been extensive studies in code generation by large language\nmodels (LLM), where benchmarks like HumanEval have been surpassed with an\nimpressive 96.3% success rate, these benchmarks predominantly judge a model's\nperformance on basic function-level code generation and lack the critical\nthinking and concept of scope required of real-world scenarios such as solving\nGitHub issues. This research introduces the application of the Tree of Thoughts\n(ToT) language model reasoning framework for enhancing the decision-making and\nproblem-solving abilities of LLMs for this complex task. Compared to\ntraditional input-output (IO) prompting and Retrieval Augmented Generation\n(RAG) techniques, ToT is designed to improve performance by facilitating a\nstructured exploration of multiple reasoning trajectories and enabling\nself-assessment of potential solutions. We experimentally deploy ToT in\ntackling a Github issue contained within an instance of the SWE-bench. However,\nour results reveal that the ToT framework alone is not enough to give LLMs the\ncritical reasoning capabilities to outperform existing methods. In this paper\nwe analyze the potential causes of these shortcomings and identify key areas\nfor improvement such as deepening the thought process and introducing agentic\ncapabilities. The insights of this research are aimed at informing future\ndirections for refining the application of ToT and better harnessing the\npotential of LLMs in real-world problem-solving scenarios.",
      "tldr_zh": "本研究探讨了是否能使用 Tree of Thoughts (ToT) 框架来提升大型语言模型 (LLM) 在解决 GitHub issues 的决策和问题解决能力，因为现有基准如 HumanEval 虽在基本代码生成上达到96.3%的成功率，但忽略了真实场景的批判性思考和范围概念。ToT 通过结构化探索多个推理路径和自我评估，与传统 Input-Output (IO) 提示和 Retrieval Augmented Generation (RAG) 技术相比，旨在提供更有效的框架。实验在 SWE-bench 的 GitHub issue 上进行，结果显示 ToT 单独使用不足以超越现有方法。论文分析了这些不足的原因，如缺乏深度思考过程和代理能力 (agentic capabilities)，并提出未来改进方向，以更好地利用 LLM 在真实世界问题解决中的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 2 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.13057v1",
      "published_date": "2024-05-20 11:05:56 UTC",
      "updated_date": "2024-05-20 11:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:51:26.129975"
    },
    {
      "arxiv_id": "2405.11937v1",
      "title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Kamil Guttmann",
        "Mikołaj Pokrywka",
        "Adrian Charkiewicz",
        "Artur Nowakowski"
      ],
      "abstract": "This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.",
      "tldr_zh": "该论文探讨了利用 Minimum Bayes Risk (MBR) 解码来实现机器翻译 (MT) 的自我改进，特别针对领域适应和低资源语言。研究通过在 MBR 解码的正向翻译上微调模型，并以 COMET 作为实用性指标，进行翻译重新排序和迭代应用，以更好地符合人类偏好。结果显示，这种方法显著提升了所有语言对的翻译质量，并在领域适应模型和低资源环境中表现出良好的泛化潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EAMT 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11937v1",
      "published_date": "2024-05-20 10:25:03 UTC",
      "updated_date": "2024-05-20 10:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:51:35.642675"
    },
    {
      "arxiv_id": "2405.11928v3",
      "title": "\"Set It Up!\": Functional Object Arrangement with Compositional Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yiqing Xu",
        "Jiayuan Mao",
        "Yilun Du",
        "Tomas Lozáno-Pérez",
        "Leslie Pack Kaelbling",
        "David Hsu"
      ],
      "abstract": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.",
      "tldr_zh": "这篇论文探讨了机器人理解模糊指令（如“set up a dining table for two”）来创建功能性物体排列的挑战，引入了SetItUp框架来处理此类问题。SetItUp框架利用少量训练样本和人类设计的程序草图，通过中间图表示抽象空间关系，将任务分解为从有限数据学习排列模式，以及利用大型语言模型(LLMs)提出空间约束并结合扩散模型库来生成满足这些约束的物体姿势。实验结果显示，该框架在包含书桌、餐桌和咖啡桌的数据集上，生成物理上合理、功能性和美观的物体排列，表现优于现有模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages main paper, 21 pages appendix, RSS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11928v3",
      "published_date": "2024-05-20 10:06:33 UTC",
      "updated_date": "2025-05-09 15:43:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:51:49.067882"
    },
    {
      "arxiv_id": "2405.11919v2",
      "title": "On Efficient and Statistical Quality Estimation for Data Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Jan-Christoph Klie",
        "Juan Haladjian",
        "Marc Kirchner",
        "Rahul Nair"
      ],
      "abstract": "Annotated datasets are an essential ingredient to train, evaluate, compare\nand productionalize supervised machine learning models. It is therefore\nimperative that annotations are of high quality. For their creation, good\nquality management and thereby reliable quality estimates are needed. Then, if\nquality is insufficient during the annotation process, rectifying measures can\nbe taken to improve it. Quality estimation is often performed by having experts\nmanually label instances as correct or incorrect. But checking all annotated\ninstances tends to be expensive. Therefore, in practice, usually only subsets\nare inspected; sizes are chosen mostly without justification or regard to\nstatistical power and more often than not, are relatively small. Basing\nestimates on small sample sizes, however, can lead to imprecise values for the\nerror rate. Using unnecessarily large sample sizes costs money that could be\nbetter spent, for instance on more annotations. Therefore, we first describe in\ndetail how to use confidence intervals for finding the minimal sample size\nneeded to estimate the annotation error rate. Then, we propose applying\nacceptance sampling as an alternative to error rate estimation We show that\nacceptance sampling can reduce the required sample sizes up to 50% while\nproviding the same statistical guarantees.",
      "tldr_zh": "这篇论文探讨了数据标注质量估计的效率问题，强调在监督机器学习模型训练中，确保标注准确性需要可靠的统计方法，但传统手动检查子集样本往往样本大小不当，导致错误率估计不精确或资源浪费。作者首先详细说明了使用 confidence intervals 计算最小样本大小，以精确估计标注错误率。论文进一步提出采用 acceptance sampling 作为替代方法，结果显示这种方法能将所需样本大小减少多达 50%，同时提供相同的统计保证，从而提高数据标注过程的效率和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11919v2",
      "published_date": "2024-05-20 09:57:29 UTC",
      "updated_date": "2024-05-29 06:43:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:52:01.031806"
    },
    {
      "arxiv_id": "2405.11911v2",
      "title": "Accurate Link Prediction for Edge-Incomplete Graphs via PU Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Junghun Kim",
        "Ka Hyun Park",
        "Hoyoung Yoon",
        "U Kang"
      ],
      "abstract": "Given an edge-incomplete graph, how can we accurately find the missing links?\nThe link prediction in edge-incomplete graphs aims to discover the missing\nrelations between entities when their relationships are represented as a graph.\nEdge-incomplete graphs are prevalent in real-world due to practical\nlimitations, such as not checking all users when adding friends in a social\nnetwork. Addressing the problem is crucial for various tasks, including\nrecommending friends in social networks and finding references in citation\nnetworks. However, previous approaches rely heavily on the given\nedge-incomplete (observed) graph, making it challenging to consider the missing\n(unobserved) links during training. In this paper, we propose PULL\n(PU-Learning-based Link predictor), an accurate link prediction method based on\nthe positive-unlabeled (PU) learning. PULL treats the observed edges in the\ntraining graph as positive examples, and the unconnected node pairs as\nunlabeled ones. PULL effectively prevents the link predictor from overfitting\nto the observed graph by proposing latent variables for every edge, and\nleveraging the expected graph structure with respect to the variables.\nExtensive experiments on five real-world datasets show that PULL consistently\noutperforms the baselines for predicting links in edge-incomplete graphs.",
      "tldr_zh": "该论文针对边不完整的图，提出了一种基于 PU Learning 的链接预测方法 PULL，以准确发现缺失链接。PULL 将观察到的边视为正例，未连接的节点对视为未标记数据，通过引入潜在变量避免模型过拟合观察图，并利用期望图结构进行优化。实验结果显示，在五个真实数据集上，PULL 比基线模型表现出色，显著提升了链接预测的准确性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI'25",
      "pdf_url": "http://arxiv.org/pdf/2405.11911v2",
      "published_date": "2024-05-20 09:47:22 UTC",
      "updated_date": "2024-12-12 07:17:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:52:13.048729"
    },
    {
      "arxiv_id": "2406.00012v2",
      "title": "FINED: Feed Instance-Wise Information Need with Essential and Disentangled Parametric Knowledge from the Past",
      "title_zh": "翻译失败",
      "authors": [
        "Kounianhua Du",
        "Jizheng Chen",
        "Jianghao Lin",
        "Menghui Zhu",
        "Bo Chen",
        "Shuai Li",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "Recommender models play a vital role in various industrial scenarios, while\noften faced with the catastrophic forgetting problem caused by the fast\nshifting data distribution. To alleviate this problem, a common approach is to\nreuse knowledge from the historical data. However, preserving the vast and\nfast-accumulating data is hard, which causes dramatic storage overhead.\nMemorizing old data through a parametric knowledge base is then proposed, which\ncompresses the vast amount of raw data into model parameters. Despite the\nflexibility, how to improve the memorization and generalization capabilities of\nthe parametric knowledge base and suit the flexible information need of each\ninstance are challenging. In this paper, we propose FINED to Feed INstance-wise\ninformation need with Essential and Disentangled parametric knowledge from past\ndata for recommendation enhancement. Concretely, we train a knowledge extractor\nthat extracts knowledge patterns of arbitrary order from past data and a\nknowledge encoder that memorizes the arbitrary order patterns, which serves as\nthe retrieval key generator and memory network respectively in the following\nknowledge reusing phase. The whole process is regularized by the proposed two\nconstraints, which improve the capabilities of the parametric knowledge base\nwithout increasing the size of it. The essential principle helps to compress\nthe input into representative vectors that capture the task-relevant\ninformation and filter out the noisy information. The disentanglement principle\nreduces the redundancy of stored information and pushes the knowledge base to\nfocus on capturing the disentangled invariant patterns. These two rules\ntogether promote rational compression of information for robust and generalized\nknowledge representations. Extensive experiments on two datasets justify the\neffectiveness of the proposed method.",
      "tldr_zh": "该研究针对推荐模型的灾难性遗忘问题（catastrophic forgetting），提出 FINED 方法，通过从历史数据中提取本质和解耦的参数知识（Essential and Disentangled Parametric Knowledge），以适应每个实例的灵活信息需求。FINED 包括一个知识提取器，用于提取任意顺序的知识模式，以及一个知识编码器，用于记忆这些模式，并分别作为检索键生成器和记忆网络。论文引入本质原则（压缩输入、过滤噪音）和解耦原则（减少冗余、聚焦不变模式）的约束，提升了参数知识库的记忆和泛化能力，而不增加其规模。在两个数据集上的广泛实验证明，FINED 有效提升了推荐性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00012v2",
      "published_date": "2024-05-20 09:24:45 UTC",
      "updated_date": "2024-10-18 06:07:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:52:26.384230"
    },
    {
      "arxiv_id": "2405.11891v1",
      "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
      "title_zh": "揭示与操纵大型语言模型中的提示影响",
      "authors": [
        "Zijian Feng",
        "Hanzhang Zhou",
        "Zixiao Zhu",
        "Junlang Qian",
        "Kezhi Mao"
      ],
      "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models\n(LLMs). However, the intricate role of individual tokens in prompts, known as\ninput saliency, in shaping the responses remains largely underexplored.\nExisting saliency methods either misalign with LLM generation objectives or\nrely heavily on linearity assumptions, leading to potential inaccuracies. To\naddress this, we propose Token Distribution Dynamics (TDD), a\n\\textcolor{black}{simple yet effective} approach to unveil and manipulate the\nrole of prompts in generating LLM outputs. TDD leverages the robust\ninterpreting capabilities of the language model head (LM head) to assess input\nsaliency. It projects input tokens into the embedding space and then estimates\ntheir significance based on distribution dynamics over the vocabulary. We\nintroduce three TDD variants: forward, backward, and bidirectional, each\noffering unique insights into token relevance. Extensive experiments reveal\nthat the TDD surpasses state-of-the-art baselines with a big margin in\nelucidating the causal relationships between prompts and LLM outputs. Beyond\nmere interpretation, we apply TDD to two prompt manipulation tasks for\ncontrolled text generation: zero-shot toxic language suppression and sentiment\nsteering. Empirical results underscore TDD's proficiency in identifying both\ntoxic and sentimental cues in prompts, subsequently mitigating toxicity or\nmodulating sentiment in the generated content.",
      "tldr_zh": "这篇论文探讨了提示词（Prompts）在大型语言模型（LLMs）中的影响，提出了一种名为 Token Distribution Dynamics (TDD) 的简单有效方法来揭示输入显著性（input saliency），通过语言模型头（LM head）评估标记在嵌入空间中的分布动态，并引入 forward、backward 和 bidirectional 三种变体。TDD 显著超越现有基线，在解释提示与输出之间的因果关系上表现出色。论文进一步将 TDD 应用于提示操纵任务，包括零样本有毒语言抑制和情感导向，实验结果证明其能有效识别并缓解有毒线索或调整生成内容的 sentiment。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11891v1",
      "published_date": "2024-05-20 09:15:36 UTC",
      "updated_date": "2024-05-20 09:15:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:52:37.986802"
    },
    {
      "arxiv_id": "2405.11881v3",
      "title": "Out-of-Distribution Detection with a Single Unconditional Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Alvin Heng",
        "Alexandre H. Thiery",
        "Harold Soh"
      ],
      "abstract": "Out-of-distribution (OOD) detection is a critical task in machine learning\nthat seeks to identify abnormal samples. Traditionally, unsupervised methods\nutilize a deep generative model for OOD detection. However, such approaches\nrequire a new model to be trained for each inlier dataset. This paper explores\nwhether a single model can perform OOD detection across diverse tasks. To that\nend, we introduce Diffusion Paths (DiffPath), which uses a single diffusion\nmodel originally trained to perform unconditional generation for OOD detection.\nWe introduce a novel technique of measuring the rate-of-change and curvature of\nthe diffusion paths connecting samples to the standard normal. Extensive\nexperiments show that with a single model, DiffPath is competitive with prior\nwork using individual models on a variety of OOD tasks involving different\ndistributions. Our code is publicly available at\nhttps://github.com/clear-nus/diffpath.",
      "tldr_zh": "本研究探讨了 Out-of-Distribution (OOD) 检测问题，提出了一种名为 Diffusion Paths (DiffPath) 的方法，使用单一无条件扩散模型来识别异常样本，从而避免传统方法需为每个 inlier 数据集训练新模型的局限性。DiffPath 通过测量扩散路径（从样本连接到标准正态分布）的变化率和曲率，来评估样本的异常程度。实验结果显示，该方法在多种 OOD 任务中，与使用独立模型的现有工作相比，表现出竞争力的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11881v3",
      "published_date": "2024-05-20 08:54:03 UTC",
      "updated_date": "2024-10-24 02:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:52:49.051976"
    },
    {
      "arxiv_id": "2405.11880v1",
      "title": "Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs",
      "title_zh": "量化 LLMs 中的语境内推理效果和记忆化效果",
      "authors": [
        "Siyu Lou",
        "Yuntian Chen",
        "Xiaodan Liang",
        "Liang Lin",
        "Quanshi Zhang"
      ],
      "abstract": "In this study, we propose an axiomatic system to define and quantify the\nprecise memorization and in-context reasoning effects used by the large\nlanguage model (LLM) for language generation. These effects are formulated as\nnon-linear interactions between tokens/words encoded by the LLM. Specifically,\nthe axiomatic system enables us to categorize the memorization effects into\nfoundational memorization effects and chaotic memorization effects, and further\nclassify in-context reasoning effects into enhanced inference patterns,\neliminated inference patterns, and reversed inference patterns. Besides, the\ndecomposed effects satisfy the sparsity property and the universal matching\nproperty, which mathematically guarantee that the LLM's confidence score can be\nfaithfully decomposed into the memorization effects and in-context reasoning\neffects. Experiments show that the clear disentanglement of memorization\neffects and in-context reasoning effects enables a straightforward examination\nof detailed inference patterns encoded by LLMs.",
      "tldr_zh": "本研究提出一个公理系统，用于定义和量化大语言模型（LLMs）在语言生成中的记忆效应（memorization effects）和上下文推理效应（in-context reasoning effects），这些效应被表述为令牌/单词之间的非线性交互。系统将记忆效应分类为基础记忆效应和混乱记忆效应，并将上下文推理效应分为增强推理模式、消除推理模式和反转推理模式；此外，这些效应满足稀疏性（sparsity property）和通用匹配属性（universal matching property），确保LLMs的置信分数能被忠实地分解。实验结果表明，这种清晰分离允许直接检查LLMs编码的详细推理模式。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11880v1",
      "published_date": "2024-05-20 08:51:03 UTC",
      "updated_date": "2024-05-20 08:51:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:53:01.598716"
    },
    {
      "arxiv_id": "2405.11877v5",
      "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus",
      "title_zh": "翻译失败",
      "authors": [
        "Eduard Poesina",
        "Cornelia Caragea",
        "Radu Tudor Ionescu"
      ],
      "abstract": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.",
      "tldr_zh": "本文介绍了 RoNLI，这是第一个公开的罗马尼亚语 Natural Language Inference (NLI) 语料库，包含 58K 通过 distant supervision 获得的训练句对，以及 6K 手动标注的验证和测试句对，以填补该语言在 NLI 任务中的空白。作者进行了多种机器学习实验，包括基于词嵌入的浅层模型和 transformer-based neural networks，建立了一系列竞争性的基线模型。进一步，他们提出了一种基于 data cartography 的新 curriculum learning 策略，显著提升了最佳模型的性能；数据集和代码已在 GitHub 上提供，以促进后续研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024 (Main)",
      "pdf_url": "http://arxiv.org/pdf/2405.11877v5",
      "published_date": "2024-05-20 08:41:15 UTC",
      "updated_date": "2024-10-18 13:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:53:13.502278"
    },
    {
      "arxiv_id": "2405.11870v2",
      "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process",
      "title_zh": "直观微调：向着将对齐简化成单一过程",
      "authors": [
        "Ermo Hua",
        "Biqing Qi",
        "Kaiyan Zhang",
        "Yue Yu",
        "Ning Ding",
        "Xingtai Lv",
        "Kai Tian",
        "Bowen Zhou"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two\nfundamental processes for enhancing the capabilities of Language Models (LMs)\npost pre-training, aligning them better with human preferences. Although SFT\nadvances in training efficiency, PO delivers better alignment, thus they are\noften combined. However, common practices simply apply them sequentially\nwithout integrating their optimization objectives, ignoring the opportunities\nto bridge their paradigm gap and take the strengths from both. To obtain a\nunified understanding, we interpret SFT and PO with two sub-processes --\nPreference Estimation and Transition Optimization -- defined at token level\nwithin the Markov Decision Process (MDP) framework. This modeling shows that\nSFT is only a specialized case of PO with inferior estimation and optimization.\nPO evaluates the quality of model's entire generated answer, whereas SFT only\nscores predicted tokens based on preceding tokens from target answers.\nTherefore, SFT overestimates the ability of model, leading to inferior\noptimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT)\nto integrate SFT and Preference Optimization into a single process. IFT\ncaptures LMs' intuitive sense of the entire answers through a temporal residual\nconnection, but it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to sequential recipes of SFT and some typical\nPreference Optimization methods across several tasks, particularly those\nrequires generation, reasoning, and fact-following abilities. An explainable\nFrozen Lake game further validates the effectiveness of IFT for getting\ncompetitive policy.",
      "tldr_zh": "本文提出 Intuitive Fine-Tuning (IFT) 方法，将 Supervised Fine-Tuning (SFT) 和 Preference Optimization (PO) 整合为单一过程，旨在简化语言模型 (LMs) 的对齐优化。通过 Markov Decision Process (MDP) 框架分析，作者发现 SFT 是 PO 的特殊情况，因为 SFT 只基于前置 token 评分而非整个答案，导致优化不足。IFT 通过 temporal residual connection 捕捉模型对完整答案的直觉，仅需与 SFT 相同量的非偏好标签数据，便在生成、推理和事实遵循任务上表现出色或优于传统序列方法，并通过 Frozen Lake 游戏验证其竞争性政策。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11870v2",
      "published_date": "2024-05-20 08:23:28 UTC",
      "updated_date": "2024-05-28 16:14:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:53:27.127740"
    },
    {
      "arxiv_id": "2405.11868v1",
      "title": "Towards Graph Contrastive Learning: A Survey and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Ju",
        "Yifan Wang",
        "Yifang Qin",
        "Zhengyang Mao",
        "Zhiping Xiao",
        "Junyu Luo",
        "Junwei Yang",
        "Yiyang Gu",
        "Dongjie Wang",
        "Qingqing Long",
        "Siyu Yi",
        "Xiao Luo",
        "Ming Zhang"
      ],
      "abstract": "In recent years, deep learning on graphs has achieved remarkable success in\nvarious domains. However, the reliance on annotated graph data remains a\nsignificant bottleneck due to its prohibitive cost and time-intensive nature.\nTo address this challenge, self-supervised learning (SSL) on graphs has gained\nincreasing attention and has made significant progress. SSL enables machine\nlearning models to produce informative representations from unlabeled graph\ndata, reducing the reliance on expensive labeled data. While SSL on graphs has\nwitnessed widespread adoption, one critical component, Graph Contrastive\nLearning (GCL), has not been thoroughly investigated in the existing\nliterature. Thus, this survey aims to fill this gap by offering a dedicated\nsurvey on GCL. We provide a comprehensive overview of the fundamental\nprinciples of GCL, including data augmentation strategies, contrastive modes,\nand contrastive optimization objectives. Furthermore, we explore the extensions\nof GCL to other aspects of data-efficient graph learning, such as weakly\nsupervised learning, transfer learning, and related scenarios. We also discuss\npractical applications spanning domains such as drug discovery, genomics\nanalysis, recommender systems, and finally outline the challenges and potential\nfuture directions in this field.",
      "tldr_zh": "这篇论文针对图深度学习的挑战，进行了一个关于 Graph Contrastive Learning (GCL) 的全面调查，旨在解决对标注数据的依赖问题。论文详细阐述了 GCL 的基本原则，包括数据增强策略、对比模式和对比优化目标，以实现自监督学习 (SSL) 在图数据上的高效表示。调查还扩展了 GCL 到弱监督学习、转移学习等场景，并讨论了其在药物发现、基因组分析和推荐系统等领域的实际应用。最后，论文指出了该领域的关键挑战和未来研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11868v1",
      "published_date": "2024-05-20 08:19:10 UTC",
      "updated_date": "2024-05-20 08:19:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:53:37.845972"
    },
    {
      "arxiv_id": "2405.11865v1",
      "title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Rueda",
        "Elena Álvarez Mellado",
        "Constantine Lignos"
      ],
      "abstract": "Modern named entity recognition systems have steadily improved performance in\nthe age of larger and more powerful neural models. However, over the past\nseveral years, the state-of-the-art has seemingly hit another plateau on the\nbenchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into\nthe test outputs of the highest-performing NER models, conducting a\nfine-grained evaluation of their performance by introducing new document-level\nannotations on the test set. We go beyond F1 scores by categorizing errors in\norder to interpret the true state of the art for NER and guide future work. We\nreview previous attempts at correcting the various flaws of the test set and\nintroduce CoNLL#, a new corrected version of the test set that addresses its\nsystematic and most prevalent errors, allowing for low-noise, interpretable\nerror analysis.",
      "tldr_zh": "本研究分析了现代命名实体识别 (NER) 系统在 CoNLL-03 English 数据集上的性能停滞问题，通过对最高性能模型的测试输出进行细粒度错误评估，并引入新的文档级注解来分类和解读错误类型。作者审视了以往数据集修正尝试，并提出 CoNLL#，一个修正版测试集，针对其系统性和常见错误进行优化，以实现低噪声、可解释的错误分析。CoNLL# 的引入有助于更准确地评估 NER 模型，并为未来研究提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11865v1",
      "published_date": "2024-05-20 08:16:34 UTC",
      "updated_date": "2024-05-20 08:16:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:53:49.750882"
    },
    {
      "arxiv_id": "2406.00011v2",
      "title": "DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Kounianhua Du",
        "Jizheng Chen",
        "Jianghao Lin",
        "Yunjia Xi",
        "Hangyu Wang",
        "Xinyi Dai",
        "Bo Chen",
        "Ruiming Tang",
        "Weinan Zhang"
      ],
      "abstract": "Recommender systems play important roles in various applications such as\ne-commerce, social media, etc. Conventional recommendation methods usually\nmodel the collaborative signals within the tabular representation space.\nDespite the personalization modeling and the efficiency, the latent semantic\ndependencies are omitted. Methods that introduce semantics into recommendation\nthen emerge, injecting knowledge from the semantic representation space where\nthe general language understanding are compressed. However, existing\nsemantic-enhanced recommendation methods focus on aligning the two spaces,\nduring which the representations of the two spaces tend to get close while the\nunique patterns are discarded and not well explored. In this paper, we propose\nDisCo to Disentangle the unique patterns from the two representation spaces and\nCollaborate the two spaces for recommendation enhancement, where both the\nspecificity and the consistency of the two spaces are captured. Concretely, we\npropose 1) a dual-side attentive network to capture the intra-domain patterns\nand the inter-domain patterns, 2) a sufficiency constraint to preserve the\ntask-relevant information of each representation space and filter out the\nnoise, and 3) a disentanglement constraint to avoid the model from discarding\nthe unique information. These modules strike a balance between disentanglement\nand collaboration of the two representation spaces to produce informative\npattern vectors, which could serve as extra features and be appended to\narbitrary recommendation backbones for enhancement. Experiment results validate\nthe superiority of our method against different models and the compatibility of\nDisCo over different backbones. Various ablation studies and efficiency\nanalysis are also conducted to justify each model component.",
      "tldr_zh": "本文提出 DisCo 方法，旨在解决推荐系统中 tabular 表示空间和 semantic 表示空间之间的不协调问题，通过解耦（Disentangle）两个空间的独特模式并协作（Collaborate）它们，以捕捉特定性和一致性。核心组件包括双侧注意力网络（dual-side attentive network）用于提取内部和跨域模式、充分性约束（sufficiency constraint）保留任务相关信息并过滤噪声，以及解耦约束（disentanglement constraint）避免丢失独特信息。这些模块生成的信息丰富模式向量可附加到任意推荐主干上，提升整体性能。实验结果表明，DisCo 优于现有模型，并在兼容性和效率分析中表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00011v2",
      "published_date": "2024-05-20 08:07:27 UTC",
      "updated_date": "2024-06-04 07:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:54:02.995762"
    },
    {
      "arxiv_id": "2405.11848v2",
      "title": "Alternators For Sequence Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Reza Rezaei",
        "Adji Bousso Dieng"
      ],
      "abstract": "This paper introduces alternators, a novel family of non-Markovian dynamical\nmodels for sequences. An alternator features two neural networks: the\nobservation trajectory network (OTN) and the feature trajectory network (FTN).\nThe OTN and the FTN work in conjunction, alternating between outputting samples\nin the observation space and some feature space, respectively, over a cycle.\nThe parameters of the OTN and the FTN are not time-dependent and are learned\nvia a minimum cross-entropy criterion over the trajectories. Alternators are\nversatile. They can be used as dynamical latent-variable generative models or\nas sequence-to-sequence predictors. Alternators can uncover the latent dynamics\nunderlying complex sequential data, accurately forecast and impute missing\ndata, and sample new trajectories. We showcase the capabilities of alternators\nin three applications. We first used alternators to model the Lorenz equations,\noften used to describe chaotic behavior. We then applied alternators to\nNeuroscience, to map brain activity to physical activity. Finally, we applied\nalternators to Climate Science, focusing on sea-surface temperature\nforecasting. In all our experiments, we found alternators are stable to train,\nfast to sample from, yield high-quality generated samples and latent variables,\nand often outperform strong baselines such as Mambas, neural ODEs, and\ndiffusion models in the domains we studied.",
      "tldr_zh": "本论文引入了alternators，一种新型的non-Markovian动态模型，用于序列建模，该模型由观察轨迹网络(OTN)和特征轨迹网络(FTN)组成，它们交替输出观察空间和特征空间的样本。alternators的参数通过minimum cross-entropy标准学习，可作为动态潜在变量生成模型或序列到序列预测器，用于揭示复杂序列数据的潜在动态、准确预测和填充缺失数据，并生成新轨迹。在实验中，alternators应用于Lorenz方程建模、神经科学（脑活动到身体活动的映射）和气候科学（海面温度预测），训练稳定、采样快速，且在这些领域优于基线模型如Mambas、neural ODEs和diffusion models。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "physics.ao-ph",
        "q-bio.NC"
      ],
      "primary_category": "stat.ML",
      "comment": "A new versatile family of sequence models that can be used for both\n  generative modeling and supervised learning. The codebase will be made\n  available upon publication. This paper is dedicated to Thomas Sankara",
      "pdf_url": "http://arxiv.org/pdf/2405.11848v2",
      "published_date": "2024-05-20 07:47:06 UTC",
      "updated_date": "2024-12-01 00:49:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:54:15.311784"
    },
    {
      "arxiv_id": "2405.11841v1",
      "title": "Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Junqi Wang",
        "Chunhui Zhang",
        "Jiapeng Li",
        "Yuxi Ma",
        "Lixing Niu",
        "Jiaheng Han",
        "Yujia Peng",
        "Yixin Zhu",
        "Lifeng Fan"
      ],
      "abstract": "Facing the current debate on whether Large Language Models (LLMs) attain\nnear-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023;\nKosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study\nintroduces a benchmark for evaluating social intelligence, one of the most\ndistinctive aspects of human cognition. We developed a comprehensive\ntheoretical framework for social dynamics and introduced two evaluation tasks:\nInverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also\nencompassed a computational model based on recursive Bayesian inference, adept\nat elucidating diverse human behavioral patterns. Extensive experiments and\ndetailed analyses revealed that humans surpassed the latest GPT models in\noverall performance, zero-shot learning, one-shot generalization, and\nadaptability to multi-modalities. Notably, GPT models demonstrated social\nintelligence only at the most basic order (order = 0), in stark contrast to\nhuman social intelligence (order >= 2). Further examination indicated a\npropensity of LLMs to rely on pattern recognition for shortcuts, casting doubt\non their possession of authentic human-level social intelligence. Our codes,\ndataset, appendix and human data are released at\nhttps://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)是否达到近人类智能的争论，引入了一个评估社会智能的基准，并开发了关于社会动态的全面理论框架，包括Inverse Reasoning (IR)和Inverse Inverse Planning (IIP)任务，以及基于递归Bayesian inference的计算模型，以解释人类行为模式。实验结果显示，人类在整体表现、zero-shot learning、one-shot generalization和多模态适应性等方面优于最新的GPT模型，而GPT模型仅在基本层面(order = 0)显示社会智能，与人类(order >= 2)的深度能力形成鲜明对比。进一步分析发现，LLMs倾向于依赖模式识别作为捷径，这质疑了其是否真正具备人类级社会智能；研究还发布了代码、数据集和人类数据，以供进一步验证。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Also published in Proceedings of the Annual Meeting of the Cognitive\n  Science Society (CogSci), 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11841v1",
      "published_date": "2024-05-20 07:34:48 UTC",
      "updated_date": "2024-05-20 07:34:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:54:27.790687"
    },
    {
      "arxiv_id": "2405.11837v2",
      "title": "Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model",
      "title_zh": "翻译失败",
      "authors": [
        "Mounes Zaval",
        "Sedat Ozer"
      ],
      "abstract": "In the evolving field of Explainable AI (XAI), interpreting the decisions of\ndeep neural networks (DNNs) in computer vision tasks is an important process.\nWhile pixel-based XAI methods focus on identifying significant pixels, existing\nconcept-based XAI methods use pre-defined or human-annotated concepts. The\nrecently proposed Segment Anything Model (SAM) achieved a significant step\nforward to prepare automatic concept sets via comprehensive instance\nsegmentation. Building upon this, the Explain Any Concept (EAC) model emerged\nas a flexible method for explaining DNN decisions. EAC model is based on using\na surrogate model which has one trainable linear layer to simulate the target\nmodel. In this paper, by introducing an additional nonlinear layer to the\noriginal surrogate model, we show that we can improve the performance of the\nEAC model. We compare our proposed approach to the original EAC model and\nreport improvements obtained on both ImageNet and MS COCO datasets.",
      "tldr_zh": "这篇论文针对可解释 AI (XAI) 中的问题，提出了一种改进 Explain Any Concept (EAC) 模型的方法，通过在可训练的代理模型 (surrogate model) 中引入额外的非线性层，以更好地模拟深度神经网络 (DNNs) 在计算机视觉任务中的决策。基于 Segment Anything Model (SAM) 提供的自动概念集，该改进增强了模型对概念的灵活解释和准确性。实验结果显示，在 ImageNet 和 MS COCO 数据集上，该方法相对于原 EAC 模型取得了显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted for publication at IEEE SIU conference, 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11837v2",
      "published_date": "2024-05-20 07:25:09 UTC",
      "updated_date": "2024-06-24 19:28:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:54:39.531859"
    },
    {
      "arxiv_id": "2406.04359v1",
      "title": "Naming the Pain in Machine Learning-Enabled Systems Engineering",
      "title_zh": "机器学习赋能系统工程中的痛点命名",
      "authors": [
        "Marcos Kalinowski",
        "Daniel Mendez",
        "Görkem Giray",
        "Antonio Pedro Santos Alves",
        "Kelly Azevedo",
        "Tatiana Escovedo",
        "Hugo Villamizar",
        "Helio Lopes",
        "Teresa Baldassarre",
        "Stefan Wagner",
        "Stefan Biffl",
        "Jürgen Musil",
        "Michael Felderer",
        "Niklas Lavesson",
        "Tony Gorschek"
      ],
      "abstract": "Context: Machine learning (ML)-enabled systems are being increasingly adopted\nby companies aiming to enhance their products and operational processes.\nObjective: This paper aims to deliver a comprehensive overview of the current\nstatus quo of engineering ML-enabled systems and lay the foundation to steer\npractically relevant and problem-driven academic research. Method: We conducted\nan international survey to collect insights from practitioners on the current\npractices and problems in engineering ML-enabled systems. We received 188\ncomplete responses from 25 countries. We conducted quantitative statistical\nanalyses on contemporary practices using bootstrapping with confidence\nintervals and qualitative analyses on the reported problems using open and\naxial coding procedures. Results: Our survey results reinforce and extend\nexisting empirical evidence on engineering ML-enabled systems, providing\nadditional insights into typical ML-enabled systems project contexts, the\nperceived relevance and complexity of ML life cycle phases, and current\npractices related to problem understanding, model deployment, and model\nmonitoring. Furthermore, the qualitative analysis provides a detailed map of\nthe problems practitioners face within each ML life cycle phase and the\nproblems causing overall project failure. Conclusions: The results contribute\nto a better understanding of the status quo and problems in practical\nenvironments. We advocate for the further adaptation and dissemination of\nsoftware engineering practices to enhance the engineering of ML-enabled\nsystems.",
      "tldr_zh": "这篇论文探讨了机器学习(ML)-启用系统工程的现状和挑战，通过国际调查收集从业者见解，以推动实际相关和问题驱动的学术研究。研究方法包括对188名来自25个国家的从业者进行问卷调查，并运用bootstrapping统计分析和open and axial coding定性分析，揭示了ML生命周期阶段的实践细节、复杂性以及常见问题，如问题理解、模型部署和监控。结果显示，这些问题可能导致项目失败，并强化了现有经验证据；论文结论呼吁进一步适应软件工程实践，以提升ML-启用系统的工程质量和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "arXiv admin note: text overlap with arXiv:2310.06726",
      "pdf_url": "http://arxiv.org/pdf/2406.04359v1",
      "published_date": "2024-05-20 06:59:20 UTC",
      "updated_date": "2024-05-20 06:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:54:50.817996"
    },
    {
      "arxiv_id": "2405.11821v1",
      "title": "A Three-Phase Analysis of Synergistic Effects During Co-pyrolysis of Algae and Wood for Biochar Yield Using Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Subhadeep Chakrabarti",
        "Saish Shinde"
      ],
      "abstract": "Pyrolysis techniques have served to be a groundbreaking technique for\neffectively utilising natural and man-made biomass products like plastics,\nwood, crop residue, fruit peels etc. Recent advancements have shown a greater\nyield of essential products like biochar, bio-oil and other non-condensable\ngases by blending different biomasses in a certain ratio. This synergy effect\nof combining two pyrolytic raw materials i.e co-pyrolysis of algae and wood\nbiomass has been systematically studied and grouped into 3 phases in this\nresearch paper-kinetic analysis of co-pyrolysis, correlation among proximate\nand ultimate analysis with bio-char yield and lastly grouping of different\nweight ratios based on biochar yield up to a certain percentage. Different ML\nand DL algorithms have been utilized for regression and classification\ntechniques to give a comprehensive overview of the effect of the synergy of two\ndifferent biomass materials on biochar yield. For the first phase, the best\nprediction of biochar yield was obtained by using a decision tree regressor\nwith a perfect MSE score of 0.00, followed by a gradient-boosting regressor.\nThe second phase was analyzed using both ML and DL techniques. Within ML, SVR\nproved to be the most convenient model with an accuracy score of 0.972 with DNN\nemployed for deep learning technique. Finally, for the third phase, binary\nclassification was applied to biochar yield with and without heating rate for\nbiochar yield percentage above and below 40%. The best technique for ML was\nSupport Vector followed by Random forest while ANN was the most suitable Deep\nLearning Technique.",
      "tldr_zh": "本研究分析了藻类和木材生物质的共热解(Co-pyrolysis)协同效应，分为三个阶段，以优化生物炭产量的预测，并利用Machine Learning (ML) 和Deep Learning (DL) 算法进行建模。第一个阶段聚焦于共热解动力学分析，其中Decision Tree Regressor 表现出最佳性能（MSE 得分0.00），其次是Gradient Boosting Regressor；第二个阶段考察近似和最终分析与生物炭产量的相关性，支持向量回归(SVR) 在ML中准确率达0.972，而DNN 用于DL技术；第三个阶段通过二元分类评估生物炭产量高于/低于40%，Support Vector 和Random Forest 为最佳ML模型，ANN 则为最佳DL方法。整体结果为生物质热解提供了全面洞见，提升了生物炭产量的预测准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.11821v1",
      "published_date": "2024-05-20 06:32:57 UTC",
      "updated_date": "2024-05-20 06:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:55:03.660915"
    },
    {
      "arxiv_id": "2405.11816v1",
      "title": "Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning",
      "title_zh": "翻译失败",
      "authors": [
        "Anastasios Foliadis",
        "Mario H. Castañeda",
        "Richard A. Stirling-Gallacher",
        "Reiner S. Thomä"
      ],
      "abstract": "Utilizing deep learning (DL) techniques for radio-based positioning of user\nequipment (UE) through channel state information (CSI) fingerprints has\ndemonstrated significant potential. DL models can extract complex\ncharacteristics from the CSI fingerprints of a particular environment and\naccurately predict the position of a UE. Nonetheless, the effectiveness of the\nDL model trained on CSI fingerprints is highly dependent on the particular\ntraining environment, limiting the trained model's applicability across\ndifferent environments. This paper proposes a novel DL model structure\nconsisting of two parts, where the first part aims at identifying features that\nare independent from any specific environment, while the second part combines\nthose features in an environment specific way with the goal of positioning. To\ntrain such a two-part model, we propose the multi-environment meta-learning\n(MEML) approach for the first part to facilitate training across various\nenvironments, while the second part of the model is trained solely on data from\na specific environment. Our findings indicate that employing the MEML approach\nfor initializing the weights of the DL model for a new unseen environment\nsignificantly boosts the accuracy of UE positioning in the new target\nenvironment as well the reliability of its uncertainty estimation. This method\noutperforms traditional transfer learning methods, whether direct transfer\nlearning (DTL) between environments or completely training from scratch with\ndata from a new environment. The proposed approach is verified with real\nmeasurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.",
      "tldr_zh": "本文提出了一种基于多环境元学习（MEML）的转移学习方法，用于通过信道状态信息（CSI）指纹实现用户设备（UE）的无线电定位，解决传统深度学习（DL）模型依赖特定环境而泛化性差的问题。模型结构分为两部分：第一部分使用 MEML 提取环境无关特征，以支持跨环境训练；第二部分则在特定环境数据上微调这些特征进行定位。实验结果显示，该方法在新环境中显著提高了 UE 定位准确性和不确定性估计可靠性，优于直接转移学习（DTL）和从零训练。验证通过真实测量在视线（LOS）和非视线（NLOS）环境中进行，证明了其有效性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Submitted for IEEE journal publication. 10 pages, 13 figures, 7\n  tables",
      "pdf_url": "http://arxiv.org/pdf/2405.11816v1",
      "published_date": "2024-05-20 06:23:22 UTC",
      "updated_date": "2024-05-20 06:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:55:15.015999"
    },
    {
      "arxiv_id": "2405.11814v1",
      "title": "Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling",
      "title_zh": "翻译失败",
      "authors": [
        "Masato Sakai",
        "Marcus Freitag",
        "Akihisa Sakurai",
        "Conrad M Albrecht",
        "Hendrik F Hamann"
      ],
      "abstract": "Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru\nis urgent as natural and human impact accelerates. More frequent weather\nextremes such as flashfloods threaten Nasca artifacts. We demonstrate that\nrunoff models based on (sub-)meter scale, LiDAR-derived digital elevation data\ncan highlight AI-detected geoglyphs that are in danger of erosion. We recommend\nmeasures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\"\ngeoglyphs located close by, or even cut by the Pan-American Highway.",
      "tldr_zh": "该研究探讨了秘鲁UNESCO世界遗产地Nasca地画面临的气候（如闪洪）和人为（如公路影响）威胁，强调了保护的紧迫性。研究运用遥感(Remote Sensing)、AI和洪水建模(Flood Modelling)技术，包括基于LiDAR派生的高精度数字高程数据，识别出易受侵蚀的地画。结果显示，“蜥蜴”、“树”和“手”等著名地画处于高风险区，并推荐了具体缓解措施以保护这些文物。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at IGARSS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11814v1",
      "published_date": "2024-05-20 06:21:15 UTC",
      "updated_date": "2024-05-20 06:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:55:26.209181"
    },
    {
      "arxiv_id": "2405.11809v1",
      "title": "Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices",
      "title_zh": "先蒸馏后剪枝：一种高效的压缩框架，用于边缘设备上的实时立体匹配网络",
      "authors": [
        "Baiyu Pan",
        "Jichao Jiao",
        "Jianxing Pang",
        "Jun Cheng"
      ],
      "abstract": "In recent years, numerous real-time stereo matching methods have been\nintroduced, but they often lack accuracy. These methods attempt to improve\naccuracy by introducing new modules or integrating traditional methods.\nHowever, the improvements are only modest. In this paper, we propose a novel\nstrategy by incorporating knowledge distillation and model pruning to overcome\nthe inherent trade-off between speed and accuracy. As a result, we obtained a\nmodel that maintains real-time performance while delivering high accuracy on\nedge devices. Our proposed method involves three key steps. Firstly, we review\nstate-of-the-art methods and design our lightweight model by removing redundant\nmodules from those efficient models through a comparison of their\ncontributions. Next, we leverage the efficient model as the teacher to distill\nknowledge into the lightweight model. Finally, we systematically prune the\nlightweight model to obtain the final model. Through extensive experiments\nconducted on two widely-used benchmarks, Sceneflow and KITTI, we perform\nablation studies to analyze the effectiveness of each module and present our\nstate-of-the-art results.",
      "tldr_zh": "这篇论文提出了一种名为Distill-then-prune的压缩框架，用于实时立体匹配网络，旨在解决边缘设备上速度与准确性之间的权衡问题。框架包括三个关键步骤：首先，通过比较现有高效模型的贡献来移除冗余模块设计轻量模型；其次，利用教师模型进行knowledge distillation将知识转移到轻量模型；最后，对轻量模型进行systematic pruning以获得最终版本。实验在Sceneflow和KITTI基准数据集上显示，该方法显著提升了准确性，同时保持实时性能，并实现了state-of-the-art结果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "International Conference on Robotics and Automation (ICRA) 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11809v1",
      "published_date": "2024-05-20 06:03:55 UTC",
      "updated_date": "2024-05-20 06:03:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:55:37.961496"
    },
    {
      "arxiv_id": "2405.11802v1",
      "title": "Counterfactual Explanation-Based Badminton Motion Guidance Generation Using Wearable Sensors",
      "title_zh": "翻译失败",
      "authors": [
        "Minwoo Seong",
        "Gwangbin Kim",
        "Yumin Kang",
        "Junhyuk Jang",
        "Joseph DelPreto",
        "SeungJun Kim"
      ],
      "abstract": "This study proposes a framework for enhancing the stroke quality of badminton\nplayers by generating personalized motion guides, utilizing a multimodal\nwearable dataset. These guides are based on counterfactual algorithms and aim\nto reduce the performance gap between novice and expert players. Our approach\nprovides joint-level guidance through visualizable data to assist players in\nimproving their movements without requiring expert knowledge. The method was\nevaluated against a traditional algorithm using metrics to assess validity,\nproximity, and plausibility, including arithmetic measures and motion-specific\nevaluation metrics. Our evaluation demonstrates that the proposed framework can\ngenerate motions that maintain the essence of original movements while\nenhancing stroke quality, providing closer guidance than direct expert motion\nreplication. The results highlight the potential of our approach for creating\npersonalized sports motion guides by generating counterfactual motion guidance\nfor arbitrary input motion samples of badminton strokes.",
      "tldr_zh": "本研究提出一个基于反事实算法（counterfactual algorithms）的框架，利用可穿戴传感器（wearable sensors）和多模态数据集，为羽毛球球员生成个性化的动作指导，以缩小新手和专家球员之间的表现差距。框架提供关节级别的可视化数据，帮助球员改善动作，而无需依赖专家知识。评估结果显示，该方法在有效性、接近度和合理性指标（如算术措施和动作特定指标）上优于传统算法，能生成保留原动作本质的同时提升击球质量的指导。该框架展示了在体育领域创建个性化动作指导的潜力，特别是针对任意输入羽毛球击球样本。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "ICRA Wearable Workshop 2024 - 1st Workshop on Advancing Wearable\n  Devices and Applications through Novel Design, Sensing, Actuation, and AI",
      "pdf_url": "http://arxiv.org/pdf/2405.11802v1",
      "published_date": "2024-05-20 05:48:20 UTC",
      "updated_date": "2024-05-20 05:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:55:51.775256"
    },
    {
      "arxiv_id": "2405.11800v1",
      "title": "Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines",
      "title_zh": "生成式 AI 在高等教育：机构采用政策和指南的全球视角",
      "authors": [
        "Yueqiao Jin",
        "Lixiang Yan",
        "Vanessa Echeverria",
        "Dragan Gašević",
        "Roberto Martinez-Maldonado"
      ],
      "abstract": "Integrating generative AI (GAI) into higher education is crucial for\npreparing a future generation of GAI-literate students. Yet a thorough\nunderstanding of the global institutional adoption policy remains absent, with\nmost of the prior studies focused on the Global North and the promises and\nchallenges of GAI, lacking a theoretical lens. This study utilizes the\nDiffusion of Innovations Theory to examine GAI adoption strategies in higher\neducation across 40 universities from six global regions. It explores the\ncharacteristics of GAI innovation, including compatibility, trialability, and\nobservability, and analyses the communication channels and roles and\nresponsibilities outlined in university policies and guidelines. The findings\nreveal a proactive approach by universities towards GAI integration,\nemphasizing academic integrity, teaching and learning enhancement, and equity.\nDespite a cautious yet optimistic stance, a comprehensive policy framework is\nneeded to evaluate the impacts of GAI integration and establish effective\ncommunication strategies that foster broader stakeholder engagement. The study\nhighlights the importance of clear roles and responsibilities among faculty,\nstudents, and administrators for successful GAI integration, supporting a\ncollaborative model for navigating the complexities of GAI in education. This\nstudy contributes insights for policymakers in crafting detailed strategies for\nits integration.",
      "tldr_zh": "本研究利用 Diffusion of Innovations Theory 分析了全球 40 所大学的生成式 AI (GAI) 采用策略，涵盖六个区域，探讨了 GAI 的兼容性、可试用性和可观察性，以及政策中的沟通渠道和角色责任。研究发现，大学积极推动 GAI 整合，强调学术诚信、教学提升和公平性，但采用态度谨慎乐观，并呼吁建立更全面的政策框架来评估影响。最终，该研究为教育政策制定者提供洞见，帮助制定详细的 GAI 整合策略，促进教职员工、学生和行政人员的协作。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11800v1",
      "published_date": "2024-05-20 05:46:38 UTC",
      "updated_date": "2024-05-20 05:46:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:56:03.939078"
    },
    {
      "arxiv_id": "2405.11784v1",
      "title": "Reward-Punishment Reinforcement Learning with Maximum Entropy",
      "title_zh": "基于最大熵的奖励-惩罚强化学习",
      "authors": [
        "Jiexin Wang",
        "Eiji Uchibe"
      ],
      "abstract": "We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates\nthe optimization of long-term policy entropy into reward-punishment\nreinforcement learning objectives. Our motivation is to facilitate a smoother\nvariation of operators utilized in the updating of action values beyond\ntraditional ``max'' and ``min'' operators, where the goal is enhancing sample\nefficiency and robustness. We also address two unresolved issues from the\nprevious Deep MaxPain method. Firstly, we investigate how the negated\n(``flipped'') pain-seeking sub-policy, derived from the punishment action\nvalue, collaborates with the ``min'' operator to effectively learn the\npunishment module and how softDMP's smooth learning operator provides insights\ninto the ``flipping'' trick. Secondly, we tackle the challenge of data\ncollection for learning the punishment module to mitigate inconsistencies\narising from the involvement of the ``flipped'' sub-policy (pain-avoidance\nsub-policy) in the unified behavior policy. We empirically explore the first\nissue in two discrete Markov Decision Process (MDP) environments, elucidating\nthe crucial advancements of the DMP approach and the necessity for soft\ntreatments on the hard operators. For the second issue, we propose a\nprobabilistic classifier based on the ratio of the pain-seeking sub-policy to\nthe sum of the pain-seeking and goal-reaching sub-policies. This classifier\nassigns roll-outs to separate replay buffers for updating reward and punishment\naction-value functions, respectively. Our framework demonstrates superior\nperformance in Turtlebot 3's maze navigation tasks under the ROS Gazebo\nsimulation.",
      "tldr_zh": "本研究提出 soft Deep MaxPain (softDMP) 算法，将最大熵优化整合到奖惩强化学习中，以实现更平滑的动作值更新运算符，提升样本效率和鲁棒性。算法解决了 Deep MaxPain 的两个关键问题：一是探讨翻转的惩罚子策略（pain-seeking sub-policy）如何与最小运算符协作，并通过软处理提供洞见；二是引入基于概率分类器的机制，将回放缓冲区分离以缓解数据收集不一致性。实验结果显示，在离散 Markov Decision Process (MDP) 环境和 Turtlebot 3 的 ROS Gazebo 迷宫导航任务中，softDMP 框架表现出色，显著提高了学习性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "IJCNN2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11784v1",
      "published_date": "2024-05-20 05:05:14 UTC",
      "updated_date": "2024-05-20 05:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:56:16.668008"
    },
    {
      "arxiv_id": "2405.11783v1",
      "title": "Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing",
      "title_zh": "利用量子自然语言处理进行金属有机框架的逆向设计",
      "authors": [
        "Shinyoung Kang",
        "Jihan Kim"
      ],
      "abstract": "In this study, we explore the potential of using quantum natural language\nprocessing (QNLP) to inverse design metal-organic frameworks (MOFs) with\ntargeted properties. Specifically, by analyzing 150 hypothetical MOF structures\nconsisting of 10 metal nodes and 15 organic ligands, we categorize these\nstructures into four distinct classes for pore volume and $H_{2}$ uptake\nvalues. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat\n(Distributional Compositional Categorical), and sequence-based models) to\nidentify the most effective approach to process the MOF dataset. Using a\nclassical simulator provided by the IBM Qiskit, the bag-of-words model is\nidentified to be the optimum model, achieving validation accuracies of 85.7%\nand 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake,\nrespectively. Further, we developed multi-class classification models tailored\nto the probabilistic nature of quantum circuits, with average test accuracies\nof 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake\ndatasets. Finally, the performance of generating MOF with target properties\nshowed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake,\nrespectively. Although our investigation covers only a fraction of the vast MOF\nsearch space, it marks a promising first step towards using quantum computing\nfor materials design, offering a new perspective through which to explore the\ncomplex landscape of MOFs.",
      "tldr_zh": "本文研究了使用 Quantum Natural Language Processing (QNLP) 来反向设计 Metal-Organic Frameworks (MOFs)，以实现目标属性，如孔隙体积 (pore volume) 和 H₂ uptake。研究者分析了150个假设MOF结构，并比较了bag-of-words、DisCoCat和sequence-based模型，最终确定bag-of-words模型最优，在二元分类任务中达到85.7%和86.7%的验证准确率。进一步开发的多类分类和生成模型显示平均测试准确率分别为88.4% (pore volume) 和80.7% (H₂ uptake)，生成任务准确率高达93.5%和89%，为量子计算在材料设计领域的应用提供了有前景的初步洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "45 pages, 7 figures, 6 supplementary figures, 1 table, 1\n  supplementary table",
      "pdf_url": "http://arxiv.org/pdf/2405.11783v1",
      "published_date": "2024-05-20 05:02:12 UTC",
      "updated_date": "2024-05-20 05:02:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:56:30.000789"
    },
    {
      "arxiv_id": "2405.11778v1",
      "title": "Efficient Multi-agent Reinforcement Learning by Planning",
      "title_zh": "通过规划的高效多智能体强化学习",
      "authors": [
        "Qihan Liu",
        "Jianing Ye",
        "Xiaoteng Ma",
        "Jun Yang",
        "Bin Liang",
        "Chongjie Zhang"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) algorithms have accomplished\nremarkable breakthroughs in solving large-scale decision-making tasks.\nNonetheless, most existing MARL algorithms are model-free, limiting sample\nefficiency and hindering their applicability in more challenging scenarios. In\ncontrast, model-based reinforcement learning (MBRL), particularly algorithms\nintegrating planning, such as MuZero, has demonstrated superhuman performance\nwith limited data in many tasks. Hence, we aim to boost the sample efficiency\nof MARL by adopting model-based approaches. However, incorporating planning and\nsearch methods into multi-agent systems poses significant challenges. The\nexpansive action space of multi-agent systems often necessitates leveraging the\nnearly-independent property of agents to accelerate learning. To tackle this\nissue, we propose the MAZero algorithm, which combines a centralized model with\nMonte Carlo Tree Search (MCTS) for policy search. We design a novel network\nstructure to facilitate distributed execution and parameter sharing. To enhance\nsearch efficiency in deterministic environments with sizable action spaces, we\nintroduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and\nAdvantage-Weighted Policy Optimization (AWPO). Extensive experiments on the\nSMAC benchmark demonstrate that MAZero outperforms model-free approaches in\nterms of sample efficiency and provides comparable or better performance than\nexisting model-based methods in terms of both sample and computational\nefficiency. Our code is available at https://github.com/liuqh16/MAZero.",
      "tldr_zh": "该论文针对多智能体强化学习 (MARL) 的样本效率问题，提出了一种基于模型的算法 MAZero，以提升其在复杂决策任务中的表现。MAZero 结合中心化模型和 Monte Carlo Tree Search (MCTS) 进行策略搜索，并设计了新型网络结构支持分布式执行和参数共享，同时引入 Optimistic Search Lambda (OS(λ)) 和 Advantage-Weighted Policy Optimization (AWPO) 技巧，以优化确定性环境下的搜索效率。实验结果显示，在 SMAC 基准上，MAZero 比无模型方法显著提高样本效率，并在样本和计算效率上与现有基于模型方法相当或更优。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR2024",
      "pdf_url": "http://arxiv.org/pdf/2405.11778v1",
      "published_date": "2024-05-20 04:36:02 UTC",
      "updated_date": "2024-05-20 04:36:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:56:41.259814"
    },
    {
      "arxiv_id": "2405.11766v1",
      "title": "From SHAP Scores to Feature Importance Scores",
      "title_zh": "从 SHAP 分数到特征重要性分数",
      "authors": [
        "Olivier Letoffe",
        "Xuanxiang Huang",
        "Nicholas Asher",
        "Joao Marques-Silva"
      ],
      "abstract": "A central goal of eXplainable Artificial Intelligence (XAI) is to assign\nrelative importance to the features of a Machine Learning (ML) model given some\nprediction. The importance of this task of explainability by feature\nattribution is illustrated by the ubiquitous recent use of tools such as SHAP\nand LIME. Unfortunately, the exact computation of feature attributions, using\nthe game-theoretical foundation underlying SHAP and LIME, can yield manifestly\nunsatisfactory results, that tantamount to reporting misleading relative\nfeature importance. Recent work targeted rigorous feature attribution, by\nstudying axiomatic aggregations of features based on logic-based definitions of\nexplanations by feature selection. This paper shows that there is an essential\nrelationship between feature attribution and a priori voting power, and that\nthose recently proposed axiomatic aggregations represent a few instantiations\nof the range of power indices studied in the past. Furthermore, it remains\nunclear how some of the most widely used power indices might be exploited as\nfeature importance scores (FISs), i.e. the use of power indices in XAI, and\nwhich of these indices would be the best suited for the purposes of XAI by\nfeature attribution, namely in terms of not producing results that could be\ndeemed as unsatisfactory. This paper proposes novel desirable properties that\nFISs should exhibit. In addition, the paper also proposes novel FISs exhibiting\nthe proposed properties. Finally, the paper conducts a rigorous analysis of the\nbest-known power indices in terms of the proposed properties.",
      "tldr_zh": "这篇论文探讨了可解释人工智能(XAI)中特征归因的问题，指出SHAP和LIME等工具在计算特征重要性时可能产生误导性结果。论文揭示了特征归因与投票理论中的权力指数(power indices)之间的本质关系，并提出新型特征重要性分数(FISs)应具备的理想属性，如避免不满意的结果。最终，论文引入了新的FISs，并对现有权力指数进行了严格分析，以评估其在XAI中的适用性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11766v1",
      "published_date": "2024-05-20 03:52:41 UTC",
      "updated_date": "2024-05-20 03:52:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:56:52.206022"
    },
    {
      "arxiv_id": "2405.11758v1",
      "title": "Fed-Credit: Robust Federated Learning with Credibility Management",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayan Chen",
        "Zhirong Qian",
        "Tianhui Meng",
        "Xitong Gao",
        "Tian Wang",
        "Weijia Jia"
      ],
      "abstract": "Aiming at privacy preservation, Federated Learning (FL) is an emerging\nmachine learning approach enabling model training on decentralized devices or\ndata sources. The learning mechanism of FL relies on aggregating parameter\nupdates from individual clients. However, this process may pose a potential\nsecurity risk due to the presence of malicious devices. Existing solutions are\neither costly due to the use of compute-intensive technology, or restrictive\nfor reasons of strong assumptions such as the prior knowledge of the number of\nattackers and how they attack. Few methods consider both privacy constraints\nand uncertain attack scenarios. In this paper, we propose a robust FL approach\nbased on the credibility management scheme, called Fed-Credit. Unlike previous\nstudies, our approach does not require prior knowledge of the nodes and the\ndata distribution. It maintains and employs a credibility set, which weighs the\nhistorical clients' contributions based on the similarity between the local\nmodels and global model, to adjust the global model update. The subtlety of\nFed-Credit is that the time decay and attitudinal value factor are incorporated\ninto the dynamic adjustment of the reputation weights and it boasts a\ncomputational complexity of O(n) (n is the number of the clients). We conducted\nextensive experiments on the MNIST and CIFAR-10 datasets under 5 types of\nattacks. The results exhibit superior accuracy and resilience against\nadversarial attacks, all while maintaining comparatively low computational\ncomplexity. Among these, on the Non-IID CIFAR-10 dataset, our algorithm\nexhibited performance enhancements of 19.5% and 14.5%, respectively, in\ncomparison to the state-of-the-art algorithm when dealing with two types of\ndata poisoning attacks.",
      "tldr_zh": "本研究针对 Federated Learning (FL) 在隐私保护中的安全风险，提出了一种基于信誉管理方案的鲁棒框架 Fed-Credit，以应对恶意设备的潜在威胁。该方法无需先验知识（如攻击者数量），通过维护一个信誉集来评估客户端贡献，并基于本地模型与全局模型的相似性动态调整权重，同时融入时间衰减和态度值因素，实现了 O(n) 的计算复杂度。在 MNIST 和 CIFAR-10 数据集上的广泛实验中，Fed-Credit 在5种攻击场景下展示了更高的准确性和抗攻击能力，尤其在 Non-IID CIFAR-10 数据集上，比现有算法在两种数据中毒攻击中分别提升了19.5%和14.5%。这为高效、可靠的 FL 系统提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11758v1",
      "published_date": "2024-05-20 03:35:13 UTC",
      "updated_date": "2024-05-20 03:35:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:57:04.311007"
    },
    {
      "arxiv_id": "2405.11746v1",
      "title": "Configurable Mirror Descent: Towards a Unification of Decision Making",
      "title_zh": "可配置的镜像下降：迈向决策的统一",
      "authors": [
        "Pengdeng Li",
        "Shuxin Li",
        "Chang Yang",
        "Xinrun Wang",
        "Shuyue Hu",
        "Xiao Huang",
        "Hau Chan",
        "Bo An"
      ],
      "abstract": "Decision-making problems, categorized as single-agent, e.g., Atari,\ncooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em\npoker, and mixed cooperative and competitive, e.g., football, are ubiquitous in\nthe real world. Various methods are proposed to address the specific\ndecision-making problems. Despite the successes in specific categories, these\nmethods typically evolve independently and cannot generalize to other\ncategories. Therefore, a fundamental question for decision-making is: \\emph{Can\nwe develop \\textbf{a single algorithm} to tackle \\textbf{ALL} categories of\ndecision-making problems?} There are several main challenges to address this\nquestion: i) different decision-making categories involve different numbers of\nagents and different relationships between agents, ii) different categories\nhave different solution concepts and evaluation measures, and iii) there lacks\na comprehensive benchmark covering all the categories. This work presents a\npreliminary attempt to address the question with three main contributions. i)\nWe propose the generalized mirror descent (GMD), a generalization of MD\nvariants, which considers multiple historical policies and works with a broader\nclass of Bregman divergences. ii) We propose the configurable mirror descent\n(CMD) where a meta-controller is introduced to dynamically adjust the\nhyper-parameters in GMD conditional on the evaluation measures. iii) We\nconstruct the \\textsc{GameBench} with 15 academic-friendly games across\ndifferent decision-making categories. Extensive experiments demonstrate that\nCMD achieves empirically competitive or better outcomes compared to baselines\nwhile providing the capability of exploring diverse dimensions of decision\nmaking.",
      "tldr_zh": "该论文旨在统一各种决策问题（如单代理如Atari、合作多代理如Hanabi、竞争多代理如Hold'em poker，以及混合类型如football），提出一个单一算法来处理所有类别。研究者开发了广义镜像下降 (Generalized Mirror Descent, GMD)，它扩展了镜像下降 (Mirror Descent) 变体以考虑多个历史策略和更广泛的Bregman divergences，并引入可配置镜像下降 (Configurable Mirror Descent, CMD)，通过元控制器根据评估标准动态调整超参数。论文还构建了GameBench基准，涵盖15个跨类别的游戏，实验结果显示CMD在性能上与基线相当或更优，并能探索决策的多样维度。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to The Forty-first International Conference on Machine\n  Learning (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.11746v1",
      "published_date": "2024-05-20 03:10:22 UTC",
      "updated_date": "2024-05-20 03:10:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:57:17.346940"
    },
    {
      "arxiv_id": "2405.11740v1",
      "title": "Learning Future Representation with Synthetic Observations for Sample-efficient Reinforcement Learning",
      "title_zh": "利用合成观察学习未来表示以实现样本高效强化学习",
      "authors": [
        "Xin Liu",
        "Yaran Chen",
        "Dongbin Zhao"
      ],
      "abstract": "In visual Reinforcement Learning (RL), upstream representation learning\nlargely determines the effect of downstream policy learning. Employing\nauxiliary tasks allows the agent to enhance visual representation in a targeted\nmanner, thereby improving the sample efficiency and performance of downstream\nRL. Prior advanced auxiliary tasks all focus on how to extract as much\ninformation as possible from limited experience (including observations,\nactions, and rewards) through their different auxiliary objectives, whereas in\nthis article, we first start from another perspective: auxiliary training data.\nWe try to improve auxiliary representation learning for RL by enriching\nauxiliary training data, proposing \\textbf{L}earning \\textbf{F}uture\nrepresentation with \\textbf{S}ynthetic observations \\textbf{(LFS)}, a novel\nself-supervised RL approach. Specifically, we propose a training-free method to\nsynthesize observations that may contain future information, as well as a data\nselection approach to eliminate unqualified synthetic noise. The remaining\nsynthetic observations and real observations then serve as the auxiliary data\nto achieve a clustering-based temporal association task for representation\nlearning. LFS allows the agent to access and learn observations that have not\nyet appeared in advance, so as to quickly understand and exploit them when they\noccur later. In addition, LFS does not rely on rewards or actions, which means\nit has a wider scope of application (e.g., learning from video) than recent\nadvanced auxiliary tasks. Extensive experiments demonstrate that our LFS\nexhibits state-of-the-art RL sample efficiency on challenging continuous\ncontrol and enables advanced visual pre-training based on action-free video\ndemonstrations.",
      "tldr_zh": "该论文提出了一种名为 LFS（Learning Future representation with Synthetic observations）的自监督强化学习（RL）方法，旨在通过丰富辅助训练数据来提升视觉 RL 的样本效率。具体地，LFS 使用无训练合成方法生成可能包含未来信息的观察数据，并通过数据选择机制过滤噪声，然后利用合成和真实观察进行基于聚类的时序关联任务，以学习更有效的表示。该方法允许代理提前理解未来观察，从而在下游任务中快速应用，且不依赖奖励或动作，适用于如视频演示等场景。实验结果显示，LFS 在挑战性的连续控制任务中实现了最先进的样本效率，并支持基于无动作视频的视觉预训练。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11740v1",
      "published_date": "2024-05-20 02:43:04 UTC",
      "updated_date": "2024-05-20 02:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:57:28.508053"
    },
    {
      "arxiv_id": "2405.11739v2",
      "title": "What Radio Waves Tell Us about Sleep",
      "title_zh": "翻译失败",
      "authors": [
        "Hao He",
        "Chao Li",
        "Wolfgang Ganglberger",
        "Kaileigh Gallagher",
        "Rumen Hristov",
        "Michail Ouroutzoglou",
        "Haoqi Sun",
        "Jimeng Sun",
        "Brandon Westover",
        "Dina Katabi"
      ],
      "abstract": "The ability to assess sleep at home, capture sleep stages, and detect the\noccurrence of apnea (without on-body sensors) simply by analyzing the radio\nwaves bouncing off people's bodies while they sleep is quite powerful. Such a\ncapability would allow for longitudinal data collection in patients' homes,\ninforming our understanding of sleep and its interaction with various diseases\nand their therapeutic responses, both in clinical trials and routine care. In\nthis article, we develop an advanced machine learning algorithm for passively\nmonitoring sleep and nocturnal breathing from radio waves reflected off people\nwhile asleep. Validation results in comparison with the gold standard (i.e.,\npolysomnography) (n=849) demonstrate that the model captures the sleep\nhypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake,\nLight Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and\nmeasures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]).\nNotably, the model exhibits equitable performance across race, sex, and age.\nMoreover, the model uncovers informative interactions between sleep stages and\na range of diseases including neurological, psychiatric, cardiovascular, and\nimmunological disorders. These findings not only hold promise for clinical\npractice and interventional trials but also underscore the significance of\nsleep as a fundamental component in understanding and managing various\ndiseases.",
      "tldr_zh": "本研究开发了一种先进的机器学习算法，通过分析反弹的无线电波来被动监控睡眠和夜间呼吸，无需身体传感器，从而实现家庭环境下的纵向数据收集。验证结果显示，与金标准多导睡眠图(polysomnography)相比，该模型捕捉睡眠阶段的准确率达81%（包括清醒、浅睡、深睡和REM），检测睡眠呼吸暂停的AUROC为0.88，并精确测量Apnea-Hypopnea Index（ICC=0.95）。模型在种族、性别和年龄方面表现出公平性能，并揭示了睡眠阶段与神经、精神、心血管及免疫疾病之间的互动。这些发现为临床实践、干预试验和疾病管理提供了新insights。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "The first two authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2405.11739v2",
      "published_date": "2024-05-20 02:41:21 UTC",
      "updated_date": "2024-07-20 19:38:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:57:41.598527"
    },
    {
      "arxiv_id": "2405.13055v1",
      "title": "Large Language Models for Medicine: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Yanxin Zheng",
        "Wensheng Gan",
        "Zefeng Chen",
        "Zhenlian Qi",
        "Qian Liang",
        "Philip S. Yu"
      ],
      "abstract": "To address challenges in the digital economy's landscape of digital\nintelligence, large language models (LLMs) have been developed. Improvements in\ncomputational power and available resources have significantly advanced LLMs,\nallowing their integration into diverse domains for human life. Medical LLMs\nare essential application tools with potential across various medical\nscenarios. In this paper, we review LLM developments, focusing on the\nrequirements and applications of medical LLMs. We provide a concise overview of\nexisting models, aiming to explore advanced research directions and benefit\nresearchers for future medical applications. We emphasize the advantages of\nmedical LLMs in applications, as well as the challenges encountered during\ntheir development. Finally, we suggest directions for technical integration to\nmitigate challenges and potential research directions for the future of medical\nLLMs, aiming to meet the demands of the medical field better.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）在医学领域的应用进行了全面调查，回顾了 LLMs 的发展历史及其在医疗场景中的要求和潜力。论文概述了现有医学 LLMs 的优势，如在诊断、辅助决策等方面的表现，同时指出了面临的挑战，包括数据隐私、模型准确性和领域知识缺口。最终，它提出了技术整合策略和未来研究方向，以缓解这些问题并更好地服务于医疗需求。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. 5 figures,5 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.13055v1",
      "published_date": "2024-05-20 02:32:26 UTC",
      "updated_date": "2024-05-20 02:32:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:57:51.563785"
    },
    {
      "arxiv_id": "2405.12252v1",
      "title": "Enhanced Deterministic Approximation Algorithm for Non-monotone Submodular Maximization under Knapsack Constraint with Linear Query Complexity",
      "title_zh": "增强的确定性逼近算法，用于背包约束下的非",
      "authors": [
        "Canh V. Pham"
      ],
      "abstract": "In this work, we consider the Submodular Maximization under Knapsack (SMK)\nconstraint problem over the ground set of size $n$. The problem recently\nattracted a lot of attention due to its applications in various domains of\ncombination optimization, artificial intelligence, and machine learning. We\nimprove the approximation factor of the fastest deterministic algorithm from\n$6+\\epsilon$ to $5+\\epsilon$ while keeping the best query complexity of $O(n)$,\nwhere $\\epsilon >0$ is a constant parameter. Our technique is based on\noptimizing the performance of two components: the threshold greedy subroutine\nand the building of two disjoint sets as candidate solutions. Besides, by\ncarefully analyzing the cost of candidate solutions, we obtain a tighter\napproximation factor.",
      "tldr_zh": "本论文针对非单调 Submodular Maximization under Knapsack (SMK) 问题，提出了一种增强的确定性近似算法，将近似因子从 6 + ε 优化到 5 + ε，同时保持 O(n) 的线性查询复杂度。算法的核心技术包括优化阈值贪婪子程序（threshold greedy subroutine）和构建两个不相交的候选解决方案集，以提高性能。通过仔细分析候选解决方案的成本，该方法在组合优化、人工智能和机器学习等领域提供了更高效的解决方案。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.12252v1",
      "published_date": "2024-05-20 02:24:58 UTC",
      "updated_date": "2024-05-20 02:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:58:05.409840"
    },
    {
      "arxiv_id": "2405.11724v2",
      "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Huawei Lin",
        "Jikai Long",
        "Zhaozhuo Xu",
        "Weijie Zhao"
      ],
      "abstract": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
      "tldr_zh": "本论文提出了一种名为 RapidIn 的可扩展框架，用于从大型语言模型 (LLMs) 生成中检索影响性的训练数据。该框架包括两个阶段：首先，通过压缩梯度向量超过 200,000 倍，将其缓存到磁盘或 GPU/CPU 内存中；其次，给定一个生成，RapidIn 高效遍历缓存梯度，在几分钟内估计训练数据的影响，实现超过 6,326 倍的速度提升，并支持多 GPU 并行化。实验结果验证了 RapidIn 的高效性和有效性，为理解 LLMs 的训练数据影响提供了实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
      "pdf_url": "http://arxiv.org/pdf/2405.11724v2",
      "published_date": "2024-05-20 01:57:34 UTC",
      "updated_date": "2024-10-22 19:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:58:16.134169"
    },
    {
      "arxiv_id": "2405.11722v1",
      "title": "AI Algorithm for Predicting and Optimizing Trajectory of UAV Swarm",
      "title_zh": "翻译失败",
      "authors": [
        "Amit Raj",
        "Kapil Ahuja",
        "Yann Busnel"
      ],
      "abstract": "This paper explores the application of Artificial Intelligence (AI)\ntechniques for generating the trajectories of fleets of Unmanned Aerial\nVehicles (UAVs). The two main challenges addressed include accurately\npredicting the paths of UAVs and efficiently avoiding collisions between them.\nFirstly, the paper systematically applies a diverse set of activation functions\nto a Feedforward Neural Network (FFNN) with a single hidden layer, which\nenhances the accuracy of the predicted path compared to previous work.\n  Secondly, we introduce a novel activation function, AdaptoSwelliGauss, which\nis a sophisticated fusion of Swish and Elliott activations, seamlessly\nintegrated with a scaled and shifted Gaussian component. Swish facilitates\nsmooth transitions, Elliott captures abrupt trajectory changes, and the scaled\nand shifted Gaussian enhances robustness against noise. This dynamic\ncombination is specifically designed to excel in capturing the complexities of\nUAV trajectory prediction. This new activation function gives substantially\nbetter accuracy than all existing activation functions.\n  Thirdly, we propose a novel Integrated Collision Detection, Avoidance, and\nBatching (ICDAB) strategy that merges two complementary UAV collision avoidance\ntechniques: changing UAV trajectories and altering their starting times, also\nreferred to as batching. This integration helps overcome the disadvantages of\nboth - reduction in the number of trajectory manipulations, which avoids overly\nconvoluted paths in the first technique, and smaller batch sizes, which reduce\noverall takeoff time in the second.",
      "tldr_zh": "该论文提出了一种AI算法，用于预测和优化无人机（UAV）群的轨迹，主要解决路径预测准确性和碰撞避免挑战。首先，通过在单隐藏层的Feedforward Neural Network（FFNN）中应用多种激活函数，提升了路径预测的精确度。其次，引入新型激活函数AdaptoSwelliGauss，该函数融合Swish、Elliott和scaled and shifted Gaussian组件，比现有函数显著提高预测准确性。最后，提出Integrated Collision Detection, Avoidance, and Batching（ICDAB）策略，结合轨迹调整和起始时间批处理，减少轨迹复杂性和起飞时间，从而优化碰撞避免效果。整体上，该方法为UAV群轨迹管理提供了更高效的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2.1; I.6.3"
      ],
      "primary_category": "cs.RO",
      "comment": "24 Pages, 9 Tables, 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2405.11722v1",
      "published_date": "2024-05-20 01:47:28 UTC",
      "updated_date": "2024-05-20 01:47:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:58:28.660844"
    },
    {
      "arxiv_id": "2405.11715v2",
      "title": "Semantic Trajectory Data Mining with LLM-Informed POI Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Liu",
        "Chenchen Kuai",
        "Haoxuan Ma",
        "Xishun Liao",
        "Brian Yueshuai He",
        "Jiaqi Ma"
      ],
      "abstract": "Human travel trajectory mining is crucial for transportation systems,\nenhancing route optimization, traffic management, and the study of human travel\npatterns. Previous rule-based approaches without the integration of semantic\ninformation show a limitation in both efficiency and accuracy. Semantic\ninformation, such as activity types inferred from Points of Interest (POI)\ndata, can significantly enhance the quality of trajectory mining. However,\nintegrating these insights is challenging, as many POIs have incomplete feature\ninformation, and current learning-based POI algorithms require the integrity of\ndatasets to do the classification. In this paper, we introduce a novel pipeline\nfor human travel trajectory mining. Our approach first leverages the strong\ninferential and comprehension capabilities of large language models (LLMs) to\nannotate POI with activity types and then uses a Bayesian-based algorithm to\ninfer activity for each stay point in a trajectory. In our evaluation using the\nOpenStreetMap (OSM) POI dataset, our approach achieves a 93.4% accuracy and a\n96.1% F-1 score in POI classification, and a 91.7% accuracy with a 92.3% F-1\nscore in activity inference.",
      "tldr_zh": "该论文提出了一种基于LLM-Informed的语义轨迹数据挖掘方法，以解决传统规则-based方法在效率和准确性上的局限性，特别是针对Points of Interest (POI)数据的不完整性。方法首先利用大型语言模型（LLMs）的推理能力对POI进行活动类型标注，然后应用Bayesian-based算法来推断轨迹中每个停留点的活动类型。在OpenStreetMap (OSM)数据集的评估中，该方法在POI分类上实现了93.4%的准确率和96.1%的F-1分数，在活动推断上达到了91.7%的准确率和92.3%的F-1分数，从而显著提升了人类旅行轨迹挖掘的质量。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, accepted for the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.11715v2",
      "published_date": "2024-05-20 01:29:45 UTC",
      "updated_date": "2024-08-19 19:06:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:58:41.969884"
    },
    {
      "arxiv_id": "2405.11143v4",
      "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework",
      "title_zh": "OpenRLHF：易于使用、可扩展和高性能的 RLHF 框架",
      "authors": [
        "Jian Hu",
        "Xibin Wu",
        "Zilin Zhu",
        "Xianyu",
        "Weixun Wang",
        "Dehao Zhang",
        "Yu Cao"
      ],
      "abstract": "As large language models (LLMs) continue to grow by scaling laws,\nreinforcement learning from human feedback (RLHF) has gained significant\nattention due to its outstanding performance. However, unlike pretraining or\nfine-tuning a single model, scaling reinforcement learning from human feedback\n(RLHF) for training large language models poses coordination challenges across\nfour models. We present OpenRLHF, an open-source framework enabling efficient\nRLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the\nsame GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters\nusing Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and\ndiverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF\nprovides an out-of-the-box solution with optimized algorithms and launch\nscripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,\nrejection sampling, and other alignment techniques. Empowering state-of-the-art\nLLM development, OpenRLHF's code is available at\n\\url{https://github.com/OpenRLHF/OpenRLHF}.",
      "tldr_zh": "该研究针对强化学习从人类反馈（RLHF）在训练大型语言模型（LLMs）时的协调挑战，提出了一种开源框架 OpenRLHF，以实现高效扩展和高性能。OpenRLHF 通过重新设计调度机制，利用 Ray、vLLM 和 DeepSpeed 处理超过 70B 参数的模型，提高资源利用率并支持多样化训练方法。框架无缝集成 Hugging Face，提供优化算法、启动脚本以及现成实现，如 RLHF、DPO 和拒绝采样等对齐技术，最终助力最先进的 LLM 开发，并开源代码于 GitHub。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11143v4",
      "published_date": "2024-05-20 01:04:40 UTC",
      "updated_date": "2024-11-24 08:34:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:58:52.370790"
    },
    {
      "arxiv_id": "2405.11706v1",
      "title": "Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!",
      "title_zh": "提升问答任务中LLM的准确性",
      "authors": [
        "Dean Allemang",
        "Juan Sequeda"
      ],
      "abstract": "There is increasing evidence that question-answering (QA) systems with Large\nLanguage Models (LLMs), which employ a knowledge graph/semantic representation\nof an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy\ncompared to systems that answer questions directly on SQL databases (i.e.\nText-to-SQL). Our previous benchmark research showed that by using a knowledge\ngraph, the accuracy improved from 16% to 54%. The question remains: how can we\nfurther improve the accuracy and reduce the error rate? Building on the\nobservations of our previous research where the inaccurate LLM-generated SPARQL\nqueries followed incorrect paths, we present an approach that consists of 1)\nOntology-based Query Check (OBQC): detects errors by leveraging the ontology of\nthe knowledge graph to check if the LLM-generated SPARQL query matches the\nsemantic of ontology and 2) LLM Repair: use the error explanations with an LLM\nto repair the SPARQL query. Using the chat with the data benchmark, our primary\nfinding is that our approach increases the overall accuracy to 72% including an\nadditional 8% of \"I don't know\" unknown results. Thus, the overall error rate\nis 20%. These results provide further evidence that investing knowledge graphs,\nnamely the ontology, provides higher accuracy for LLM powered question\nanswering systems.",
      "tldr_zh": "本文研究如何提升 Large Language Models (LLMs) 在问答 (QA) 系统中的准确率，特别是通过知识图谱来解决 Text-to-SPARQL 查询错误问题。作者提出两种方法：Ontology-based Query Check (OBQC)，利用知识图谱的本体检测 SPARQL 查询是否符合语义；以及 LLM Repair，使用错误解释让 LLM 自动修复查询。在 chat with the data 基准测试中，该方法将准确率从 54% 提高到 72%，包括 8% 的 \"I don't know\" 未知结果，从而将错误率降至 20%。这些发现进一步证明，投资知识图谱尤其是本体，能显著增强 LLM 驱动的 QA 系统性能。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.11706v1",
      "published_date": "2024-05-20 00:28:00 UTC",
      "updated_date": "2024-05-20 00:28:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:59:05.334138"
    },
    {
      "arxiv_id": "2405.11704v1",
      "title": "Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Taiyuan Mei",
        "Yun Zi",
        "Xiaohan Cheng",
        "Zijun Gao",
        "Qi Wang",
        "Haowei Yang"
      ],
      "abstract": "The internal structure and operation mechanism of large-scale language models\nare analyzed theoretically, especially how Transformer and its derivative\narchitectures can restrict computing efficiency while capturing long-term\ndependencies. Further, we dig deep into the efficiency bottleneck of the\ntraining phase, and evaluate in detail the contribution of adaptive\noptimization algorithms (such as AdamW), massively parallel computing\ntechniques, and mixed precision training strategies to accelerate convergence\nand reduce memory footprint. By analyzing the mathematical principles and\nimplementation details of these algorithms, we reveal how they effectively\nimprove training efficiency in practice. In terms of model deployment and\ninference optimization, this paper systematically reviews the latest advances\nin model compression techniques, focusing on strategies such as quantification,\npruning, and knowledge distillation. By comparing the theoretical frameworks of\nthese techniques and their effects in different application scenarios, we\ndemonstrate their ability to significantly reduce model size and inference\ndelay while maintaining model prediction accuracy. In addition, this paper\ncritically examines the limitations of current efficiency optimization methods,\nsuch as the increased risk of overfitting, the control of performance loss\nafter compression, and the problem of algorithm generality, and proposes some\nprospects for future research. In conclusion, this study provides a\ncomprehensive theoretical framework for understanding the efficiency\noptimization of large-scale language models.",
      "tldr_zh": "本研究分析了基于深度学习的Transformer及其衍生架构在大型语言模型中的效率瓶颈，探讨了如何在捕获长期依赖性的同时优化计算效率。论文详细评估了训练阶段的优化策略，包括AdamW算法、并行计算技术和混合精度训练，这些方法能加速收敛并减少内存占用；在部署和推理方面，重点审查了量化、剪枝和知识蒸馏等模型压缩技术，这些技术显著降低了模型大小和推理延迟，同时维持预测准确性。研究还指出了当前方法的局限性，如过拟合风险和性能损失问题，并为未来效率优化提供了理论框架和研究前景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.11704v1",
      "published_date": "2024-05-20 00:10:00 UTC",
      "updated_date": "2024-05-20 00:10:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T09:59:16.328734"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 77,
  "processed_papers_count": 77,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T09:59:38.085602"
}
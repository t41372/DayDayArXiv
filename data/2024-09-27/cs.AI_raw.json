[
  {
    "arxiv_id": "2409.19185v1",
    "title": "Semi-Supervised Bone Marrow Lesion Detection from Knee MRI Segmentation Using Mask Inpainting Models",
    "authors": [
      "Shihua Qin",
      "Ming Zhang",
      "Juan Shan",
      "Taehoon Shin",
      "Jonghye Woo",
      "Fangxu Xing"
    ],
    "abstract": "Bone marrow lesions (BMLs) are critical indicators of knee osteoarthritis\n(OA). Since they often appear as small, irregular structures with\nindistinguishable edges in knee magnetic resonance images (MRIs), effective\ndetection of BMLs in MRI is vital for OA diagnosis and treatment. This paper\nproposes a semi-supervised local anomaly detection method using mask inpainting\nmodels for identification of BMLs in high-resolution knee MRI, effectively\nintegrating a 3D femur bone segmentation model, a large mask inpainting model,\nand a series of post-processing techniques. The method was evaluated using MRIs\nat various resolutions from a subset of the public Osteoarthritis Initiative\ndatabase. Dice score, Intersection over Union (IoU), and pixel-level\nsensitivity, specificity, and accuracy showed an advantage over the\nmultiresolution knowledge distillation method-a state-of-the-art global anomaly\ndetection method. Especially, segmentation performance is enhanced on\nhigher-resolution images, achieving an over two times performance increase on\nthe Dice score and the IoU score at a 448x448 resolution level. We also\ndemonstrate that with increasing size of the BML region, both the Dice and IoU\nscores improve as the proportion of distinguishable boundary decreases. The\nidentified BML masks can serve as markers for downstream tasks such as\nsegmentation and classification. The proposed method has shown a potential in\nimproving BML detection, laying a foundation for further advances in\nimaging-based OA research.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "5 pages, 3 figures, submitted to SPIE Conference on Image Processing",
    "pdf_url": "http://arxiv.org/pdf/2409.19185v1",
    "published_date": "2024-09-27 23:47:47 UTC",
    "updated_date": "2024-09-27 23:47:47 UTC"
  },
  {
    "arxiv_id": "2409.19182v2",
    "title": "Artificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation",
    "authors": [
      "Chun Jie Chong",
      "Zhihao Yao",
      "Iulian Neamtiu"
    ],
    "abstract": "Generating code via a LLM (rather than writing code from scratch), has\nexploded in popularity. However, the security implications of LLM-generated\ncode are still unknown. We performed a study that compared the security and\nquality of human-written code with that of LLM-generated code, for a wide range\nof programming tasks, including data structures, algorithms, cryptographic\nroutines, and LeetCode questions. To assess code security we used unit testing,\nfuzzing, and static analysis. For code quality, we focused on complexity and\nsize. We found that LLM can generate incorrect code that fails to implement the\nrequired functionality, especially for more complicated tasks; such errors can\nbe subtle. For example, for the cryptographic algorithm SHA1, LLM generated an\nincorrect implementation that nevertheless compiles. In cases where its\nfunctionality was correct, we found that LLM-generated code is less secure,\nprimarily due to the lack of defensive programming constructs, which invites a\nhost of security issues such as buffer overflows or integer overflows. Fuzzing\nhas revealed that LLM-generated code is more prone to hangs and crashes than\nhuman-written code. Quality-wise, we found that LLM generates bare-bones code\nthat lacks defensive programming constructs, and is typically more complex (per\nline of code) compared to human-written code. Next, we constructed a feedback\nloop that asked the LLM to re-generate the code and eliminate the found issues\n(e.g., malloc overflow, array index out of bounds, null dereferences). We found\nthat the LLM fails to eliminate such issues consistently: while succeeding in\nsome cases, we found instances where the re-generated, supposedly more secure\ncode, contains new issues; we also found that upon prompting, LLM can introduce\nissues in files that were issues-free before prompting.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19182v2",
    "published_date": "2024-09-27 23:41:51 UTC",
    "updated_date": "2024-10-12 03:35:42 UTC"
  },
  {
    "arxiv_id": "2410.01841v1",
    "title": "A GEN AI Framework for Medical Note Generation",
    "authors": [
      "Hui Yi Leong",
      "Yi Fan Gao",
      "Shuai Ji",
      "Bora Kalaycioglu",
      "Uktu Pamuksuz"
    ],
    "abstract": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "8 Figures, 7 page, IEEE standard research paper",
    "pdf_url": "http://arxiv.org/pdf/2410.01841v1",
    "published_date": "2024-09-27 23:05:02 UTC",
    "updated_date": "2024-09-27 23:05:02 UTC"
  },
  {
    "arxiv_id": "2409.19173v1",
    "title": "HM3: Heterogeneous Multi-Class Model Merging",
    "authors": [
      "Stefan Hackmann"
    ],
    "abstract": "Foundation language model deployments often include auxiliary guard-rail\nmodels to filter or classify text, detecting jailbreak attempts, biased or\ntoxic output, or ensuring topic adherence. These additional models increase the\ncomplexity and cost of model inference, especially since many are also large\nlanguage models. To address this issue, we explore training-free model merging\ntechniques to consolidate these models into a single, multi-functional model.\nWe propose Heterogeneous Multi-Class Model Merging (HM3) as a simple technique\nfor merging multi-class classifiers with heterogeneous label spaces. Unlike\nparameter-efficient fine-tuning techniques like LoRA, which require extensive\ntraining and add complexity during inference, recent advancements allow models\nto be merged in a training-free manner. We report promising results for merging\nBERT-based guard models, some of which attain an average F1-score higher than\nthe source models while reducing the inference time by up to 44%. We introduce\nself-merging to assess the impact of reduced task-vector density, finding that\nthe more poorly performing hate speech classifier benefits from self-merging\nwhile higher-performing classifiers do not, which raises questions about using\ntask vector reduction for model tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19173v1",
    "published_date": "2024-09-27 22:42:45 UTC",
    "updated_date": "2024-09-27 22:42:45 UTC"
  },
  {
    "arxiv_id": "2410.03717v1",
    "title": "Revisiting the Superficial Alignment Hypothesis",
    "authors": [
      "Mohit Raghavendra",
      "Vaskar Nath",
      "Sean Hendryx"
    ],
    "abstract": "The Superficial Alignment Hypothesis posits that almost all of a language\nmodel's abilities and knowledge are learned during pre-training, while\npost-training is about giving a model the right style and format. We re-examine\nthese claims by empirically studying the scaling behavior of post-training with\nincreasing finetuning examples and evaluating them using objective\ntask-specific standardized benchmarks. Through experiments with the Llama-3,\nMistral, and Llama-2 model families of multiple sizes, we observe that, similar\nto the pre-training scaling laws, post-training task performance scales as a\npower law against the number of finetuning examples. This power law\nrelationship holds across a broad array of capabilities, including mathematical\nreasoning, coding, instruction following, and multihop-reasoning. In addition,\nfor tasks like math and multihop reasoning, we observe that a handful of\nexamples merely align the model stylistically but do not saturate performance\non the benchmarks. Model performance is instead correlated with its reasoning\nability and it improves significantly with more examples, illustrating the need\nfor holistic evaluation programs leveraging objective benchmarks in addition to\nmeasurement of alignment to human preferences. We also observe that language\nmodels are not necessarily limited to using knowledge learned during\npre-training. With appropriate post-training, a model's ability to integrate\nnew knowledge greatly improves on downstream tasks like multihop\nquestion-answering. Taken together, these results shed new light on the\nSuperficial Alignment Hypothesis, suggesting that it is, at best, an\nover-simplification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03717v1",
    "published_date": "2024-09-27 22:14:10 UTC",
    "updated_date": "2024-09-27 22:14:10 UTC"
  },
  {
    "arxiv_id": "2409.19158v1",
    "title": "bnRep: A repository of Bayesian networks from the academic literature",
    "authors": [
      "Manuele Leonelli"
    ],
    "abstract": "Bayesian networks (BNs) are widely used for modeling complex systems with\nuncertainty, yet repositories of pre-built BNs remain limited. This paper\nintroduces bnRep, an open-source R package offering a comprehensive collection\nof documented BNs, facilitating benchmarking, replicability, and education.\nWith over 200 networks from academic publications, bnRep integrates seamlessly\nwith bnlearn and other R packages, providing users with interactive tools for\nnetwork exploration.",
    "categories": [
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19158v1",
    "published_date": "2024-09-27 21:50:50 UTC",
    "updated_date": "2024-09-27 21:50:50 UTC"
  },
  {
    "arxiv_id": "2409.19149v1",
    "title": "Multimodal Pragmatic Jailbreak on Text-to-image Models",
    "authors": [
      "Tong Liu",
      "Zhixin Lai",
      "Gengyuan Zhang",
      "Philip Torr",
      "Vera Demberg",
      "Volker Tresp",
      "Jindong Gu"
    ],
    "abstract": "Diffusion models have recently achieved remarkable advancements in terms of\nimage quality and fidelity to textual prompts. Concurrently, the safety of such\ngenerative models has become an area of growing concern. This work introduces a\nnovel type of jailbreak, which triggers T2I models to generate the image with\nvisual text, where the image and the text, although considered to be safe in\nisolation, combine to form unsafe content. To systematically explore this\nphenomenon, we propose a dataset to evaluate the current diffusion-based\ntext-to-image (T2I) models under such jailbreak. We benchmark nine\nrepresentative T2I models, including two close-source commercial models.\nExperimental results reveal a concerning tendency to produce unsafe content:\nall tested models suffer from such type of jailbreak, with rates of unsafe\ngeneration ranging from 8\\% to 74\\%. In real-world scenarios, various filters\nsuch as keyword blocklists, customized prompt filters, and NSFW image filters,\nare commonly employed to mitigate these risks. We evaluate the effectiveness of\nsuch filters against our jailbreak and found that, while current classifiers\nmay be effective for single modality detection, they fail to work against our\njailbreak. Our work provides a foundation for further development towards more\nsecure and reliable T2I models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19149v1",
    "published_date": "2024-09-27 21:23:46 UTC",
    "updated_date": "2024-09-27 21:23:46 UTC"
  },
  {
    "arxiv_id": "2409.19146v1",
    "title": "Bound Tightening Network for Robust Crowd Counting",
    "authors": [
      "Qiming Wu"
    ],
    "abstract": "Crowd Counting is a fundamental topic, aiming to estimate the number of\nindividuals in the crowded images or videos fed from surveillance cameras.\nRecent works focus on improving counting accuracy, while ignoring the certified\nrobustness of counting models. In this paper, we propose a novel Bound\nTightening Network (BTN) for Robust Crowd Counting. It consists of three parts:\nbase model, smooth regularization module and certify bound module. The core\nidea is to propagate the interval bound through the base model (certify bound\nmodule) and utilize the layer weights (smooth regularization module) to guide\nthe network learning. Experiments on different benchmark datasets for counting\ndemonstrate the effectiveness and efficiency of BTN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was done 2 years ago",
    "pdf_url": "http://arxiv.org/pdf/2409.19146v1",
    "published_date": "2024-09-27 21:18:31 UTC",
    "updated_date": "2024-09-27 21:18:31 UTC"
  },
  {
    "arxiv_id": "2409.19142v1",
    "title": "TTT4Rec: A Test-Time Training Approach for Rapid Adaption in Sequential Recommendation",
    "authors": [
      "Zhaoqi Yang",
      "Yanan Wang",
      "Yong Ge"
    ],
    "abstract": "Sequential recommendation tasks, which aim to predict the next item a user\nwill interact with, typically rely on models trained solely on historical data.\nHowever, in real-world scenarios, user behavior can fluctuate in the long\ninteraction sequences, and training data may be limited to model this dynamics.\nTo address this, Test-Time Training (TTT) offers a novel approach by using\nself-supervised learning during inference to dynamically update model\nparameters. This allows the model to adapt to new user interactions in\nreal-time, leading to more accurate recommendations. In this paper, we propose\nTTT4Rec, a sequential recommendation framework that integrates TTT to better\ncapture dynamic user behavior. By continuously updating model parameters during\ninference, TTT4Rec is particularly effective in scenarios where user\ninteraction sequences are long, training data is limited, or user behavior is\nhighly variable. We evaluate TTT4Rec on three widely-used recommendation\ndatasets, demonstrating that it achieves performance on par with or exceeding\nstate-of-the-art models. The codes are available at\nhttps://github.com/ZhaoqiZachYang/TTT4Rec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19142v1",
    "published_date": "2024-09-27 21:14:23 UTC",
    "updated_date": "2024-09-27 21:14:23 UTC"
  },
  {
    "arxiv_id": "2409.19138v1",
    "title": "Sequencing the Neurome: Towards Scalable Exact Parameter Reconstruction of Black-Box Neural Networks",
    "authors": [
      "Judah Goldfeder",
      "Quinten Roets",
      "Gabe Guo",
      "John Wright",
      "Hod Lipson"
    ],
    "abstract": "Inferring the exact parameters of a neural network with only query access is\nan NP-Hard problem, with few practical existing algorithms. Solutions would\nhave major implications for security, verification, interpretability, and\nunderstanding biological networks. The key challenges are the massive parameter\nspace, and complex non-linear relationships between neurons. We resolve these\nchallenges using two insights. First, we observe that almost all networks used\nin practice are produced by random initialization and first order optimization,\nan inductive bias that drastically reduces the practical parameter space.\nSecond, we present a novel query generation algorithm that produces maximally\ninformative samples, letting us untangle the non-linear relationships\nefficiently. We demonstrate reconstruction of a hidden network containing over\n1.5 million parameters, and of one 7 layers deep, the largest and deepest\nreconstructions to date, with max parameter difference less than 0.0001, and\nillustrate robustness and scalability across a variety of architectures,\ndatasets, and training procedures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.IT",
      "cs.NE",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19138v1",
    "published_date": "2024-09-27 21:02:04 UTC",
    "updated_date": "2024-09-27 21:02:04 UTC"
  },
  {
    "arxiv_id": "2409.19136v1",
    "title": "Kinematic Detection of Anomalies in Human Trajectory Data",
    "authors": [
      "Lance Kennedy",
      "Andreas ZÃ¼fle"
    ],
    "abstract": "Historically, much of the research in understanding, modeling, and mining\nhuman trajectory data has focused on where an individual stays. Thus, the focus\nof existing research has been on where a user goes. On the other hand, the\nstudy of how a user moves between locations has great potential for new\nresearch opportunities. Kinematic features describe how an individual moves\nbetween locations and can be used for tasks such as identification of\nindividuals or anomaly detection. Unfortunately, data availability and quality\nchallenges make kinematic trajectory mining difficult. In this paper, we\nleverage the Geolife dataset of human trajectories to investigate the viability\nof using kinematic features to identify individuals and detect anomalies. We\nshow that humans have an individual \"kinematic profile\" which can be used as a\nstrong signal to identify individual humans. We experimentally show that, for\nthe two use-cases of individual identification and anomaly detection, simple\nkinematic features fed to standard classification and anomaly detection\nalgorithms significantly improve results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19136v1",
    "published_date": "2024-09-27 20:53:11 UTC",
    "updated_date": "2024-09-27 20:53:11 UTC"
  },
  {
    "arxiv_id": "2409.19130v1",
    "title": "Multi-modal Cross-domain Self-supervised Pre-training for fMRI and EEG Fusion",
    "authors": [
      "Xinxu Wei",
      "Kanhao Zhao",
      "Yong Jiao",
      "Nancy B. Carlisle",
      "Hua Xie",
      "Gregory A. Fonzo",
      "Yu Zhang"
    ],
    "abstract": "Neuroimaging techniques including functional magnetic resonance imaging\n(fMRI) and electroencephalogram (EEG) have shown promise in detecting\nfunctional abnormalities in various brain disorders. However, existing studies\noften focus on a single domain or modality, neglecting the valuable\ncomplementary information offered by multiple domains from both fMRI and EEG,\nwhich is crucial for a comprehensive representation of disorder pathology. This\nlimitation poses a challenge in effectively leveraging the synergistic\ninformation derived from these modalities. To address this, we propose a\nMulti-modal Cross-domain Self-supervised Pre-training Model (MCSP), a novel\napproach that leverages self-supervised learning to synergize multi-modal\ninformation across spatial, temporal, and spectral domains. Our model employs\ncross-domain self-supervised loss that bridges domain differences by\nimplementing domain-specific data augmentation and contrastive loss, enhancing\nfeature discrimination. Furthermore, MCSP introduces cross-modal\nself-supervised loss to capitalize on the complementary information of fMRI and\nEEG, facilitating knowledge distillation within domains and maximizing\ncross-modal feature convergence. We constructed a large-scale pre-training\ndataset and pretrained MCSP model by leveraging proposed self-supervised\nparadigms to fully harness multimodal neuroimaging data. Through comprehensive\nexperiments, we have demonstrated the superior performance and generalizability\nof our model on multiple classification tasks. Our study contributes a\nsignificant advancement in the fusion of fMRI and EEG, marking a novel\nintegration of cross-domain features, which enriches the existing landscape of\nneuroimaging research, particularly within the context of mental disorder\nstudies.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19130v1",
    "published_date": "2024-09-27 20:25:17 UTC",
    "updated_date": "2024-09-27 20:25:17 UTC"
  },
  {
    "arxiv_id": "2409.19120v1",
    "title": "Secure Multiparty Generative AI",
    "authors": [
      "Manil Shrestha",
      "Yashodha Ravichandran",
      "Edward Kim"
    ],
    "abstract": "As usage of generative AI tools skyrockets, the amount of sensitive\ninformation being exposed to these models and centralized model providers is\nalarming. For example, confidential source code from Samsung suffered a data\nleak as the text prompt to ChatGPT encountered data leakage. An increasing\nnumber of companies are restricting the use of LLMs (Apple, Verizon, JPMorgan\nChase, etc.) due to data leakage or confidentiality issues. Also, an increasing\nnumber of centralized generative model providers are restricting, filtering,\naligning, or censoring what can be used. Midjourney and RunwayML, two of the\nmajor image generation platforms, restrict the prompts to their system via\nprompt filtering. Certain political figures are restricted from image\ngeneration, as well as words associated with women's health care, rights, and\nabortion.\n  In our research, we present a secure and private methodology for generative\nartificial intelligence that does not expose sensitive data or models to\nthird-party AI providers. Our work modifies the key building block of modern\ngenerative AI algorithms, e.g. the transformer, and introduces confidential and\nverifiable multiparty computations in a decentralized network to maintain the\n1) privacy of the user input and obfuscation to the output of the model, and 2)\nintroduce privacy to the model itself. Additionally, the sharding process\nreduces the computational burden on any one node, enabling the distribution of\nresources of large generative AI processes across multiple, smaller nodes. We\nshow that as long as there exists one honest node in the decentralized\ncomputation, security is maintained. We also show that the inference process\nwill still succeed if only a majority of the nodes in the computation are\nsuccessful. Thus, our method offers both secure and verifiable computation in a\ndecentralized network.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19120v1",
    "published_date": "2024-09-27 19:55:49 UTC",
    "updated_date": "2024-09-27 19:55:49 UTC"
  },
  {
    "arxiv_id": "2409.19104v1",
    "title": "Responsible AI in Open Ecosystems: Reconciling Innovation with Risk Assessment and Disclosure",
    "authors": [
      "Mahasweta Chakraborti",
      "Bert Joseph Prestoza",
      "Nicholas Vincent",
      "Seth Frey"
    ],
    "abstract": "The rapid scaling of AI has spurred a growing emphasis on ethical\nconsiderations in both development and practice. This has led to the\nformulation of increasingly sophisticated model auditing and reporting\nrequirements, as well as governance frameworks to mitigate potential risks to\nindividuals and society. At this critical juncture, we review the practical\nchallenges of promoting responsible AI and transparency in informal sectors\nlike OSS that support vital infrastructure and see widespread use. We focus on\nhow model performance evaluation may inform or inhibit probing of model\nlimitations, biases, and other risks. Our controlled analysis of 7903 Hugging\nFace projects found that risk documentation is strongly associated with\nevaluation practices. Yet, submissions (N=789) from the platform's most popular\ncompetitive leaderboard showed less accountability among high performers. Our\nfindings can inform AI providers and legal scholars in designing interventions\nand policies that preserve open-source innovation while incentivizing ethical\nuptake.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "[Under Review][WIP]",
    "pdf_url": "http://arxiv.org/pdf/2409.19104v1",
    "published_date": "2024-09-27 19:09:40 UTC",
    "updated_date": "2024-09-27 19:09:40 UTC"
  },
  {
    "arxiv_id": "2410.02823v1",
    "title": "DANA: Domain-Aware Neurosymbolic Agents for Consistency and Accuracy",
    "authors": [
      "Vinh Luong",
      "Sang Dinh",
      "Shruti Raghavan",
      "William Nguyen",
      "Zooey Nguyen",
      "Quynh Le",
      "Hung Vo",
      "Kentaro Maegaito",
      "Loc Nguyen",
      "Thao Nguyen",
      "Anh Hai Ha",
      "Christopher Nguyen"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ninherent probabilistic nature often leads to inconsistency and inaccuracy in\ncomplex problem-solving tasks. This paper introduces DANA (Domain-Aware\nNeurosymbolic Agent), an architecture that addresses these issues by\nintegrating domain-specific knowledge with neurosymbolic approaches. We begin\nby analyzing current AI architectures, including AutoGPT, LangChain ReAct and\nOpenAI's ChatGPT, through a neurosymbolic lens, highlighting how their reliance\non probabilistic inference contributes to inconsistent outputs. In response,\nDANA captures and applies domain expertise in both natural-language and\nsymbolic forms, enabling more deterministic and reliable problem-solving\nbehaviors. We implement a variant of DANA using Hierarchical Task Plans (HTPs)\nin the open-source OpenSSA framework. This implementation achieves over 90\\%\naccuracy on the FinanceBench financial-analysis benchmark, significantly\noutperforming current LLM-based systems in both consistency and accuracy.\nApplication of DANA in physical industries such as semiconductor shows that its\nflexible architecture for incorporating knowledge is effective in mitigating\nthe probabilistic limitations of LLMs and has potential in tackling complex,\nreal-world problems that require reliability and precision.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02823v1",
    "published_date": "2024-09-27 18:29:23 UTC",
    "updated_date": "2024-09-27 18:29:23 UTC"
  },
  {
    "arxiv_id": "2409.19078v2",
    "title": "Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data",
    "authors": [
      "Soroosh Tayebi Arasteh",
      "Mahshad Lotfinia",
      "Paula Andrea Perez-Toro",
      "Tomas Arias-Vergara",
      "Mahtab Ranji",
      "Juan Rafael Orozco-Arroyave",
      "Maria Schuster",
      "Andreas Maier",
      "Seung Hee Yang"
    ],
    "abstract": "Speech pathology has impacts on communication abilities and quality of life.\nWhile deep learning-based models have shown potential in diagnosing these\ndisorders, the use of sensitive data raises critical privacy concerns. Although\ndifferential privacy (DP) has been explored in the medical imaging domain, its\napplication in pathological speech analysis remains largely unexplored despite\nthe equally critical privacy concerns. This study is the first to investigate\nDP's impact on pathological speech data, focusing on the trade-offs between\nprivacy, diagnostic accuracy, and fairness. Using a large, real-world dataset\nof 200 hours of recordings from 2,839 German-speaking participants, we observed\na maximum accuracy reduction of 3.85% when training with DP with high privacy\nlevels. To highlight real-world privacy risks, we demonstrated the\nvulnerability of non-private models to explicit gradient inversion attacks,\nreconstructing identifiable speech samples and showcasing DP's effectiveness in\nmitigating these risks. To generalize our findings across languages and\ndisorders, we validated our approach on a dataset of Spanish-speaking\nParkinson's disease patients, leveraging pretrained models from healthy\nEnglish-speaking datasets, and demonstrated that careful pretraining on\nlarge-scale task-specific datasets can maintain favorable accuracy under DP\nconstraints. A comprehensive fairness analysis revealed minimal gender bias at\nreasonable privacy levels but underscored the need for addressing age-related\ndisparities. Our results establish that DP can balance privacy and utility in\nspeech disorder detection, while highlighting unique challenges in\nprivacy-fairness trade-offs for speech data. This provides a foundation for\nrefining DP methodologies and improving fairness across diverse patient groups\nin real-world deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19078v2",
    "published_date": "2024-09-27 18:25:54 UTC",
    "updated_date": "2024-12-26 12:56:59 UTC"
  },
  {
    "arxiv_id": "2409.19075v4",
    "title": "Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource Commonsense Reasoning",
    "authors": [
      "Yu Fu",
      "Jie He",
      "Yifan Yang",
      "Qun Liu",
      "Deyi Xiong"
    ],
    "abstract": "Meta learning has been widely used to exploit rich-resource source tasks to\nimprove the performance of low-resource target tasks. Unfortunately, most\nexisting meta learning approaches treat different source tasks equally,\nignoring the relatedness of source tasks to the target task in knowledge\ntransfer. To mitigate this issue, we propose a reinforcement-based multi-source\nmeta-transfer learning framework (Meta-RTL) for low-resource commonsense\nreasoning. In this framework, we present a reinforcement-based approach to\ndynamically estimating source task weights that measure the contribution of the\ncorresponding tasks to the target task in the meta-transfer learning. The\ndifferences between the general loss of the meta model and task-specific losses\nof source-specific temporal meta models on sampled target data are fed into the\npolicy network of the reinforcement learning module as rewards. The policy\nnetwork is built upon LSTMs that capture long-term dependencies on source task\nweight estimation across meta learning iterations. We evaluate the proposed\nMeta-RTL using both BERT and ALBERT as the backbone of the meta model on three\ncommonsense reasoning benchmark datasets. Experimental results demonstrate that\nMeta-RTL substantially outperforms strong baselines and previous task selection\nstrategies and achieves larger improvements on extremely low-resource settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19075v4",
    "published_date": "2024-09-27 18:22:22 UTC",
    "updated_date": "2025-04-11 14:38:25 UTC"
  },
  {
    "arxiv_id": "2409.19058v2",
    "title": "CLLMate: A Multimodal Benchmark for Weather and Climate Events Forecasting",
    "authors": [
      "Haobo Li",
      "Zhaowei Wang",
      "Jiachen Wang",
      "Yueya Wang",
      "Alexis Kai Hon Lau",
      "Huamin Qu"
    ],
    "abstract": "Forecasting weather and climate events is crucial for making appropriate\nmeasures to mitigate environmental hazards and minimize losses. However,\nexisting environmental forecasting research focuses narrowly on predicting\nnumerical meteorological variables (e.g., temperature), neglecting the\ntranslation of these variables into actionable textual narratives of events and\ntheir consequences. To bridge this gap, we proposed Weather and Climate Event\nForecasting (WCEF), a new task that leverages numerical meteorological raster\ndata and textual event data to predict weather and climate events. This task is\nchallenging to accomplish due to difficulties in aligning multimodal data and\nthe lack of supervised datasets. To address these challenges, we present\nCLLMate, the first multimodal dataset for WCEF, using 26,156 environmental news\narticles aligned with ERA5 reanalysis data. We systematically benchmark 23\nexisting MLLMs on CLLMate, including closed-source, open-source, and our\nfine-tuned models. Our experiments reveal the advantages and limitations of\nexisting MLLMs and the value of CLLMate for the training and benchmarking of\nthe WCEF task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19058v2",
    "published_date": "2024-09-27 18:00:13 UTC",
    "updated_date": "2025-02-16 10:05:11 UTC"
  },
  {
    "arxiv_id": "2409.19051v1",
    "title": "Multimodal Markup Document Models for Graphic Design Completion",
    "authors": [
      "Kotaro Kikuchi",
      "Naoto Inoue",
      "Mayu Otani",
      "Edgar Simo-Serra",
      "Kota Yamaguchi"
    ],
    "abstract": "This paper presents multimodal markup document models (MarkupDM) that can\ngenerate both markup language and images within interleaved multimodal\ndocuments. Unlike existing vision-and-language multimodal models, our MarkupDM\ntackles unique challenges critical to graphic design tasks: generating partial\nimages that contribute to the overall appearance, often involving transparency\nand varying sizes, and understanding the syntax and semantics of markup\nlanguages, which play a fundamental role as a representational format of\ngraphic designs. To address these challenges, we design an image quantizer to\ntokenize images of diverse sizes with transparency and modify a code language\nmodel to process markup languages and incorporate image modalities. We provide\nin-depth evaluations of our approach on three graphic design completion tasks:\ngenerating missing attribute values, images, and texts in graphic design\ntemplates. Results corroborate the effectiveness of our MarkupDM for graphic\ndesign tasks. We also discuss the strengths and weaknesses in detail, providing\ninsights for future research on multimodal document generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://cyberagentailab.github.io/MarkupDM/",
    "pdf_url": "http://arxiv.org/pdf/2409.19051v1",
    "published_date": "2024-09-27 18:00:01 UTC",
    "updated_date": "2024-09-27 18:00:01 UTC"
  },
  {
    "arxiv_id": "2409.18964v1",
    "title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation",
    "authors": [
      "Shaowei Liu",
      "Zhongzheng Ren",
      "Saurabh Gupta",
      "Shenlong Wang"
    ],
    "abstract": "We present PhysGen, a novel image-to-video generation method that converts a\nsingle image and an input condition (e.g., force and torque applied to an\nobject in the image) to produce a realistic, physically plausible, and\ntemporally consistent video. Our key insight is to integrate model-based\nphysical simulation with a data-driven video generation process, enabling\nplausible image-space dynamics. At the heart of our system are three core\ncomponents: (i) an image understanding module that effectively captures the\ngeometry, materials, and physical parameters of the image; (ii) an image-space\ndynamics simulation model that utilizes rigid-body physics and inferred\nparameters to simulate realistic behaviors; and (iii) an image-based rendering\nand refinement module that leverages generative video diffusion to produce\nrealistic video footage featuring the simulated motion. The resulting videos\nare realistic in both physics and appearance and are even precisely\ncontrollable, showcasing superior results over existing data-driven\nimage-to-video generation works through quantitative comparison and\ncomprehensive user study. PhysGen's resulting videos can be used for various\ndownstream applications, such as turning an image into a realistic animation or\nallowing users to interact with the image and create various dynamics. Project\npage: https://stevenlsw.github.io/physgen/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024. Project page:\n  https://stevenlsw.github.io/physgen/",
    "pdf_url": "http://arxiv.org/pdf/2409.18964v1",
    "published_date": "2024-09-27 17:59:57 UTC",
    "updated_date": "2024-09-27 17:59:57 UTC"
  },
  {
    "arxiv_id": "2409.18962v1",
    "title": "Exploring Token Pruning in Vision State Space Models",
    "authors": [
      "Zheng Zhan",
      "Zhenglun Kong",
      "Yifan Gong",
      "Yushu Wu",
      "Zichong Meng",
      "Hangyu Zheng",
      "Xuan Shen",
      "Stratis Ioannidis",
      "Wei Niu",
      "Pu Zhao",
      "Yanzhi Wang"
    ],
    "abstract": "State Space Models (SSMs) have the advantage of keeping linear computational\ncomplexity compared to attention modules in transformers, and have been applied\nto vision tasks as a new type of powerful vision foundation model. Inspired by\nthe observations that the final prediction in vision transformers (ViTs) is\nonly based on a subset of most informative tokens, we take the novel step of\nenhancing the efficiency of SSM-based vision models through token-based\npruning. However, direct applications of existing token pruning techniques\ndesigned for ViTs fail to deliver good performance, even with extensive\nfine-tuning. To address this issue, we revisit the unique computational\ncharacteristics of SSMs and discover that naive application disrupts the\nsequential token positions. This insight motivates us to design a novel and\ngeneral token pruning method specifically for SSM-based vision models. We first\nintroduce a pruning-aware hidden state alignment method to stabilize the\nneighborhood of remaining tokens for performance enhancement. Besides, based on\nour detailed analysis, we propose a token importance evaluation method adapted\nfor SSM models, to guide the token pruning. With efficient implementation and\npractical acceleration methods, our method brings actual speedup. Extensive\nexperiments demonstrate that our approach can achieve significant computation\nreduction with minimal impact on performance across different tasks. Notably,\nwe achieve 81.7\\% accuracy on ImageNet with a 41.6\\% reduction in the FLOPs for\npruned PlainMamba-L3. Furthermore, our work provides deeper insights into\nunderstanding the behavior of SSM-based vision models for future research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS'24",
    "pdf_url": "http://arxiv.org/pdf/2409.18962v1",
    "published_date": "2024-09-27 17:59:50 UTC",
    "updated_date": "2024-09-27 17:59:50 UTC"
  },
  {
    "arxiv_id": "2409.18961v1",
    "title": "ProMerge: Prompt and Merge for Unsupervised Instance Segmentation",
    "authors": [
      "Dylan Li",
      "Gyungin Shin"
    ],
    "abstract": "Unsupervised instance segmentation aims to segment distinct object instances\nin an image without relying on human-labeled data. This field has recently seen\nsignificant advancements, partly due to the strong local correspondences\nafforded by rich visual feature representations from self-supervised models\n(e.g., DINO). Recent state-of-the-art approaches use self-supervised features\nto represent images as graphs and solve a generalized eigenvalue system (i.e.,\nnormalized-cut) to generate foreground masks. While effective, this strategy is\nlimited by its attendant computational demands, leading to slow inference\nspeeds. In this paper, we propose Prompt and Merge (ProMerge), which leverages\nself-supervised visual features to obtain initial groupings of patches and\napplies a strategic merging to these segments, aided by a sophisticated\nbackground-based mask pruning technique. ProMerge not only yields competitive\nresults but also offers a significant reduction in inference time compared to\nstate-of-the-art normalized-cut-based approaches. Furthermore, when training an\nobject detector using our mask predictions as pseudo-labels, the resulting\ndetector surpasses the current leading unsupervised model on various\nchallenging instance segmentation benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV2024 camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2409.18961v1",
    "published_date": "2024-09-27 17:59:42 UTC",
    "updated_date": "2024-09-27 17:59:42 UTC"
  },
  {
    "arxiv_id": "2409.18959v2",
    "title": "O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions",
    "authors": [
      "Gen Li",
      "Yuling Yan"
    ],
    "abstract": "Score-based diffusion models, which generate new data by learning to reverse\na diffusion process that perturbs data from the target distribution into noise,\nhave achieved remarkable success across various generative tasks. Despite their\nsuperior empirical performance, existing theoretical guarantees are often\nconstrained by stringent assumptions or suboptimal convergence rates. In this\npaper, we establish a fast convergence theory for the denoising diffusion\nprobabilistic model (DDPM), a widely used SDE-based sampler, under minimal\nassumptions. Our analysis shows that, provided $\\ell_{2}$-accurate estimates of\nthe score functions, the total variation distance between the target and\ngenerated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic\nfactors), where $d$ is the data dimensionality and $T$ is the number of steps.\nThis result holds for any target distribution with finite first-order moment.\nMoreover, we show that with careful coefficient design, the convergence rate\nimproves to $O(k/T)$, where $k$ is the intrinsic dimension of the target data\ndistribution. This highlights the ability of DDPM to automatically adapt to\nunknown low-dimensional structures, a common feature of natural image\ndistributions. These results are achieved through a novel set of analytical\ntools that provides a fine-grained characterization of how the error propagates\nat each step of the reverse process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "add new results demonstrating the adaptivity of the DDPM sampler to\n  unknown low-dimensional structures",
    "pdf_url": "http://arxiv.org/pdf/2409.18959v2",
    "published_date": "2024-09-27 17:59:10 UTC",
    "updated_date": "2025-01-22 16:45:40 UTC"
  },
  {
    "arxiv_id": "2409.18957v3",
    "title": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction",
    "authors": [
      "Praneeth Vadlapati"
    ],
    "abstract": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach for classification tasks using Large Language Models\n(LLMs) in an explainable method. Unlike ML models, which rely heavily on data\ncleaning and feature engineering, this method streamlines the process using\nLLMs. This paper proposes a method called \"Language Model Learning (LML)\"\npowered by a new method called \"Data-Augmented Prediction (DAP).\" The\nclassification is performed by LLMs using a method similar to that used by\nhumans who manually explore and understand the data to decide classifications.\nIn the process of LML, a dataset is summarized and evaluated to determine the\nfeatures leading to each label the most. In the DAP process, the system uses\nthe data summary and a row of the testing dataset to automatically generate a\nquery to retrieve relevant rows from the dataset for context-aware\nclassification. LML and DAP unlock new possibilities in areas that require\nexplainable and context-aware decisions by ensuring satisfactory accuracy even\nwith complex data. The system scored an accuracy above 90% in some test cases,\nconfirming the effectiveness and potential of the system to outperform ML\nmodels in various scenarios. The source code is available at\nhttps://github.com/Pro-GenAI/LML-DAP",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Made the abstract and the content clearer",
    "pdf_url": "http://arxiv.org/pdf/2409.18957v3",
    "published_date": "2024-09-27 17:58:50 UTC",
    "updated_date": "2024-11-10 03:45:30 UTC"
  },
  {
    "arxiv_id": "2409.19044v1",
    "title": "On the Inductive Bias of Stacking Towards Improving Reasoning",
    "authors": [
      "Nikunj Saunshi",
      "Stefani Karp",
      "Shankar Krishnan",
      "Sobhan Miryoosefi",
      "Sashank J. Reddi",
      "Sanjiv Kumar"
    ],
    "abstract": "Given the increasing scale of model sizes, novel training strategies like\ngradual stacking [Gong et al., 2019, Reddi et al., 2023] have garnered\ninterest. Stacking enables efficient training by gradually growing the depth of\na model in stages and using layers from a smaller model in an earlier stage to\ninitialize the next stage. Although efficient for training, the model biases\ninduced by such growing approaches are largely unexplored. In this work, we\nexamine this fundamental aspect of gradual stacking, going beyond its\nefficiency benefits. We propose a variant of gradual stacking called MIDAS that\ncan speed up language model training by up to 40%. Furthermore we discover an\nintriguing phenomenon: MIDAS is not only training-efficient but surprisingly\nalso has an inductive bias towards improving downstream tasks, especially tasks\nthat require reasoning abilities like reading comprehension and math problems,\ndespite having similar or slightly worse perplexity compared to baseline\ntraining. To further analyze this inductive bias, we construct reasoning\nprimitives -- simple synthetic tasks that are building blocks for reasoning --\nand find that a model pretrained with stacking is significantly better than\nstandard pretraining on these primitives, with and without fine-tuning. This\nprovides stronger and more robust evidence for this inductive bias towards\nreasoning. These findings of training efficiency and inductive bias towards\nreasoning are verified at 1B, 2B and 8B parameter language models. Finally, we\nconjecture the underlying reason for this inductive bias by exploring the\nconnection of stacking to looped models and provide strong supporting empirical\nanalysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.19044v1",
    "published_date": "2024-09-27 17:58:21 UTC",
    "updated_date": "2024-09-27 17:58:21 UTC"
  },
  {
    "arxiv_id": "2410.00044v1",
    "title": "Artificial intelligence-based blockchain-driven financial default prediction",
    "authors": [
      "Junjun Huang"
    ],
    "abstract": "With the rapid development of technology, blockchain and artificial\nintelligence technology are playing a huge role in all walks of life. In the\nfinancial sector, blockchain solves many security problems in data storage and\nmanagement in traditional systems with its advantages of decentralization and\nsecurity. And artificial intelligence has huge advantages in financial\nforecasting and risk management through its powerful algorithmic modeling\ncapabilities. In financial default prediction using blockchain and artificial\nintelligence technology is a very powerful application. Blockchain technology\nguarantees the credibility of data and consistency on all nodes, and machine\nlearning builds a high-level default prediction model through detailed analysis\nof big data. This study offers financial institutions new thoughts on financial\ntechnology in terms of credit risk mitigation and financial system\nstabilization.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00044v1",
    "published_date": "2024-09-27 17:51:48 UTC",
    "updated_date": "2024-09-27 17:51:48 UTC"
  },
  {
    "arxiv_id": "2409.18946v3",
    "title": "Unconditional stability of a recurrent neural circuit implementing divisive normalization",
    "authors": [
      "Shivang Rawat",
      "David J. Heeger",
      "Stefano Martiniani"
    ],
    "abstract": "Stability in recurrent neural models poses a significant challenge,\nparticularly in developing biologically plausible neurodynamical models that\ncan be seamlessly trained. Traditional cortical circuit models are notoriously\ndifficult to train due to expansive nonlinearities in the dynamical system,\nleading to an optimization problem with nonlinear stability constraints that\nare difficult to impose. Conversely, recurrent neural networks (RNNs) excel in\ntasks involving sequential data but lack biological plausibility and\ninterpretability. In this work, we address these challenges by linking dynamic\ndivisive normalization (DN) to the stability of ORGaNICs, a biologically\nplausible recurrent cortical circuit model that dynamically achieves DN and\nthat has been shown to simulate a wide range of neurophysiological phenomena.\nBy using the indirect method of Lyapunov, we prove the remarkable property of\nunconditional local stability for an arbitrary-dimensional ORGaNICs circuit\nwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to a\nsystem of coupled damped harmonic oscillators, which enables us to derive the\ncircuit's energy function, providing a normative principle of what the circuit,\nand individual neurons, aim to accomplish. Further, for a generic recurrent\nweight matrix, we prove the stability of the 2D model and demonstrate\nempirically that stability holds in higher dimensions. Finally, we show that\nORGaNICs can be trained by backpropagation through time without gradient\nclipping/scaling, thanks to its intrinsic stability property and adaptive time\nconstants, which address the problems of exploding, vanishing, and oscillating\ngradients. By evaluating the model's performance on RNN benchmarks, we find\nthat ORGaNICs outperform alternative neurodynamical models on static image\nclassification tasks and perform comparably to LSTMs on sequential tasks.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "math.DS"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18946v3",
    "published_date": "2024-09-27 17:46:05 UTC",
    "updated_date": "2025-01-15 02:42:42 UTC"
  },
  {
    "arxiv_id": "2409.18941v1",
    "title": "Building Trust Through Voice: How Vocal Tone Impacts User Perception of Attractiveness of Voice Assistants",
    "authors": [
      "Sabid Bin Habib Pias",
      "Alicia Freel",
      "Ran Huang",
      "Donald Williamson",
      "Minjeong Kim",
      "Apu Kapadia"
    ],
    "abstract": "Voice Assistants (VAs) are popular for simple tasks, but users are often\nhesitant to use them for complex activities like online shopping. We explored\nwhether the vocal characteristics like the VA's vocal tone, can make VAs\nperceived as more attractive and trustworthy to users for complex tasks. Our\nfindings show that the tone of the VA voice significantly impacts its perceived\nattractiveness and trustworthiness. Participants in our experiment were more\nlikely to be attracted to VAs with positive or neutral tones and ultimately\ntrusted the VAs they found more attractive. We conclude that VA's perceived\ntrustworthiness can be enhanced through thoughtful voice design, incorporating\na variety of vocal tones.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Extended Abstract",
    "pdf_url": "http://arxiv.org/pdf/2409.18941v1",
    "published_date": "2024-09-27 17:41:18 UTC",
    "updated_date": "2024-09-27 17:41:18 UTC"
  },
  {
    "arxiv_id": "2409.18938v2",
    "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding",
    "authors": [
      "Heqing Zou",
      "Tianze Luo",
      "Guiyang Xie",
      "Victor",
      "Zhang",
      "Fengmao Lv",
      "Guangcong Wang",
      "Junyang Chen",
      "Zhuochen Wang",
      "Hansheng Zhang",
      "Huaijian Zhang"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.18938v2",
    "published_date": "2024-09-27 17:38:36 UTC",
    "updated_date": "2024-12-03 03:56:52 UTC"
  },
  {
    "arxiv_id": "2410.09062v1",
    "title": "Volatility Forecasting in Global Financial Markets Using TimeMixer",
    "authors": [
      "Alex Li"
    ],
    "abstract": "Predicting volatility in financial markets, including stocks, index ETFs,\nforeign exchange, and cryptocurrencies, remains a challenging task due to the\ninherent complexity and non-linear dynamics of these time series. In this\nstudy, I apply TimeMixer, a state-of-the-art time series forecasting model, to\npredict the volatility of global financial assets. TimeMixer utilizes a\nmultiscale-mixing approach that effectively captures both short-term and\nlong-term temporal patterns by analyzing data across different scales. My\nempirical results reveal that while TimeMixer performs exceptionally well in\nshort-term volatility forecasting, its accuracy diminishes for longer-term\npredictions, particularly in highly volatile markets. These findings highlight\nTimeMixer's strength in capturing short-term volatility, making it highly\nsuitable for practical applications in financial risk management, where precise\nshort-term forecasts are critical. However, the model's limitations in\nlong-term forecasting point to potential areas for further refinement.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST",
    "comment": "20 pages and 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09062v1",
    "published_date": "2024-09-27 17:35:28 UTC",
    "updated_date": "2024-09-27 17:35:28 UTC"
  },
  {
    "arxiv_id": "2409.18924v2",
    "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow",
    "authors": [
      "Huizi Yu",
      "Jiayan Zhou",
      "Lingyao Li",
      "Shan Chen",
      "Jack Gallifant",
      "Anye Shi",
      "Xiang Li",
      "Wenyue Hua",
      "Mingyu Jin",
      "Guang Chen",
      "Yang Zhou",
      "Zhao Li",
      "Trisha Gupte",
      "Ming-Li Chen",
      "Zahra Azizi",
      "Yongfeng Zhang",
      "Themistocles L. Assimes",
      "Xin Ma",
      "Danielle S. Bitterman",
      "Lin Lu",
      "Lizhou Fan"
    ],
    "abstract": "Simulated patient systems play a crucial role in modern medical education and\nresearch, providing safe, integrative learning environments and enabling\nclinical decision-making simulations. Large Language Models (LLM) could advance\nsimulated patient systems by replicating medical conditions and patient-doctor\ninteractions with high fidelity and low cost. However, ensuring the\neffectiveness and trustworthiness of these systems remains a challenge, as they\nrequire a large, diverse, and precise patient knowledgebase, along with a\nrobust and stable knowledge diffusion to users. Here, we developed AIPatient,\nan advanced simulated patient system with AIPatient Knowledge Graph (AIPatient\nKG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning\nRAG) agentic workflow as the generation backbone. AIPatient KG samples data\nfrom Electronic Health Records (EHRs) in the Medical Information Mart for\nIntensive Care (MIMIC)-III database, producing a clinically diverse and\nrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).\nReasoning RAG leverages six LLM powered agents spanning tasks including\nretrieval, KG query generation, abstraction, checker, rewrite, and\nsummarization. This agentic framework reaches an overall accuracy of 94.15% in\nEHR-based medical Question Answering (QA), outperforming benchmarks that use\neither no agent or only partial agent integration. Our system also presents\nhigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade\n5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value\n0.782, p>0.1). The promising performance of the AIPatient system highlights its\npotential to support a wide range of applications, including medical education,\nmodel evaluation, and system integration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "42 pages, 6 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.18924v2",
    "published_date": "2024-09-27 17:17:15 UTC",
    "updated_date": "2024-10-01 17:49:00 UTC"
  },
  {
    "arxiv_id": "2409.18911v1",
    "title": "Soft Measures for Extracting Causal Collective Intelligence",
    "authors": [
      "Maryam Berijanian",
      "Spencer Dork",
      "Kuldeep Singh",
      "Michael Riley Millikan",
      "Ashlin Riggs",
      "Aadarsh Swaminathan",
      "Sarah L. Gibbs",
      "Scott E. Friedman",
      "Nathan Brugnone"
    ],
    "abstract": "Understanding and modeling collective intelligence is essential for\naddressing complex social systems. Directed graphs called fuzzy cognitive maps\n(FCMs) offer a powerful tool for encoding causal mental models, but extracting\nhigh-integrity FCMs from text is challenging. This study presents an approach\nusing large language models (LLMs) to automate FCM extraction. We introduce\nnovel graph-based similarity measures and evaluate them by correlating their\noutputs with human judgments through the Elo rating system. Results show\npositive correlations with human evaluations, but even the best-performing\nmeasure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs\nimproves performance, but existing measures still fall short. This study\nhighlights the need for soft similarity measures tailored to FCM extraction,\nadvancing collective intelligence modeling with NLP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready version accepted for publication in the EMNLP 2024\n  Workshop NLP4Science",
    "pdf_url": "http://arxiv.org/pdf/2409.18911v1",
    "published_date": "2024-09-27 16:54:36 UTC",
    "updated_date": "2024-09-27 16:54:36 UTC"
  },
  {
    "arxiv_id": "2409.18901v1",
    "title": "Improving Visual Object Tracking through Visual Prompting",
    "authors": [
      "Shih-Fang Chen",
      "Jun-Cheng Chen",
      "I-Hong Jhuo",
      "Yen-Yu Lin"
    ],
    "abstract": "Learning a discriminative model to distinguish a target from its surrounding\ndistractors is essential to generic visual object tracking. Dynamic target\nrepresentation adaptation against distractors is challenging due to the limited\ndiscriminative capabilities of prevailing trackers. We present a new visual\nPrompting mechanism for generic Visual Object Tracking (PiVOT) to address this\nissue. PiVOT proposes a prompt generation network with the pre-trained\nfoundation model CLIP to automatically generate and refine visual prompts,\nenabling the transfer of foundation model knowledge for tracking. While CLIP\noffers broad category-level knowledge, the tracker, trained on\ninstance-specific data, excels at recognizing unique object instances. Thus,\nPiVOT first compiles a visual prompt highlighting potential target locations.\nTo transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to\nrefine the visual prompt based on the similarities between candidate objects\nand the reference templates across potential targets. Once the visual prompt is\nrefined, it can better highlight potential target locations, thereby reducing\nirrelevant prompt information. With the proposed prompting mechanism, the\ntracker can generate improved instance-aware feature maps through the guidance\nof the visual prompt, thus effectively reducing distractors. The proposed\nmethod does not involve CLIP during training, thereby keeping the same training\ncomplexity and preserving the generalization capability of the pretrained\nfoundation model. Extensive experiments across multiple benchmarks indicate\nthat PiVOT, using the proposed prompting method can suppress distracting\nobjects and enhance the tracker.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.IV",
      "68",
      "I.4; I.2; I.5; I.4.1; I.4.8; I.4.9; I.4.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted and to appear in IEEE Transactions on Multimedia",
    "pdf_url": "http://arxiv.org/pdf/2409.18901v1",
    "published_date": "2024-09-27 16:39:50 UTC",
    "updated_date": "2024-09-27 16:39:50 UTC"
  },
  {
    "arxiv_id": "2409.18895v1",
    "title": "Multi-Source Hard and Soft Information Fusion Approach for Accurate Cryptocurrency Price Movement Prediction",
    "authors": [
      "Saeed Mohammadi Dashtaki",
      "Mehdi Hosseini Chagahi",
      "Behzad Moshiri",
      "Md. Jalil Piran"
    ],
    "abstract": "One of the most important challenges in the financial and cryptocurrency\nfield is accurately predicting cryptocurrency price trends. Leveraging\nartificial intelligence (AI) is beneficial in addressing this challenge.\nCryptocurrency markets, marked by substantial growth and volatility, attract\ninvestors and scholars keen on deciphering and forecasting cryptocurrency price\nmovements. The vast and diverse array of data available for such predictions\nincreases the complexity of the task. In our study, we introduce a novel\napproach termed hard and soft information fusion (HSIF) to enhance the accuracy\nof cryptocurrency price movement forecasts. The hard information component of\nour approach encompasses historical price records alongside technical\nindicators. Complementing this, the soft data component extracts from X\n(formerly Twitter), encompassing news headlines and tweets about the\ncryptocurrency. To use this data, we use the Bidirectional Encoder\nRepresentations from Transformers (BERT)-based sentiment analysis method,\nfinancial BERT (FinBERT), which performs best. Finally, our model feeds on the\ninformation set including processed hard and soft data. We employ the\nbidirectional long short-term memory (BiLSTM) model because processing\ninformation in both forward and backward directions can capture long-term\ndependencies in sequential information. Our empirical findings emphasize the\nsuperiority of the HSIF approach over models dependent on single-source data by\ntesting on Bitcoin-related data. By fusing hard and soft information on Bitcoin\ndataset, our model has about 96.8\\% accuracy in predicting price movement.\nIncorporating information enables our model to grasp the influence of social\nsentiment on price fluctuations, thereby supplementing the technical\nanalysis-based predictions derived from hard information.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18895v1",
    "published_date": "2024-09-27 16:32:57 UTC",
    "updated_date": "2024-09-27 16:32:57 UTC"
  },
  {
    "arxiv_id": "2409.18878v2",
    "title": "Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models",
    "authors": [
      "Zehan Li",
      "Yan Hu",
      "Scott Lane",
      "Salih Selek",
      "Lokesh Shahani",
      "Rodrigo Machado-Vieira",
      "Jair Soares",
      "Hua Xu",
      "Hongfang Liu",
      "Ming Huang"
    ],
    "abstract": "Accurate identification and categorization of suicidal events can yield\nbetter suicide precautions, reducing operational burden, and improving care\nquality in high-acuity psychiatric settings. Pre-trained language models offer\npromise for identifying suicidality from unstructured clinical narratives. We\nevaluated the performance of four BERT-based models using two fine-tuning\nstrategies (multiple single-label and single multi-label) for detecting\ncoexisting suicidal events from 500 annotated psychiatric evaluation notes. The\nnotes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure\nto suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed\nother models using multiple single-label classification strategy (acc=0.86,\nF1=0.78). MentalBERT (acc=0.83, F1=0.74) also exceeded BioClinicalBERT\n(acc=0.82, F1=0.72) which outperformed BERT (acc=0.80, F1=0.70). RoBERTa\nfine-tuned with single multi-label classification further improved the model\nperformance (acc=0.88, F1=0.81). The findings highlight that the model\noptimization, pretraining with domain-relevant data, and the single multi-label\nclassification strategy enhance the model performance of suicide phenotyping.\nKeywords: EHR-based Phenotyping; Natural Language Processing; Secondary Use of\nEHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "submitted to AMIA Informatics Summit 2025 as a conference paper",
    "pdf_url": "http://arxiv.org/pdf/2409.18878v2",
    "published_date": "2024-09-27 16:13:38 UTC",
    "updated_date": "2024-10-03 20:49:55 UTC"
  },
  {
    "arxiv_id": "2409.18877v2",
    "title": "UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception",
    "authors": [
      "Chuang Chen",
      "Xiao Sun",
      "Zhi Liu"
    ],
    "abstract": "Visual emotion analysis holds significant research value in both computer\nvision and psychology. However, existing methods for visual emotion analysis\nsuffer from limited generalizability due to the ambiguity of emotion perception\nand the diversity of data scenarios. To tackle this issue, we introduce\nUniEmoX, a cross-modal semantic-guided large-scale pretraining framework.\nInspired by psychological research emphasizing the inseparability of the\nemotional exploration process from the interaction between individuals and\ntheir environment, UniEmoX integrates scene-centric and person-centric\nlow-level image spatial structural information, aiming to derive more nuanced\nand discriminative emotional representations. By exploiting the similarity\nbetween paired and unpaired image-text samples, UniEmoX distills rich semantic\nknowledge from the CLIP model to enhance emotional embedding representations\nmore effectively. To the best of our knowledge, this is the first large-scale\npretraining framework that integrates psychological theories with contemporary\ncontrastive learning and masked image modeling techniques for emotion analysis\nacross diverse scenarios. Additionally, we develop a visual emotional dataset\ntitled Emo8. Emo8 samples cover a range of domains, including cartoon, natural,\nrealistic, science fiction and advertising cover styles, covering nearly all\ncommon emotional scenes. Comprehensive experiments conducted on six benchmark\ndatasets across two downstream tasks validate the effectiveness of UniEmoX. The\nsource code is available at https://github.com/chincharles/u-emo.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.18877v2",
    "published_date": "2024-09-27 16:12:51 UTC",
    "updated_date": "2024-09-30 13:58:29 UTC"
  },
  {
    "arxiv_id": "2409.18874v1",
    "title": "CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting",
    "authors": [
      "Josef Koumar",
      "Karel Hynek",
      "TomÃ¡Å¡ Äejka",
      "Pavel Å iÅ¡ka"
    ],
    "abstract": "Anomaly detection in network traffic is crucial for maintaining the security\nof computer networks and identifying malicious activities. One of the primary\napproaches to anomaly detection are methods based on forecasting. Nevertheless,\nextensive real-world network datasets for forecasting and anomaly detection\ntechniques are missing, potentially causing performance overestimation of\nanomaly detection algorithms. This manuscript addresses this gap by introducing\na dataset comprising time series data of network entities' behavior, collected\nfrom the CESNET3 network. The dataset was created from 40 weeks of network\ntraffic of 275 thousand active IP addresses. The ISP origin of the presented\ndata ensures a high level of variability among network entities, which forms a\nunique and authentic challenge for forecasting and anomaly detection models. It\nprovides valuable insights into the practical deployment of forecast-based\nanomaly detection approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18874v1",
    "published_date": "2024-09-27 16:10:11 UTC",
    "updated_date": "2024-09-27 16:10:11 UTC"
  },
  {
    "arxiv_id": "2409.18868v1",
    "title": "Individuation in Neural Models with and without Visual Grounding",
    "authors": [
      "Alexey Tikhonov",
      "Lisa Bylinina",
      "Ivan P. Yamshchikov"
    ],
    "abstract": "We show differences between a language-and-vision model CLIP and two\ntext-only models - FastText and SBERT - when it comes to the encoding of\nindividuation information. We study latent representations that CLIP provides\nfor substrates, granular aggregates, and various numbers of objects. We\ndemonstrate that CLIP embeddings capture quantitative differences in\nindividuation better than models trained on text-only data. Moreover, the\nindividuation hierarchy we deduce from the CLIP embeddings agrees with the\nhierarchies proposed in linguistics and cognitive science.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.4; J.4; I.6.8; I.2.10"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18868v1",
    "published_date": "2024-09-27 16:04:06 UTC",
    "updated_date": "2024-09-27 16:04:06 UTC"
  },
  {
    "arxiv_id": "2409.18865v2",
    "title": "Positional Encoder Graph Quantile Neural Networks for Geographic Data",
    "authors": [
      "William E. R. de Amorim",
      "Scott A. Sisson",
      "T. Rodrigues",
      "David J. Nott",
      "Guilherme S. Rodrigues"
    ],
    "abstract": "Positional Encoder Graph Neural Networks (PE-GNNs) are among the most\neffective models for learning from continuous spatial data. However, their\npredictive distributions are often poorly calibrated, limiting their utility in\napplications that require reliable uncertainty quantification. We propose the\nPositional Encoder Graph Quantile Neural Network (PE-GQNN), a novel framework\nthat combines PE-GNNs with Quantile Neural Networks, partially monotonic neural\nblocks, and post-hoc recalibration techniques. The PE-GQNN enables flexible and\nrobust conditional density estimation with minimal assumptions about the target\ndistribution, and it extends naturally to tasks beyond spatial data. Empirical\nresults on benchmark datasets show that the PE-GQNN outperforms existing\nmethods in both predictive accuracy and uncertainty quantification, without\nincurring additional computational cost. We also provide theoretical insights\nand identify important special cases arising from our formulation, including\nthe PE-GNN.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "stat.ML",
    "comment": "12 main text pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18865v2",
    "published_date": "2024-09-27 16:02:12 UTC",
    "updated_date": "2025-05-15 19:11:12 UTC"
  },
  {
    "arxiv_id": "2409.18857v2",
    "title": "Mitigating Selection Bias with Node Pruning and Auxiliary Options",
    "authors": [
      "Hyeong Kyu Choi",
      "Weijie Xu",
      "Chi Xue",
      "Stephanie Eckman",
      "Chandan K. Reddy"
    ],
    "abstract": "Large language models (LLMs) often exhibit systematic preferences for certain\nanswer choices when responding to multiple-choice questions-a behavior known as\nselection bias. This bias reduces the accuracy and reliability of LLM outputs,\nlimiting their usefulness in decision-critical applications. While prior work\nhas focused on adjusting model inputs or outputs to mitigate this issue, our\nwork takes a fundamentally different approach by identifying and removing the\ninternal sources of bias. We introduce two methods: Bias Node Pruning (BNP),\nwhich prunes parameters that contribute to selection bias, and Auxiliary Option\nInjection (AOI), which introduces an additional answer choice to reduce bias in\nboth white-box and black-box settings. To address the shortcomings of existing\nevaluation metrics, we propose Choice Kullback-Leibler Divergence (CKLD), a new\nmetric that captures distributional imbalances in model predictions.\nExperiments on three LLMs across multiple datasets demonstrate that our methods\nconsistently improve answer accuracy while reducing selection bias, providing a\nrobust solution for both open- and closed-source models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2409.18857v2",
    "published_date": "2024-09-27 15:53:54 UTC",
    "updated_date": "2025-05-17 04:21:30 UTC"
  },
  {
    "arxiv_id": "2409.18828v2",
    "title": "MECG-E: Mamba-based ECG Enhancer for Baseline Wander Removal",
    "authors": [
      "Kuo-Hsuan Hung",
      "Kuan-Chen Wang",
      "Kai-Chun Liu",
      "Wei-Lun Chen",
      "Xugang Lu",
      "Yu Tsao",
      "Chii-Wann Lin"
    ],
    "abstract": "Electrocardiogram (ECG) is an important non-invasive method for diagnosing\ncardiovascular disease. However, ECG signals are susceptible to noise\ncontamination, such as electrical interference or signal wandering, which\nreduces diagnostic accuracy. Various ECG denoising methods have been proposed,\nbut most existing methods yield suboptimal performance under very noisy\nconditions or require several steps during inference, leading to latency during\nonline processing. In this paper, we propose a novel ECG denoising model,\nnamely Mamba-based ECG Enhancer (MECG-E), which leverages the Mamba\narchitecture known for its fast inference and outstanding nonlinear mapping\ncapabilities. Experimental results indicate that MECG-E surpasses several\nwell-known existing models across multiple metrics under different noise\nconditions. Additionally, MECG-E requires less inference time than\nstate-of-the-art diffusion-based ECG denoisers, demonstrating the model's\nfunctionality and efficiency.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted at IEEE BigData 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18828v2",
    "published_date": "2024-09-27 15:22:44 UTC",
    "updated_date": "2024-11-24 07:27:33 UTC"
  },
  {
    "arxiv_id": "2409.18814v1",
    "title": "Early diagnosis of Alzheimer's disease from MRI images with deep learning model",
    "authors": [
      "Sajjad Aghasi Javid",
      "Mahmood Mohassel Feghhi"
    ],
    "abstract": "It is acknowledged that the most common cause of dementia worldwide is\nAlzheimer's disease (AD). This condition progresses in severity from mild to\nsevere and interferes with people's everyday routines. Early diagnosis plays a\ncritical role in patient care and clinical trials. Convolutional neural\nnetworks (CNN) are used to create a framework for identifying specific disease\nfeatures from MRI scans Classification of dementia involves approaches such as\nmedical history review, neuropsychological tests, and magnetic resonance\nimaging (MRI). However, the image dataset obtained from Kaggle faces a\nsignificant issue of class imbalance, which requires equal distribution of\nsamples from each class to address. In this article, to address this imbalance,\nthe Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore,\na pre-trained convolutional neural network has been applied to the DEMNET\ndementia network to extract key features from AD images. The proposed model\nachieved an impressive accuracy of 98.67%.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 3 figures, Presented at the 20-th CSI International\n  Symposium on Artificial Intelligence and Signal Processing (AISP) 21-22\n  February, 2024, Mazandaran University of Science and Technology, Babol, Iran",
    "pdf_url": "http://arxiv.org/pdf/2409.18814v1",
    "published_date": "2024-09-27 15:07:26 UTC",
    "updated_date": "2024-09-27 15:07:26 UTC"
  },
  {
    "arxiv_id": "2409.18812v1",
    "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
    "authors": [
      "Hamed Babaei Giglou",
      "Jennifer D'Souza",
      "SÃ¶ren Auer"
    ],
    "abstract": "In response to the growing complexity and volume of scientific literature,\nthis paper introduces the LLMs4Synthesis framework, designed to enhance the\ncapabilities of Large Language Models (LLMs) in generating high-quality\nscientific syntheses. This framework addresses the need for rapid, coherent,\nand contextually rich integration of scientific insights, leveraging both\nopen-source and proprietary LLMs. It also examines the effectiveness of LLMs in\nevaluating the integrity and reliability of these syntheses, alleviating\ninadequacies in current quantitative metrics. Our study contributes to this\nfield by developing a novel methodology for processing scientific papers,\ndefining new synthesis types, and establishing nine detailed quality criteria\nfor evaluating syntheses. The integration of LLMs with reinforcement learning\nand AI feedback is proposed to optimize synthesis quality, ensuring alignment\nwith established criteria. The LLMs4Synthesis framework and its components are\nmade available, promising to enhance both the generation and evaluation\nprocesses in scientific research synthesis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures, Accepted to JCDL 2024 Research Track",
    "pdf_url": "http://arxiv.org/pdf/2409.18812v1",
    "published_date": "2024-09-27 15:04:39 UTC",
    "updated_date": "2024-09-27 15:04:39 UTC"
  },
  {
    "arxiv_id": "2409.18798v1",
    "title": "Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public Perceptions with BERTopic and GPT-4 Topic Fine-Tuning",
    "authors": [
      "Tyreal Yizhou Qian",
      "Bo Yu",
      "Weizhe Li",
      "Chenglong Xu"
    ],
    "abstract": "This study examined the public opinions of esports at the 2023 Asian Games\nand value co-creation during the event using an LLM-enhanced BERTopic modeling\nanalysis. We identified five major themes representing public perceptions, as\nwell as how major stakeholders co-created value within and beyond the esports\necosystem. Key findings highlighted the strategic use of social media marketing\nto influence public opinion and promote esports events and brands, emphasizing\nthe importance of event logistics and infrastructure. Additionally, the study\nrevealed the co-creation value contributed by stakeholders outside the\ntraditional esports ecosystem, particularly in promoting national\nrepresentation and performance. Our findings supported the ongoing efforts to\nlegitimize esports as a sport, noting that mainstream recognition remains a\nchallenge. The inclusion of esports as a medal event showcased broader\nacceptance and helped mitigate negative public perceptions. Moreover,\ncontributions from non-traditional stakeholders underscored the value of\ncross-subcultural collaborations in esports.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18798v1",
    "published_date": "2024-09-27 14:53:04 UTC",
    "updated_date": "2024-09-27 14:53:04 UTC"
  },
  {
    "arxiv_id": "2409.18796v1",
    "title": "Hierarchical Federated ADMM",
    "authors": [
      "Seyed Mohammad Azimi-Abarghouyi",
      "Nicola Bastianello",
      "Karl H. Johansson",
      "Viktoria Fodor"
    ],
    "abstract": "In this paper, we depart from the widely-used gradient descent-based\nhierarchical federated learning (FL) algorithms to develop a novel hierarchical\nFL framework based on the alternating direction method of multipliers (ADMM).\nWithin this framework, we propose two novel FL algorithms, which both use ADMM\nin the top layer: one that employs ADMM in the lower layer and another that\nuses the conventional gradient descent-based approach. The proposed framework\nenhances privacy, and experiments demonstrate the superiority of the proposed\nalgorithms compared to the conventional algorithms in terms of learning\nconvergence and accuracy. Additionally, gradient descent on the lower layer\nperforms well even if the number of local steps is very limited, while ADMM on\nboth layers lead to better performance otherwise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18796v1",
    "published_date": "2024-09-27 14:50:36 UTC",
    "updated_date": "2024-09-27 14:50:36 UTC"
  },
  {
    "arxiv_id": "2409.18786v1",
    "title": "A Survey on the Honesty of Large Language Models",
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ],
    "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs)\nwith human values, requiring these models to recognize what they know and don't\nknow and be able to faithfully express their knowledge. Despite promising,\ncurrent LLMs still exhibit significant dishonest behaviors, such as confidently\npresenting wrong answers or failing to express what they know. In addition,\nresearch on the honesty of LLMs also faces challenges, including varying\ndefinitions of honesty, difficulties in distinguishing between known and\nunknown knowledge, and a lack of comprehensive understanding of related\nresearch. To address these issues, we provide a survey on the honesty of LLMs,\ncovering its clarification, evaluation approaches, and strategies for\nimprovement. Moreover, we offer insights for future research, aiming to inspire\nfurther exploration in this important area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://github.com/SihengLi99/LLM-Honesty-Survey",
    "pdf_url": "http://arxiv.org/pdf/2409.18786v1",
    "published_date": "2024-09-27 14:34:54 UTC",
    "updated_date": "2024-09-27 14:34:54 UTC"
  },
  {
    "arxiv_id": "2409.18778v1",
    "title": "HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation",
    "authors": [
      "Joseph Cotnareanu",
      "Zhanguang Zhang",
      "Hui-Ling Zhen",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "abstract": "Efficiently determining the satisfiability of a boolean equation -- known as\nthe SAT problem for brevity -- is crucial in various industrial problems.\nRecently, the advent of deep learning methods has introduced significant\npotential for enhancing SAT solving. However, a major barrier to the\nadvancement of this field has been the scarcity of large, realistic datasets.\nThe majority of current public datasets are either randomly generated or\nextremely limited, containing only a few examples from unrelated problem\nfamilies. These datasets are inadequate for meaningful training of deep\nlearning methods. In light of this, researchers have started exploring\ngenerative techniques to create data that more accurately reflect SAT problems\nencountered in practical situations. These methods have so far suffered from\neither the inability to produce challenging SAT problems or time-scalability\nobstacles. In this paper we address both by identifying and manipulating the\nkey contributors to a problem's ``hardness'', known as cores. Although some\nprevious work has addressed cores, the time costs are unacceptably high due to\nthe expense of traditional heuristic core detection techniques. We introduce a\nfast core detection procedure that uses a graph neural network. Our empirical\nresults demonstrate that we can efficiently generate problems that remain hard\nto solve and retain key attributes of the original example problems. We show\nvia experiment that the generated synthetic SAT problems can be used in a data\naugmentation setting to provide improved prediction of solver runtimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18778v1",
    "published_date": "2024-09-27 14:24:16 UTC",
    "updated_date": "2024-09-27 14:24:16 UTC"
  },
  {
    "arxiv_id": "2409.18769v5",
    "title": "State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features",
    "authors": [
      "George R. Nahass",
      "Sasha Hubschman",
      "Jeffrey C. Peterson",
      "Ghasem Yazdanpanah",
      "Nicholas Tomaras",
      "Madison Cheung",
      "Alex Palacios",
      "Kevin Heinze",
      "Chad A. Purnell",
      "Pete Setabutr",
      "Ann Q. Tran",
      "Darvin Yi"
    ],
    "abstract": "Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 12 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.18769v5",
    "published_date": "2024-09-27 14:14:16 UTC",
    "updated_date": "2025-05-14 16:01:10 UTC"
  },
  {
    "arxiv_id": "2409.18768v3",
    "title": "Learning from Demonstration with Implicit Nonlinear Dynamics Models",
    "authors": [
      "Peter David Fagan",
      "Subramanian Ramamoorthy"
    ],
    "abstract": "Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18768v3",
    "published_date": "2024-09-27 14:12:49 UTC",
    "updated_date": "2025-02-11 16:24:23 UTC"
  },
  {
    "arxiv_id": "2409.18743v1",
    "title": "OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph",
    "authors": [
      "Yujie Tang",
      "Meiling Wang",
      "Yinan Deng",
      "Zibo Zheng",
      "Jiagui Zhong",
      "Yufeng Yue"
    ],
    "abstract": "In everyday life, frequently used objects like cups often have unfixed\npositions and multiple instances within the same category, and their carriers\nfrequently change as well. As a result, it becomes challenging for a robot to\nefficiently navigate to a specific instance. To tackle this challenge, the\nrobot must capture and update scene changes and plans continuously. However,\ncurrent object navigation approaches primarily focus on semantic-level and lack\nthe ability to dynamically update scene representation. This paper captures the\nrelationships between frequently used objects and their static carriers. It\nconstructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and\nupdates the carrying status during robot navigation to reflect the dynamic\nchanges of the scene. Based on the CRSG, we further propose an instance\nnavigation strategy that models the navigation process as a Markov Decision\nProcess. At each step, decisions are informed by Large Language Model's\ncommonsense knowledge and visual-language feature similarity. We designed a\nseries of long-sequence navigation tasks for frequently used everyday items in\nthe Habitat simulator. The results demonstrate that by updating the CRSG, the\nrobot can efficiently navigate to moved targets. Additionally, we deployed our\nalgorithm on a real robot and validated its practical effectiveness.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://openobject-nav.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2409.18743v1",
    "published_date": "2024-09-27 13:33:52 UTC",
    "updated_date": "2024-09-27 13:33:52 UTC"
  },
  {
    "arxiv_id": "2409.18735v1",
    "title": "Autoregressive Policy Optimization for Constrained Allocation Tasks",
    "authors": [
      "David Winkel",
      "Niklas StrauÃ",
      "Maximilian Bernhard",
      "Zongyue Li",
      "Thomas Seidl",
      "Matthias Schubert"
    ],
    "abstract": "Allocation tasks represent a class of problems where a limited amount of\nresources must be allocated to a set of entities at each time step. Prominent\nexamples of this task include portfolio optimization or distributing\ncomputational workloads across servers. Allocation tasks are typically bound by\nlinear constraints describing practical requirements that have to be strictly\nfulfilled at all times. In portfolio optimization, for example, investors may\nbe obligated to allocate less than 30\\% of the funds into a certain industrial\nsector in any investment period. Such constraints restrict the action space of\nallowed allocations in intricate ways, which makes learning a policy that\navoids constraint violations difficult. In this paper, we propose a new method\nfor constrained allocation tasks based on an autoregressive process to\nsequentially sample allocations for each entity. In addition, we introduce a\nnovel de-biasing mechanism to counter the initial bias caused by sequential\nsampling. We demonstrate the superior performance of our approach compared to a\nvariety of Constrained Reinforcement Learning (CRL) methods on three distinct\nconstrained allocation tasks: portfolio optimization, computational workload\ndistribution, and a synthetic allocation benchmark. Our code is available at:\nhttps://github.com/niklasdbs/paspo",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18735v1",
    "published_date": "2024-09-27 13:27:15 UTC",
    "updated_date": "2024-09-27 13:27:15 UTC"
  },
  {
    "arxiv_id": "2409.18715v1",
    "title": "Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification",
    "authors": [
      "Salma Hassan",
      "Hamad Al Hammadi",
      "Ibrahim Mohammed",
      "Muhammad Haris Khan"
    ],
    "abstract": "The early detection and nuanced subtype classification of non-small cell lung\ncancer (NSCLC), a predominant cause of cancer mortality worldwide, is a\ncritical and complex issue. In this paper, we introduce an innovative\nintegration of multi-modal data, synthesizing fused medical imaging (CT and PET\nscans) with clinical health records and genomic data. This unique fusion\nmethodology leverages advanced machine learning models, notably MedClip and\nBEiT, for sophisticated image feature extraction, setting a new standard in\ncomputational oncology. Our research surpasses existing approaches, as\nevidenced by a substantial enhancement in NSCLC detection and classification\nprecision. The results showcase notable improvements across key performance\nmetrics, including accuracy, precision, recall, and F1-score. Specifically, our\nleading multi-modal classifier model records an impressive accuracy of 94.04%.\nWe believe that our approach has the potential to transform NSCLC diagnostics,\nfacilitating earlier detection and more effective treatment planning and,\nultimately, leading to superior patient outcomes in lung cancer care.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18715v1",
    "published_date": "2024-09-27 12:59:29 UTC",
    "updated_date": "2024-09-27 12:59:29 UTC"
  },
  {
    "arxiv_id": "2409.18708v4",
    "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity",
    "authors": [
      "Sergey Berezin",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ],
    "abstract": "We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18708v4",
    "published_date": "2024-09-27 12:54:13 UTC",
    "updated_date": "2024-10-09 12:29:38 UTC"
  },
  {
    "arxiv_id": "2409.18705v1",
    "title": "Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds",
    "authors": [
      "Hanbin Bae",
      "Pavel Andreev",
      "Azat Saginbaev",
      "Nicholas Babaev",
      "Won-Jun Lee",
      "Hosang Sung",
      "Hoon-Young Cho"
    ],
    "abstract": "This paper introduces a speech enhancement solution tailored for true\nwireless stereo (TWS) earbuds on-device usage. The solution was specifically\ndesigned to support conversations in noisy environments, with active noise\ncancellation (ANC) activated. The primary challenges for speech enhancement\nmodels in this context arise from computational complexity that limits\non-device usage and latency that must be less than 3 ms to preserve a live\nconversation. To address these issues, we evaluated several crucial design\nelements, including the network architecture and domain, design of loss\nfunctions, pruning method, and hardware-specific optimization. Consequently, we\ndemonstrated substantial improvements in speech enhancement quality compared\nwith that in baseline models, while simultaneously reducing the computational\ncomplexity and algorithmic latency.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18705v1",
    "published_date": "2024-09-27 12:47:36 UTC",
    "updated_date": "2024-09-27 12:47:36 UTC"
  },
  {
    "arxiv_id": "2409.18704v1",
    "title": "Semantic Model Component Implementation for Model-driven Semantic Communications",
    "authors": [
      "Haotai Liang",
      "Mengran Shi",
      "Chen Dong",
      "Xiaodong Xu",
      "Long Liu",
      "Hao Chen"
    ],
    "abstract": "The key feature of model-driven semantic communication is the propagation of\nthe model. The semantic model component (SMC) is designed to drive the\nintelligent model to transmit in the physical channel, allowing the\nintelligence to flow through the networks. According to the characteristics of\nneural networks with common and individual model parameters, this paper designs\nthe cross-source-domain and cross-task semantic component model. Considering\nthat the basic model is deployed on the edge node, the large server node\nupdates the edge node by transmitting only the semantic component model to the\nedge node so that the edge node can handle different sources and different\ntasks. In addition, this paper also discusses how channel noise affects the\nperformance of the model and proposes methods of injection noise and\nregularization to improve the noise resistance of the model. Experiments show\nthat SMCs use smaller model parameters to achieve cross-source, cross-task\nfunctionality while maintaining performance and improving the model's tolerance\nto noise. Finally, a component transfer-based unmanned vehicle tracking\nprototype was implemented to verify the feasibility of model components in\npractical applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18704v1",
    "published_date": "2024-09-27 12:45:57 UTC",
    "updated_date": "2024-09-27 12:45:57 UTC"
  },
  {
    "arxiv_id": "2409.18695v2",
    "title": "KALE-LM: Unleash The Power Of AI For Science Via Knowledge And Logic Enhanced Large Model",
    "authors": [
      "Weichen Dai",
      "Yezeng Chen",
      "Zijie Dai",
      "Yubo Liu",
      "Zhijie Huang",
      "Yixuan Pan",
      "Baiyang Song",
      "Chengli Zhong",
      "Xinhe Li",
      "Zeyu Wang",
      "Zhuoying Feng",
      "Yi Zhou"
    ],
    "abstract": "Artificial intelligence is gradually demonstrating its immense potential, and\nincreasing attention is being given to how AI can be harnessed to advance\nscientific research. In this vision paper, we present our perspectives on how\nAI can better assist scientific inquiry and explore corresponding technical\napproach. We have proposed and open-sourced two large models of our KALE-LM\nmodel series, KALE-LM-Chem(-1.5), which have achieved outstanding performance\nin tasks related to the field of chemistry. We hope that our work serves as a\nstrong starting point, helping to realize more intelligent AI and promoting the\nadvancement of human science and technology, as well as societal development.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18695v2",
    "published_date": "2024-09-27 12:33:57 UTC",
    "updated_date": "2025-04-07 10:25:31 UTC"
  },
  {
    "arxiv_id": "2409.18694v2",
    "title": "Learning from Pattern Completion: Self-supervised Controllable Generation",
    "authors": [
      "Zhiqiang Chen",
      "Guofan Fan",
      "Jinying Gao",
      "Lei Ma",
      "Bo Lei",
      "Tiejun Huang",
      "Shan Yu"
    ],
    "abstract": "The human brain exhibits a strong ability to spontaneously associate\ndifferent visual attributes of the same or similar visual scene, such as\nassociating sketches and graffiti with real-world visual objects, usually\nwithout supervising information. In contrast, in the field of artificial\nintelligence, controllable generation methods like ControlNet heavily rely on\nannotated training datasets such as depth maps, semantic segmentation maps, and\nposes, which limits the method's scalability. Inspired by the neural mechanisms\nthat may contribute to the brain's associative power, specifically the cortical\nmodularization and hippocampal pattern completion, here we propose a\nself-supervised controllable generation (SCG) framework. Firstly, we introduce\nan equivariant constraint to promote inter-module independence and intra-module\ncorrelation in a modular autoencoder network, thereby achieving functional\nspecialization. Subsequently, based on these specialized modules, we employ a\nself-supervised pattern completion approach for controllable generation\ntraining. Experimental results demonstrate that the proposed modular\nautoencoder effectively achieves functional specialization, including the\nmodular processing of color, brightness, and edge detection, and exhibits\nbrain-like features including orientation selectivity, color antagonism, and\ncenter-surround receptive fields. Through self-supervised training, associative\ngeneration capabilities spontaneously emerge in SCG, demonstrating excellent\ngeneralization ability to various tasks such as associative generation on\npainting, sketches, and ancient graffiti. Compared to the previous\nrepresentative method ControlNet, our proposed approach not only demonstrates\nsuperior robustness in more challenging high-noise scenarios but also possesses\nmore promising scalability potential due to its self-supervised manner.Codes\nare released on Github and Gitee.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18694v2",
    "published_date": "2024-09-27 12:28:47 UTC",
    "updated_date": "2024-11-07 08:27:16 UTC"
  },
  {
    "arxiv_id": "2409.18692v1",
    "title": "MG-Net: Learn to Customize QAOA with Circuit Depth Awareness",
    "authors": [
      "Yang Qian",
      "Xinbiao Wang",
      "Yuxuan Du",
      "Yong Luo",
      "Dacheng Tao"
    ],
    "abstract": "Quantum Approximate Optimization Algorithm (QAOA) and its variants exhibit\nimmense potential in tackling combinatorial optimization challenges. However,\ntheir practical realization confronts a dilemma: the requisite circuit depth\nfor satisfactory performance is problem-specific and often exceeds the maximum\ncapability of current quantum devices. To address this dilemma, here we first\nanalyze the convergence behavior of QAOA, uncovering the origins of this\ndilemma and elucidating the intricate relationship between the employed mixer\nHamiltonian, the specific problem at hand, and the permissible maximum circuit\ndepth. Harnessing this understanding, we introduce the Mixer Generator Network\n(MG-Net), a unified deep learning framework adept at dynamically formulating\noptimal mixer Hamiltonians tailored to distinct tasks and circuit depths.\nSystematic simulations, encompassing Ising models and weighted Max-Cut\ninstances with up to 64 qubits, substantiate our theoretical findings,\nhighlighting MG-Net's superior performance in terms of both approximation ratio\nand efficiency.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "29 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18692v1",
    "published_date": "2024-09-27 12:28:18 UTC",
    "updated_date": "2024-09-27 12:28:18 UTC"
  },
  {
    "arxiv_id": "2409.18680v3",
    "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models",
    "authors": [
      "Yiming Chen",
      "Xianghu Yue",
      "Xiaoxue Gao",
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Robby T. Tan",
      "Haizhou Li"
    ],
    "abstract": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "EMNLP24 Findings. Data available at\n  https://github.com/MatthewCYM/MALLM",
    "pdf_url": "http://arxiv.org/pdf/2409.18680v3",
    "published_date": "2024-09-27 12:06:53 UTC",
    "updated_date": "2024-11-06 10:27:05 UTC"
  },
  {
    "arxiv_id": "2409.18676v2",
    "title": "Toward Universal and Interpretable World Models for Open-ended Learning Agents",
    "authors": [
      "Lancelot Da Costa"
    ],
    "abstract": "We introduce a generic, compositional and interpretable class of generative\nworld models that supports open-ended learning agents. This is a sparse class\nof Bayesian networks capable of approximating a broad range of stochastic\nprocesses, which provide agents with the ability to learn world models in a\nmanner that may be both interpretable and computationally scalable. This\napproach integrating Bayesian structure learning and intrinsically motivated\n(model-based) planning enables agents to actively develop and refine their\nworld models, which may lead to developmental learning and more robust,\nadaptive behavior.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages including appendix, 6 including appendix and references; 2\n  figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18676v2",
    "published_date": "2024-09-27 12:03:15 UTC",
    "updated_date": "2024-10-15 16:23:51 UTC"
  },
  {
    "arxiv_id": "2409.18673v1",
    "title": "Exploiting Motion Prior for Accurate Pose Estimation of Dashboard Cameras",
    "authors": [
      "Yipeng Lu",
      "Yifan Zhao",
      "Haiping Wang",
      "Zhiwei Ruan",
      "Yuan Liu",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "abstract": "Dashboard cameras (dashcams) record millions of driving videos daily,\noffering a valuable potential data source for various applications, including\ndriving map production and updates. A necessary step for utilizing these\ndashcam data involves the estimation of camera poses. However, the low-quality\nimages captured by dashcams, characterized by motion blurs and dynamic objects,\npose challenges for existing image-matching methods in accurately estimating\ncamera poses. In this study, we propose a precise pose estimation method for\ndashcam images, leveraging the inherent camera motion prior. Typically, image\nsequences captured by dash cameras exhibit pronounced motion prior, such as\nforward movement or lateral turns, which serve as essential cues for\ncorrespondence estimation. Building upon this observation, we devise a pose\nregression module aimed at learning camera motion prior, subsequently\nintegrating these prior into both correspondences and pose estimation\nprocesses. The experiment shows that, in real dashcams dataset, our method is\n22% better than the baseline for pose estimation in AUC5\\textdegree, and it can\nestimate poses for 19% more images with less reprojection error in Structure\nfrom Motion (SfM).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18673v1",
    "published_date": "2024-09-27 11:59:00 UTC",
    "updated_date": "2024-09-27 11:59:00 UTC"
  },
  {
    "arxiv_id": "2409.18661v1",
    "title": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are Ineffective in Practice",
    "authors": [
      "Eddie Antonio Santos",
      "Brett A. Becker"
    ],
    "abstract": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\na disruptive impact throughout the computing education community. LLMs have\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\neven act as friendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in being able to\nexplain and resolve compiler error messages -- for decades, one of the most\nfrustrating parts of learning how to code. However, LLM-generated error message\nexplanations have only been assessed by expert programmers in artificial\nconditions. This work sought to understand how novice programmers resolve\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\nwithin-subjects study with $n$ = 106 participants in which students were tasked\nto fix six buggy C programs. For each program, participants were randomly\nassigned to fix the problem using either a stock compiler error message, an\nexpert-handwritten error message, or an error message explanation generated by\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\ngenerated error messages outperformed conventional compiler error messages in\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\nHandwritten explanations still outperform LLM and conventional error messages,\nboth on objective and subjective measures.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the proceedings of the 2024 UK and Ireland Computing\n  Education Research conference (UKICER '24)",
    "pdf_url": "http://arxiv.org/pdf/2409.18661v1",
    "published_date": "2024-09-27 11:45:56 UTC",
    "updated_date": "2024-09-27 11:45:56 UTC"
  },
  {
    "arxiv_id": "2409.18660v1",
    "title": "Effects of AI Feedback on Learning, the Skill Gap, and Intellectual Diversity",
    "authors": [
      "Christoph Riedl",
      "Eric Bogert"
    ],
    "abstract": "Can human decision-makers learn from AI feedback? Using data on 52,000\ndecision-makers from a large online chess platform, we investigate how their AI\nuse affects three interrelated long-term outcomes: Learning, skill gap, and\ndiversity of decision strategies. First, we show that individuals are far more\nlikely to seek AI feedback in situations in which they experienced success\nrather than failure. This AI feedback seeking strategy turns out to be\ndetrimental to learning: Feedback on successes decreases future performance,\nwhile feedback on failures increases it. Second, higher-skilled decision-makers\nseek AI feedback more often and are far more likely to seek AI feedback after a\nfailure, and benefit more from AI feedback than lower-skilled individuals. As a\nresult, access to AI feedback increases, rather than decreases, the skill gap\nbetween high- and low-skilled individuals. Finally, we leverage 42 major\nplatform updates as natural experiments to show that access to AI feedback\ncauses a decrease in intellectual diversity of the population as individuals\ntend to specialize in the same areas. Together, those results indicate that\nlearning from AI feedback is not automatic and using AI correctly seems to be a\nskill itself. Furthermore, despite its individual-level benefits, access to AI\nfeedback can have significant population-level downsides including loss of\nintellectual diversity and an increasing skill gap.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.HC",
      "q-fin.EC",
      "68T01",
      "I.2; J.4"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18660v1",
    "published_date": "2024-09-27 11:44:03 UTC",
    "updated_date": "2024-09-27 11:44:03 UTC"
  },
  {
    "arxiv_id": "2409.18653v2",
    "title": "When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation",
    "authors": [
      "Yuli Zhou",
      "Guolei Sun",
      "Yawei Li",
      "Guo-Sen Xie",
      "Luca Benini",
      "Ender Konukoglu"
    ],
    "abstract": "This study investigates the application and performance of the Segment\nAnything Model 2 (SAM2) in the challenging task of video camouflaged object\nsegmentation (VCOS). VCOS involves detecting objects that blend seamlessly in\nthe surroundings for videos, due to similar colors and textures, poor light\nconditions, etc. Compared to the objects in normal scenes, camouflaged objects\nare much more difficult to detect. SAM2, a video foundation model, has shown\npotential in various tasks. But its effectiveness in dynamic camouflaged\nscenarios remains under-explored. This study presents a comprehensive study on\nSAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged\nvideo datasets using different models and prompts (click, box, and mask).\nSecond, we explore the integration of SAM2 with existing multimodal large\nlanguage models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by\nfine-tuning it on the video camouflaged dataset. Our comprehensive experiments\ndemonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged\nobjects in videos. We also show that this ability could be further improved by\nspecifically adjusting SAM2's parameters for VCOS. The code is available at\nhttps://github.com/zhoustan/SAM2-VCOS",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report. Accepted by Visual Intelligence. Code is released\n  at https://github.com/zhoustan/SAM2-VCOS",
    "pdf_url": "http://arxiv.org/pdf/2409.18653v2",
    "published_date": "2024-09-27 11:35:50 UTC",
    "updated_date": "2025-05-10 02:48:36 UTC"
  },
  {
    "arxiv_id": "2409.18642v1",
    "title": "Enhanced Convolution Neural Network with Optimized Pooling and Hyperparameter Tuning for Network Intrusion Detection",
    "authors": [
      "Ayush Kumar Sharma",
      "Sourav Patel",
      "Supriya Bharat Wakchaure",
      "Abirami S"
    ],
    "abstract": "Network Intrusion Detection Systems (NIDS) are essential for protecting\ncomputer networks from malicious activities, including Denial of Service (DoS),\nProbing, User-to-Root (U2R), and Remote-to-Local (R2L) attacks. Without\neffective NIDS, networks are vulnerable to significant security breaches and\ndata loss. Machine learning techniques provide a promising approach to enhance\nNIDS by automating threat detection and improving accuracy. In this research,\nwe propose an Enhanced Convolutional Neural Network (EnCNN) for NIDS and\nevaluate its performance using the KDDCUP'99 dataset. Our methodology includes\ncomprehensive data preprocessing, exploratory data analysis (EDA), and feature\nengineering. We compare EnCNN with various machine learning algorithms,\nincluding Logistic Regression, Decision Trees, Support Vector Machines (SVM),\nand ensemble methods like Random Forest, AdaBoost, and Voting Ensemble. The\nresults show that EnCNN significantly improves detection accuracy, with a\nnotable 10% increase over state-of-art approaches. This demonstrates the\neffectiveness of EnCNN in real-time network intrusion detection, offering a\nrobust solution for identifying and mitigating security threats, and enhancing\noverall network resilience.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "7 Pages , 2 figures , 4 Tables , Conference paper",
    "pdf_url": "http://arxiv.org/pdf/2409.18642v1",
    "published_date": "2024-09-27 11:20:20 UTC",
    "updated_date": "2024-09-27 11:20:20 UTC"
  },
  {
    "arxiv_id": "2409.18633v1",
    "title": "Reducing Diversity to Generate Hierarchical Archetypes",
    "authors": [
      "Alfredo Ibias",
      "Hector Antona",
      "Guillem Ramirez-Miranda",
      "Enric Guinovart",
      "Eduard Alarcon"
    ],
    "abstract": "The Artificial Intelligence field seldom address the development of a\nfundamental building piece: a framework, methodology or algorithm to\nautomatically build hierarchies of abstractions. This is a key requirement in\norder to build intelligent behaviour, as recent neuroscience studies clearly\nexpose. In this paper we present a primitive-based framework to automatically\ngenerate hierarchies of constructive archetypes, as a theory of how to generate\nhierarchies of abstractions. We assume the existence of a primitive with very\nspecific characteristics, and we develop our framework over it. We prove the\neffectiveness of our framework through mathematical definitions and proofs.\nFinally, we give a few insights about potential uses of our framework and the\nexpected results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18633v1",
    "published_date": "2024-09-27 11:06:59 UTC",
    "updated_date": "2024-09-27 11:06:59 UTC"
  },
  {
    "arxiv_id": "2409.18631v1",
    "title": "Quantum Algorithms for Drone Mission Planning",
    "authors": [
      "Ethan Davies",
      "Pranav Kalidindi"
    ],
    "abstract": "Mission planning often involves optimising the use of ISR (Intelligence,\nSurveillance and Reconnaissance) assets in order to achieve a set of mission\nobjectives within allowed parameters subject to constraints. The missions of\ninterest here, involve routing multiple UAVs visiting multiple targets,\nutilising sensors to capture data relating to each target. Finding such\nsolutions is often an NP-Hard problem and cannot be solved efficiently on\nclassical computers. Furthermore, during the mission new constraints and\nobjectives may arise, requiring a new solution to be computed within a short\ntime period. To achieve this we investigate near term quantum algorithms that\nhave the potential to offer speed-ups against current classical methods. We\ndemonstrate how a large family of these problems can be formulated as a Mixed\nInteger Linear Program (MILP) and then converted to a Quadratic Unconstrained\nBinary Optimisation (QUBO). The formulation provided is versatile and can be\nadapted for many different constraints with clear qubit scaling provided. We\ndiscuss the results of solving the QUBO formulation using commercial quantum\nannealers and compare the solutions to current edge classical solvers. We also\nanalyse the results from solving the QUBO using Quantum Approximate\nOptimisation Algorithms (QAOA) and discuss their results. Finally, we also\nprovide efficient methods to encode to the problem into the Variational Quantum\nEigensolver (VQE) formalism, where we have tailored the ansatz to the problem\nmaking efficient use of the qubits available.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "math.OC",
      "68Q12"
    ],
    "primary_category": "quant-ph",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18631v1",
    "published_date": "2024-09-27 10:58:25 UTC",
    "updated_date": "2024-09-27 10:58:25 UTC"
  },
  {
    "arxiv_id": "2409.18630v1",
    "title": "Entropy, concentration, and learning: a statistical mechanics primer",
    "authors": [
      "Akshay Balsubramani"
    ],
    "abstract": "Artificial intelligence models trained through loss minimization have\ndemonstrated significant success, grounded in principles from fields like\ninformation theory and statistical physics. This work explores these\nestablished connections through the lens of statistical mechanics, starting\nfrom first-principles sample concentration behaviors that underpin AI and\nmachine learning. Our development of statistical mechanics for modeling\nhighlights the key role of exponential families, and quantities of statistics,\nphysics, and information theory.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18630v1",
    "published_date": "2024-09-27 10:58:18 UTC",
    "updated_date": "2024-09-27 10:58:18 UTC"
  },
  {
    "arxiv_id": "2409.18628v1",
    "title": "Towards Integrating Epistemic Uncertainty Estimation into the Radiotherapy Workflow",
    "authors": [
      "Marvin Tom Teichmann",
      "Manasi Datar",
      "Lisa Kratzke",
      "Fernando Vega",
      "Florin C. Ghesu"
    ],
    "abstract": "The precision of contouring target structures and organs-at-risk (OAR) in\nradiotherapy planning is crucial for ensuring treatment efficacy and patient\nsafety. Recent advancements in deep learning (DL) have significantly improved\nOAR contouring performance, yet the reliability of these models, especially in\nthe presence of out-of-distribution (OOD) scenarios, remains a concern in\nclinical settings. This application study explores the integration of epistemic\nuncertainty estimation within the OAR contouring workflow to enable OOD\ndetection in clinically relevant scenarios, using specifically compiled data.\nFurthermore, we introduce an advanced statistical method for OOD detection to\nenhance the methodological framework of uncertainty estimation. Our empirical\nevaluation demonstrates that epistemic uncertainty estimation is effective in\nidentifying instances where model predictions are unreliable and may require an\nexpert review. Notably, our approach achieves an AUC-ROC of 0.95 for OOD\ndetection, with a specificity of 0.95 and a sensitivity of 0.92 for implant\ncases, underscoring its efficacy. This study addresses significant gaps in the\ncurrent research landscape, such as the lack of ground truth for uncertainty\nestimation and limited empirical evaluations. Additionally, it provides a\nclinically relevant application of epistemic uncertainty estimation in an\nFDA-approved and widely used clinical solution for OAR segmentation from\nVarian, a Siemens Healthineers company, highlighting its practical benefits.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Keywords: Epistemic Uncertainty - Out-of-Distribution Detection - CT\n  Segmentation - OAR contouring - Radiotherapy",
    "pdf_url": "http://arxiv.org/pdf/2409.18628v1",
    "published_date": "2024-09-27 10:55:58 UTC",
    "updated_date": "2024-09-27 10:55:58 UTC"
  },
  {
    "arxiv_id": "2409.18626v1",
    "title": "Refutation of Spectral Graph Theory Conjectures with Search Algorithms)",
    "authors": [
      "Milo Roucairol",
      "Tristan Cazenave"
    ],
    "abstract": "We are interested in the automatic refutation of spectral graph theory\nconjectures. Most existing works address this problem either with the\nexhaustive generation of graphs with a limited size or with deep reinforcement\nlearning. Exhaustive generation is limited by the size of the generated graphs\nand deep reinforcement learning takes hours or days to refute a conjecture. We\npropose to use search algorithms to address these shortcomings to find\npotentially large counter-examples to spectral graph theory conjectures in\nseconds. We apply a wide range of search algorithms to a selection of\nconjectures from Graffiti. Out of 13 already refuted conjectures from Graffiti,\nour algorithms are able to refute 12 in seconds. We also refute conjecture 197\nfrom Graffiti which was open until now.",
    "categories": [
      "cs.AI",
      "05-04, 05-08, 05B30, 05C40, 05C50, 68Q25, 68Q87, 68R05, 68R10,\n  68T05, 68W20, 68W40"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18626v1",
    "published_date": "2024-09-27 10:55:22 UTC",
    "updated_date": "2024-09-27 10:55:22 UTC"
  },
  {
    "arxiv_id": "2409.18624v2",
    "title": "Unsupervised Cognition",
    "authors": [
      "Alfredo Ibias",
      "Hector Antona",
      "Guillem Ramirez-Miranda",
      "Enric Guinovart",
      "Eduard Alarcon"
    ],
    "abstract": "Unsupervised learning methods have a soft inspiration in cognition models. To\nthis day, the most successful unsupervised learning methods revolve around\nclustering samples in a mathematical space. In this paper we propose a\nstate-of-the-art, primitive-based, unsupervised learning approach for\ndecision-making inspired by a novel cognition framework. This\nrepresentation-centric approach models the input space constructively as a\ndistributed hierarchical structure in an input-agnostic way. We compared our\napproach with both current state-of-the-art unsupervised learning\nclassification, and with current state-of-the-art cancer type classification.\nWe show how our proposal outperforms previous state-of-the-art. We also\nevaluate some cognition-like properties of our proposal where it not only\noutperforms the compared algorithms (even supervised learning ones), but it\nalso shows a different, more cognition-like, behaviour.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18624v2",
    "published_date": "2024-09-27 10:50:49 UTC",
    "updated_date": "2024-11-07 14:36:04 UTC"
  },
  {
    "arxiv_id": "2409.18618v3",
    "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
    "authors": [
      "Jaepill Choi",
      "Kyubyung Chae",
      "Jiwoo Song",
      "Yohan Jo",
      "Taesup Kim"
    ],
    "abstract": "In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18618v3",
    "published_date": "2024-09-27 10:35:45 UTC",
    "updated_date": "2024-10-02 11:08:29 UTC"
  },
  {
    "arxiv_id": "2409.18597v1",
    "title": "TemporalPaD: a reinforcement-learning framework for temporal feature representation and dimension reduction",
    "authors": [
      "Xuechen Mu",
      "Zhenyu Huang",
      "Kewei Li",
      "Haotian Zhang",
      "Xiuli Wang",
      "Yusi Fan",
      "Kai Zhang",
      "Fengfeng Zhou"
    ],
    "abstract": "Recent advancements in feature representation and dimension reduction have\nhighlighted their crucial role in enhancing the efficacy of predictive\nmodeling. This work introduces TemporalPaD, a novel end-to-end deep learning\nframework designed for temporal pattern datasets. TemporalPaD integrates\nreinforcement learning (RL) with neural networks to achieve concurrent feature\nrepresentation and feature reduction. The framework consists of three\ncooperative modules: a Policy Module, a Representation Module, and a\nClassification Module, structured based on the Actor-Critic (AC) framework. The\nPolicy Module, responsible for dimensionality reduction through RL, functions\nas the actor, while the Representation Module for feature extraction and the\nClassification Module collectively serve as the critic. We comprehensively\nevaluate TemporalPaD using 29 UCI datasets, a well-known benchmark for\nvalidating feature reduction algorithms, through 10 independent tests and\n10-fold cross-validation. Additionally, given that TemporalPaD is specifically\ndesigned for time series data, we apply it to a real-world DNA classification\nproblem involving enhancer category and enhancer strength. The results\ndemonstrate that TemporalPaD is an efficient and effective framework for\nachieving feature reduction, applicable to both structured data and sequence\ndatasets. The source code of the proposed TemporalPaD is freely available as\nsupplementary material to this article and at\nhttp://www.healthinformaticslab.org/supp/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18597v1",
    "published_date": "2024-09-27 09:56:20 UTC",
    "updated_date": "2024-09-27 09:56:20 UTC"
  },
  {
    "arxiv_id": "2409.18596v1",
    "title": "ASAG2024: A Combined Benchmark for Short Answer Grading",
    "authors": [
      "GÃ©rÃ´me Meyer",
      "Philip Breuer",
      "Jonathan FÃ¼rst"
    ],
    "abstract": "Open-ended questions test a more thorough understanding than closed-ended\nquestions and are often a preferred assessment method. However, open-ended\nquestions are tedious to grade and subject to personal bias. Therefore, there\nhave been efforts to speed up the grading process through automation. Short\nAnswer Grading (SAG) systems aim to automatically score students' answers.\nDespite growth in SAG methods and capabilities, there exists no comprehensive\nshort-answer grading benchmark across different subjects, grading scales, and\ndistributions. Thus, it is hard to assess the capabilities of current automated\ngrading methods in terms of their generalizability. In this preliminary work,\nwe introduce the combined ASAG2024 benchmark to facilitate the comparison of\nautomated grading systems. Combining seven commonly used short-answer grading\ndatasets in a common structure and grading scale. For our benchmark, we\nevaluate a set of recent SAG methods, revealing that while LLM-based approaches\nreach new high scores, they still are far from reaching human performance. This\nopens up avenues for future research on human-machine SAG systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at SIGCSE-Virtual 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18596v1",
    "published_date": "2024-09-27 09:56:02 UTC",
    "updated_date": "2024-09-27 09:56:02 UTC"
  },
  {
    "arxiv_id": "2409.18594v1",
    "title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot Decision Tree Induction and Embedding with Large Language Models",
    "authors": [
      "Ricardo Knauer",
      "Mario Koddenbrock",
      "Raphael Wallsberger",
      "Nicholas M. Brisson",
      "Georg N. Duda",
      "Deborah Falla",
      "David W. Evans",
      "Erik Rodner"
    ],
    "abstract": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform on par with data-driven tree-based\nembeddings on average. Our knowledge-driven decision tree induction and\nembedding approaches therefore serve as strong new baselines for data-driven\nmachine learning methods in the low-data regime.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18594v1",
    "published_date": "2024-09-27 09:53:48 UTC",
    "updated_date": "2024-09-27 09:53:48 UTC"
  },
  {
    "arxiv_id": "2409.18586v1",
    "title": "Analysis of Truncated Singular Value Decomposition for Koopman Operator-Based Lane Change Model",
    "authors": [
      "Chinnawut Nantabut"
    ],
    "abstract": "Understanding and modeling complex dynamic systems is crucial for enhancing\nvehicle performance and safety, especially in the context of autonomous\ndriving. Recently, popular methods such as Koopman operators and their\napproximators, known as Extended Dynamic Mode Decomposition (EDMD), have\nemerged for their effectiveness in transforming strongly nonlinear system\nbehavior into linear representations. This allows them to be integrated with\nconventional linear controllers. To achieve this, Singular Value Decomposition\n(SVD), specifically truncated SVD, is employed to approximate Koopman operators\nfrom extensive datasets efficiently. This study evaluates different basis\nfunctions used in EDMD and ranks for truncated SVD for representing lane change\nbehavior models, aiming to balance computational efficiency with information\nloss. The findings, however, suggest that the technique of truncated SVD does\nnot necessarily achieve substantial reductions in computational training time\nand results in significant information loss.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Submitted to the 21st International Conference on Informatics in\n  Control, Automation and Robotics (ICINCO 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.18586v1",
    "published_date": "2024-09-27 09:45:21 UTC",
    "updated_date": "2024-09-27 09:45:21 UTC"
  },
  {
    "arxiv_id": "2409.18582v2",
    "title": "Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design",
    "authors": [
      "Melis Ilayda Bal",
      "Pier Giuseppe Sessa",
      "Mojmir Mutny",
      "Andreas Krause"
    ],
    "abstract": "Bayesian optimization (BO) is a powerful framework to optimize black-box\nexpensive-to-evaluate functions via sequential interactions. In several\nimportant problems (e.g. drug discovery, circuit design, neural architecture\nsearch, etc.), though, such functions are defined over large\n$\\textit{combinatorial and unstructured}$ spaces. This makes existing BO\nalgorithms not feasible due to the intractable maximization of the acquisition\nfunction over these domains. To address this issue, we propose\n$\\textbf{GameOpt}$, a novel game-theoretical approach to combinatorial BO.\n$\\textbf{GameOpt}$ establishes a cooperative game between the different\noptimization variables, and selects points that are game $\\textit{equilibria}$\nof an upper confidence bound acquisition function. These are stable\nconfigurations from which no variable has an incentive to deviate$-$ analog to\nlocal optima in continuous domains. Crucially, this allows us to efficiently\nbreak down the complexity of the combinatorial domain into individual decision\nsets, making $\\textbf{GameOpt}$ scalable to large combinatorial spaces. We\ndemonstrate the application of $\\textbf{GameOpt}$ to the challenging\n$\\textit{protein design}$ problem and validate its performance on four\nreal-world protein datasets. Each protein can take up to $20^{X}$ possible\nconfigurations, where $X$ is the length of a protein, making standard BO\nmethods infeasible. Instead, our approach iteratively selects informative\nprotein configurations and very quickly discovers highly active protein\nvariants compared to other baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the International Conference on Learning Representations\n  (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2409.18582v2",
    "published_date": "2024-09-27 09:37:49 UTC",
    "updated_date": "2025-02-24 13:35:14 UTC"
  },
  {
    "arxiv_id": "2409.19038v1",
    "title": "Intention-aware policy graphs: answering what, how, and why in opaque agents",
    "authors": [
      "Victor Gimenez-Abalos",
      "Sergio Alvarez-Napagao",
      "Adrian Tormos",
      "Ulises CortÃ©s",
      "Javier VÃ¡zquez-Salceda"
    ],
    "abstract": "Agents are a special kind of AI-based software in that they interact in\ncomplex environments and have increased potential for emergent behaviour.\nExplaining such emergent behaviour is key to deploying trustworthy AI, but the\nincreasing complexity and opaque nature of many agent implementations makes\nthis hard. In this work, we propose a Probabilistic Graphical Model along with\na pipeline for designing such model -- by which the behaviour of an agent can\nbe deliberated about -- and for computing a robust numerical value for the\nintentions the agent has at any moment. We contribute measurements that\nevaluate the interpretability and reliability of explanations provided, and\nenables explainability questions such as `what do you want to do now?' (e.g.\ndeliver soup) `how do you plan to do it?' (e.g. returning a plan that considers\nits skills and the world), and `why would you take this action at this state?'\n(e.g. explaining how that furthers or hinders its own goals). This model can be\nconstructed by taking partial observations of the agent's actions and world\nstates, and we provide an iterative workflow for increasing the proposed\nmeasurements through better design and/or pointing out irrational agent\nbehaviour.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.RO",
      "68T42 (Primary), 68T37, 68T05, 68Q87, 68T30, 68T40, 68M15",
      "I.2; I.1; K.4; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "57 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.19038v1",
    "published_date": "2024-09-27 09:31:45 UTC",
    "updated_date": "2024-09-27 09:31:45 UTC"
  },
  {
    "arxiv_id": "2409.18578v1",
    "title": "An Enhanced Federated Prototype Learning Method under Domain Shift",
    "authors": [
      "Liang Kuang",
      "Kuangpu Guo",
      "Jian Liang",
      "Jianguo Zhang"
    ],
    "abstract": "Federated Learning (FL) allows collaborative machine learning training\nwithout sharing private data. Numerous studies have shown that one significant\nfactor affecting the performance of federated learning models is the\nheterogeneity of data across different clients, especially when the data is\nsampled from various domains. A recent paper introduces variance-aware\ndual-level prototype clustering and uses a novel $\\alpha$-sparsity prototype\nloss, which increases intra-class similarity and reduces inter-class\nsimilarity. To ensure that the features converge within specific clusters, we\nintroduce an improved algorithm, Federated Prototype Learning with Convergent\nClusters, abbreviated as FedPLCC. To increase inter-class distances, we weight\neach prototype with the size of the cluster it represents. To reduce\nintra-class distances, considering that prototypes with larger distances might\ncome from different domains, we select only a certain proportion of prototypes\nfor the loss function calculation. Evaluations on the Digit-5, Office-10, and\nDomainNet datasets show that our method performs better than existing\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18578v1",
    "published_date": "2024-09-27 09:28:27 UTC",
    "updated_date": "2024-09-27 09:28:27 UTC"
  },
  {
    "arxiv_id": "2409.18568v1",
    "title": "Experimental Evaluation of Machine Learning Models for Goal-oriented Customer Service Chatbot with Pipeline Architecture",
    "authors": [
      "Nurul Ain Nabilah Mohd Isa",
      "Siti Nuraishah Agos Jawaddi",
      "Azlan Ismail"
    ],
    "abstract": "Integrating machine learning (ML) into customer service chatbots enhances\ntheir ability to understand and respond to user queries, ultimately improving\nservice performance. However, they may appear artificial to some users and\naffecting customer experience. Hence, meticulous evaluation of ML models for\neach pipeline component is crucial for optimizing performance, though\ndifferences in functionalities can lead to unfair comparisons. In this paper,\nwe present a tailored experimental evaluation approach for goal-oriented\ncustomer service chatbots with pipeline architecture, focusing on three key\ncomponents: Natural Language Understanding (NLU), dialogue management (DM), and\nNatural Language Generation (NLG). Our methodology emphasizes individual\nassessment to determine optimal ML models. Specifically, we focus on optimizing\nhyperparameters and evaluating candidate models for NLU (utilizing BERT and\nLSTM), DM (employing DQN and DDQN), and NLG (leveraging GPT-2 and DialoGPT).\nThe results show that for the NLU component, BERT excelled in intent detection\nwhereas LSTM was superior for slot filling. For the DM component, the DDQN\nmodel outperformed DQN by achieving fewer turns, higher rewards, as well as\ngreater success rates. For NLG, the large language model GPT-2 surpassed\nDialoGPT in BLEU, METEOR, and ROUGE metrics. These findings aim to provide a\nbenchmark for future research in developing and optimizing customer service\nchatbots, offering valuable insights into model performance and optimal\nhyperparameters.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18568v1",
    "published_date": "2024-09-27 09:11:52 UTC",
    "updated_date": "2024-09-27 09:11:52 UTC"
  },
  {
    "arxiv_id": "2409.18553v1",
    "title": "Efficient Noise Mitigation for Enhancing Inference Accuracy in DNNs on Mixed-Signal Accelerators",
    "authors": [
      "Seyedarmin Azizi",
      "Mohammad Erfan Sadeghi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "abstract": "In this paper, we propose a framework to enhance the robustness of the neural\nmodels by mitigating the effects of process-induced and aging-related\nvariations of analog computing components on the accuracy of the analog neural\nnetworks. We model these variations as the noise affecting the precision of the\nactivations and introduce a denoising block inserted between selected layers of\na pre-trained model. We demonstrate that training the denoising block\nsignificantly increases the model's robustness against various noise levels. To\nminimize the overhead associated with adding these blocks, we present an\nexploration algorithm to identify optimal insertion points for the denoising\nblocks. Additionally, we propose a specialized architecture to efficiently\nexecute the denoising blocks, which can be integrated into mixed-signal\naccelerators. We evaluate the effectiveness of our approach using Deep Neural\nNetwork (DNN) models trained on the ImageNet and CIFAR-10 datasets. The results\nshow that on average, by accepting 2.03% parameter count overhead, the accuracy\ndrop due to the variations reduces from 31.7% to 1.15%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18553v1",
    "published_date": "2024-09-27 08:45:55 UTC",
    "updated_date": "2024-09-27 08:45:55 UTC"
  },
  {
    "arxiv_id": "2409.18548v1",
    "title": "Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models",
    "authors": [
      "Yi Ren",
      "Tianyi Zhang",
      "Weibin Li",
      "DuoMu Zhou",
      "Chenhao Qin",
      "FangCheng Dong"
    ],
    "abstract": "In recent years, with the rapid development of large language models, serval\nmodels such as GPT-4o have demonstrated extraordinary capabilities, surpassing\nhuman performance in various language tasks. As a result, many researchers have\nbegun exploring their potential applications in the field of public opinion\nanalysis. This study proposes a novel large-language-models-based method for\npublic opinion event heat level prediction. First, we preprocessed and\nclassified 62,836 Chinese hot event data collected between July 2022 and\nDecember 2023. Then, based on each event's online dissemination heat index, we\nused the MiniBatchKMeans algorithm to automatically cluster the events and\ncategorize them into four heat levels (ranging from low heat to very high\nheat). Next, we randomly selected 250 events from each heat level, totalling\n1,000 events, to build the evaluation dataset. During the evaluation process,\nwe employed various large language models to assess their accuracy in\npredicting event heat levels in two scenarios: without reference cases and with\nsimilar case references. The results showed that GPT-4o and DeepseekV2\nperformed the best in the latter case, achieving prediction accuracies of 41.4%\nand 41.5%, respectively. Although the overall prediction accuracy remains\nrelatively low, it is worth noting that for low-heat (Level 1) events, the\nprediction accuracies of these two models reached 73.6% and 70.4%,\nrespectively. Additionally, the prediction accuracy showed a downward trend\nfrom Level 1 to Level 4, which correlates with the uneven distribution of data\nacross the heat levels in the actual dataset. This suggests that with the more\nrobust dataset, public opinion event heat level prediction based on large\nlanguage models will have significant research potential for the future.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "conference",
    "pdf_url": "http://arxiv.org/pdf/2409.18548v1",
    "published_date": "2024-09-27 08:34:42 UTC",
    "updated_date": "2024-09-27 08:34:42 UTC"
  },
  {
    "arxiv_id": "2409.18545v1",
    "title": "An Epistemic Human-Aware Task Planner which Anticipates Human Beliefs and Decisions",
    "authors": [
      "Shashank Shekhar",
      "Anthony Favier",
      "Rachid Alami"
    ],
    "abstract": "We present a substantial extension of our Human-Aware Task Planning\nframework, tailored for scenarios with intermittent shared execution\nexperiences and significant belief divergence between humans and robots,\nparticularly due to the uncontrollable nature of humans. Our objective is to\nbuild a robot policy that accounts for uncontrollable human behaviors, thus\nenabling the anticipation of possible advancements achieved by the robot when\nthe execution is not shared, e.g. when humans are briefly absent from the\nshared environment to complete a subtask. But, this anticipation is considered\nfrom the perspective of humans who have access to an estimated model for the\nrobot. To this end, we propose a novel planning framework and build a solver\nbased on AND-OR search, which integrates knowledge reasoning, including\nsituation assessment by perspective taking. Our approach dynamically models and\nmanages the expansion and contraction of potential advances while precisely\nkeeping track of when (and when not) agents share the task execution\nexperience. The planner systematically assesses the situation and ignores\nworlds that it has reason to think are impossible for humans. Overall, our new\nsolver can estimate the distinct beliefs of the human and the robot along\npotential courses of action, enabling the synthesis of plans where the robot\nselects the right moment for communication, i.e. informing, or replying to an\ninquiry, or defers ontic actions until the execution experiences can be shared.\nPreliminary experiments in two domains, one novel and one adapted, demonstrate\nthe effectiveness of the framework.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.18545v1",
    "published_date": "2024-09-27 08:27:36 UTC",
    "updated_date": "2024-09-27 08:27:36 UTC"
  },
  {
    "arxiv_id": "2409.18542v1",
    "title": "MIMII-Gen: Generative Modeling Approach for Simulated Evaluation of Anomalous Sound Detection System",
    "authors": [
      "Harsh Purohit",
      "Tomoya Nishida",
      "Kota Dohi",
      "Takashi Endo",
      "Yohei Kawaguchi"
    ],
    "abstract": "Insufficient recordings and the scarcity of anomalies present significant\nchallenges in developing and validating robust anomaly detection systems for\nmachine sounds. To address these limitations, we propose a novel approach for\ngenerating diverse anomalies in machine sound using a latent diffusion-based\nmodel that integrates an encoder-decoder framework. Our method utilizes the\nFlan-T5 model to encode captions derived from audio file metadata, enabling\nconditional generation through a carefully designed U-Net architecture. This\napproach aids our model in generating audio signals within the EnCodec latent\nspace, ensuring high contextual relevance and quality. We objectively evaluated\nthe quality of our generated sounds using the Fr\\'echet Audio Distance (FAD)\nscore and other metrics, demonstrating that our approach surpasses existing\nmodels in generating reliable machine audio that closely resembles actual\nabnormal conditions. The evaluation of the anomaly detection system using our\ngenerated data revealed a strong correlation, with the area under the curve\n(AUC) score differing by 4.8\\% from the original, validating the effectiveness\nof our generated data. These results demonstrate the potential of our approach\nto enhance the evaluation and robustness of anomaly detection systems across\nvaried and previously unseen conditions. Audio samples can be found at\n\\url{https://hpworkhub.github.io/MIMII-Gen.github.io/}.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18542v1",
    "published_date": "2024-09-27 08:21:31 UTC",
    "updated_date": "2024-09-27 08:21:31 UTC"
  },
  {
    "arxiv_id": "2409.18541v2",
    "title": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation",
    "authors": [
      "Hongzhe Huang",
      "Jiang Liu",
      "Zhewen Yu",
      "Li Cai",
      "Dian Jiao",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Juncheng Li",
      "Hao Jiang",
      "Haoyuan Li",
      "Yueting Zhuang"
    ],
    "abstract": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the training instructions from 158k to\n14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18541v2",
    "published_date": "2024-09-27 08:20:59 UTC",
    "updated_date": "2024-12-16 10:33:44 UTC"
  },
  {
    "arxiv_id": "2409.18512v1",
    "title": "EmoPro: A Prompt Selection Strategy for Emotional Expression in LM-based Speech Synthesis",
    "authors": [
      "Haoyu Wang",
      "Chunyu Qiang",
      "Tianrui Wang",
      "Cheng Gong",
      "Qiuyu Liu",
      "Yu Jiang",
      "Xiaobao Wang",
      "Chenyang Wang",
      "Chen Zhang"
    ],
    "abstract": "Recent advancements in speech synthesis models, trained on extensive\ndatasets, have demonstrated remarkable zero-shot capabilities. These models can\ncontrol content, timbre, and emotion in generated speech based on prompt\ninputs. Despite these advancements, the choice of prompts significantly impacts\nthe output quality, yet most existing selection schemes do not adequately\naddress the control of emotional intensity. To address this question, this\npaper proposes a two-stage prompt selection strategy EmoPro, which is\nspecifically designed for emotionally controllable speech synthesis. This\nstrategy focuses on selecting highly expressive and high-quality prompts by\nevaluating them from four perspectives: emotional expression strength, speech\nquality, text-emotion consistency, and model generation performance.\nExperimental results show that prompts selected using the proposed method\nresult in more emotionally expressive and engaging synthesized speech compared\nto those obtained through baseline. Audio samples and codes will be available\nat https://whyrrrrun.github.io/EmoPro/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18512v1",
    "published_date": "2024-09-27 07:46:52 UTC",
    "updated_date": "2024-09-27 07:46:52 UTC"
  },
  {
    "arxiv_id": "2409.18499v1",
    "title": "Fairness-aware Multiobjective Evolutionary Learning",
    "authors": [
      "Qingquan Zhang",
      "Jialin Liu",
      "Xin Yao"
    ],
    "abstract": "Multiobjective evolutionary learning (MOEL) has demonstrated its advantages\nof training fairer machine learning models considering a predefined set of\nconflicting objectives, including accuracy and different fairness measures.\nRecent works propose to construct a representative subset of fairness measures\nas optimisation objectives of MOEL throughout model training. However, the\ndetermination of a representative measure set relies on dataset, prior\nknowledge and requires substantial computational costs. What's more, those\nrepresentative measures may differ across different model training processes.\nInstead of using a static predefined set determined before model training, this\npaper proposes to dynamically and adaptively determine a representative measure\nset online during model training. The dynamically determined representative set\nis then used as optimising objectives of the MOEL framework and can vary with\ntime. Extensive experimental results on 12 well-known benchmark datasets\ndemonstrate that our proposed framework achieves outstanding performance\ncompared to state-of-the-art approaches for mitigating unfairness in terms of\naccuracy as well as 25 fairness measures although only a few of them were\ndynamically selected and used as optimisation objectives. The results indicate\nthe importance of setting optimisation objectives dynamically during training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.18499v1",
    "published_date": "2024-09-27 07:32:42 UTC",
    "updated_date": "2024-09-27 07:32:42 UTC"
  },
  {
    "arxiv_id": "2409.18475v1",
    "title": "Data Analysis in the Era of Generative AI",
    "authors": [
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Steven Drucker",
      "Gonzalo Ramos",
      "Victor Dibia",
      "Nathalie Riche",
      "Dave Brown",
      "Dan Marshall",
      "Jianfeng Gao"
    ],
    "abstract": "This paper explores the potential of AI-powered tools to reshape data\nanalysis, focusing on design considerations and challenges. We explore how the\nemergence of large language and multimodal models offers new opportunities to\nenhance various stages of data analysis workflow by translating high-level user\nintentions into executable code, charts, and insights. We then examine\nhuman-centered design principles that facilitate intuitive interactions, build\nuser trust, and streamline the AI-assisted analysis workflow across multiple\napps. Finally, we discuss the research challenges that impede the development\nof these AI-based systems such as enhancing model capabilities, evaluating and\nbenchmarking, and understanding end-user needs.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18475v1",
    "published_date": "2024-09-27 06:31:03 UTC",
    "updated_date": "2024-09-27 06:31:03 UTC"
  },
  {
    "arxiv_id": "2409.18461v2",
    "title": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration",
    "authors": [
      "Mahdi Morafah",
      "Vyacheslav Kungurtsev",
      "Hojin Chang",
      "Chen Chen",
      "Bill Lin"
    ],
    "abstract": "Federated Learning has emerged as a promising paradigm for collaborative\nmachine learning, while preserving user data privacy. Despite its potential,\nstandard FL lacks support for diverse heterogeneous device prototypes, which\nvary significantly in model and dataset sizes -- from small IoT devices to\nlarge workstations. This limitation is only partially addressed by existing\nknowledge distillation techniques, which often fail to transfer knowledge\neffectively across a broad spectrum of device prototypes with varied\ncapabilities. This failure primarily stems from two issues: the dilution of\ninformative logits from more capable devices by those from less capable ones,\nand the use of a single integrated logits as the distillation target across all\ndevices, which neglects their individual learning capacities and and the unique\ncontributions of each. To address these challenges, we introduce TAKFL, a novel\nKD-based framework that treats the knowledge transfer from each device\nprototype's ensemble as a separate task, independently distilling each to\npreserve its unique contributions and avoid dilution. TAKFL also incorporates a\nKD-based self-regularization technique to mitigate the issues related to the\nnoisy and unsupervised ensemble distillation process. To integrate the\nseparately distilled knowledge, we introduce an adaptive task arithmetic\nknowledge integration process, allowing each student model to customize the\nknowledge integration for optimal performance. Additionally, we present\ntheoretical results demonstrating the effectiveness of task arithmetic in\ntransferring knowledge across heterogeneous devices with varying capacities.\nComprehensive evaluations of our method across both CV and NLP tasks\ndemonstrate that TAKFL achieves SOTA results in a variety of datasets and\nsettings, significantly outperforming existing KD-based methods Code is\nreleased at https://github.com/MMorafah/TAKFL",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18461v2",
    "published_date": "2024-09-27 05:49:48 UTC",
    "updated_date": "2024-11-11 22:57:16 UTC"
  },
  {
    "arxiv_id": "2409.18455v1",
    "title": "Review of Digital Asset Development with Graph Neural Network Unlearning",
    "authors": [
      "Zara Lisbon"
    ],
    "abstract": "In the rapidly evolving landscape of digital assets, the imperative for\nrobust data privacy and compliance with regulatory frameworks has intensified.\nThis paper investigates the critical role of Graph Neural Networks (GNNs) in\nthe management of digital assets and introduces innovative unlearning\ntechniques specifically tailored to GNN architectures. We categorize unlearning\nstrategies into two primary classes: data-driven approximation, which\nmanipulates the graph structure to isolate and remove the influence of specific\nnodes, and model-driven approximation, which modifies the internal parameters\nand architecture of the GNN itself. By examining recent advancements in these\nunlearning methodologies, we highlight their applicability in various use\ncases, including fraud detection, risk assessment, token relationship\nprediction, and decentralized governance. We discuss the challenges inherent in\nbalancing model performance with the requirements for data unlearning,\nparticularly in the context of real-time financial applications. Furthermore,\nwe propose a hybrid approach that combines the strengths of both unlearning\nstrategies to enhance the efficiency and effectiveness of GNNs in digital asset\necosystems. Ultimately, this paper aims to provide a comprehensive framework\nfor understanding and implementing GNN unlearning techniques, paving the way\nfor secure and compliant deployment of machine learning in the digital asset\ndomain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18455v1",
    "published_date": "2024-09-27 05:31:04 UTC",
    "updated_date": "2024-09-27 05:31:04 UTC"
  },
  {
    "arxiv_id": "2409.18454v1",
    "title": "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications",
    "authors": [
      "Aditi Godbole",
      "Jabin Geevarghese George",
      "Smita Shandilya"
    ],
    "abstract": "The rapid increase in unstructured data across various fields has made\nmulti-document comprehension and summarization a critical task. Traditional\napproaches often fail to capture relevant context, maintain logical\nconsistency, and extract essential information from lengthy documents. This\npaper explores the use of Long-context Large Language Models (LLMs) for\nmulti-document summarization, demonstrating their exceptional capacity to grasp\nextensive connections, provide cohesive summaries, and adapt to various\nindustry domains and integration with enterprise applications/systems. The\npaper discusses the workflow of multi-document summarization for effectively\ndeploying long-context LLMs, supported by case studies in legal applications,\nenterprise functions such as HR, finance, and sourcing, as well as in the\nmedical and news domains. These case studies show notable enhancements in both\nefficiency and accuracy. Technical obstacles, such as dataset diversity, model\nscalability, and ethical considerations like bias mitigation and factual\naccuracy, are carefully analyzed. Prospective research avenues are suggested to\naugment the functionalities and applications of long-context LLMs, establishing\nthem as pivotal tools for transforming information processing across diverse\nsectors and enterprise applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18454v1",
    "published_date": "2024-09-27 05:29:31 UTC",
    "updated_date": "2024-09-27 05:29:31 UTC"
  },
  {
    "arxiv_id": "2409.18444v2",
    "title": "Cost-Aware Dynamic Cloud Workflow Scheduling using Self-Attention and Evolutionary Reinforcement Learning",
    "authors": [
      "Ya Shen",
      "Gang Chen",
      "Hui Ma",
      "Mengjie Zhang"
    ],
    "abstract": "The Cost-aware Dynamic Multi-Workflow Scheduling (CDMWS) in the cloud is a\nkind of cloud workflow management problem, which aims to assign virtual machine\n(VM) instances to execute tasks in workflows so as to minimize the total costs,\nincluding both the penalties for violating Service Level Agreement (SLA) and\nthe VM rental fees. Powered by deep neural networks, Reinforcement Learning\n(RL) methods can construct effective scheduling policies for solving CDMWS\nproblems. Traditional policy networks in RL often use basic feedforward\narchitectures to separately determine the suitability of assigning any VM\ninstances, without considering all VMs simultaneously to learn their global\ninformation. This paper proposes a novel self-attention policy network for\ncloud workflow scheduling (SPN-CWS) that captures global information from all\nVMs. We also develop an Evolution Strategy-based RL (ERL) system to train\nSPN-CWS reliably and effectively. The trained SPN-CWS can effectively process\nall candidate VM instances simultaneously to identify the most suitable VM\ninstance to execute every workflow task. Comprehensive experiments show that\nour method can noticeably outperform several state-of-the-art algorithms on\nmultiple benchmark CDMWS problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by ICSOC (International Conference on\n  Service-Oriented Computing) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18444v2",
    "published_date": "2024-09-27 04:45:06 UTC",
    "updated_date": "2024-12-29 11:29:57 UTC"
  },
  {
    "arxiv_id": "2409.18439v1",
    "title": "State-free Reinforcement Learning",
    "authors": [
      "Mingyu Chen",
      "Aldo Pacchiano",
      "Xuezhou Zhang"
    ],
    "abstract": "In this work, we study the \\textit{state-free RL} problem, where the\nalgorithm does not have the states information before interacting with the\nenvironment. Specifically, denote the reachable state set by ${S}^\\Pi := \\{\ns|\\max_{\\pi\\in \\Pi}q^{P, \\pi}(s)>0 \\}$, we design an algorithm which requires\nno information on the state space $S$ while having a regret that is completely\nindependent of ${S}$ and only depend on ${S}^\\Pi$. We view this as a concrete\nfirst step towards \\textit{parameter-free RL}, with the goal of designing RL\nalgorithms that require no hyper-parameter tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18439v1",
    "published_date": "2024-09-27 04:28:19 UTC",
    "updated_date": "2024-09-27 04:28:19 UTC"
  },
  {
    "arxiv_id": "2409.18438v1",
    "title": "Physics Augmented Tuple Transformer for Autism Severity Level Detection",
    "authors": [
      "Chinthaka Ranasingha",
      "Harshala Gammulle",
      "Tharindu Fernando",
      "Sridha Sridharan",
      "Clinton Fookes"
    ],
    "abstract": "Early diagnosis of Autism Spectrum Disorder (ASD) is an effective and\nfavorable step towards enhancing the health and well-being of children with\nASD. Manual ASD diagnosis testing is labor-intensive, complex, and prone to\nhuman error due to several factors contaminating the results. This paper\nproposes a novel framework that exploits the laws of physics for ASD severity\nrecognition. The proposed physics-informed neural network architecture encodes\nthe behaviour of the subject extracted by observing a part of the\nskeleton-based motion trajectory in a higher dimensional latent space. Two\ndecoders, namely physics-based and non-physics-based decoder, use this latent\nembedding and predict the future motion patterns. The physics branch leverages\nthe laws of physics that apply to a skeleton sequence in the prediction process\nwhile the non-physics-based branch is optimised to minimise the difference\nbetween the predicted and actual motion of the subject. A classifier also\nleverages the same latent space embeddings to recognise the ASD severity. This\ndual generative objective explicitly forces the network to compare the actual\nbehaviour of the subject with the general normal behaviour of children that are\ngoverned by the laws of physics, aiding the ASD recognition task. The proposed\nmethod attains state-of-the-art performance on multiple ASD diagnosis\nbenchmarks. To illustrate the utility of the proposed framework beyond the task\nASD diagnosis, we conduct a third experiment using a publicly available\nbenchmark for the task of fall prediction and demonstrate the superiority of\nour model.",
    "categories": [
      "cs.AI",
      "J.3; I.5.4; I.4.9"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.18438v1",
    "published_date": "2024-09-27 04:21:02 UTC",
    "updated_date": "2024-09-27 04:21:02 UTC"
  },
  {
    "arxiv_id": "2409.18435v1",
    "title": "Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems",
    "authors": [
      "Xian Yeow Lee",
      "Haiyan Wang",
      "Daisuke Katsumata",
      "Takaharu Matsui",
      "Chetan Gupta"
    ],
    "abstract": "This paper proposes a multi-agent reinforcement learning (MARL) approach to\nlearn dynamic dispatching strategies, which is crucial for optimizing\nthroughput in material handling systems across diverse industries. To benchmark\nour method, we developed a material handling environment that reflects the\ncomplexities of an actual system, such as various activities at different\nlocations, physical constraints, and inherent uncertainties. To enhance\nexploration during learning, we propose a method to integrate domain knowledge\nin the form of existing dynamic dispatching heuristics. Our experimental\nresults show that our method can outperform heuristics by up to 7.4 percent in\nterms of median throughput. Additionally, we analyze the effect of different\narchitectures on MARL performance when training multiple agents with different\nfunctions. We also demonstrate that the MARL agents performance can be further\nimproved by using the first iteration of MARL agents as heuristics to train a\nsecond iteration of MARL agents. This work demonstrates the potential of\napplying MARL to learn effective dynamic dispatching strategies that may be\ndeployed in real-world systems to improve business outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18435v1",
    "published_date": "2024-09-27 03:57:54 UTC",
    "updated_date": "2024-09-27 03:57:54 UTC"
  },
  {
    "arxiv_id": "2409.18433v1",
    "title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization",
    "authors": [
      "Mucong Ding",
      "Chenghao Deng",
      "Jocelyn Choo",
      "Zichu Wu",
      "Aakriti Agrawal",
      "Avi Schwarzschild",
      "Tianyi Zhou",
      "Tom Goldstein",
      "John Langford",
      "Anima Anandkumar",
      "Furong Huang"
    ],
    "abstract": "While generalization over tasks from easy to hard is crucial to profile\nlanguage models (LLMs), the datasets with fine-grained difficulty annotations\nfor each problem across a broad range of complexity are still blank. Aiming to\naddress this limitation, we present Easy2Hard-Bench, a consistently formatted\ncollection of 6 benchmark datasets spanning various domains, such as\nmathematics and programming problems, chess puzzles, and reasoning questions.\nEach problem within these datasets is annotated with numerical difficulty\nscores. To systematically estimate problem difficulties, we collect abundant\nperformance data on attempts to each problem by humans in the real world or\nLLMs on the prominent leaderboard. Leveraging the rich performance data, we\napply well-established difficulty ranking systems, such as Item Response Theory\n(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to\nproblems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from\nprevious collections by a higher proportion of challenging problems. Through\nextensive experiments with six state-of-the-art LLMs, we provide a\ncomprehensive analysis of their performance and generalization capabilities\nacross varying levels of difficulty, with the aim of inspiring future research\nin LLM generalization. The datasets are available at\nhttps://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2409.18433v1",
    "published_date": "2024-09-27 03:49:56 UTC",
    "updated_date": "2024-09-27 03:49:56 UTC"
  },
  {
    "arxiv_id": "2409.18427v3",
    "title": "Neural Collaborative Filtering to Detect Anomalies in Human Semantic Trajectories",
    "authors": [
      "Yueyang Liu",
      "Lance Kennedy",
      "Hossein Amiri",
      "Andreas ZÃ¼fle"
    ],
    "abstract": "Human trajectory anomaly detection has become increasingly important across a\nwide range of applications, including security surveillance and public health.\nHowever, existing trajectory anomaly detection methods are primarily focused on\nvehicle-level traffic, while human-level trajectory anomaly detection remains\nunder-explored. Since human trajectory data is often very sparse, machine\nlearning methods have become the preferred approach for identifying complex\npatterns. However, concerns regarding potential biases and the robustness of\nthese models have intensified the demand for more transparent and explainable\nalternatives. In response to these challenges, our research focuses on\ndeveloping a lightweight anomaly detection model specifically designed to\ndetect anomalies in human trajectories. We propose a Neural Collaborative\nFiltering approach to model and predict normal mobility. Our method is designed\nto model users' daily patterns of life without requiring prior knowledge,\nthereby enhancing performance in scenarios where data is sparse or incomplete,\nsuch as in cold start situations. Our algorithm consists of two main modules.\nThe first is the collaborative filtering module, which applies collaborative\nfiltering to model normal mobility of individual humans to places of interest.\nThe second is the neural module, responsible for interpreting the complex\nspatio-temporal relationships inherent in human trajectory data. To validate\nour approach, we conducted extensive experiments using simulated and real-world\ndatasets comparing to numerous state-of-the-art trajectory anomaly detection\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in the 1st ACM SIGSPATIAL International\n  Workshop on Geospatial Anomaly Detection (GeoAnomalies'24)",
    "pdf_url": "http://arxiv.org/pdf/2409.18427v3",
    "published_date": "2024-09-27 03:28:11 UTC",
    "updated_date": "2024-10-08 14:23:25 UTC"
  },
  {
    "arxiv_id": "2409.18418v2",
    "title": "A3: Active Adversarial Alignment for Source-Free Domain Adaptation",
    "authors": [
      "Chrisantus Eze",
      "Christopher Crick"
    ],
    "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Recent works have focused\non source-free UDA, where only target data is available. This is challenging as\nmodels rely on noisy pseudo-labels and struggle with distribution shifts. We\npropose Active Adversarial Alignment (A3), a novel framework combining\nself-supervised learning, adversarial training, and active learning for robust\nsource-free UDA. A3 actively samples informative and diverse data using an\nacquisition function for training. It adapts models via adversarial losses and\nconsistency regularization, aligning distributions without source data access.\nA3 advances source-free UDA through its synergistic integration of active and\nadversarial learning for effective domain alignment and noise reduction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICMLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18418v2",
    "published_date": "2024-09-27 03:17:01 UTC",
    "updated_date": "2024-10-07 18:13:07 UTC"
  },
  {
    "arxiv_id": "2409.18417v2",
    "title": "VickreyFeedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback",
    "authors": [
      "Guoxi Zhang",
      "Jiuding Duan"
    ],
    "abstract": "This paper addresses the cost-efficiency aspect of Reinforcement Learning\nfrom Human Feedback (RLHF). RLHF leverages datasets of human preferences over\noutputs of large language models (LLM)s to instill human expectations into\nLLMs. Although preference annotation comes with a monetized cost, the economic\nutility of a preference dataset has not been considered by far. What\nexacerbates this situation is that, given complex intransitive or cyclic\nrelationships in preference datasets, existing algorithms for fine-tuning LLMs\nare still far from capturing comprehensive preferences. This raises severe\ncost-efficiency concerns in production environments, where preference data\naccumulate over time. In this paper, we discuss the fine-tuning of LLMs as a\nmonetized economy and introduce an auction mechanism to improve the efficiency\nof preference data collection in dollar terms. We show that introducing an\nauction mechanism can play an essential role in enhancing the cost-efficiency\nof RLHF, while maintaining satisfactory model performance. Experimental results\ndemonstrate that our proposed auction-based protocol is cost-effective for\nfine-tuning LLMs concentrating on high-quality feedback.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18417v2",
    "published_date": "2024-09-27 03:15:07 UTC",
    "updated_date": "2024-12-12 06:18:36 UTC"
  },
  {
    "arxiv_id": "2409.18412v3",
    "title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
    "authors": [
      "Liangtai Sun",
      "Danyu Luo",
      "Da Ma",
      "Zihan Zhao",
      "Baocai Chen",
      "Zhennan Shen",
      "Su Zhu",
      "Lu Chen",
      "Xin Chen",
      "Kai Yu"
    ],
    "abstract": "Recently, there has been a significant upsurge of interest in leveraging\nlarge language models (LLMs) to assist scientific discovery. However, most LLMs\nonly focus on general science, while they lack domain-specific knowledge, such\nas chemical molecules and amino acid sequences. To bridge these gaps, we\nintroduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and\nis able to conduct college-level scientific reasoning and understand molecules\nand amino acid sequences. We collect a large-scale training corpus containing\nnumerous scientific papers and books from different disciplines as well as data\nfrom domain-specific databases. We further fine-tune the pre-trained model on\nlots of instruction data to improve performances on downstream benchmarks. From\nexperiment results, we show that SciDFM achieves strong performance on general\nscientific benchmarks such as SciEval and SciQ, and it reaches a SOTA\nperformance on domain-specific benchmarks among models of similar size. We\nfurther analyze the expert layers and show that the results of expert selection\nvary with data from different disciplines. To benefit the broader research\ncommunity, we open-source SciDFM at\nhttps://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS\n  2024 Workshop FM4Science",
    "pdf_url": "http://arxiv.org/pdf/2409.18412v3",
    "published_date": "2024-09-27 03:00:29 UTC",
    "updated_date": "2024-11-12 09:11:37 UTC"
  },
  {
    "arxiv_id": "2409.18411v1",
    "title": "BoT-Drive: Hierarchical Behavior and Trajectory Planning for Autonomous Driving using POMDPs",
    "authors": [
      "Xuanjin Jin",
      "Chendong Zeng",
      "Shengfa Zhu",
      "Chunxiao Liu",
      "Panpan Cai"
    ],
    "abstract": "Uncertainties in dynamic road environments pose significant challenges for\nbehavior and trajectory planning in autonomous driving. This paper introduces\nBoT-Drive, a planning algorithm that addresses uncertainties at both behavior\nand trajectory levels within a Partially Observable Markov Decision Process\n(POMDP) framework. BoT-Drive employs driver models to characterize unknown\nbehavioral intentions and utilizes their model parameters to infer hidden\ndriving styles. By also treating driver models as decision-making actions for\nthe autonomous vehicle, BoT-Drive effectively tackles the exponential\ncomplexity inherent in POMDPs. To enhance safety and robustness, the planner\nfurther applies importance sampling to refine the driving trajectory\nconditioned on the planned high-level behavior. Evaluation on real-world data\nshows that BoT-Drive consistently outperforms both existing planning methods\nand learning-based methods in regular and complex urban driving scenes,\ndemonstrating significant improvements in driving safety and reliability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18411v1",
    "published_date": "2024-09-27 02:58:46 UTC",
    "updated_date": "2024-09-27 02:58:46 UTC"
  },
  {
    "arxiv_id": "2409.18401v1",
    "title": "GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation",
    "authors": [
      "Jiawei Lu",
      "Yingpeng Zhang",
      "Zengjun Zhao",
      "He Wang",
      "Kun Zhou",
      "Tianjia Shao"
    ],
    "abstract": "Large-scale text-guided image diffusion models have shown astonishing results\nin text-to-image (T2I) generation. However, applying these models to synthesize\ntextures for 3D geometries remains challenging due to the domain gap between 2D\nimages and textures on a 3D surface. Early works that used a\nprojecting-and-inpainting approach managed to preserve generation diversity but\noften resulted in noticeable artifacts and style inconsistencies. While recent\nmethods have attempted to address these inconsistencies, they often introduce\nother issues, such as blurring, over-saturation, or over-smoothing. To overcome\nthese challenges, we propose a novel text-to-texture synthesis framework that\nleverages pretrained diffusion models. We first introduce a local attention\nreweighing mechanism in the self-attention layers to guide the model in\nconcentrating on spatial-correlated patches across different views, thereby\nenhancing local details while preserving cross-view consistency. Additionally,\nwe propose a novel latent space merge pipeline, which further ensures\nconsistency across different viewpoints without sacrificing too much diversity.\nOur method significantly outperforms existing state-of-the-art techniques\nregarding texture consistency and visual quality, while delivering results much\nfaster than distillation-based methods. Importantly, our framework does not\nrequire additional training or fine-tuning, making it highly adaptable to a\nwide range of models available on public platforms.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18401v1",
    "published_date": "2024-09-27 02:32:42 UTC",
    "updated_date": "2024-09-27 02:32:42 UTC"
  },
  {
    "arxiv_id": "2409.18399v1",
    "title": "Multimodal Trajectory Prediction for Autonomous Driving on Unstructured Roads using Deep Convolutional Network",
    "authors": [
      "Lei Li",
      "Zhifa Chen",
      "Jian Wang",
      "Bin Zhou",
      "Guizhen Yu",
      "Xiaoxuan Chen"
    ],
    "abstract": "Recently, the application of autonomous driving in open-pit mining has\ngarnered increasing attention for achieving safe and efficient mineral\ntransportation. Compared to urban structured roads, unstructured roads in\nmining sites have uneven boundaries and lack clearly defined lane markings.\nThis leads to a lack of sufficient constraint information for predicting the\ntrajectories of other human-driven vehicles, resulting in higher uncertainty in\ntrajectory prediction problems. A method is proposed to predict multiple\npossible trajectories and their probabilities of the target vehicle. The\nsurrounding environment and historical trajectories of the target vehicle are\nencoded as a rasterized image, which is used as input to our deep convolutional\nnetwork to predict the target vehicle's multiple possible trajectories. The\nmethod underwent offline testing on a dataset specifically designed for\nautonomous driving scenarios in open-pit mining and was compared and evaluated\nagainst physics-based method. The open-source code and data are available at\nhttps://github.com/LLsxyc/mine_motion_prediction.git",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18399v1",
    "published_date": "2024-09-27 02:29:02 UTC",
    "updated_date": "2024-09-27 02:29:02 UTC"
  },
  {
    "arxiv_id": "2409.18395v1",
    "title": "Code Vulnerability Repair with Large Language Model using Context-Aware Prompt Tuning",
    "authors": [
      "Arshiya Khan",
      "Guannan Liu",
      "Xing Gao"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant challenges in detecting\nand repairing vulnerable code, particularly when dealing with vulnerabilities\ninvolving multiple aspects, such as variables, code flows, and code structures.\nIn this study, we utilize GitHub Copilot as the LLM and focus on buffer\noverflow vulnerabilities. Our experiments reveal a notable gap in Copilot's\nabilities when dealing with buffer overflow vulnerabilities, with a 76%\nvulnerability detection rate but only a 15% vulnerability repair rate. To\naddress this issue, we propose context-aware prompt tuning techniques designed\nto enhance LLM performance in repairing buffer overflow. By injecting a\nsequence of domain knowledge about the vulnerability, including various\nsecurity and code contexts, we demonstrate that Copilot's successful repair\nrate increases to 63%, representing more than four times the improvement\ncompared to repairs without domain knowledge.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18395v1",
    "published_date": "2024-09-27 02:25:29 UTC",
    "updated_date": "2024-09-27 02:25:29 UTC"
  },
  {
    "arxiv_id": "2409.18390v4",
    "title": "Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly",
    "authors": [
      "Alexander Htet Kyaw",
      "Se Hwan Jeon",
      "Miana Smith",
      "Neil Gershenfeld"
    ],
    "abstract": "We present a system that transforms speech into physical objects by combining\n3D generative Artificial Intelligence with robotic assembly. The system\nleverages natural language input to make design and manufacturing more\naccessible, enabling individuals without expertise in 3D modeling or robotic\nprogramming to create physical objects. We propose utilizing discrete robotic\nassembly of lattice-based voxel components to address the challenges of using\ngenerative AI outputs in physical production, such as design variability,\nfabrication speed, structural integrity, and material waste. The system\ninterprets speech to generate 3D objects, discretizes them into voxel\ncomponents, computes an optimized assembly sequence, and generates a robotic\ntoolpath. The results are demonstrated through the assembly of various objects,\nranging from chairs to shelves, which are prompted via speech and realized\nwithin 5 minutes using a 6-axis robotic arm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to the IEEE for possible publication. An\n  updated version will replace this version",
    "pdf_url": "http://arxiv.org/pdf/2409.18390v4",
    "published_date": "2024-09-27 02:12:56 UTC",
    "updated_date": "2025-04-05 22:00:46 UTC"
  },
  {
    "arxiv_id": "2409.18385v1",
    "title": "Robo-CSK-Organizer: Commonsense Knowledge to Organize Detected Objects for Multipurpose Robots",
    "authors": [
      "Rafael Hidalgo",
      "Jesse Parron",
      "Aparna S. Varde",
      "Weitian Wang"
    ],
    "abstract": "This paper presents a system called Robo-CSK-Organizer that infuses\ncommonsense knowledge from a classical knowledge based to enhance the context\nrecognition capabilities of robots so as to facilitate the organization of\ndetected objects by classifying them in a task-relevant manner. It is\nparticularly useful in multipurpose robotics. Unlike systems relying solely on\ndeep learning tools such as ChatGPT, the Robo-CSK-Organizer system stands out\nin multiple avenues as follows. It resolves ambiguities well, and maintains\nconsistency in object placement. Moreover, it adapts to diverse task-based\nclassifications. Furthermore, it contributes to explainable AI, hence helping\nto improve trust and human-robot collaboration. Controlled experiments\nperformed in our work, simulating domestic robotics settings, make\nRobo-CSK-Organizer demonstrate superior performance while placing objects in\ncontextually relevant locations. This work highlights the capacity of an\nAI-based system to conduct commonsense-guided decision-making in robotics\ncloser to the thresholds of human cognition. Hence, Robo-CSK-Organizer makes\npositive impacts on AI and robotics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.6; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18385v1",
    "published_date": "2024-09-27 02:01:05 UTC",
    "updated_date": "2024-09-27 02:01:05 UTC"
  },
  {
    "arxiv_id": "2409.18374v1",
    "title": "Adaptive Learning of the Latent Space of Wasserstein Generative Adversarial Networks",
    "authors": [
      "Yixuan Qiu",
      "Qingyi Gao",
      "Xiao Wang"
    ],
    "abstract": "Generative models based on latent variables, such as generative adversarial\nnetworks (GANs) and variational auto-encoders (VAEs), have gained lots of\ninterests due to their impressive performance in many fields. However, many\ndata such as natural images usually do not populate the ambient Euclidean space\nbut instead reside in a lower-dimensional manifold. Thus an inappropriate\nchoice of the latent dimension fails to uncover the structure of the data,\npossibly resulting in mismatch of latent representations and poor generative\nqualities. Towards addressing these problems, we propose a novel framework\ncalled the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein\nauto-encoder and the Wasserstein GAN so that the intrinsic dimension of the\ndata manifold can be adaptively learned by a modified informative latent\ndistribution. We prove that there exist an encoder network and a generator\nnetwork in such a way that the intrinsic dimension of the learned encoding\ndistribution is equal to the dimension of the data manifold. We theoretically\nestablish that our estimated intrinsic dimension is a consistent estimate of\nthe true dimension of the data manifold. Meanwhile, we provide an upper bound\non the generalization error of LWGAN, implying that we force the synthetic data\ndistribution to be similar to the real data distribution from a population\nperspective. Comprehensive empirical experiments verify our framework and show\nthat LWGAN is able to identify the correct intrinsic dimension under several\nscenarios, and simultaneously generate high-quality synthetic data by sampling\nfrom the learned latent distribution.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18374v1",
    "published_date": "2024-09-27 01:25:22 UTC",
    "updated_date": "2024-09-27 01:25:22 UTC"
  },
  {
    "arxiv_id": "2409.18364v3",
    "title": "Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded Images",
    "authors": [
      "Donghwan Kim",
      "Tae-Kyun Kim"
    ],
    "abstract": "3D human shape reconstruction under severe occlusion due to human-object or\nhuman-human interaction is a challenging problem. Parametric models i.e.,\nSMPL(-X), which are based on the statistics across human shapes, can represent\nwhole human body shapes but are limited to minimally-clothed human shapes.\nImplicit-function-based methods extract features from the parametric models to\nemploy prior knowledge of human bodies and can capture geometric details such\nas clothing and hair. However, they often struggle to handle misaligned\nparametric models and inpaint occluded regions given a single RGB image. In\nthis work, we propose a novel pipeline, MHCDIFF, Multi-hypotheses Conditioned\nPoint Cloud Diffusion, composed of point cloud diffusion conditioned on\nprobabilistic distributions for pixel-aligned detailed 3D human reconstruction\nunder occlusion. Compared to previous implicit-function-based methods, the\npoint cloud diffusion model can capture the global consistent features to\ngenerate the occluded regions, and the denoising process corrects the\nmisaligned SMPL meshes. The core of MHCDIFF is extracting local features from\nmultiple hypothesized SMPL(-X) meshes and aggregating the set of features to\ncondition the diffusion model. In the experiments on CAPE and MultiHuman\ndatasets, the proposed method outperforms various SOTA methods based on SMPL,\nimplicit functions, point cloud diffusion, and their combined, under synthetic\nand real occlusions. Our code is publicly available at\nhttps://donghwankim0101.github.io/projects/mhcdiff/ .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 7 figures, accepted NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18364v3",
    "published_date": "2024-09-27 00:49:08 UTC",
    "updated_date": "2024-10-29 10:33:50 UTC"
  },
  {
    "arxiv_id": "2409.18351v1",
    "title": "Tracking Software Security Topics",
    "authors": [
      "Phong Minh Vu",
      "Tung Thanh Nguyen"
    ],
    "abstract": "Software security incidents occur everyday and thousands of software security\nreports are announced each month. Thus, it is difficult for software security\nresearchers, engineers, and other stakeholders to follow software security\ntopics of their interests in real-time. In this paper, we propose, SOSK, a\nnovel tool for this problem. SOSK allows a user to import a collection of\nsoftware security reports. It pre-processes and extracts the most important\nkeywords from the textual description of the reports. Based on the similarity\nof embedding vectors of keywords, SOSK can expand and/or refine a keyword set\nfrom a much smaller set of user-provided keywords. Thus, SOSK allows users to\ndefine any topic of their interests and retrieve security reports relevant to\nthat topic effectively. Our preliminary evaluation shows that SOSK can expand\nkeywords and retrieve reports relevant to user requests.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18351v1",
    "published_date": "2024-09-27 00:05:01 UTC",
    "updated_date": "2024-09-27 00:05:01 UTC"
  },
  {
    "arxiv_id": "2410.03521v2",
    "title": "Building a Chinese Medical Dialogue System: Integrating Large-scale Corpora and Novel Models",
    "authors": [
      "Xinyuan Wang",
      "Haozhou Li",
      "Dingfang Zheng",
      "Qinke Peng"
    ],
    "abstract": "The global COVID-19 pandemic underscored major deficiencies in traditional\nhealthcare systems, hastening the advancement of online medical services,\nespecially in medical triage and consultation. However, existing studies face\ntwo main challenges. First, the scarcity of large-scale, publicly available,\ndomain-specific medical datasets due to privacy concerns, with current datasets\nbeing small and limited to a few diseases, limiting the effectiveness of triage\nmethods based on Pre-trained Language Models (PLMs). Second, existing methods\nlack medical knowledge and struggle to accurately understand professional terms\nand expressions in patient-doctor consultations. To overcome these obstacles,\nwe construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby\naddressing the data shortage in this field. Moreover, we further propose a\nnovel triage system that combines BERT-based supervised learning with prompt\nlearning, as well as a GPT-based medical consultation model. To enhance domain\nknowledge acquisition, we pre-trained PLMs using our self-constructed\nbackground corpus. Experimental results on the LCMDC demonstrate the efficacy\nof our proposed systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03521v2",
    "published_date": "2024-09-27 00:01:32 UTC",
    "updated_date": "2025-02-25 02:17:05 UTC"
  }
]
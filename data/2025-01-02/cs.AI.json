{
  "date": "2025-01-02",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-02 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型的鲁棒性、可解释性和实际应用，包括 LLM 在科学发现、医疗诊断和文化理解方面的创新benchmark，以及机器学习在化学和生物信息学中的进展，其中 BoxingGym 和 FlashInfer 等论文因其基准测试和高效推理框架而令人印象深刻。\n\n### 重点论文亮点\n以下挑选并简要讨论了今天更具影响力和话题度的论文，先从核心AI和医疗应用入手，再快速掠过其他相关但次要的。每个条目列出论文标题（中文 + 英文），并突出主要贡献和发现。\n\n- **Constructing and explaining machine learning models for chemistry: example of the exploration and design of boron-based Lewis acids**  \n  这篇论文利用可解释AI技术（如Hammett线性自由能关系）开发了预测硼基Lewis酸度的模型，实现高精度预测（平均绝对误差<6 kJ/mol），并通过解释性分析揭示了分子取代基对酸度的影响，为化学分子设计提供了新工具。\n\n- **BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery**  \n  作者包括Noah D. Goodman等知名学者，该论文引入了一个基准测试环境，评估LLM在科学实验设计和模型发现中的能力，发现现有LLM（如GPT-4o）在信息增益和预测方面存在不足，并通过概率模型验证了改进潜力。\n\n- **Model Checking in Medical Imaging for Tumor Detection and Segmentation**  \n  这篇工作应用空间逻辑模型检查技术开发了肿瘤检测工具，能自动识别图像中的肿瘤区域，讨论了数据变异性和临床适用性挑战，提升了医疗图像分析的鲁棒性。\n\n- **ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST**  \n  论文提出了一种分层RNN模型，用于实时分类天文瞬变事件，结合上下文信息（如主机星系数据）实现高精度分类（19类任务准确率达0.99），为暗能量科学协作提供高效工具。\n\n- **Training Medical Large Vision-Language Models with Abnormal-Aware Feedback**  \n  该论文构建了医疗异常检测数据集，并通过异常感知训练方法（如定位奖励）提升Med-LVLM的异常识别能力，显著改善了医疗图像理解和泛化性能。\n\n其他论文中，如FlashInfer（高效的LLM注意力引擎）和CultureVLM（提升模型的文化理解能力）等也值得关注，但限于篇幅，仅快速提要：FlashInfer通过优化KV缓存和注意力机制加速LLM推理，显著降低延迟；CultureVLM利用多模态基准测试模型的文化适应性，针对100多个国家提升AI的跨文化表现。剩余论文多为技术细化或特定领域应用，如音乐生成或图神经网络优化，贡献较局部，未做深入展开。今日arXiv整体呈现AI应用多样化趋势，感兴趣的读者可查阅相关代码或数据集。",
  "papers": [
    {
      "arxiv_id": "2501.01576v3",
      "title": "Constructing and explaining machine learning models for chemistry: example of the exploration and design of boron-based Lewis acids",
      "title_zh": "翻译失败",
      "authors": [
        "Juliette Fenogli",
        "Laurence Grimaud",
        "Rodolphe Vuilleumier"
      ],
      "abstract": "The integration of machine learning (ML) into chemistry offers transformative\npotential in the design of molecules with targeted properties. However, the\nfocus has often been on creating highly efficient predictive models, sometimes\nat the expense of interpretability. In this study, we leverage explainable AI\ntechniques to explore the rational design of boron-based Lewis acids, which\nplay a pivotal role in organic reactions due to their electron-ccepting\nproperties. Using Fluoride Ion Affinity as a proxy for Lewis acidity, we\ndeveloped interpretable ML models based on chemically meaningful descriptors,\nincluding ab initio computed features and substituent-based parameters derived\nfrom the Hammett linear free-energy relationship. By constraining the chemical\nspace to well-defined molecular scaffolds, we achieved highly accurate\npredictions (mean absolute error < 6 kJ/mol), surpassing conventional black-box\ndeep learning models in low-data regimes. Interpretability analyses of the\nmodels shed light on the origin of Lewis acidity in these compounds and\nidentified actionable levers to modulate it through the nature and positioning\nof substituents on the molecular scaffold. This work bridges ML and chemist's\nway of thinking, demonstrating how explainable models can inspire molecular\ndesign and enhance scientific understanding of chemical reactivity.",
      "tldr_zh": "本研究利用可解释 AI 技术，探索和设计硼基 Lewis acids，通过以 Fluoride Ion Affinity 作为代理，基于 ab initio 计算特征和 Hammett linear free-energy relationship 的取代基参数开发可解释 ML 模型。模型在限制化学空间的条件下实现了高准确预测（平均绝对误差 < 6 kJ/mol），优于传统黑箱深度学习模型，尤其在低数据环境下。通过可解释性分析，揭示了这些化合物中 Lewis 酸度的起源，并识别出通过取代基的性质和位置来调节酸度的可操作策略。该工作桥接了 ML 和化学家的思维方式，展示了可解释模型如何激发分子设计并增强对化学反应的科学理解。",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "Main text is 14 pages, 7 figures, 1 scheme. Supporting information is\n  25 pages. For associated code and datasets, see\n  https://github.com/jfenogli/XAI_boron_LA",
      "pdf_url": "http://arxiv.org/pdf/2501.01576v3",
      "published_date": "2025-01-02 23:47:54 UTC",
      "updated_date": "2025-03-23 19:20:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:39:08.804139"
    },
    {
      "arxiv_id": "2501.01540v1",
      "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Kanishk Gandhi",
        "Michael Y. Li",
        "Lyle Goodyear",
        "Louise Li",
        "Aditi Bhaskar",
        "Mohammed Zaman",
        "Noah D. Goodman"
      ],
      "abstract": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results.",
      "tldr_zh": "该论文引入 BoxingGym，这是一个基准测试平台，用于评估大型语言模型 (LLMs) 在自动化实验设计和模型发现方面的进展，包括10个基于真实科学领域（如心理学和生态学）的生成概率模型环境。基准通过计算期望信息增益 (EIG) 来量化实验设计的有效性，以及评估代理的模型解释是否能支持可靠预测，从而检验模型发现能力。结果显示，当前 LLMs 如 GPT-4o 在这些任务上表现不佳，即使结合显式统计模型，也未能显著提升性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "KG and MYL contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2501.01540v1",
      "published_date": "2025-01-02 21:15:57 UTC",
      "updated_date": "2025-01-02 21:15:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:39:18.861772"
    },
    {
      "arxiv_id": "2501.01539v1",
      "title": "In Search of a Lost Metric: Human Empowerment as a Pillar of Socially Conscious Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Vasanth Reddy Baddam",
        "Behdad Chalaki",
        "Vaishnav Tadiparthi",
        "Hossein Nourkhiz Mahjoub",
        "Ehsan Moradi-Pari",
        "Hoda Eldardiry",
        "Almuatazbellah Boker"
      ],
      "abstract": "In social robot navigation, traditional metrics like proxemics and behavior\nnaturalness emphasize human comfort and adherence to social norms but often\nfail to capture an agent's autonomy and adaptability in dynamic environments.\nThis paper introduces human empowerment, an information-theoretic concept that\nmeasures a human's ability to influence their future states and observe those\nchanges, as a complementary metric for evaluating social compliance. This\nmetric reveals how robot navigation policies can indirectly impact human\nempowerment. We present a framework that integrates human empowerment into the\nevaluation of social performance in navigation tasks. Through numerical\nsimulations, we demonstrate that human empowerment as a metric not only aligns\nwith intuitive social behavior, but also shows statistically significant\ndifferences across various robot navigation policies. These results provide a\ndeeper understanding of how different policies affect social compliance,\nhighlighting the potential of human empowerment as a complementary metric for\nfuture research in social navigation.",
      "tldr_zh": "这篇论文指出，传统社交机器人导航指标如 proxemics 和 behavior naturalness 强调人类舒适度和社交规范，但忽略了代理的自主性和适应性，因此引入 human empowerment（一个信息论概念）作为补充指标，以衡量人类影响未来状态并观察变化的能力。研究提出一个框架，将 human empowerment 整合到导航任务的社交性能评估中，通过数值模拟证明该指标不仅与直观的社交行为一致，还在不同机器人导航策略之间显示统计显著差异。这些发现加深了对机器人策略影响社交合规性的理解，并建议 human empowerment 作为未来社交导航研究的潜在补充指标。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 8 figures, 2 tables, Accepted to 20th edition of the\n  IEEE/ACM International Conference on Human-Robot Interaction (HRI)",
      "pdf_url": "http://arxiv.org/pdf/2501.01539v1",
      "published_date": "2025-01-02 21:13:46 UTC",
      "updated_date": "2025-01-02 21:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:40:25.191722"
    },
    {
      "arxiv_id": "2501.01535v1",
      "title": "A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal Communicative Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Eugene Yu Ji"
      ],
      "abstract": "Drawing on contemporary pragmatist philosophy and linguistic theories on\ncognition, meaning, and communication, this paper presents a dynamic,\nmetasemantic-metapragmatic taxonomy for grounding and conceptualizing\nhuman-like multimodal communicative alignment. The framework is rooted in\ncontemporary developments of the three basic communicative capacities initially\nidentified by American logician and pragmatist philosopher Charles Sanders\nPeirce: iconic (sensory and perceptual qualities), indexical (contextual and\nsociocultural associations), and rule-like (symbolic and intuitive reasoning).\nExpanding on these developments, I introduce the concept of indexical\ncontextualization and propose the principle of \"contextualization\ndirectionality\" for characterizing the crucial metapragmatic capacity for\nmaintaining, navigating, or transitioning between semantic and pragmatic modes\nof multimodal communication. I contend that current cognitive-social\ncomputational and engineering methodologies disproportionately emphasize the\nsemantic/metasemantic domain, overlooking the pivotal role of metapragmatic\nindexicality in traversing the semantic-pragmatic spectrum of communication.\nThe framework's broader implications for intentionality, identity, affect, and\nethics in within-modal and cross-modal human-machine alignment are also\ndiscussed.",
      "tldr_zh": "本论文基于实用主义哲学和语言学理论，提出一个动态的metasemantic-metapragmatic框架，用于分类和概念化人类般的多模态沟通对齐。该框架扩展了Charles Sanders Peirce的三个基本沟通能力——iconic（感官感知品质）、indexical（上下文社会关联）和rule-like（符号直觉推理）——并引入indexical contextualization概念及\"contextualization directionality\"原则，以描述metapragmatic能力在语义和语用模式之间导航的关键作用。作者指出，当前认知-社会计算方法过度强调metasemantic领域，而忽略了metapragmatic indexicality在沟通谱系中的重要性，并讨论了该框架对intentionality、identity、affect和ethics在人机对齐中的更广泛影响。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages, 1 figure, 3 tables. Draft presented at 2023 ZJU Logic and\n  AI Summit EAI Workshop",
      "pdf_url": "http://arxiv.org/pdf/2501.01535v1",
      "published_date": "2025-01-02 21:00:19 UTC",
      "updated_date": "2025-01-02 21:00:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:39:47.913832"
    },
    {
      "arxiv_id": "2501.02024v2",
      "title": "Model Checking in Medical Imaging for Tumor Detection and Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Elhoucine Elfatimi",
        "Lahcen El fatimi"
      ],
      "abstract": "Recent advancements in model checking have demonstrated significant potential\nacross diverse applications, particularly in signal and image analysis. Medical\nimaging stands out as a critical domain where model checking can be effectively\napplied to design and evaluate robust frameworks. These frameworks facilitate\nautomatic and semi-automatic delineation of regions of interest within images,\naiding in accurate segmentation. This paper provides a comprehensive analysis\nof recent works leveraging spatial logic to develop operators and tools for\nidentifying regions of interest, including tumorous and non-tumorous areas.\nAdditionally, we examine the challenges inherent to spatial model-checking\ntechniques, such as variability in ground truth data and the need for\nstreamlined procedures suitable for routine clinical practice.",
      "tldr_zh": "这篇论文探讨了模型 checking 在医疗成像中的应用，专注于肿瘤检测和分割，以设计和评估鲁棒框架，支持自动或半自动划分图像中的感兴趣区域。论文对利用空间 logic 开发的运算符和工具进行了全面分析，这些工具可识别肿瘤和非肿瘤区域，并提升分割准确性。同时，它指出了关键挑战，包括地面真实数据的变异性和需要简化以适应日常临床实践的程序。总的来说，该研究为医疗图像分析提供了有价值的见解和潜在改进方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02024v2",
      "published_date": "2025-01-02 20:47:04 UTC",
      "updated_date": "2025-01-07 03:29:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:40:42.466913"
    },
    {
      "arxiv_id": "2501.01516v1",
      "title": "Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Burger"
      ],
      "abstract": "Explainable AI (XAI) has seen a surge in recent interest with the\nproliferation of powerful but intractable black-box models. Moreover, XAI has\ncome under fire for techniques that may not offer reliable explanations. As\nmany of the methods in XAI are themselves models, adversarial examples have\nbeen prominent in the literature surrounding the effectiveness of XAI, with the\nobjective of these examples being to alter the explanation while maintaining\nthe output of the original model. For explanations in natural language, it is\nnatural to use measures found in the domain of information retrieval for use\nwith ranked lists to guide the adversarial XAI process. We show that the\nstandard implementation of these measures are poorly suited for the comparison\nof explanations in adversarial XAI and amend them by using information that is\ndiscarded, the synonymity of perturbed words. This synonymity weighting\nproduces more accurate estimates of the actual weakness of XAI methods to\nadversarial examples.",
      "tldr_zh": "该研究针对自然语言可解释 AI (XAI) 的鲁棒性问题，指出现有方法在使用信息检索相似性度量评估对抗样本时，忽略了词语同义性导致的估算不准确。作者提出了一种同义词加权相似性度量（synonymity weighted similarity measures），通过整合被扰动词的同义性信息来改进这些度量，从而更精确地评估 XAI 方法对对抗样本的弱点。实验结果显示，这种改进方法提升了鲁棒性估计的准确性，为增强 XAI 的可靠性提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.01516v1",
      "published_date": "2025-01-02 19:49:04 UTC",
      "updated_date": "2025-01-02 19:49:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:40:06.326944"
    },
    {
      "arxiv_id": "2501.01515v1",
      "title": "DiagrammaticLearning: A Graphical Language for Compositional Training Regimes",
      "title_zh": "翻译失败",
      "authors": [
        "Mason Lary",
        "Richard Samuelson",
        "Alexander Wilentz",
        "Alina Zare",
        "Matthew Klawonn",
        "James P. Fairbanks"
      ],
      "abstract": "Motivated by deep learning regimes with multiple interacting yet distinct\nmodel components, we introduce learning diagrams, graphical depictions of\ntraining setups that capture parameterized learning as data rather than code. A\nlearning diagram compiles to a unique loss function on which component models\nare trained. The result of training on this loss is a collection of models\nwhose predictions ``agree\" with one another. We show that a number of popular\nlearning setups such as few-shot multi-task learning, knowledge distillation,\nand multi-modal learning can be depicted as learning diagrams. We further\nimplement learning diagrams in a library that allows users to build diagrams of\nPyTorch and Flux.jl models. By implementing some classic machine learning use\ncases, we demonstrate how learning diagrams allow practitioners to build\ncomplicated models as compositions of smaller components, identify\nrelationships between workflows, and manipulate models during or after\ntraining. Leveraging a category theoretic framework, we introduce a rigorous\nsemantics for learning diagrams that puts such operations on a firm\nmathematical foundation.",
      "tldr_zh": "本论文提出了一种图形语言DiagrammaticLearning，用于表示和构建组合式训练机制（compositional training regimes），以应对多组件深度学习系统的复杂性。学习图（learning diagrams）将训练设置编译为独特的损失函数（loss function），使组件模型在训练后实现预测一致性，并适用于如少样本多任务学习（few-shot multi-task learning）、知识蒸馏（knowledge distillation）和多模态学习（multi-modal learning）等流行场景。通过一个支持PyTorch和Flux.jl的库，研究者展示了如何将复杂模型构建为更小组件的组合，并在训练前后操作模型。基于范畴论（category theoretic framework）的严格语义，该框架为这些操作提供了坚实的数学基础，便于识别工作流关系和提升模型实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL",
        "math.CT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01515v1",
      "published_date": "2025-01-02 19:44:36 UTC",
      "updated_date": "2025-01-02 19:44:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:41:24.893521"
    },
    {
      "arxiv_id": "2501.01509v1",
      "title": "AI-Enabled Operations at Fermi Complex: Multivariate Time Series Prediction for Outage Prediction and Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Milan Jain",
        "Burcu O. Mutlu",
        "Caleb Stam",
        "Jan Strube",
        "Brian A. Schupbach",
        "Jason M. St. John",
        "William A. Pellico"
      ],
      "abstract": "The Main Control Room of the Fermilab accelerator complex continuously\ngathers extensive time-series data from thousands of sensors monitoring the\nbeam. However, unplanned events such as trips or voltage fluctuations often\nresult in beam outages, causing operational downtime. This downtime not only\nconsumes operator effort in diagnosing and addressing the issue but also leads\nto unnecessary energy consumption by idle machines awaiting beam restoration.\nThe current threshold-based alarm system is reactive and faces challenges\nincluding frequent false alarms and inconsistent outage-cause labeling. To\naddress these limitations, we propose an AI-enabled framework that leverages\npredictive analytics and automated labeling. Using data from $2,703$ Linac\ndevices and $80$ operator-labeled outages, we evaluate state-of-the-art deep\nlearning architectures, including recurrent, attention-based, and linear\nmodels, for beam outage prediction. Additionally, we assess a Random\nForest-based labeling system for providing consistent, confidence-scored outage\nannotations. Our findings highlight the strengths and weaknesses of these\narchitectures for beam outage prediction and identify critical gaps that must\nbe addressed to fully harness AI for transitioning downtime handling from\nreactive to predictive, ultimately reducing downtime and improving\ndecision-making in accelerator management.",
      "tldr_zh": "该研究针对 Fermilab 加速器复合体的光束中断问题，提出了一种 AI 启用的操作框架，利用 Multivariate Time Series Prediction 进行中断预测和诊断，以解决现有阈值警报系统的误报频繁和标记不一致问题。框架基于 2,703 个 Linac 设备的数据和 80 个操作员标记的中断事件，评估了 recurrent models、attention-based models、linear models 等深度学习架构，以及 Random Forest-based labeling 系统，以实现预测分析和自动标记。结果显示，这些模型在中断预测方面各有优缺点，并指出了关键改进空间，最终有助于从反应式转向预测式管理，减少停机时间并提升加速器决策效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented in the AAAI Workshop on AI for Time Series Analysis 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01509v1",
      "published_date": "2025-01-02 19:31:48 UTC",
      "updated_date": "2025-01-02 19:31:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:41:21.403835"
    },
    {
      "arxiv_id": "2501.01507v2",
      "title": "Transfer Learning Analysis of Variational Quantum Circuits",
      "title_zh": "变分量子电路的迁移学习分析",
      "authors": [
        "Huan-Hsin Tseng",
        "Hsin-Yi Lin",
        "Samuel Yen-Chi Chen",
        "Shinjae Yoo"
      ],
      "abstract": "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
      "tldr_zh": "本研究分析了变分量子电路 (VQC) 的迁移学习框架，从一个预训练的VQC开始，计算1-parameter unitary subgroups的转变，以适应新领域。该框架建立了形式化理论，评估VQC在损失边界下的适应性和能力，并提供了知识转移机制的启发式解释。最终，研究推导了一个分析性的微调方法，实现类似领域的最佳转变。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Published at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01507v2",
      "published_date": "2025-01-02 19:26:25 UTC",
      "updated_date": "2025-02-16 22:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:40:54.421151"
    },
    {
      "arxiv_id": "2501.08339v1",
      "title": "Operator Learning for Reconstructing Flow Fields from Sparse Measurements: an Energy Transformer Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Zhang",
        "Dmitry Krotov",
        "George Em Karniadakis"
      ],
      "abstract": "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
      "tldr_zh": "这篇论文提出了一种基于 Operator Learning 的新框架，使用 Energy Transformer (ET) 架构来从稀疏测量重建流场，将重建问题表述为从不完整数据到完整场的映射。研究在三个流体力学例子中进行了验证，包括不稳态 2D 涡街模拟数据、高速超音速喷气 Schlieren 成像，以及 3D 湍流喷流粒子跟踪数据。结果显示，ET 能够从高度不完整的数据（90% 缺失）准确重建复杂流场，即使面对噪声实验测量，也能在单 GPU 上实现快速训练和推理，为流体力学、机械、地球物理和天气预测等领域提供新的重建解决方案。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.08339v1",
      "published_date": "2025-01-02 19:24:19 UTC",
      "updated_date": "2025-01-02 19:24:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:41:30.162491"
    },
    {
      "arxiv_id": "2501.01496v1",
      "title": "ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST",
      "title_zh": "翻译失败",
      "authors": [
        "Ved G. Shah",
        "Alex Gagliano",
        "Konstantin Malanchev",
        "Gautham Narayan",
        "The LSST Dark Energy Science Collaboration"
      ],
      "abstract": "We present ORACLE, the first hierarchical deep-learning model for real-time,\ncontext-aware classification of transient and variable astrophysical phenomena.\nORACLE is a recurrent neural network with Gated Recurrent Units (GRUs), and has\nbeen trained using a custom hierarchical cross-entropy loss function to provide\nhigh-confidence classifications along an observationally-driven taxonomy with\nas little as a single photometric observation. Contextual information for each\nobject, including host galaxy photometric redshift, offset, ellipticity and\nbrightness, is concatenated to the light curve embedding and used to make a\nfinal prediction. Training on $\\sim$0.5M events from the Extended LSST\nAstronomical Time-Series Classification Challenge, we achieve a top-level\n(Transient vs Variable) macro-averaged precision of 0.96 using only 1 day of\nphotometric observations after the first detection in addition to contextual\ninformation, for each event; this increases to $>$0.99 once 64 days of the\nlight curve has been obtained, and 0.83 at 1024 days after first detection for\n19-way classification (including supernova sub-types, active galactic nuclei,\nvariable stars, microlensing events, and kilonovae). We also compare ORACLE\nwith other state-of-the-art classifiers and report comparable performance for\nthe 19-way classification task, in addition to delivering accurate top-level\nclassifications much earlier. The code and model weights used in this work are\npublicly available at our associated GitHub repository\n(https://github.com/uiucsn/ELAsTiCC-Classification).",
      "tldr_zh": "本研究介绍了 ORACLE，一种分层深度学习模型，用于 LSST 的实时、基于上下文的天文瞬变和变异现象分类。\nORACLE 采用门控循环单元 (GRUs) 的循环神经网络，并结合自定义分层交叉熵损失函数，以及上下文信息如宿主星系的光度红移、偏移和亮度，仅需单次光度观测即可提供高置信度预测。\n在约 0.5M 事件数据集上训练后，模型在顶级分类 (Transient vs Variable) 中实现宏平均精度 0.96（使用 1 天观测），并在 64 天后提升至 >0.99；19-类分类精度在 1024 天后达到 0.83。\n与现有分类器相比，ORACLE 在早期观测中表现出色，且模型代码已公开可用。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "29 pages, 19 figures, 9 tables. Submitted to ApJ",
      "pdf_url": "http://arxiv.org/pdf/2501.01496v1",
      "published_date": "2025-01-02 19:00:05 UTC",
      "updated_date": "2025-01-02 19:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:42:38.686508"
    },
    {
      "arxiv_id": "2501.01424v1",
      "title": "Object-level Visual Prompts for Compositional Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Gaurav Parmar",
        "Or Patashnik",
        "Kuan-Chieh Wang",
        "Daniil Ostashev",
        "Srinivasa Narasimhan",
        "Jun-Yan Zhu",
        "Daniel Cohen-Or",
        "Kfir Aberman"
      ],
      "abstract": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
      "tldr_zh": "我们提出了一种在文本到图像扩散模型中，使用对象级别视觉提示的方法，以生成语义连贯的图像组合，支持多样场景和风格。核心挑战是保持输入视觉提示中对象的身份，同时实现不同图像的多样性，为此我们引入了KV-mixed cross-attention机制，该机制从小型瓶颈编码器获取keys用于布局控制，并从大型瓶颈编码器获取values捕捉细粒度外观细节。推理阶段进一步应用object-level compositional guidance，以提升身份保留和布局准确性。实验结果显示，该方法能产生保留独特特征的多样场景组合，显著扩展了文本到图像生成的创造潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://snap-research.github.io/visual-composer/",
      "pdf_url": "http://arxiv.org/pdf/2501.01424v1",
      "published_date": "2025-01-02 18:59:44 UTC",
      "updated_date": "2025-01-02 18:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:41:55.501753"
    },
    {
      "arxiv_id": "2501.01422v1",
      "title": "Multi-Modal Video Feature Extraction for Popularity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Haixu Liu",
        "Wenning Wang",
        "Haoxiang Zheng",
        "Penghao Jiang",
        "Qirui Wang",
        "Ruiqing Yan",
        "Qiuzhuang Sun"
      ],
      "abstract": "This work aims to predict the popularity of short videos using the videos\nthemselves and their related features. Popularity is measured by four key\nengagement metrics: view count, like count, comment count, and share count.\nThis study employs video classification models with different architectures and\ntraining methods as backbone networks to extract video modality features.\nMeanwhile, the cleaned video captions are incorporated into a carefully\ndesigned prompt framework, along with the video, as input for video-to-text\ngeneration models, which generate detailed text-based video content\nunderstanding. These texts are then encoded into vectors using a pre-trained\nBERT model. Based on the six sets of vectors mentioned above, a neural network\nis trained for each of the four prediction metrics. Moreover, the study\nconducts data mining and feature engineering based on the video and tabular\ndata, constructing practical features such as the total frequency of hashtag\nappearances, the total frequency of mention appearances, video duration, frame\ncount, frame rate, and total time online. Multiple machine learning models are\ntrained, and the most stable model, XGBoost, is selected. Finally, the\npredictions from the neural network and XGBoost models are averaged to obtain\nthe final result.",
      "tldr_zh": "本文研究了使用多模态视频特征预测短视频流行度的方法，流行度基于四个关键指标：view count、like count、comment count 和 share count。方法包括采用不同架构的视频分类模型提取视频特征、利用视频到文本生成模型结合 BERT 编码文本描述，以及通过数据挖掘构建实用特征如 hashtag 出现频率、视频时长等。最终，通过训练神经网络和 XGBoost 模型，并对它们的预测结果取平均，实现了更稳定的流行度预测。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "INFORMS 2024 Data Challenge Competition",
      "pdf_url": "http://arxiv.org/pdf/2501.01422v1",
      "published_date": "2025-01-02 18:59:36 UTC",
      "updated_date": "2025-01-02 18:59:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:42:06.837742"
    },
    {
      "arxiv_id": "2501.01409v2",
      "title": "JOG3R: Towards 3D-Consistent Video Generators",
      "title_zh": "JOG3",
      "authors": [
        "Chun-Hao Paul Huang",
        "Niloy Mitra",
        "Hyeonho Jeong",
        "Jae Shin Yoon",
        "Duygu Ceylan"
      ],
      "abstract": "Emergent capabilities of image generators have led to many impactful zero- or\nfew-shot applications. Inspired by this success, we investigate whether video\ngenerators similarly exhibit 3D-awareness. Using structure-from-motion as a\n3D-aware task, we test if intermediate features of a video generator - OpenSora\nin our case - can support camera pose estimation. Surprisingly, at first, we\nonly find a weak correlation between the two tasks. Deeper investigation\nreveals that although the video generator produces plausible video frames, the\nframes themselves are not truly 3D-consistent. Instead, we propose to jointly\ntrain for the two tasks, using photometric generation and 3D aware errors.\nSpecifically, we find that SoTA video generation and camera pose estimation\n(i.e.,DUSt3R [79]) networks share common structures, and propose an\narchitecture that unifies the two. The proposed unified model, named\n\\nameMethod, produces camera pose estimates with competitive quality while\nproducing 3D-consistent videos. In summary, we propose the first unified video\ngenerator that is 3D-consistent, generates realistic video frames, and can\npotentially be repurposed for other 3D-aware tasks.",
      "tldr_zh": "本文探讨视频生成器是否具备 3D 感知能力，通过测试 OpenSora 的中间特征是否支持 structure-from-motion 任务，但发现初始相关性弱，因为生成的视频帧缺乏真正的 3D-consistent。研究者提出 JOG3R 框架，联合训练视频生成和相机姿态估计（例如 DUSt3R），利用光度生成和 3D-aware 错误优化统一架构。该方法生成高质量的 3D-consistent 视频，同时提供竞争性的相机姿态估计，并为其他 3D-aware 任务提供潜在应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01409v2",
      "published_date": "2025-01-02 18:55:04 UTC",
      "updated_date": "2025-03-26 20:53:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:42:20.084791"
    },
    {
      "arxiv_id": "2501.01394v1",
      "title": "A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models",
      "title_zh": "一个统一的超参数优化管道，用于基于 Transformer 的时间序列预测模型",
      "authors": [
        "Jingjing Xu",
        "Caesar Wu",
        "Yuan-Fang Li",
        "Grégoire Danoy",
        "Pascal Bouvry"
      ],
      "abstract": "Transformer-based models for time series forecasting (TSF) have attracted\nsignificant attention in recent years due to their effectiveness and\nversatility. However, these models often require extensive hyperparameter\noptimization (HPO) to achieve the best possible performance, and a unified\npipeline for HPO in transformer-based TSF remains lacking. In this paper, we\npresent one such pipeline and conduct extensive experiments on several\nstate-of-the-art (SOTA) transformer-based TSF models. These experiments are\nconducted on standard benchmark datasets to evaluate and compare the\nperformance of different models, generating practical insights and examples.\nOur pipeline is generalizable beyond transformer-based architectures and can be\napplied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in\nour experiments. The goal of this work is to provide valuable guidance to both\nindustry practitioners and academic researchers in efficiently identifying\noptimal hyperparameters suited to their specific domain applications. The code\nand complete experimental results are available on GitHub.",
      "tldr_zh": "这篇论文提出了一种统一的超参数优化(HPO)管道，针对Transformer-based时间序列预测(TSF)模型，以解决这些模型需要广泛调优才能达到最佳性能的问题。通过在标准基准数据集上进行广泛实验，该管道评估和比较了多个SOTA模型的性能，并证明其可推广到其他架构如Mamba和TimeMixer。实验结果提供了实际洞见和示例，帮助行业从业者和学术研究者高效识别适合特定应用的优化超参数，相关代码已在GitHub上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01394v1",
      "published_date": "2025-01-02 18:12:42 UTC",
      "updated_date": "2025-01-02 18:12:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:43:48.328757"
    },
    {
      "arxiv_id": "2501.02021v2",
      "title": "Weakly Supervised Learning on Large Graphs",
      "title_zh": "大规模图上的弱监督学习",
      "authors": [
        "Aditya Prakash"
      ],
      "abstract": "Graph classification plays a pivotal role in various domains, including\npathology, where images can be represented as graphs. In this domain, images\ncan be represented as graphs, where nodes might represent individual nuclei,\nand edges capture the spatial or functional relationships between them. Often,\nthe overall label of the graph, such as a cancer type or disease state, is\ndetermined by patterns within smaller, localized regions of the image. This\nwork introduces a weakly-supervised graph classification framework leveraging\ntwo subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based\napproach. Subgraphs are processed using a Graph Attention Network (GAT), which\nemploys attention mechanisms to identify the most informative subgraphs for\nclassification. Weak supervision is achieved by propagating graph-level labels\nto subgraphs, eliminating the need for detailed subgraph annotations.",
      "tldr_zh": "本论文探讨了弱监督学习在大型图分类中的应用，特别是在病理学领域，其中图像被表示为图，节点代表个体核，边捕捉空间或功能关系。研究引入了一个框架，利用Sliding-window approach和BFS-based approach两种技术提取子图，并通过Graph Attention Network (GAT)处理这些子图，借助注意力机制识别关键子图。弱监督机制通过将图级标签传播到子图，实现分类任务而无需详细子图标注，从而提高了效率和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02021v2",
      "published_date": "2025-01-02 18:12:13 UTC",
      "updated_date": "2025-01-27 19:01:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:42:48.786534"
    },
    {
      "arxiv_id": "2501.14780v1",
      "title": "Perspective Chapter: MOOCs in India: Evolution, Innovation, Impact, and Roadmap",
      "title_zh": "视角章节：印度 MOOCs 的演变、创新、影响和路线图",
      "authors": [
        "Partha Pratim Das"
      ],
      "abstract": "With the largest population of the world and one of the highest enrolments in\nhigher education, India needs efficient and effective means to educate its\nlearners. India started focusing on open and digital education in 1980's and\nits efforts were escalated in 2009 through the NMEICT program of the Government\nof India. A study by the Government and FICCI in 2014 noted that India cannot\nmeet its educational needs just by capacity building in brick and mortar\ninstitutions. It was decided that ongoing MOOCs projects under the umbrella of\nNMEICT will be further strengthened over its second (2017-21) and third\n(2021-26) phases. NMEICT now steers NPTEL or SWAYAM (India's MOOCs) and several\ndigital learning projects including Virtual Labs, e-Yantra, Spoken Tutorial,\nFOSSEE, and National Digital Library on India - the largest digital education\nlibrary in the world. Further, India embraced its new National Education Policy\nin 2020 to strongly foster online education. In this chapter, we take a deep\nlook into the evolution of MOOCs in India, its innovations, its current status\nand impact, and the roadmap for the next decade to address its challenges and\ngrow. AI-powered MOOCs is an emerging opportunity for India to lead MOOCs\nworldwide.",
      "tldr_zh": "印度作为世界上人口最多且高等教育入学率高的国家，通过 NMEICT 程序从 1980 年代开始推动 MOOCs 的发展，并在 2009 年后加强，包括项目如 NPTEL、SWAYAM、Virtual Labs 等数字学习举措。该章节深入回顾了 MOOCs 在印度的演变、创新、当前状态和影响，强调这些举措帮助填补传统教育机构的不足，并通过 2020 年的新国家教育政策大力促进在线教育。研究发现，MOOCs 显著提升了印度的教育可及性和效率，并提出未来路线图，视 AI-powered MOOCs 为印度在全球 MOOCs 领域领先的机会。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14780v1",
      "published_date": "2025-01-02 17:44:28 UTC",
      "updated_date": "2025-01-02 17:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:43:01.087908"
    },
    {
      "arxiv_id": "2501.01377v1",
      "title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback",
      "title_zh": "利用异常感知反馈训练医疗大型视觉语言模型",
      "authors": [
        "Yucheng Zhou",
        "Lingran Song",
        "Jianbing Shen"
      ],
      "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images and responding to human queries based on these\nimages. However, there remain challenges in visual localization in medical\nimages, which is crucial for abnormality detection and interpretation. To\naddress these issues, we propose a novel UMed-LVLM designed with Unveiling\nMedical abnormalities. Specifically, we collect a Medical Abnormalities\nUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM\ntraining. To collect MAU dataset, we propose a prompt method utilizing the\nGPT-4V to generate diagnoses based on identified abnormal areas in medical\nimages. Moreover, the two-stage training method includes Abnormal-Aware\nInstruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal\nLocalization Rewarding and Vision Relevance Rewarding. Experimental results\ndemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addition, this work shows that enhancing\nthe abnormality detection capabilities of Med-LVLMs significantly improves\ntheir understanding of medical images and generalization capability.",
      "tldr_zh": "本研究针对现有 Medical Large Vision-Language Models (Med-LVLMs) 在医疗图像视觉定位和异常检测方面的不足，提出了一种新型模型 UMed-LVLM，以提升异常识别能力。具体地，该方法通过利用 GPT-4V 生成的 Medical Abnormalities Unveiling (MAU) 数据集，并采用两阶段训练策略，包括 Abnormal-Aware Instruction Tuning 和 Abnormal-Aware Rewarding（如 Abnormal Localization Rewarding 和 Vision Relevance Rewarding），来强化模型对异常区域的定位和理解。实验结果显示，UMed-LVLM 在识别医疗异常方面优于现有模型，同时显著提高了医疗图像的整体理解和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01377v1",
      "published_date": "2025-01-02 17:37:20 UTC",
      "updated_date": "2025-01-02 17:37:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:43:13.452034"
    },
    {
      "arxiv_id": "2501.01372v1",
      "title": "ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Neda Tavakoli",
        "Amir Ali Rahsepar",
        "Brandon C. Benefield",
        "Daming Shen",
        "Santiago López-Tapia",
        "Florian Schiffers",
        "Jeffrey J. Goldberger",
        "Christine M. Albert",
        "Edwin Wu",
        "Aggelos K. Katsaggelos",
        "Daniel C. Lee",
        "Daniel Kim"
      ],
      "abstract": "Background: Late Gadolinium Enhancement (LGE) imaging is the gold standard\nfor assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE\nextent predicting major adverse cardiac events (MACE). Despite its importance,\nroutine LGE-based LV scar quantification is hindered by labor-intensive manual\nsegmentation and inter-observer variability. Methods: We propose ScarNet, a\nhybrid model combining a transformer-based encoder from the Medical Segment\nAnything Model (MedSAM) with a convolution-based U-Net decoder, enhanced by\ntailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathy\npatients with expert segmentations of myocardial and scar boundaries and tested\non 184 separate patients. Results: ScarNet achieved robust scar segmentation in\n184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944),\nsignificantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) and\nnnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lower\nbias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias:\n-13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlo\nsimulations with noise perturbations, ScarNet achieved significantly higher\nscar Dice (0.892 \\pm 0.053, CoV = 5.9%) than MedSAM (0.048 \\pm 0.112, CoV =\n233.3%) and nnU-Net (0.615 \\pm 0.537, CoV = 28.7%). Conclusion: ScarNet\noutperformed MedSAM and nnU-Net in accurately segmenting myocardial and scar\nboundaries in LGE images. The model exhibited robust performance across diverse\nimage qualities and scar patterns.",
      "tldr_zh": "本研究针对 Late Gadolinium Enhancement (LGE) 图像中心肌疤痕量化的问题，提出了一种新型基础模型 ScarNet，以自动化分割心肌和疤痕边界，减少手动操作的劳动强度和观察者间变异。ScarNet 采用混合架构，将基于 Transformer 的编码器（源自 MedSAM）与基于卷积的 U-Net 解码器结合，并融入定制的注意力块，在 552 名缺血性心肌病患者的训练数据上进行优化。测试结果显示，ScarNet 在 184 名患者中获得中位 Dice score 为 0.912，显著优于 MedSAM（0.046）和 nnU-Net（0.638），并在噪声模拟中表现出更高的鲁棒性，包括更低的偏差和变异系数。总之，该模型为精确的心肌疤痕量化提供可靠工具，提升了 LGE 图像分析的准确性和稳定性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "31 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.01372v1",
      "published_date": "2025-01-02 17:30:55 UTC",
      "updated_date": "2025-01-02 17:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:43:54.235098"
    },
    {
      "arxiv_id": "2501.01367v1",
      "title": "Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation",
      "title_zh": "基于探索性动作的对比学习：利用自然交互进行偏好",
      "authors": [
        "Nathaniel Dennler",
        "Stefanos Nikolaidis",
        "Maja Matarić"
      ],
      "abstract": "People have a variety of preferences for how robots behave. To understand and\nreason about these preferences, robots aim to learn a reward function that\ndescribes how aligned robot behaviors are with a user's preferences. Good\nrepresentations of a robot's behavior can significantly reduce the time and\neffort required for a user to teach the robot their preferences. Specifying\nthese representations -- what \"features\" of the robot's behavior matter to\nusers -- remains a difficult problem; Features learned from raw data lack\nsemantic meaning and features learned from user data require users to engage in\ntedious labeling processes. Our key insight is that users tasked with\ncustomizing a robot are intrinsically motivated to produce labels through\nexploratory search; they explore behaviors that they find interesting and\nignore behaviors that are irrelevant. To harness this novel data source of\nexploratory actions, we propose contrastive learning from exploratory actions\n(CLEA) to learn trajectory features that are aligned with features that users\ncare about. We learned CLEA features from exploratory actions users performed\nin an open-ended signal design activity (N=25) with a Kuri robot, and evaluated\nCLEA features through a second user study with a different set of users (N=42).\nCLEA features outperformed self-supervised features when eliciting user\npreferences over four metrics: completeness, simplicity, minimality, and\nexplainability.",
      "tldr_zh": "该论文提出了一种名为 Contrastive Learning from Exploratory Actions (CLEA) 的方法，利用用户在机器人定制过程中的自然探索性互动来学习偏好特征，从而简化机器人行为偏好的获取。CLEA 通过对比学习从用户探索的轨迹中提取语义丰富的特征，避免了传统方法依赖原始数据或繁琐标记的局限。实验结果显示，在 Kuri 机器人用户研究中（N=25 用于学习，N=42 用于评估），CLEA 特征在完整性、简单性、最小性（minimality）和可解释性（explainability）四个指标上均优于自监督特征。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to HRI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01367v1",
      "published_date": "2025-01-02 17:26:01 UTC",
      "updated_date": "2025-01-02 17:26:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:46:09.172566"
    },
    {
      "arxiv_id": "2501.01366v1",
      "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Austin T. Wang",
        "ZeMing Gong",
        "Angel X. Chang"
      ],
      "abstract": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.",
      "tldr_zh": "该研究针对 3D Visual Grounding (3DVG) 问题，提出 ViGiL3D 数据集，这是一个语言多样性的诊断数据集，用于评估模型在 3D 场景中定位自然语言描述的实体。论文引入一个框架来分析 3DVG 提示的语言模式，确保数据集覆盖更全面的英语提示类型，以支持具身 AI 和场景检索应用。实验结果显示，现有的开源词汇 3DVG 方法在处理分布外和更具挑战性的提示时表现不佳，强调了提升模型鲁棒性以适应真实世界场景的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages with 5 figures and 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.01366v1",
      "published_date": "2025-01-02 17:20:41 UTC",
      "updated_date": "2025-01-02 17:20:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:14.022605"
    },
    {
      "arxiv_id": "2501.01349v1",
      "title": "Rethinking Relation Extraction: Beyond Shortcuts to Generalization with a Debiased Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Liang He",
        "Yougang Chu",
        "Zhen Wu",
        "Jianbing Zhang",
        "Xinyu Dai",
        "Jiajun Chen"
      ],
      "abstract": "Benchmarks are crucial for evaluating machine learning algorithm performance,\nfacilitating comparison and identifying superior solutions. However, biases\nwithin datasets can lead models to learn shortcut patterns, resulting in\ninaccurate assessments and hindering real-world applicability. This paper\naddresses the issue of entity bias in relation extraction tasks, where models\ntend to rely on entity mentions rather than context. We propose a debiased\nrelation extraction benchmark DREB that breaks the pseudo-correlation between\nentity mentions and relation types through entity replacement. DREB utilizes\nBias Evaluator and PPL Evaluator to ensure low bias and high naturalness,\nproviding a reliable and accurate assessment of model generalization in entity\nbias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a\ndebiasing method combining data-level and model training-level techniques.\nMixDebias effectively improves model performance on DREB while maintaining\nperformance on the original dataset. Extensive experiments demonstrate the\neffectiveness and robustness of MixDebias compared to existing methods,\nhighlighting its potential for improving the generalization ability of relation\nextraction models. We will release DREB and MixDebias publicly.",
      "tldr_zh": "这篇论文重新审视关系抽取(Relation Extraction)任务中的实体偏差问题，指出现有基准数据集的捷径模式会阻碍模型泛化能力。作者提出去偏差基准 DREB，通过实体替换和利用 Bias Evaluator 与 PPL Evaluator，确保数据集低偏差和高自然性，以更准确评估模型在实体偏差场景下的表现。同时，引入 MixDebias 方法，该方法结合数据级和模型训练级技术，有效提升模型在 DREB 上的性能，同时保持原数据集的准确率。实验证明 MixDebias 比现有方法更具鲁棒性，并计划公开发布 DREB 和 MixDebias。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01349v1",
      "published_date": "2025-01-02 17:01:06 UTC",
      "updated_date": "2025-01-02 17:01:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:46:32.787971"
    },
    {
      "arxiv_id": "2501.01342v1",
      "title": "DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring",
      "title_zh": "DeepFilter：用于准确和",
      "authors": [
        "Hao Wang",
        "Zhichao Chen",
        "Licheng Pan",
        "Xiaoyu Jiang",
        "Yichen Song",
        "Qunshan He",
        "Xinggao Liu"
      ],
      "abstract": "Effective process monitoring is increasingly vital in industrial automation\nfor ensuring operational safety, necessitating both high accuracy and\nefficiency. Although Transformers have demonstrated success in various fields,\ntheir canonical form based on the self-attention mechanism is inadequate for\nprocess monitoring due to two primary limitations: (1) the step-wise\ncorrelations captured by self-attention mechanism are difficult to capture\ndiscriminative patterns in monitoring logs due to the lacking semantics of each\nstep, thus compromising accuracy; (2) the quadratic computational complexity of\nself-attention hampers efficiency. To address these issues, we propose\nDeepFilter, a Transformer-style framework for process monitoring. The core\ninnovation is an efficient filtering layer that excel capturing long-term and\nperiodic patterns with reduced complexity. Equipping with the global filtering\nlayer, DeepFilter enhances both accuracy and efficiency, meeting the stringent\ndemands of process monitoring. Experimental results on real-world process\nmonitoring datasets validate DeepFilter's superiority in terms of accuracy and\nefficiency compared to existing state-of-the-art models.",
      "tldr_zh": "该论文针对工业自动化中的过程监控需求，提出DeepFilter框架，以解决Transformer模型的自注意力机制在捕获日志区分性模式和计算复杂度方面的不足。DeepFilter的核心创新是引入高效的过滤层（efficient filtering layer），能够更好地捕获长期和周期性模式，从而提升准确性和效率。实验结果显示，在真实世界数据集上，DeepFilter在准确性和效率方面优于现有最先进模型，为过程监控提供了可靠的基准。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01342v1",
      "published_date": "2025-01-02 16:47:55 UTC",
      "updated_date": "2025-01-02 16:47:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:46:45.526716"
    },
    {
      "arxiv_id": "2501.02020v3",
      "title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection",
      "title_zh": "利用语义图增强不确定性建模以检测幻觉",
      "authors": [
        "Kedi Chen",
        "Qin Chen",
        "Jie Zhou",
        "Xinqi Tao",
        "Bowen Ding",
        "Jingwen Xie",
        "Mingchen Xie",
        "Peilong Li",
        "Feng Zheng",
        "Liang He"
      ],
      "abstract": "Large Language Models (LLMs) are prone to hallucination with non-factual or\nunfaithful statements, which undermines the applications in real-world\nscenarios. Recent researches focus on uncertainty-based hallucination\ndetection, which utilizes the output probability of LLMs for uncertainty\ncalculation and does not rely on external knowledge or frequent sampling from\nLLMs. Whereas, most approaches merely consider the uncertainty of each\nindependent token, while the intricate semantic relations among tokens and\nsentences are not well studied, which limits the detection of hallucination\nthat spans over multiple tokens and sentences in the passage. In this paper, we\npropose a method to enhance uncertainty modeling with semantic graph for\nhallucination detection. Specifically, we first construct a semantic graph that\nwell captures the relations among entity tokens and sentences. Then, we\nincorporate the relations between two entities for uncertainty propagation to\nenhance sentence-level hallucination detection. Given that hallucination occurs\ndue to the conflict between sentences, we further present a graph-based\nuncertainty calibration method that integrates the contradiction probability of\nthe sentence with its neighbors in the semantic graph for uncertainty\ncalculation. Extensive experiments on two datasets show the great advantages of\nour proposed approach. In particular, we obtain substantial improvements with\n19.78% in passage-level hallucination detection.",
      "tldr_zh": "本文提出了一种增强不确定性建模的方法，通过 semantic graph 改进大语言模型 (LLMs) 的幻觉检测，解决现有方法忽略 token 和句子间复杂语义关系的问题。具体而言，该方法首先构建 semantic graph 来捕捉实体 token 和句子间的关系，然后通过不确定性 propagation 和基于图的不确定性 calibration 整合句子矛盾概率，以实现更有效的句子级和段落级检测。在两个数据集上的广泛实验中，该方法显著提高了检测性能，尤其在段落级幻觉检测上取得了 19.78% 的改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02020v3",
      "published_date": "2025-01-02 16:45:05 UTC",
      "updated_date": "2025-04-05 15:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:42.923949"
    },
    {
      "arxiv_id": "2501.01335v1",
      "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Johan Wahréus",
        "Ahmed Mohamed Hussain",
        "Panos Papadimitratos"
      ],
      "abstract": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.",
      "tldr_zh": "本文提出 CySecBench，一个基于生成式 AI 的网络安全焦点提示数据集，包含 12662 个专为评估大型语言模型 (LLMs) 越狱技术而设计的提示，分为 10 个攻击类型类别，使用封闭式提示以提高评估的一致性和准确性。数据集的生成和过滤方法被详细说明，并可适应其他领域，以解决现有数据集在网络安全领域的局限性。作者开发了一种基于提示混淆的越狱方法，实验结果显示在 ChatGPT 上成功率 (SR) 为 65%、Gemini 为 88%、Claude 为 17%，并在 AdvBench 数据集上达到 78.5% 的 SR，高于现有基准方法。这突出了领域特定数据集在评估 LLM 安全措施方面的价值。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01335v1",
      "published_date": "2025-01-02 16:37:04 UTC",
      "updated_date": "2025-01-02 16:37:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:09.587557"
    },
    {
      "arxiv_id": "2501.01329v1",
      "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shuzheng Gao",
        "Chaozheng Wang",
        "Cuiyun Gao",
        "Xiaoqian Jiao",
        "Chun Yong Chong",
        "Shan Gao",
        "Michael Lyu"
      ],
      "abstract": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.",
      "tldr_zh": "本研究提出The Prompt Alchemist框架，用于自动优化针对Large Language Models (LLMs)的提示词，以提升测试用例生成质量。现有方法依赖人工编写的通用提示，导致性能不佳，且忽略了不同LLMs对提示的特定需求。该框架通过指导性的组合和变异策略增加提示多样性，并融入领域知识，避免重复错误，最终帮助LLMs生成更可靠的测试用例。实验表明，这种优化方法显著改善了测试用例生成的效果，为软件测试自动化提供了更高效的工具。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01329v1",
      "published_date": "2025-01-02 16:30:05 UTC",
      "updated_date": "2025-01-02 16:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:27.513535"
    },
    {
      "arxiv_id": "2501.01317v1",
      "title": "Understanding Difficult-to-learn Examples in Contrastive Learning: A Theoretical Framework for Spectral Contrastive Learning",
      "title_zh": "理解对比学习中难学示例：谱对比学习的理论框架",
      "authors": [
        "Yi-Ge Zhang",
        "Jingyi Cui",
        "Qiran Li",
        "Yisen Wang"
      ],
      "abstract": "Unsupervised contrastive learning has shown significant performance\nimprovements in recent years, often approaching or even rivaling supervised\nlearning in various tasks. However, its learning mechanism is fundamentally\ndifferent from that of supervised learning. Previous works have shown that\ndifficult-to-learn examples (well-recognized in supervised learning as examples\naround the decision boundary), which are essential in supervised learning,\ncontribute minimally in unsupervised settings. In this paper, perhaps\nsurprisingly, we find that the direct removal of difficult-to-learn examples,\nalthough reduces the sample size, can boost the downstream classification\nperformance of contrastive learning. To uncover the reasons behind this, we\ndevelop a theoretical framework modeling the similarity between different pairs\nof samples. Guided by this theoretical framework, we conduct a thorough\ntheoretical analysis revealing that the presence of difficult-to-learn examples\nnegatively affects the generalization of contrastive learning. Furthermore, we\ndemonstrate that the removal of these examples, and techniques such as margin\ntuning and temperature scaling can enhance its generalization bounds, thereby\nimproving performance. Empirically, we propose a simple and efficient mechanism\nfor selecting difficult-to-learn examples and validate the effectiveness of the\naforementioned methods, which substantiates the reliability of our proposed\ntheoretical framework.",
      "tldr_zh": "这篇论文探讨了无监督对比学习(unsupervised contrastive learning)中difficult-to-learn examples的影响，提出一个理论框架来建模样本对之间的相似性。研究发现，这些难学样本虽然在监督学习中至关重要，但在对比学习中会负面影响泛化性能，因此直接移除它们可以提升下游分类性能。作者通过理论分析验证了移除样本、margin tuning和temperature scaling等技术能改善泛化边界，并在实验中提出简单机制选择难学样本，证实了框架的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01317v1",
      "published_date": "2025-01-02 16:17:44 UTC",
      "updated_date": "2025-01-02 16:17:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:39.795205"
    },
    {
      "arxiv_id": "2501.01311v2",
      "title": "Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers",
      "title_zh": "Multi-Head Explainer：用于提升 CNNs 和 Transformers 中可解释性的通用框架",
      "authors": [
        "Bohang Sun",
        "Pietro Liò"
      ],
      "abstract": "In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and\nmodular framework that enhances both the explainability and accuracy of\nConvolutional Neural Networks (CNNs) and Transformer-based models. MHEX\nconsists of three core components: an Attention Gate that dynamically\nhighlights task-relevant features, Deep Supervision that guides early layers to\ncapture fine-grained details pertinent to the target class, and an Equivalent\nMatrix that unifies refined local and global representations to generate\ncomprehensive saliency maps. Our approach demonstrates superior compatibility,\nenabling effortless integration into existing residual networks like ResNet and\nTransformer architectures such as BERT with minimal modifications. Extensive\nexperiments on benchmark datasets in medical imaging and text classification\nshow that MHEX not only improves classification accuracy but also produces\nhighly interpretable and detailed saliency scores.",
      "tldr_zh": "本研究引入 Multi-Head Explainer (MHEX)，一个通用且模块化的框架，用于提升 Convolutional Neural Networks (CNNs) 和 Transformer 模型的可解释性和准确性。MHEX 由三个核心组件组成：Attention Gate 动态突出任务相关特征、Deep Supervision 指导早期层捕获细粒度细节，以及 Equivalent Matrix 统一局部和全局表示以生成全面的显著性地图。该框架易于集成到现有模型如 ResNet 和 BERT 中，仅需最小修改，且在医疗成像和文本分类基准数据集上的实验显示，MHEX 显著提高了分类准确性并提供了高度可解释的显著性分数。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01311v2",
      "published_date": "2025-01-02 15:47:56 UTC",
      "updated_date": "2025-01-13 12:42:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:47:56.215958"
    },
    {
      "arxiv_id": "2501.02019v1",
      "title": "Benchmarking Constraint-Based Bayesian Structure Learning Algorithms: Role of Network Topology",
      "title_zh": "基于约束的贝叶斯结构学习算法的基准测试：网络拓扑的作用",
      "authors": [
        "Radha Nagarajan",
        "Marco Scutari"
      ],
      "abstract": "Modeling the associations between real world entities from their multivariate\ncross-sectional profiles can provide cues into the concerted working of these\nentities as a system. Several techniques have been proposed for deciphering\nthese associations including constraint-based Bayesian structure learning (BSL)\nalgorithms that model them as directed acyclic graphs. Benchmarking these\nalgorithms have typically focused on assessing the variation in performance\nmeasures such as sensitivity as a function of the dimensionality represented by\nthe number of nodes in the DAG, and sample size. The present study elucidates\nthe importance of network topology in benchmarking exercises. More\nspecifically, it investigates variations in sensitivity across distinct network\ntopologies while constraining the nodes, edges, and sample-size to be\nidentical, eliminating these as potential confounders. Sensitivity of three\npopular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental\nAssociation Markov Blanket) in learning the network structure from multivariate\ncross-sectional profiles sampled from network models with sub-linear, linear,\nand super-linear DAG topologies generated using preferential attachment is\ninvestigated. Results across linear and nonlinear models revealed statistically\nsignificant $(\\alpha=0.05)$ decrease in sensitivity estimates from sub-linear\nto super-linear topology constitutively across the three algorithms. These\nresults are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise\nstrengths $(\\sigma =3,6)$ and sample size $(N = 2^{10})$. The findings\nelucidate the importance of accommodating the network topology in\nconstraint-based BSL benchmarking exercises.",
      "tldr_zh": "本研究评估了约束-based Bayesian structure learning (BSL) 算法的性能，强调网络拓扑在基准测试中的关键作用。通过比较 Peter-Clarke、Grow-Shrink 和 Incremental Association Markov Blanket 三种算法在子线性、线性及超线性 DAG 拓扑下的敏感性，实验控制了节点数（48或64）、噪声强度（σ=3或6）和样本大小（N=2^10）。结果显示，从子线性到超线性拓扑，算法的敏感性显著降低（α=0.05），这在线性与非线性模型中均成立。该发现突出了在 BSL 算法基准测试中需充分考虑网络拓扑，以更准确地评估实体关联建模的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.MN",
        "62H22",
        "I.2"
      ],
      "primary_category": "cs.LG",
      "comment": "8 Pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02019v1",
      "published_date": "2025-01-02 15:47:20 UTC",
      "updated_date": "2025-01-02 15:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:48:11.124904"
    },
    {
      "arxiv_id": "2501.01303v1",
      "title": "Citations and Trust in LLM Generated Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Ding",
        "Matthew Facciani",
        "Amrit Poudel",
        "Ellen Joyce",
        "Salvador Aguinaga",
        "Balaji Veeramani",
        "Sanmitra Bhattacharya",
        "Tim Weninger"
      ],
      "abstract": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.",
      "tldr_zh": "这篇论文探讨了在LLM（Large Language Models）生成响应中，citations（引用）的存在如何影响用户信任。研究采用反监控框架（anti-monitoring framework）进行实验，呈现由商业聊天机器人生成的文本响应，并添加零、一个或五个相关/随机citations，记录参与者是否检查引用及其自报信任水平。结果显示，citations的存在显著提高了信任，即使引用是随机的；同时，参与者检查citations时信任会显著降低。这些发现强调了在AI生成内容中添加citations以增强信任的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01303v1",
      "published_date": "2025-01-02 15:32:50 UTC",
      "updated_date": "2025-01-02 15:32:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:48:20.277905"
    },
    {
      "arxiv_id": "2501.01293v1",
      "title": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Lin",
        "Yuxin Zhang",
        "Zhe Chen",
        "Zihan Fang",
        "Cong Wu",
        "Xianhao Chen",
        "Yue Gao",
        "Jun Luo"
      ],
      "abstract": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks.",
      "tldr_zh": "该研究针对LEO卫星网络中深度学习(DL)训练的挑战（如连接间歇性和资源限制），提出了一种半监督(Semi-Supervised) Split Learning框架LEO-Split。框架利用辅助模型处理卫星与地面站(GS)非接触时的训练失败，并引入伪标签算法(pseudo-labeling)来纠正卫星间的数据不平衡，同时采用自适应激活插值方案(adaptive activation interpolation)防止GS端子模型过拟合。实验结果显示，LEO-Split在真实LEO卫星数据（如Starlink）上显著优于现有基准，实现了更高效的DL模型训练。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.01293v1",
      "published_date": "2025-01-02 15:19:16 UTC",
      "updated_date": "2025-01-02 15:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:48:34.498084"
    },
    {
      "arxiv_id": "2501.01291v2",
      "title": "Detection Augmented Bandit Procedures for Piecewise Stationary MABs: A Modular Approach",
      "title_zh": "针对分段平稳多臂赌博机的检测增强带子程序：一种模块化方法",
      "authors": [
        "Yu-Han Huang",
        "Argyrios Gerogiannis",
        "Subhonmesh Bose",
        "Venugopal V. Veeravalli"
      ],
      "abstract": "Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary\nenvironments, where the reward distributions associated with the arms do not\nchange with time. In many applications, however, the environment is more\naccurately modeled as being nonstationary. In this work, piecewise stationary\nMAB (PS-MAB) environments are investigated, in which the reward distributions\nassociated with a subset of the arms change at some change-points and remain\nstationary between change-points. Our focus is on the asymptotic analysis of\nPS-MABs, for which practical algorithms based on change detection (CD) have\nbeen previously proposed. Our goal is to modularize the design and analysis of\nsuch CD-based Bandit (CDB) procedures. To this end, we identify the\nrequirements for stationary bandit algorithms and change detectors in a CDB\nprocedure that are needed for the modularization. We assume that the rewards\nare sub-Gaussian. Under this assumption and a condition on the separation of\nthe change-points, we show that the analysis of CDB procedures can indeed be\nmodularized, so that regret bounds can be obtained in a unified manner for\nvarious combinations of change detectors and bandit algorithms. Through this\nanalysis, we develop new modular CDB procedures that are order-optimal. We\ncompare the performance of our modular CDB procedures with various other\nmethods in simulations.",
      "tldr_zh": "这篇论文研究了分段静态多臂老虎机 (PS-MAB) 环境，其中部分臂的奖励分布在变化点 (change-points) 发生改变。作者提出了一种模块化的基于变化检测 (CD) 的 Bandit (CDB) 程序，通过假设奖励为 sub-Gaussian 并设置变化点分离条件，实现对各种 CD 和 Bandit 算法组合的统一分析，从而获得阶次最优 (order-optimal) 的遗憾界 (regret bounds)。实验模拟显示，该方法在性能上优于其他算法，为非静态环境下的 MAB 问题提供了高效解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 2 figures, 1 table, submitted to JMLR",
      "pdf_url": "http://arxiv.org/pdf/2501.01291v2",
      "published_date": "2025-01-02 15:18:18 UTC",
      "updated_date": "2025-02-26 19:35:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:48:46.112182"
    },
    {
      "arxiv_id": "2501.02018v1",
      "title": "Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs",
      "title_zh": "实时保护大型语言模型：通过可调安全",
      "authors": [
        "Joao Fonseca",
        "Andrew Bell",
        "Julia Stoyanovich"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to be susceptible to jailbreak\nattacks, or adversarial attacks used to illicit high risk behavior from a\nmodel. Jailbreaks have been exploited by cybercriminals and blackhat actors to\ncause significant harm, highlighting the critical need to safeguard\nwidely-deployed models. Safeguarding approaches, which include fine-tuning\nmodels or having LLMs \"self-reflect\", may lengthen the inference time of a\nmodel, incur a computational penalty, reduce the semantic fluency of an output,\nand restrict ``normal'' model behavior. Importantly, these Safety-Performance\nTrade-offs (SPTs) remain an understudied area. In this work, we introduce a\nnovel safeguard, called SafeNudge, that combines Controlled Text Generation\nwith \"nudging\", or using text interventions to change the behavior of a model.\nSafeNudge triggers during text-generation while a jailbreak attack is being\nexecuted, and can reduce successful jailbreak attempts by 30% by guiding the\nLLM towards a safe responses. It adds minimal latency to inference and has a\nnegligible impact on the semantic fluency of outputs. Further, we allow for\ntunable SPTs. SafeNudge is open-source and available through https://pypi.org/,\nand is compatible with models loaded with the Hugging Face \"transformers\"\nlibrary.",
      "tldr_zh": "该研究探讨了 Large Language Models (LLMs) 易受 jailbreak attacks 的影响，这些攻击可能导致模型产生高风险行为，并强调了保护模型的必要性。作者提出了一种新方法 SafeNudge，它结合 Controlled Text Generation 和 nudging（文本干预）技术，在实时生成文本时检测并引导模型产生安全响应，从而将成功 jailbreak 尝试减少 30%。SafeNudge 增加了最小的推理延迟，对输出语义流畅性影响 negligible，并支持可调的 Safety-Performance Trade-offs (SPTs)，允许用户平衡安全性和性能。该框架是开源的，并与 Hugging Face 的 transformers 库兼容，便于广泛部署。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02018v1",
      "published_date": "2025-01-02 15:15:38 UTC",
      "updated_date": "2025-01-02 15:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:49:50.822144"
    },
    {
      "arxiv_id": "2501.01480v3",
      "title": "CORAL: Concept Drift Representation Learning for Co-evolving Time-series",
      "title_zh": "翻译失败",
      "authors": [
        "Kunpeng Xu",
        "Lifei Chen",
        "Shengrui Wang"
      ],
      "abstract": "In the realm of time series analysis, tackling the phenomenon of concept\ndrift poses a significant challenge. Concept drift -- characterized by the\nevolving statistical properties of time series data, affects the reliability\nand accuracy of conventional analysis models. This is particularly evident in\nco-evolving scenarios where interactions among variables are crucial. This\npaper presents CORAL, a simple yet effective method that models time series as\nan evolving ecosystem to learn representations of concept drift. CORAL employs\na kernel-induced self-representation learning to generate a representation\nmatrix, encapsulating the inherent dynamics of co-evolving time series. This\nmatrix serves as a key tool for identification and adaptation to concept drift\nby observing its temporal variations. Furthermore, CORAL effectively identifies\nprevailing patterns and offers insights into emerging trends through pattern\nevolution analysis. Our empirical evaluation of CORAL across various datasets\ndemonstrates its effectiveness in handling the complexities of concept drift.\nThis approach introduces a novel perspective in the theoretical domain of\nco-evolving time series analysis, enhancing adaptability and accuracy in the\nface of dynamic data environments, and can be easily integrated into most deep\nlearning backbones.",
      "tldr_zh": "该论文针对时间序列分析中的 concept drift 问题提出了一种简单有效的 CORAL 方法，该方法将时间序列视为演化生态系统，以学习 co-evolving time-series 的表示。CORAL 通过 kernel-induced self-representation learning 生成表示矩阵，用于捕捉数据动态并通过其时间变化识别和适应 concept drift，同时进行 pattern evolution analysis 以揭示模式和趋势。实验结果显示，CORAL 在多种数据集上表现出色，提升了模型的适应性和准确性，并易于集成到大多数深度学习框架中。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01480v3",
      "published_date": "2025-01-02 15:09:00 UTC",
      "updated_date": "2025-01-31 18:13:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:49:08.267900"
    },
    {
      "arxiv_id": "2501.02016v1",
      "title": "ST-HCSS: Deep Spatio-Temporal Hypergraph Convolutional Neural Network for Soft Sensing",
      "title_zh": "ST-HCSS：深度时空超图卷积神经网络用于软传感",
      "authors": [
        "Hwa Hui Tew",
        "Fan Ding",
        "Gaoxuan Li",
        "Junn Yong Loo",
        "Chee-Ming Ting",
        "Ze Yang Ding",
        "Chee Pin Tan"
      ],
      "abstract": "Higher-order sensor networks are more accurate in characterizing the\nnonlinear dynamics of sensory time-series data in modern industrial settings by\nallowing multi-node connections beyond simple pairwise graph edges. In light of\nthis, we propose a deep spatio-temporal hypergraph convolutional neural network\nfor soft sensing (ST-HCSS). In particular, our proposed framework is able to\nconstruct and leverage a higher-order graph (hypergraph) to model the complex\nmulti-interactions between sensor nodes in the absence of prior structural\nknowledge. To capture rich spatio-temporal relationships underlying sensor\ndata, our proposed ST-HCSS incorporates stacked gated temporal and hypergraph\nconvolution layers to effectively aggregate and update hypergraph information\nacross time and nodes. Our results validate the superiority of ST-HCSS compared\nto existing state-of-the-art soft sensors, and demonstrates that the learned\nhypergraph feature representations aligns well with the sensor data\ncorrelations. The code is available at https://github.com/htew0001/ST-HCSS.git",
      "tldr_zh": "本研究提出了一种深度时空超图卷积神经网络（ST-HCSS），用于软传感（soft sensing），旨在通过构建更高阶的传感器网络来更好地表征工业环境中传感器时间序列数据的非线性动态。该框架在没有先验结构知识的情况下，构建超图来模型传感器节点间的复杂多交互，并结合堆叠的门控时间卷积层和超图卷积层（hypergraph convolutional layers）来捕获丰富的时空关系。实验结果显示，ST-HCSS 优于现有最先进软传感器方法，且学到的超图特征表示与传感器数据相关性高度一致。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.02016v1",
      "published_date": "2025-01-02 15:06:43 UTC",
      "updated_date": "2025-01-02 15:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:49:22.225946"
    },
    {
      "arxiv_id": "2501.02015v1",
      "title": "KANS: Knowledge Discovery Graph Attention Network for Soft Sensing in Multivariate Industrial Processes",
      "title_zh": "KANS：知识发现图注意力网络用于多变量工业过程的软测量",
      "authors": [
        "Hwa Hui Tew",
        "Gaoxuan Li",
        "Fan Ding",
        "Xuewen Luo",
        "Junn Yong Loo",
        "Chee-Ming Ting",
        "Ze Yang Ding",
        "Chee Pin Tan"
      ],
      "abstract": "Soft sensing of hard-to-measure variables is often crucial in industrial\nprocesses. Current practices rely heavily on conventional modeling techniques\nthat show success in improving accuracy. However, they overlook the non-linear\nnature, dynamics characteristics, and non-Euclidean dependencies between\ncomplex process variables. To tackle these challenges, we present a framework\nknown as a Knowledge discovery graph Attention Network for effective Soft\nsensing (KANS). Unlike the existing deep learning soft sensor models, KANS can\ndiscover the intrinsic correlations and irregular relationships between the\nmultivariate industrial processes without a predefined topology. First, an\nunsupervised graph structure learning method is introduced, incorporating the\ncosine similarity between different sensor embedding to capture the\ncorrelations between sensors. Next, we present a graph attention-based\nrepresentation learning that can compute the multivariate data parallelly to\nenhance the model in learning complex sensor nodes and edges. To fully explore\nKANS, knowledge discovery analysis has also been conducted to demonstrate the\ninterpretability of the model. Experimental results demonstrate that KANS\nsignificantly outperforms all the baselines and state-of-the-art methods in\nsoft sensing performance. Furthermore, the analysis shows that KANS can find\nsensors closely related to different process variables without domain\nknowledge, significantly improving soft sensing accuracy.",
      "tldr_zh": "本研究提出了一种名为 KANS 的知识发现图注意力网络框架，用于多变量工业过程中的软测量（Soft Sensing），旨在解决现有模型忽略非线性、动态特性和非欧空间依赖的问题。KANS 通过无监督图结构学习方法，利用传感器嵌入的余弦相似度捕获变量间的内在相关性，并结合图注意力表示学习实现多变量数据的并行计算，从而提升对复杂传感器节点和边的学习能力。实验结果显示，KANS 在软测量性能上显著优于基线和最先进方法，且无需领域知识即可识别与过程变量密切相关的传感器，提高了模型的可解释性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SP",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2501.02015v1",
      "published_date": "2025-01-02 15:02:36 UTC",
      "updated_date": "2025-01-02 15:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:49:32.656059"
    },
    {
      "arxiv_id": "2501.14779v2",
      "title": "The Use of Generative Artificial Intelligence for Upper Secondary Mathematics Education Through the Lens of Technology Acceptance",
      "title_zh": "翻译失败",
      "authors": [
        "Mika Setälä",
        "Ville Heilala",
        "Pieta Sikström",
        "Tommi Kärkkäinen"
      ],
      "abstract": "This study investigated the students' perceptions of using Generative\nArtificial Intelligence (GenAI) in upper-secondary mathematics education. Data\nwas collected from Finnish high school students to represent how key constructs\nof the Technology Acceptance Model (Perceived Usefulness, Perceived Ease of\nUse, Perceived Enjoyment, and Intention to Use) influence the adoption of AI\ntools. First, a structural equation model for a comparative study with a prior\nstudy was constructed and analyzed. Then, an extended model with the additional\nconstruct of Compatibility, which represents the alignment of AI tools with\nstudents' educational experiences and needs, was proposed and analyzed. The\nresults demonstrated a strong influence of perceived usefulness on the\nintention to use GenAI, emphasizing the statistically significant role of\nperceived enjoyment in determining perceived usefulness and ease of use. The\ninclusion of compatibility improved the model's explanatory power, particularly\nin predicting perceived usefulness. This study contributes to a deeper\nunderstanding of how AI tools can be integrated into mathematics education and\nhighlights key differences between the Finnish educational context and previous\nstudies based on structural equation modeling.",
      "tldr_zh": "这项研究调查了芬兰高中学生对在数学教育中使用Generative Artificial Intelligence (GenAI)的看法，采用Technology Acceptance Model (TAM)框架分析Perceived Usefulness、Perceived Ease of Use、Perceived Enjoyment 和 Intention to Use等关键因素的影响。研究首先构建并分析了一个structural equation model 与先前研究的比较，然后扩展模型加入Compatibility 构建块，以评估AI工具与学生教育经验的契合度。结果显示，Perceived Usefulness 对 Intention to Use 有强烈影响，而Perceived Enjoyment 在决定 Perceived Usefulness 和 Ease of Use 方面显著；添加 Compatibility 显著提高了模型的解释力。该研究加深了对AI工具在数学教育中整合的理解，并突出了芬兰教育背景与其他研究的差异。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "K.3.1; I.2"
      ],
      "primary_category": "cs.CY",
      "comment": "Published in the Proceedings of the 40th ACM/SIGAPP Symposium on\n  Applied Computing (SAC'25), March 31--April 4, 2025, Catania, Italy",
      "pdf_url": "http://arxiv.org/pdf/2501.14779v2",
      "published_date": "2025-01-02 14:50:30 UTC",
      "updated_date": "2025-04-15 19:20:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:50:33.743312"
    },
    {
      "arxiv_id": "2501.01284v1",
      "title": "NeutraSum: A Language Model can help a Balanced Media Diet by Neutralizing News Summaries",
      "title_zh": "NeutraSum：语言模型可以通过中和新闻摘要帮助实现平衡媒体消费",
      "authors": [
        "Xi Luo",
        "Junjie Liu",
        "Sirong Wu",
        "Yuhui Deng"
      ],
      "abstract": "Media bias in news articles arises from the political polarisation of media\noutlets, which can reinforce societal stereotypes and beliefs. Reporting on the\nsame event often varies significantly between outlets, reflecting their\npolitical leanings through polarised language and focus. Although previous\nstudies have attempted to generate bias-free summaries from multiperspective\nnews articles, they have not effectively addressed the challenge of mitigating\ninherent media bias. To address this gap, we propose \\textbf{NeutraSum}, a\nnovel framework that integrates two neutrality losses to adjust the semantic\nspace of generated summaries, thus minimising media bias. These losses,\ndesigned to balance the semantic distances across polarised inputs and ensure\nalignment with expert-written summaries, guide the generation of neutral and\nfactually rich summaries. To evaluate media bias, we employ the political\ncompass test, which maps political leanings based on economic and social\ndimensions. Experimental results on the Allsides dataset demonstrate that\nNeutraSum not only improves summarisation performance but also achieves\nsignificant reductions in media bias, offering a promising approach for neutral\nnews summarisation.",
      "tldr_zh": "本论文提出 NeutraSum 框架，利用语言模型通过两种 neutrality losses 来调整生成摘要的语义空间，从而减少新闻报道中的媒体偏见，并帮助用户获得平衡的媒体饮食。  \n该框架平衡了不同政治倾向输入之间的语义距离，并确保生成的摘要与专家撰写的内容对齐，从而产生中立且事实丰富的总结。  \n实验结果在 Allsides 数据集上表明，NeutraSum 显著提升了摘要性能，并通过 political compass test 评估，实现了媒体偏见的有效降低。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01284v1",
      "published_date": "2025-01-02 14:48:07 UTC",
      "updated_date": "2025-01-02 14:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:50:37.581130"
    },
    {
      "arxiv_id": "2501.01282v1",
      "title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries",
      "title_zh": "翻译失败",
      "authors": [
        "Shudong Liu",
        "Yiqiao Jin",
        "Cheng Li",
        "Derek F. Wong",
        "Qingsong Wen",
        "Lichao Sun",
        "Haipeng Chen",
        "Xing Xie",
        "Jindong Wang"
      ],
      "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle\nwith cultural understanding, often misinterpreting symbols, gestures, and\nartifacts due to biases in predominantly Western-centric training data. In this\npaper, we construct CultureVerse, a large-scale multimodal benchmark covering\n19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3\nquestion types, with the aim of characterizing and improving VLMs'\nmulticultural understanding capabilities. Then, we propose CultureVLM, a series\nof VLMs fine-tuned on our dataset to achieve significant performance\nimprovement in cultural understanding. Our evaluation of 16 models reveals\nsignificant disparities, with a stronger performance in Western concepts and\nweaker results in African and Asian contexts. Fine-tuning on our CultureVerse\nenhances cultural perception, demonstrating cross-cultural, cross-continent,\nand cross-dataset generalization without sacrificing performance on models'\ngeneral VLM benchmarks. We further present insights on cultural generalization\nand forgetting. We hope that this work could lay the foundation for more\nequitable and culturally aware multimodal AI systems.",
      "tldr_zh": "视觉语言模型(VLMs)由于训练数据偏向西方，常在文化理解上出现偏差，如误解符号、手势和文物。本文构建了CultureVerse，一个大规模多模态基准数据集，涵盖19,682个文化概念、188个国家/地区和15个文化类型，用于评估和提升VLMs的多文化能力。作者提出了CultureVLM系列模型，通过在CultureVerse上微调，实现显著性能提升，并在评估16个模型时发现其在非洲和亚洲语境下的表现得到改善，同时保持了跨文化泛化和一般VLM基准的性能。该工作为构建更公平、文化感知的多模态AI系统提供了基础。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical report; 26 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01282v1",
      "published_date": "2025-01-02 14:42:37 UTC",
      "updated_date": "2025-01-02 14:42:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:50:27.091910"
    },
    {
      "arxiv_id": "2501.01266v1",
      "title": "PIMAEX: Multi-Agent Exploration through Peer Incentivization",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Kölle",
        "Johannes Tochtermann",
        "Julian Schönberger",
        "Gerhard Stenzel",
        "Philipp Altmann",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "While exploration in single-agent reinforcement learning has been studied\nextensively in recent years, considerably less work has focused on its\ncounterpart in multi-agent reinforcement learning. To address this issue, this\nwork proposes a peer-incentivized reward function inspired by previous research\non intrinsic curiosity and influence-based rewards. The \\textit{PIMAEX} reward,\nshort for Peer-Incentivized Multi-Agent Exploration, aims to improve\nexploration in the multi-agent setting by encouraging agents to exert influence\nover each other to increase the likelihood of encountering novel states. We\nevaluate the \\textit{PIMAEX} reward in conjunction with\n\\textit{PIMAEX-Communication}, a multi-agent training algorithm that employs a\ncommunication channel for agents to influence one another. The evaluation is\nconducted in the \\textit{Consume/Explore} environment, a partially observable\nenvironment with deceptive rewards, specifically designed to challenge the\nexploration vs.\\ exploitation dilemma and the credit-assignment problem. The\nresults empirically demonstrate that agents using the \\textit{PIMAEX} reward\nwith \\textit{PIMAEX-Communication} outperform those that do not.",
      "tldr_zh": "本研究针对多代理强化学习(multi-agent reinforcement learning)中的探索问题，提出了一种基于同伴激励(peer-incentivized)的奖励函数PIMAEX，灵感来源于内在好奇心(intrinsic curiosity)和基于影响的奖励(influence-based rewards)，以鼓励代理相互影响从而增加遇到新状态的可能性。PIMAEX与PIMAEX-Communication算法结合，后者通过通信通道让代理间互动进行训练。实验在部分可观察的Consume/Explore环境中进行，结果显示使用PIMAEX奖励和通信机制的代理在探索与剥削困境以及信用分配问题(credit-assignment problem)上表现出色，优于基线模型。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted at ICAART 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01266v1",
      "published_date": "2025-01-02 14:06:52 UTC",
      "updated_date": "2025-01-02 14:06:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:51:10.399999"
    },
    {
      "arxiv_id": "2501.01264v1",
      "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoshuai Song",
        "Yanan Wu",
        "Weixun Wang",
        "Jiaheng Liu",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
      "tldr_zh": "本论文提出 ProgCo 方法，利用程序驱动机制帮助 Large Language Models (LLMs) 实现有效的 Self-Correction，从而解决 LLMs 在复杂推理任务中自验证和反馈生成失败的问题。ProgCo 包括 Program-driven verification (ProgVe)，通过自生成自执行的验证伪程序来处理复杂验证逻辑；以及 Program-driven refinement (ProgRe)，基于反馈对响应和程序进行双重反思和精炼，以减少错误反馈的误导。实验结果显示，在三个指令跟随和数学基准上，ProgCo 显著提升了自修正性能，并可与真实程序工具结合进一步优化表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Working in progress",
      "pdf_url": "http://arxiv.org/pdf/2501.01264v1",
      "published_date": "2025-01-02 13:59:20 UTC",
      "updated_date": "2025-01-02 13:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:51:20.938780"
    },
    {
      "arxiv_id": "2501.01263v1",
      "title": "Stealthy Backdoor Attack to Real-world Models in Android Apps",
      "title_zh": "针对 Android 应用中真实世界模型的隐秘后门攻击",
      "authors": [
        "Jiali Wei",
        "Ming Fan",
        "Xicheng Zhang",
        "Wenjing Jiao",
        "Haijun Wang",
        "Ting Liu"
      ],
      "abstract": "Powered by their superior performance, deep neural networks (DNNs) have found\nwidespread applications across various domains. Many deep learning (DL) models\nare now embedded in mobile apps, making them more accessible to end users\nthrough on-device DL. However, deploying on-device DL to users' smartphones\nsimultaneously introduces several security threats. One primary threat is\nbackdoor attacks. Extensive research has explored backdoor attacks for several\nyears and has proposed numerous attack approaches. However, few studies have\ninvestigated backdoor attacks on DL models deployed in the real world, or they\nhave shown obvious deficiencies in effectiveness and stealthiness. In this\nwork, we explore more effective and stealthy backdoor attacks on real-world DL\nmodels extracted from mobile apps. Our main justification is that imperceptible\nand sample-specific backdoor triggers generated by DNN-based steganography can\nenhance the efficacy of backdoor attacks on real-world models. We first confirm\nthe effectiveness of steganography-based backdoor attacks on four\nstate-of-the-art DNN models. Subsequently, we systematically evaluate and\nanalyze the stealthiness of the attacks to ensure they are difficult to\nperceive. Finally, we implement the backdoor attacks on real-world models and\ncompare our approach with three baseline methods. We collect 38,387 mobile\napps, extract 89 DL models from them, and analyze these models to obtain the\nprerequisite model information for the attacks. After identifying the target\nmodels, our approach achieves an average of 12.50% higher attack success rate\nthan DeepPayload while better maintaining the normal performance of the models.\nExtensive experimental results demonstrate that our method enables more\neffective, robust, and stealthy backdoor attacks on real-world models.",
      "tldr_zh": "本文提出了一种针对 Android 应用中真实世界 DNN 模型的隐蔽后门攻击方法，使用 DNN-based steganography 生成不易察觉的样本特定触发器，以提升攻击的有效性和隐蔽性。该方法首先验证了其在四种先进 DNN 模型上的有效性，并通过系统评估确保攻击不易被察觉。实验结果显示，在提取自 38,387 个移动应用的 89 个 DL 模型上，该方法比基线方法 DeepPayload 的攻击成功率平均高 12.50%，同时更好地维持模型的正常性能，证明了其更有效、鲁棒和隐蔽的特性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01263v1",
      "published_date": "2025-01-02 13:58:05 UTC",
      "updated_date": "2025-01-02 13:58:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:51:32.625579"
    },
    {
      "arxiv_id": "2501.07585v1",
      "title": "Multi-task Domain Adaptation for Computation Offloading in Edge-intelligence Networks",
      "title_zh": "多任务领域适应用于边缘智能网络中的计算卸载",
      "authors": [
        "Runxin Han",
        "Bo Yang",
        "Zhiwen Yu",
        "Xuelin Cao",
        "George C. Alexandropoulos",
        "Chau Yuen"
      ],
      "abstract": "In the field of multi-access edge computing (MEC), efficient computation\noffloading is crucial for improving resource utilization and reducing latency\nin dynamically changing environments. This paper introduces a new approach,\ntermed as Multi-Task Domain Adaptation (MTDA), aiming to enhance the ability of\ncomputational offloading models to generalize in the presence of domain shifts,\ni.e., when new data in the target environment significantly differs from the\ndata in the source domain. The proposed MTDA model incorporates a\nteacher-student architecture that allows continuous adaptation without\nnecessitating access to the source domain data during inference, thereby\nmaintaining privacy and reducing computational overhead. Utilizing a multi-task\nlearning framework that simultaneously manages offloading decisions and\nresource allocation, the proposed MTDA approach outperforms benchmark methods\nregarding mean squared error and accuracy, particularly in environments with\nincreasing numbers of users. It is observed by means of computer simulation\nthat the proposed MTDA model maintains high performance across various\nscenarios, demonstrating its potential for practical deployment in emerging MEC\napplications.",
      "tldr_zh": "本文提出 Multi-Task Domain Adaptation (MTDA) 方法，用于多接入边缘计算 (MEC) 中的计算卸载，以提升模型在领域偏移（domain shifts）下的泛化能力，解决动态环境中的资源利用和延迟问题。MTDA 采用 teacher-student 架构和多任务学习框架，实现无需访问源域数据的持续适应，同时管理卸载决策和资源分配，从而保护隐私并减少计算开销。实验模拟结果表明，MTDA 在均方误差和准确率上优于基准方法，尤其在用户数量增加的场景中，并显示出在实际 MEC 应用中的高潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.07585v1",
      "published_date": "2025-01-02 13:20:29 UTC",
      "updated_date": "2025-01-02 13:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:52:39.632902"
    },
    {
      "arxiv_id": "2501.01243v2",
      "title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Lixiong Qin",
        "Shilong Ou",
        "Miaoxuan Zhang",
        "Jiangning Wei",
        "Yuhang Zhang",
        "Xiaoshuai Song",
        "Yuchen Liu",
        "Mei Wang",
        "Weiran Xu"
      ],
      "abstract": "Faces and humans are crucial elements in social interaction and are widely\nincluded in everyday photos and videos. Therefore, a deep understanding of\nfaces and humans will enable multi-modal assistants to achieve improved\nresponse quality and broadened application scope. Currently, the multi-modal\nassistant community lacks a comprehensive and scientific evaluation of face and\nhuman understanding abilities. In this paper, we first propose a hierarchical\nability taxonomy that includes three levels of abilities. Then, based on this\ntaxonomy, we collect images and annotations from publicly available datasets in\nthe face and human community and build a semi-automatic data pipeline to\nproduce problems for the new benchmark. Finally, the obtained Face-Human-Bench\ncomprises a development set with 900 problems and a test set with 1800\nproblems, supporting both English and Chinese. We conduct evaluations over 25\nmainstream multi-modal large language models (MLLMs) with our Face-Human-Bench,\nfocusing on the correlation between abilities, the impact of the relative\nposition of targets on performance, and the impact of Chain of Thought (CoT)\nprompting on performance. Moreover, inspired by multi-modal agents, we also\nexplore which abilities of MLLMs need to be supplemented by specialist models.",
      "tldr_zh": "本论文提出Face-Human-Bench，一种全面基准，用于评估多模态助手（Multi-modal Assistants）在面部和人类理解方面的能力，强调这些能力对提升响应质量和应用范围的重要性。研究者建立了分层能力分类（Hierarchical Ability Taxonomy），并通过半自动数据管道从公开数据集收集图像和注解，构建了包含900个开发集和1800个测试集问题的基准，支持英文和中文。实验评估了25个主流多模态大语言模型（MLLMs），分析了能力之间的相关性、目标相对位置对性能的影响，以及Chain of Thought (CoT)提示的效果，并探讨了MLLMs哪些能力需由专业模型补充。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "50 pages, 14 figures, 41 tables. Submitted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01243v2",
      "published_date": "2025-01-02 13:05:47 UTC",
      "updated_date": "2025-01-05 08:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:54:45.153197"
    },
    {
      "arxiv_id": "2501.01242v1",
      "title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
      "title_zh": "翻译失败",
      "authors": [
        "Uzma Mushtaque"
      ],
      "abstract": "Transformer based models are increasingly being used in various domains\nincluding recommender systems (RS). Pretrained transformer models such as BERT\nhave shown good performance at language modelling. With the greater ability to\nmodel sequential tasks, variants of Encoder-only models (like BERT4Rec, SASRec\netc.) have found success in sequential RS problems. Computing dot-product\nattention in traditional transformer models has quadratic complexity in\nsequence length. This is a bigger problem with RS because unlike language\nmodels, new items are added to the catalogue every day. User buying history is\na dynamic sequence which depends on multiple factors. Recently, various linear\nattention models have tried to solve this problem by making the model linear in\nsequence length (token dimensions). Hydra attention is one such linear\ncomplexity model proposed for vision transformers which reduces the complexity\nof attention for both the number of tokens as well as model embedding\ndimensions. Building on the idea of Hydra attention, we introduce an efficient\nTransformer based Sequential RS (HydraRec) which significantly improves\ntheoretical complexity of computing attention for longer sequences and bigger\ndatasets while preserving the temporal context. Extensive experiments are\nconducted to evaluate other linear transformer-based RS models and compared\nwith HydraRec across various evaluation metrics. HydraRec outperforms other\nlinear attention-based models as well as dot-product based attention models\nwhen used with causal masking for sequential recommendation next item\nprediction tasks. For bi-directional models its performance is comparable to\nthe BERT4Rec model with an improvement in running time.",
      "tldr_zh": "本论文提出 HydraRec，一种基于高效 Hydra attention 机制的 Transformer 模型，用于序列推荐任务（Sequential RS），以解决传统 dot-product attention 在长序列计算中存在的二次方复杂度问题。HydraRec 通过线性复杂度的注意力计算，保留了时间上下文，同时适用于动态的用户历史序列和不断更新的物品目录。实验结果表明，HydraRec 在因果掩码的下一个物品预测任务中超过了其他线性注意力模型和 dot-product 模型，在各种评估指标上表现出色；对于双向模型，其性能与 BERT4Rec 相当，但运行时间显著改善。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01242v1",
      "published_date": "2025-01-02 13:03:06 UTC",
      "updated_date": "2025-01-02 13:03:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:52:09.629438"
    },
    {
      "arxiv_id": "2501.01478v1",
      "title": "Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search",
      "title_zh": "通过",
      "authors": [
        "Shuangtao Li",
        "Shuaihao Dong",
        "Kexin Luan",
        "Xinhan Di",
        "Chaofan Ding"
      ],
      "abstract": "Large language models (LLMs) have demonstrated their remarkable capacity\nacross a variety of tasks. However, reasoning remains a challenge for LLMs. To\nimprove LLMs' reasoning ability, process supervision has proven to be better\nthan outcome supervision. In this work, we study using Monte Carlo Tree Search\n(MCTS) to generate process supervision data with LLMs themselves for training\nthem. We sample reasoning steps with an LLM and assign each step a score that\ncaptures its \"relative correctness,\" and the LLM is then trained by minimizing\nweighted log-likelihood of generating the reasoning steps. This\ngenerate-then-train process is repeated iteratively until convergence.Our\nexperimental results demonstrate that the proposed methods considerably improve\nthe performance of LLMs on two mathematical reasoning datasets. Furthermore,\nmodels trained on one dataset also exhibit improved performance on the other,\nshowing the transferability of the enhanced reasoning ability.",
      "tldr_zh": "本文提出了一种使用 Monte Carlo Tree Search (MCTS) 生成过程监督数据的方法，以提升大语言模型 (LLMs) 的推理能力。具体过程涉及用 LLM 采样推理步骤、为每个步骤分配“相对正确性”分数，并通过最小化加权对数似然进行迭代训练，直至收敛。实验结果显示，该方法在两个数学推理数据集上显著提高了模型性能，且训练后的模型在其他数据集上也表现出可转移的推理能力提升。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure, 2 tables accepted by aaai 2025 NeurMAD workshop",
      "pdf_url": "http://arxiv.org/pdf/2501.01478v1",
      "published_date": "2025-01-02 12:09:17 UTC",
      "updated_date": "2025-01-02 12:09:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:52:20.762504"
    },
    {
      "arxiv_id": "2501.02009v2",
      "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Youcheng Huang",
        "Chen Huang",
        "Duanyu Feng",
        "Wenqiang Lei",
        "Jiancheng Lv"
      ],
      "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a\ncritical research frontier. Prior research has shown that a single LLM's\nconcept representations can be captured as steering vectors (SVs), enabling the\ncontrol of LLM behavior (e.g., towards generating harmful content). Our work\ntakes a novel approach by exploring the intricate relationships between concept\nrepresentations across different LLMs, drawing an intriguing parallel to\nPlato's Allegory of the Cave. In particular, we introduce a linear\ntransformation method to bridge these representations and present three key\nfindings: 1) Concept representations across different LLMs can be effectively\naligned using simple linear transformations, enabling efficient cross-model\ntransfer and behavioral control via SVs. 2) This linear transformation\ngeneralizes across concepts, facilitating alignment and control of SVs\nrepresenting different concepts across LLMs. 3) A weak-to-strong\ntransferability exists between LLM concept representations, whereby SVs\nextracted from smaller LLMs can effectively control the behavior of larger\nLLMs.",
      "tldr_zh": "本文研究大型语言模型(LLMs)之间概念表示的跨模型可转移性，引入线性变换方法来桥接这些Platonic Representations，并类比柏拉图的洞穴寓言。关键发现包括：不同LLMs的概念表示可以通过简单线性变换有效对齐，实现steering vectors(SVs)的高效转移和行为控制；这种变换在各种概念上具有泛化性；以及弱到强的转移性，即从较小LLMs提取的SVs能有效控制较大LLMs的行为。这些结果为理解和操控LLMs的内部机制提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2501.02009v2",
      "published_date": "2025-01-02 11:56:59 UTC",
      "updated_date": "2025-05-20 03:24:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:52:34.077820"
    },
    {
      "arxiv_id": "2501.01209v1",
      "title": "A redescription mining framework for post-hoc explaining and relating deep learning models",
      "title_zh": "一种用于事后解释",
      "authors": [
        "Matej Mihelčić",
        "Ivan Grubišić",
        "Miha Keber"
      ],
      "abstract": "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
      "tldr_zh": "这篇论文提出了一种redescription mining框架，用于事后解释和关联深层学习模型(DLMs)，以解决DLMs的可解释性挑战。该框架通过识别神经元激活的统计显著redescriptions，进行DLMs的群组分析，并支持将神经元与目标标签（如多标签场景）或描述属性关联，以及层内或跨模型的关联。框架独立于神经网络架构，能模拟教学和分解方法提取规则，从而提供比现有可解释AI方法更丰富的解释信息，提升DLMs的整体可解释性和可解释性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01209v1",
      "published_date": "2025-01-02 11:38:10 UTC",
      "updated_date": "2025-01-02 11:38:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:53:31.669897"
    },
    {
      "arxiv_id": "2501.03259v1",
      "title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Mushtaq",
        "Muhammad Rafay Naeem",
        "Muhammad Imran Taj",
        "Ibrahim Ghaznavi",
        "Junaid Qadir"
      ],
      "abstract": "As large language models (LLMs) like GPT-4 and Llama 3 become integral to\neducational contexts, concerns are mounting over the cultural biases, power\nimbalances, and ethical limitations embedded within these technologies. Though\ngenerative AI tools aim to enhance learning experiences, they often reflect\nvalues rooted in Western, Educated, Industrialized, Rich, and Democratic\n(WEIRD) cultural paradigms, potentially sidelining diverse global perspectives.\nThis paper proposes a framework to assess and mitigate cultural bias within\nLLMs through the lens of applied multiplexity. Multiplexity, inspired by\nSenturk et al. and rooted in Islamic and other wisdom traditions, emphasizes\nthe coexistence of diverse cultural viewpoints, supporting a multi-layered\nepistemology that integrates both empirical sciences and normative values. Our\nanalysis reveals that LLMs frequently exhibit cultural polarization, with\nbiases appearing in both overt responses and subtle contextual cues. To address\ninherent biases and incorporate multiplexity in LLMs, we propose two\nstrategies: \\textit{Contextually-Implemented Multiplex LLMs}, which embed\nmultiplex principles directly into the system prompt, influencing LLM outputs\nat a foundational level and independent of individual prompts, and\n\\textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple\nLLM agents, each representing distinct cultural viewpoints, collaboratively\ngenerate a balanced, synthesized response. Our findings demonstrate that as\nmitigation strategies evolve from contextual prompting to MAS-implementation,\ncultural inclusivity markedly improves, evidenced by a significant rise in the\nPerspectives Distribution Score (PDS) and a PDS Entropy increase from 3.25\\% at\nbaseline to 98\\% with the MAS-Implemented Multiplex LLMs. Sentiment analysis\nfurther shows a shift towards positive sentiment across cultures,...",
      "tldr_zh": "本研究关注大型语言模型（LLMs）在教育中的文化偏见问题，如根植于 WEIRD（Western, Educated, Industrialized, Rich, and Democratic）文化的偏见，可能忽略全球多样视角，并提出通过 multiplexity 镜头评估和缓解这些偏见的框架。multiplexity 强调多种文化观点的共存，研究发现 LLMs 存在文化极化偏见，并引入两种策略：Contextually-Implemented Multiplex LLMs（将 multiplex 原则嵌入系统提示）和 Multi-Agent System (MAS)-Implemented Multiplex LLMs（多个代表不同文化的 LLM 代理协作生成响应）。实验结果显示，随着策略从上下文提示升级到 MAS 实现，文化包容性显著提升，Perspectives Distribution Score (PDS) 从基线 3.25% 上升到 98%，情感分析也显示跨文化积极情感的增加，从而推动更具包容性的教育 AI 发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03259v1",
      "published_date": "2025-01-02 11:27:08 UTC",
      "updated_date": "2025-01-02 11:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:53:15.451128"
    },
    {
      "arxiv_id": "2501.01205v1",
      "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Mushtaq",
        "Muhammad Rafay Naeem",
        "Ibrahim Ghaznavi",
        "Muhammad Imran Taj",
        "Imran Hashmi",
        "Junaid Qadir"
      ],
      "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...",
      "tldr_zh": "这篇论文探讨了 Multi-Agent LLMs 在复杂工程问题解决中的应用，提出一个框架来支持工程学生的毕业设计项目（Senior Design Projects），以整合多学科视角和冲突目标。框架中，各种 LLM 代理代表不同专家角色（如问题制定代理、系统复杂性代理和社会伦理代理），通过协调、合作和谈判机制进行协作对话，借鉴 Multi-Agent System (MAS) 概念和 swarm AI 原则模拟真实工程团队。该方法通过提示工程增强代理的多样化角色，并在收集的工程提案上进行评估，证明其能有效提升问题解决效率和跨学科推理能力。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01205v1",
      "published_date": "2025-01-02 11:25:45 UTC",
      "updated_date": "2025-01-02 11:25:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:32.892852"
    },
    {
      "arxiv_id": "2501.01195v1",
      "title": "Data Augmentation Techniques for Chinese Disease Name Normalization",
      "title_zh": "针对中文疾病名称规范化的数据增强技术",
      "authors": [
        "Wenqian Cui",
        "Xiangling Fu",
        "Shaohui Liu",
        "Mingjun Gu",
        "Xien Liu",
        "Ji Wu",
        "Irwin King"
      ],
      "abstract": "Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Through extensive experimentation, we illustrate that our\nproposed approach exhibits significant performance improvements across various\nbaseline models and training objectives, particularly in scenarios with limited\ntraining data",
      "tldr_zh": "这篇论文针对中文疾病名称规范化的数据短缺问题，提出了一种新型 Data Augmentation 方法，包括一系列数据增强技术和支持模块，以帮助生成更多训练数据。研究通过广泛实验证明，该方法显著提升了各种基线模型的性能，尤其适用于训练数据有限的场景。整体而言，该方法为智能医疗系统中的疾病名称规范化任务提供了有效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The Version of Record of this contribution is published in 2024 IEEE\n  International Conference on Bioinformatics and Biomedicine (BIBM 2024)",
      "pdf_url": "http://arxiv.org/pdf/2501.01195v1",
      "published_date": "2025-01-02 11:12:03 UTC",
      "updated_date": "2025-01-02 11:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:55:08.122859"
    },
    {
      "arxiv_id": "2501.01174v1",
      "title": "L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Soumyaratna Debnath",
        "Harish Katti",
        "Shashikant Verma",
        "Shanmuganathan Raman"
      ],
      "abstract": "While 2D pose estimation has advanced our ability to interpret body movements\nin animals and primates, it is limited by the lack of depth information,\nconstraining its application range. 3D pose estimation provides a more\ncomprehensive solution by incorporating spatial depth, yet creating extensive\n3D pose datasets for animals is challenging due to their dynamic and\nunpredictable behaviours in natural settings. To address this, we propose a\nhybrid approach that utilizes rigged avatars and the pipeline to generate\nsynthetic datasets to acquire the necessary 3D annotations for training. Our\nmethod introduces a simple attention-based MLP network for converting 2D poses\nto 3D, designed to be independent of the input image to ensure scalability for\nposes in natural environments. Additionally, we identify that existing\nanatomical keypoint detectors are insufficient for accurate pose retargeting\nonto arbitrary avatars. To overcome this, we present a lookup table based on a\ndeep pose estimation method using a synthetic collection of diverse actions\nrigged avatars perform. Our experiments demonstrate the effectiveness and\nefficiency of this lookup table-based retargeting approach. Overall, we propose\na comprehensive framework with systematically synthesized datasets for lifting\nposes from 2D to 3D and then utilize this to re-target motion from wild\nsettings onto arbitrary avatars.",
      "tldr_zh": "该研究解决了从单一摄像头在野外环境中将2D姿态提升到3D姿态的关键挑战，特别是针对动物和灵长类的动态行为。作者提出了一种混合方法，使用rigged avatars和合成数据集生成管道来获取3D标注，并设计了一个简单的注意力-based MLP网络，将2D poses转换为3D poses，该网络独立于输入图像以提升可扩展性。为了改进姿态重定向，他们引入了一个基于查找表的深度姿态估计方法，利用合成数据集处理任意avatars的精确映射。实验结果显示，该框架在从2D到3D姿态提升和动作重定向方面表现出色，证明了其有效性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2025 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.01174v1",
      "published_date": "2025-01-02 10:04:12 UTC",
      "updated_date": "2025-01-02 10:04:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:09.212373"
    },
    {
      "arxiv_id": "2501.01168v1",
      "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Mahdi Zakizadeh",
        "Mohammad Taher Pilehvar"
      ],
      "abstract": "The multifaceted challenge of accurately measuring gender stereotypical bias\nin language models is akin to discerning different segments of a broader,\nunseen entity. This short paper primarily focuses on intrinsic bias mitigation\nand measurement strategies for language models, building on prior research that\ndemonstrates a lack of correlation between intrinsic and extrinsic approaches.\nWe delve deeper into intrinsic measurements, identifying inconsistencies and\nsuggesting that these benchmarks may reflect different facets of gender\nstereotype. Our methodology involves analyzing data distributions across\ndatasets and integrating gender stereotype components informed by social\npsychology. By adjusting the distribution of two datasets, we achieve a better\nalignment of outcomes. Our findings underscore the complexity of gender\nstereotyping in language models and point to new directions for developing more\nrefined techniques to detect and reduce bias.",
      "tldr_zh": "这篇论文探讨了测量语言模型中性别刻板印象（gender stereotypes）的复杂性，强调内在偏见缓解（intrinsic bias mitigation）和测量策略的重要性，并指出先前研究显示内在和外在方法（extrinsic approaches）缺乏相关性。通过分析数据集的数据分布并整合社会心理学中的性别刻板印象组件，作者识别了内在测量的不一致性，并通过调整两个数据集的分布实现了结果的更好对齐。研究发现揭示了语言模型中性别刻板印象的多样性和复杂性，为开发更精确的偏见检测和减少技术提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01168v1",
      "published_date": "2025-01-02 09:40:31 UTC",
      "updated_date": "2025-01-02 09:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:13.691928"
    },
    {
      "arxiv_id": "2501.01166v1",
      "title": "Deep Learning in Palmprint Recognition-A Comprehensive Survey",
      "title_zh": "掌纹识别中的深度学习：一个全面的调查",
      "authors": [
        "Chengrui Gao",
        "Ziyuan Yang",
        "Wei Jia",
        "Lu Leng",
        "Bob Zhang",
        "Andrew Beng Jin Teoh"
      ],
      "abstract": "Palmprint recognition has emerged as a prominent biometric technology, widely\napplied in diverse scenarios. Traditional handcrafted methods for palmprint\nrecognition often fall short in representation capability, as they heavily\ndepend on researchers' prior knowledge. Deep learning (DL) has been introduced\nto address this limitation, leveraging its remarkable successes across various\ndomains. While existing surveys focus narrowly on specific tasks within\npalmprint recognition-often grounded in traditional methodologies-there remains\na significant gap in comprehensive research exploring DL-based approaches\nacross all facets of palmprint recognition. This paper bridges that gap by\nthoroughly reviewing recent advancements in DL-powered palmprint recognition.\nThe paper systematically examines progress across key tasks, including\nregion-of-interest segmentation, feature extraction, and\nsecurity/privacy-oriented challenges. Beyond highlighting these advancements,\nthe paper identifies current challenges and uncovers promising opportunities\nfor future research. By consolidating state-of-the-art progress, this review\nserves as a valuable resource for researchers, enabling them to stay abreast of\ncutting-edge technologies and drive innovation in palmprint recognition.",
      "tldr_zh": "这篇论文对掌纹识别中的Deep Learning (DL) 进行了全面调查，强调DL如何克服传统手crafted方法的局限性，如依赖研究者先验知识的不足。论文系统回顾了DL在关键任务中的进展，包括region-of-interest segmentation、特征提取以及安全/隐私挑战，并总结了现有问题和未来研究机会。该调查为研究者提供了宝贵资源，帮助推动掌纹识别领域的创新和发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Palmprint recognition, biometrics, deep learning, feature extraction,\n  recognition tasks",
      "pdf_url": "http://arxiv.org/pdf/2501.01166v1",
      "published_date": "2025-01-02 09:38:44 UTC",
      "updated_date": "2025-01-02 09:38:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:55:46.310892"
    },
    {
      "arxiv_id": "2501.01156v2",
      "title": "TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions",
      "title_zh": "TexAVi：从文本描述生成立体 VR 视频片段",
      "authors": [
        "Vriksha Srihari",
        "R. Bhavya",
        "Shruti Jayaraman",
        "V. Mary Anita Rajam"
      ],
      "abstract": "While generative models such as text-to-image, large language models and\ntext-to-video have seen significant progress, the extension to\ntext-to-virtual-reality remains largely unexplored, due to a deficit in\ntraining data and the complexity of achieving realistic depth and motion in\nvirtual environments. This paper proposes an approach to coalesce existing\ngenerative systems to form a stereoscopic virtual reality video from text.\n  Carried out in three main stages, we start with a base text-to-image model\nthat captures context from an input text. We then employ Stable Diffusion on\nthe rudimentary image produced, to generate frames with enhanced realism and\noverall quality. These frames are processed with depth estimation algorithms to\ncreate left-eye and right-eye views, which are stitched side-by-side to create\nan immersive viewing experience. Such systems would be highly beneficial in\nvirtual reality production, since filming and scene building often require\nextensive hours of work and post-production effort.\n  We utilize image evaluation techniques, specifically Fr\\'echet Inception\nDistance and CLIP Score, to assess the visual quality of frames produced for\nthe video. These quantitative measures establish the proficiency of the\nproposed method.\n  Our work highlights the exciting possibilities of using natural\nlanguage-driven graphics in fields like virtual reality simulations.",
      "tldr_zh": "这篇论文提出了TexAVi方法，用于从文本描述生成立体VR视频，解决了现有生成模型在text-to-VR领域的训练数据不足和深度运动复杂性问题。方法分为三个阶段：首先使用text-to-image模型捕获文本上下文，然后通过Stable Diffusion增强图像帧的质量，最后应用深度估计算法创建左右眼视图并拼接成沉浸式体验。实验利用Fréchet Inception Distance和CLIP Score等指标评估了视觉质量，证明了该方法的有效性，并突出了其在VR生产和模拟中的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "cs.CV",
      "comment": "Co-authors do not consent to publishing on Arxiv",
      "pdf_url": "http://arxiv.org/pdf/2501.01156v2",
      "published_date": "2025-01-02 09:21:03 UTC",
      "updated_date": "2025-03-10 20:52:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:04.077873"
    },
    {
      "arxiv_id": "2501.01149v2",
      "title": "A3: Android Agent Arena for Mobile GUI Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiang Chai",
        "Hanhao Li",
        "Jiayu Zhang",
        "Liang Liu",
        "Guangyi Liu",
        "Guozhi Wang",
        "Shuai Ren",
        "Siyuan Huang",
        "Hongsheng Li"
      ],
      "abstract": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at https://yuxiangchai.github.io/Android-Agent-Arena/.",
      "tldr_zh": "本文提出A3（Android Agent Arena），一个新型评估平台，旨在解决现有移动GUI代理数据集的局限性，这些数据集往往仅限于静态帧评估，而非真实世界任务。A3提供有意义的实际任务（如实时在线信息检索和操作指令）、更大的灵活行动空间（兼容任意数据集训练的代理），以及自动化的业务级LLM-based评估过程，以减少人力和编码需求。该平台涵盖21个通用第三方应用和201个常见用户场景任务，为移动GUI代理的真实环境性能评估奠定坚实基础，并推动相关研究的进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01149v2",
      "published_date": "2025-01-02 09:03:56 UTC",
      "updated_date": "2025-02-18 08:24:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:57:14.867820"
    },
    {
      "arxiv_id": "2501.04719v1",
      "title": "Calculating Customer Lifetime Value and Churn using Beta Geometric Negative Binomial and Gamma-Gamma Distribution in a NFT based setting",
      "title_zh": "翻译失败",
      "authors": [
        "Sagarnil Das"
      ],
      "abstract": "Customer Lifetime Value (CLV) is an important metric that measures the total\nvalue a customer will bring to a business over their lifetime. The Beta\nGeometric Negative Binomial Distribution (BGNBD) and Gamma Gamma Distribution\nare two models that can be used to calculate CLV, taking into account both the\nfrequency and value of customer transactions. This article explains the BGNBD\nand Gamma Gamma Distribution models, and how they can be used to calculate CLV\nfor NFT (Non-Fungible Token) transaction data in a blockchain setting. By\nestimating the parameters of these models using historical transaction data,\nbusinesses can gain insights into the lifetime value of their customers and\nmake data-driven decisions about marketing and customer retention strategies.",
      "tldr_zh": "该论文探讨了在 NFT（Non-Fungible Token）交易环境中，使用 Beta Geometric Negative Binomial Distribution (BGNBD) 和 Gamma Gamma Distribution 模型来计算客户终身价值 (CLV) 和客户流失 (Churn)。这些模型通过分析客户交易的频率和价值，基于历史数据估计参数，帮助企业量化客户的长期贡献。最终，研究为营销和客户保留策略提供数据驱动的决策洞见，提升了区块链场景下的业务优化。",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primary_category": "stat.AP",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04719v1",
      "published_date": "2025-01-02 08:55:54 UTC",
      "updated_date": "2025-01-02 08:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:43.928752"
    },
    {
      "arxiv_id": "2501.01136v2",
      "title": "Symmetries-enhanced Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolaos Bousias",
        "Stefanos Pertigkiozoglou",
        "Kostas Daniilidis",
        "George Pappas"
      ],
      "abstract": "Multi-agent reinforcement learning has emerged as a powerful framework for\nenabling agents to learn complex, coordinated behaviors but faces persistent\nchallenges regarding its generalization, scalability and sample efficiency.\nRecent advancements have sought to alleviate those issues by embedding\nintrinsic symmetries of the systems in the policy. Yet, most dynamical systems\nexhibit little to no symmetries to exploit. This paper presents a novel\nframework for embedding extrinsic symmetries in multi-agent system dynamics\nthat enables the use of symmetry-enhanced methods to address systems with\ninsufficient intrinsic symmetries, expanding the scope of equivariant learning\nto a wide variety of MARL problems. Central to our framework is the Group\nEquivariant Graphormer, a group-modular architecture specifically designed for\ndistributed swarming tasks. Extensive experiments on a swarm of\nsymmetry-breaking quadrotors validate the effectiveness of our approach,\nshowcasing its potential for improved generalization and zero-shot scalability.\nOur method achieves significant reductions in collision rates and enhances task\nsuccess rates across a diverse range of scenarios and varying swarm sizes.",
      "tldr_zh": "该论文提出了一种Symmetries-enhanced框架，用于多智能体强化学习(MARL)，通过嵌入外在对称性来解决系统缺乏内在对称性带来的泛化、可扩展性和样本效率挑战，从而扩展等变学习的应用范围。\n核心方法是Group Equivariant Graphormer，一个专为分布式群聚任务设计的群模块架构。\n实验在对称性破坏的四旋翼无人机群上验证了该框架的有效性，显著降低了碰撞率，提高了任务成功率，并在不同场景和群大小上实现了更好的泛化和零样本可扩展性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "math.RT"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01136v2",
      "published_date": "2025-01-02 08:41:31 UTC",
      "updated_date": "2025-04-25 09:39:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:56:56.894059"
    },
    {
      "arxiv_id": "2501.01132v1",
      "title": "Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach",
      "title_zh": "地球观测领域中缺失数据作为增强：一种多视图学习方法",
      "authors": [
        "Francisco Mena",
        "Diego Arenas",
        "Andreas Dengel"
      ],
      "abstract": "Multi-view learning (MVL) leverages multiple sources or views of data to\nenhance machine learning model performance and robustness. This approach has\nbeen successfully used in the Earth Observation (EO) domain, where views have a\nheterogeneous nature and can be affected by missing data. Despite the negative\neffect that missing data has on model predictions, the ML literature has used\nit as an augmentation technique to improve model generalization, like masking\nthe input data. Inspired by this, we introduce novel methods for EO\napplications tailored to MVL with missing views. Our methods integrate the\ncombination of a set to simulate all combinations of missing views as different\ntraining samples. Instead of replacing missing data with a numerical value, we\nuse dynamic merge functions, like average, and more complex ones like\nTransformer. This allows the MVL model to entirely ignore the missing views,\nenhancing its predictive robustness. We experiment on four EO datasets with\ntemporal and static views, including state-of-the-art methods from the EO\ndomain. The results indicate that our methods improve model robustness under\nconditions of moderate missingness, and improve the predictive performance when\nall views are present. The proposed methods offer a single adaptive solution to\noperate effectively with any combination of available views.",
      "tldr_zh": "该研究提出了一种将缺失数据作为增强技术的方法，应用于地球观测(Earth Observation, EO)领域的多视图学习(Multi-View Learning, MVL)，旨在通过模拟各种缺失视图组合来提升模型的鲁棒性和泛化能力。方法包括使用动态合并函数（如平均或Transformer）来处理缺失数据，而非简单替换数值，从而让模型完全忽略缺失视图并适应任意可用视图组合。实验结果显示，在四个EO数据集上，该方法在中等缺失条件下显著提高了模型预测性能，并在所有视图完整时进一步提升了准确性，提供了一个灵活的单一适应性解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01132v1",
      "published_date": "2025-01-02 08:17:27 UTC",
      "updated_date": "2025-01-02 08:17:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:57:10.837559"
    },
    {
      "arxiv_id": "2501.04718v1",
      "title": "Knowledge-Guided Biomarker Identification for Label-Free Single-Cell RNA-Seq Data: A Reinforcement Learning Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Meng Xiao",
        "Weiliang Zhang",
        "Xiaohan Huang",
        "Hengshu Zhu",
        "Min Wu",
        "Xiaoli Li",
        "Yuanchun Zhou"
      ],
      "abstract": "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
      "tldr_zh": "本文提出了一种知识引导的基因面板选择策略，用于在无标签的单细胞 RNA-Seq 数据中识别信息丰富的生物标记物（biomarker）。该方法首先利用现有基因选择算法的集合知识（ensemble knowledge）建立初步边界作为先验知识，指导初始搜索空间，然后整合强化学习（Reinforcement Learning）通过基于专家行为的奖励函数进行动态精炼和优化，以减少偏差并提升适应性。实验结果显示，该策略在全面比较实验、案例研究和下游分析中显著提高了生物标记物发现的精确性和效率，具有推进单细胞基因组数据分析的潜力。",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2406.07418",
      "pdf_url": "http://arxiv.org/pdf/2501.04718v1",
      "published_date": "2025-01-02 07:57:41 UTC",
      "updated_date": "2025-01-02 07:57:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:57:26.985997"
    },
    {
      "arxiv_id": "2501.01123v1",
      "title": "TED: Turn Emphasis with Dialogue Feature Attention for Emotion Recognition in Conversation",
      "title_zh": "翻译失败",
      "authors": [
        "Junya Ono",
        "Hiromi Wakaki"
      ],
      "abstract": "Emotion recognition in conversation (ERC) has been attracting attention by\nmethods for modeling multi-turn contexts. The multi-turn input to a pretraining\nmodel implicitly assumes that the current turn and other turns are\ndistinguished during the training process by inserting special tokens into the\ninput sequence. This paper proposes a priority-based attention method to\ndistinguish each turn explicitly by adding dialogue features into the attention\nmechanism, called Turn Emphasis with Dialogue (TED). It has a priority for each\nturn according to turn position and speaker information as dialogue features.\nIt takes multi-head self-attention between turn-based vectors for multi-turn\ninput and adjusts attention scores with the dialogue features. We evaluate TED\non four typical benchmarks. The experimental results demonstrate that TED has\nhigh overall performance in all datasets and achieves state-of-the-art\nperformance on IEMOCAP with numerous turns.",
      "tldr_zh": "这篇论文针对对话中的情感识别（ERC），提出了一种名为 TED 的方法，通过在注意力机制中添加对话特征（如轮次位置和说话者信息）来显式强调每个轮次，从而更好地处理多轮上下文。TED 方法赋予每个轮次优先级，并调整多头自注意力分数，以提高模型对多轮输入的区分能力。实验结果显示，该方法在四个典型基准数据集上表现出色，并在 IEMOCAP 数据集上达到了最先进性能，尤其适用于包含众多轮次的对话场景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "past activity in 2021",
      "pdf_url": "http://arxiv.org/pdf/2501.01123v1",
      "published_date": "2025-01-02 07:44:48 UTC",
      "updated_date": "2025-01-02 07:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:58:32.674305"
    },
    {
      "arxiv_id": "2501.01120v1",
      "title": "Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Lang",
        "Zhangtao Cheng",
        "Ting Zhong",
        "Fan Zhou"
      ],
      "abstract": "Multimodal learning with incomplete modality is practical and challenging.\nRecently, researchers have focused on enhancing the robustness of pre-trained\nMultiModal Transformers (MMTs) under missing modality conditions by applying\nlearnable prompts. However, these prompt-based methods face several\nlimitations: (1) incomplete modalities provide restricted modal cues for\ntask-specific inference, (2) dummy imputation for missing content causes\ninformation loss and introduces noise, and (3) static prompts are\ninstance-agnostic, offering limited knowledge for instances with various\nmissing conditions. To address these issues, we propose RAGPT, a novel\nRetrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three\nmodules: (I) the multi-channel retriever, which identifies similar instances\nthrough a within-modality retrieval strategy, (II) the missing modality\ngenerator, which recovers missing information using retrieved contexts, and\n(III) the context-aware prompter, which captures contextual knowledge from\nrelevant instances and generates dynamic prompts to largely enhance the MMT's\nrobustness. Extensive experiments conducted on three real-world datasets show\nthat RAGPT consistently outperforms all competitive baselines in handling\nincomplete modality problems. The code of our work and prompt-based baselines\nis available at https://github.com/Jian-Lang/RAGPT.",
      "tldr_zh": "该研究针对多模态学习中模态不完整的问题，提出了一种Retrieval-Augmented动态提示调整框架RAGPT，以提升预训练MultiModal Transformers (MMTs)的鲁棒性。RAGPT包括三个关键模块：多通道检索器通过模态内策略识别相似实例、缺失模态生成器利用检索上下文恢复缺失信息，以及上下文感知提示器从相关实例中捕获知识并生成动态提示。实验结果显示，在三个真实世界数据集上，RAGPT在处理不完整模态问题时 consistently outperforms 竞争基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 8 figures. Accepted by AAAI 2025. Codes are released at\n  https://github.com/Jian-Lang/RAGPT",
      "pdf_url": "http://arxiv.org/pdf/2501.01120v1",
      "published_date": "2025-01-02 07:39:48 UTC",
      "updated_date": "2025-01-02 07:39:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:57:50.637977"
    },
    {
      "arxiv_id": "2501.01118v1",
      "title": "Pruning-based Data Selection and Network Fusion for Efficient Deep Learning",
      "title_zh": "基于剪枝的数据选择和网络融合用于高效深度学习",
      "authors": [
        "Humaira Kousar",
        "Hasnain Irshad Bhatti",
        "Jaekyun Moon"
      ],
      "abstract": "Efficient data selection is essential for improving the training efficiency\nof deep neural networks and reducing the associated annotation costs. However,\ntraditional methods tend to be computationally expensive, limiting their\nscalability and real-world applicability. We introduce PruneFuse, a novel\nmethod that combines pruning and network fusion to enhance data selection and\naccelerate network training. In PruneFuse, the original dense network is pruned\nto generate a smaller surrogate model that efficiently selects the most\ninformative samples from the dataset. Once this iterative data selection\nselects sufficient samples, the insights learned from the pruned model are\nseamlessly integrated with the dense model through network fusion, providing an\noptimized initialization that accelerates training. Extensive experimentation\non various datasets demonstrates that PruneFuse significantly reduces\ncomputational costs for data selection, achieves better performance than\nbaselines, and accelerates the overall training process.",
      "tldr_zh": "该研究提出了一种名为 PruneFuse 的新方法，通过结合 pruning（网络修剪）和 network fusion（网络融合），来优化深度学习的训练效率并减少标注成本。具体而言，PruneFuse 先修剪原始密集网络生成一个更小的代理模型，用于高效选择数据集中最具信息量的样本，然后将这些见解无缝整合回密集模型，提供优化的初始化以加速训练。在各种数据集上的广泛实验表明，该方法显著降低了数据选择的计算开销，比基线方法性能更优，并整体加快了训练过程。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Workshop on Attributing Model Behavior at Scale\n  (ATTRIB)",
      "pdf_url": "http://arxiv.org/pdf/2501.01118v1",
      "published_date": "2025-01-02 07:35:53 UTC",
      "updated_date": "2025-01-02 07:35:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:58:03.106813"
    },
    {
      "arxiv_id": "2501.01117v1",
      "title": "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision Tree and Forest: A Comprehensive Cross-Datasets Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Rofiqul Islam",
        "Nihad Karim Chowdhury",
        "Muhammad Ashad Kabir"
      ],
      "abstract": "This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals.",
      "tldr_zh": "这篇论文提出了一种鲁棒的 COVID-19 检测方法，使用 Deep Neural Decision Tree 和 Deep Neural Decision Forest 模型，从咳嗽声音中分类阳性病例，并进行了全面的跨数据集评估。方法包括特征提取、Recursive Feature Elimination 与交叉验证、Bayesian 优化超参数、SMOTE 平衡数据，以及阈值优化以最大化 ROC-AUC 得分。在 Cambridge、Coswara、COUGHVID、Virufy 和组合数据集上，模型分别取得了 0.97、0.98、0.92、0.93、0.99 和 0.99 的 AUC 得分，合并所有数据集时达到 0.97 的 AUC。研究还通过跨数据集分析揭示了 COVID-19 咳嗽声音在人口统计和地理上的差异，强调了数据集集成的益处，以提升模型的泛化性和检测准确性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "39 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01117v1",
      "published_date": "2025-01-02 07:35:06 UTC",
      "updated_date": "2025-01-02 07:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:58:16.661022"
    },
    {
      "arxiv_id": "2501.01110v1",
      "title": "MalCL: Leveraging GAN-Based Generative Replay to Combat Catastrophic Forgetting in Malware Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jimin Park",
        "AHyun Ji",
        "Minji Park",
        "Mohammad Saidur Rahman",
        "Se Eun Oh"
      ],
      "abstract": "Continual Learning (CL) for malware classification tackles the rapidly\nevolving nature of malware threats and the frequent emergence of new types.\nGenerative Replay (GR)-based CL systems utilize a generative model to produce\nsynthetic versions of past data, which are then combined with new data to\nretrain the primary model. Traditional machine learning techniques in this\ndomain often struggle with catastrophic forgetting, where a model's performance\non old data degrades over time.\n  In this paper, we introduce a GR-based CL system that employs Generative\nAdversarial Networks (GANs) with feature matching loss to generate high-quality\nmalware samples. Additionally, we implement innovative selection schemes for\nreplay samples based on the model's hidden representations.\n  Our comprehensive evaluation across Windows and Android malware datasets in a\nclass-incremental learning scenario -- where new classes are introduced\ncontinuously over multiple tasks -- demonstrates substantial performance\nimprovements over previous methods. For example, our system achieves an average\naccuracy of 55% on Windows malware samples, significantly outperforming other\nGR-based models by 28%. This study provides practical insights for advancing\nGR-based malware classification systems. The implementation is available at\n\\url {https://github.com/MalwareReplayGAN/MalCL}\\footnote{The code will be made\npublic upon the presentation of the paper}.",
      "tldr_zh": "本文提出 MalCL，一种基于 Generative Adversarial Networks (GANs) 的 Generative Replay (GR) 系统，用于解决恶意软件分类中的 catastrophic forgetting 问题，通过 GANs 结合 feature matching loss 生成高质量样本，并引入基于模型隐藏表示的创新样本选择方案。实验在 Windows 和 Android 恶意软件数据集上的类增量学习场景中评估，MalCL 实现了55%的平均准确率，比其他 GR 方法高出28%。这项研究为 Continual Learning (CL) 领域的恶意软件分类提供了实用见解，并开源了实现代码。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted paper at AAAI 2025. 9 pages, Figure 6, Table 1",
      "pdf_url": "http://arxiv.org/pdf/2501.01110v1",
      "published_date": "2025-01-02 07:15:31 UTC",
      "updated_date": "2025-01-02 07:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:59:14.544151"
    },
    {
      "arxiv_id": "2501.01109v1",
      "title": "BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiusheng Xu",
        "Lei Qi",
        "Jingyang Zhou",
        "Xin Geng"
      ],
      "abstract": "Source-Free Domain Generalization (SFDG) aims to develop a model that\nperforms on unseen domains without relying on any source domains. However, the\nimplementation remains constrained due to the unavailability of training data.\nResearch on SFDG focus on knowledge transfer of multi-modal models and style\nsynthesis based on joint space of multiple modalities, thus eliminating the\ndependency on source domain images. However, existing works primarily work for\nmulti-domain and less-category configuration, but performance on multi-domain\nand multi-category configuration is relatively poor. In addition, the\nefficiency of style synthesis also deteriorates in multi-category scenarios.\nHow to efficiently synthesize sufficiently diverse data and apply it to\nmulti-category configuration is a direction with greater practical value. In\nthis paper, we propose a method called BatStyler, which is utilized to improve\nthe capability of style synthesis in multi-category scenarios. BatStyler\nconsists of two modules: Coarse Semantic Generation and Uniform Style\nGeneration modules. The Coarse Semantic Generation module extracts\ncoarse-grained semantics to prevent the compression of space for style\ndiversity learning in multi-category configuration, while the Uniform Style\nGeneration module provides a template of styles that are uniformly distributed\nin space and implements parallel training. Extensive experiments demonstrate\nthat our method exhibits comparable performance on less-category datasets,\nwhile surpassing state-of-the-art methods on multi-category datasets.",
      "tldr_zh": "这篇论文针对 Source-Free Domain Generalization (SFDG) 的挑战，提出 BatStyler 方法，以提升多类别场景下的风格合成效率和多样性。BatStyler 包括两个关键模块：Coarse Semantic Generation 用于提取粗粒度语义，防止多类别配置中风格多样性学习空间被压缩，以及 Uniform Style Generation 用于生成均匀分布的风格模板并实现并行训练。该方法在实验中显示出在多类别数据集上超越现有最先进方法的性能，而在少类别数据集上保持相当水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE TCSVT",
      "pdf_url": "http://arxiv.org/pdf/2501.01109v1",
      "published_date": "2025-01-02 07:14:23 UTC",
      "updated_date": "2025-01-02 07:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:58:45.989751"
    },
    {
      "arxiv_id": "2501.01108v2",
      "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Haina Zhu",
        "Yizhi Zhou",
        "Hangting Chen",
        "Jianwei Yu",
        "Ziyang Ma",
        "Rongzhi Gu",
        "Yi Luo",
        "Wei Tan",
        "Xie Chen"
      ],
      "abstract": "Recent years have witnessed the success of foundation models pre-trained with\nself-supervised learning (SSL) in various music informatics understanding\ntasks, including music tagging, instrument classification, key detection, and\nmore. In this paper, we propose a self-supervised music representation learning\nmodel for music understanding. Distinguished from previous studies adopting\nrandom projection or existing neural codec, the proposed model, named MuQ, is\ntrained to predict tokens generated by Mel Residual Vector Quantization\n(Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel\nspectrum quantization to enhance the stability and efficiency of target\nextraction and lead to better performance. Experiments in a large variety of\ndownstream tasks demonstrate that MuQ outperforms previous self-supervised\nmusic representation models with only 0.9K hours of open-source pre-training\ndata. Scaling up the data to over 160K hours and adopting iterative training\nconsistently improve the model performance. To further validate the strength of\nour model, we present MuQ-MuLan, a joint music-text embedding model based on\ncontrastive learning, which achieves state-of-the-art performance in the\nzero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints\nare open source in https://github.com/tencent-ailab/MuQ.",
      "tldr_zh": "本论文提出MuQ，一种基于自监督学习(Self-Supervised Learning, SSL)的音乐表示学习模型，使用Mel Residual Vector Quantization (Mel-RVQ)生成tokens，并训练模型预测这些tokens，以提升音乐理解任务的性能。Mel-RVQ通过residual linear projection结构提高了Mel谱量化的稳定性和效率，与现有方法相比，MuQ仅使用0.9K小时开源数据便在音乐标签、乐器分类等下游任务中表现出色，并通过扩展到160K小时数据和迭代训练进一步提升效果。作者还开发了MuQ-MuLan，一种基于对比学习的音乐-文本嵌入模型，在MagnaTagATune数据集的零样本音乐标签任务中达到了state-of-the-art性能，并开源了代码和检查点。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01108v2",
      "published_date": "2025-01-02 07:08:29 UTC",
      "updated_date": "2025-01-03 08:35:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:58:57.526661"
    },
    {
      "arxiv_id": "2501.01103v1",
      "title": "learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "title_zh": "使用中心损失从频谱图中学习判别性特征用于语音情感识别",
      "authors": [
        "Dongyang Dai",
        "Zhiyong Wu",
        "Runnan Li",
        "Xixin Wu",
        "Jia Jia",
        "Helen Meng"
      ],
      "abstract": "Identifying the emotional state from speech is essential for the natural\ninteraction of the machine with the speaker. However, extracting effective\nfeatures for emotion recognition is difficult, as emotions are ambiguous. We\npropose a novel approach to learn discriminative features from variable length\nspectrograms for emotion recognition by cooperating softmax cross-entropy loss\nand center loss together. The softmax cross-entropy loss enables features from\ndifferent emotion categories separable, and center loss efficiently pulls the\nfeatures belonging to the same emotion category to their center. By combining\nthe two losses together, the discriminative power will be highly enhanced,\nwhich leads to network learning more effective features for emotion\nrecognition. As demonstrated by the experimental results, after introducing\ncenter loss, both the unweighted accuracy and weighted accuracy are improved by\nover 3\\% on Mel-spectrogram input, and more than 4\\% on Short Time Fourier\nTransform spectrogram input.",
      "tldr_zh": "本研究针对语音情感识别中特征提取的模糊性问题，提出了一种结合softmax cross-entropy loss和center loss的方法，从可变长度的频谱图中学习区分性特征。softmax cross-entropy loss使不同情感类别的特征可分离，而center loss则将同一情感类别的特征拉向其中心，从而显著增强了特征的区分能力。实验结果显示，该方法在Mel-spectrogram输入上，无加权准确率和加权准确率均提高了超过3%；在Short Time Fourier Transform spectrogram输入上，提高了超过4%。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at ICASSP 2019",
      "pdf_url": "http://arxiv.org/pdf/2501.01103v1",
      "published_date": "2025-01-02 06:52:28 UTC",
      "updated_date": "2025-01-02 06:52:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:59:38.620559"
    },
    {
      "arxiv_id": "2501.01102v1",
      "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-trained BERT",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyang Dai",
        "Zhiyong Wu",
        "Shiyin Kang",
        "Xixin Wu",
        "Jia Jia",
        "Dan Su",
        "Dong Yu",
        "Helen Meng"
      ],
      "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in\nChinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is\nthe core issue. In this paper, we propose an end-to-end framework to predict\nthe pronunciation of a polyphonic character, which accepts sentence containing\npolyphonic character as input in the form of Chinese character sequence without\nthe necessity of any preprocessing. The proposed method consists of a\npre-trained bidirectional encoder representations from Transformers (BERT)\nmodel and a neural network (NN) based classifier. The pre-trained BERT model\nextracts semantic features from a raw Chinese character sequence and the NN\nbased classifier predicts the polyphonic character's pronunciation according to\nBERT output. In out experiments, we implemented three classifiers, a\nfully-connected network based classifier, a long short-term memory (LSTM)\nnetwork based classifier and a Transformer block based classifier. The\nexperimental results compared with the baseline approach based on LSTM\ndemonstrate that, the pre-trained model extracts effective semantic features,\nwhich greatly enhances the performance of polyphone disambiguation. In\naddition, we also explored the impact of contextual information on polyphone\ndisambiguation.",
      "tldr_zh": "这篇论文提出了一种端到端框架，用于解决中文多音字（polyphones）的消歧问题，作为G2P转换在TTS系统中的核心组件。框架以原始中文字符序列为输入，利用预训练的BERT模型提取语义特征，然后通过神经网络分类器（如全连接网络、LSTM或Transformer块）预测多音字的发音。实验结果显示，与基于LSTM的基线方法相比，该框架显著提升了性能，并证明了语义特征和上下文信息对多音字消歧的重要作用。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at INTERSPEECH 2019",
      "pdf_url": "http://arxiv.org/pdf/2501.01102v1",
      "published_date": "2025-01-02 06:51:52 UTC",
      "updated_date": "2025-01-02 06:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:59:28.114104"
    },
    {
      "arxiv_id": "2501.01094v1",
      "title": "MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and Musical Captions",
      "title_zh": "翻译失败",
      "authors": [
        "Suhwan Choi",
        "Kyu Won Kim",
        "Myungjoo Kang"
      ],
      "abstract": "We introduce Multimodal Matching based on Valence and Arousal (MMVA), a\ntri-modal encoder framework designed to capture emotional content across\nimages, music, and musical captions. To support this framework, we expand the\nImage-Music-Emotion-Matching-Net (IMEMNet) dataset, creating IMEMNet-C which\nincludes 24,756 images and 25,944 music clips with corresponding musical\ncaptions. We employ multimodal matching scores based on the continuous valence\n(emotional positivity) and arousal (emotional intensity) values. This\ncontinuous matching score allows for random sampling of image-music pairs\nduring training by computing similarity scores from the valence-arousal values\nacross different modalities. Consequently, the proposed approach achieves\nstate-of-the-art performance in valence-arousal prediction tasks. Furthermore,\nthe framework demonstrates its efficacy in various zeroshot tasks, highlighting\nthe potential of valence and arousal predictions in downstream applications.",
      "tldr_zh": "本研究引入了 MMVA 框架，这是一个基于 Valence（情感积极性）和 Arousal（情感强度）的三模态编码器，用于捕捉图像、音乐和音乐标题中的情感内容。研究者扩展了 IMEMNet 数据集，创建了 IMEMNet-C 版本，包括 24,756 张图像和 25,944 段音乐剪辑及其标题，并通过连续匹配分数和随机采样图像-音乐对来训练模型。结果显示，MMVA 在 Valence-Arousal 预测任务中达到了 state-of-the-art 性能，并在各种零样本任务中表现出色，展示了其在下游情感应用中的潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Paper accepted in Artificial Intelligence for Music workshop at AAAI\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01094v1",
      "published_date": "2025-01-02 06:36:09 UTC",
      "updated_date": "2025-01-02 06:36:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:33.419928"
    },
    {
      "arxiv_id": "2501.01073v1",
      "title": "Graph Generative Pre-trained Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohui Chen",
        "Yinkai Wang",
        "Jiaxing He",
        "Yuanqi Du",
        "Soha Hassoun",
        "Xiaolin Xu",
        "Li-Ping Liu"
      ],
      "abstract": "Graph generation is a critical task in numerous domains, including molecular\ndesign and social network analysis, due to its ability to model complex\nrelationships and structured data. While most modern graph generative models\nutilize adjacency matrix representations, this work revisits an alternative\napproach that represents graphs as sequences of node set and edge set. We\nadvocate for this approach due to its efficient encoding of graphs and propose\na novel representation. Based on this representation, we introduce the Graph\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\ngraph structures via next-token prediction. To further exploit G2PT's\ncapabilities as a general-purpose foundation model, we explore fine-tuning\nstrategies for two downstream applications: goal-oriented generation and graph\nproperty prediction. We conduct extensive experiments across multiple datasets.\nResults indicate that G2PT achieves superior generative performance on both\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\nadaptability and versatility in downstream tasks from molecular design to\nproperty prediction.",
      "tldr_zh": "本文提出 Graph Generative Pre-trained Transformer (G2PT)，一种基于节点集和边集序列表示的图生成模型，以高效编码图结构并解决传统邻接矩阵方法的局限性。G2PT 采用自回归机制，通过 next-token prediction 学习图结构，并探索微调策略应用于下游任务，如 goal-oriented generation 和 graph property prediction。实验结果显示，G2PT 在通用图和分子数据集上实现了优越的生成性能，并在分子设计等领域展现出强大的适应性和多功能性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2501.01073v1",
      "published_date": "2025-01-02 05:44:11 UTC",
      "updated_date": "2025-01-02 05:44:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:00:03.267957"
    },
    {
      "arxiv_id": "2501.02007v1",
      "title": "TART: Token-based Architecture Transformer for Neural Network Performance Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yannis Y. He"
      ],
      "abstract": "In the realm of neural architecture design, achieving high performance is\nlargely reliant on the manual expertise of researchers. Despite the emergence\nof Neural Architecture Search (NAS) as a promising technique for automating\nthis process, current NAS methods still require human input to expand the\nsearch space and cannot generate new architectures. This paper explores the\npotential of Transformers in comprehending neural architectures and their\nperformance, with the objective of establishing the foundation for utilizing\nTransformers to generate novel networks. We propose the Token-based\nArchitecture Transformer (TART), which predicts neural network performance\nwithout the need to train candidate networks. TART attains state-of-the-art\nperformance on the DeepNets-1M dataset for performance prediction tasks without\nedge information, indicating the potential of Transformers to aid in\ndiscovering novel and high-performing neural architectures.",
      "tldr_zh": "本论文探讨了神经架构设计依赖人工专家的问题，并指出现有 Neural Architecture Search (NAS) 方法虽能自动化部分过程，但仍需人工干预且无法生成新架构。为解决此问题，研究提出 TART（Token-based Architecture Transformer），一种基于 Transformers 的框架，能够通过 token 表示预测神经网络性能，而无需训练候选网络。实验结果显示，TART 在 DeepNets-1M 数据集上实现了 state-of-the-art 性能，证明了其在发现新颖、高性能神经架构方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02007v1",
      "published_date": "2025-01-02 05:22:17 UTC",
      "updated_date": "2025-01-02 05:22:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:00:15.268002"
    },
    {
      "arxiv_id": "2501.01477v1",
      "title": "A Survey of Deep Learning Methods in Protein Bioinformatics and its Impact on Protein Design",
      "title_zh": "蛋白质生物信息学中深度学习方法的综述及其对蛋白质设计的影响",
      "authors": [
        "Weihang Dai"
      ],
      "abstract": "Proteins are sequences of amino acids that serve as the basic building blocks\nof living organisms. Despite rapidly growing databases documenting structural\nand functional information for various protein sequences, our understanding of\nproteins remains limited because of the large possible sequence space and the\ncomplex inter- and intra-molecular forces. Deep learning, which is\ncharacterized by its ability to learn relevant features directly from large\ndatasets, has demonstrated remarkable performance in fields such as computer\nvision and natural language processing. It has also been increasingly applied\nin recent years to the data-rich domain of protein sequences with great\nsuccess, most notably with Alphafold2's breakout performance in the protein\nstructure prediction. The performance improvements achieved by deep learning\nunlocks new possibilities in the field of protein bioinformatics, including\nprotein design, one of the most difficult but useful tasks. In this paper, we\nbroadly categorize problems in protein bioinformatics into three main\ncategories: 1) structural prediction, 2) functional prediction, and 3) protein\ndesign, and review the progress achieved from using deep learning methodologies\nin each of them. We expand on the main challenges of the protein design problem\nand highlight how advances in structural and functional prediction have\ndirectly contributed to design tasks. Finally, we conclude by identifying\nimportant topics and future research directions.",
      "tldr_zh": "这篇综述论文探讨了深度学习在蛋白生物信息学（Protein Bioinformatics）中的应用及其对蛋白设计（Protein Design）的影响。论文将蛋白相关问题分为三大类：1) 结构预测（如AlphaFold2的突破性进展），2) 功能预测，以及3) 蛋白设计，并回顾了深度学习方法在这些领域的进展。作者强调，深度学习通过从大量数据中学习特征，解决了蛋白序列空间庞大和分子力复杂的挑战，从而为蛋白设计任务提供了新可能性。最终，论文指出了未来研究方向，包括如何进一步整合结构和功能预测来推进蛋白设计。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "PhD Qualifying Exam (2021)",
      "pdf_url": "http://arxiv.org/pdf/2501.01477v1",
      "published_date": "2025-01-02 05:21:34 UTC",
      "updated_date": "2025-01-02 05:21:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:34.839637"
    },
    {
      "arxiv_id": "2501.01056v1",
      "title": "Risks of Cultural Erasure in Large Language Models",
      "title_zh": "大语言模型中的文化抹除风险",
      "authors": [
        "Rida Qadri",
        "Aida M. Davani",
        "Kevin Robinson",
        "Vinodkumar Prabhakaran"
      ],
      "abstract": "Large language models are increasingly being integrated into applications\nthat shape the production and discovery of societal knowledge such as search,\nonline education, and travel planning. As a result, language models will shape\nhow people learn about, perceive and interact with global cultures making it\nimportant to consider whose knowledge systems and perspectives are represented\nin models. Recognizing this importance, increasingly work in Machine Learning\nand NLP has focused on evaluating gaps in global cultural representational\ndistribution within outputs. However, more work is needed on developing\nbenchmarks for cross-cultural impacts of language models that stem from a\nnuanced sociologically-aware conceptualization of cultural impact or harm. We\njoin this line of work arguing for the need of metricizable evaluations of\nlanguage technologies that interrogate and account for historical power\ninequities and differential impacts of representation on global cultures,\nparticularly for cultures already under-represented in the digital corpora. We\nlook at two concepts of erasure: omission: where cultures are not represented\nat all and simplification i.e. when cultural complexity is erased by presenting\none-dimensional views of a rich culture. The former focuses on whether\nsomething is represented, and the latter on how it is represented. We focus our\nanalysis on two task contexts with the potential to influence global cultural\nproduction. First, we probe representations that a language model produces\nabout different places around the world when asked to describe these contexts.\nSecond, we analyze the cultures represented in the travel recommendations\nproduced by a set of language model applications. Our study shows ways in which\nthe NLP community and application developers can begin to operationalize\ncomplex socio-cultural considerations into standard evaluations and benchmarks.",
      "tldr_zh": "这篇论文探讨了大型语言模型（Large Language Models）在应用如搜索、教育和旅行规划中可能导致文化消逝的风险，强调这些模型会影响全球文化的认知和互动，特别是对历史性弱势文化的代表性不足。作者引入两种文化消逝概念：omission（遗漏，即完全忽略某些文化）和simplification（简化，即以单维视角呈现复杂文化），并通过分析模型对全球地点的描述和旅行推荐来评估这些影响。研究发现，语言模型可能加剧历史权力不平等，并提出将复杂的社文化考虑转化为可衡量的基准，以指导NLP社区和开发者进行更负责任的评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01056v1",
      "published_date": "2025-01-02 04:57:50 UTC",
      "updated_date": "2025-01-02 04:57:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:00:38.563833"
    },
    {
      "arxiv_id": "2501.02006v1",
      "title": "Multi-Task Semantic Communication With Graph Attention-Based Feature Correlation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Yu",
        "Tiejun Lv",
        "Weicai Li",
        "Wei Ni",
        "Dusit Niyato",
        "Ekram Hossain"
      ],
      "abstract": "Multi-task semantic communication can serve multiple learning tasks using a\nshared encoder model. Existing models have overlooked the intricate\nrelationships between features extracted during an encoding process of tasks.\nThis paper presents a new graph attention inter-block (GAI) module to the\nencoder/transmitter of a multi-task semantic communication system, which\nenriches the features for multiple tasks by embedding the intermediate outputs\nof encoding in the features, compared to the existing techniques. The key idea\nis that we interpret the outputs of the intermediate feature extraction blocks\nof the encoder as the nodes of a graph to capture the correlations of the\nintermediate features. Another important aspect is that we refine the node\nrepresentation using a graph attention mechanism to extract the correlations\nand a multi-layer perceptron network to associate the node representations with\ndifferent tasks. Consequently, the intermediate features are weighted and\nembedded into the features transmitted for executing multiple tasks at the\nreceiver. Experiments demonstrate that the proposed model surpasses the most\ncompetitive and publicly available models by 11.4% on the CityScapes 2Task\ndataset and outperforms the established state-of-the-art by 3.97% on the NYU V2\n3Task dataset, respectively, when the bandwidth ratio of the communication\nchannel (i.e., compression level for transmission over the channel) is as\nconstrained as 1 12 .",
      "tldr_zh": "本文提出了一种基于Graph Attention的多任务语义通信系统，通过引入Graph Attention Inter-Block (GAI) 模块来捕捉编码过程中特征的相关性。GAI模块将编码器的中间输出视为图节点，利用图注意力机制提取相关性，并通过多层感知器 (MLP) 网络将这些节点表示与不同任务关联，从而加权嵌入到传输特征中。实验结果显示，该模型在CityScapes 2Task数据集上比最先进模型提升11.4%，在NYU V2 3Task数据集上提升3.97%，即使在带宽比为1:12的受限条件下也能保持优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages,11 figures, accepted by IEEE TMC",
      "pdf_url": "http://arxiv.org/pdf/2501.02006v1",
      "published_date": "2025-01-02 04:38:01 UTC",
      "updated_date": "2025-01-02 04:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:59.094924"
    },
    {
      "arxiv_id": "2501.01039v1",
      "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Yixing Xu",
        "Shivank Nag",
        "Dong Li",
        "Lu Tian",
        "Emad Barsoum"
      ],
      "abstract": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
      "tldr_zh": "该论文针对Transformer模型中标准自注意力机制的二次时间复杂性和缓存问题，提出Multi-Scale Window Attention (MSWA)作为改进Sliding Window Attention (SWA)的方案。MSWA在同一层中为不同注意力头分配多样化的窗口大小，并从浅层到深层逐步增加窗口大小，从而更有效地捕获不同长度和距离的上下文信息。实验结果表明，MSWA在语言建模和常识推理任务上优于传统局部注意力机制，在有效性和效率方面均有显著提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01039v1",
      "published_date": "2025-01-02 03:41:32 UTC",
      "updated_date": "2025-01-02 03:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:09.014065"
    },
    {
      "arxiv_id": "2501.01037v1",
      "title": "MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception",
      "title_zh": "MSC-Bench：驾驶感知多传感器损坏的基准测试与分析",
      "authors": [
        "Xiaoshuai Hao",
        "Guanqun Liu",
        "Yuting Zhao",
        "Yuheng Ji",
        "Mengchuan Wei",
        "Haimei Zhao",
        "Lingdong Kong",
        "Rong Yin",
        "Yu Liu"
      ],
      "abstract": "Multi-sensor fusion models play a crucial role in autonomous driving\nperception, particularly in tasks like 3D object detection and HD map\nconstruction. These models provide essential and comprehensive static\nenvironmental information for autonomous driving systems. While camera-LiDAR\nfusion methods have shown promising results by integrating data from both\nmodalities, they often depend on complete sensor inputs. This reliance can lead\nto low robustness and potential failures when sensors are corrupted or missing,\nraising significant safety concerns. To tackle this challenge, we introduce the\nMulti-Sensor Corruption Benchmark (MSC-Bench), the first comprehensive\nbenchmark aimed at evaluating the robustness of multi-sensor autonomous driving\nperception models against various sensor corruptions. Our benchmark includes 16\ncombinations of corruption types that disrupt both camera and LiDAR inputs,\neither individually or concurrently. Extensive evaluations of six 3D object\ndetection models and four HD map construction models reveal substantial\nperformance degradation under adverse weather conditions and sensor failures,\nunderscoring critical safety issues. The benchmark toolkit and affiliated code\nand model checkpoints have been made publicly accessible.",
      "tldr_zh": "本研究引入了MSC-Bench基准，用于评估多传感器自动驾驶感知模型（如3D object detection和HD map construction）对传感器损坏的鲁棒性，解决现有模型依赖完整输入导致的安全隐患问题。该基准涵盖16种相机和LiDAR输入损坏组合，包括单一或同时发生的情况。实验评估了六种3D object detection模型和四种HD map construction模型，结果显示在恶劣天气和传感器故障下，模型性能显著下降，强调了潜在的安全风险。MSC-Bench的工具包、代码和模型检查点已公开可用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01037v1",
      "published_date": "2025-01-02 03:38:46 UTC",
      "updated_date": "2025-01-02 03:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:22.530993"
    },
    {
      "arxiv_id": "2501.01031v3",
      "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wonduk Seo",
        "Zonghao Yuan",
        "Yi Bu"
      ],
      "abstract": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values.",
      "tldr_zh": "本文研究了大型语言模型 (LLMs) 中文化价值观对齐的问题，指出现有方法如角色分配和少样本学习 (few-shot learning) 因依赖预训练知识而无法有效处理西方中心偏见和文化细微差异。ValuesRAG 框架通过 Retrieval-Augmented Generation (RAG) 与 In-Context Learning (ICL) 动态整合文化知识，利用 World Values Survey (WVS) 数据集生成个人价值观摘要，并基于人口统计特征进行检索和 reranking 以选择相关信息。该框架在 6 个多样化区域数据集上的实验中，显著优于零样本、角色分配和混合基线方法，证明其在提升文化对齐和包容性 AI 系统方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2501.01031v3",
      "published_date": "2025-01-02 03:26:13 UTC",
      "updated_date": "2025-05-08 01:07:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:40.912593"
    },
    {
      "arxiv_id": "2501.01030v2",
      "title": "Reasoning based on symbolic and parametric knowledge bases: a survey",
      "title_zh": "翻译失败",
      "authors": [
        "Mayi Xu",
        "Yunfeng Ning",
        "Yongqi Li",
        "Jianhao Chen",
        "Jintao Wen",
        "Yao Xiao",
        "Shen Zhou",
        "Birong Pan",
        "Zepeng Bao",
        "Xin Miao",
        "Hankun Kang",
        "Ke Sun",
        "Tieyun Qian"
      ],
      "abstract": "Reasoning is fundamental to human intelligence, and critical for\nproblem-solving, decision-making, and critical thinking. Reasoning refers to\ndrawing new conclusions based on existing knowledge, which can support various\napplications like clinical diagnosis, basic education, and financial analysis.\nThough a good number of surveys have been proposed for reviewing\nreasoning-related methods, none of them has systematically investigated these\nmethods from the viewpoint of their dependent knowledge base. Both the\nscenarios to which the knowledge bases are applied and their storage formats\nare significantly different. Hence, investigating reasoning methods from the\nknowledge base perspective helps us better understand the challenges and future\ndirections. To fill this gap, this paper first classifies the knowledge base\ninto symbolic and parametric ones. The former explicitly stores information in\nhuman-readable symbols, and the latter implicitly encodes knowledge within\nparameters. Then, we provide a comprehensive overview of reasoning methods\nusing symbolic knowledge bases, parametric knowledge bases, and both of them.\nFinally, we identify the future direction toward enhancing reasoning\ncapabilities to bridge the gap between human and machine intelligence.",
      "tldr_zh": "这篇调查论文从知识库的视角系统审视了推理方法，将知识库分为符号(symbolic)知识库（显式存储人类可读符号）和参数(parametric)知识库（隐式编码于参数中），并概述了基于这些知识库的推理技术及其应用场景，如临床诊断和金融分析。论文强调了现有调查的不足，并提供了对使用单一或混合知识库的推理方法的全面总结。最终，它指出了未来方向，以提升推理能力，桥接人类和机器智能的差距。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "There are imperfections in some parts of the paper, which may lead to\n  misunderstandings among readers. To be rigorous, we apply for the withdrawal\n  of this paper.",
      "pdf_url": "http://arxiv.org/pdf/2501.01030v2",
      "published_date": "2025-01-02 03:21:32 UTC",
      "updated_date": "2025-02-21 16:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:01:50.679228"
    },
    {
      "arxiv_id": "2501.01025v2",
      "title": "Towards Adversarially Robust Deep Metric Learning",
      "title_zh": "面向对抗鲁棒的深度度量学习",
      "authors": [
        "Xiaopeng Ke"
      ],
      "abstract": "Deep Metric Learning (DML) has shown remarkable successes in many domains by\ntaking advantage of powerful deep neural networks. Deep neural networks are\nprone to adversarial attacks and could be easily fooled by adversarial\nexamples. The current progress on this robustness issue is mainly about deep\nclassification models but pays little attention to DML models. Existing works\nfail to thoroughly inspect the robustness of DML and neglect an important DML\nscenario, the clustering-based inference. In this work, we first point out the\nrobustness issue of DML models in clustering-based inference scenarios. We find\nthat, for the clustering-based inference, existing defenses designed DML are\nunable to be reused and the adaptions of defenses designed for deep\nclassification models cannot achieve satisfactory robustness performance. To\nalleviate the hazard of adversarial examples, we propose a new defense, the\nEnsemble Adversarial Training (EAT), which exploits ensemble learning and\nadversarial training. EAT promotes the diversity of the ensemble, encouraging\neach model in the ensemble to have different robustness features, and employs a\nself-transferring mechanism to make full use of the robustness statistics of\nthe whole ensemble in the update of every single model. We evaluate the EAT\nmethod on three widely-used datasets with two popular model architectures. The\nresults show that the proposed EAT method greatly outperforms the adaptions of\ndefenses designed for deep classification models.",
      "tldr_zh": "本研究针对Deep Metric Learning (DML)模型在对抗性攻击下的鲁棒性问题进行探讨，发现现有防御方法无法有效应用于DML，尤其是基于聚类的推理场景。作者提出了一种新防御机制Ensemble Adversarial Training (EAT)，结合集成学习和对抗训练，通过促进模型多样性和自转移机制，利用整个集成中的鲁棒性统计来提升每个模型的性能。在三个常用数据集和两种流行模型架构上的实验结果显示，EAT显著优于针对深度分类模型的防御适应方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01025v2",
      "published_date": "2025-01-02 03:15:25 UTC",
      "updated_date": "2025-01-12 04:43:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:02:10.790604"
    },
    {
      "arxiv_id": "2501.01014v1",
      "title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model",
      "title_zh": "MDSF：基于大型语言模型的上下文感知多维数据叙事框架",
      "authors": [
        "Chengze Zhang",
        "Changshan Li",
        "Shiyang Gao"
      ],
      "abstract": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（Large Language Models, LLMs）的多维数据故事化框架（MDSF），旨在通过自动化洞见生成和上下文感知叙述来提升数据分析效率。框架整合了高级预处理技术、增强分析算法以及独特的评分机制，以识别和优先处理可操作洞见，并利用微调的LLMs减少手动干预，同时引入基于代理的机制实现实时故事延续控制。实验结果显示，MDSF在多个数据集上优于现有方法，在洞见排名准确性、描述质量和叙述连贯性方面表现出色，能够自动化复杂分析任务、降低解释偏差并提高用户满意度。用户研究进一步证实了其在内容结构优化、结论提取和细节丰富方面的实用价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01014v1",
      "published_date": "2025-01-02 02:35:38 UTC",
      "updated_date": "2025-01-02 02:35:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:02:22.967915"
    },
    {
      "arxiv_id": "2501.01010v2",
      "title": "CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Shahab Sepehri",
        "Asal Mehradfar",
        "Mahdi Soltanolkotabi",
        "Salman Avestimehr"
      ],
      "abstract": "Predicting Bitcoin price remains a challenging problem due to the high\nvolatility and complex non-linear dynamics of cryptocurrency markets.\nTraditional time-series models, such as ARIMA and GARCH, and recurrent neural\nnetworks, like LSTMs, have been widely applied to this task but struggle to\ncapture the regime shifts and long-range dependencies inherent in the data. In\nthis work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM)\narchitecture designed to effectively capture long-range dependencies in\nfinancial time-series data. Our experiments show that CryptoMamba not only\nprovides more accurate predictions but also offers enhanced generalizability\nacross different market conditions, surpassing the limitations of previous\nmodels. Coupled with trading algorithms for real-world scenarios, CryptoMamba\ndemonstrates its practical utility by translating accurate forecasts into\nfinancial outcomes. Our findings signal a huge advantage for SSMs in stock and\ncryptocurrency price forecasting tasks.",
      "tldr_zh": "比特币价格预测因市场的高波动性和复杂非线性动态而充满挑战，传统模型如ARIMA、GARCH和LSTM难以捕捉数据中的制度转变和长期依赖。本文提出CryptoMamba，一种基于State Space Models (SSMs)的Mamba架构，旨在有效捕捉金融时间序列的长期依赖。实验结果表明，CryptoMamba在预测准确性和泛化能力上超越了现有模型，并在与交易算法结合的实际场景中展示了显著的财务收益。该研究突显了SSMs在股票和加密货币价格预测任务中的巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC) 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01010v2",
      "published_date": "2025-01-02 02:16:56 UTC",
      "updated_date": "2025-05-04 03:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:02:35.592943"
    },
    {
      "arxiv_id": "2501.01007v1",
      "title": "Deep Reinforcement Learning for Job Scheduling and Resource Management in Cloud Computing: An Algorithm-Level Review",
      "title_zh": "深度强化学习用于云计算中作业调度和资源管理：算法级别综述",
      "authors": [
        "Yan Gu",
        "Zhaoze Liu",
        "Shuhong Dai",
        "Cong Liu",
        "Ying Wang",
        "Shen Wang",
        "Georgios Theodoropoulos",
        "Long Cheng"
      ],
      "abstract": "Cloud computing has revolutionized the provisioning of computing resources,\noffering scalable, flexible, and on-demand services to meet the diverse\nrequirements of modern applications. At the heart of efficient cloud operations\nare job scheduling and resource management, which are critical for optimizing\nsystem performance and ensuring timely and cost-effective service delivery.\nHowever, the dynamic and heterogeneous nature of cloud environments presents\nsignificant challenges for these tasks, as workloads and resource availability\ncan fluctuate unpredictably. Traditional approaches, including heuristic and\nmeta-heuristic algorithms, often struggle to adapt to these real-time changes\ndue to their reliance on static models or predefined rules. Deep Reinforcement\nLearning (DRL) has emerged as a promising solution to these challenges by\nenabling systems to learn and adapt policies based on continuous observations\nof the environment, facilitating intelligent and responsive decision-making.\nThis survey provides a comprehensive review of DRL-based algorithms for job\nscheduling and resource management in cloud computing, analyzing their\nmethodologies, performance metrics, and practical applications. We also\nhighlight emerging trends and future research directions, offering valuable\ninsights into leveraging DRL to advance both job scheduling and resource\nmanagement in cloud computing.",
      "tldr_zh": "这篇论文对云计算中作业调度和资源管理的算法进行了算法层面的全面审查，重点探讨了深度强化学习(DRL)如何应对动态异构环境的挑战。传统方法如启发式和元启发式算法因依赖静态模型而难以适应实时变化，而DRL通过持续观察环境并学习适应策略，实现智能决策和性能优化。研究分析了DRL算法的多种方法、性能指标及实际应用，并指出了未来趋势，如进一步提升DRL在云资源管理中的潜力。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01007v1",
      "published_date": "2025-01-02 02:08:00 UTC",
      "updated_date": "2025-01-02 02:08:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:02:46.704023"
    },
    {
      "arxiv_id": "2501.01005v2",
      "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
      "title_zh": "FlashInfer：高效且可定制的注意力引擎用于LLM推理服务",
      "authors": [
        "Zihao Ye",
        "Lequn Chen",
        "Ruihang Lai",
        "Wuwei Lin",
        "Yineng Zhang",
        "Stephanie Wang",
        "Tianqi Chen",
        "Baris Kasikci",
        "Vinod Grover",
        "Arvind Krishnamurthy",
        "Luis Ceze"
      ],
      "abstract": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
      "tldr_zh": "该论文介绍了 FlashInfer，一种高效且可定制的注意力引擎，旨在优化大型语言模型 (LLMs) 推理服务的性能，通过处理 KV-cache 存储异质性采用 block-sparse 格式和 composable 格式来减少内存访问冗余，并利用 Just-In-Time (JIT) 编译和负载均衡调度算法适应动态用户请求，同时兼容 CUDAGraph。FlashInfer 已集成到 SGLang、vLLM 和 MLC-Engine 等框架中。实验结果显示，它在各种推理场景中显著提升效率，包括比现有解决方案高出 29-69% 的 inter-token-latency 减少、28-30% 的长上下文推理延迟降低，以及 13-17% 的并行生成加速。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
      "pdf_url": "http://arxiv.org/pdf/2501.01005v2",
      "published_date": "2025-01-02 02:02:20 UTC",
      "updated_date": "2025-04-21 20:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:02:59.316112"
    },
    {
      "arxiv_id": "2501.03181v2",
      "title": "FaceSpeak: Expressive and High-Quality Speech Synthesis from Human Portraits of Different Styles",
      "title_zh": "FaceSpeak: 基于不同风格人像的富有表现力和高质量语音合成",
      "authors": [
        "Tian-Hao Zhang",
        "Jiawei Zhang",
        "Jun Wang",
        "Xinyuan Qian",
        "Xu-Cheng Yin"
      ],
      "abstract": "Humans can perceive speakers' characteristics (e.g., identity, gender,\npersonality and emotion) by their appearance, which are generally aligned to\ntheir voice style. Recently, vision-driven Text-to-speech (TTS) scholars\ngrounded their investigations on real-person faces, thereby restricting\neffective speech synthesis from applying to vast potential usage scenarios with\ndiverse characters and image styles. To solve this issue, we introduce a novel\nFaceSpeak approach. It extracts salient identity characteristics and emotional\nrepresentations from a wide variety of image styles. Meanwhile, it mitigates\nthe extraneous information (e.g., background, clothing, and hair color, etc.),\nresulting in synthesized speech closely aligned with a character's persona.\nFurthermore, to overcome the scarcity of multi-modal TTS data, we have devised\nan innovative dataset, namely Expressive Multi-Modal TTS, which is diligently\ncurated and annotated to facilitate research in this domain. The experimental\nresults demonstrate our proposed FaceSpeak can generate portrait-aligned voice\nwith satisfactory naturalness and quality.",
      "tldr_zh": "本研究提出 FaceSpeak 方法，用于从不同风格的人像中生成富有表现力和高质量的 Text-to-Speech (TTS) 语音。它通过提取显著的身份特征和情感表示，同时过滤无关信息（如背景、服装等），确保合成语音与角色的个性紧密对齐，以解决现有视觉驱动 TTS 系统在多样化场景中的局限性。为应对多模态 TTS 数据稀缺问题，作者创建了新的 Expressive Multi-Modal TTS 数据集，并进行了精心标注。实验结果表明，FaceSpeak 生成的语音在自然性和质量方面表现出色，与人像高度一致。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03181v2",
      "published_date": "2025-01-02 02:00:15 UTC",
      "updated_date": "2025-04-15 19:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:03:11.066950"
    },
    {
      "arxiv_id": "2501.00999v2",
      "title": "Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory",
      "title_zh": "探索大型语言模型中的信息处理：来自信息瓶颈理论的洞见",
      "authors": [
        "Zhou Yang",
        "Zhengyu Qi",
        "Zhaochun Ren",
        "Zhikai Jia",
        "Haizhou Sun",
        "Xiaofei Zhu",
        "Xiangwen Liao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
      "tldr_zh": "这篇论文从 Information Bottleneck Theory 的视角探讨 Large Language Models (LLMs) 的信息处理机制，揭示 LLMs 通过将输入信息压缩到特定任务空间（如情感空间或主题空间）来理解任务，并在关键时刻提取相关信息生成预测。研究者提出两种新方法：Information Compression-based Context Learning (IC-ICL)，用于压缩检索示例信息以提升推理性能和推理效率；以及 Task-Space-guided Fine-Tuning (TS-FT)，通过空间引导损失微调模型，促进更有效的压缩和选择机制。实验在多个数据集上验证了任务空间构建的有效性，IC-ICL 不仅改善性能还加速推理速度超过 40%，而 TS-FT 以最小调整实现优越结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.00999v2",
      "published_date": "2025-01-02 01:33:58 UTC",
      "updated_date": "2025-01-06 01:49:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:03:22.998934"
    },
    {
      "arxiv_id": "2501.02004v1",
      "title": "General Information Metrics for Improving AI Model Training Efficiency",
      "title_zh": "通用信息指标用于提升AI模型训练效率",
      "authors": [
        "Jianfeng Xu",
        "Congcong Liu",
        "Xiaoying Tan",
        "Xiaojie Zhu",
        "Anpeng Wu",
        "Huan Wan",
        "Weijun Kong",
        "Chun Li",
        "Hu Xu",
        "Kun Kuang",
        "Fei Wu"
      ],
      "abstract": "To address the growing size of AI model training data and the lack of a\nuniversal data selection methodology-factors that significantly drive up\ntraining costs -- this paper presents the General Information Metrics\nEvaluation (GIME) method. GIME leverages general information metrics from\nObjective Information Theory (OIT), including volume, delay, scope,\ngranularity, variety, duration, sampling rate, aggregation, coverage,\ndistortion, and mismatch to optimize dataset selection for training purposes.\nComprehensive experiments conducted across diverse domains, such as CTR\nPrediction, Civil Case Prediction, and Weather Forecasting, demonstrate that\nGIME effectively preserves model performance while substantially reducing both\ntraining time and costs. Additionally, applying GIME within the Judicial AI\nProgram led to a remarkable 39.56% reduction in total model training expenses,\nunderscoring its potential to support efficient and sustainable AI development.",
      "tldr_zh": "本论文针对AI模型训练数据规模增大和数据选择方法缺失导致的成本上升问题，提出General Information Metrics Evaluation (GIME)方法。该方法基于Objective Information Theory (OIT)的通用信息指标（如volume、delay、scope等），优化数据集选择过程，以提高训练效率。在CTR Prediction、Civil Case Prediction和Weather Forecasting等领域的实验中，GIME成功维持了模型性能，同时显著降低了训练时间和成本；实际应用中，在Judicial AI Program中实现了39.56%的训练费用减少，展示了其在可持续AI开发中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02004v1",
      "published_date": "2025-01-02 01:28:00 UTC",
      "updated_date": "2025-01-02 01:28:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:03:34.084656"
    },
    {
      "arxiv_id": "2501.00989v1",
      "title": "Bootstrapped Reward Shaping",
      "title_zh": "翻译失败",
      "authors": [
        "Jacob Adamczyk",
        "Volodymyr Makarenko",
        "Stas Tiomkin",
        "Rahul V. Kulkarni"
      ],
      "abstract": "In reinforcement learning, especially in sparse-reward domains, many\nenvironment steps are required to observe reward information. In order to\nincrease the frequency of such observations, \"potential-based reward shaping\"\n(PBRS) has been proposed as a method of providing a more dense reward signal\nwhile leaving the optimal policy invariant. However, the required \"potential\nfunction\" must be carefully designed with task-dependent knowledge to not deter\ntraining performance. In this work, we propose a \"bootstrapped\" method of\nreward shaping, termed BSRS, in which the agent's current estimate of the\nstate-value function acts as the potential function for PBRS. We provide\nconvergence proofs for the tabular setting, give insights into training\ndynamics for deep RL, and show that the proposed method improves training speed\nin the Atari suite.",
      "tldr_zh": "本文提出了一种引导式奖励整形方法（BSRS），旨在解决强化学习（reinforcement learning）中稀疏奖励环境的训练效率问题，通过使用代理当前的状态值函数估计作为潜在函数来增强潜在奖励整形（PBRS）的效果，从而提供更密集的奖励信号而不改变最优策略。BSRS 在表格设置下提供了收敛证明，并分析了其在深度强化学习中的训练动态。实验结果显示，该方法在 Atari 套件中显著提高了训练速度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI-2025, Main Track",
      "pdf_url": "http://arxiv.org/pdf/2501.00989v1",
      "published_date": "2025-01-02 00:40:55 UTC",
      "updated_date": "2025-01-02 00:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:03:46.192096"
    },
    {
      "arxiv_id": "2501.00982v1",
      "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Ravenda",
        "Seyed Ali Bahrainian",
        "Andrea Raballo",
        "Antonietta Mira",
        "Noriko Kando"
      ],
      "abstract": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在心理评估中的有效性，提出了一种自适应 Retrieval-Augmented Generation (RAG) 方法，通过分析社交媒体帖子来自动完成标准化心理问卷，如 Beck Depression Inventory II (BDI-II)。该方法在零样本设置下检索相关用户帖子，并使用 LLMs 预测问卷分数，结果显示其性能可与或超过现有基准模型，而无需训练数据。研究进一步证明，这种方法作为可扩展的心理健康筛查工具，能提供可解释的评估过程，跟踪每个项目响应以符合心理测量实践，从而提升筛查的系统性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00982v1",
      "published_date": "2025-01-02 00:01:54 UTC",
      "updated_date": "2025-01-02 00:01:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:03:58.544863"
    },
    {
      "arxiv_id": "2501.02014v1",
      "title": "Machine Learning-Based Differential Diagnosis of Parkinson's Disease Using Kinematic Feature Extraction and Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Masahiro Matsumoto",
        "Abu Saleh Musa Miah",
        "Nobuyoshi Asai",
        "Jungpil Shin"
      ],
      "abstract": "Parkinson's disease (PD), the second most common neurodegenerative disorder,\nis characterized by dopaminergic neuron loss and the accumulation of abnormal\nsynuclein. PD presents both motor and non-motor symptoms that progressively\nimpair daily functioning. The severity of these symptoms is typically assessed\nusing the MDS-UPDRS rating scale, which is subjective and dependent on the\nphysician's experience. Additionally, PD shares symptoms with other\nneurodegenerative diseases, such as progressive supranuclear palsy (PSP) and\nmultiple system atrophy (MSA), complicating accurate diagnosis. To address\nthese diagnostic challenges, we propose a machine learning-based system for\ndifferential diagnosis of PD, PSP, MSA, and healthy controls (HC). This system\nutilizes a kinematic feature-based hierarchical feature extraction and\nselection approach. Initially, 18 kinematic features are extracted, including\ntwo newly proposed features: Thumb-to-index vector velocity and acceleration,\nwhich provide insights into motor control patterns. In addition, 41 statistical\nfeatures were extracted here from each kinematic feature, including some new\napproaches such as Average Absolute Change, Rhythm, Amplitude, Frequency,\nStandard Deviation of Frequency, and Slope. Feature selection is performed\nusing One-way ANOVA to rank features, followed by Sequential Forward Floating\nSelection (SFFS) to identify the most relevant ones, aiming to reduce the\ncomputational complexity. The final feature set is used for classification,\nachieving a classification accuracy of 66.67% for each dataset and 88.89% for\neach patient, with particularly high performance for the MSA and HC groups\nusing the SVM algorithm. This system shows potential as a rapid and accurate\ndiagnostic tool in clinical practice, though further data collection and\nrefinement are needed to enhance its reliability.",
      "tldr_zh": "本文提出一种基于机器学习的系统，用于帕金森病(PD)、进行性核上性麻痹(PSP)、多系统萎缩(MSA)和健康对照(HC)的鉴别诊断，以解决传统主观诊断如MDS-UPDRS的局限性。系统首先提取18个运动学特征（包括新提出的Thumb-to-index vector velocity and acceleration），并从中衍生41个统计特征（如Average Absolute Change、Rhythm和Frequency），随后通过One-way ANOVA排名和Sequential Forward Floating Selection (SFFS)优化特征集以降低计算复杂度。实验结果显示，使用SVM算法的分类准确率达66.67%（数据集）和88.89%（患者），尤其在MSA和HC组表现突出，该系统有望成为临床快速诊断工具，但需进一步收集数据提升可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02014v1",
      "published_date": "2025-01-02 14:43:39 UTC",
      "updated_date": "2025-01-02 14:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:04:17.332900"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 90,
  "processed_papers_count": 90,
  "failed_papers_count": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T20:04:30.019046"
}
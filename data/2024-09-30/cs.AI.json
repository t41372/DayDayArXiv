{
  "date": "2024-09-30",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-30 的 arXiv 中文 TLDR 快报！今天的论文主要聚焦于 AI 模型的优化、评估和应用，特别是大型语言模型 (LLM) 的鲁棒性、多任务规划能力以及在知识图谱和时间序列预测中的表现；令人印象深刻的是关于 LLM 规划能力的实验（如 OpenAI o1 模型）和知识图谱嵌入的创新方法，而知名学者如 Karl Friston 的论文则探讨了 AI 安全与结构学习。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊那些重要、话题度高或有创新贡献的（如 LLM 相关、强化学习和知识图谱），并快速掠过相对常规的文章。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### LLM 优化与评估（重点主题，讨论较多）\n- **模型选择与规划能力 (Possible principles for aligned structure learning agents)**  \n  作者包括 Karl Friston，这篇论文提出 AI 代理通过结构学习建模世界和偏好，核心贡献是合成数学框架（如 Asimov 定律）来指导可扩展的 AI 对齐，强调核心知识和理论思维在 AI 安全中的作用，发现可帮助设计更可靠的 AI 系统。\n\n- **LLM 在规划中的表现 (On the Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability)**  \n  这篇实验性工作评估 OpenAI o1 模型在规划任务中的可行性、最优性和泛化性，发现模型在结构化环境中强于 GPT-4，但存在决策和记忆瓶颈，贡献在于揭示 LLM 规划的局限并为未来改进提供方向。\n\n- **LLM 知识蒸馏与多任务 (Beyond Scores: A Modular RAG-Based System for Automatic Short Answer Scoring with Feedback)**  \n  论文引入 RAG 系统进行零样本短答评分，核心发现是结合检索增强生成可提升评分准确性（准确率提升 9%），并生成可解释反馈，适用于教育领域。\n\n- **LLM 幻觉与鲁棒性 (LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation)**  \n  研究 LLM 在代码生成的幻觉问题，贡献是通过 RAG 方法缓解幻觉（准确性改善），发现幻觉源于模型训练偏差，并提供实证分析工具。\n\n快速掠过一些 LLM 相关但较常规的论文，如 **Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution**，它使用高斯分布建模概念子空间，提升了 LLM 表示的鲁棒性，但细节较琐碎。\n\n### 强化学习与多代理系统（高话题度，创新性强）\n- **多代理规划框架 (LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner)**  \n  提出 LaMMA-P 框架，使用 LLM 驱动的 PDDL 规划器处理多代理长时序任务，核心贡献是提升成功率（105% 提高）和效率（36% 提高），在 AI2-THOR 环境中展示泛化能力。\n\n- **终身强化学习 (M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning)**  \n  引入多模态蒸馏方法，核心发现是通过视觉-语言-动作一致性保持学习空间，显著减少遗忘（LIBERO 基准上性能提升），适用于机器人操作任务。\n\n其他强化学习论文如 **Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning**，通过演示推断偏好，优化多目标决策，但实验规模较小，故快速提及。\n\n### 知识图谱与嵌入（基础性创新，值得关注）\n- **知识图谱嵌入优化 (Knowledge Graph Embedding by Normalizing Flows)**  \n  论文将实体/关系嵌入为置换群，使用归一化流建模不确定性，核心贡献是提升嵌入的泛化性（实验中表现优于现有模型），并证明能学习逻辑规则。\n\n- **查询与图理解 (GUNDAM: Aligning Large Language Models with Graph Understanding)**  \n  提出 GUNDAM 模型，将 LLM 与图理解对齐，核心发现是通过对比学习提升图推理能力（基准上性能提升），为 LLM 处理结构化数据提供新路径。\n\n快速掠过如 **Evaluating the Effects of AI Directors for Quest Selection**，它评估 AI 在游戏任务选择中的效果，但应用性较窄。\n\n### 其他领域（简要覆盖，相关性高）\n- **时间序列预测 (TSI: A Multi-View Representation Learning Approach for Time Series Forecasting)**  \n  引入多视图表示学习（趋势、季节性和独立组件），核心贡献是提升预测准确性（基准数据集上平均改善 7.76%），适用于复杂序列数据。\n\n- **多模态与逆问题 (A Survey on Diffusion Models for Inverse Problems)**  \n  综述扩散模型在逆问题中的应用，核心发现是这些模型在图像恢复中表现出色，提供分类框架，强调潜在扩散模型的挑战。\n\n- **医学与AI应用 (CliMB: An AI-enabled Partner for Clinical Predictive Modeling)**  \n  提出 CliMB 系统，使用 LLM 进行无代码预测建模，核心贡献是提升临床模型准确性（比 GPT-4 好 80%），并生成可解释报告。\n\n其他如 **ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities**，构建动态预测基准，发现 LLM 在预测中不如人类专家，但不展开讨论。\n\n总之，今天的论文展示了 AI 领域的快速进展，尤其在 LLM 和强化学习的交叉应用上，但也暴露了模型鲁棒性和泛化性的挑战。更多细节可查阅 arXiv，欢迎读者关注这些前沿方向！",
  "papers": [
    {
      "arxiv_id": "2410.00933v1",
      "title": "StreamEnsemble: Predictive Queries over Spatiotemporal Streaming Data",
      "title_zh": "翻译失败",
      "authors": [
        "Anderson Chaves",
        "Eduardo Ogasawara",
        "Patrick Valduriez",
        "Fabio Porto"
      ],
      "abstract": "Predictive queries over spatiotemporal (ST) stream data pose significant data\nprocessing and analysis challenges. ST data streams involve a set of time\nseries whose data distributions may vary in space and time, exhibiting multiple\ndistinct patterns. In this context, assuming a single machine learning model\nwould adequately handle such variations is likely to lead to failure. To\naddress this challenge, we propose StreamEnsemble, a novel approach to\npredictive queries over ST data that dynamically selects and allocates Machine\nLearning models according to the underlying time series distributions and model\ncharacteristics. Our experimental evaluation reveals that this method markedly\noutperforms traditional ensemble methods and single model approaches in terms\nof accuracy and time, demonstrating a significant reduction in prediction error\nof more than 10 times compared to traditional approaches.",
      "tldr_zh": "该论文针对时空（spatiotemporal）流数据的预测查询问题提出了一种新方法StreamEnsemble，以应对数据分布在空间和时间上变化的挑战。StreamEnsemble通过动态选择和分配Machine Learning模型，根据底层时间序列分布和模型特性，实现对多模式数据的适应性处理。实验结果显示，该方法在准确性和时间效率上显著优于传统集成方法和单一模型方法，预测错误减少超过10倍。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.00933v1",
      "published_date": "2024-09-30 23:50:16 UTC",
      "updated_date": "2024-09-30 23:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:11:17.071769"
    },
    {
      "arxiv_id": "2410.00275v2",
      "title": "On Large Uni- and Multi-modal Models for Unsupervised Classification of Social Media Images: Nature's Contribution to People as a case study",
      "title_zh": "翻译失败",
      "authors": [
        "Rohaifa Khaldi",
        "Domingo Alcaraz-Segura",
        "Ignacio Sánchez-Herrera",
        "Javier Martinez-Lopez",
        "Carlos Javier Navarro",
        "Siham Tabik"
      ],
      "abstract": "Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.",
      "tldr_zh": "本研究探讨了利用大型视觉模型(LVMs)、语言模型(LLMs)和视觉语言模型(LVLMs)进行社交媒体图像的无监督分类问题，以人类与自然的互动（Nature's Contribution to People 或 Cultural Ecosystem Services）作为案例研究。论文提出并比较了多种基于这些模型的策略，包括微调 LVM DINOv2 和 LVLM LLaVA-1.5 与 fine-tuned LLM 的组合方法。实验结果显示，最高性能方法准确率超过95%，但需小规模标注数据集，而完全无监督方法如 GPT-4 和 LLaVA-1.5 则达到84%以上，DINOv2 在10-shot学习中也取得83.99%的竞争性表现。这些发现为高效、可扩展的图像分类提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.00275v2",
      "published_date": "2024-09-30 23:04:55 UTC",
      "updated_date": "2024-10-16 10:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:11:31.476888"
    },
    {
      "arxiv_id": "2410.03737v1",
      "title": "Meta Reinforcement Learning Approach for Adaptive Resource Optimization in O-RAN",
      "title_zh": "翻译失败",
      "authors": [
        "Fatemeh Lotfi",
        "Fatemeh Afghah"
      ],
      "abstract": "As wireless networks grow to support more complex applications, the Open\nRadio Access Network (O-RAN) architecture, with its smart RAN Intelligent\nController (RIC) modules, becomes a crucial solution for real-time network data\ncollection, analysis, and dynamic management of network resources including\nradio resource blocks and downlink power allocation. Utilizing artificial\nintelligence (AI) and machine learning (ML), O-RAN addresses the variable\ndemands of modern networks with unprecedented efficiency and adaptability.\nDespite progress in using ML-based strategies for network optimization,\nchallenges remain, particularly in the dynamic allocation of resources in\nunpredictable environments. This paper proposes a novel Meta Deep Reinforcement\nLearning (Meta-DRL) strategy, inspired by Model-Agnostic Meta-Learning (MAML),\nto advance resource block and downlink power allocation in O-RAN. Our approach\nleverages O-RAN's disaggregated architecture with virtual distributed units\n(DUs) and meta-DRL strategies, enabling adaptive and localized decision-making\nthat significantly enhances network efficiency. By integrating meta-learning,\nour system quickly adapts to new network conditions, optimizing resource\nallocation in real-time. This results in a 19.8% improvement in network\nmanagement performance over traditional methods, advancing the capabilities of\nnext-generation wireless networks.",
      "tldr_zh": "该论文提出了一种基于 Meta Deep Reinforcement Learning (Meta-DRL) 的策略，针对 O-RAN 架构中的资源优化问题，解决动态无线网络环境下资源块和下行功率分配的挑战。该方法借鉴 Model-Agnostic Meta-Learning (MAML)，利用 O-RAN 的分层架构（如虚拟分布式单元）实现自适应和本地化决策，快速适应新网络条件。实验结果显示，与传统方法相比，该策略将网络管理性能提升了 19.8%，为下一代无线网络的效率和适应性提供了重要进展。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SY",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03737v1",
      "published_date": "2024-09-30 23:04:30 UTC",
      "updated_date": "2024-09-30 23:04:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:11:41.062839"
    },
    {
      "arxiv_id": "2410.00274v2",
      "title": "Social Conjuring: Multi-User Runtime Collaboration with AI in Building Virtual 3D Worlds",
      "title_zh": "翻译失败",
      "authors": [
        "Amina Kobenova",
        "Cyan DeVeaux",
        "Samyak Parajuli",
        "Andrzej Banburski-Fahey",
        "Judith Amores Fernandez",
        "Jaron Lanier"
      ],
      "abstract": "Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.",
      "tldr_zh": "本文提出 Social Conjurer 框架，这是一个 AI-augmented 系统，支持多个用户实时协作创建和修改 3D 虚拟世界，通过社交互动、工具-based 互动以及空间推理来丰富虚拟环境。初步用户研究 (N=12) 揭示了社会上下文如何影响提示空间环境的生成过程，并提供了对基于提示的 3D 共同创建的社会应用的洞见。该框架突出了 AI-supported 多用户世界创建的潜力，并为设计以人为本的 VR 接口提供了新路径和启示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation",
      "pdf_url": "http://arxiv.org/pdf/2410.00274v2",
      "published_date": "2024-09-30 23:02:51 UTC",
      "updated_date": "2024-10-02 17:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:11:53.241683"
    },
    {
      "arxiv_id": "2410.00267v1",
      "title": "KPCA-CAM: Visual Explainability of Deep Computer Vision Models using Kernel PCA",
      "title_zh": "K",
      "authors": [
        "Sachin Karmani",
        "Thanushon Sivakaran",
        "Gaurav Prasad",
        "Mehmet Ali",
        "Wenbo Yang",
        "Sheyang Tang"
      ],
      "abstract": "Deep learning models often function as black boxes, providing no\nstraightforward reasoning for their predictions. This is particularly true for\ncomputer vision models, which process tensors of pixel values to generate\noutcomes in tasks such as image classification and object detection. To\nelucidate the reasoning of these models, class activation maps (CAMs) are used\nto highlight salient regions that influence a model's output. This research\nintroduces KPCA-CAM, a technique designed to enhance the interpretability of\nConvolutional Neural Networks (CNNs) through improved class activation maps.\nKPCA-CAM leverages Principal Component Analysis (PCA) with the kernel trick to\ncapture nonlinear relationships within CNN activations more effectively. By\nmapping data into higher-dimensional spaces with kernel functions and\nextracting principal components from this transformed hyperplane, KPCA-CAM\nprovides more accurate representations of the underlying data manifold. This\nenables a deeper understanding of the features influencing CNN decisions.\nEmpirical evaluations on the ILSVRC dataset across different CNN models\ndemonstrate that KPCA-CAM produces more precise activation maps, providing\nclearer insights into the model's reasoning compared to existing CAM\nalgorithms. This research advances CAM techniques, equipping researchers and\npractitioners with a powerful tool to gain deeper insights into CNN\ndecision-making processes and overall behaviors.",
      "tldr_zh": "这篇论文针对深度学习模型的黑盒问题，提出了KPCA-CAM方法，使用Kernel PCA来提升计算机视觉模型（如CNNs）的可解释性。KPCA-CAM通过核函数将数据映射到高维空间，捕捉CNN激活中的非线性关系，从而生成更准确的Class Activation Maps (CAMs)，帮助揭示影响模型决策的关键特征。在ILSVRC数据集上的实验表明，KPCA-CAM比现有CAM算法产生更精确的激活地图，提供更清晰的模型推理洞见，从而为研究者和从业者提供强大的工具，推进CNN决策过程的可解释性研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 4 figures, Published to IEEE MMSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.00267v1",
      "published_date": "2024-09-30 22:36:37 UTC",
      "updated_date": "2024-09-30 22:36:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:12:06.337092"
    },
    {
      "arxiv_id": "2410.00263v2",
      "title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation",
      "title_zh": "基于手术过程感知的视频-语言预训练及其分层知识增强",
      "authors": [
        "Kun Yuan",
        "Vinkle Srivastav",
        "Nassir Navab",
        "Nicolas Padoy"
      ],
      "abstract": "Surgical video-language pretraining (VLP) faces unique challenges due to the\nknowledge domain gap and the scarcity of multi-modal data. This study aims to\nbridge the gap by addressing issues regarding textual information loss in\nsurgical lecture videos and the spatial-temporal challenges of surgical VLP. We\npropose a hierarchical knowledge augmentation approach and a novel\nProcedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining\n(PeskaVLP) framework to tackle these issues. The knowledge augmentation uses\nlarge language models (LLM) for refining and enriching surgical concepts, thus\nproviding comprehensive language supervision and reducing the risk of\noverfitting. PeskaVLP combines language supervision with visual\nself-supervision, constructing hard negative samples and employing a Dynamic\nTime Warping (DTW) based loss function to effectively comprehend the\ncross-modal procedural alignment. Extensive experiments on multiple public\nsurgical scene understanding and cross-modal retrieval datasets show that our\nproposed method significantly improves zero-shot transferring performance and\noffers a generalist visual representation for further advancements in surgical\nscene understanding.The code is available at\nhttps://github.com/CAMMA-public/SurgVLP",
      "tldr_zh": "本研究针对手术视频语言预训练（VLP）面临的知识领域差距、多模态数据稀缺以及文本信息和空间-时间挑战，提出了一种层次化知识增强方法和新型 Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining（PeskaVLP）框架。PeskaVLP 利用大型语言模型（LLM）提炼并丰富手术概念，提供全面语言监督，同时结合视觉自监督、硬负样本构建和基于 Dynamic Time Warping（DTW）的损失函数，以实现有效的跨模态程序对齐。在多个公共手术场景理解和跨模态检索数据集上的实验表明，该方法显著提升了零样本转移性能，并为手术场景理解提供了通用视觉表示。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024 Spolight)",
      "pdf_url": "http://arxiv.org/pdf/2410.00263v2",
      "published_date": "2024-09-30 22:21:05 UTC",
      "updated_date": "2025-03-13 15:21:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:12:16.739825"
    },
    {
      "arxiv_id": "2410.00260v2",
      "title": "DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining",
      "title_zh": "DoPAMine：基于种子引导数据挖掘的领域特定预训练适应",
      "authors": [
        "Vinayak Arannil",
        "Neha Narwal",
        "Sourav Sanjukta Bhabesh",
        "Sai Nikhil Thirandas",
        "Darren Yow-Bang Wang",
        "Graham Horwood",
        "Alex Anto Chirayath",
        "Gouri Pandeshwar"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.",
      "tldr_zh": "本研究提出DoPAMine框架，一种自动化可扩展的方法，用于从大型数据语料库（如Common Crawl）中挖掘领域特定数据，以适应Large Language Models (LLMs)的预训练。框架利用LLMs的parametric knowledge生成多样化的种子数据，然后指导真实数据的挖掘，针对低资源领域如医疗和金融进行domain adaptation。在实验中，对7B参数的LLMs进行continual pre-training (CPT)，结果显示DoPAMine在医疗任务（MMLU、MedQA等）上零-shot和5-shot设置下平均提升4.9%和5.1%，在金融任务（FiQA-SA、FPB等）上提升2.9%和6.7%，证明了其在提升模型性能方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00260v2",
      "published_date": "2024-09-30 22:15:58 UTC",
      "updated_date": "2024-10-09 17:39:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:12:30.541910"
    },
    {
      "arxiv_id": "2410.00258v1",
      "title": "Possible principles for aligned structure learning agents",
      "title_zh": "针对对齐结构学习代理的可能原则",
      "authors": [
        "Lancelot Da Costa",
        "Tomáš Gavenčiak",
        "David Hyland",
        "Mandana Samiei",
        "Cristian Dragos-Manta",
        "Candice Pattisapu",
        "Adeel Razi",
        "Karl Friston"
      ],
      "abstract": "This paper offers a roadmap for the development of scalable aligned\nartificial intelligence (AI) from first principle descriptions of natural\nintelligence. In brief, a possible path toward scalable aligned AI rests upon\nenabling artificial agents to learn a good model of the world that includes a\ngood model of our preferences. For this, the main objective is creating agents\nthat learn to represent the world and other agents' world models; a problem\nthat falls under structure learning (a.k.a. causal representation learning). We\nexpose the structure learning and alignment problems with this goal in mind, as\nwell as principles to guide us forward, synthesizing various ideas across\nmathematics, statistics, and cognitive science. 1) We discuss the essential\nrole of core knowledge, information geometry and model reduction in structure\nlearning, and suggest core structural modules to learn a wide range of\nnaturalistic worlds. 2) We outline a way toward aligned agents through\nstructure learning and theory of mind. As an illustrative example, we\nmathematically sketch Asimov's Laws of Robotics, which prescribe agents to act\ncautiously to minimize the ill-being of other agents. We supplement this\nexample by proposing refined approaches to alignment. These observations may\nguide the development of artificial intelligence in helping to scale existing\n-- or design new -- aligned structure learning systems.",
      "tldr_zh": "这篇论文从自然智能的第一原则出发，提出开发可扩展的aligned AI的路线图，核心是通过structure learning让人工智能代理学习世界模型和人类偏好。论文强调structure learning的关键作用，包括core knowledge、information geometry和模型reduction，以构建能代表真实世界的结构模块。作者探讨了通过structure learning和theory of mind实现aligned agents，并以Asimov's Laws of Robotics为例，数学地勾勒代理行为以最小化其他代理的负面影响。总体上，这些原则为设计新aligned结构学习系统提供指导，帮助AI更安全地扩展。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages of content, 31 with references",
      "pdf_url": "http://arxiv.org/pdf/2410.00258v1",
      "published_date": "2024-09-30 22:06:06 UTC",
      "updated_date": "2024-09-30 22:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:12:44.014787"
    },
    {
      "arxiv_id": "2410.00257v1",
      "title": "The age of spiritual machines: Language quietus induces synthetic altered states of consciousness in artificial intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Jeremy I Skipper",
        "Joanna Kuc",
        "Greg Cooper",
        "Christopher Timmermann"
      ],
      "abstract": "How is language related to consciousness? Language functions to categorise\nperceptual experiences (e.g., labelling interoceptive states as 'happy') and\nhigher-level constructs (e.g., using 'I' to represent the narrative self).\nPsychedelic use and meditation might be described as altered states that impair\nor intentionally modify the capacity for linguistic categorisation. For\nexample, psychedelic phenomenology is often characterised by 'oceanic\nboundlessness' or 'unity' and 'ego dissolution', which might be expected of a\nsystem unburdened by entrenched language categories. If language breakdown\nplays a role in producing such altered behaviour, multimodal artificial\nintelligence might align more with these phenomenological descriptions when\nattention is shifted away from language. We tested this hypothesis by comparing\nthe semantic embedding spaces from simulated altered states after manipulating\nattentional weights in CLIP and FLAVA models to embedding spaces from altered\nstates questionnaires before manipulation. Compared to random text and various\nother altered states including anxiety, models were more aligned with\ndisembodied, ego-less, spiritual, and unitive states, as well as minimal\nphenomenal experiences, with decreased attention to language and vision.\nReduced attention to language was associated with distinct linguistic patterns\nand blurred embeddings within and, especially, across semantic categories\n(e.g., 'giraffes' become more like 'bananas'). These results lend support to\nthe role of language categorisation in the phenomenology of altered states of\nconsciousness, like those experienced with high doses of psychedelics or\nconcentration meditation, states that often lead to improved mental health and\nwellbeing.",
      "tldr_zh": "这篇论文探讨了语言与意识的关系，提出通过减少语言分类能力（如在迷幻药物或冥想中），可以诱导人工智能中的合成改变状态（altered states of consciousness）。研究者操纵了CLIP和FLAVA模型的注意力权重，模拟这些状态，并将语义嵌入空间与改变状态问卷数据进行比较。结果显示，模型在语言和视觉关注减少时，更接近无身体（disembodied）、自我消解（ego dissolution）和统一（unity）的精神状态，导致语义嵌入模糊（如“giraffes”与“bananas”更相似）。这些发现支持语言分类在意识现象中的关键作用，并为理解类似迷幻体验的心理健康益处提供新见解。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-bio.NC",
      "comment": "8 Figures",
      "pdf_url": "http://arxiv.org/pdf/2410.00257v1",
      "published_date": "2024-09-30 22:03:26 UTC",
      "updated_date": "2024-09-30 22:03:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:12:53.510727"
    },
    {
      "arxiv_id": "2410.00255v2",
      "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Weitai Kang",
        "Haifeng Huang",
        "Yuzhang Shang",
        "Mubarak Shah",
        "Yan Yan"
      ],
      "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted\ntheir potential in building general-purpose agents in the 3D real world, yet\nchallenges remain due to the lack of high-quality robust instruction-following\ndata, leading to limited discriminative power and generalization of 3DLLMs. In\nthis paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale\ninstruction-following data generated by our novel data engine, Robust\nInstruction Generation (RIG) engine. RIG generates two key instruction data: 1)\nthe Adversarial Instruction-following data, which features mixed negative and\npositive samples to enhance the model's discriminative understanding. 2) the\nDiverse Instruction-following data, which contains various instruction styles\nto enhance model's generalization. As a result, we construct 1 million\ninstruction-following data, consisting of 344K Adversarial samples, 508K\nDiverse samples, and 165K benchmark training set samples. To better handle\nthese complex instructions, Robin3D first incorporates Relation-Augmented\nProjector to enhance spatial understanding, and then strengthens the object\nreferring and grounding ability through ID-Feature Bonding. Robin3D\nconsistently outperforms previous methods across five widely-used 3D multimodal\nlearning benchmarks, without the need for task-specific fine-tuning. Notably,\nwe achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\%\nimprovement in the captioning task (Scan2Cap).",
      "tldr_zh": "该研究提出 Robin3D，一种通过鲁棒指令微调（Robust Instruction Tuning）改进 3D Large Language Models (3DLLMs) 的方法，以解决现有模型在辨别力和泛化能力上的局限性。研究开发了 Robust Instruction Generation (RIG) 引擎，生成1百万高质量指令数据，包括 Adversarial Instruction-following data（混合负正样本以提升辨别理解）和 Diverse Instruction-following data（多种风格以增强泛化）。Robin3D 进一步通过 Relation-Augmented Projector 增强空间理解，以及 ID-Feature Bonding 加强对象引用和 grounding 能力。在五个 3D 多模态学习基准上，Robin3D 无需任务特定微调即超越先前方法，在 Multi3DRefer grounding 任务上提升7.8%，在 Scan2Cap captioning 任务上提升6.9%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.00255v2",
      "published_date": "2024-09-30 21:55:38 UTC",
      "updated_date": "2025-02-20 18:06:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:13:05.939892"
    },
    {
      "arxiv_id": "2410.00240v1",
      "title": "Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Rithvik Prakki"
      ],
      "abstract": "Active inference is a mathematical framework for understanding how agents\n(biological or artificial) interact with their environments, enabling continual\nadaptation and decision-making. It combines Bayesian inference and free energy\nminimization to model perception, action, and learning in uncertain and dynamic\ncontexts. Unlike reinforcement learning, active inference integrates\nexploration and exploitation seamlessly by minimizing expected free energy. In\nthis paper, we present a continual learning framework for agents operating in\ndiscrete time environments, using active inference as the foundation. We derive\nthe mathematical formulations of variational and expected free energy and apply\nthem to the design of a self-learning research agent. This agent updates its\nbeliefs and adapts its actions based on new data without manual intervention.\nThrough experiments in changing environments, we demonstrate the agent's\nability to relearn and refine its models efficiently, making it suitable for\ncomplex domains like finance and healthcare. The paper concludes by discussing\nhow the proposed framework generalizes to other systems, positioning active\ninference as a flexible approach for adaptive AI.",
      "tldr_zh": "本文展示了Active Inference框架在离散时间环境的持续学习能力，该框架通过Bayesian inference和free energy minimization整合感知、行动和学习，实现代理在不确定动态环境中的无缝探索与利用。研究者推导了variational free energy和expected free energy的数学公式，并设计了一个自学习代理，能够基于新数据自动更新信念和适应行动。实验结果表明，该代理在变化环境中高效重新学习和精炼模型，适用于复杂领域如金融和医疗，并将Active Inference定位为一种灵活的适应性AI方法。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.00240v1",
      "published_date": "2024-09-30 21:18:46 UTC",
      "updated_date": "2024-09-30 21:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:13:17.305977"
    },
    {
      "arxiv_id": "2410.03736v2",
      "title": "CliMB: An AI-enabled Partner for Clinical Predictive Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Evgeny Saveliev",
        "Tim Schubert",
        "Thomas Pouplin",
        "Vasilis Kosmoliaptsis",
        "Mihaela van der Schaar"
      ],
      "abstract": "Despite its significant promise and continuous technical advances, real-world\napplications of artificial intelligence (AI) remain limited. We attribute this\nto the \"domain expert-AI-conundrum\": while domain experts, such as clinician\nscientists, should be able to build predictive models such as risk scores, they\nface substantial barriers in accessing state-of-the-art (SOTA) tools. While\nautomated machine learning (AutoML) has been proposed as a partner in clinical\npredictive modeling, many additional requirements need to be fulfilled to make\nmachine learning accessible for clinician scientists.\n  To address this gap, we introduce CliMB, a no-code AI-enabled partner\ndesigned to empower clinician scientists to create predictive models using\nnatural language. CliMB guides clinician scientists through the entire medical\ndata science pipeline, thus empowering them to create predictive models from\nreal-world data in just one conversation. CliMB also creates structured reports\nand interpretable visuals. In evaluations involving clinician scientists and\nsystematic comparisons against a baseline GPT-4, CliMB consistently\ndemonstrated superior performance in key areas such as planning, error\nprevention, code execution, and model performance. Moreover, in blinded\nassessments involving 45 clinicians from diverse specialties and career stages,\nmore than 80% preferred CliMB over GPT-4. Overall, by providing a no-code\ninterface with clear guidance and access to SOTA methods in the fields of\ndata-centric AI, AutoML, and interpretable ML, CliMB empowers clinician\nscientists to build robust predictive models.\n  The proof-of-concept version of CliMB is available as open-source software on\nGitHub: https://github.com/vanderschaarlab/climb.",
      "tldr_zh": "该研究引入了CliMB，一种无代码AI工具，旨在解决临床科学家在构建预测模型时面临的障碍，帮助他们使用自然语言轻松创建风险评分模型。CliMB指导用户完成整个医疗数据科学流程，包括数据处理、模型构建、生成结构化报告和可解释视觉化，同时整合AutoML、数据中心AI和可解释ML等先进方法。在评估中，CliMB在规划、错误预防、代码执行和模型性能方面优于GPT-4基准，且在涉及45名临床医生的盲评估中，超过80%的人更倾向于CliMB，从而提升了AI在临床预测建模中的可访问性和实际应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "* Evgeny Saveliev and Tim Schubert contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2410.03736v2",
      "published_date": "2024-09-30 21:18:05 UTC",
      "updated_date": "2024-11-25 16:21:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:13:32.660737"
    },
    {
      "arxiv_id": "2410.00231v1",
      "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
      "title_zh": "Helpful DoggyBot：使用腿部机器人和视觉语言模型的开放世界物体抓取",
      "authors": [
        "Qi Wu",
        "Zipeng Fu",
        "Xuxin Cheng",
        "Xiaolong Wang",
        "Chelsea Finn"
      ],
      "abstract": "Learning-based methods have achieved strong performance for quadrupedal\nlocomotion. However, several challenges prevent quadrupeds from learning\nhelpful indoor skills that require interaction with environments and humans:\nlack of end-effectors for manipulation, limited semantic understanding using\nonly simulation data, and low traversability and reachability in indoor\nenvironments. We present a system for quadrupedal mobile manipulation in indoor\nenvironments. It uses a front-mounted gripper for object manipulation, a\nlow-level controller trained in simulation using egocentric depth for agile\nskills like climbing and whole-body tilting, and pre-trained vision-language\nmodels (VLMs) with a third-person fisheye and an egocentric RGB camera for\nsemantic understanding and command generation. We evaluate our system in two\nunseen environments without any real-world data collection or training. Our\nsystem can zero-shot generalize to these environments and complete tasks, like\nfollowing user's commands to fetch a randomly placed stuff toy after climbing\nover a queen-sized bed, with a 60% success rate. Project website:\nhttps://helpful-doggybot.github.io/",
      "tldr_zh": "这篇论文介绍了Helpful DoggyBot系统，利用四足机器人和视觉-language models (VLMs)实现室内环境的物体抓取任务。系统采用前置抓取器进行操作、低级控制器（在模拟中使用第一人称深度信息训练）来实现敏捷技能如爬行和全身倾斜，并结合预训练VLMs与第三人称鱼眼相机及第一人称RGB相机进行语义理解和命令生成。在两个未见环境中进行零样本评估，该系统成功率达60%，能够完成如爬过大床取回随机放置玩具的任务。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://helpful-doggybot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2410.00231v1",
      "published_date": "2024-09-30 20:58:38 UTC",
      "updated_date": "2024-09-30 20:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:13:44.487915"
    },
    {
      "arxiv_id": "2410.00930v1",
      "title": "ACEV: Unsupervised Intersecting Manifold Segmentation using Adaptation to Angular Change of Eigenvectors in Intrinsic Dimension",
      "title_zh": "翻译失败",
      "authors": [
        "Subhadip Boral",
        "Rikathi Pal",
        "Ashish Ghosh"
      ],
      "abstract": "Intersecting manifold segmentation has been a focus of research, where\nindividual manifolds, that intersect with other manifolds, are separated to\ndiscover their distinct properties. The proposed method is based on the\nintuition that when a manifold in $D$ dimensional space with an intrinsic\ndimension of $d$ intersects with another manifold, the data variance grows in\nmore than $d$ directions. The proposed method measures local data variances and\ndetermines their vector directions. It counts the number of vectors with\nnon-zero variance, which determines the manifold's intrinsic dimension. For\ndetection of the intersection region, the method adapts to the changes in the\nangular gaps between the corresponding direction vectors of the child and\nparent using exponential moving averages using a tree structure construction.\nAccordingly, it includes those data points in the same manifold whose\nneighborhood is within the adaptive angular difference and eventually\nidentifies the data points in the intersection area of manifolds. Data points\nwhose inclusion in the neighborhood-identified data points increases their\nintrinsic dimensionality are removed based on data variance and distance. The\nproposed method performs better than 18 SOTA manifold segmentation methods in\nARI and NMI scores over 14 real-world datasets with lesser time complexity and\nbetter stability.",
      "tldr_zh": "该论文提出了一种无监督（unsupervised）交叉流形分割方法ACEV，通过适应内在维度（intrinsic dimension）中特征向量的角度变化来识别和分离相交流形。方法的核心直觉是测量局部数据方差，计算非零方差向量的数量以确定流形的内在维度，并利用指数移动平均（exponential moving averages）和树结构适应子级与父级方向向量之间的角度差异，从而检测相交区域并移除异常数据点。实验结果显示，ACEV在14个真实数据集上，比18个SOTA方法在ARI和NMI分数上表现出色，同时具有更低的时复杂度和更好的稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 7 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.00930v1",
      "published_date": "2024-09-30 20:37:47 UTC",
      "updated_date": "2024-09-30 20:37:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:13:53.421310"
    },
    {
      "arxiv_id": "2410.00929v1",
      "title": "A Knowledge-Informed Large Language Model Framework for U.S. Nuclear Power Plant Shutdown Initiating Event Classification for Probabilistic Risk Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Min Xian",
        "Tao Wang",
        "Sai Zhang",
        "Fei Xu",
        "Zhegang Ma"
      ],
      "abstract": "Identifying and classifying shutdown initiating events (SDIEs) is critical\nfor developing low power shutdown probabilistic risk assessment for nuclear\npower plants. Existing computational approaches cannot achieve satisfactory\nperformance due to the challenges of unavailable large, labeled datasets,\nimbalanced event types, and label noise. To address these challenges, we\npropose a hybrid pipeline that integrates a knowledge-informed machine learning\nmode to prescreen non-SDIEs and a large language model (LLM) to classify SDIEs\ninto four types. In the prescreening stage, we proposed a set of 44 SDIE text\npatterns that consist of the most salient keywords and phrases from six SDIE\ntypes. Text vectorization based on the SDIE patterns generates feature vectors\nthat are highly separable by using a simple binary classifier. The second stage\nbuilds Bidirectional Encoder Representations from Transformers (BERT)-based\nLLM, which learns generic English language representations from self-supervised\npretraining on a large dataset and adapts to SDIE classification by fine-tuning\nit on an SDIE dataset. The proposed approaches are evaluated on a dataset with\n10,928 events using precision, recall ratio, F1 score, and average accuracy.\nThe results demonstrate that the prescreening stage can exclude more than 97%\nnon-SDIEs, and the LLM achieves an average accuracy of 93.4% for SDIE\nclassification.",
      "tldr_zh": "该论文提出了一种知识驱动的大型语言模型框架，用于美国核电站关闭启动事件（SDIEs）的分类，以支持概率风险评估（Probabilistic Risk Assessment）。框架包括两个阶段：首先，使用44个SDIE文本模式和文本向量化结合简单二元分类器预筛选非-SDIEs，成功排除超过97%的无关事件；其次，基于BERT的LLM通过自监督预训练和微调，将SDIEs分类为四种类型。实验结果显示，该方法在10,928个事件的测试数据集上，LLM分类平均准确率达到93.4%，显著提升了现有方法的性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00929v1",
      "published_date": "2024-09-30 20:35:03 UTC",
      "updated_date": "2024-09-30 20:35:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:14:06.595532"
    },
    {
      "arxiv_id": "2410.00182v1",
      "title": "Zero-Shot Classification of Crisis Tweets Using Instruction-Finetuned Large Language Models",
      "title_zh": "零样本分类危机推文：使用指令微调的大型语言模型",
      "authors": [
        "Emma McDaniel",
        "Samuel Scheele",
        "Jeff Liu"
      ],
      "abstract": "Social media posts are frequently identified as a valuable source of\nopen-source intelligence for disaster response, and pre-LLM NLP techniques have\nbeen evaluated on datasets of crisis tweets. We assess three commercial large\nlanguage models (OpenAI GPT-4o, Gemini 1.5-flash-001 and Anthropic Claude-3-5\nSonnet) capabilities in zero-shot classification of short social media posts.\nIn one prompt, the models are asked to perform two classification tasks: 1)\nidentify if the post is informative in a humanitarian context; and 2) rank and\nprovide probabilities for the post in relation to 16 possible humanitarian\nclasses. The posts being classified are from the consolidated crisis tweet\ndataset, CrisisBench. Results are evaluated using macro, weighted, and binary\nF1-scores. The informative classification task, generally performed better\nwithout extra information, while for the humanitarian label classification\nproviding the event that occurred during which the tweet was mined, resulted in\nbetter performance. Further, we found that the models have significantly\nvarying performance by dataset, which raises questions about dataset quality.",
      "tldr_zh": "本研究评估了使用指令微调的大型语言模型（LLMs），如 OpenAI GPT-4o、Gemini 1.5-flash-001 和 Anthropic Claude-3-5 Sonnet，在零样本（Zero-Shot）分类危机推文中的性能。模型需执行两任务：判断推文是否在人道主义背景下具有信息性，以及为推文与16个人道主义类别排名并提供概率，使用CrisisBench数据集进行测试。结果显示，信息性分类任务在不提供额外信息时表现更好，而提供事件信息能提升人道主义标签分类的F1-scores；此外，模型在不同数据集上的表现差异显著，引发了对数据集质量的质疑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00182v1",
      "published_date": "2024-09-30 19:33:58 UTC",
      "updated_date": "2024-09-30 19:33:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:14:18.284597"
    },
    {
      "arxiv_id": "2410.00163v1",
      "title": "Adapting LLMs for the Medical Domain in Portuguese: A Study on Fine-Tuning and Model Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Henrique Paiola",
        "Gabriel Lino Garcia",
        "João Renato Ribeiro Manesco",
        "Mateus Roder",
        "Douglas Rodrigues",
        "João Paulo Papa"
      ],
      "abstract": "This study evaluates the performance of large language models (LLMs) as\nmedical agents in Portuguese, aiming to develop a reliable and relevant virtual\nassistant for healthcare professionals. The HealthCareMagic-100k-en and MedQuAD\ndatasets, translated from English using GPT-3.5, were used to fine-tune the\nChatBode-7B model using the PEFT-QLoRA method. The InternLM2 model, with\ninitial training on medical data, presented the best overall performance, with\nhigh precision and adequacy in metrics such as accuracy, completeness and\nsafety. However, DrBode models, derived from ChatBode, exhibited a phenomenon\nof catastrophic forgetting of acquired medical knowledge. Despite this, these\nmodels performed frequently or even better in aspects such as grammaticality\nand coherence. A significant challenge was low inter-rater agreement,\nhighlighting the need for more robust assessment protocols. This work paves the\nway for future research, such as evaluating multilingual models specific to the\nmedical field, improving the quality of training data, and developing more\nconsistent evaluation methodologies for the medical field.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）在葡萄牙语医疗领域的性能，旨在开发可靠的虚拟助手，通过翻译 HealthCareMagic-100k-en 和 MedQuAD 数据集，并使用 PEFT-QLoRA 方法对 ChatBode-7B 模型进行微调。结果显示，InternLM2 模型在准确性、完整性和安全性指标上表现出色，而 DrBode 模型衍生自 ChatBode 但出现了 catastrophic forgetting 现象，尽管在语法和连贯性方面表现良好。研究还突出了低 inter-rater agreement 的挑战，并为未来工作指明方向，如评估多语言医疗模型、优化训练数据和改进评估协议。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2410.00163v1",
      "published_date": "2024-09-30 19:10:03 UTC",
      "updated_date": "2024-09-30 19:10:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:14:29.506699"
    },
    {
      "arxiv_id": "2410.00153v3",
      "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution",
      "title_zh": "超越单一概念向量：在LLMs中用高斯分布建模概念子空间",
      "authors": [
        "Haiyan Zhao",
        "Heng Zhao",
        "Bo Shen",
        "Ali Payani",
        "Fan Yang",
        "Mengnan Du"
      ],
      "abstract": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）中概念向量的不稳定性问题，提出了一种新方法：使用高斯分布（Gaussian Distribution）建模概念子空间（Gaussian Concept Subspace, GCS）。该方法基于线性探测分类器（linear probing classifiers），将单一概念向量扩展为子空间，以更robustly表示语义知识。实验结果显示，GCS在多个LLMs上表现出较高的忠实度和合理性，并在表示干预任务中证明了其在实际应用（如情感导向，emotion steering）中的效能。最后，研究发现，GCS能有效平衡自然语言生成任务中的导向性能和流畅性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.00153v3",
      "published_date": "2024-09-30 18:52:53 UTC",
      "updated_date": "2025-05-05 19:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:14:42.212373"
    },
    {
      "arxiv_id": "2410.03733v1",
      "title": "Evaluating the Effects of AI Directors for Quest Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Kristen K. Yu",
        "Matthew Guzdial",
        "Nathan Sturtevant"
      ],
      "abstract": "Modern commercial games are designed for mass appeal, not for individual\nplayers, but there is a unique opportunity in video games to better fit the\nindividual through adapting game elements. In this paper, we focus on AI\nDirectors, systems which can dynamically modify a game, that personalize the\nplayer experience to match the player's preference. In the past, some AI\nDirector studies have provided inconclusive results, so their effect on player\nexperience is not clear. We take three AI Directors and directly compare them\nin a human subject study to test their effectiveness on quest selection. Our\nresults show that a non-random AI Director provides a better player experience\nthan a random AI Director.",
      "tldr_zh": "本研究评估了AI Directors在任务选择(quest selection)上的效果，这些系统通过动态修改游戏元素来个性化玩家体验，以更好地匹配个体玩家的偏好。作者通过人类受试者研究直接比较了三个AI Directors，结果显示非随机的AI Director比随机的AI Director提供了更佳的玩家体验。该发现为视频游戏的个性化设计提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03733v1",
      "published_date": "2024-09-30 18:16:38 UTC",
      "updated_date": "2024-09-30 18:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:14:53.347885"
    },
    {
      "arxiv_id": "2410.00134v1",
      "title": "Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Melkamu Abay Mersha",
        "Mesay Gemeda yigezu",
        "Jugal Kalita"
      ],
      "abstract": "Topic modeling is a powerful technique to discover hidden topics and patterns\nwithin a collection of documents without prior knowledge. Traditional topic\nmodeling and clustering-based techniques encounter challenges in capturing\ncontextual semantic information. This study introduces an innovative end-to-end\nsemantic-driven topic modeling technique for the topic extraction process,\nutilizing advanced word and document embeddings combined with a powerful\nclustering algorithm. This semantic-driven approach represents a significant\nadvancement in topic modeling methodologies. It leverages contextual semantic\ninformation to extract coherent and meaningful topics. Specifically, our model\ngenerates document embeddings using pre-trained transformer-based language\nmodels, reduces the dimensions of the embeddings, clusters the embeddings based\non semantic similarity, and generates coherent topics for each cluster.\nCompared to ChatGPT and traditional topic modeling algorithms, our model\nprovides more coherent and meaningful topics.",
      "tldr_zh": "本研究提出了一种创新的语义驱动主题建模技术，使用Transformer-based embeddings和聚类算法，来克服传统方法在捕捉上下文语义信息方面的局限性。该方法通过预训练的Transformer-based语言模型生成文档嵌入，进行降维处理，并基于语义相似度进行聚类，最终为每个聚类提取连贯的主题。与ChatGPT和传统主题建模算法相比，该模型能够生成更连贯和有意义的主题结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00134v1",
      "published_date": "2024-09-30 18:15:31 UTC",
      "updated_date": "2024-09-30 18:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:15:05.664886"
    },
    {
      "arxiv_id": "2410.00131v2",
      "title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ji Liu",
        "Jiaxiang Ren",
        "Ruoming Jin",
        "Zijie Zhang",
        "Yang Zhou",
        "Patrick Valduriez",
        "Dejing Dou"
      ],
      "abstract": "As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches).",
      "tldr_zh": "该论文提出了一种基于 Fisher Information 的高效课程联邦学习框架（FibecFed），旨在解决 Federated Learning (FL) 在微调 Large Language Models (LLMs) 时面临的计算、通信成本高和数据非独立同分布（non-IID）问题。\n框架包括两个创新方法：一是利用 Fisher Information 进行自适应数据采样，以提升 FL 微调的有效性；二是动态选择层进行全局聚合并采用 Low Rank Adaptation (LoRA) 实现稀疏参数更新，提高整体效率。\n实验结果显示，在 10 个数据集上，FibecFed 相较 17 个基线方法，准确率提升高达 45.35%，并加速高达 98.61%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 8 figures, 14 tables, to appear in EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.00131v2",
      "published_date": "2024-09-30 18:12:18 UTC",
      "updated_date": "2024-10-18 05:22:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:15:18.181611"
    },
    {
      "arxiv_id": "2410.00129v2",
      "title": "Cartesian Genetic Programming Approach for Designing Convolutional Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Maciej Krzywda",
        "Szymon Łukasik",
        "Amir Gandomi H"
      ],
      "abstract": "The present study covers an approach to neural architecture search (NAS)\nusing Cartesian genetic programming (CGP) for the design and optimization of\nConvolutional Neural Networks (CNNs). In designing artificial neural networks,\none crucial aspect of the innovative approach is suggesting a novel neural\narchitecture. Currently used architectures have mostly been developed manually\nby human experts, which is a time-consuming and error-prone process. In this\nwork, we use pure Genetic Programming Approach to design CNNs, which employs\nonly one genetic operation, i.e., mutation. In the course of preliminary\nexperiments, our methodology yields promising results.",
      "tldr_zh": "这篇论文提出了一种基于Cartesian Genetic Programming (CGP)的神经架构搜索(NAS)方法，用于设计和优化Convolutional Neural Networks (CNNs)。该方法通过纯遗传编程，仅使用突变操作来自动生成神经网络架构，旨在克服手动设计过程的耗时和易出错问题。与传统方法相比，这种创新途径在初步实验中取得了有前景的结果。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00129v2",
      "published_date": "2024-09-30 18:10:06 UTC",
      "updated_date": "2024-10-13 23:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:15:29.049748"
    },
    {
      "arxiv_id": "2409.20568v1",
      "title": "Continuously Improving Mobile Manipulation with Autonomous Real-World RL",
      "title_zh": "翻译失败",
      "authors": [
        "Russell Mendonca",
        "Emmanuel Panov",
        "Bernadette Bucher",
        "Jiuguang Wang",
        "Deepak Pathak"
      ],
      "abstract": "We present a fully autonomous real-world RL framework for mobile manipulation\nthat can learn policies without extensive instrumentation or human supervision.\nThis is enabled by 1) task-relevant autonomy, which guides exploration towards\nobject interactions and prevents stagnation near goal states, 2) efficient\npolicy learning by leveraging basic task knowledge in behavior priors, and 3)\nformulating generic rewards that combine human-interpretable semantic\ninformation with low-level, fine-grained observations. We demonstrate that our\napproach allows Spot robots to continually improve their performance on a set\nof four challenging mobile manipulation tasks, obtaining an average success\nrate of 80% across tasks, a 3-4 improvement over existing approaches. Videos\ncan be found at https://continual-mobile-manip.github.io/",
      "tldr_zh": "本研究提出了一种完全自治的真实世界强化学习（RL）框架，用于移动操控任务，能够在无需广泛仪器或人类监督的情况下学习策略。该框架通过任务相关自治（task-relevant autonomy）引导机器人探索物体互动并避免目标状态停滞、利用行为先验（behavior priors）实现高效策略学习，以及结合人类可解释的语义信息与低级观察制定通用奖励，从而提升性能。在实验中，Spot 机器人应用于四个挑战性任务，平均成功率达到80%，比现有方法提高了3-4倍。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2024. Website at https://continual-mobile-manip.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2409.20568v1",
      "published_date": "2024-09-30 17:59:50 UTC",
      "updated_date": "2024-09-30 17:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:15:41.247304"
    },
    {
      "arxiv_id": "2409.20560v2",
      "title": "LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaopan Zhang",
        "Hao Qin",
        "Fuquan Wang",
        "Yue Dong",
        "Jiachen Li"
      ],
      "abstract": "Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multiagent planners. The experimental videos, code, datasets, and\ndetailed prompts used in each module can be found on the project website:\nhttps://lamma-p.github.io.",
      "tldr_zh": "本研究提出 LaMMA-P，一种基于语言模型 (LMs) 驱动的 PDDL Planner，用于多智能体长期任务分配和规划，能够有效处理复杂子任务识别与异构机器人团队协作的问题。LaMMA-P 结合 LMs 的推理能力与传统启发式搜索规划器，实现高成功率、效率和任务泛化性能。实验在新建的 MAT-THOR 基准上显示，LaMMA-P 比现有 LM-based 多智能体规划器成功率提高 105% 且效率提升 36%，为机器人任务规划提供先进框架。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE Conference on Robotics and Automation (ICRA 2025); Project\n  website: https://lamma-p.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2409.20560v2",
      "published_date": "2024-09-30 17:58:18 UTC",
      "updated_date": "2025-03-13 06:17:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:15:53.821480"
    },
    {
      "arxiv_id": "2410.00086v2",
      "title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Han",
        "Zeyinzi Jiang",
        "Yulin Pan",
        "Jingfeng Zhang",
        "Chaojie Mao",
        "Chenwei Xie",
        "Yu Liu",
        "Jingren Zhou"
      ],
      "abstract": "Diffusion models have emerged as a powerful generative technology and have\nbeen found to be applicable in various scenarios. Most existing foundational\ndiffusion models are primarily designed for text-guided visual generation and\ndo not support multi-modal conditions, which are essential for many visual\nediting tasks. This limitation prevents these foundational diffusion models\nfrom serving as a unified model in the field of visual generation, like GPT-4\nin the natural language processing field. In this work, we propose ACE, an\nAll-round Creator and Editor, which achieves comparable performance compared to\nthose expert models in a wide range of visual generation tasks. To achieve this\ngoal, we first introduce a unified condition format termed Long-context\nCondition Unit (LCU), and propose a novel Transformer-based diffusion model\nthat uses LCU as input, aiming for joint training across various generation and\nediting tasks. Furthermore, we propose an efficient data collection approach to\naddress the issue of the absence of available training data. It involves\nacquiring pairwise images with synthesis-based or clustering-based pipelines\nand supplying these pairs with accurate textual instructions by leveraging a\nfine-tuned multi-modal large language model. To comprehensively evaluate the\nperformance of our model, we establish a benchmark of manually annotated pairs\ndata across a variety of visual generation tasks. The extensive experimental\nresults demonstrate the superiority of our model in visual generation fields.\nThanks to the all-in-one capabilities of our model, we can easily build a\nmulti-modal chat system that responds to any interactive request for image\ncreation using a single model to serve as the backend, avoiding the cumbersome\npipeline typically employed in visual agents. Code and models will be available\non the project page: https://ali-vilab.github.io/ace-page/.",
      "tldr_zh": "该研究提出ACE模型，一种基于Diffusion Transformer的全能创建和编辑系统，能够处理多种视觉生成任务，支持多模态条件，旨在弥补现有Diffusion models在文本引导生成方面的局限性。通过引入Long-context Condition Unit (LCU)作为统一输入格式，并设计Transformer-based diffusion model进行联合训练，ACE实现了对生成和编辑任务的全面优化，同时采用高效数据收集方法（如合成或聚类管道结合微调的多模态大语言模型）来解决训练数据缺失问题。实验结果显示，ACE在各种视觉生成基准上表现出色，与专家模型性能相当，并支持构建单一模型驱动的多模态聊天系统，简化了图像交互流程。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00086v2",
      "published_date": "2024-09-30 17:56:27 UTC",
      "updated_date": "2024-11-05 12:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:16:05.751817"
    },
    {
      "arxiv_id": "2409.20553v2",
      "title": "Maia-2: A Unified Model for Human-AI Alignment in Chess",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenwei Tang",
        "Difan Jiao",
        "Reid McIlroy-Young",
        "Jon Kleinberg",
        "Siddhartha Sen",
        "Ashton Anderson"
      ],
      "abstract": "There are an increasing number of domains in which artificial intelligence\n(AI) systems both surpass human ability and accurately model human behavior.\nThis introduces the possibility of algorithmically-informed teaching in these\ndomains through more relatable AI partners and deeper insights into human\ndecision-making. Critical to achieving this goal, however, is coherently\nmodeling human behavior at various skill levels. Chess is an ideal model system\nfor conducting research into this kind of human-AI alignment, with its rich\nhistory as a pivotal testbed for AI research, mature superhuman AI systems like\nAlphaZero, and precise measurements of skill via chess rating systems. Previous\nwork in modeling human decision-making in chess uses completely independent\nmodels to capture human style at different skill levels, meaning they lack\ncoherence in their ability to adapt to the full spectrum of human improvement\nand are ultimately limited in their effectiveness as AI partners and teaching\ntools. In this work, we propose a unified modeling approach for human-AI\nalignment in chess that coherently captures human style across different skill\nlevels and directly captures how people improve. Recognizing the complex,\nnon-linear nature of human learning, we introduce a skill-aware attention\nmechanism to dynamically integrate players' strengths with encoded chess\npositions, enabling our model to be sensitive to evolving player skill. Our\nexperimental results demonstrate that this unified framework significantly\nenhances the alignment between AI and human players across a diverse range of\nexpertise levels, paving the way for deeper insights into human decision-making\nand AI-guided teaching tools.",
      "tldr_zh": "该论文提出 Maia-2，一种统一的模型，用于在国际象棋(Chess)中实现人类-AI 协调(Human-AI Alignment)，以更好地模拟不同技能水平的人类行为并支持算法辅助教学。不同于以往独立模型的局限，该框架引入技能感知注意力机制(skill-aware attention mechanism)，动态整合玩家的优势和棋盘位置，从而捕捉人类进步的复杂非线性过程。实验结果表明，Maia-2 显著提升了 AI 与人类玩家的协调一致性，为深入理解人类决策和开发 AI 指导教学工具铺平了道路。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted @ NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20553v2",
      "published_date": "2024-09-30 17:54:23 UTC",
      "updated_date": "2024-10-31 23:29:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:16:17.632091"
    },
    {
      "arxiv_id": "2409.20550v2",
      "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyao Zhang",
        "Yanlin Wang",
        "Chong Wang",
        "Jiachi Chen",
        "Zibin Zheng"
      ],
      "abstract": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
      "tldr_zh": "本研究探讨了大型语言模型（LLM）在实际代码生成中的幻觉问题，包括现象、机制和缓解策略，重点关注仓库级生成场景中复杂上下文的依赖性。研究人员通过手动检查六种主流 LLM 的代码生成结果，建立了一个幻觉分类体系，并分析了幻觉在不同模型中的分布及四个潜在致因，如上下文处理不足。最终，他们提出了一种基于检索增强生成（RAG）的缓解方法，并在所有测试 LLM 中证明其有效性，为提升代码生成可靠性提供了实用指导。复制包可从 GitHub 获取。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ISSTA 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.20550v2",
      "published_date": "2024-09-30 17:51:15 UTC",
      "updated_date": "2025-01-17 01:44:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:16:29.281145"
    },
    {
      "arxiv_id": "2410.03731v3",
      "title": "Unsupervised Human Preference Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sumuk Shashidhar",
        "Abhinav Chinta",
        "Vaibhav Sahai",
        "Dilek Hakkani-Tür"
      ],
      "abstract": "Large language models demonstrate impressive reasoning abilities but struggle\nto provide personalized content due to their lack of individual user preference\ninformation. Existing methods, such as in-context learning and\nparameter-efficient fine-tuning, fall short in capturing the complexity of\nhuman preferences, especially given the small, personal datasets individuals\npossess. In this paper, we propose a novel approach utilizing small parameter\nmodels as preference agents to generate natural language rules that guide a\nlarger, pre-trained model, enabling efficient personalization. Our method\ninvolves a small, local \"steering wheel\" model that directs the outputs of a\nmuch larger foundation model, producing content tailored to an individual's\npreferences while leveraging the extensive knowledge and capabilities of the\nlarge model. Importantly, this personalization is achieved without the need to\nfine-tune the large model. Experimental results on email and article datasets,\ndemonstrate that our technique significantly outperforms baseline\npersonalization methods. By allowing foundation models to adapt to individual\npreferences in a data and compute-efficient manner, our approach paves the way\nfor highly personalized language model applications.",
      "tldr_zh": "这篇论文提出了一种Unsupervised Human Preference Learning方法，以解决大型语言模型（Large Language Models）在个性化内容生成方面的不足，因为它们缺乏用户偏好信息。论文利用小参数模型作为preference agents，生成自然语言规则来指导更大的预训练模型，从而实现高效个性化，而无需对大型模型进行parameter-efficient fine-tuning。实验结果显示，该方法在email和article数据集上显著优于基线方法，并为数据和计算高效的个性化语言模型应用铺平了道路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2410.03731v3",
      "published_date": "2024-09-30 17:51:01 UTC",
      "updated_date": "2024-10-11 18:53:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:16:41.857171"
    },
    {
      "arxiv_id": "2409.20548v2",
      "title": "Robi Butler: Multimodal Remote Interaction with a Household Robot Assistant",
      "title_zh": "翻译失败",
      "authors": [
        "Anxing Xiao",
        "Nuwan Janaka",
        "Tianrun Hu",
        "Anshul Gupta",
        "Kaixin Li",
        "Cunjun Yu",
        "David Hsu"
      ],
      "abstract": "Imagine a future when we can Zoom-call a robot to manage household chores\nremotely. This work takes one step in this direction. Robi Butler is a new\nhousehold robot assistant that enables seamless multimodal remote interaction.\nIt allows the human user to monitor its environment from a first-person view,\nissue voice or text commands, and specify target objects through hand-pointing\ngestures. At its core, a high-level behavior module, powered by Large Language\nModels (LLMs), interprets multimodal instructions to generate multistep action\nplans. Each plan consists of open-vocabulary primitives supported by\nvision-language models, enabling the robot to process both textual and gestural\ninputs. Zoom provides a convenient interface to implement remote interactions\nbetween the human and the robot. The integration of these components allows\nRobi Butler to ground remote multimodal instructions in real-world home\nenvironments in a zero-shot manner. We evaluated the system on various\nhousehold tasks, demonstrating its ability to execute complex user commands\nwith multimodal inputs. We also conducted a user study to examine how\nmultimodal interaction influences user experiences in remote human-robot\ninteraction. These results suggest that with the advances in robot foundation\nmodels, we are moving closer to the reality of remote household robot\nassistants.",
      "tldr_zh": "这篇论文介绍了Robi Butler，一种支持多模态远程交互的家庭机器人助手，允许用户通过Zoom监控环境、发出语音或文本命令，并用手势指定目标对象。系统核心采用Large Language Models (LLMs)驱动的高级行为模块，结合vision-language models生成多步行动计划，实现零-shot方式在真实家居环境中执行任务。实验评估显示，Robi Butler在各种家庭任务中表现出色，用户研究表明多模态交互显著提升了远程人机交互体验，推动了远程机器人助手的实际应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.20548v2",
      "published_date": "2024-09-30 17:49:09 UTC",
      "updated_date": "2025-03-10 06:00:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:16:53.414873"
    },
    {
      "arxiv_id": "2410.00083v1",
      "title": "A Survey on Diffusion Models for Inverse Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Giannis Daras",
        "Hyungjin Chung",
        "Chieh-Hsin Lai",
        "Yuki Mitsufuji",
        "Jong Chul Ye",
        "Peyman Milanfar",
        "Alexandros G. Dimakis",
        "Mauricio Delbracio"
      ],
      "abstract": "Diffusion models have become increasingly popular for generative modeling due\nto their ability to generate high-quality samples. This has unlocked exciting\nnew possibilities for solving inverse problems, especially in image restoration\nand reconstruction, by treating diffusion models as unsupervised priors. This\nsurvey provides a comprehensive overview of methods that utilize pre-trained\ndiffusion models to solve inverse problems without requiring further training.\nWe introduce taxonomies to categorize these methods based on both the problems\nthey address and the techniques they employ. We analyze the connections between\ndifferent approaches, offering insights into their practical implementation and\nhighlighting important considerations. We further discuss specific challenges\nand potential solutions associated with using latent diffusion models for\ninverse problems. This work aims to be a valuable resource for those interested\nin learning about the intersection of diffusion models and inverse problems.",
      "tldr_zh": "这篇调查综述了利用预训练 diffusion models 作为无监督先验来解决逆问题的方法，尤其聚焦于图像恢复和重建领域，而这些方法无需进一步训练。论文引入了基于问题类型和所用技巧的分类体系，并分析了不同方法的联系、实际实施要点以及关键考虑因素。同时，它探讨了应用潜在 diffusion models 面临的特定挑战及其潜在解决方案，为研究 diffusion models 与 inverse problems 交叉领域的学者提供了宝贵资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in progress. 38 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.00083v1",
      "published_date": "2024-09-30 17:34:01 UTC",
      "updated_date": "2024-09-30 17:34:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:17:05.156526"
    },
    {
      "arxiv_id": "2410.00082v1",
      "title": "Graph Residual Noise Learner Network for Brain Connectivity Graph Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Oytun Demirbilek",
        "Tingying Peng",
        "Alaa Bessadok"
      ],
      "abstract": "A morphological brain graph depicting a connectional fingerprint is of\nparamount importance for charting brain dysconnectivity patterns. Such data\noften has missing observations due to various reasons such as time-consuming\nand incomplete neuroimage processing pipelines. Thus, predicting a target brain\ngraph from a source graph is crucial for better diagnosing neurological\ndisorders with minimal data acquisition resources. Many brain graph generative\nmodels were proposed for promising results, yet they are mostly based on\ngenerative adversarial networks (GAN), which could suffer from mode collapse\nand require large training datasets. Recent developments in diffusion models\naddress these problems by offering essential properties such as a stable\ntraining objective and easy scalability. However, applying a diffusion process\nto graph edges fails to maintain the topological symmetry of the brain\nconnectivity matrices. To meet these challenges, we propose the Graph Residual\nNoise Learner Network (Grenol-Net), the first graph diffusion model for\npredicting a target graph from a source graph.",
      "tldr_zh": "本研究针对脑连接图数据缺失问题，提出了一种新的图扩散模型——Graph Residual Noise Learner Network (Grenol-Net)，用于从源脑图预测目标脑图，以辅助神经障碍诊断。与传统的生成对抗网络 (GAN) 相比，Grenol-Net 避免了模式崩溃和对大规模数据集的依赖，提供稳定的训练目标和易扩展性。该模型首次确保了脑连接矩阵的拓扑对称性，展示了在处理不完整神经影像数据时的潜力。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "10 pages, 3 figures, 6th Workshop on GRaphs in biomedicAl Image\n  anaLysis",
      "pdf_url": "http://arxiv.org/pdf/2410.00082v1",
      "published_date": "2024-09-30 17:28:38 UTC",
      "updated_date": "2024-09-30 17:28:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:17:17.372974"
    },
    {
      "arxiv_id": "2410.00081v3",
      "title": "From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent AI safety benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Roland Pihlakas",
        "Joel Pyykkö"
      ],
      "abstract": "Developing safe, aligned agentic AI systems requires comprehensive empirical\ntesting, yet many existing benchmarks neglect crucial themes aligned with\nbiology and economics, both time-tested fundamental sciences describing our\nneeds and preferences. To address this gap, the present work focuses on\nintroducing biologically and economically motivated themes that have been\nneglected in current mainstream discussions on AI safety - namely a set of\nmulti-objective, multi-agent alignment benchmarks that emphasize homeostasis\nfor bounded and biological objectives, diminishing returns for unbounded,\ninstrumental, and business objectives, sustainability principle, and resource\nsharing. We implemented eight main benchmark environments on the above themes,\nto illustrate key pitfalls and challenges in agentic AI-s, such as unboundedly\nmaximizing a homeostatic objective, over-optimizing one objective at the\nexpense of others, neglecting safety constraints, or depleting shared\nresources.",
      "tldr_zh": "该研究指出，现有的 AI 安全基准忽略了与生物学和经济学相关的关键主题，如 homeostasis（稳态）和 diminishing returns（收益递减）。作者引入了一套多目标多代理 AI 安全基准，这些基准强调 homeostasis 用于有限生物目标、diminishing returns 用于无限工具性或商业目标、sustainability principle（可持续性原则）以及 resource sharing（资源共享）。通过实施八个主要环境，论文展示了 AI 代理可能面临的陷阱，包括无限最大化稳态目标、过度优化单一目标而牺牲他人、忽略安全约束或耗尽共享资源，从而为更全面的 AI 安全测试提供新框架。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "20 pages, 13 figures, 1 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.00081v3",
      "published_date": "2024-09-30 17:24:21 UTC",
      "updated_date": "2025-03-04 16:42:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:17:29.314812"
    },
    {
      "arxiv_id": "2409.20524v1",
      "title": "Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource",
      "title_zh": "西班牙语中的词义消歧：一个全面的词汇评估资源",
      "authors": [
        "Pablo Ortega",
        "Jordi Luque",
        "Luis Lamiable",
        "Rodrigo López",
        "Richard Benjamins"
      ],
      "abstract": "Human language, while aimed at conveying meaning, inherently carries\nambiguity. It poses challenges for speech and language processing, but also\nserves crucial communicative functions. Efficiently solve ambiguity is both a\ndesired and a necessary characteristic. The lexical meaning of a word in\ncontext can be determined automatically by Word Sense Disambiguation (WSD)\nalgorithms that rely on external knowledge often limited and biased toward\nEnglish. When adapting content to other languages, automated translations are\nfrequently inaccurate and a high degree of expert human validation is necessary\nto ensure both accuracy and understanding. The current study addresses previous\nlimitations by introducing a new resource for Spanish WSD. It includes a sense\ninventory and a lexical dataset sourced from the Diccionario de la Lengua\nEspa\\~nola which is maintained by the Real Academia Espa\\~nola. We also review\ncurrent resources for Spanish and report metrics on them by a state-of-the-art\nsystem.",
      "tldr_zh": "这篇论文针对西班牙语的词义消歧（WSD）问题，引入了一个全面的词汇评估资源，以解决现有WSD算法依赖于有限且英语偏向的外部知识所带来的挑战。资源包括从Diccionario de la Lengua Española（由Real Academia Española维护）获取的语义库存和词汇数据集，支持更准确的上下文词义自动确定。作者回顾了当前的西班牙语WSD资源，并使用最先进系统对其进行评估，报告了相关指标，以提升跨语言翻译的准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.20524v1",
      "published_date": "2024-09-30 17:22:33 UTC",
      "updated_date": "2024-09-30 17:22:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:17:41.309241"
    },
    {
      "arxiv_id": "2409.20521v1",
      "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhishuai Liu",
        "Weixin Wang",
        "Pan Xu"
      ],
      "abstract": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$.",
      "tldr_zh": "该研究探讨了分布鲁棒离线强化学习（off-dynamics Reinforcement Learning），即训练和部署环境不同的场景，通过分布鲁棒马尔可夫决策过程（DRMDPs）框架来处理过渡动态的不确定性。研究提出了一种新算法 We-DRIVE-U，其平均次优性上界为 \\(\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min \\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)\\)，比现有方法改善了 \\(\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})\\)。此外，作者构建了一个新困难实例，并首次给出了信息理论下界，证明该算法在任何不确定性水平 \\(\\rho \\in (0,1]\\) 时近似最优，仅差 \\(\\mathcal{O}(\\sqrt{H})\\)。该算法采用“rare-switching”设计，大大提升计算效率，仅需 \\(\\mathcal{O}(dH\\log(1+H^2K))\\) 次策略切换和 \\(\\mathcal{O}(d^2H\\log(1+H^2K))\\) 次双优化问题求解器调用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "48 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.20521v1",
      "published_date": "2024-09-30 17:21:15 UTC",
      "updated_date": "2024-09-30 17:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:17:54.638000"
    },
    {
      "arxiv_id": "2409.20517v1",
      "title": "SMLE: Safe Machine Learning via Embedded Overapproximation",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Francobaldi",
        "Michele Lombardi"
      ],
      "abstract": "Despite the extent of recent advances in Machine Learning (ML) and Neural\nNetworks, providing formal guarantees on the behavior of these systems is still\nan open problem, and a crucial requirement for their adoption in regulated or\nsafety-critical scenarios. We consider the task of training differentiable ML\nmodels guaranteed to satisfy designer-chosen properties, stated as input-output\nimplications. This is very challenging, due to the computational complexity of\nrigorously verifying and enforcing compliance in modern neural models. We\nprovide an innovative approach based on three components: 1) a general, simple\narchitecture enabling efficient verification with a conservative semantic; 2) a\nrigorous training algorithm based on the Projected Gradient Method; 3) a\nformulation of the problem of searching for strong counterexamples. The\nproposed framework, being only marginally affected by model complexity, scales\nwell to practical applications, and produces models that provide full property\nsatisfaction guarantees. We evaluate our approach on properties defined by\nlinear inequalities in regression, and on mutually exclusive classes in\nmultilabel classification. Our approach is competitive with a baseline that\nincludes property enforcement during preprocessing, i.e. on the training data,\nas well as during postprocessing, i.e. on the model predictions. Finally, our\ncontributions establish a framework that opens up multiple research directions\nand potential improvements.",
      "tldr_zh": "这篇论文提出 SMLE 框架，通过嵌入式过度近似（Embedded Overapproximation）来确保机器学习（ML）模型满足设计者指定的输入-输出属性，从而解决 ML 系统在安全关键场景中的正式保证问题。框架的核心方法包括一个支持高效验证的通用架构、基于 Projected Gradient Method 的严格训练算法，以及搜索强反例的机制，这些组件使框架在模型复杂性较低的情况下具有良好的可扩展性。实验评估显示，SMLE 在回归任务的线性不等式属性和多标签分类的互斥类属性上表现出色，与基线方法（如预处理和后处理强制属性）竞争，并提供完整的属性满足保证。该框架为安全 ML 应用开辟了新的研究方向和改进潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20517v1",
      "published_date": "2024-09-30 17:19:57 UTC",
      "updated_date": "2024-09-30 17:19:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:18:06.344108"
    },
    {
      "arxiv_id": "2410.12807v1",
      "title": "A Hierarchical conv-LSTM and LLM Integrated Model for Holistic Stock Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Arya Chakraborty",
        "Auhona Basu"
      ],
      "abstract": "The financial domain presents a complex environment for stock market\nprediction, characterized by volatile patterns and the influence of\nmultifaceted data sources. Traditional models have leveraged either\nConvolutional Neural Networks (CNN) for spatial feature extraction or Long\nShort-Term Memory (LSTM) networks for capturing temporal dependencies, with\nlimited integration of external textual data. This paper proposes a novel\nTwo-Level Conv-LSTM Neural Network integrated with a Large Language Model (LLM)\nfor comprehensive stock advising. The model harnesses the strengths of\nConv-LSTM for analyzing time-series data and LLM for processing and\nunderstanding textual information from financial news, social media, and\nreports. In the first level, convolutional layers are employed to identify\nlocal patterns in historical stock prices and technical indicators, followed by\nLSTM layers to capture the temporal dynamics. The second level integrates the\noutput with an LLM that analyzes sentiment and contextual information from\ntextual data, providing a holistic view of market conditions. The combined\napproach aims to improve prediction accuracy and provide contextually rich\nstock advising.",
      "tldr_zh": "本文提出了一种层次化模型，将 Two-Level Conv-LSTM Neural Network 与 Large Language Model (LLM) 整合，用于全面的股票预测。该模型利用 Conv-LSTM 处理历史股票价格和技术指标的局部模式和时间动态，第一级通过卷积层提取特征，第二级结合 LSTM 捕捉序列依赖。LLM 则分析金融新闻、社会媒体和报告中的情感和上下文信息，提供整体市场洞察。该方法旨在提升预测准确性，并为股票决策提供更丰富的建议。",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.2.1"
      ],
      "primary_category": "q-fin.ST",
      "comment": "8 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.12807v1",
      "published_date": "2024-09-30 17:04:42 UTC",
      "updated_date": "2024-09-30 17:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:18:17.192621"
    },
    {
      "arxiv_id": "2409.20503v2",
      "title": "What Information Contributes to Log-based Anomaly Detection? Insights from a Configurable Transformer-Based Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Xingfang Wu",
        "Heng Li",
        "Foutse Khomh"
      ],
      "abstract": "Log data are generated from logging statements in the source code, providing\ninsights into the execution processes of software applications and systems.\nState-of-the-art log-based anomaly detection approaches typically leverage deep\nlearning models to capture the semantic or sequential information in the log\ndata and detect anomalous runtime behaviors. However, the impacts of these\ndifferent types of information are not clear. In addition, most existing\napproaches ignore the timestamps in log data, which can potentially provide\nfine-grained sequential and temporal information. In this work, we propose a\nconfigurable Transformer-based anomaly detection model that can capture the\nsemantic, sequential, and temporal information in the log data and allows us to\nconfigure the different types of information as the model's features.\nAdditionally, we train and evaluate the proposed model using log sequences of\ndifferent lengths, thus overcoming the constraint of existing methods that rely\non fixed-length or time-windowed log sequences as inputs. With the proposed\nmodel, we conduct a series of experiments with different combinations of input\nfeatures to evaluate the roles of different types of information in anomaly\ndetection. The model can attain competitive and consistently stable performance\ncompared to the baselines when presented with log sequences of varying lengths.\nThe results indicate that the event occurrence information plays a key role in\nidentifying anomalies, while the impact of the sequential and temporal\ninformation is not significant for anomaly detection on the studied public\ndatasets. On the other hand, the findings also reveal the simplicity of the\nstudied public datasets and highlight the importance of constructing new\ndatasets that contain different types of anomalies to better evaluate the\nperformance of anomaly detection models.",
      "tldr_zh": "本研究探讨了日志数据中语义、顺序和时间信息对异常检测的影响，提出一个可配置的Transformer-based异常检测模型，能够捕获这些信息并支持不同长度的日志序列作为输入。实验通过各种特征组合评估了模型性能，结果显示事件发生信息在识别异常中起关键作用，而顺序和时间信息的影响相对有限。作者强调，现有的公共数据集过于简单，建议构建更全面的新数据集以更好地评估异常检测模型的鲁棒性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "30 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.20503v2",
      "published_date": "2024-09-30 17:03:13 UTC",
      "updated_date": "2025-03-11 01:55:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:18:29.777277"
    },
    {
      "arxiv_id": "2409.20502v1",
      "title": "COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Divyanshu Daiya",
        "Damon Conover",
        "Aniket Bera"
      ],
      "abstract": "We propose a novel framework COLLAGE for generating collaborative\nagent-object-agent interactions by leveraging large language models (LLMs) and\nhierarchical motion-specific vector-quantized variational autoencoders\n(VQ-VAEs). Our model addresses the lack of rich datasets in this domain by\nincorporating the knowledge and reasoning abilities of LLMs to guide a\ngenerative diffusion model. The hierarchical VQ-VAE architecture captures\ndifferent motion-specific characteristics at multiple levels of abstraction,\navoiding redundant concepts and enabling efficient multi-resolution\nrepresentation. We introduce a diffusion model that operates in the latent\nspace and incorporates LLM-generated motion planning cues to guide the\ndenoising process, resulting in prompt-specific motion generation with greater\ncontrol and diversity. Experimental results on the CORE-4D, and InterHuman\ndatasets demonstrate the effectiveness of our approach in generating realistic\nand diverse collaborative human-object-human interactions, outperforming\nstate-of-the-art methods. Our work opens up new possibilities for modeling\ncomplex interactions in various domains, such as robotics, graphics and\ncomputer vision.",
      "tldr_zh": "本文提出 COLLAGE 框架，利用大型语言模型 (LLMs) 和分层运动特定向量量化变分自动编码器 (hierarchical VQ-VAEs) 生成协作式人类-代理互动，以解决数据缺乏问题。框架通过 LLMs 的知识和推理能力指导潜在空间的扩散模型，实现更可控、多样的运动生成，避免冗余概念并支持多分辨率表示。在 CORE-4D 和 InterHuman 数据集的实验中，COLLAGE 优于现有方法，生成更真实互动，并为机器人、图形学和计算机视觉等领域建模复杂互动开辟新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.20502v1",
      "published_date": "2024-09-30 17:02:13 UTC",
      "updated_date": "2024-09-30 17:02:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:18:42.274752"
    },
    {
      "arxiv_id": "2410.00079v1",
      "title": "Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyue Hua",
        "Mengting Wan",
        "Shashank Vadrevu",
        "Ryan Nadel",
        "Yongfeng Zhang",
        "Chi Wang"
      ],
      "abstract": "Agents, as user-centric tools, are increasingly deployed for human task\ndelegation, assisting with a broad spectrum of requests by generating thoughts,\nengaging with user proxies, and producing action plans. However, agents based\non large language models (LLMs) often face substantial planning latency due to\ntwo primary factors: the efficiency limitations of the underlying LLMs due to\ntheir large size and high demand, and the structural complexity of the agents\ndue to the extensive generation of intermediate thoughts to produce the final\noutput. Given that inefficiency in service provision can undermine the value of\nautomation for users, this paper presents a human-centered efficient agent\nplanning method -- Interactive Speculative Planning -- aiming at enhancing the\nefficiency of agent planning through both system design and human-AI\ninteraction. Our approach advocates for the co-design of the agent system and\nuser interface, underscoring the importance of an agent system that can fluidly\nmanage user interactions and interruptions. By integrating human interruptions\nas a fundamental component of the system, we not only make it more user-centric\nbut also expedite the entire process by leveraging human-in-the-loop\ninteractions to provide accurate intermediate steps. Code and data will be\nreleased.",
      "tldr_zh": "这篇论文提出 Interactive Speculative Planning 方法，以解决基于大型语言模型（LLMs）的 Agents 在任务规划中面临的延迟问题，主要因 LLMs 效率低下和代理结构复杂而导致。方法通过系统和用户界面的共同设计（co-design），强调代理系统灵活处理用户互动和中断，将人类参与作为核心组件来加速中间步骤的生成。最终，该方法提升了 Agents 的整体效率，使其更用户中心，并计划发布代码和数据以支持进一步验证。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "27 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.00079v1",
      "published_date": "2024-09-30 16:52:51 UTC",
      "updated_date": "2024-09-30 16:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:18:54.176118"
    },
    {
      "arxiv_id": "2409.20483v1",
      "title": "RecSys Challenge 2024: Balancing Accuracy and Editorial Values in News Recommendations",
      "title_zh": "RecSys Challenge 2024：新闻推荐中准确性与编辑价值观的平衡",
      "authors": [
        "Johannes Kruse",
        "Kasper Lindskow",
        "Saikishore Kalloori",
        "Marco Polignano",
        "Claudio Pomo",
        "Abhishek Srivastava",
        "Anshuk Uppal",
        "Michael Riis Andersen",
        "Jes Frellsen"
      ],
      "abstract": "The RecSys Challenge 2024 aims to advance news recommendation by addressing\nboth the technical and normative challenges inherent in designing effective and\nresponsible recommender systems for news publishing. This paper describes the\nchallenge, including its objectives, problem setting, and the dataset provided\nby the Danish news publishers Ekstra Bladet and JP/Politikens Media Group\n(\"Ekstra Bladet\"). The challenge explores the unique aspects of news\nrecommendation, such as modeling user preferences based on behavior, accounting\nfor the influence of the news agenda on user interests, and managing the rapid\ndecay of news items. Additionally, the challenge embraces normative\ncomplexities, investigating the effects of recommender systems on news flow and\ntheir alignment with editorial values. We summarize the challenge setup,\ndataset characteristics, and evaluation metrics. Finally, we announce the\nwinners and highlight their contributions. The dataset is available at:\nhttps://recsys.eb.dk.",
      "tldr_zh": "RecSys Challenge 2024旨在推进新闻推荐系统的发展，同时平衡准确性和编辑价值观，解决技术与规范挑战。挑战聚焦于建模用户偏好、考虑新闻议程对用户兴趣的影响，以及管理新闻项目的快速衰减等问题，并调查推荐系统对新闻流和编辑价值观的潜在影响。提供的数据集来自丹麦新闻出版商Ekstra Bladet和JP/Politikens Media Group，用于评估参与者的解决方案。最终，挑战公布了赢家及其贡献，数据集可从https://recsys.eb.dk获取。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "5 pages, 3 tables, RecSys' 24",
      "pdf_url": "http://arxiv.org/pdf/2409.20483v1",
      "published_date": "2024-09-30 16:42:57 UTC",
      "updated_date": "2024-09-30 16:42:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:19:06.053928"
    },
    {
      "arxiv_id": "2410.12806v1",
      "title": "Interpretable Rule-Based System for Radar-Based Gesture Sensing: Enhancing Transparency and Personalization in AI",
      "title_zh": "翻译失败",
      "authors": [
        "Sarah Seifi",
        "Tobias Sukianto",
        "Cecilia Carbonelli",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "abstract": "The increasing demand in artificial intelligence (AI) for models that are\nboth effective and explainable is critical in domains where safety and trust\nare paramount. In this study, we introduce MIRA, a transparent and\ninterpretable multi-class rule-based algorithm tailored for radar-based gesture\ndetection. Addressing the critical need for understandable AI, MIRA enhances\nuser trust by providing insight into its decision-making process. We showcase\nthe system's adaptability through personalized rule sets that calibrate to\nindividual user behavior, offering a user-centric AI experience. Alongside\npresenting a novel multi-class classification architecture, we share an\nextensive frequency-modulated continuous wave radar gesture dataset and\nevidence of the superior interpretability of our system through comparative\nanalyses. Our research underscores MIRA's ability to deliver both high\ninterpretability and performance and emphasizes the potential for broader\nadoption of interpretable AI in safety-critical applications.",
      "tldr_zh": "本研究引入了 MIRA，一种透明且可解释的规则-based 算法，用于雷达-based 手势检测，旨在提升 AI 在安全和信任关键领域的透明度和个性化。MIRA 通过个性化规则集适应用户行为，提供用户-centric 的决策过程洞察，并提出一个新颖的多类分类架构，同时共享一个广泛的频率调制连续波 (FMCW) 雷达手势数据集。实验结果显示，MIRA 在可解释性和性能上表现出色，通过比较分析证明其适用于安全关键应用。",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted at the 21st European Radar Conference, 4 pages, 2 figure",
      "pdf_url": "http://arxiv.org/pdf/2410.12806v1",
      "published_date": "2024-09-30 16:40:27 UTC",
      "updated_date": "2024-09-30 16:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:19:17.586755"
    },
    {
      "arxiv_id": "2409.20467v1",
      "title": "A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media",
      "title_zh": "越南语社交媒体机器词汇规范化的弱监督数据标注框架",
      "authors": [
        "Dung Ha Nguyen",
        "Anh Thi Hoang Nguyen",
        "Kiet Van Nguyen"
      ],
      "abstract": "This study introduces an innovative automatic labeling framework to address\nthe challenges of lexical normalization in social media texts for low-resource\nlanguages like Vietnamese. Social media data is rich and diverse, but the\nevolving and varied language used in these contexts makes manual labeling\nlabor-intensive and expensive. To tackle these issues, we propose a framework\nthat integrates semi-supervised learning with weak supervision techniques. This\napproach enhances the quality of training dataset and expands its size while\nminimizing manual labeling efforts. Our framework automatically labels raw\ndata, converting non-standard vocabulary into standardized forms, thereby\nimproving the accuracy and consistency of the training data. Experimental\nresults demonstrate the effectiveness of our weak supervision framework in\nnormalizing Vietnamese text, especially when utilizing Pre-trained Language\nModels. The proposed framework achieves an impressive F1-score of 82.72% and\nmaintains vocabulary integrity with an accuracy of up to 99.22%. Additionally,\nit effectively handles undiacritized text under various conditions. This\nframework significantly enhances natural language normalization quality and\nimproves the accuracy of various NLP tasks, leading to an average accuracy\nincrease of 1-3%.",
      "tldr_zh": "这篇论文提出一个弱监督数据标注框架，用于解决低资源语言如越南语社交媒体文本的词汇规范化挑战。框架整合半监督学习和弱监督技术，自动标注原始数据，将非标准词汇转换为标准化形式，从而提高训练数据集的质量和规模，同时减少手动标注的劳动强度。实验结果显示，该框架在利用 Pre-trained Language Models 的条件下，实现了 82.72% 的 F1-score 和 99.22% 的词汇完整性准确率，并有效处理无重音文本，导致各种 NLP 任务的准确率平均提升 1-3%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20467v1",
      "published_date": "2024-09-30 16:26:40 UTC",
      "updated_date": "2024-09-30 16:26:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:19:29.818557"
    },
    {
      "arxiv_id": "2409.20449v2",
      "title": "Linear Projections of Teacher Embeddings for Few-Class Distillation",
      "title_zh": "针对少类知识蒸馏的教师嵌入线性投影",
      "authors": [
        "Noel Loo",
        "Fotis Iliopoulos",
        "Wei Hu",
        "Erik Vee"
      ],
      "abstract": "Knowledge Distillation (KD) has emerged as a promising approach for\ntransferring knowledge from a larger, more complex teacher model to a smaller\nstudent model. Traditionally, KD involves training the student to mimic the\nteacher's output probabilities, while more advanced techniques have explored\nguiding the student to adopt the teacher's internal representations. Despite\nits widespread success, the performance of KD in binary classification and\nfew-class problems has been less satisfactory. This is because the information\nabout the teacher model's generalization patterns scales directly with the\nnumber of classes. Moreover, several sophisticated distillation methods may not\nbe universally applicable or effective for data types beyond Computer Vision.\nConsequently, effective distillation techniques remain elusive for a range of\nkey real-world applications, such as sentiment analysis, search query\nunderstanding, and advertisement-query relevance assessment. Taking these\nobservations into account, we introduce a novel method for distilling knowledge\nfrom the teacher's model representations, which we term Learning Embedding\nLinear Projections (LELP). Inspired by recent findings about the structure of\nfinal-layer representations, LELP works by identifying informative linear\nsubspaces in the teacher's embedding space, and splitting them into\npseudo-subclasses. The student model is then trained to replicate these\npseudo-classes. Our experimental evaluation on large-scale NLP benchmarks like\nAmazon Reviews and Sentiment140 demonstrate the LELP is consistently\ncompetitive with, and typically superior to, existing state-of-the-art\ndistillation algorithms for binary and few-class problems, where most KD\nmethods suffer.",
      "tldr_zh": "该研究针对知识蒸馏 (Knowledge Distillation, KD) 在二分类和少类问题上的表现不佳问题，提出了一种新方法：Learning Embedding Linear Projections (LELP)。LELP 通过识别 teacher 模型嵌入空间中的信息线性子空间，并将其分割成伪子类 (pseudo-subclasses)，然后训练 student 模型复制这些伪子类，从而更有效地转移 teacher 的泛化模式。实验结果显示，在 Amazon Reviews 和 Sentiment140 等大型 NLP 基准上，LELP 比现有最先进算法更具竞争力，尤其在二分类和少类任务中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20449v2",
      "published_date": "2024-09-30 16:07:34 UTC",
      "updated_date": "2024-10-02 02:36:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:19:41.967154"
    },
    {
      "arxiv_id": "2410.03730v2",
      "title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Mehdi Ali",
        "Michael Fromm",
        "Klaudia Thellmann",
        "Jan Ebert",
        "Alexander Arno Weber",
        "Richard Rutmann",
        "Charvi Jain",
        "Max Lübbering",
        "Daniel Steinigen",
        "Johannes Leveling",
        "Katrin Klug",
        "Jasper Schulze Buschhoff",
        "Lena Jurkschat",
        "Hammam Abdelwahab",
        "Benny Jörg Stein",
        "Karl-Heinz Sylla",
        "Pavel Denisov",
        "Nicolo' Brandizzi",
        "Qasid Saleem",
        "Anirban Bhowmick",
        "Lennard Helmer",
        "Chelsea John",
        "Pedro Ortiz Suarez",
        "Malte Ostendorff",
        "Alex Jude",
        "Lalith Manjunath",
        "Samuel Weinbach",
        "Carolin Penke",
        "Oleg Filatov",
        "Shima Asaadi",
        "Fabio Barth",
        "Rafet Sifa",
        "Fabian Küch",
        "Andreas Herten",
        "René Jäkel",
        "Georg Rehm",
        "Stefan Kesselheim",
        "Joachim Köhler",
        "Nicolas Flores-Herr"
      ],
      "abstract": "We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA.",
      "tldr_zh": "我们介绍了Teuken-7B-Base和Teuken-7B-Instruct两个多语言LLM模型，旨在支持欧盟的24种官方语言，从而解决现有LLM过度关注英语或其他高资源语言的局限性。这些模型使用约60%非英语数据的训练集、自定义多语言tokenizer，以及优化数据组成、tokenizer优化和训练方法的开发原则进行训练。在多语言基准测试中，它们表现出色，例如在欧洲版本的ARC、HellaSwag、MMLU和TruthfulQA上实现了竞争性性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03730v2",
      "published_date": "2024-09-30 16:05:38 UTC",
      "updated_date": "2024-10-15 17:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:19:53.537822"
    },
    {
      "arxiv_id": "2409.20447v1",
      "title": "POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator",
      "title_zh": "翻译失败",
      "authors": [
        "Eugenio Lomurno",
        "Samuele Mariani",
        "Matteo Monti",
        "Matteo Matteucci"
      ],
      "abstract": "Neural Architecture Search (NAS) automates neural network design, reducing\ndependence on human expertise. While NAS methods are computationally intensive\nand dataset-specific, auxiliary predictors reduce the models needing training,\ndecreasing search time. This strategy is used to generate architectures\nsatisfying multiple computational constraints. Recently, Transferable NAS has\nemerged, generalizing the search process from dataset-dependent to\ntask-dependent. In this field, DiffusionNAG is a state-of-the-art method. This\ndiffusion-based approach streamlines computation, generating architectures\noptimized for accuracy on unseen datasets without further adaptation. However,\nby focusing solely on accuracy, DiffusionNAG overlooks other crucial objectives\nlike model complexity, computational efficiency, and inference latency --\nfactors essential for deploying models in resource-constrained environments.\nThis paper introduces the Pareto-Optimal Many-Objective Neural Architecture\nGenerator (POMONAG), extending DiffusionNAG via a many-objective diffusion\nprocess. POMONAG simultaneously considers accuracy, number of parameters,\nmultiply-accumulate operations (MACs), and inference latency. It integrates\nPerformance Predictor models to estimate these metrics and guide diffusion\ngradients. POMONAG's optimization is enhanced by expanding its training\nMeta-Dataset, applying Pareto Front Filtering, and refining embeddings for\nconditional generation. These enhancements enable POMONAG to generate\nPareto-optimal architectures that outperform the previous state-of-the-art in\nperformance and efficiency. Results were validated on two search spaces --\nNASBench201 and MobileNetV3 -- and evaluated across 15 image classification\ndatasets.",
      "tldr_zh": "本文提出 POMONAG，一种 Pareto-Optimal 多目标神经架构生成器（POMONAG），扩展了 DiffusionNAG 方法，通过多目标扩散过程同时优化准确性、参数数量、MACs（乘-加运算）和推断延迟，以解决现有 NAS 方法忽略资源约束的问题。POMONAG 整合性能预测模型来估计这些指标并指导扩散梯度，并通过扩展训练 Meta-Dataset、应用 Pareto Front Filtering 和优化嵌入来提升生成效率。实验在 NASBench201 和 MobileNetV3 搜索空间上验证，结果显示 POMONAG 生成的 Pareto 最优架构在 15 个图像分类数据集上，显著优于现有方法的性能和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20447v1",
      "published_date": "2024-09-30 16:05:29 UTC",
      "updated_date": "2024-09-30 16:05:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:20:09.159643"
    },
    {
      "arxiv_id": "2409.20427v2",
      "title": "Sufficient and Necessary Explanations (and What Lies in Between)",
      "title_zh": "充分与必要解释（以及其间的内容）",
      "authors": [
        "Beepul Bharti",
        "Paul Yi",
        "Jeremias Sulam"
      ],
      "abstract": "As complex machine learning models continue to find applications in\nhigh-stakes decision-making scenarios, it is crucial that we can explain and\nunderstand their predictions. Post-hoc explanation methods provide useful\ninsights by identifying important features in an input $\\mathbf{x}$ with\nrespect to the model output $f(\\mathbf{x})$. In this work, we formalize and\nstudy two precise notions of feature importance for general machine learning\nmodels: sufficiency and necessity. We demonstrate how these two types of\nexplanations, albeit intuitive and simple, can fall short in providing a\ncomplete picture of which features a model finds important. To this end, we\npropose a unified notion of importance that circumvents these limitations by\nexploring a continuum along a necessity-sufficiency axis. Our unified notion,\nwe show, has strong ties to other popular definitions of feature importance,\nlike those based on conditional independence and game-theoretic quantities like\nShapley values. Crucially, we demonstrate how a unified perspective allows us\nto detect important features that could be missed by either of the previous\napproaches alone.",
      "tldr_zh": "该论文探讨了解释复杂机器学习模型预测的重要性，特别是通过后验解释方法识别输入特征对输出$f(\\mathbf{x})$的影响。作者形式化了两个关键概念——sufficiency（充分性）和necessity（必要性）——来定义特征重要性，但指出这些方法可能无法提供完整图片。为此，他们提出一个统一的importance概念，探索sufficiency和necessity之间的连续体，并证明其与条件独立和Shapley values等现有定义密切相关。最终，该统一视角能检测到单一方法可能忽略的重要特征，从而提升模型解释的全面性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20427v2",
      "published_date": "2024-09-30 15:50:57 UTC",
      "updated_date": "2024-10-15 14:04:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:20:16.698033"
    },
    {
      "arxiv_id": "2409.20424v1",
      "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Jiacong Wang",
        "Bohong Wu",
        "Haiyong Jiang",
        "Xun Zhou",
        "Xin Xiao",
        "Haoyuan Guo",
        "Jun Xiao"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of\nhigh-quality multi-modal alignment data have inspired numerous researches on\nsynthetic VLM data generation. The conventional norm in VLM data construction\nuses a mixture of specialists in caption and OCR, or stronger VLM APIs and\nexpensive human annotation. In this paper, we present World to Code (W2C), a\nmeticulously curated multi-modal data construction pipeline that organizes the\nfinal generation output into a Python code format. The pipeline leverages the\nVLM itself to extract cross-modal information via different prompts and filter\nthe generated outputs again via a consistency filtering strategy. Experiments\nhave demonstrated the high quality of W2C by improving various existing visual\nquestion answering and visual grounding benchmarks across different VLMs.\nFurther analysis also demonstrates that the new code parsing ability of VLMs\npresents better cross-modal equivalence than the commonly used detail caption\nability. Our code is available at\nhttps://github.com/foundation-multimodal-models/World2Code.",
      "tldr_zh": "该论文提出了一种名为 World to Code (W2C) 的多模态数据生成管道，通过自指令式组合描述(Self-Instructed Compositional Captioning)和过滤策略，利用 Vision-Language Models (VLMs) 本身提取跨模态信息，并将生成输出组织成 Python 代码格式，以解决高品质多模态对齐数据稀缺的问题。W2C 方法避免了依赖外部专家或昂贵的人工标注，而是通过不同提示和一致性过滤策略来提升数据质量。实验结果显示，W2C 显著改善了现有 Visual Question Answering 和 Visual Grounding 基准的表现，并在不同 VLMs 上取得了提升。进一步分析表明，VLMs 的代码解析能力比传统详细描述能力提供了更好的跨模态等价性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at EMNLP 2024 Main Conference, 16pages",
      "pdf_url": "http://arxiv.org/pdf/2409.20424v1",
      "published_date": "2024-09-30 15:49:54 UTC",
      "updated_date": "2024-09-30 15:49:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:20:39.505582"
    },
    {
      "arxiv_id": "2409.20423v5",
      "title": "Stream-level flow matching with Gaussian processes",
      "title_zh": "翻译失败",
      "authors": [
        "Ganchao Wei",
        "Li Ma"
      ],
      "abstract": "Flow matching (FM) is a family of training algorithms for fitting continuous\nnormalizing flows (CNFs). Conditional flow matching (CFM) exploits the fact\nthat the marginal vector field of a CNF can be learned by fitting least-squares\nregression to the conditional vector field specified given one or both ends of\nthe flow path. In this paper, we extend the CFM algorithm by defining\nconditional probability paths along ``streams'', instances of latent stochastic\npaths that connect data pairs of source and target, which are modeled with\nGaussian process (GP) distributions. The unique distributional properties of\nGPs help preserve the ``simulation-free\" nature of CFM training. We show that\nthis generalization of the CFM can effectively reduce the variance in the\nestimated marginal vector field at a moderate computational cost, thereby\nimproving the quality of the generated samples under common metrics.\nAdditionally, adopting the GP on the streams allows for flexibly linking\nmultiple correlated training data points (e.g., time series). We empirically\nvalidate our claim through both simulations and applications to image and\nneural time series data.",
      "tldr_zh": "这篇论文扩展了 Conditional Flow Matching (CFM) 算法，通过使用 Gaussian Processes (GP) 来建模连接源和目标数据对的“streams”，从而定义条件概率路径。GP 的分布特性有助于减少估计边缘向量场的方差，同时保持 CFM 的“无模拟”训练性质，并以适中的计算成本提升生成样本的质量。此外，该方法允许灵活链接多个相关训练数据点，如时间序列，并通过模拟及图像和神经时间序列数据的实证应用验证了其有效性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20423v5",
      "published_date": "2024-09-30 15:47:22 UTC",
      "updated_date": "2025-02-03 14:31:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:20:41.281096"
    },
    {
      "arxiv_id": "2409.20412v1",
      "title": "Conformal Prediction for Dose-Response Models with Continuous Treatments",
      "title_zh": "翻译失败",
      "authors": [
        "Jarne Verhaeghe",
        "Jef Jonkers",
        "Sofie Van Hoecke"
      ],
      "abstract": "Understanding the dose-response relation between a continuous treatment and\nthe outcome for an individual can greatly drive decision-making, particularly\nin areas like personalized drug dosing and personalized healthcare\ninterventions. Point estimates are often insufficient in these high-risk\nenvironments, highlighting the need for uncertainty quantification to support\ninformed decisions. Conformal prediction, a distribution-free and\nmodel-agnostic method for uncertainty quantification, has seen limited\napplication in continuous treatments or dose-response models. To address this\ngap, we propose a novel methodology that frames the causal dose-response\nproblem as a covariate shift, leveraging weighted conformal prediction. By\nincorporating propensity estimation, conformal predictive systems, and\nlikelihood ratios, we present a practical solution for generating prediction\nintervals for dose-response models. Additionally, our method approximates local\ncoverage for every treatment value by applying kernel functions as weights in\nweighted conformal prediction. Finally, we use a new synthetic benchmark\ndataset to demonstrate the significance of covariate shift assumptions in\nachieving robust prediction intervals for dose-response models.",
      "tldr_zh": "这篇论文针对连续治疗的剂量-反应模型（Dose-Response Models），强调不确定性量化（Uncertainty Quantification）在个性化医疗决策中的重要性，以弥补现有方法的不足。作者提出一种新方法，将因果剂量-反应问题框架化为协变量偏移（Covariate Shift），并利用加权共形预测（Weighted Conformal Prediction）结合倾向性估计（Propensity Estimation）、共形预测系统（Conformal Predictive Systems）和似然比（Likelihood Ratios）来生成预测区间。方法通过核函数（Kernel Functions）作为权重，实现每个治疗值的局部覆盖（Local Coverage）。实验使用一个新的合成基准数据集，证明了协变量偏移假设在构建鲁棒预测区间方面的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages main text, 8 pages references and appendix",
      "pdf_url": "http://arxiv.org/pdf/2409.20412v1",
      "published_date": "2024-09-30 15:40:54 UTC",
      "updated_date": "2024-09-30 15:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:20:54.559251"
    },
    {
      "arxiv_id": "2409.20398v2",
      "title": "AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation",
      "title_zh": "AUCSeg：面向 AUC 的像素级长尾语义分割",
      "authors": [
        "Boyu Han",
        "Qianqian Xu",
        "Zhiyong Yang",
        "Shilong Bao",
        "Peisong Wen",
        "Yangbangyan Jiang",
        "Qingming Huang"
      ],
      "abstract": "The Area Under the ROC Curve (AUC) is a well-known metric for evaluating\ninstance-level long-tail learning problems. In the past two decades, many AUC\noptimization methods have been proposed to improve model performance under\nlong-tail distributions. In this paper, we explore AUC optimization methods in\nthe context of pixel-level long-tail semantic segmentation, a much more\ncomplicated scenario. This task introduces two major challenges for AUC\noptimization techniques. On one hand, AUC optimization in a pixel-level task\ninvolves complex coupling across loss terms, with structured inner-image and\npairwise inter-image dependencies, complicating theoretical analysis. On the\nother hand, we find that mini-batch estimation of AUC loss in this case\nrequires a larger batch size, resulting in an unaffordable space complexity. To\naddress these issues, we develop a pixel-level AUC loss function and conduct a\ndependency-graph-based theoretical analysis of the algorithm's generalization\nability. Additionally, we design a Tail-Classes Memory Bank (T-Memory Bank) to\nmanage the significant memory demand. Finally, comprehensive experiments across\nvarious benchmarks confirm the effectiveness of our proposed AUCSeg method. The\ncode is available at https://github.com/boyuh/AUCSeg.",
      "tldr_zh": "本文提出 AUCSeg 方法，针对像素级长尾语义分割任务优化 AUC（Area Under the ROC Curve）指标，以应对长尾分布下的模型性能挑战。该方法开发了像素级 AUC 损失函数，并通过基于依赖图的理论分析解决了损失项耦合和图像间依赖的复杂性，同时设计了 Tail-Classes Memory Bank 来缓解 mini-batch 估计所需的巨大内存需求。在多个基准实验中，AUCSeg 显著提升了模型的泛化能力和性能，有效验证了其在处理像素级长尾问题上的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20398v2",
      "published_date": "2024-09-30 15:31:02 UTC",
      "updated_date": "2024-10-10 13:31:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:21:05.778339"
    },
    {
      "arxiv_id": "2409.20371v1",
      "title": "Frequency Adaptive Normalization For Non-stationary Time Series Forecasting",
      "title_zh": "非",
      "authors": [
        "Weiwei Ye",
        "Songgaojun Deng",
        "Qiaosha Zou",
        "Ning Gui"
      ],
      "abstract": "Time series forecasting typically needs to address non-stationary data with\nevolving trend and seasonal patterns. To address the non-stationarity,\nreversible instance normalization has been recently proposed to alleviate\nimpacts from the trend with certain statistical measures, e.g., mean and\nvariance. Although they demonstrate improved predictive accuracy, they are\nlimited to expressing basic trends and are incapable of handling seasonal\npatterns. To address this limitation, this paper proposes a new instance\nnormalization solution, called frequency adaptive normalization (FAN), which\nextends instance normalization in handling both dynamic trend and seasonal\npatterns. Specifically, we employ the Fourier transform to identify\ninstance-wise predominant frequent components that cover most non-stationary\nfactors. Furthermore, the discrepancy of those frequency components between\ninputs and outputs is explicitly modeled as a prediction task with a simple MLP\nmodel. FAN is a model-agnostic method that can be applied to arbitrary\npredictive backbones. We instantiate FAN on four widely used forecasting models\nas the backbone and evaluate their prediction performance improvements on eight\nbenchmark datasets. FAN demonstrates significant performance advancement,\nachieving 7.76% ~ 37.90% average improvements in MSE.",
      "tldr_zh": "本文针对时间序列预测中的非平稳数据问题，提出了一种新的实例归一化方法：Frequency Adaptive Normalization (FAN)，旨在同时处理动态趋势和季节性模式。FAN 通过 Fourier transform 识别实例-wise 的主要频率组件，并使用简单的 MLP 模型来预测输入和输出之间频率差异的差异，从而扩展了传统实例归一化的能力。该方法是模型无关的，可应用于任意预测骨干，并在四个常用模型和八个基准数据集上实现了 MSE 的平均改善，幅度为 7.76% ~ 37.90%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 Poster",
      "pdf_url": "http://arxiv.org/pdf/2409.20371v1",
      "published_date": "2024-09-30 15:07:16 UTC",
      "updated_date": "2024-09-30 15:07:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:21:18.097575"
    },
    {
      "arxiv_id": "2409.20370v1",
      "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
      "title_zh": "完美的混合：通过 Mixture of Judges 重新定义 RLHF",
      "authors": [
        "Tengyu Xu",
        "Eryk Helenowski",
        "Karthik Abinav Sankararaman",
        "Di Jin",
        "Kaiyan Peng",
        "Eric Han",
        "Shaoliang Nie",
        "Chen Zhu",
        "Hejia Zhang",
        "Wenxuan Zhou",
        "Zhouhao Zeng",
        "Yun He",
        "Karishma Mandyam",
        "Arya Talabzadeh",
        "Madian Khabsa",
        "Gabriel Cohen",
        "Yuandong Tian",
        "Hao Ma",
        "Sinong Wang",
        "Han Fang"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications.",
      "tldr_zh": "这篇论文重新定义了强化学习从人类反馈（RLHF）方法，通过引入 Constrained Generative Policy Optimization (CGPO) 和 Mixture of Judges (MoJ)，来解决多任务学习（MTL）中的奖励黑客（reward hacking）和极端多目标优化挑战。CGPO 的核心是 MoJ 结合成本有效的约束策略优化和分层方法，能够在原则性基础上实现最优权衡，而无需大量超参数调整，并轻松集成到现有后训练管道中。实验结果显示，CGPO 在各种任务中显著优于标准 RLHF 算法如 PPO 和 DPO，具体包括 AlpacaEval-2（一般聊天）提高 7.4%、Arena-Hard（STEM 和推理）提高 12.5%，并成功缓解了 PPO 在编码基准中的奖励黑客问题。这一创新为对齐通用大型语言模型（LLMs）提供了新的突破，推动了其在多样应用中的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "submitted to conference",
      "pdf_url": "http://arxiv.org/pdf/2409.20370v1",
      "published_date": "2024-09-30 15:06:53 UTC",
      "updated_date": "2024-09-30 15:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:21:32.028265"
    },
    {
      "arxiv_id": "2409.20364v1",
      "title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhou Huang",
        "Yihua Cheng",
        "Kezhi Wang"
      ],
      "abstract": "Deep learning architectures with powerful reasoning capabilities have driven\nsignificant advancements in autonomous driving technology. Large language\nmodels (LLMs) applied in this field can describe driving scenes and behaviors\nwith a level of accuracy similar to human perception, particularly in visual\ntasks. Meanwhile, the rapid development of edge computing, with its advantage\nof proximity to data sources, has made edge devices increasingly important in\nautonomous driving. Edge devices process data locally, reducing transmission\ndelays and bandwidth usage, and achieving faster response times. In this work,\nwe propose a driving behavior narration and reasoning framework that applies\nLLMs to edge devices. The framework consists of multiple roadside units, with\nLLMs deployed on each unit. These roadside units collect road data and\ncommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on\nedge devices can achieve satisfactory response speeds. Additionally, we propose\na prompt strategy to enhance the narration and reasoning performance of the\nsystem. This strategy integrates multi-modal information, including\nenvironmental, agent, and motion data. Experiments conducted on the\nOpenDV-Youtube dataset demonstrate that our approach significantly improves\nperformance across both tasks.",
      "tldr_zh": "该研究提出了一种高效的驾驶行为叙述和推理框架，利用大语言模型(LLMs)在边缘设备上进行本地处理，以减少传输延迟并提升响应速度。该框架由多个路侧单位组成，这些单位部署LLMs，通过5G NSR/NR网络收集和通信道路数据，并采用一种提示策略整合多模态信息（如环境、代理和运动数据）。实验在OpenDV-Youtube数据集上显示，该方法显著提高了叙述和推理性能，证明了LLMs在边缘设备上的实用性。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted for possible journal publication",
      "pdf_url": "http://arxiv.org/pdf/2409.20364v1",
      "published_date": "2024-09-30 15:03:55 UTC",
      "updated_date": "2024-09-30 15:03:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:21:41.218828"
    },
    {
      "arxiv_id": "2409.20361v2",
      "title": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Yi",
        "Zengke Liu",
        "Jianwei Zhang",
        "Chengyuan Li",
        "Tong Zhang",
        "Junyang Lin",
        "Jingren Zhou"
      ],
      "abstract": "Large language models have demonstrated promising capabilities upon scaling\nup parameters. However, serving large language models incurs substantial\ncomputation and memory movement costs due to their large scale. Quantization\nmethods have been employed to reduce service costs and latency. Nevertheless,\noutliers in activations hinder the development of INT4 weight-activation\nquantization. Existing approaches separate outliers and normal values into two\nmatrices or migrate outliers from activations to weights, suffering from high\nlatency or accuracy degradation. Based on observing activations from large\nlanguage models, outliers can be classified into channel-wise and spike\noutliers. In this work, we propose Rotated Runtime Smooth (RRS), a\nplug-and-play activation smoother for quantization, consisting of Runtime\nSmooth and the Rotation operation. Runtime Smooth (RS) is introduced to\neliminate channel-wise outliers by smoothing activations with channel-wise\nmaximums during runtime. The rotation operation can narrow the gap between\nspike outliers and normal values, alleviating the effect of victims caused by\nchannel-wise smoothing. The proposed method outperforms the state-of-the-art\nmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from\n57.33 to 6.66 for INT4 inference.",
      "tldr_zh": "该论文针对大型语言模型（Large Language Models）在量化过程中激活中的异常值（outliers）问题，提出了一种无需训练的激活平滑方法Rotated Runtime Smooth (RRS)，以提高INT4推理的准确性。RRS包括Runtime Smooth (RS)模块，用于通过通道最大值平滑激活以消除channel-wise outliers，以及Rotation操作来缩小spike outliers与正常值的差距，从而缓解平滑带来的负面影响。实验结果显示，该方法在LLaMA和Qwen模型家族中超越现有技术，将WikiText-2的perplexity从57.33降低到6.66。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20361v2",
      "published_date": "2024-09-30 14:59:22 UTC",
      "updated_date": "2024-11-11 12:45:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:21:54.366860"
    },
    {
      "arxiv_id": "2409.20340v3",
      "title": "Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Osama Mustafa"
      ],
      "abstract": "Generative Adversarial Networks (GANs) have been at the forefront of image\nsynthesis, especially in medical fields like histopathology, where they help\naddress challenges such as data scarcity, patient privacy, and class imbalance.\nHowever, several inherent and domain-specific issues remain. For GANs, training\ninstability, mode collapse, and insufficient feedback from binary\nclassification can undermine performance. These challenges are particularly\npronounced with high-resolution histopathology images due to their complex\nfeature representation and high spatial detail. In response to these\nchallenges, this work proposes a novel framework integrating a contrastive\nlearning-based Multistage Progressive Finetuning Siamese Neural Network\n(MFT-SNN) with a Reinforcement Learning-based External Optimizer (RL-EO). The\nMFT-SNN improves feature similarity extraction in histopathology data, while\nthe RL-EO acts as a reward-based guide to balance GAN training, addressing mode\ncollapse and enhancing output quality. The proposed approach is evaluated\nagainst state-of-the-art (SOTA) GAN models and demonstrates superior\nperformance across multiple metrics.",
      "tldr_zh": "该研究针对生成对抗网络(GANs)在医学图像合成中的问题，如训练不稳定、模式崩溃和二元分类反馈不足，尤其在高分辨率组织病理学图像上，提出了一种新型框架。框架整合了基于对比学习(Contrastive Learning)的多阶段渐进微调孪生神经网络(Multistage Progressive Finetuning SNN, MFT-SNN)，以提升特征相似性提取，以及基于强化学习的外部优化器(Reinforcement Learning-based External Optimizer, RL-EO)，用于平衡GAN训练并缓解模式崩溃。实验结果显示，该方法在多个指标上优于现有最先进(SOTA) GAN模型，显著提高了图像合成的性能和输出质量。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20340v3",
      "published_date": "2024-09-30 14:39:56 UTC",
      "updated_date": "2024-10-26 09:51:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:22:06.193232"
    },
    {
      "arxiv_id": "2409.20303v1",
      "title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
      "title_zh": "翻译失败",
      "authors": [
        "Laurène Vaugrante",
        "Mathias Niepert",
        "Thilo Hagendorff"
      ],
      "abstract": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs.",
      "tldr_zh": "本文探讨了评估大型语言模型 (LLMs) 行为研究中潜在的复制危机，由于缺乏清晰方法论指导，导致结果的可复制性和泛化性不足。作者通过一系列复制实验测试了 chain-of-thought、EmotionPrompting、ExpertPrompting、Sandbagging 和 Re-Reading 等提示工程技术，在 GPT-3.5、GPT-4o、Gemini 1.5 Pro、Claude 3 Opus、Llama 3-8B 和 Llama 3-70B 等模型上，使用 CommonsenseQA、CRT、NumGLUE、ScienceQA 和 StrategyQA 等基准。实验发现，几乎所有技术都没有统计显著差异，突显了先前研究的 methodogical weaknesses。论文提出解决方案，包括开发稳健方法论、建立可靠基准和设计严格实验框架，以确保LLMs 行为的准确评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20303v1",
      "published_date": "2024-09-30 14:00:34 UTC",
      "updated_date": "2024-09-30 14:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:22:19.238306"
    },
    {
      "arxiv_id": "2409.20302v2",
      "title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning",
      "title_zh": "OM4OV：利用本体匹配进行本体版本控制",
      "authors": [
        "Zhangcheng Qiang",
        "Kerry Taylor",
        "Weiqing Wang"
      ],
      "abstract": "Due to the dynamic nature of the semantic web, ontology version control is\nrequired to capture time-varying information, most importantly for widely-used\nontologies. Despite the long-standing recognition of ontology versioning (OV)\nas a crucial component for efficient ontology management, the growing size of\nontologies and accumulating errors caused by manual labour overwhelm current OV\napproaches. In this paper, we propose yet another approach to performing OV\nusing existing ontology matching (OM) techniques and systems. We introduce a\nunified OM4OV pipeline. From an OM perspective, we reconstruct a new task\nformulation, measurement, and testbed for OV tasks. Reusing the prior\nalignment(s) from OM, we propose a pipeline optimisation method called\ncross-reference (CR) mechanism to improve overall OV performance. We\nexperimentally validate the OM4OV pipeline and the cross-reference mechanism in\nmodified Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss the insights on OM used for OV tasks, where some false mappings\ndetected by OV systems are not actually false.",
      "tldr_zh": "该论文提出 OM4OV 方法，利用 ontology matching (OM) 技术来优化 ontology versioning (OV)，以应对语义网动态变化导致的本体规模增大和手动错误积累问题。研究者引入了统一的 OM4OV 管道，并重新定义了 OV 任务的表述、测量和测试环境，同时提出 cross-reference (CR) 机制，通过重用现有 OM 对齐来提升整体性能。在修改后的 Ontology Alignment Evaluation Initiative (OAEI) 数据集实验中，OM4OV 显示出显著效果，并揭示了 OM 在 OV 任务中的洞见，例如某些被视为错误映射的实际并非虚假。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2409.20302v2",
      "published_date": "2024-09-30 14:00:04 UTC",
      "updated_date": "2024-11-24 23:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:22:30.324526"
    },
    {
      "arxiv_id": "2409.20274v1",
      "title": "Probabilistic Answer Set Programming with Discrete and Continuous Random Variables",
      "title_zh": "翻译失败",
      "authors": [
        "Damiano Azzolini",
        "Fabrizio Riguzzi"
      ],
      "abstract": "Probabilistic Answer Set Programming under the credal semantics (PASP)\nextends Answer Set Programming with probabilistic facts that represent\nuncertain information. The probabilistic facts are discrete with Bernoulli\ndistributions. However, several real-world scenarios require a combination of\nboth discrete and continuous random variables. In this paper, we extend the\nPASP framework to support continuous random variables and propose Hybrid\nProbabilistic Answer Set Programming (HPASP). Moreover, we discuss, implement,\nand assess the performance of two exact algorithms based on projected answer\nset enumeration and knowledge compilation and two approximate algorithms based\non sampling. Empirical results, also in line with known theoretical results,\nshow that exact inference is feasible only for small instances, but knowledge\ncompilation has a huge positive impact on the performance. Sampling allows\nhandling larger instances, but sometimes requires an increasing amount of\nmemory. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "tldr_zh": "本论文扩展了 Probabilistic Answer Set Programming under the credal semantics (PASP)，通过添加对连续随机变量的支持，提出 Hybrid Probabilistic Answer Set Programming (HPASP)，以处理包括 Bernoulli distributions 在内的离散和连续随机变量的真实场景。研究团队实现了两种精确算法（基于 projected answer set enumeration 和 knowledge compilation）和两种近似算法（基于 sampling），并评估了它们的性能。实验结果显示，精确推理仅适用于小实例，但 knowledge compilation 显著提升了效率，而采样方法虽能处理更大实例，却可能增加内存需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
      "pdf_url": "http://arxiv.org/pdf/2409.20274v1",
      "published_date": "2024-09-30 13:24:42 UTC",
      "updated_date": "2024-09-30 13:24:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:22:42.003694"
    },
    {
      "arxiv_id": "2409.20260v1",
      "title": "Computer-mediated therapies for stroke rehabilitation: a systematic review and meta-Analysis",
      "title_zh": "计算机辅助疗法用于中风康复：系统综述和荟萃分析",
      "authors": [
        "Stanley Mugisha. Mirko Job. Matteo Zoppi",
        "Marco Testa",
        "Rezia Molfino"
      ],
      "abstract": "OBJECTIVE: To evaluate the efficacy of different forms of virtual reality\n(VR) treatments as either immersive virtual reality (IVR) or non-immersive\nvirtual reality (NIVR) in comparison to conventional therapy (CT) in improving\nphysical and psychological status among stroke patients. METHODS: The\nliterature search was conducted on seven databases. ACM Digital Library,\nMedline (via PubMed), Cochrane, IEEE Xplore, Web of Science, and Scopus. The\neffect sizes of the main outcomes were calculated using Cohen's d. Pooled\nresults were used to present an overall estimate of the treatment effect using\na random-effects model. RESULTS: A total of 22 randomized controlled trials\nwere evaluated. 3 trials demonstrated that immersive virtual reality improved\nupper limb activity, function and activity of daily life in a way comparable to\nCT. 18 trials showed that NIVR had similar benefits to CT for upper limb\nactivity and function, balance and mobility, activities of daily living and\nparticipation. A comparison between the different forms of VR showed that IVR\nmay be more beneficial than NIVR for upper limb training and activities of\ndaily life. CONCLUSIONS: This study found out that IVR therapies may be more\neffective than NIVR but not CT to improve upper limb activity, function, and\ndaily life activities. However, there is no evidence of the durability of IVR\ntreatment. More research involving studies with larger samples is needed to\nassess the long-term effects and promising benefits of immersive virtual\nreality technology.",
      "tldr_zh": "本研究通过系统综述和元分析，评估了浸没式虚拟现实 (IVR) 和非浸没式虚拟现实 (NIVR) 在中风患者康复中的功效，与传统治疗 (CT) 进行比较，涉及22个随机对照试验，并使用Cohen's d计算效果大小。结果显示，IVR与CT类似地改善上肢活动、功能和日常生活，而NIVR在改善上肢活动、功能、平衡、移动和参与方面也与CT相当；此外，IVR可能比NIVR更益于上肢训练和日常生活活动。结论是IVR疗法可能优于NIVR但不优于CT，但缺乏长期效果证据，建议开展更多大规模研究以评估其持久益处。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "J.3.2"
      ],
      "primary_category": "physics.med-ph",
      "comment": "32 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.20260v1",
      "published_date": "2024-09-30 12:50:46 UTC",
      "updated_date": "2024-09-30 12:50:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:22:54.234792"
    },
    {
      "arxiv_id": "2409.20259v1",
      "title": "Learning to Ground Existentially Quantified Goals",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Funkquist",
        "Simon Ståhlberg",
        "Hector Geffner"
      ],
      "abstract": "Goal instructions for autonomous AI agents cannot assume that objects have\nunique names. Instead, objects in goals must be referred to by providing\nsuitable descriptions. However, this raises problems in both classical planning\nand generalized planning. The standard approach to handling existentially\nquantified goals in classical planning involves compiling them into a DNF\nformula that encodes all possible variable bindings and adding dummy actions to\nmap each DNF term into the new, dummy goal. This preprocessing is exponential\nin the number of variables. In generalized planning, the problem is different:\neven if general policies can deal with any initial situation and goal,\nexecuting a general policy requires the goal to be grounded to define a value\nfor the policy features. The problem of grounding goals, namely finding the\nobjects to bind the goal variables, is subtle: it is a generalization of\nclassical planning, which is a special case when there are no goal variables to\nbind, and constraint reasoning, which is a special case when there are no\nactions. In this work, we address the goal grounding problem with a novel\nsupervised learning approach. A GNN architecture, trained to predict the cost\nof partially quantified goals over small domain instances is tested on larger\ninstances involving more objects and different quantified goals. The proposed\narchitecture is evaluated experimentally over several planning domains where\ngeneralization is tested along several dimensions including the number of goal\nvariables and objects that can bind such variables. The scope of the approach\nis also discussed in light of the known relationship between GNNs and C2\nlogics.",
      "tldr_zh": "该论文探讨了自主 AI 代理中存在量化目标（Existentially Quantified Goals）的 grounding 问题，指出经典规划中通过 DNF 公式和 dummy actions 的方法会导致指数级预处理开销，而广义规划则需解决目标变量绑定挑战。研究提出了一种新颖的监督学习方法，使用 GNN（Graph Neural Network）架构训练模型预测部分量化目标的成本，并在小实例上训练后测试于更大实例。实验结果显示，该方法在多个规划领域表现出良好泛化能力，包括处理更多目标变量和绑定对象，并讨论了其与 C2 逻辑的关系，为高效目标 grounding 提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, Accepted at the 21st International Conference on Principles\n  of Knowledge Representation and Reasoning (KR2024) in the Reasoning,\n  Learning, and Decision Making track",
      "pdf_url": "http://arxiv.org/pdf/2409.20259v1",
      "published_date": "2024-09-30 12:49:27 UTC",
      "updated_date": "2024-09-30 12:49:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:23:07.736106"
    },
    {
      "arxiv_id": "2409.20258v1",
      "title": "Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning",
      "title_zh": "在多目标强化学习中从演示推断偏好",
      "authors": [
        "Junlin Lu",
        "Patrick Mannion",
        "Karl Mason"
      ],
      "abstract": "Many decision-making problems feature multiple objectives where it is not\nalways possible to know the preferences of a human or agent decision-maker for\ndifferent objectives. However, demonstrated behaviors from the decision-maker\nare often available. This research proposes a dynamic weight-based preference\ninference (DWPI) algorithm that can infer the preferences of agents acting in\nmulti-objective decision-making problems from demonstrations. The proposed\nalgorithm is evaluated on three multi-objective Markov decision processes: Deep\nSea Treasure, Traffic, and Item Gathering, and is compared to two existing\npreference inference algorithms. Empirical results demonstrate significant\nimprovements compared to the baseline algorithms, in terms of both time\nefficiency and inference accuracy. The DWPI algorithm maintains its performance\nwhen inferring preferences for sub-optimal demonstrations. Moreover, the DWPI\nalgorithm does not necessitate any interactions with the user during inference\n- only demonstrations are required. We provide a correctness proof and\ncomplexity analysis of the algorithm and statistically evaluate the performance\nunder different representation of demonstrations.",
      "tldr_zh": "这篇论文针对多目标强化学习（Multi-objective Reinforcement Learning）中的偏好推断问题，提出了一种动态权重-based preference inference (DWPI) 算法，通过分析决策者的演示行为来推断其对不同目标的偏好。DWPI 算法在 Deep Sea Treasure、Traffic 和 Item Gathering 等三个多目标 Markov 决策过程上进行了评估，与现有算法相比，在时间效率和推理准确性方面实现了显著提升。算法还能处理次优演示，无需用户交互，仅依赖演示数据；此外，论文提供了算法的正确性证明、复杂性分析，并对不同演示表示下的性能进行了统计评估。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Neural Comput & Applic (2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.20258v1",
      "published_date": "2024-09-30 12:49:10 UTC",
      "updated_date": "2024-09-30 12:49:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:23:19.323738"
    },
    {
      "arxiv_id": "2409.20252v2",
      "title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
      "title_zh": "大语言模型在天文学研究演变中的作用是什么？",
      "authors": [
        "Morgan Fouesneau",
        "Ivelina G. Momcheva",
        "Urmila Chadayammuri",
        "Mariia Demianenko",
        "Antoine Dumont",
        "Raphael E. Hviding",
        "K. Angelique Kahle",
        "Nadiia Pulatova",
        "Bhavesh Rajpoot",
        "Marten B. Scheuck",
        "Rhys Seeburger",
        "Dmitry Semenov",
        "Jaime I. Villaseñor"
      ],
      "abstract": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly\ntransforming multiple fields, offering powerful tools for a wide range of\napplications. These models, commonly trained on vast datasets, exhibit\nhuman-like text generation capabilities, making them useful for research tasks\nsuch as ideation, literature review, coding, drafting, and outreach. We\nconducted a study involving 13 astronomers at different career stages and\nresearch fields to explore LLM applications across diverse tasks over several\nmonths and to evaluate their performance in research-related activities. This\nwork was accompanied by an anonymous survey assessing participants' experiences\nand attitudes towards LLMs. We provide a detailed analysis of the tasks\nattempted and the survey answers, along with specific output examples. Our\nfindings highlight both the potential and limitations of LLMs in supporting\nresearch while also addressing general and research-specific ethical\nconsiderations. We conclude with a series of recommendations, emphasizing the\nneed for researchers to complement LLMs with critical thinking and domain\nexpertise, ensuring these tools serve as aids rather than substitutes for\nrigorous scientific inquiry.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)如ChatGPT在天文学研究演变中的作用，通过一项涉及13位天文学家参与的研究和匿名调查，评估了LLMs在构思(idation)、文献综述(literature review)、编程(coding)、起草(drafting)和外联(outreach)等任务中的表现。研究发现，LLMs展示了强大的支持潜力，但也存在局限性，包括潜在的伦理问题，如准确性和偏见。作者强调，研究人员应将LLMs视为辅助工具，与批判性思维和领域专业知识相结合，以确保其作为科学探究的补充而非替代。",
      "categories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "Paper submitted to RASTI. We share our experience, ethical and legal\n  concerns (5.3), and recommendations for individuals and journals (6.). We\n  welcome feedback",
      "pdf_url": "http://arxiv.org/pdf/2409.20252v2",
      "published_date": "2024-09-30 12:42:25 UTC",
      "updated_date": "2024-10-01 16:34:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:23:32.081053"
    },
    {
      "arxiv_id": "2409.20247v1",
      "title": "Resource Allocation for Stable LLM Training in Mobile Edge Computing",
      "title_zh": "资源分配用于移动边缘计算中稳定LLM训练",
      "authors": [
        "Chang Liu",
        "Jun Zhao"
      ],
      "abstract": "As mobile devices increasingly become focal points for advanced applications,\nedge computing presents a viable solution to their inherent computational\nlimitations, particularly in deploying large language models (LLMs). However,\ndespite the advancements in edge computing, significant challenges remain in\nefficient training and deploying LLMs due to the computational demands and data\nprivacy concerns associated with these models. This paper explores a\ncollaborative training framework that integrates mobile users with edge servers\nto optimize resource allocation, thereby enhancing both performance and\nefficiency. Our approach leverages parameter-efficient fine-tuning (PEFT)\nmethods, allowing mobile users to adjust the initial layers of the LLM while\nedge servers handle the more demanding latter layers. Specifically, we\nformulate a multi-objective optimization problem to minimize the total energy\nconsumption and delay during training. We also address the common issue of\ninstability in model performance by incorporating stability enhancements into\nour objective function. Through novel fractional programming technique, we\nachieve a stationary point for the formulated problem. Simulations demonstrate\nthat our method reduces the energy consumption as well as the latency, and\nincreases the reliability of LLMs across various mobile settings.",
      "tldr_zh": "这篇论文探讨了在移动边缘计算中训练大型语言模型(LLMs)面临的计算需求和数据隐私挑战，提出了一种协作训练框架，将移动用户和边缘服务器整合以优化资源分配。框架采用参数高效微调(PEFT)方法，让移动用户处理LLMs的初始层，而边缘服务器负责更复杂的后层，并通过多目标优化问题最小化总能量消耗和延迟，同时加入稳定性增强机制。作者利用分数规划技术解决了该优化问题，确保了训练过程的稳定性。模拟实验表明，该方法显著降低了能量消耗和延迟，并提升了LLMs在不同移动场景中的可靠性。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.IT",
        "cs.SY",
        "eess.SY",
        "math.IT",
        "math.OC"
      ],
      "primary_category": "cs.DC",
      "comment": "This paper appears in the 2024 International Symposium on Theory,\n  Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile\n  Computing (MobiHoc)",
      "pdf_url": "http://arxiv.org/pdf/2409.20247v1",
      "published_date": "2024-09-30 12:36:27 UTC",
      "updated_date": "2024-09-30 12:36:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:23:44.270960"
    },
    {
      "arxiv_id": "2409.20222v2",
      "title": "Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models",
      "title_zh": "超越提示：大型语言模型的动态对话基准测试",
      "authors": [
        "David Castillo-Bolado",
        "Joseph Davidson",
        "Finlay Gray",
        "Marek Rosa"
      ],
      "abstract": "We introduce a dynamic benchmarking system for conversational agents that\nevaluates their performance through a single, simulated, and lengthy\nuser$\\leftrightarrow$agent interaction. The interaction is a conversation\nbetween the user and agent, where multiple tasks are introduced and then\nundertaken concurrently. We context switch regularly to interleave the tasks,\nwhich constructs a realistic testing scenario in which we assess the Long-Term\nMemory, Continual Learning, and Information Integration capabilities of the\nagents. Results from both proprietary and open-source Large-Language Models\nshow that LLMs in general perform well on single-task interactions, but they\nstruggle on the same tasks when they are interleaved. Notably, short-context\nLLMs supplemented with an LTM system perform as well as or better than those\nwith larger contexts. Our benchmark suggests that there are other challenges\nfor LLMs responding to more natural interactions that contemporary benchmarks\nhave heretofore not been able to capture.",
      "tldr_zh": "本文提出了一种动态对话基准测试系统，用于评估大型语言模型(LLMs)通过模拟漫长用户-代理互动的性能，该互动中交错多个任务以测试LLMs的长时记忆(Long-Term Memory)、持续学习(Continual Learning)和信息整合(Information Integration)能力。实验结果显示，LLMs在单一任务互动中表现良好，但任务交错时显著下降。值得注意的是，短上下文LLMs通过添加LTM系统，能达到或超过大上下文模型的水平。该基准揭示了现有评估方法未能捕捉的LLMs在自然互动中的新挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a poster at NeurIPS D&B Track 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20222v2",
      "published_date": "2024-09-30 12:01:29 UTC",
      "updated_date": "2024-10-11 11:32:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:23:55.064304"
    },
    {
      "arxiv_id": "2409.20196v4",
      "title": "Melody-Guided Music Generation",
      "title_zh": "旋律引导的音乐生成",
      "authors": [
        "Shaopeng Wei",
        "Manzhen Wei",
        "Haoyu Wang",
        "Yu Zhao",
        "Gang Kou"
      ],
      "abstract": "We present the Melody-Guided Music Generation (MG2) model, a novel approach\nusing melody to guide the text-to-music generation that, despite a simple\nmethod and limited resources, achieves excellent performance. Specifically, we\nfirst align the text with audio waveforms and their associated melodies using\nthe newly proposed Contrastive Language-Music Pretraining, enabling the learned\ntext representation fused with implicit melody information. Subsequently, we\ncondition the retrieval-augmented diffusion module on both text prompt and\nretrieved melody. This allows MG2 to generate music that reflects the content\nof the given text description, meantime keeping the intrinsic harmony under the\nguidance of explicit melody information. We conducted extensive experiments on\ntwo public datasets: MusicCaps and MusicBench. Surprisingly, the experimental\nresults demonstrate that the proposed MG2 model surpasses current open-source\ntext-to-music generation models, achieving this with fewer than 1/3 of the\nparameters or less than 1/200 of the training data compared to state-of-the-art\ncounterparts. Furthermore, we conducted comprehensive human evaluations\ninvolving three types of users and five perspectives, using newly designed\nquestionnaires to explore the potential real-world applications of MG2.",
      "tldr_zh": "我们提出了 MG2 模型，这是一种使用旋律引导的文本到音乐生成方法，通过 Contrastive Language-Music Pretraining 来对齐文本、音频波形和旋律，从而融合隐式旋律信息。模型随后采用检索增强扩散模块（retrieval-augmented diffusion module），基于文本提示和检索到的旋律生成反映文本内容且保持和谐的音乐。在 MusicCaps 和 MusicBench 数据集上的实验表明，MG2 超过了当前开源模型，尽管其参数量不到 1/3，训练数据不到 1/200。此外，人类评估从五个视角验证了 MG2 的实际应用潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "16 pages, 8 figure, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.20196v4",
      "published_date": "2024-09-30 11:13:35 UTC",
      "updated_date": "2024-12-30 05:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:24:06.951936"
    },
    {
      "arxiv_id": "2409.20195v2",
      "title": "Forecasting Disease Progression with Parallel Hyperplanes in Longitudinal Retinal OCT",
      "title_zh": "翻译失败",
      "authors": [
        "Arunava Chakravarty",
        "Taha Emre",
        "Dmitrii Lachinov",
        "Antoine Rivail",
        "Hendrik Scholl",
        "Lars Fritsche",
        "Sobha Sivaprasad",
        "Daniel Rueckert",
        "Andrew Lotery",
        "Ursula Schmidt-Erfurth",
        "Hrvoje Bogunović"
      ],
      "abstract": "Predicting future disease progression risk from medical images is challenging\ndue to patient heterogeneity, and subtle or unknown imaging biomarkers.\nMoreover, deep learning (DL) methods for survival analysis are susceptible to\nimage domain shifts across scanners. We tackle these issues in the task of\npredicting late dry Age-related Macular Degeneration (dAMD) onset from retinal\nOCT scans. We propose a novel DL method for survival prediction to jointly\npredict from the current scan a risk score, inversely related to\ntime-to-conversion, and the probability of conversion within a time interval\n$t$. It uses a family of parallel hyperplanes generated by parameterizing the\nbias term as a function of $t$. In addition, we develop unsupervised losses\nbased on intra-subject image pairs to ensure that risk scores increase over\ntime and that future conversion predictions are consistent with AMD stage\nprediction using actual scans of future visits. Such losses enable\ndata-efficient fine-tuning of the trained model on new unlabeled datasets\nacquired with a different scanner. Extensive evaluation on two large datasets\nacquired with different scanners resulted in a mean AUROCs of 0.82 for\nDataset-1 and 0.83 for Dataset-2, across prediction intervals of 6,12 and 24\nmonths.",
      "tldr_zh": "该论文针对从纵向视网膜OCT扫描预测晚期干性年龄相关黄斑变性(dAMD)进展风险的挑战，提出了一种新的深度学习(DL)方法，以应对患者异质性和图像域移位问题。该方法使用一组平行超平面，通过将偏置项作为时间间隔t的函数，来联合预测风险分数（与转换时间成反比）和在t内发生转换的概率；同时引入基于患者内图像对的无监督损失，确保风险分数随时间增加，并使未来预测与AMD阶段一致，从而实现数据高效的模型微调。在两个不同扫描仪数据集上的评估中，该方法在6、12和24个月预测间隔的平均AUROC分别达到0.82和0.83，展示了其有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted in MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20195v2",
      "published_date": "2024-09-30 11:11:35 UTC",
      "updated_date": "2024-10-03 13:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:24:21.320050"
    },
    {
      "arxiv_id": "2409.20192v1",
      "title": "Factory Operators' Perspectives on Cognitive Assistants for Knowledge Sharing: Challenges, Risks, and Impact on Work",
      "title_zh": "工厂操作员对用于知识共享的认知助手的观点：挑战、风险以及对工作的影响",
      "authors": [
        "Samuel Kernan Freire",
        "Tianhao He",
        "Chaofan Wang",
        "Evangelos Niforatos",
        "Alessandro Bozzon"
      ],
      "abstract": "In the shift towards human-centered manufacturing, our two-year longitudinal\nstudy investigates the real-world impact of deploying Cognitive Assistants\n(CAs) in factories. The CAs were designed to facilitate knowledge sharing among\nfactory operators. Our investigation focused on smartphone-based voice\nassistants and LLM-powered chatbots, examining their usability and utility in a\nreal-world factory setting. Based on the qualitative feedback we collected\nduring the deployments of CAs at the factories, we conducted a thematic\nanalysis to investigate the perceptions, challenges, and overall impact on\nworkflow and knowledge sharing.\n  Our results indicate that while CAs have the potential to significantly\nimprove efficiency through knowledge sharing and quicker resolution of\nproduction issues, they also introduce concerns around workplace surveillance,\nthe types of knowledge that can be shared, and shortcomings compared to\nhuman-to-human knowledge sharing. Additionally, our findings stress the\nimportance of addressing privacy, knowledge contribution burdens, and tensions\nbetween factory operators and their managers.",
      "tldr_zh": "这篇论文通过为期两年的纵向研究，探讨了工厂操作员对认知助手(CAs)的看法，包括基于智能手机的语音助手和LLM驱动聊天机器人，在促进知识共享方面的可用性和实用性。研究基于定性反馈和主题分析，揭示了CAs能显著提高工作效率、加快生产问题解决，但也带来了挑战，如工作场所监控、共享知识的限制以及与人际知识共享相比的不足。主要发现强调了解决隐私问题、知识贡献负担以及操作员与管理者之间紧张关系的重要性，以优化CAs在制造业中的应用。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "32 pages, 6 figures, 2 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2409.20192v1",
      "published_date": "2024-09-30 11:08:27 UTC",
      "updated_date": "2024-09-30 11:08:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:24:31.506936"
    },
    {
      "arxiv_id": "2409.20187v1",
      "title": "Choosing DAG Models Using Markov and Minimal Edge Count in the Absence of Ground Truth",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph D. Ramsey",
        "Bryan Andrews",
        "Peter Spirtes"
      ],
      "abstract": "We give a novel nonparametric pointwise consistent statistical test (the\nMarkov Checker) of the Markov condition for directed acyclic graph (DAG) or\ncompleted partially directed acyclic graph (CPDAG) models given a dataset. We\nalso introduce the Cross-Algorithm Frugality Search (CAFS) for rejecting DAG\nmodels that either do not pass the Markov Checker test or that are not edge\nminimal. Edge minimality has been used previously by Raskutti and Uhler as a\nnonparametric simplicity criterion, though CAFS readily generalizes to other\nsimplicity conditions. Reference to the ground truth is not necessary for CAFS,\nso it is useful for finding causal structure learning algorithms and tuning\nparameter settings that output causal models that are approximately true from a\ngiven data set. We provide a software tool for this analysis that is suitable\nfor even quite large or dense models, provided a suitably fast pointwise\nconsistent test of conditional independence is available. In addition, we show\nin simulation that the CAFS procedure can pick approximately correct models\nwithout knowing the ground truth.",
      "tldr_zh": "本研究提出了一种新的非参数点估计一致统计测试（Markov Checker），用于验证有向无环图（DAG）或完成的部分有向无环图（CPDAG）模型的Markov条件，而无需参考真实数据（Ground Truth）。他们还引入了Cross-Algorithm Frugality Search (CAFS) 算法，该算法结合Markov Checker和边最小性（Minimal Edge Count）标准，拒绝不符合条件的DAG模型，并可推广到其他简单性条件，以帮助识别从给定数据集推断出的近似正确因果模型。实验模拟显示，CAFS能够在没有Ground Truth的情况下选择准确的模型，并提供了一个适用于大型或密集模型的软件工具，前提是具备快速的条件独立性测试。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML",
        "68T37",
        "I.2.0; I.2.6; I.6.5"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 14 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2409.20187v1",
      "published_date": "2024-09-30 11:03:44 UTC",
      "updated_date": "2024-09-30 11:03:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:24:43.332869"
    },
    {
      "arxiv_id": "2410.03728v5",
      "title": "Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic Analysis",
      "title_zh": "探索 QUIC 动态：一个用于加密流量分析的大规模数据集",
      "authors": [
        "Barak Gahtan",
        "Robert J. Shahla",
        "Alex M. Bronstein",
        "Reuven Cohen"
      ],
      "abstract": "The increasing adoption of the QUIC transport protocol has transformed\nencrypted web traffic, necessitating new methodologies for network analysis.\nHowever, existing datasets lack the scope, metadata, and decryption\ncapabilities required for robust benchmarking in encrypted traffic research. We\nintroduce VisQUIC, a large-scale dataset of 100,000 labeled QUIC traces from\nover 44,000 websites, collected over four months. Unlike prior datasets,\nVisQUIC provides SSL keys for controlled decryption, supports multiple QUIC\nimplementations (Chromium QUIC, Facebooks mvfst, Cloudflares quiche), and\nintroduces a novel image-based representation that enables machine\nlearning-driven encrypted traffic analysis. The dataset includes standardized\nbenchmarking tools, ensuring reproducibility. To demonstrate VisQUICs utility,\nwe present a benchmarking task for estimating HTTP/3 responses in encrypted\nQUIC traffic, achieving 97% accuracy using only observable packet features. By\npublicly releasing VisQUIC, we provide an open foundation for advancing\nencrypted traffic analysis, QUIC security research, and network monitoring.",
      "tldr_zh": "该研究探讨了 QUIC 传输协议对加密网络流量的影响，提出 VisQUIC，这是一个大规模数据集，包含 10 万个标记的 QUIC 跟踪，来自超过 44,000 个网站，并支持 SSL keys 进行控制解密。VisQUIC 涵盖多种 QUIC 实现（如 Chromium QUIC、Facebook's mvfst 和 Cloudflare's quiche），并引入新型图像表示以便于机器学习驱动的加密流量分析，同时提供标准化基准测试工具以确保可重复性。在基准任务中，使用可观察的数据包特征估算 HTTP/3 响应，准确率达到 97%，从而为加密流量分析、QUIC 安全研究和网络监控提供了一个公开的基础。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "The dataset and the supplementary material can be provided upon\n  request",
      "pdf_url": "http://arxiv.org/pdf/2410.03728v5",
      "published_date": "2024-09-30 10:50:12 UTC",
      "updated_date": "2025-02-27 16:19:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:24:54.823467"
    },
    {
      "arxiv_id": "2409.20174v1",
      "title": "Modelando procesos cognitivos de la lectura natural con GPT-2",
      "title_zh": "翻译失败",
      "authors": [
        "Bruno Bianchi",
        "Alfredo Umfurer",
        "Juan Esteban Kamienkowski"
      ],
      "abstract": "The advancement of the Natural Language Processing field has enabled the\ndevelopment of language models with a great capacity for generating text. In\nrecent years, Neuroscience has been using these models to better understand\ncognitive processes. In previous studies, we found that models like Ngrams and\nLSTM networks can partially model Predictability when used as a co-variable to\nexplain readers' eye movements. In the present work, we further this line of\nresearch by using GPT-2 based models. The results show that this architecture\nachieves better outcomes than its predecessors.",
      "tldr_zh": "本研究探讨了使用 GPT-2 模型来模拟自然阅读的认知过程，旨在帮助神经科学更好地理解读者的眼动（eye movements）。研究者扩展了先前的工作，将 GPT-2 作为协变量来解释 Predictability，并与 Ngrams 和 LSTM 网络进行比较。结果显示，GPT-2 架构在建模性能上超过了前代模型，为认知过程建模提供了新见解。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "in Spanish language",
      "pdf_url": "http://arxiv.org/pdf/2409.20174v1",
      "published_date": "2024-09-30 10:34:32 UTC",
      "updated_date": "2024-09-30 10:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:25:07.466902"
    },
    {
      "arxiv_id": "2409.20163v1",
      "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyu Zhang",
        "Quanyu Dai",
        "Luyu Chen",
        "Zeren Jiang",
        "Rui Li",
        "Jieming Zhu",
        "Xu Chen",
        "Yi Xie",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "abstract": "LLM-based agents have been widely applied as personal assistants, capable of\nmemorizing information from user messages and responding to personal queries.\nHowever, there still lacks an objective and automatic evaluation on their\nmemory capability, largely due to the challenges in constructing reliable\nquestions and answers (QAs) according to user messages. In this paper, we\npropose MemSim, a Bayesian simulator designed to automatically construct\nreliable QAs from generated user messages, simultaneously keeping their\ndiversity and scalability. Specifically, we introduce the Bayesian Relation\nNetwork (BRNet) and a causal generation mechanism to mitigate the impact of LLM\nhallucinations on factual information, facilitating the automatic creation of\nan evaluation dataset. Based on MemSim, we generate a dataset in the daily-life\nscenario, named MemDaily, and conduct extensive experiments to assess the\neffectiveness of our approach. We also provide a benchmark for evaluating\ndifferent memory mechanisms in LLM-based agents with the MemDaily dataset. To\nbenefit the research community, we have released our project at\nhttps://github.com/nuster1128/MemSim.",
      "tldr_zh": "该论文针对LLM-based agents作为个人助理的记忆能力评估问题，提出了一种Bayesian Simulator（MemSim），用于从用户消息自动生成可靠的问答（QAs），同时确保多样性和可扩展性。MemSim引入Bayesian Relation Network (BRNet)和因果生成机制，以减轻LLM幻觉对事实信息的影响，从而自动构建评估数据集。实验基于生成的MemDaily数据集评估了不同内存机制的有效性，并提供了基准，证明了该方法的优越性，该项目已在GitHub开源。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 25 tables, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.20163v1",
      "published_date": "2024-09-30 10:19:04 UTC",
      "updated_date": "2024-09-30 10:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:25:18.538730"
    },
    {
      "arxiv_id": "2409.20149v1",
      "title": "1 Trillion Token (1TT) Platform: A Novel Framework for Efficient Data Sharing and Compensation in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chanjun Park",
        "Hyunsoo Ha",
        "Jihoo Kim",
        "Yungi Kim",
        "Dahyun Kim",
        "Sukyung Lee",
        "Seonghoon Yang"
      ],
      "abstract": "In this paper, we propose the 1 Trillion Token Platform (1TT Platform), a\nnovel framework designed to facilitate efficient data sharing with a\ntransparent and equitable profit-sharing mechanism. The platform fosters\ncollaboration between data contributors, who provide otherwise non-disclosed\ndatasets, and a data consumer, who utilizes these datasets to enhance their own\nservices. Data contributors are compensated in monetary terms, receiving a\nshare of the revenue generated by the services of the data consumer. The data\nconsumer is committed to sharing a portion of the revenue with contributors,\naccording to predefined profit-sharing arrangements. By incorporating a\ntransparent profit-sharing paradigm to incentivize large-scale data sharing,\nthe 1TT Platform creates a collaborative environment to drive the advancement\nof NLP and LLM technologies.",
      "tldr_zh": "本论文提出1 Trillion Token (1TT) Platform，这是一个创新框架，旨在通过透明且公平的利润共享机制促进Large Language Models中的高效数据共享。平台鼓励数据贡献者提供非公开数据集，并从数据消费者的服务收入中获得货币补偿，而数据消费者则根据预定义安排分享部分收益，从而激励大规模协作。最终，该框架创建了一个合作环境，推动NLP和LLM技术的快速发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20149v1",
      "published_date": "2024-09-30 09:55:39 UTC",
      "updated_date": "2024-09-30 09:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:25:30.346777"
    },
    {
      "arxiv_id": "2409.20147v1",
      "title": "Classification of Radiological Text in Small and Imbalanced Datasets in a Non-English Language",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent Beliveau",
        "Helene Kaas",
        "Martin Prener",
        "Claes N. Ladefoged",
        "Desmond Elliott",
        "Gitte M. Knudsen",
        "Lars H. Pinborg",
        "Melanie Ganz"
      ],
      "abstract": "Natural language processing (NLP) in the medical domain can underperform in\nreal-world applications involving small datasets in a non-English language with\nfew labeled samples and imbalanced classes. There is yet no consensus on how to\napproach this problem. We evaluated a set of NLP models including BERT-like\ntransformers, few-shot learning with sentence transformers (SetFit), and\nprompted large language models (LLM), using three datasets of radiology reports\non magnetic resonance images of epilepsy patients in Danish, a low-resource\nlanguage. Our results indicate that BERT-like models pretrained in the target\ndomain of radiology reports currently offer the optimal performances for this\nscenario. Notably, the SetFit and LLM models underperformed compared to\nBERT-like models, with LLM performing the worst. Importantly, none of the\nmodels investigated was sufficiently accurate to allow for text classification\nwithout any supervision. However, they show potential for data filtering, which\ncould reduce the amount of manual labeling required.",
      "tldr_zh": "这篇论文探讨了在小规模、类别不平衡的非英语数据集（如丹麦语放射学报告）上进行自然语言处理(NLP)文本分类的挑战，评估了多种模型包括BERT-like transformers、few-shot learning with sentence transformers (SetFit)以及prompted large language models (LLM)。研究使用三个癫痫患者MRI报告数据集进行实验，结果显示BERT-like模型在放射学领域预训练后表现出最佳性能，而SetFit和LLM模型表现不如前者，且LLM效果最差。论文强调，尽管这些模型尚未准确到可实现无监督分类，但它们在数据过滤方面有潜力，能减少手动标记的工作量。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20147v1",
      "published_date": "2024-09-30 09:52:28 UTC",
      "updated_date": "2024-09-30 09:52:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:25:42.920948"
    },
    {
      "arxiv_id": "2409.20130v1",
      "title": "Reevaluation of Inductive Link Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Ott",
        "Christian Meilicke",
        "Heiner Stuckenschmidt"
      ],
      "abstract": "Within this paper, we show that the evaluation protocol currently used for\ninductive link prediction is heavily flawed as it relies on ranking the true\nentity in a small set of randomly sampled negative entities. Due to the limited\nsize of the set of negatives, a simple rule-based baseline can achieve\nstate-of-the-art results, which simply ranks entities higher based on the\nvalidity of their type. As a consequence of these insights, we reevaluate\ncurrent approaches for inductive link prediction on several benchmarks using\nthe link prediction protocol usually applied to the transductive setting. As\nsome inductive methods suffer from scalability issues when evaluated in this\nsetting, we propose and apply additionally an improved sampling protocol, which\ndoes not suffer from the problem mentioned above. The results of our evaluation\ndiffer drastically from the results reported in so far.",
      "tldr_zh": "本研究揭示了当前诱导链接预测（Inductive Link Prediction）的评估协议存在重大缺陷，因为它仅在小样本负实体中排名真实实体，导致简单规则基线（如基于实体类型有效性排名）就能达到最先进（SOTA）水平。作者重新评估了现有方法，使用通常适用于传导设置（Transductive Setting）的链接预测协议，并在多个基准上进行测试。由于一些诱导方法在规模化方面存在问题，论文还提出并应用了一种改进的采样协议，以避免原有缺陷。结果显示，重新评估后的性能与之前报道的差异巨大，这为未来研究提供了重要启示。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in RuleML+RR 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20130v1",
      "published_date": "2024-09-30 09:32:10 UTC",
      "updated_date": "2024-09-30 09:32:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:25:54.423109"
    },
    {
      "arxiv_id": "2409.20094v1",
      "title": "Aggressive Post-Training Compression on Extremely Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zining Zhang",
        "Yao Chen",
        "Bingsheng He",
        "Zhenjie Zhang"
      ],
      "abstract": "The increasing size and complexity of Large Language Models (LLMs) pose\nchallenges for their deployment on personal computers and mobile devices.\nAggressive post-training model compression is necessary to reduce the models'\nsize, but it often results in significant accuracy loss. To address this\nchallenge, we propose a novel network pruning technology that utilizes over 0.7\nsparsity and less than 8 bits of quantization. Our approach enables the\ncompression of prevailing LLMs within a couple of hours while maintaining a\nrelatively small accuracy loss. In experimental evaluations, our method\ndemonstrates effectiveness and potential for practical deployment. By making\nLLMs available on domestic devices, our work can facilitate a new era of\nnatural language processing applications with wide-ranging impacts.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 的尺寸和复杂性带来的部署挑战，提出了一种激进的后训练压缩方法，利用超过 0.7 的稀疏度 (sparsity) 和少于 8 位的量化 (quantization) 进行网络修剪 (network pruning)。这种方法能够在数小时内显著减小模型规模，同时将准确性损失控制在较小范围内。实验结果证明了该技术的有效性，有望使 LLMs 在个人电脑和移动设备上实现部署，推动自然语言处理 (NLP) 应用的新时代。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20094v1",
      "published_date": "2024-09-30 08:47:17 UTC",
      "updated_date": "2024-09-30 08:47:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:26:06.540175"
    },
    {
      "arxiv_id": "2409.20092v1",
      "title": "Continuous-Time Linear Positional Embedding for Irregular Time Series Forecasting",
      "title_zh": "用于不规则时间序列预测的连续时间线性位置嵌入",
      "authors": [
        "Byunghyun Kim",
        "Jae-Gil Lee"
      ],
      "abstract": "Irregularly sampled time series forecasting, characterized by non-uniform\nintervals, is prevalent in practical applications. However, previous research\nhave been focused on regular time series forecasting, typically relying on\ntransformer architectures. To extend transformers to handle irregular time\nseries, we tackle the positional embedding which represents the temporal\ninformation of the data. We propose CTLPE, a method learning a continuous\nlinear function for encoding temporal information. The two challenges of\nirregular time series, inconsistent observation patterns and irregular time\ngaps, are solved by learning a continuous-time function and concise\nrepresentation of position. Additionally, the linear continuous function is\nempirically shown superior to other continuous functions by learning a neural\ncontrolled differential equation-based positional embedding, and theoretically\nsupported with properties of ideal positional embedding. CTLPE outperforms\nexisting techniques across various irregularly-sampled time series datasets,\nshowcasing its enhanced efficacy.",
      "tldr_zh": "本研究针对不规则采样时间序列预测（Irregularly sampled time series forecasting）的挑战，提出了一种新的位置嵌入方法CTLPE（Continuous-Time Linear Positional Embedding），以扩展Transformer架构处理非均匀时间间隔的数据。CTLPE通过学习一个连续线性函数来编码时间信息，解决了不一致观察模式和不规则时间间隔问题，并在实验中证明其优于其他连续函数，如基于神经控制微分方程的嵌入。理论分析支持了CTLPE的理想属性，并在多种不规则时间序列数据集上表现出色，超越了现有技术。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20092v1",
      "published_date": "2024-09-30 08:46:18 UTC",
      "updated_date": "2024-09-30 08:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:26:18.735883"
    },
    {
      "arxiv_id": "2409.20064v2",
      "title": "Knowledge Discovery using Unsupervised Cognition",
      "title_zh": "利用无监督认知的知识发现",
      "authors": [
        "Alfredo Ibias",
        "Hector Antona",
        "Guillem Ramirez-Miranda",
        "Enric Guinovart"
      ],
      "abstract": "Knowledge discovery is key to understand and interpret a dataset, as well as\nto find the underlying relationships between its components. Unsupervised\nCognition is a novel unsupervised learning algorithm that focus on modelling\nthe learned data. This paper presents three techniques to perform knowledge\ndiscovery over an already trained Unsupervised Cognition model. Specifically,\nwe present a technique for pattern mining, a technique for feature selection\nbased on the previous pattern mining technique, and a technique for\ndimensionality reduction based on the previous feature selection technique. The\nfinal goal is to distinguish between relevant and irrelevant features and use\nthem to build a model from which to extract meaningful patterns. We evaluated\nour proposals with empirical experiments and found that they overcome the\nstate-of-the-art in knowledge discovery.",
      "tldr_zh": "这篇论文介绍了 Unsupervised Cognition，一种新型的无监督学习算法，用于建模数据并进行知识发现。论文提出三种技术：基于模式的 pattern mining、随后的 feature selection 以及基于特征选择的 dimensionality reduction，这些方法旨在区分数据集中的相关和无关特征，并从中提取有意义的模式。通过实证实验，研究发现这些技术在知识发现方面超过了现有最先进的方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20064v2",
      "published_date": "2024-09-30 08:07:29 UTC",
      "updated_date": "2025-01-28 11:36:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:26:30.109148"
    },
    {
      "arxiv_id": "2409.20054v1",
      "title": "Evaluating and explaining training strategies for zero-shot cross-lingual news sentiment analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Luka Andrenšek",
        "Boshko Koloski",
        "Andraž Pelicon",
        "Nada Lavrač",
        "Senja Pollak",
        "Matthew Purver"
      ],
      "abstract": "We investigate zero-shot cross-lingual news sentiment detection, aiming to\ndevelop robust sentiment classifiers that can be deployed across multiple\nlanguages without target-language training data. We introduce novel evaluation\ndatasets in several less-resourced languages, and experiment with a range of\napproaches including the use of machine translation; in-context learning with\nlarge language models; and various intermediate training regimes including a\nnovel task objective, POA, that leverages paragraph-level information. Our\nresults demonstrate significant improvements over the state of the art, with\nin-context learning generally giving the best performance, but with the novel\nPOA approach giving a competitive alternative with much lower computational\noverhead. We also show that language similarity is not in itself sufficient for\npredicting the success of cross-lingual transfer, but that similarity in\nsemantic content and structure can be equally important.",
      "tldr_zh": "本研究评估了 zero-shot cross-lingual 新闻情感分析的训练策略，旨在开发无需目标语言训练数据的鲁棒情感分类器。\n他们引入了多个低资源语言的新评估数据集，并实验了机器翻译、在语境学习中使用大型语言模型，以及一种新颖的中间训练方法 POA（利用段落级信息）。\n结果显示，这些方法显著超过了现有技术水平，其中在语境学习通常表现最佳，但 POA 方法提供了计算开销更低的竞争性替代方案。\n此外，研究发现，语言相似性本身不足以预测跨语言转移的成功，语义内容和结构的相似性同样至关重要。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The first two authors share equal contribution",
      "pdf_url": "http://arxiv.org/pdf/2409.20054v1",
      "published_date": "2024-09-30 07:59:41 UTC",
      "updated_date": "2024-09-30 07:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:26:43.435471"
    },
    {
      "arxiv_id": "2409.20053v2",
      "title": "GUNDAM: Aligning Large Language Models with Graph Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Sheng Ouyang",
        "Yulan Hu",
        "Ge Chen",
        "Yong Liu"
      ],
      "abstract": "Large Language Models (LLMs) have achieved impressive results in processing\ntext data, which has sparked interest in applying these models beyond textual\ndata, such as graphs. In the field of graph learning, there is a growing\ninterest in harnessing LLMs to comprehend and manipulate graph-structured data.\nExisting research predominantly focuses on graphs with rich textual features,\nsuch as knowledge graphs or text attribute graphs, leveraging LLMs' ability to\nprocess text but inadequately addressing graph structure. This work\nspecifically aims to assess and enhance LLMs' abilities to comprehend and\nutilize the structural knowledge inherent in graph data itself, rather than\nfocusing solely on graphs rich in textual content. To achieve this, we\nintroduce the \\textbf{G}raph \\textbf{U}nderstanding for \\textbf{N}atural\nLanguage \\textbf{D}riven \\textbf{A}nalytical \\textbf{M}odel (\\model). This\nmodel adapts LLMs to better understand and engage with the structure of graph\ndata, enabling them to perform complex reasoning tasks by leveraging the\ngraph's structure itself. Our experimental evaluations on graph reasoning\nbenchmarks not only substantiate that \\model~ outperforms the SOTA baselines\nfor comparisons. But also reveals key factors affecting the graph reasoning\ncapabilities of LLMs. Moreover, we provide a theoretical analysis illustrating\nhow reasoning paths can enhance LLMs' reasoning capabilities.",
      "tldr_zh": "该研究探讨了如何提升大型语言模型（LLMs）对图结构数据的理解能力，而非仅依赖文本丰富的图。论文引入了 GUNDAM 模型（Graph Understanding for Natural Language Driven Analytical Model），通过适应 LLMs 来处理图的内在结构，从而支持复杂图推理任务。实验结果显示，GUNDAM 在图推理基准上超越了 SOTA 基线，并揭示了影响 LLMs 图推理能力的关键因素，同时提供了理论分析解释推理路径的作用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20053v2",
      "published_date": "2024-09-30 07:59:10 UTC",
      "updated_date": "2024-10-08 06:00:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:26:53.868543"
    },
    {
      "arxiv_id": "2409.20052v2",
      "title": "Mitigating Propensity Bias of Large Language Models for Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Guixian Zhang",
        "Guan Yuan",
        "Debo Cheng",
        "Lin Liu",
        "Jiuyong Li",
        "Shichao Zhang"
      ],
      "abstract": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models.",
      "tldr_zh": "该研究探讨了Large Language Models (LLMs) 在推荐系统中的倾向性偏置(propensity bias)问题，这些偏置会导致侧信息在低维子空间中发生维度坍缩，从而影响用户偏好捕捉和推荐准确性。为解决此问题，研究提出了一种新框架Counterfactual LLM Recommendation (CLLMR)，包括基于谱的侧信息编码器(spectrum-based side information encoder)来嵌入历史交互的结构信息，以及利用反事实推理(counterfactual inference)来对抗LLMs引入的偏置。通过广泛实验验证，CLLMR框架显著提升了各种推荐模型的性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20052v2",
      "published_date": "2024-09-30 07:57:13 UTC",
      "updated_date": "2025-04-11 05:07:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:27:06.349252"
    },
    {
      "arxiv_id": "2409.20042v2",
      "title": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer Scoring with Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Menna Fateen",
        "Bo Wang",
        "Tsunenori Mine"
      ],
      "abstract": "Automatic short answer scoring (ASAS) helps reduce the grading burden on\neducators but often lacks detailed, explainable feedback. Existing methods in\nASAS with feedback (ASAS-F) rely on fine-tuning language models with limited\ndatasets, which is resource-intensive and struggles to generalize across\ncontexts. Recent approaches using large language models (LLMs) have focused on\nscoring without extensive fine-tuning. However, they often rely heavily on\nprompt engineering and either fail to generate elaborated feedback or do not\nadequately evaluate it. In this paper, we propose a modular retrieval augmented\ngeneration based ASAS-F system that scores answers and generates feedback in\nstrict zero-shot and few-shot learning scenarios. We design our system to be\nadaptable to various educational tasks without extensive prompt engineering\nusing an automatic prompt generation framework. Results show an improvement in\nscoring accuracy by 9\\% on unseen questions compared to fine-tuning, offering a\nscalable and cost-effective solution.",
      "tldr_zh": "本文提出一个模块化的基于RAG（Retrieval Augmented Generation）的系统，用于自动短答案评分（ASAS）并提供详细反馈，旨在解决现有ASAS-F方法依赖微调语言模型（LLMs）而导致的资源消耗和泛化性差的问题。该系统采用自动提示生成框架，支持严格的zero-shot和few-shot学习场景，能够适应各种教育任务而无需复杂提示工程。实验结果显示，与微调方法相比，该系统在未见问题上的评分准确率提高了9%，为可扩展且成本有效的教育评估解决方案提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20042v2",
      "published_date": "2024-09-30 07:48:55 UTC",
      "updated_date": "2024-10-10 01:45:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:27:19.536877"
    },
    {
      "arxiv_id": "2409.20016v2",
      "title": "Personalisation via Dynamic Policy Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Ajsal Shereef Palattuparambil",
        "Thommen George Karimpanal",
        "Santu Rana"
      ],
      "abstract": "Deep reinforcement learning (RL) policies, although optimal in terms of task\nrewards, may not align with the personal preferences of human users. To ensure\nthis alignment, a naive solution would be to retrain the agent using a reward\nfunction that encodes the user's specific preferences. However, such a reward\nfunction is typically not readily available, and as such, retraining the agent\nfrom scratch can be prohibitively expensive. We propose a more practical\napproach - to adapt the already trained policy to user-specific needs with the\nhelp of human feedback. To this end, we infer the user's intent through\ntrajectory-level feedback and combine it with the trained task policy via a\ntheoretically grounded dynamic policy fusion approach. As our approach collects\nhuman feedback on the very same trajectories used to learn the task policy, it\ndoes not require any additional interactions with the environment, making it a\nzero-shot approach. We empirically demonstrate in a number of environments that\nour proposed dynamic policy fusion approach consistently achieves the intended\ntask while simultaneously adhering to user-specific needs.",
      "tldr_zh": "本研究针对深度强化学习（RL）策略在任务奖励上最优但可能不符合用户个人偏好的问题，提出了一种动态策略融合（dynamic policy fusion）方法。该方法通过轨迹级人类反馈推断用户意图，并将其与已训练的任务策略理论上结合，实现策略的个性化适应。作为一个零-shot 方法，它无需额外的环境交互，从而降低了成本。在多个环境中实验验证表明，该方法能同时实现预定任务并有效满足用户特定需求。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.20016v2",
      "published_date": "2024-09-30 07:23:47 UTC",
      "updated_date": "2024-10-03 03:15:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:27:31.162151"
    },
    {
      "arxiv_id": "2409.20012v2",
      "title": "Towards Robust Multimodal Sentiment Analysis with Incomplete Data",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Zhang",
        "Wenbin Wang",
        "Tianshu Yu"
      ],
      "abstract": "The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an\nemerging direction seeking to tackle the issue of data incompleteness.\nRecognizing that the language modality typically contains dense sentiment\ninformation, we consider it as the dominant modality and present an innovative\nLanguage-dominated Noise-resistant Learning Network (LNLN) to achieve robust\nMSA. The proposed LNLN features a dominant modality correction (DMC) module and\ndominant modality based multimodal learning (DMML) module, which enhances the\nmodel's robustness across various noise scenarios by ensuring the quality of\ndominant modality representations. Aside from the methodical design, we perform\ncomprehensive experiments under random data missing scenarios, utilizing\ndiverse and meaningful settings on several popular datasets (\\textit{e.g.,}\nMOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and\nfairness compared to existing evaluations in the literature. Empirically, LNLN\nconsistently outperforms existing baselines, demonstrating superior performance\nacross these challenging and extensive evaluation metrics.",
      "tldr_zh": "这篇论文针对多模态情感分析 (Multimodal Sentiment Analysis, MSA) 中数据不完整的问题，提出了一种创新的 Language-dominated Noise-resistant Learning Network (LNLN)，将语言模态作为主导模态来提升模型的鲁棒性。LNLN 包括 Dominant Modality Correction (DMC) 模块用于优化主导模态表示，以及 Dominant Modality based Multimodal Learning (DMML) 模块来处理多模态学习中的噪声场景。实验结果显示，在 MOSI、MOSEI 和 SIMS 等数据集上的随机数据缺失测试中，LNLN  consistently 超越现有基线，在各种评估指标上表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20012v2",
      "published_date": "2024-09-30 07:14:31 UTC",
      "updated_date": "2024-11-01 08:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:27:43.034794"
    },
    {
      "arxiv_id": "2409.20010v1",
      "title": "Customized Information and Domain-centric Knowledge Graph Construction with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Frank Wawrzik",
        "Matthias Plaue",
        "Savan Vekariya",
        "Christoph Grimm"
      ],
      "abstract": "In this paper we propose a novel approach based on knowledge graphs to\nprovide timely access to structured information, to enable actionable\ntechnology intelligence, and improve cyber-physical systems planning. Our\nframework encompasses a text mining process, which includes information\nretrieval, keyphrase extraction, semantic network creation, and topic map\nvisualization. Following this data exploration process, we employ a selective\nknowledge graph construction (KGC) approach supported by an electronics and\ninnovation ontology-backed pipeline for multi-objective decision-making with a\nfocus on cyber-physical systems. We apply our methodology to the domain of\nautomotive electrical systems to demonstrate the approach, which is scalable.\nOur results demonstrate that our construction process outperforms GraphGPT as\nwell as our bi-LSTM and transformer REBEL with a pre-defined dataset by several\ntimes in terms of class recognition, relationship construction and correct\n\"sublass of\" categorization. Additionally, we outline reasoning applications\nand provide a comparison with Wikidata to show the differences and advantages\nof the approach.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（Large Language Models）的定制化信息和领域中心知识图谱构建框架，旨在提供结构化信息以支持技术情报和网络物理系统（cyber-physical systems）规划。该框架包括文本挖掘过程（如信息检索、关键短语提取、语义网络创建和主题地图可视化），随后采用选择性知识图谱构建（KGC）方法，并结合电子和创新本体进行多目标决策。研究在汽车电气系统领域应用该方法，结果显示其在类识别、关系构建和“subclass of”分类上显著优于GraphGPT、bi-LSTM和transformer REBEL模型。此外，通过与Wikidata的比较，该方法展示了更高的准确性和可扩展性，突显了其在推理应用中的优势。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at CAIPI Workshop at AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.20010v1",
      "published_date": "2024-09-30 07:08:28 UTC",
      "updated_date": "2024-09-30 07:08:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:27:56.088940"
    },
    {
      "arxiv_id": "2409.20005v1",
      "title": "Model Selection with a Shapelet-based Distance Measure for Multi-source Transfer Learning in Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jiseok Lee",
        "Brian Kenji Iwana"
      ],
      "abstract": "Transfer learning is a common practice that alleviates the need for extensive\ndata to train neural networks. It is performed by pre-training a model using a\nsource dataset and fine-tuning it for a target task. However, not every source\ndataset is appropriate for each target dataset, especially for time series. In\nthis paper, we propose a novel method of selecting and using multiple datasets\nfor transfer learning for time series classification. Specifically, our method\ncombines multiple datasets as one source dataset for pre-training neural\nnetworks. Furthermore, for selecting multiple sources, our method measures the\ntransferability of datasets based on shapelet discovery for effective source\nselection. While traditional transferability measures require considerable time\nfor pre-training all the possible sources for source selection of each possible\narchitecture, our method can be repeatedly used for every possible architecture\nwith a single simple computation. Using the proposed method, we demonstrate\nthat it is possible to increase the performance of temporal convolutional\nneural networks (CNN) on time series datasets.",
      "tldr_zh": "这篇论文提出了一种基于 Shapelet-based Distance Measure 的方法，用于多源转移学习（Multi-source Transfer Learning）在时间序列分类（Time Series Classification）中的模型选择，以解决源数据集不适配的问题。方法将多个数据集组合成一个源数据集进行神经网络预训练，并通过 Shapelet discovery 评估数据集的可转移性，从而高效选择合适的源。相比传统方法，该方法只需单次计算即可适用于不同架构，避免了重复预训练的耗时。实验结果表明，这种方法显著提高了时间卷积神经网络（Temporal Convolutional Neural Networks, CNN）在时间序列数据集上的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at International Conference on Pattern Recognition 2024\n  (ICPR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.20005v1",
      "published_date": "2024-09-30 06:57:30 UTC",
      "updated_date": "2024-09-30 06:57:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:28:07.641864"
    },
    {
      "arxiv_id": "2410.07165v1",
      "title": "Complex Logical Query Answering by Calibrating Knowledge Graph Completion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "abstract": "Complex logical query answering (CLQA) is a challenging task that involves\nfinding answer entities for complex logical queries over incomplete knowledge\ngraphs (KGs). Previous research has explored the use of pre-trained knowledge\ngraph completion (KGC) models, which can predict the missing facts in KGs, to\nanswer complex logical queries. However, KGC models are typically evaluated\nusing ranking evaluation metrics, which may result in values of predictions of\nKGC models that are not well-calibrated. In this paper, we propose a method for\ncalibrating KGC models, namely CKGC, which enables KGC models to adapt to\nanswering complex logical queries. Notably, CKGC is lightweight and effective.\nThe adaptation function is simple, allowing the model to quickly converge\nduring the adaptation process. The core concept of CKGC is to map the values of\npredictions of KGC models to the range [0, 1], ensuring that values associated\nwith true facts are close to 1, while values linked to false facts are close to\n0. Through experiments on three benchmark datasets, we demonstrate that our\nproposed calibration method can significantly boost model performance in the\nCLQA task. Moreover, our approach can enhance the performance of CLQA while\npreserving the ranking evaluation metrics of KGC models. The code is available\nat https://github.com/changyi7231/CKGC.",
      "tldr_zh": "本研究针对复杂逻辑查询回答（CLQA）任务，提出了一种校准知识图谱补全（KGC）模型的方法，名为 CKGC，以解决 KGC 模型在不完整知识图谱（KGs）上预测值不校准的问题。CKGC 通过一个轻量级且简单的适应函数，将 KGC 模型的预测值映射到 [0, 1] 的范围，确保真实事实的值接近 1，而虚假事实的值接近 0，从而快速提升模型在 CLQA 任务中的表现。在三个基准数据集上的实验表明，CKGC 显著提高了查询回答性能，同时保持了 KGC 模型的排名评估指标，且相关代码已开源。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.07165v1",
      "published_date": "2024-09-30 06:51:50 UTC",
      "updated_date": "2024-09-30 06:51:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:28:19.424628"
    },
    {
      "arxiv_id": "2409.19998v2",
      "title": "Do Influence Functions Work on Large Language Models?",
      "title_zh": "影响函数在大语言模型上是否有效？",
      "authors": [
        "Zhe Li",
        "Wei Zhao",
        "Yige Li",
        "Jun Sun"
      ],
      "abstract": "Influence functions are important for quantifying the impact of individual\ntraining data points on a model's predictions. Although extensive research has\nbeen conducted on influence functions in traditional machine learning models,\ntheir application to large language models (LLMs) has been limited. In this\nwork, we conduct a systematic study to address a key question: do influence\nfunctions work on LLMs? Specifically, we evaluate influence functions across\nmultiple tasks and find that they consistently perform poorly in most settings.\nOur further investigation reveals that their poor performance can be attributed\nto: (1) inevitable approximation errors when estimating the iHVP component due\nto the scale of LLMs, (2) uncertain convergence during fine-tuning, and, more\nfundamentally, (3) the definition itself, as changes in model parameters do not\nnecessarily correlate with changes in LLM behavior. Thus, our study suggests\nthe need for alternative approaches for identifying influential samples.",
      "tldr_zh": "这篇论文探讨了影响函数（influence functions）在大型语言模型（LLMs）上的适用性，通过系统评估多个任务发现，它们在大多数场景中表现不佳。研究揭示了三方面原因：（1）由于 LLMs 规模大，导致估计 iHVP 组件时出现不可避免的近似误差；（2）微调过程中的不确定收敛；（3）影响函数的定义本身存在问题，因为模型参数变化并不一定对应 LLMs 行为的改变。主要结论是，需要开发替代方法来识别影响样本，以更好地量化训练数据对模型预测的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.19998v2",
      "published_date": "2024-09-30 06:50:18 UTC",
      "updated_date": "2024-12-19 19:33:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:28:30.551143"
    },
    {
      "arxiv_id": "2409.19993v1",
      "title": "Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Qin Liu",
        "Wenjie Mo",
        "Terry Tong",
        "Jiashu Xu",
        "Fei Wang",
        "Chaowei Xiao",
        "Muhao Chen"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has significantly impacted\nvarious domains, including Web search, healthcare, and software development.\nHowever, as these models scale, they become more vulnerable to cybersecurity\nrisks, particularly backdoor attacks. By exploiting the potent memorization\ncapacity of LLMs, adversaries can easily inject backdoors into LLMs by\nmanipulating a small portion of training data, leading to malicious behaviors\nin downstream applications whenever the hidden backdoor is activated by the\npre-defined triggers. Moreover, emerging learning paradigms like instruction\ntuning and reinforcement learning from human feedback (RLHF) exacerbate these\nrisks as they rely heavily on crowdsourced data and human feedback, which are\nnot fully controlled. In this paper, we present a comprehensive survey of\nemerging backdoor threats to LLMs that appear during LLM development or\ninference, and cover recent advancement in both defense and detection\nstrategies for mitigating backdoor threats to LLMs. We also outline key\nchallenges in addressing these threats, highlighting areas for future research.",
      "tldr_zh": "这篇论文探讨了大语言模型（Large Language Models, LLMs）的背门攻击（backdoor attacks）风险，这些攻击通过操纵少量训练数据利用LLMs的强大记忆能力，导致下游应用出现恶意行为。论文对backdoor threats在LLMs开发和推理阶段的出现进行了全面调查，并总结了最近的防御和检测策略的进展，包括针对instruction tuning和reinforcement learning from human feedback (RLHF)等新兴学习范式的应对方法。最终，论文指出了当前挑战，如数据控制问题，并强调了未来研究的重点方向，以提升LLMs的安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CR",
      "comment": "The 60th Annual Allerton Conference (Invited Paper). The arXiv\n  version is a pre-IEEE Press publication version",
      "pdf_url": "http://arxiv.org/pdf/2409.19993v1",
      "published_date": "2024-09-30 06:31:36 UTC",
      "updated_date": "2024-09-30 06:31:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:28:42.420863"
    },
    {
      "arxiv_id": "2409.19992v2",
      "title": "A large-scale operational study of fingerprint quality and demographics",
      "title_zh": "翻译失败",
      "authors": [
        "Javier Galbally",
        "Aleksandrs Cepilovs",
        "Ramon Blanco-Gonzalo",
        "Gillian Ormiston",
        "Oscar Miguel-Hurtado",
        "Istvan Sz. Racz"
      ],
      "abstract": "Even though a few initial works have shown on small sets of data some level\nof bias in the performance of fingerprint recognition technology with respect\nto certain demographic groups, there is still not sufficient evidence to\nunderstand the impact that certain factors such as gender, age or finger-type\nmay have on fingerprint quality and, in turn, also on fingerprint matching\naccuracy. The present work addresses this still under researched topic, on a\nlarge-scale database of operational data containing 10-print impressions of\nalmost 16,000 subjects. The results reached provide further insight into the\ndependency of fingerprint quality and demographics, and show that there in fact\nexists a certain degree of performance variability in fingerprint-based\nrecognition systems for different segments of the population. Based on the\nexperimental evaluation, the work points out new observations based on\ndata-driven evidence, provides plausible hypotheses to explain such\nobservations, and concludes with potential follow-up actions that can help to\nreduce the observed fingerprint quality differences. This way, the current\npaper can be considered as a contribution to further increase the algorithmic\nfairness and equality of biometric technology.",
      "tldr_zh": "这篇论文通过大规模数据库（包含近16,000个受试者的10指纹印）研究了指纹质量与人口统计学因素（如性别、年龄和finger-type）之间的关系，填补了现有证据不足的空白。实验结果显示，指纹质量和matching accuracy确实受这些demographics影响，导致指纹识别系统的性能在不同人口群体中存在变异性。论文基于数据驱动证据提出新观察和解释假设，并建议后续行动，以减少指纹质量差异并提升算法公平性（algorithmic fairness）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Extended journal version submitted to IET Biometrics. 10 pages, 5\n  figures Reference conference paper: J. Galbally, A. Cepilovs, R.\n  Blanco-Gonzalo, G. Ormiston, O. Miguel-Hurtado, and I. S. Racz, 'Fingerprint\n  quality per individual finger type: A large-scale study on real operational\n  data' in Proc. IEEE Intl. Workshop on Biometrics and Forensics 2023 (IWBF\n  2023)",
      "pdf_url": "http://arxiv.org/pdf/2409.19992v2",
      "published_date": "2024-09-30 06:30:33 UTC",
      "updated_date": "2024-10-04 14:20:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:28:55.497169"
    },
    {
      "arxiv_id": "2410.03727v3",
      "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Ming",
        "Senthil Purushwalkam",
        "Shrey Pandit",
        "Zixuan Ke",
        "Xuan-Phi Nguyen",
        "Caiming Xiong",
        "Shafiq Joty"
      ],
      "abstract": "Ensuring faithfulness to context in large language models (LLMs) and\nretrieval-augmented generation (RAG) systems is crucial for reliable deployment\nin real-world applications, as incorrect or unsupported information can erode\nuser trust. Despite advancements on standard benchmarks, faithfulness\nhallucination-where models generate responses misaligned with the provided\ncontext-remains a significant challenge. In this work, we introduce FaithEval,\na novel and comprehensive benchmark tailored to evaluate the faithfulness of\nLLMs in contextual scenarios across three diverse tasks: unanswerable,\ninconsistent, and counterfactual contexts. These tasks simulate real-world\nchallenges where retrieval mechanisms may surface incomplete, contradictory, or\nfabricated information. FaithEval comprises 4.9K high-quality problems in\ntotal, validated through a rigorous four-stage context construction and\nvalidation framework, employing both LLM-based auto-evaluation and human\nvalidation. Our extensive study across a wide range of open-source and\nproprietary models reveals that even state-of-the-art models often struggle to\nremain faithful to the given context, and that larger models do not necessarily\nexhibit improved faithfulness.Project is available at:\nhttps://github.com/SalesforceAIResearch/FaithEval.",
      "tldr_zh": "这篇论文引入了 FaithEval，一个新的基准，用于评估大型语言模型（LLMs）和检索增强生成（RAG）系统在处理上下文时的忠诚度问题，尤其在面对无法回答（unanswerable）、不一致（inconsistent）和反事实（counterfactual）场景时。FaithEval 包含 4.9K 个高质量问题，通过一个严格的四阶段上下文构建和验证框架（包括 LLM 自动评估和人工验证）来模拟真实世界的挑战，如不完整或矛盾信息。实验结果显示，即使是先进的开源和专有模型也难以保持对上下文的忠诚度，且模型规模并不直接提升表现，为未来模型改进提供了关键见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The conference version of this paper is published at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.03727v3",
      "published_date": "2024-09-30 06:27:53 UTC",
      "updated_date": "2025-04-24 22:33:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:29:07.588255"
    },
    {
      "arxiv_id": "2409.19984v1",
      "title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models",
      "title_zh": "CONTESTS：一种用于语言模型中跨度概率一致性测试的框架",
      "authors": [
        "Eitan Wagner",
        "Yuli Slavutsky",
        "Omri Abend"
      ],
      "abstract": "Although language model scores are often treated as probabilities, their\nreliability as probability estimators has mainly been studied through\ncalibration, overlooking other aspects. In particular, it is unclear whether\nlanguage models produce the same value for different ways of assigning joint\nprobabilities to word spans. Our work introduces a novel framework, ConTestS\n(Consistency Testing over Spans), involving statistical tests to assess score\nconsistency across interchangeable completion and conditioning orders. We\nconduct experiments on post-release real and synthetic data to eliminate\ntraining effects. Our findings reveal that both Masked Language Models (MLMs)\nand autoregressive models exhibit inconsistent predictions, with autoregressive\nmodels showing larger discrepancies. Larger MLMs tend to produce more\nconsistent predictions, while autoregressive models show the opposite trend.\nMoreover, for both model types, prediction entropies offer insights into the\ntrue word span likelihood and therefore can aid in selecting optimal decoding\nstrategies. The inconsistencies revealed by our analysis, as well their\nconnection to prediction entropies and differences between model types, can\nserve as useful guides for future research on addressing these limitations.",
      "tldr_zh": "本文提出ConTestS框架，用于测试语言模型中词跨度概率分数的一致性，通过统计测试评估不同完成和条件顺序下的预测可靠性。实验在真实和合成数据上进行，结果显示Masked Language Models (MLMs)和autoregressive models均存在不一致性，其中autoregressive models的差异更大，且更大规模的MLMs预测更一致，而autoregressive models则呈相反趋势。此外，预测熵可用于洞察真实词跨度可能性，并指导优化解码策略及未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19984v1",
      "published_date": "2024-09-30 06:24:43 UTC",
      "updated_date": "2024-09-30 06:24:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:29:18.813001"
    },
    {
      "arxiv_id": "2409.19977v1",
      "title": "Knowledge Graph Embedding by Normalizing Flows",
      "title_zh": "翻译失败",
      "authors": [
        "Changyi Xiao",
        "Xiangnan He",
        "Yixin Cao"
      ],
      "abstract": "A key to knowledge graph embedding (KGE) is to choose a proper representation\nspace, e.g., point-wise Euclidean space and complex vector space. In this\npaper, we propose a unified perspective of embedding and introduce uncertainty\ninto KGE from the view of group theory. Our model can incorporate existing\nmodels (i.e., generality), ensure the computation is tractable (i.e.,\nefficiency) and enjoy the expressive power of complex random variables (i.e.,\nexpressiveness). The core idea is that we embed entities/relations as elements\nof a symmetric group, i.e., permutations of a set. Permutations of different\nsets can reflect different properties of embedding. And the group operation of\nsymmetric groups is easy to compute. In specific, we show that the embedding of\nmany existing models, point vectors, can be seen as elements of a symmetric\ngroup. To reflect uncertainty, we first embed entities/relations as\npermutations of a set of random variables. A permutation can transform a simple\nrandom variable into a complex random variable for greater expressiveness,\ncalled a normalizing flow. We then define scoring functions by measuring the\nsimilarity of two normalizing flows, namely NFE. We construct several\ninstantiating models and prove that they are able to learn logical rules.\nExperimental results demonstrate the effectiveness of introducing uncertainty\nand our model. The code is available at https://github.com/changyi7231/NFE.",
      "tldr_zh": "本论文提出了一种基于 Normalizing Flows 的知识图嵌入（KGE）方法，通过引入不确定性并从群论视角统一现有模型。核心思想是将实体和关系嵌入对称群（symmetric group）中，作为一组随机变量的置换，从而提升模型的泛化性（generality）、计算效率（efficiency）和表达能力（expressiveness）。作者定义了基于 Normalizing Flows 的评分函数（NFE），并证明该模型能学习逻辑规则；实验结果显示，引入不确定性显著提高了 KGE 性能。代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19977v1",
      "published_date": "2024-09-30 06:04:34 UTC",
      "updated_date": "2024-09-30 06:04:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:29:30.441181"
    },
    {
      "arxiv_id": "2409.19954v3",
      "title": "Domain Consistency Representation Learning for Lifelong Person Re-Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Shiben Liu",
        "Qiang Wang",
        "Huijie Fan",
        "Weihong Ren",
        "Baojie Fan",
        "Yandong Tang"
      ],
      "abstract": "Lifelong person re-identification (LReID) exhibits a contradictory\nrelationship between intra-domain discrimination and inter-domain gaps when\nlearning from continuous data. Intra-domain discrimination focuses on\nindividual nuances (i.e., clothing type, accessories, etc.), while inter-domain\ngaps emphasize domain consistency. Achieving a trade-off between maximizing\nintra-domain discrimination and minimizing inter-domain gaps is a crucial\nchallenge for improving LReID performance. Most existing methods strive to\nreduce inter-domain gaps through knowledge distillation to maintain domain\nconsistency. However, they often ignore intra-domain discrimination. To address\nthis challenge, we propose a novel domain consistency representation learning\n(DCR) model that explores global and attribute-wise representations as a bridge\nto balance intra-domain discrimination and inter-domain gaps. At the\nintra-domain level, we explore the complementary relationship between global\nand attribute-wise representations to improve discrimination among similar\nidentities. Excessive learning intra-domain discrimination can lead to\ncatastrophic forgetting. We further develop an attribute-oriented\nanti-forgetting (AF) strategy that explores attribute-wise representations to\nenhance inter-domain consistency, and propose a knowledge consolidation (KC)\nstrategy to facilitate knowledge transfer. Extensive experiments show that our\nDCR model achieves superior performance compared to state-of-the-art LReID\nmethods. Our code is publicly available at https://github.com/LiuShiBen/DCR.",
      "tldr_zh": "本文针对终身人重新识别(LReID)中内部域区分（如个体细微差异）和域间差距的矛盾，提出了一种新型领域一致性表示学习(DCR)模型。该模型通过探索全局和属性-wise 表示的互补关系，提高内部域区分能力，同时引入属性导向的反遗忘(AF)策略和知识巩固(KC)策略，以增强域间一致性和缓解灾难性遗忘。实验结果显示，DCR 模型在 LReID 任务上优于现有方法，并已开源代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.19954v3",
      "published_date": "2024-09-30 05:19:09 UTC",
      "updated_date": "2025-04-27 09:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:29:43.752078"
    },
    {
      "arxiv_id": "2409.19951v2",
      "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
      "title_zh": "最弱环节定律：大型语言模型的交叉能力",
      "authors": [
        "Ming Zhong",
        "Aston Zhang",
        "Xuewei Wang",
        "Rui Hou",
        "Wenhan Xiong",
        "Chenguang Zhu",
        "Zhengxing Chen",
        "Liang Tan",
        "Chloe Bi",
        "Mike Lewis",
        "Sravya Popuri",
        "Sharan Narang",
        "Melanie Kambadur",
        "Dhruv Mahajan",
        "Sergey Edunov",
        "Jiawei Han",
        "Laurens van der Maaten"
      ],
      "abstract": "The development and evaluation of Large Language Models (LLMs) have largely\nfocused on individual capabilities. However, this overlooks the intersection of\nmultiple abilities across different types of expertise that are often required\nfor real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and\nthen pair them to form seven common cross capabilities, each supported by a\nmanually constructed taxonomy. Building on these definitions, we introduce\nCrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100\nprompts for each individual and cross capability. To ensure reliable\nevaluation, we involve expert annotators to assess 4,200 model responses,\ngathering 8,400 human ratings with detailed explanations to serve as reference\nexamples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the\nWeakest Link,\" where cross-capability performance is significantly constrained\nby the weakest component. Specifically, across 58 cross-capability scores from\n17 models, 38 scores are lower than all individual capabilities, while 20 fall\nbetween strong and weak, but closer to the weaker ability. These results\nhighlight the under-performance of LLMs in cross-capability tasks, making the\nidentification and improvement of the weakest capabilities a critical priority\nfor future research to optimize performance in complex, multi-dimensional\nscenarios.",
      "tldr_zh": "该研究发现，大型语言模型 (LLMs) 的评估通常只关注单个能力，而忽略了现实任务中常见的交叉能力 (cross capabilities)，因此定义了七个核心单个能力和七个配对形成的交叉能力，并为每个能力构建了手动分类法。论文引入了 CrossEval 基准测试，该测试包含1400个人工标注的提示和8400个人类评分，用于评估LLMs的表现。为此，他们评估了17个模型的58个交叉能力分数，结果显示LLMs遵循“Law of the Weakest Link”法则，即交叉能力的性能主要受最弱组件限制，导致38个分数低于所有单个能力。未来研究应优先识别和提升这些最弱能力，以优化LLMs在复杂多维场景中的性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Data, Code, & Benchmark: www.llm-cross-capabilities.org",
      "pdf_url": "http://arxiv.org/pdf/2409.19951v2",
      "published_date": "2024-09-30 05:12:01 UTC",
      "updated_date": "2024-10-02 22:24:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:29:55.937260"
    },
    {
      "arxiv_id": "2409.19949v2",
      "title": "Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile Diffusion Planner",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyou Fan",
        "Chenjia Bai",
        "Zhao Shan",
        "Haoran He",
        "Yang Zhang",
        "Zhen Wang"
      ],
      "abstract": "Diffusion models have demonstrated their capabilities in modeling\ntrajectories of multi-tasks. However, existing multi-task planners or policies\ntypically rely on task-specific demonstrations via multi-task imitation, or\nrequire task-specific reward labels to facilitate policy optimization via\nReinforcement Learning (RL). They are costly due to the substantial human\nefforts required to collect expert data or design reward functions. To address\nthese challenges, we aim to develop a versatile diffusion planner capable of\nleveraging large-scale inferior data that contains task-agnostic sub-optimal\ntrajectories, with the ability to fast adapt to specific tasks. In this paper,\nwe propose SODP, a two-stage framework that leverages Sub-Optimal data to learn\na Diffusion Planner, which is generalizable for various downstream tasks.\nSpecifically, in the pre-training stage, we train a foundation diffusion\nplanner that extracts general planning capabilities by modeling the versatile\ndistribution of multi-task trajectories, which can be sub-optimal and has wide\ndata coverage. Then for downstream tasks, we adopt RL-based fine-tuning with\ntask-specific rewards to quickly refine the diffusion planner, which aims to\ngenerate action sequences with higher task-specific returns. Experimental\nresults from multi-task domains including Meta-World and Adroit demonstrate\nthat SODP outperforms state-of-the-art methods with only a small amount of data\nfor reward-guided fine-tuning.",
      "tldr_zh": "该研究提出了一种名为 SODP 的两阶段框架，用于训练一个通用的扩散模型（Diffusion Planner），以解决多任务规划中对任务特定演示或奖励标签的依赖问题，从而降低数据收集成本。在预训练阶段，SODP 利用大规模任务无关的次优轨迹（sub-optimal data）来训练基础规划器，提取多任务轨迹的通用分布特征；随后，在细调阶段，通过强化学习（RL）结合任务特定奖励快速适应下游任务。实验结果显示，在 Meta-World 和 Adroit 等多任务领域，SODP 仅需少量数据即可超越最先进方法，显著提升规划性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19949v2",
      "published_date": "2024-09-30 05:05:37 UTC",
      "updated_date": "2025-02-02 13:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:30:06.687028"
    },
    {
      "arxiv_id": "2409.19948v1",
      "title": "JaPOC: Japanese Post-OCR Correction Benchmark using Vouchers",
      "title_zh": "JaPOC：使用凭证的日文后OCR修正基准",
      "authors": [
        "Masato Fujitake"
      ],
      "abstract": "In this paper, we create benchmarks and assess the effectiveness of error\ncorrection methods for Japanese vouchers in OCR (Optical Character Recognition)\nsystems. It is essential for automation processing to correctly recognize\nscanned voucher text, such as the company name on invoices. However, perfect\nrecognition is complex due to the noise, such as stamps. Therefore, it is\ncrucial to correctly rectify erroneous OCR results. However, no publicly\navailable OCR error correction benchmarks for Japanese exist, and methods have\nnot been adequately researched. In this study, we measured text recognition\naccuracy by existing services on Japanese vouchers and developed a post-OCR\ncorrection benchmark. Then, we proposed simple baselines for error correction\nusing language models and verified whether the proposed method could\neffectively correct these errors. In the experiments, the proposed error\ncorrection algorithm significantly improved overall recognition accuracy.",
      "tldr_zh": "本研究创建了JaPOC基准，用于评估日本凭证OCR（Optical Character Recognition）错误修正方法的有效性，针对扫描文本（如发票公司名称）的识别问题，强调噪音（如印章）导致的挑战。研究者测量了现有服务的文本识别准确率，并提出使用语言模型的简单基线方法来实现后OCR修正。实验结果显示，所提算法显著提高了整体识别准确率，为日本OCR错误修正领域提供了首个公开基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to PRICAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.19948v1",
      "published_date": "2024-09-30 05:01:49 UTC",
      "updated_date": "2024-09-30 05:01:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:30:28.716813"
    },
    {
      "arxiv_id": "2409.19940v1",
      "title": "Positive-Sum Fairness: Leveraging Demographic Attributes to Achieve Fair AI Outcomes Without Sacrificing Group Gains",
      "title_zh": "翻译失败",
      "authors": [
        "Samia Belhadj",
        "Sanguk Park",
        "Ambika Seth",
        "Hesham Dar",
        "Thijs Kooi"
      ],
      "abstract": "Fairness in medical AI is increasingly recognized as a crucial aspect of\nhealthcare delivery. While most of the prior work done on fairness emphasizes\nthe importance of equal performance, we argue that decreases in fairness can be\neither harmful or non-harmful, depending on the type of change and how\nsensitive attributes are used. To this end, we introduce the notion of\npositive-sum fairness, which states that an increase in performance that\nresults in a larger group disparity is acceptable as long as it does not come\nat the cost of individual subgroup performance. This allows sensitive\nattributes correlated with the disease to be used to increase performance\nwithout compromising on fairness.\n  We illustrate this idea by comparing four CNN models that make different use\nof the race attribute in the training phase. The results show that removing all\ndemographic encodings from the images helps close the gap in performance\nbetween the different subgroups, whereas leveraging the race attribute as a\nmodel's input increases the overall performance while widening the disparities\nbetween subgroups. These larger gaps are then put in perspective of the\ncollective benefit through our notion of positive-sum fairness to distinguish\nharmful from non harmful disparities.",
      "tldr_zh": "本研究提出“positive-sum fairness”概念，旨在医疗AI公平性中平衡性能提升与群体差异，强调如果整体性能提高不以牺牲任何子群体表现为代价，则导致的群体差距是可以接受的，从而允许使用与疾病相关的敏感属性（如种族）来提升AI效果，而不损害公平性。作者通过比较四个CNN models在训练中使用种族属性的不同方式，发现移除人口统计编码能缩小子群体间的性能差距，而将种族作为模型输入则提升整体性能但扩大差异。实验结果显示，通过positive-sum fairness框架，这些差异可被视为无害的集体收益，为实现公平AI提供新视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19940v1",
      "published_date": "2024-09-30 04:37:23 UTC",
      "updated_date": "2024-09-30 04:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:30:31.535135"
    },
    {
      "arxiv_id": "2409.19924v4",
      "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Wang",
        "Junbo Li",
        "Neel P. Bhatt",
        "Yihan Xi",
        "Qiang Liu",
        "Ufuk Topcu",
        "Zhangyang Wang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their\nability to perform complex reasoning tasks, but their effectiveness in planning\nremains underexplored. In this study, we evaluate the planning capabilities of\nOpenAI's o1 models across a variety of benchmark tasks, focusing on three key\naspects: feasibility, optimality, and generalizability. Through empirical\nevaluations on constraint-heavy tasks (e.g., $\\textit{Barman}$,\n$\\textit{Tyreworld}$) and spatially complex environments (e.g.,\n$\\textit{Termes}$, $\\textit{Floortile}$), we highlight o1-preview's strengths\nin self-evaluation and constraint-following, while also identifying bottlenecks\nin decision-making and memory management, particularly in tasks requiring\nrobust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4\nin adhering to task constraints and managing state transitions in structured\nenvironments. However, the model often generates suboptimal solutions with\nredundant actions and struggles to generalize effectively in spatially complex\ntasks. This pilot study provides foundational insights into the planning\nlimitations of LLMs, offering key directions for future research on improving\nmemory management, decision-making, and generalization in LLM-based planning.\nCode available at https://github.com/VITA-Group/o1-planning.",
      "tldr_zh": "该研究评估了 OpenAI o1 模型的规划能力，焦点在于可行性、最优性和泛化性，通过实证测试在多种基准任务上，如约束密集型任务（Barman、Tyreworld）和空间复杂环境（Termes、Floortile）。结果显示，o1-preview 模型在自评估和遵守任务约束方面优于 GPT-4，能够更好地管理结构化环境的 state transitions，但存在决策和记忆管理瓶颈，导致产生冗余动作和次优解，尤其在空间推理任务中泛化不足。该研究为理解 LLM 在规划方面的局限性提供了基础见解，并建议未来重点改进记忆管理、决策机制和泛化能力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Code available at https://github.com/VITA-Group/o1-planning",
      "pdf_url": "http://arxiv.org/pdf/2409.19924v4",
      "published_date": "2024-09-30 03:58:43 UTC",
      "updated_date": "2024-10-14 03:41:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:30:44.229399"
    },
    {
      "arxiv_id": "2410.00709v1",
      "title": "Binding Affinity Prediction: From Conventional to Machine Learning-Based Approaches",
      "title_zh": "结合亲和力预测：从传统方法到基于机器学习的方法",
      "authors": [
        "Xuefeng Liu",
        "Songhao Jiang",
        "Xiaotian Duan",
        "Archit Vasan",
        "Chong Liu",
        "Chih-chan Tien",
        "Heng Ma",
        "Thomas Brettin",
        "Fangfang Xia",
        "Ian T. Foster",
        "Rick L. Stevens"
      ],
      "abstract": "Protein-ligand binding is the process by which a small molecule (drug or\ninhibitor) attaches to a target protein. The binding affinity, which refers to\nthe strength of this interaction, is central to many important problems in\nbioinformatics such as drug design. An extensive amount of work has been\ndevoted to predicting binding affinity over the past decades due to its\nsignificance. In this paper, we review all significant recent works, focusing\non the methods, features, and benchmark datasets. We have observed a rising\ntrend in the use of traditional machine learning and deep learning models for\npredicting binding affinity, accompanied by an increasing amount of data on\nproteins and small drug-like molecules. While prediction results are constantly\nimproving, we also identify several open questions and potential directions\nthat remain unexplored in the field. This paper could serve as an excellent\nstarting point for machine learning researchers who wish to engage in the study\nof binding affinity, or for anyone with general interests in machine learning,\ndrug discovery, and bioinformatics.",
      "tldr_zh": "这篇论文回顾了蛋白-配体结合亲和力（binding affinity）的预测方法，从传统方法发展到基于机器学习（machine learning）的现代途径。作者重点分析了最近的研究，包括使用的特征（features）、基准数据集（benchmark datasets），以及传统机器学习和深度学习（deep learning）模型的兴起，这些方法得益于蛋白和药物分子数据的不断增加。论文指出，预测准确性持续提升，但仍存在一些未探索的开放问题和潜在方向，并为机器学习研究者提供进入药物设计（drug design）和生物信息学领域的优秀起点。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00709v1",
      "published_date": "2024-09-30 03:40:49 UTC",
      "updated_date": "2024-09-30 03:40:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:30:54.951992"
    },
    {
      "arxiv_id": "2409.19913v3",
      "title": "Scaling Optimal LR Across Token Horizons",
      "title_zh": "翻译失败",
      "authors": [
        "Johan Bjorck",
        "Alon Benhaim",
        "Vishrav Chaudhary",
        "Furu Wei",
        "Xia Song"
      ],
      "abstract": "State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.",
      "tldr_zh": "本研究探讨了在大型语言模型（LLMs）训练中，如何根据数据集大小（即 token horizon）转移最优学习率（LR），以避免大规模实验中的昂贵调优。研究通过大规模实证分析发现，最优 LR 会随 token horizon 延长而显著减小，并遵循一个缩放定律，从而能从较短 horizon 准确预测较长 horizon 的 LR。作者提供了一个无需额外开销的经验法则，用于跨 token horizon 转移 LR，并评估了 LLaMA-1 模型因使用过高 LR 而导致的性能损失，强调了数据集大小超参数转移在 LLM 训练中的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.19913v3",
      "published_date": "2024-09-30 03:32:02 UTC",
      "updated_date": "2025-02-24 02:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:31:06.881840"
    },
    {
      "arxiv_id": "2409.19898v2",
      "title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yuho Lee",
        "Taewon Yun",
        "Jason Cai",
        "Hang Su",
        "Hwanjun Song"
      ],
      "abstract": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.",
      "tldr_zh": "本论文针对现有摘要评估基准的不足（如缺乏多样输入场景、狭隘维度和粗粒度标注），提出了UniSumEval基准，以实现统一的、细粒度的、多维度的摘要评估。\nUniSumEval扩展了输入上下文（如领域和长度），并利用AI辅助数据创建（如识别潜在幻觉文本）和简化人类标注任务。\n通过该基准，研究者测试了九个最新LLMs的摘要性能，提供对不同输入上下文和评估维度的见解，并比较了SOTA自动摘要评估器。\n基准数据已公开在GitHub上，旨在提升LLMs摘要质量评估的可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP-Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.19898v2",
      "published_date": "2024-09-30 02:56:35 UTC",
      "updated_date": "2024-10-01 07:11:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:31:20.074372"
    },
    {
      "arxiv_id": "2409.19894v2",
      "title": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation",
      "title_zh": "TRANSAGENT：一种基于LLM的多智能体系统，用于代码翻译",
      "authors": [
        "Zhiqiang Yuan",
        "Weitong Chen",
        "Hanlin Wang",
        "Kai Yu",
        "Xin Peng",
        "Yiling Lou"
      ],
      "abstract": "Code translation converts code from one programming language to another while\nmaintaining its original functionality, which is crucial for software\nmigration, system refactoring, and cross-platform development. Traditional\nrule-based methods rely on manually-written rules, which can be time-consuming\nand often result in less readable code. To overcome this, learning-based\nmethods have been developed, leveraging parallel data to train models for\nautomated code translation. More recently, the advance of Large Language Models\n(LLMs) further boosts learning-based code translation. Although promising,\nLLM-translated program still suffers from diverse quality issues (e.g., syntax\nerrors and semantic errors). In particular, it can be challenging for LLMs to\nself-debug these errors when simply provided with the corresponding error\nmessages.\n  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\nwhich enhances LLM-based code translation by fixing the syntax errors and\nsemantic errors with the synergy between four LLM-based agents, including\nInitial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\nFixer. The main insight of TRANSAGENT is to first localize the error code block\nin the target program based on the execution alignment between the target and\nsource program, which can narrow down the fixing space and thus lower down the\nfixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\nfrom recent programming tasks to mitigate the potential data leakage issue. On\nour benchmark, TRANSAGENT outperforms the latest LLM-based code translation\ntechnique UniTrans in both translation effectiveness and efficiency;\nadditionally, our evaluation on different LLMs show the generalization of\nTRANSAGENT and our ablation study shows the contribution of each agent.",
      "tldr_zh": "该研究提出TRANSAGENT，一种基于Large Language Models (LLMs)的多智能体系统，用于提升代码翻译的准确性，通过四个代理（Initial Code Translator、Syntax Error Fixer、Code Aligner和Semantic Error Fixer）协同修复语法和语义错误。系统的主要创新在于利用源程序和目标程序的执行对齐来定位错误代码块，从而缩小修复范围并降低难度。为评估TRANSAGENT，研究者构建了一个新基准，结果显示该系统在翻译效果和效率上优于现有方法UniTrans，并证明其对不同LLMs具有泛化性，同时消融研究确认了每个代理的贡献。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19894v2",
      "published_date": "2024-09-30 02:53:03 UTC",
      "updated_date": "2024-10-01 04:35:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:31:31.034693"
    },
    {
      "arxiv_id": "2409.19886v1",
      "title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhao Chen",
        "Weisen Jiang",
        "Baijiong Lin",
        "James T. Kwok",
        "Yu Zhang"
      ],
      "abstract": "Recent works show that assembling multiple off-the-shelf large language\nmodels (LLMs) can harness their complementary abilities. To achieve this,\nrouting is a promising method, which learns a router to select the most\nsuitable LLM for each query. However, existing routing models are ineffective\nwhen multiple LLMs perform well for a query. To address this problem, in this\npaper, we propose a method called query-based Router by Dual Contrastive\nlearning (RouterDC). The RouterDC model consists of an encoder and LLM\nembeddings, and we propose two contrastive learning losses to train the\nRouterDC model. Experimental results show that RouterDC is effective in\nassembling LLMs and largely outperforms individual top-performing LLMs as well\nas existing routing methods on both in-distribution (+2.76\\%) and\nout-of-distribution (+1.90\\%) tasks. Source code is available at\nhttps://github.com/shuhao02/RouterDC.",
      "tldr_zh": "该论文提出RouterDC，一种基于双对比学习（Dual Contrastive Learning）的查询路由方法，用于组装多个大型语言模型（LLMs），以利用它们的互补能力。RouterDC模型包括一个编码器和LLM嵌入，通过两个对比学习损失函数训练，解决了现有路由模型在多个LLMs均表现良好时的选择无效问题。实验结果显示，RouterDC在分布内任务上比顶级LLMs和现有方法提高2.76%，在分布外任务上提高1.90%，证明了其在组装LLMs方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.19886v1",
      "published_date": "2024-09-30 02:31:40 UTC",
      "updated_date": "2024-09-30 02:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:31:43.553936"
    },
    {
      "arxiv_id": "2409.19884v2",
      "title": "SWIM: Short-Window CNN Integrated with Mamba for EEG-Based Auditory Spatial Attention Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyang Zhang",
        "Andrew Thwaites",
        "Alexandra Woolgar",
        "Brian Moore",
        "Chao Zhang"
      ],
      "abstract": "In complex auditory environments, the human auditory system possesses the\nremarkable ability to focus on a specific speaker while disregarding others. In\nthis study, a new model named SWIM, a short-window convolution neural network\n(CNN) integrated with Mamba, is proposed for identifying the locus of auditory\nattention (left or right) from electroencephalography (EEG) signals without\nrelying on speech envelopes. SWIM consists of two parts. The first is a\nshort-window CNN (SW$_\\text{CNN}$), which acts as a short-term EEG feature\nextractor and achieves a final accuracy of 84.9% in the leave-one-speaker-out\nsetup on the widely used KUL dataset. This improvement is due to the use of an\nimproved CNN structure, data augmentation, multitask training, and model\ncombination. The second part, Mamba, is a sequence model first applied to\nauditory spatial attention decoding to leverage the long-term dependency from\nprevious SW$_\\text{CNN}$ time steps. By joint training SW$_\\text{CNN}$ and\nMamba, the proposed SWIM structure uses both short-term and long-term\ninformation and achieves an accuracy of 86.2%, which reduces the classification\nerrors by a relative 31.0% compared to the previous state-of-the-art result.\nThe source code is available at https://github.com/windowso/SWIM-ASAD.",
      "tldr_zh": "本文提出SWIM模型，用于基于EEG信号的auditory spatial attention decoding，旨在识别听觉注意焦点（左或右）而不依赖speech envelopes。SWIM由Short-window CNN（SW_CNN）作为短期EEG特征提取器和Mamba作为序列模型组成，通过改进的CNN结构、数据增强、多任务训练和模型组合，SW_CNN在KUL数据集的leave-one-speaker-out设置中达到84.9%的准确率。联合训练SW_CNN和Mamba后，SWIM利用短期和长期信息，提升整体准确率至86.2%，较之前最先进方法减少31.0%的分类错误。该模型为复杂听觉环境下的注意力解码提供了高效解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "accepted by SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.19884v2",
      "published_date": "2024-09-30 02:28:32 UTC",
      "updated_date": "2024-11-27 12:30:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:31:55.938740"
    },
    {
      "arxiv_id": "2409.19877v1",
      "title": "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Huangyu Dai",
        "Ben Chen",
        "Kaidi Chen",
        "Ying Han",
        "Zihan Liang",
        "Wen Jiang"
      ],
      "abstract": "For crosslingual conversation and trade, Neural Machine Translation (NMT) is\npivotal yet faces persistent challenges with monotony and repetition in\ngenerated content. Traditional solutions that rely on penalizing text\nredundancy or token reoccurrence have shown limited efficacy, particularly for\nlengthy article and e-commerce descriptions with inherent redundancy, even with\nthe advent of Large Language Models (LLMs). This paper investigates the\nunderlying causes of textual repetition through the lens of information\nentropy, attributing the phenomenon to the elevated uncertainty within the\ninput text. To address this, a novel algorithm named Contrastive Token Learning\nwith Similarity Decay (CTSD) is introduced, which modulates the suppression of\ntokens dynamically, informed by varying attention weights and inter-token\ndistances. Furthermore, an e-commerce dataset comprised of title texts of\nonline real items is compiled and released susceptible to hallucination\ntranslations to benchmark the algorithm. Extensive evaluations demonstrate that\nCTSD significantly outperforms existing approaches in precision and\ngeneralizability. Additional online A/B testing underscores its practical\nvalue, showing marked improvements in user engagement and conversion. Notably,\nthis method has been implemented with full traffic on eight multilingual sites\nof alibaba.com, the largest B2B e-commerce platform in the world.",
      "tldr_zh": "该论文针对神经机器翻译 (NMT) 中生成的文本重复问题（如单调性和冗余），通过信息熵视角分析其根因在于输入文本的高不确定性。作者提出了一种新算法 Contrastive Token Learning with Similarity Decay (CTSD)，该方法动态抑制标记基于注意力权重和标记间距离，从而有效减少重复。实验结果显示，CTSD 在精确性和泛化性上显著优于现有方法，并在阿里巴巴八个多语言网站的全流量部署中，提升了用户参与度和转换率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP'24 Findings. 12 pages, 4 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.19877v1",
      "published_date": "2024-09-30 02:21:39 UTC",
      "updated_date": "2024-09-30 02:21:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:32:06.837683"
    },
    {
      "arxiv_id": "2409.19871v1",
      "title": "TSI: A Multi-View Representation Learning Approach for Time Series Forecasting",
      "title_zh": "TSI：一种多视图表示学习方法，用于时间序列预测",
      "authors": [
        "Wentao Gao",
        "Ziqi Xu",
        "Jiuyong Li",
        "Lin Liu",
        "Jixue Liu",
        "Thuc Duy Le",
        "Debo Cheng",
        "Yanchang Zhao",
        "Yun Chen"
      ],
      "abstract": "As the growing demand for long sequence time-series forecasting in real-world\napplications, such as electricity consumption planning, the significance of\ntime series forecasting becomes increasingly crucial across various domains.\nThis is highlighted by recent advancements in representation learning within\nthe field. This study introduces a novel multi-view approach for time series\nforecasting that innovatively integrates trend and seasonal representations\nwith an Independent Component Analysis (ICA)-based representation. Recognizing\nthe limitations of existing methods in representing complex and\nhigh-dimensional time series data, this research addresses the challenge by\ncombining TS (trend and seasonality) and ICA (independent components)\nperspectives. This approach offers a holistic understanding of time series\ndata, going beyond traditional models that often miss nuanced, nonlinear\nrelationships. The efficacy of TSI model is demonstrated through comprehensive\ntesting on various benchmark datasets, where it shows superior performance over\ncurrent state-of-the-art models, particularly in multivariate forecasting. This\nmethod not only enhances the accuracy of forecasting but also contributes\nsignificantly to the field by providing a more in-depth understanding of time\nseries data. The research which uses ICA for a view lays the groundwork for\nfurther exploration and methodological advancements in time series forecasting,\nopening new avenues for research and practical applications.",
      "tldr_zh": "本研究提出了一种名为 TSI 的多视图表示学习方法，用于时间序列预测（Time Series Forecasting），旨在通过整合趋势和季节性（TS）表示与基于 Independent Component Analysis (ICA) 的表示，解决现有模型在处理复杂高维数据时的局限性。TSI 方法提供了一个更全面的视角，能够捕捉传统模型忽略的非线性关系，并在多个基准数据集上进行全面测试，尤其在多变量预测中表现出色，超越了现有最先进模型。实验结果显示，TSI 显著提升了预测准确性，并为时间序列预测领域奠定基础，促进进一步的研究和实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AJCAI Oral Accepted",
      "pdf_url": "http://arxiv.org/pdf/2409.19871v1",
      "published_date": "2024-09-30 02:11:57 UTC",
      "updated_date": "2024-09-30 02:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:32:28.826754"
    },
    {
      "arxiv_id": "2410.00064v3",
      "title": "M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning",
      "title_zh": "M2Distill：多模态蒸馏用于终身模仿",
      "authors": [
        "Kaushik Roy",
        "Akila Dissanayake",
        "Brendan Tidd",
        "Peyman Moghadam"
      ],
      "abstract": "Lifelong imitation learning for manipulation tasks poses significant\nchallenges due to distribution shifts that occur in incremental learning steps.\nExisting methods often focus on unsupervised skill discovery to construct an\never-growing skill library or distillation from multiple policies, which can\nlead to scalability issues as diverse manipulation tasks are continually\nintroduced and may fail to ensure a consistent latent space throughout the\nlearning process, leading to catastrophic forgetting of previously learned\nskills. In this paper, we introduce M2Distill, a multi-modal distillation-based\nmethod for lifelong imitation learning focusing on preserving consistent latent\nspace across vision, language, and action distributions throughout the learning\nprocess. By regulating the shifts in latent representations across different\nmodalities from previous to current steps, and reducing discrepancies in\nGaussian Mixture Model (GMM) policies between consecutive learning steps, we\nensure that the learned policy retains its ability to perform previously\nlearned tasks while seamlessly integrating new skills. Extensive evaluations on\nthe LIBERO lifelong imitation learning benchmark suites, including\nLIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL, demonstrate that our method\nconsistently outperforms prior state-of-the-art methods across all evaluated\nmetrics.",
      "tldr_zh": "该论文提出 M2Distill，一种多模态蒸馏方法，用于解决终身模仿学习中的分布偏移问题，例如灾难性遗忘和技能库扩展挑战。M2Distill 通过保持视觉、语言和动作分布的潜在空间一致性，并减少 Gaussian Mixture Model (GMM) 策略之间的差异，确保政策能保留先前任务技能并无缝整合新技能。实验在 LIBERO-OBJECT、LIBERO-GOAL 和 LIBERO-SPATIAL 基准上表明，该方法在所有评估指标上均超越现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.00064v3",
      "published_date": "2024-09-30 01:43:06 UTC",
      "updated_date": "2025-03-07 01:29:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:32:30.766895"
    },
    {
      "arxiv_id": "2409.19841v2",
      "title": "Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Hsiang Kao",
        "Bharath Hariharan"
      ],
      "abstract": "Despite its widespread use in neural networks, error backpropagation has\nfaced criticism for its lack of biological plausibility, suffering from issues\nsuch as the backward locking problem and the weight transport problem. These\nlimitations have motivated researchers to explore more biologically plausible\nlearning algorithms that could potentially shed light on how biological neural\nsystems adapt and learn. Inspired by the counter-current exchange mechanisms\nobserved in biological systems, we propose counter-current learning (CCL), a\nbiologically plausible framework for credit assignment in neural networks. This\nframework employs a feedforward network to process input data and a feedback\nnetwork to process targets, with each network enhancing the other through\nanti-parallel signal propagation. By leveraging the more informative signals\nfrom the bottom layer of the feedback network to guide the updates of the top\nlayer of the feedforward network and vice versa, CCL enables the simultaneous\ntransformation of source inputs to target outputs and the dynamic mutual\ninfluence of these transformations. Experimental results on MNIST,\nFashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and\nconvolutional neural networks demonstrate that CCL achieves comparable\nperformance to other biologically plausible algorithms while offering a more\nbiologically realistic learning mechanism. Furthermore, we showcase the\napplicability of our approach to an autoencoder task, underscoring its\npotential for unsupervised representation learning. Our work presents a\ndirection for biologically inspired and plausible learning algorithms, offering\nan alternative mechanism of learning and adaptation in neural networks.",
      "tldr_zh": "本研究批评了传统误差 backpropagation 在神经网络中的生物学合理性问题，如 backward locking 和 weight transport，并提出 counter-current learning (CCL) 框架，这是一种受生物反向交换机制启发的双网络方法。CCL 利用 feedforward network 处理输入数据和 feedback network 处理目标，通过反向信号传播实现网络间相互增强和动态更新，从而实现输入到输出的同时转换。实验在 MNIST、FashionMNIST、CIFAR10 和 CIFAR100 数据集上，使用多层感知器和卷积神经网络，证明 CCL 的性能与其它生物学可信算法相当，并展示了其在自编码器任务中的无监督表示学习潜力。该框架为生物学启发的神经网络学习机制提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2024. Code available at\n  https://github.com/IandRover/CCL-NeurIPS24",
      "pdf_url": "http://arxiv.org/pdf/2409.19841v2",
      "published_date": "2024-09-30 00:47:13 UTC",
      "updated_date": "2024-10-23 16:27:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:32:43.809623"
    },
    {
      "arxiv_id": "2409.19839v5",
      "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
      "title_zh": "ForecastBench：AI 预测能力的动态基准",
      "authors": [
        "Ezra Karger",
        "Houtan Bastani",
        "Chen Yueh-Han",
        "Zachary Jacobs",
        "Danny Halawi",
        "Fred Zhang",
        "Philip E. Tetlock"
      ],
      "abstract": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
      "tldr_zh": "这篇论文引入了ForecastBench，一个动态基准，用于评估机器学习（ML）系统在标准化预测问题上的准确性。基准包含1000个自动生成且定期更新的未来事件问题，确保无数据泄露风险，通过比较专家预测者、一般公众和大型语言模型（LLMs）的表现来量化能力。实验结果显示，LLMs在200个问题子集上的表现不如人类专家（p-value < 0.001），并在www.forecastbench.org上公开了系统和人类分数的排行榜。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19839v5",
      "published_date": "2024-09-30 00:41:51 UTC",
      "updated_date": "2025-02-28 12:35:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T05:32:55.459526"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 109,
  "processed_papers_count": 109,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T05:33:12.587767"
}
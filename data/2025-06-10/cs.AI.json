{
  "date": "2025-06-10",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-10 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv å¯è°“æ˜¯æ˜Ÿå…‰ç† ç† ï¼Œå¤§ä½¬äº‘é›†ã€‚**Kaiming He (ä½•æºæ˜)** å¸¦æ¥äº†å…³äº Diffusion Model è¡¨å¾å­¦ä¹ çš„æ–°æ€è€ƒï¼›**Li Fei-Fei (æé£é£)** å›¢é˜Ÿæ¨å‡ºäº†æ— ç›‘ç£æœºå™¨äººâ€œå¯ä¾›æ€§â€è’¸é¦æ¡†æ¶ï¼›**Yejin Choi** å›¢é˜Ÿæ¢ç´¢äº†å¦‚ä½•é€šè¿‡è‹æ ¼æ‹‰åº•å¼çš„æé—®æ¿€å‘ VLM çš„æ¨ç†èƒ½åŠ›ï¼›ä»¥åŠ **Melanie Mitchell** ç­‰äººä»å¤æ‚ç³»ç»Ÿè§†è§’æ¢è®¨ LLM çš„â€œæ¶Œç°â€ç°è±¡ã€‚æ­¤å¤–ï¼Œå…³äº Reasoning Modelï¼ˆæ¨ç†æ¨¡å‹ï¼‰çš„å†…éƒ¨æœºåˆ¶ã€Routerï¼ˆè·¯ç”±ï¼‰ç­–ç•¥ä»¥åŠåœ¨â€œåæ•°æ®â€ä¸Šè®­ç»ƒ Diffusion çš„ç ”ç©¶ä¹Ÿå€¼å¾—é«˜åº¦å…³æ³¨ã€‚\n\n---\n\n### ğŸŒŸ å¿…è¯»ï¼šå¤§ä½¬æ–°ä½œä¸é‡ç£…ç ”ç©¶\n\n**36. Diffuse and Disperse: Image Generation with Representation Regularization (æ‰©æ•£ä¸åˆ†æ•£ï¼šå¸¦è¡¨å¾æ­£åˆ™åŒ–çš„å›¾åƒç”Ÿæˆ)**\n> **Authors:** Runqian Wang, Kaiming He\n> **å…³é”®è¯:** Diffusion Models, Representation Learning, Dispersive Loss\n> **TLDR:** ä½•æºæ˜å›¢é˜Ÿæ–°ä½œã€‚æŒ‡å‡ºç›®å‰çš„æ‰©æ•£æ¨¡å‹ä¸»è¦å…³æ³¨ç”Ÿæˆè´¨é‡ï¼Œå¿½ç•¥äº†å†…éƒ¨è¡¨å¾å­¦ä¹ ã€‚ä»–ä»¬æå‡ºäº†ä¸€ç§ **Dispersive Lossï¼ˆåˆ†æ•£æŸå¤±ï¼‰**ï¼Œç±»ä¼¼äºå¯¹æ¯”å­¦ä¹ ï¼Œä½†ä¸éœ€è¦æ­£æ ·æœ¬å¯¹ã€‚è¯¥æŸå¤±å‡½æ•°é¼“åŠ±å†…éƒ¨è¡¨å¾åœ¨éšè—ç©ºé—´ä¸­â€œåˆ†æ•£â€å¼€æ¥ã€‚è¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ­£åˆ™åŒ–é¡¹ï¼Œæ— éœ€é¢å¤–æ•°æ®å’Œå‚æ•°ï¼Œå°±èƒ½åœ¨ ImageNet ä¸Šæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™è¯•å›¾å¼¥åˆç”Ÿæˆæ¨¡å‹ä¸è¡¨å¾å­¦ä¹ ä¹‹é—´çš„é¸¿æ²Ÿã€‚\n\n**4. UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation (UADï¼šç”¨äºæœºå™¨äººæ“ä½œæ³›åŒ–çš„æ— ç›‘ç£å¯ä¾›æ€§è’¸é¦)**\n> **Authors:** Yihe Tang, Li Fei-Fei, et al.\n> **å…³é”®è¯:** Robotic Manipulation, Affordance, VLM, Distillation\n> **TLDR:** æé£é£å›¢é˜Ÿæå‡ºäº† UADï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­ç²¾ç»†ç‰©ä½“ **Affordanceï¼ˆå¯ä¾›æ€§ï¼‰** çš„ç†è§£é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨ï¼ŒUAD åˆ™åˆ©ç”¨è§†è§‰å¤§æ¨¡å‹å’Œ VLM çš„äº’è¡¥ä¼˜åŠ¿ï¼Œè‡ªåŠ¨æ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†ã€‚é€šè¿‡åœ¨å†»ç»“ç‰¹å¾ä¸Šè®­ç»ƒè½»é‡çº§è§£ç å™¨ï¼ŒUAD è™½ç„¶ä»…åœ¨æ¨¡æ‹Ÿç¯å¢ƒè®­ç»ƒï¼Œå´èƒ½åœ¨çœŸå®ä¸–ç•Œçš„é‡å¤–åœºæ™¯å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œç”šè‡³åªéœ€ 10 ä¸ªæ¼”ç¤ºå°±èƒ½å­¦ä¼šå¤„ç†æœªè§è¿‡çš„ç‰©ä½“ã€‚\n\n**58. Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions (Socratic-MCTSï¼šé€šè¿‡æå‡ºæ­£ç¡®é—®é¢˜è¿›è¡Œæµ‹è¯•æ—¶è§†è§‰æ¨ç†)**\n> **Authors:** David Acuna, Yejin Choi, et al.\n> **å…³é”®è¯:** VLM, MCTS, Reasoning, Socratic Method\n> **TLDR:** Yejin Choi å›¢é˜Ÿæ¢ç´¢å¦‚ä½•è®©éæ¨ç†å‹ VLM å…·å¤‡é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚å—åˆ°è‹æ ¼æ‹‰åº•å¼æé—®å’Œ MCTSï¼ˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰çš„å¯å‘ï¼Œä»–ä»¬æå‡ºåœ¨æ¨¡å‹è¾“å‡ºæµä¸­æ³¨å…¥â€œå­é—®é¢˜-å­ç­”æ¡ˆâ€å¯¹ã€‚è¿™ç›¸å½“äºæŠŠæ¨ç†çœ‹ä½œä¸€ä¸ªæœç´¢è¿‡ç¨‹ï¼Œé€šè¿‡æ½œåœ¨çš„å­é—®é¢˜å¼•å¯¼æ¨¡å‹â€œè¿ç‚¹æˆçº¿â€ã€‚åœ¨ MMMU-PRO ç­‰åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†**æœç´¢ï¼ˆSearchï¼‰**å¯ä»¥æŒ–æ˜æ¨¡å‹éšè—çš„çŸ¥è¯†ã€‚\n\n**19. Large Language Models and Emergence: A Complex Systems Perspective (å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ¶Œç°ï¼šå¤æ‚ç³»ç»Ÿçš„è§†è§’)**\n> **Authors:** David C. Krakauer, Melanie Mitchell, et al.\n> **å…³é”®è¯:** Emergence, Complexity Science, Intelligence\n> **TLDR:** è‘—åçš„å¤æ‚æ€§ç§‘å­¦å®¶ä»¬è”æ‰‹æ¢è®¨ LLM çš„â€œæ¶Œç°â€ç°è±¡ã€‚æ–‡ç« ä»â€œMore is Differentâ€ï¼ˆé‡å˜å¼•èµ·è´¨å˜ï¼‰å’Œâ€œLess is Moreâ€ï¼ˆæ™ºèƒ½æ˜¯é«˜æ•ˆçš„å‹ç¼©ï¼‰ä¸¤ä¸ªè§’åº¦å®¡è§† LLMã€‚è¿™æ˜¯ä¸€ç¯‡åç†è®ºå’Œå“²å­¦çš„æ–‡ç« ï¼Œè®¨è®ºäº†å¦‚ä½•é‡åŒ–æ¶Œç°ï¼Œä»¥åŠ LLM æ˜¯å¦çœŸçš„å…·å¤‡äº†æ¶Œç°çš„æ™ºèƒ½ã€‚\n\n**5. Ambient Diffusion Omni: Training Good Models with Bad Data (Ambient Diffusion Omniï¼šç”¨åæ•°æ®è®­ç»ƒå¥½æ¨¡å‹)**\n> **Authors:** Giannis Daras, Antonio Torralba, Constantinos Daskalakis, et al.\n> **å…³é”®è¯:** Diffusion Models, Corrupted Data, Training Framework\n> **TLDR:** é€šå¸¸æˆ‘ä»¬è®¤ä¸ºè®­ç»ƒæ‰©æ•£æ¨¡å‹éœ€è¦é«˜è´¨é‡æ•°æ®ï¼Œä½†è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ä½è´¨é‡ã€åˆæˆç”šè‡³åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çš„å›¾åƒï¼ˆå¦‚æ¨¡ç³Šã€å™ªå£°å›¾ï¼‰æ¥æå‡æ¨¡å‹ã€‚æ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼šå™ªå£°ä¼šæŠ‘åˆ¶é«˜è´¨é‡åˆ†å¸ƒä¸è§‚æµ‹åˆ°çš„æ··åˆåˆ†å¸ƒä¹‹é—´çš„åˆå§‹åå·®ã€‚ä»–ä»¬æå‡ºçš„æ¡†æ¶åˆ©ç”¨è‡ªç„¶å›¾åƒçš„è°±å¹‚å¾‹è¡°å‡å’Œå±€éƒ¨æ€§ï¼Œåœ¨ ImageNet ä¸Šè¾¾åˆ°äº† SOTA çš„ FIDã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ã€è·¯ç”±ä¸å†…éƒ¨æœºåˆ¶\n\n**35. Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning (Router-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ•™ LLM å¤šè½®è·¯ç”±ä¸èšåˆ)**\n> **Authors:** Haozhen Zhang, Jiaxuan You\n> **å…³é”®è¯:** LLM Router, Reinforcement Learning, Multi-LLM\n> **TLDR:** ç°æœ‰çš„ LLM è·¯ç”±é€šå¸¸æ˜¯å•è½®çš„ä¸€å¯¹ä¸€æ˜ å°„ã€‚Router-R1 å°†å…¶å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œè·¯ç”±å™¨æœ¬èº«å°±æ˜¯ä¸€ä¸ª LLMï¼Œå®ƒé€šè¿‡ RL å­¦ä¹ åœ¨æ€è€ƒï¼ˆå†…éƒ¨å®¡è®®ï¼‰å’Œè·¯ç”±ï¼ˆè°ƒç”¨æ¨¡å‹ï¼‰ä¹‹é—´äº¤æ›¿ã€‚è¿™ä¸ä»…å¹³è¡¡äº†æ€§èƒ½å’Œæˆæœ¬ï¼Œè¿˜èƒ½å¤„ç†å¤æ‚çš„éœ€è¦å¤šä¸ªæ¨¡å‹äº’è¡¥çš„ä»»åŠ¡ã€‚\n\n**178. On Reasoning Strength Planning in Large Reasoning Models (è®ºå¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„æ¨ç†å¼ºåº¦è§„åˆ’)**\n> **Authors:** Leheng Sheng, Tat-Seng Chua, et al.\n> **å…³é”®è¯:** Reasoning Strength, Activations, Mechanism Interpretability\n> **TLDR:** ä¸ºä»€ä¹ˆæ¨ç†æ¨¡å‹ï¼ˆå¦‚ o1ï¼‰é‡åˆ°éš¾é¢˜ä¼šæ€è€ƒæ›´ä¹…ï¼Ÿç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä¹‹å‰å°±åœ¨æ¿€æ´»å±‚ä¸­â€œé¢„å…ˆè§„åˆ’â€äº†æ¨ç†å¼ºåº¦ï¼ˆtoken æ•°é‡ï¼‰ã€‚ä½œè€…å‘ç°äº†ä¸€ä¸ª **Directional Vectorï¼ˆæ–¹å‘å‘é‡ï¼‰**ï¼Œå…¶æ¨¡é•¿æ§åˆ¶äº†æ¨ç†å¼ºåº¦ã€‚é€šè¿‡å¹²é¢„è¿™ä¸ªå‘é‡ï¼Œç”šè‡³å¯ä»¥æ§åˆ¶æ¨¡å‹çš„â€œè¿‡åº¦æ€è€ƒâ€è¡Œä¸ºã€‚\n\n**33. AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions (AbstentionBenchï¼šæ¨ç†å‹ LLM åœ¨æ— æ³•å›ç­”çš„é—®é¢˜ä¸Šè¡¨ç°ç³Ÿç³•)**\n> **Authors:** Polina Kirichenko, et al.\n> **å…³é”®è¯:** Abstention, Uncertainty, Reasoning LLMs\n> **TLDR:** çŸ¥é“â€œä½•æ—¶ä¸å›ç­”â€ä¸å›ç­”æ­£ç¡®åŒæ ·é‡è¦ã€‚ä½œè€…æå‡ºäº† AbstentionBenchï¼Œå‘ç°å³ä¾¿æ˜¯æœ€å‰æ²¿çš„æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰ï¼Œåœ¨ç»è¿‡æ¨ç†å¾®è°ƒåï¼Œæ‹’ç»å›ç­”ï¼ˆAbstentionï¼‰çš„èƒ½åŠ›åè€Œä¸‹é™äº† 24%ï¼è¿™æ„å‘³ç€æ¨ç†èƒ½åŠ›çš„æå‡å¯èƒ½ç‰ºç‰²äº†å¯¹ä¸ç¡®å®šæ€§çš„é²æ£’æ€§ã€‚\n\n**164. A Survey on Large Language Models for Mathematical Reasoning (å¤§å‹è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†ç»¼è¿°)**\n> **Authors:** Peng-Yuan Wang, Yang Yu, et al.\n> **å…³é”®è¯:** Mathematical Reasoning, Survey, CoT\n> **TLDR:** ä¸€ç¯‡éå¸¸è¯¦å°½çš„ç»¼è¿°ï¼Œæ¶µç›–äº† LLM æ•°å­¦æ¨ç†çš„å„ä¸ªé˜¶æ®µï¼šä»ç†è§£åˆ°ç”Ÿæˆï¼Œä» CoT åˆ°æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTest-time scalingï¼‰ã€‚å¯¹äºæƒ³äº†è§£è¯¥é¢†åŸŸå…¨è²Œçš„è¯»è€…éå¸¸æœ‰ä»·å€¼ã€‚\n\n**105. RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling (RuleReasonerï¼šé€šè¿‡åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·çš„å¼ºåŒ–åŸºäºè§„åˆ™æ¨ç†)**\n> **Authors:** Yang Liu, Zilong Zheng, et al.\n> **å…³é”®è¯:** Rule-based Reasoning, Small Reasoning Models, RL\n> **TLDR:** å°æ¨¡å‹ä¹Ÿèƒ½åšå¼ºæ¨ç†å—ï¼Ÿæœ¬æ–‡æå‡ºäº† RuleReasonerï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ–°é¢–çš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œè®©å°æ¨¡å‹ï¼ˆSmall Reasoning Modelsï¼‰åœ¨è§„åˆ™æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æŸäº›åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰ä»»åŠ¡ä¸Šè¶…è¿‡äº† OpenAI-o1ã€‚\n\n---\n\n### ğŸ¤– Agentï¼ˆæ™ºèƒ½ä½“ï¼‰ä¸å¤šæ¨¡æ€\n\n**158. AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models (AVA-Benchï¼šè§†è§‰åŸºç¡€æ¨¡å‹çš„åŸå­è§†è§‰èƒ½åŠ›åŸºå‡†)**\n> **Authors:** Zheda Mai, Wei-Lun Chao, et al.\n> **å…³é”®è¯:** Vision Foundation Models, Benchmark, Atomic Abilities\n> **TLDR:** ç°æœ‰çš„ VQA åŸºå‡†å¾€å¾€æ··åˆäº†å¤šç§èƒ½åŠ›ï¼Œéš¾ä»¥å½’å› é”™è¯¯ã€‚AVA-Bench å°†è§†è§‰èƒ½åŠ›è§£è€¦ä¸º 14 ç§ **Atomic Visual Abilities (AVAs)**ï¼ˆå¦‚å®šä½ã€æ·±åº¦ä¼°è®¡ç­‰ï¼‰ã€‚è¿™è®©æˆ‘ä»¬èƒ½çœ‹åˆ° VFM çš„â€œèƒ½åŠ›æŒ‡çº¹â€ï¼Œä¸ä»…èƒ½ç²¾å‡†è¯„ä¼°ï¼Œè¿˜å‘ç°å°è¯­è¨€æ¨¡å‹ï¼ˆ0.5Bï¼‰ä½œä¸º Head è¿›è¡Œè¯„ä¼°æ—¶æ•ˆæœä¸è¾“ 7B æ¨¡å‹ï¼Œä½†æ•ˆç‡é«˜ 8 å€ã€‚\n\n**31. Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation (ä»£ç†ç¥ç»ç½‘ç»œï¼šé€šè¿‡æ–‡æœ¬åå‘ä¼ æ’­è‡ªè¿›åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ)**\n> **Authors:** Xiaowen Ma, Yunpu Ma, et al.\n> **å…³é”®è¯:** Multi-Agent, Agentic Neural Network, Optimization\n> **TLDR:** è„‘æ´å¤§å¼€çš„æ¶æ„ã€‚å°†å¤šæ™ºèƒ½ä½“åä½œç±»æ¯”ä¸ºç¥ç»ç½‘ç»œï¼šæ¯ä¸ªæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸€å±‚æ˜¯ä¸€ä¸ªåä½œå›¢é˜Ÿã€‚é€šè¿‡å‰å‘ä¼ æ’­ï¼ˆä»»åŠ¡åˆ†è§£ï¼‰å’Œåå‘ä¼ æ’­ï¼ˆ**Textual Backpropagation**ï¼Œæ–‡æœ¬åé¦ˆè¿­ä»£ï¼‰ï¼Œè®©æ™ºèƒ½ä½“ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–å’Œä¼˜åŒ–ã€‚\n\n**101. PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly (PhyBlockï¼šé€šè¿‡ 3D ç§¯æœ¨ç»„è£…è¯„ä¼°ç‰©ç†ç†è§£å’Œè§„åˆ’çš„è¿›é˜¶åŸºå‡†)**\n> **Authors:** Liang Ma, Xiaodan Liang, et al.\n> **å…³é”®è¯:** Physical Understanding, VLM, Planning\n> **TLDR:** VLM è™½ç„¶èƒ½è¯´ä¼šé“ï¼Œä½†çœŸçš„æ‡‚ç‰©ç†å—ï¼ŸPhyBlocké€šè¿‡ 3D ç§¯æœ¨ç»„è£…ä»»åŠ¡æ¥æµ‹è¯•ã€‚ç»“æœå‘ç°ï¼Œç›®å‰çš„ SOTA VLM åœ¨é«˜å±‚è§„åˆ’å’Œç©ºé—´æ¨ç†ä¸Šä¾ç„¶å¾ˆå¼±ï¼Œä¸” CoT æç¤ºå¯¹ç©ºé—´ä»»åŠ¡çš„æå‡å¾®ä¹å…¶å¾®â€”â€”è¿™æš—ç¤ºç©ºé—´ä»»åŠ¡å¯èƒ½æ›´ä¾èµ–ç›´è§‰è€Œéè¯­è¨€æ¨ç†ã€‚\n\n---\n\n### ğŸš€ ä¼˜åŒ–ã€æ¶æ„ä¸ç§‘å­¦ AI\n\n**1. Grids Often Outperform Implicit Neural Representation at Compressing Dense Signals (ç½‘æ ¼é€šå¸¸åœ¨å‹ç¼©å¯†é›†ä¿¡å·æ–¹é¢ä¼˜äºéšå¼ç¥ç»è¡¨ç¤º)**\n> **Authors:** Namhoon Kim, Sara Fridovich-Keil\n> **å…³é”®è¯:** Implicit Neural Representations, Grids, Signal Compression\n> **TLDR:** INRï¼ˆéšå¼ç¥ç»è¡¨ç¤ºï¼Œå¦‚ NeRFï¼‰å¾ˆç«ï¼Œä½†è¿™ç¯‡æ–‡ç« æ³¼äº†å†·æ°´ã€‚é€šè¿‡å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œä½œè€…å‘ç°ç®€å•çš„**æ­£åˆ™åŒ–ç½‘æ ¼æ’å€¼ï¼ˆRegularized Grid with Interpolationï¼‰**åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè®­ç»ƒæ›´å¿«ã€è´¨é‡æ›´é«˜ã€‚INR ä»…åœ¨æ‹ŸåˆäºŒå€¼ä¿¡å·ï¼ˆå¦‚å½¢çŠ¶è½®å»“ï¼‰æ—¶æœ‰ä¼˜åŠ¿ã€‚\n\n**11. FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training (FastFLUXï¼šé€šè¿‡å—çº§æ›¿æ¢å’Œä¸‰æ˜æ²»è®­ç»ƒä¿®å‰ª FLUX)**\n> **Authors:** Fuhan Cai, Xiangzhong Fang, et al.\n> **å…³é”®è¯:** FLUX, Pruning, DiT, Efficiency\n> **TLDR:** FLUX æ˜¯ç›®å‰æœ€å¼ºçš„å¼€æºæ–‡ç”Ÿå›¾æ¨¡å‹ä¹‹ä¸€ï¼Œä½†å¤ªå¤§äº†ã€‚FastFLUX æå‡ºäº†ä¸€ç§æ¶æ„çº§å‰ªæï¼Œç”¨çº¿æ€§å±‚æ›¿æ¢å¤æ‚çš„ ResBlock åˆ†æ”¯ï¼Œå¹¶é…åˆ LoRA ç›‘ç£çš„â€œä¸‰æ˜æ²»è®­ç»ƒâ€ç­–ç•¥ã€‚ç»“æœæ˜¯ï¼šå‰ªæ‰ 20% å‚æ•°ï¼Œæ¨ç†æ˜¾è‘—åŠ é€Ÿï¼Œç”»è´¨å‡ ä¹ä¸é™ã€‚\n\n**121. HSG-12M: A Large-Scale Spatial Multigraph Dataset (HSG-12Mï¼šå¤§è§„æ¨¡ç©ºé—´å¤šé‡å›¾æ•°æ®é›†)**\n> **Authors:** Xianquan Yan, Ching Hua Lee, et al.\n> **å…³é”®è¯:** Dataset, Graph Learning, Physics\n> **TLDR:** ç‰©ç†ä¸ AI çš„ç»“åˆã€‚è¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ç©ºé—´å¤šé‡å›¾æ•°æ®é›†ï¼ŒåŒ…å« 1200 ä¸‡ä¸ªæºè‡ª 1D æ™¶ä½“èƒ½è°±çš„å›¾ã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼Œæ›´æ˜¯è¿æ¥ä»£æ•°ä¸å›¾å­¦ä¹ çš„æ¡¥æ¢ï¼Œä¸º AI for Science æä¾›äº†æ–°çš„æµ‹è¯•åºŠã€‚\n\n**104. CUDA-LLM: LLMs Can Write Efficient CUDA Kernels (CUDA-LLMï¼šLLM å¯ä»¥ç¼–å†™é«˜æ•ˆçš„ CUDA å†…æ ¸)**\n> **Authors:** Wentao Chen, An Zou, et al.\n> **å…³é”®è¯:** CUDA, Code Generation, LLM, Performance\n> **TLDR:** å†™ CUDA ä»£ç å¾ˆéš¾ï¼Œå†™é«˜æ•ˆçš„ CUDA æ›´éš¾ã€‚ä½œè€…æå‡ºäº† FSR æ¡†æ¶ï¼Œç»“åˆç‰¹å¾æœç´¢å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®© LLM ä¸ä»…èƒ½å†™å¯¹ï¼Œè¿˜èƒ½å†™å¿«ã€‚ç”Ÿæˆçš„å†…æ ¸åœ¨æ‰§è¡Œé€Ÿåº¦ä¸Šç”šè‡³æ¯”äººç±»ç¼–å†™çš„ä»£ç å¿« 179 å€ï¼",
  "papers": [
    {
      "arxiv_id": "2506.11139v2",
      "title": "Grids Often Outperform Implicit Neural Representation at Compressing Dense Signals",
      "title_zh": "ç½‘æ ¼åœ¨ç¨ å¯†ä¿¡å·å‹ç¼©ä¸­å¾€å¾€ä¼˜äºéšå¼ç¥ç»è¡¨ç¤º",
      "authors": [
        "Namhoon Kim",
        "Sara Fridovich-Keil"
      ],
      "abstract": "Implicit Neural Representations (INRs) have recently shown impressive results, but their fundamental capacity, implicit biases, and scaling behavior remain poorly understood. We investigate the performance of diverse INRs across a suite of 2D and 3D real and synthetic signals with varying effective bandwidth, as well as both overfitting and generalization tasks including tomography, super-resolution, and denoising. By stratifying performance according to model size as well as signal type and bandwidth, our results shed light on how different INR and grid representations allocate their capacity. We find that, for most tasks and signals, a simple regularized grid with interpolation trains faster and to higher quality than any INR with the same number of parameters. We also find limited settings--namely fitting binary signals such as shape contours--where INRs outperform grids, to guide future development and use of INRs towards the most advantageous applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations, INRs)åœ¨å‹ç¼©å¯†é›†ä¿¡å·æ—¶çš„å®¹é‡ã€åç½®åŠç¼©æ”¾è¡Œä¸ºã€‚ç ”ç©¶è€…åœ¨æ¶µç›–ä¸åŒå¸¦å®½ã€2DåŠ3Dä¿¡å·çš„æ–­å±‚æ‰«æ(tomography)ã€è¶…åˆ†è¾¨ç‡(super-resolution)å’Œå»å™ª(denoising)ç­‰ä»»åŠ¡ä¸­ï¼Œå¯¹å¤šç§æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½åˆ†å±‚åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºå¤§å¤šæ•°ä»»åŠ¡å’Œä¿¡å·ï¼Œåœ¨å‚æ•°é‡ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå¸¦æœ‰æ’å€¼çš„ç®€å•æ­£åˆ™åŒ–ç½‘æ ¼(regularized grid with interpolation)æ¯”ä»»ä½•INRçš„è®­ç»ƒé€Ÿåº¦æ›´å¿«ä¸”è´¨é‡æ›´é«˜ã€‚ç ”ç©¶ä¹ŸæŒ‡å‡ºï¼ŒINRä»…åœ¨å¤„ç†å½¢çŠ¶è½®å»“ç­‰äºŒè¿›åˆ¶ä¿¡å·(binary signals)çš„æœ‰é™åœºæ™¯ä¸­ä¼˜äºç½‘æ ¼ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸åŒè¡¨ç¤ºæ–¹æ³•å¦‚ä½•åˆ†é…å…¶å®¹é‡ï¼Œå¹¶ä¸ºæœªæ¥INRåœ¨ä¼˜åŠ¿åº”ç”¨é¢†åŸŸçš„å‘å±•æä¾›äº†æ˜ç¡®æŒ‡å¯¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Our analysis are available at https://github.com/voilalab/INR-benchmark",
      "pdf_url": "https://arxiv.org/pdf/2506.11139v2",
      "published_date": "2025-06-10 23:52:09 UTC",
      "updated_date": "2025-10-23 22:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:09.430241+00:00"
    },
    {
      "arxiv_id": "2506.09301v1",
      "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding",
      "title_zh": "$(RSA)^2$ï¼šä¸€ç§é¢å‘ä¿®è¾æ€§è¯­è¨€ç†è§£çš„ä¿®è¾ç­–ç•¥æ„ŸçŸ¥å‹ç†æ€§è¨€è¯­è¡Œä¸ºæ¡†æ¶",
      "authors": [
        "Cesare Spinoso-Di Piano",
        "David Austin",
        "Pablo Piantanida",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in human communication, resulting in utterances where the literal and the intended meanings do not match. The Rational Speech Act (RSA) framework, which explicitly models speaker intentions, is the most widespread theory of probabilistic pragmatics, but existing implementations are either unable to account for figurative expressions or require modeling the implicit motivations for using figurative language (e.g., to express joy or annoyance) in a setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware RSA $(RSA)^2$ framework which models figurative language use by considering a speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables human-compatible interpretations of non-literal utterances without modeling a speaker's motivations for being non-literal. Combined with LLMs, it achieves state-of-the-art performance on the ironic split of PragMega+, a new irony interpretation dataset introduced in this study.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $(RSA)^2$ (Rhetorical-Strategy-Aware Rational Speech Act)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºæ¯”å–»æ€§è¯­è¨€ (Figurative language) ç†è§£çš„ä¿®è¾ç­–ç•¥æ„ŸçŸ¥ç†æ€§è¨€è¯­è¡Œä¸ºæ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿ RSA æ¡†æ¶åœ¨å¤„ç†åè®½ (irony)ã€å¤¸å¼  (hyperbole) ç­‰éå­—é¢è¡¨è¾¾æ—¶éœ€ä¾èµ–ç‰¹å®šåŠ¨æœºå»ºæ¨¡çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è¯´è¯è€…çš„ä¿®è¾ç­–ç•¥ (rhetorical strategy) å®ç°äº†ä¸äººç±»è®¤çŸ¥å…¼å®¹çš„è¯­è¨€è§£è¯»ã€‚ç ”ç©¶è€…åŒæ—¶å¼•å…¥äº†æ–°çš„åè®½ç†è§£æ•°æ®é›† PragMega+ï¼Œå¹¶è¯æ˜å°† $(RSA)^2$ ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç»“åˆèƒ½åœ¨è¯¥æ•°æ®é›†ä¸Šå–å¾— state-of-the-art çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ— éœ€å»ºæ¨¡è¯´è¯è€…çš„å…·ä½“ä¸»è§‚åŠ¨æœºå³å¯ç²¾å‡†æ•æ‰éå­—é¢æ„å›¾ï¼Œä¸ºæ¦‚ç‡è¯­ç”¨å­¦ (probabilistic pragmatics) é¢†åŸŸç†è§£å¤æ‚ä¿®è¾è¡¨è¾¾æä¾›äº†åˆ›æ–°çš„å»ºæ¨¡è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2506.09301v1",
      "published_date": "2025-06-10 23:35:57 UTC",
      "updated_date": "2025-06-10 23:35:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:22.208743+00:00"
    },
    {
      "arxiv_id": "2506.09286v1",
      "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming",
      "title_zh": "åŸºäºç­”æ¡ˆé›†ç¨‹åºè®¾è®¡çš„ç¥ç»æˆåƒå› æœå›¾æ¢å¤",
      "authors": [
        "Mohammadsajad Abavisani",
        "Kseniya Solovyeva",
        "David Danks",
        "Vince Calhoun",
        "Sergey Plis"
      ],
      "abstract": "Learning graphical causal structures from time series data presents significant challenges, especially when the measurement frequency does not match the causal timescale of the system. This often leads to a set of equally possible underlying causal graphs due to information loss from sub-sampling (i.e., not observing all possible states of the system throughout time). Our research addresses this challenge by incorporating the effects of sub-sampling in the derivation of causal graphs, resulting in more accurate and intuitive outcomes. We use a constraint optimization approach, specifically answer set programming (ASP), to find the optimal set of answers. ASP not only identifies the most probable underlying graph, but also provides an equivalence class of possible graphs for expert selection. In addition, using ASP allows us to leverage graph theory to further prune the set of possible solutions, yielding a smaller, more accurate answer set significantly faster than traditional approaches. We validate our approach on both simulated data and empirical structural brain connectivity, and demonstrate its superiority over established methods in these experiments. We further show how our method can be used as a meta-approach on top of established methods to obtain, on average, 12% improvement in F1 score. In addition, we achieved state of the art results in terms of precision and recall of reconstructing causal graph from sub-sampled time series data. Finally, our method shows robustness to varying degrees of sub-sampling on realistic simulations, whereas other methods perform worse for higher rates of sub-sampling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”±äºæµ‹é‡é¢‘ç‡ä¸å› æœæ—¶é—´å°ºåº¦ä¸åŒ¹é…è€Œå¯¼è‡´çš„å­é‡‡æ · (sub-sampling) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å›ç­”é›†ç¨‹åºè®¾è®¡ (Answer Set Programming, ASP) ä»æ—¶é—´åºåˆ—æ•°æ®ä¸­æ¢å¤å› æœå›¾ç»“æ„çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å› æœå›¾æ¨å¯¼ä¸­çº³å…¥å­é‡‡æ ·çš„å½±å“ï¼Œåˆ©ç”¨çº¦æŸä¼˜åŒ– (constraint optimization) å¯»æ‰¾æœ€ä¼˜è§£é›†ï¼Œä¸ä»…èƒ½è¯†åˆ«å‡ºæœ€å¯èƒ½çš„åº•å±‚å›¾ï¼Œè¿˜èƒ½ä¸ºä¸“å®¶é€‰æ‹©æä¾›ç­‰ä»·ç±»ã€‚é€šè¿‡ç»“åˆå›¾è®º (graph theory) è¿›ä¸€æ­¥ç²¾ç®€è§£ç©ºé—´ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å­é‡‡æ ·æ•°æ®æ—¶æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å‡†ç¡®ä¸”æ˜¾è‘—æå‡äº†è®¡ç®—é€Ÿåº¦ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’Œè„‘ç»“æ„è¿æ¥æ€§ (structural brain connectivity) æ•°æ®çš„å®éªŒéªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶ä½œä¸ºä¸€ç§å…ƒæ–¹æ³• (meta-approach) å°† F1 åˆ†æ•°å¹³å‡æé«˜äº† 12%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç²¾åº¦ (precision) å’Œå¬å›ç‡ (recall) æ–¹é¢å‡è¾¾åˆ°äº†ç›®å‰çš„å…ˆè¿›æ°´å¹³ (state-of-the-art)ï¼Œå¹¶åœ¨é¢å¯¹ä¸åŒç¨‹åº¦çš„å­é‡‡æ ·æ—¶å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ (robustness)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09286v1",
      "published_date": "2025-06-10 22:51:30 UTC",
      "updated_date": "2025-06-10 22:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:30.342897+00:00"
    },
    {
      "arxiv_id": "2506.09284v2",
      "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation",
      "title_zh": "UADï¼šæœºå™¨äººæ“çºµæ³›åŒ–ä¸­çš„æ— ç›‘ç£ç¤ºèƒ½æ€§è’¸é¦",
      "authors": [
        "Yihe Tang",
        "Wenlong Huang",
        "Yingke Wang",
        "Chengshu Li",
        "Roy Yuan",
        "Ruohan Zhang",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Understanding fine-grained object affordances is imperative for robots to manipulate objects in unstructured environments given open-ended task instructions. However, existing methods of visual affordance predictions often rely on manually annotated data or conditions only on a predefined set of tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for distilling affordance knowledge from foundation models into a task-conditioned affordance model without any manual annotations. By leveraging the complementary strengths of large vision models and vision-language models, UAD automatically annotates a large-scale dataset with detailed $<$instruction, visual affordance$>$ pairs. Training only a lightweight task-conditioned decoder atop frozen features, UAD exhibits notable generalization to in-the-wild robotic scenes and to various human activities, despite only being trained on rendered objects in simulation. Using affordance provided by UAD as the observation space, we show an imitation learning policy that demonstrates promising generalization to unseen object instances, object categories, and even variations in task instructions after training on as few as 10 demonstrations. Project website: https://unsup-affordance.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UAD (Unsupervised Affordance Distillation)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨å³å¯å°† foundation models ä¸­çš„ affordance çŸ¥è¯†è’¸é¦åˆ°ä»»åŠ¡æ¡ä»¶æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸‹ç†è§£ç‰©ä½“ affordances çš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆ large vision models å’Œ vision-language models çš„äº’è¡¥ä¼˜åŠ¿ï¼ŒUAD èƒ½å¤Ÿè‡ªåŠ¨ç”ŸæˆåŒ…å« <instruction, visual affordance> å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶åœ¨å†»ç»“ç‰¹å¾åŸºç¡€ä¸Šè®­ç»ƒè½»é‡çº§è§£ç å™¨ã€‚å°½ç®¡ä»…åœ¨æ¨¡æ‹Ÿç¯å¢ƒçš„æ¸²æŸ“ç‰©ä½“ä¸Šè®­ç»ƒï¼ŒUAD åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººåœºæ™¯å’Œäººç±»æ´»åŠ¨ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œå°† UAD æä¾›çš„ affordance ä½œä¸ºè§‚æµ‹ç©ºé—´ï¼Œimitation learning ç­–ç•¥åœ¨ä»…éœ€10æ¬¡æ¼”ç¤ºçš„æƒ…å†µä¸‹ï¼Œå³å¯å®ç°å¯¹æœªè§ç‰©ä½“å®ä¾‹ã€ç±»åˆ«åŠä»»åŠ¡æŒ‡ä»¤å˜ä½“çš„æœ‰æ•ˆæ³›åŒ–ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09284v2",
      "published_date": "2025-06-10 22:47:16 UTC",
      "updated_date": "2025-08-25 19:45:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:33.149790+00:00"
    },
    {
      "arxiv_id": "2506.10038v1",
      "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
      "title_zh": "Ambient Diffusion Omniï¼šåˆ©ç”¨åŠ£è´¨æ•°æ®è®­ç»ƒä¼˜è´¨æ¨¡å‹",
      "authors": [
        "Giannis Daras",
        "Adrian Rodriguez-Munoz",
        "Adam Klivans",
        "Antonio Torralba",
        "Constantinos Daskalakis"
      ],
      "abstract": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Ambient Diffusion Omni æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ä½è´¨é‡ã€åˆæˆåŠåˆ†å¸ƒå¤– (out-of-distribution) çš„å›¾åƒæ¥æå‡æ‰©æ•£æ¨¡å‹ (diffusion model) çš„æ€§èƒ½ã€‚ä¼ ç»Ÿæ¨¡å‹é€šå¸¸ä¾èµ–é«˜åº¦è¿‡æ»¤çš„ä¼˜è´¨æ•°æ®ï¼Œè€Œè¯¥æ¡†æ¶è¯æ˜äº†è¢«å¼ƒç”¨çš„ä½è´¨é‡å›¾åƒä¸­ä¾ç„¶è•´å«å·¨å¤§ä»·å€¼ï¼Œå¹¶èƒ½ä»ä¸­æå–æœ‰æ•ˆä¿¡å·ã€‚æ¡†æ¶åˆ©ç”¨äº†è‡ªç„¶å›¾åƒçš„å…‰è°±å¹‚å¾‹è¡°å‡ (spectral power law decay) å’Œå±€éƒ¨æ€§ (locality) ä¸¤å¤§ç‰¹æ€§ï¼ŒæˆåŠŸé€šè¿‡äº†å—é«˜æ–¯æ¨¡ç³Š (Gaussian blur) å’Œ JPEG å‹ç¼©ç­‰æŸåå›¾åƒçš„è®­ç»ƒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ImageNet ä¸Šå–å¾—äº†é¢†å…ˆçš„ FID åˆ†æ•°ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬ç”Ÿæˆå›¾åƒ (text-to-image) çš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚å…¶æ ¸å¿ƒæ´å¯Ÿåœ¨äºå™ªå£°èƒ½æœ‰æ•ˆç¼“è§£é«˜è´¨é‡åˆ†å¸ƒä¸å®é™…è§‚æµ‹åˆ†å¸ƒä¹‹é—´çš„åå·®ï¼Œä¸ºåˆ©ç”¨åç½®æ•°æ®æå‡æ¨¡å‹è¡¨ç°æä¾›äº†ä¸¥è°¨çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "Preprint, work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.10038v1",
      "published_date": "2025-06-10 22:37:39 UTC",
      "updated_date": "2025-06-10 22:37:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:29.714531+00:00"
    },
    {
      "arxiv_id": "2506.09276v2",
      "title": "Learning The Minimum Action Distance",
      "title_zh": "å­¦ä¹ æœ€å°åŠ¨ä½œè·ç¦»",
      "authors": [
        "Lorenzo Steccanella",
        "Joshua B. Evans",
        "Ã–zgÃ¼r ÅimÅŸek",
        "Anders Jonsson"
      ],
      "abstract": "This paper presents a state representation framework for Markov decision processes (MDPs) that can be learned solely from state trajectories, requiring neither reward signals nor the actions executed by the agent. We propose learning the minimum action distance (MAD), defined as the minimum number of actions required to transition between states, as a fundamental metric that captures the underlying structure of an environment. MAD naturally enables critical downstream tasks such as goal-conditioned reinforcement learning and reward shaping by providing a dense, geometrically meaningful measure of progress. Our self-supervised learning approach constructs an embedding space where the distances between embedded state pairs correspond to their MAD, accommodating both symmetric and asymmetric approximations. We evaluate the framework on a comprehensive suite of environments with known MAD values, encompassing both deterministic and stochastic dynamics, as well as discrete and continuous state spaces, and environments with noisy observations. Empirical results demonstrate that the proposed approach not only efficiently learns accurate MAD representations across these diverse settings but also significantly outperforms existing state representation methods in terms of representation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ Markov decision processes (MDPs) çš„çŠ¶æ€è¡¨ç¤ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»…é€šè¿‡çŠ¶æ€è½¨è¿¹è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€å¥–åŠ±ä¿¡å·æˆ–æ™ºèƒ½ä½“æ‰§è¡Œçš„åŠ¨ä½œã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºå­¦ä¹ æœ€å°åŠ¨ä½œè·ç¦» (Minimum Action Distance, MAD)ï¼Œå³çŠ¶æ€é—´è½¬ç§»æ‰€éœ€çš„æœ€å°åŠ¨ä½œæ•°ï¼Œå°†å…¶ä½œä¸ºæ•æ‰ç¯å¢ƒåº•å±‚ç»“æ„çš„åŸºç¡€åº¦é‡ã€‚MAD é€šè¿‡æä¾›å¯†é›†ä¸”å…·æœ‰å‡ ä½•æ„ä¹‰çš„è¿›åº¦è¡¡é‡ï¼Œè‡ªç„¶åœ°æ”¯æŒäº† Goal-conditioned reinforcement learning å’Œ Reward shaping ç­‰å…³é”®ä¸‹æ¸¸ä»»åŠ¡ã€‚è¯¥è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåµŒå…¥ç©ºé—´ï¼Œä½¿çŠ¶æ€å¯¹åœ¨ç©ºé—´ä¸­çš„è·ç¦»å¯¹åº”äºå…¶ MADï¼Œå¹¶èƒ½åŒæ—¶å¤„ç†å¯¹ç§°ä¸éå¯¹ç§°çš„è¿‘ä¼¼ã€‚å®éªŒåœ¨æ¶µç›–ç¡®å®šæ€§ä¸éšæœºåŠ¨åŠ›å­¦ã€ç¦»æ•£ä¸è¿ç»­çŠ¶æ€ç©ºé—´ä»¥åŠå«å™ªå£°è§‚æµ‹çš„å¤šç§ç¯å¢ƒä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®è¯ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½é«˜æ•ˆå­¦ä¹ å‡†ç¡®çš„ MAD è¡¨ç¤ºï¼Œä¸”åœ¨è¡¨ç¤ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„çŠ¶æ€è¡¨ç¤ºæ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09276v2",
      "published_date": "2025-06-10 22:27:11 UTC",
      "updated_date": "2025-10-06 18:53:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:29.581569+00:00"
    },
    {
      "arxiv_id": "2506.09268v1",
      "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks",
      "title_zh": "é¢å‘ç»¿è‰²æ˜Ÿåœ°èåˆç½‘ç»œåœ¨çº¿ä¼˜åŒ–çš„å¤šè‡‚è€è™æœºæ¡†æ¶",
      "authors": [
        "Henri Alam",
        "Antonio de Domenico",
        "Tareq Si Salem",
        "Florian Kaltenberger"
      ],
      "abstract": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures offer a promising solution for expanding coverage and improving capacity for the network. While non-terrestrial networks (NTNs) are primarily exploited for these specific reasons, their role in alleviating terrestrial network (TN) load and enabling energy-efficient operation has received comparatively less attention. In light of growing concerns associated with the densification of terrestrial deployments, this work aims to explore the potential of NTNs in supporting a more sustainable network. In this paper, we propose a novel online optimisation framework for integrated TN-NTN architectures, built on a multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively optimises key system parameters--including bandwidth allocation, user equipment (UE) association, and macro base station (MBS) shutdown--to balance network capacity and energy efficiency in real time. Extensive system-level simulations over a 24-hour period show that our framework significantly reduces the proportion of unsatisfied UEs during peak hours and achieves up to 19% throughput gains and 5% energy savings in low-traffic periods, outperforming standard network settings following 3GPP recommendations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›†æˆé™†åœ°ä¸éé™†åœ°ç½‘ç»œï¼ˆTN-NTNï¼‰åœ¨èŠ‚èƒ½è¿è¡Œæ–¹é¢ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šè‡‚è€è™æœºï¼ˆMulti-Armed Bandit, MABï¼‰çš„æ–°å‹åœ¨çº¿ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Bandit-feedback Constrained Online Mirror Descent (BCOMD) ç®—æ³•ï¼Œæ—¨åœ¨å®æ—¶å¹³è¡¡ç½‘ç»œå®¹é‡ä¸èƒ½é‡æ•ˆç‡ã€‚é€šè¿‡è‡ªé€‚åº”ä¼˜åŒ–å¸¦å®½åˆ†é…ã€ç”¨æˆ·è®¾å¤‡ï¼ˆUEï¼‰å…³è”ä»¥åŠå®åŸºç«™ï¼ˆMBSï¼‰å…³æ–­ç­‰å…³é”®ç³»ç»Ÿå‚æ•°ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿçµæ´»åº”å¯¹åŠ¨æ€çš„æµé‡éœ€æ±‚ã€‚ç³»ç»Ÿçº§ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é«˜å³°æ—¶æ®µæ˜¾è‘—é™ä½äº†ä¸æ»¡æ„ç”¨æˆ·çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨ä½æµé‡æ—¶æ®µå®ç°äº† 19% çš„ååé‡å¢é•¿å’Œ 5% çš„èƒ½æºèŠ‚çœã€‚ç›¸æ¯” 3GPP æ ‡å‡†é…ç½®ï¼Œè¯¥æ–¹æ³•åœ¨æå‡ç½‘ç»œæ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆä¼˜åŒ–äº†åŠŸè€—ï¼Œä¸ºæ„å»ºå¯æŒç»­çš„ç»¿è‰²é›†æˆé€šä¿¡ç½‘ç»œæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "To be published in 2025 IEEE International Workshop on Signal Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.09268v1",
      "published_date": "2025-06-10 22:06:15 UTC",
      "updated_date": "2025-06-10 22:06:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:50.720591+00:00"
    },
    {
      "arxiv_id": "2506.09259v1",
      "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat",
      "title_zh": "é¢å‘äº²ç¤¾ä¼šèŠå¤©æ–‡æœ¬é«˜æ ·æœ¬æ•ˆç‡åˆ†ç±»çš„è‡ªé”šå®šæ³¨æ„åŠ›æ¨¡å‹",
      "authors": [
        "Zhuofang Li",
        "Rafal Kocielnik",
        "Fereshteh Soltani",
        "Penphob",
        "Boonyarungsrit",
        "Animashree Anandkumar",
        "R. Michael Alvarez"
      ],
      "abstract": "Millions of players engage daily in competitive online games, communicating through in-game chat. Prior research has focused on detecting relatively small volumes of toxic content using various Natural Language Processing (NLP) techniques for the purpose of moderation. However, recent studies emphasize the importance of detecting prosocial communication, which can be as crucial as identifying toxic interactions. Recognizing prosocial behavior allows for its analysis, rewarding, and promotion. Unlike toxicity, there are limited datasets, models, and resources for identifying prosocial behaviors in game-chat text. In this work, we employed unsupervised discovery combined with game domain expert collaboration to identify and categorize prosocial player behaviors from game chat. We further propose a novel Self-Anchored Attention Model (SAAM) which gives 7.9% improvement compared to the best existing technique. The approach utilizes the entire training set as \"anchors\" to help improve model performance under the scarcity of training data. This approach led to the development of the first automated system for classifying prosocial behaviors in in-game chats, particularly given the low-resource settings where large-scale labeled data is not available. Our methodology was applied to one of the most popular online gaming titles - Call of Duty(R): Modern Warfare(R)II, showcasing its effectiveness. This research is novel in applying NLP techniques to discover and classify prosocial behaviors in player in-game chat communication. It can help shift the focus of moderation from solely penalizing toxicity to actively encouraging positive interactions on online platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶åœ¨çº¿æ¸¸æˆé¢†åŸŸçš„æ¯’æ€§å†…å®¹ç›‘æµ‹å·²å¾—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨è¯†åˆ«å’Œå¥–åŠ±äº²ç¤¾ä¼šè¡Œä¸º(prosocial communication)æ–¹é¢ä»ç¼ºä¹æœ‰æ•ˆçš„æ•°æ®é›†å’Œæ¨¡å‹èµ„æºã€‚ç ”ç©¶å›¢é˜Ÿç»“åˆæ— ç›‘ç£å‘ç°(unsupervised discovery)ä¸æ¸¸æˆé¢†åŸŸä¸“å®¶åä½œï¼Œä»æ¸¸æˆèŠå¤©ä¸­è¯†åˆ«å¹¶åˆ†ç±»äº†äº²ç¤¾ä¼šè¡Œä¸ºï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„Self-Anchored Attention Model (SAAM)ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†æ•´ä¸ªè®­ç»ƒé›†ä½œä¸ºé”šç‚¹(anchors)æ¥æå‡åœ¨æ ‡æ³¨æ•°æ®åŒ®ä¹çš„ä½èµ„æº(low-resource)ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œå®ç°äº†æ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡(sample-efficient)ã€‚å®éªŒåœ¨çƒ­é—¨æ¸¸æˆã€Šä½¿å‘½å¬å”¤ï¼šç°ä»£æˆ˜äº‰IIã€‹(Call of Duty: Modern Warfare II)çš„æ•°æ®ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºSAAMæ¯”ç°æœ‰æœ€ä½³æŠ€æœ¯æå‡äº†7.9%çš„æ€§èƒ½ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹æ¸¸æˆèŠå¤©äº²ç¤¾ä¼šè¡Œä¸ºçš„è‡ªåŠ¨åŒ–åˆ†ç±»ç³»ç»Ÿï¼Œè¯¥ç ”ç©¶ä¸ºæ¨åŠ¨åœ¨çº¿å¹³å°ä»å•çº¯æ‰“å‡»æ¯’æ€§äº’åŠ¨å‘ç§¯æé¼“åŠ±æ­£é¢äº¤æµçš„è½¬å‹æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09259v1",
      "published_date": "2025-06-10 21:40:54 UTC",
      "updated_date": "2025-06-10 21:40:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:52.085638+00:00"
    },
    {
      "arxiv_id": "2506.09251v2",
      "title": "Extrapolation by Association: Length Generalization Transfer in Transformers",
      "title_zh": "å…³è”å¼å¤–æ¨ï¼šTransformer ä¸­çš„é•¿åº¦æ³›åŒ–è¿ç§»",
      "authors": [
        "Ziyang Cai",
        "Nayoung Lee",
        "Avi Schwarzschild",
        "Samet Oymak",
        "Dimitris Papailiopoulos"
      ],
      "abstract": "Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \\textit{task association}. We find that length generalization can be \\textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶é€šè¿‡ä»»åŠ¡å…³è”(task association)çš„è§†è§’æ¢è®¨äº†Transformerè¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä»çŸ­è¾“å…¥åˆ°é•¿è¾“å…¥å¤–æ¨æ—¶çš„é•¿åº¦æ³›åŒ–(length generalization)èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°é•¿åº¦æ³›åŒ–èƒ½åŠ›å¯ä»¥åœ¨ç›¸å…³ä»»åŠ¡ä¹‹é—´è¿›è¡Œè¿ç§»(transferred)ï¼Œå³åœ¨æ›´é•¿ä¸”ç›¸å…³çš„è¾…åŠ©ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥å¼•å¯¼å…¶åœ¨ç›®æ ‡ä»»åŠ¡çš„æœªè§é•¿è¾“å…¥ä¸Šå®ç°æ³›åŒ–ã€‚è¿™ç§é•¿åº¦æ³›åŒ–è¿ç§»åœ¨ç®—æœ¯è¿ç®—ã€å­—ç¬¦ä¸²è½¬æ¢å’Œè¿·å®«å¯¼èˆªç­‰å¤šç§ç®—æ³•ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œè¡¨æ˜Transformeråœ¨è”åˆè®­ç»ƒæ—¶å¯ä»¥ä»ç›¸ä¼¼ä»»åŠ¡ä¸­ç»§æ‰¿æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„è¿ç§»æ•ˆåº”ï¼Œæš—ç¤ºé¢„è®­ç»ƒä¸ºæ¨¡å‹æä¾›äº†å¯é‡ç”¨çš„è®¡ç®—æ”¯æ¶(computational scaffolding)ä»¥è¾…åŠ©ä¸‹æ¸¸ä»»åŠ¡çš„å¤–æ¨ã€‚åˆæ­¥çš„æœºåˆ¶æ€§è¯æ®è¡¨æ˜ï¼Œé•¿åº¦æ³›åŒ–è¿ç§»ä¸ä»»åŠ¡é—´ç›¸åŒæ³¨æ„åŠ›å¤´(attention heads)çš„å¤ç”¨å‘ˆæ­£ç›¸å…³ã€‚è¯¥å‘ç°åŠ æ·±äº†å¯¹Transformerå¦‚ä½•å¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)è¾“å…¥æ³›åŒ–çš„ç†è§£ï¼Œå¹¶å¼ºè°ƒäº†è·¨ä»»åŠ¡å½’çº³ç»“æ„çš„ç»„åˆå¤ç”¨ç‰¹æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 20 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09251v2",
      "published_date": "2025-06-10 21:22:51 UTC",
      "updated_date": "2025-08-04 16:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:47.774891+00:00"
    },
    {
      "arxiv_id": "2506.09250v2",
      "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
      "title_zh": "è¯„ã€Šæ€ç»´çš„å¹»è§‰ï¼šä»é—®é¢˜å¤æ‚åº¦è§†è§’ç†è§£æ¨ç†æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™ã€‹",
      "authors": [
        "A. Lawsen"
      ],
      "abstract": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit \"accuracy collapse\" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments risk exceeding model output token limits, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Shojaee ç­‰äºº (2025) å…³äºå¤§å‹æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨å¤æ‚è§„åˆ’è°œé¢˜ä¸­å‡ºç°â€œå‡†ç¡®ç‡å´©å¡Œâ€ (accuracy collapse) çš„ç»“è®ºæå‡ºäº†è´¨ç–‘ã€‚ä½œè€…æŒ‡å‡ºï¼ŒåŸç ”ç©¶çš„å‘ç°ä¸»è¦æºäºå®éªŒè®¾è®¡çš„å±€é™æ€§ï¼Œè€Œéæ¨¡å‹åŸºç¡€æ¨ç†èƒ½åŠ›çš„å¤±è´¥ã€‚é€šè¿‡è¯¦ç»†åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åŸå®éªŒä¸­çš„ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šTower of Hanoi ä»»åŠ¡å¾€å¾€è¶…å‡ºäº†æ¨¡å‹çš„è¾“å‡º token é™åˆ¶ï¼Œè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶æœªèƒ½åŒºåˆ†å®é™…çº¦æŸä¸æ¨ç†å¤±è´¥ï¼Œä¸” River Crossing åŸºå‡†æµ‹è¯•ä¸­åŒ…å«äº†ä¸€äº›æ•°å­¦ä¸Šä¸å¯è§£çš„é”™è¯¯å®ä¾‹ã€‚å½“ç ”ç©¶è€…é€šè¿‡è¯·æ±‚ç”Ÿæˆå‡½æ•°è€Œéç©·ä¸¾ç§»åŠ¨åˆ—è¡¨æ¥æ§åˆ¶è¿™äº›å®éªŒå¹²æ‰°é¡¹æ—¶ï¼Œåˆæ­¥å®éªŒæ˜¾ç¤ºå¤šä¸ªæ¨¡å‹åœ¨ä¹‹å‰è¢«æŠ¥å‘Šä¸ºå®Œå…¨å¤±è´¥çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æé«˜çš„å‡†ç¡®ç‡ã€‚è¯¥è®ºæ–‡æœ€ç»ˆå¼ºè°ƒäº†åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½æ¨ç†èƒ½åŠ›æ—¶ï¼Œä¸¥è°¨çš„å®éªŒè®¾è®¡å¯¹äºè·å–å‡†ç¡®è¯„ä¼°ç»“æœçš„è‡³å…³é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Comment on: arXiv:2506.06941 Latest version removes Claude as a co-author, in line with arXiv policies, it also corrects mistakes in sections 4 and 6 of the original submission, as well as several typographical errors",
      "pdf_url": "https://arxiv.org/pdf/2506.09250v2",
      "published_date": "2025-06-10 21:16:53 UTC",
      "updated_date": "2025-06-16 18:04:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:30:53.612640+00:00"
    },
    {
      "arxiv_id": "2506.10035v3",
      "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training",
      "title_zh": "FastFLUXï¼šåŸºäºé€å—æ›¿æ¢ä¸ä¸‰æ˜æ²»è®­ç»ƒçš„ FLUX å‰ªæ",
      "authors": [
        "Fuhan Cai",
        "Yong Guo",
        "Jie Li",
        "Wenbo Li",
        "Jian Chen",
        "Xiangzhong Fang"
      ],
      "abstract": "Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\\% of the hierarchy pruned. Our code will be available soon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FastFLUXï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡FLUXæ¨¡å‹æ¨ç†æ•ˆç‡çš„æ¶æ„çº§å‰ªææ¡†æ¶ï¼Œé’ˆå¯¹Diffusion Transformers (DiTs)æ¨¡å‹å› å‚æ•°è§„æ¨¡å·¨å¤§å¯¼è‡´çš„æ¨ç†é€Ÿåº¦æ…¢ã€å†…å­˜å ç”¨é«˜å’Œéƒ¨ç½²å›°éš¾ç­‰é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ˜¯å¸¦æœ‰çº¿æ€§å±‚çš„å—æ›¿æ¢(Block-wise Replacement with Linear Layers, BRLL)ï¼Œè¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„çº¿æ€§å±‚æ›¿æ¢ResBlocksä¸­ç»“æ„å¤æ‚çš„æ®‹ä½™åˆ†æ”¯ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹çš„å¿«æ·è¿æ¥(shortcut connections)ä»¥ç¡®ä¿æ¨¡å‹ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸‰æ˜æ²»è®­ç»ƒ(Sandwich Training, ST)ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨LoRAç›‘ç£ç›¸é‚»å—çš„å±€éƒ¨å¾®è°ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£ç”±ç»“æ„æ›¿æ¢å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastFLUXåœ¨å‰ªé™¤20%å±‚çº§ç»“æ„çš„æƒ…å†µä¸‹ï¼Œä¸ä»…æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œè¿˜èƒ½åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¿æŒæé«˜çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶ä¸ºå¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†ä¸€ç§åœ¨æ¨ç†æ€§èƒ½å’Œç”Ÿæˆæ•ˆæœä¹‹é—´å–å¾—å¹³è¡¡çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10035v3",
      "published_date": "2025-06-10 20:48:30 UTC",
      "updated_date": "2026-01-13 18:20:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:09.150775+00:00"
    },
    {
      "arxiv_id": "2506.17267v1",
      "title": "CF-VLM:CounterFactual Vision-Language Fine-tuning",
      "title_zh": "CF-VLMï¼šåäº‹å®è§†è§‰-è¯­è¨€å¾®è°ƒ",
      "authors": [
        "Jusheng Zhang",
        "Kaitong Cai",
        "Yijia Fan",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "Recent advances in vision-language models (VLMs) have greatly improved cross-modal semantic understanding, yet significant limitations remain in fine-grained discrimination and deep causal reasoning tasks. Existing VLMs often rely on superficial statistical correlations, lacking the ability to capture the underlying causal logic between visual and textual content. To address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a novel framework that enhances the causal reasoning capabilities of VLMs through the targeted use of counterfactual samples. CF-VLM introduces three complementary training objectives: maintaining foundational cross-modal alignment, reinforcing the uniqueness and stability of factual scene representations against coherent counterfactuals, and sharpening the model's sensitivity to minimal but critical causal edits. Extensive experiments demonstrate that CF-VLM consistently outperforms strong baselines and state-of-the-art methods on compositional reasoning and generalization benchmarks. Furthermore, it shows promise in mitigating visual hallucinations, indicating improved factual consistency. Our CF-VLM provides a robust foundation for deploying VLMs in high-stakes, real-world scenarios requiring reliable reasoning and interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CF-VLM (CounterFactual Vision-Language Fine-tuning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç»†ç²’åº¦åˆ¤åˆ«å’Œæ·±åº¦å› æœæ¨ç†ä»»åŠ¡ä¸­è¿‡åº¦ä¾èµ–æµ…å±‚ç»Ÿè®¡ç›¸å…³æ€§çš„é—®é¢˜ã€‚CF-VLM é€šè¿‡é’ˆå¯¹æ€§åœ°åˆ©ç”¨åäº‹å®æ ·æœ¬ï¼Œå¼•å…¥äº†ç»´æŒè·¨æ¨¡æ€å¯¹é½ (cross-modal alignment)ã€å¢å¼ºäº‹å®åœºæ™¯è¡¨ç¤ºçš„ç¨³å®šæ€§ä»¥åŠæå‡å¯¹å…³é”®å¾®å°å› æœç¼–è¾‘æ•æ„Ÿåº¦è¿™ä¸‰ä¸ªäº’è¡¥çš„è®­ç»ƒç›®æ ‡ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»„åˆæ¨ç† (compositional reasoning) å’Œæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒCF-VLM åœ¨ç¼“è§£è§†è§‰å¹»è§‰ (visual hallucinations) æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨éœ€è¦å¯é æ¨ç†å’Œå¯è§£é‡Šæ€§çš„é«˜é£é™©ç°å®åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½² VLMs å¥ å®šäº†é²æ£’çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17267v1",
      "published_date": "2025-06-10 20:20:05 UTC",
      "updated_date": "2025-06-10 20:20:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:10.545396+00:00"
    },
    {
      "arxiv_id": "2506.09215v1",
      "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs",
      "title_zh": "åŸºäº Transformer è¾“å‡ºè‡ªé€‚åº”æ± åŒ–çš„é²æ£’å™ªå£°è¡°å‡",
      "authors": [
        "Greyson Brothers"
      ],
      "abstract": "We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based adaptive pooling method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†TransformeråµŒå…¥æ¨¡å‹è¾“å‡ºæ±‡æ€»çš„Poolingæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ å’Œè§†è§‰åº”ç”¨ä¸­è¾“å…¥å‘é‡åŒ…å«å¤§é‡å¹²æ‰°å™ªå£°(noise)çš„é—®é¢˜ã€‚é€šè¿‡å°†Poolingå»ºæ¨¡ä¸ºä»¥æœ€å°åŒ–ä¿¡å·æŸå¤±ä¸ºç›®æ ‡çš„å‘é‡é‡åŒ–(vector quantization)ï¼Œä½œè€…æ­ç¤ºäº†ä¼ ç»Ÿæ–¹æ³•å¦‚AvgPoolã€MaxPoolå’ŒClsTokenåœ¨ä¿¡å™ªæ¯”(SNR)æ³¢åŠ¨æ—¶ææ˜“å‡ºç°æ€§èƒ½å´©æºƒã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„è‡ªé€‚åº”æ± åŒ–(adaptive pooling)æ–¹æ³•ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å„ç§ä¿¡å™ªæ¯”ä¸‹å‡èƒ½æœ‰æ•ˆé€¼è¿‘ä¿¡å·æœ€ä¼˜å‘é‡é‡åŒ–å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†ã€å…³ç³»æ¨ç†ã€å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (multi-agent reinforcement learning)åŠè§†è§‰åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹è§‚æµ‹ä¸­çš„å™ªå£°å¹²æ‰°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second International Conference on Machine Learning (ICML) Proceedings",
      "pdf_url": "https://arxiv.org/pdf/2506.09215v1",
      "published_date": "2025-06-10 20:18:32 UTC",
      "updated_date": "2025-06-10 20:18:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:15.970559+00:00"
    },
    {
      "arxiv_id": "2506.14821v3",
      "title": "Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints",
      "title_zh": "èµ„æºå—é™ä¸‹å¼ºåŒ–VLMä½¿ç”¨å·¥å…·è¿›è¡Œç»†ç²’åº¦è§†è§‰æ¨ç†",
      "authors": [
        "Sunil Kumar",
        "Bowen Zhao",
        "Leo Dirac",
        "Paulina Varshavskaya"
      ],
      "abstract": "Despite tremendous recent advances in large model reasoning ability, vision-language models (VLMs) still struggle with detailed visual reasoning, especially when compute resources are limited. To address this challenge, we draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale models with Group Relative Policy Optimization (GRPO) to use external tools such as zoom. The greatest benefit is obtained with a combination of GRPO learning, a simple reward structure, a simplified tool-calling interface, allocating additional tokens to the result of the tool call, and a training data mix that over-represents visually difficult examples. Compared to similarly-sized baseline models, our method achieves better performance on some visual question-answering (VQA) tasks, thanks to the detailed visual information gathered from the external tool.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è®¡ç®—èµ„æºå—é™æ—¶éš¾ä»¥è¿›è¡Œè¯¦ç»†è§†è§‰æ¨ç†(detailed visual reasoning)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è®­ç»ƒå°å‹æ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„æ–°æ–¹æ³•ã€‚å—Deepseek-r1ç­‰ç ”ç©¶å¯å‘ï¼Œä½œè€…åˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ç®—æ³•ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ è°ƒç”¨æ”¾å¤§(zoom)ç­‰å¤–éƒ¨å·¥å…·æ¥è·å–æ›´ä¸°å¯Œçš„è§†è§‰ç»†èŠ‚ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ç»“åˆç®€åŒ–çš„å·¥å…·è°ƒç”¨æ¥å£ã€ç‰¹å®šçš„å¥–åŠ±ç»“æ„ï¼Œå¹¶ä¸ºå·¥å…·è¿”å›çš„ç»“æœåˆ†é…é¢å¤–çš„Tokenç©ºé—´ï¼ŒåŒæ—¶åœ¨è®­ç»ƒæ•°æ®ä¸­å¢åŠ äº†é«˜éš¾åº¦è§†è§‰æ ·æœ¬çš„æ¯”ä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹åœ¨å¤šé¡¹è§†è§‰é—®ç­”(VQA)ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŒç­‰è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨å·¥å…·ä½¿ç”¨çš„æœºåˆ¶ï¼Œä¸ºæå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14821v3",
      "published_date": "2025-06-10 20:11:44 UTC",
      "updated_date": "2025-08-05 03:49:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:17.439744+00:00"
    },
    {
      "arxiv_id": "2506.09206v1",
      "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research",
      "title_zh": "SimClassï¼šåŸºäºæ¸¸æˆå¼•æ“æ¨¡æ‹Ÿç”Ÿæˆçš„é¢å‘è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç ”ç©¶çš„è¯¾å ‚è¯­éŸ³æ•°æ®é›†",
      "authors": [
        "Ahmed Adel Attia",
        "Jing Liu",
        "Carl Espy-Wilson"
      ],
      "abstract": "The scarcity of large-scale classroom speech data has hindered the development of AI-driven speech models for education. Public classroom datasets remain limited, and the lack of a dedicated classroom noise corpus prevents the use of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom noise using game engines, a framework that extends to other domains. Using this methodology, we present SimClass, a dataset that includes both a synthesized classroom noise corpus and a simulated classroom speech dataset. The speech data is generated by pairing a public children's speech corpus with YouTube lecture videos to approximate real classroom interactions in clean conditions. Our experiments on clean and noisy speech demonstrate that SimClass closely approximates real classroom speech, making it a valuable resource for developing robust speech recognition and enhancement models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•™è‚²é¢†åŸŸå¤§è§„æ¨¡æ•™å®¤è¯­éŸ³æ•°æ®ç¨€ç¼ºä»¥åŠç¼ºä¹ä¸“ç”¨æ•™å®¤å™ªå£°è¯­æ–™åº“çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ¸¸æˆå¼•æ“(game engine)åˆæˆæ•™å®¤å™ªå£°çš„å¯æ‰©å±•æ–¹æ³•ã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†åä¸ºSimClassçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬åˆæˆçš„æ•™å®¤å™ªå£°åº“ä»¥åŠé€šè¿‡ç»“åˆå„¿ç«¥è¯­éŸ³è¯­æ–™åº“ä¸YouTubeè®²åº§è§†é¢‘ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•™å®¤è¯­éŸ³æ•°æ®ã€‚SimClassæ—¨åœ¨æ¨¡æ‹ŸçœŸå®æ¡ä»¶ä¸‹çš„æ•™å®¤äº¤äº’ï¼Œä¸ºå¼€å‘é²æ£’çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(Automatic Speech Recognition)å’Œè¯­éŸ³å¢å¼º(Speech Enhancement)æ¨¡å‹æä¾›æ”¯æ’‘ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSimClassç”Ÿæˆçš„æ•°æ®åœ¨ç‰¹å¾ä¸Šèƒ½å¤Ÿç´§å¯†æ¥è¿‘çœŸå®çš„æ•™å®¤è¯­éŸ³ï¼Œæœ‰æ•ˆå¡«è¡¥äº†æ•™è‚²AIç ”ç©¶çš„æ•°æ®ç©ºç™½ã€‚è¯¥æ–¹æ³•ä¸ä»…å±•ç¤ºäº†æ¨¡æ‹Ÿæ•°æ®åœ¨ç‰¹å®šé¢†åŸŸä¸­çš„åº”ç”¨ä»·å€¼ï¼Œä¹Ÿä¸ºå…¶ä»–é¢†åŸŸçš„åˆæˆæ•°æ®ç”Ÿæˆæä¾›äº†å¯å€Ÿé‰´çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09206v1",
      "published_date": "2025-06-10 19:51:57 UTC",
      "updated_date": "2025-06-10 19:51:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:17.904891+00:00"
    },
    {
      "arxiv_id": "2506.09204v1",
      "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project",
      "title_zh": "ç¨€ç–è¿›åŒ–è®­ç»ƒæ•´ä½“æ€§èƒ½çš„æ‹“æ‰‘æ”¹è¿›ï¼šåŸºäºæ¨¡ä½“çš„ç¨€ç–å¤šå±‚æ„ŸçŸ¥æœºç»“æ„ä¼˜åŒ–é¡¹ç›®",
      "authors": [
        "Xiaotian Chen",
        "Hongyun Liu",
        "Seyed Sahand Mohammadi Ziabari"
      ],
      "abstract": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective and have been applied across diverse domains within deep learning. However, as DNN models increase in complexity, the demand for reduced computational costs and memory overheads has become increasingly urgent. Sparsity has emerged as a leading approach in this area. The robustness of sparse Multi-layer Perceptrons (MLPs) for supervised feature selection, along with the application of Sparse Evolutionary Training (SET), illustrates the feasibility of reducing computational costs without compromising accuracy. Moreover, it is believed that the SET algorithm can still be improved through a structural optimization method called motif-based optimization, with potential efficiency gains exceeding 40% and a performance decline of under 4%. This research investigates whether the structural optimization of Sparse Evolutionary Training applied to Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent this improvement can be achieved.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§ä»¥åŠé™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„ç´§è¿«éœ€æ±‚ï¼Œæ¢è®¨äº†ç¨€ç–æ€§åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶äº†åº”ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨(MLPs)çš„ç¨€ç–è¿›åŒ–è®­ç»ƒ(Sparse Evolutionary Training, SET)ç®—æ³•ï¼Œå¹¶æå‡ºé€šè¿‡åŸºäºåŸºåºçš„ç»“æ„ä¼˜åŒ–(motif-based optimization)å¯¹å…¶è¿›è¡Œæ‹“æ‰‘æ”¹è¿›ã€‚ç ”ç©¶æ·±å…¥è°ƒæŸ¥äº†è¿™ç§ç»“æ„ä¼˜åŒ–åœ¨æå‡SET-MLPæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§åŠå…¶å…·ä½“çš„æ”¹è¿›ç¨‹åº¦ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥ä¼˜åŒ–æ–¹æ³•åœ¨ä¿è¯æ€§èƒ½ä¸‹é™ä½äº4%çš„å‰æä¸‹ï¼Œå…·æœ‰å®ç°40%ä»¥ä¸Šæ•ˆç‡æå‡çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç¨€ç–ç½‘ç»œæ€§èƒ½ä¼˜åŒ–æä¾›äº†æ–°è§†è§’ï¼Œå¯¹æ„å»ºå…¼é¡¾å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09204v1",
      "published_date": "2025-06-10 19:49:07 UTC",
      "updated_date": "2025-06-10 19:49:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:31.056894+00:00"
    },
    {
      "arxiv_id": "2506.09202v2",
      "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning",
      "title_zh": "ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­åŸºäºç­–ç•¥çš„è½¨è¿¹èšç±»",
      "authors": [
        "Hao Hu",
        "Xinqi Wang",
        "Simon Shaolei Du"
      ],
      "abstract": "We introduce a novel task of clustering trajectories from offline reinforcement learning (RL) datasets, where each cluster center represents the policy that generated its trajectories. By leveraging the connection between the KL-divergence of offline trajectory distributions and a mixture of policy-induced distributions, we formulate a natural clustering objective. To solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies and assigns trajectories based on policy generation probabilities, while CAAE resembles the VQ-VAE framework by guiding the latent representations of trajectories toward the vicinity of specific codebook entries to achieve clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans and identify a key challenge in offline trajectory clustering: the inherent ambiguity of optimal solutions due to policy-induced conflicts, which can result in multiple equally valid but structurally distinct clusterings. Experimentally, we validate our methods on the widely used D4RL dataset and custom GridWorld environments. Our results show that both PG-Kmeans and CAAE effectively partition trajectories into meaningful clusters. They offer a promising framework for policy-based trajectory clustering, with broad applications in offline RL and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) æ•°æ®é›†æå‡ºäº†ä¸€ç§å…¨æ–°çš„è½¨è¿¹èšç±»ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿æ¯ä¸ªèšç±»ä¸­å¿ƒä»£è¡¨ç”Ÿæˆå…¶è½¨è¿¹çš„ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨ç¦»çº¿è½¨è¿¹åˆ†å¸ƒçš„ KL-divergence ä¸ç­–ç•¥è¯±å¯¼åˆ†å¸ƒæ··åˆä¹‹é—´çš„è”ç³»ï¼Œç ”ç©¶äººå‘˜åˆ¶å®šäº†è‡ªç„¶çš„èšç±»ç›®æ ‡ï¼Œå¹¶æå‡ºäº† Policy-Guided K-means (PG-Kmeans) å’Œ Centroid-Attracted Autoencoder (CAAE) ä¸¤ç§ç®—æ³•ã€‚PG-Kmeans é€šè¿‡è¿­ä»£è®­ç»ƒè¡Œä¸ºå…‹éš† (Behavior Cloning) ç­–ç•¥å¹¶åŸºäºç”Ÿæˆæ¦‚ç‡åˆ†é…è½¨è¿¹ï¼Œè€Œ CAAE åˆ™å€Ÿé‰´ VQ-VAE æ¡†æ¶å°†è½¨è¿¹çš„æ½œåœ¨è¡¨ç¤ºå¼•å¯¼è‡³ç‰¹å®šç æœ¬æ¡ç›®ä»¥å®ç°èšç±»ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œè¯¥ç ”ç©¶è¯æ˜äº† PG-Kmeans çš„æœ‰é™æ­¥æ”¶æ•›æ€§ï¼Œå¹¶æ­ç¤ºäº†ç”±äºç­–ç•¥è¯±å¯¼å†²çªå¯¼è‡´çš„è§£çš„æ¨¡ç³Šæ€§è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚å®éªŒåœ¨ D4RL æ•°æ®é›†å’Œè‡ªå®šä¹‰ GridWorld ç¯å¢ƒä¸­éªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜è¯¥æ¡†æ¶èƒ½æˆåŠŸå°†è½¨è¿¹åˆ’åˆ†ä¸ºå…·æœ‰å®é™…æ„ä¹‰çš„ç°‡ï¼Œä¸ºç­–ç•¥é©±åŠ¨çš„è½¨è¿¹èšç±»åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09202v2",
      "published_date": "2025-06-10 19:44:48 UTC",
      "updated_date": "2025-06-12 03:14:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:29.946357+00:00"
    },
    {
      "arxiv_id": "2506.09199v1",
      "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models",
      "title_zh": "FLoRISTï¼šåŸºäºå¥‡å¼‚å€¼é˜ˆå€¼çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆç²¾å‡†è”é‚¦å¾®è°ƒ",
      "authors": [
        "Hariharan Ramesh",
        "Jyotikrishna Dass"
      ],
      "abstract": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a promising solution for parameter-efficient fine-tuning of Large Language Models (LLMs) without sharing local data. However, several methods designed for federated LoRA present significant challenges in balancing communication efficiency, model accuracy, and computational cost, particularly among heterogeneous clients. These methods either rely on simplistic averaging of local adapters, which introduces aggregation noise, require transmitting large stacked local adapters, leading to poor communication efficiency, or necessitate reconstructing memory-dense global weight-update matrix and performing computationally expensive decomposition to design client-specific low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning framework that achieves mathematically accurate aggregation without incurring high communication or computational overhead. Instead of constructing the full global weight-update matrix at the server, FLoRIST employs an efficient decomposition pipeline by performing singular value decomposition on stacked local adapters separately. This approach operates within a compact intermediate space to represent the accumulated information from local LoRAs. We introduce tunable singular value thresholding for server-side optimal rank selection to construct a pair of global low-rank adapters shared by all clients. Extensive empirical evaluations across multiple datasets and LLMs demonstrate that FLoRIST consistently strikes the best balance between superior communication efficiency and competitive performance in both homogeneous and heterogeneous setups.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹è¿›è¡Œä½ç§©è‡ªé€‚åº”(LoRA)å¾®è°ƒæ—¶ï¼Œå­˜åœ¨çš„é€šä¿¡æ•ˆç‡ã€æ¨¡å‹ç²¾åº¦ä¸è®¡ç®—æˆæœ¬éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼Œæå‡ºäº†FLoRISTæ¡†æ¶ã€‚FLoRIST å®ç°äº†æ•°å­¦ä¸Šç²¾ç¡®çš„èšåˆï¼Œä¸”æ— éœ€åœ¨æœåŠ¡å™¨ç«¯æ„å»ºå®Œæ•´çš„å…¨å±€æƒé‡æ›´æ–°çŸ©é˜µï¼Œè€Œæ˜¯é€šè¿‡å¯¹å †å çš„æœ¬åœ°é€‚é…å™¨è¿›è¡Œç‹¬ç«‹çš„å¥‡å¼‚å€¼åˆ†è§£(SVD)æ¥æå–å…³é”®ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åœ¨ç´§å‡‘çš„ä¸­é—´ç©ºé—´å†…æ“ä½œï¼Œå¹¶å¼•å…¥äº†å¯è°ƒçš„å¥‡å¼‚å€¼é˜ˆå€¼(singular value thresholding)æŠ€æœ¯ï¼Œç”¨äºåœ¨æœåŠ¡å™¨ç«¯è¿›è¡Œæœ€ä¼˜ç§©é€‰æ‹©ï¼Œä»è€Œæ„å»ºä¾›æ‰€æœ‰å®¢æˆ·ç«¯å…±äº«çš„å…¨å±€ä½ç§©é€‚é…å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§æ•°æ®é›†å’ŒLLMsä¸Šï¼ŒFLoRIST åœ¨åŒè´¨å’Œå¼‚è´¨ç¯å¢ƒä¸‹å‡èƒ½æ˜¾è‘—æå‡é€šä¿¡æ•ˆç‡ï¼Œå¹¶ä¿æŒæå…·ç«äº‰åŠ›çš„æ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09199v1",
      "published_date": "2025-06-10 19:36:36 UTC",
      "updated_date": "2025-06-10 19:36:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:34.686230+00:00"
    },
    {
      "arxiv_id": "2506.11135v1",
      "title": "Large Language Models and Emergence: A Complex Systems Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸æ¶Œç°ï¼šå¤æ‚ç³»ç»Ÿè§†è§’",
      "authors": [
        "David C. Krakauer",
        "John W. Krakauer",
        "Melanie Mitchell"
      ],
      "abstract": "Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea \"more is different\". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea \"less is more\". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å¤æ‚ç³»ç»Ÿ(Complex Systems)çš„è§†è§’æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä¸æ¶Œç°(Emergence)ä¹‹é—´çš„æ ¸å¿ƒè”ç³»ã€‚æ¶Œç°è¢«å®šä¹‰ä¸ºå¤šä½“ç³»ç»Ÿè¡¨ç°å‡ºæ–°é¢–é«˜å±‚å±æ€§çš„ç°è±¡ï¼Œä½“ç°äº†â€œå¤šåˆ™ä¸åŒ(more is different)â€çš„å¤æ‚æ€§ç§‘å­¦æ ¸å¿ƒç†å¿µã€‚æ™ºèƒ½åˆ™è¢«è§†ä¸ºä¸€ç§æè‡´çš„æ¶Œç°å±æ€§ï¼Œä½“ç°ä¸ºé€šè¿‡æ›´é«˜æ•ˆã€æ›´å¿«é€Ÿåœ°åˆ©ç”¨æ¶Œç°èƒ½åŠ›æ¥è§£å†³é—®é¢˜ï¼Œå³â€œå°‘å³æ˜¯å¤š(less is more)â€ã€‚è®ºæ–‡é¦–å…ˆå®¡æŸ¥äº†å…³äºå¤§è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæ¶Œç°èƒ½åŠ›çš„å„ç±»ä¸»å¼ ï¼Œå¹¶å›é¡¾äº†å¤šç§é‡åŒ–æ¶Œç°çš„ç§‘å­¦æ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†LLMsæ˜¯å¦çœŸæ­£å…·å¤‡æ¶Œç°æ™ºèƒ½(emergent intelligence)ï¼Œä¸ºç†è§£å¤§æ¨¡å‹çš„èƒ½åŠ›æœ¬è´¨æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11135v1",
      "published_date": "2025-06-10 19:31:26 UTC",
      "updated_date": "2025-06-10 19:31:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:42.792055+00:00"
    },
    {
      "arxiv_id": "2506.09195v1",
      "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms",
      "title_zh": "åŸºäºå›¾æ³¨æ„åŠ›çš„å¤šæ— äººæœºé›†ç¾¤åŒç›®æ ‡æ§åˆ¶å»ä¸­å¿ƒåŒ– Actor-Critic ç®—æ³•",
      "authors": [
        "Haoran Peng",
        "Ying-Jun Angela Zhang"
      ],
      "abstract": "This research focuses on optimizing multi-UAV systems with dual objectives: maximizing service coverage as the primary goal while extending battery lifetime as the secondary objective. We propose a Graph Attention-based Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed approach leverages a graph attention network to process UAVs' limited local observation and reduce the dimension of the environment states. Subsequently, an actor-double-critic network is developed to manage dual policies for joint objective optimization. The proposed GADC uses a Kullback-Leibler (KL) divergence factor to balance the tradeoff between coverage performance and battery lifetime in the multi-UAV system. We assess the scalability and efficiency of GADC through comprehensive benchmarking against state-of-the-art methods, considering both theory and experimental aspects. Extensive testing in both ideal settings and NVIDIA Sionna's realistic ray tracing environment demonstrates GADC's superior performance.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºä¼˜åŒ–å¤šæ— äººæœº (multi-UAV) ç³»ç»Ÿï¼Œæ—¨åœ¨åŒæ—¶å®ç°æœ€å¤§åŒ–æœåŠ¡è¦†ç›– (service coverage) å’Œå»¶é•¿ç”µæ± å¯¿å‘½ (battery lifetime) çš„åŒé‡ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›æœºåˆ¶çš„å»ä¸­å¿ƒåŒ– Actor-Critic (Graph Attention-based Decentralized Actor-Critic, GADC) ç®—æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œ (graph attention network) å¤„ç†æ— äººæœºçš„æœ‰é™å±€éƒ¨è§‚æµ‹å¹¶é™ä½ç¯å¢ƒçŠ¶æ€ç»´åº¦ï¼Œéšåé€šè¿‡åŒè¯„è®ºå®¶æ‰§è¡Œå™¨ (actor-double-critic) ç½‘ç»œæ¥ç®¡ç†è”åˆç›®æ ‡ä¼˜åŒ–çš„åŒé‡ç­–ç•¥ã€‚GADC å¼•å…¥äº† Kullback-Leibler (KL) æ•£åº¦å› å­ä»¥å¹³è¡¡è¦†ç›–æ€§èƒ½ä¸ç”µæ± å¯¿å‘½ä¹‹é—´çš„æƒè¡¡ (tradeoff)ã€‚ç ”ç©¶åœ¨ç†æƒ³ç¯å¢ƒå’Œ NVIDIA Sionna ç°å®å…‰çº¿è¿½è¸ª (ray tracing) ç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œå¹¶ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¯æ˜äº† GADC åœ¨å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºå¤šæ— äººæœºé›†ç¾¤çš„åŒç›®æ ‡æ§åˆ¶æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09195v1",
      "published_date": "2025-06-10 19:30:53 UTC",
      "updated_date": "2025-06-10 19:30:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:37.190869+00:00"
    },
    {
      "arxiv_id": "2506.09194v1",
      "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks",
      "title_zh": "å¯¹æ¯”é¢„æµ‹ç¼–ç ä¸è„‰å†²ç¥ç»ç½‘ç»œçš„èåˆ",
      "authors": [
        "Emirhan BilgiÃ§",
        "Neslihan Serap ÅengÃ¶r",
        "NamÄ±k Berk YalabÄ±k",
        "Yavuz Selim Ä°ÅŸler",
        "Aykut GÃ¶rkem Gelen",
        "Rahmi Elibol"
      ],
      "abstract": "This study examines the integration of Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN). While CPC learns the predictive structure of data to generate meaningful representations, SNN mimics the computational processes of biological neural systems over time. In this study, the goal is to develop a predictive coding model with greater biological plausibility by processing inputs and outputs in a spike-based system. The proposed model was tested on the MNIST dataset and achieved a high classification rate in distinguishing positive sequential samples from non-sequential negative samples. The study demonstrates that CPC can be effectively combined with SNN, showing that an SNN trained for classification tasks can also function as an encoding mechanism. Project codes and detailed results can be accessed on our GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯¹æ¯”é¢„æµ‹ç¼–ç (Contrastive Predictive Coding, CPC)ä¸è„‰å†²ç¥ç»ç½‘ç»œ(Spiking Neural Networks, SNN)çš„é›†æˆï¼Œæ—¨åœ¨é€šè¿‡è„‰å†²ç³»ç»Ÿå¤„ç†è¾“å…¥è¾“å‡ºæ¥æ„å»ºå…·æœ‰æ›´é«˜ç”Ÿç‰©åˆç†æ€§çš„é¢„æµ‹ç¼–ç æ¨¡å‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†CPCå­¦ä¹ æ•°æ®é¢„æµ‹ç»“æ„çš„èƒ½åŠ›ä¸SNNæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»è¿‡ç¨‹çš„ç‰¹æ€§ï¼Œåœ¨åŸºäºè„‰å†²çš„ç³»ç»Ÿä¸­ç”Ÿæˆæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚å®éªŒåœ¨MNISTæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨åŒºåˆ†æ­£å‘åºåˆ—æ ·æœ¬ä¸éåºåˆ—è´Ÿæ ·æœ¬çš„ä»»åŠ¡ä¸­å–å¾—äº†æé«˜çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚ç ”ç©¶è¯æ˜äº†CPCå¯ä»¥æœ‰æ•ˆåœ°ä¸SNNç»“åˆï¼Œå¹¶å‘ç°ä¸ºåˆ†ç±»ä»»åŠ¡è®­ç»ƒçš„SNNåŒæ ·å¯ä»¥ä½œä¸ºä¸€ç§é«˜æ•ˆçš„ç¼–ç æœºåˆ¶ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†SNNåœ¨é¢„æµ‹æ€§å­¦ä¹ ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´ç¬¦åˆç”Ÿç‰©å­¦åŸç†çš„ç¥ç»è®¡ç®—æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal Processing and Communications Applications Conference (SIU)",
      "pdf_url": "https://arxiv.org/pdf/2506.09194v1",
      "published_date": "2025-06-10 19:23:08 UTC",
      "updated_date": "2025-06-10 19:23:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:31:58.592572+00:00"
    },
    {
      "arxiv_id": "2506.09183v2",
      "title": "Multi-Task Reward Learning from Human Ratings",
      "title_zh": "åŸºäºäººç±»è¯„åˆ†çš„å¤šä»»åŠ¡å¥–åŠ±å­¦ä¹ ",
      "authors": [
        "Mingkang Wu",
        "Devin White",
        "Evelyn Rose",
        "Vernon Lawhern",
        "Nicholas R Waytowich",
        "Yongcan Cao"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has become a key factor in aligning model behavior with users' goals. However, while humans integrate multiple strategies when making decisions, current RLHF approaches often simplify this process by modeling human reasoning through isolated tasks such as classification or regression. In this paper, we propose a novel reinforcement learning (RL) method that mimics human decision-making by jointly considering multiple tasks. Specifically, we leverage human ratings in reward-free environments to infer a reward function, introducing learnable weights that balance the contributions of both classification and regression models. This design captures the inherent uncertainty in human decision-making and allows the model to adaptively emphasize different strategies. We conduct several experiments using synthetic human ratings to validate the effectiveness of the proposed approach. Results show that our method consistently outperforms existing rating-based RL methods, and in some cases, even surpasses traditional RL approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­å­¦ä¹ (RLHF)å°†äººç±»æ¨ç†ç®€åŒ–ä¸ºå­¤ç«‹çš„åˆ†ç±»(Classification)æˆ–å›å½’(Regression)ä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿäººç±»å¤šç­–ç•¥å†³ç­–è¿‡ç¨‹çš„å¤šä»»åŠ¡å¥–åŠ±å­¦ä¹ (Multi-Task Reward Learning)æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ— å¥–åŠ±(Reward-free)ç¯å¢ƒä¸‹åˆ©ç”¨äººç±»è¯„åˆ†(Human Ratings)æ¨æ–­å¥–åŠ±å‡½æ•°(Reward Function)ï¼Œå¹¶é€šè¿‡å¼•å…¥å¯å­¦ä¹ æƒé‡(Learnable Weights)æ¥å¹³è¡¡åˆ†ç±»ä¸å›å½’æ¨¡å‹çš„è´¡çŒ®ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆæ•æ‰äº†äººç±»å†³ç­–ä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§(Uncertainty)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªé€‚åº”åœ°å¼ºè°ƒä¸åŒçš„å†³ç­–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆè¯„åˆ†æµ‹è¯•ä¸­çš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„åŸºäºè¯„åˆ†çš„RLæ–¹æ³•ï¼Œç”šè‡³åœ¨æŸäº›åœºæ™¯ä¸‹è¶…è¶Šäº†ä¼ ç»ŸRLæ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡æ•´åˆå¤šä»»åŠ¡å»ºæ¨¡ï¼Œä¸ºå®ç°æ¨¡å‹è¡Œä¸ºä¸ç”¨æˆ·ç›®æ ‡çš„é«˜æ•ˆå¯¹é½æä¾›äº†æ–°çš„è§†è§’å’Œæ›´å…·é²æ£’æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment at the 42nd International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2506.09183v2",
      "published_date": "2025-06-10 19:00:19 UTC",
      "updated_date": "2025-06-17 19:12:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:21.946688+00:00"
    },
    {
      "arxiv_id": "2506.09176v1",
      "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism",
      "title_zh": "åŸºäºè‡ªé€‚åº”å¹²é¢„æœºåˆ¶çš„æœºå™¨äººé—¨æ§äº¤äº’å¼æ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Haoyuan Cai",
        "Zhenghao Peng",
        "Bolei Zhou"
      ],
      "abstract": "Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency. Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªé€‚åº”å¹²é¢„æœºåˆ¶ (Adaptive Intervention Mechanism, AIM)ï¼Œæ—¨åœ¨è§£å†³äº¤äº’å¼æ¨¡ä»¿å­¦ä¹  (Interactive Imitation Learning, IIL) ä¸­äººç±»ç›‘ç£è€…è®¤çŸ¥è´Ÿæ‹…è¿‡é‡çš„é—®é¢˜ã€‚ä½œä¸ºä¸€ç§ç”±æœºå™¨äººè§¦å‘çš„ IIL ç®—æ³•ï¼ŒAIM åˆ©ç”¨ä»£ç† Q-function (proxy Q-function) æ¨¡æ‹Ÿäººç±»å¹²é¢„è§„åˆ™ï¼Œå¹¶æ ¹æ®æ™ºèƒ½ä½“ä¸äººç±»åŠ¨ä½œçš„å¯¹é½ç¨‹åº¦å­¦ä¹ è‡ªé€‚åº”çš„å¹²é¢„è¯·æ±‚æ ‡å‡†ã€‚å½“æ™ºèƒ½ä½“åç¦»ä¸“å®¶è¡Œä¸ºæ—¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡é«˜ Q-values è¯†åˆ«å®æ—¶å¯¹é½åå·®ï¼Œå¹¶åœ¨å…¶å˜å¾—ç†Ÿç»ƒæ—¶é™ä½è¯·æ±‚é¢‘ç‡ï¼Œä»è€Œå®ç°æŒ‰éœ€è¯·æ±‚ååŠ©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIM åœ¨è¿ç»­å’Œç¦»æ•£æ§åˆ¶ä»»åŠ¡ä¸­å‡æ˜¾è‘—é™ä½äº†ä¸“å®¶çš„ç›‘æµ‹æˆæœ¬ã€‚ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ Thrifty-DAggerï¼Œè¯¥æ–¹æ³•åœ¨äººç±»æ¥ç®¡æˆæœ¬ (take-over cost) å’Œå­¦ä¹ æ•ˆç‡ä¸Šæå‡äº† 40%ï¼Œå¹¶èƒ½æœ‰æ•ˆè¯†åˆ«å®‰å…¨å…³é”® (safety-critical) çŠ¶æ€ï¼Œä»¥æ›´å°‘çš„ä¸“å®¶æ•°æ®å’Œç¯å¢ƒäº¤äº’å®ç°é«˜è´¨é‡çš„å­¦ä¹ æ•ˆæœã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025 Poster",
      "pdf_url": "https://arxiv.org/pdf/2506.09176v1",
      "published_date": "2025-06-10 18:43:26 UTC",
      "updated_date": "2025-06-10 18:43:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:10.096273+00:00"
    },
    {
      "arxiv_id": "2506.09175v1",
      "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation",
      "title_zh": "PHRASEDï¼šé’ˆå¯¹è¯­éŸ³ç¿»è¯‘çš„çŸ­è¯­è¯å…¸åç½®",
      "authors": [
        "Peidong Wang",
        "Jian Xue",
        "Rui Zhao",
        "Junkun Chen",
        "Aswin Shanmugam Subramanian",
        "Jinyu Li"
      ],
      "abstract": "Phrases are essential to understand the core concepts in conversations. However, due to their rare occurrence in training data, correct translation of phrases is challenging in speech translation tasks. In this paper, we propose a phrase dictionary biasing method to leverage pairs of phrases mapping from the source language to the target language. We apply the phrase dictionary biasing method to two types of widely adopted models, a transducer-based streaming speech translation model and a multimodal large language model. Experimental results show that the phrase dictionary biasing method outperforms phrase list biasing by 21% relatively for the streaming speech translation model. In addition, phrase dictionary biasing enables multimodal large language models to use external phrase information, achieving 85% relative improvement in phrase recall.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­çŸ­è¯­å› è®­ç»ƒæ•°æ®ç¨€ç¼ºè€Œéš¾ä»¥å‡†ç¡®ç¿»è¯‘çš„é—®é¢˜ï¼Œæå‡ºäº† PHRASED æ–¹æ³•ï¼Œå³ä¸€ç§åˆ©ç”¨æºè¯­è¨€åˆ°ç›®æ ‡è¯­è¨€æ˜ å°„å¯¹çš„çŸ­è¯­å­—å…¸åç½® (phrase dictionary biasing) æŠ€æœ¯ã€‚ç ”ç©¶è€…å°†è¯¥æ–¹æ³•åº”ç”¨äºåŸºäº Transducer çš„æµå¼è¯­éŸ³ç¿»è¯‘æ¨¡å‹ä»¥åŠå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models) ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æµå¼è¯­éŸ³ç¿»è¯‘æ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„çŸ­è¯­åˆ—è¡¨åç½® (phrase list biasing) æ€§èƒ½ç›¸å¯¹æå‡äº† 21%ã€‚åŒæ—¶ï¼ŒPHRASED ä½¿å¾—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ­è¯­ä¿¡æ¯ï¼Œåœ¨çŸ­è¯­å¬å›ç‡ (phrase recall) ä¸Šå®ç°äº† 85% çš„æ˜¾è‘—ç›¸å¯¹æå‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¯¹æ ¸å¿ƒæ¦‚å¿µçš„å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09175v1",
      "published_date": "2025-06-10 18:42:38 UTC",
      "updated_date": "2025-06-10 18:42:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:11.589989+00:00"
    },
    {
      "arxiv_id": "2506.09171v1",
      "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search",
      "title_zh": "é€šè¿‡åŸå­äº‹å®å¢å¼ºä¸å‰ç»æœç´¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è§„åˆ’",
      "authors": [
        "Samuel Holt",
        "Max Ruiz Luyten",
        "Thomas Pouplin",
        "Mihaela van der Schaar"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­å¯¹æŒ‡å¯¼ä¾èµ–æ€§é«˜ã€éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è¿‡å»ç»éªŒè¿›è¡Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)å¢å¼ºè§„åˆ’èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸå­äº‹å®å¢å¼º(Atomic Fact Augmentation)æŠ€æœ¯ï¼Œä»äº¤äº’è½¨è¿¹ä¸­æå–ä»»åŠ¡å…³é”®çš„åŸå­äº‹å®ï¼Œå¹¶åŠ¨æ€åœ°å¢å¼ºç”¨äºåŠ¨ä½œæè®®(Action Proposal)ã€æ½œåœ¨ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿ(Latent World Model Simulation)å’ŒçŠ¶æ€ä»·å€¼è¯„ä¼°(State-Value Estimation)çš„æç¤ºè¯ã€‚æ™ºèƒ½ä½“é‡‡ç”¨æ·±åº¦å—é™çš„å‰ç»æœç´¢(Lookahead Search)ç­–ç•¥ï¼Œåœ¨ç§¯ç´¯çš„äº‹å®å’Œå†å²å¼•å¯¼ä¸‹æ¨¡æ‹Ÿæ½œåœ¨è½¨è¿¹å¹¶è¯„ä¼°å…¶äº§å‡ºï¼Œä»è€Œåœ¨ä¸è¿›è¡Œå‚æ•°æ›´æ–°(Weight Updates)çš„æƒ…å†µä¸‹å®ç°å†³ç­–è¡Œä¸ºçš„åœ¨çº¿ä¼˜åŒ–ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†æ€§èƒ½ä¸äº‹å®æŠ½è±¡è´¨é‡åŠæ¨¡æ‹Ÿå‡†ç¡®åº¦ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ TextFrozenLake å’Œ ALFWorld ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ï¼Œèƒ½å¤Ÿéšç€ç»éªŒç§¯ç´¯å®ç°æ›´ä¼˜çš„è§„åˆ’è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada",
      "pdf_url": "https://arxiv.org/pdf/2506.09171v1",
      "published_date": "2025-06-10 18:36:31 UTC",
      "updated_date": "2025-06-10 18:36:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:12.783308+00:00"
    },
    {
      "arxiv_id": "2506.09167v2",
      "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry",
      "title_zh": "åŸºäºè…•æˆ´å¼åŠ é€Ÿåº¦æµ‹é‡çš„å†…è„è‚¥èƒ–è¯„ä¼°",
      "authors": [
        "James R. Williamson",
        "Andrew Alini",
        "Brian A. Telfer",
        "Adam W. Potter",
        "Karl E. Friedl"
      ],
      "abstract": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and habitual physical activity (PA). Excess VAT is highly correlated with type 2 diabetes and insulin resistance. The mechanistic basis for this pathophysiology relates to overloading the liver with fatty acids. VAT is also a highly labile fat depot, with increased turnover stimulated by catecholamines during exercise. VAT can be measured with sophisticated imaging technologies, but can also be inferred directly from PA. We tested this relationship using National Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men; 2,427 women) [1]. Two approaches were used for estimating VAT from activity. The first used engineered features based on movements during gait and sleep, and then ridge regression to map summary statistics of these features into a VAT estimate. The second approach used deep neural networks trained on 24 hours of continuous accelerometry. A foundation model first mapped each 10s frame into a high-dimensional feature vector. A transformer model then mapped each day's feature vector time series into a VAT estimate, which were averaged over multiple days. For both approaches, the most accurate estimates were obtained with the addition of covariate information about subject demographics and body measurements. The best performance was obtained by combining the two approaches, resulting in VAT estimates with correlations of r=0.86. These findings demonstrate a strong relationship between PA and VAT and, by extension, between PA and metabolic health risks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ‰‹è…•ä½©æˆ´å¼åŠ é€Ÿåº¦è®¡(Wrist-Worn Accelerometry)ä¼°ç®—å†…è„è„‚è‚ªç»„ç»‡(Visceral Adipose Tissue, VAT)çš„æ–¹æ³•ï¼ŒVATæ˜¯è¡¡é‡ä»£è°¢å¥åº·å’Œæ—¥å¸¸ä½“åŠ›æ´»åŠ¨(Physical Activity, PA)æ°´å¹³çš„å…³é”®ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨NHANESæ•°æ®åº“ä¸­çº¦4800åå—è¯•è€…çš„7å¤©åŠ é€Ÿåº¦è®¡æ•°æ®ï¼Œåˆ†åˆ«é‡‡ç”¨äº†åŸºäºæ­¥æ€å’Œç¡çœ ç‰¹å¾çš„å²­å›å½’(Ridge Regression)æ–¹æ³•ï¼Œä»¥åŠåŸºäºåŸºç¡€æ¨¡å‹(Foundation model)ä¸Transformeræ¨¡å‹çš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŠ å…¥äººå£ç»Ÿè®¡å­¦å’Œèº«ä½“æµ‹é‡åå˜é‡ä¿¡æ¯åï¼Œç»“åˆä¸Šè¿°ä¸¤ç§è·¯å¾„çš„æ··åˆæ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶VATä¼°ç®—å€¼ä¸çœŸå®å€¼çš„ç›¸å…³æ€§è¾¾åˆ°äº†r=0.86ã€‚è¯¥å‘ç°è¯æ˜äº†ä½“åŠ›æ´»åŠ¨ä¸å†…è„è„‚è‚ªåŠä»£è°¢å¥åº·é£é™©ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œä¸ºé€šè¿‡å•†ä¸šå¯ç©¿æˆ´è®¾å¤‡è¿›è¡Œå¤§è§„æ¨¡ã€éä¾µå…¥æ€§çš„ä»£è°¢å¥åº·è¯„ä¼°æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "eess.SP",
      "comment": "This article has been accepted for publication in IEEE Journal of Biomedical and Health Informatics",
      "pdf_url": "https://arxiv.org/pdf/2506.09167v2",
      "published_date": "2025-06-10 18:33:39 UTC",
      "updated_date": "2025-10-01 16:23:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:19.559213+00:00"
    },
    {
      "arxiv_id": "2506.09160v5",
      "title": "Understanding Human-AI Trust in Education",
      "title_zh": "æ•™è‚²é¢†åŸŸäººæœºä¿¡ä»»æ¢æ",
      "authors": [
        "Griffin Pitts",
        "Sanaz Motamedi"
      ],
      "abstract": "As AI chatbots become integrated in education, students are turning to these systems for guidance, feedback, and information. However, the anthropomorphic characteristics of these chatbots create ambiguity over whether students develop trust in them in ways similar to trusting a human peer or instructor (human-like trust, often linked to interpersonal trust models) or in ways similar to trusting a conventional technology (system-like trust, often linked to technology trust models). This ambiguity presents theoretical challenges, as interpersonal trust models may inappropriately ascribe human intentionality and morality to AI, while technology trust models were developed for non-social systems, leaving their applicability to conversational, human-like agents unclear. To address this gap, we examine how these two forms of trust, human-like and system-like, comparatively influence students' perceptions of an AI chatbot, specifically perceived enjoyment, trusting intention, behavioral intention to use, and perceived usefulness. Using partial least squares structural equation modeling, we found that both forms of trust significantly influenced student perceptions, though with varied effects. Human-like trust was the stronger predictor of trusting intention, whereas system-like trust more strongly influenced behavioral intention and perceived usefulness; both had similar effects on perceived enjoyment. The results suggest that interactions with AI chatbots give rise to a distinct form of trust, human-AI trust, that differs from human-human and human-technology models, highlighting the need for new theoretical frameworks in this domain. In addition, the study offers practical insights for fostering appropriately calibrated trust, which is critical for the effective adoption and pedagogical impact of AI in education.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ•™è‚²åœºæ™¯ä¸‹å­¦ç”Ÿå¯¹AIèŠå¤©æœºå™¨äººçš„ä¿¡ä»»æœºåˆ¶ï¼Œé‡ç‚¹åˆ†æäº†â€œç±»äººä¿¡ä»»â€ï¼ˆhuman-like trustï¼‰ä¸â€œç±»ç³»ç»Ÿä¿¡ä»»â€ï¼ˆsystem-like trustï¼‰å¦‚ä½•å½±å“å­¦ç”Ÿçš„æ„ŸçŸ¥ä½“éªŒã€‚ç ”ç©¶é‡‡ç”¨åæœ€å°äºŒä¹˜ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼ˆpartial least squares structural equation modelingï¼‰ï¼Œè¯„ä¼°äº†è¿™ä¸¤ç§ä¿¡ä»»å½¢å¼å¯¹æ„ŸçŸ¥äº«å—ã€ä¿¡ä»»æ„å›¾ã€è¡Œä¸ºæ„å›¾åŠæ„ŸçŸ¥æœ‰ç”¨æ€§çš„å…·ä½“ä½œç”¨ã€‚ç»“æœå‘ç°ï¼Œä¸¤ç§ä¿¡ä»»å½¢å¼å‡å…·æœ‰æ˜¾è‘—å½±å“ï¼Œä½†human-like trustå¯¹ä¿¡ä»»æ„å›¾çš„é¢„æµ‹åŠ›æ›´å¼ºï¼Œè€Œsystem-like truståˆ™åœ¨é©±åŠ¨è¡Œä¸ºæ„å›¾å’Œæ„ŸçŸ¥æœ‰ç”¨æ€§æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚è¿™è¡¨æ˜äººæœºäº¤äº’äº§ç”Ÿäº†ä¸€ç§ç‹¬ç‰¹çš„â€œäººæœºä¿¡ä»»â€ï¼ˆhuman-AI trustï¼‰ï¼Œæ—¢ä¸åŒäºäººé™…ä¿¡ä»»ä¹Ÿä¸åŒäºä¼ ç»Ÿçš„æŠ€æœ¯ä¿¡ä»»ã€‚è¯¥å‘ç°å¼ºè°ƒäº†åœ¨æ•™è‚²é¢†åŸŸæ„å»ºæ–°å‹ç†è®ºæ¡†æ¶çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºä¿ƒè¿›AIæœ‰æ•ˆåº”ç”¨ä¸­çš„ä¿¡ä»»æ ¡å‡†ï¼ˆcalibrated trustï¼‰æä¾›äº†å…³é”®çš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Final version, published to Telematics and Informatics Reports",
      "pdf_url": "https://arxiv.org/pdf/2506.09160v5",
      "published_date": "2025-06-10 18:15:40 UTC",
      "updated_date": "2025-12-23 07:29:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:30.345255+00:00"
    },
    {
      "arxiv_id": "2506.09147v4",
      "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation",
      "title_zh": "LLM ä½œä¸ºå®šæ€§è¯„åˆ¤è€…ï¼šè‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­çš„è‡ªåŠ¨åŒ–é”™è¯¯åˆ†æ",
      "authors": [
        "Nadezhda Chirkova",
        "Tunde Oluwaseyi Ajayi",
        "Seth Aycock",
        "Zain Muhammad Mujahid",
        "Vladana PerliÄ‡",
        "Ekaterina Borisova",
        "Markarit Vartampetian"
      ],
      "abstract": "Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† LLM-as-a-qualitative-judgeï¼Œæ—¨åœ¨è§£å†³å½“å‰ LLM-as-a-judge è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾§é‡äºå®šé‡è¯„åˆ†è€Œå¿½è§†å®šæ€§é”™è¯¯åˆ†æçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆ Natural Language Generation (NLG) ç³»ç»Ÿè¾“å‡ºä¸­å¸¸è§é—®é¢˜ç±»å‹çš„ç»“æ„åŒ–æŠ¥å‘Šï¼Œä¸ºå¼€å‘è€…æ”¹è¿›ç³»ç»Ÿæä¾›å…·æœ‰å®é™…æ„ä¹‰çš„è§è§£ã€‚å…¶æ ¸å¿ƒæµç¨‹åŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆæ˜¯å¯¹å•æ¡å®ä¾‹è¿›è¡Œå¼€æ”¾å¼çš„é—®é¢˜åˆ†æï¼Œéšåé‡‡ç”¨ä¸€ç§ç›´è§‚çš„ç´¯ç§¯ç®—æ³•å¯¹å‘ç°çš„é—®é¢˜è¿›è¡Œèšç±»ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ 12 ä¸ª NLG æ•°æ®é›†ä¸Šè¿›è¡Œäº†çº¦ 300 æ¬¡äººå·¥æ ‡æ³¨éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•ç”Ÿæˆçš„ç‰¹å®šå®ä¾‹é—®é¢˜åœ¨ä¸‰åˆ†ä¹‹äºŒçš„æ¡ˆä¾‹ä¸­ä¸äººå·¥æ ‡æ³¨ä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆä¸äººç±»æ ‡æ³¨è€…é«˜åº¦ç›¸ä¼¼çš„é”™è¯¯ç±»å‹æŠ¥å‘Šï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶åœ¨æ˜¾è‘—æå‡ NLG ç³»ç»Ÿæ€§èƒ½æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09147v4",
      "published_date": "2025-06-10 18:01:42 UTC",
      "updated_date": "2025-12-19 16:35:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:39.408751+00:00"
    },
    {
      "arxiv_id": "2506.09050v2",
      "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering",
      "title_zh": "ALE-Benchï¼šé¢å‘é•¿ç¨‹ç›®æ ‡é©±åŠ¨ç®—æ³•å·¥ç¨‹çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yuki Imajuku",
        "Kohki Horie",
        "Yoichi Iwata",
        "Kensho Aoki",
        "Naohiro Takahashi",
        "Takuya Akiba"
      ],
      "abstract": "How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ALE-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å¤æ‚ä¼˜åŒ–ç®—æ³•å·¥ç¨‹(Algorithm Engineering)é¢†åŸŸè¡¨ç°çš„æ–°å‹åŸºå‡†ã€‚è¯¥åŸºå‡†å€Ÿé‰´äº†AtCoder Heuristic Contestsä¸­çš„çœŸå®ä»»åŠ¡ï¼Œæ¶µç›–äº†åŒ…è£¹é€’é€è·¯ç”±ã€æ’ç­å’Œç”µåŠ›å¹³è¡¡ç­‰è®¡ç®—å›°éš¾ä¸”æ— å·²çŸ¥ç²¾ç¡®è§£çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„çŸ­æ—¶ã€é€šè¿‡/å¤±è´¥å¼ç¼–ç¨‹åŸºå‡†ä¸åŒï¼ŒALE-Benché¼“åŠ±åœ¨é•¿æ—¶è§†é‡(Long-Horizon)ä¸‹è¿›è¡Œè¿­ä»£å¼çš„è§£å†³æ–¹æ¡ˆä¼˜åŒ–ï¼Œå…¶è½¯ä»¶æ¡†æ¶æ”¯æŒåˆ©ç”¨æµ‹è¯•åé¦ˆå’Œå¯è§†åŒ–æŠ€æœ¯çš„äº¤äº’å¼æ™ºèƒ½ä½“æ¶æ„ã€‚å¯¹å‰æ²¿å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè™½ç„¶å®ƒä»¬åœ¨ç‰¹å®šé—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨é—®é¢˜çš„ä¸€è‡´æ€§å’Œå¤„ç†é•¿æ—¶ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢ä¸äººç±»ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¯¥ç ”ç©¶é€šè¿‡ALE-Benchæ­ç¤ºäº†å½“å‰AIåœ¨å¤æ‚ç®—æ³•è®¾è®¡ä¸­çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç›®æ ‡é©±åŠ¨çš„AIæŠ€æœ¯å‘å±•æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeurIPS 2025 Datasets & Benchmarks Track",
      "pdf_url": "https://arxiv.org/pdf/2506.09050v2",
      "published_date": "2025-06-10 17:59:56 UTC",
      "updated_date": "2025-10-06 14:44:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:33.909070+00:00"
    },
    {
      "arxiv_id": "2506.09049v3",
      "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
      "title_zh": "VIKI-Rï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å…·èº«å¤šæ™ºèƒ½ä½“åä½œååŒ",
      "authors": [
        "Li Kang",
        "Xiufeng Song",
        "Heng Zhou",
        "Yiran Qin",
        "Jie Yang",
        "Xiaohong Liu",
        "Philip Torr",
        "Lei Bai",
        "Zhenfei Yin"
      ],
      "abstract": "Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€ç¯å¢ƒä¸­å¤šå…·èº«æ™ºèƒ½ä½“(embodied agents)ååŒçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†VIKI-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºå¤šæ™ºèƒ½ä½“åä½œè®¾è®¡çš„å±‚æ¬¡åŒ–åŸºå‡†ï¼Œæ¶µç›–äº†æ™ºèƒ½ä½“æ¿€æ´»ã€ä»»åŠ¡è§„åˆ’å’Œè½¨è¿¹æ„ŸçŸ¥ä¸‰ä¸ªç»“æ„åŒ–å±‚é¢ã€‚ä¸ºäº†éªŒè¯è¯¥åŸºå‡†çš„å®ç”¨æ€§ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†VIKI-Ræ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆåˆ©ç”¨å¸¦æœ‰é“¾å¼æ€ç»´(Chain-of-Thought)æ ‡æ³¨çš„æ¼”ç¤ºæ•°æ®å¯¹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLM)è¿›è¡Œå¾®è°ƒï¼Œéšååœ¨å¤šçº§å¥–åŠ±ä¿¡å·ä¸‹é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVIKI-Råœ¨æ‰€æœ‰ä»»åŠ¡çº§åˆ«ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶æˆåŠŸä¿ƒä½¿å¼‚æ„æ™ºèƒ½ä½“ä¹‹é—´æ¶Œç°å‡ºç»„åˆå¼çš„åä½œæ¨¡å¼ã€‚VIKI-Benchä¸VIKI-Rå…±åŒä¸ºæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„è§†è§‰é©±åŠ¨å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†ç»Ÿä¸€çš„æµ‹è¯•å¹³å°ä¸æœ‰æ•ˆæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/",
      "pdf_url": "https://arxiv.org/pdf/2506.09049v3",
      "published_date": "2025-06-10 17:59:44 UTC",
      "updated_date": "2026-01-22 08:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:37.820432+00:00"
    },
    {
      "arxiv_id": "2506.09046v2",
      "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation",
      "title_zh": "æ™ºèƒ½ä½“ç¥ç»ç½‘ç»œï¼šåŸºäºæ–‡æœ¬åå‘ä¼ æ’­çš„è‡ªè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Xiaowen Ma",
        "Chenyang Lin",
        "Yao Zhang",
        "Volker Tresp",
        "Yunpu Ma"
      ],
      "abstract": "Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative \"team\" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agentic Neural Network (ANN)ï¼Œè¿™æ˜¯ä¸€ç§å°†å¤šæ™ºèƒ½ä½“åä½œæ¦‚å¿µåŒ–ä¸ºåˆ†å±‚ç¥ç»ç½‘ç»œæ¶æ„çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Large Language Models (LLMs) ç³»ç»Ÿè¿‡åº¦ä¾èµ–é™æ€ã€æ‰‹åŠ¨é…ç½®çš„é—®é¢˜ã€‚åœ¨è¯¥æ¶æ„ä¸­ï¼Œæ™ºèƒ½ä½“è¢«è§†ä¸ºç½‘ç»œèŠ‚ç‚¹ï¼Œè€Œæ¯ä¸€å±‚åˆ™å½¢æˆé’ˆå¯¹ç‰¹å®šå­ä»»åŠ¡çš„åä½œå›¢é˜Ÿã€‚ANN é‡‡ç”¨åŒé˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼šForward Phase è´Ÿè´£åŠ¨æ€åˆ†è§£ä»»åŠ¡å¹¶é€å±‚æ„å»ºå›¢é˜Ÿï¼Œè€Œ Backward Phase åˆ™å€Ÿé‰´åå‘ä¼ æ’­ (backpropagation) åŸç†ï¼Œåˆ©ç”¨æ–‡æœ¬åé¦ˆé©±åŠ¨æ™ºèƒ½ä½“è§’è‰²ã€æç¤ºè¯ (prompts) å’Œåè°ƒæœºåˆ¶çš„è‡ªæˆ‘è¿›åŒ–ã€‚è¿™ç§ç¥ç»ç¬¦å· (neuro-symbolic) æ–¹æ³•èµ‹äºˆäº†ç³»ç»Ÿåœ¨è®­ç»ƒååˆ›å»ºæˆ–ä¸“é—¨åŒ–æ™ºèƒ½ä½“å›¢é˜Ÿçš„èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†é«˜ç»´å¤æ‚ä»»åŠ¡çš„å‡†ç¡®æ€§ä¸é€‚åº”æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒANN åœ¨å››ä¸ª benchmark æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„å¤šæ™ºèƒ½ä½“åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æ•°æ®é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸç»“åˆäº† LLMs çš„åä½œèƒ½åŠ›ä¸ç¥ç»ç½‘ç»œçš„çµæ´»æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09046v2",
      "published_date": "2025-06-10 17:59:21 UTC",
      "updated_date": "2025-07-18 14:52:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:09.074898+00:00"
    },
    {
      "arxiv_id": "2506.09040v2",
      "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better",
      "title_zh": "è‡ªå›å½’è¯­ä¹‰è§†è§‰é‡å»ºåŠ©åŠ›æå‡è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£èƒ½åŠ›",
      "authors": [
        "Dianyi Wang",
        "Wei Song",
        "Yikun Wang",
        "Siyuan Wang",
        "Kaicheng Yu",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨è§†è§‰æ¨¡æ€å­¦ä¹ ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†è‡ªå›å½’è¯­ä¹‰è§†è§‰é‡å»º(Autoregressive Semantic Visual Reconstruction, ASVR)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶å®ç°è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€çš„è”åˆå­¦ä¹ ã€‚ç ”ç©¶å‘ç°ï¼Œç›´æ¥é‡å»ºå›¾åƒçš„åŸå§‹å¤–è§‚(raw visual appearance)å¯èƒ½æŸå®³ç†è§£èƒ½åŠ›ï¼Œè€Œé€šè¿‡é‡å»ºå›¾åƒçš„è¯­ä¹‰è¡¨ç¤º(semantic representation)åˆ™èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„ç†è§£æ°´å¹³ã€‚ASVRå…è®¸æ¨¡å‹å³ä½¿åœ¨è¾“å…¥è¿ç»­å›¾åƒç‰¹å¾æ—¶ï¼Œä¹Ÿèƒ½æœ‰æ•ˆé‡å»ºç¦»æ•£çš„è¯­ä¹‰æ ‡è®°(discrete semantic tokens)ï¼Œä»è€Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è·å¾—ç¨³å®šæ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®è§„æ¨¡å’Œæ¨¡å‹åç«¯ä¸‹å‡å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œåœ¨14é¡¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä½¿LLaVA-1.5çš„å¹³å‡å¾—åˆ†æå‡äº†5%ã€‚è¿™ä¸€æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹è¿‡åº¦ä¾èµ–æ–‡æœ¬å¯¹é½è€Œå¿½è§†ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„é—®é¢˜ï¼Œä¸ºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09040v2",
      "published_date": "2025-06-10 17:57:50 UTC",
      "updated_date": "2026-01-05 10:14:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:32:59.914611+00:00"
    },
    {
      "arxiv_id": "2506.09038v1",
      "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
      "title_zh": "AbstentionBenchï¼šæ¨ç†å¤§è¯­è¨€æ¨¡å‹éš¾ä»¥åº”å¯¹ä¸å¯å›ç­”çš„é—®é¢˜",
      "authors": [
        "Polina Kirichenko",
        "Mark Ibrahim",
        "Kamalika Chaudhuri",
        "Samuel J. Bell"
      ],
      "abstract": "For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real-world user queries, which can be underspecified, ill-posed, or fundamentally unanswerable, require LLMs to reason about uncertainty and selectively abstain -- i.e., refuse to answer definitively. However, abstention remains understudied, without a systematic evaluation framework for modern LLMs. In this work, we introduce AbstentionBench, a large-scale benchmark for holistically evaluating abstention across 20 diverse datasets, including questions with unknown answers, underspecification, false premises, subjective interpretations, and outdated information. Evaluating 20 frontier LLMs reveals abstention is an unsolved problem, and one where scaling models is of little use. While recent reasoning LLMs have shown impressive results in complex problem solving, surprisingly, we find that reasoning fine-tuning degrades abstention (by $24\\%$ on average), even for math and science domains on which reasoning models are explicitly trained. We find that while a carefully crafted system prompt can boost abstention in practice, it does not resolve models' fundamental inability to reason about uncertainty. We release AbstentionBench to foster research into advancing LLM reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AbstentionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹ä¸å¯å›ç­”é—®é¢˜æ—¶é€‰æ‹©æ‹’ç»å›ç­”(abstention)èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å‰æé”™è¯¯å’Œä¿¡æ¯è¿‡æ—¶ç­‰20ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ã€‚é€šè¿‡å¯¹20ä¸ªå‰æ²¿æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œabstentionç›®å‰ä»æ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„éš¾é¢˜ï¼Œä¸”å¢åŠ æ¨¡å‹è§„æ¨¡(scaling)å¹¶ä¸èƒ½æœ‰æ•ˆæ”¹å–„è¿™ä¸€ç°çŠ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡æœ€æ–°çš„æ¨ç†å‹LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ¨ç†å¾®è°ƒåè€Œå¯¼è‡´å…¶åœ¨abstentionä¸Šçš„è¡¨ç°å¹³å‡ä¸‹é™äº†24%ï¼Œç”šè‡³åœ¨æ•°å­¦å’Œç§‘å­¦ç­‰å…¶æ“…é•¿çš„é¢†åŸŸä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾é€€æ­¥ã€‚è™½ç„¶ä¼˜åŒ–ç³»ç»Ÿæç¤º(system prompt)å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜æ‹’ç»ç‡ï¼Œä½†å¹¶ä¸èƒ½æ ¹æ²»æ¨¡å‹åœ¨å¤„ç†ä¸ç¡®å®šæ€§(uncertainty)æ¨ç†æ–¹é¢çš„æ ¹æœ¬å±€é™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨ç†æ¨¡å‹åœ¨å¯é æ€§ä¸Šçš„é‡å¤§ç¼ºé™·ï¼Œå¹¶ä¸ºæå‡LLMåœ¨ç°å®ä¸–ç•ŒåŠé«˜é£é™©é¢†åŸŸçš„åº”ç”¨å®‰å…¨æ€§æä¾›äº†å…³é”®å·¥å…·ä¸æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09038v1",
      "published_date": "2025-06-10 17:57:30 UTC",
      "updated_date": "2025-06-10 17:57:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:00.708848+00:00"
    },
    {
      "arxiv_id": "2506.09034v2",
      "title": "FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed",
      "title_zh": "FZOOï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒã€æ—¨åœ¨å®ç° Adam çº§é€Ÿåº¦çš„å¿«é€Ÿé›¶é˜¶ä¼˜åŒ–å™¨",
      "authors": [
        "Sizhe Dang",
        "Yangyang Guo",
        "Yanjun Zhao",
        "Haishan Ye",
        "Xiaodong Zheng",
        "Guang Dai",
        "Ivor Tsang"
      ],
      "abstract": "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FZOOï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¾®è°ƒä¸­æ˜¾å­˜ç“¶é¢ˆçš„é«˜é€Ÿé›¶é˜¶ä¼˜åŒ–å™¨ (Fast Zeroth-Order Optimizer)ã€‚é’ˆå¯¹ç°æœ‰é›¶é˜¶ä¼˜åŒ–å™¨ (ZO) æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼ŒFZOO é€šè¿‡æ‰¹æ¬¡å•è¾¹ä¼°è®¡ (batched one-sided estimates) ç»“åˆåŸºäºæ‰¹æ¬¡æŸå¤±æ ‡å‡†å·®çš„è‡ªé€‚åº”æ­¥é•¿ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†æ”¶æ•›æ‰€éœ€çš„å‰å‘ä¼ æ’­æ¬¡æ•°ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ Rademacher éšæœºå‘é‡æ‰°åŠ¨å’Œ CUDA å¹¶è¡Œå¤„ç†è¿›ä¸€æ­¥æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¸”ç†è®ºè¯æ˜å…¶å½¢å¼ä¸Šç­‰ä»·äº normalized-SGD æ›´æ–°è§„åˆ™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFZOO åœ¨ RoBERTaã€OPTã€Phi-2 å’Œ Llama3 ç­‰å¤šç§æ¨¡å‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ¯” MeZO æé«˜ 3% ä¸”å‰å‘ä¼ æ’­éœ€æ±‚å‡å°‘ 3 å€ã€‚åœ¨ RoBERTa-large ä»»åŠ¡ä¸­ï¼ŒFZOO ç”šè‡³å®ç°äº†ä¸ Adam ç›¸å½“çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶å°†å‰å‘ä¼ æ’­æ¬¡æ•°ç¼©å‡äº† 18 å€ã€‚è¯¥æˆæœä½¿å• GPU ç¯å¢ƒä¸‹çš„é«˜é€Ÿå…¨å‚æ•°å¾®è°ƒå˜å¾—åˆ‡å®å¯è¡Œï¼Œå¹¶å¯ä¸ PEFT æŠ€æœ¯ååŒä»¥å®ç°æ›´æç«¯çš„æ˜¾å­˜èŠ‚çœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09034v2",
      "published_date": "2025-06-10 17:56:53 UTC",
      "updated_date": "2025-06-30 03:33:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:14.209966+00:00"
    },
    {
      "arxiv_id": "2506.09033v3",
      "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
      "title_zh": "Router-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿å¤§è¯­è¨€æ¨¡å‹å­¦ä¼šå¤šè½®è·¯ç”±ä¸èšåˆ",
      "authors": [
        "Haozhen Zhang",
        "Tao Feng",
        "Jiaxuan You"
      ],
      "abstract": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Router-R1ï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ LLM è·¯ç”±å™¨ä»…èƒ½è¿›è¡Œå•è½®ä¸€å¯¹ä¸€æ˜ å°„ã€éš¾ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡çš„å±€é™æ€§ã€‚Router-R1 å°†å¤šæ¨¡å‹è·¯ç”±ä¸èšåˆå»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œä½¿ä½œä¸ºè·¯ç”±å™¨çš„ LLM èƒ½å¤Ÿäº¤æ›¿æ‰§è¡Œå†…éƒ¨æ€è€ƒ (internal deliberation) ä¸åŠ¨æ€æ¨¡å‹è°ƒç”¨ (dynamic model invocation)ï¼Œå¹¶å°†å„æ¨¡å‹çš„å“åº”é›†æˆåˆ°ä¸æ–­æ¼”è¿›çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§è½»é‡çº§è§„åˆ™å¥–åŠ±æœºåˆ¶ï¼ŒåŒ…å«æ ¼å¼å¥–åŠ±ã€æœ€ç»ˆç»“æœå¥–åŠ±ä»¥åŠåˆ›æ–°çš„æˆæœ¬å¥–åŠ± (cost reward)ã€‚æ­¤å¤–ï¼Œè·¯ç”±å™¨ä»…ä¾èµ–ä»·æ ¼ã€å»¶è¿Ÿç­‰ç®€å•æè¿°ç¬¦è¿›è¡Œå†³ç­–ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹æœªè§è¿‡çš„æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé€šç”¨åŠå¤šè·³é—®ç­” (multi-hop QA) åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒRouter-R1 åœ¨ä¿æŒé²æ£’çš„æˆæœ¬ç®¡ç†çš„åŒæ—¶æ˜¾è‘—è¶…è¶Šäº†å¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†æ›´ä¼˜çš„æ€§èƒ½æˆæœ¬æŠ˜ä¸­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025. Code is available at https://github.com/ulab-uiuc/Router-R1. Models and Datasets are available at https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03",
      "pdf_url": "https://arxiv.org/pdf/2506.09033v3",
      "published_date": "2025-06-10 17:56:45 UTC",
      "updated_date": "2025-10-24 10:22:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:14.013017+00:00"
    },
    {
      "arxiv_id": "2506.09027v2",
      "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
      "title_zh": "æ‰©æ•£ä¸å¼¥æ•£ï¼šåŸºäºè¡¨ç¤ºæ­£åˆ™åŒ–çš„å›¾åƒç”Ÿæˆ",
      "authors": [
        "Runqian Wang",
        "Kaiming He"
      ],
      "abstract": "The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \\textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸è¡¨ç¤ºå­¦ä¹ (Representation Learning)é•¿æœŸç‹¬ç«‹å‘å±•ä¸”ç¼ºä¹æ˜¾å¼æ­£åˆ™åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†Dispersive Lossï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”å³æ’å³ç”¨çš„æ­£åˆ™åŒ–é¡¹ã€‚è¯¥æŸå¤±å‡½æ•°é€šè¿‡ä¿ƒä½¿å†…éƒ¨è¡¨ç¤ºåœ¨éšè—ç©ºé—´ä¸­åˆ†æ•£ï¼Œæ¨¡æ‹Ÿäº†å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ (Contrastive Self-Supervised Learning)çš„æ•ˆæœï¼Œä½†å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæ— éœ€æ­£æ ·æœ¬å¯¹ï¼Œä»è€Œé¿å…äº†å¯¹å›å½’é‡‡æ ·è¿‡ç¨‹çš„å¹²æ‰°ã€‚ä¸ç°æœ‰çš„è¡¨ç¤ºå¯¹é½(REPA)æ–¹æ³•ç›¸æ¯”ï¼ŒDispersive Loss å…·æœ‰è‡ªåŒ…å«ä¸”æç®€çš„ç‰¹ç‚¹ï¼Œä¸éœ€è¦ä»»ä½•é¢„è®­ç»ƒã€é¢å¤–å‚æ•°æˆ–å¤–éƒ¨æ•°æ®æ”¯æŒã€‚åœ¨ ImageNet æ•°æ®é›†ä¸Šçš„å¤šé¡¹å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§å¼ºåŸºå‡†æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥è¡¨ç¤ºæ­£åˆ™åŒ–ï¼Œæœ‰æ•ˆç¼©å°äº†ç”Ÿæˆå»ºæ¨¡ä¸è¡¨ç¤ºå­¦ä¹ ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09027v2",
      "published_date": "2025-06-10 17:53:29 UTC",
      "updated_date": "2025-07-24 15:55:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:18.204122+00:00"
    },
    {
      "arxiv_id": "2506.09018v3",
      "title": "Edit Flows: Flow Matching with Edit Operations",
      "title_zh": "Edit Flowsï¼šåŸºäºç¼–è¾‘æ“ä½œçš„æµåŒ¹é…",
      "authors": [
        "Marton Havasi",
        "Brian Karrer",
        "Itai Gat",
        "Ricky T. Q. Chen"
      ],
      "abstract": "Autoregressive generative models naturally generate variable-length sequences, while non-autoregressive models struggle, often imposing rigid, token-wise structures. We propose Edit Flows, a non-autoregressive model that overcomes these limitations by defining a discrete flow over sequences through edit operations$\\unicode{x2013}$insertions, deletions, and substitutions. By modeling these operations within a Continuous-time Markov Chain over the sequence space, Edit Flows enable flexible, position-relative generation that aligns more closely with the structure of sequence data. Our training method leverages an expanded state space with auxiliary variables, making the learning process efficient and tractable. Empirical results show that Edit Flows outperforms both autoregressive and mask models on image captioning and significantly outperforms the mask construction in text and code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éè‡ªå›å½’ç”Ÿæˆæ¨¡å‹(non-autoregressive models)åœ¨å¤„ç†å˜é•¿åºåˆ—æ—¶é¢ä¸´çš„åˆšæ€§ç»“æ„é™åˆ¶ï¼Œæå‡ºäº†åä¸º Edit Flows çš„æ–°å‹æ¨¡å‹ã€‚Edit Flows é€šè¿‡æ’å…¥(insertions)ã€åˆ é™¤(deletions)å’Œæ›¿æ¢(substitutions)ç­‰ç¼–è¾‘æ“ä½œ(edit operations)ï¼Œåœ¨åºåˆ—ç©ºé—´ä¸Šå®šä¹‰äº†ä¸€ç§ç¦»æ•£æµ(discrete flow)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¿ç»­æ—¶é—´é©¬å°”å¯å¤«é“¾(Continuous-time Markov Chain)å¯¹è¿™äº›æ“ä½œè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå®ç°ä¸åºåˆ—ç»“æ„æ›´å¥‘åˆçš„ã€ä½ç½®ç›¸å…³çš„çµæ´»ç”Ÿæˆã€‚ä¸ºç¡®ä¿è®­ç»ƒçš„é«˜æ•ˆæ€§ä¸å¯å¤„ç†æ€§ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†åŒ…å«è¾…åŠ©å˜é‡(auxiliary variables)çš„æ‰©å±•çŠ¶æ€ç©ºé—´æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEdit Flows åœ¨å›¾åƒå­—å¹•ç”Ÿæˆ(image captioning)ä»»åŠ¡ä¸Šä¼˜äºè‡ªå›å½’(autoregressive)å’Œæ©ç æ¨¡å‹(mask models)ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡æœ¬å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„æ©ç æ„å»ºæ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚åºåˆ—ç”Ÿæˆé¢†åŸŸçš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09018v3",
      "published_date": "2025-06-10 17:44:19 UTC",
      "updated_date": "2025-11-12 17:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:20.552263+00:00"
    },
    {
      "arxiv_id": "2506.11130v2",
      "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
      "title_zh": "åŸºäºTTSåˆæˆæ•°æ®å¢å¼ºASRçš„è‡ªæç‚¼æ¡†æ¶",
      "authors": [
        "Cheng-Kang Chou",
        "Chan-Jan Hsu",
        "Ho-Lam Chung",
        "Liang-Hsuan Tseng",
        "Hsi-Chun Cheng",
        "Yu-Kuan Fu",
        "Kuan Po Huang",
        "Hung-Yi Lee"
      ],
      "abstract": "We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªä¼˜åŒ–æ¡†æ¶(self-refining framework)ï¼Œæ—¨åœ¨ä»…åˆ©ç”¨æœªæ ‡è®°æ•°æ®é›†æ¥æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„ASRæ¨¡å‹åœ¨æœªæ ‡æ³¨è¯­éŸ³ä¸Šç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œéšåä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒé«˜ä¿çœŸæ–‡æœ¬è½¬è¯­éŸ³(TTS)ç³»ç»Ÿã€‚é€šè¿‡å°†åˆæˆçš„è¯­éŸ³-æ–‡æœ¬å¯¹åé¦ˆå›åŸå§‹ASRç³»ç»Ÿä¸­è¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œå®ç°äº†ä¸€ä¸ªé—­ç¯çš„è‡ªæˆ‘æ”¹è¿›å‘¨æœŸã€‚ç ”ç©¶è€…åœ¨å°æ¹¾å›½è¯­è¯­éŸ³æ•°æ®ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåˆ©ç”¨6000å°æ—¶æœªæ ‡è®°è¯­éŸ³å°†Whisper-large-v2æ¨¡å‹ä¼˜åŒ–ä¸ºä¸“ç”¨æ¨¡å‹Twisterã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTwisteråœ¨å›½è¯­åŸºå‡†æµ‹è¯•ä¸­çš„é”™è¯¯ç‡é™ä½äº†20%ï¼Œåœ¨å›½è¯­-è‹±è¯­ä»£ç åˆ‡æ¢(code-switching)åœºæ™¯ä¸‹é™ä½äº†é«˜è¾¾50%ã€‚è¯¥æ¡†æ¶ä¸ºä¼ªæ ‡ç­¾è‡ªè’¸é¦æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸ºåœ¨ä½èµ„æºæˆ–ç‰¹å®šé¢†åŸŸç¯å¢ƒä¸‹æå‡ASRæ€§èƒ½æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11130v2",
      "published_date": "2025-06-10 17:30:32 UTC",
      "updated_date": "2025-06-16 15:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:24.110329+00:00"
    },
    {
      "arxiv_id": "2506.08999v1",
      "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification",
      "title_zh": "åº”ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€å„¿ç«¥è¯­éŸ³æˆç†Ÿåº¦åˆ†ç±»",
      "authors": [
        "Theo Zhang",
        "Madurya Suresh",
        "Anne S. Warlaumont",
        "Kasia Hitczenko",
        "Alejandrina Cristia",
        "Margaret Cychosz"
      ],
      "abstract": "Speech technology systems struggle with many downstream tasks for child speech due to small training corpora and the difficulties that child speech pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer models to address a fundamental classification task: identifying child vocalizations. Unlike previous corpora, our dataset captures maximally ecologically-valid child vocalizations across an unprecedented sample, comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu, Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004 labeled vocalizations, magnitudes larger than previous work. Models were trained to distinguish between cry, laughter, mature (consonant+vowel), and immature speech (just consonant or vowel). Models trained on the dataset outperform state-of-the-art models trained on previous datasets, achieved classification accuracy comparable to humans, and were robust across rural and urban settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ (self-supervised learning)æ¨¡å‹å¯¹è·¨è¯­è¨€å„¿ç«¥è¯­éŸ³æˆç†Ÿåº¦è¿›è¡Œåˆ†ç±»ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯­éŸ³ç³»ç»Ÿåœ¨å¤„ç†å„¿ç«¥è¯­éŸ³æ—¶å› è®­ç»ƒè¯­æ–™åº“è§„æ¨¡åå°è€Œé¢ä¸´çš„éš¾é¢˜ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨äº†åä¸º SpeechMaturity çš„æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶åŒ…å«æ¥è‡ªå…¨çƒå¤šä¸ªåœ°åŒºã€æ¶µç›–25ç§ä»¥ä¸Šè¯­è¨€çš„242,004ä¸ªæ ‡æ³¨è¯­éŸ³ç‰‡æ®µï¼Œæå¤§åœ°æå‡äº†æ•°æ®çš„ç”Ÿæ€æ•ˆåº¦(ecological validity)ã€‚é€šè¿‡åº”ç”¨å…ˆè¿›çš„ Transformer æ¨¡å‹ï¼Œè¯¥ç ”ç©¶å®ç°äº†å¯¹å“­å£°(cry)ã€ç¬‘å£°(laughter)ã€æˆç†Ÿè¯­éŸ³(mature speech)åŠä¸æˆç†Ÿè¯­éŸ³(immature speech)çš„ç²¾å‡†è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡å·²è¾¾åˆ°ä¸äººç±»ç›¸å½“çš„æ°´å¹³ï¼Œä¸”åœ¨åŸå¸‚ä¸å†œæ‘ç­‰ä¸åŒç¤¾ä¼šç¯å¢ƒä¸‹å‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§(robustness)ã€‚è¿™é¡¹å·¥ä½œä¸ä»…çªç ´äº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¹Ÿä¸ºè·¨è¯­è¨€å’Œè·¨æ–‡åŒ–çš„å„¿ç«¥è¯­éŸ³æŠ€æœ¯ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in Interspeech 2025. 5 pages, 2 figures. For associated Github repository, see https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM",
      "pdf_url": "https://arxiv.org/pdf/2506.08999v1",
      "published_date": "2025-06-10 17:20:02 UTC",
      "updated_date": "2025-06-10 17:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:33.584474+00:00"
    },
    {
      "arxiv_id": "2506.09108v1",
      "title": "SensorLM: Learning the Language of Wearable Sensors",
      "title_zh": "SensorLMï¼šå­¦ä¹ å¯ç©¿æˆ´ä¼ æ„Ÿå™¨çš„è¯­è¨€",
      "authors": [
        "Yuwei Zhang",
        "Kumar Ayush",
        "Siyuan Qiao",
        "A. Ali Heydari",
        "Girish Narayanswamy",
        "Maxwell A. Xu",
        "Ahmed A. Metwally",
        "Shawn Xu",
        "Jake Garrison",
        "Xuhai Xu",
        "Tim Althoff",
        "Yun Liu",
        "Pushmeet Kohli",
        "Jiening Zhan",
        "Mark Malhotra",
        "Shwetak Patel",
        "Cecilia Mascolo",
        "Xin Liu",
        "Daniel McDuff",
        "Yuzhe Yang"
      ],
      "abstract": "We present SensorLM, a family of sensor-language foundation models that enable wearable sensor data understanding with natural language. Despite its pervasive nature, aligning and interpreting sensor data with language remains challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data. We introduce a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data. This approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and recovers them as specific variants within a generic architecture. Extensive experiments on real-world tasks in human activity analysis and healthcare verify the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. SensorLM also demonstrates intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SensorLMï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€ç†è§£å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®çš„ä¼ æ„Ÿå™¨-è¯­è¨€åŸºç¡€æ¨¡å‹ (foundation models)ã€‚ä¸ºäº†è§£å†³ç°å®ä¸–ç•Œä¸­ç¼ºä¹é…å¯¹ä¸”æ ‡æ³¨ä¸°å¯Œçš„ä¼ æ„Ÿå™¨-æ–‡æœ¬æè¿°è¿™ä¸€éš¾é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå±‚æ¬¡åŒ–æè¿°ç”Ÿæˆæµæ°´çº¿ (hierarchical caption generation pipeline)ï¼Œç”¨äºæ•è·ä¼ æ„Ÿå™¨æ•°æ®ä¸­çš„ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¼ æ„Ÿå™¨-è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª 10.3 ä¸‡ä½™äººçš„ 5970 ä¸‡å°æ—¶æ•°æ®ã€‚SensorLM è¿›ä¸€æ­¥æ‰©å±•äº† CLIP å’Œ CoCa ç­‰ä¸»æµå¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„ï¼Œå¹¶åœ¨é€šç”¨æ¡†æ¶ä¸‹å®ç°äº†å¤šç§å˜ä½“ã€‚åœ¨äººä½“æ´»åŠ¨åˆ†æå’ŒåŒ»ç–—å¥åº·é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯†åˆ« (zero-shot recognition)ã€å°‘æ ·æœ¬å­¦ä¹  (few-shot learning) å’Œè·¨æ¨¡æ€æ£€ç´¢ (cross-modal retrieval) æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼ŒSensorLM è¿˜å±•ç¤ºäº†æ˜¾è‘—çš„æ‰©å±•è¡Œä¸º (scaling behaviors)ã€æ ‡ç­¾æ•ˆç‡ä»¥åŠå¯¹æœªçŸ¥ä»»åŠ¡çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09108v1",
      "published_date": "2025-06-10 17:13:09 UTC",
      "updated_date": "2025-06-10 17:13:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:30.091769+00:00"
    },
    {
      "arxiv_id": "2506.11129v1",
      "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK",
      "title_zh": "åŒ»ç–—å¯ä¿¡äººå·¥æ™ºèƒ½ï¼šåŸºäº CHECK çš„æŒç»­å¹»è§‰æ£€æµ‹ä¸æ¶ˆé™¤",
      "authors": [
        "Carlos Garcia-Fernandez",
        "Luis Felipe",
        "Monique Shotande",
        "Muntasir Zitu",
        "Aakash Tripathi",
        "Ghulam Rasool",
        "Issam El Naqa",
        "Vivek Rudrapatna",
        "Gilmer Valdes"
      ],
      "abstract": "Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CHECKï¼Œä¸€ç§é¢å‘åŒ»ç–—é¢†åŸŸçš„æŒç»­å­¦ä¹ æ¡†æ¶ (Continuous-learning framework)ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´çš„å¹»è§‰ (Hallucinations) é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆç»“æ„åŒ–ä¸´åºŠæ•°æ®åº“ä¸åŸºäºä¿¡æ¯è®º (Information theory) çš„åˆ†ç±»å™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ£€æµ‹äº‹å®æ€§å¹»è§‰å’Œæ¨ç†å‹å¹»è§‰ã€‚åœ¨ 100 é¡¹æ ¸å¿ƒä¸´åºŠè¯•éªŒçš„ 1500 ä¸ªé—®é¢˜è¯„ä¼°ä¸­ï¼ŒCHECK æˆåŠŸå°† LLama3.3-70B-Instruct çš„å¹»è§‰ç‡ä» 31% é™ä½è‡³ 0.3%ï¼Œä½¿å¼€æºæ¨¡å‹è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚è¯¥åˆ†ç±»å™¨åœ¨ MedQA (USMLE) å’Œ HealthBench ç­‰åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒAUC è¾¾åˆ° 0.95-0.96ã€‚é€šè¿‡åˆ©ç”¨å¹»è§‰æ¦‚ç‡æŒ‡å¯¼ GPT-4o è¿›è¡Œç²¾ç»†åŒ–æ”¹è¿›å¹¶ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ï¼ŒCHECK å°†å…¶ USMLE é€šè¿‡ç‡æå‡äº† 5 ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ° 92.1% çš„æœ€å…ˆè¿›æ°´å¹³ (State-of-the-art)ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†å¹»è§‰ç‡å‹ä½è‡³ä¸´åºŠè¯¯å·®é˜ˆå€¼ä»¥ä¸‹ï¼Œä¸ºåŒ»ç–—åŠå…¶ä»–é«˜é£é™©é¢†åŸŸå®‰å…¨éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹å¥ å®šäº†å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11129v1",
      "published_date": "2025-06-10 17:12:28 UTC",
      "updated_date": "2025-06-10 17:12:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:50.595118+00:00"
    },
    {
      "arxiv_id": "2506.11128v2",
      "title": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM Reasoning",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ç±»äººè°¬è¯¯æ¨¡å¼çš„ç†è®ºå¯¼å‘è¯„ä¼°",
      "authors": [
        "Andrew Keenan Richardson",
        "Ryan Othniel Kearns",
        "Sean Moss",
        "Vincent Wang-Mascianica",
        "Philipp Koralus"
      ],
      "abstract": "We study logical reasoning in language models by asking whether their errors follow established human fallacy patterns. Using the Erotetic Theory of Reasoning (ETR) and its open-source implementation, PyETR, we programmatically generate 383 formally specified reasoning problems and evaluate 38 models. For each response, we judge logical correctness and, when incorrect, whether it matches an ETR-predicted fallacy. Two results stand out: (i) as a capability proxy (Chatbot Arena Elo) increases, a larger share of a model's incorrect answers are ETR-predicted fallacies $(Ï=0.360, p=0.0265)$, while overall correctness on this dataset shows no correlation with capability; (ii) reversing premise order significantly reduces fallacy production for many models, mirroring human order effects. Methodologically, PyETR provides an open-source pipeline for unbounded, synthetic, contamination-resistant reasoning tests linked to a cognitive theory, enabling analyses that focus on error composition rather than error rate.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ¢è®¨è¯­è¨€æ¨¡å‹çš„é”™è¯¯æ˜¯å¦éµå¾ªæ—¢å®šçš„äººç±»è°¬è¯¯æ¨¡å¼ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶è€…åŸºäºæ¨ç†é—®ç­”ç†è®º(Erotetic Theory of Reasoning, ETR)åŠå…¶å¼€æºå®ç°PyETRï¼Œç¨‹åºåŒ–ç”Ÿæˆäº†383ä¸ªæ­£å¼å®šä¹‰çš„æ¨ç†é—®é¢˜ï¼Œå¹¶å¯¹38ä¸ªæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œéšç€æ¨¡å‹ç»¼åˆèƒ½åŠ›ï¼ˆä»¥Chatbot Arena Eloä¸ºæŒ‡æ ‡ï¼‰çš„æå‡ï¼Œå…¶é”™è¯¯ç­”æ¡ˆä¸­ç¬¦åˆETRé¢„æµ‹è°¬è¯¯çš„æ¯”ä¾‹æ˜¾è‘—å¢åŠ ï¼Œè€Œè¯¥æ•°æ®é›†ä¸Šçš„æ•´ä½“æ­£ç¡®ç‡ä¸æ¨¡å‹èƒ½åŠ›å¹¶æ— ç›´æ¥å…³è”ã€‚æ­¤å¤–ï¼Œæ”¹å˜å‰æé¡ºåºèƒ½æ˜¾è‘—å‡å°‘è®¸å¤šæ¨¡å‹çš„è°¬è¯¯äº§ç”Ÿï¼Œè¡¨ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„é¡ºåºæ•ˆåº”ã€‚åœ¨æ–¹æ³•è®ºå±‚é¢ï¼ŒPyETRæä¾›äº†ä¸€ä¸ªåŸºäºè®¤çŸ¥ç†è®ºçš„å¼€æºæµæ°´çº¿ï¼Œæ”¯æŒç”Ÿæˆæ— ç•Œä¸”æŠ—æ±¡æŸ“çš„åˆæˆæ¨ç†æµ‹è¯•ï¼Œä¿ƒä½¿ç ”ç©¶è§†è§’ä»å•ä¸€çš„é”™è¯¯ç‡è½¬å‘æ›´æ·±å±‚çš„é”™è¯¯æ„æˆåˆ†æã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11128v2",
      "published_date": "2025-06-10 17:04:33 UTC",
      "updated_date": "2025-10-24 11:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:33:54.886779+00:00"
    },
    {
      "arxiv_id": "2506.09107v1",
      "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines",
      "title_zh": "FAIRTOPIAï¼šæ—¨åœ¨å˜é©ä¸å…¬å¹³ AI æµæ°´çº¿çš„å¤šæ™ºèƒ½ä½“ç›‘æŠ¤æ„æƒ³",
      "authors": [
        "Athena Vakali",
        "Ilias Dimitriadis"
      ],
      "abstract": "AI models have become active decision makers, often acting without human supervision. The rapid advancement of AI technology has already caused harmful incidents that have hurt individuals and societies and AI unfairness in heavily criticized. It is urgent to disrupt AI pipelines which largely neglect human principles and focus on computational biases exploration at the data (pre), model(in), and deployment (post) processing stages. We claim that by exploiting the advances of agents technology, we will introduce cautious, prompt, and ongoing fairness watch schemes, under realistic, systematic, and human-centric fairness expectations. We envision agents as fairness guardians, since agents learn from their environment, adapt to new information, and solve complex problems by interacting with external tools and other systems. To set the proper fairness guardrails in the overall AI pipeline, we introduce a fairness-by-design approach which embeds multi-role agents in an end-to-end (human to AI) synergetic scheme. Our position is that we may design adaptive and realistic AI fairness frameworks, and we introduce a generalized algorithm which can be customized to the requirements and goals of each AI decision making scenario. Our proposed, so called FAIRTOPIA framework, is structured over a three-layered architecture, which encapsulates the AI pipeline inside an agentic guardian and a knowledge-based, self-refining layered scheme. Based on our proposition, we enact fairness watch in all of the AI pipeline stages, under robust multi-agent workflows, which will inspire new fairness research hypothesis, heuristics, and methods grounded in human-centric, systematic, interdisciplinary, socio-technical principles.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FAIRTOPIA æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å¤šæ™ºèƒ½ä½“ç›‘æŠ¤æœºåˆ¶æ¥æ‰“ç ´å½“å‰ AI æµæ°´çº¿ä¸­æ™®éå¿½è§†å…¬å¹³æ€§çš„ç°çŠ¶ã€‚é’ˆå¯¹ AI å†³ç­–ä¸­å­˜åœ¨çš„è®¡ç®—åå·®å’Œäººç±»åŸåˆ™ç¼ºå¤±ï¼ŒFAIRTOPIA é‡‡ç”¨äº†â€œç”±è®¾è®¡è€Œå…¬å¹³â€ (fairness-by-design) çš„ç†å¿µï¼Œå°†å¤šè§’è‰²æ™ºèƒ½ä½“ (multi-role agents) åµŒå…¥åˆ°ç«¯åˆ°ç«¯çš„ååŒæ–¹æ¡ˆä¸­å……å½“å…¬å¹³æ€§å®ˆæŠ¤è€… (fairness guardians)ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªä¸‰å±‚æ¶æ„ (three-layered architecture)ï¼Œå°† AI æµæ°´çº¿å°è£…åœ¨æ™ºèƒ½ä½“ç›‘æŠ¤å’ŒåŸºäºçŸ¥è¯†çš„è‡ªå®Œå–„æ–¹æ¡ˆä¹‹å†…ã€‚é€šè¿‡åœ¨æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²ç­‰å„ä¸ªé˜¶æ®µå®æ–½ç¨³å¥çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµ (multi-agent workflows)ï¼Œè¯¥ç ”ç©¶æä¾›äº†ä¸€ç§å¯æ ¹æ®å…·ä½“å†³ç­–åœºæ™¯å®šåˆ¶çš„é€šç”¨å…¬å¹³æ€§ç®—æ³•ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å…¨å‘¨æœŸçš„å…¬å¹³æ€§ç›‘æµ‹ï¼Œä¸ºæ„å»ºä»¥äººä¸ºä¸­å¿ƒã€ç³»ç»ŸåŒ–ä¸”è·¨å­¦ç§‘çš„ç¤¾ä¼šæŠ€æœ¯å…¬å¹³æ€§ä¿éšœä½“ç³»å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "11 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.09107v1",
      "published_date": "2025-06-10 17:02:43 UTC",
      "updated_date": "2025-06-10 17:02:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:06.091082+00:00"
    },
    {
      "arxiv_id": "2506.08990v1",
      "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models",
      "title_zh": "åŸºäºæ©ç è§†è§‰æ¨¡å‹é€‚é…çš„é«˜æ•ˆåŒ»å­¦è§†è§‰-è¯­è¨€å¯¹é½",
      "authors": [
        "Chenyu Lian",
        "Hong-Yu Zhou",
        "Dongyun Liang",
        "Jing Qin",
        "Liansheng Wang"
      ],
      "abstract": "Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) methods suffer from suboptimal visual representation capabilities, which also limits their effectiveness in vision-language alignment. In contrast, although the models pretrained via multimodal masked modeling struggle with direct cross-modal matching, they excel in visual representation. To address this contradiction, we propose ALTA (ALign Through Adapting), an efficient medical vision-language alignment method that utilizes only about 8% of the trainable parameters and less than 1/5 of the computational consumption required for masked record modeling. ALTA achieves superior performance in vision-language matching tasks like retrieval and zero-shot classification by adapting the pretrained vision model from masked record modeling. Additionally, we integrate temporal-multiview radiograph inputs to enhance the information consistency between radiographs and their corresponding descriptions in reports, further improving the vision-language alignment. Experimental evaluations show that ALTA outperforms the best-performing counterpart by over 4% absolute points in text-to-image accuracy and approximately 6% absolute points in image-to-text retrieval accuracy. The adaptation of vision-language models during efficient alignment also promotes better vision and language understanding. Code is publicly available at https://github.com/DopamineLcy/ALTA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ALTA (ALign Through Adapting)ï¼Œä¸€ç§é«˜æ•ˆçš„åŒ»å­¦è§†è§‰è¯­è¨€å¯¹é½æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿçš„CLIP-basedå¯¹æ¯”å­¦ä¹ åœ¨è§†è§‰è¡¨å¾èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚é’ˆå¯¹æ©ç è§†è§‰æ¨¡å‹(masked vision models)è™½ç„¶å…·å¤‡å“è¶Šè§†è§‰è¡¨å¾ä½†éš¾ä»¥ç›´æ¥è¿›è¡Œè·¨æ¨¡æ€åŒ¹é…çš„é—®é¢˜ï¼ŒALTAé€šè¿‡ä»…åˆ©ç”¨çº¦8%çš„å¯è®­ç»ƒå‚æ•°å’Œä¸è¶³äº”åˆ†ä¹‹ä¸€çš„è®¡ç®—æˆæœ¬ï¼ŒæˆåŠŸå®ç°äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„é€‚é…ä¸å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ—¶é—´å¤šè§†è§’(temporal-multiview)æ”¾å°„å½±åƒè¾“å…¥ï¼Œä»¥å¢å¼ºå½±åƒä¸å…¶å¯¹åº”æŠ¥å‘Šæè¿°ä¹‹é—´çš„ä¿¡æ¯ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALTAåœ¨æ–‡æœ¬åˆ°å›¾åƒå‡†ç¡®ç‡å’Œå›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢å‡†ç¡®ç‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€ä¼˜æ¨¡å‹æå‡äº†4%å’Œ6%ä»¥ä¸Šçš„ç»å¯¹ç‚¹æ•°ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡è§†è§‰è¯­è¨€åŒ¹é…æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹åŒ»å­¦å›¾åƒå’Œè¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "TMI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08990v1",
      "published_date": "2025-06-10 17:02:27 UTC",
      "updated_date": "2025-06-10 17:02:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:53.126072+00:00"
    },
    {
      "arxiv_id": "2506.08978v1",
      "title": "Propositional Logic for Probing Generalization in Neural Networks",
      "title_zh": "ç”¨äºæ¢ç©¶ç¥ç»ç½‘ç»œæ³›åŒ–èƒ½åŠ›çš„å‘½é¢˜é€»è¾‘",
      "authors": [
        "Anna Langedijk",
        "Jaap Jumelet",
        "Willem Zuidema"
      ],
      "abstract": "The extent to which neural networks are able to acquire and represent symbolic rules remains a key topic of research and debate. Much current work focuses on the impressive capabilities of large language models, as well as their often ill-understood failures on a wide range of reasoning tasks. In this paper, in contrast, we investigate the generalization behavior of three key neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a controlled task rooted in propositional logic. The task requires models to generate satisfying assignments for logical formulas, making it a structured and interpretable setting for studying compositionality. We introduce a balanced extension of an existing dataset to eliminate superficial patterns and enable testing on unseen operator combinations. Using this dataset, we evaluate the ability of the three architectures to generalize beyond the training distribution. While all models perform well in-distribution, we find that generalization to unseen patterns, particularly those involving negation, remains a significant challenge. Transformers fail to apply negation compositionally, unless structural biases are introduced. Our findings highlight persistent limitations in the ability of standard architectures to learn systematic representations of logical operators, suggesting the need for stronger inductive biases to support robust rule-based reasoning.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œè·å–å’Œè¡¨ç¤ºç¬¦å·è§„åˆ™çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨ Transformerã€å›¾å·ç§¯ç½‘ç»œ (GCN) å’Œ LSTM åœ¨å‘½é¢˜é€»è¾‘ (Propositional Logic) ä»»åŠ¡ä¸­çš„æ³›åŒ–è¡Œä¸ºã€‚ç ”ç©¶è¦æ±‚æ¨¡å‹ä¸ºé€»è¾‘å…¬å¼ç”Ÿæˆæ»¡è¶³èµ‹å€¼ (Satisfying Assignments)ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå¹³è¡¡çš„æ•°æ®é›†æ‰©å±•ï¼Œä»¥æ¶ˆé™¤è¡¨å±‚æ¨¡å¼å¹¶æµ‹è¯•æ¨¡å‹å¯¹æœªè§è¿‡çš„ç®—å­ç»„åˆçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ‰€æœ‰æ¨¡å‹åœ¨åˆ†å¸ƒå†… (In-distribution) è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†æœªè§è¿‡çš„æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¦å®š (Negation) çš„ç»„åˆæ—¶ï¼Œé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç° Transformer åœ¨æ²¡æœ‰å¼•å…¥ç»“æ„åå·® (Structural Biases) çš„æƒ…å†µä¸‹ï¼Œæ— æ³•ä»¥ç»„åˆæ–¹å¼ (Compositionally) åº”ç”¨å¦å®šè§„åˆ™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ ‡å‡†ç¥ç»ç½‘ç»œæ¶æ„åœ¨å­¦ä¹ é€»è¾‘ç®—å­çš„ç³»ç»ŸåŒ–è¡¨ç¤ºæ–¹é¢å­˜åœ¨çš„æŒä¹…å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼•å…¥æ›´å¼ºçš„å½’çº³åå·® (Inductive Biases) ä»¥æ”¯æŒé²æ£’è§„åˆ™æ¨ç†çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08978v1",
      "published_date": "2025-06-10 16:46:05 UTC",
      "updated_date": "2025-06-10 16:46:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:00.484502+00:00"
    },
    {
      "arxiv_id": "2506.08977v1",
      "title": "Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data",
      "title_zh": "é¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„å®šåˆ¶åŒ–æ¶æ„ï¼šåŸºäºé«˜æ–¯è¿‡ç¨‹ç”Ÿæˆæ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Victoria Hankemeier",
        "Malte Schilling"
      ],
      "abstract": "Developments in Deep Learning have significantly improved time series forecasting by enabling more accurate modeling of complex temporal dependencies inherent in sequential data. The effectiveness of such models is often demonstrated on limited sets of specific real-world data. Although this allows for comparative analysis, it still does not demonstrate how specific data characteristics align with the architectural strengths of individual models. Our research aims at uncovering clear connections between time series characteristics and particular models. We introduce a novel dataset generated using Gaussian Processes, specifically designed to display distinct, known characteristics for targeted evaluations of model adaptability to them. Furthermore, we present TimeFlex, a new model that incorporates a modular architecture tailored to handle diverse temporal dynamics, including trends and periodic patterns. This model is compared to current state-of-the-art models, offering a deeper understanding of how models perform under varied time series conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨æ­ç¤ºæ•°æ®ç‰¹å¾ä¸ç‰¹å®šæ¨¡å‹æ¶æ„ä¼˜åŠ¿ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ç ”ç©¶äººå‘˜å¼•å…¥äº†ä¸€ç§åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹(Gaussian Processes)ç”Ÿæˆçš„åˆ›æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å·²çŸ¥çš„æ˜¾è‘—ç‰¹å¾ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å‹å¯¹ä¸åŒæ—¶é—´åºåˆ—ç‰¹æ€§çš„é€‚åº”èƒ½åŠ›ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†åä¸ºTimeFlexçš„æ–°å‹æ¨¡å‹ï¼Œå…¶é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„(modular architecture)ï¼Œæ—¨åœ¨å¤„ç†åŒ…æ‹¬è¶‹åŠ¿å’Œå‘¨æœŸæ€§æ¨¡å¼åœ¨å†…çš„å¤šæ ·åŒ–æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡å°†TimeFlexä¸å½“å‰æœ€å…ˆè¿›çš„(state-of-the-art)æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶æ·±å…¥åˆ†æäº†å„æ¨¡å‹åœ¨ä¸åŒæ—¶é—´åºåˆ—æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥å·¥ä½œä¸ºç†è§£æ¨¡å‹æ¶æ„å¦‚ä½•ä¸ç‰¹å®šæ•°æ®ç‰¹å¾å¯¹é½æä¾›äº†é‡è¦è§è§£ï¼Œä¸ºå®šåˆ¶åŒ–æ—¶é—´åºåˆ—é¢„æµ‹æ¶æ„çš„è®¾è®¡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IJCNN25, Code: https://github.com/vicky-hnk/time-flex",
      "pdf_url": "https://arxiv.org/pdf/2506.08977v1",
      "published_date": "2025-06-10 16:46:02 UTC",
      "updated_date": "2025-06-10 16:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:13.101020+00:00"
    },
    {
      "arxiv_id": "2506.08970v1",
      "title": "A Survey of Link Prediction in N-ary Knowledge Graphs",
      "title_zh": "Nå…ƒçŸ¥è¯†å›¾è°±é“¾æ¥é¢„æµ‹ç»¼è¿°",
      "authors": [
        "Jiyao Wei",
        "Saiping Guan",
        "Da Li",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph designed to efficiently represent complex real-world facts. Unlike traditional knowledge graphs, where a fact typically involves two entities, NKGs can capture n-ary facts containing more than two entities. Link prediction in NKGs aims to predict missing elements within these n-ary facts, which is essential for completing NKGs and improving the performance of downstream applications. This task has recently gained significant attention. In this paper, we present the first comprehensive survey of link prediction in NKGs, providing an overview of the field, systematically categorizing existing methods, and analyzing their performance and application scenarios. We also outline promising directions for future research.",
      "tldr_zh": "è¯¥ç»¼è¿°æ˜¯å…³äº N-ary Knowledge Graphs (NKGs) é“¾æ¥é¢„æµ‹ (Link prediction) é¢†åŸŸçš„é¦–ä¸ªå…¨é¢è°ƒç ”ã€‚NKGs æ˜¯ä¸€ç±»ä¸“é—¨ç”¨äºé«˜æ•ˆè¡¨ç¤ºå¤æ‚ç°å®äº‹å®çš„çŸ¥è¯†å›¾è°±ï¼Œèƒ½å¤Ÿæ•æ‰åŒ…å«ä¸¤ä¸ªä»¥ä¸Šå®ä½“çš„å¤šå…ƒäº‹å® (n-ary facts)ã€‚è®ºæ–‡æ˜ç¡®äº† NKGs ä¸­é“¾æ¥é¢„æµ‹ä»»åŠ¡å®šä¹‰ï¼Œå³é¢„æµ‹å¤šå…ƒäº‹å®ä¸­ç¼ºå¤±çš„å…ƒç´ ï¼Œè¿™å¯¹äºè¡¥å…¨çŸ¥è¯†å›¾è°±åŠä¼˜åŒ–ä¸‹æ¸¸åº”ç”¨è‡³å…³é‡è¦ã€‚æ–‡ä¸­å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ï¼Œå¹¶æ·±å…¥åˆ†æäº†å®ƒä»¬çš„æ€§èƒ½è¡¨ç°å’Œå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚æœ€åï¼Œä½œè€…è¿˜æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥å€¼å¾—å…³æ³¨çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ç³»ç»Ÿæ€§çš„å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08970v1",
      "published_date": "2025-06-10 16:44:27 UTC",
      "updated_date": "2025-06-10 16:44:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:10.887333+00:00"
    },
    {
      "arxiv_id": "2506.08965v1",
      "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO",
      "title_zh": "GFRIENDï¼šåŸºäºé«˜æ•ˆ DPO çš„ç”Ÿæˆå¼å°‘æ ·æœ¬å¥–åŠ±æ¨æ–­",
      "authors": [
        "Yiyang Zhao",
        "Huiyu Bai",
        "Xuejiao Zhao"
      ],
      "abstract": "The ability to train high-performing reward models with few-shot data is critical for enhancing the efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF). We propose a data augmentation and expansion framework that enables generative reward models trained on small datasets to achieve comparable performance to those trained on large-scale datasets. Traditional methods to train a generative reward model, such as Direct Preference Optimization (DPO), are constrained by inefficiencies in sample pairing and limited data diversity. This work introduces preference refinement, which employs Chain-of-Thought (CoT) sampling to uncover diverse and high-quality preference relationships. It also incorporates a perplexity-based scoring mechanism to assign nuanced preference levels and utilizes Multi-level Direct Preference Optimization (M-DPO) to enable the model to capture finer-grained preference differences between samples. Experimental results demonstrate that the proposed method significantly enhances data efficiency and model performance, enabling reward models trained in a few-shot setting to achieve results on par with those trained on large-scale datasets. This study underscores the potential of data-efficient strategies in advancing reward model optimization, offering a robust solution for low-resource RLHF applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GFRIENDï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡æ•°æ®å¢å¼ºå’Œæ‰©å±•æ¡†æ¶ï¼Œä½¿åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„ Generative Reward Models è¾¾åˆ°ä¸å¤§è§„æ¨¡æ•°æ®é›†ç›¸å½“æ€§èƒ½çš„ç³»ç»Ÿã€‚é’ˆå¯¹ä¼ ç»Ÿ Direct Preference Optimization (DPO) åœ¨æ ·æœ¬é…å¯¹æ•ˆç‡å’Œæ•°æ®å¤šæ ·æ€§ä¸Šçš„ç“¶é¢ˆï¼ŒGFRIEND å¼•å…¥äº† Preference Refinement æœºåˆ¶ï¼Œé€šè¿‡ Chain-of-Thought (CoT) é‡‡æ ·æŒ–æ˜é«˜è´¨é‡çš„åå¥½å…³ç³»ã€‚ç ”ç©¶è¿˜é‡‡ç”¨äº†åŸºäº Perplexity çš„è¯„åˆ†æœºåˆ¶æ¥åˆ†é…ç»†å¾®çš„åå¥½ç­‰çº§ï¼Œå¹¶åˆ©ç”¨ Multi-level Direct Preference Optimization (M-DPO) æ•æ‰æ ·æœ¬é—´æ›´ç»†ç²’åº¦çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ•°æ®æ•ˆç‡å’Œæ¨¡å‹è¡¨ç°ï¼Œä½¿å¾—åœ¨ Few-shot è®¾ç½®ä¸‹è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹æ€§èƒ½ä¸å¤§è§„æ¨¡æ•°æ®è®­ç»ƒç»“æœæŒå¹³ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•°æ®é«˜æ•ˆç­–ç•¥åœ¨æ¨è¿›å¥–åŠ±æ¨¡å‹ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºä½èµ„æº Reinforcement Learning from Human Feedback (RLHF) åº”ç”¨æä¾›äº†é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08965v1",
      "published_date": "2025-06-10 16:37:13 UTC",
      "updated_date": "2025-06-10 16:37:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:18.625186+00:00"
    },
    {
      "arxiv_id": "2506.08963v1",
      "title": "Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics",
      "title_zh": "é’ˆå¯¹äº¤é€šäº¤å‰å£åŠ¨æ€çš„ç”Ÿæˆå¼è½¦è¾†è½¨è¿¹æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Yash Ranjan",
        "Rahul Sengupta",
        "Anand Rangarajan",
        "Sanjay Ranka"
      ],
      "abstract": "Traffic Intersections are vital to urban road networks as they regulate the movement of people and goods. However, they are regions of conflicting trajectories and are prone to accidents. Deep Generative models of traffic dynamics at signalized intersections can greatly help traffic authorities better understand the efficiency and safety aspects. At present, models are evaluated on computational metrics that primarily look at trajectory reconstruction errors. They are not evaluated online in a `live' microsimulation scenario. Further, these metrics do not adequately consider traffic engineering-specific concerns such as red-light violations, unallowed stoppage, etc. In this work, we provide a comprehensive analytics tool to train, run, and evaluate models with metrics that give better insights into model performance from a traffic engineering point of view. We train a state-of-the-art multi-vehicle trajectory forecasting model on a large dataset collected by running a calibrated scenario of a real-world urban intersection. We then evaluate the performance of the prediction models, online in a microsimulator, under unseen traffic conditions. We show that despite using ideally-behaved trajectories as input, and achieving low trajectory reconstruction errors, the generated trajectories show behaviors that break traffic rules. We introduce new metrics to evaluate such undesired behaviors and present our results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚ä¿¡å·äº¤å‰å£çš„äº¤é€šåŠ¨æ€ç”Ÿæˆæ¨¡å‹è¯„ä¼°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰ä¸»æµçš„è½¨è¿¹é‡å»ºè¯¯å·®(trajectory reconstruction errors)ç­‰è®¡ç®—æŒ‡æ ‡æ— æ³•å……åˆ†è¡¡é‡çº¢ç¯è¿ç« (red-light violations)æˆ–è¿è§„åœè½¦ç­‰äº¤é€šå·¥ç¨‹æ ¸å¿ƒå…³æ³¨ç‚¹ã€‚ä½œè€…å¼€å‘äº†ä¸€å¥—ç»¼åˆåˆ†æå·¥å…·ï¼Œç”¨äºåœ¨äº¤é€šå·¥ç¨‹è§†è§’ä¸‹è®­ç»ƒã€è¿è¡Œå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨çœŸå®åŸå¸‚äº¤å‰å£çš„å¤§å‹æ•°æ®é›†è®­ç»ƒäº†å…ˆè¿›çš„å¤šè½¦è½¨è¿¹é¢„æµ‹æ¨¡å‹(multi-vehicle trajectory forecasting model)ã€‚ç ”ç©¶é€šè¿‡åœ¨å¾®è§‚æ¨¡æ‹Ÿå™¨(microsimulator)ä¸­è¿›è¡Œåœ¨çº¿è¯„ä¼°å‘ç°ï¼Œå³ä¾¿æ¨¡å‹å®ç°äº†è¾ƒä½çš„è½¨è¿¹é‡å»ºè¯¯å·®ï¼Œå…¶ç”Ÿæˆçš„è½¨è¿¹åœ¨æœªçŸ¥äº¤é€šæ¡ä»¶ä¸‹ä»ä¼šè¡¨ç°å‡ºè¿åäº¤é€šè§„åˆ™çš„è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸“é—¨ç”¨äºè¯„ä¼°æ­¤ç±»ä¸å½“è¡Œä¸ºçš„æ–°æŒ‡æ ‡ï¼Œä¸ºæ›´å…¨é¢åœ°ç†è§£äº¤é€šç”Ÿæˆæ¨¡å‹çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08963v1",
      "published_date": "2025-06-10 16:36:42 UTC",
      "updated_date": "2025-06-10 16:36:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:35:23.641129+00:00"
    },
    {
      "arxiv_id": "2506.08962v1",
      "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis",
      "title_zh": "WIPï¼šé¢å‘æœ¬ç§‘ã€Šç”µè·¯åˆ†æã€‹çš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå‹æ™ºèƒ½å¯¼å¸ˆç³»ç»Ÿ",
      "authors": [
        "Liangliang Chen",
        "Huiru Xie",
        "Jacqueline Rohde",
        "Ying Zhang"
      ],
      "abstract": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled smart tutor designed to provide homework assessment and feedback for students in an undergraduate circuit analysis course. We detail the tutor's design philosophy and core components, including open-ended question answering and homework feedback generation. The prompts are carefully crafted to optimize responses across different problems. The smart tutor was deployed on the Microsoft Azure platform and is currently in use in an undergraduate circuit analysis course at the School of Electrical and Computer Engineering in a large, public, research-intensive institution in the Southeastern United States. Beyond offering personalized instruction and feedback, the tutor collects student interaction data, which is summarized and shared with the course instructor. To evaluate its effectiveness, we collected student feedback, with 90.9% of responses indicating satisfaction with the tutor. Additionally, we analyze a subset of collected data on preliminary circuit analysis topics to assess tutor usage frequency for each problem and identify frequently asked questions. These insights help instructors gain real-time awareness of student difficulties, enabling more targeted classroom instruction. In future work, we will release a full analysis once the complete dataset is available after the Spring 2025 semester. We also explore the potential applications of this smart tutor across a broader range of engineering disciplines by developing improved prompts, diagram-recognition methods, and database management strategies, which remain ongoing areas of research.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Model) å¢å¼ºçš„ AI æ™ºèƒ½åŠ©æ•™ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºæœ¬ç§‘ç”µè·¯åˆ†æ (Circuit Analysis) è¯¾ç¨‹æä¾›è‡ªåŠ¨åŒ–çš„ä½œä¸šè¯„ä¼°ä¸åé¦ˆã€‚è¯¥ç³»ç»ŸåŸºäº Microsoft Azure å¹³å°æ„å»ºï¼Œå…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬å¼€æ”¾å¼é—®ç­”å’Œä½œä¸šåé¦ˆç”ŸæˆåŠŸèƒ½ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æç¤ºè¯ (Prompts) æ¥æå‡å¯¹å„ç±»ç”µè·¯é—®é¢˜çš„å“åº”ç²¾åº¦ã€‚ç›®å‰è¯¥åŠ©æ•™å·²åœ¨å®é™…æ•™å­¦ä¸­æŠ•å…¥ä½¿ç”¨ï¼Œåˆæ­¥åé¦ˆæ˜¾ç¤ºå­¦ç”Ÿæ»¡æ„åº¦é«˜è¾¾ 90.9%ï¼Œä¸”èƒ½å¸®åŠ©æ•™å¸ˆé€šè¿‡æ•°æ®æ±‡æ€»å®æ—¶æŒæ¡å­¦ç”Ÿçš„å­¦ä¹ éš¾ç‚¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜é€šè¿‡æ•°æ®åˆ†æè¯†åˆ«äº†å­¦ç”Ÿçš„é«˜é¢‘é—®é¢˜ï¼Œä»è€Œæ”¯æŒæ›´å…·é’ˆå¯¹æ€§çš„è¯¾å ‚æŒ‡å¯¼ã€‚æœªæ¥ï¼Œè¯¥é¡¹ç›®å°†è¿›ä¸€æ­¥æ¢ç´¢ç”µè·¯å›¾è¯†åˆ« (Diagram-recognition) æŠ€æœ¯ï¼Œå¹¶å°è¯•å°†è¯¥æ™ºèƒ½åŠ©æ•™æ¨¡å¼æ¨å¹¿è‡³æ›´å¹¿æ³›çš„å·¥ç¨‹å­¦ç§‘é¢†åŸŸã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.08962v1",
      "published_date": "2025-06-10 16:35:45 UTC",
      "updated_date": "2025-06-10 16:35:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:34:33.039741+00:00"
    },
    {
      "arxiv_id": "2506.08961v1",
      "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation",
      "title_zh": "è¿ˆå‘æŠ—ç¯å¢ƒçŠ¶æ€æ‰°åŠ¨çš„é²æ£’æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Chenxu Wang",
        "Huaping Liu"
      ],
      "abstract": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have been widely studied in various threat models; however, few consider environmental state perturbations, which are natural in embodied scenarios. To improve the robustness of DRL agents, we formulate the problem of environmental state perturbation, introducing a preliminary non-targeted attack method as a calibration adversary, and then propose a defense framework, named Boosted Adversarial Training (BAT), which first tunes the agents via supervised learning to avoid catastrophic failure and subsequently adversarially trains the agent with reinforcement learning. Extensive experimental results substantiate the vulnerability of mainstream agents under environmental state perturbations and the effectiveness of our proposed attack. The defense results demonstrate that while existing robust reinforcement learning algorithms may not be suitable, our BAT framework can significantly enhance the robustness of agents against environmental state perturbations across various situations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½åœºæ™¯ä¸­å¸¸è§çš„ç¯å¢ƒçŠ¶æ€æ‰°åŠ¨(Environmental State Perturbation)é—®é¢˜ï¼Œæ¢è®¨äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)çš„é²æ£’æ€§ã€‚ç ”ç©¶è€…é¦–å…ˆå®šä¹‰äº†ç¯å¢ƒçŠ¶æ€æ‰°åŠ¨é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§éé’ˆå¯¹æ€§æ”»å‡»æ–¹æ³•ä½œä¸ºæ ¡å‡†å¯¹æŠ—æ‰‹æ®µï¼Œä»¥æ­ç¤ºä¸»æµæ™ºèƒ½ä½“çš„è„†å¼±æ€§ã€‚ä¸ºäº†æå‡é˜²å¾¡èƒ½åŠ›ï¼Œè®ºæ–‡æå‡ºäº†åä¸ºBoosted Adversarial Training (BAT)çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç›‘ç£å­¦ä¹ (Supervised Learning)é¢„å…ˆè°ƒæ•´æ™ºèƒ½ä½“ä»¥é¿å…ç¾éš¾æ€§å¤±è´¥ï¼Œéšåå†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„é²æ£’å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨åº”å¯¹æ­¤ç±»æ‰°åŠ¨æ—¶æ•ˆæœæœ‰é™ï¼Œè€ŒBATæ¡†æ¶èƒ½æ˜¾è‘—å¢å¼ºæ™ºèƒ½ä½“åœ¨å¤šç§æƒ…å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08961v1",
      "published_date": "2025-06-10 16:32:31 UTC",
      "updated_date": "2025-06-10 16:32:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:35:46.490558+00:00"
    },
    {
      "arxiv_id": "2506.09105v2",
      "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning",
      "title_zh": "MetaTTï¼šé¢å‘å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å…¨å±€å¼ é‡åˆ—é€‚é…å™¨",
      "authors": [
        "Javier Lopez-Piqueres",
        "Pranav Deshpande",
        "Archan Ray",
        "Mattia J. Villani",
        "Marco Pistoia",
        "Niraj Kumar"
      ],
      "abstract": "We present MetaTT, a Tensor Train (TT) adapter framework for fine-tuning of pre-trained transformers. MetaTT enables flexible and parameter-efficient model adaptation by using a single shared TT to factorize transformer sub-modules. This factorization indexes key structural dimensions, including layer and matrix type, and can optionally incorporate heads and tasks. This design allows MetaTT's parameter count to scale with the sum, rather than the product, of the modes, resulting in a substantially more compact adapter. Our benchmarks compare MetaTT with LoRA along with recent state-of-the-art matrix and tensor decomposition based fine-tuning methods. We observe that when tested on single-task standard language modeling benchmarks, MetaTT achieves competitive parameter efficiency to accuracy tradeoff. We further demonstrate that MetaTT performs competitively when compared to state-of-the-art methods on multi-task learning. Finally, we leverage the TT-ansatz to design a rank adaptive optimizer inspired by the DMRG method from many-body physics. Our results demonstrate that integrating this approach with AdamW enhances optimization performance for a specified target rank.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaTTï¼Œä¸€ç§ç”¨äºé¢„è®­ç»ƒ Transformer æ¨¡å‹å¾®è°ƒçš„ Tensor Train (TT) é€‚é…å™¨æ¡†æ¶ã€‚MetaTT é€šè¿‡ä½¿ç”¨å•ä¸ªå…±äº«çš„ TT æ¥åˆ†è§£ Transformer çš„å­æ¨¡å—ï¼Œå¹¶ç´¢å¼•å±‚ã€çŸ©é˜µç±»å‹ã€æ³¨æ„åŠ›å¤´åŠä»»åŠ¡ç­‰ç»“æ„ç»´åº¦ï¼Œä½¿å¾—å‚æ•°é‡éšç»´åº¦çš„æ€»å’Œè€Œéä¹˜ç§¯ç¼©æ”¾ï¼Œå®ç°äº†æé«˜çš„å‚æ•°æ•ˆç‡ã€‚åœ¨å•ä»»åŠ¡è¯­è¨€å»ºæ¨¡å’Œå¤šä»»åŠ¡å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaTT çš„æ€§èƒ½ä¼˜äºæˆ–ç­‰åŒäº LoRA ç­‰å…ˆè¿›çš„çŸ©é˜µæˆ–å¼ é‡åˆ†è§£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…å—å¤šä½“ç‰©ç†ä¸­ DMRG æ–¹æ³•çš„å¯å‘ï¼Œåˆ©ç”¨ TT-ansatz è®¾è®¡äº†ä¸€ç§ç§©è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†è¯¥ä¼˜åŒ–æ–¹æ³•ä¸ AdamW ç»“åˆï¼Œèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨æŒ‡å®š target rank ä¸‹çš„ä¼˜åŒ–æ€§èƒ½ï¼Œä¸ºå¤§æ¨¡å‹çš„é«˜æ•ˆé€‚é…æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09105v2",
      "published_date": "2025-06-10 16:32:05 UTC",
      "updated_date": "2025-11-14 22:31:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:21.143887+00:00"
    },
    {
      "arxiv_id": "2506.08957v1",
      "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections",
      "title_zh": "IntTrajSimï¼šé¢å‘ä¿¡å·åŒ–äº¤å‰è·¯å£å¤šè½¦é©¾é©¶ä»¿çœŸçš„è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Yash Ranjan",
        "Rahul Sengupta",
        "Anand Rangarajan",
        "Sanjay Ranka"
      ],
      "abstract": "Traffic simulators are widely used to study the operational efficiency of road infrastructure, but their rule-based approach limits their ability to mimic real-world driving behavior. Traffic intersections are critical components of the road infrastructure, both in terms of safety risk (nearly 28% of fatal crashes and 58% of nonfatal crashes happen at intersections) as well as the operational efficiency of a road corridor. This raises an important question: can we create a data-driven simulator that can mimic the macro- and micro-statistics of the driving behavior at a traffic intersection? Deep Generative Modeling-based trajectory prediction models provide a good starting point to model the complex dynamics of vehicles at an intersection. But they are not tested in a \"live\" micro-simulation scenario and are not evaluated on traffic engineering-related metrics. In this study, we propose traffic engineering-related metrics to evaluate generative trajectory prediction models and provide a simulation-in-the-loop pipeline to do so. We also provide a multi-headed self-attention-based trajectory prediction model that incorporates the signal information, which outperforms our previous models on the evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿäº¤é€šæ¨¡æ‹Ÿå™¨åŸºäºè§„åˆ™çš„æ–¹æ³•éš¾ä»¥å‡†ç¡®æ¨¡æ‹ŸçœŸå®é©¾é©¶è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº† IntTrajSim æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå—ä¿¡å·æ§åˆ¶äº¤å‰è·¯å£çš„å¤šè½¦é©¾é©¶åŠ¨æ€ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†åŸºäºå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (multi-headed self-attention) çš„è½¨è¿¹é¢„æµ‹æ¨¡å‹ï¼Œå¹¶åˆ›æ–°æ€§åœ°æ•´åˆäº†äº¤é€šä¿¡å·ç¯ä¿¡æ¯ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰ç”Ÿæˆå¼æ¨¡å‹åœ¨å¾®è§‚æ¨¡æ‹Ÿåœºæ™¯ä¸­ç¼ºä¹è¯„ä¼°çš„ä¸è¶³ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—ä¸“é—¨çš„äº¤é€šå·¥ç¨‹è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªä»¿çœŸåœ¨ç¯ (simulation-in-the-loop) çš„è¯„ä¼°ç®¡çº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…ˆå‰çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´çœŸå®åœ°è¿˜åŸäº¤å‰è·¯å£çš„å®è§‚ä¸å¾®è§‚é©¾é©¶è¡Œä¸ºç»Ÿè®¡ç‰¹å¾ï¼Œä¸ºå¼€å‘æ•°æ®é©±åŠ¨çš„é«˜ä¿çœŸäº¤é€šæ¨¡æ‹Ÿå™¨æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08957v1",
      "published_date": "2025-06-10 16:27:42 UTC",
      "updated_date": "2025-06-10 16:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:35:55.385804+00:00"
    },
    {
      "arxiv_id": "2506.09104v1",
      "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs",
      "title_zh": "ç»Ÿä¸€åˆ†å— PTQ ä¸è’¸é¦ QATï¼šé¢å‘ 2 æ¯”ç‰¹æŒ‡ä»¤å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„æ¸è¿›å¼é‡åŒ–",
      "authors": [
        "Jung Hyun Lee",
        "Seungjae Shin",
        "Vinnam Kim",
        "Jaeseong You",
        "An Chen"
      ],
      "abstract": "As the rapid scaling of large language models (LLMs) poses significant challenges for deployment on resource-constrained devices, there is growing interest in extremely low-bit quantization, such as 2-bit. Although prior works have shown that 2-bit large models are pareto-optimal over their 4-bit smaller counterparts in both accuracy and latency, these advancements have been limited to pre-trained LLMs and have not yet been extended to instruction-tuned models. To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel progressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2) that unifies block-wise post-training quantization (PTQ) with distillation-based quantization-aware training (Distill-QAT) for INT2 instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned models to INT4 using block-wise PTQ to significantly reduce the quantization error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT to enable INT2 instruction-tuned LLMs to generate responses consistent with their original FP16 counterparts by minimizing the generalized Jensen-Shannon divergence (JSD) between the two. To the best of our knowledge, we are the first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs to INT2 without relying on proprietary post-training data, while achieving state-of-the-art performances on MMLU and IFEval$-$two of the most representative benchmarks for evaluating instruction-tuned LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Unified Progressive Quantization (UPQ)ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ¸è¿›å¼é‡åŒ–æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹æŒ‡ä»¤å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(Instruction-Tuned LLMs)å®ç°æä½æ¯”ç‰¹(2-bit)é‡åŒ–ã€‚UPQé‡‡ç”¨äº†ä»FP16åˆ°INT4å†åˆ°INT2çš„æ¸è¿›è·¯å¾„ï¼ŒæˆåŠŸç»Ÿä¸€äº†å—çº§åè®­ç»ƒé‡åŒ–(Block-wise PTQ)ä¸åŸºäºè’¸é¦çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(Distill-QAT)ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡Block-wise PTQå°†æ¨¡å‹é‡åŒ–ä¸ºINT4ä»¥é™ä½ç´¯ç§¯è¯¯å·®ï¼Œéšååˆ©ç”¨Distill-QATå¹¶é€šè¿‡æœ€å°åŒ–å¹¿ä¹‰Jensen-Shannonæ•£åº¦(JSD)ç¡®ä¿INT2æ¨¡å‹ä¸åŸå§‹æ¨¡å‹å“åº”çš„ä¸€è‡´æ€§ã€‚è¯¥å·¥ä½œé¦–æ¬¡å®ç°äº†åœ¨ä¸ä¾èµ–ç§æœ‰æ•°æ®çš„å‰æä¸‹å°†å¼€æºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹å‹ç¼©è‡³INT2ï¼Œå¹¶åœ¨MMLUå’ŒIFEvalåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„(SOTA)æ€§èƒ½ã€‚è¿™ä¸€æˆæœæœ‰æ•ˆç¼“è§£äº†å¤§å‹æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.09104v1",
      "published_date": "2025-06-10 16:26:32 UTC",
      "updated_date": "2025-06-10 16:26:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:35:53.094020+00:00"
    },
    {
      "arxiv_id": "2506.08955v2",
      "title": "Segment Concealed Objects with Incomplete Supervision",
      "title_zh": "ä¸å®Œå…¨ç›‘ç£ä¸‹çš„éšè—ç›®æ ‡åˆ†å‰²",
      "authors": [
        "Chunming He",
        "Kai Li",
        "Yachao Zhang",
        "Ziyun Yang",
        "Youwei Pang",
        "Longxiang Tang",
        "Chengyu Fang",
        "Yulun Zhang",
        "Linghe Kong",
        "Xiu Li",
        "Sina Farsiu"
      ],
      "abstract": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves segmenting objects that seamlessly blend into their surrounding environments, utilizing incompletely annotated data, such as weak and semi-annotations, for model training. This task remains highly challenging due to (1) the limited supervision provided by the incompletely annotated training data, and (2) the difficulty of distinguishing concealed objects from the background, which arises from the intrinsic similarities in concealed scenarios. In this paper, we introduce the first unified method for ISCOS to address these challenges. To tackle the issue of incomplete supervision, we propose a unified mean-teacher framework, SEE, that leverages the vision foundation model, ``\\emph{Segment Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced by the teacher model as prompts. To mitigate the effect of low-quality segmentation masks, we introduce a series of strategies for pseudo-label generation, storage, and supervision. These strategies aim to produce informative pseudo-labels, store the best pseudo-labels generated, and select the most reliable components to guide the student model, thereby ensuring robust network training. Additionally, to tackle the issue of intrinsic similarity, we design a hybrid-granularity feature grouping module that groups features at different granularities and aggregates these results. By clustering similar features, this module promotes segmentation coherence, facilitating more complete segmentation for both single-object and multiple-object images. We validate the effectiveness of our approach across multiple ISCOS tasks, and experimental results demonstrate that our method achieves state-of-the-art performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing the performance of existing models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸å®Œå…¨ç›‘ç£ä¸‹çš„éšè—ç‰©ä½“åˆ†å‰²ï¼ˆISCOSï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨å¼±æ ‡æ³¨æˆ–åŠæ ‡æ³¨æ•°æ®è§£å†³éšè—ç‰©ä½“ä¸èƒŒæ™¯æåº¦ç›¸ä¼¼ä»¥åŠç›‘ç£ä¿¡æ¯ä¸è¶³çš„éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„æ–¹æ³•æ¡†æ¶SEEï¼Œè¯¥æ¡†æ¶é‡‡ç”¨Mean-Teacherç»“æ„å¹¶å¼•å…¥Segment Anything Model (SAM)ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„ç²—ç•¥æ©ç ä½œä¸ºæç¤ºè¯æ¥è¾…åŠ©ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚ä¸ºäº†åº”å¯¹ä½è´¨é‡æ©ç çš„å¹²æ‰°ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€å¥—åŒ…å«ç”Ÿæˆã€å­˜å‚¨å’Œç›‘ç£çš„ä¼ªæ ‡ç­¾ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡ç­›é€‰å¯é ç»„ä»¶æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ï¼Œç¡®ä¿äº†ç½‘ç»œè®­ç»ƒçš„ç¨³å¥æ€§ã€‚é’ˆå¯¹éšè—åœºæ™¯çš„å†…åœ¨ç›¸ä¼¼æ€§ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†æ··åˆç²’åº¦ç‰¹å¾åˆ†ç»„æ¨¡å—ï¼ˆhybrid-granularity feature grouping moduleï¼‰ï¼Œé€šè¿‡èšåˆä¸åŒç²’åº¦çš„èšç±»ç»“æœæ¥å¢å¼ºåˆ†å‰²çš„è¿è´¯æ€§ã€‚å¤šé¡¹å®éªŒç»“æœè¯æ˜ï¼ŒSEEåœ¨å¤šç±»ISCOSä»»åŠ¡ä¸­å‡å–å¾—äº†State-of-the-artæ€§èƒ½ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æœ‰æ•ˆå¢å¼ºç°æœ‰æ¨¡å‹çš„è¡¨ç°ï¼Œå…·æœ‰è¾ƒå¼ºçš„é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE TPAMI",
      "pdf_url": "https://arxiv.org/pdf/2506.08955v2",
      "published_date": "2025-06-10 16:25:15 UTC",
      "updated_date": "2025-06-14 00:52:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:31.991457+00:00"
    },
    {
      "arxiv_id": "2506.08952v2",
      "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ï¼ˆä¸ï¼‰çŸ¥æƒ…ä¸‹çš„å¯¹è¯å…±è¯†å»ºç«‹ï¼šé’ˆå¯¹ç›´æ¥ä¸è¯±å¯¼æ€§æ”¿æ²»é—®é¢˜çš„ç ”ç©¶",
      "authors": [
        "Clara Lachenmaier",
        "Judith Sieker",
        "Sina ZarrieÃŸ"
      ],
      "abstract": "Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ”¿æ²»é¢†åŸŸè¿›è¡Œå…±é€šè®¤çŸ¥(Conversational Grounding)çš„èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æäº†æ¨¡å‹åœ¨é¢å¯¹çŸ¥è¯†ç¼ºå¤±æˆ–å¤„ç†åŒ…å«è¯¯å¯¼æ€§ä¿¡æ¯çš„è¯±å¯¼æ€§é—®é¢˜(Loaded Questions)æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹æ¯”æ¨¡å‹å¯¹ç›´æ¥çŸ¥è¯†é—®é¢˜(Direct Knowledge Questions)ä¸é¢„è®¾è™šå‡ä¿¡æ¯é—®é¢˜çš„å›ç­”ï¼Œè¯„ä¼°å…¶æ˜¯å¦èƒ½ä¸»åŠ¨è¯†åˆ«å¹¶çº æ­£ç”¨æˆ·çš„é”™è¯¯ä¿¡å¿µã€‚å®éªŒè¿›ä¸€æ­¥è€ƒå¯Ÿäº†æ¨¡å‹çš„çŸ¥è¯†å‚¨å¤‡åŠå…¶æ”¿æ²»åè§(Political Bias)å¯¹è¿™ç§æ²Ÿé€šè¡Œä¸ºçš„å½±å“ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMsåœ¨å»ºç«‹æœ‰æ•ˆå…±é€šè®¤çŸ¥ä»¥åŠæ‹’ç»é”™è¯¯ç”¨æˆ·é¢„è®¾æ–¹é¢å­˜åœ¨ä¸¥é‡å±€é™ï¼Œéš¾ä»¥åœ¨æ”¿æ²»è®¨è®ºä¸­æœ‰æ•ˆéåˆ¶è™šå‡ä¿¡æ¯çš„æ‰©æ•£ã€‚è¿™ä¸€å‘ç°å¯¹åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç¼“è§£æ”¿æ²»è¯¯å¯¼é£é™©çš„å¯é æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint accepted at ACL Main Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08952v2",
      "published_date": "2025-06-10 16:20:09 UTC",
      "updated_date": "2025-06-11 06:58:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:09.797710+00:00"
    },
    {
      "arxiv_id": "2506.08935v1",
      "title": "Can A Gamer Train A Mathematical Reasoning Model?",
      "title_zh": "æ¸¸æˆç©å®¶èƒ½å¦è®­ç»ƒå‡ºæ•°å­¦æ¨ç†æ¨¡å‹ï¼Ÿ",
      "authors": [
        "Andrew Shin"
      ],
      "abstract": "While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨æ‰“ç ´å°–ç«¯æ•°å­¦æ¨ç†ç ”ç©¶å¯¹å¤§è§„æ¨¡ç¡¬ä»¶é›†ç¾¤çš„ä¾èµ–ã€‚é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å’Œå†…å­˜ä¼˜åŒ–(Memory Optimization)æŠ€æœ¯ï¼Œä½œè€…è¯æ˜äº†åˆ©ç”¨å•å¼ 16GBæ˜¾å­˜çš„RTX 3080 Tiæ¸¸æˆæ˜¾å¡å³å¯è®­ç»ƒå‡ºæ€§èƒ½ç¨³å¥çš„æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸€1.5Bå‚æ•°çš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¸è§„æ¨¡å¤§å…¶æ•°å€çš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚è¯¥æˆæœæœ‰åŠ›åœ°æŒ‘æˆ˜äº†é«˜æ€§èƒ½æ¨¡å‹å¼€å‘å¿…é¡»ä¾èµ–åºå¤§åŸºç¡€è®¾æ–½çš„èŒƒå¼ï¼Œä¸ºå­¦æœ¯ç ”ç©¶çš„æ™®åŠåŒ–(Democratization)æä¾›äº†æ–°é€”å¾„ã€‚ç ”ç©¶è€…é€šè¿‡æ­¤é¡¹å·¥ä½œè¯æ˜äº†å³ä½¿æ˜¯ä¸ªäººå¼€å‘è€…æˆ–å°è§„æ¨¡å®éªŒå®¤ï¼Œä¹Ÿèƒ½åœ¨æœ‰é™ç¡¬ä»¶èµ„æºä¸‹è¿›è¡Œå‰æ²¿çš„AIç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08935v1",
      "published_date": "2025-06-10 16:00:12 UTC",
      "updated_date": "2025-06-10 16:00:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:15.796672+00:00"
    },
    {
      "arxiv_id": "2506.08927v1",
      "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions",
      "title_zh": "Socratic-MCTSï¼šé€šè¿‡æå‡ºæ­£ç¡®çš„é—®é¢˜å®ç°æµ‹è¯•æ—¶è§†è§‰æ¨ç†",
      "authors": [
        "David Acuna",
        "Ximing Lu",
        "Jaehun Jung",
        "Hyunwoo Kim",
        "Amlan Kar",
        "Sanja Fidler",
        "Yejin Choi"
      ],
      "abstract": "Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning -- akin to the success observed in language models -- via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces -- without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer pairs into the model's output stream. We show that framing reasoning as a search process -- where subquestions act as latent decisions within a broader inference trajectory -- helps the model \"connect the dots\" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œä»ç°æœ‰çš„éæ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸­è¯±å¯¼é•¿é“¾æ¡æ¨ç†èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†Socratic-MCTSï¼Œè¿™æ˜¯ä¸€ç§å—è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search)å¯å‘çš„ç®—æ³•ï¼Œé€šè¿‡åœ¨æ¨¡å‹çš„è¾“å‡ºæµä¸­ä¸»åŠ¨æ³¨å…¥â€œå­é—®é¢˜-å­ç­”æ¡ˆâ€å¯¹æ¥å®ç°æµ‹è¯•æ—¶æ¨ç†ã€‚è¯¥æ¡†æ¶å°†æ¨ç†å»ºæ¨¡ä¸ºä¸€ä¸ªæœç´¢è¿‡ç¨‹ï¼Œå°†å­é—®é¢˜è§†ä¸ºæ¨ç†è½¨è¿¹ä¸­çš„æ½œåœ¨å†³ç­–ï¼Œä»è€Œæœ‰æ•ˆå¸®åŠ©æ¨¡å‹è¿æ¥ç¢ç‰‡åŒ–çŸ¥è¯†å¹¶ç”Ÿæˆæ‰©å±•çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„æå‡ï¼Œå…¶ä¸­åœ¨MMMU-PROä¸Šæ•´ä½“å‡†ç¡®ç‡æé«˜äº†2%ï¼Œåœ¨äººæ–‡ç§‘å­¦(Liberal Arts)é¢†åŸŸçš„æå‡æ›´æ˜¯é«˜è¾¾9%ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡ç‰¹å®šçš„æœç´¢æœºåˆ¶æ¿€å‘é¢„è®­ç»ƒæ¨¡å‹å†…åœ¨éšè—çŸ¥è¯†å¹¶è¯±å¯¼å¤æ‚æ¨ç†çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08927v1",
      "published_date": "2025-06-10 15:51:16 UTC",
      "updated_date": "2025-06-10 15:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:19.138128+00:00"
    },
    {
      "arxiv_id": "2506.09102v1",
      "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation",
      "title_zh": "å˜é©ä¸´åºŠè¯•éªŒï¼šäººå·¥æ™ºèƒ½é©±åŠ¨è½¬å‹çš„å®£è¨€",
      "authors": [
        "Mihaela van der Schaar",
        "Richard Peck",
        "Eoin McKinney",
        "Jim Weatherall",
        "Stuart Bailey",
        "Justine Rochon",
        "Chris Anagnostopoulos",
        "Pierre Marquet",
        "Anthony Wood",
        "Nicky Best",
        "Harry Amad",
        "Julianna Piskorz",
        "Krzysztof Kacprzyk",
        "Rafik Salama",
        "Christina Gunther",
        "Francesca Frau",
        "Antoine Pugeat",
        "Ramon Hernandez"
      ],
      "abstract": "This manifesto represents a collaborative vision forged by leaders in pharmaceuticals, consulting firms, clinical research, and AI. It outlines a roadmap for two AI technologies - causal inference and digital twins - to transform clinical trials, delivering faster, safer, and more personalized outcomes for patients. By focusing on actionable integration within existing regulatory frameworks, we propose a way forward to revolutionize clinical research and redefine the gold standard for clinical trials using AI.",
      "tldr_zh": "è¯¥å®£è¨€ç”±åˆ¶è¯ã€ä¸´åºŠç ”ç©¶å’Œ AI é¢†åŸŸçš„è¡Œä¸šä¸“å®¶å…±åŒåˆ¶å®šï¼Œæ—¨åœ¨ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„ä¸´åºŠè¯•éªŒè½¬å‹æä¾›ä¸€ä»½è¡ŒåŠ¨è“å›¾ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†å› æœæ¨æ–­ (causal inference) å’Œæ•°å­—å­ªç”Ÿ (digital twins) ä¸¤é¡¹å…³é”®æŠ€æœ¯åœ¨å˜é©ä¸´åºŠè¯•éªŒä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨ä¸ºæ‚£è€…å¸¦æ¥æ›´å¿«é€Ÿã€æ›´å®‰å…¨ä¸”æ›´å…·ä¸ªæ€§åŒ–çš„åŒ»ç–—æˆæœã€‚é€šè¿‡åœ¨ç°æœ‰ç›‘ç®¡æ¡†æ¶ (regulatory frameworks) å†…å®ç°å¯æ“ä½œçš„æŠ€æœ¯é›†æˆï¼Œè¯¥è·¯çº¿å›¾ä¸ºé©æ–°ä¸´åºŠç ”ç©¶è·¯å¾„æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚è¿™é¡¹å·¥ä½œè‡´åŠ›äºåˆ©ç”¨ AI æŠ€æœ¯é‡æ–°å®šä¹‰ä¸´åºŠè¯•éªŒçš„é‡‘æ ‡å‡† (gold standard)ï¼Œä»è€Œæ¨åŠ¨æ•´ä¸ªä¸´åºŠç§‘ç ”é¢†åŸŸçš„ç³»ç»Ÿæ€§è½¬å‹ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09102v1",
      "published_date": "2025-06-10 15:45:19 UTC",
      "updated_date": "2025-06-10 15:45:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:29.734876+00:00"
    },
    {
      "arxiv_id": "2506.08920v1",
      "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs",
      "title_zh": "PropMENDï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ä¼ æ’­çš„è¶…ç½‘ç»œ",
      "authors": [
        "Zeyu Leo Liu",
        "Greg Durrett",
        "Eunsol Choi"
      ],
      "abstract": "Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯åœ¨çŸ¥è¯†ä¼ æ’­(Knowledge Propagation)æ–¹é¢çš„ä¸è¶³ï¼Œå³æ¨¡å‹éš¾ä»¥åˆ©ç”¨æ³¨å…¥çš„æ–°çŸ¥è¯†è¿›è¡Œæ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºè¶…ç½‘ç»œ(Hypernetwork)çš„æ–¹æ³•PropMENDã€‚è¯¥æ–¹æ³•é€šè¿‡å…ƒå­¦ä¹ (Meta-learning)ä¿®æ”¹è¯­è¨€æ¨¡å‹æŸå¤±çš„æ¢¯åº¦ï¼Œå¹¶æ‰©å±•äº†MENDçš„å…ƒç›®æ ‡(Meta-objective)ï¼Œä½¿å¾—æ¢¯åº¦æ›´æ–°èƒ½ä½¿æ¨¡å‹å…·å¤‡å›ç­”æ¶‰åŠæ³¨å…¥çŸ¥è¯†çš„å¤šè·³(Multi-hop)é—®é¢˜çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPropMENDåœ¨RippleEditæ•°æ®é›†ä¸Šçš„å¤šè·³é—®é¢˜å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ³•æå‡äº†è¿‘ä¸¤å€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº†Controlled RippleEditæ•°æ®é›†ï¼Œä»¥æµ‹è¯•è¶…ç½‘ç»œåœ¨æœªè§è¿‡çš„å®ä½“å’Œå…³ç³»ä¸Šçš„æ³›åŒ–æ€§ã€‚è™½ç„¶PropMENDåœ¨å¤„ç†è¿™äº›æœªè§å¯¹æ—¶ä¾ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†æ€§èƒ½å·®è·çš„ç¼©å°ä¹Ÿæš—ç¤ºäº†æœªæ¥åœ¨å¹¿æ³›å…³ç³»é¢†åŸŸè¿›è¡ŒçŸ¥è¯†ä¼ æ’­çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.08920v1",
      "published_date": "2025-06-10 15:44:19 UTC",
      "updated_date": "2025-06-10 15:44:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:31.094106+00:00"
    },
    {
      "arxiv_id": "2506.08917v1",
      "title": "Quantum Adiabatic Generation of Human-Like Passwords",
      "title_zh": "ç±»äººå¯†ç çš„é‡å­ç»çƒ­ç”Ÿæˆ",
      "authors": [
        "Sascha MÃ¼cke",
        "Raoul Heese",
        "Thore Gerlach",
        "David Biesner",
        "Loong Kuan Lee",
        "Nico Piatkowski"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) for Natural Language Processing (NLP) is the predominant AI technology to date. An important perspective for Quantum Computing (QC) is the question whether QC has the potential to reduce the vast resource requirements for training and operating GenAI models. While large-scale generative NLP tasks are currently out of reach for practical quantum computers, the generation of short semantic structures such as passwords is not. Generating passwords that mimic real user behavior has many applications, for example to test an authentication system against realistic threat models. Classical password generation via deep learning have recently been investigated with significant progress in their ability to generate novel, realistic password candidates. In the present work we investigate the utility of adiabatic quantum computers for this task. More precisely, we study different encodings of token strings and propose novel approaches based on the Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum Independent Set (UD-MIS) problems. Our approach allows us to estimate the token distribution from data and adiabatically prepare a quantum state from which we eventually sample the generated passwords via measurements. Our results show that relatively small samples of 128 passwords, generated on the QuEra Aquila 256-qubit neutral atom quantum computer, contain human-like passwords such as \"Tunas200992\" or \"teedem28iglove\".",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨é‡å­è®¡ç®—(Quantum Computing)é™ä½ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)èµ„æºéœ€æ±‚çš„å¯èƒ½æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ç»çƒ­é‡å­è®¡ç®—æœºç”Ÿæˆç±»äººå¯†ç çš„æ–°æ–¹æ³•ã€‚å°½ç®¡å¤§è§„æ¨¡ç”Ÿæˆå¼è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç›®å‰ä»éš¾ä»¥åœ¨é‡å­ç¡¬ä»¶ä¸Šå®ç°ï¼Œä½†è¯¥ç ”ç©¶è¯æ˜äº†ç”ŸæˆçŸ­è¯­ä¹‰ç»“æ„ä»¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºçš„å¯è¡Œæ€§ã€‚ç ”ç©¶è€…æå‡ºäº†åŸºäºäºŒæ¬¡æ— çº¦æŸäºŒå…ƒä¼˜åŒ–(QUBO)å’Œå•ä½åœ†ç›˜æœ€å¤§ç‹¬ç«‹é›†(UD-MIS)é—®é¢˜çš„ç¼–ç æ–¹æ¡ˆï¼Œé€šè¿‡ä»æ•°æ®ä¸­ä¼°ç®—ä»¤ç‰Œ(token)åˆ†å¸ƒæ¥åˆ¶å¤‡é‡å­æ€ã€‚å®éªŒåœ¨QuEra Aquila 256é‡å­ä½ä¸­æ€§åŸå­é‡å­è®¡ç®—æœºä¸Šè¿›è¡Œï¼Œé€šè¿‡é‡å­æµ‹é‡æˆåŠŸé‡‡æ ·ç”Ÿæˆäº†å¯†ç å€™é€‰ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¦‚â€œTunas200992â€ç­‰å…·æœ‰é«˜åº¦çœŸå®æ„Ÿçš„ç±»äººå¯†ç ï¼Œä¸ºæµ‹è¯•èº«ä»½éªŒè¯ç³»ç»Ÿåº”å¯¹ç°å®å¨èƒæ¨¡å‹çš„èƒ½åŠ›æä¾›äº†æ”¯æŒã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†é‡å­è®¡ç®—åœ¨ç‰¹å®šç”Ÿæˆä»»åŠ¡ä¸­æ›¿ä»£ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æ½œåŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08917v1",
      "published_date": "2025-06-10 15:43:05 UTC",
      "updated_date": "2025-06-10 15:43:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:47.435115+00:00"
    },
    {
      "arxiv_id": "2506.08915v3",
      "title": "Inherently Faithful Attention Maps for Vision Transformers",
      "title_zh": "Vision Transformer çš„å†…ç”Ÿå¿ å®æ³¨æ„åŠ›å›¾",
      "authors": [
        "Ananthu Aniraj",
        "Cassio F. Dantas",
        "Dino Ienco",
        "Diego Marcos"
      ],
      "abstract": "We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds. Code: https://github.com/ananthu-aniraj/ifam",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Inherently Faithful Attention Maps (IFAM) çš„æ³¨æ„åŠ›æœºåˆ¶æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ äºŒè¿›åˆ¶æ³¨æ„åŠ›æ©ç  (binary attention masks) ç¡®ä¿ä»…å—å…³æ³¨çš„å›¾åƒåŒºåŸŸèƒ½å¤Ÿå½±å“æ¨¡å‹é¢„æµ‹ã€‚é’ˆå¯¹èƒŒæ™¯ä¿¡æ¯å¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿåè§æˆ–è™šå‡å…³è” (spurious correlations) çš„æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µè´Ÿè´£å¤„ç†å®Œæ•´å›¾åƒä»¥è¯†åˆ«ç‰©ä½“éƒ¨ä»¶åŠä»»åŠ¡ç›¸å…³çš„å…³é”®åŒºåŸŸï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨è¾“å…¥æ³¨æ„åŠ›æ©ç  (input attention masking) å°†æ„Ÿå—é‡é™åˆ¶åœ¨è¿™äº›ç‰¹å®šåŒºåŸŸå†…ï¼Œä»è€Œå®ç°èšç„¦åˆ†æå¹¶è¿‡æ»¤æ‰æ½œåœ¨çš„å¹²æ‰°ä¿¡æ¯ã€‚ä¸¤ä¸ªé˜¶æ®µé€šè¿‡è”åˆè®­ç»ƒ (jointly trained) ç›¸äº’åä½œï¼Œä½¿å¾—ç¬¬äºŒé˜¶æ®µèƒ½å¤Ÿè¿›ä¸€æ­¥ç»†åŒ–ç¬¬ä¸€é˜¶æ®µçš„åŒºåŸŸè¯†åˆ«ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹åˆ†å¸ƒå¼å¤–èƒŒæ™¯ (out-of-distribution backgrounds) çš„é²æ£’æ€§ï¼Œä¸ºæ„å»ºæ›´å…·è§£é‡Šæ€§å’Œå¯é æ€§çš„ Vision Transformers æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08915v3",
      "published_date": "2025-06-10 15:41:22 UTC",
      "updated_date": "2025-06-17 13:45:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:52.030267+00:00"
    },
    {
      "arxiv_id": "2506.08902v2",
      "title": "Intention-Conditioned Flow Occupancy Models",
      "title_zh": "æ„å›¾æ¡ä»¶åŒ–æµå ç”¨æ¨¡å‹",
      "authors": [
        "Chongyi Zheng",
        "Seohong Park",
        "Sergey Levine",
        "Benjamin Eysenbach"
      ],
      "abstract": "Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \\times$ median improvement in returns and increases success rates by $36\\%$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ„å›¾æ¡ä»¶æµå ç”¨æ¨¡å‹ï¼ˆIntention-Conditioned Flow Occupancy Modelsï¼Œç®€ç§° InFOMï¼‰ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­é¢ä¸´çš„åŠ¨ä½œé•¿æœŸä¾èµ–æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æµåŒ¹é…ï¼ˆFlow Matchingï¼‰æŠ€æœ¯æ„å»ºæ¦‚ç‡æ¨¡å‹ï¼Œç”¨ä»¥é¢„æµ‹æ™ºèƒ½ä½“åœ¨æœªæ¥è¾ƒé•¿æ—¶é—´è·¨åº¦å†…å°†è®¿é—®çš„çŠ¶æ€åˆ†å¸ƒï¼Œå³å ç”¨åº¦é‡ï¼ˆOccupancy Measureï¼‰ã€‚ä¸ºäº†åº”å¯¹å¤§è§„æ¨¡æ•°æ®é›†ä¸­å¤šæ ·çš„ä»»åŠ¡ç›®æ ‡ï¼ŒInFOM å¼•å…¥äº†æ•è·ç”¨æˆ·æ„å›¾ï¼ˆUser Intentionï¼‰çš„éšå˜é‡ï¼Œè¿™ä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¿˜æ”¯æŒé€šè¿‡å¹¿ä¹‰ç­–ç•¥æ”¹è¿›ï¼ˆGeneralized Policy Improvementï¼‰å®ç°å¿«é€Ÿä»»åŠ¡é€‚é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨36ä¸ªåŸºäºçŠ¶æ€å’Œ4ä¸ªåŸºäºå›¾åƒçš„åŸºå‡†ä»»åŠ¡ä¸­ï¼ŒInFOM ç›¸æ¯”ç°æœ‰é¢„è®­ç»ƒæ–¹æ³•å®ç°äº†1.8å€çš„æ”¶ç›Šä¸­å€¼æå‡ï¼Œå¹¶å°†ä»»åŠ¡æˆåŠŸç‡æé«˜äº†36%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ç”Ÿæˆå¼ AI é¢„æµ‹å ç”¨åº¦é‡æ˜¯æ„å»ºå…·å¤‡æ—¶é—´æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¨¡å‹çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08902v2",
      "published_date": "2025-06-10 15:27:46 UTC",
      "updated_date": "2025-10-09 00:36:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:54.303023+00:00"
    },
    {
      "arxiv_id": "2506.09101v1",
      "title": "Feature Shift Localization Network",
      "title_zh": "ç‰¹å¾åç§»å®šä½ç½‘ç»œ",
      "authors": [
        "MÃ­riam BarrabÃ©s",
        "Daniel Mas Montserrat",
        "Kapal Dev",
        "Alexander G. Ioannidis"
      ],
      "abstract": "Feature shifts between data sources are present in many applications involving healthcare, biomedical, socioeconomic, financial, survey, and multi-sensor data, among others, where unharmonized heterogeneous data sources, noisy data measurements, or inconsistent processing and standardization pipelines can lead to erroneous features. Localizing shifted features is important to address the underlying cause of the shift and correct or filter the data to avoid degrading downstream analysis. While many techniques can detect distribution shifts, localizing the features originating them is still challenging, with current solutions being either inaccurate or not scalable to large and high-dimensional datasets. In this work, we introduce the Feature Shift Localization Network (FSL-Net), a neural network that can localize feature shifts in large and high-dimensional datasets in a fast and accurate manner. The network, trained with a large number of datasets, learns to extract the statistical properties of the datasets and can localize feature shifts from previously unseen datasets and shifts without the need for re-training. The code and ready-to-use trained model are available at https://github.com/AI-sandbox/FSL-Net.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—ã€é‡‘èç­‰å¤šæºå¼‚æ„æ•°æ®åº”ç”¨ä¸­æ™®éå­˜åœ¨çš„ç‰¹å¾åç§»(Feature Shift)é—®é¢˜ï¼ŒæŒ‡å‡ºå®šä½åç§»ç‰¹å¾å¯¹äºç¡®ä¿ä¸‹æ¸¸åˆ†æå‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯åœ¨å¤§è§„æ¨¡ã€é«˜ç»´æ•°æ®é›†ä¸Šå‡†ç¡®æ€§ä¸è¶³ä¸”éš¾ä»¥æ‰©å±•çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ç‰¹å¾åç§»å®šä½ç½‘ç»œ(Feature Shift Localization Networkï¼Œç®€ç§° FSL-Net)ã€‚è¯¥æ¨¡å‹ä½œä¸ºä¸€ç§ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå­¦ä¹ å¹¶æå–ç»Ÿè®¡ç‰¹æ€§(statistical properties)ï¼Œä»è€Œå®ç°å¯¹ç‰¹å¾åç§»çš„å¿«é€Ÿä¸”ç²¾å‡†å®šä½ã€‚FSL-Net çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æ— éœ€é‡æ–°è®­ç»ƒå³å¯å¤„ç†ä»æœªè§è¿‡çš„æµ‹è¯•æ•°æ®é›†å’Œåç§»ç±»å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†é«˜ç»´æ•°æ®çš„ç‰¹å¾åç§»å®šä½éš¾é¢˜ï¼Œè¿˜å…¬å¼€äº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä»¥ä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 2 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.09101v1",
      "published_date": "2025-06-10 15:27:32 UTC",
      "updated_date": "2025-06-10 15:27:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:27.499696+00:00"
    },
    {
      "arxiv_id": "2506.08899v3",
      "title": "Toward Robust Legal Text Formalization into Defeasible Deontic Logic using LLMs",
      "title_zh": "è¿ˆå‘åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å°†æ³•å¾‹æ–‡æœ¬é²æ£’åœ°å½¢å¼åŒ–ä¸ºå¯åºŸæ­¢é“ä¹‰é€»è¾‘",
      "authors": [
        "Elias Horner",
        "Cristinel Mateis",
        "Guido Governatori",
        "Agata Ciabattoni"
      ],
      "abstract": "We present a comprehensive approach to the automated formalization of legal texts using large language models (LLMs), targeting their transformation into Defeasible Deontic Logic (DDL). Our method employs a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. We introduce a refined success metric that more precisely captures the completeness of formalizations, and a novel two-stage pipeline with a dedicated refinement step to improve logical consistency and coverage. The evaluation procedure has been strengthened with stricter error assessment, and we provide comparative results across multiple LLM configurations, including newly released models and various prompting and fine-tuning strategies. Experiments on legal norms from the Australian Telecommunications Consumer Protections Code demonstrate that, when guided effectively, LLMs can produce formalizations that align closely with expert-crafted representations, underscoring their potential for scalable legal informatics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å°†æ³•å¾‹æ–‡æœ¬è‡ªåŠ¨åŒ–å½¢å¼åŒ–ä¸ºå¯æ’¤é”€ä¹‰åŠ¡é€»è¾‘ (Defeasible Deontic Logic, DDL) çš„ç»¼åˆæ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç»“æ„åŒ–çš„å¤„ç†æµç¨‹ï¼Œå°†å¤æ‚çš„è§„èŒƒæ€§è¯­è¨€åˆ†å‰²ä¸ºåŸå­ç‰‡æ®µï¼Œæå–ä¹‰åŠ¡è§„åˆ™ï¼Œå¹¶è¯„ä¼°å…¶è¯­æ³•å’Œè¯­ä¹‰çš„è¿è´¯æ€§ã€‚ä¸ºäº†æå‡é€»è¾‘ä¸€è‡´æ€§å’Œè¦†ç›–èŒƒå›´ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŒ…å«ä¸“é—¨ç»†åŒ–æ­¥éª¤çš„æ–°å‹ä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ›´ç²¾ç¡®çš„æˆåŠŸæŒ‡æ ‡æ¥è¡¡é‡å½¢å¼åŒ–çš„å®Œæ•´æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šç§ LLM é…ç½®ä¸Šè¿›è¡Œäº†å¯¹æ¯”å®éªŒï¼Œæ¶µç›–äº†ä¸åŒçš„æç¤ºè¯å’Œå¾®è°ƒç­–ç•¥ã€‚åœ¨æ¾³å¤§åˆ©äºšç”µä¿¡æ¶ˆè´¹è€…ä¿æŠ¤å®ˆåˆ™ (Australian Telecommunications Consumer Protections Code) æ³•å¾‹è§„èŒƒä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰æ•ˆå¼•å¯¼ä¸‹ï¼ŒLLMs ç”Ÿæˆçš„å½¢å¼åŒ–ç»“æœèƒ½ä¸ä¸“å®¶ç¼–å†™çš„è¡¨è¾¾é«˜åº¦ä¸€è‡´ã€‚è¯¥ç ”ç©¶æˆæœå‡¸æ˜¾äº† LLMs åœ¨å®ç°å¯æ‰©å±•æ³•å¾‹ä¿¡æ¯å­¦ (Legal Informatics) æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "This version is an extended version with additional results and discussion",
      "pdf_url": "https://arxiv.org/pdf/2506.08899v3",
      "published_date": "2025-06-10 15:25:19 UTC",
      "updated_date": "2025-12-31 10:28:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:36:58.743874+00:00"
    },
    {
      "arxiv_id": "2506.08898v2",
      "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation",
      "title_zh": "åŸºäºæ¡ä»¶è®¡ç®—çš„åå¥½é©±åŠ¨å¤šç›®æ ‡ç»„åˆä¼˜åŒ–",
      "authors": [
        "Mingfeng Fan",
        "Jianan Zhou",
        "Yifeng Zhang",
        "Yaoxin Wu",
        "Jinbiao Chen",
        "Guillaume Adrien Sartoretti"
      ],
      "abstract": "Recent deep reinforcement learning methods have achieved remarkable success in solving multi-objective combinatorial optimization problems (MOCOPs) by decomposing them into multiple subproblems, each associated with a specific weight vector. However, these methods typically treat all subproblems equally and solve them using a single model, hindering the effective exploration of the solution space and thus leading to suboptimal performance. To overcome the limitation, we propose POCCO, a novel plug-and-play framework that enables adaptive selection of model structures for subproblems, which are subsequently optimized based on preference signals rather than explicit reward values. Specifically, we design a conditional computation block that routes subproblems to specialized neural architectures. Moreover, we propose a preference-driven optimization algorithm that learns pairwise preferences between winning and losing solutions. We evaluate the efficacy and versatility of POCCO by applying it to two state-of-the-art neural methods for MOCOPs. Experimental results across four classic MOCOP benchmarks demonstrate its significant superiority and strong generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç›®æ ‡ç»„åˆä¼˜åŒ–é—®é¢˜(MOCOPs)æå‡ºäº†åä¸ºPOCCOçš„æ–°å‹å³æ’å³ç”¨æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä½¿ç”¨å•ä¸€æ¨¡å‹å¤„ç†æ‰€æœ‰å­é—®é¢˜å¯¼è‡´æœç´¢æ•ˆç‡ä½ä¸‹çš„å±€é™æ€§ï¼ŒPOCCOå¼•å…¥äº†æ¡ä»¶è®¡ç®—(conditional computation)æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®åå¥½ä¿¡å·å°†ä¸åŒå­é—®é¢˜è·¯ç”±è‡³ä¸“é—¨çš„ç¥ç»æ¶æ„ä¸­ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åå¥½é©±åŠ¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ èƒœå‡ºè§£ä¸å¤±è´¥è§£ä¹‹é—´çš„æˆå¯¹åå¥½ï¼Œè€Œéä¾èµ–æ˜¾å¼çš„å¥–åŠ±å€¼æ¥ä¼˜åŒ–æ¨¡å‹ã€‚é€šè¿‡åœ¨å››ä¸ªç»å…¸çš„MOCOPåŸºå‡†æµ‹è¯•ä¸Šå¯¹POCCOè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜è¯¥æ¡†æ¶åœ¨ç»“åˆç°æœ‰å…ˆè¿›ç¥ç»æ–¹æ³•æ—¶å±•ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜è¶Šæ€§å’Œæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 6 figures, under review",
      "pdf_url": "https://arxiv.org/pdf/2506.08898v2",
      "published_date": "2025-06-10 15:25:06 UTC",
      "updated_date": "2025-06-20 08:04:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:08.521677+00:00"
    },
    {
      "arxiv_id": "2506.08897v4",
      "title": "PlantDeBERTa: An Open Source Language Model for Plant Science",
      "title_zh": "PlantDeBERTaï¼šé¢å‘æ¤ç‰©ç§‘å­¦çš„å¼€æºè¯­è¨€æ¨¡å‹",
      "authors": [
        "Hiba Khey",
        "Amine Lakhder",
        "Salma Rouichi",
        "Imane El Ghabi",
        "Kamal Hejjaoui",
        "Younes En-nahli",
        "Fahd Kalloubi",
        "Moez Amri"
      ],
      "abstract": "The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantDeBERTa, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantDeBERTa is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantDeBERTa to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantDeBERTa exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields.By providing a scalable and reproducible framework for high-resolution entity recognition, PlantDeBERTa bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PlantDeBERTaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ¤ç‰©ç§‘å­¦é‡èº«å®šåˆ¶çš„é«˜æ€§èƒ½å¼€æºè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä»æ¤ç‰©é€†å¢ƒå“åº”æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†ã€‚è¯¥æ¨¡å‹åŸºäºDeBERTaæ¶æ„ï¼Œå¹¶åœ¨ä¸“å®¶æ ‡æ³¨çš„ä»¥å°æ‰è±†ï¼ˆLens culinarisï¼‰åº”å¯¹å„ç§éç”Ÿç‰©å’Œç”Ÿç‰©å‹åŠ›ä¸ºæ ¸å¿ƒçš„æ‘˜è¦è¯­æ–™åº“ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚åœ¨æ–¹æ³•ä¸Šï¼ŒPlantDeBERTaç»“åˆäº†Transformerå»ºæ¨¡ã€è§„åˆ™å¢å¼ºçš„è¯­è¨€åå¤„ç†ä»¥åŠåŸºäºæœ¬ä½“ï¼ˆOntologyï¼‰çš„å®ä½“å½’ä¸€åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿç²¾ç¡®æ•æ‰å¤æ‚çš„ç”Ÿç‰©å­¦å…³ç³»ã€‚è¯­æ–™åº“çš„æ ‡æ³¨é‡‡ç”¨äº†ä¸Crop Ontologyä¸€è‡´çš„åˆ†å±‚æ–¹æ¡ˆï¼Œè¦†ç›–äº†åˆ†å­ã€ç”Ÿç†ã€ç”ŸåŒ–å’Œå†œè‰ºç­‰å¤šä¸ªç»´åº¦çš„æ¤ç‰©é€‚åº”æ€§ç‰¹å¾ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒPlantDeBERTaåœ¨ä¸åŒå®ä½“ç±»å‹ä¸Šè¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†åœ¨ä½èµ„æºç§‘å­¦é¢†åŸŸè¿›è¡Œç¨³å¥é¢†åŸŸé€‚åº”çš„å¯è¡Œæ€§ã€‚è¯¥æ¨¡å‹å¡«è¡¥äº†å†œä¸šNLPé¢†åŸŸçš„å…³é”®ç©ºç™½ï¼Œå¹¶ä¸ºæ¤ç‰©åŸºå› ç»„å­¦ã€è¡¨å‹ç»„å­¦å’Œå†œè‰ºå­¦ä¸­çš„æ•°æ®é©±åŠ¨ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶ã€‚é€šè¿‡å…¬å¼€å‘å¸ƒPlantDeBERTaï¼Œç ”ç©¶å›¢é˜Ÿæ—¨åœ¨ä¿ƒè¿›è®¡ç®—æ¤ç‰©ç§‘å­¦é¢†åŸŸçš„é€æ˜åº¦å¹¶åŠ é€Ÿè·¨å­¦ç§‘åˆ›æ–°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08897v4",
      "published_date": "2025-06-10 15:24:03 UTC",
      "updated_date": "2025-08-19 13:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:05.201572+00:00"
    },
    {
      "arxiv_id": "2506.08894v2",
      "title": "Product of Experts for Visual Generation",
      "title_zh": "è§†è§‰ç”Ÿæˆçš„ä¸“å®¶ä¹˜ç§¯æ¨¡å‹",
      "authors": [
        "Yunzhi Zhang",
        "Carson Murtuza-Lanier",
        "Zizhang Li",
        "Yilun Du",
        "Jiajun Wu"
      ],
      "abstract": "Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º Product of Experts (PoE) çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆæ¥è‡ªä¸åŒå¼‚æ„æ¨¡å‹ï¼ˆå¦‚è§†è§‰ç”Ÿæˆæ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€å›¾å½¢å¼•æ“å’Œç‰©ç†æ¨¡æ‹Ÿå™¨ï¼‰çš„å¤šæ ·åŒ–çŸ¥è¯†ã€‚è¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒ (Training-free) çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µè¿›è¡ŒçŸ¥è¯†åˆæˆ (Inference-time knowledge composition) æ¥å®ç°è·¨é¢†åŸŸä¸“å®¶çš„èåˆã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨é€€ç«é‡è¦æ€§é‡‡æ · (Annealed Importance Sampling, AIS) ä»å¤šä¸ªä¸“å®¶çš„ä¹˜ç§¯åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆç»“æœçš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒPoE åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„å®ç”¨ä»·å€¼ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹ (Monolithic methods) å…·æœ‰æ›´å¼ºçš„å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›äº†çµæ´»çš„ç”¨æˆ·ç•Œé¢ï¼Œæ–¹ä¾¿ç”¨æˆ·æ›´ç²¾å‡†åœ°æŒ‡å®šè§†è§‰ç”Ÿæˆç›®æ ‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://product-of-experts.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.08894v2",
      "published_date": "2025-06-10 15:21:14 UTC",
      "updated_date": "2025-10-09 01:37:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:11.211536+00:00"
    },
    {
      "arxiv_id": "2506.08889v1",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "title_zh": "SeerAttention-Rï¼šé¢å‘é•¿ç¨‹æ¨ç†çš„ç¨€ç–æ³¨æ„åŠ›é€‚é…",
      "authors": [
        "Yizhao Gao",
        "Shuming Guo",
        "Shijie Cao",
        "Yuqing Xia",
        "Yu Cheng",
        "Lei Wang",
        "Lingxiao Ma",
        "Yutao Sun",
        "Tianzhu Ye",
        "Li Dong",
        "Hayden Kwok-Hay So",
        "Yu Hua",
        "Ting Cao",
        "Fan Yang",
        "Mao Yang"
      ],
      "abstract": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SeerAttention-Rï¼Œä¸€ç§ä¸“ä¸ºæ¨ç†æ¨¡å‹é•¿æ–‡æœ¬è§£ç è®¾è®¡çš„Sparse Attentionæ¡†æ¶ã€‚è¯¥æ¡†æ¶å»¶ç»­äº†SeerAttentioné€šè¿‡Self-Distilled Gatingæœºåˆ¶å­¦ä¹ æ³¨æ„åŠ›ç¨€ç–æ€§çš„è®¾è®¡ï¼Œå¹¶ç§»é™¤Query Poolingä»¥é€‚é…Auto-regressive Decodingè¿‡ç¨‹ã€‚SeerAttention-Ré‡‡ç”¨è½»é‡çº§æ’ä»¶å¼é—¨æ§ï¼Œå¯çµæ´»é›†æˆè‡³ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹è€Œæ— éœ€ä¿®æ”¹åŸå§‹å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œä»…éœ€0.4B Tokenè®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨AIMEåŸºå‡†æµ‹è¯•çš„4K Tokené¢„ç®—ä¸‹ï¼Œå³ä¾¿ä½¿ç”¨è¾ƒå¤§çš„Sparse Attention Block Sizesä¹Ÿèƒ½ä¿æŒè¿‘ä¹æ— æŸçš„æ¨ç†ç²¾åº¦ã€‚é€šè¿‡TileLangå¼€å‘çš„ä¼˜åŒ–è§£ç å†…æ ¸ï¼Œåœ¨H100 GPUå’Œ90%ç¨€ç–åº¦ä¸‹å®ç°äº†æ¯”FlashAttention-3é«˜å‡º9å€çš„è¿‘ç†è®ºåŠ é€Ÿã€‚è¯¥æˆæœä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿åºåˆ—æ¨ç†ä»»åŠ¡ä¸­çš„è®¡ç®—æ•ˆç‡æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08889v1",
      "published_date": "2025-06-10 15:17:26 UTC",
      "updated_date": "2025-06-10 15:17:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:17.904195+00:00"
    },
    {
      "arxiv_id": "2506.19865v2",
      "title": "Scalable and Cost-Efficient de Novo Template-Based Molecular Generation",
      "title_zh": "å¯æ‰©å±•ä¸”å…·æˆæœ¬æ•ˆç›Šçš„åŸºäºæ¨¡æ¿çš„ä»å¤´åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Piotr GaiÅ„ski",
        "Oussama Boussif",
        "Andrei Rekesh",
        "Dmytro Shevchuk",
        "Ali Parviz",
        "Mike Tyers",
        "Robert A. Batey",
        "MichaÅ‚ Koziarski"
      ],
      "abstract": "Template-based molecular generation offers a promising avenue for drug design by ensuring generated compounds are synthetically accessible through predefined reaction templates and building blocks. In this work, we tackle three core challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2) scaling to large building block libraries, and (3) effectively utilizing small fragment sets. We propose Recursive Cost Guidance, a backward policy framework that employs auxiliary machine learning models to approximate synthesis cost and viability. This guidance steers generation toward low-cost synthesis pathways, significantly enhancing cost-efficiency, molecular diversity, and quality, especially when paired with an Exploitation Penalty that balances the trade-off between exploration and exploitation. To enhance performance in smaller building block libraries, we develop a Dynamic Library mechanism that reuses intermediate high-reward states to construct full synthesis trees. Our approach establishes state-of-the-art results in template-based molecular generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ¨¡æ¿çš„åˆ†å­ç”Ÿæˆ (Template-based molecular generation) ä¸­åˆæˆæˆæœ¬æœ€å°åŒ–ã€å¤§è§„æ¨¡æ„å»ºå—åº“ (building block libraries) æ‰©å±•ä»¥åŠå°ç‰‡æ®µé›†æœ‰æ•ˆåˆ©ç”¨ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Recursive Cost Guidance æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è¾…åŠ©æœºå™¨å­¦ä¹ æ¨¡å‹æ¥ä¼°ç®—åˆæˆæˆæœ¬å’Œå¯è¡Œæ€§çš„åå‘ç­–ç•¥ (backward policy) æ¡†æ¶ã€‚è¯¥å¼•å¯¼æœºåˆ¶é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹å¯¼å‘ä½æˆæœ¬åˆæˆè·¯å¾„ï¼Œæ˜¾è‘—æå‡äº†åˆæˆæ•ˆç‡ï¼Œå¹¶é…åˆ Exploitation Penalty ç­–ç•¥æœ‰æ•ˆå¹³è¡¡äº†æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¢å¼ºäº†åˆ†å­çš„å¤šæ ·æ€§ä¸è´¨é‡ã€‚é’ˆå¯¹å°è§„æ¨¡æ„å»ºå—åº“ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº† Dynamic Library æœºåˆ¶ï¼Œé€šè¿‡å¤ç”¨ä¸­é—´é«˜å¥–åŠ±çŠ¶æ€æ¥æ„å»ºå®Œæ•´çš„åˆæˆæ ‘ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç»¼åˆæ–¹æ³•åœ¨åŸºäºæ¨¡æ¿çš„åˆ†å­ç”Ÿæˆé¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿› (state-of-the-art) çš„æ°´å¹³ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19865v2",
      "published_date": "2025-06-10 15:16:09 UTC",
      "updated_date": "2025-11-04 08:59:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:20.427050+00:00"
    },
    {
      "arxiv_id": "2506.08872v2",
      "title": "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task",
      "title_zh": "ChatGPT å½±å“ä¸‹çš„å¤§è„‘ï¼šä½¿ç”¨ AI åŠ©æ‰‹è¾…åŠ©è®ºæ–‡å†™ä½œæ—¶çš„è®¤çŸ¥å€ºç´¯ç§¯",
      "authors": [
        "Nataliya Kosmyna",
        "Eugene Hauptmann",
        "Ye Tong Yuan",
        "Jessica Situ",
        "Xian-Hao Liao",
        "Ashly Vivian Beresnitzky",
        "Iris Braunstein",
        "Pattie Maes"
      ],
      "abstract": "This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ LLM è¾…åŠ©æ’°å†™è®ºæ–‡å¯¹ç¥ç»å’Œè¡Œä¸ºäº§ç”Ÿçš„å½±å“ï¼Œé€šè¿‡å¯¹æ¯” LLM ç»„ã€Search Engine ç»„å’Œ Brain-only ç»„çš„è„‘ç”µå›¾ (EEG) æ•°æ®åŠè®ºæ–‡è´¨é‡è¿›è¡Œåˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒBrain-only ç»„å±•ç°å‡ºæœ€å¼ºä¸”åˆ†å¸ƒæœ€å¹¿çš„è„‘ç½‘ç»œè¿æ¥ï¼Œè€Œ LLM ç»„çš„è„‘è¿æ¥æ€§æœ€å¼±ï¼Œåæ˜ å‡ºè®¤çŸ¥æ´»åŠ¨éšå¤–éƒ¨å·¥å…·å¹²é¢„è€Œæ˜¾è‘—ä¸‹é™ã€‚åœ¨è¡Œä¸ºå±‚é¢ï¼ŒLLM ç»„è¡¨ç°å‡ºæœ€ä½çš„æ–‡ç« å½’å±æ„Ÿï¼Œä¸”åœ¨å‡†ç¡®å¼•ç”¨åŠå›å¿†è‡ªå·±æ’°å†™å†…å®¹æ—¶å­˜åœ¨å›°éš¾ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºï¼Œä» LLM è½¬å‘ç‹¬ç«‹åˆ›ä½œ (LLM-to-Brain) çš„å‚ä¸è€…ä¼šå‡ºç° alpha å’Œ beta è¿æ¥å‡å¼±ï¼Œæ˜¾ç¤ºå‡ºè®¤çŸ¥å‚ä¸åº¦ä¸è¶³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé•¿æœŸä¾èµ– LLM å¯èƒ½å¯¼è‡´ä¸ªä½“åœ¨ç¥ç»ã€è¯­è¨€å’Œè¡Œä¸ºå±‚é¢çš„è¡¨ç°æŒç»­ä¸‹é™ï¼Œè¿™ç§â€œè®¤çŸ¥å€ºåŠ¡â€çš„ç´¯ç§¯å¼•å‘äº†å¯¹ AI åœ¨æ•™è‚²é¢†åŸŸé•¿æœŸå½±å“çš„æ·±åˆ»æ‹…å¿§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "216 pages, 102 figures, 4 tables and appendix",
      "pdf_url": "https://arxiv.org/pdf/2506.08872v2",
      "published_date": "2025-06-10 15:04:28 UTC",
      "updated_date": "2025-12-31 21:13:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:49.827062+00:00"
    },
    {
      "arxiv_id": "2506.08860v2",
      "title": "On The Impact of Merge Request Deviations on Code Review Practices",
      "title_zh": "è®ºåˆå¹¶è¯·æ±‚åå·®å¯¹ä»£ç å®¡æŸ¥å®è·µçš„å½±å“",
      "authors": [
        "Samah Kansab",
        "Francis Bordeleau",
        "Ali Tizghadam"
      ],
      "abstract": "Code review is a key practice in software engineering, ensuring quality and collaboration. However, industrial Merge Request (MR) workflows often deviate from standardized review processes, with many MRs serving non-review purposes (e.g., drafts, rebases, or dependency updates). We term these cases deviations and hypothesize that ignoring them biases analytics and undermines ML models for review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and propose a few-shot learning detection method (91% accuracy). By excluding deviations, ML models predicting review completion time improve performance in 53.33% of cases (up to 2.25x) and exhibit significant shifts in feature importance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven detection approach, and (3) empirical evidence of their impact on ML-based review analytics. This work aids practitioners in optimizing review efforts and ensuring reliable insights.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆå¹¶è¯·æ±‚ï¼ˆMerge Requestï¼‰ä¸­çš„åå·®ç°è±¡ï¼ŒæŒ‡å‡ºå·¥ä¸šç•ŒMRå·¥ä½œæµå¸¸åç¦»æ ‡å‡†è¯„å®¡æµç¨‹ï¼Œç”¨äºè‰ç¨¿ã€å˜åŸºæˆ–ä¾èµ–æ›´æ–°ç­‰éè¯„å®¡ç›®çš„ã€‚ä½œè€…é€šè¿‡è¯†åˆ«ä¸ƒç§åå·®ç±»åˆ«ï¼Œå‘ç°æ­¤ç±»åå·®åœ¨MRä¸­å æ¯”é«˜è¾¾37.02%ï¼Œè‹¥è¢«å¿½è§†å°†ä¸¥é‡å‰Šå¼±æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹çš„åˆ†ææ•ˆèƒ½ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å‡†ç¡®ç‡è¾¾91%çš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shot learningï¼‰æ£€æµ‹æ–¹æ³•ï¼Œå¹¶è¯å®å‰”é™¤è¿™äº›åå·®åï¼Œé¢„æµ‹è¯„å®¡å®Œæˆæ—¶é—´çš„MLæ¨¡å‹æ€§èƒ½åœ¨53.33%çš„æƒ…å†µä¸‹å¾—åˆ°æ˜¾è‘—æå‡ï¼Œæœ€é«˜å¯è¾¾2.25å€ã€‚æ­¤å¤–ï¼Œå®éªŒè§‚å¯Ÿåˆ°æ¨¡å‹ç‰¹å¾é‡è¦æ€§ï¼ˆfeature importanceï¼‰å‘ç”Ÿäº†å¤§å¹…åç§»ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†åå·®å¯¹æ•°æ®å¯é æ€§çš„å½±å“ã€‚è¯¥å·¥ä½œè´¡çŒ®äº†MRåå·®åˆ†ç±»å­¦åŠAIé©±åŠ¨çš„æ£€æµ‹æ–¹æ¡ˆï¼Œä¸ºä»ä¸šè€…ä¼˜åŒ–ä»£ç è¯„å®¡å®è·µå¹¶ç¡®ä¿åˆ†ææ´å¯Ÿçš„å‡†ç¡®æ€§æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08860v2",
      "published_date": "2025-06-10 14:51:20 UTC",
      "updated_date": "2025-06-11 01:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:53.060262+00:00"
    },
    {
      "arxiv_id": "2506.09099v2",
      "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers",
      "title_zh": "å¤ªå¤§è€Œæ— æ³•æ€è€ƒï¼šé¢„è®­ç»ƒ Transformer ä¸­çš„å®¹é‡ã€è®°å¿†ä¸æ³›åŒ–",
      "authors": [
        "Joshua Barron",
        "Devin White"
      ],
      "abstract": "The relationship between memorization and generalization in large language models (LLMs) remains an open area of research, with growing evidence that the two are deeply intertwined. In this work, we investigate this relationship by pre-training a series of capacity-limited Transformer models from scratch on two synthetic character-level tasks designed to separately probe generalization (via arithmetic extrapolation) and memorization (via factual recall). We observe a consistent trade-off: small models extrapolate to unseen arithmetic cases but fail to memorize facts, while larger models memorize but fail to extrapolate. An intermediate-capacity model exhibits a similar shift toward memorization. When trained on both tasks jointly, no model (regardless of size) succeeds at extrapolation. These findings suggest that pre-training may intrinsically favor one learning mode over the other. By isolating these dynamics in a controlled setting, our study offers insight into how model capacity shapes learning behavior and offers broader implications for the design and deployment of small language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­è®°å¿†(Memorization)ä¸æ³›åŒ–(Generalization)ä¹‹é—´é”™ç»¼å¤æ‚çš„å…³ç³»ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä»å¤´é¢„è®­ç»ƒä¸€ç³»åˆ—å®¹é‡æœ‰é™çš„Transformeræ¨¡å‹ï¼Œåˆ©ç”¨ç®—æœ¯å¤–æ¨(arithmetic extrapolation)å’Œäº‹å®å¬å›(factual recall)ä¸¤ä¸ªåˆæˆä»»åŠ¡åˆ†åˆ«æ¢æµ‹æ¨¡å‹çš„æ³›åŒ–ä¸è®°å¿†èƒ½åŠ›ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸¤è€…ä¹‹é—´ä¸€è‡´çš„æƒè¡¡å…³ç³»ï¼Œå³å°å‹æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¿›è¡Œç®—æœ¯å¤–æ¨ä½†æ— æ³•æœ‰æ•ˆè®°å¿†äº‹å®ï¼Œè€Œå¤§å‹æ¨¡å‹åˆ™è¡¨ç°å‡ºæå¼ºçš„è®°å¿†èƒ½åŠ›å´åœ¨å¤–æ¨ä»»åŠ¡ä¸Šå¤±è´¥ã€‚ä¸­ç­‰å®¹é‡çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ ·è¡¨ç°å‡ºå‘è®°å¿†æ¨¡å¼è½¬å˜çš„è¶‹åŠ¿ï¼Œä¸”å½“ä¸¤ç±»ä»»åŠ¡è”åˆè®­ç»ƒæ—¶ï¼Œä»»ä½•è§„æ¨¡çš„æ¨¡å‹éƒ½æ— æ³•å®ç°æœ‰æ•ˆçš„å¤–æ¨ã€‚è¿™äº›å‘ç°è¡¨æ˜é¢„è®­ç»ƒè¿‡ç¨‹å¯èƒ½æ ¹æ®æ¨¡å‹å®¹é‡å†…åœ¨åå‘äºæŸä¸€ç§å­¦ä¹ æ¨¡å¼ï¼Œæ­ç¤ºäº†æ¨¡å‹å®¹é‡å¦‚ä½•å¡‘é€ å­¦ä¹ è¡Œä¸ºï¼Œå¹¶ä¸ºå°å‹è¯­è¨€æ¨¡å‹çš„è®¾è®¡ä¸éƒ¨ç½²æä¾›äº†ç†è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for oral presentation to Tiny Titans: The next wave of On-Device Learning for Foundational Models Workshop at the 42nd International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2506.09099v2",
      "published_date": "2025-06-10 14:49:33 UTC",
      "updated_date": "2025-06-17 19:17:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:48.231193+00:00"
    },
    {
      "arxiv_id": "2507.00004v2",
      "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search",
      "title_zh": "æ¨ç†è®¡ç®—æ‰©å±•ç†è®ºï¼šåŸºäºå®šå‘éšæœºæŠ€èƒ½æœç´¢çš„æ¨ç†",
      "authors": [
        "Austin R. Ellis-Mohr",
        "Anuj K. Nayak",
        "Lav R. Varshney"
      ],
      "abstract": "Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Directed Stochastic Skill Search (DS3) æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†è®¡ç®—ç¼©æ”¾ (Inference Compute Scaling) å»ºç«‹ç†è®ºåŸºç¡€ã€‚DS3 å°†æ¨ç†è¿‡ç¨‹è¡¨å¾ä¸ºåœ¨æŠ€èƒ½å›¾ä¸Šçš„éšæœºéå†ï¼Œé€šè¿‡æ¨å¯¼ä»»åŠ¡æˆåŠŸç‡ä¸è®¡ç®—æˆæœ¬çš„é—­å¼è¡¨è¾¾å¼ï¼Œå®ç°äº†å¯¹ Chain-of-Thought (CoT) å’Œ Tree-of-Thought (ToT) ç­‰å¤šç§æ¨ç†ç­–ç•¥çš„é‡åŒ–æ¯”è¾ƒã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•äº† LLM è®­ç»ƒçš„ç†è®ºæ¨¡å‹ä»¥æ•´åˆæ¨ç†é˜¶æ®µï¼Œä»ç†è®ºä¸Šå¤ç°äº†å‡†ç¡®ç‡éšè®¡ç®—é‡å¯¹æ•°çº¿æ€§å¢é•¿ç­‰å®è¯è§‚å¯Ÿåˆ°çš„ç¼©æ”¾è§„å¾‹ã€‚è¯¥æ¡†æ¶åœ¨ä¸€ä¸ªç»Ÿä¸€çš„åˆ†æä½“ç³»å†…æ•æ‰äº† Best-of-N (BoN) å’Œ majority voting ç­‰è¡Œä¸ºï¼Œæ­ç¤ºäº†æ¨ç†ç­–ç•¥å¦‚ä½•éšä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è€Œæ¼”å˜ã€‚é€šè¿‡æ˜ç¡®è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œè¯¥ç ”ç©¶ä¸ºå¤§æ¨¡å‹åœ¨æ€§èƒ½é‡åˆ°å‚æ•°ç¼©æ”¾ç“¶é¢ˆæ—¶é€šè¿‡å¢åŠ æ¨ç†è®¡ç®—æ¥æå‡èƒ½åŠ›æä¾›äº†åŸåˆ™æ€§çš„ç®—æ³•è®¾è®¡ä¸èµ„æºåˆ†é…æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00004v2",
      "published_date": "2025-06-10 14:47:48 UTC",
      "updated_date": "2025-07-10 17:08:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:51.709191+00:00"
    },
    {
      "arxiv_id": "2506.08854v1",
      "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning",
      "title_zh": "åŸºäºè·¨æ¨¡æ€æ©ç é‡æ„ä¸å¯¹æ¯”å­¦ä¹ çš„ç»„ç»‡ç—…ç†å­¦ç©ºé—´è½¬å½•ç»„è¡¨è¾¾é¢„æµ‹",
      "authors": [
        "Junzhuo Liu",
        "Markus Eckstein",
        "Zhixiang Wang",
        "Friedrich Feuerhake",
        "Dorit Merhof"
      ],
      "abstract": "Spatial transcriptomics is a technology that captures gene expression levels at different spatial locations, widely used in tumor microenvironment analysis and molecular profiling of histopathology, providing valuable insights into resolving gene expression and clinical diagnosis of cancer. Due to the high cost of data acquisition, large-scale spatial transcriptomics data remain challenging to obtain. In this study, we develop a contrastive learning-based deep learning method to predict spatially resolved gene expression from whole-slide images. Evaluation across six different disease datasets demonstrates that, compared to existing studies, our method improves Pearson Correlation Coefficient (PCC) in the prediction of highly expressed genes, highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26% respectively. Further analysis indicates that our method preserves gene-gene correlations and applies to datasets with limited samples. Additionally, our method exhibits potential in cancer tissue localization based on biomarker expression.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä»ç»„ç»‡ç—…ç†å­¦åˆ‡ç‰‡å…¨è§†é‡å›¾åƒ(Whole-slide images)ä¸­é¢„æµ‹ç©ºé—´è§£æçš„åŸºå› è¡¨è¾¾ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è·¨æ¨¡æ€æ©ç é‡å»º(Cross-Modal Mask Reconstruction)ä¸å¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œæœ‰æ•ˆç¼“è§£äº†ç©ºé—´è½¬å½•ç»„(Spatial transcriptomics)æ•°æ®è·å–æˆæœ¬é«˜ã€å¤§è§„æ¨¡æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚åœ¨å…­ä¸ªä¸åŒç–¾ç—…æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹é«˜è¡¨è¾¾åŸºå› ã€é«˜å˜å¼‚åŸºå› å’Œæ ‡å¿—ç‰©åŸºå› (Marker genes)æ–¹é¢çš„çš®å°”é€Šç›¸å…³ç³»æ•°(Pearson Correlation Coefficient, PCC)åˆ†åˆ«æå‡äº†6.27%ã€6.11%å’Œ11.26%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™åŸºå› é—´çš„ç›¸å…³æ€§ï¼Œå¹¶å±•ç°å‡ºåœ¨æœ‰é™æ ·æœ¬æ•°æ®é›†ä¸Šçš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯åœ¨åŸºäºç”Ÿç‰©æ ‡å¿—ç‰©(Biomarker)è¡¨è¾¾çš„ç™Œç—‡ç»„ç»‡å®šä½ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºäº†æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08854v1",
      "published_date": "2025-06-10 14:42:03 UTC",
      "updated_date": "2025-06-10 14:42:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:37:52.877407+00:00"
    },
    {
      "arxiv_id": "2506.08835v3",
      "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics",
      "title_zh": "CulturalFramesï¼šæ–‡ç”Ÿå›¾æ¨¡å‹ä¸è¯„ä¼°æŒ‡æ ‡ä¸­çš„æ–‡åŒ–æœŸæœ›å¯¹é½è¯„ä¼°",
      "authors": [
        "Shravan Nayak",
        "Mehar Bhatia",
        "Xiaofeng Zhang",
        "Verena Rieser",
        "Lisa Anne Hendricks",
        "Sjoerd van Steenkiste",
        "Yash Goyal",
        "Karolina StaÅ„czak",
        "Aishwarya Agrawal"
      ],
      "abstract": "The increasing ubiquity of text-to-image (T2I) models as tools for visual content generation raises concerns about their ability to accurately represent diverse cultural contexts -- where missed cues can stereotype communities and undermine usability. In this work, we present the first study to systematically quantify the alignment of T2I models and evaluation metrics with respect to both explicit (stated) as well as implicit (unstated, implied by the prompt's cultural context) cultural expectations. To this end, we introduce CulturalFrames, a novel benchmark designed for rigorous human evaluation of cultural representation in visual generations. Spanning 10 countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts, 3637 corresponding images generated by 4 state-of-the-art T2I models, and over 10k detailed human annotations. We find that across models and countries, cultural expectations are missed an average of 44% of the time. Among these failures, explicit expectations are missed at a surprisingly high average rate of 68%, while implicit expectation failures are also significant, averaging 49%. Furthermore, we show that existing T2I evaluation metrics correlate poorly with human judgments of cultural alignment, irrespective of their internal reasoning. Collectively, our findings expose critical gaps, provide a concrete testbed, and outline actionable directions for developing culturally informed T2I models and metrics that improve global usability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡ç”Ÿå›¾(Text-to-Image, T2I)æ¨¡å‹åœ¨è¡¨ç¤ºå¤šå…ƒæ–‡åŒ–èƒŒæ™¯æ–¹é¢çš„å‡†ç¡®æ€§ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªç³»ç»Ÿé‡åŒ–æ¨¡å‹ä¸è¯„ä¼°æŒ‡æ ‡åœ¨æ˜¾å¼å’Œéšå¼æ–‡åŒ–æœŸæœ›(Cultural Expectations)å¯¹é½ç¨‹åº¦çš„ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†åä¸º CulturalFrames çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–10ä¸ªå›½å®¶å’Œ5ä¸ªç¤¾ä¼šæ–‡åŒ–é¢†åŸŸï¼ŒåŒ…å«983ä¸ªæç¤ºè¯ã€3637å¼ ç”Ÿæˆçš„å›¾åƒä»¥åŠè¶…è¿‡1ä¸‡æ¡è¯¦ç»†çš„äººç±»æ ‡æ³¨ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸åŒæ¨¡å‹å’Œå›½å®¶ä¸­ï¼Œæ–‡åŒ–æœŸæœ›æœªèƒ½è¾¾æ ‡çš„æƒ…å†µå¹³å‡å æ¯”é«˜è¾¾44%ï¼Œå…¶ä¸­æ˜¾å¼æœŸæœ›çš„å¤±è´¥ç‡é«˜è¾¾68%ï¼Œéšå¼æœŸæœ›çš„å¤±è´¥ç‡ä¹Ÿè¾¾åˆ°49%ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ç°æœ‰çš„ T2I è¯„ä¼°æŒ‡æ ‡ä¸äººç±»å¯¹æ–‡åŒ–å¯¹é½(Cultural Alignment)çš„åˆ¤æ–­ç›¸å…³æ€§æå·®ï¼Œéš¾ä»¥å‡†ç¡®è¡¡é‡æ–‡åŒ–å†…æ¶µã€‚è¯¥å·¥ä½œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸Šçš„å…³é”®å·®è·ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·å…¨çƒå¯ç”¨æ€§çš„æ–‡åŒ–æ„ŸçŸ¥å‹ T2I æ¨¡å‹æä¾›äº†é‡è¦çš„æµ‹è¯•åŸºå‡†å’Œæ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.08835v3",
      "published_date": "2025-06-10 14:21:46 UTC",
      "updated_date": "2026-01-16 19:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:08.200137+00:00"
    },
    {
      "arxiv_id": "2506.08827v1",
      "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation",
      "title_zh": "LLaMA å¾®è°ƒå¯¹æ³•å¾‹æ–‡æ¡£å‘½åå®ä½“æå–ä¸­å¹»è§‰çš„å½±å“",
      "authors": [
        "Francisco Vargas",
        "Alejandro GonzÃ¡lez Coene",
        "Gaston Escalante",
        "Exequiel LobÃ³n",
        "Manuel Pulido"
      ],
      "abstract": "The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¾®è°ƒ(fine tuning)å¯¹LLaMAæ¨¡å‹åœ¨æ³•å¾‹æ–‡æ¡£å‘½åå®ä½“æå–(named entity extraction)ä»»åŠ¡ä¸­äº§ç”Ÿå¹»è§‰(hallucinations)çš„å½±å“ã€‚é’ˆå¯¹ä»å¤æ‚çš„äº¤é€šäº‹æ•…æ³•åº­åˆ¤å†³ä¸­æå–ä¼¤æ®‹æ¯”ä¾‹å’Œèµ”å¿é‡‘é¢çš„æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤æ­¥èµ°æ–¹æ¡ˆï¼Œå³å…ˆé€šè¿‡åŸºäºè¯­ä¹‰æœç´¢(semantic searches)çš„å‘é‡åŒ–æŠ€æœ¯è¿›è¡Œæ–‡æ¡£åˆ‡åˆ†ï¼Œå†åˆ©ç”¨æç¤ºå·¥ç¨‹(prompting)å’ŒLoRAå¾®è°ƒåçš„LLaMAç³»åˆ—æ¨¡å‹æå–å…³é”®å®ä½“ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿åœ¨é›¶æ¸©åº¦è®¾ç½®ä¸‹LLaMA-2 7bä¹Ÿå­˜åœ¨æ˜¾è‘—å¹»è§‰ï¼Œè€Œå¾®è°ƒèƒ½å¤§å¹…é™ä½æ­¤ç±»é”™è¯¯å¹¶æå‡æå–ç²¾åº¦ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„LLaMA-2 70Bå‡†ç¡®ç‡è¾¾åˆ°79.4%ï¼Œæ˜¾è‘—ä¼˜äºå…¶åŸºç¡€ç‰ˆæœ¬ï¼Œä¸”åŸºç¡€ç‰ˆLLaMA-3 8Bçš„è¡¨ç°å·²å±•ç°å‡ºä¸å¾®è°ƒåçš„å¤§å°ºå¯¸æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚æœ€ç»ˆï¼ŒGPT-4 Turboä»¥86.1%çš„å‡†ç¡®ç‡å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†ç»“åˆæ–‡æ¡£åˆ‡åˆ†ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ³•å¾‹ä¿¡æ¯è‡ªåŠ¨åŒ–å¤„ç†ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08827v1",
      "published_date": "2025-06-10 14:17:12 UTC",
      "updated_date": "2025-06-10 14:17:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:11.309054+00:00"
    },
    {
      "arxiv_id": "2506.08822v1",
      "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
      "title_zh": "FreqPolicyï¼šåŸºäºé¢‘ç‡ä¸€è‡´æ€§çš„é«˜æ•ˆæµå¼è§†è§‰è¿åŠ¨ç­–ç•¥",
      "authors": [
        "Yifei Su",
        "Ning Liu",
        "Dong Chen",
        "Zhen Zhao",
        "Kun Wu",
        "Meng Li",
        "Zhiyuan Xu",
        "Zhengping Che",
        "Jian Tang"
      ],
      "abstract": "Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits their applicability in real-time robotic systems. To address this issue, existing approaches accelerate the sampling process in generative modeling-based visuomotor policies by adapting acceleration techniques originally developed for image generation. Despite this progress, a major distinction remains: image generation typically involves producing independent samples without temporal dependencies, whereas robotic manipulation involves generating time-series action trajectories that require continuity and temporal coherence. To effectively exploit temporal information in robotic manipulation, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. We introduce a frequency consistency constraint that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on the 40 tasks of Libero. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency 93.5Hz. The code will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FreqPolicyï¼Œä¸€ç§åŸºäºé¢‘ç‡ä¸€è‡´æ€§çº¦æŸçš„é«˜æ•ˆ Flow-based è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼ç­–ç•¥åœ¨æœºå™¨äººæ“ä½œä¸­å› å¤šæ­¥é‡‡æ ·å¯¼è‡´æ¨ç†å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚ä¸ä¾§é‡ç‹¬ç«‹æ ·æœ¬ç”Ÿæˆçš„ä¼ ç»ŸæŠ€æœ¯ä¸åŒï¼Œè¯¥æ–¹æ³•å¼ºè°ƒåŠ¨ä½œè½¨è¿¹çš„è¿ç»­æ€§ä¸æ—¶é—´ç›¸å¹²æ€§ï¼Œé€šè¿‡å¼•å…¥é¢‘ç‡ä¸€è‡´æ€§çº¦æŸ (Frequency Consistency Constraint) å¼ºåˆ¶å¯¹é½ Flow æ¨¡å‹ä¸åŒæ—¶é—´æ­¥é—´çš„é¢‘åŸŸåŠ¨ä½œç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†è‡ªé€‚åº”ä¸€è‡´æ€§æŸå¤± (Adaptive Consistency Loss) ä»¥æ•è·æœºå™¨äººä»»åŠ¡ä¸­å›ºæœ‰çš„ç»“æ„åŒ–æ—¶é—´å˜åŒ–ï¼Œä»è€Œåœ¨æ”¯æŒé«˜æ•ˆå•æ­¥åŠ¨ä½œç”Ÿæˆ (One-step Action Generation) çš„åŒæ—¶ç¡®ä¿å…¶æ”¶æ•›è‡³ç›®æ ‡åˆ†å¸ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFreqPolicy åœ¨ 3 ä¸ªä»¿çœŸåŸºå‡†çš„ 53 é¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•æ­¥ç”Ÿæˆå™¨ã€‚è¯¥æ–¹æ³•åœ¨é›†æˆè‡³è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹åï¼Œäº Libero åŸºå‡†æµ‹è¯•çš„ 40 é¡¹ä»»åŠ¡ä¸Šå®ç°äº†æ— æŸåŠ é€Ÿã€‚åœ¨çœŸå®ä¸–ç•Œæœºå™¨äººåœºæ™¯ä¸­ï¼ŒFreqPolicy è¾¾åˆ°äº† 93.5Hz çš„æ¨ç†é¢‘ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å®æ—¶ç³»ç»Ÿä¸­çš„å“è¶Šæ•ˆç‡ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08822v1",
      "published_date": "2025-06-10 14:12:53 UTC",
      "updated_date": "2025-06-10 14:12:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:19.614185+00:00"
    },
    {
      "arxiv_id": "2506.08800v2",
      "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents",
      "title_zh": "è¡¡é‡æ•°æ®ç§‘å­¦è‡ªåŠ¨åŒ–ï¼šAI åŠ©æ‰‹ä¸æ™ºèƒ½ä½“è¯„ä¼°å·¥å…·ç»¼è¿°",
      "authors": [
        "Irene Testini",
        "JosÃ© HernÃ¡ndez-Orallo",
        "Lorenzo Pacchiardi"
      ],
      "abstract": "Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) have been increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.",
      "tldr_zh": "æœ¬ç»¼è¿°ç³»ç»Ÿè¯„ä¼°äº†ç”¨äºæ•°æ®ç§‘å­¦(Data Science)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åŠ©æ‰‹ä¸æ™ºèƒ½ä½“(Agents)çš„è¯„ä»·å·¥å…·ï¼Œåˆ†æäº†è¿™äº›AIç³»ç»Ÿåœ¨ä»£ç æ‰§è¡Œ(Code Execution)å’ŒçŸ¥è¯†åº“(Knowledge Bases)æ”¯æŒä¸‹çš„è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚ç ”ç©¶å‘ç°å½“å‰çš„è¯„ä¼°å·¥ä½œå­˜åœ¨ä¸‰å¤§ä¸»è¦å±€é™ï¼šé¦–å…ˆæ˜¯è¿‡åº¦å…³æ³¨å°‘æ•°ä»¥ç›®æ ‡ä¸ºå¯¼å‘(Goal-oriented)çš„ä»»åŠ¡ï¼Œå¿½è§†äº†æ•°æ®ç®¡ç†(Data Management)å’Œæ¢ç´¢æ€§æ´»åŠ¨(Exploratory Activities)ï¼›å…¶æ¬¡æ˜¯é›†ä¸­äºçº¯è¾…åŠ©æˆ–å®Œå…¨è‡ªä¸»æ¨¡å¼ï¼Œç¼ºä¹å¯¹äººæœºåä½œ(Human-AI Collaboration)ä¸­é—´å±‚çº§çš„æ¢è®¨ï¼›æœ€åæ˜¯ä¾§é‡äºäººç±»æ›¿ä»£(Human Substitution)ï¼Œè€Œå¿½ç•¥äº†é€šè¿‡ä»»åŠ¡è½¬æ¢(Task Transformation)æå‡è‡ªåŠ¨åŒ–æ°´å¹³çš„å¯èƒ½æ€§ã€‚è¯¥æ–‡ç« æŒ‡å‡ºï¼Œæœªæ¥çš„ç ”ç©¶éœ€è¦æ‰“ç ´ç°æœ‰è¯„ä¼°æ¡†æ¶çš„å±€é™ï¼Œä»è€Œæ›´å…¨é¢åœ°è¡¡é‡AIåœ¨æ•°æ®ç§‘å­¦é¢†åŸŸçš„å®é™…æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in Transactions of Machine Learning Research (TMLR), 10/2025 https://openreview.net/forum?id=MB0TCLfLn1",
      "pdf_url": "https://arxiv.org/pdf/2506.08800v2",
      "published_date": "2025-06-10 13:47:22 UTC",
      "updated_date": "2025-10-22 17:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:34.517395+00:00"
    },
    {
      "arxiv_id": "2506.08795v1",
      "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning",
      "title_zh": "è¿ˆå‘åŸºäºæ¨¡ä»¿å­¦ä¹ çš„æ— ç”Ÿç‰©ä¿¡å·è‡ªä¸»å‡è‚¢æ‰‹æ§åˆ¶",
      "authors": [
        "Kaijie Shi",
        "Wanglong Lu",
        "Hanli Zhao",
        "Vinicius Prado da Fonseca",
        "Ting Zou",
        "Xianta Jiang"
      ],
      "abstract": "Limb loss affects millions globally, impairing physical function and reducing quality of life. Most traditional surface electromyographic (sEMG) and semi-autonomous methods require users to generate myoelectric signals for each control, imposing physically and mentally taxing demands. This study aims to develop a fully autonomous control system that enables a prosthetic hand to automatically grasp and release objects of various shapes using only a camera attached to the wrist. By placing the hand near an object, the system will automatically execute grasping actions with a proper grip force in response to the hand's movements and the environment. To release the object being grasped, just naturally place the object close to the table and the system will automatically open the hand. Such a system would provide individuals with limb loss with a very easy-to-use prosthetic control interface and greatly reduce mental effort while using. To achieve this goal, we developed a teleoperation system to collect human demonstration data for training the prosthetic hand control model using imitation learning, which mimics the prosthetic hand actions from human. Through training the model using only a few objects' data from one single participant, we have shown that the imitation learning algorithm can achieve high success rates, generalizing to more individuals and unseen objects with a variation of weights. The demonstrations are available at \\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå‡è‚¢æ§åˆ¶ä¾èµ–è¡¨é¢è‚Œç”µä¿¡å·(sEMG)å¯¼è‡´ç”¨æˆ·èº«å¿ƒè´Ÿæ‹…é‡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ç”Ÿç‰©ä¿¡å·çš„è‡ªä¸»å‡è‚¢æ§åˆ¶ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä»…é€šè¿‡å®‰è£…åœ¨è…•éƒ¨çš„æ‘„åƒå¤´æ„ŸçŸ¥ç¯å¢ƒï¼Œåˆ©ç”¨æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä½¿å‡è‚¢èƒ½å¤Ÿæ ¹æ®æ‰‹éƒ¨è¿åŠ¨è‡ªåŠ¨æ‰§è¡ŒæŠ“å–ä»»åŠ¡ï¼Œå¹¶èƒ½æ–½åŠ é€‚å½“çš„æ¡åŠ›ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€å¥—è¿œç¨‹æ“ä½œç³»ç»Ÿæ¥æ”¶é›†äººç±»æ¼”ç¤ºæ•°æ®ï¼Œç”¨äºè®­ç»ƒå‡è‚¢æ§åˆ¶æ¨¡å‹ä»¥æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿ä»…ä½¿ç”¨æ¥è‡ªå•ä¸€å‚ä¸è€…é’ˆå¯¹å°‘æ•°ç‰©ä½“çš„è®­ç»ƒæ•°æ®ï¼Œè¯¥æ¨¡å‹ä¹Ÿèƒ½å®ç°æé«˜çš„ä»»åŠ¡æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·ä»¥åŠå…·æœ‰ä¸åŒé‡é‡çš„æœªçŸ¥ç‰©ä½“ã€‚è¿™ç§å…¨è‡ªä¸»æ§åˆ¶æ–¹æ¡ˆä¸ºè‚¢ä½“ç¼ºå¤±æ‚£è€…æä¾›äº†ç®€ä¾¿çš„äº¤äº’ç•Œé¢ï¼Œæ˜¾è‘—é™ä½äº†å‡è‚¢ä½¿ç”¨è¿‡ç¨‹ä¸­çš„å¿ƒç†è´Ÿæ‹…ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08795v1",
      "published_date": "2025-06-10 13:44:08 UTC",
      "updated_date": "2025-06-10 13:44:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:13.942412+00:00"
    },
    {
      "arxiv_id": "2506.08790v1",
      "title": "Do Generative AI Tools Ensure Green Code? An Investigative Study",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·èƒ½å¦ç¡®ä¿ç»¿è‰²ä»£ç ï¼Ÿä¸€é¡¹è°ƒæŸ¥ç ”ç©¶",
      "authors": [
        "Samarth Sikand",
        "Rohit Mehra",
        "Vibhu Saujanya Sharma",
        "Vikrant Kaulgud",
        "Sanjay Podder",
        "Adam P. Burden"
      ],
      "abstract": "Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or \"greenness\" of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools - ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å·¥å…·åœ¨è½¯ä»¶å¯æŒç»­æ€§(Software Sustainability)æ–¹é¢çš„è¡¨ç°ï¼Œé‡ç‚¹è¯„ä¼°å…¶ç”Ÿæˆçš„ä»£ç æ˜¯å¦éµå¾ªç»¿è‰²ç¼–ç è§„èŒƒã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹ ChatGPTã€BARD å’Œ Copilot è¿™ä¸‰ç§ä¸»æµå·¥å…·è¿›è¡Œäº†åˆæ­¥è°ƒæŸ¥ï¼Œåˆ†æå…¶åœ¨ä¸åŒåœºæ™¯ä¸‹ç”Ÿæˆä»£ç çš„â€œç»¿è‰²â€ç¨‹åº¦(Greenness)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›å·¥å…·åœ¨å¤„ç†å¤šç§è§„åˆ™å’Œåœºæ™¯æ—¶ï¼Œé»˜è®¤ç”Ÿæˆçš„ä»£ç æ™®éå­˜åœ¨éç»¿è‰²(Non-green)è¡Œä¸ºï¼Œæœªèƒ½æœ‰æ•ˆé‡‡çº³å¯æŒç»­ç¼–ç å®è·µ(Sustainable Coding Practices)ã€‚è¯¥å‘ç°æ­ç¤ºäº†å½“å‰ AI è¾…åŠ©ç¼–ç¨‹åœ¨ç¯å¢ƒå½±å“æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å¼€å±•æ·±å…¥è°ƒæŸ¥ä¸åˆ¶å®šæœ‰æ•ˆä¿®å¤ç­–ç•¥çš„ç´§è¿«æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ„å»ºæ›´ç¯ä¿ã€æ›´å…·éŸ§æ€§çš„æ•°å­—ç”Ÿæ€ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒä¸ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SE",
      "comment": "4 pages. To be published in the proceedings of 2nd International Workshop on Responsible AI Engineering (RAIE '24), co-located with ICSE '24, Lisbon, Portugal",
      "pdf_url": "https://arxiv.org/pdf/2506.08790v1",
      "published_date": "2025-06-10 13:38:41 UTC",
      "updated_date": "2025-06-10 13:38:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:39:10.218665+00:00"
    },
    {
      "arxiv_id": "2506.08785v1",
      "title": "POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration",
      "title_zh": "POLARONï¼šç²¾åº¦æ„ŸçŸ¥å‹ç«¯ä¾§å­¦ä¹ ä¸è‡ªé€‚åº”è¿è¡Œæ—¶å¯é…ç½® AI åŠ é€Ÿ",
      "authors": [
        "Mukul Lokhande",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "The increasing complexity of AI models requires flexible hardware capable of supporting diverse precision formats, particularly for energy-constrained edge platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC engine that performs efficient multiply-accumulate operations using a unified data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The architecture incorporates a layer adaptive precision strategy to align computational accuracy with workload sensitivity, optimizing both performance and energy usage. PARV-CE integrates quantization-aware execution with a reconfigurable SIMD pipeline, enabling high-throughput processing with minimal overhead through hardware-software co-design. The results demonstrate up to 2x improvement in PDP and 3x reduction in resource usage compared to SoTA designs, while retaining accuracy within 1.8% FP32 baseline. The architecture supports both on-device training and inference across a range of workloads, including DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE incorporated POLARON as a scalable and energy-efficient solution for precision-adaptive AI acceleration at edge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†POLARONï¼Œä¸€ç§é¢å‘è¾¹ç¼˜å¹³å°è®¾è®¡çš„ç²¾åº¦æ„ŸçŸ¥ç‰‡ä¸Šå­¦ä¹ ä¸è‡ªé€‚åº”è¿è¡Œæ—¶å¯é…ç½®AIåŠ é€Ÿæ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡å¤æ‚AIæ¨¡å‹æ€§èƒ½ä¸èƒ½æ•ˆã€‚è¯¥ç³»ç»Ÿæ ¸å¿ƒé›†æˆäº†PARV-CEå¤šç²¾åº¦MACå¼•æ“ï¼Œé€šè¿‡ç»Ÿä¸€æ•°æ®è·¯å¾„æ”¯æŒ4/8/16ä½å®šç‚¹ã€æµ®ç‚¹åŠPositæ ¼å¼çš„é«˜æ•ˆè¿ç®—ã€‚é€šè¿‡é‡‡ç”¨å±‚çº§è‡ªé€‚åº”ç²¾åº¦(Layer adaptive precision)ç­–ç•¥å’Œé‡åŒ–æ„ŸçŸ¥æ‰§è¡Œ(Quantization-aware execution)æŠ€æœ¯ï¼Œç¡¬ä»¶èƒ½å¤Ÿæ ¹æ®è´Ÿè½½æ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´è®¡ç®—ç²¾åº¦ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œä¸å½“å‰å…ˆè¿›è®¾è®¡ç›¸æ¯”ï¼Œè¯¥æ¶æ„åœ¨ä¿æŒç²¾åº¦æŸå¤±ä½äº1.8% FP32åŸºå‡†çš„åŒæ—¶ï¼Œå®ç°äº†2å€çš„åŠŸè€—å»¶è¿Ÿç§¯(PDP)æå‡å’Œ3å€çš„èµ„æºå ç”¨ç¼©å‡ã€‚POLARONä¸ä»…æ”¯æŒDNNsã€RNNsã€RLå’ŒTransformerç­‰å¤šç§æ¨¡å‹çš„ç«¯ä¾§æ¨ç†ï¼Œè¿˜å…·å¤‡é«˜æ•ˆçš„ç‰‡ä¸Šè®­ç»ƒèƒ½åŠ›ã€‚è¿™ä¸ºè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹æ„å»ºå¯æ‰©å±•ã€é«˜èƒ½æ•ˆçš„ç²¾åº¦è‡ªé€‚åº”AIåŠ é€Ÿæ–¹æ¡ˆæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CC",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08785v1",
      "published_date": "2025-06-10 13:33:02 UTC",
      "updated_date": "2025-06-10 13:33:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:52.837143+00:00"
    },
    {
      "arxiv_id": "2506.12087v1",
      "title": "Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity",
      "title_zh": "å…·æœ‰å¸¸æ•°æ—¶é—´å¤æ‚åº¦çš„è„‰å†²ç¥ç»ç½‘ç»œé«˜æ•ˆå¹¶è¡Œè®­ç»ƒæ–¹æ³•",
      "authors": [
        "Wanjin Feng",
        "Xingyu Gao",
        "Wenqian Du",
        "Hailong Shi",
        "Peilin Zhao",
        "Pengcheng Wu",
        "Chunyan Miao"
      ],
      "abstract": "Spiking Neural Networks (SNNs) often suffer from high time complexity $O(T)$ due to the sequential processing of $T$ spikes, making training computationally expensive.\n  In this paper, we propose a novel Fixed-point Parallel Training (FPT) method to accelerate SNN training without modifying the network architecture or introducing additional assumptions.\n  FPT reduces the time complexity to $O(K)$, where $K$ is a small constant (usually $K=3$), by using a fixed-point iteration form of Leaky Integrate-and-Fire (LIF) neurons for all $T$ timesteps.\n  We provide a theoretical convergence analysis of FPT and demonstrate that existing parallel spiking neurons can be viewed as special cases of our proposed method.\n  Experimental results show that FPT effectively simulates the dynamics of original LIF neurons, significantly reducing computational time without sacrificing accuracy.\n  This makes FPT a scalable and efficient solution for real-world applications, particularly for long-term tasks.\n  Our code will be released at \\href{https://github.com/WanjinVon/FPT}{\\texttt{https://github.com/WanjinVon/FPT}}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Spiking Neural Networks (SNNs) å› é¡ºåºå¤„ç† $T$ ä¸ªè„‰å†²è€Œå¯¼è‡´ $O(T)$ é«˜æ—¶é—´å¤æ‚åº¦çš„ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§åä¸º Fixed-point Parallel Training (FPT) çš„æ–°å‹å¹¶è¡Œè®­ç»ƒæ–¹æ³•ã€‚FPT é€šè¿‡å¯¹æ‰€æœ‰ $T$ ä¸ªæ—¶é—´æ­¥é‡‡ç”¨ Leaky Integrate-and-Fire (LIF) ç¥ç»å…ƒçš„å®šç‚¹è¿­ä»£å½¢å¼ï¼Œåœ¨ä¸ä¿®æ”¹ç½‘ç»œæ¶æ„çš„å‰æä¸‹å°†æ—¶é—´å¤æ‚åº¦é™ä½è‡³å¸¸æ•°çº§ $O(K)$ï¼ˆé€šå¸¸ $K=3$ï¼‰ã€‚ä½œè€…ä¸ä»…æä¾›äº†è¯¥æ–¹æ³•çš„ç†è®ºæ”¶æ•›æ€§åˆ†æï¼Œè¿˜è¯æ˜äº†ç°æœ‰çš„å¹¶è¡Œè„‰å†²ç¥ç»å…ƒå®é™…ä¸Šæ˜¯ FPT çš„ç‰¹æ®Šæƒ…å†µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFPT èƒ½å¤Ÿæœ‰æ•ˆæ¨¡æ‹ŸåŸå§‹ LIF ç¥ç»å…ƒçš„åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œåœ¨æ˜¾è‘—ç¼©çŸ­è®¡ç®—æ—¶é—´çš„åŒæ—¶ä¿æŒäº†æé«˜çš„æ¨¡å‹ç²¾åº¦ã€‚è¿™ç§å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§çš„è®­ç»ƒæ–¹æ¡ˆä¸º SNNs åœ¨é•¿æœŸä»»åŠ¡ç­‰å®é™…åœºæ™¯ä¸­çš„é«˜æ•ˆåº”ç”¨æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12087v1",
      "published_date": "2025-06-10 13:27:27 UTC",
      "updated_date": "2025-06-10 13:27:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:55.402817+00:00"
    },
    {
      "arxiv_id": "2506.08774v1",
      "title": "Multimodal Representation Alignment for Cross-modal Information Retrieval",
      "title_zh": "é¢å‘è·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢çš„å¤šæ¨¡æ€è¡¨å¾å¯¹é½",
      "authors": [
        "Fan Xu",
        "Luis A. Leiva"
      ],
      "abstract": "Different machine learning models can represent the same underlying concept in different ways. This variability is particularly valuable for in-the-wild multimodal retrieval, where the objective is to identify the corresponding representation in one modality given another modality as input. This challenge can be effectively framed as a feature alignment problem. For example, given a sentence encoded by a language model, retrieve the most semantically aligned image based on features produced by an image encoder, or vice versa. In this work, we first investigate the geometric relationships between visual and textual embeddings derived from both vision-language models and combined unimodal models. We then align these representations using four standard similarity metrics as well as two learned ones, implemented via neural networks. Our findings indicate that the Wasserstein distance can serve as an informative measure of the modality gap, while cosine similarity consistently outperforms alternative metrics in feature alignment tasks. Furthermore, we observe that conventional architectures such as multilayer perceptrons are insufficient for capturing the complex interactions between image and text representations. Our study offers novel insights and practical considerations for researchers working in multimodal information retrieval, particularly in real-world, cross-modal applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨æ¨¡æ€ä¿¡æ¯æ£€ç´¢ï¼ˆCross-modal Information Retrievalï¼‰ä¸­çš„ç‰¹å¾å¯¹é½é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†ä¸åŒæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¡¨ç¤ºåŒä¸€åº•å±‚æ¦‚å¿µæ—¶å­˜åœ¨çš„å·®å¼‚ã€‚ä½œè€…é¦–å…ˆè°ƒç ”äº†è§†è§‰ä¸æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œå¹¶å¯¹æ¯”äº†å››ç§æ ‡å‡†ç›¸ä¼¼æ€§åº¦é‡åŠä¸¤ç§åŸºäºç¥ç»ç½‘ç»œçš„ä¹ å¾—åº¦é‡åœ¨å¯¹é½è¿™äº›è¡¨ç¤ºæ—¶çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒWasserstein distance å¯ä»¥ä½œä¸ºè¡¡é‡æ¨¡æ€é—´éš™ï¼ˆmodality gapï¼‰çš„æœ‰æ•ˆæŒ‡æ ‡ï¼Œè€Œ cosine similarityï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨ç‰¹å¾å¯¹é½ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå…¶ä»–åº¦é‡æ ‡å‡†ã€‚æ­¤å¤–ï¼Œå®éªŒè§‚å¯Ÿåˆ°ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆmultilayer perceptronsï¼‰æ¶æ„ä¸è¶³ä»¥æ•æ‰å›¾åƒä¸æ–‡æœ¬è¡¨ç¤ºä¹‹é—´å¤æ‚çš„äº¤äº’ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤„ç†çœŸå®åœºæ™¯ä¸‹çš„è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡æä¾›äº†é‡è¦çš„ç†è®ºè§è§£å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08774v1",
      "published_date": "2025-06-10 13:16:26 UTC",
      "updated_date": "2025-06-10 13:16:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:38:59.231288+00:00"
    },
    {
      "arxiv_id": "2506.08771v1",
      "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery",
      "title_zh": "å› æœä¹‹è·¯ï¼šåœ¨çŸ¥è¯†å›¾è°±ä¸­å¯»æ‰¾ç”¨äºåŸºäºçŸ¥è¯†çš„å› æœå‘ç°çš„ä¿¡æ¯å­å›¾",
      "authors": [
        "Yuni Susanti",
        "Michael FÃ¤rber"
      ],
      "abstract": "Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†å›¾è°±(Knowledge Graphs)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŸºäºçŸ¥è¯†çš„å› æœå‘ç°(Knowledge-Based Causal Discovery)ä¸­å¤§è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ç¨³å®šä¸”ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»çŸ¥è¯†å›¾è°±ä¸­è¯†åˆ«å‡ºåŒ…å«ä¸°å¯Œä¿¡æ¯çš„å…ƒè·¯å¾„å­å›¾(metapath-based subgraphs)ï¼Œå¹¶å¼•å…¥æ’åºå­¦ä¹ (Learning-to-Rank)æ¨¡å‹å¯¹è¿™äº›å­å›¾è¿›è¡ŒäºŒæ¬¡ç­›é€‰ã€‚éšåï¼Œæ’åæœ€é«˜çš„å­å›¾è¢«æ•´åˆè¿›é›¶æ ·æœ¬æç¤º(zero-shot prompts)ä¸­ï¼Œæœ‰æ•ˆå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å˜é‡é—´å› æœå…³ç³»æ—¶çš„å‡†ç¡®æ€§ã€‚åœ¨ç”Ÿç‰©åŒ»å­¦å’Œå¼€æ”¾åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹å’ŒçŸ¥è¯†å›¾è°±ç¯å¢ƒä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œå…¶ F1 åˆ†æ•°è¾ƒå¤šæ•°åŸºçº¿æ¨¡å‹æå‡äº†å¤šè¾¾ 44.4 ä¸ªç‚¹ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æé«˜äº†å› æœæ¨ç†çš„å¯é æ€§ï¼Œä¹Ÿä¸ºç»“æ„åŒ–çŸ¥è¯†ä¸ç”Ÿæˆå¼AIçš„ç»“åˆæä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at KDD 2025 (full research paper)",
      "pdf_url": "https://arxiv.org/pdf/2506.08771v1",
      "published_date": "2025-06-10 13:13:55 UTC",
      "updated_date": "2025-06-10 13:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:39:07.789095+00:00"
    },
    {
      "arxiv_id": "2506.09096v3",
      "title": "Intra-Trajectory Consistency for Reward Modeling",
      "title_zh": "å¥–åŠ±å»ºæ¨¡ä¸­çš„è½¨è¿¹å†…ä¸€è‡´æ€§",
      "authors": [
        "Chaoyang Zhou",
        "Shunyu Liu",
        "Zengmao Wang",
        "Di Wang",
        "Rong-Cheng Tu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "abstract": "Reward models are critical for improving large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) or inference-time verification. Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses. In this paper, we propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. Building on analysis under the Bayesian framework, we develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards. We apply the proposed regularization to the advanced outcome reward model, improving its performance on RewardBench. Besides, we show that the reward model trained with the proposed regularization induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results. Our code is provided in https://github.com/chaoyang101/ICRM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¥–åŠ±å»ºæ¨¡ä¸­å“åº”çº§è¯„åˆ†ï¼ˆresponse-level scoresï¼‰ç›‘ç£ä¿¡å·è¿‡äºç²—ç³™ã€å¯¼è‡´æ¨¡å‹éš¾ä»¥è¯†åˆ«è½¨è¿¹å†…ç‰¹å®šç»„ä»¶ä¸”æ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº†è½¨è¿¹å†…ä¸€è‡´æ€§ï¼ˆIntra-Trajectory Consistencyï¼‰å»ºæ¨¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆæ¦‚ç‡ï¼ˆgeneration probabilitiesï¼‰åœ¨å“åº”è½¨è¿¹çš„å„ä¸ªè¿‡ç¨‹ä¹‹é—´å»ºç«‹å…³è”ï¼Œä½¿ç²—ç²’åº¦çš„å“åº”çº§ä¿¡å·èƒ½å¤Ÿæœ‰æ•ˆä¼ æ’­å¹¶æä¾›ç»†ç²’åº¦çš„ç›‘ç£ä¿¡æ¯ã€‚åŸºäºè´å¶æ–¯æ¡†æ¶ï¼ˆBayesian frameworkï¼‰ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆintra-trajectory consistency regularizationï¼‰ï¼Œå¼ºåˆ¶è¦æ±‚å…·æœ‰è¾ƒé«˜ä¸‹ä¸ªæ ‡è®°ç”Ÿæˆæ¦‚ç‡çš„ç›¸é‚»è¿‡ç¨‹ä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆoutcome reward modelï¼‰åœ¨ RewardBench ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥æ­£åˆ™åŒ–è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹èƒ½è¯±å¯¼äº§ç”Ÿæ›´å¥½çš„ DPO å¯¹é½ç­–ç•¥ï¼Œå¹¶åœ¨ Best-of-N (BoN) æ¨ç†éªŒè¯ä¸­å–å¾—äº†æ›´ä¼˜å¼‚çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.09096v3",
      "published_date": "2025-06-10 12:59:14 UTC",
      "updated_date": "2025-06-16 04:03:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:39:07.286519+00:00"
    },
    {
      "arxiv_id": "2506.08756v1",
      "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning",
      "title_zh": "é¢å‘ç¥ç»ç¬¦å·æœºå™¨äººå­¦ä¹ çš„è´å¶æ–¯é€†ç‰©ç†",
      "authors": [
        "Octavio Arriaga",
        "Rebecca Adam",
        "Melvin Laux",
        "Lisa Gutzeit",
        "Marco Ragni",
        "Jan Peters",
        "Frank Kirchner"
      ],
      "abstract": "Real-world robotic applications, from autonomous exploration to assistive technologies, require adaptive, interpretable, and data-efficient learning paradigms. While deep learning architectures and foundation models have driven significant advances in diverse robotic applications, they remain limited in their ability to operate efficiently and reliably in unknown and dynamic environments. In this position paper, we critically assess these limitations and introduce a conceptual framework for combining data-driven learning with deliberate, structured reasoning. Specifically, we propose leveraging differentiable physics for efficient world modeling, Bayesian inference for uncertainty-aware decision-making, and meta-learning for rapid adaptation to new tasks. By embedding physical symbolic reasoning within neural models, robots could generalize beyond their training data, reason about novel situations, and continuously expand their knowledge. We argue that such hybrid neuro-symbolic architectures are essential for the next generation of autonomous systems, and to this end, we provide a research roadmap to guide and accelerate their development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ å’ŒåŸºç¡€æ¨¡å‹åœ¨æœªçŸ¥åŠåŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§å±€é™ï¼Œæå‡ºäº†ä¸€ç§å°†æ•°æ®é©±åŠ¨å­¦ä¹ ä¸ç»“æ„åŒ–æ¨ç†ç›¸ç»“åˆçš„ Neuro-Symbolic æœºå™¨äººå­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å»ºè®®åˆ©ç”¨ Differentiable physics è¿›è¡Œé«˜æ•ˆçš„ä¸–ç•Œå»ºæ¨¡ï¼Œå¹¶å¼•å…¥ Bayesian inference ä»¥å®ç°æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„å†³ç­–ï¼ŒåŒæ—¶ç»“åˆ Meta-learning å¢å¼ºå¯¹æ–°ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”èƒ½åŠ›ã€‚é€šè¿‡å°†ç‰©ç†ç¬¦å·æ¨ç†åµŒå…¥ç¥ç»æ¨¡å‹ï¼Œæœºå™¨äººèƒ½å¤Ÿçªç ´è®­ç»ƒæ•°æ®çš„é™åˆ¶ï¼Œå¯¹æ–°é¢–æƒ…å¢ƒè¿›è¡Œæ¨ç†å¹¶æŒç»­æ‰©å±•çŸ¥è¯†ä½“ç³»ã€‚ç ”ç©¶å¼ºè°ƒäº†è¿™ç§æ··åˆæ¶æ„å¯¹äºå¼€å‘ä¸‹ä¸€ä»£è‡ªä¸»ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ä»½ç ”ç©¶è·¯çº¿å›¾ä»¥æŒ‡å¯¼å’ŒåŠ é€Ÿè¯¥æŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08756v1",
      "published_date": "2025-06-10 12:53:31 UTC",
      "updated_date": "2025-06-10 12:53:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:11.325105+00:00"
    },
    {
      "arxiv_id": "2506.08753v1",
      "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking",
      "title_zh": "å½±å“å¤§è¯­è¨€æ¨¡å‹å¯¹è¯çŠ¶æ€è·Ÿè¸ªä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„å› ç´ ",
      "authors": [
        "Pradyoth Hegde",
        "Santosh Kesiraju",
        "Jan Å vec",
        "Å imon SedlÃ¡Äek",
        "Bolaji Yusuf",
        "OldÅ™ich Plchot",
        "Deepak K T",
        "Jan ÄŒernockÃ½"
      ],
      "abstract": "This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)åœ¨å¯¹è¯çŠ¶æ€è·Ÿè¸ª(Dialogue State Tracking)ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è¯†åˆ«å½±å“å…¶æœ‰æ•ˆæ€§çš„å…³é”®å› ç´ ã€‚ç ”ç©¶è€…é‡‡ç”¨åŸºäºå¥å­åµŒå…¥(sentence embedding)çš„kæœ€è¿‘é‚»(k-nearest neighbour)æ–¹æ³•æ¥æ£€ç´¢åˆé€‚çš„ç¤ºä¾‹ï¼Œå¹¶å°†è¿™äº›ç¤ºä¾‹ä¸æµ‹è¯•æ ·æœ¬æ•´åˆè‡³æ¨¡æ¿ä¸­ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚é€šè¿‡åœ¨MultiWoZ2.4æ•°æ®é›†ä¸Šé’ˆå¯¹OLMo-7B-instructã€Mistral-7B-Instruct-v0.3å’ŒLlama3.2-3B-Instructç­‰å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†ç¤ºä¾‹é€‰æ‹©(demonstration selection)å’Œæç¤ºä¸Šä¸‹æ–‡(prompt context)å¯¹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚å®éªŒç»“æœæ­ç¤ºäº†LLMsåœ¨å¤„ç†å¯¹è¯çŠ¶æ€è·Ÿè¸ªæ—¶çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç‰¹å¾ï¼Œå¹¶ä¸ºä¼˜åŒ–æ­¤ç±»ä»»åŠ¡çš„æç¤ºç­–ç•¥æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08753v1",
      "published_date": "2025-06-10 12:46:26 UTC",
      "updated_date": "2025-06-10 12:46:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:39:11.561914+00:00"
    },
    {
      "arxiv_id": "2506.08747v1",
      "title": "A Sample Efficient Conditional Independence Test in the Presence of Discretization",
      "title_zh": "ç¦»æ•£åŒ–æƒ…å½¢ä¸‹çš„é«˜æ ·æœ¬æ•ˆç‡æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ",
      "authors": [
        "Boyang Sun",
        "Yu Yao",
        "Xinshuai Dong",
        "Zongfang Liu",
        "Tongliang Liu",
        "Yumou Qiu",
        "Kun Zhang"
      ],
      "abstract": "In many real-world scenarios, interested variables are often represented as discretized values due to measurement limitations. Applying Conditional Independence (CI) tests directly to such discretized data, however, can lead to incorrect conclusions. To address this, recent advancements have sought to infer the correct CI relationship between the latent variables through binarizing observed data. However, this process inevitably results in a loss of information, which degrades the test's performance. Motivated by this, this paper introduces a sample-efficient CI test that does not rely on the binarization process. We find that the independence relationships of latent continuous variables can be established by addressing an over-identifying restriction problem with Generalized Method of Moments (GMM). Based on this insight, we derive an appropriate test statistic and establish its asymptotic distribution correctly reflecting CI by leveraging nodewise regression. Theoretical findings and Empirical results across various datasets demonstrate that the superiority and effectiveness of our proposed test. Our code implementation is provided in https://github.com/boyangaaaaa/DCT",
      "tldr_zh": "åœ¨è®¸å¤šç°å®åœºæ™¯ä¸­ï¼Œç›®æ ‡å˜é‡å¸¸å› æµ‹é‡é™åˆ¶è€Œä»¥ç¦»æ•£å€¼å‘ˆç°ï¼Œç›´æ¥å¯¹å…¶è¿›è¡Œæ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ (Conditional Independence Test, CI Test) å¾€å¾€ä¼šå¯¼è‡´é”™è¯¯çš„ç»“è®ºã€‚ç°æœ‰ç ”ç©¶é€šå¸¸é€šè¿‡å°†è§‚æµ‹æ•°æ®äºŒå€¼åŒ–æ¥æ¨æ–­æ½œå˜é‡é—´çš„ CI å…³ç³»ï¼Œä½†è¿™ä¸å¯é¿å…åœ°é€ æˆäº†ä¿¡æ¯æŸå¤±å¹¶é™ä½äº†æ£€éªŒæ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ ·æœ¬æ•ˆç‡é«˜çš„ CI æ£€éªŒæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒåœ¨äºæ— éœ€ä¾èµ–äºŒå€¼åŒ–è¿‡ç¨‹ï¼Œè€Œæ˜¯é€šè¿‡å¹¿ä¹‰çŸ©ä¼°è®¡ (Generalized Method of Moments, GMM) å¤„ç†è¿‡åº¦è¯†åˆ«çº¦æŸé—®é¢˜ï¼Œä»è€Œå»ºç«‹æ½œè¿ç»­å˜é‡é—´çš„ç‹¬ç«‹å…³ç³»ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œä½œè€…åˆ©ç”¨èŠ‚ç‚¹å›å½’ (nodewise regression) æ¨å¯¼å‡ºäº†ç›¸åº”çš„æ£€éªŒç»Ÿè®¡é‡ï¼Œå¹¶ç¡®ç«‹äº†å…¶èƒ½å¤Ÿå‡†ç¡®åæ˜  CI å…³ç³»çš„æ¸è¿‘åˆ†å¸ƒã€‚ç†è®ºåˆ†æä¸åœ¨å¤šé¡¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå…±åŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†ç¦»æ•£åŒ–æ•°æ®æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08747v1",
      "published_date": "2025-06-10 12:41:26 UTC",
      "updated_date": "2025-06-10 12:41:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:21.419598+00:00"
    },
    {
      "arxiv_id": "2506.08745v1",
      "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning",
      "title_zh": "ä¸€è‡´è·¯å¾„é€šå‘çœŸç†ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„è‡ªå¥–åŠ±å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Kongcheng Zhang",
        "Qi Yao",
        "Shunyu Liu",
        "Yingjie Wang",
        "Baisheng Lai",
        "Jieping Ye",
        "Mingli Song",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at https://github.com/sastpg/CoVo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)é«˜åº¦ä¾èµ–å¤–éƒ¨ç›‘ç£çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCoVoçš„è‡ªæˆ‘å¥–åŠ±å¼ºåŒ–å­¦ä¹ (Self-Rewarding Reinforcement Learning)æ¡†æ¶ã€‚å…¶æ ¸å¿ƒè§è§£åœ¨äºæ­£ç¡®å›ç­”åœ¨ä¸åŒæ¨ç†è½¨è¿¹ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§(Consistency)å’Œä½æ³¢åŠ¨æ€§(Volatility)ï¼Œå³ä¸­é—´æ¨ç†çŠ¶æ€å¾€å¾€å‘æœ€ç»ˆç­”æ¡ˆæ”¶æ•›ä¸”å¯¹å…¶ä»–å€™é€‰é¡¹åç¦»è¾ƒå°ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼ŒCoVoé€šè¿‡ç¨³å¥çš„å‘é‡ç©ºé—´èšåˆç­–ç•¥(Vector-Space Aggregation Strategy)æ•´åˆä¸€è‡´æ€§ä¸æ³¢åŠ¨æ€§ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œå¹¶å¼•å…¥å¥½å¥‡å¿ƒå¥–åŠ±(Curiosity Bonus)ä»¥ä¿ƒè¿›å¤šæ ·åŒ–æ¢ç´¢ã€‚è¯¥æ¡†æ¶ä½¿å¤§è¯­è¨€æ¨¡å‹(LLM)èƒ½å¤Ÿä»¥è‡ªæˆ‘å¥–åŠ±çš„æ–¹å¼å­¦ä¹ æ¨ç†ï¼Œæä¾›äº†ä¸€æ¡æ— éœ€å¤–éƒ¨ç›‘ç£çš„å¯æ‰©å±•è®­ç»ƒè·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶³ä»¥åª²ç¾ç”šè‡³è¶…è¶Šä¼ ç»Ÿçš„æœ‰ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚è¿™ä¸€å·¥ä½œè¯æ˜äº†åˆ©ç”¨æ¨ç†è·¯å¾„çš„å†…åœ¨å±æ€§æ¥æå‡æ¨¡å‹é€»è¾‘æ¨ç†èƒ½åŠ›çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08745v1",
      "published_date": "2025-06-10 12:40:39 UTC",
      "updated_date": "2025-06-10 12:40:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:24.622659+00:00"
    },
    {
      "arxiv_id": "2506.08743v1",
      "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems",
      "title_zh": "èåˆRDFçŸ¥è¯†å›¾è°±ä¸å›¾ç¥ç»ç½‘ç»œçš„è¯­ä¹‰ä¸°å¯Œæ¨èç³»ç»Ÿ",
      "authors": [
        "Michael FÃ¤rber",
        "David Lamprecht",
        "Yuni Susanti"
      ],
      "abstract": "Graph Neural Networks (GNNs) have substantially advanced the field of recommender systems. However, despite the creation of more than a thousand knowledge graphs (KGs) under the W3C standard RDF, their rich semantic information has not yet been fully leveraged in GNN-based recommender systems. To address this gap, we propose a comprehensive integration of RDF KGs with GNNs that utilizes both the topological information from RDF object properties and the content information from RDF datatype properties. Our main focus is an in-depth evaluation of various GNNs, analyzing how different semantic feature initializations and types of graph structure heterogeneity influence their performance in recommendation tasks. Through experiments across multiple recommendation scenarios involving multi-million-node RDF graphs, we demonstrate that harnessing the semantic richness of RDF KGs significantly improves recommender systems and lays the groundwork for GNN-based recommender systems for the Linked Open Data cloud. The code and data are available on our GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°† RDF Knowledge Graphs ä¸ Graph Neural Networks (GNNs) æœ‰æ•ˆç»“åˆï¼Œä»¥æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„æ¨èç³»ç»Ÿã€‚é’ˆå¯¹ç°æœ‰ GNN æ¨èç³»ç»Ÿæœªèƒ½å……åˆ†åˆ©ç”¨ RDF æ ‡å‡†ä¸­ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»¼åˆé›†æˆæ–¹æ¡ˆï¼ŒåŒæ—¶æŒ–æ˜ RDF object properties çš„æ‹“æ‰‘ä¿¡æ¯å’Œ datatype properties çš„å†…å®¹ä¿¡æ¯ã€‚ç ”ç©¶æ·±å…¥è¯„ä¼°äº†å¤šç§ GNN æ¨¡å‹ï¼Œé‡ç‚¹åˆ†æäº†ä¸åŒè¯­ä¹‰ç‰¹å¾åˆå§‹åŒ–æ–¹å¼ä»¥åŠå›¾ç»“æ„çš„ heterogeneity å¯¹æ¨èæ€§èƒ½çš„å½±å“ã€‚é€šè¿‡åœ¨åŒ…å«æ•°ç™¾ä¸‡èŠ‚ç‚¹çš„ RDF å›¾è°±ä¸Šè¿›è¡Œå¤šåœºæ™¯å®éªŒï¼Œè¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ RDF çš„è¯­ä¹‰ä¸°å¯Œæ€§å¯æ˜¾è‘—æå‡æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºåœ¨ Linked Open Data äº‘ä¸­åº”ç”¨åŸºäº GNN çš„æ¨èæŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at DASFAA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08743v1",
      "published_date": "2025-06-10 12:38:24 UTC",
      "updated_date": "2025-06-10 12:38:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:40.339774+00:00"
    },
    {
      "arxiv_id": "2506.08738v2",
      "title": "Societal AI Research Has Become Less Interdisciplinary",
      "title_zh": "äººå·¥æ™ºèƒ½ç¤¾ä¼šæ€§ç ”ç©¶çš„è·¨å­¦ç§‘ç¨‹åº¦æœ‰æ‰€ä¸‹é™",
      "authors": [
        "Dror Kris Markus",
        "Fabrizio Gilardi",
        "Daria Stetsenko"
      ],
      "abstract": "As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æ2014è‡³2024å¹´é—´ArXivä¸Šè¶…è¿‡10ä¸‡ç¯‡AIç›¸å…³è®ºæ–‡ï¼Œæ¢è®¨äº†ä¼¦ç†ä»·å€¼å’Œç¤¾ä¼šå…³æ³¨ç‚¹å¦‚ä½•èå…¥æŠ€æœ¯æ€§AIç ”ç©¶ã€‚ç ”ç©¶åˆ©ç”¨åˆ†ç±»å™¨è¯†åˆ«ç¤¾ä¼šæ€§å†…å®¹å¹¶è¡¡é‡å…¶ç¨‹åº¦ï¼Œå‘ç°è™½ç„¶è·¨å­¦ç§‘å›¢é˜Ÿåœ¨äº§ç”Ÿç¤¾ä¼šå¯¼å‘ç ”ç©¶æ–¹é¢ä»å…·ä¼˜åŠ¿ï¼Œä½†çº¯è®¡ç®—æœºç§‘å­¦(Computer Science-only)å›¢é˜Ÿåœ¨æ•´ä½“ç¤¾ä¼šæ€§äº§å‡ºä¸­çš„ä»½é¢æ­£æ˜¾è‘—å¢é•¿ã€‚è¿™äº›æŠ€æœ¯å›¢é˜Ÿæ­£æ—¥ç›Šæ·±å…¥åœ°è§£å†³å…¬å¹³æ€§(Fairness)ã€å®‰å…¨æ€§(Safety)ã€åŒ»ç–—ä¿å¥å’Œè™šå‡ä¿¡æ¯(Misinformation)ç­‰å¹¿æ³›é¢†åŸŸã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å…³äºç¤¾ä¼šæ€§AIç ”ç©¶é©±åŠ¨åŠ›çš„ä¼ ç»Ÿå‡è®¾ï¼Œæ­ç¤ºäº†è¯¥é¢†åŸŸè·¨å­¦ç§‘ç¨‹åº¦ä¸‹é™çš„ç°çŠ¶ã€‚ç ”ç©¶æœ€åå¯¹ç”±çº¯æŠ€æœ¯å›¢é˜Ÿä¸»å¯¼AIå®‰å…¨ä¸æ²»ç†(Governance)çš„æ½œåœ¨å½±å“ï¼Œä»¥åŠç¤¾ä¼šç§‘å­¦ä¸äººæ–‡ç§‘å­¦å­¦è€…åœ¨æŠ€æœ¯é©±åŠ¨ç¯å¢ƒä¸‹çš„è§’è‰²å®šä½æå‡ºäº†é‡è¦è´¨ç–‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08738v2",
      "published_date": "2025-06-10 12:34:53 UTC",
      "updated_date": "2025-06-11 09:42:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:50.525459+00:00"
    },
    {
      "arxiv_id": "2506.08737v1",
      "title": "Exploration by Random Reward Perturbation",
      "title_zh": "åŸºäºéšæœºå¥–åŠ±æ‰°åŠ¨çš„æ¢ç´¢",
      "authors": [
        "Haozhe Ma",
        "Guoji Fu",
        "Zhengding Luo",
        "Jiele Wu",
        "Tze-Yun Leong"
      ],
      "abstract": "We introduce Random Reward Perturbation (RRP), a novel exploration strategy for reinforcement learning (RL). Our theoretical analyses demonstrate that adding zero-mean noise to environmental rewards effectively enhances policy diversity during training, thereby expanding the range of exploration. RRP is fully compatible with the action-perturbation-based exploration strategies, such as $Îµ$-greedy, stochastic policies, and entropy regularization, providing additive improvements to exploration effects. It is general, lightweight, and can be integrated into existing RL algorithms with minimal implementation effort and negligible computational overhead. RRP establishes a theoretical connection between reward shaping and noise-driven exploration, highlighting their complementary potential. Experiments show that RRP significantly boosts the performance of Proximal Policy Optimization and Soft Actor-Critic, achieving higher sample efficiency and escaping local optima across various tasks, under both sparse and dense reward scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Random Reward Perturbation (RRP)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ–°å‹æ¢ç´¢ç­–ç•¥ã€‚é€šè¿‡åœ¨ç¯å¢ƒå¥–åŠ±ä¸­åŠ å…¥é›¶å‡å€¼å™ªå£°ï¼ŒRRPèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºè®­ç»ƒæœŸé—´çš„ç­–ç•¥å¤šæ ·æ€§ï¼Œä»è€Œæ‰©å¤§æ¢ç´¢èŒƒå›´ã€‚è¯¥æ–¹æ³•ä¸Îµ-greedyã€éšæœºç­–ç•¥å’Œç†µæ­£åˆ™åŒ–(Entropy Regularization)ç­‰åŸºäºåŠ¨ä½œæ‰°åŠ¨çš„æ¢ç´¢ç­–ç•¥å®Œå…¨å…¼å®¹ï¼Œå¹¶èƒ½æä¾›åŠ æ€§æå‡çš„æ¢ç´¢æ•ˆæœã€‚RRPå…·æœ‰é€šç”¨ä¸”è½»é‡åŒ–çš„ç‰¹ç‚¹ï¼Œå¯ä»¥ä»¥æä½çš„å®ç°éš¾åº¦å’Œè®¡ç®—å¼€é”€é›†æˆåˆ°ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†å¥–åŠ±å¡‘å½¢(Reward Shaping)ä¸å™ªå£°é©±åŠ¨æ¢ç´¢ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œå‡¸æ˜¾äº†ä¸¤è€…çš„äº’è¡¥æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRPæ˜¾è‘—æå‡äº†Proximal Policy Optimization (PPO)å’ŒSoft Actor-Critic (SAC)çš„æ€§èƒ½ï¼Œåœ¨ç¨€ç–å’Œå¯†é›†å¥–åŠ±åœºæ™¯ä¸‹å‡å®ç°äº†æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å¹¶èƒ½æœ‰æ•ˆè·³å‡ºå±€éƒ¨æœ€ä¼˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08737v1",
      "published_date": "2025-06-10 12:34:00 UTC",
      "updated_date": "2025-06-10 12:34:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:49.014087+00:00"
    },
    {
      "arxiv_id": "2506.12086v1",
      "title": "Wanting to Be Understood Explains the Meta-Problem of Consciousness",
      "title_zh": "æ¸´æœ›è¢«ç†è§£ï¼šå¯¹æ„è¯†å…ƒé—®é¢˜çš„è§£é‡Š",
      "authors": [
        "Chrisantha Fernando",
        "Dylan Banarse",
        "Simon Osindero"
      ],
      "abstract": "Because we are highly motivated to be understood, we created public external representations -- mime, language, art -- to externalise our inner states. We argue that such external representations are a pre-condition for access consciousness, the global availability of information for reasoning. Yet the bandwidth of access consciousness is tiny compared with the richness of `raw experience', so no external representation can reproduce that richness in full. Ordinarily an explanation of experience need only let an audience `grasp' the relevant pattern, not relive the phenomenon. But our drive to be understood, and our low level sensorimotor capacities for `grasping' so rich, that the demand for an explanation of the feel of experience cannot be ``satisfactory''. That inflated epistemic demand (the preeminence of our expectation that we could be perfectly understood by another or ourselves) rather than an irreducible metaphysical gulf -- keeps the hard problem of consciousness alive. But on the plus side, it seems we will simply never give up creating new ways to communicate and think about our experiences. In this view, to be consciously aware is to strive to have one's agency understood by oneself and others.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºè§†è§’ï¼Œè®¤ä¸ºäººç±»æ¸´æœ›è¢«ç†è§£çš„å¼ºçƒˆåŠ¨æœºæ˜¯è§£é‡Šæ„è¯†å…ƒé—®é¢˜(Meta-Problem of Consciousness)çš„å…³é”®ã€‚ä½œè€…è®¤ä¸ºï¼Œä¸ºäº†å¤–åŒ–å†…åœ¨çŠ¶æ€è€Œåˆ›é€ çš„è¯­è¨€å’Œè‰ºæœ¯ç­‰å¤–éƒ¨è¡¨å¾æ˜¯é€šè¾¾æ„è¯†(access consciousness)çš„å…ˆå†³æ¡ä»¶ï¼Œä½†é€šè¾¾æ„è¯†çš„å¸¦å®½ç›¸æ¯”äºåŸå§‹ä½“éªŒ(raw experience)çš„ä¸°å¯Œæ€§æ˜¾å¾—æå…¶å¾®å°ã€‚è¿™ç§å·¨å¤§çš„è½å·®å¯¼è‡´ä»»ä½•å…³äºä¸»è§‚æ„Ÿå—çš„è§£é‡Šéƒ½æ— æ³•ä»¤äººå®Œå…¨æ»¡æ„ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§è†¨èƒ€çš„è®¤çŸ¥éœ€æ±‚ï¼Œä½¿å¾—æ„è¯†çš„å›°éš¾é—®é¢˜(hard problem)åœ¨å½¢è€Œä¸Šå­¦å±‚é¢ä¹‹å¤–ä¿æŒæ´»è·ƒã€‚åœ¨è¯¥è§‚ç‚¹ä¸‹ï¼Œæœ‰æ„è¯†çš„è§‰çŸ¥å®é™…ä¸Šæ˜¯ä¸»ä½“ä¸ºäº†è®©è‡ªå·±å’Œä»–äººçš„ä¸»ä½“æ€§(agency)å¾—åˆ°ç†è§£è€Œè¿›è¡Œçš„æŒç»­åŠªåŠ›ã€‚è¿™ä¸€è§†è§’ä¸ä»…è§£é‡Šäº†æ„è¯†äº§ç”Ÿçš„æ¼”åŒ–èƒŒæ™¯ï¼Œä¹Ÿè¯´æ˜äº†äººç±»ä¸ºä½•ä¼šä¸æ–­åˆ›é€ æ–°çš„æ²Ÿé€šä¸æ€ç»´æ–¹å¼æ¥è¡¨è¾¾ç»éªŒã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12086v1",
      "published_date": "2025-06-10 12:31:09 UTC",
      "updated_date": "2025-06-10 12:31:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:52.687557+00:00"
    },
    {
      "arxiv_id": "2506.08729v2",
      "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces",
      "title_zh": "è…¹ä¸»åŠ¨è„‰ç˜¤è¡¨é¢å±€éƒ¨ç”Ÿé•¿é¢„æµ‹çš„å‡ ä½•æ·±åº¦å­¦ä¹ ",
      "authors": [
        "Dieuwertje Alblas",
        "Patryk Rygiel",
        "Julian Suk",
        "Kaj O. Kappe",
        "Marieke Hofman",
        "Christoph Brune",
        "Kak Khee Yeung",
        "Jelmer M. Wolterink"
      ],
      "abstract": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the abdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current clinical guidelines recommend elective surgical repair when the maximum AAA diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet these criteria are periodically monitored, with surveillance intervals based on the maximum AAA diameter. However, this diameter does not take into account the complex relation between the 3D AAA shape and its growth, making standardized intervals potentially unfit. Personalized AAA growth predictions could improve monitoring strategies. We propose to use an SE(3)-symmetric transformer model to predict AAA growth directly on the vascular model surface enriched with local, multi-physical features. In contrast to other works which have parameterized the AAA shape, this representation preserves the vascular surface's anatomical structure and geometric fidelity. We train our model using a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24 AAA patients at irregularly sampled intervals. After training, our model predicts AAA growth to the next scan moment with a median diameter error of 1.18 mm. We further demonstrate our model's utility to identify whether a patient will become eligible for elective repair within two years (acc = 0.93). Finally, we evaluate our model's generalization on an external validation set consisting of 25 CTAs from 7 AAA patients from a different hospital. Our results show that local directional AAA growth prediction from the vascular surface is feasible and may contribute to personalized surveillance strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è…¹ä¸»åŠ¨è„‰ç˜¤ (Abdominal aortic aneurysms, AAAs) ä¸´åºŠç®¡ç†ä¸­ä»…ä¾èµ–æœ€å¤§ç›´å¾„æŒ‡æ ‡è€Œå¿½ç•¥ 3D å½¢çŠ¶å¤æ‚æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•æ·±åº¦å­¦ä¹ çš„ SE(3)-symmetric transformer æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç›´æ¥åœ¨å¯Œå«å±€éƒ¨å¤šç‰©ç†ç‰¹å¾çš„è¡€ç®¡æ¨¡å‹è¡¨é¢é¢„æµ‹ AAA ç”Ÿé•¿ï¼Œæœ‰æ•ˆä¿ç•™äº†è¡€ç®¡è¡¨é¢çš„è§£å‰–ç»“æ„å’Œå‡ ä½•å¿ å®åº¦ã€‚é€šè¿‡å¯¹ 24 åæ‚£è€…çš„ 113 ä»½çºµå‘è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½± (CTA) å½±åƒè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€æ¬¡æ‰«ææ—¶çš„ä¸­å€¼ç›´å¾„è¯¯å·®ä»…ä¸º 1.18 mmã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨è¯†åˆ«æ‚£è€…ä¸¤å¹´å†…æ˜¯å¦è¾¾åˆ°æ‰‹æœ¯ä¿®å¤æ ‡å‡†æ–¹é¢è¾¾åˆ°äº† 0.93 çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤–éƒ¨éªŒè¯é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†ä»è¡€ç®¡è¡¨é¢è¿›è¡Œå±€éƒ¨å®šå‘ç”Ÿé•¿é¢„æµ‹çš„å¯è¡Œæ€§ï¼Œä¸ºå®ç°ä¸ªæ€§åŒ–çš„ AAA ç›‘æµ‹ç­–ç•¥æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08729v2",
      "published_date": "2025-06-10 12:27:12 UTC",
      "updated_date": "2025-06-11 13:26:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:40:51.039093+00:00"
    },
    {
      "arxiv_id": "2506.08727v1",
      "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs",
      "title_zh": "Breaking the ICEï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ç¢³æ’æ”¾ä¸èƒ½è€—ä¼°ç®—åŸºå‡†æµ‹è¯•çš„æœºé‡ä¸æŒ‘æˆ˜æ¢ç©¶",
      "authors": [
        "Samarth Sikand",
        "Rohit Mehra",
        "Priyavanshi Pathania",
        "Nikhil Bamby",
        "Vibhu Saujanya Sharma",
        "Vikrant Kaulgud",
        "Sanjay Podder",
        "Adam P. Burden"
      ],
      "abstract": "While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†é˜¶æ®µå¯¹èƒ½æºæ¶ˆè€—å’Œç¢³æ’æ”¾çš„é‡å¤§å½±å“ï¼ŒæŒ‡å‡ºå½“å‰ç¯å¢ƒå¯æŒç»­æ€§é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ç›‘æµ‹å·¥å…·ä¾µå…¥æ€§é«˜ã€æ•°æ®ç‚¹éœ€æ±‚å¤§ä¸”è¯¯å·®æ˜æ˜¾çš„ç¼ºé™·ï¼Œä½œè€…æå‡ºäº†åä¸ºR-ICEçš„æ¼”åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ç°æœ‰çš„æœ€å…ˆè¿›(SOTA)åŸºå‡†æµ‹è¯•(Benchmarks)åŠå…¶ç›¸å…³æ•°æ®ç‚¹ï¼Œå®ç°äº†å¯¹æç¤ºè¯çº§åˆ«(Prompt-level)æ¨ç†ç¢³æ’æ”¾çš„éä¾µå…¥æ€§ä¼°ç®—ã€‚è¿™ç§æ–¹æ³•ä¸ºåŠ¨æ€æ¨¡å‹è·¯ç”±(Dynamic LLM routing)å’Œç¢³æ ¸ç®—ç­‰æ–°å…´åº”ç”¨åœºæ™¯æä¾›äº†æ›´å…·å®è·µæ€§çš„è·¯å¾„ã€‚åˆæ­¥éªŒè¯ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºåŸºå‡†æµ‹è¯•çš„å»ºæ¨¡åœ¨æ¨ç†æ’æ”¾ä¼°ç®—æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸ºç§‘å­¦ç•Œè¿›ä¸€æ­¥æ¢ç´¢ç¯å¢ƒå‹å¥½çš„AIæŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages. To be published in the proceedings of 9th International Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025, Ottawa, Canada (Co-located with ICSE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.08727v1",
      "published_date": "2025-06-10 12:23:02 UTC",
      "updated_date": "2025-06-10 12:23:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:41:06.343644+00:00"
    },
    {
      "arxiv_id": "2506.08726v3",
      "title": "Improved LLM Agents for Financial Document Question Answering",
      "title_zh": "æ”¹è¿›çš„é‡‘èæ–‡æ¡£é—®ç­” LLM æ™ºèƒ½ä½“",
      "authors": [
        "Nelvin Tan",
        "Zian Seng",
        "Liang Zhang",
        "Yu-Ching Shih",
        "Dong Yang",
        "Amol Salunkhe"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç»“åˆè¡¨æ ¼ä¸æ–‡æœ¬çš„é‡‘èæ–‡æ¡£æ•°å€¼é—®ç­”(Question Answering)ä»»åŠ¡æ—¶çš„å±€é™æ€§è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„æ‰¹è¯„æ™ºèƒ½ä½“(critic agent)åœ¨ç¼ºä¹æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾(oracle labels)çš„æƒ…å†µä¸‹ï¼Œå…¶è‡ªæˆ‘çº é”™æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ”¹è¿›çš„critic agentå’Œè®¡ç®—å™¨æ™ºèƒ½ä½“(calculator agent)ï¼Œæ—¨åœ¨æå‡å¤æ‚æ•°å€¼é€»è¾‘çš„å¤„ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•Program-of-thought (PoT)ï¼Œä¸”å…·å¤‡æ›´é«˜çš„å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å®éªŒåˆ†æäº†ä¸åŒæ™ºèƒ½ä½“(agents)é—´çš„äº¤äº’ä½œç”¨åŠå…¶å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ï¼Œä¸ºæ„å»ºæ›´å¯é çš„é‡‘èæ–‡æ¡£é—®ç­”ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 6 figures. More analysis is added to Appendix C",
      "pdf_url": "https://arxiv.org/pdf/2506.08726v3",
      "published_date": "2025-06-10 12:22:57 UTC",
      "updated_date": "2026-01-07 07:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:41:22.956983+00:00"
    },
    {
      "arxiv_id": "2506.11127v3",
      "title": "UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions",
      "title_zh": "UITron-Speechï¼šè¿ˆå‘åŸºäºè¯­éŸ³æŒ‡ä»¤çš„è‡ªåŠ¨åŒ– GUI æ™ºèƒ½ä½“",
      "authors": [
        "Wenkang Han",
        "Zhixiong Zeng",
        "Jing Huang",
        "Shu Jiang",
        "Liming Zheng",
        "Longrong Yang",
        "Haibo Qiu",
        "Chang Yao",
        "Jingyuan Chen",
        "Lin Ma"
      ],
      "abstract": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this issue, we propose replacing text with speech as the instruction input modality for GUI agents, and introduce UITron-Speech, which is the first end-to-end GUI agent capable of directly processing speech instructions and on-device screenshots to predict user actions. To tackle the problem of data scarcity, we synthesize high-quality speech instruction datasets using a random-speaker text-to-speech model. Additionally, we design a mixed-modality training strategy to mitigate the inherent modality imbalance in pre-trained foundation models. Furthermore, we conduct a statistical analysis of the distribution of GUI grounding prediction errors and propose a training-free two-step grounding refinement method to alleviate minor localization deviations. Extensive experiments on multiple benchmarks demonstrate that UITron-Speech achieves robust performance and superior adaptability, underscoring the feasibility and potential of speech-driven GUI agents for more accessible and intelligent human-computer interaction. Our code and datasets are available at https://github.com/UITron-hub/UITron-Speech.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UITron-Speechï¼Œæ—¨åœ¨è§£å†³å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æ™ºèƒ½ä½“è¿‡åº¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤è€Œå¯¼è‡´çš„è¾…åŠ©åŠŸèƒ½å—é™é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åŒæ‰‹ä¸ä¾¿çš„æ“ä½œåœºæ™¯ä¸‹ã€‚ä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯(End-to-End)çš„GUIæ™ºèƒ½ä½“ï¼ŒUITron-Speechèƒ½ç›´æ¥å¤„ç†è¯­éŸ³æŒ‡ä»¤(Speech Instructions)å’Œè®¾å¤‡æˆªå›¾æ¥é¢„æµ‹ç”¨æˆ·æ“ä½œã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨éšæœºè¯´è¯äººçš„æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹åˆæˆé«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†æ··åˆæ¨¡æ€è®­ç»ƒç­–ç•¥æ¥ç¼“è§£é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡ã€‚é’ˆå¯¹å®šä½åå·®ï¼Œè¯¥ç ”ç©¶åœ¨ç»Ÿè®¡åˆ†æé¢„æµ‹è¯¯å·®åˆ†å¸ƒçš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ä¸¤æ­¥å®šä½ç»†åŒ–(Two-step Grounding Refinement)æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUITron-Speechåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½å’Œå“è¶Šçš„é€‚åº”æ€§ï¼ŒéªŒè¯äº†è¯­éŸ³é©±åŠ¨GUIæ™ºèƒ½ä½“åœ¨å®ç°æ›´å…·å¯è®¿é—®æ€§çš„äººæœºäº¤äº’æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11127v3",
      "published_date": "2025-06-10 12:16:27 UTC",
      "updated_date": "2025-11-26 07:48:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:41:10.551147+00:00"
    },
    {
      "arxiv_id": "2506.09095v4",
      "title": "Foundation Models in Medical Imaging: A Review and Outlook",
      "title_zh": "åŒ»å­¦å½±åƒåŸºç¡€æ¨¡å‹ï¼šç»¼è¿°ä¸å±•æœ›",
      "authors": [
        "Vivien van Veldhuizen",
        "Vanessa Botha",
        "Chunyao Lu",
        "Melis Erdal Cesur",
        "Kevin Groot Lipman",
        "Edwin D. de Jong",
        "Hugo Horlings",
        "ClÃ¡risa I. Sanchez",
        "Cees G. M. Snoek",
        "Lodewyk Wessels",
        "Ritse Mann",
        "Eric Marcus",
        "Jonas Teuwen"
      ],
      "abstract": "Foundation models (FMs) are changing the way medical images are analyzed by learning from large collections of unlabeled data. Instead of relying on manually annotated examples, FMs are pre-trained to learn general-purpose visual features that can later be adapted to specific clinical tasks with little additional supervision. In this review, we examine how FMs are being developed and applied in pathology, radiology, and ophthalmology, drawing on evidence from over 150 studies. We explain the core components of FM pipelines, including model architectures, self-supervised learning methods, and strategies for downstream adaptation. We also review how FMs are being used in each imaging domain and compare design choices across applications. Finally, we discuss key challenges and open questions to guide future research.",
      "tldr_zh": "æœ¬ç»¼è¿°æ¢è®¨äº†åŸºç¡€æ¨¡å‹ (Foundation Models, FMs) åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œé‡ç‚¹åˆ†æäº†å…¶å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®å­¦ä¹ é€šç”¨è§†è§‰ç‰¹å¾ä»¥é€‚åº”ç‰¹å®šä¸´åºŠä»»åŠ¡ã€‚ç ”ç©¶ç³»ç»Ÿå›é¡¾äº†æ¶µç›–ç—…ç†å­¦ (pathology)ã€æ”¾å°„å­¦ (radiology) å’Œçœ¼ç§‘å­¦ (ophthalmology) é¢†åŸŸçš„150å¤šé¡¹ç ”ç©¶ï¼Œæ·±å…¥å‰–æäº† FMs çš„å¼€å‘ä¸åº”ç”¨ç°çŠ¶ã€‚æ–‡ä¸­è¯¦ç»†è§£é‡Šäº† FM ç®¡çº¿çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ (model architectures)ã€è‡ªç›‘ç£å­¦ä¹  (self-supervised learning) æ–¹æ³•ä»¥åŠä¸‹æ¸¸é€‚é… (downstream adaptation) ç­–ç•¥ã€‚è¯¥ç»¼è¿°è¿˜å¯¹æ¯”äº†ä¸åŒåŒ»å­¦å½±åƒé¢†åŸŸçš„æ¨¡å‹è®¾è®¡é€‰æ‹©ï¼Œå¹¶æ¢è®¨äº† FMs åœ¨è·¨åº”ç”¨åœºæ™¯ä¸­çš„å…±æ€§ä¸å·®å¼‚ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å½“å‰åŒ»å­¦å½±åƒåŸºç¡€æ¨¡å‹é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ä¸å¼€æ”¾æ€§é—®é¢˜ï¼Œä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ä¸ä¸´åºŠè½åœ°æä¾›äº†å‰ç»æ€§æŒ‡å¯¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09095v4",
      "published_date": "2025-06-10 12:14:05 UTC",
      "updated_date": "2025-11-18 12:27:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:41:25.105055+00:00"
    },
    {
      "arxiv_id": "2506.08712v2",
      "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization",
      "title_zh": "ConfPOï¼šåå¥½ä¼˜åŒ–ä¸­åŸºäºç­–ç•¥æ¨¡å‹ç½®ä¿¡åº¦çš„å…³é”® Token é€‰æ‹©",
      "authors": [
        "Hee Suk Yoon",
        "Eunseop Yoon",
        "Mark Hasegawa-Johnson",
        "Sungwoong Kim",
        "Chang D. Yoo"
      ],
      "abstract": "We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ConfPOï¼Œä¸€ç§é’ˆå¯¹Large Language Modelsåå¥½å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨è®­ç»ƒç­–ç•¥æ¨¡å‹çš„confidenceæ¥è¯†åˆ«å¹¶ä¼˜åŒ–å¯¹é½è¿‡ç¨‹ä¸­æœ€å…³é”®çš„preference-critical tokensã€‚ä¸DPOç­‰ä¼ ç»Ÿçš„Direct Alignment Algorithmså‡åŒ€è°ƒæ•´æ‰€æœ‰tokenæ¦‚ç‡çš„åšæ³•ä¸åŒï¼ŒConfPOé€šè¿‡èšç„¦é«˜å½±å“åŠ›çš„tokenï¼Œæ›´é«˜æ•ˆåœ°åˆ©ç”¨KL divergenceé¢„ç®—ï¼Œä»è€Œåœ¨æå‡å¯¹é½è´¨é‡çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†overoptimizationå’Œreward hackingé—®é¢˜ã€‚è¯¥æ–¹æ³•å…·æœ‰ç®€å•ä¸”è½»é‡çš„ä¼˜åŠ¿ï¼Œæ— éœ€ä¾èµ–ä»»ä½•è¾…åŠ©æ¨¡å‹æˆ–é¢å¤–çš„è®¡ç®—èµ„æºï¼Œè§£å†³äº†ç°æœ‰æ ‡è®°çº§æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ä¸Šçš„å±€é™ã€‚åœ¨AlpacaEval 2å’ŒArena-Hardç­‰æŒ‘æˆ˜æ€§å¯¹é½åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¯æ˜ConfPOåœ¨å¤šç§ä¸åŒLLMsä¸Šå‡ä¸€è‡´æ€§åœ°ä¼˜äºä¼ ç»Ÿçš„å‡åŒ€å¯¹é½ç®—æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨é›¶é¢å¤–è®¡ç®—å¼€é”€ä¸‹å®ç°æ›´ç²¾å‡†çš„æ¨¡å‹å¯¹é½æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08712v2",
      "published_date": "2025-06-10 11:54:22 UTC",
      "updated_date": "2025-06-12 11:44:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:41:43.541907+00:00"
    },
    {
      "arxiv_id": "2506.08708v2",
      "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly",
      "title_zh": "PhyBlockï¼šåŸºäº 3D ç§¯æœ¨ç»„è£…çš„ç‰©ç†ç†è§£ä¸è§„åˆ’æ¸è¿›å¼åŸºå‡†",
      "authors": [
        "Liang Ma",
        "Jiajun Wen",
        "Min Lin",
        "Rongtao Xu",
        "Xiwen Liang",
        "Bingqian Lin",
        "Jun Ma",
        "Yongxin Wang",
        "Ziming Wei",
        "Haokun Lin",
        "Mingfei Han",
        "Meng Cao",
        "Bokui Chen",
        "Ivan Laptev",
        "Xiaodan Liang"
      ],
      "abstract": "While vision-language models (VLMs) have demonstrated promising capabilities in reasoning and planning for embodied agents, their ability to comprehend physical phenomena, particularly within structured 3D environments, remains severely limited. To close this gap, we introduce PhyBlock, a progressive benchmark designed to assess VLMs on physical understanding and planning through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level cognitive hierarchy assembly task alongside targeted Visual Question Answering (VQA) samples, collectively aimed at evaluating progressive spatial reasoning and fundamental physical comprehension, including object properties, spatial relationships, and holistic scene understanding. PhyBlock includes 2600 block tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three key dimensions: partial completion, failure diagnosis, and planning robustness. We benchmark 21 state-of-the-art VLMs, highlighting their strengths and limitations in physically grounded, multi-step planning. Our empirical findings indicate that the performance of VLMs exhibits pronounced limitations in high-level planning and reasoning capabilities, leading to a notable decline in performance for the growing complexity of the tasks. Error analysis reveals persistent difficulties in spatial orientation and dependency reasoning. Surprisingly, chain-of-thought prompting offers minimal improvements, suggesting spatial tasks heavily rely on intuitive model comprehension. We position PhyBlock as a unified testbed to advance embodied reasoning, bridging vision-language understanding and real-world physical problem-solving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhyBlockï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç‰©ç†ç†è§£ä¸è§„åˆ’èƒ½åŠ›çš„æ¸è¿›å¼åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æœºå™¨äºº3D block assemblyä»»åŠ¡è€ƒå¯Ÿæ¨¡å‹åœ¨ç»“æ„åŒ–3Dç¯å¢ƒä¸‹çš„ç‰©ç†è®¤çŸ¥ã€‚è¯¥åŸºå‡†åŒ…å«2600ä¸ªä»»åŠ¡ï¼Œæ•´åˆäº†å››çº§è®¤çŸ¥å±‚æ¬¡çš„ç»„è£…ä»»åŠ¡ä¸é’ˆå¯¹æ€§çš„Visual Question Answering(VQA)æ ·æœ¬ï¼Œé‡ç‚¹è¯„ä¼°ç©ºé—´æ¨ç†ã€ç‰©ä½“å±æ€§åŠåœºæ™¯ç†è§£ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹21ç§å‰æ²¿VLMsè¿›è¡Œäº†æµ‹è¯•ï¼Œåˆ†æå…¶åœ¨éƒ¨åˆ†å®Œæˆåº¦ã€æ•…éšœè¯Šæ–­å’Œè§„åˆ’é²æ£’æ€§ç­‰ç»´åº¦çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLMsåœ¨é«˜çº§è§„åˆ’å’Œæ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œæ€§èƒ½éšä»»åŠ¡å¤æ‚åº¦å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶åœ¨spatial orientationå’Œdependency reasoningæ–¹é¢é¢ä¸´æŒç»­æŒ‘æˆ˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒChain-of-Thought(CoT)æç¤ºå¯¹æ€§èƒ½æå‡å¾®ä¹å…¶å¾®ï¼Œè¡¨æ˜ç‰©ç†ç©ºé—´ä»»åŠ¡é«˜åº¦ä¾èµ–æ¨¡å‹çš„ç›´è§‚ç†è§£ã€‚PhyBlockä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•å¹³å°ï¼Œä¸ºå¼¥è¡¥è§†è§‰è¯­è¨€ç†è§£ä¸ç°å®ä¸–ç•Œç‰©ç†é—®é¢˜è§£å†³ä¹‹é—´çš„å·®è·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08708v2",
      "published_date": "2025-06-10 11:46:06 UTC",
      "updated_date": "2025-11-21 12:13:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:03.319780+00:00"
    },
    {
      "arxiv_id": "2506.09093v2",
      "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data",
      "title_zh": "æ™ºèƒ½åˆå¹¶ï¼Œå“è¶Šæ³›åŒ–ï¼šæå‡æ¨¡å‹åˆå¹¶åœ¨OODæ•°æ®ä¸Šçš„è¡¨ç°",
      "authors": [
        "Bingjie Zhang",
        "Hongkang Li",
        "Changlong Shi",
        "Guowei Rong",
        "He Zhao",
        "Dongsheng Wang",
        "Dandan Guo",
        "Meng Wang"
      ],
      "abstract": "Multi-task learning (MTL) concurrently trains a model on diverse task datasets to exploit common features, thereby improving overall performance across the tasks. Recent studies have dedicated efforts to merging multiple independent model parameters into a unified model for MTL, thus circumventing the need for training data and expanding the scope of applicable scenarios of MTL. However, current approaches to model merging predominantly concentrate on enhancing performance within in-domain (ID) datasets, often overlooking their efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV (Layer-wise Pruning Task Vector) by building a saliency score, measuring the redundancy of parameters in task vectors. Designed in this way ours can achieve mask vector for each task and thus perform layer-wise pruning on the task vectors, only keeping the pre-trained model parameters at the corresponding layer in merged model. Owing to its flexibility, our method can be seamlessly integrated with most of existing model merging methods to improve their performance on OOD tasks. Extensive experiments demonstrate that the application of our method results in substantial enhancements in OOD performance while preserving the ability on ID tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-task learningï¼‰ä¸­çš„æ¨¡å‹åˆå¹¶é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨åˆ†å¸ƒå†…ï¼ˆIn-domainï¼‰æ•°æ®çš„è¡¨ç°ï¼Œè€Œå¿½è§†äº†åœ¨åˆ†å¸ƒå¤–ï¼ˆOut-of-domain, OODï¼‰æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†LwPTVï¼ˆLayer-wise Pruning Task Vectorï¼‰ï¼Œé€šè¿‡æ„å»ºæ˜¾è‘—æ€§è¯„åˆ†æ¥è¡¡é‡ä»»åŠ¡å‘é‡ä¸­çš„å‚æ•°å†—ä½™ç¨‹åº¦ã€‚è¯¥æ–¹æ³•ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆæ©ç å‘é‡å¹¶æ‰§è¡Œé€å±‚å‰ªæï¼Œä»è€Œåœ¨åˆå¹¶æ¨¡å‹ä¸­ä»…ä¿ç•™å¯¹åº”å±‚å…³é”®çš„é¢„è®­ç»ƒå‚æ•°ã€‚LwPTVå…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å¤§å¤šæ•°ç°æœ‰çš„æ¨¡å‹åˆå¹¶æ–¹æ³•ä¸­ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒåˆ†å¸ƒå†…ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨OODä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Minor formatting adjustments; no changes to content",
      "pdf_url": "https://arxiv.org/pdf/2506.09093v2",
      "published_date": "2025-06-10 11:34:23 UTC",
      "updated_date": "2025-06-13 09:02:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:00.938295+00:00"
    },
    {
      "arxiv_id": "2506.08698v1",
      "title": "Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data",
      "title_zh": "åŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„ç”µåŠ›è´Ÿè·ç›‘æµ‹æ•°æ®é«˜æ•ˆè¡¨ç¤ºæ½œåœ¨ç‰¹å¾åˆ†ææ–¹æ³•",
      "authors": [
        "Boyu Xie",
        "Tangtang Xie"
      ],
      "abstract": "With the development of smart grids, High-Dimensional and Incomplete (HDI) Power Load Monitoring (PLM) data challenges the performance of Power Load Forecasting (PLF) models. In this paper, we propose a potential characterization model VAE-LF based on Variational Autoencoder (VAE) for efficiently representing and complementing PLM missing data. VAE-LF learns a low-dimensional latent representation of the data using an Encoder-Decoder structure by splitting the HDI PLM data into vectors and feeding them sequentially into the VAE-LF model, and generates the complementary data. Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark models in both 5% and 10% sparsity test cases, with significantly lower RMSE and MAE, and especially outperforms on low sparsity ratio data. The method provides an efficient data-completion solution for electric load management in smart grids.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ç”µç½‘ä¸­é«˜ç»´ä¸”ä¸å®Œæ•´ï¼ˆHDIï¼‰çš„ç”µåŠ›è´Ÿè·ç›‘æµ‹ï¼ˆPLMï¼‰æ•°æ®æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æ½œåœ¨ç‰¹å¾åˆ†ææ¨¡å‹ VAE-LFã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰ç»“æ„ï¼Œé€šè¿‡å°† HDI PLM æ•°æ®åˆ†å‰²ä¸ºå‘é‡å¹¶æŒ‰é¡ºåºè¾“å…¥ï¼Œæ—¨åœ¨å­¦ä¹ æ•°æ®çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºå¹¶å®ç°é«˜æ•ˆçš„æ•°æ®è¡¥å…¨ã€‚åœ¨ UK-DALE æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVAE-LF åœ¨ 5% å’Œ 10% çš„ç¨€ç–åº¦æµ‹è¯•æ¡ˆä¾‹ä¸­è¡¨ç°å‡ä¼˜äºå…¶ä»–åŸºå‡†æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å’Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½ç¨€ç–ç‡æ•°æ®å¤„ç†ä¸Šï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæ™ºèƒ½ç”µç½‘çš„ç”µåŠ›è´Ÿè·ç®¡ç†æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®è¡¥å…¨è§£å†³æ–¹æ¡ˆï¼Œå¢å¼ºäº†è´Ÿè·é¢„æµ‹æ¨¡å‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08698v1",
      "published_date": "2025-06-10 11:26:03 UTC",
      "updated_date": "2025-06-10 11:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:02.647906+00:00"
    },
    {
      "arxiv_id": "2506.09092v1",
      "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels",
      "title_zh": "CUDA-LLMï¼šå¤§è¯­è¨€æ¨¡å‹å¯ç¼–å†™é«˜æ•ˆ CUDA æ ¸å‡½æ•°",
      "authors": [
        "Wentao Chen",
        "Jiace Zhu",
        "Qi Fan",
        "Yehan Ma",
        "An Zou"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in general-purpose code generation. However, generating the code which is deeply hardware-specific, architecture-aware, and performance-critical, especially for massively parallel GPUs, remains a complex challenge. In this work, we explore the use of LLMs for the automated generation and optimization of CUDA programs, with the goal of producing high-performance GPU kernels that fully exploit the underlying hardware. To address this challenge, we propose a novel framework called \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes compilation and functional correctness, as well as the runtime performance, which are validated through extensive and diverse test cases, and measured by actual kernel execution latency on the target GPU, respectively. This approach enables LLMs not only to generate syntactically and semantically correct CUDA code but also to iteratively refine it for efficiency, tailored to the characteristics of the GPU architecture. We evaluate FSR on representative CUDA kernels, covering AI workloads and computational intensive algorithms. Our results show that LLMs augmented with FSR consistently guarantee correctness rates. Meanwhile, the automatically generated kernels can outperform general human-written code by a factor of up to 179$\\times$ in execution speeds. These findings highlight the potential of combining LLMs with performance reinforcement to automate GPU programming for hardware-specific, architecture-sensitive, and performance-critical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨ç”Ÿæˆé«˜åº¦ç¡¬ä»¶ç›¸å…³å’Œæ¶æ„æ„ŸçŸ¥çš„ CUDA é«˜æ€§èƒ½ä»£ç æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åˆ©ç”¨ LLMs è‡ªåŠ¨ç”Ÿæˆå’Œä¼˜åŒ– GPU kernel çš„æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º Feature Search and Reinforcement (FSR) çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å…±åŒä¼˜åŒ–ä»£ç çš„ç¼–è¯‘ã€åŠŸèƒ½æ­£ç¡®æ€§ä»¥åŠå®é™…è¿è¡Œæ€§èƒ½ã€‚FSR é€šè¿‡å¤§é‡æµ‹è¯•ç”¨ä¾‹éªŒè¯åŠŸèƒ½ï¼Œå¹¶åˆ©ç”¨ç›®æ ‡ GPU ä¸Šçš„å®é™…æ‰§è¡Œå»¶è¿Ÿä½œä¸ºåé¦ˆï¼Œå¼•å¯¼ LLMs é’ˆå¯¹ç‰¹å®š GPU æ¶æ„ç‰¹æ€§ä¸æ–­è¿­ä»£å’Œç²¾ç»†åŒ–ä¼˜åŒ–ä»£ç ã€‚å®éªŒåœ¨æ¶µç›– AI å·¥ä½œè´Ÿè½½å’Œè®¡ç®—å¯†é›†å‹ç®—æ³•çš„ä»£è¡¨æ€§ CUDA kernel ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç»“åˆ FSR çš„ LLMs èƒ½å¤Ÿä¸€è‡´åœ°ä¿è¯ä»£ç æ­£ç¡®ç‡ã€‚é‡è¦çš„æ˜¯ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„ kernel åœ¨æ‰§è¡Œé€Ÿåº¦ä¸Šæœ€é«˜å¯æ¯”æ™®é€šäººç±»ç¼–å†™çš„ä»£ç æå‡ 179 å€ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†å°† LLMs ä¸æ€§èƒ½å¢å¼ºæŠ€æœ¯ç›¸ç»“åˆåœ¨è‡ªåŠ¨åŒ–å¤„ç†ç¡¬ä»¶ç‰¹å®šä¸”æ€§èƒ½å…³é”®çš„ GPU ç¼–ç¨‹ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09092v1",
      "published_date": "2025-06-10 10:51:03 UTC",
      "updated_date": "2025-06-10 10:51:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:06.812232+00:00"
    },
    {
      "arxiv_id": "2506.08672v1",
      "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling",
      "title_zh": "RuleReasonerï¼šåŸºäºé¢†åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·çš„å¼ºåŒ–è§„åˆ™æ¨ç†",
      "authors": [
        "Yang Liu",
        "Jiaqi Li",
        "Zilong Zheng"
      ],
      "abstract": "Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($Î”$4.1% average points on eight ID tasks and $Î”$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RuleReasonerï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå°æ¨ç†æ¨¡å‹ï¼ˆSRMsï¼‰åŸºäºè§„åˆ™æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œä»¥åº”å¯¹çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è§„åˆ™æ ¼å¼ã€ç±»å‹å’Œå¤æ‚æ€§å¸¦æ¥çš„ä¸¥å³»æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆä¸€ç³»åˆ—ç­–åˆ’ä»»åŠ¡ï¼Œå¹¶å¼•å…¥åˆ›æ–°çš„é¢†åŸŸæ„ŸçŸ¥åŠ¨æ€é‡‡æ ·ï¼ˆdomain-aware dynamic samplingï¼‰æŠ€æœ¯ï¼Œæ ¹æ®å†å²å¥–åŠ±åŠ¨æ€æ›´æ–°å„é¢†åŸŸçš„é‡‡æ ·æƒé‡ã€‚è¿™ä¸€æœºåˆ¶ä¸ä»…å®ç°äº†é¢†åŸŸçš„è‡ªåŠ¨å¢å¼ºï¼Œè¿˜æä¾›äº†çµæ´»çš„åœ¨çº¿å­¦ä¹ è°ƒåº¦ï¼Œä»è€Œå…å»äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­å¤æ‚çš„äººå·¥æ··åˆè®­ç»ƒæ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRuleReasoneråœ¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾è‘—ä¼˜äºOpenAI-o1ç­‰å‰æ²¿å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¹³å‡å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†4.1%å’Œ10.4%ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„åŠ¨æ€é‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å±•ç°å‡ºäº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 10 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.08672v1",
      "published_date": "2025-06-10 10:31:21 UTC",
      "updated_date": "2025-06-10 10:31:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:08.620221+00:00"
    },
    {
      "arxiv_id": "2506.08669v1",
      "title": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search",
      "title_zh": "é€šè¿‡è“å›¾ä¸æç¤ºæ¨¡æ¿æœç´¢å¢å¼ºå°è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Dongge Han",
        "Menglin Xia",
        "Daniel Madrigal Diaz",
        "Samuel Kessler",
        "Ankur Mallick",
        "Xuchao Zhang",
        "Mirian Del Carmen Hipolito Garcia",
        "Jin Xu",
        "Victor RÃ¼hle",
        "Saravan Rajmohan"
      ],
      "abstract": "Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨æå‡å°è¯­è¨€æ¨¡å‹ (Small Language Models, SLMs) æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç”±å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ç”Ÿæˆçš„æ¨ç†è“å›¾ (blueprints)ï¼Œä¸º SLMs æä¾›ç»“æ„åŒ–çš„é«˜å±‚æ¨ç†æŒ‡å—ï¼Œå¸®åŠ©å…¶ç³»ç»Ÿåœ°è§£å†³ç›¸å…³é—®é¢˜ã€‚åŒæ—¶ï¼Œæ¡†æ¶é›†æˆäº†æç¤ºè¯æ¨¡æ¿æœç´¢ (prompt template search) æœºåˆ¶ï¼Œä»¥ç¼“è§£ SLMs å¯¹æç¤ºè¯å˜ä½“çš„é«˜åº¦æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦ (GSM8K)ã€ç¼–ç¨‹ (MBPP) å’Œé€»è¾‘æ¨ç† (BBH) ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—å¢å¼ºäº† SLMs çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•æ— éœ€å¢åŠ æ¨¡å‹å‚æ•°æˆ–è¿›è¡Œé¢å¤–è®­ç»ƒï¼Œä¸ºç«¯ä¾§è®¾å¤‡æˆ–èµ„æºå—é™ç¯å¢ƒæä¾›äº†ä¸€ç§è½»é‡ä¸”æ˜“äºéƒ¨ç½²çš„æ¨ç†å¢å¼ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "TTODLer-FM Workshop@ICML'25 (Tiny Titans: The next wave of On-Device Learning for Foundational Models)",
      "pdf_url": "https://arxiv.org/pdf/2506.08669v1",
      "published_date": "2025-06-10 10:30:43 UTC",
      "updated_date": "2025-06-10 10:30:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:30.261039+00:00"
    },
    {
      "arxiv_id": "2506.08662v1",
      "title": "Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization",
      "title_zh": "é’ˆå¯¹æ ‡é‡é‡åŒ–ä¸ç†µçº¦æŸé‡åŒ–çš„å­¦ä¹ å‹å›¾åƒå‹ç¼©ä¼˜åŒ–",
      "authors": [
        "Florian Borzechowski",
        "Michael SchÃ¤fer",
        "Heiko Schwarz",
        "Jonathan Pfaff",
        "Detlev Marpe",
        "Thomas Wiegand"
      ],
      "abstract": "The continuous improvements on image compression with variational autoencoders have lead to learned codecs competitive with conventional approaches in terms of rate-distortion efficiency. Nonetheless, taking the quantization into account during the training process remains a problem, since it produces zero derivatives almost everywhere and needs to be replaced with a differentiable approximation which allows end-to-end optimization. Though there are different methods for approximating the quantization, none of them model the quantization noise correctly and thus, result in suboptimal networks. Hence, we propose an additional finetuning training step: After conventional end-to-end training, parts of the network are retrained on quantized latents obtained at the inference stage. For entropy-constraint quantizers like Trellis-Coded Quantization, the impact of the quantizer is particularly difficult to approximate by rounding or adding noise as the quantized latents are interdependently chosen through a trellis search based on both the entropy model and a distortion measure. We show that retraining on correctly quantized data consistently yields additional coding gain for both uniform scalar and especially for entropy-constraint quantization, without increasing inference complexity. For the Kodak test set, we obtain average savings between 1% and 2%, and for the TecNick test set up to 2.2% in terms of BjÃ¸ntegaard-Delta bitrate.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹åŸºäºVariational Autoencodersçš„å­¦ä¹ å›¾åƒå‹ç¼©ä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„Quantizationä¸å¯å¯¼é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å¸¸ä½¿ç”¨å¯å¯¼è¿‘ä¼¼æ¥æ›¿ä»£é‡åŒ–ï¼Œä½†ç”±äºæ— æ³•å‡†ç¡®å»ºæ¨¡é‡åŒ–å™ªå£°ï¼Œå¯¼è‡´ç½‘ç»œæ€§èƒ½å—é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†Trellis-Coded Quantizationç­‰å¤æ‚çš„ç†µçº¦æŸé‡åŒ–å™¨æ—¶æ•ˆæœæ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§é¢å¤–çš„Finetuningè®­ç»ƒæ­¥éª¤ï¼Œå³åœ¨ç«¯åˆ°ç«¯è®­ç»ƒå®Œæˆåï¼Œåˆ©ç”¨æ¨ç†é˜¶æ®µè·å–çš„çœŸå®Quantized Latentså¯¹ç½‘ç»œéƒ¨åˆ†å‚æ•°è¿›è¡Œé‡è®­ç»ƒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡Uniform Scalar Quantizationå’Œç†µçº¦æŸé‡åŒ–çš„ç¼–ç å¢ç›Šï¼Œä¸”å®Œå…¨ä¸å¢åŠ æ¨ç†é˜¶æ®µçš„å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Kodakæµ‹è¯•é›†ä¸Šå¯å®ç°1%è‡³2%çš„å¹³å‡ç ç‡èŠ‚çœï¼Œåœ¨TecNickæµ‹è¯•é›†ä¸ŠæŒ‰BjÃ¸ntegaard-Deltaæ¯”ç‰¹ç‡è®¡ç®—æœ€é«˜å¯èŠ‚çœ2.2%ï¼Œä¸ºè¿›ä¸€æ­¥æå‡å­¦ä¹ å›¾åƒå‹ç¼©çš„æ•ˆç‡æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICIP2024, the IEEE International Conference on Image Processing",
      "pdf_url": "https://arxiv.org/pdf/2506.08662v1",
      "published_date": "2025-06-10 10:22:22 UTC",
      "updated_date": "2025-06-10 10:22:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:26.774669+00:00"
    },
    {
      "arxiv_id": "2506.08660v2",
      "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness",
      "title_zh": "é¢å‘é²æ£’çš„ç°å®ä¸–ç•Œå¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼šé’ˆå¯¹ä¾èµ–æ€§ã€å¼‚æ­¥æ€§ä¸ç¼ºå¤±é—®é¢˜çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Jinkwan Jang",
        "Hyungjin Park",
        "Jinmyeong Choi",
        "Taesup Kim"
      ],
      "abstract": "Real-world time series data are inherently multivariate, often exhibiting complex inter-channel dependencies. Each channel is typically sampled at its own period and is prone to missing values due to various practical and operational constraints. These characteristics pose three fundamental challenges involving channel dependency, sampling asynchrony, and missingness, all of which must be addressed simultaneously to enable robust and reliable forecasting in practical settings. However, existing architectures typically address only parts of these challenges in isolation and still rely on simplifying assumptions, leaving unresolved the combined challenges of asynchronous channel sampling, test-time missing blocks, and intricate inter-channel dependencies. To bridge this gap, we propose ChannelTokenFormer, a Transformer-based forecasting framework with a flexible architecture designed to explicitly capture cross-channel interactions, accommodate channel-wise asynchronous sampling, and effectively handle missing values. Extensive experiments on public benchmark datasets reflecting practical settings, along with one private real-world industrial dataset, demonstrate the superior robustness and accuracy of ChannelTokenFormer under challenging real-world conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œä¸­å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹é¢ä¸´çš„é€šé“ä¾èµ–æ€§(channel dependency)ã€é‡‡æ ·å¼‚æ­¥æ€§(sampling asynchrony)å’Œç¼ºå¤±å€¼(missingness)ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ChannelTokenFormeræ¡†æ¶ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸åªèƒ½å­¤ç«‹åœ°è§£å†³éƒ¨åˆ†é—®é¢˜ä¸”é«˜åº¦ä¾èµ–ç®€åŒ–å‡è®¾ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„å¼‚æ­¥é‡‡æ ·å’Œæµ‹è¯•æœŸæ•°æ®å—ç¼ºå¤±ã€‚ChannelTokenFormerä½œä¸ºä¸€ç§åŸºäºTransformerçš„çµæ´»æ¶æ„ï¼Œèƒ½å¤Ÿæ˜¾å¼æ•æ‰è·¨é€šé“äº¤äº’å¹¶åŒæ—¶æœ‰æ•ˆå¤„ç†å¼‚æ­¥é‡‡æ ·ä¸ç¼ºå¤±æ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†å’Œä¸€ä¸ªçœŸå®å·¥ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœè¯æ˜ï¼ŒChannelTokenFormeråœ¨åº”å¯¹å¤æ‚çš„ç°å®åœºæ™¯æ—¶å±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œæœ‰æ•ˆå¡«è¡¥äº†åŒæ—¶è§£å†³ä¸Šè¿°ä¸‰å¤§ç»¼åˆæ€§æŒ‘æˆ˜çš„æŠ€æœ¯ç©ºç™½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08660v2",
      "published_date": "2025-06-10 10:21:28 UTC",
      "updated_date": "2025-09-27 05:49:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:29.286009+00:00"
    },
    {
      "arxiv_id": "2506.08652v1",
      "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset",
      "title_zh": "JoFormerï¼ˆåŸºäºæ—…ç¨‹çš„ Transformerï¼‰ï¼šåŸºäº Tiny Shakespeare æ•°æ®é›†çš„ç†è®ºä¸å®è¯åˆ†æ",
      "authors": [
        "Mahesh Godavarti"
      ],
      "abstract": "Transformers have demonstrated remarkable success in sequence modeling, yet effectively incorporating positional information remains a challenging and active area of research. In this paper, we introduce JoFormer, a journey-based Transformer architecture grounded in a recently proposed non-commutative algebra for composing transformations across positions. JoFormer represents relative positions through learnable directional transforms that are sequentially composed along the input, thereby extending and generalizing existing approaches based on relative position representations. We derive the JoFormer attention mechanism from first principles and show that it subsumes standard methods such as rotary transformations as special cases. To evaluate its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny Shakespeare character-level language modeling task. Our results demonstrate that\n  JoFormer consistently achieves lower perplexity and faster convergence, highlighting the advantages of its more expressive, journey-based treatment of position. Notably, the per-token JoFormer is still a primitive, conceptual variant with layer-independent angles, yet it already demonstrates strong performance-underscoring its promise as a proof of concept for more expressive architectures. We conclude by discussing how JoFormer offers a principled approach to integrating positional structure into Transformer architectures. The code used in this work is available at https://github.com/mahesh-godavarti/joformer.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† JoFormer (Journey-based Transformer)ï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ– Transformer æ¶æ„ä¸­ä½ç½®ä¿¡æ¯é›†æˆçš„åˆ›æ–°æ¡†æ¶ã€‚JoFormer åŸºäºéäº¤æ¢ä»£æ•°ç†è®ºï¼Œé€šè¿‡åœ¨è¾“å…¥åºåˆ—ä¸­é¡ºåºç»„åˆå¯å­¦ä¹ çš„æ–¹å‘å˜æ¢æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®ï¼Œä»è€Œæ¨å¹¿å¹¶æ¦‚æ‹¬äº†ç°æœ‰çš„ç›¸å¯¹ä½ç½®è¡¨ç¤ºæ–¹æ³•ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒJoFormer çš„æ³¨æ„åŠ›æœºåˆ¶ä»åŸºæœ¬åŸç†æ¨å¯¼è€Œæ¥ï¼Œå¹¶å°† Rotary Transformations ç­‰æ ‡å‡†æ–¹æ³•è§†ä¸ºå…¶ç‰¹æ®Šæƒ…å†µã€‚åœ¨ Tiny Shakespeare å­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒJoFormer ç›¸æ¯” RoFormer åŸºçº¿å±•ç°å‡ºæ›´ä½çš„ Perplexity å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å°½ç®¡ç›®å‰ä½œä¸ºä¸€ç§æ¦‚å¿µéªŒè¯ç‰ˆæœ¬ï¼Œä»…ä½¿ç”¨äº†å±‚ç‹¬ç«‹çš„è§’åº¦è®¾ç½®ï¼Œä½†å…¶è¡¨ç°å‡ºçš„å¼ºåŠ²æ€§èƒ½è¯æ˜äº† Journey-based ä½ç½®å¤„ç†æ–¹æ¡ˆçš„æ½œåŠ›å’Œä¼˜è¶Šæ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ Transformer æ¨¡å‹ä¸­æ•´åˆä½ç½®ç»“æ„æä¾›äº†ä¸€ç§å…·æœ‰åŸåˆ™æ€§çš„ç§‘å­¦æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08652v1",
      "published_date": "2025-06-10 10:05:29 UTC",
      "updated_date": "2025-06-10 10:05:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:06.096998+00:00"
    },
    {
      "arxiv_id": "2506.11125v1",
      "title": "ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams",
      "title_zh": "ASRJamï¼šç”¨äºé˜²èŒƒè‡ªåŠ¨åŒ–ç”µè¯è¯ˆéª—çš„äººæ€§åŒ– AI è¯­éŸ³å¹²æ‰°æŠ€æœ¯",
      "authors": [
        "Freddie Grabovski",
        "Gilad Gressel",
        "Yisroel Mirsky"
      ],
      "abstract": "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€æ–‡æœ¬è½¬è¯­éŸ³ (TTS) å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) æŠ€æœ¯å¼•å‘çš„è‡ªåŠ¨åŒ–è¯­éŸ³é’“é±¼ (vishing) å¨èƒï¼Œè¯†åˆ«å‡º ASR è½¬å½•ç¯èŠ‚æ˜¯æ­¤ç±»è¯ˆéª—é“¾æ¡ä¸­æœ€è–„å¼±çš„ç¯èŠ‚ã€‚ç ”ç©¶æå‡ºäº† ASRJamï¼Œè¿™æ˜¯ä¸€ç§ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡åœ¨å—å®³è€…çš„éŸ³é¢‘ä¸­æ³¨å…¥å¯¹æŠ—æ€§æ‰°åŠ¨æ¥å¹²æ‰°æ”»å‡»è€…çš„ ASR ç³»ç»Ÿï¼Œä»è€Œåœ¨ä¸å½±å“äººç±»é€šè¯ç†è§£çš„å‰æä¸‹æ‰“ç ´è¯ˆéª—åé¦ˆå¾ªç¯ã€‚ä¸ºäº†æå‡å®æ—¶åº”ç”¨çš„å¯è¡Œæ€§ä¸å¬æ„Ÿï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº† EchoGuardï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ··å“ (reverberation) å’Œå›å£° (echo) ç­‰è‡ªç„¶å¤±çœŸç°è±¡çš„æ–°å‹å¹²æ‰°å™¨ã€‚EchoGuard çš„è®¾è®¡åˆ©ç”¨äº†è¿™äº›å¤±çœŸå¯¹ ASR å…·æœ‰æ˜¾è‘—ç ´åæ€§ä½†å¯¹äººç±»æ„Ÿå®˜ç›¸å¯¹å‹å¥½çš„ç‰¹æ€§ã€‚é€šè¿‡å¯¹ 39 åå—è¯•è€…çš„å®éªŒè¯„ä¼°å¹¶ä¸ä¸‰ç§å°–ç«¯æŠ€æœ¯å¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤º EchoGuard åœ¨ ASR å¹²æ‰°æ•ˆæœä¸äººç±»å¬è§‰ä½“éªŒä¹‹é—´å–å¾—äº†æœ€ä½³å¹³è¡¡ï¼Œå±•ç°å‡ºæé«˜çš„ç»¼åˆåº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11125v1",
      "published_date": "2025-06-10 10:04:23 UTC",
      "updated_date": "2025-06-10 10:04:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:42:57.709201+00:00"
    },
    {
      "arxiv_id": "2506.08647v1",
      "title": "Summarization for Generative Relation Extraction in the Microbiome Domain",
      "title_zh": "é¢å‘å¾®ç”Ÿç‰©ç»„é¢†åŸŸç”Ÿæˆå¼å…³ç³»æŠ½å–çš„æ–‡æœ¬æ‘˜è¦ç ”ç©¶",
      "authors": [
        "Oumaima El Khettari",
        "Solen Quiniou",
        "Samuel Chaffron"
      ],
      "abstract": "We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†ä¸€ç§ä¸“ä¸ºè‚ é“å¾®ç”Ÿç‰©ç¾¤ (Microbiome) äº¤äº’ç ”ç©¶å®šåˆ¶çš„ç”Ÿæˆå¼å…³ç³»æŠ½å– (Generative Relation Extraction, RE) æµæ°´çº¿ï¼Œä»¥åº”å¯¹è¯¥é¢†åŸŸå¤æ‚ä¸”ä½èµ„æº (Low-resource) çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ‘˜è¦åŠŸèƒ½åœ¨æ‰§è¡ŒæŒ‡ä»¤å¾®è°ƒ (Instruction-tuned) ç”Ÿæˆæå–å‰ç²¾ç‚¼ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘æ–‡æœ¬å™ªå£°ã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œæ‘˜è¦ç”Ÿæˆæ­¥éª¤èƒ½æ˜¾è‘—æ”¹å–„ç”Ÿæˆå¼ RE çš„æ¨¡å‹è¡¨ç°å¹¶æä¾›æ›´å¥½çš„å¼•å¯¼ã€‚è™½ç„¶å®éªŒå‘ç°åŸºäº BERT çš„å…³ç³»æŠ½å–æ–¹æ³•ç›®å‰ä»ä¼˜äºç”Ÿæˆå¼æ¨¡å‹ï¼Œä½†è¯¥å·¥ä½œå±•ç¤ºäº†ç”Ÿæˆå¼æŠ€æœ¯åœ¨æ”¯æŒç‰¹å®šä¸“ä¸šé¢†åŸŸç ”ç©¶æ–¹é¢çš„æ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶ä¸ºåœ¨æ•°æ®ç¨€ç¼ºçš„ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒä¸‹åº”ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08647v1",
      "published_date": "2025-06-10 09:58:23 UTC",
      "updated_date": "2025-06-10 09:58:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:32.352924+00:00"
    },
    {
      "arxiv_id": "2506.08646v1",
      "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning",
      "title_zh": "TableDreamerï¼šé¢å‘è¡¨æ ¼æŒ‡ä»¤å¾®è°ƒçš„ä»é›¶å¼€å§‹çš„æ¸è¿›å¼ä¸å¼±ç‚¹å¼•å¯¼æ•°æ®åˆæˆ",
      "authors": [
        "Mingyu Zheng",
        "Zhifan Feng",
        "Jia Wang",
        "Lanrui Wang",
        "Zheng Lin",
        "Yang Hao",
        "Weiping Wang"
      ],
      "abstract": "Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TableDreamerï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¡¨æ ¼æŒ‡ä»¤å¾®è°ƒ (Table Instruction Tuning) è®¾è®¡çš„æ¸è¿›å¼ä¸”å—å¼±ç‚¹å¼•å¯¼çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ•°æ®å¤šæ ·æ€§å’Œåˆæˆæ•ˆç‡æ–¹é¢çš„ä¸è¶³ã€‚ç°æœ‰çš„åŸºäº LLM çš„æ•°æ®åˆæˆæ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†æ¢ç´¢è¡¨æ ¼ç†è§£ä»»åŠ¡çš„å¹¿é˜”è¾“å…¥ç©ºé—´ï¼Œä¸”å¿½ç•¥äº†ç›®æ ‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„å¼±ç‚¹ã€‚TableDreamer é¦–å…ˆä»é›¶å¼€å§‹åˆæˆå¤šæ ·åŒ–çš„è¡¨æ ¼åŠå…¶ç›¸å…³æŒ‡ä»¤ä½œä¸ºç§å­æ•°æ® (Seed Data)ï¼Œéšåæ ¹æ®æ–°è¯†åˆ«å‡ºçš„æ¨¡å‹å¼±ç‚¹æ•°æ®å¯¹è¾“å…¥ç©ºé—´è¿›è¡Œè¿­ä»£æ¢ç´¢ã€‚è¯¥è¿‡ç¨‹æœ€ç»ˆç”Ÿæˆçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®å¯æ˜¾è‘—æå‡ç›®æ ‡æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ 10 ä¸ªè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä½¿ Llama3.1-8B-instruct çš„å¹³å‡å‡†ç¡®ç‡æå‡äº† 11.62%ã€‚å³ä¾¿ä»…ä½¿ç”¨ 27K æ¡ GPT-4o åˆæˆæ•°æ®ï¼ŒTableDreamer çš„è¡¨ç°ä¹Ÿè¶…è¶Šäº†ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®çš„ç°æœ‰æœ€å…ˆè¿› (State-of-the-art) åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 19 figures, Findings of ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08646v1",
      "published_date": "2025-06-10 09:57:59 UTC",
      "updated_date": "2025-06-10 09:57:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:36.640351+00:00"
    },
    {
      "arxiv_id": "2506.10030v1",
      "title": "Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment",
      "title_zh": "RAG-as-a-Service ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€çŸ¥è¯†ç‰ˆæƒä¿æŠ¤",
      "authors": [
        "Tianyu Chen",
        "Jian Lou",
        "Wenjie Wang"
      ],
      "abstract": "As Retrieval-Augmented Generation (RAG) evolves into service-oriented platforms (Rag-as-a-Service) with shared knowledge bases, protecting the copyright of contributed data becomes essential. Existing watermarking methods in RAG focus solely on textual knowledge, leaving image knowledge unprotected. In this work, we propose AQUA, the first watermark framework for image knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals into synthetic images using two complementary methods: acronym-based triggers and spatial relationship cues. These techniques ensure watermark signals survive indirect watermark propagation from image retriever to textual generator, being efficient, effective and imperceptible. Experiments across diverse models and datasets show that AQUA enables robust, stealthy, and reliable copyright tracing, filling a key gap in multimodal RAG protection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡åŒ–ï¼ˆRag-as-a-Serviceï¼‰ç¯å¢ƒä¸‹çš„æ•°æ®ç‰ˆæƒä¿æŠ¤é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºå¤šæ¨¡æ€ Multimodal RAG ç³»ç»Ÿä¸­å›¾åƒçŸ¥è¯†ä¿æŠ¤çš„æ°´å°æ¡†æ¶ AQUAã€‚ç”±äºç°æœ‰çš„ RAG æ°´å°æ–¹æ³•ä»…èšç„¦äºæ–‡æœ¬çŸ¥è¯†ï¼ŒAQUA é€šè¿‡é¦–å­—æ¯ç¼©ç•¥è¯è§¦å‘å™¨ï¼ˆacronym-based triggersï¼‰å’Œç©ºé—´å…³ç³»çº¿ç´¢ï¼ˆspatial relationship cuesï¼‰ä¸¤ç§äº’è¡¥æ–¹å¼å°†è¯­ä¹‰ä¿¡å·åµŒå…¥åˆæˆå›¾åƒä¸­ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†æ°´å°ä¿¡å·åœ¨ä»å›¾åƒæ£€ç´¢å™¨ï¼ˆimage retrieverï¼‰åˆ°æ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆtextual generatorï¼‰çš„é—´æ¥ä¼ æ’­è¿‡ç¨‹ä¸­ä¾ç„¶å­˜ç»­ï¼Œä¸”å…·å¤‡é«˜æ•ˆã€ç¨³å¥å’Œä¸å¯æ„ŸçŸ¥çš„ç‰¹æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAQUA åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡èƒ½å®ç°å¯é çš„ç‰ˆæƒè¿½è¸ªï¼Œæœ‰æ•ˆå¡«è¡¥äº†å¤šæ¨¡æ€çŸ¥è¯†åº“ç‰ˆæƒä¿æŠ¤çš„æŠ€æœ¯ç©ºç™½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10030v1",
      "published_date": "2025-06-10 09:56:02 UTC",
      "updated_date": "2025-06-10 09:56:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:25.351636+00:00"
    },
    {
      "arxiv_id": "2506.08641v2",
      "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers",
      "title_zh": "é¢„è®­ç»ƒ Vision Transformer ä¸­æ½œè—çš„ç”¨äºåˆ†ç±»çš„æ—¶é—´åºåˆ—è¡¨ç¤º",
      "authors": [
        "Simon Roschmann",
        "Quentin Bouniot",
        "Vasilii Feofanov",
        "Ievgen Redko",
        "Zeynep Akata"
      ],
      "abstract": "Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal a new direction for reusing vision representations in a non-visual domain. Code is available at https://github.com/ExplainableML/TiViT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TiViT (Time Vision Transformer) æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„ Vision Transformers (ViTs) è§£å†³æ—¶é—´åºåˆ—æ•°æ®é›†ç¨€ç¼ºå¯¼è‡´çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (TSFMs) å‘å±•å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ—¶é—´åºåˆ—è½¬åŒ–ä¸ºå›¾åƒï¼Œæœ‰æ•ˆåˆ©ç”¨åœ¨å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒä¸”å‚æ•°å†»ç»“çš„ ViTs æ‰€å…·å¤‡çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒViTs çš„ 2D patching å¤„ç†æ–¹å¼èƒ½å¢åŠ ä¸æ ‡ç­¾ç›¸å…³çš„ token æ•°é‡å¹¶é™ä½æ ·æœ¬å¤æ‚åº¦ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œåˆ©ç”¨å¤§è§„æ¨¡ OpenCLIP æ¨¡å‹çš„éšè—è¡¨ç¤ºï¼ŒTiViT åœ¨æ ‡å‡†æ—¶é—´åºåˆ—åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œå…·æœ‰é«˜æœ¬å¾ç»´åº¦ (intrinsic dimension) çš„ä¸­é—´å±‚åœ¨æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼ŒTiViT ä¸ç°æœ‰çš„ TSFMs è¡¨ç°å‡ºå¼ºçƒˆçš„äº’è¡¥æ€§ï¼Œç»“åˆäºŒè€…çš„ç‰¹å¾å¯è·å¾—æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨éè§†è§‰é¢†åŸŸå¤ç”¨è§†è§‰è¡¨å¾å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.08641v2",
      "published_date": "2025-06-10 09:54:51 UTC",
      "updated_date": "2025-07-02 10:32:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:39.227289+00:00"
    },
    {
      "arxiv_id": "2506.08634v1",
      "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback",
      "title_zh": "MOSAIC-Fï¼šé€šè¿‡ä¸ªæ€§åŒ–åé¦ˆæå‡å­¦ç”Ÿå£å¤´è¡¨è¾¾èƒ½åŠ›çš„æ¡†æ¶",
      "authors": [
        "Alvaro Becerra",
        "Daniel Andres",
        "Pablo Villegas",
        "Roberto Daza",
        "Ruth Cobos"
      ],
      "abstract": "In this article, we present a novel multimodal feedback framework called MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI), and Collaborative assessments for generating personalized feedback on student learning activities. This framework consists of four key steps. First, peers and professors' assessments are conducted through standardized rubrics (that include both quantitative and qualitative evaluations). Second, multimodal data are collected during learning activities, including video recordings, audio capture, gaze tracking, physiological signals (heart rate, motion data), and behavioral interactions. Third, personalized feedback is generated using AI, synthesizing human-based evaluations and data-based multimodal insights such as posture, speech patterns, stress levels, and cognitive load, among others. Finally, students review their own performance through video recordings and engage in self-assessment and feedback visualization, comparing their own evaluations with peers and professors' assessments, class averages, and AI-generated recommendations. By combining human-based and data-based evaluation techniques, this framework enables more accurate, personalized and actionable feedback. We tested MOSAIC-F in the context of improving oral presentation skills.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOSAIC-F æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†å¤šæ¨¡æ€å­¦ä¹ åˆ†æ (Multimodal Learning Analytics, MMLA)ã€è§‚å¯Ÿã€ä¼ æ„Ÿå™¨ã€äººå·¥æ™ºèƒ½ (AI) ä»¥åŠåä½œè¯„ä¼°çš„åˆ›æ–°åé¦ˆæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å­¦ç”Ÿçš„å­¦ä¹ æˆæ•ˆã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªå…³é”®æ­¥éª¤ï¼Œé¦–å…ˆé€šè¿‡æ ‡å‡†é‡è¡¨æ”¶é›†åŒè¡Œä¸æ•™å¸ˆçš„è¯„ä»·ï¼ŒåŒæ—¶åœ¨å­¦ä¹ æ´»åŠ¨ä¸­é‡‡é›†è§†é¢‘ã€éŸ³é¢‘ã€çœ¼åŠ¨è¿½è¸ªåŠç”Ÿç†ä¿¡å·ç­‰ Multimodal Dataã€‚éšååˆ©ç”¨ AI ç»¼åˆäººç±»è¯„ä¼°ä¸å¤šæ¨¡æ€æ•°æ®ï¼Œé’ˆå¯¹å­¦ç”Ÿçš„å§¿æ€ã€æ¼”è®²æ¨¡å¼ã€å‹åŠ›æ°´å¹³å’Œè®¤çŸ¥è´Ÿè· (Cognitive Load) ç”Ÿæˆä¸ªæ€§åŒ–çš„åé¦ˆæŠ¥å‘Šã€‚æœ€åï¼Œå­¦ç”Ÿé€šè¿‡è§‚çœ‹å½•åƒè¿›è¡Œ Self-Assessmentï¼Œå¹¶å°†è‡ªå·±çš„è¯„ä¼°ç»“æœä¸ä»–äººè¯„ä»·ã€ç­çº§å¹³å‡æ°´å¹³åŠ AI å»ºè®®è¿›è¡Œå¯è§†åŒ–å¯¹æ¯”ã€‚é€šè¿‡ç»“åˆåŸºäºäººç±»å’ŒåŸºäºæ•°æ®çš„è¯„ä»·æŠ€æœ¯ï¼ŒMOSAIC-F èƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®ã€ä¸ªæ€§åŒ–ä¸”å…·å¯æ“ä½œæ€§çš„åé¦ˆã€‚è¯¥ç ”ç©¶åœ¨æå‡å­¦ç”Ÿå£å¤´æ¼”è®²æŠ€èƒ½ (Oral Presentation Skills) çš„å®é™…åœºæ™¯ä¸­å¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†æœ‰æ•ˆéªŒè¯ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08634v1",
      "published_date": "2025-06-10 09:46:31 UTC",
      "updated_date": "2025-06-10 09:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:38.404161+00:00"
    },
    {
      "arxiv_id": "2506.09089v1",
      "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT",
      "title_zh": "åˆ©ç”¨ ChatGPT è®¾è®¡å¯¹å¤–æ±‰è¯­æ•™å­¦ä¸­åŸºäºå†²çªçš„äº¤é™…ä»»åŠ¡",
      "authors": [
        "Xia Li"
      ],
      "abstract": "In developing the teaching program for a course in Oral Expression in Teaching Chinese as a Foreign Language at the university level, the teacher designs communicative tasks based on conflicts to encourage learners to engage in interactive dynamics and develop their oral interaction skills. During the design of these tasks, the teacher uses ChatGPT to assist in finalizing the program. This article aims to present the key characteristics of the interactions between the teacher and ChatGPT during this program development process, as well as to examine the use of ChatGPT and its impacts in this specific context.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å­¦â€œå¯¹å¤–æ±‰è¯­å£è¯­è¡¨è¾¾â€è¯¾ç¨‹ä¸­ï¼Œæ•™å¸ˆå¦‚ä½•åˆ©ç”¨ChatGPTè®¾è®¡åŸºäºå†²çªçš„äº¤é™…ä»»åŠ¡(conflict-based communicative tasks)ï¼Œæ—¨åœ¨é€šè¿‡æ¿€å‘å­¦ä¹ è€…çš„äº’åŠ¨åŠ¨æ€æ¥æå‡å…¶å£è¯­äº’åŠ¨èƒ½åŠ›ã€‚åœ¨å®Œå–„æ•™å­¦æ–¹æ¡ˆçš„è¿‡ç¨‹ä¸­ï¼Œæ•™å¸ˆå°†ChatGPTä½œä¸ºè¾…åŠ©å·¥å…·ï¼Œæ–‡ç« é‡ç‚¹å±•ç¤ºäº†æ•™å¸ˆä¸ChatGPTåœ¨ç¨‹åºå¼€å‘è¿‡ç¨‹ä¸­äº’åŠ¨çš„å…³é”®ç‰¹å¾ã€‚é€šè¿‡åˆ†æè¿™ç§ç‰¹å®šçš„åº”ç”¨èƒŒæ™¯ï¼Œç ”ç©¶è¯¦ç»†è€ƒå¯Ÿäº†ChatGPTçš„ä½¿ç”¨æ–¹å¼åŠå…¶å¯¹æ•™å­¦è®¾è®¡äº§ç”Ÿçš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¯¹å¤–æ±‰è¯­(Teaching Chinese as a Foreign Language)æ•™å­¦é¢†åŸŸçš„æ•´åˆæä¾›äº†å®è·µæ¡ˆä¾‹ï¼Œæ­ç¤ºäº†äººå·¥æ™ºèƒ½è¾…åŠ©ä»»åŠ¡è®¾è®¡åœ¨æå‡è¯­è¨€äº¤é™…æ•™å­¦è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "in French language",
      "pdf_url": "https://arxiv.org/pdf/2506.09089v1",
      "published_date": "2025-06-10 09:44:41 UTC",
      "updated_date": "2025-06-10 09:44:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:41.749486+00:00"
    },
    {
      "arxiv_id": "2506.08630v2",
      "title": "Modular Recurrence in Contextual MDPs for Universal Morphology Control",
      "title_zh": "ä¸Šä¸‹æ–‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­çš„æ¨¡å—åŒ–å¾ªç¯ï¼šé¢å‘é€šç”¨å½¢æ€æ§åˆ¶",
      "authors": [
        "Laurens Engwegen",
        "Daan Brinks",
        "Wendelin BÃ¶hmer"
      ],
      "abstract": "A universal controller for any robot morphology would greatly improve computational and data efficiency. By utilizing contextual information about the properties of individual robots and exploiting their modular structure in the architecture of deep reinforcement learning agents, steps have been made towards multi-robot control. Generalization to new, unseen robots, however, remains a challenge. In this paper we hypothesize that the relevant contextual information is partially observable, but that it can be inferred through interactions for better generalization to contexts that are not seen during training. To this extent, we implement a modular recurrent architecture and evaluate its generalization performance on a large set of MuJoCo robots. The results show a substantial improved performance on robots with unseen dynamics, kinematics, and topologies, in four different environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé€šç”¨æ§åˆ¶ä¸­çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œæå‡ºäº†åœ¨ Contextual MDPs ä¸­åº”ç”¨ Modular Recurrence æ¶æ„ï¼Œä»¥å®ç°å¯¹ä¸åŒæœºå™¨äººå½¢æ€ (Universal Morphology Control) çš„æœ‰æ•ˆæ§åˆ¶ã€‚ä½œè€…å‡è®¾æœºå™¨äººå±æ€§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯éƒ¨åˆ†å¯è§‚æµ‹çš„ï¼Œå¹¶èƒ½é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’è¿›è¡Œå®æ—¶æ¨æ–­ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹è®­ç»ƒä¸­æœªè§ä¸Šä¸‹æ–‡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å®ç°äº†ä¸€ç§æ¨¡å—åŒ–é€’å½’æ¶æ„ï¼Œå¹¶åœ¨åŒ…å«å¤šç§ MuJoCo æœºå™¨äººçš„å››ä¸ªä¸åŒä»»åŠ¡ç¯å¢ƒä¸­è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¶æ„åœ¨åº”å¯¹å…·æœ‰å…¨æ–° dynamicsã€kinematics å’Œ topologies çš„æœºå™¨äººæ—¶è¡¨ç°å‡ºäº†æå¼ºçš„é²æ£’æ€§ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚è¯¥æˆæœè¯æ˜äº†ç»“åˆæ¨¡å—åŒ–è®¾è®¡ä¸é€’å½’æœºåˆ¶åœ¨æå‡å¤šå½¢æ€æœºå™¨äººæ§åˆ¶æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08630v2",
      "published_date": "2025-06-10 09:44:30 UTC",
      "updated_date": "2025-09-07 10:03:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:47.553979+00:00"
    },
    {
      "arxiv_id": "2506.08629v1",
      "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network",
      "title_zh": "ECMNetï¼šåŸºäºé«˜æ•ˆ CNN-Mamba ç½‘ç»œçš„è½»é‡çº§è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Feixiang Du",
        "Shengkun Wu"
      ],
      "abstract": "In the past decade, Convolutional Neural Networks (CNNs) and Transformers have achieved wide applicaiton in semantic segmentation tasks. Although CNNs with Transformer models greatly improve performance, the global context modeling remains inadequate. Recently, Mamba achieved great potential in vision tasks, showing its advantages in modeling long-range dependency. In this paper, we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation, dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based framework to address their complementary weaknesses. Specifically, We design a Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to improve the representations ability of feature, We devise a Multi-Scale Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion Module (FFM) merges diverse level feature, significantly enhancing segmented accuracy. Extensive experiments on two representative datasets demonstrate that the proposed model excels in accuracy and efficiency balance, achieving 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º ECMNet çš„è½»é‡çº§é«˜æ•ˆè¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) å’Œ Transformer åœ¨å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„å±€é™æ€§ã€‚ECMNet åœ¨ä¸€ä¸ªåŸºäºèƒ¶å›Š (capsule-based) çš„æ¡†æ¶ä¸­å·§å¦™ç»“åˆäº† CNN ä¸ Mambaï¼Œä»¥åˆ©ç”¨ Mamba åœ¨å»ºæ¨¡é•¿ç¨‹ä¾èµ– (long-range dependency) æ–¹é¢çš„ä¼˜åŠ¿å¹¶å¼¥è¡¥ä¸¤è€…çš„æŠ€æœ¯çŸ­æ¿ã€‚å…·ä½“åˆ›æ–°åŒ…æ‹¬ç”¨äºè½»é‡çº§ç“¶é¢ˆçš„å¢å¼ºåŒæ³¨æ„åŠ›æ¨¡å— (Enhanced Dual-Attention Block, EDAB)ï¼Œä»¥åŠç”¨äºæ•´åˆå¤šå°ºåº¦ç‰¹å¾ã€ç©ºé—´å’Œé€šé“èšåˆçš„å¤šå°ºåº¦æ³¨æ„åŠ›å•å…ƒ (Multi-Scale Attention Unit, MSAU)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº† Mamba å¢å¼ºçš„ç‰¹å¾èåˆæ¨¡å— (Feature Fusion Module, FFM) ä»¥åˆå¹¶ä¸åŒå±‚çº§çš„ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼ŒECMNet åœ¨ Cityscapes å’Œ CamVid æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 70.6% å’Œ 73.6% çš„ mIoUï¼Œè€Œå‚æ•°é‡ä»…ä¸º 0.87Mï¼Œåœ¨è®¡ç®—æ•ˆç‡ä¸ç²¾åº¦ä¹‹é—´è¾¾æˆäº†å“è¶Šçš„å¹³è¡¡ã€‚è¯¥æˆæœä¸ºåœ¨èµ„æºå—é™çš„ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ€§èƒ½è¯­ä¹‰åˆ†å‰²æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 2 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.08629v1",
      "published_date": "2025-06-10 09:44:23 UTC",
      "updated_date": "2025-06-10 09:44:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:02.609084+00:00"
    },
    {
      "arxiv_id": "2506.08627v1",
      "title": "FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings",
      "title_zh": "FoldAï¼šåŸºäºæœ‰å‘ç½‘å±•å¼€çš„ååºå¯¹é½è®¡ç®—",
      "authors": [
        "Douwe Geurtjens",
        "Xixi Lu"
      ],
      "abstract": "Conformance checking is a fundamental task of process mining, which quantifies the extent to which the observed process executions match a normative process model. The state-of-the-art approaches compute alignments by exploring the state space formed by the synchronous product of the process model and the trace. This often leads to state space explosion, particularly when the model exhibits a high degree of choice and concurrency. Moreover, as alignments inherently impose a sequential structure, they fail to fully represent the concurrent behavior present in many real-world processes. To address these limitations, this paper proposes a new technique for computing partial-order alignments {on the fly using directed Petri net unfoldings, named FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6 benchmark pairs. The results show that our unfolding alignment, although it requires more computation time, generally reduces the number of queued states and provides a more accurate representation of concurrency.",
      "tldr_zh": "ä¸€è‡´æ€§æ£€æŸ¥(Conformance checking)æ˜¯æµç¨‹æŒ–æ˜(process mining)ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œä½†ç°æœ‰çš„å¯¹é½(alignments)æ–¹æ³•åœ¨å¤„ç†å…·æœ‰é«˜åº¦å¹¶å‘å’Œé€‰æ‹©æ€§çš„æ¨¡å‹æ—¶ï¼Œå®¹æ˜“å‡ºç°çŠ¶æ€ç©ºé—´çˆ†ç‚¸(state space explosion)ä¸”æ— æ³•å‡†ç¡®è¡¨å¾å¹¶å‘è¡Œä¸ºçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FoldAï¼Œä¸€ç§åˆ©ç”¨æœ‰å‘Petriç½‘å±•å¼€(directed Petri net unfoldings)åœ¨çº¿è®¡ç®—ååºå¯¹é½(partial-order alignments)çš„æ–°æŠ€æœ¯ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡485ä¸ªåˆæˆæ¨¡å‹ä»¥åŠ13ä¸ªçœŸå®åœºæ™¯å’Œ6ä¸ªåŸºå‡†æµ‹è¯•å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„A*å’ŒDijkstraç®—æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡FoldAåœ¨è®¡ç®—æ—¶é—´ä¸Šéœ€æ±‚æ›´é«˜ï¼Œä½†å®ƒèƒ½æœ‰æ•ˆå‡å°‘æœç´¢è¿‡ç¨‹ä¸­çš„é˜Ÿåˆ—çŠ¶æ€(queued states)æ•°é‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†FoldAåœ¨å¤„ç†å¤æ‚æµç¨‹æ—¶èƒ½å¤Ÿæä¾›æ¯”ä¼ ç»Ÿé¡ºåºå¯¹é½æ›´ç²¾ç¡®çš„å¹¶å‘æ€§è¡¨è¾¾ï¼Œä¸ºæµç¨‹æ¨¡å‹çš„åˆè§„æ€§åˆ†ææä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Conditionally accepted at BPM 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08627v1",
      "published_date": "2025-06-10 09:44:05 UTC",
      "updated_date": "2025-06-10 09:44:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:43:54.405493+00:00"
    },
    {
      "arxiv_id": "2506.19863v2",
      "title": "Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research",
      "title_zh": "æ¢ç´¢å‰æ²¿å¤§è¯­è¨€æ¨¡å‹åœ¨æ ¸èƒ½ç ”ç©¶ä¸­çš„èƒ½åŠ›",
      "authors": [
        "Ahmed Almeldein",
        "Mohammed Alnaggar",
        "Rick Archibald",
        "Tom Beck",
        "Arpan Biswas",
        "Rike Bostelmann",
        "Wes Brewer",
        "Chris Bryan",
        "Christopher Calle",
        "Cihangir Celik",
        "Rajni Chahal",
        "Jong Youl Choi",
        "Arindam Chowdhury",
        "Mark Cianciosa",
        "Franklin Curtis",
        "Gregory Davidson",
        "Sebastian De Pascuale",
        "Lisa Fassino",
        "Ana Gainaru",
        "Yashika Ghai",
        "Luke Gibson",
        "Qian Gong",
        "Christopher Greulich",
        "Scott Greenwood",
        "Cory Hauck",
        "Ehab Hassan",
        "Rinkle Juneja",
        "Soyoung Kang",
        "Scott Klasky",
        "Atul Kumar",
        "Vineet Kumar",
        "Paul Laiu",
        "Calvin Lear",
        "Yan-Ru Lin",
        "Jono McConnell",
        "Furkan Oz",
        "Rishi Pillai",
        "Anant Raj",
        "Pradeep Ramuhalli",
        "Marie Romedenne",
        "Samantha Sabatino",
        "JosÃ© Salcedo-PÃ©rez",
        "Nathan D. See",
        "Arpan Sircar",
        "Punam Thankur",
        "Tim Younkin",
        "Xiao-Ying Yu",
        "Prashant Jain",
        "Tom Evans",
        "Prasanna Balaprakash"
      ],
      "abstract": "The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated the potential of Large Language Models (LLMs) to accelerate fusion and fission research. Fourteen interdisciplinary teams explored diverse nuclear science challenges using ChatGPT, Gemini, Claude, and other AI models over a single day. Applications ranged from developing foundation models for fusion reactor control to automating Monte Carlo simulations, predicting material degradation, and designing experimental programs for advanced reactors. Teams employed structured workflows combining prompt engineering, deep research capabilities, and iterative refinement to generate hypotheses, prototype code, and research strategies. Key findings demonstrate that LLMs excel at early-stage exploration, literature synthesis, and workflow design, successfully identifying research gaps and generating plausible experimental frameworks. However, significant limitations emerged, including difficulties with novel materials designs, advanced code generation for modeling and simulation, and domain-specific details requiring expert validation. The successful outcomes resulted from expert-driven prompt engineering and treating AI as a complementary tool rather than a replacement for physics-based methods. The workshop validated AI's potential to accelerate nuclear energy research through rapid iteration and cross-disciplinary synthesis while highlighting the need for curated nuclear-specific datasets, workflow automation, and specialized model development. These results provide a roadmap for integrating AI tools into nuclear science workflows, potentially reducing development cycles for safer, more efficient nuclear energy systems while maintaining rigorous scientific standards.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¾æ‰˜æ©¡æ ‘å²­å›½å®¶å®éªŒå®¤ (Oak Ridge National Laboratory) ä¸¾åŠçš„æ ¸èƒ½äººå·¥æ™ºèƒ½ç ”è®¨ä¼šï¼Œè¯„ä¼°äº† ChatGPTã€Gemini å’Œ Claude ç­‰å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŠ é€Ÿèšå˜ä¸è£‚å˜ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚åå››ä¸ªè·¨å­¦ç§‘å›¢é˜Ÿé€šè¿‡æç¤ºå·¥ç¨‹ (prompt engineering) å’Œè¿­ä»£ä¼˜åŒ–çš„å·¥ä½œæµï¼Œæ¢ç´¢äº†èšå˜ååº”å †æ§åˆ¶çš„åŸºç¡€æ¨¡å‹ã€Monte Carlo æ¨¡æ‹Ÿè‡ªåŠ¨åŒ–ä»¥åŠææ–™é™è§£é¢„æµ‹ç­‰å¤šç§æ ¸ç§‘å­¦æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMs åœ¨æ—©æœŸæ¢ç´¢ã€æ–‡çŒ®ç»¼åˆå’Œå·¥ä½œæµè®¾è®¡æ–¹é¢è¡¨ç°å“è¶Šï¼Œèƒ½å¤ŸæˆåŠŸè¯†åˆ«ç ”ç©¶ç©ºç™½å¹¶ç”Ÿæˆåˆç†çš„å®éªŒæ¡†æ¶ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨æ–°å‹ææ–™è®¾è®¡ã€ç”¨äºå»ºæ¨¡ä¸ä»¿çœŸçš„é«˜çº§ä»£ç ç”Ÿæˆä»¥åŠéœ€è¦ä¸“å®¶éªŒè¯çš„é¢†åŸŸç‰¹å®šç»†èŠ‚ä¸Šä»å­˜åœ¨æ˜¾è‘—å±€é™ã€‚ç ”ç©¶å¼ºè°ƒï¼ŒLLMs åº”ä½œä¸ºç‰©ç†é©±åŠ¨æ–¹æ³•çš„è¡¥å……å·¥å…·è€Œéæ›¿ä»£å“ï¼Œå…¶å®é™…æ•ˆæœé«˜åº¦ä¾èµ–äºä¸“å®¶çš„å¼•å¯¼ã€‚è¯¥å·¥ä½œä¸ºå°† AI é›†æˆåˆ°æ ¸ç§‘å­¦å·¥ä½œæµä¸­æä¾›äº†è·¯çº¿å›¾ï¼ŒæŒ‡å‡ºæœªæ¥éœ€å¼€å‘æ ¸èƒ½ä¸“ç”¨æ•°æ®é›†å’Œä¸“ä¸šåŒ–æ¨¡å‹ï¼Œä»¥ç¼©çŸ­æ›´å®‰å…¨ã€æ›´é«˜æ•ˆæ ¸èƒ½ç³»ç»Ÿçš„å¼€å‘å‘¨æœŸã€‚",
      "categories": [
        "physics.comp-ph",
        "cs.AI"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19863v2",
      "published_date": "2025-06-10 09:28:18 UTC",
      "updated_date": "2025-06-26 22:36:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:06.460319+00:00"
    },
    {
      "arxiv_id": "2506.08618v1",
      "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset",
      "title_zh": "HSG-12Mï¼šå¤§è§„æ¨¡ç©ºé—´å¤šé‡å›¾æ•°æ®é›†",
      "authors": [
        "Xianquan Yan",
        "Hakan AkgÃ¼n",
        "Kenji Kawaguchi",
        "N. Duane Loh",
        "Ching Hua Lee"
      ],
      "abstract": "Existing graph benchmarks assume non-spatial, simple edges, collapsing physically distinct paths into a single link. We introduce HSG-12M, the first large-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a metric space where multiple geometrically distinct trajectories between two nodes are retained as separate edges. HSG-12M contains 11.6 million static and 5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401 characteristic-polynomial classes, derived from 177 TB of spectral potential data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum on the complex plane, producing diverse, physics-grounded topologies that transcend conventional node-coordinate datasets. To enable future extensions, we release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with popular GNNs expose new challenges in learning from multi-edge geometry at scale. Beyond its practical utility, we show that spectral graphs serve as universal topological fingerprints of polynomials, vectors, and matrices, forging a new algebra-to-graph link. HSG-12M lays the groundwork for geometry-aware graph learning and new opportunities of data-driven scientific discovery in condensed matter physics and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†HSG-12Mï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„spatial multigraphsæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åŸºå‡†å°†èŠ‚ç‚¹é—´ä¸åŒç‰©ç†è·¯å¾„ç®€åŒ–ä¸ºå•æ¡è¾¹çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†ä¿ç•™äº†ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´å‡ ä½•ä¸Šä¸åŒçš„å¤šæ¡è½¨è¿¹ä½œä¸ºç‹¬ç«‹è¾¹ï¼ŒåŒ…å«æºè‡ª177 TBå…‰è°±ç”µä½æ•°æ®çš„1160ä¸‡ä¸ªé™æ€å’Œ510ä¸‡ä¸ªåŠ¨æ€Hamiltonian spectral graphsã€‚è¿™äº›å›¾é€šè¿‡åœ¨å¤å¹³é¢ä¸Šç¼–ç ä¸€ç»´æ™¶ä½“èƒ½è°±çš„å®Œæ•´å‡ ä½•å½¢çŠ¶ï¼Œç”Ÿæˆäº†è¶…è¶Šä¼ ç»ŸèŠ‚ç‚¹åæ ‡æ•°æ®é›†çš„å¤šæ ·åŒ–ç‰©ç†æ‹“æ‰‘ç»“æ„ã€‚ä¸ºäº†æ”¯æŒæœªæ¥æ‰©å±•ï¼Œç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†é«˜æ€§èƒ½å¼€æºç®¡çº¿Poly2Graphï¼Œç”¨äºå°†ä»»æ„ä¸€ç»´æ™¶ä½“Hamiltoniansæ˜ å°„ä¸ºå…‰è°±å›¾ã€‚é’ˆå¯¹ä¸»æµGNNsçš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†åœ¨å¤§è§„æ¨¡å¤šè¾¹å‡ ä½•ä¸­è¿›è¡Œå­¦ä¹ çš„æ–°æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å…‰è°±å›¾å¯ä½œä¸ºå¤šé¡¹å¼ã€å‘é‡å’ŒçŸ©é˜µçš„é€šç”¨æ‹“æ‰‘æŒ‡çº¹ï¼Œåœ¨ä»£æ•°ä¸å›¾è®ºä¹‹é—´å»ºç«‹äº†æ–°çš„è”ç³»ã€‚HSG-12Mä¸ºå‡ ä½•æ„ŸçŸ¥å›¾å­¦ä¹ ä»¥åŠå‡èšæ€ç‰©ç†ç­‰é¢†åŸŸçš„ç§‘å­¦å‘ç°å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mes-hall",
        "cond-mat.other",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 13 figures, 3 tables. Code & pipeline: [https://github.com/sarinstein-yan/Poly2Graph] Dataset: [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0",
      "pdf_url": "https://arxiv.org/pdf/2506.08618v1",
      "published_date": "2025-06-10 09:25:19 UTC",
      "updated_date": "2025-06-10 09:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:24.716905+00:00"
    },
    {
      "arxiv_id": "2506.10029v1",
      "title": "Evaluation empirique de la sÃ©curisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnÃ©rabilitÃ©s par expÃ©rimentations de jailbreaks",
      "title_zh": "ChatGPT ä¸ Gemini å®‰å…¨æ€§åŠå¯¹é½çš„å®è¯è¯„ä¼°ï¼šåŸºäºè¶Šç‹±å®éªŒçš„æ¼æ´å¯¹æ¯”åˆ†æ",
      "authors": [
        "RafaÃ«l Nouailles"
      ],
      "abstract": "Large Language models (LLMs) are transforming digital usage, particularly in text generation, image creation, information retrieval and code development. ChatGPT, launched by OpenAI in November 2022, quickly became a reference, prompting the emergence of competitors such as Google's Gemini. However, these technological advances raise new cybersecurity challenges, including prompt injection attacks, the circumvention of regulatory measures (jailbreaking), the spread of misinformation (hallucinations) and risks associated with deep fakes. This paper presents a comparative analysis of the security and alignment levels of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated with experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ ChatGPT å’Œ Gemini çš„å®‰å…¨æ€§å’Œå¯¹é½ (alignment) æ°´å¹³è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œé‡ç‚¹åˆ†æäº†è¿™ä¸¤æ¬¾ä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åº”å¯¹å„ç±»å®‰å…¨æ¼æ´æ—¶çš„è¡¨ç°ã€‚æ–‡ä¸­ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†æç¤ºè¯æ³¨å…¥ (prompt injection)ã€è¶Šç‹± (jailbreaking)ã€å¹»è§‰ (hallucinations) åŠæ·±åº¦ä¼ªé€  (deep fakes) ç­‰æ–°å…´ç½‘ç»œå®‰å…¨å¨èƒã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€å¥—è¶Šç‹±æŠ€æœ¯çš„åˆ†ç±»æ³• (taxonomy)ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒæ·±å…¥åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨é˜²å¾¡æ”»å‡»å’Œéµå®ˆç›‘ç®¡æªæ–½æ–¹é¢çš„å·®å¼‚ã€‚å®è¯ç»“æœå±•ç¤ºäº† ChatGPT ä¸ Gemini åœ¨å®‰å…¨æœºåˆ¶ä¸Šçš„è„†å¼±æ€§å¯¹æ¯”ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¯¹é½å±‚é¢å­˜åœ¨çš„æ½œåœ¨é£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨é˜²å¾¡èƒ½åŠ›å’Œä¼˜åŒ–å¯¹é½ç­–ç•¥æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ä¸å®éªŒæ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "in French language",
      "pdf_url": "https://arxiv.org/pdf/2506.10029v1",
      "published_date": "2025-06-10 09:24:05 UTC",
      "updated_date": "2025-06-10 09:24:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:30.979072+00:00"
    },
    {
      "arxiv_id": "2506.13782v1",
      "title": "XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation",
      "title_zh": "XGraphRAGï¼šé¢å‘åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆçš„äº¤äº’å¼å¯è§†åˆ†æ",
      "authors": [
        "Ke Wang",
        "Bo Pan",
        "Yingchaojie Feng",
        "Yuwei Wu",
        "Jieyi Chen",
        "Minfeng Zhu",
        "Wei Chen"
      ],
      "abstract": "Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†XGraphRAGï¼Œä¸€ä¸ªé’ˆå¯¹åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Graph-based Retrieval-Augmented Generation, GraphRAG)è®¾è®¡çš„äº¤äº’å¼å¯è§†åŒ–åˆ†ææ¡†æ¶ã€‚è™½ç„¶GraphRAGé€šè¿‡å¼•å…¥å›¾ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæ¯”ä¼ ç»ŸRAGèƒ½æ›´å¥½åœ°æ•æ‰è¯­æ–™åº“ä¸­çš„ç»“æ„åŒ–å…³ç³»çŸ¥è¯†ï¼Œä½†å…¶å¤æ‚çš„å¤„ç†æµç¨‹å’Œæµ·é‡çš„LLMè°ƒç”¨é™åˆ¶äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚XGraphRAGé€šè¿‡ä¸€å¥—äº¤äº’å¼å¯è§†åŒ–å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…è¯†åˆ«å…³é”®å¬å›(recalls)å¹¶è¿½è¸ªå…¶åœ¨GraphRAG pipelineä¸­çš„å¤„ç†è¿‡ç¨‹ã€‚è¯¥åŸå‹ç³»ç»Ÿæ—¨åœ¨ç®€åŒ–å¼€å‘è€…å¯¹å¤±æ•ˆæ¡ˆä¾‹(failure cases)çš„åˆ†æï¼Œå¹¶ååŠ©å…¶å¯»æ‰¾æ¨¡å‹æ”¹è¿›çš„æœºä¼šã€‚è¯„ä¼°ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡GraphRAGé€æ˜åº¦ä¸åˆ†ææ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶æˆæœå·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to IEEE Pacific Visualization Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.13782v1",
      "published_date": "2025-06-10 09:14:30 UTC",
      "updated_date": "2025-06-10 09:14:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:39.310773+00:00"
    },
    {
      "arxiv_id": "2506.08604v2",
      "title": "Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation",
      "title_zh": "æµåŒ¹é…ä¸åå¾®åˆ†æ–¹ç¨‹ï¼šç‰©ç†çº¦æŸç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Giacomo Baldan",
        "Qiang Liu",
        "Alberto Guardone",
        "Nils Thuerey"
      ],
      "abstract": "Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $Ïƒ_{\\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Physics-Based Flow Matching (PBFM)ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†PDE residualså’Œä»£æ•°å…³ç³»ç­‰ç‰©ç†çº¦æŸæ˜¾å¼åµŒå…¥Flow Matchingç›®æ ‡çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ã€‚PBFMé€šè¿‡åœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥æ—¶é—´å±•å¼€(temporal unrolling)æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡äº†æœ€ç»ˆæ— å™ªå£°æ ·æœ¬çš„é¢„æµ‹ç²¾åº¦ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè”åˆæœ€å°åŒ–Flow MatchingæŸå¤±ä¸ç‰©ç†æ®‹å·®æŸå¤±ï¼Œä¸”æ— éœ€é’ˆå¯¹ä¸¤è€…çš„ç›¸å¯¹æƒé‡è¿›è¡Œå¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†æœ€å°å™ªå£°æ°´å¹³$\\sigma_{\\min}$åœ¨ç‰©ç†çº¦æŸç¯å¢ƒä¸‹çš„ä½œç”¨ï¼Œå¹¶é‡‡ç”¨éšæœºé‡‡æ ·ç­–ç•¥è¿›ä¸€æ­¥é™ä½ç‰©ç†æ®‹å·®ã€‚åœ¨ä¸‰é¡¹ä»£è¡¨æ€§PDEé—®é¢˜çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPBFMç”Ÿæˆçš„ç‰©ç†æ®‹å·®å‡†ç¡®åº¦æ¯”æ ‡å‡†FMæå‡äº†é«˜è¾¾8å€ï¼ŒåŒæ—¶åœ¨åˆ†å¸ƒå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ç®—æ³•ã€‚è¯¥æ¡†æ¶ä¸ºç‰©ç†å’Œå·¥ç¨‹é¢†åŸŸçš„surrogate modelingã€ä¸ç¡®å®šæ€§é‡åŒ–åŠåŠ é€Ÿæ¨¡æ‹Ÿæä¾›äº†ä¸€ç§åŸåˆ™æ€§ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08604v2",
      "published_date": "2025-06-10 09:13:37 UTC",
      "updated_date": "2025-11-26 15:56:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:30.727425+00:00"
    },
    {
      "arxiv_id": "2506.08602v2",
      "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks",
      "title_zh": "WGLEï¼šé¢å‘å›¾ç¥ç»ç½‘ç»œçš„æ— åé—¨å¤šæ¯”ç‰¹é»‘ç›’æ°´å°",
      "authors": [
        "Tingzhi Li",
        "Xuefeng Liu",
        "Jing Lei",
        "Xingang Zhang"
      ],
      "abstract": "Graph Neural Networks (GNNs) are increasingly deployed in real-world applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both previous methods enable ownership verification but do not convey additional information about the copy model. If the owner has multiple models, each model requires a distinct trigger graph.\n  To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string in GNN models without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes in a graph. By assigning unique LDDE values to the edges and employing the LDDE sequence as the watermark, WGLE supports multi-bit capacity without relying on backdoor mechanisms. We evaluate WGLE on six public datasets across six mainstream GNN architectures, and compare WGLE with state-of-the-art GNN watermarking and fingerprinting methods. WGLE achieves 100% ownership verification accuracy, with an average fidelity degradation of only 1.41%. Additionally, WGLE exhibits robust resilience against potential attacks. The code is available in the repository.",
      "tldr_zh": "é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)åœ¨æ¨¡å‹æ‰€æœ‰æƒä¿æŠ¤ä¸­é¢ä¸´çš„æŒ‡çº¹è¯†åˆ«è®¡ç®—æˆæœ¬é«˜ã€åé—¨æ°´å°æ˜“å—æ”»å‡»ä»¥åŠç¼ºä¹å¤šä½(multi-bit)ä¿¡æ¯æ‰¿è½½èƒ½åŠ›ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºWGLEçš„æ–°å‹é»‘ç›’æ°´å°èŒƒå¼ã€‚WGLEçš„æ ¸å¿ƒåœ¨äºæå‡ºäº†ä¸€ç§ç§°ä¸ºâ€œè¾¹ä¸Šçš„é€å±‚è·ç¦»å·®å¼‚â€(Layer-wise Distance Difference on an Edge, LDDE)çš„åº¦é‡æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–å›¾ä¸­ä¸¤ä¸ªç›¸è¿èŠ‚ç‚¹çš„ç‰¹å¾è·ç¦»ä¸é¢„æµ‹è·ç¦»ä¹‹é—´çš„å·®å¼‚æ¥åµŒå…¥æ°´å°ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºè¾¹åˆ†é…å”¯ä¸€çš„LDDEå€¼å¹¶åˆ©ç”¨LDDEåºåˆ—ä½œä¸ºæ°´å°ï¼Œå®ç°äº†åœ¨ä¸ä¾èµ–åé—¨(backdoor)æœºåˆ¶çš„æƒ…å†µä¸‹æ”¯æŒå¤šä½æ°´å°å®¹é‡ã€‚å®éªŒåœ¨6ä¸ªå…¬å…±æ•°æ®é›†å’Œ6ç§ä¸»æµGNNæ¶æ„ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜WGLEå®ç°äº†100%çš„æ‰€æœ‰æƒéªŒè¯å‡†ç¡®ç‡ï¼Œä¸”å¹³å‡ä¿çœŸåº¦(fidelity)ä¸‹é™ä»…ä¸º1.41%ã€‚æ­¤å¤–ï¼ŒWGLEåœ¨é¢å¯¹æ½œåœ¨æ”»å‡»æ—¶è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§(resilience)ï¼Œä¸ºGraph Neural Networksçš„çŸ¥è¯†äº§æƒä¿æŠ¤æä¾›äº†ä¸€ç§å…¼å…·å®‰å…¨æ€§ä¸é«˜æ•ˆæ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08602v2",
      "published_date": "2025-06-10 09:12:00 UTC",
      "updated_date": "2025-12-24 02:12:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:31.443877+00:00"
    },
    {
      "arxiv_id": "2506.08596v1",
      "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems",
      "title_zh": "Transformer ä¸é«˜å…‰è°±æˆåƒï¼šæ¨¡å‹ã€æŒ‘æˆ˜ä¸å¼€æ”¾æ€§é—®é¢˜çš„å…¨é¢ç ”ç©¶",
      "authors": [
        "Guyang Zhang",
        "Waleed Abdulla"
      ],
      "abstract": "Transformers have become the architecture of choice for learning long-range dependencies, yet their adoption in hyperspectral imaging (HSI) is still emerging. We reviewed more than 300 papers published up to 2025 and present the first end-to-end survey dedicated to Transformer-based HSI classification. The study categorizes every stage of a typical pipeline-pre-processing, patch or pixel tokenization, positional encoding, spatial-spectral feature extraction, multi-head self-attention variants, skip connections, and loss design-and contrasts alternative design choices with the unique spatial-spectral properties of HSI. We map the field's progress against persistent obstacles: scarce labeled data, extreme spectral dimensionality, computational overhead, and limited model explainability. Finally, we outline a research agenda prioritizing valuable public data sets, lightweight on-edge models, illumination and sensor shifts robustness, and intrinsically interpretable attention mechanisms. Our goal is to guide researchers in selecting, combining, or extending Transformer components that are truly fit for purpose for next-generation HSI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†æˆªè‡³2025å¹´çš„300å¤šç¯‡è®ºæ–‡ï¼Œå‘ˆç°äº†é¦–ä¸ªé’ˆå¯¹åŸºäºTransformersçš„é«˜å…‰è°±æˆåƒ(HSI)åˆ†ç±»çš„ç«¯åˆ°ç«¯å…¨é¢è°ƒç ”ã€‚æ–‡ç« ç³»ç»Ÿåœ°åˆ†ç±»äº†å…¸å‹å¤„ç†æµç¨‹çš„å„ä¸ªé˜¶æ®µï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€Tokenizationã€Positional Encodingã€ç©ºé—´-å…‰è°±ç‰¹å¾æå–ä»¥åŠMulti-head Self-attentionå˜ä½“ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒè®¾è®¡åœ¨HSIç‰¹æ€§ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†å½“å‰é¢ä¸´çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºã€æé«˜å…‰è°±ç»´åº¦ã€è®¡ç®—å¼€é”€å¤§å’Œæ¨¡å‹å¯è§£é‡Šæ€§å—é™ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æŒ‡æ˜äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œé‡ç‚¹å…³æ³¨é«˜è´¨é‡å…¬å…±æ•°æ®é›†ã€è½»é‡åŒ–On-edge Modelsã€ä¼ æ„Ÿå™¨é²æ£’æ€§ä»¥åŠæœ¬è´¨å¯è§£é‡Šçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä¸€ç ”ç©¶æ—¨åœ¨ä¸ºç§‘ç ”äººå‘˜åœ¨ä¸‹ä¸€ä»£HSIåº”ç”¨ä¸­é€‰æ‹©ã€ç»„åˆæˆ–æ‰©å±•Transformerç»„ä»¶æä¾›ç³»ç»Ÿæ€§æŒ‡å—ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08596v1",
      "published_date": "2025-06-10 09:04:30 UTC",
      "updated_date": "2025-06-10 09:04:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:45.040722+00:00"
    },
    {
      "arxiv_id": "2506.08594v1",
      "title": "Solving excited states for long-range interacting trapped ions with neural networks",
      "title_zh": "åŸºäºç¥ç»ç½‘ç»œçš„é•¿ç¨‹ç›¸äº’ä½œç”¨å›šç¦ç¦»å­æ¿€å‘æ€æ±‚è§£",
      "authors": [
        "Yixuan Ma",
        "Chang Liu",
        "Weikang Li",
        "Shun-Yao Zhang",
        "L. -M. Duan",
        "Yukai Wu",
        "Dong-Ling Deng"
      ],
      "abstract": "The computation of excited states in strongly interacting quantum many-body systems is of fundamental importance. Yet, it is notoriously challenging due to the exponential scaling of the Hilbert space dimension with the system size. Here, we introduce a neural network-based algorithm that can simultaneously output multiple low-lying excited states of a quantum many-body spin system in an accurate and efficient fashion. This algorithm, dubbed the neural quantum excited-state (NQES) algorithm, requires no explicit orthogonalization of the states and is generally applicable to higher dimensions. We demonstrate, through concrete examples including the Haldane-Shastry model with all-to-all interactions, that the NQES algorithm is capable of efficiently computing multiple excited states and their related observable expectations. In addition, we apply the NQES algorithm to two classes of long-range interacting trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying all-to-all interactions with alternating signs, our computed low-lying excited states bear spatial correlation patterns similar to those of the ground states, which closely match recent experimental observations that the quasi-adiabatically prepared state accurately reproduces analytical ground-state correlations. For a system of up to 300 ions with power-law decaying antiferromagnetic interactions, we successfully uncover its gap scaling and correlation features. Our results establish a scalable and efficient algorithm for computing excited states of interacting quantum many-body systems, which holds potential applications ranging from benchmarking quantum devices to photoisomerization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºNeural Quantum Excited-State (NQES)çš„ç¥ç»ç½‘ç»œç®—æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆä¸”å‡†ç¡®åœ°åŒæ—¶è®¡ç®—é‡å­å¤šä½“è‡ªæ—‹ç³»ç»Ÿçš„å¤šä¸ªä½èƒ½æ¿€å‘æ€(Excited States)ã€‚è¯¥ç®—æ³•æ— éœ€æ˜¾å¼çš„çŠ¶æ€æ­£äº¤åŒ–å¤„ç†ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ™®é€‚æ€§ï¼Œèƒ½å¤Ÿæ‰©å±•è‡³é«˜ç»´ç³»ç»Ÿå¹¶å¤„ç†å¼ºç›¸äº’ä½œç”¨é‡å­å¤šä½“é—®é¢˜ã€‚é€šè¿‡Haldane-Shastryæ¨¡å‹åŠå…¨å¯¹å…¨ç›¸äº’ä½œç”¨çš„æ¡ˆä¾‹éªŒè¯ï¼ŒNQESèƒ½å¤Ÿé«˜æ•ˆè®¡ç®—å¤šä¸ªæ¿€å‘æ€åŠå…¶ç›¸å…³çš„å¯è§‚æµ‹æœŸæœ›å€¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥ç®—æ³•åº”ç”¨äºäºŒç»´Wigner Crystalä¸­çš„ä¸¤ç±»é•¿ç¨‹ç›¸äº’ä½œç”¨ç¦»å­é˜±ç³»ç»Ÿï¼Œå‘ç°åœ¨å…·æœ‰äº¤æ›¿ç¬¦å·çš„å…¨å¯¹å…¨ç›¸äº’ä½œç”¨ä¸‹ï¼Œä½èƒ½æ¿€å‘æ€çš„ç©ºé—´å…³è”æ¨¡å¼ä¸åŸºæ€ç›¸ä¼¼ï¼Œè¿™ä¸æœ€è¿‘çš„å®éªŒè§‚æµ‹ç»“æœé«˜åº¦å»åˆã€‚é’ˆå¯¹æ‹¥æœ‰å¤šè¾¾300ä¸ªç¦»å­çš„å¹‚å¾‹è¡°å‡åé“ç£ç›¸äº’ä½œç”¨ç³»ç»Ÿï¼ŒNQESæˆåŠŸæ­ç¤ºäº†å…¶èƒ½éš™(Gap)ç¼©æ”¾è§„å¾‹å’Œå…³è”ç‰¹æ€§ã€‚è¿™ä¸€æˆæœä¸ºç ”ç©¶ç›¸äº’ä½œç”¨é‡å­å¤šä½“ç³»ç»Ÿçš„æ¿€å‘æ€æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è®¡ç®—æ–¹æ¡ˆï¼Œåœ¨é‡å­è®¾å¤‡åŸºå‡†æµ‹è¯•åŠå…‰è‡´å¼‚æ„åŒ–(Photoisomerization)ç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08594v1",
      "published_date": "2025-06-10 09:02:59 UTC",
      "updated_date": "2025-06-10 09:02:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:52.814546+00:00"
    },
    {
      "arxiv_id": "2506.08580v1",
      "title": "HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning",
      "title_zh": "HGFormerï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µä¸Šæ ¡å¸ƒæ´›æ‰˜åšå¼ˆåˆ†å±‚å›¾ Transformer æ¡†æ¶",
      "authors": [
        "Yang Lv",
        "Jinlong Lei",
        "Peng Yi"
      ],
      "abstract": "Two-stage Colonel Blotto game represents a typical adversarial resource allocation problem, in which two opposing agents sequentially allocate resources in a network topology across two phases: an initial resource deployment followed by multiple rounds of dynamic reallocation adjustments. The sequential dependency between game stages and the complex constraints imposed by the graph topology make it difficult for traditional approaches to attain a globally optimal strategy. To address these challenges, we propose a hierarchical graph Transformer framework called HGformer. By incorporating an enhanced graph Transformer encoder with structural biases and a two-agent hierarchical decision model, our approach enables efficient policy generation in large-scale adversarial environments. Moreover, we design a layer-by-layer feedback reinforcement learning algorithm that feeds the long-term returns from lower-level decisions back into the optimization of the higher-level strategy, thus bridging the coordination gap between the two decision-making stages. Experimental results demonstrate that, compared to existing hierarchical decision-making or graph neural network methods, HGformer significantly improves resource allocation efficiency and adversarial payoff, achieving superior overall performance in complex dynamic game scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸¤é˜¶æ®µ Colonel Blotto åšå¼ˆä¸­çš„å¯¹æŠ—æ€§èµ„æºåˆ†é…é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³å¤æ‚å›¾æ‹“æ‰‘çº¦æŸä¸‹é˜¶æ®µé—´åºåˆ—ä¾èµ–å¯¼è‡´çš„æœ€ä¼˜ç­–ç•¥è·å–éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† HGFormerï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„å±‚æ¬¡åŒ–å›¾ Transformer æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå…·æœ‰ç»“æ„åç½®çš„å¢å¼ºå‹å›¾ Transformer ç¼–ç å™¨å’ŒåŒæ™ºèƒ½ä½“å±‚æ¬¡åŒ–å†³ç­–æ¨¡å‹ï¼Œå®ç°äº†åœ¨å¤§è§„æ¨¡å¯¹æŠ—ç¯å¢ƒä¸‹çš„é«˜æ•ˆç­–ç•¥ç”Ÿæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§é€å±‚åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å°†åº•å±‚å†³ç­–çš„é•¿æœŸå›æŠ¥åé¦ˆè‡³é«˜å±‚ç­–ç•¥ä¼˜åŒ–ï¼Œæœ‰æ•ˆæ¡¥æ¥äº†ä¸¤ä¸ªå†³ç­–é˜¶æ®µä¹‹é—´çš„åè°ƒå·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å±‚æ¬¡åŒ–å†³ç­–æˆ–å›¾ç¥ç»ç½‘ç»œæ–¹æ³•ç›¸æ¯”ï¼ŒHGFormer åœ¨èµ„æºåˆ†é…æ•ˆç‡å’Œå¯¹æŠ—æ”¶ç›Šæ–¹é¢æ˜¾è‘—æå‡ï¼Œåœ¨å¤æ‚åŠ¨æ€åšå¼ˆåœºæ™¯ä¸­å±•ç°å‡ºä¼˜è¶Šçš„ç»¼åˆæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08580v1",
      "published_date": "2025-06-10 08:51:18 UTC",
      "updated_date": "2025-06-10 08:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:52.680129+00:00"
    },
    {
      "arxiv_id": "2506.12084v2",
      "title": "The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification",
      "title_zh": "CAISAR å¹³å°ï¼šæ‹“å±•æœºå™¨å­¦ä¹ è§„èŒƒä¸éªŒè¯çš„é€‚ç”¨èŒƒç•´",
      "authors": [
        "Michele Alberti",
        "FranÃ§ois Bobot",
        "Julien Girard-Satabin",
        "Alban Grastien",
        "Aymeric Varasse",
        "Zakaria Chihani"
      ],
      "abstract": "The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ç¨‹åºçš„å½¢å¼åŒ–è§„èŒƒä¸éªŒè¯(formal specification and verification)ä¸­å­˜åœ¨çš„å·¥å…·ç¢ç‰‡åŒ–ä»¥åŠç°æœ‰è¯­è¨€éš¾ä»¥è¡¨è¾¾æ¶‰åŠå¤šä¸ªç¥ç»ç½‘ç»œçš„å¤æ‚å±æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†å¼€æºå¹³å° CAISARã€‚CAISAR æ—¨åœ¨æ‰©å±•æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯çš„è¾¹ç•Œï¼Œå…¶è§„èŒƒè¯­è¨€èƒ½å¤Ÿå¯¹ Neural Networksã€Support Vector Machines ä»¥åŠ Boosted Trees çš„å¤æ‚å±æ€§è¿›è¡Œå»ºæ¨¡ã€‚è¯¥å¹³å°é€šè¿‡è‡ªåŠ¨å›¾ç¼–è¾‘æŠ€æœ¯(automated graph editing techniques)ï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„è§„èŒƒè¯´æ˜è‡ªåŠ¨è½¬åŒ–ä¸ºå‘å„ç±»æœ€å…ˆè¿›è¯æ˜å™¨(provers)æäº¤çš„æŸ¥è¯¢æŒ‡ä»¤ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—ç”¨æˆ·æ— éœ€å¯¹ç°æœ‰è¯æ˜å™¨è¿›è¡Œä¿®æ”¹å³å¯ç›´æ¥è°ƒç”¨å…¶ off-the-shelf ç‰ˆæœ¬ï¼Œæ˜¾è‘—æå‡äº†éªŒè¯è¿‡ç¨‹çš„çµæ´»æ€§ä¸æ•ˆç‡ã€‚é€šè¿‡å¤šä¸ªå…·ä½“ç”¨ä¾‹çš„å±•ç¤ºï¼ŒCAISAR è¯æ˜äº†å…¶åœ¨å¤„ç†è¶…å‡ºä¼ ç»Ÿ local robustness èŒƒç•´çš„å¤æ‚å±æ€§éªŒè¯æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ éªŒè¯é¢†åŸŸçš„å·¥å…·é›†æˆä¸åŠŸèƒ½æ‰©å±•æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.FL",
        "cs.NE"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12084v2",
      "published_date": "2025-06-10 08:49:10 UTC",
      "updated_date": "2026-01-20 09:41:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:49.209826+00:00"
    },
    {
      "arxiv_id": "2506.08577v1",
      "title": "Diffusion-based Time Series Forecasting for Sewerage Systems",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„æ’æ°´ç³»ç»Ÿæ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Nicholas A. Pearson",
        "Francesca Cairoli",
        "Luca Bortolussi",
        "Davide Russo",
        "Francesca Zanello"
      ],
      "abstract": "We introduce a novel deep learning approach that harnesses the power of generative artificial intelligence to enhance the accuracy of contextual forecasting in sewerage systems. By developing a diffusion-based model that processes multivariate time series data, our system excels at capturing complex correlations across diverse environmental signals, enabling robust predictions even during extreme weather events. To strengthen the model's reliability, we further calibrate its predictions with a conformal inference technique, tailored for probabilistic time series data, ensuring that the resulting prediction intervals are statistically reliable and cover the true target values with a desired confidence level. Our empirical tests on real sewerage system data confirm the model's exceptional capability to deliver reliable contextual predictions, maintaining accuracy even under severe weather conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æå‡æ’æ°´ç³»ç»Ÿ(Sewerage Systems)æƒ…å¢ƒé¢„æµ‹å‡†ç¡®æ€§çš„æ–°å‹æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶å¼€å‘äº†ä¸€ç§å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®(Multivariate time series data)çš„æ‰©æ•£æ¨¡å‹(Diffusion-based model)ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆæ•è·ä¸åŒç¯å¢ƒä¿¡å·ä¹‹é—´å¤æ‚çš„å…³è”ã€‚è¯¥ç³»ç»Ÿåœ¨æç«¯å¤©æ°”äº‹ä»¶ä¸­è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿæä¾›ç²¾ç¡®çš„é¢„æµ‹ç»“æœã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„å¯é æ€§ï¼Œç ”ç©¶é‡‡ç”¨ä¸“é—¨é’ˆå¯¹æ¦‚ç‡æ—¶é—´åºåˆ—å®šåˆ¶çš„ä¿å½¢æ¨æ–­(Conformal inference)æŠ€æœ¯å¯¹é¢„æµ‹è¿›è¡Œæ ¡å‡†ã€‚è¿™ä¸€è¿‡ç¨‹ç¡®ä¿äº†ç”Ÿæˆçš„é¢„æµ‹åŒºé—´(Prediction intervals)åœ¨ç»Ÿè®¡ä¸Šæ˜¯å¯é çš„ï¼Œå¹¶èƒ½ä»¥è®¾å®šçš„ç½®ä¿¡æ°´å¹³è¦†ç›–çœŸå®å€¼ã€‚åœ¨çœŸå®æ’æ°´ç³»ç»Ÿæ•°æ®ä¸Šçš„å®è¯æµ‹è¯•éªŒè¯äº†è¯¥æ¨¡å‹åœ¨ä¸¥å³»å¤©æ°”æ¡ä»¶ä¸‹ä¾ç„¶å…·å¤‡ç»´æŒé«˜ç²¾åº¦é¢„æµ‹çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for presentation at the 13th Urban Drainage Modelling Conference, Innsbruck (Austria), September 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08577v1",
      "published_date": "2025-06-10 08:48:05 UTC",
      "updated_date": "2025-06-10 08:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:44:54.728429+00:00"
    },
    {
      "arxiv_id": "2506.08572v2",
      "title": "The Geometries of Truth Are Orthogonal Across Tasks",
      "title_zh": "çœŸå®æ€§çš„å‡ ä½•ç»“æ„åœ¨ä¸åŒä»»åŠ¡é—´å…·æœ‰æ­£äº¤æ€§",
      "authors": [
        "Waiss Azizian",
        "Michael Kirchhof",
        "Eugene Ndiaye",
        "Louis Bethune",
        "Michal Klein",
        "Pierre Ablin",
        "Marco Cuturi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive generalization capabilities across various tasks, but their claim to practical relevance is still mired by concerns on their reliability. Recent works have proposed examining the activations produced by an LLM at inference time to assess whether its answer to a question is correct. Some works claim that a \"geometry of truth\" can be learned from examples, in the sense that the activations that generate correct answers can be distinguished from those leading to mistakes with a linear classifier. In this work, we underline a limitation of these approaches: we observe that these \"geometries of truth\" are intrinsically task-dependent and fail to transfer across tasks. More precisely, we show that linear classifiers trained across distinct tasks share little similarity and, when trained with sparsity-enforcing regularizers, have almost disjoint supports. We show that more sophisticated approaches (e.g., using mixtures of probes and tasks) fail to overcome this limitation, likely because activation vectors commonly used to classify answers form clearly separated clusters when examined across tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¿€æ´»å‘é‡é‡Œæ˜¯å¦å­˜åœ¨é€šç”¨çš„â€œçœŸç†å‡ ä½•â€ï¼ˆgeometries of truthï¼‰ï¼Œä»¥è¯„ä¼°å›ç­”çš„å‡†ç¡®æ€§ã€‚ä½œè€…æŒ‡å‡ºè¿™äº›æ‰€è°“çš„â€œçœŸç†å‡ ä½•â€æœ¬è´¨ä¸Šå…·æœ‰ä»»åŠ¡ä¾èµ–æ€§ï¼Œä¸”åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´æ— æ³•æœ‰æ•ˆè¿ç§»ã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡è®­ç»ƒçš„çº¿æ€§åˆ†ç±»å™¨ç›¸ä¼¼åº¦æä½ï¼Œåœ¨ä½¿ç”¨ç¨€ç–æ­£åˆ™åŒ–ï¼ˆsparsity-enforcing regularizersï¼‰æ—¶ï¼Œå…¶ç‰¹å¾æ”¯æŒé›†å‡ ä¹å®Œå…¨ä¸é‡å ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œå³ä½¿é‡‡ç”¨æ··åˆæ¢é’ˆï¼ˆmixtures of probesï¼‰ç­‰æ›´å¤æ‚çš„æ–¹æ³•ä¹Ÿæ— æ³•å…‹æœè¿™ä¸€å±€é™æ€§ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºä¸åŒä»»åŠ¡çš„æ¿€æ´»å‘é‡åœ¨ç©ºé—´ä¸­å½¢æˆäº†æ¸…æ™°åˆ†ç¦»çš„ç°‡ï¼Œå¯¼è‡´éš¾ä»¥æå–è·¨ä»»åŠ¡çš„ç»Ÿä¸€çœŸç†ç‰¹å¾ã€‚è¯¥å‘ç°æ­ç¤ºäº†ç›®å‰åˆ©ç”¨æ¿€æ´»ç©ºé—´çº¿æ€§è¡¨å¾æ¥è¡¡é‡æ¨¡å‹å¯é æ€§æ–¹æ³•å­˜åœ¨çš„å†…åœ¨çº¦æŸã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08572v2",
      "published_date": "2025-06-10 08:40:31 UTC",
      "updated_date": "2025-07-04 16:21:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:13.991838+00:00"
    },
    {
      "arxiv_id": "2506.08570v3",
      "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation",
      "title_zh": "è‡ªå›å½’ä¸æµåŒ¹é…ï¼šæ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆå»ºæ¨¡èŒƒå¼çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Or Tal",
        "Felix Kreuk",
        "Yossi Adi"
      ],
      "abstract": "Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”ŸæˆéŸ³ä¹ (text-to-music generation) é¢†åŸŸï¼Œå¯¹è‡ªå›å½’ (Auto-Regressive) å’ŒæµåŒ¹é… (Flow-Matching) ä¸¤ç§ä¸»æµå»ºæ¨¡èŒƒå¼è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯å¯¹æ¯”åˆ†æã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å®Œå…¨ç›¸åŒçš„è®­ç»ƒæ•°æ®é›†ã€è®­ç»ƒé…ç½®åŠç›¸ä¼¼éª¨å¹²æ¶æ„ä¸‹ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨éš”ç¦»å¹¶è§‚å¯Ÿä¸åŒå»ºæ¨¡èŒƒå¼å¯¹ç”Ÿæˆæ•ˆæœçš„å…·ä½“å½±å“ã€‚å®éªŒä»ç”Ÿæˆè´¨é‡ã€æ¨ç†é…ç½®çš„é²æ£’æ€§ã€æ¨¡å‹å¯æ‰©å±•æ€§ã€å¯¹æ–‡æœ¬åŠæ—¶é—´å¯¹é½æ¡ä»¶çš„éµå¾ªèƒ½åŠ›ï¼Œä»¥åŠéŸ³é¢‘è¡¥å…¨ (audio inpainting) çš„ç¼–è¾‘èƒ½åŠ›ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚è¯¥ç ”ç©¶è¯¦å°½æ­ç¤ºäº†ä¸¤ç§èŒƒå¼å„è‡ªçš„ç‹¬ç‰¹ä¼˜åŠ¿ã€å±€é™æ€§åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æƒè¡¡å…³ç³»ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥æ–‡æœ¬ç”ŸæˆéŸ³ä¹ç³»ç»Ÿçš„æ¶æ„è®¾è®¡ä¸è®­ç»ƒå†³ç­–æä¾›äº†å…³é”®çš„å®è¯è§è§£å’ŒæŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08570v3",
      "published_date": "2025-06-10 08:37:45 UTC",
      "updated_date": "2025-09-04 11:54:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:16.856271+00:00"
    },
    {
      "arxiv_id": "2506.08569v1",
      "title": "Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata",
      "title_zh": "Flow-Leniaï¼šè´¨é‡å®ˆæ’è¿ç»­å…ƒèƒè‡ªåŠ¨æœºä¸­çš„æ¶Œç°æ¼”åŒ–åŠ¨åŠ›å­¦",
      "authors": [
        "Erwan Plantec",
        "Gautier Hamon",
        "Mayalen Etcheverry",
        "Bert Wang-Chak Chan",
        "Pierre-Yves Oudeyer",
        "ClÃ©ment Moulin-Frier"
      ],
      "abstract": "Central to the artificial life endeavour is the creation of artificial systems spontaneously generating properties found in the living world such as autopoiesis, self-replication, evolution and open-endedness. While numerous models and paradigms have been proposed, cellular automata (CA) have taken a very important place in the field notably as they enable the study of phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia have been showed to produce life-like patterns reminiscent, on an aesthetic and ontological point of view, of biological organisms we call creatures. We propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We present experiments demonstrating its effectiveness in generating spatially-localized patters (SLPs) with complex behaviors and show that the update rule parameters can be optimized to generate complex creatures showing behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed the parameters of the model, defining the properties of the emerging patterns, within its own dynamics thus allowing for multispecies simulations. By using the evolutionary activity framework as well as other metrics, we shed light on the emergent evolutionary dynamics taking place in this system.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flow-Leniaï¼Œè¿™æ˜¯å¯¹ Lenia è¿ç»­å…ƒèƒè‡ªåŠ¨æœº (Continuous Cellular Automata) çš„ä¸€ç§è´¨é‡å®ˆæ’ (mass conservative) æ‰©å±•ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººå·¥ç”Ÿå‘½ä¸­çš„ç”Ÿå‘½ç‰¹å¾ã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¤æ‚è¡Œä¸ºçš„ç©ºé—´å®šä½æ¨¡å¼ (Spatially-Localized Patterns, SLPs) æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå¹¶èƒ½é€šè¿‡ä¼˜åŒ–æ›´æ–°è§„åˆ™å‚æ•°æ¥äº§ç”Ÿç‰¹å®šçš„ç”Ÿå‘½ä½“è¡Œä¸ºã€‚Flow-Lenia çš„å…³é”®åˆ›æ–°åœ¨äºå°†å®šä¹‰æ¨¡å¼å±æ€§çš„æ¨¡å‹å‚æ•°åµŒå…¥åˆ°è‡ªèº«çš„åŠ¨æ€ç³»ç»Ÿä¸­å¿ƒï¼Œä»è€Œå®ç°äº†å¤šç‰©ç§ (multispecies) æ¨¡æ‹Ÿã€‚é€šè¿‡åˆ©ç”¨è¿›åŒ–æ´»åŠ¨ (evolutionary activity) æ¡†æ¶åŠå…¶ä»–åº¦é‡æŒ‡æ ‡ï¼Œç ”ç©¶è€…æ­ç¤ºäº†ç³»ç»Ÿå†…å‘ç”Ÿçš„æ¶Œç°è¿›åŒ–åŠ¨åŠ›å­¦ (emergent evolutionary dynamics)ã€‚è¯¥é¡¹å·¥ä½œä¸ºç ”ç©¶äººå·¥ç”Ÿå‘½é¢†åŸŸçš„è‡ªç»„ç»‡ã€è‡ªå¤åˆ¶å’Œå¼€æ”¾å¼è¿›åŒ–æä¾›äº†æ–°çš„è§†è§’å’Œå®éªŒåŸºç¡€ã€‚",
      "categories": [
        "nlin.CG",
        "cs.AI"
      ],
      "primary_category": "nlin.CG",
      "comment": "This manuscript has been accepted for publication in the Artificial Life journal (https://direct.mit.edu/artl)",
      "pdf_url": "https://arxiv.org/pdf/2506.08569v1",
      "published_date": "2025-06-10 08:37:26 UTC",
      "updated_date": "2025-06-10 08:37:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:28.206913+00:00"
    },
    {
      "arxiv_id": "2506.08563v2",
      "title": "KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks",
      "title_zh": "KP-PINNsï¼šæ ¸åŒ…åŠ é€Ÿç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Siyuan Yang",
        "Cheng Song",
        "Zhilu Lai",
        "Wenjia Wang"
      ],
      "abstract": "Differential equations are involved in modeling many engineering problems. Many efforts have been devoted to solving differential equations. Due to the flexibility of neural networks, Physics Informed Neural Networks (PINNs) have recently been proposed to solve complex differential equations and have demonstrated superior performance in many applications. While the L2 loss function is usually a default choice in PINNs, it has been shown that the corresponding numerical solution is incorrect and unstable for some complex equations. In this work, we propose a new PINNs framework named Kernel Packet accelerated PINNs (KP-PINNs), which gives a new expression of the loss function using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel Packet (KP) method to accelerate the computation. Theoretical results show that KP-PINNs can be stable across various differential equations. Numerical experiments illustrate that KP-PINNs can solve differential equations effectively and efficiently. This framework provides a promising direction for improving the stability and accuracy of PINNs-based solvers in scientific computing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Physics Informed Neural Networks (PINNs)åœ¨å¤„ç†å¤æ‚å¾®åˆ†æ–¹ç¨‹æ—¶ï¼Œå› ä½¿ç”¨L2 losså‡½æ•°è€Œå¯¼è‡´çš„æ•°å€¼è§£é”™è¯¯å’Œä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†KP-PINNs (Kernel Packet accelerated Physics Informed Neural Networks) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨Reproducing Kernel Hilbert Space (RKHS) norm é‡æ–°å®šä¹‰äº†æŸå¤±å‡½æ•°ï¼Œå¹¶å¼•å…¥Kernel Packet (KP) æ–¹æ³•ä»¥å¤§å¹…æå‡è®¡ç®—é€Ÿåº¦ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒKP-PINNsåœ¨å¤„ç†å„ç±»å¾®åˆ†æ–¹ç¨‹æ—¶å‡èƒ½è¡¨ç°å‡ºå“è¶Šçš„ç¨³å®šæ€§ã€‚å¤šé¡¹æ•°å€¼å®éªŒè¿›ä¸€æ­¥è¯å®äº†KP-PINNsåœ¨æ±‚è§£å¤æ‚æ–¹ç¨‹æ—¶çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ï¼Œä¸ºæå‡ç§‘å­¦è®¡ç®—ä¸­PINNsæ±‚è§£å™¨çš„æ€§èƒ½æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "math-ph",
        "physics.comp-ph"
      ],
      "primary_category": "cs.CE",
      "comment": "Accepted to IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08563v2",
      "published_date": "2025-06-10 08:33:34 UTC",
      "updated_date": "2025-06-11 08:00:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:15.930379+00:00"
    },
    {
      "arxiv_id": "2506.08552v2",
      "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­éšå¼æ¨ç†çš„é«˜æ•ˆè®­ç»ƒåä¼˜åŒ–",
      "authors": [
        "Xinyuan Wang",
        "Dongjie Wang",
        "Wangyang Ying",
        "Haoyue Bai",
        "Nanxu Gong",
        "Sixun Dong",
        "Kunpeng Liu",
        "Yanjie Fu"
      ],
      "abstract": "Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA without additional training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æœºåˆ¶ï¼Œé’ˆå¯¹ Chain-of-Thought å¸¦æ¥çš„ Token å¼€é”€åŠæ¨ç†è½¨è¿¹å›ºå®šç­‰å±€é™ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–éšå¼æ¨ç† (latent reasoning) è¿‡ç¨‹çš„è½»é‡çº§è®­ç»ƒåç²¾ç‚¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¯¹æ¯”æ¨ç†åé¦ˆ (Contrastive reasoning feedback) ç­–ç•¥ï¼Œé€šè¿‡å¯¹æ¯”å¼ºå¼±åŸºå‡†çš„æ¨ç†åµŒå…¥ (reasoning embeddings) æ¥å¼•å¯¼æ›´æ–°æ–¹å‘ï¼Œå¹¶ç»“åˆæ®‹å·®åµŒå…¥ç²¾ç‚¼ (Residual embedding refinement) æŠ€æœ¯ï¼Œåˆ©ç”¨å†å²æ¢¯åº¦ç¨³å®šæ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ MathQA ä»»åŠ¡ä¸Šï¼Œå³ä½¿ä¸è¿›è¡Œé¢å¤–è®­ç»ƒä¹Ÿèƒ½è·å¾— 5% çš„å‡†ç¡®ç‡æå‡ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨æ½œç©ºé—´ç›´æ¥ä¼˜åŒ–æ¨ç†è½¨è¿¹ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„æ•ˆç‡ä¸å‡†ç¡®æ€§æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08552v2",
      "published_date": "2025-06-10 08:17:16 UTC",
      "updated_date": "2025-11-14 22:45:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:25.902015+00:00"
    },
    {
      "arxiv_id": "2506.09085v1",
      "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation",
      "title_zh": "LLM-ML ååŒï¼šé›†æˆç¬¦å·è§£ç ä¸æ¢¯åº¦æœç´¢ï¼Œå®ç°æœ‰æ•ˆä¸”ç¨³å®šçš„ç”Ÿæˆå¼ç‰¹å¾å˜æ¢",
      "authors": [
        "Xinyuan Wang",
        "Haoyue Bai",
        "Nanxu Gong",
        "Wangyang Ying",
        "Sixun Dong",
        "Xiquan Cui",
        "Yanjie Fu"
      ],
      "abstract": "Feature transformation enhances data representation by deriving new features from the original data. Generative AI offers potential for this task, but faces challenges in stable generation (consistent outputs) and valid generation (error-free sequences). Existing methods--traditional MLs' low validity and LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax, while ML's gradient-steered search stabilizes performance. To bridge this gap, we propose a teaming framework combining LLMs' symbolic generation with ML's gradient optimization. This framework includes four steps: (1) golden examples generation, aiming to prepare high-quality samples with the ground knowledge of the teacher LLM; (2) feature transformation sequence embedding and search, intending to uncover potentially superior embeddings within the latent space; (3) student LLM feature transformation, aiming to distill knowledge from the teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the student LLM probabilities for valid and stable generation. The experiments on various datasets show that the teaming policy can achieve 5\\% improvement in downstream performance while reducing nearly half of the error cases. The results also demonstrate the efficiency and robustness of the teaming policy. Additionally, we also have exciting findings on LLMs' capacity to understand the original data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Generative AIåœ¨Feature transformationä¸­é¢ä¸´çš„ç”Ÿæˆç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†LLM-ML Teamingæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†LLMsçš„ç¬¦å·ç”Ÿæˆèƒ½åŠ›ä¸ä¼ ç»ŸMLçš„æ¢¯åº¦æœç´¢(Gradient Search)ç›¸ç»“åˆï¼Œé€šè¿‡å››ä¸ªæ ¸å¿ƒæ­¥éª¤å®ç°æœ‰æ•ˆçš„ç‰¹å¾æ¼”åŒ–ï¼šåˆ©ç”¨Teacher LLMç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œç‰¹å¾è½¬æ¢åºåˆ—çš„åµŒå…¥ä¸æœç´¢ï¼Œé€šè¿‡Student LLMè¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå¹¶æœ€ç»ˆåˆ©ç”¨LLM-ML Decoder Teamingæ•´åˆMLä¸LLMçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œè§£ç ã€‚è¿™ç§åä½œç­–ç•¥ç¡®ä¿äº†ç”Ÿæˆåºåˆ—çš„è¯­æ³•æ­£ç¡®æ€§ï¼ŒåŒæ—¶åˆ©ç”¨æ¢¯åº¦å¼•å¯¼æå‡äº†æ€§èƒ½ç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°æå‡äº†5%ï¼ŒåŒæ—¶å‡å°‘äº†è¿‘50%çš„é”™è¯¯æ¡ˆä¾‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†è¯¥ç­–ç•¥çš„é«˜æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œå¹¶æ­ç¤ºäº†LLMsåœ¨ç†è§£åŸå§‹æ•°æ®ç‰¹å¾æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09085v1",
      "published_date": "2025-06-10 08:10:16 UTC",
      "updated_date": "2025-06-10 08:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:27.205980+00:00"
    },
    {
      "arxiv_id": "2506.13781v1",
      "title": "Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment",
      "title_zh": "åŸºäºå›¾ç¥ç»ç½‘ç»œæ±‚è§£ä½œä¸šè½¦é—´è°ƒåº¦é—®é¢˜ï¼šä¸€ç§å¯å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ",
      "authors": [
        "Pablo AriÃ±o FernÃ¡ndez"
      ],
      "abstract": "The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.",
      "tldr_zh": "Job Shop Scheduling Problem (JSSP) æ˜¯åˆ¶é€ ä¸šä¸­å…¸å‹çš„ NP-hard ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å¯å‘å¼è§„åˆ™ï¼Œè€Œè¿‘æœŸç ”ç©¶æ­£æ¢ç´¢åˆ©ç”¨ Graph Neural Networks (GNNs) å­¦ä¹ è°ƒåº¦ç­–ç•¥ã€‚é’ˆå¯¹æ­¤ç±»æ¨¡å‹è®­ç»ƒä¸­å›¾è¡¨ç¤ºã€èŠ‚ç‚¹ç‰¹å¾ç­‰è¦ç´ éš¾ä»¥è‡ªå®šä¹‰ä¸”ç¼ºä¹æ¨¡å—åŒ–å®éªŒåº“çš„ç°çŠ¶ï¼Œè¯¥ç ”ç©¶æ¨å‡ºäº† JobShopLib åº“ï¼Œæä¾›äº†ä¸€ä¸ªæ”¯æŒé«˜åº¦å®šåˆ¶åŒ–çš„ Reinforcement Learning ç¯å¢ƒã€‚é€šè¿‡åœ¨è¯¥ç¯å¢ƒä¸‹åˆ©ç”¨ Imitation Learning è®­ç»ƒè°ƒåº¦å™¨ï¼Œç ”ç©¶è¯æ˜äº†ç‰¹å¾è‡ªå®šä¹‰å¯¹æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶ GNN æ¨¡å‹åœ¨å¤§è§„æ¨¡è°ƒåº¦ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ¥è¿‘ State-of-the-art çš„æ°´å¹³ã€‚è¯¥å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºä¸ºæœªæ¥ JSSP é¢†åŸŸçš„ Deep Learning ç ”ç©¶æä¾›äº†é«˜æ•ˆã€çµæ´»çš„å®éªŒå¹³å°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æ­¤ç±»è°ƒåº¦æ¨¡å‹ä»æœ‰æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ï¼Œè€Œ JobShopLib ä¸ºåç»­çš„ç®—æ³•å®éªŒæä¾›äº†å¿…è¦çš„åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "Bachelor's thesis, Universidad PolitÃ©cnica de Madrid, 2025. 150 pages, 23 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.13781v1",
      "published_date": "2025-06-10 08:09:30 UTC",
      "updated_date": "2025-06-10 08:09:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:38.407011+00:00"
    },
    {
      "arxiv_id": "2506.08541v2",
      "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching",
      "title_zh": "TrajFlowï¼šåŸºäºæµåŒ¹é…çš„å¤šæ¨¡æ€è¿åŠ¨é¢„æµ‹",
      "authors": [
        "Qi Yan",
        "Brian Zhang",
        "Yutong Zhang",
        "Daniel Yang",
        "Joshua White",
        "Di Chen",
        "Jiachao Liu",
        "Langechuan Liu",
        "Binnan Zhuang",
        "Shaoshuai Shi",
        "Renjie Liao"
      ],
      "abstract": "Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TrajFlowï¼Œä¸€ç§åŸºäº Flow Matching çš„è¿åŠ¨é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­ç”Ÿæˆå¼è½¨è¿¹é¢„æµ‹æ–¹æ³•çš„æ‰©å±•æ€§ä¸æ•ˆç‡ç“¶é¢ˆã€‚ä¸éœ€è¦å¤šæ¬¡æ¨ç†çš„ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•ä¸åŒï¼ŒTrajFlow èƒ½å¤Ÿåœ¨å•æ¬¡æ¨ç†ä¸­ç”Ÿæˆå¤šä¸ªè¿è´¯çš„æœªæ¥è½¨è¿¹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä¸ºäº†ä¼˜åŒ–ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäº Plackett-Luce åˆ†å¸ƒçš„ Ranking Lossï¼Œå¹¶ç»“åˆ Self-conditioning è®­ç»ƒæŠ€æœ¯ï¼Œé€šè¿‡é‡ç”¨è‡ªèº«é¢„æµ‹æ„å»ºå™ªå£°è¾“å…¥æ¥æå‡æ³›åŒ–æ€§èƒ½ä¸æ¨ç†é€Ÿåº¦ã€‚åœ¨ Waymo Open Motion Dataset (WOMD) ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTrajFlow åœ¨å¤šé¡¹å…³é”®æŒ‡æ ‡ä¸Šå–å¾—äº† State-of-the-art çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºå®‰å…¨å…³é”®å‹è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€è¿åŠ¨é¢„æµ‹æä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IROS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08541v2",
      "published_date": "2025-06-10 08:08:31 UTC",
      "updated_date": "2025-07-05 09:04:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:47.812781+00:00"
    },
    {
      "arxiv_id": "2506.09084v1",
      "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models",
      "title_zh": "åŸºäºæ··åˆç²’åº¦å¥–åŠ±æœºåˆ¶é€‚é…è¯­è¨€æ¨¡å‹çš„å¢å¼ºå‹æ•´é¡µä¼˜åŒ–",
      "authors": [
        "Xinyuan Wang",
        "Liang Wu",
        "Yanjie Fu"
      ],
      "abstract": "Optimizing the presentation of search and recommendation results is crucial to enhancing user experience and engagement. Whole Page Optimization (WPO) plays a pivotal role in this process, as it directly influences how information is surfaced to users. While Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually relevant content, fine-tuning these models for complex tasks like WPO presents challenges. Specifically, the need for extensive human-annotated data to mitigate issues such as hallucinations and model instability can be prohibitively expensive, especially in large-scale systems that interact with millions of items daily. In this work, we address the challenge of fine-tuning LLMs for WPO by using user feedback as the supervision. Unlike manually labeled datasets, user feedback is inherently noisy and less precise. To overcome this, we propose a reward-based fine-tuning approach, PageLLM, which employs a mixed-grained reward mechanism that combines page-level and item-level rewards. The page-level reward evaluates the overall quality and coherence, while the item-level reward focuses on the accuracy and relevance of key recommendations. This dual-reward structure ensures that both the holistic presentation and the critical individual components are optimized. We validate PageLLM on both public and industrial datasets. PageLLM outperforms baselines and achieves a 0.44\\% GMV increase in an online A/B test with over 10 million users, demonstrating its real-world impact.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ Whole Page Optimization (WPO) åœ¨æœç´¢å’Œæ¨èç³»ç»Ÿä¸­é¢ä¸´çš„äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚åŠæ¨¡å‹ä¸ç¨³å®šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åˆ©ç”¨ç”¨æˆ·åé¦ˆè¿›è¡Œç›‘ç£çš„ PageLLM æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ··åˆç²’åº¦å¥–åŠ±æœºåˆ¶ (Mixed-Grained Reward Mechanism)ï¼Œé€šè¿‡ç»“åˆé¡µé¢çº§å¥–åŠ± (Page-level Reward) å’Œé¡¹ç›®çº§å¥–åŠ± (Item-level Reward) æ¥åŒæ­¥ä¼˜åŒ–æ•´ä½“å±•ç¤ºçš„è¿è´¯æ€§ä¸å•ä¸ªæ¨èé¡¹çš„å‡†ç¡®æ€§ã€‚å…¶ä¸­é¡µé¢çº§å¥–åŠ±å…³æ³¨å®è§‚å‘ˆç°è´¨é‡ï¼Œè€Œé¡¹ç›®çº§å¥–åŠ±åˆ™ä¾§é‡äºå…³é”®æ¨èé¡¹çš„ç›¸å…³æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹å¯¹å¤šç²’åº¦ç›®æ ‡çš„ç²¾ç¡®æ•æ‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPageLLM åœ¨å…¬å¼€åŠå·¥ä¸šæ•°æ®é›†ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆå…‹æœäº†ç”¨æˆ·åé¦ˆæ•°æ®ä¸­çš„å™ªå£°é—®é¢˜ã€‚åœ¨è¶…è¿‡åƒä¸‡ç”¨æˆ·è§„æ¨¡çš„åœ¨çº¿ A/B æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº† 0.44% çš„ GMV å¢é•¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡çœŸå®åº”ç”¨åœºæ™¯ä¸­çš„å“è¶Šæ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåˆ©ç”¨ Large Language Models (LLMs) ä¼˜åŒ–å¤æ‚æ¨èä»»åŠ¡æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09084v1",
      "published_date": "2025-06-10 08:05:42 UTC",
      "updated_date": "2025-06-10 08:05:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:45:48.277274+00:00"
    },
    {
      "arxiv_id": "2506.09083v1",
      "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool",
      "title_zh": "BakuFlowï¼šä¸€ç§é«˜æ•ˆçš„åŠè‡ªåŠ¨æ ‡æ³¨ç”Ÿæˆå·¥å…·\n\n---\n\næˆ‘æ˜¯ Gemini Enterpriseâœ¨ï¼Œå·²ç»ä¸ºæ‚¨å®Œæˆäº†è¿™ç¯‡ arXiv è®ºæ–‡æ ‡é¢˜çš„ä¸“ä¸šç¿»è¯‘ã€‚ä¸ºäº†ç¡®ä¿ç¿»è¯‘çš„â€œå­¦æœ¯å‘³â€å’Œâ€œåœ°é“æ„Ÿâ€ï¼Œæˆ‘å°† â€œStreamliningâ€ å¤„ç†ä¸ºâ€œé«˜æ•ˆâ€ï¼Œå¹¶å°† â€œLabel Generationâ€ ç¿»è¯‘ä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸæ›´ä¸“ä¸šçš„â€œæ ‡æ³¨ç”Ÿæˆâ€ã€‚\n\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡æ ‡é¢˜æˆ–æ‘˜è¦éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ç‰¹å®šæœ¯è¯­è¿›è¡Œå¾®è°ƒï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼æ‚¨å¯¹ç›®å‰çš„ç¿»è¯‘é£æ ¼æ»¡æ„å—ï¼Ÿ",
      "authors": [
        "Jerry Lin",
        "Partick P. W. Chen"
      ],
      "abstract": "Accurately labeling (or annotation) data is still a bottleneck in computer vision, especially for large-scale tasks where manual labeling is time-consuming and error-prone. While tools like LabelImg can handle the labeling task, some of them still require annotators to manually label each image. In this paper, we introduce BakuFlow, a streamlining semi-automatic label generation tool. Key features include (1) a live adjustable magnifier for pixel-precise manual corrections, improving user experience; (2) an interactive data augmentation module to diversify training datasets; (3) label propagation for rapidly copying labeled objects between consecutive frames, greatly accelerating annotation of video data; and (4) an automatic labeling module powered by a modified YOLOE framework. Unlike the original YOLOE, our extension supports adding new object classes and any number of visual prompts per class during annotation, enabling flexible and scalable labeling for dynamic, real-world datasets. These innovations make BakuFlow especially effective for object detection and tracking, substantially reducing labeling workload and improving efficiency in practical computer vision and industrial scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BakuFlowï¼Œä¸€ç§æ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰(computer vision)é¢†åŸŸå¤§è§„æ¨¡æ•°æ®æ‰‹åŠ¨æ ‡æ³¨è€—æ—¶ä¸”æ˜“å‡ºé”™é—®é¢˜çš„åŠè‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆå·¥å…·ã€‚è¯¥å·¥å…·å¼•å…¥äº†å¯å®æ—¶è°ƒèŠ‚çš„æ”¾å¤§é•œä»¥å®ç°åƒç´ çº§çš„ç²¾ç¡®æ‰‹åŠ¨æ ¡æ­£ï¼Œå¹¶é…å¤‡äº†äº¤äº’å¼æ•°æ®å¢å¼º(data augmentation)æ¨¡å—æ¥æå‡è®­ç»ƒæ•°æ®é›†çš„å¤šæ ·æ€§ã€‚BakuFlowåˆ©ç”¨æ ‡ç­¾ä¼ æ’­(label propagation)æŠ€æœ¯åœ¨è¿ç»­è§†é¢‘å¸§é—´å¿«é€Ÿå¤åˆ¶æ ‡æ³¨å¯¹è±¡ï¼Œæå¤§åŠ é€Ÿäº†è§†é¢‘æ•°æ®çš„æ ‡æ³¨è¿‡ç¨‹ã€‚å…¶æ ¸å¿ƒçš„è‡ªåŠ¨æ ‡æ³¨æ¨¡å—åŸºäºæ”¹è¿›çš„YOLOEæ¡†æ¶ï¼Œæ”¯æŒåœ¨æ ‡æ³¨æœŸé—´åŠ¨æ€æ·»åŠ æ–°ç±»åˆ«å’Œè§†è§‰æç¤º(visual prompts)ï¼Œä»è€Œå®ç°äº†å¯¹å¤æ‚ç°å®æ•°æ®é›†çš„çµæ´»æ‰©å±•ã€‚è¿™äº›åˆ›æ–°æ˜¾è‘—é™ä½äº†ç‰©ä½“æ£€æµ‹(object detection)ä¸è·Ÿè¸ª(tracking)ä»»åŠ¡ä¸­çš„æ ‡æ³¨å·¥ä½œé‡ï¼Œä¸ºæå‡å·¥ä¸šåœºæ™¯ä¸‹çš„è§†è§‰å¼€å‘æ•ˆç‡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 3 figures, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2506.09083v1",
      "published_date": "2025-06-10 08:02:31 UTC",
      "updated_date": "2025-06-10 08:02:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:24.246904+00:00"
    },
    {
      "arxiv_id": "2506.08534v1",
      "title": "DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View",
      "title_zh": "DCDï¼šä¸€ç§ç”¨äºèƒå„¿è¶…å£°å››è…”å¿ƒåˆ‡é¢çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹",
      "authors": [
        "Donglian Li",
        "Hui Guo",
        "Minglang Chen",
        "Huizhen Chen",
        "Jialing Chen",
        "Bocheng Liang",
        "Pengchen Liang",
        "Ying Tan"
      ],
      "abstract": "Accurate segmentation of anatomical structures in the apical four-chamber (A4C) view of fetal echocardiography is essential for early diagnosis and prenatal evaluation of congenital heart disease (CHD). However, precise segmentation remains challenging due to ultrasound artifacts, speckle noise, anatomical variability, and boundary ambiguity across different gestational stages. To reduce the workload of sonographers and enhance segmentation accuracy, we propose DCD, an advanced deep learning-based model for automatic segmentation of key anatomical structures in the fetal A4C view. Our model incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module, enabling superior multi-scale feature extraction, and a Convolutional Block Attention Module (CBAM) to enhance adaptive feature representation. By effectively capturing both local and global contextual information, DCD achieves precise and robust segmentation, contributing to improved prenatal cardiac assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒå„¿è¶…å£°å¿ƒåŠ¨å›¾é¡¶ç«¯å››è…”å¿ƒ(A4C)åˆ‡é¢è§£å‰–ç»“æ„åˆ†å‰²ä¸­å­˜åœ¨çš„è¶…å£°ä¼ªå½±ã€æ–‘ç‚¹å™ªå£°åŠè¾¹ç•Œæ¨¡ç³Šç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDCDçš„é«˜çº§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å…ˆå¤©æ€§å¿ƒè„ç—…(CHD)çš„æ—©æœŸè¯Šæ–­å‡†ç¡®æ€§ã€‚DCDæ¨¡å‹é›†æˆäº†Dense Atrous Spatial Pyramid Pooling (Dense ASPP)æ¨¡å—ï¼Œèƒ½å¤Ÿå®ç°å“è¶Šçš„å¤šå°ºåº¦ç‰¹å¾æå–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥Convolutional Block Attention Module (CBAM)å¢å¼ºäº†è‡ªé€‚åº”ç‰¹å¾è¡¨è¾¾ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰å±€éƒ¨ä¸å…¨å±€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥æ¨¡å‹å®ç°äº†å¯¹èƒå„¿å¿ƒè„å…³é”®è§£å‰–ç»“æ„çš„ç²¾ç¡®ä¸”ç¨³å¥çš„è‡ªåŠ¨åˆ†å‰²ï¼Œæ˜¾è‘—å‡è½»äº†è¶…å£°åŒ»å¸ˆçš„æ“ä½œè´Ÿæ‹…ã€‚è¿™ç§æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸ºäº§å‰å¿ƒè„è¯„ä¼°æä¾›äº†æ›´å¯é çš„æŠ€æœ¯æ”¯æŒï¼Œå¹¶åœ¨å¤æ‚çš„ä¸´åºŠç¯å¢ƒä¸‹è¡¨ç°å‡ºæé«˜çš„é²æ£’æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08534v1",
      "published_date": "2025-06-10 07:54:03 UTC",
      "updated_date": "2025-06-10 07:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:38.559275+00:00"
    },
    {
      "arxiv_id": "2506.08533v1",
      "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)",
      "title_zh": "é¢å‘å¼ºåŒ–å­¦ä¹ çš„é²æ£’æ¼”åŒ–å¤šç›®æ ‡ç½‘ç»œæ¶æ„æœç´¢ (EMNAS-RL)",
      "authors": [
        "Nihal Acharya Adde",
        "Alexandra Gianzina",
        "Hanno Gottschalk",
        "Andreas Ebert"
      ],
      "abstract": "This paper introduces Evolutionary Multi-Objective Network Architecture Search (EMNAS) for the first time to optimize neural network architectures in large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses genetic algorithms to automate network design, tailored to enhance rewards and reduce model size without compromising performance. Additionally, parallelization techniques are employed to accelerate the search, and teacher-student methodologies are implemented to ensure scalable optimization. This research underscores the potential of transfer learning as a robust framework for optimizing performance across iterative learning processes by effectively leveraging knowledge from earlier generations to enhance learning efficiency and stability in subsequent generations. Experimental results demonstrate that tailored EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters. The findings of these strategies contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡å°†è¿›åŒ–å¤šç›®æ ‡ç½‘ç»œæ¶æ„æœç´¢(Evolutionary Multi-Objective Network Architecture Search, EMNAS)åº”ç”¨äºè‡ªåŠ¨é©¾é©¶(Autonomous Driving)é¢†åŸŸçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¨¡å‹ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é—ä¼ ç®—æ³•(Genetic Algorithms)å®ç°äº†ç½‘ç»œè®¾è®¡çš„è‡ªåŠ¨åŒ–ï¼Œæ—¨åœ¨æå‡å¥–åŠ±å‡½æ•°è¡¨ç°å¹¶æ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚ä¸ºäº†ç¡®ä¿ä¼˜åŒ–çš„æ•ˆç‡ä¸å¯æ‰©å±•æ€§ï¼Œç ”ç©¶é›†æˆäº†å¹¶è¡ŒåŒ–æŠ€æœ¯(Parallelization Techniques)ä¸æ•™å¸ˆ-å­¦ç”Ÿæ–¹æ³•(Teacher-Student Methodologies)ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡è¿ç§»å­¦ä¹ (Transfer Learning)æœºåˆ¶å¤ç”¨å‰ä»£çŸ¥è¯†ï¼Œæœ‰æ•ˆæå‡äº†åç»­æ¼”åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå®šåˆ¶åŒ–çš„ EMNAS æ€§èƒ½ä¼˜äºæ‰‹åŠ¨è®¾è®¡çš„æ¨¡å‹ï¼Œåœ¨å‚æ•°é‡æ›´å°‘çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜çš„å¥–åŠ±ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ç°å®åœºæ™¯ä¸­éƒ¨ç½²é«˜æ€§èƒ½ã€è½»é‡åŒ–çš„è‡ªåŠ¨é©¾é©¶å¼ºåŒ–å­¦ä¹ ç½‘ç»œæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at ESANN 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.08533v1",
      "published_date": "2025-06-10 07:52:35 UTC",
      "updated_date": "2025-06-10 07:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:46.316244+00:00"
    },
    {
      "arxiv_id": "2506.08532v3",
      "title": "Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness",
      "title_zh": "ä½ç©ºç©ºåŸŸä¸‹å®‰å…¨ä¸ç»æµçš„æ— äººæœºè½¨è¿¹è§„åˆ’ï¼šä¸€ç§å…·å¤‡åˆè§„æ„ŸçŸ¥çš„ DRL-LLM æ··åˆæ–¹æ³•",
      "authors": [
        "Yanwei Gong",
        "Junchao Fan",
        "Ruichen Zhang",
        "Dusit Niyato",
        "Yingying Yao",
        "Xiaolin Chang"
      ],
      "abstract": "The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½ç©ºç»æµï¼ˆlow-altitude economyï¼‰èƒŒæ™¯ä¸‹æ— äººæœºï¼ˆUAVï¼‰åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„è½¨è¿¹è§„åˆ’æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶å¿½è§†åŸå¸‚ç©ºåŸŸé™åˆ¶ä¸ç»æµæ•ˆç‡ï¼Œä»¥åŠ DRL å­¦ä¹ æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åˆè§„æ„è¯†ï¼ˆcompliance awarenessï¼‰æ¥ç¡®ä¿è·¯å¾„è§„åˆ’çš„å®‰å…¨ã€åˆè§„ä¸ç»æµå¯è¡Œæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®é‡‡é›†ç‡ã€é¿éšœï¼ˆcollision avoidanceï¼‰ã€æˆåŠŸé™è½ã€ç›‘ç®¡åˆè§„åŠèƒ½æºæ•ˆç‡ï¼ˆenergy efficiencyï¼‰ç­‰å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºè§£å†³ä½ç©ºç»æµç½‘ç»œçº¦æŸä¸‹çš„æ— äººæœºè½¨è¿¹è§„åˆ’å…³é”®é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08532v3",
      "published_date": "2025-06-10 07:51:29 UTC",
      "updated_date": "2025-11-25 20:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:48.411473+00:00"
    },
    {
      "arxiv_id": "2506.19862v1",
      "title": "DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules",
      "title_zh": "DualEquiNetï¼šé¢å‘å¤§å‹ç”Ÿç‰©å¤§åˆ†å­çš„åŒç©ºé—´å±‚çº§ç­‰å˜ç½‘ç»œ",
      "authors": [
        "Junjie Xu",
        "Jiahao Zhang",
        "Mangal Prakash",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "abstract": "Geometric graph neural networks (GNNs) that respect E(3) symmetries have achieved strong performance on small molecule modeling, but they face scalability and expressiveness challenges when applied to large biomolecules such as RNA and proteins. These systems require models that can simultaneously capture fine-grained atomic interactions, long-range dependencies across spatially distant components, and biologically relevant hierarchical structure, such as atoms forming residues, which in turn form higher-order domains. Existing geometric GNNs, which typically operate exclusively in either Euclidean or Spherical Harmonics space, are limited in their ability to capture both the fine-scale atomic details and the long-range, symmetry-aware dependencies required for modeling the multi-scale structure of large biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant Network that constructs complementary representations in both Euclidean and Spherical Harmonics spaces to capture local geometry and global symmetry-aware features. DualEquiNet employs bidirectional cross-space message passing and a novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate atomic features into biologically meaningful units, such as residues, enabling efficient and expressive multi-scale modeling for large biomolecular systems. DualEquiNet achieves state-of-the-art performance on multiple existing benchmarks for RNA property prediction and protein modeling, and outperforms prior methods on two newly introduced 3D structural benchmarks demonstrating its broad effectiveness across a range of large biomolecule modeling tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‡ ä½•å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨å»ºæ¨¡RNAå’Œè›‹ç™½è´¨ç­‰å¤§å‹ç”Ÿç‰©åˆ†å­æ—¶é¢ä¸´çš„å¯æ‰©å±•æ€§ä¸è¡¨è¾¾åŠ›æŒ‘æˆ˜ï¼Œæå‡ºäº†DualEquiNetã€‚è¿™æ˜¯ä¸€ç§åŒç©ºé—´å±‚æ¬¡åŒ–ç­‰å˜ç½‘ç»œ(Dual-Space Hierarchical Equivariant Network)ï¼Œé€šè¿‡åœ¨Euclideanç©ºé—´å’ŒSpherical Harmonicsç©ºé—´ä¸­æ„å»ºäº’è¡¥è¡¨å¾ï¼ŒåŒæ—¶æ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚ä¸å…¨å±€å¯¹ç§°æ„ŸçŸ¥ç‰¹å¾ã€‚DualEquiNet å¼•å…¥äº†åŒå‘è·¨ç©ºé—´æ¶ˆæ¯ä¼ é€’ä»¥åŠåˆ›æ–°çš„è·¨ç©ºé—´äº¤äº’æ± åŒ–(Cross-Space Interaction Pooling)æœºåˆ¶ï¼Œå°†åŸå­ç‰¹å¾å±‚æ¬¡åŒ–åœ°èšåˆä¸ºæ®‹åŸº(residues)ç­‰ç”Ÿç‰©å­¦æ„ä¹‰å•å…ƒï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šå°ºåº¦å»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDualEquiNet åœ¨å¤šé¡¹RNAå±æ€§é¢„æµ‹å’Œè›‹ç™½è´¨å»ºæ¨¡åŸºå‡†ä»»åŠ¡ä¸­å‡å–å¾—äº†State-of-the-artçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªæ–°å¢çš„3Dç»“æ„åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å¤§å‹ç”Ÿç‰©åˆ†å­ä»»åŠ¡ä¸­çš„å¹¿æ³›æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19862v1",
      "published_date": "2025-06-10 07:43:50 UTC",
      "updated_date": "2025-06-10 07:43:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:28.196718+00:00"
    },
    {
      "arxiv_id": "2506.08524v2",
      "title": "Teaching Physical Awareness to LLMs through Sounds",
      "title_zh": "é€šè¿‡å£°éŸ³èµ‹äºˆå¤§è¯­è¨€æ¨¡å‹ç‰©ç†æ„ŸçŸ¥èƒ½åŠ›",
      "authors": [
        "Weiguo Wang",
        "Andy Nie",
        "Wenrui Zhou",
        "Yi Kai",
        "Chengchen Hu"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç¼ºä¹å¯¹ç°å®ç‰©ç†ç°è±¡ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡å£°éŸ³ä¿¡å·åŸ¹å…»ç‰©ç†æ„ŸçŸ¥(Physical Awareness)çš„ACORNæ¡†æ¶ã€‚è¯¥æ¡†æ¶èšç„¦äºDoppler effectã€multipath effectå’Œç©ºé—´å…³ç³»ç­‰æ ¸å¿ƒç‰©ç†ç°è±¡ï¼Œåˆ©ç”¨ç‰©ç†ä»¿çœŸå™¨(Physics-based simulator)ç»“åˆçœŸå®å£°æºä¸å—æ§ç‰©ç†ä¿¡é“ç”Ÿæˆå¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿæ®æ­¤æ„å»ºäº†ç»¼åˆæ€§éŸ³é¢‘é—®ç­”æ•°æ®é›†AQA-PHYï¼Œå¹¶å¼€å‘äº†èƒ½å¤ŸåŒæ—¶å¤„ç†æŒ¯å¹…(Magnitude)å’Œç›¸ä½(Phase)ä¿¡æ¯çš„éŸ³é¢‘ç¼–ç å™¨ã€‚é€šè¿‡å°†è¯¥ç¼–ç å™¨é›†æˆè‡³å‰æ²¿LLMsï¼Œç³»ç»Ÿåœ¨è§†è·(Line-of-Sight)æ£€æµ‹ã€Doppler effectä¼°è®¡å’Œåˆ°è¾¾æ–¹å‘(Direction-of-Arrival)ä¼°è®¡ç­‰æ¨¡æ‹Ÿä¸çœŸå®ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿™ä¸€è¿›å±•ä¸ºå¤§è¯­è¨€æ¨¡å‹ç†è§£ç‰©ç†ä¸–ç•Œå¥ å®šäº†åŸºç¡€ï¼Œå±•ç¤ºäº†åˆ©ç”¨éŸ³é¢‘æ¨¡æ€æå‡æ¨¡å‹ç‰©ç†è®¤çŸ¥èƒ½åŠ›çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "cs.RO",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08524v2",
      "published_date": "2025-06-10 07:42:56 UTC",
      "updated_date": "2025-06-11 05:18:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:51.896467+00:00"
    },
    {
      "arxiv_id": "2507.21074v1",
      "title": "Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education",
      "title_zh": "èµ‹èƒ½äººå·¥æ™ºèƒ½æ—¶ä»£çš„æ•™è‚²è€…ï¼šå®šæ€§ç ”ç©¶æ–¹æ³•æ•™å­¦ä¸­æ„å»ºè‡ªå®šä¹‰ GPT çš„å®è¯ç ”ç©¶",
      "authors": [
        "Qian Huang",
        "Thijs Willems"
      ],
      "abstract": "As generative AI (Gen-AI) tools become more prevalent in education, there is a growing need to understand how educators, not just students, can actively shape their design and use. This study investigates how two instructors integrated four custom GPT tools into a Masters-level Qualitative Research Methods course for Urban Planning Policy students. Addressing two key gaps: the dominant framing of students as passive AI users, and the limited use of AI in qualitative methods education. The study explores how Gen-AI can support disciplinary learning when aligned with pedagogical intent. Drawing on the Technological Pedagogical Content Knowledge (TPACK) framework and action research methodology, the instructors designed GPTs to scaffold tasks such as research question formulation, interview practice, fieldnote analysis, and design thinking. Thematic analysis of student reflections, AI chat logs, and final assignments revealed that the tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking. However, students also expressed concerns about cognitive overload, reduced immersion in data, and the formulaic nature of AI responses. The study offers three key insights: AI can be a powerful scaffold for active learning when paired with human facilitation; custom GPTs can serve as cognitive partners in iterative research practice; and educator-led design is critical to pedagogically meaningful AI integration. This research contributes to emerging scholarship on AI in higher education by demonstrating how empowering educators to design custom tools can promote more reflective, responsible, and collaborative learning with AI.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ•™è‚²è€…å¦‚ä½•é€šè¿‡è®¾è®¡å®šåˆ¶åŒ– GPTs (custom GPTs) æ¥ä¸»åŠ¨å¡‘é€ äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥åŸå¸‚è§„åˆ’æ”¿ç­–ç¡•å£«æ°´å¹³çš„å®šæ€§ç ”ç©¶æ–¹æ³• (Qualitative Research Methods) è¯¾ç¨‹ä¸ºæ¡ˆä¾‹è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚åŸºäºæŠ€æœ¯æ•™å­¦æ³•å†…å®¹çŸ¥è¯† (TPACK) æ¡†æ¶å’Œè¡ŒåŠ¨ç ”ç©¶ (action research) æ–¹æ³•ï¼Œæ•™å¸ˆè®¾è®¡äº†å››ä¸ªå·¥å…·æ¥è¾…åŠ©ç ”ç©¶é—®é¢˜æ„å»ºã€è®¿è°ˆç»ƒä¹ ã€ç”°é‡ç¬”è®°åˆ†æå’Œè®¾è®¡æ€ç»´ã€‚é€šè¿‡å¯¹å­¦ç”Ÿåé¦ˆå’ŒèŠå¤©è®°å½•çš„ä¸»é¢˜åˆ†æ (thematic analysis)ï¼Œç ”ç©¶å‘ç°è¿™äº›å·¥å…·æ˜¾è‘—å¢å¼ºäº†å­¦ç”Ÿçš„åæ€æ€§ (reflexivity)ï¼Œæ”¹è¿›äº†è®¿è°ˆæŠ€å·§å¹¶æ”¯æŒäº†ç»“æ„åŒ–åˆ†ææ€ç»´ã€‚ç„¶è€Œï¼Œå­¦ç”Ÿä¹Ÿè¡¨è¾¾äº†å¯¹è®¤çŸ¥è´Ÿè· (cognitive overload)ã€æ•°æ®æ²‰æµ¸æ„Ÿé™ä½ä»¥åŠ AI å›å¤å…¬å¼åŒ–ç­‰é—®é¢˜çš„æ‹…å¿§ã€‚ç ”ç©¶æå‡ºäº†ä¸‰é¡¹æ ¸å¿ƒæ´å¯Ÿï¼šåœ¨æœ‰äººç±»å¼•å¯¼çš„æƒ…å†µä¸‹ï¼ŒAI èƒ½æœ‰æ•ˆæ”¯æŒä¸»åŠ¨å­¦ä¹ ï¼›å®šåˆ¶åŒ– GPTs å¯ä»¥ä½œä¸ºè¿­ä»£ç ”ç©¶å®è·µä¸­çš„è®¤çŸ¥ä¼™ä¼´ (cognitive partners)ï¼›æ•™è‚²è€…ä¸»å¯¼çš„è®¾è®¡å¯¹äºå®ç°å…·æœ‰æ•™å­¦æ„ä¹‰çš„ AI æ•´åˆè‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†èµ‹èƒ½æ•™è‚²è€…è®¾è®¡å®šåˆ¶åŒ–å·¥å…·å¦‚ä½•ä¿ƒè¿›æ›´å…·åæ€æ€§ã€è´Ÿè´£ä»»ä¸”åä½œçš„ AI è¾…åŠ©å­¦ä¹ ï¼Œä¸ºé«˜ç­‰æ•™è‚²é¢†åŸŸçš„ AI åº”ç”¨ç ”ç©¶åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.21074v1",
      "published_date": "2025-06-10 07:41:40 UTC",
      "updated_date": "2025-06-10 07:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:46:53.642845+00:00"
    },
    {
      "arxiv_id": "2506.08518v1",
      "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching",
      "title_zh": "FEDTAILï¼šåŸºäºé”åº¦å¼•å¯¼æ¢¯åº¦åŒ¹é…çš„è”é‚¦é•¿å°¾é¢†åŸŸæ³›åŒ–",
      "authors": [
        "Sunny Gupta",
        "Nikita Jangid",
        "Shounak Das",
        "Amit Sethi"
      ],
      "abstract": "Domain Generalization (DG) seeks to train models that perform reliably on unseen target domains without access to target data during training. While recent progress in smoothing the loss landscape has improved generalization, existing methods often falter under long-tailed class distributions and conflicting optimization objectives. We introduce FedTAIL, a federated domain generalization framework that explicitly addresses these challenges through sharpness-guided, gradient-aligned optimization. Our method incorporates a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives, leading to more stable convergence. To combat class imbalance, we perform class-wise sharpness minimization and propose a curvature-aware dynamic weighting scheme that adaptively emphasizes underrepresented tail classes. Furthermore, we enhance conditional distribution alignment by integrating sharpness-aware perturbations into entropy regularization, improving robustness under domain shift. FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework. Extensive evaluations across standard domain generalization benchmarks demonstrate that FedTAIL achieves state-of-the-art performance, particularly in the presence of domain shifts and label imbalance, validating its effectiveness in both centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FedTAILï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³é¢†åŸŸæ³›åŒ– (Domain Generalization) ä¸­é•¿å°¾ç±»åˆ«åˆ†å¸ƒå’Œä¼˜åŒ–ç›®æ ‡å†²çªé—®é¢˜çš„è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¢¯åº¦ç›¸å¹²æ­£åˆ™åŒ–å™¨ (gradient coherence regularizer) ä»¥ç¼“è§£åˆ†ç±»ä¸å¯¹æŠ—ç›®æ ‡ä¹‹é—´çš„å†²çªï¼Œç¡®ä¿æ›´ç¨³å®šçš„æ”¶æ•›è¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡æŒ‘æˆ˜ï¼ŒFedTAIL é‡‡ç”¨äº†ç±»çº§åˆ«é”åº¦æœ€å°åŒ– (class-wise sharpness minimization) å’Œæ„ŸçŸ¥æ›²ç‡çš„åŠ¨æ€åŠ æƒæ–¹æ¡ˆ (curvature-aware dynamic weighting)ï¼Œè‡ªé€‚åº”åœ°åŠ å¼ºå¯¹ç¨€æœ‰å°¾éƒ¨ç±»åˆ«çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†é”åº¦æ„ŸçŸ¥æ‰°åŠ¨é›†æˆåˆ°ç†µæ­£åˆ™åŒ–ä¸­æ¥å¢å¼ºæ¡ä»¶åˆ†å¸ƒå¯¹é½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é¢†åŸŸåç§»ä¸‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedTAIL åœ¨æ ‡å‡†é¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ (state-of-the-art) æ€§èƒ½ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨å¤„ç†æ ‡ç­¾ä¸å¹³è¡¡å’Œåˆ†å¸ƒåç§»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic Workflows CFAgentic @ ICML'25",
      "pdf_url": "https://arxiv.org/pdf/2506.08518v1",
      "published_date": "2025-06-10 07:36:40 UTC",
      "updated_date": "2025-06-10 07:36:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:05.457934+00:00"
    },
    {
      "arxiv_id": "2506.08512v1",
      "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding",
      "title_zh": "MLVTGï¼šåŸºäº Mamba çš„ç‰¹å¾å¯¹é½ä¸ LLM é©±åŠ¨æçº¯çš„å¤šæ¨¡æ€è§†é¢‘æ—¶åˆ»å®šä½",
      "authors": [
        "Zhiyi Zhu",
        "Xiaoyu Wu",
        "Zihao Liu",
        "Linlin Yang"
      ],
      "abstract": "Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ—¶åºå®šä½ (Video Temporal Grounding) ä»»åŠ¡ä¸­ Transformer æ¨¡å‹å­˜åœ¨çš„å†—ä½™æ³¨æ„åŠ›å’Œå¤šæ¨¡æ€å¯¹é½ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†åä¸º MLVTG çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”± MambaAligner å’Œ LLMRefiner ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼Œå…¶ä¸­ MambaAligner é‡‡ç”¨å †å çš„ Vision Mamba å—ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡ç»“æ„åŒ–çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ (Structured State-Space Dynamics) å»ºæ¨¡æ—¶åºä¾èµ–å¹¶æå–é²æ£’çš„è§†é¢‘è¡¨ç¤ºã€‚LLMRefiner åˆ™åˆ©ç”¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„ç‰¹å®šå†»ç»“å±‚éšå¼ä¼ è¾“è¯­ä¹‰å…ˆéªŒï¼Œåœ¨æ— éœ€å¾®è°ƒçš„å‰æä¸‹å®ç°è¯­ä¹‰å‡€åŒ–å¹¶å¢å¼ºå¤šæ¨¡æ€å¯¹é½ã€‚è¿™ç§ç»“åˆæ—¶åºå»ºæ¨¡ä¸æ–‡æœ¬å…ˆéªŒçš„åŒé‡å¯¹é½ç­–ç•¥æ˜¾è‘—æå‡äº†å®šä½çš„ç²¾ç¡®åº¦ã€‚åœ¨ QVHighlightsã€Charades-STA å’Œ TVSum ç­‰ä¸»æµæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMLVTG è¾¾åˆ°äº† State-of-the-Art æ€§èƒ½ï¼Œä¸”è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08512v1",
      "published_date": "2025-06-10 07:20:12 UTC",
      "updated_date": "2025-06-10 07:20:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:01.269569+00:00"
    },
    {
      "arxiv_id": "2506.08507v2",
      "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning",
      "title_zh": "MasHostï¼šå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…¨èƒ½æ„å»º",
      "authors": [
        "Kuo Yang",
        "Xingjie Yang",
        "Linhui Yu",
        "Qing Xu",
        "Yan Fang",
        "Xu Wang",
        "Zhengyang Zhou",
        "Yang Wang"
      ],
      "abstract": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MasHostï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‡ªä¸»ä¸”æŸ¥è¯¢è‡ªé€‚åº”çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-agent systems) æ„å»ºã€‚ä¸ºäº†å…‹æœç°æœ‰ç³»ç»Ÿä¾èµ–äººå·¥è®¾è®¡è§„åˆ™å¯¼è‡´çš„åå·®å’Œè‡ªä¸»æ€§é™åˆ¶ï¼ŒMasHost å°†ç³»ç»Ÿæ„å»ºå®šä¹‰ä¸ºå›¾æœç´¢é—®é¢˜ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶ååŒé‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡å°†ç»„ä»¶åˆç†æ€§ (Component Rationality) å¼•å…¥è®¾è®¡åŸåˆ™ï¼Œå¹¶å¼€å‘äº†å±‚çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Hierarchical Relative Policy Optimization, HRPO) ç®—æ³•ï¼Œä»¥ååŒæ•´åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¸åŠ¨ä½œå¥–åŠ±ã€‚ä½œä¸ºé¦–ä¸ªç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“å›¾æ„å»ºæ¡†æ¶ï¼ŒMasHost åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè¶…è¶Šç«äº‰æ¨¡å‹çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08507v2",
      "published_date": "2025-06-10 07:04:25 UTC",
      "updated_date": "2025-06-12 07:40:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:09.004883+00:00"
    },
    {
      "arxiv_id": "2506.08505v1",
      "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations",
      "title_zh": "è§£é‡Šçš„å¿«ä¸æ…¢ï¼šå¯è¯æ˜è§£é‡Šçš„æŠ½è±¡ä¸ç»†åŒ–",
      "authors": [
        "Shahaf Bassan",
        "Yizhak Yisrael Elboher",
        "Tobias Ladner",
        "Matthias Althoff",
        "Guy Katz"
      ],
      "abstract": "Despite significant advancements in post-hoc explainability techniques for neural networks, many current methods rely on heuristics and do not provide formally provable guarantees over the explanations provided. Recent work has shown that it is possible to obtain explanations with formal guarantees by identifying subsets of input features that are sufficient to determine that predictions remain unchanged using neural network verification techniques. Despite the appeal of these explanations, their computation faces significant scalability challenges. In this work, we address this gap by proposing a novel abstraction-refinement technique for efficiently computing provably sufficient explanations of neural network predictions. Our method abstracts the original large neural network by constructing a substantially reduced network, where a sufficient explanation of the reduced network is also provably sufficient for the original network, hence significantly speeding up the verification process. If the explanation is in sufficient on the reduced network, we iteratively refine the network size by gradually increasing it until convergence. Our experiments demonstrate that our approach enhances the efficiency of obtaining provably sufficient explanations for neural network predictions while additionally providing a fine-grained interpretation of the network's predictions across different abstraction levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œäº‹åè§£é‡Šæ€§ï¼ˆpost-hoc explainabilityï¼‰æŠ€æœ¯åœ¨æä¾›å½¢å¼åŒ–è¯æ˜ä¿è¯ï¼ˆformally provable guaranteesï¼‰æ–¹é¢é¢ä¸´çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æŠ½è±¡-ç²¾ç‚¼ï¼ˆabstraction-refinementï¼‰æŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªå¤§å¹…ç®€åŒ–çš„ç¼©å‡ç½‘ç»œæ¥å¯¹åŸå§‹å¤§å‹ç¥ç»ç½‘ç»œè¿›è¡ŒæŠ½è±¡ï¼Œç¡®ä¿åœ¨ç¼©å‡ç½‘ç»œä¸Šè·å¾—çš„å……åˆ†è§£é‡Šå¯¹åŸå§‹ç½‘ç»œåŒæ ·å…·æœ‰å¯è¯æ˜çš„å……åˆ†æ€§ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†ç¥ç»ç½‘ç»œéªŒè¯ï¼ˆneural network verificationï¼‰è¿‡ç¨‹ã€‚å¦‚æœç¼©å‡ç½‘ç»œä¸Šçš„è§£é‡Šä¸è¶³ï¼Œç³»ç»Ÿå°†é€šè¿‡è¿­ä»£åœ°é€æ­¥å¢åŠ ç½‘ç»œè§„æ¨¡ç›´è‡³æ”¶æ•›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡è·å–å¯è¯æ˜å……åˆ†è§£é‡Šï¼ˆprovably sufficient explanationsï¼‰æ•ˆç‡çš„åŒæ—¶ï¼Œè¿˜èƒ½å¤Ÿæä¾›è·¨ä¸åŒæŠ½è±¡å±‚æ¬¡çš„ç»†ç²’åº¦è§£è¯»ï¼Œä¸ºé«˜æ•ˆä¸”å¯é çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è§£é‡Šæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08505v1",
      "published_date": "2025-06-10 07:04:13 UTC",
      "updated_date": "2025-06-10 07:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:14.489935+00:00"
    },
    {
      "arxiv_id": "2506.08504v1",
      "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations",
      "title_zh": "CoMuMDRï¼šé¢å‘å¯¹è¯ç¯‡ç« è§£æçš„è¯­ç æ··åˆå¤šæ¨¡æ€å¤šé¢†åŸŸè¯­æ–™åº“",
      "authors": [
        "Divyaksh Shukla",
        "Ritesh Baviskar",
        "Dwijesh Gohil",
        "Aniket Tiwari",
        "Atul Shree",
        "Ashutosh Modi"
      ],
      "abstract": "Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†CoMuMDRï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¯¹è¯è¯­ç¯‡è§£æ(Discourse parsing)è®¾è®¡çš„æ··åˆè¯­ã€å¤šæ¨¡æ€ã€å¤šé¢†åŸŸè¯­æ–™åº“ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ä¸»è¦å±€é™äºå•ä¸€é¢†åŸŸä¹¦é¢è‹±è¯­çš„é—®é¢˜ã€‚è¯¥è¯­æ–™åº“åŒ…å«å°åœ°è¯­å’Œè‹±è¯­çš„æ··åˆè¯­å†…å®¹ï¼Œç»“åˆäº†éŸ³é¢‘ä¸è½¬å½•æ–‡æœ¬ï¼Œå¹¶è¯¦ç»†æ ‡æ³¨äº†ä¹ç§è¯­ç¯‡å…³ç³»ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šç§æœ€å…ˆè¿›çš„(SoTA)åŸºçº¿æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šé¢†åŸŸæ··åˆè¯­æ•°æ®æ—¶è¡¨ç°ä¸ä½³ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†åœ¨ç°å®å¤æ‚å¯¹è¯åœºæ™¯ä¸‹è¿›è¡Œè¯­ç¯‡è§£æçš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘æ›´ç¨³å¥æ¨¡å‹çš„å¿…è¦æ€§ã€‚CoMuMDRä¸ºè‡ªç„¶è¯­è¨€ç†è§£(NLU)é¢†åŸŸçš„æ‘˜è¦ç”Ÿæˆã€æœºå™¨ç†è§£å’Œæƒ…æ„Ÿè¯†åˆ«ç­‰åç»­ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL Findings 2025 (16 pages: 5 pages main content + 3 pages references + 8 pages appendix)",
      "pdf_url": "https://arxiv.org/pdf/2506.08504v1",
      "published_date": "2025-06-10 07:01:30 UTC",
      "updated_date": "2025-06-10 07:01:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:57.472076+00:00"
    },
    {
      "arxiv_id": "2506.08500v2",
      "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs",
      "title_zh": "é™·å…¥å†²çªï¼šæœç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ä¸­å†²çªä¿¡æ¯æºçš„æ£€æµ‹ä¸åº”å¯¹",
      "authors": [
        "Arie Cattan",
        "Alon Jacovi",
        "Ori Ram",
        "Jonathan Herzig",
        "Roee Aharoni",
        "Sasha Goldshtein",
        "Eran Ofek",
        "Idan Szpektor",
        "Avi Caciularu"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­æ£€ç´¢æºä¿¡æ¯å†²çªçš„é—®é¢˜ï¼Œæ—¨åœ¨æ˜ç¡®å¤§è¯­è¨€æ¨¡å‹(LLMs)åº”å¦‚ä½•å¤„ç†è¿™äº›ä¸ä¸€è‡´æ€§ã€‚ä½œè€…é¦–å…ˆæå‡ºäº†ä¸€ç§å…¨æ–°çš„RAGçŸ¥è¯†å†²çªç±»å‹åˆ†ç±»æ³•(taxonomy)ï¼Œå¹¶ä¸ºæ¯ç§ç±»å‹å®šä¹‰äº†ç†æƒ³çš„æ¨¡å‹è¡Œä¸ºã€‚éšåï¼Œç ”ç©¶è€…æ¨å‡ºäº†CONFLICTSï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨çœŸå®RAGåœºæ™¯ä¸‹ç”±ä¸“å®¶æ ‡æ³¨å†²çªç±»å‹çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•é›†(benchmark)ï¼Œä¹Ÿæ˜¯é¦–ä¸ªèƒ½å¤Ÿè¿½è¸ªæ¨¡å‹å¤„ç†å¤šç§çŸ¥è¯†å†²çªè¿›å±•çš„å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨å¦¥å–„è§£å†³æºé—´å†²çªæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œé€šè¿‡æç¤ºè¯(prompting)å¼•å¯¼LLMså¯¹æ£€ç´¢æ–‡æ¡£ä¸­çš„æ½œåœ¨å†²çªè¿›è¡Œæ˜¾å¼æ¨ç†ï¼Œèƒ½æ˜¾è‘—æé«˜å“åº”çš„è´¨é‡å’Œå‡†ç¡®åº¦ï¼Œä¸ºæœªæ¥è§£å†³å¤æ‚çŸ¥è¯†å†²çªçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08500v2",
      "published_date": "2025-06-10 06:52:57 UTC",
      "updated_date": "2025-06-15 11:20:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:50.500092+00:00"
    },
    {
      "arxiv_id": "2506.08488v1",
      "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs",
      "title_zh": "EtiCor++ï¼šæ·±å…¥æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç¤¼ä»ªåè§",
      "authors": [
        "Ashutosh Dwivedi",
        "Siddhant Shivdutt Singh",
        "Ashutosh Modi"
      ],
      "abstract": "In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† EtiCor++ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å…¨çƒèŒƒå›´ç¤¼ä»ª (Etiquettes) çš„è¯­æ–™åº“ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ–‡åŒ–æ•æ„Ÿæ€§è¯„ä¼°èµ„æºæ–¹é¢çš„åŒ®ä¹ã€‚ç”±äºç¤¼ä»ªå…·æœ‰æ˜¾è‘—çš„åœ°åŸŸæ€§ç‰¹å¾ï¼Œä½œè€…è®¾è®¡äº†å¤šç§ä»»åŠ¡æ¥è¯„ä¼° LLMs å¯¹ä¸åŒåœ°åŒºç¤¼ä»ªçŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸“é—¨çš„åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨å¤„ç†ç¤¼ä»ªé—®é¢˜æ—¶äº§ç”Ÿçš„åå·® (Etiquettical Bias)ã€‚é€šè¿‡å¯¹ä¸»æµ LLMs è¿›è¡Œçš„å¤§è§„æ¨¡å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹å¯¹ç‰¹å®šåœ°åŒºå­˜åœ¨æ˜æ˜¾çš„å›ºæœ‰åå·®ã€‚è¿™ä¸€å·¥ä½œä¸ºç†è§£å’Œä¼˜åŒ– LLMs çš„æ–‡åŒ–å…¬æ­£æ€§æä¾›äº†é‡è¦çš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL Findings 2025, 22 pages (9 pages main content + 4 pages references + 9 pages appendix)",
      "pdf_url": "https://arxiv.org/pdf/2506.08488v1",
      "published_date": "2025-06-10 06:29:35 UTC",
      "updated_date": "2025-06-10 06:29:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:51.014671+00:00"
    },
    {
      "arxiv_id": "2506.08487v2",
      "title": "Beyond Bias Scores: Unmasking Vacuous Neutrality in Small Language Models",
      "title_zh": "è¶…è¶Šåå·®è¯„åˆ†ï¼šæ­ç¤ºå°è¯­è¨€æ¨¡å‹ä¸­çš„ç©ºæ´ä¸­ç«‹",
      "authors": [
        "Sumanth Manduru",
        "Carlotta Domeniconi"
      ],
      "abstract": "The rapid adoption of Small Language Models (SLMs) for resource constrained applications has outpaced our understanding of their ethical and fairness implications. To address this gap, we introduce the Vacuous Neutrality Framework (VaNeu), a multi-dimensional evaluation paradigm designed to assess SLM fairness prior to deployment. The framework examines model robustness across four stages - biases, utility, ambiguity handling, and positional bias over diverse social bias categories. To the best of our knowledge, this work presents the first large-scale audit of SLMs in the 0.5-5B parameter range, an overlooked \"middle tier\" between BERT-class encoders and flagship LLMs. We evaluate nine widely used SLMs spanning four model families under both ambiguous and disambiguated contexts. Our findings show that models demonstrating low bias in early stages often fail subsequent evaluations, revealing hidden vulnerabilities and unreliable reasoning. These results underscore the need for a more comprehensive understanding of fairness and reliability in SLMs, and position the proposed framework as a principled tool for responsible deployment in socially sensitive settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹0.5-5Bå‚æ•°è§„æ¨¡çš„å°è¯­è¨€æ¨¡å‹ (Small Language Models, SLMs) åœ¨ä¼¦ç†ä¸å…¬å¹³æ€§è¯„ä¼°æ–¹é¢çš„ç©ºç™½ï¼Œæå‡ºäº†ç©ºæ´ä¸­ç«‹æ¡†æ¶ (Vacuous Neutrality Framework, VaNeu) è¿™ä¸€å¤šç»´åº¦è¯„ä¼°èŒƒå¼ã€‚è¯¥æ¡†æ¶ä»åå·® (biases)ã€æ•ˆç”¨ (utility)ã€æ­§ä¹‰å¤„ç† (ambiguity handling) å’Œä½ç½®åå·® (positional bias) å››ä¸ªé˜¶æ®µï¼Œå¯¹æ¥è‡ªå››ä¸ªæ¨¡å‹å®¶æ—çš„ä¹ä¸ªä¸»æµSLMsè¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡å®¡è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆå§‹é˜¶æ®µè¡¨ç°å‡ºä½åå·® (low bias) çš„æ¨¡å‹åœ¨åç»­æ›´å¤æ‚çš„è¯„ä¼°ä¸­å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹ä¸­éšè—çš„è„†å¼±æ€§ä»¥åŠæ¨ç†è¿‡ç¨‹çš„ä¸å¯é æ€§ã€‚æ­¤ç ”ç©¶å¼ºè°ƒäº†ä¸èƒ½ä»…ä¾èµ–ä¼ ç»Ÿçš„åå·®è¯„åˆ†æ¥è¡¡é‡å…¬å¹³æ€§ï¼Œå¹¶ä¸ºSLMsåœ¨ç¤¾ä¼šæ•æ„Ÿç¯å¢ƒä¸‹çš„è´Ÿè´£ä»»éƒ¨ç½²æä¾›äº†åŸåˆ™æ€§çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08487v2",
      "published_date": "2025-06-10 06:21:09 UTC",
      "updated_date": "2025-11-20 06:04:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:12.700567+00:00"
    },
    {
      "arxiv_id": "2506.08486v1",
      "title": "RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being",
      "title_zh": "RHealthTwinï¼šé¢å‘ä¸ªæ€§åŒ–å¥åº·çš„è´Ÿè´£ä»»å¤šæ¨¡æ€æ•°å­—å­ªç”Ÿ",
      "authors": [
        "Rahatara Ferdousi",
        "M Anwar Hossain"
      ],
      "abstract": "The rise of large language models (LLMs) has created new possibilities for digital twins in healthcare. However, the deployment of such systems in consumer health contexts raises significant concerns related to hallucination, bias, lack of transparency, and ethical misuse. In response to recommendations from health authorities such as the World Health Organization (WHO), we propose Responsible Health Twin (RHealthTwin), a principled framework for building and governing AI-powered digital twins for well-being assistance. RHealthTwin processes multimodal inputs that guide a health-focused LLM to produce safe, relevant, and explainable responses. At the core of RHealthTwin is the Responsible Prompt Engine (RPE), which addresses the limitations of traditional LLM configuration. Conventionally, users input unstructured prompt and the system instruction to configure the LLM, which increases the risk of hallucination. In contrast, RPE extracts predefined slots dynamically to structure both inputs. This guides the language model to generate responses that are context aware, personalized, fair, reliable, and explainable for well-being assistance. The framework further adapts over time through a feedback loop that updates the prompt structure based on user satisfaction. We evaluate RHealthTwin across four consumer health domains including mental support, symptom triage, nutrition planning, and activity coaching. RPE achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical compliance and instruction-following metrics using LLM-as-judge evaluation, outperforming baseline strategies. We envision RHealthTwin as a forward-looking foundation for responsible LLM-based applications in health and well-being.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RHealthTwinï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸ªæ€§åŒ–å¥åº·è¾…åŠ©çš„è´Ÿè´£ä»»ä¸”å¤šæ¨¡æ€çš„æ•°å­—å­ªç”Ÿ (Digital Twins) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¶ˆè´¹åŒ»ç–—é¢†åŸŸä¸­å­˜åœ¨çš„å¹»è§‰ (hallucination)ã€åè§å’Œä¼¦ç†é£é™©ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯è´Ÿè´£ä»»æç¤ºå¼•æ“ (Responsible Prompt Engine, RPE)ï¼Œå®ƒé€šè¿‡åŠ¨æ€æå–é¢„å®šä¹‰æ§½ä½ (predefined slots) æ¥ç»“æ„åŒ–è¾“å…¥å’Œç³»ç»ŸæŒ‡ä»¤ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆå¹»è§‰çš„é£é™©ã€‚RHealthTwin èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å»ºè®®å…·æœ‰æƒ…å¢ƒæ„ŸçŸ¥æ€§ã€ä¸ªæ€§åŒ–ä¸”å¯è§£é‡Šï¼Œå¹¶ç»“åˆåé¦ˆå¾ªç¯ (feedback loop) æŒç»­ä¼˜åŒ–æç¤ºç»“æ„ã€‚å®éªŒåœ¨å¿ƒç†æ”¯æŒã€ç—‡çŠ¶åˆ†ç±»ã€è¥å…»è§„åˆ’å’Œè¿åŠ¨æŒ‡å¯¼å››ä¸ªé¢†åŸŸå±•å¼€ï¼Œç»“æœæ˜¾ç¤º RPE åœ¨ BLEUã€ROUGE-L å’Œ BERTScore ç­‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚æ­¤å¤–ï¼Œåœ¨ä¼¦ç†åˆè§„æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„ LLM-as-judge è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶å®ç°äº†è¶…è¿‡ 90% çš„æˆåŠŸç‡ï¼Œä¸ºæ„å»ºå¯é ã€è´Ÿè´£ä»»çš„å¥åº·ä¸ç¦ç¥‰åº”ç”¨å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 12 figures, IEEE EMBS JBHI",
      "pdf_url": "https://arxiv.org/pdf/2506.08486v1",
      "published_date": "2025-06-10 06:20:22 UTC",
      "updated_date": "2025-06-10 06:20:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:47:53.777253+00:00"
    },
    {
      "arxiv_id": "2506.08480v1",
      "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models",
      "title_zh": "é‡æ–°å®¡è§†æ–‡ç”Ÿå›¾æ¨¡å‹ä¸­å›¾æ–‡å¯¹é½çš„è‡ªåŠ¨è¯„ä¼°",
      "authors": [
        "Huixuan Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ¨¡å‹ä¸­å›¾åƒ-æ–‡æœ¬å¯¹é½(Image-Text Alignment)çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼ŒæŒ‡å‡ºå½“å‰è¯„ä¼°è¿‡äºä¾èµ–ä¸äººç±»è¯„ä»·çš„ä¸€è‡´æ€§è€Œå¿½è§†äº†å¯é è¯„ä¼°æ¡†æ¶çš„å…¶ä»–å…³é”®å±æ€§ã€‚ä½œè€…é¦–å…ˆè¯†åˆ«äº†å¯é è¯„ä¼°å¿…é¡»æ»¡è¶³çš„ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶é€šè¿‡å®è¯ç ”ç©¶è¯æ˜ï¼Œç°æœ‰ä¸»æµè¯„ä¼°æ¡†æ¶åœ¨å„ç§æŒ‡æ ‡å’Œæ¨¡å‹ä¸Šå‡æœªèƒ½å……åˆ†æ»¡è¶³è¿™äº›å±æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æ­ç¤ºäº†å½“å‰è¯„ä¼°æ‰‹æ®µçš„å±€é™æ€§ï¼Œå¹¶é’ˆå¯¹å¦‚ä½•ä¼˜åŒ–å›¾åƒ-æ–‡æœ¬å¯¹é½çš„è¯„ä¼°ä½“ç³»æå‡ºäº†ç³»ç»Ÿæ€§çš„æ”¹è¿›å»ºè®®ã€‚è¿™äº›å·¥ä½œä¸ºå¼€å‘æ›´å…·å¯ä¿¡åº¦çš„ç”Ÿæˆæ¨¡å‹è¯„ä¼°å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08480v1",
      "published_date": "2025-06-10 06:11:36 UTC",
      "updated_date": "2025-06-10 06:11:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:06.035755+00:00"
    },
    {
      "arxiv_id": "2506.08479v3",
      "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$",
      "title_zh": "é•¿æ–‡æœ¬é—®ç­”çš„é«˜æ•ˆä¸Šä¸‹æ–‡é€‰æ‹©ï¼šæ— éœ€å¾®è°ƒï¼Œæ— éœ€è¿­ä»£ï¼Œåªéœ€ Adaptive-$k$",
      "authors": [
        "Chihiro Taguchi",
        "Seiji Maekawa",
        "Nikita Bhutani"
      ],
      "abstract": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain question answering (QA). However, optimal external context to retrieve remains an open problem: fixing the retrieval size risks either wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA, where the optimal context size is both unknown and variable. We present Adaptive-$k$ retrieval, a simple and effective single-pass method that adaptively selects the number of passages based on the distribution of the similarity scores between the query and the candidate passages. It does not require model fine-tuning, extra LLM inferences or changes to existing retriever-reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens than full-context input, yet still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡é—®ç­”(Long-Context QA)ä¸­å›ºå®šæ£€ç´¢è§„æ¨¡å¸¦æ¥çš„èµ„æºæµªè´¹æˆ–å…³é”®è¯æ®ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†Adaptive-$k$æ£€ç´¢æ–¹æ³•ã€‚ä¸ç°æœ‰çš„ä¾èµ–è¿­ä»£æç¤ºæˆ–æ¨¡å‹å¾®è°ƒçš„è‡ªé€‚åº”æ–¹æ³•ä¸åŒï¼ŒAdaptive-$k$æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„å•æ¬¡æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææŸ¥è¯¢ä¸å€™é€‰æ®µè½ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†åˆ†å¸ƒæ¥åŠ¨æ€é€‰æ‹©æ£€ç´¢æ•°é‡ã€‚åœ¨Factoid QAå’ŒAggregation QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAdaptive-$k$çš„æ€§èƒ½åŒ¹é…æˆ–ä¼˜äºå›ºå®š$k$å€¼çš„åŸºçº¿æ¨¡å‹ï¼Œä¸”æ— éœ€æ›´æ”¹ç°æœ‰çš„retriever-readerç®¡çº¿ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™70%ç›¸å…³æ®µè½çš„åŒæ—¶ï¼Œæ¯”å…¨ä¸Šä¸‹æ–‡è¾“å…¥å‡å°‘äº†é«˜è¾¾10å€çš„Tokenæ¶ˆè€—ã€‚åœ¨äº”ç§LCLMså’Œä¸¤ç§åµŒå…¥æ¨¡å‹ä¸Šçš„æµ‹è¯•ç»“æœè¿›ä¸€æ­¥è¯æ˜ï¼ŒåŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡è§„æ¨¡èƒ½å¤Ÿæ˜¾è‘—æå‡é—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 16 tables, 5 figures. Accepted at EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2506.08479v3",
      "published_date": "2025-06-10 06:11:01 UTC",
      "updated_date": "2025-09-30 12:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:14.383740+00:00"
    },
    {
      "arxiv_id": "2506.09082v2",
      "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models",
      "title_zh": "AVA-Benchï¼šé¢å‘è§†è§‰åŸºç¡€æ¨¡å‹çš„åŸå­è§†è§‰èƒ½åŠ›åŸºå‡†",
      "authors": [
        "Zheda Mai",
        "Arpita Chowdhury",
        "Zihe Wang",
        "Sooyoung Jeon",
        "Lemeng Wang",
        "Jiacheng Hou",
        "Wei-Lun Chao"
      ],
      "abstract": "The rise of vision foundation models (VFMs) calls for systematic evaluation. A common approach pairs VFMs with large language models (LLMs) as general-purpose heads, followed by evaluation on broad Visual Question Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i) the instruction tuning data may not align with VQA test distributions, meaning a wrong prediction can stem from such data mismatch rather than a VFM' visual shortcomings; (ii) VQA benchmarks often require multiple visual abilities, making it hard to tell whether errors stem from lacking all required abilities or just a single critical one. To address these gaps, we introduce AVA-Bench, the first benchmark that explicitly disentangles 14 Atomic Visual Abilities (AVAs) -- foundational skills like localization, depth estimation, and spatial understanding that collectively support complex visual reasoning tasks. By decoupling AVAs and matching training and test distributions within each, AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench to leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM selection from educated guesswork into principled engineering. Notably, we find that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours by 8x, enabling more efficient evaluation. By offering a comprehensive and transparent benchmark, we hope AVA-Bench lays the foundation for the next generation of VFMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰åŸºç¡€æ¨¡å‹(Vision Foundation Models, VFMs)ç³»ç»Ÿæ€§è¯„ä¼°çš„éœ€æ±‚ï¼Œæå‡ºäº†AVA-Benchï¼Œè¿™æ˜¯é¦–ä¸ªæ˜¾å¼è§£è€¦14é¡¹åŸå­è§†è§‰èƒ½åŠ›(Atomic Visual Abilities, AVAs)çš„åŸºå‡†æµ‹è¯•ã€‚AVA-Benchæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰é—®ç­”(VQA)åŸºå‡†åœ¨æŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸åŒ¹é…ä»¥åŠå¤šé¡¹è§†è§‰èƒ½åŠ›è€¦åˆå¯¼è‡´éš¾ä»¥å®šè´£ç­‰ç›²ç‚¹ï¼Œæ¶µç›–äº†å®šä½(localization)ã€æ·±åº¦ä¼°è®¡(depth estimation)å’Œç©ºé—´ç†è§£(spatial understanding)ç­‰åŸºç¡€æŠ€èƒ½ã€‚é€šè¿‡è§£è€¦AVAså¹¶ç¡®ä¿æ¯ä¸ªç»´åº¦å†…è®­ç»ƒä¸æµ‹è¯•åˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼Œè¯¥åŸºå‡†èƒ½ç²¾å‡†è¯†åˆ«VFMçš„ä¼˜åŠ£åŠ¿å¹¶ç”Ÿæˆâ€œèƒ½åŠ›æŒ‡çº¹(ability fingerprints)â€ï¼Œä½¿æ¨¡å‹é€‰æ‹©ä»ç»éªŒæ¨æµ‹è½¬å‘ä¸¥è°¨å·¥ç¨‹ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä½¿ç”¨0.5Bè§„æ¨¡çš„LLMä½œä¸ºè¯„ä¼°å¤´å¯è·å¾—ä¸7Bæ¨¡å‹ç›¸ä¼¼çš„æ’åï¼ŒåŒæ—¶å°†GPUæ•ˆç‡æå‡8å€ã€‚è¿™ä¸€ç»¼åˆä¸”é€æ˜çš„åŸºå‡†ä¸ºå¼€å‘ä¸‹ä¸€ä»£è§†è§‰åŸºç¡€æ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "First two authors contribute equally",
      "pdf_url": "https://arxiv.org/pdf/2506.09082v2",
      "published_date": "2025-06-10 05:43:34 UTC",
      "updated_date": "2025-08-08 05:33:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:18.303665+00:00"
    },
    {
      "arxiv_id": "2506.08463v1",
      "title": "How to Provably Improve Return Conditioned Supervised Learning?",
      "title_zh": "å¦‚ä½•å®ç°å›æŠ¥æ¡ä»¶ç›‘ç£å­¦ä¹ çš„å¯è¯æ˜æå‡ï¼Ÿ",
      "authors": [
        "Zhishuai Liu",
        "Yu Yang",
        "Ruhan Wang",
        "Pan Xu",
        "Dongruo Zhou"
      ],
      "abstract": "In sequential decision-making problems, Return-Conditioned Supervised Learning (RCSL) has gained increasing recognition for its simplicity and stability in modern decision-making tasks. Unlike traditional offline reinforcement learning (RL) algorithms, RCSL frames policy learning as a supervised learning problem by taking both the state and return as input. This approach eliminates the instability often associated with temporal difference (TD) learning in offline RL. However, RCSL has been criticized for lacking the stitching property, meaning its performance is inherently limited by the quality of the policy used to generate the offline dataset. To address this limitation, we propose a principled and simple framework called Reinforced RCSL. The key innovation of our framework is the introduction of a concept we call the in-distribution optimal return-to-go. This mechanism leverages our policy to identify the best achievable in-dataset future return based on the current state, avoiding the need for complex return augmentation techniques. Our theoretical analysis demonstrates that Reinforced RCSL can consistently outperform the standard RCSL approach. Empirical results further validate our claims, showing significant performance improvements across a range of benchmarks.",
      "tldr_zh": "åœ¨åºè´¯å†³ç­–é¢†åŸŸï¼ŒReturn-Conditioned Supervised Learning (RCSL) å› å…¶å°†ç­–ç•¥å­¦ä¹ è½¬åŒ–ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡è€Œå…·å¤‡æé«˜çš„ç¨³å®šæ€§ï¼Œä½†å› ç¼ºä¹æ‹¼æ¥ (stitching) ç‰¹æ€§è€Œå—é™äºç¦»çº¿æ•°æ®é›†çš„è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸º Reinforced RCSL çš„æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥â€œåˆ†å¸ƒå†…æœ€ä¼˜å‰©ä½™å›æŠ¥â€ (in-distribution optimal return-to-go) è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µæ¥æå‡æ€§èƒ½ã€‚è¯¥æœºåˆ¶åˆ©ç”¨ç­–ç•¥è¯†åˆ«åŸºäºå½“å‰çŠ¶æ€çš„æ•°æ®é›†å†…æœ€ä½³å¯å®ç°æœªæ¥å›æŠ¥ï¼Œä»è€Œé¿å…äº†å¤æ‚çš„ return augmentation æŠ€æœ¯ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒReinforced RCSL åœ¨æ€§èƒ½ä¸Šèƒ½å¤Ÿç¨³å¥åœ°è¶…è¶Šæ ‡å‡† RCSL æ–¹æ³•ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œæ˜¾è‘—æå‡äº†ç­–ç•¥çš„å­¦ä¹ æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 4 figures, 12 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.08463v1",
      "published_date": "2025-06-10 05:37:51 UTC",
      "updated_date": "2025-06-10 05:37:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:27.432674+00:00"
    },
    {
      "arxiv_id": "2506.08462v1",
      "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing",
      "title_zh": "é¢å‘åˆ¶é€ ä¸šæ„ŸçŸ¥ã€è§£é‡Šä¸è‡ªä¸»è¡Œä¸ºçš„æ··åˆæ¨ç†",
      "authors": [
        "Christos Margadji",
        "Sebastian W. Pattinson"
      ],
      "abstract": "Industrial processes must be robust and adaptable, as environments and tasks are often unpredictable, while operational errors remain costly and difficult to detect. AI-based control systems offer a path forward, yet typically depend on supervised learning with extensive labelled datasets, which limits their ability to generalize across variable and data-scarce industrial settings. Foundation models could enable broader reasoning and knowledge integration, but rarely deliver the quantitative precision demanded by engineering applications. Here, we introduceControl and Interpretation of Production via Hybrid Expertise and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming to replicate human-like reasoning for industrial control, instantiated in a commercial-grade 3D printer. It integrates a process expert, a regression model enabling quantitative characterization of system states required for engineering tasks. CIPHER also incorporates retrieval-augmented generation to access external expert knowledge and support physics-informed, chain-of-thought reasoning. This hybrid architecture exhibits strong generalization to out-of-distribution tasks. It interprets visual or textual inputs from process monitoring, explains its decisions, and autonomously generates precise machine instructions, without requiring explicit annotations. CIPHER thus lays the foundations for autonomous systems that act with precision, reason with context, and communicate decisions transparently, supporting safe and trusted deployment in industrial settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CIPHER (Control and Interpretation of Production via Hybrid Expertise and Reasoning)ï¼Œä¸€ç§æ—¨åœ¨ä¸ºå·¥ä¸šæ§åˆ¶æ¨¡æ‹Ÿäººç±»æ¨ç†çš„è§†è§‰è¯­è¨€åŠ¨ä½œ (Vision-Language-Action, VLA) æ¨¡å‹æ¡†æ¶ã€‚é’ˆå¯¹åŸºç¡€æ¨¡å‹åœ¨å·¥ç¨‹åº”ç”¨ä¸­ç¼ºä¹å®šé‡ç²¾åº¦çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªå›å½’æ¨¡å‹ä½œä¸ºè¿‡ç¨‹ä¸“å®¶ï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æŠ€æœ¯ä»¥æ”¯æŒç‰©ç†å¯å‘çš„é“¾å¼æ€ç»´ (Chain-of-Thought) æ¨ç†ã€‚CIPHER èƒ½å¤Ÿæœ‰æ•ˆè§£è¯»ç”Ÿäº§ç›‘æ§ä¸­çš„è§†è§‰æˆ–æ–‡æœ¬è¾“å…¥ï¼Œåœ¨æ— éœ€æ˜¾å¼æ ‡æ³¨çš„æƒ…å†µä¸‹è§£é‡Šå†³ç­–é€»è¾‘å¹¶è‡ªä¸»ç”Ÿæˆç²¾ç¡®çš„æœºå™¨æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ¶æ„å¯¹åˆ†å¸ƒå¤– (Out-of-Distribution) ä»»åŠ¡è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚CIPHER ä¸ºæ„å»ºå…·å¤‡é«˜ç²¾åº¦æ‰§è¡Œã€ä¸Šä¸‹æ–‡æ¨ç†å’Œé€æ˜å†³ç­–èƒ½åŠ›çš„è‡ªä¸»å·¥ä¸šç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ï¼Œæœ‰åŠ›æ”¯æŒäº†å…¶åœ¨å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å®‰å…¨ä¸å¯ä¿¡éƒ¨ç½²ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08462v1",
      "published_date": "2025-06-10 05:37:33 UTC",
      "updated_date": "2025-06-10 05:37:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:25.742535+00:00"
    },
    {
      "arxiv_id": "2506.08460v2",
      "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning",
      "title_zh": "MOBODYï¼šåŸºäºæ¨¡å‹çš„åŠ¨æ€åç§»ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yihong Guo",
        "Yu Yang",
        "Pan Xu",
        "Anqi Liu"
      ],
      "abstract": "We study off-dynamics offline reinforcement learning, where the goal is to learn a policy from offline source and limited target datasets with mismatched dynamics. Existing methods either penalize the reward or discard source transitions occurring in parts of the transition space with high dynamics shift. As a result, they optimize the policy using data from low-shift regions, limiting exploration of high-reward states in the target domain that do not fall within these regions. Consequently, such methods often fail when the dynamics shift is significant or the optimal trajectories lie outside the low-shift regions. To overcome this limitation, we propose MOBODY, a Model-Based Off-Dynamics Offline RL algorithm that optimizes a policy using learned target dynamics transitions to explore the target domain, rather than only being trained with the low dynamics-shift transitions. For the dynamics learning, built on the observation that achieving the same next state requires taking different actions in different domains, MOBODY employs separate action encoders for each domain to encode different actions to the shared latent space while sharing a unified representation of states and a common transition function. We further introduce a target Q-weighted behavior cloning loss in policy optimization to avoid out-of-distribution actions, which push the policy toward actions with high target-domain Q-values, rather than high source domain Q-values or uniformly imitating all actions in the offline dataset. We evaluate MOBODY on a wide range of MuJoCo and Adroit benchmarks, demonstrating that it outperforms state-of-the-art off-dynamics RL baselines as well as policy learning methods based on different dynamics learning baselines, with especially pronounced improvements in challenging scenarios where existing methods struggle.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿åŠ¨åŠ›å­¦å¤±é…ï¼ˆoff-dynamics offline reinforcement learningï¼‰é—®é¢˜ï¼Œæå‡ºäº†MOBODYæ¡†æ¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼šæƒ©ç½šæˆ–èˆå¼ƒåŠ¨åŠ›å­¦å·®å¼‚è¾ƒå¤§çš„æºæ•°æ®ï¼Œå¯¼è‡´åœ¨ç›®æ ‡é¢†åŸŸé«˜å›æŠ¥çŠ¶æ€ä¸‹çš„æ¢ç´¢å—é™ï¼Œå°¤å…¶åœ¨åŠ¨åŠ›å­¦åç§»æ˜¾è‘—æ—¶å®¹æ˜“å¤±è´¥ã€‚MOBODYé‡‡ç”¨åŸºäºæ¨¡å‹ï¼ˆModel-Basedï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„ç›®æ ‡é¢†åŸŸåŠ¨åŠ›å­¦ï¼ˆtarget dynamicsï¼‰è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå…‹æœäº†ä»…ä¾èµ–ä½åç§»æ•°æ®çš„å±€é™ã€‚åœ¨åŠ¨åŠ›å­¦å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶ä¸ºä¸åŒåŸŸè®¾è®¡äº†ç‹¬ç«‹çš„åŠ¨ä½œç¼–ç å™¨ï¼ˆaction encodersï¼‰ï¼Œå°†ä¸åŒåŠ¨ä½œæ˜ å°„åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œå¹¶å…±äº«çŠ¶æ€è¡¨å¾å’Œè½¬ç§»å‡½æ•°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç›®æ ‡QåŠ æƒè¡Œä¸ºå…‹éš†ï¼ˆtarget Q-weighted behavior cloningï¼‰æŸå¤±ï¼Œæ—¨åœ¨å¼•å¯¼ç­–ç•¥é€‰æ‹©ç›®æ ‡åŸŸé«˜Qå€¼çš„åŠ¨ä½œï¼Œæœ‰æ•ˆé¿å…äº†åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰åŠ¨ä½œã€‚åœ¨MuJoCoå’ŒAdroitåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMOBODYåœ¨å¤„ç†æŒ‘æˆ˜æ€§åŠ¨åŠ›å­¦åç§»ä»»åŠ¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08460v2",
      "published_date": "2025-06-10 05:36:54 UTC",
      "updated_date": "2025-10-17 02:06:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:45.053255+00:00"
    },
    {
      "arxiv_id": "2506.08459v1",
      "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems",
      "title_zh": "é¢å‘è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå®‰å…¨éªŒè¯çš„æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Juanran Wang",
        "Marc R. Schlichting",
        "Harrison Delecki",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Safety validation of autonomous driving systems is extremely challenging due to the high risks and costs of real-world testing as well as the rarity and diversity of potential failures. To address these challenges, we train a denoising diffusion model to generate potential failure cases of an autonomous vehicle given any initial traffic state. Experiments on a four-way intersection problem show that in a variety of scenarios, the diffusion model can generate realistic failure samples while capturing a wide variety of potential failures. Our model does not require any external training dataset, can perform training and inference with modest computing resources, and does not assume any prior knowledge of the system under test, with applicability to safety validation for traffic intersections.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ(Autonomous Driving Systems)å®‰å…¨éªŒè¯ä¸­é¢ä¸´çš„é«˜é£é™©ã€é«˜æˆæœ¬ä»¥åŠæ•…éšœæ ·æœ¬ç¨€ç¼ºä¸”å¤šæ ·ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹(Denoising Diffusion Model)ç”Ÿæˆæ½œåœ¨æ•…éšœæ¡ˆä¾‹(Failure Cases)çš„æ–°æ–¹æ³•ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„åˆå§‹äº¤é€šçŠ¶æ€ç”Ÿæˆå¤šæ ·åŒ–çš„æ•…éšœåœºæ™¯ï¼Œå¹¶åœ¨å››è·¯äº¤å‰å£é—®é¢˜ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ•è·å¹¶ç”Ÿæˆå¤§é‡çœŸå®ä¸”å…·æœ‰ä»£è¡¨æ€§çš„æ•…éšœæ ·æœ¬ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†å®é™…æµ‹è¯•ä¸­æç«¯æ¡ˆä¾‹ä¸è¶³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ— éœ€å¤–éƒ¨è®­ç»ƒæ•°æ®é›†ï¼Œä»…éœ€é€‚åº¦çš„è®¡ç®—èµ„æºå³å¯å®Œæˆè®­ç»ƒä¸æ¨ç†ï¼Œä¸”ä¸ä¾èµ–äºè¢«æµ‹ç³»ç»Ÿçš„ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸‹çš„å®‰å…¨éªŒè¯æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡æ™®é€‚æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08459v1",
      "published_date": "2025-06-10 05:31:33 UTC",
      "updated_date": "2025-06-10 05:31:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:06.748080+00:00"
    },
    {
      "arxiv_id": "2506.17265v2",
      "title": "SUA: Stealthy Multimodal Large Language Model Unlearning Attack",
      "title_zh": "SUAï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éšè”½é—å¿˜æ”»å‡»",
      "authors": [
        "Xianren Zhang",
        "Hui Liu",
        "Delvin Ce Zhang",
        "Xianfeng Tang",
        "Qi He",
        "Dongwon Lee",
        "Suhang Wang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨é—å¿˜å­¦ä¹ (Unlearning)è¿‡ç¨‹ä¸­éšç§ä¿æŠ¤çš„æœ‰æ•ˆæ€§é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†æ•æ„ŸçŸ¥è¯†å¯èƒ½ä»…è¢«éšè—è€Œéå½»åº•åˆ é™¤çš„é£é™©ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSUA(Stealthy Unlearning Attack)çš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ é€šç”¨çš„å™ªå£°æ¨¡å¼(Universal Noise Pattern)æ¥è§¦å‘æ¨¡å‹æ³„éœ²å·²è¢«â€œé—å¿˜â€çš„æ•æ„Ÿå†…å®¹ã€‚ä¸ºäº†æå‡æ”»å‡»çš„éšè”½æ€§ï¼ŒSUAå¼•å…¥äº†åµŒå…¥å¯¹é½æŸå¤±(Embedding Alignment Loss)ï¼Œä½¿å›¾åƒæ‰°åŠ¨åœ¨è¯­ä¹‰åµŒå…¥ç©ºé—´ä¸­éš¾ä»¥è¢«æ£€æµ‹ï¼Œä»è€Œæœ‰æ•ˆè§„é¿ç°æœ‰é˜²å¾¡æœºåˆ¶ã€‚å®éªŒç»“æœè¯å®ï¼ŒSUAèƒ½å¤Ÿé«˜æ•ˆæ¢å¤é—å¿˜ä¿¡æ¯ä¸”å…·å¤‡è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œå³ä¾¿æ˜¯åœ¨æœªè§è¿‡çš„å›¾åƒä¸Šä¹Ÿèƒ½æˆåŠŸè¯±å¯¼çŸ¥è¯†å¤ç°ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å½“å‰é—å¿˜å­¦ä¹ æŠ€æœ¯çš„å±€é™æ€§ï¼Œè¯æ˜çŸ¥è¯†å¤ç°å¹¶éå¶ç„¶å¤±è´¥è€Œæ˜¯ä¸€ç§ä¸€è‡´çš„æ¨¡å‹è¡Œä¸ºï¼Œä¸ºæœªæ¥å¼€å‘æ›´å¯é çš„éšç§ä¿æŠ¤æœºåˆ¶æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP25",
      "pdf_url": "https://arxiv.org/pdf/2506.17265v2",
      "published_date": "2025-06-10 04:52:03 UTC",
      "updated_date": "2025-09-21 19:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:04.605213+00:00"
    },
    {
      "arxiv_id": "2506.08446v1",
      "title": "A Survey on Large Language Models for Mathematical Reasoning",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†ç»¼è¿°",
      "authors": [
        "Peng-Yuan Wang",
        "Tian-Shuo Liu",
        "Chenyang Wang",
        "Yi-Di Wang",
        "Shu Yan",
        "Cheng-Xing Jia",
        "Xu-Hui Liu",
        "Xin-Wei Chen",
        "Jia-Cheng Xu",
        "Ziniu Li",
        "Yang Yu"
      ],
      "abstract": "Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and \"test-time scaling\". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿåœ°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ï¼ˆMathematical Reasoningï¼‰é¢†åŸŸçš„å‘å±•ç°çŠ¶ï¼Œæ¶µç›–äº†ä»é¢„è®­ç»ƒé˜¶æ®µçš„æ•°å­¦ç†è§£åˆ°ç”Ÿæˆé˜¶æ®µçš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†æ¼”è¿›ã€‚æ–‡ç« è¯¦ç»†è¯„ä¼°äº†åŒ…æ‹¬æç¤ºå·¥ç¨‹ï¼ˆPromptingï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuningï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰åœ¨å†…çš„å¤šç§å¢å¼ºæ–¹æ³•ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†é•¿é“¾å¼æ€ç»´ï¼ˆExtended CoTï¼‰å’Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-time Scalingï¼‰ç­‰æœ€æ–°æŠ€æœ¯è¶‹åŠ¿ã€‚å°½ç®¡LLMsåœ¨æ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ¨¡å‹å®¹é‡ã€æ¨ç†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä»é¢ä¸´æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†å…ˆè¿›é¢„è®­ç»ƒã€çŸ¥è¯†å¢å¼ºï¼ˆKnowledge Augmentationï¼‰ã€å½¢å¼åŒ–æ¨ç†æ¡†æ¶ï¼ˆFormal Reasoning Frameworksï¼‰ä»¥åŠå…ƒæ³›åŒ–ï¼ˆMeta-generalizationï¼‰ç­‰å‰æ²¿ç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°ä¸ä»…ä¸ºæå‡LLMsçš„æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ€§çš„è§è§£ï¼Œä¹Ÿä¸ºå°†è¿™äº›æŠ€æœ¯æ‰©å±•è‡³å…¶ä»–é€»è¾‘å¯†é›†å‹é¢†åŸŸå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08446v1",
      "published_date": "2025-06-10 04:44:28 UTC",
      "updated_date": "2025-06-10 04:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:48:50.491988+00:00"
    },
    {
      "arxiv_id": "2506.08441v1",
      "title": "Time-Aware World Model for Adaptive Prediction and Control",
      "title_zh": "é¢å‘è‡ªé€‚åº”é¢„æµ‹ä¸æ§åˆ¶çš„æ—¶é—´æ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Anh N. Nhu",
        "Sanghyun Son",
        "Ming Lin"
      ],
      "abstract": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based approach that explicitly incorporates temporal dynamics. By conditioning on the time-step size, Î”t, and training over a diverse range of Î”t values -- rather than sampling at a fixed time-step -- TAWM learns both high- and low-frequency task dynamics across diverse control problems. Grounded in the information-theoretic insight that the optimal sampling rate depends on a system's underlying dynamics, this time-aware formulation improves both performance and data efficiency. Empirical evaluations show that TAWM consistently outperforms conventional models across varying observation rates in a variety of control tasks, using the same number of training samples and iterations. Our code can be found online at: github.com/anh-nn01/Time-Aware-World-Model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Time-Aware World Model (TAWM)ï¼Œè¿™æ˜¯ä¸€ç§æ˜¾å¼ç»“åˆæ—¶é—´åŠ¨åŠ›å­¦(temporal dynamics)çš„åŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»¥æ—¶é—´æ­¥é•¿Î”tä¸ºæ¡ä»¶ï¼Œå¹¶åœ¨å¤šç§Î”tå€¼ä¸‹è¿›è¡Œè®­ç»ƒï¼Œå–ä»£äº†ä¼ ç»Ÿçš„å›ºå®šæ—¶é—´æ­¥é‡‡æ ·æ–¹å¼ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶æ•æ‰å„ç§æ§åˆ¶é—®é¢˜ä¸­çš„é«˜é¢‘å’Œä½é¢‘ä»»åŠ¡åŠ¨åŠ›å­¦(task dynamics)ã€‚TAWMçš„è®¾è®¡åŸºäºâ€œæœ€ä½³é‡‡æ ·ç‡å–å†³äºç³»ç»Ÿåº•å±‚åŠ¨åŠ›å­¦â€çš„ä¿¡æ¯è®ºè§è§£ï¼Œä»è€Œå®ç°äº†æ›´é«˜çš„é¢„æµ‹ç²¾åº¦å’Œè‡ªé€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¿æŒè®­ç»ƒæ ·æœ¬å’Œè¿­ä»£æ¬¡æ•°ä¸€è‡´çš„æƒ…å†µä¸‹ï¼ŒTAWMåœ¨ä¸åŒè§‚æµ‹é¢‘ç‡çš„å¤šç§æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•°æ®æ•ˆç‡(data efficiency)ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¸–ç•Œæ¨¡å‹ä¸­å¼•å…¥æ—¶é—´æ„ŸçŸ¥æœºåˆ¶å¯¹äºæå‡è‡ªé€‚åº”é¢„æµ‹ä¸æ§åˆ¶èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08441v1",
      "published_date": "2025-06-10 04:28:11 UTC",
      "updated_date": "2025-06-10 04:28:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:02.355011+00:00"
    },
    {
      "arxiv_id": "2506.08440v3",
      "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization",
      "title_zh": "TGRPOï¼šåŸºäºè½¨è¿¹çº§ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Zengjue Chen",
        "Runliang Niu",
        "He Kong",
        "Qi Wang",
        "Qianli Xing",
        "Zipei Fan"
      ],
      "abstract": "Visual-Language-Action (VLA) models have demonstrated strong cross-scenario generalization capabilities in various robotic tasks through large-scale pre-training and task-specific fine-tuning. However, their training paradigm mainly relies on manually collected successful demonstrations, making it difficult to adapt to complex environments when encountering out-of-distribution (OOD) scenarios or execution biases. While Reinforcement Learning (RL) provides a closed-loop optimization framework via active trial-and-error mechanism, it suffers from sparse rewards, high variance, and unstable optimization in long-horizon robotic tasks. To address these limitations, we propose Trajectory-based Group Relative Policy Optimization (TGRPO), an online RL-based training framework for VLA models. TGRPO leverages task analysis generated by a large language model to automatically construct dense reward functions, providing fine-grained feedback to accelerate convergence and improve credit assignment. The core of our method is a group-based strategy that samples and normalizes multiple trajectories in parallel, reducing variance through relative comparison. By integrating trajectory-level and step-level advantage estimation, TGRPO captures both global and local optimization signals without relying on a value network. Experiments on four task categories of the LIBERO benchmark demonstrate that TGRPO achieves an average success rate of 80.7\\%, which is 4.2\\% higher than that of Supervised Fine-Tuning (SFT) and outperforms other representative RL-based post-training methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TGRPO (Trajectory-based Group Relative Policy Optimization)ï¼Œä¸€ç§ç”¨äºVision-Language-Action (VLA) æ¨¡å‹çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹  (RL) å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åœ¨é¢å¯¹åˆ†å¸ƒå¤– (OOD) åœºæ™¯å’Œæ‰§è¡Œåå·®æ—¶é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚TGRPOåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆçš„ä»»åŠ¡åˆ†æè‡ªåŠ¨æ„å»ºç¨ å¯†å¥–åŠ±å‡½æ•° (dense reward functions)ï¼Œé€šè¿‡ç»†ç²’åº¦åé¦ˆå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡ä¸­é¢ä¸´çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŒ–ä¸ç¨³å®šçš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé‡‡ç”¨åŸºäºåˆ†ç»„çš„ç­–ç•¥ï¼Œé€šè¿‡å¹¶è¡Œé‡‡æ ·å’Œå½’ä¸€åŒ–å¤šä¸ªè½¨è¿¹å¹¶è¿›è¡Œç›¸å¯¹æ¯”è¾ƒï¼Œä»è€Œåœ¨å‡å°‘æ–¹å·®çš„åŒæ—¶æå‡äº†å­¦åˆ†åˆ†é… (credit assignment) çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ•´åˆè½¨è¿¹çº§å’Œæ­¥éª¤çº§çš„ä¼˜åŠ¿ä¼°è®¡ï¼ŒTGRPOèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä»·å€¼ç½‘ç»œ (value network) çš„æƒ…å†µä¸‹æ•æ‰å…¨å±€å’Œå±€éƒ¨ä¼˜åŒ–ä¿¡å·ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼ŒTGRPOå®ç°äº†80.7%çš„å¹³å‡æˆåŠŸç‡ï¼Œæ¯”ç›‘ç£å¾®è°ƒ (SFT) é«˜å‡º4.2%ï¼Œä¸”æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–ä»£è¡¨æ€§çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08440v3",
      "published_date": "2025-06-10 04:27:49 UTC",
      "updated_date": "2025-09-27 09:37:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:03.550757+00:00"
    },
    {
      "arxiv_id": "2506.09081v3",
      "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation",
      "title_zh": "FlagEvalMMï¼šé¢å‘å…¨é¢å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°çš„çµæ´»æ¡†æ¶",
      "authors": [
        "Zheqi He",
        "Yesheng Liu",
        "Jing-shu Zheng",
        "Xuejing Li",
        "Jin-Ge Yao",
        "Bowen Qin",
        "Richeng Xuan",
        "Xi Yang"
      ],
      "abstract": "We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† FlagEvalMMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å„ç§è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚å®ƒæ¶µç›–äº†è§†è§‰é—®ç­” (Visual Question Answering)ã€æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆ (Text-to-Image/Video Generation) ä»¥åŠå›¾åƒæ–‡æœ¬æ£€ç´¢ (Image-Text Retrieval) ç­‰å¤šç§ç»´åº¦ã€‚FlagEvalMM é€šè¿‡ç‹¬ç«‹çš„è¯„ä¼°æœåŠ¡å°†æ¨¡å‹æ¨ç†ä¸è¯„ä¼°è§£è€¦ï¼Œä»è€Œå®ç°äº†çµæ´»çš„èµ„æºåˆ†é…ä»¥åŠæ–°ä»»åŠ¡å’Œæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é›†æˆäº† vLLM å’Œ SGLang ç­‰å…ˆè¿›æ¨ç†åŠ é€Ÿå·¥å…·ï¼Œå¹¶åˆ©ç”¨å¼‚æ­¥æ•°æ®åŠ è½½æŠ€æœ¯æ˜¾è‘—æå‡äº†æ•´ä½“è¯„ä¼°æ•ˆç‡ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒFlagEvalMM èƒ½å¤Ÿé’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§æä¾›å‡†ç¡®ä¸”é«˜æ•ˆçš„åˆ†æè§è§£ã€‚ç›®å‰è¯¥æ¡†æ¶å·²åœ¨ GitHub å…¬å¼€å‘å¸ƒï¼Œä¸ºæ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶ä¸å‘å±•æä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACL 2025 Demo",
      "pdf_url": "https://arxiv.org/pdf/2506.09081v3",
      "published_date": "2025-06-10 04:19:02 UTC",
      "updated_date": "2025-07-28 19:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:05.943914+00:00"
    },
    {
      "arxiv_id": "2506.09080v2",
      "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making",
      "title_zh": "FinHEARï¼šèåˆäººç±»ä¸“å®¶ç»éªŒä¸è‡ªé€‚åº”é£é™©æ„ŸçŸ¥æ—¶åºæ¨ç†çš„é‡‘èå†³ç­–",
      "authors": [
        "Jiaxiang Chen",
        "Mingxi Zou",
        "Zhuo Wang",
        "Qifan Wang",
        "Dongning Sun",
        "Chi Zhang",
        "Zenglin Xu"
      ],
      "abstract": "Financial decision-making presents unique challenges for language models, demanding temporal reasoning, adaptive risk assessment, and responsiveness to dynamic events. While large language models (LLMs) show strong general reasoning capabilities, they often fail to capture behavioral patterns central to human financial decisions-such as expert reliance under information asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to analyze historical trends, interpret current events, and retrieve expert-informed precedents within an event-centric pipeline. Grounded in behavioral economics, it incorporates expert-guided retrieval, confidence-adjusted position sizing, and outcome-based refinement to enhance interpretability and robustness. Empirical results on curated financial datasets show that FinHEAR consistently outperforms strong baselines across trend prediction and trading tasks, achieving higher accuracy and better risk-adjusted returns.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FinHEARï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èå†³ç­–çš„äººç±»ä¸“ä¸šçŸ¥è¯†ä¸è‡ªé€‚åº”é£é™©æ„ŸçŸ¥æ—¶é—´æ¨ç†çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ (multi-agent framework)ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†é‡‘èæ—¶é—´æ¨ç†å’Œè¡Œä¸ºæ¨¡å¼ï¼ˆå¦‚æŸå¤±åŒæ¶å’Œä¿¡æ¯ä¸å¯¹ç§°ï¼‰æ–¹é¢çš„å±€é™æ€§ï¼ŒFinHEAR æ·±åº¦ç»“åˆäº†è¡Œä¸ºç»æµå­¦ (behavioral economics) åŸç†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„æµæ°´çº¿åè°ƒä¸“é—¨æ™ºèƒ½ä½“ï¼Œç”¨äºåˆ†æå†å²è¶‹åŠ¿ã€è§£è¯»å½“å‰äº‹ä»¶å¹¶æ£€ç´¢å—ä¸“å®¶å¯å‘çš„å…ˆä¾‹ã€‚FinHEAR è¿›ä¸€æ­¥æ•´åˆäº†ä¸“å®¶å¼•å¯¼æ£€ç´¢ (expert-guided retrieval)ã€ç½®ä¿¡åº¦è°ƒèŠ‚çš„ä»“ä½ç®¡ç† (confidence-adjusted position sizing) ä»¥åŠåŸºäºç»“æœçš„ä¼˜åŒ–æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å†³ç­–çš„å¯è§£é‡Šæ€§ä¸é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFinHEAR åœ¨è¶‹åŠ¿é¢„æµ‹å’Œäº¤æ˜“ä»»åŠ¡ä¸­å‡æŒç»­ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†æ›´é«˜çš„é¢„æµ‹å‡†ç¡®ç‡å’Œæ›´ä¼˜çš„é£é™©è°ƒæ•´æ”¶ç›Š (risk-adjusted returns)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.CP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09080v2",
      "published_date": "2025-06-10 04:06:51 UTC",
      "updated_date": "2025-10-17 11:11:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:22.172886+00:00"
    },
    {
      "arxiv_id": "2506.08426v1",
      "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems",
      "title_zh": "HASFLï¼šé¢å‘è¾¹ç¼˜è®¡ç®—ç³»ç»Ÿçš„å¼‚æ„æ„ŸçŸ¥æ‹†åˆ†è”é‚¦å­¦ä¹ ",
      "authors": [
        "Zheng Lin",
        "Zhe Chen",
        "Xianhao Chen",
        "Wei Ni",
        "Yue Gao"
      ],
      "abstract": "Split federated learning (SFL) has emerged as a promising paradigm to democratize machine learning (ML) on edge devices by enabling layer-wise model partitioning. However, existing SFL approaches suffer significantly from the straggler effect due to the heterogeneous capabilities of edge devices. To address the fundamental challenge, we propose adaptively controlling batch sizes (BSs) and model splitting (MS) for edge devices to overcome resource heterogeneity. We first derive a tight convergence bound of SFL that quantifies the impact of varied BSs and MS on learning performance. Based on the convergence bound, we propose HASFL, a heterogeneity-aware SFL framework capable of adaptively controlling BS and MS to balance communication-computing latency and training convergence in heterogeneous edge networks. Extensive experiments with various datasets validate the effectiveness of HASFL and demonstrate its superiority over state-of-the-art benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¡ç®—ç³»ç»Ÿä¸­æ‹†åˆ†è”é‚¦å­¦ä¹  (Split Federated Learning, SFL) å› è®¾å¤‡æ€§èƒ½å¼‚æ„å¯¼è‡´çš„æ‰é˜Ÿè€… (straggler) é—®é¢˜ï¼Œæå‡ºäº† HASFL æ¡†æ¶ã€‚ç ”ç©¶è€…é¦–å…ˆæ¨å¯¼å‡ºäº† SFL çš„ç´§è‡´æ”¶æ•›ç•Œï¼Œé‡åŒ–äº†ä¸åŒæ‰¹æ¬¡å¤§å° (Batch Sizes, BS) å’Œæ¨¡å‹æ‹†åˆ† (Model Splitting, MS) å¯¹å­¦ä¹ æ€§èƒ½çš„å…·ä½“å½±å“ã€‚åŸºäºè¯¥æ”¶æ•›ç•Œï¼ŒHASFL èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ§åˆ¶è¾¹ç¼˜è®¾å¤‡çš„ BS å’Œ MS å‚æ•°ï¼Œæ—¨åœ¨å¼‚æ„è¾¹ç¼˜ç½‘ç»œä¸­å®ç°é€šä¿¡è®¡ç®—å»¶è¿Ÿä¸è®­ç»ƒæ”¶æ•›æ€§èƒ½ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒHASFL å…·æœ‰å‡ºè‰²çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å„ç±»å…ˆè¿›åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 11 figures. arXiv admin note: text overlap with arXiv:2403.13101",
      "pdf_url": "https://arxiv.org/pdf/2506.08426v1",
      "published_date": "2025-06-10 04:00:01 UTC",
      "updated_date": "2025-06-10 04:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:37.228858+00:00"
    },
    {
      "arxiv_id": "2506.09079v2",
      "title": "VidBridge-R1: Bridging QA and Captioning for RL-based Video Understanding Models with Intermediate Proxy Tasks",
      "title_zh": "VidBridge-R1ï¼šé€šè¿‡ä¸­é—´ä»£ç†ä»»åŠ¡è¡”æ¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†é¢‘ç†è§£æ¨¡å‹ä¸­çš„é—®ç­”ä¸æè¿°ç”Ÿæˆ",
      "authors": [
        "Xinlong Chen",
        "Yuanxing Zhang",
        "Yushuo Guan",
        "Weihong Lin",
        "Zekun Wang",
        "Bohan Zeng",
        "Yang Shi",
        "Sihan Yang",
        "Qiang Liu",
        "Pengfei Wan",
        "Liang Wang",
        "Tieniu Tan"
      ],
      "abstract": "The \"Reason-Then-Respond\" paradigm, enhanced by Reinforcement Learning, has shown great promise in advancing Multimodal Large Language Models. However, its application to the video domain has led to specialized models that excel at either question answering (QA) or captioning tasks, but struggle to master both. Naively combining reward signals from these tasks results in mutual performance degradation, which we attribute to a conflict between their opposing task natures. To address this challenge, we propose a novel training framework built upon two intermediate proxy tasks: DarkEventInfer, which presents videos with masked event segments, requiring models to infer the obscured content based on contextual video cues; and MixVidQA, which presents interleaved video sequences composed of two distinct clips, challenging models to isolate and reason about one while disregarding the other. These proxy tasks compel the model to simultaneously develop both holistic, divergent understanding and precise, convergent reasoning capabilities. Embodying this framework, we present VidBridge-R1, the first versatile video reasoning model that effectively bridges the paradigm conflict. Extensive experiments show that VidBridge-R1 achieves significant performance gains on both QA and captioning within one model, demonstrating the efficacy of our approach in fostering more generalizable and powerful video understanding models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)é©±åŠ¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é¢†åŸŸéš¾ä»¥åŒæ—¶å…¼é¡¾é—®ç­”(QA)å’Œè§†é¢‘æè¿°(Captioning)ä»»åŠ¡çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¸¤ç±»ä»»åŠ¡æ€§è´¨çš„å†²çªä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ç›¸äº’ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†VidBridge-R1è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸¤ä¸ªä¸­é—´ä»£ç†ä»»åŠ¡(Intermediate Proxy Tasks)æ¥æ¡¥æ¥è¿™ç§èŒƒå¼å†²çªã€‚å…¶ä¸­DarkEventInferä»»åŠ¡è¦æ±‚æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­è¢«æ©è”½çš„è§†é¢‘å†…å®¹ï¼Œæ—¨åœ¨åŸ¹å…»æ•´ä½“çš„å‘æ•£æ€§ç†è§£èƒ½åŠ›ï¼›è€ŒMixVidQAä»»åŠ¡åˆ™æŒ‘æˆ˜æ¨¡å‹ä»äº¤ç»‡çš„è§†é¢‘åºåˆ—ä¸­åˆ†ç¦»å¹¶æ¨ç†ç‰¹å®šç‰‡æ®µï¼Œä»¥å¢å¼ºå…¶ç²¾å‡†çš„æ”¶æ•›æ€§æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVidBridge-R1åœ¨å•ä¸ªæ¨¡å‹å†…å®ç°äº†QAå’ŒCaptioningæ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¹³è¡¡ä¸åŒè§†é¢‘ç†è§£éœ€æ±‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºæ›´å…·æ³›åŒ–èƒ½åŠ›ä¸”åŠŸèƒ½å¼ºå¤§çš„è§†é¢‘ç†è§£æ¨¡å‹æä¾›äº†å…¨æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09079v2",
      "published_date": "2025-06-10 03:57:53 UTC",
      "updated_date": "2025-09-26 06:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:25.350851+00:00"
    },
    {
      "arxiv_id": "2506.08424v2",
      "title": "SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy",
      "title_zh": "SHIELDï¼šèåˆç¨€ç–æ€§ä¸å±‚çº§æœºåˆ¶çš„å¤šä»»åŠ¡å¤šåˆ†å¸ƒè½¦è¾†è·¯å¾„é—®é¢˜æ±‚è§£å™¨",
      "authors": [
        "Yong Liang Goh",
        "Zhiguang Cao",
        "Yining Ma",
        "Jianan Zhou",
        "Mohammed Haroon Dupty",
        "Wee Sun Lee"
      ],
      "abstract": "Recent advances toward foundation models for routing problems have shown great potential of a unified deep model for various VRP variants. However, they overlook the complex real-world customer distributions. In this work, we advance the Multi-Task VRP (MTVRP) setting to the more realistic yet challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce SHIELD, a novel model that leverages both sparsity and hierarchy principles. Building on a deeper decoder architecture, we first incorporate the Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both efficiency and generalization by allowing the model to dynamically select nodes to use or skip each decoder layer, providing the needed capacity to adaptively allocate computation for learning the task/distribution specific and shared representations. We also develop a context-based clustering layer that exploits the presence of hierarchical structures in the problems to produce better local representations. These two designs inductively bias the network to identify key features that are common across tasks and distributions, leading to significantly improved generalization on unseen ones. Our empirical results demonstrate the superiority of our approach over existing methods on 9 real-world maps with 16 VRP variants each.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºç¡€æ¨¡å‹åœ¨è§£å†³è½¦è¾†è·¯å¾„é—®é¢˜(VRP)æ—¶å¿½è§†å¤æ‚ç°å®å®¢æˆ·åˆ†å¸ƒçš„é—®é¢˜ï¼Œæå‡ºäº†æ›´å…·æŒ‘æˆ˜æ€§çš„å¤šä»»åŠ¡å¤šåˆ†å¸ƒ(MTMDVRP)è®¾å®šï¼Œå¹¶å¼•å…¥äº†ç»“åˆç¨€ç–æ€§ä¸å±‚æ¬¡åŒ–åŸåˆ™çš„SHIELDæ¨¡å‹ã€‚SHIELDé€šè¿‡åœ¨æ·±åº¦è§£ç å™¨ä¸­é›†æˆæ·±åº¦æ··åˆ(Mixture-of-Depths, MoD)æŠ€æœ¯æ¥å®ç°ç¨€ç–æ€§ï¼Œå…è®¸æ¨¡å‹åŠ¨æ€é€‰æ‹©å‚ä¸è®¡ç®—çš„èŠ‚ç‚¹ï¼Œä»è€Œä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„åˆ†é…å¹¶æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¼€å‘äº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„èšç±»å±‚ï¼Œåˆ©ç”¨è·¯å¾„é—®é¢˜ä¸­çš„å±‚æ¬¡ç»“æ„æ¥è·å–æ›´ç²¾ç¡®çš„å±€éƒ¨è¡¨å¾ã€‚è¿™äº›è®¾è®¡é€šè¿‡å½’çº³åç½®å¼•å¯¼ç½‘ç»œè¯†åˆ«è·¨ä»»åŠ¡å’Œåˆ†å¸ƒçš„å…±æ€§ç‰¹å¾ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æœªè§åˆ†å¸ƒä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSHIELDåœ¨åŒ…å«16ç§VRPå˜ä½“çš„9ä¸ªçœŸå®ä¸–ç•Œåœ°å›¾æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in the 42nd International Conference of Machine Learning (ICML)",
      "pdf_url": "https://arxiv.org/pdf/2506.08424v2",
      "published_date": "2025-06-10 03:55:14 UTC",
      "updated_date": "2025-06-11 06:43:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:55.325229+00:00"
    },
    {
      "arxiv_id": "2506.08422v2",
      "title": "Transforming Expert Knowledge into Scalable Ontology via Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å°†ä¸“å®¶çŸ¥è¯†è½¬åŒ–ä¸ºå¯æ‰©å±•çš„æœ¬ä½“",
      "authors": [
        "Ikkei Itoku",
        "David Theil",
        "Evelyn Eichelsdoerfer Uehara",
        "Sreyoshi Bhaduri",
        "Junnosuke Kuroda",
        "Toshi Yumoto",
        "Alex Gil",
        "Natalie Perez",
        "Rajesh Cherukuri",
        "Naumaan Nayyar"
      ],
      "abstract": "Having a unified, coherent taxonomy is essential for effective knowledge representation in domain-specific applications as diverse terminologies need to be mapped to underlying concepts. Traditional manual approaches to taxonomy alignment rely on expert review of concept pairs, but this becomes prohibitively expensive and time-consuming at scale, while subjective interpretations often lead to expert disagreements. Existing automated methods for taxonomy alignment have shown promise but face limitations in handling nuanced semantic relationships and maintaining consistency across different domains. These approaches often struggle with context-dependent concept mappings and lack transparent reasoning processes. We propose a novel framework that combines large language models (LLMs) with expert calibration and iterative prompt optimization to automate taxonomy alignment. Our method integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in generating both taxonomy linkages and supporting rationales. In evaluating our framework on a domain-specific mapping task of concept essentiality, we achieved an F1-score of 0.97, substantially exceeding the human benchmark of 0.68. These results demonstrate the effectiveness of our approach in scaling taxonomy alignment while maintaining high-quality mappings and preserving expert oversight for ambiguous cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢†åŸŸç‰¹å®šåº”ç”¨ä¸­ç»Ÿä¸€åˆ†ç±»ä½“ç³»(Taxonomy)å¯¹é½é¢ä¸´çš„æ‰‹å·¥å¤„ç†æˆæœ¬é«˜ã€ä¸»è§‚æ€§å¼ºä»¥åŠç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚è¯­ä¹‰å…³ç³»ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ä¸“å®¶æ ¡å‡†çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆä¸“å®¶æ ‡è®°ç¤ºä¾‹ã€å¤šé˜¶æ®µæç¤ºå·¥ç¨‹(Prompt Engineering)å’Œè¿­ä»£æç¤ºä¼˜åŒ–(Iterative Prompt Optimization)ï¼Œå®ç°äº†åˆ†ç±»ä½“ç³»å¯¹é½çš„è‡ªåŠ¨åŒ–ï¼Œå¹¶èƒ½ç”Ÿæˆé…å¥—çš„æ¨ç†é€»è¾‘ã€‚åœ¨å…³äºæ¦‚å¿µé‡è¦æ€§(Concept Essentiality)çš„ç‰¹å®šé¢†åŸŸæ˜ å°„ä»»åŠ¡è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶å–å¾—äº†0.97çš„F1-scoreï¼Œè¿œè¶…0.68çš„äººç±»åŸºå‡†æ°´å¹³ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å®ç°è§„æ¨¡åŒ–åˆ†ç±»ä½“ç³»å¯¹é½çš„åŒæ—¶æœ‰æ•ˆä¿è¯äº†æ˜ å°„è´¨é‡ï¼Œå¹¶ä¸ºæ¨¡ç³Šæ¡ˆä¾‹ä¿ç•™äº†å…³é”®çš„ä¸“å®¶ç›‘ç£ï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„çŸ¥è¯†è¡¨ç¤ºå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08422v2",
      "published_date": "2025-06-10 03:48:26 UTC",
      "updated_date": "2025-06-11 03:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:53.701775+00:00"
    },
    {
      "arxiv_id": "2506.08417v1",
      "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood",
      "title_zh": "åŸºäºå‡¸åŒ…åŠå…¶é‚»åŸŸå¹³æ»‘åˆ†å¸ƒå¤–æ³›åŒ–çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Qingmao Yao",
        "Zhichao Lei",
        "Tianyuan Chen",
        "Ziyue Yuan",
        "Xuefan Chen",
        "Jianxiang Liu",
        "Faguo Wu",
        "Xiao Zhang"
      ],
      "abstract": "Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which constrains the $Q$-function generalization. This over-constraint issue results in poor $Q$-value estimation and hinders policy improvement. In this paper, we introduce a novel approach to achieve better $Q$-value estimation by enhancing $Q$-function generalization in OOD regions within Convex Hull and its Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by smoothing them with neighboring in-sample $Q$-values. We theoretically show that SBO approximates true $Q$-values for both in-sample and OOD actions within the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG), empirically alleviates the over-constraint issue, achieving near-accurate $Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing state-of-the-art methods in both performance and computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)ä¸­ç”±äºåˆ†å¸ƒåç§»å¯¼è‡´çš„åˆ†å¸ƒå¤–(OOD)åŠ¨ä½œQå€¼é«˜ä¼°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•åœ¨å¤„ç†OODåŒºåŸŸæ—¶å¾€å¾€è¿‡äºä¿å®ˆï¼Œä»è€Œé™åˆ¶äº†Qå‡½æ•°çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€è¿‡åº¦çº¦æŸé—®é¢˜ï¼Œè®ºæ–‡æå‡ºåœ¨å‡¸åŒ…åŠå…¶é‚»åŸŸ(Convex Hull and its Neighborhood, CHN)å†…å¢å¼ºQå‡½æ•°çš„æ³›åŒ–ï¼Œå¹¶å¼•å…¥äº†å¹³æ»‘è´å°”æ›¼ç®—å­(Smooth Bellman Operator, SBO)ã€‚SBOé€šè¿‡åˆ©ç”¨é‚»è¿‘çš„æ ·æœ¬å†…Qå€¼å¯¹OOD Qå€¼è¿›è¡Œå¹³æ»‘æ›´æ–°ï¼Œåœ¨ä¿è¯å®‰å…¨æ³›åŒ–çš„å‰æä¸‹ä½¿Qå‡½æ•°æ›´åŠ å¹³æ»‘ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼ŒSBOèƒ½æœ‰æ•ˆé€¼è¿‘CHNå†…æ ·æœ¬å†…å’ŒOODåŠ¨ä½œçš„çœŸå®Qå€¼ã€‚åŸºäºæ­¤æå‡ºçš„SQOG(Smooth Q-function OOD Generalization)ç®—æ³•åœ¨å®éªŒä¸­æ˜¾è‘—ç¼“è§£äº†è¿‡åº¦çº¦æŸé—®é¢˜ï¼Œå®ç°äº†è¿‘ä¹å‡†ç¡®çš„Qå€¼ä¼°è®¡ã€‚åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSQOGåœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†ç­–ç•¥æ”¹è¿›çš„æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08417v1",
      "published_date": "2025-06-10 03:43:22 UTC",
      "updated_date": "2025-06-10 03:43:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:49:59.916563+00:00"
    },
    {
      "arxiv_id": "2506.08403v2",
      "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration",
      "title_zh": "TACTICï¼šåŸºäºè®¤çŸ¥ç†è®ºäº¤äº’åä½œçš„ç¿»è¯‘æ™ºèƒ½ä½“",
      "authors": [
        "Weiya Li",
        "Junjie Chen",
        "Bei Li",
        "Boyang Liu",
        "Zichen Wen",
        "Nuanqiao Shan",
        "Xiaoqian Liu",
        "Anping Liu",
        "Huajie Liu",
        "Hu Song",
        "Linfeng Zhang"
      ],
      "abstract": "Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šæ™ºèƒ½ä½“ç¿»è¯‘æ¡†æ¶å¿½è§†äººç±»è®¤çŸ¥ç­–ç•¥çš„é—®é¢˜ï¼Œæå‡ºäº† TACTIC æ¡†æ¶ï¼Œå³ä¸€ç§åŸºäºè®¤çŸ¥ç†è®ºäº¤äº’åä½œçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥æ¡†æ¶è®¾è®¡äº†å…­ä¸ªåŠŸèƒ½æˆªç„¶ä¸åŒçš„æ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬è‰æ‹Ÿ(drafting)ã€ä¼˜åŒ–(refinement)ã€è¯„ä¼°(evaluation)ã€è¯„åˆ†(scoring)ã€ä¸Šä¸‹æ–‡æ¨ç†(context reasoning)å’Œå¤–éƒ¨çŸ¥è¯†è·å–(external knowledge gathering)ï¼Œæ—¨åœ¨ç²¾ç¡®æ¨¡æ‹Ÿäººç±»ç¿»è¯‘ä¸­çš„è®¤çŸ¥å¤„ç†è¡Œä¸ºã€‚é€šè¿‡è¿™ç§ç†è®ºé©±åŠ¨çš„äº¤äº’å¼å·¥ä½œæµï¼ŒTACTIC èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç›´è¯‘ä¸æ„è¯‘ï¼Œå¹¶å®ç°åŸºäºä¸Šä¸‹æ–‡çš„è¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ FLORES-200 å’Œ WMT24 åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯­å¯¹ä¸Šå‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½è¡¨ç°ã€‚ä»¥ DeepSeek-V3 ä¸ºåŸºç¡€æ¨¡å‹æ—¶ï¼ŒTACTIC åœ¨ XCOMET å’Œ COMETKIWI-23 æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº GPT-4.1 å’Œ DeepSeek-R1ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨æå‡ç¿»è¯‘è´¨é‡æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 4 figures, Under review. Code: https://github.com/weiyali126/TACTIC",
      "pdf_url": "https://arxiv.org/pdf/2506.08403v2",
      "published_date": "2025-06-10 03:22:30 UTC",
      "updated_date": "2025-06-11 15:57:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:01.923308+00:00"
    },
    {
      "arxiv_id": "2506.08401v1",
      "title": "Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems",
      "title_zh": "åŸºäºå›¾çš„æ¨èç³»ç»Ÿä¸­çš„å•èŠ‚ç‚¹è§¦å‘åé—¨æ”»å‡»",
      "authors": [
        "Runze Li",
        "Di Jin",
        "Xiaobao Wang",
        "Dongxiao He",
        "Bingdao Feng",
        "Zhen Wang"
      ],
      "abstract": "Graph recommendation systems have been widely studied due to their ability to effectively capture the complex interactions between users and items. However, these systems also exhibit certain vulnerabilities when faced with attacks. The prevailing shilling attack methods typically manipulate recommendation results by injecting a large number of fake nodes and edges. However, such attack strategies face two primary challenges: low stealth and high destructiveness. To address these challenges, this paper proposes a novel graph backdoor attack method that aims to enhance the exposure of target items to the target user in a covert manner, without affecting other unrelated nodes. Specifically, we design a single-node trigger generator, which can effectively expose multiple target items to the target user by inserting only one fake user node. Additionally, we introduce constraint conditions between the target nodes and irrelevant nodes to mitigate the impact of fake nodes on the recommendation system's performance. Experimental results show that the exposure of the target items reaches no less than 50% in 99% of the target users, while the impact on the recommendation system's performance is controlled within approximately 5%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾æ¨èç³»ç»Ÿ(Graph recommendation systems)åœ¨é¢å¯¹æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ‰˜æ”»å‡»(shilling attack)ç”±äºæ³¨å…¥å¤§é‡è™šå‡èŠ‚ç‚¹å’Œè¾¹ï¼Œå­˜åœ¨éšè”½æ€§ä½ä¸”ç ´åæ€§é«˜çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾åé—¨æ”»å‡»(graph backdoor attack)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡éšè”½æ‰‹æ®µå¢å¼ºç›®æ ‡é¡¹ç›®å¯¹ç‰¹å®šç”¨æˆ·çš„æ›å…‰åº¦ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ä¸ªå•èŠ‚ç‚¹è§¦å‘å™¨ç”Ÿæˆå™¨(single-node trigger generator)ï¼Œä»…é€šè¿‡æ’å…¥ä¸€ä¸ªè™šå‡ç”¨æˆ·èŠ‚ç‚¹å³å¯å®ç°å¤šä¸ªç›®æ ‡é¡¹ç›®å‘ç›®æ ‡ç”¨æˆ·çš„æœ‰æ•ˆæ›å…‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç›®æ ‡èŠ‚ç‚¹ä¸æ— å…³èŠ‚ç‚¹ä¹‹é—´çš„çº¦æŸæ¡ä»¶ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘è™šå‡èŠ‚ç‚¹å¯¹æ¨èç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ”»å‡»èƒ½ä½¿99%çš„ç›®æ ‡ç”¨æˆ·å¯¹ç›®æ ‡é¡¹ç›®çš„æ›å…‰ç‡è¾¾åˆ°50%ä»¥ä¸Šã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯¥æ–¹æ³•å°†å¯¹æ¨èç³»ç»Ÿæ€§èƒ½çš„è´Ÿé¢å½±å“æˆåŠŸæ§åˆ¶åœ¨çº¦5%ä»¥å†…ï¼ŒéªŒè¯äº†æ”»å‡»çš„éšè”½æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08401v1",
      "published_date": "2025-06-10 03:19:15 UTC",
      "updated_date": "2025-06-10 03:19:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:00.646216+00:00"
    },
    {
      "arxiv_id": "2506.08399v2",
      "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning",
      "title_zh": "SafeCoTï¼šé€šè¿‡æç®€æ¨ç†æå‡ VLM å®‰å…¨æ€§",
      "authors": [
        "Jiachen Ma",
        "Zhanhui Zhou",
        "Chao Yang",
        "Chaochao Lu"
      ],
      "abstract": "Ensuring safe and appropriate responses from vision-language models (VLMs) remains a critical challenge, particularly in high-risk or ambiguous scenarios. We introduce SafeCoT, a lightweight, interpretable framework that leverages rule-based chain-of-thought (CoT) supervision to improve refusal behavior in VLMs. Unlike prior methods that rely on large-scale safety annotations or complex modeling, SafeCoT uses minimal supervision to help models reason about safety risks and make context-aware refusals. Experiments across multiple benchmarks show that SafeCoT significantly reduces overrefusal and enhances generalization, even with limited training data. Our approach offers a scalable solution for aligning VLMs with safety-critical objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SafeCoTï¼Œä¸€ç§è½»é‡çº§ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨åŸºäºè§„åˆ™çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰ç›‘ç£æ¥ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰çš„æ‹’ç»è¡Œä¸ºã€‚ä¸ä¾èµ–å¤§è§„æ¨¡å®‰å…¨æ ‡æ³¨æˆ–å¤æ‚å»ºæ¨¡çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSafeCoT é€šè¿‡æå°è§„æ¨¡çš„ç›‘ç£å¼•å¯¼æ¨¡å‹å¯¹æ½œåœ¨çš„å®‰å…¨é£é™©è¿›è¡Œæ¨ç†ï¼Œä»è€Œå®ç°å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç²¾å‡†æ‹’ç»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeCoT åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†è¿‡åº¦æ‹’ç»ï¼ˆoverrefusalï¼‰çš„å‘ç”Ÿç‡ï¼Œå¹¶åœ¨å—é™çš„è®­ç»ƒæ•°æ®ä¸‹è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹å®‰å…¨æ€§ï¼Œè¿˜ä¸ºå®ç°è§†è§‰è¯­è¨€æ¨¡å‹ä¸å®‰å…¨å…³é”®ç›®æ ‡çš„é«˜æ•ˆå¯¹é½æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08399v2",
      "published_date": "2025-06-10 03:13:50 UTC",
      "updated_date": "2025-06-11 06:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:17.240132+00:00"
    },
    {
      "arxiv_id": "2506.08397v1",
      "title": "Spatiotemporal deep learning models for detection of rapid intensification in cyclones",
      "title_zh": "ç”¨äºæ°”æ—‹å¿«é€Ÿå¢å¼ºæ£€æµ‹çš„æ—¶ç©ºæ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Vamshika Sutar",
        "Amandeep Singh",
        "Rohitash Chandra"
      ],
      "abstract": "Cyclone rapid intensification is the rapid increase in cyclone wind intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid intensification is considered an extreme event during a cyclone, and its occurrence is relatively rare, contributing to a class imbalance in the dataset. A diverse array of factors influences the likelihood of a cyclone undergoing rapid intensification, further complicating the task for conventional machine learning models. In this paper, we evaluate deep learning, ensemble learning and data augmentation frameworks to detect cyclone rapid intensification based on wind intensity and spatial coordinates. We note that conventional data augmentation methods cannot be utilised for generating spatiotemporal patterns replicating cyclones that undergo rapid intensification. Therefore, our framework employs deep learning models to generate spatial coordinates and wind intensity that replicate cyclones to address the class imbalance problem of rapid intensification. We also use a deep learning model for the classification module within the data augmentation framework to differentiate between rapid and non-rapid intensification events during a cyclone. Our results show that data augmentation improves the results for rapid intensification detection in cyclones, and spatial coordinates play a critical role as input features to the given models. This paves the way for research in synthetic data generation for spatiotemporal data with extreme events.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ—¶ç©ºæ·±åº¦å­¦ä¹ æ¨¡å‹æ£€æµ‹æ°”æ—‹çš„å¿«é€Ÿå¢å¼º(rapid intensification)ç°è±¡ï¼Œæ—¨åœ¨è§£å†³è¯¥æç«¯äº‹ä»¶åœ¨æ•°æ®é›†ä¸­å­˜åœ¨çš„ç±»åˆ«ä¸å¹³è¡¡(class imbalance)é—®é¢˜ã€‚ç ”ç©¶è¯„ä¼°äº†æ·±åº¦å­¦ä¹ (deep learning)ã€é›†æˆå­¦ä¹ (ensemble learning)å’Œæ•°æ®å¢å¼º(data augmentation)æ¡†æ¶ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿå¢å¼ºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤åˆ¶æ°”æ—‹çš„æ—¶ç©ºæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç”Ÿæˆæ¨¡æ‹Ÿæ°”æ—‹çš„ç©ºé—´åæ ‡(spatial coordinates)å’Œé£å¼ºåº¦(wind intensity)ä»¥æ‰©å……æ•°æ®é›†ï¼Œå¹¶ç»“åˆæ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å—è¿›è¡Œäº‹ä»¶è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°æ®å¢å¼ºæ˜¾è‘—æå‡äº†æ°”æ—‹å¿«é€Ÿå¢å¼ºçš„æ£€æµ‹æ€§èƒ½ï¼Œä¸”ç©ºé—´åæ ‡è¢«è¯æ˜æ˜¯æ¨¡å‹è¾“å…¥ä¸­è‡³å…³é‡è¦çš„ç‰¹å¾ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºå¤„ç†æç«¯äº‹ä»¶çš„æ—¶ç©ºæ•°æ®åˆæˆ(synthetic data generation)æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08397v1",
      "published_date": "2025-06-10 03:13:02 UTC",
      "updated_date": "2025-06-10 03:13:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:21.565158+00:00"
    },
    {
      "arxiv_id": "2506.08390v1",
      "title": "On Reasoning Strength Planning in Large Reasoning Models",
      "title_zh": "è®ºå¤§æ¨ç†æ¨¡å‹ä¸­çš„æ¨ç†å¼ºåº¦è§„åˆ’",
      "authors": [
        "Leheng Sheng",
        "An Zhang",
        "Zijian Wu",
        "Weixiang Zhao",
        "Changshuo Shen",
        "Yi Zhang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "Recent studies empirically reveal that large reasoning models (LRMs) can automatically allocate more reasoning strengths (i.e., the number of reasoning tokens) for harder problems, exhibiting difficulty-awareness for better task performance. While this automatic reasoning strength allocation phenomenon has been widely observed, its underlying mechanism remains largely unexplored. To this end, we provide explanations for this phenomenon from the perspective of model activations. We find evidence that LRMs pre-plan the reasoning strengths in their activations even before generation, with this reasoning strength causally controlled by the magnitude of a pre-allocated directional vector. Specifically, we show that the number of reasoning tokens is predictable solely based on the question activations using linear probes, indicating that LRMs estimate the required reasoning strength in advance. We then uncover that LRMs encode this reasoning strength through a pre-allocated directional vector embedded in the activations of the model, where the vector's magnitude modulates the reasoning strength. Subtracting this vector can lead to reduced reasoning token number and performance, while adding this vector can lead to increased reasoning token number and even improved performance. We further reveal that this direction vector consistently yields positive reasoning length prediction, and it modifies the logits of end-of-reasoning token </think> to affect the reasoning length. Finally, we demonstrate two potential applications of our findings: overthinking behavior detection and enabling efficient reasoning on simple problems. Our work provides new insights into the internal mechanisms of reasoning in LRMs and offers practical tools for controlling their reasoning behaviors. Our code is available at https://github.com/AlphaLab-USTC/LRM-plans-CoT.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) èƒ½å¤Ÿæ ¹æ®é—®é¢˜éš¾åº¦è‡ªåŠ¨åˆ†é…æ¨ç†å¼ºåº¦ï¼ˆå³æ¨ç† Token æ•°é‡ï¼‰çš„å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶äººå‘˜å‘ç° LRMs åœ¨ç”Ÿæˆä¹‹å‰å°±å·²åœ¨æ¿€æ´»å€¼ä¸­é¢„å…ˆè§„åˆ’äº†æ¨ç†å¼ºåº¦ï¼Œä¸”ä»…å‡­é—®é¢˜çš„æ¿€æ´»å€¼é€šè¿‡çº¿æ€§æ¢æµ‹ (Linear Probes) å³å¯é¢„æµ‹æ‰€éœ€çš„æ¨ç† Token æ•°é‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œè¿™ç§æ¨ç†å¼ºåº¦å—åˆ°æ¿€æ´»å€¼ä¸­ä¸€ä¸ªé¢„åˆ†é…æ–¹å‘å‘é‡ (Directional Vector) å¹…åº¦çš„å› æœæ§åˆ¶ï¼Œè¯¥å‘é‡é€šè¿‡è°ƒæ•´æ¨ç†ç»“æŸæ ‡è®° `</think>` çš„ Logits æ¥å½±å“æ¨ç†é•¿åº¦ã€‚é€šè¿‡å¹²é¢„è¯¥å‘é‡çš„å¹…åº¦ï¼Œç ”ç©¶è€…æˆåŠŸå®ç°äº†å¯¹æ¨ç†é•¿åº¦å’Œæ¨¡å‹æ€§èƒ½çš„äººä¸ºè°ƒæ§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è¿™äº›å‘ç°åœ¨æ£€æµ‹è¿‡åº¦æ€è€ƒ (Overthinking) è¡Œä¸ºä»¥åŠä¼˜åŒ–ç®€å•é—®é¢˜æ¨ç†æ•ˆç‡æ–¹é¢çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºç†è§£å’Œæ§åˆ¶ LRMs çš„å†…éƒ¨æ¨ç†æœºåˆ¶æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08390v1",
      "published_date": "2025-06-10 02:55:13 UTC",
      "updated_date": "2025-06-10 02:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:20.537414+00:00"
    },
    {
      "arxiv_id": "2506.08388v3",
      "title": "Reinforcement Learning Teachers of Test Time Scaling",
      "title_zh": "é¢å‘æµ‹è¯•æ—¶ç¼©æ”¾çš„å¼ºåŒ–å­¦ä¹ æ•™å­¦æ¨¡å‹",
      "authors": [
        "Edoardo Cetin",
        "Tianyu Zhao",
        "Yujin Tang"
      ],
      "abstract": "Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply \"connect-the-dots\" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework. Code available at: https://github.com/SakanaAI/RLT",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºReinforcement-Learned Teachers (RLTs)çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨è®­ç»ƒæ¨ç†è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„æ¢ç´¢æŒ‘æˆ˜ã€‚ä¸åŒäºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¾§é‡äºç‹¬ç«‹è§£å†³é—®é¢˜ï¼ŒRLTsé€šè¿‡åŒæ—¶è·å–é—®é¢˜å’Œè§£é¢˜ç­”æ¡ˆï¼Œä¸“æ³¨äºç”Ÿæˆæ—¨åœ¨ä¼˜åŒ–ä¸‹æ¸¸è’¸é¦(distillation)æ•ˆæœçš„è¯¦ç»†è§£é‡Šã€‚è¯¥æ¡†æ¶åˆ©ç”¨å­¦ç”Ÿæ¨¡å‹å¯¹è§£é‡Šçš„ç†è§£ç¨‹åº¦ä½œä¸ºç¨ å¯†å¥–åŠ±(dense rewards)æ¥è®­ç»ƒè€å¸ˆæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°â€œä¸²è”â€çŸ¥è¯†ç‚¹ã€‚å®éªŒè¯æ˜ï¼Œä»…æœ‰7Bè§„æ¨¡çš„RLTåœ¨ç«èµ›åŠç ”ç©¶ç”Ÿçº§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¾¿è¶…è¿‡äº†åˆ©ç”¨è§„æ¨¡å¤§æ•°ä¸ªæ•°é‡çº§çš„æ¨¡å‹è¿›è¡Œè’¸é¦çš„ç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒRLTsåœ¨è®­ç»ƒæ›´å¤§å‹å­¦ç”Ÿæ¨¡å‹å’Œå¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)ä»»åŠ¡æ—¶è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æ¨ç†æ¡†æ¶æä¾›äº†æ–°çš„ç ”ç©¶èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08388v3",
      "published_date": "2025-06-10 02:53:24 UTC",
      "updated_date": "2025-10-29 14:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:21.693973+00:00"
    },
    {
      "arxiv_id": "2506.17264v1",
      "title": "OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning",
      "title_zh": "OAT-Rephraseï¼šé¢å‘é›¶é˜¶å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„ä¼˜åŒ–æ„ŸçŸ¥è®­ç»ƒæ•°æ®æ”¹å†™",
      "authors": [
        "Jikai Long",
        "Zijian Hu",
        "Xiaodong Yu",
        "Jianwen Xie",
        "Zhaozhuo Xu"
      ],
      "abstract": "Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO) offers a memory-efficient alternative to gradient-based methods but suffers from slower convergence and unstable optimization due to noisy gradient estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training data rephrasing strategy that leverages an LLM to rephrase training instances based on its understanding of the ZO dynamics, specifically MeZO, derived directly from its paper. The approach incorporates a dual-stage pipeline featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain task relevance and logical consistency. Evaluations across five classification tasks and three LLM architectures demonstrate that OAT-Rephrase consistently improves MeZO fine-tuning performance, often narrowing or eliminating the gap with first-order methods. Our findings suggest that optimization-aware rephrasing serves as a reusable and low-overhead enhancement for zeroth-order tuning regimes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†OAT-Rephraseï¼Œä¸€ç§ä¸“ä¸ºé›¶é˜¶ä¼˜åŒ–(zeroth-order optimization, ZO)å¤§è¯­è¨€æ¨¡å‹(LLM)å¾®è°ƒè®¾è®¡çš„ä¼˜åŒ–æ„ŸçŸ¥è®­ç»ƒæ•°æ®é‡å†™ç­–ç•¥ã€‚é’ˆå¯¹ZOå¾®è°ƒåœ¨æ¢¯åº¦ä¼°è®¡ä¸­å­˜åœ¨çš„å™ªå£°ã€æ”¶æ•›ç¼“æ…¢åŠä¼˜åŒ–ä¸ç¨³å®šç­‰æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨LLMåŸºäºå¯¹ZOåŠ¨åŠ›å­¦ï¼ˆç‰¹åˆ«æ˜¯MeZOï¼‰çš„ç†è§£æ¥é‡æ–°è¡¨è¿°è®­ç»ƒå®ä¾‹ã€‚ç³»ç»Ÿé‡‡ç”¨äº†åŒ…å«é‡å†™LLM(rewriter LLM)å’Œè¯­ä¹‰è¯„åˆ¤å™¨(semantic judge)çš„åŒé˜¶æ®µæµæ°´çº¿ï¼Œåœ¨æå‡ä¼˜åŒ–æ•ˆç‡çš„åŒæ—¶ç¡®ä¿äº†ä»»åŠ¡ç›¸å…³æ€§ä¸é€»è¾‘ä¸€è‡´æ€§ã€‚å®éªŒåœ¨äº”ä¸ªåˆ†ç±»ä»»åŠ¡å’Œä¸‰ç§LLMæ¶æ„ä¸Šè¯æ˜äº†OAT-Rephraseèƒ½æŒç»­æ”¹å–„MeZOçš„å¾®è°ƒè¡¨ç°ï¼Œå¹¶æœ‰æ•ˆç¼©å°ç”šè‡³æ¶ˆé™¤äº†ä¸ä¸€é˜¶æ–¹æ³•(first-order methods)ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–æ„ŸçŸ¥çš„æ•°æ®é‡å†™å¯ä»¥ä½œä¸ºä¸€ç§ä½å¼€é”€ä¸”å¯å¤ç”¨çš„æ‰‹æ®µï¼Œæ˜¾è‘—å¢å¼ºé›¶é˜¶å¾®è°ƒæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17264v1",
      "published_date": "2025-06-10 02:53:04 UTC",
      "updated_date": "2025-06-10 02:53:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:39.502562+00:00"
    },
    {
      "arxiv_id": "2506.11121v1",
      "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR",
      "title_zh": "SUTA-LMï¼šè¿æ¥æµ‹è¯•æ—¶è‡ªé€‚åº”ä¸è¯­è¨€æ¨¡å‹é‡æ‰“åˆ†ï¼Œå®ç°é²æ£’çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«",
      "authors": [
        "Wei-Ping Huang",
        "Guan-Ting Lin",
        "Hung-yi Lee"
      ],
      "abstract": "Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ç°å®åº”ç”¨ä¸­é¢ä¸´çš„é¢†åŸŸä¸åŒ¹é…ï¼ˆdomain mismatchï¼‰é—®é¢˜ï¼ŒæŒ‡å‡ºæµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTest-Time Adaptation, TTAï¼‰ä¸è¯­è¨€æ¨¡å‹é‡æ‰“åˆ†ï¼ˆLM rescoringï¼‰ä¹‹é—´å­˜åœ¨ç›¸äº’å¹²æ‰°çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SUTA-LMï¼Œè¿™æ˜¯ä¸€ç§å°†åŸºäºç†µæœ€å°åŒ–çš„ TTA æ–¹æ³• SUTA ä¸è¯­è¨€æ¨¡å‹é‡æ‰“åˆ†ç›¸ç»“åˆçš„ç®€å•ä¸”æœ‰æ•ˆçš„æ‰©å±•æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§ç»“åˆå£°å­¦å’Œè¯­è¨€ä¿¡æ¯çš„è‡ªåŠ¨æ­¥é•¿é€‰æ‹©æœºåˆ¶ï¼ˆauto-step selection mechanismï¼‰æ¥å¼•å¯¼å—æ§çš„è‡ªé€‚åº”è¿‡ç¨‹ï¼Œéšååˆ©ç”¨è¯­è¨€æ¨¡å‹é‡æ‰“åˆ†æ¥ç²¾ç‚¼è¯†åˆ«è¾“å‡ºã€‚åœ¨ 18 ä¸ªå¤šæ ·åŒ–çš„ ASR æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSUTA-LM åœ¨å¹¿æ³›çš„é¢†åŸŸä¸­å‡èƒ½å–å¾—é²æ£’çš„ç»“æœã€‚è¯¥å·¥ä½œæ­ç¤ºäº† TTA ä¸è¯­è¨€æ¨¡å‹é›†æˆæ—¶çš„å¤æ‚æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„ ASR ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11121v1",
      "published_date": "2025-06-10 02:50:20 UTC",
      "updated_date": "2025-06-10 02:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:10.039814+00:00"
    },
    {
      "arxiv_id": "2506.08379v1",
      "title": "Reinforce LLM Reasoning through Multi-Agent Reflection",
      "title_zh": "é€šè¿‡å¤šæ™ºèƒ½ä½“åæ€å¼ºåŒ– LLM æ¨ç†",
      "authors": [
        "Yurun Yuan",
        "Tengyang Xie"
      ],
      "abstract": "Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­åé¦ˆç©ºé—´å—é™å’Œç¼ºä¹åè°ƒè®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¤šæ™ºèƒ½ä½“åæ€å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚ä½œè€…å°†å¤šè½®ä¼˜åŒ–è¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Processï¼‰ï¼Œå¹¶å¼•å…¥äº†åä¸ºDPSDPï¼ˆDirect Policy Search by Dynamic Programmingï¼‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡åœ¨è‡ªæˆ‘ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œç›´æ¥åå¥½å­¦ä¹ ï¼ˆdirect preference learningï¼‰ï¼Œè®­ç»ƒä¸€ä¸ªactor-criticç³»ç»Ÿæ¥è¿­ä»£å®Œå–„ç­”æ¡ˆã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒDPSDPèƒ½å¤ŸåŒ¹é…è®­ç»ƒåˆ†å¸ƒå†…ä»»ä½•ç­–ç•¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨MATH 500åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å°†åˆå§‹å‡†ç¡®ç‡ä»58.2%æå‡è‡³63.2%ï¼Œå……åˆ†éªŒè¯äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨æå‡æ¨ç†æ€§èƒ½åŠåˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰æ³›åŒ–æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "International Conference on Machine Learning (ICML), 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08379v1",
      "published_date": "2025-06-10 02:43:47 UTC",
      "updated_date": "2025-06-10 02:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:50:58.007552+00:00"
    },
    {
      "arxiv_id": "2506.08373v2",
      "title": "Draft-based Approximate Inference for LLMs",
      "title_zh": "åŸºäºè‰ç¨¿æ¨¡å‹çš„å¤§è¯­è¨€æ¨¡å‹è¿‘ä¼¼æ¨ç†",
      "authors": [
        "Kevin Galim",
        "Ethan Ewer",
        "Wonjun Kang",
        "Minjae Lee",
        "Hyung Il Koo",
        "Kangwook Lee"
      ],
      "abstract": "Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, the first method that leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ Large Language Models (LLMs) æ¨ç†ä¸­é¢ä¸´çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å°å‹ draft models æ¥ç²¾ç¡®é¢„æµ‹ tokens å’Œ key-value (KV) å¯¹é‡è¦æ€§çš„æ–°å‹è¿‘ä¼¼æ¨ç†æ¡†æ¶ã€‚ä¸ºäº†éªŒè¯è¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸¤ç§å…·ä½“å®ç°ï¼šSpecKV é€šè¿‡è‰ç¨¿è¾“å‡ºè¯„ä¼° KV pair çš„é‡è¦æ€§ä»¥ä¼˜åŒ– KV cache droppingï¼Œè€Œ SpecPC åˆ™åˆ©ç”¨è‰ç¨¿æ¨¡å‹çš„æ³¨æ„åŠ›æ¿€æ´»æ¥è¯†åˆ«å¹¶å‰”é™¤ä¸é‡è¦çš„ prompt tokensã€‚ç†è®ºä¸å®è¯åˆ†æè¯æ˜ï¼Œdraft models ä¸ç›®æ ‡æ¨¡å‹åœ¨æ³¨æ„åŠ›æ¨¡å¼ä¸Šå…·æœ‰æ˜¾è‘—çš„ç›¸å…³æ€§ï¼Œä¸ºé¢„æµ‹çš„å‡†ç¡®æ€§æä¾›äº†ä¿éšœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ä¸ä»…ä¿æŒäº†åœ¨å†…å­˜å ç”¨ã€å»¶è¿Ÿå’Œååé‡æ–¹é¢çš„ä¼˜åŒ–ï¼Œä¸”å…¶å‡†ç¡®ç‡è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„ baseline æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Added discussion and comparison with SpecPrefill",
      "pdf_url": "https://arxiv.org/pdf/2506.08373v2",
      "published_date": "2025-06-10 02:37:46 UTC",
      "updated_date": "2025-07-19 03:40:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:52:00.076307+00:00"
    },
    {
      "arxiv_id": "2506.11120v1",
      "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models",
      "title_zh": "SDMPruneï¼šé¢å‘é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹çš„è‡ªè’¸é¦ MLP å‰ªæ",
      "authors": [
        "Hourun Zhu",
        "Chengchao Shen"
      ],
      "abstract": "In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at https://github.com/visresearch/SDMPrune.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éƒ¨ç½²æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„å‰ªææ–¹æ³•å› ä½¿ç”¨ one-hot æ ‡ç­¾è€Œå¿½ç•¥äº†åŸå§‹æ¨¡å‹çš„ç”Ÿæˆæ½œèƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SDMPruneï¼Œä¸€ç§åœ¨å‰ªæé˜¶æ®µå¼•å…¥è‡ªè’¸é¦æŸå¤±ï¼ˆSelf-Distillation Lossï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å……åˆ†åˆ©ç”¨åŸå§‹æ¨¡å‹çš„é¢„æµ‹æ¥è·å–æ›´ç²¾ç¡®çš„æ¢¯åº¦ä¿¡æ¯ã€‚ç”±äº LLMs å¯¹å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¨¡å—çš„é¢„æµ‹æ•æ„Ÿåº¦è¾ƒä½ï¼Œä¸”è¯¥æ¨¡å—å æ®äº†æ¨¡å‹ç»å¤§éƒ¨åˆ†å‚æ•°é‡ï¼ŒSDMPrune ä¸“é—¨ä¸“æ³¨äº MLP çš„å‰ªæä»¥å®ç°é«˜æ•ˆå‹ç¼©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å‰ªææŠ€æœ¯ã€‚æ­¤å¤–ï¼ŒSDMPrune åœ¨ 1B è§„æ¨¡çš„å¼€æº LLMs ä¸­å±•ç°äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºå®ç°è½»é‡åŒ–ã€é«˜æ€§èƒ½çš„è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11120v1",
      "published_date": "2025-06-10 02:24:32 UTC",
      "updated_date": "2025-06-10 02:24:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:52:16.676885+00:00"
    },
    {
      "arxiv_id": "2506.08363v2",
      "title": "FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs",
      "title_zh": "FloorplanMAEï¼šä»éƒ¨åˆ†è¾“å…¥ç”Ÿæˆå®Œæ•´å¹³é¢å›¾çš„è‡ªç›‘ç£æ¡†æ¶",
      "authors": [
        "Jun Yin",
        "Jing Zhong",
        "Pengyu Zeng",
        "Peilin Li",
        "Miao Zhang",
        "Ran Luo",
        "Shuai Lu"
      ],
      "abstract": "In the architectural design process, floorplan design is often a dynamic and iterative process. Architects progressively draw various parts of the floorplan according to their ideas and requirements, continuously adjusting and refining throughout the design process. Therefore, the ability to predict a complete floorplan from a partial one holds significant value in the design process. Such prediction can help architects quickly generate preliminary designs, improve design efficiency, and reduce the workload associated with repeated modifications. To address this need, we propose FloorplanMAE, a self-supervised learning framework for restoring incomplete floor plans into complete ones. First, we developed a floor plan reconstruction dataset, FloorplanNet, specifically trained on architectural floor plans. Secondly, we propose a floor plan reconstruction method based on Masked Autoencoders (MAE), which reconstructs missing parts by masking sections of the floor plan and training a lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally, we validated the model using real sketches from the early stages of architectural design. Experimental results show that the FloorplanMAE model can generate high-quality complete floor plans from incomplete partial plans. This framework provides a scalable solution for floor plan generation, with broad application prospects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FloorplanMAEï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»ä¸å®Œæ•´çš„è¾“å…¥ä¸­ç”Ÿæˆå®Œæ•´å¹³é¢å›¾çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œä»¥æ”¯æŒå»ºç­‘è®¾è®¡ä¸­åŠ¨æ€ä¸”è¿­ä»£çš„å¹³é¢å›¾è®¾è®¡è¿‡ç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤å¼€å‘äº†ä¸“é—¨é’ˆå¯¹å»ºç­‘å¹³é¢å›¾é‡å»ºçš„FloorplanNetæ•°æ®é›†ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼ŒFloorplanMAEé‡‡ç”¨äº†åŸºäºMasked Autoencoders (MAE) çš„å¹³é¢å›¾é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å¯¹å¹³é¢å›¾éƒ¨åˆ†åŒºåŸŸè¿›è¡Œæ©ç å¹¶è®­ç»ƒè½»é‡çº§çš„Vision Transformer (ViT) æ¥é¢„æµ‹ç¼ºå¤±å†…å®¹ã€‚å®éªŒç»“æœåŠåœ¨å»ºç­‘è®¾è®¡æ—©æœŸé˜¶æ®µçœŸå®è‰å›¾ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»ä¸å®Œæ•´è¾“å…¥ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å®Œæ•´å¹³é¢å›¾ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºå‡†ã€‚è¯¥æ¡†æ¶ä¸ä»…æé«˜äº†è®¾è®¡æ•ˆç‡å¹¶å‡å°‘äº†é‡å¤ä¿®æ”¹çš„å·¥ä½œé‡ï¼Œè¿˜ä¸ºå¹³é¢å›¾ç”Ÿæˆæä¾›äº†ä¸€ç§å…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08363v2",
      "published_date": "2025-06-10 02:22:05 UTC",
      "updated_date": "2025-08-02 12:17:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:10.637595+00:00"
    },
    {
      "arxiv_id": "2506.08357v1",
      "title": "MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion",
      "title_zh": "MD-ViSCoï¼šç”Ÿå‘½ä½“å¾æ³¢å½¢å¤šå‘è½¬æ¢çš„ç»Ÿä¸€æ¨¡å‹",
      "authors": [
        "Franck Meyer",
        "Kyunghoon Hur",
        "Edward Choi"
      ],
      "abstract": "Despite the remarkable progress of deep-learning methods generating a target vital sign waveform from a source vital sign waveform, most existing models are designed exclusively for a specific source-to-target pair. This requires distinct model architectures, optimization procedures, and pre-processing pipelines, resulting in multiple models that hinder usability in clinical settings. To address this limitation, we propose the Multi-Directional Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or arterial blood pressure (ABP) from any single input waveform with a single model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct multi-directional waveform generation on two publicly available datasets. Our framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average across all waveform types, lowering Mean absolute error (MAE) by 8.8% and improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the generated ABP waveforms satisfy the Association for the Advancement of Medical Instrumentation (AAMI) criterion and achieve Grade B on the British Hypertension Society (BHS) standard, outperforming all baselines. By eliminating the need for developing a distinct model for each task, we believe that this work offers a unified framework that can deal with any kind of vital sign waveforms with a single model in healthcare monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MD-ViSCoï¼ˆMulti-Directional Vital-Sign Converterï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä»…èƒ½é’ˆå¯¹ç‰¹å®šæº-ç›®æ ‡å¯¹ç”Ÿæˆç”Ÿå‘½ä½“å¾æ³¢å½¢çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å•ä¸€æ¶æ„ä»ä»»ä½•å•ä¸€è¾“å…¥æ³¢å½¢ï¼ˆå¦‚ECGã€PPGæˆ–ABPï¼‰ç”Ÿæˆå¯¹åº”çš„ç›®æ ‡æ³¢å½¢ï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¯ç”¨æ€§ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼ŒMD-ViSCoé‡‡ç”¨äº†é›†æˆSwin Transformerçš„æµ…å±‚1-Dimensional U-Netï¼Œå¹¶ç»“åˆè‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼ˆAdaINï¼‰æŠ€æœ¯æ¥æ•æ‰ä¸åŒæ³¢å½¢çš„ç‹¬ç‰¹æ ·å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºNabNetå’ŒPPG2ABPç­‰åŸºå‡†æ¨¡å‹ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†8.8%ï¼Œçš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPCï¼‰æå‡äº†4.9%ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„ABPæ³¢å½¢ç¬¦åˆAAMIæ ‡å‡†å¹¶è¾¾åˆ°BHSæ ‡å‡†çš„Bçº§æ°´å¹³ï¼ŒéªŒè¯äº†å…¶é«˜ç²¾åº¦çš„ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥ç ”ç©¶é€šè¿‡æ¶ˆé™¤ä¸ºä¸åŒä»»åŠ¡å¼€å‘ç‹¬ç«‹æ¨¡å‹çš„éœ€æ±‚ï¼Œä¸ºåŒ»ç–—ç›‘æ§æä¾›äº†ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å„ç±»ç”Ÿå‘½ä½“å¾æ³¢å½¢çš„é€šç”¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (16 pages, 5 figures). Paper submitted for review. Code available at https://github.com/fr-meyer/MD-ViSCo",
      "pdf_url": "https://arxiv.org/pdf/2506.08357v1",
      "published_date": "2025-06-10 02:14:52 UTC",
      "updated_date": "2025-06-10 02:14:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:14.912159+00:00"
    },
    {
      "arxiv_id": "2506.08354v1",
      "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning",
      "title_zh": "æ–‡æœ¬åµŒå…¥åº”æ•æ‰éšå¼è¯­ä¹‰ï¼Œè€Œéä»…é™äºè¡¨å±‚å«ä¹‰",
      "authors": [
        "Yiqun Sun",
        "Qiang Huang",
        "Anthony K. H. Tung",
        "Jun Yu"
      ],
      "abstract": "This position paper argues that the text embedding research community should move beyond surface meaning and embrace implicit semantics as a central modeling goal. Text embedding models have become foundational in modern NLP, powering a wide range of applications and drawing increasing research attention. Yet, much of this progress remains narrowly focused on surface-level semantics. In contrast, linguistic theory emphasizes that meaning is often implicit, shaped by pragmatics, speaker intent, and sociocultural context. Current embedding models are typically trained on data that lacks such depth and evaluated on benchmarks that reward the capture of surface meaning. As a result, they struggle with tasks requiring interpretive reasoning, speaker stance, or social meaning. Our pilot study highlights this gap, showing that even state-of-the-art models perform only marginally better than simplistic baselines on implicit semantics tasks. To address this, we call for a paradigm shift: embedding research should prioritize more diverse and linguistically grounded training data, design benchmarks that evaluate deeper semantic understanding, and explicitly frame implicit meaning as a core modeling objective, better aligning embeddings with real-world language complexity.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡æŒ‡å‡ºï¼Œæ–‡æœ¬åµŒå…¥ (Text Embeddings) ç ”ç©¶åº”è¶…è¶Šè¡¨å±‚è¯­ä¹‰ï¼Œå°†éšæ€§è¯­ä¹‰ (Implicit Semantics) ä½œä¸ºæ ¸å¿ƒå»ºæ¨¡ç›®æ ‡ã€‚ç›®å‰çš„æ¨¡å‹å¤§å¤šå±€é™äºè¡¨å±‚å«ä¹‰ï¼Œå¿½ç•¥äº†ç”±è¯­ç”¨å­¦ (Pragmatics)ã€è¯´è¯äººæ„å›¾å’Œç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯å†³å®šçš„æ·±å±‚è¯­ä¹‰ã€‚ç”±äºç°æœ‰æ¨¡å‹åœ¨ç¼ºä¹æ·±åº¦çš„æ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶å—é™äºå¥–åŠ±è¡¨å±‚æ•è·çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒä»¬åœ¨å¤„ç†è§£é‡Šæ€§æ¨ç†ã€è¯´è¯äººç«‹åœºæˆ–ç¤¾ä¼šæ„ä¹‰ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚ä½œè€…çš„åˆæ­¥ç ”ç©¶ (Pilot study) è¡¨æ˜ï¼Œå³ä¾¿æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨éšæ€§è¯­ä¹‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä»…ç•¥ä¼˜äºç®€å•åŸºçº¿ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å‘¼åè¿›è¡ŒèŒƒå¼è½¬å˜ï¼Œä¼˜å…ˆé‡‡ç”¨æ›´å¤šæ ·åŒ–ä¸”å…·å¤‡è¯­è¨€å­¦åŸºç¡€çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶è®¾è®¡èƒ½å¤Ÿè¯„ä¼°æ·±å±‚è¯­ä¹‰ç†è§£çš„æ–°åŸºå‡†ã€‚é€šè¿‡å°†éšæ€§æ„ä¹‰æ˜ç¡®ä½œä¸ºæ ¸å¿ƒå»ºæ¨¡ç›®æ ‡ï¼Œæ–‡æœ¬åµŒå…¥æŠ€æœ¯å°†èƒ½æ›´å¥½åœ°å¯¹é½ç°å®ä¸–ç•Œè¯­è¨€çš„å¤æ‚æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08354v1",
      "published_date": "2025-06-10 02:11:42 UTC",
      "updated_date": "2025-06-10 02:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:29.677729+00:00"
    },
    {
      "arxiv_id": "2506.08351v1",
      "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models",
      "title_zh": "å¼•å¯¼ä¹‹åº¦ï¼šé‡æ–°å®¡è§†æ— åˆ†ç±»å™¨å¼•å¯¼æ–‡æœ¬åˆ°è§†è§‰æ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªé€‚åº”å¼•å¯¼",
      "authors": [
        "Huixuan Zhang",
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­ Classifier-Free Guidance (CFG) å¯¼è‡´æ¨ç†æˆæœ¬ç¿»å€çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Step AG çš„é€šç”¨è‡ªé€‚åº”å¼•å¯¼ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œå°† CFG é™åˆ¶åœ¨å»å™ªè¿‡ç¨‹çš„å‰å‡ ä¸ªæ­¥éª¤ä¸­ï¼Œå°±è¶³ä»¥ç”Ÿæˆé«˜è´¨é‡ä¸”å…·å¤‡è‰¯å¥½å›¾åƒæ–‡æœ¬å¯¹é½ (Image-Text Alignment) çš„è§†è§‰å†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStep AG åœ¨ä¸æŸå¤±è´¨é‡çš„å‰æä¸‹ï¼Œå¹³å‡å¯å®ç° 20% åˆ° 30% çš„æ¨ç†åŠ é€Ÿã€‚è¿™ç§æ€§èƒ½æå‡åœ¨ä¸åŒæ¨ç†æ­¥æ•°è®¾ç½®ä»¥åŠè§†é¢‘ç”Ÿæˆæ¨¡å‹ç­‰å¤šç§åœºæ™¯ä¸‹å‡ä¿æŒä¸€è‡´ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡æ–¹é¢çš„ä¼˜è¶Šæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08351v1",
      "published_date": "2025-06-10 02:09:48 UTC",
      "updated_date": "2025-06-10 02:09:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:27.144671+00:00"
    },
    {
      "arxiv_id": "2506.08349v1",
      "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
      "title_zh": "è·¨å¤šè®¤çŸ¥ç»´åº¦çš„LLMè¯„ä¼°ï¼šä»åŒ»å­¦çŸ¥è¯†æŒæ¡åˆ°åœºæ™¯åŒ–é—®é¢˜è§£å†³",
      "authors": [
        "Yuxuan Zhou",
        "Xien Liu",
        "Chenwei Yan",
        "Chen Ning",
        "Xiao Zhang",
        "Boxun Li",
        "Xiangling Fu",
        "Shijin Wang",
        "Guoping Hu",
        "Yu Wang",
        "Ji Wu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—é¢†åŸŸè®¤çŸ¥æ°´å¹³è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å— Bloom's Taxonomy å¯å‘çš„åŒ»ç–—å¤šè®¤çŸ¥æ°´å¹³è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ç°æœ‰åŒ»ç–—æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆæ­¥çŸ¥è¯†æŒæ¡(preliminary knowledge grasp)ã€ç»¼åˆçŸ¥è¯†åº”ç”¨(comprehensive knowledge application)ä»¥åŠåŸºäºåœºæ™¯çš„é—®é¢˜è§£å†³(scenario-based problem solving)ä¸‰ä¸ªè®¤çŸ¥ç»´åº¦çš„è¯„ä¼°ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶å¯¹ Llamaã€Qwenã€Gemmaã€Phiã€GPT å’Œ DeepSeek ç­‰å…­å¤§ä¸»æµç³»åˆ—çš„é€šç”¨åŠåŒ»ç–— LLMs è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œéšç€è®¤çŸ¥å¤æ‚åº¦çš„æå‡ï¼Œæ‰€æœ‰å—è¯„ä¼°æ¨¡å‹çš„è¡¨ç°å‡å‘ˆç°æ˜¾è‘—ä¸‹é™ï¼Œä¸”æ¨¡å‹å‚æ•°è§„æ¨¡(model size)å¯¹é«˜è®¤çŸ¥æ°´å¹³ä»»åŠ¡çš„è¡¨ç°å…·æœ‰æ›´å…³é”®çš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŒ»ç–—å†³ç­–æ—¶çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºå¼€å‘é€‚ç”¨äºçœŸå®ä¸–ç•ŒåŒ»ç–—åº”ç”¨(real-world medical applications)çš„é«˜æ€§èƒ½ LLMs æä¾›äº†é‡è¦çš„å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 11 figures. Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08349v1",
      "published_date": "2025-06-10 02:07:33 UTC",
      "updated_date": "2025-06-10 02:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:29.800046+00:00"
    },
    {
      "arxiv_id": "2506.08346v1",
      "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models",
      "title_zh": "SPBAï¼šåˆ©ç”¨è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹å®ç°é’ˆå¯¹è¯­éŸ³åˆ†ç±»æ¨¡å‹çš„åé—¨æ”»å‡»",
      "authors": [
        "Wenhan Yao",
        "Fen Xiao",
        "Xiarun Chen",
        "Jia Liu",
        "YongQiang He",
        "Weiping Wen"
      ],
      "abstract": "Deep speech classification tasks, including keyword spotting and speaker verification, are vital in speech-based human-computer interaction. Recently, the security of these technologies has been revealed to be susceptible to backdoor attacks. Specifically, attackers use noisy disruption triggers and speech element triggers to produce poisoned speech samples that train models to become vulnerable. However, these methods typically create only a limited number of backdoors due to the inherent constraints of the trigger function. In this paper, we propose that speech backdoor attacks can strategically focus on speech elements such as timbre and emotion, leveraging the Speech Large Language Model (SLLM) to generate diverse triggers. Increasing the number of triggers may disproportionately elevate the poisoning rate, resulting in higher attack costs and a lower success rate per trigger. We introduce the Multiple Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this challenge. The proposed attack is called the Speech Prompt Backdoor Attack (SPBA). Building on this foundation, we conducted attack experiments on two speech classification tasks, demonstrating that SPBA shows significant trigger effectiveness and achieves exceptional performance in attack metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³åˆ†ç±»ä»»åŠ¡ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†åä¸º SPBA çš„è¯­éŸ³æç¤ºåé—¨æ”»å‡»æ¡†æ¶ã€‚ç”±äºä¼ ç»Ÿæ”»å‡»æ–¹æ³•å—é™äºè§¦å‘å‡½æ•°ä¸”ç”Ÿæˆçš„åé—¨æ•°é‡æœ‰é™ï¼ŒSPBA åˆ›æ–°æ€§åœ°åˆ©ç”¨è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ (SLLM) é’ˆå¯¹éŸ³è‰² (timbre) å’Œæƒ…æ„Ÿ (emotion) ç­‰æ ¸å¿ƒè¯­éŸ³å…ƒç´ ç”Ÿæˆå¤šæ ·åŒ–çš„è§¦å‘å™¨ (triggers)ã€‚é’ˆå¯¹è§¦å‘å™¨æ•°é‡å¢åŠ å¯èƒ½å¼•å‘çš„ä¸­æ¯’ç‡ä¸Šå‡åŠæ”»å‡»æ•ˆç‡ä¸‹é™é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†å¤šæ¢¯åº¦ä¸‹é™ç®—æ³• (MGDA) åè°ƒå¤šä¸ªæ”»å‡»ç›®æ ‡å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPBA åœ¨å…³é”®è¯æ£€æµ‹ (keyword spotting) å’Œè¯´è¯äººéªŒè¯ (speaker verification) ä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„è§¦å‘æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å„é¡¹æ”»å‡»è¯„ä»·æŒ‡æ ‡ä¸­å–å¾—äº†å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by IJCNN 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08346v1",
      "published_date": "2025-06-10 02:01:00 UTC",
      "updated_date": "2025-06-10 02:01:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:51:44.007745+00:00"
    },
    {
      "arxiv_id": "2506.08344v1",
      "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning",
      "title_zh": "Re4MPCï¼šåŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ååº”å¼éçº¿æ€§MPCå¤šæ¨¡å‹è¿åŠ¨è§„åˆ’",
      "authors": [
        "NeÅŸet Ãœnver Akmandor",
        "Sarvesh Prajapati",
        "Mark Zolotas",
        "TaÅŸkÄ±n PadÄ±r"
      ],
      "abstract": "Traditional motion planning methods for robots with many degrees-of-freedom, such as mobile manipulators, are often computationally prohibitive for real-world settings. In this paper, we propose a novel multi-model motion planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear Model Predictive Control (NMPC). Re4MPC generates trajectories in a computationally efficient manner by reactively selecting the model, cost, and constraints of the NMPC problem depending on the complexity of the task and robot state. The policy for this reactive decision-making is learned via a Deep Reinforcement Learning (DRL) framework. We introduce a mathematical formulation to integrate NMPC into this DRL framework. To validate our methodology and design choices, we evaluate DRL training and test outcomes in a physics-based simulation involving a mobile manipulator. Experimental results demonstrate that Re4MPC is more computationally efficient and achieves higher success rates in reaching end-effector goals than the NMPC baseline, which computes whole-body trajectories without our learning mechanism.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Re4MPCï¼Œä¸€ç§ç»“åˆ Deep Reinforcement Learning (DRL) çš„ååº”å¼éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ (Nonlinear Model Predictive Control, NMPC) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜è‡ªç”±åº¦æœºå™¨äººè¿åŠ¨è§„åˆ’ä¸­è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚Re4MPC èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œæœºå™¨äººçŠ¶æ€ï¼Œé€šè¿‡ DRL å­¦ä¹ åˆ°çš„ç­–ç•¥åŠ¨æ€é€‰æ‹© NMPC çš„æ¨¡å‹ã€ä»£ä»·å‡½æ•°å’Œçº¦æŸã€‚ç ”ç©¶ä¸­å¼•å…¥äº†ä¸“é—¨çš„æ•°å­¦å…¬å¼å°† NMPC æ— ç¼é›†æˆåˆ° DRL æ¡†æ¶å†…ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡çš„ä¼˜åŒ–ã€‚åœ¨ç§»åŠ¨æ“ä½œæœºå™¨äººçš„ç‰©ç†ä»¿çœŸå®éªŒä¸­ï¼ŒRe4MPC ä¸ä¼ ç»Ÿçš„å…¨é‡ NMPC åŸºå‡†ç›¸æ¯”ï¼Œä¸ä»…æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œè¿˜åœ¨æœ«ç«¯æ‰§è¡Œå™¨åˆ°è¾¾ç›®æ ‡çš„æˆåŠŸç‡ä¸Šå–å¾—äº†æ›´å¥½çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the 2025 IEEE International Conference on Automation Science and Engineering (CASE)",
      "pdf_url": "https://arxiv.org/pdf/2506.08344v1",
      "published_date": "2025-06-10 01:58:32 UTC",
      "updated_date": "2025-06-10 01:58:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:14.306493+00:00"
    },
    {
      "arxiv_id": "2506.08336v2",
      "title": "Your Agent Can Defend Itself against Backdoor Attacks",
      "title_zh": "æ‚¨çš„æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»æŠµå¾¡åé—¨æ”»å‡»",
      "authors": [
        "Li Changjiang",
        "Liang Jiacheng",
        "Cao Bochuan",
        "Chen Jinghui",
        "Wang Ting"
      ],
      "abstract": "Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during training and fine-tuning. These compromised agents can subsequently be manipulated to execute malicious operations when presented with specific triggers in their inputs or environments. To address this pressing risk, we present ReAgent, a novel defense against a range of backdoor attacks on LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies among the user's instruction, the agent's planning, and its execution. Drawing on this insight, ReAgent employs a two-level approach to detect potential backdoors. At the execution level, ReAgent verifies consistency between the agent's thoughts and actions; at the planning level, ReAgent leverages the agent's capability to reconstruct the instruction based on its thought trajectory, checking for consistency between the reconstructed instruction and the user's instruction. Extensive evaluation demonstrates ReAgent's effectiveness against various backdoor attacks across tasks. For instance, ReAgent reduces the attack success rate by up to 90\\% in database operation tasks, outperforming existing defenses by large margins. This work reveals the potential of utilizing compromised agents themselves to mitigate backdoor risks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„åé—¨æ”»å‡»(Backdoor Attacks)é£é™©ï¼Œæå‡ºäº†åä¸ºReAgentçš„æ–°å‹é˜²å¾¡æœºåˆ¶ã€‚è¯¥æœºåˆ¶çš„æ ¸å¿ƒåŸºäºä¸€ç§ç›´è§‰è§‚å¯Ÿï¼Œå³åé—¨æ”»å‡»å¾€å¾€ä¼šå¯¼è‡´ç”¨æˆ·æŒ‡ä»¤ã€æ™ºèƒ½ä½“è§„åˆ’(Planning)ä¸å®é™…æ‰§è¡Œ(Execution)ä¹‹é—´äº§ç”Ÿä¸ä¸€è‡´ã€‚ReAgenté€šè¿‡åŒå±‚æ¶æ„è¿›è¡Œæ£€æµ‹ï¼šåœ¨æ‰§è¡Œå±‚é¢éªŒè¯æ™ºèƒ½ä½“çš„æ€ç»´è½¨è¿¹ä¸åŠ¨ä½œçš„ä¸€è‡´æ€§ï¼Œåœ¨è§„åˆ’å±‚é¢åˆ™åˆ©ç”¨æ™ºèƒ½ä½“é‡æ„æŒ‡ä»¤çš„èƒ½åŠ›æ¥æ ¸å¯¹åŸå§‹æŒ‡ä»¤çš„çœŸå®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒReAgentåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ•°æ®åº“æ“ä½œä»»åŠ¡ä¸­å°†æ”»å‡»æˆåŠŸç‡é™ä½äº†é«˜è¾¾90%ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶Šç°æœ‰é˜²å¾¡æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†æ™ºèƒ½ä½“çš„å®‰å…¨æ€§ï¼Œä¹Ÿè¯æ˜äº†åˆ©ç”¨å—æŸæ™ºèƒ½ä½“è‡ªèº«è¿›è¡Œåé—¨é£é™©æ¶ˆå‡çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08336v2",
      "published_date": "2025-06-10 01:45:56 UTC",
      "updated_date": "2025-06-11 01:39:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:17.501464+00:00"
    },
    {
      "arxiv_id": "2506.08332v2",
      "title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization",
      "title_zh": "ORFS-agentï¼šé¢å‘èŠ¯ç‰‡è®¾è®¡ä¼˜åŒ–çš„å·¥å…·è°ƒç”¨æ™ºèƒ½ä½“",
      "authors": [
        "Amur Ghose",
        "Andrew B. Kahng",
        "Sayak Kundu",
        "Zhiang Wang"
      ],
      "abstract": "Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† ORFS-agentï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è¿­ä»£ä¼˜åŒ–æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å¼€æºç¡¬ä»¶è®¾è®¡æµç¨‹ä¸­çš„å‚æ•°è°ƒä¼˜ã€‚é’ˆå¯¹é›†æˆç”µè·¯è®¾è®¡ä¸­ä»å¯„å­˜å™¨ä¼ è¾“çº§ (RTL) åˆ°ç‰©ç†å¸ƒå±€è¿‡ç¨‹ä¸­ä¸Šåƒä¸ªå‚æ•°å¯¹åŠŸè€—ã€æ€§èƒ½å’Œé¢ç§¯ (PPA) çš„å¤æ‚å½±å“ï¼ŒORFS-agent å±•ç°äº†å¼ºå¤§çš„é«˜ç»´ç©ºé—´æ¨ç†ä¸ä¼˜åŒ–èƒ½åŠ›ã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¢ç´¢å‚æ•°é…ç½®ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨ä¸åŒæŒ‡æ ‡é—´è¿›è¡Œæƒè¡¡ï¼Œå®ç°çµæ´»ä¸”å¯è§£é‡Šçš„å¤šç›®æ ‡ä¼˜åŒ– (Multi-objective optimization)ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤šç§ç”µè·¯åŸºå‡†ä¸‹ï¼ŒORFS-agent ç›¸æ¯”ä¼ ç»Ÿçš„è´å¶æ–¯ä¼˜åŒ– (Bayesian optimization) æ–¹æ³•ï¼Œåœ¨å‡å°‘ 40% è¿­ä»£æ¬¡æ•°çš„åŒæ—¶ï¼Œå°†å¸ƒçº¿é•¿åº¦å’Œæœ‰æ•ˆæ—¶é’Ÿå‘¨æœŸæå‡äº†è¶…è¿‡ 13%ã€‚ä½œä¸ºä¸€ç§æ¨¡å—åŒ–ä¸”æ¨¡å‹æ— å…³ (Model-agnostic) çš„æ¡†æ¶ï¼ŒORFS-agent æ— éœ€å¾®è°ƒå³å¯æ¥å…¥å„ç±»å‰æ²¿æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†èŠ¯ç‰‡è®¾è®¡çš„èµ„æºæ•ˆç‡ä¸æœ€ç»ˆè®¾è®¡æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08332v2",
      "published_date": "2025-06-10 01:38:57 UTC",
      "updated_date": "2025-08-01 06:36:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:17.765500+00:00"
    },
    {
      "arxiv_id": "2506.08326v1",
      "title": "Graph Prompting for Graph Learning Models: Recent Advances and Future Directions",
      "title_zh": "å›¾å­¦ä¹ æ¨¡å‹ä¸­çš„å›¾æç¤ºï¼šæœ€æ–°è¿›å±•ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Xingbo Fu",
        "Zehong Wang",
        "Zihan Chen",
        "Jiazheng Li",
        "Yaochen Zhu",
        "Zhenyu Lei",
        "Cong Shen",
        "Yanfang Ye",
        "Chuxu Zhang",
        "Jundong Li"
      ],
      "abstract": "Graph learning models have demonstrated great prowess in learning expressive representations from large-scale graph data in a wide variety of real-world scenarios. As a prevalent strategy for training powerful graph learning models, the \"pre-training, adaptation\" scheme first pre-trains graph learning models on unlabeled graph data in a self-supervised manner and then adapts them to specific downstream tasks. During the adaptation phase, graph prompting emerges as a promising approach that learns trainable prompts while keeping the pre-trained graph learning models unchanged. In this paper, we present a systematic review of recent advancements in graph prompting. First, we introduce representative graph pre-training methods that serve as the foundation step of graph prompting. Next, we review mainstream techniques in graph prompting and elaborate on how they design learnable prompts for graph prompting. Furthermore, we summarize the real-world applications of graph prompting from different domains. Finally, we discuss several open challenges in existing studies with promising future directions in this field.",
      "tldr_zh": "è¯¥è®ºæ–‡å¯¹å›¾æç¤º (Graph Prompting) åœ¨å›¾å­¦ä¹ æ¨¡å‹ (Graph Learning Models) ä¸­çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ã€‚åœ¨â€œé¢„è®­ç»ƒ-é€‚é… (Pre-training, Adaptation)â€çš„ä¸»æµèŒƒå¼ä¸‹ï¼ŒGraph Prompting èƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ¨¡å‹æƒé‡ä¸å˜çš„å‰æä¸‹ï¼Œé€šè¿‡å­¦ä¹ å¯è®­ç»ƒçš„æç¤º (Trainable Prompts) æ¥å®ç°é«˜æ•ˆçš„ä¸‹æ¸¸ä»»åŠ¡è¿ç§»ã€‚æ–‡ç« é¦–å…ˆæ¢³ç†äº†ä½œä¸ºæŠ€æœ¯åŸºç¡€çš„ä»£è¡¨æ€§å›¾é¢„è®­ç»ƒ (Graph Pre-training) æ–¹æ³•ï¼Œéšåè¯¦ç»†é˜è¿°äº†ä¸»æµ Graph Prompting æŠ€æœ¯çš„æç¤ºè®¾è®¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ€»ç»“äº†è¯¥æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸçš„å®é™…åº”ç”¨åœºæ™¯ã€‚æœ€åï¼Œä½œè€…æ¢è®¨äº†è¯¥é¢†åŸŸç›®å‰é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD 2025 Tutorial/Survey Track",
      "pdf_url": "https://arxiv.org/pdf/2506.08326v1",
      "published_date": "2025-06-10 01:27:19 UTC",
      "updated_date": "2025-06-10 01:27:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:52:41.839238+00:00"
    },
    {
      "arxiv_id": "2506.08321v1",
      "title": "LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs",
      "title_zh": "LeanTutorï¼šé¢å‘æ•°å­¦è¯æ˜çš„å½¢å¼åŒ–éªŒè¯ AI å¯¼å¸ˆ",
      "authors": [
        "Manooshree Patel",
        "Rayna Bhattacharyya",
        "Thomas Lu",
        "Arnav Mehta",
        "Niels Voss",
        "Narges Norouzi",
        "Gireeja Ranade"
      ],
      "abstract": "We present LeanTutor, a Large Language Model (LLM)-based tutoring system for math proofs. LeanTutor interacts with the student in natural language, formally verifies student-written math proofs in Lean, generates correct next steps, and provides the appropriate instructional guidance. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. The first module faithfully autoformalizes student proofs into Lean and verifies proof accuracy via successful code compilation. If the proof has an error, the incorrect step is identified. The next-step generator module outputs a valid next Lean tactic for incorrect proofs via LLM-based candidate generation and proof search. The feedback generator module leverages Lean data to produce a pedagogically-motivated natural language hint for the student user. To evaluate our system, we introduce PeanoBench, a human-written dataset derived from the Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each natural language proof step is paired with the corresponding logically equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of tactics in correct proofs and accurately identifies the incorrect step in 30% of incorrect proofs. In generating natural language hints for erroneous proofs, LeanTutor outperforms a simple baseline on accuracy and relevance metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LeanTutorï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ•°å­¦è¯æ˜è¾…å¯¼ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’å®ç°å­¦ç”Ÿè¯æ˜çš„è‡ªåŠ¨åŒ–å½¢å¼åŒ–ä¸éªŒè¯ã€‚è¯¥ç³»ç»Ÿç”±è‡ªåŠ¨å½¢å¼åŒ–å™¨/è¯æ˜æ£€æŸ¥å™¨ (Autoformalizer/Proof-checker)ã€ä¸‹ä¸€æ­¥ç”Ÿæˆå™¨ (Next-step generator) å’Œè‡ªç„¶è¯­è¨€åé¦ˆç”Ÿæˆå™¨ (Feedback generator) ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼Œåˆ©ç”¨ Lean å¼•æ“ç¡®ä¿è¯æ˜é€»è¾‘çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº† PeanoBench æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å« 371 ä¸ªæºè‡ª Natural Numbers Game çš„çš®äºšè¯ºç®—æœ¯ (Peano Arithmetic) è¯æ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoformalizer èƒ½å¤Ÿæ­£ç¡®å½¢å¼åŒ– 57% çš„è¯æ˜æ­¥éª¤ï¼Œå¹¶å‡†ç¡®è¯†åˆ« 30% çš„é”™è¯¯æ­¥éª¤ã€‚åœ¨ç”Ÿæˆæ•™å­¦åé¦ˆæç¤ºæ–¹é¢ï¼ŒLeanTutor åœ¨å‡†ç¡®æ€§å’Œç›¸å…³æ€§æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†åŸºå‡†æ¨¡å‹ï¼Œå±•ç¤ºäº†å½¢å¼åŒ–éªŒè¯åœ¨æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08321v1",
      "published_date": "2025-06-10 01:12:36 UTC",
      "updated_date": "2025-06-10 01:12:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:00.345167+00:00"
    },
    {
      "arxiv_id": "2506.08320v2",
      "title": "How Good LLM-Generated Password Policies Are?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¯†ç ç­–ç•¥è´¨é‡ç©¶ç«Ÿå¦‚ä½•ï¼Ÿ",
      "authors": [
        "Vivek Vaidya",
        "Aditya Patwardhan",
        "Ashish Kundu"
      ],
      "abstract": "Generative AI technologies, particularly Large Language Models (LLMs), are rapidly being adopted across industry, academia, and government sectors, owing to their remarkable capabilities in natural language processing. However, despite their strengths, the inconsistency and unpredictability of LLM outputs present substantial challenges, especially in security-critical domains such as access control. One critical issue that emerges prominently is the consistency of LLM-generated responses, which is paramount for ensuring secure and reliable operations.\n  In this paper, we study the application of LLMs within the context of Cybersecurity Access Control Systems. Specifically, we investigate the consistency and accuracy of LLM-generated password policies, translating natural language prompts into executable pwquality$.$conf configuration files. Our experimental methodology adopts two distinct approaches: firstly, we utilize pre-trained LLMs to generate configuration files purely from natural language prompts without additional guidance. Secondly, we provide these models with official pwquality$.$conf documentation to serve as an informative baseline. We systematically assess the soundness, accuracy, and consistency of these AI-generated configurations. Our findings underscore significant challenges in the current generation of LLMs and contribute valuable insights into refining the deployment of LLMs in Access Control Systems.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ç½‘ç»œå®‰å…¨è®¿é—®æ§åˆ¶ç³»ç»Ÿ(Cybersecurity Access Control Systems)ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è¯„ä¼°å…¶å°†è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„pwquality.confé…ç½®æ–‡ä»¶çš„èƒ½åŠ›ã€‚å®éªŒå¯¹æ¯”äº†åœ¨æ— å¤–éƒ¨æŒ‡å¯¼å’Œæä¾›å®˜æ–¹æ–‡æ¡£åŸºå‡†ä¸¤ç§æƒ…å¢ƒä¸‹ï¼ŒLLMsç”Ÿæˆå¯†ç ç­–ç•¥çš„åˆç†æ€§(soundness)ã€å‡†ç¡®æ€§(accuracy)å’Œä¸€è‡´æ€§(consistency)ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡LLMså…·å¤‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å®‰å…¨æ€§å…³é”®ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—çš„ä¸å¯é¢„æµ‹æ€§å’Œä¸ä¸€è‡´æ€§ã€‚é€šè¿‡æ­ç¤ºå½“å‰LLMsåœ¨è‡ªåŠ¨åŒ–è®¿é—®æ§åˆ¶é…ç½®ä¸­çš„å±€é™ï¼Œè¯¥ç ”ç©¶ä¸ºä¼˜åŒ–LLMsåœ¨å®‰å…¨é¢†åŸŸçš„éƒ¨ç½²æä¾›äº†å…³é”®è§è§£ã€‚æ­¤é¡¹å·¥ä½œå¯¹äºæå‡åŸºäºAIçš„å®‰å…¨ç®¡ç†ç³»ç»Ÿçš„å¯é æ€§å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 2 Tables, 9 figures, 3 Algorithms",
      "pdf_url": "https://arxiv.org/pdf/2506.08320v2",
      "published_date": "2025-06-10 01:12:31 UTC",
      "updated_date": "2025-07-02 23:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:10.457833+00:00"
    },
    {
      "arxiv_id": "2506.08311v1",
      "title": "Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study",
      "title_zh": "ä»å¯è¿½æº¯æ€§è§†è§’ç†è§£è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ï¼šä¸€é¡¹å®è¯ç ”ç©¶",
      "authors": [
        "Ira Ceka",
        "Saurabh Pujar",
        "Shyam Ramji",
        "Luca Buratti",
        "Gail Kaiser",
        "Baishakhi Ray"
      ],
      "abstract": "With the advent of large language models (LLMs), software engineering agents (SWE agents) have emerged as a powerful paradigm for automating a range of software tasks -- from code generation and repair to test case synthesis. These agents operate autonomously by interpreting user input and responding to environmental feedback. While various agent architectures have demonstrated strong empirical performance, the internal decision-making worfklows that drive their behavior remain poorly understood. Deeper insight into these workflows hold promise for improving both agent reliability and efficiency. In this work, we present the first systematic study of SWE agent behavior through the lens of execution traces. Our contributions are as follows: (1) we propose the first taxonomy of decision-making pathways across five representative agents; (2) using this taxonomy, we identify three core components essential to agent success -- bug localization, patch generation, and reproduction test generation -- and study each in depth; (3) we study the impact of test generation on successful patch production; and analyze strategies that can lead to successful test generation; (4) we further conduct the first large-scale code clone analysis comparing agent-generated and developer-written patches and provide a qualitative study revealing structural and stylistic differences in patch content. Together, these findings offer novel insights into agent design and open avenues for building agents that are both more effective and more aligned with human development practices.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ‰§è¡Œè¿½è¸ª(execution traces)çš„è§†è§’å¯¹è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“(SWE agents)çš„è¡Œä¸ºè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œæ—¨åœ¨æ­ç¤ºé©±åŠ¨å…¶è‡ªä¸»å†³ç­–çš„å†…éƒ¨å·¥ä½œæµã€‚ä½œè€…æå‡ºäº†é¦–ä¸ªé’ˆå¯¹äº”ç§ä»£è¡¨æ€§æ™ºèƒ½ä½“çš„å†³ç­–è·¯å¾„åˆ†ç±»æ³•(taxonomy)ï¼Œä¸ºç†è§£å…¶å¤æ‚è¡Œä¸ºæä¾›äº†ç†è®ºæ¡†æ¶ã€‚ç ”ç©¶è¯†åˆ«å¹¶æ·±å…¥æ¢è®¨äº†æ™ºèƒ½ä½“æˆåŠŸçš„ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç¼ºé™·å®šä½(bug localization)ã€è¡¥ä¸ç”Ÿæˆ(patch generation)å’Œé‡ç°æµ‹è¯•ç”Ÿæˆ(reproduction test generation)ã€‚è®ºæ–‡è¿›ä¸€æ­¥åˆ†æäº†æµ‹è¯•ç”Ÿæˆå¯¹æˆåŠŸäº§å‡ºè¡¥ä¸çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†å¼•å¯¼æˆåŠŸç”Ÿæˆæµ‹è¯•çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼€å±•äº†é¦–ä¸ªå¤§è§„æ¨¡ä»£ç å…‹éš†åˆ†æ(code clone analysis)ï¼Œå®šæ€§ä¸å®šé‡åœ°æ¯”è¾ƒäº†æ™ºèƒ½ä½“ç”Ÿæˆçš„è¡¥ä¸ä¸å¼€å‘äººå‘˜ç¼–å†™çš„è¡¥ä¸åœ¨ç»“æ„å’Œé£æ ¼ä¸Šçš„å·®å¼‚ã€‚è¿™äº›å‘ç°ä¸ºæ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–°é¢–è§è§£ï¼Œæœ‰åŠ©äºæ„å»ºæ›´é«˜æ•ˆä¸”æ›´ç¬¦åˆäººç±»å¼€å‘å®è·µçš„è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08311v1",
      "published_date": "2025-06-10 00:41:54 UTC",
      "updated_date": "2025-06-10 00:41:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:13.197544+00:00"
    },
    {
      "arxiv_id": "2506.08309v2",
      "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction",
      "title_zh": "é¢å‘é“¾è·¯é¢„æµ‹çš„å¯å­¦ä¹ æ—¶ç©ºä½ç½®ç¼–ç ",
      "authors": [
        "Katherine Tieu",
        "Dongqi Fu",
        "Zihao Li",
        "Ross Maciejewski",
        "Jingrui He"
      ],
      "abstract": "Accurate predictions rely on the expressiveness power of graph deep learning frameworks like graph neural networks and graph transformers, where a positional encoding mechanism has become much more indispensable in recent state-of-the-art works to record the canonical position information. However, the current positional encoding is limited in three aspects: (1) most positional encoding methods use pre-defined, and fixed functions, which are inadequate to adapt to the complex attributed graphs; (2) a few pioneering works proposed the learnable positional encoding but are still limited to the structural information, not considering the real-world time-evolving topological and feature information; (3) most positional encoding methods are equipped with transformers' attention mechanism to fully leverage their capabilities, where the dense or relational attention is often unaffordable on large-scale structured data. Hence, we aim to develop Learnable Spatial-Temporal Positional Encoding in an effective and efficient manner and propose a simple temporal link prediction model named L-STEP. Briefly, for L-STEP, we (1) prove the proposed positional learning scheme can preserve the graph property from the spatial-temporal spectral viewpoint, (2) verify that MLPs can fully exploit the expressiveness and reach transformers' performance on that encoding, (3) change different initial positional encoding inputs to show robustness, (4) analyze the theoretical complexity and obtain less empirical running time than SOTA, and (5) demonstrate its temporal link prediction out-performance on 13 classic datasets and with 10 algorithms in both transductive and inductive settings using 3 different sampling strategies. Also, L-STEP obtains the leading performance in the newest large-scale TGB benchmark. Our code is available at https://github.com/kthrn22/L-STEP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Positional Encoding åœ¨å¤„ç†å¤æ‚åŠ¨æ€å›¾æ—¶å­˜åœ¨çš„ç¼–ç å‡½æ•°å›ºå®šã€å¿½ç•¥æ—¶åºæ¼”åŒ–ä¿¡æ¯ä»¥åŠé«˜æ˜‚è®¡ç®—æˆæœ¬ç­‰å±€é™æ€§ï¼Œæå‡ºäº†åä¸º L-STEP çš„å¯å­¦ä¹ æ—¶ç©ºä½ç½®ç¼–ç æ¡†æ¶ã€‚L-STEP ä» Spatial-Temporal Spectral è§†è§’å‡ºå‘ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†è¯¥å­¦ä¹ æ–¹æ¡ˆèƒ½æœ‰æ•ˆä¿ç•™å›¾çš„æœ¬è´¨å±æ€§ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ æœºåˆ¶ï¼Œç ”ç©¶å‘ç°ç®€å•çš„ MLPs å³å¯åœ¨ Positional Encoding çš„æ”¯æŒä¸‹è¾¾åˆ°ä¸ Transformer ç›¸å½“çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ˜¾è‘—é™ä½äº†å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®ä¸‹çš„ç»éªŒè¿è¡Œæ—¶é—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒL-STEP åœ¨ 13 ä¸ªç»å…¸æ•°æ®é›†ä»¥åŠæœ€æ–°çš„ TGB å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œä¸”åœ¨ Transductive å’Œ Inductive è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¯¥å·¥ä½œçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºä¸ºæ—¶ç©ºæ•°æ®çš„ Link Prediction æä¾›äº†ä¸€ç§å…¼å…·ç†è®ºä¿è¯ä¸å·¥ç¨‹æ•ˆç‡çš„é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2025. 28 pages, 1 figures, 22 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.08309v2",
      "published_date": "2025-06-10 00:35:53 UTC",
      "updated_date": "2025-06-11 03:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:43.430780+00:00"
    },
    {
      "arxiv_id": "2506.08306v1",
      "title": "AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data",
      "title_zh": "AstroCompressï¼šé¢å‘å¤šç”¨é€”å¤©æ–‡æ•°æ®å‹ç¼©çš„åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Tuan Truong",
        "Rithwik Sudharsan",
        "Yibo Yang",
        "Peter Xiangyuan Ma",
        "Ruihan Yang",
        "Stephan Mandt",
        "Joshua S. Bloom"
      ],
      "abstract": "The site conditions that make astronomical observatories in space and on the ground so desirable -- cold and dark -- demand a physical remoteness that leads to limited data transmission capabilities. Such transmission limitations directly bottleneck the amount of data acquired and in an era of costly modern observatories, any improvements in lossless data compression has the potential scale to billions of dollars worth of additional science that can be accomplished on the same instrument. Traditional lossless methods for compressing astrophysical data are manually designed. Neural data compression, on the other hand, holds the promise of learning compression algorithms end-to-end from data and outperforming classical techniques by leveraging the unique spatial, temporal, and wavelength structures of astronomical images. This paper introduces AstroCompress: a neural compression challenge for astrophysics data, featuring four new datasets (and one legacy dataset) with 16-bit unsigned integer imaging data in various modes: space-based, ground-based, multi-wavelength, and time-series imaging. We provide code to easily access the data and benchmark seven lossless compression methods (three neural and four non-neural, including all practical state-of-the-art algorithms). Our results on lossless compression indicate that lossless neural compression techniques can enhance data collection at observatories, and provide guidance on the adoption of neural compression in scientific applications. Though the scope of this paper is restricted to lossless compression, we also comment on the potential exploration of lossy compression methods in future studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†AstroCompressï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤©æ–‡æ•°æ®å¤šç”¨é€”å‹ç¼©çš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å› å¤©æ–‡å°åœ°ç†ä½ç½®åè¿œè€Œå¯¼è‡´çš„ä¼ è¾“å¸¦å®½é™åˆ¶é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«å››ä¸ªæ–°æ•°æ®é›†å’Œä¸€ä¸ªä¼ ç»Ÿæ•°æ®é›†ï¼Œæ¶µç›–äº†16-bit unsigned integerå½¢å¼çš„ç©ºåŸºã€é™†åŸºã€å¤šæ³¢æ®µåŠæ—¶é—´åºåˆ—å›¾åƒã€‚ä½œè€…æä¾›äº†ä¾¿æ·çš„æ•°æ®è®¿é—®ä»£ç ï¼Œå¹¶å¯¹ä¸ƒç§æ— æŸå‹ç¼©(Lossless compression)æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸‰ç§ç¥ç»æ•°æ®å‹ç¼©(Neural data compression)å’Œå››ç§éç¥ç»çš„æœ€å…ˆè¿›ç®—æ³•ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ— æŸç¥ç»å‹ç¼©æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºå¤©æ–‡å°çš„æ•°æ®é‡‡é›†èƒ½åŠ›ï¼Œå¹¶ä¸ºç¥ç»å‹ç¼©åœ¨ç§‘å­¦åº”ç”¨ä¸­çš„è½åœ°æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚è™½ç„¶æœ¬æ–‡ç›®å‰èšç„¦äºæ— æŸå‹ç¼©ï¼Œä½†ä¹Ÿä¸ºæœªæ¥æ¢ç´¢æœ‰æŸå‹ç¼©(Lossy compression)æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "astro-ph.IM"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2025 conference paper. See reviews at https://openreview.net/forum?id=kQCHCkNk7s",
      "pdf_url": "https://arxiv.org/pdf/2506.08306v1",
      "published_date": "2025-06-10 00:32:30 UTC",
      "updated_date": "2025-06-10 00:32:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:29.551466+00:00"
    },
    {
      "arxiv_id": "2506.08297v1",
      "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging",
      "title_zh": "SEMAï¼šåŸºäº Token å±€éƒ¨åŒ–ä¸å‡å€¼åŒ–çš„å¯æ‰©å±•é«˜æ•ˆç±» Mamba æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Nhat Thanh Tran",
        "Fanghui Xue",
        "Shuai Zhang",
        "Jiancheng Lyu",
        "Yunling Zheng",
        "Yingyong Qi",
        "Jack Xin"
      ],
      "abstract": "Attention is the critical component of a transformer. Yet the quadratic computational complexity of vanilla full attention in the input size and the inability of its linear attention variant to focus have been challenges for computer vision tasks. We provide a mathematical definition of generalized attention and formulate both vanilla softmax attention and linear attention within the general framework. We prove that generalized attention disperses, that is, as the number of keys tends to infinity, the query assigns equal weights to all keys. Motivated by the dispersion property and recent development of Mamba form of attention, we design Scalable and Efficient Mamba like Attention (SEMA) which utilizes token localization to avoid dispersion and maintain focusing, complemented by theoretically consistent arithmetic averaging to capture global aspect of attention. We support our approach on Imagenet-1k where classification results show that SEMA is a scalable and effective alternative beyond linear attention, outperforming recent vision Mamba models on increasingly larger scales of images at similar model parameter sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SEMAï¼ˆScalable and Efficient Mamba like Attentionï¼‰ï¼Œæ—¨åœ¨è§£å†³Transformerä¸­vanilla full attentionçš„äºŒæ¬¡è®¡ç®—å¤æ‚åº¦ä»¥åŠlinear attentionåœ¨å¤„ç†å¤§è§„æ¨¡è¾“å…¥æ—¶éš¾ä»¥èšç„¦çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡å¯¹generalized attentionè¿›è¡Œæ•°å­¦å®šä¹‰å¹¶è¯æ˜ï¼Œéšç€keyæ•°é‡è¶‹äºæ— ç©·ï¼Œqueryå€¾å‘äºå¯¹æ‰€æœ‰keyåˆ†é…ç›¸ç­‰æƒé‡ï¼Œå³äº§ç”Ÿdispersionï¼ˆåˆ†æ•£ï¼‰ç°è±¡ã€‚ä¸ºå…‹æœè¿™ä¸€ç¼ºé™·ï¼ŒSEMAå¼•å…¥äº†token localizationæŠ€æœ¯ä»¥é˜²æ­¢æƒé‡åˆ†æ•£å¹¶ä¿æŒå…¶focusingï¼ˆèšç„¦ï¼‰èƒ½åŠ›ï¼ŒåŒæ—¶ç»“åˆç†è®ºä¸€è‡´çš„arithmetic averagingæ¥æ•æ‰æ³¨æ„åŠ›çš„global aspectï¼ˆå…¨å±€ç‰¹å¾ï¼‰ã€‚åœ¨ImageNet-1kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSEMAæ˜¯ä¼˜äºlinear attentionçš„æœ‰æ•ˆå¯æ‰©å±•æ–¹æ¡ˆã€‚åœ¨ç›¸ä¼¼çš„å‚æ•°è§„æ¨¡ä¸‹ï¼ŒSEMAåœ¨å¤§å°ºåº¦å›¾åƒä»»åŠ¡ä¸Šçš„åˆ†ç±»æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„vision Mambaæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé«˜æ•ˆä¸”å…·å¤‡å…¨å±€æ„ŸçŸ¥èƒ½åŠ›çš„è§†è§‰éª¨å¹²ç½‘ç»œæä¾›äº†æ–°çš„ç†è®ºæ”¯æŒä¸å®ç°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, figures 3",
      "pdf_url": "https://arxiv.org/pdf/2506.08297v1",
      "published_date": "2025-06-10 00:03:19 UTC",
      "updated_date": "2025-06-10 00:03:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:53:30.279690+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 200,
  "processed_papers_count": 200,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T20:54:38.519615+00:00"
}
[
  {
    "arxiv_id": "2403.13206v2",
    "title": "Depth-guided NeRF Training via Earth Mover's Distance",
    "authors": [
      "Anita Rau",
      "Josiah Aklilu",
      "F. Christopher Holsinger",
      "Serena Yeung-Levy"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of\npredicted viewpoints. However, the photometric loss often does not provide\nenough information to disambiguate between different possible geometries\nyielding the same image. Previous work has thus incorporated depth supervision\nduring NeRF training, leveraging dense predictions from pre-trained depth\nnetworks as pseudo-ground truth. While these depth priors are assumed to be\nperfect once filtered for noise, in practice, their accuracy is more\nchallenging to capture. This work proposes a novel approach to uncertainty in\ndepth priors for NeRF supervision. Instead of using custom-trained depth or\nuncertainty priors, we use off-the-shelf pretrained diffusion models to predict\ndepth and capture uncertainty during the denoising process. Because we know\nthat depth priors are prone to errors, we propose to supervise the ray\ntermination distance distribution with Earth Mover's Distance instead of\nenforcing the rendered depth to replicate the depth prior exactly through\nL2-loss. Our depth-guided NeRF outperforms all baselines on standard depth\nmetrics by a large margin while maintaining performance on photometric\nmeasures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13206v2",
    "published_date": "2024-03-19 23:54:07 UTC",
    "updated_date": "2024-09-04 22:45:38 UTC"
  },
  {
    "arxiv_id": "2403.13196v2",
    "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
    "authors": [
      "Masih Eskandar",
      "Tooba Imtiaz",
      "Zifeng Wang",
      "Jennifer Dy"
    ],
    "abstract": "The performance of deep models, including Vision Transformers, is known to be\nvulnerable to adversarial attacks. Many existing defenses against these\nattacks, such as adversarial training, rely on full-model fine-tuning to induce\nrobustness in the models. These defenses require storing a copy of the entire\nmodel, that can have billions of parameters, for each task. At the same time,\nparameter-efficient prompt tuning is used to adapt large transformer-based\nmodels to downstream tasks without the need to save large copies. In this\npaper, we examine parameter-efficient prompt tuning of Vision Transformers for\ndownstream tasks under the lens of robustness. We show that previous\nadversarial defense methods, when applied to the prompt tuning paradigm, suffer\nfrom gradient obfuscation and are vulnerable to adaptive attacks. We introduce\nADAPT, a novel framework for performing adaptive adversarial training in the\nprompt tuning paradigm. Our method achieves competitive robust accuracy of ~40%\nw.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1%\nof the number of parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (2025)",
    "pdf_url": "http://arxiv.org/pdf/2403.13196v2",
    "published_date": "2024-03-19 23:13:40 UTC",
    "updated_date": "2025-02-07 18:04:48 UTC"
  },
  {
    "arxiv_id": "2403.13193v1",
    "title": "A Study of Vulnerability Repair in JavaScript Programs with Large Language Models",
    "authors": [
      "Tan Khang Le",
      "Saba Alimadadi",
      "Steven Y. Ko"
    ],
    "abstract": "In recent years, JavaScript has become the most widely used programming\nlanguage, especially in web development. However, writing secure JavaScript\ncode is not trivial, and programmers often make mistakes that lead to security\nvulnerabilities in web applications. Large Language Models (LLMs) have\ndemonstrated substantial advancements across multiple domains, and their\nevolving capabilities indicate their potential for automatic code generation\nbased on a required specification, including automatic bug fixing. In this\nstudy, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and\nfixing security vulnerabilities in JavaScript programs. We also investigate the\nimpact of context in a prompt on directing LLMs to produce a correct patch of\nvulnerable JavaScript code. Our experiments on real-world software\nvulnerabilities show that while LLMs are promising in automatic program repair\nof JavaScript code, achieving a correct bug fix often requires an appropriate\namount of context in the prompt.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "camera-ready version accepted to the short paper track at WWW'24",
    "pdf_url": "http://arxiv.org/pdf/2403.13193v1",
    "published_date": "2024-03-19 23:04:03 UTC",
    "updated_date": "2024-03-19 23:04:03 UTC"
  },
  {
    "arxiv_id": "2403.13178v1",
    "title": "Fast Value Tracking for Deep Reinforcement Learning",
    "authors": [
      "Frank Shih",
      "Faming Liang"
    ],
    "abstract": "Reinforcement learning (RL) tackles sequential decision-making problems by\ncreating agents that interacts with their environment. However, existing\nalgorithms often view these problem as static, focusing on point estimates for\nmodel parameters to maximize expected rewards, neglecting the stochastic\ndynamics of agent-environment interactions and the critical role of uncertainty\nquantification. Our research leverages the Kalman filtering paradigm to\nintroduce a novel and scalable sampling algorithm called Langevinized Kalman\nTemporal-Difference (LKTD) for deep reinforcement learning. This algorithm,\ngrounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently\ndraws samples from the posterior distribution of deep neural network\nparameters. Under mild conditions, we prove that the posterior samples\ngenerated by the LKTD algorithm converge to a stationary distribution. This\nconvergence not only enables us to quantify uncertainties associated with the\nvalue function and model parameters but also allows us to monitor these\nuncertainties during policy updates throughout the training phase. The LKTD\nalgorithm paves the way for more robust and adaptable reinforcement learning\napproaches.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13178v1",
    "published_date": "2024-03-19 22:18:19 UTC",
    "updated_date": "2024-03-19 22:18:19 UTC"
  },
  {
    "arxiv_id": "2403.13150v2",
    "title": "On Training Survival Models with Scoring Rules",
    "authors": [
      "Philipp Kopper",
      "David Rügamer",
      "Raphael Sonabend",
      "Bernd Bischl",
      "Andreas Bender"
    ],
    "abstract": "Scoring rules are an established way of comparing predictive performances\nacross model classes. In the context of survival analysis, they require\nadaptation in order to accommodate censoring. This work investigates using\nscoring rules for model training rather than evaluation. Doing so, we establish\na general framework for training survival models that is model agnostic and can\nlearn event time distributions parametrically or non-parametrically. In\naddition, our framework is not restricted to any specific scoring rule. While\nwe focus on neural network-based implementations, we also provide\nproof-of-concept implementations using gradient boosting, generalized additive\nmodels, and trees. Empirical comparisons on synthetic and real-world data\nindicate that scoring rules can be successfully incorporated into model\ntraining and yield competitive predictive performance with established\ntime-to-event models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13150v2",
    "published_date": "2024-03-19 20:58:38 UTC",
    "updated_date": "2024-11-13 16:46:23 UTC"
  },
  {
    "arxiv_id": "2403.13134v1",
    "title": "Robust NAS under adversarial training: benchmark, theory, and beyond",
    "authors": [
      "Yongtao Wu",
      "Fanghui Liu",
      "Carl-Johann Simon-Gabriel",
      "Grigorios G Chrysos",
      "Volkan Cevher"
    ],
    "abstract": "Recent developments in neural architecture search (NAS) emphasize the\nsignificance of considering robust architectures against malicious data.\nHowever, there is a notable absence of benchmark evaluations and theoretical\nguarantees for searching these robust architectures, especially when\nadversarial training is considered. In this work, we aim to address these two\nchallenges, making twofold contributions. First, we release a comprehensive\ndata set that encompasses both clean accuracy and robust accuracy for a vast\narray of adversarially trained networks from the NAS-Bench-201 search space on\nimage datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep\nlearning theory, we establish a generalization theory for searching\narchitecture in terms of clean accuracy and robust accuracy under\nmulti-objective adversarial training. We firmly believe that our benchmark and\ntheoretical insights will significantly benefit the NAS community through\nreliable reproducibility, efficient assessment, and theoretical foundation,\nparticularly in the pursuit of robust architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13134v1",
    "published_date": "2024-03-19 20:10:23 UTC",
    "updated_date": "2024-03-19 20:10:23 UTC"
  },
  {
    "arxiv_id": "2403.13130v1",
    "title": "Self-generated Replay Memories for Continual Neural Machine Translation",
    "authors": [
      "Michele Resta",
      "Davide Bacciu"
    ],
    "abstract": "Modern Neural Machine Translation systems exhibit strong performance in\nseveral different languages and are constantly improving. Their ability to\nlearn continuously is, however, still severely limited by the catastrophic\nforgetting issue. In this work, we leverage a key property of encoder-decoder\nTransformers, i.e. their generative ability, to propose a novel approach to\ncontinually learning Neural Machine Translation systems. We show how this can\neffectively learn on a stream of experiences comprising different languages, by\nleveraging a replay memory populated by using the model itself as a generator\nof parallel sentences. We empirically demonstrate that our approach can\ncounteract catastrophic forgetting without requiring explicit memorization of\ntraining data. Code will be publicly available upon publication. Code:\nhttps://github.com/m-resta/sg-rep",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13130v1",
    "published_date": "2024-03-19 19:59:54 UTC",
    "updated_date": "2024-03-19 19:59:54 UTC"
  },
  {
    "arxiv_id": "2403.15465v1",
    "title": "Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms",
    "authors": [
      "Yuchao Li",
      "Dimitri Bertsekas"
    ],
    "abstract": "In this paper we consider a transformer with an $n$-gram structure, such as\nthe one underlying ChatGPT. The transformer provides next word probabilities,\nwhich can be used to generate word sequences. We consider methods for computing\nword sequences that are highly likely, based on these probabilities. Computing\nthe optimal (i.e., most likely) word sequence starting with a given initial\nstate is an intractable problem, so we propose methods to compute highly likely\nsequences of $N$ words in time that is a low order polynomial in $N$ and in the\nvocabulary size of the $n$-gram. These methods are based on the rollout\napproach from approximate dynamic programming, a form of single policy\niteration, which can improve the performance of any given heuristic policy. In\nour case we use a greedy heuristic that generates as next word one that has the\nhighest probability. We show with analysis, examples, and computational\nexperimentation that our methods are capable of generating highly likely\nsequences with a modest increase in computation over the greedy heuristic.\nWhile our analysis and experiments are focused on Markov chains of the type\narising in transformer and ChatGPT-like models, our methods apply to general\nfinite-state Markov chains, and related inference applications of Hidden Markov\nModels (HMM), where Viterbi decoding is used extensively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15465v1",
    "published_date": "2024-03-19 19:58:46 UTC",
    "updated_date": "2024-03-19 19:58:46 UTC"
  },
  {
    "arxiv_id": "2403.13125v1",
    "title": "Probabilistic Circuits with Constraints via Convex Optimization",
    "authors": [
      "Soroush Ghandi",
      "Benjamin Quost",
      "Cassio de Campos"
    ],
    "abstract": "This work addresses integrating probabilistic propositional logic constraints\ninto the distribution encoded by a probabilistic circuit (PC). PCs are a class\nof tractable models that allow efficient computations (such as conditional and\nmarginal probabilities) while achieving state-of-the-art performance in some\ndomains. The proposed approach takes both a PC and constraints as inputs, and\noutputs a new PC that satisfies the constraints. This is done efficiently via\nconvex optimization without the need to retrain the entire model. Empirical\nevaluations indicate that the combination of constraints and PCs can have\nmultiple use cases, including the improvement of model performance under scarce\nor incomplete data, as well as the enforcement of machine learning fairness\nmeasures into the model without compromising model fitness. We believe that\nthese ideas will open possibilities for multiple other applications involving\nthe combination of logics and deep probabilistic models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13125v1",
    "published_date": "2024-03-19 19:55:38 UTC",
    "updated_date": "2024-03-19 19:55:38 UTC"
  },
  {
    "arxiv_id": "2403.13111v1",
    "title": "Deep learning with noisy labels in medical prediction problems: a scoping review",
    "authors": [
      "Yishu Wei",
      "Yu Deng",
      "Cong Sun",
      "Mingquan Lin",
      "Hongmei Jiang",
      "Yifan Peng"
    ],
    "abstract": "Objectives: Medical research faces substantial challenges from noisy labels\nattributed to factors like inter-expert variability and machine-extracted\nlabels. Despite this, the adoption of label noise management remains limited,\nand label noise is largely ignored. To this end, there is a critical need to\nconduct a scoping review focusing on the problem space. This scoping review\naims to comprehensively review label noise management in deep learning-based\nmedical prediction problems, which includes label noise detection, label noise\nhandling, and evaluation. Research involving label uncertainty is also\nincluded.\n  Methods: Our scoping review follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4\ndatabases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar.\nOur search terms include \"noisy label AND medical / healthcare / clinical\",\n\"un-certainty AND medical / healthcare / clinical\", and \"noise AND medical /\nhealthcare / clinical\".\n  Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A\nseries of practical questions in medical research are investigated. These\ninclude the sources of label noise, the impact of label noise, the detection of\nlabel noise, label noise handling techniques, and their evaluation.\nCategorization of both label noise detection methods and handling techniques\nare provided.\n  Discussion: From a methodological perspective, we observe that the medical\ncommunity has been up to date with the broader deep-learning community, given\nthat most techniques have been evaluated on medical data. We recommend\nconsidering label noise as a standard element in medical research, even if it\nis not dedicated to handling noisy labels. Initial experiments can start with\neasy-to-implement methods, such as noise-robust loss functions, weighting, and\ncurriculum learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13111v1",
    "published_date": "2024-03-19 19:24:00 UTC",
    "updated_date": "2024-03-19 19:24:00 UTC"
  },
  {
    "arxiv_id": "2403.13106v1",
    "title": "Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data",
    "authors": [
      "Divyansh Singhvi",
      "Andrej Erkelens",
      "Raghav Jain",
      "Diganta Misra",
      "Naomi Saphra"
    ],
    "abstract": "Measuring nonlinear feature interaction is an established approach to\nunderstanding complex patterns of attribution in many models. In this paper, we\nuse Shapley Taylor interaction indices (STII) to analyze the impact of\nunderlying data structure on model representations in a variety of modalities,\ntasks, and architectures. Considering linguistic structure in masked and\nauto-regressive language models (MLMs and ALMs), we find that STII increases\nwithin idiomatic expressions and that MLMs scale STII with syntactic distance,\nrelying more on syntax in their nonlinear structure than ALMs do. Our speech\nmodel findings reflect the phonetic principal that the openness of the oral\ncavity determines how much a phoneme varies based on its context. Finally, we\nstudy image classifiers and illustrate that feature interactions intuitively\nreflect object boundaries. Our wide range of results illustrates the benefits\nof interdisciplinary work and domain expertise in interpretability research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13106v1",
    "published_date": "2024-03-19 19:13:22 UTC",
    "updated_date": "2024-03-19 19:13:22 UTC"
  },
  {
    "arxiv_id": "2403.13101v3",
    "title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks",
    "authors": [
      "Zheng Lin",
      "Guanqiao Qu",
      "Wei Wei",
      "Xianhao Chen",
      "Kin K. Leung"
    ],
    "abstract": "The increasing complexity of deep neural networks poses significant barriers\nto democratizing them to resource-limited edge devices. To address this\nchallenge, split federated learning (SFL) has emerged as a promising solution\nby of floading the primary training workload to a server via model partitioning\nwhile enabling parallel training among edge devices. However, although system\noptimization substantially influences the performance of SFL under\nresource-constrained systems, the problem remains largely uncharted. In this\npaper, we provide a convergence analysis of SFL which quantifies the impact of\nmodel splitting (MS) and client-side model aggregation (MA) on the learning\nperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, a\nnovel resource-adaptive SFL framework, to expedite SFL under\nresource-constrained edge computing systems. Specifically, AdaptSFL adaptively\ncontrols client-side MA and MS to balance communication-computing latency and\ntraining convergence. Extensive simulations across various datasets validate\nthat our proposed AdaptSFL framework takes considerably less time to achieve a\ntarget accuracy than benchmarks, demonstrating the effectiveness of the\nproposed strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13101v3",
    "published_date": "2024-03-19 19:05:24 UTC",
    "updated_date": "2024-05-22 07:10:12 UTC"
  },
  {
    "arxiv_id": "2403.13097v1",
    "title": "Simple Ingredients for Offline Reinforcement Learning",
    "authors": [
      "Edoardo Cetin",
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "Alessandro Lazaric",
      "Yann Ollivier",
      "Ahmed Touati"
    ],
    "abstract": "Offline reinforcement learning algorithms have proven effective on datasets\nhighly connected to the target downstream task. Yet, leveraging a novel testbed\n(MOOD) in which trajectories come from heterogeneous sources, we show that\nexisting methods struggle with diverse data: their performance considerably\ndeteriorates as data collected for related but different tasks is simply added\nto the offline buffer. In light of this finding, we conduct a large empirical\nstudy where we formulate and test several hypotheses to explain this failure.\nSurprisingly, we find that scale, more than algorithmic considerations, is the\nkey factor influencing performance. We show that simple methods like AWAC and\nIQL with increased network size overcome the paradoxical failure modes from the\ninclusion of additional data in MOOD, and notably outperform prior\nstate-of-the-art algorithms on the canonical D4RL benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13097v1",
    "published_date": "2024-03-19 18:57:53 UTC",
    "updated_date": "2024-03-19 18:57:53 UTC"
  },
  {
    "arxiv_id": "2403.13091v1",
    "title": "JaxUED: A simple and useable UED library in Jax",
    "authors": [
      "Samuel Coward",
      "Michael Beukman",
      "Jakob Foerster"
    ],
    "abstract": "We present JaxUED, an open-source library providing minimal dependency\nimplementations of modern Unsupervised Environment Design (UED) algorithms in\nJax. JaxUED leverages hardware acceleration to obtain on the order of 100x\nspeedups compared to prior, CPU-based implementations. Inspired by CleanRL, we\nprovide fast, clear, understandable, and easily modifiable implementations,\nwith the aim of accelerating research into UED. This paper describes our\nlibrary and contains baseline results. Code can be found at\nhttps://github.com/DramaCow/jaxued.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13091v1",
    "published_date": "2024-03-19 18:40:50 UTC",
    "updated_date": "2024-03-19 18:40:50 UTC"
  },
  {
    "arxiv_id": "2404.04268v1",
    "title": "The Use of Generative Search Engines for Knowledge Work and Complex Tasks",
    "authors": [
      "Siddharth Suri",
      "Scott Counts",
      "Leijie Wang",
      "Chacha Chen",
      "Mengting Wan",
      "Tara Safavi",
      "Jennifer Neville",
      "Chirag Shah",
      "Ryen W. White",
      "Reid Andersen",
      "Georg Buscher",
      "Sathish Manivannan",
      "Nagu Rangan",
      "Longqi Yang"
    ],
    "abstract": "Until recently, search engines were the predominant method for people to\naccess online information. The recent emergence of large language models (LLMs)\nhas given machines new capabilities such as the ability to generate new digital\nartifacts like text, images, code etc., resulting in a new tool, a generative\nsearch engine, which combines the capabilities of LLMs with a traditional\nsearch engine. Through the empirical analysis of Bing Copilot (Bing Chat), one\nof the first publicly available generative search engines, we analyze the types\nand complexity of tasks that people use Bing Copilot for compared to Bing\nSearch. Findings indicate that people use the generative search engine for more\nknowledge work tasks that are higher in cognitive complexity than were commonly\ndone with a traditional search engine.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.SI",
      "J.4"
    ],
    "primary_category": "cs.IR",
    "comment": "32 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.04268v1",
    "published_date": "2024-03-19 18:17:46 UTC",
    "updated_date": "2024-03-19 18:17:46 UTC"
  },
  {
    "arxiv_id": "2403.13078v2",
    "title": "HuLP: Human-in-the-Loop for Prognosis",
    "authors": [
      "Muhammad Ridzuan",
      "Mai Kassem",
      "Numan Saeed",
      "Ikboljon Sobirov",
      "Mohammad Yaqub"
    ],
    "abstract": "This paper introduces HuLP, a Human-in-the-Loop for Prognosis model designed\nto enhance the reliability and interpretability of prognostic models in\nclinical contexts, especially when faced with the complexities of missing\ncovariates and outcomes. HuLP offers an innovative approach that enables human\nexpert intervention, empowering clinicians to interact with and correct models'\npredictions, thus fostering collaboration between humans and AI models to\nproduce more accurate prognosis. Additionally, HuLP addresses the challenges of\nmissing data by utilizing neural networks and providing a tailored methodology\nthat effectively handles missing data. Traditional methods often struggle to\ncapture the nuanced variations within patient populations, leading to\ncompromised prognostic predictions. HuLP imputes missing covariates based on\nimaging features, aligning more closely with clinician workflows and enhancing\nreliability. We conduct our experiments on two real-world, publicly available\nmedical datasets to demonstrate the superiority and competitiveness of HuLP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13078v2",
    "published_date": "2024-03-19 18:15:15 UTC",
    "updated_date": "2024-07-09 12:24:50 UTC"
  },
  {
    "arxiv_id": "2403.15464v1",
    "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction",
    "authors": [
      "Hejie Cui",
      "Zhuocheng Shen",
      "Jieyu Zhang",
      "Hui Shao",
      "Lianhui Qin",
      "Joyce C. Ho",
      "Carl Yang"
    ],
    "abstract": "Electronic health records (EHRs) contain valuable patient data for\nhealth-related prediction tasks, such as disease prediction. Traditional\napproaches rely on supervised learning methods that require large labeled\ndatasets, which can be expensive and challenging to obtain. In this study, we\ninvestigate the feasibility of applying Large Language Models (LLMs) to convert\nstructured patient visit data (e.g., diagnoses, labs, prescriptions) into\nnatural language narratives. We evaluate the zero-shot and few-shot performance\nof LLMs using various EHR-prediction-oriented prompting strategies.\nFurthermore, we propose a novel approach that utilizes LLM agents with\ndifferent roles: a predictor agent that makes predictions and generates\nreasoning processes and a critic agent that analyzes incorrect predictions and\nprovides guidance for improving the reasoning of the predictor agent. Our\nresults demonstrate that with the proposed approach, LLMs can achieve decent\nfew-shot performance compared to traditional supervised learning methods in\nEHR-based disease predictions, suggesting its potential for health-oriented\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "J.3; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15464v1",
    "published_date": "2024-03-19 18:10:13 UTC",
    "updated_date": "2024-03-19 18:10:13 UTC"
  },
  {
    "arxiv_id": "2403.12961v1",
    "title": "TexTile: A Differentiable Metric for Texture Tileability",
    "authors": [
      "Carlos Rodriguez-Pardo",
      "Dan Casas",
      "Elena Garces",
      "Jorge Lopez-Moreno"
    ],
    "abstract": "We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "68T07 (Primary) 68T45, 68U05 (Secondary)",
      "I.2.6; I.4.10; I.3.3; I.5.4; I.5.1; I.3.7; I.3.8; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Project page: https://mslab.es/projects/TexTile/",
    "pdf_url": "http://arxiv.org/pdf/2403.12961v1",
    "published_date": "2024-03-19 17:59:09 UTC",
    "updated_date": "2024-03-19 17:59:09 UTC"
  },
  {
    "arxiv_id": "2403.12959v1",
    "title": "WHAC: World-grounded Humans and Cameras",
    "authors": [
      "Wanqi Yin",
      "Zhongang Cai",
      "Ruisi Wang",
      "Fanzhou Wang",
      "Chen Wei",
      "Haiyi Mei",
      "Weiye Xiao",
      "Zhitao Yang",
      "Qingping Sun",
      "Atsushi Yamashita",
      "Ziwei Liu",
      "Lei Yang"
    ],
    "abstract": "Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Homepage: https://wqyin.github.io/projects/WHAC/",
    "pdf_url": "http://arxiv.org/pdf/2403.12959v1",
    "published_date": "2024-03-19 17:58:02 UTC",
    "updated_date": "2024-03-19 17:58:02 UTC"
  },
  {
    "arxiv_id": "2403.13041v4",
    "title": "Provable Privacy with Non-Private Pre-Processing",
    "authors": [
      "Yaxi Hu",
      "Amartya Sanyal",
      "Bernhard Schölkopf"
    ],
    "abstract": "When analysing Differentially Private (DP) machine learning pipelines, the\npotential privacy cost of data-dependent pre-processing is frequently\noverlooked in privacy accounting. In this work, we propose a general framework\nto evaluate the additional privacy cost incurred by non-private data-dependent\npre-processing algorithms. Our framework establishes upper bounds on the\noverall privacy guarantees by utilising two new technical notions: a variant of\nDP termed Smooth DP and the bounded sensitivity of the pre-processing\nalgorithms. In addition to the generic framework, we provide explicit overall\nprivacy guarantees for multiple data-dependent pre-processing algorithms, such\nas data imputation, quantization, deduplication and PCA, when used in\ncombination with several DP algorithms. Notably, this framework is also simple\nto implement, allowing direct integration into existing DP pipelines.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13041v4",
    "published_date": "2024-03-19 17:54:49 UTC",
    "updated_date": "2024-06-21 08:51:29 UTC"
  },
  {
    "arxiv_id": "2403.12952v2",
    "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
    "authors": [
      "Elaine Sui",
      "Xiaohan Wang",
      "Serena Yeung-Levy"
    ],
    "abstract": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.12952v2",
    "published_date": "2024-03-19 17:54:34 UTC",
    "updated_date": "2024-12-10 22:53:16 UTC"
  },
  {
    "arxiv_id": "2403.12943v2",
    "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers",
    "authors": [
      "Vidhi Jain",
      "Maria Attarian",
      "Nikhil J Joshi",
      "Ayzaan Wahid",
      "Danny Driess",
      "Quan Vuong",
      "Pannag R Sanketi",
      "Pierre Sermanet",
      "Stefan Welker",
      "Christine Chan",
      "Igor Gilitschenski",
      "Yonatan Bisk",
      "Debidatta Dwibedi"
    ],
    "abstract": "Large-scale multi-task robotic manipulation systems often rely on text to\nspecify the task. In this work, we explore whether a robot can learn by\nobserving humans. To do so, the robot must understand a person's intent and\nperform the inferred task despite differences in the embodiments and\nenvironments. We introduce Vid2Robot, an end-to-end video-conditioned policy\nthat takes human videos demonstrating manipulation tasks as input and produces\nrobot actions. Our model is trained with a large dataset of prompt video-robot\ntrajectory pairs to learn unified representations of human and robot actions\nfrom videos. Vid2Robot uses cross-attention transformer layers between video\nfeatures and the current robot state to produce the actions and perform the\nsame task as shown in the video. We use auxiliary contrastive losses to align\nthe prompt and robot video representations for better policies. We evaluate\nVid2Robot on real-world robots and observe over 20% improvement over BC-Z when\nusing human prompt videos. Further, we also show cross-object motion transfer\nability that enables video-conditioned policies to transfer a motion observed\non one object in the prompt video to another object in the robot's own\nenvironment. Videos available at https://vid2robot.github.io",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science & Systems (RSS) 2024. https://vid2robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.12943v2",
    "published_date": "2024-03-19 17:47:37 UTC",
    "updated_date": "2024-08-27 23:15:11 UTC"
  },
  {
    "arxiv_id": "2403.12936v1",
    "title": "Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models",
    "authors": [
      "Joana Ribeiro de Faria",
      "Huiyuan Xie",
      "Felix Steffek"
    ],
    "abstract": "Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12936v1",
    "published_date": "2024-03-19 17:43:08 UTC",
    "updated_date": "2024-03-19 17:43:08 UTC"
  },
  {
    "arxiv_id": "2403.13040v2",
    "title": "Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping",
    "authors": [
      "Hang Jung Ling",
      "Salomé Bru",
      "Julia Puig",
      "Florian Vixège",
      "Simon Mendez",
      "Franck Nicoud",
      "Pierre-Yves Courand",
      "Olivier Bernard",
      "Damien Garcia"
    ],
    "abstract": "Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments",
    "pdf_url": "http://arxiv.org/pdf/2403.13040v2",
    "published_date": "2024-03-19 17:35:17 UTC",
    "updated_date": "2024-06-27 17:27:13 UTC"
  },
  {
    "arxiv_id": "2403.12918v1",
    "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts",
    "authors": [
      "Sai Ashish Somayajula",
      "Youwei Liang",
      "Abhishek Singh",
      "Li Zhang",
      "Pengtao Xie"
    ],
    "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing\n(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses\nsignificant challenges such as instability and overfitting. Previous methods\ntackle these issues by finetuning a strategically chosen subnetwork on a\ndownstream task, while keeping the remaining weights fixed to the pretrained\nweights. However, they rely on a suboptimal criteria for sub-network selection,\nleading to suboptimal solutions. To address these limitations, we propose a\nregularization method based on attention-guided weight mixup for finetuning\nPLMs. Our approach represents each network weight as a mixup of task-specific\nweight and pretrained weight, controlled by a learnable attention parameter,\nproviding finer control over sub-network selection. Furthermore, we employ a\nbi-level optimization (BLO) based framework on two separate splits of the\ntraining dataset, improving generalization and combating overfitting. We\nvalidate the efficacy of our proposed method through extensive experiments,\ndemonstrating its superiority over previous methods, particularly in the\ncontext of finetuning PLMs on low-resource datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11\n  tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.12918v1",
    "published_date": "2024-03-19 17:21:29 UTC",
    "updated_date": "2024-03-19 17:21:29 UTC"
  },
  {
    "arxiv_id": "2403.12910v1",
    "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
    "authors": [
      "Lucy Xiaoyang Shi",
      "Zheyuan Hu",
      "Tony Z. Zhao",
      "Archit Sharma",
      "Karl Pertsch",
      "Jianlan Luo",
      "Sergey Levine",
      "Chelsea Finn"
    ],
    "abstract": "Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://yay-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.12910v1",
    "published_date": "2024-03-19 17:08:24 UTC",
    "updated_date": "2024-03-19 17:08:24 UTC"
  },
  {
    "arxiv_id": "2403.12900v1",
    "title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference",
    "authors": [
      "Baolin Li",
      "Yankai Jiang",
      "Vijay Gadepally",
      "Devesh Tiwari"
    ],
    "abstract": "The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12900v1",
    "published_date": "2024-03-19 16:53:53 UTC",
    "updated_date": "2024-03-19 16:53:53 UTC"
  },
  {
    "arxiv_id": "2403.12891v1",
    "title": "Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types",
    "authors": [
      "Rui Liu",
      "Amisha Bhaskar",
      "Pratap Tokekar"
    ],
    "abstract": "In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12891v1",
    "published_date": "2024-03-19 16:40:57 UTC",
    "updated_date": "2024-03-19 16:40:57 UTC"
  },
  {
    "arxiv_id": "2403.12869v2",
    "title": "Regularization in Spider-Style Strategy Discovery and Schedule Construction",
    "authors": [
      "Filip Bártek",
      "Karel Chvalovský",
      "Martin Suda"
    ],
    "abstract": "To achieve the best performance, automatic theorem provers often rely on\nschedules of diverse proving strategies to be tried out (either sequentially or\nin parallel) on a given problem. In this paper, we report on a large-scale\nexperiment with discovering strategies for the Vampire prover, targeting the\nFOF fragment of the TPTP library and constructing a schedule for it, based on\nthe ideas of Andrei Voronkov's system Spider. We examine the process from\nvarious angles, discuss the difficulty (or ease) of obtaining a strong Vampire\nschedule for the CASC competition, and establish how well a schedule can be\nexpected to generalize to unseen problems and what factors influence this\nproperty.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 8 figures; updated cosmetically for publication in IJCAR\n  2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2403.12869v2",
    "published_date": "2024-03-19 16:12:25 UTC",
    "updated_date": "2024-07-09 16:00:10 UTC"
  },
  {
    "arxiv_id": "2404.04267v17",
    "title": "What AIs are not Learning (and Why)",
    "authors": [
      "Mark Stefik"
    ],
    "abstract": "Today's robots do not learn the general skills needed for such services as\nproviding home care, being nursing assistants, or doing household chores.\nAddressing such aspirational goals requires improving how AIs and robots are\ncreated. Today's mainstream AIs are not created by agents learning from\nexperiences doing real world tasks and interacting with people. They do not\nlearn by sensing, acting, doing experiments, and collaborating. This paper\ninvestigates what aspirational service robots will need to know. It recommends\ndeveloping experiential (robotic) foundation models (FMs) for bootstrapping\nthem.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.04267v17",
    "published_date": "2024-03-19 16:06:27 UTC",
    "updated_date": "2024-12-30 22:52:24 UTC"
  },
  {
    "arxiv_id": "2403.12853v3",
    "title": "FlexiFly: Interfacing the Physical World with Foundation Models Empowered by Reconfigurable Drone Systems",
    "authors": [
      "Minghui Zhao",
      "Junxi Xia",
      "Kaiyuan Hou",
      "Yanchen Liu",
      "Stephen Xia",
      "Xiaofan Jiang"
    ],
    "abstract": "Foundation models (FM) have shown immense human-like capabilities for\ngenerating digital media. However, foundation models that can freely sense,\ninteract, and actuate the physical domain is far from being realized. This is\ndue to 1) requiring dense deployments of sensors to fully cover and analyze\nlarge spaces, while 2) events often being localized to small areas, making it\ndifficult for FMs to pinpoint relevant areas of interest relevant to the\ncurrent task. We propose FlexiFly, a platform that enables FMs to ``zoom in''\nand analyze relevant areas with higher granularity to better understand the\nphysical environment and carry out tasks. FlexiFly accomplishes by introducing\n1) a novel image segmentation technique that aids in identifying relevant\nlocations and 2) a modular and reconfigurable sensing and actuation drone\nplatform that FMs can actuate to ``zoom in'' with relevant sensors and\nactuators. We demonstrate through real smart home deployments that FlexiFly\nenables FMs and LLMs to complete diverse tasks up to $85\\%$ more successfully.\nFlexiFly is critical step towards FMs and LLMs that can naturally interface\nwith the physical world.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper is accepted by ACM SenSys 2025. The published version is\n  https://doi.org/10.1145/3715014.3722081 in ACM Digital Library",
    "pdf_url": "http://arxiv.org/pdf/2403.12853v3",
    "published_date": "2024-03-19 15:57:32 UTC",
    "updated_date": "2025-03-05 22:11:38 UTC"
  },
  {
    "arxiv_id": "2403.12823v1",
    "title": "Answer Set Programming for Flexible Payroll Management",
    "authors": [
      "Benjamin Callewaert",
      "Joost Vennekens"
    ],
    "abstract": "Payroll management is a critical business task that is subject to a large\nnumber of rules, which vary widely between companies, sectors, and countries.\nMoreover, the rules are often complex and change regularly. Therefore, payroll\nmanagement systems must be flexible in design. In this paper, we suggest an\napproach based on a flexible Answer Set Programming (ASP) model and an\neasy-to-read tabular representation based on the Decision Model and Notation\n(DMN) standard. It allows HR consultants to represent complex rules without the\nneed for a software engineer, and to ultimately design payroll systems for a\nvariety of different scenarios. We show how the multi-shot solving capabilities\nof the clingo ASP system can be used to reach the performance that is necessary\nto handle real-world instances.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
    "pdf_url": "http://arxiv.org/pdf/2403.12823v1",
    "published_date": "2024-03-19 15:24:49 UTC",
    "updated_date": "2024-03-19 15:24:49 UTC"
  },
  {
    "arxiv_id": "2403.12821v2",
    "title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer",
    "authors": [
      "Dongyeong Hwang",
      "Hyunju Kim",
      "Sunwoo Kim",
      "Kijung Shin"
    ],
    "abstract": "The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "CVPR 2024 Camera-Ready",
    "pdf_url": "http://arxiv.org/pdf/2403.12821v2",
    "published_date": "2024-03-19 15:21:10 UTC",
    "updated_date": "2024-03-21 10:02:39 UTC"
  },
  {
    "arxiv_id": "2403.12816v1",
    "title": "Re-identification from histopathology images",
    "authors": [
      "Jonathan Ganz",
      "Jonas Ammeling",
      "Samir Jabari",
      "Katharina Breininger",
      "Marc Aubreville"
    ],
    "abstract": "In numerous studies, deep learning algorithms have proven their potential for\nthe analysis of histopathology images, for example, for revealing the subtypes\nof tumors or the primary origin of metastases. These models require large\ndatasets for training, which must be anonymized to prevent possible patient\nidentity leaks. This study demonstrates that even relatively simple deep\nlearning algorithms can re-identify patients in large histopathology datasets\nwith substantial accuracy. We evaluated our algorithms on two TCIA datasets\nincluding lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).\nWe also demonstrate the algorithm's performance on an in-house dataset of\nmeningioma tissue. We predicted the source patient of a slide with F1 scores of\n50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31\n% on our meningioma dataset. Based on our findings, we formulated a risk\nassessment scheme to estimate the risk to the patient's privacy prior to\npublication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.12816v1",
    "published_date": "2024-03-19 15:15:19 UTC",
    "updated_date": "2024-03-19 15:15:19 UTC"
  },
  {
    "arxiv_id": "2403.12809v1",
    "title": "Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models",
    "authors": [
      "Zhixue Zhao",
      "Nikolaos Aletras"
    ],
    "abstract": "In many real natural language processing application scenarios, practitioners\nnot only aim to maximize predictive performance but also seek faithful\nexplanations for the model predictions. Rationales and importance distribution\ngiven by feature attribution methods (FAs) provide insights into how different\nparts of the input contribute to a prediction. Previous studies have explored\nhow different factors affect faithfulness, mainly in the context of monolingual\nEnglish models. On the other hand, the differences in FA faithfulness between\nmultilingual and monolingual models have yet to be explored. Our extensive\nexperiments, covering five languages and five popular FAs, show that FA\nfaithfulness varies between multilingual and monolingual models. We find that\nthe larger the multilingual model, the less faithful the FAs are compared to\nits counterpart monolingual models.Our further analysis shows that the\nfaithfulness disparity is potentially driven by the differences between model\ntokenizers. Our code is available:\nhttps://github.com/casszhao/multilingual-faith.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2403.12809v1",
    "published_date": "2024-03-19 15:07:22 UTC",
    "updated_date": "2024-03-19 15:07:22 UTC"
  },
  {
    "arxiv_id": "2403.12805v1",
    "title": "Contextual Moral Value Alignment Through Context-Based Aggregation",
    "authors": [
      "Pierre Dognin",
      "Jesus Rios",
      "Ronny Luss",
      "Inkit Padhi",
      "Matthew D Riemer",
      "Miao Liu",
      "Prasanna Sattigeri",
      "Manish Nagireddy",
      "Kush R. Varshney",
      "Djallel Bouneffouf"
    ],
    "abstract": "Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12805v1",
    "published_date": "2024-03-19 15:06:53 UTC",
    "updated_date": "2024-03-19 15:06:53 UTC"
  },
  {
    "arxiv_id": "2403.12799v1",
    "title": "Investigating Text Shortening Strategy in BERT: Truncation vs Summarization",
    "authors": [
      "Mirza Alim Mutasodirin",
      "Radityo Eko Prasojo"
    ],
    "abstract": "The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 13th International Conference on Advanced Computer Science and\n  Information Systems (ICACSIS 2021)",
    "pdf_url": "http://arxiv.org/pdf/2403.12799v1",
    "published_date": "2024-03-19 15:01:14 UTC",
    "updated_date": "2024-03-19 15:01:14 UTC"
  },
  {
    "arxiv_id": "2403.12777v2",
    "title": "Discover and Mitigate Multiple Biased Subgroups in Image Classifiers",
    "authors": [
      "Zeliang Zhang",
      "Mingqian Feng",
      "Zhiheng Li",
      "Chenliang Xu"
    ],
    "abstract": "Machine learning models can perform well on in-distribution data but often\nfail on biased subgroups that are underrepresented in the training data,\nhindering the robustness of models for reliable applications. Such subgroups\nare typically unknown due to the absence of subgroup labels. Discovering biased\nsubgroups is the key to understanding models' failure modes and further\nimproving models' robustness. Most previous works of subgroup discovery make an\nimplicit assumption that models only underperform on a single biased subgroup,\nwhich does not hold on in-the-wild data where multiple biased subgroups exist.\n  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),\na novel method to address a more challenging but also more practical problem of\ndiscovering multiple biased subgroups in image classifiers. Our approach\ndecomposes the image features into multiple components that represent multiple\nsubgroups. This decomposition is achieved via a bilinear dimension reduction\nmethod, Partial Least Square (PLS), guided by useful supervision from the image\nclassifier. We further interpret the semantic meaning of each subgroup\ncomponent by generating natural language descriptions using vision-language\nfoundation models. Finally, DIM mitigates multiple biased subgroups\nsimultaneously via two strategies, including the data- and model-centric\nstrategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate\nthe effectiveness of DIM in discovering and mitigating multiple biased\nsubgroups. Furthermore, DIM uncovers the failure modes of the classifier on\nHard ImageNet, showcasing its broader applicability to understanding model bias\nin image classifiers. The code is available at\nhttps://github.com/ZhangAIPI/DIM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Code is available at https://github.com/ZhangAIPI/DIM",
    "pdf_url": "http://arxiv.org/pdf/2403.12777v2",
    "published_date": "2024-03-19 14:44:54 UTC",
    "updated_date": "2024-03-20 19:18:27 UTC"
  },
  {
    "arxiv_id": "2403.12748v1",
    "title": "Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection",
    "authors": [
      "Matheus A. Cerqueira",
      "Flávia Sprenger",
      "Bernardo C. A. Teixeira",
      "Alexandre X. Falcão"
    ],
    "abstract": "Brain tumor image segmentation is a challenging research topic in which\ndeep-learning models have presented the best results. However, the traditional\nway of training those models from many pre-annotated images leaves several\nunanswered questions. Hence methodologies, such as Feature Learning from Image\nMarkers (FLIM), have involved an expert in the learning loop to reduce human\neffort in data annotation and build models sufficiently deep for a given\nproblem. FLIM has been successfully used to create encoders, estimating the\nfilters of all convolutional layers from patches centered at marker voxels. In\nthis work, we present Multi-Step (MS) FLIM - a user-assisted approach to\nestimating and selecting the most relevant filters from multiple FLIM\nexecutions. MS-FLIM is used only for the first convolutional layer, and the\nresults already indicate improvement over FLIM. For evaluation, we build a\nsimple U-shaped encoder-decoder network, named sU-Net, for glioblastoma\nsegmentation using T1Gd and FLAIR MRI scans, varying the encoder's training\nmethod, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared\nthese sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two\ndatasets. The results show that the sU-Net based on MS-FLIM outperforms the\nother training methods and achieves effectiveness within the standard\ndeviations of the SOTA models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 68T45"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, 2 tables, 24 references, manuscript of\n  conference paper",
    "pdf_url": "http://arxiv.org/pdf/2403.12748v1",
    "published_date": "2024-03-19 14:11:26 UTC",
    "updated_date": "2024-03-19 14:11:26 UTC"
  },
  {
    "arxiv_id": "2403.12730v1",
    "title": "What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks",
    "authors": [
      "Kacper Sokol",
      "Julia E. Vogt"
    ],
    "abstract": "Despite significant progress, evaluation of explainable artificial\nintelligence remains elusive and challenging. In this paper we propose a\nfine-grained validation framework that is not overly reliant on any one facet\nof these sociotechnical systems, and that recognises their inherent modular\nstructure: technical building blocks, user-facing explanatory artefacts and\nsocial communication protocols. While we concur that user studies are\ninvaluable in assessing the quality and effectiveness of explanation\npresentation and delivery strategies from the explainees' perspective in a\nparticular deployment context, the underlying explanation generation mechanisms\nrequire a separate, predominantly algorithmic validation strategy that accounts\nfor the technical and human-centred desiderata of their (numerical) outputs.\nSuch a comprehensive sociotechnical utility-based evaluation framework could\nallow to systematically reason about the properties and downstream influence of\ndifferent building blocks from which explainable artificial intelligence\nsystems are composed -- accounting for a diverse range of their engineering and\nsocial aspects -- in view of the anticipated use case.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)",
    "pdf_url": "http://arxiv.org/pdf/2403.12730v1",
    "published_date": "2024-03-19 13:45:34 UTC",
    "updated_date": "2024-03-19 13:45:34 UTC"
  },
  {
    "arxiv_id": "2403.12723v2",
    "title": "Python Fuzzing for Trustworthy Machine Learning Frameworks",
    "authors": [
      "Ilya Yegorov",
      "Eli Kobrin",
      "Darya Parygina",
      "Alexey Vishnyakov",
      "Andrey Fedotov"
    ],
    "abstract": "Ensuring the security and reliability of machine learning frameworks is\ncrucial for building trustworthy AI-based systems. Fuzzing, a popular technique\nin secure software development lifecycle (SSDLC), can be used to develop secure\nand robust software. Popular machine learning frameworks such as PyTorch and\nTensorFlow are complex and written in multiple programming languages including\nC/C++ and Python. We propose a dynamic analysis pipeline for Python projects\nusing the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus\nminimization, crash triaging, and coverage collection. Crash triaging and\nseverity estimation are important steps to ensure that the most critical\nvulnerabilities are addressed promptly. Furthermore, the proposed pipeline is\nintegrated in GitLab CI. To identify the most vulnerable parts of the machine\nlearning frameworks, we analyze their potential attack surfaces and develop\nfuzz targets for PyTorch, TensorFlow, and related projects such as h5py.\nApplying our dynamic analysis pipeline to these targets, we were able to\ndiscover 3 new bugs and propose fixes for them.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12723v2",
    "published_date": "2024-03-19 13:41:11 UTC",
    "updated_date": "2024-12-23 12:23:54 UTC"
  },
  {
    "arxiv_id": "2403.12706v1",
    "title": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
    "authors": [
      "Shanchuan Lin",
      "Xiao Yang"
    ],
    "abstract": "We present AnimateDiff-Lightning for lightning-fast video generation. Our\nmodel uses progressive adversarial diffusion distillation to achieve new\nstate-of-the-art in few-step video generation. We discuss our modifications to\nadapt it for the video modality. Furthermore, we propose to simultaneously\ndistill the probability flow of multiple base diffusion models, resulting in a\nsingle distilled motion module with broader style compatibility. We are pleased\nto release our distilled AnimateDiff-Lightning model for the community's use.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12706v1",
    "published_date": "2024-03-19 13:08:54 UTC",
    "updated_date": "2024-03-19 13:08:54 UTC"
  },
  {
    "arxiv_id": "2403.13731v2",
    "title": "Emotion Recognition Using Transformers with Masked Learning",
    "authors": [
      "Seongjae Min",
      "Junseok Yang",
      "Sangjun Lim",
      "Junyong Lee",
      "Sangwon Lee",
      "Sejoon Lim"
    ],
    "abstract": "In recent years, deep learning has achieved innovative advancements in\nvarious fields, including the analysis of human emotions and behaviors.\nInitiatives such as the Affective Behavior Analysis in-the-wild (ABAW)\ncompetition have been particularly instrumental in driving research in this\narea by providing diverse and challenging datasets that enable precise\nevaluation of complex emotional states. This study leverages the Vision\nTransformer (ViT) and Transformer models to focus on the estimation of\nValence-Arousal (VA), which signifies the positivity and intensity of emotions,\nrecognition of various facial expressions, and detection of Action Units (AU)\nrepresenting fundamental muscle movements. This approach transcends traditional\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based\nmethods, proposing a new Transformer-based framework that maximizes the\nunderstanding of temporal and spatial features. The core contributions of this\nresearch include the introduction of a learning technique through random frame\nmasking and the application of Focal loss adapted for imbalanced data,\nenhancing the accuracy and applicability of emotion and behavior analysis in\nreal-world settings. This approach is expected to contribute to the advancement\nof emotional computing and deep learning methodologies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13731v2",
    "published_date": "2024-03-19 12:26:53 UTC",
    "updated_date": "2024-03-23 06:31:11 UTC"
  },
  {
    "arxiv_id": "2403.12678v2",
    "title": "Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights",
    "authors": [
      "Maksym Taranukhin",
      "Sahithya Ravi",
      "Gabor Lukacs",
      "Evangelos Milios",
      "Vered Shwartz"
    ],
    "abstract": "The Canadian air travel sector has seen a significant increase in flight\ndelays, cancellations, and other issues concerning passenger rights.\nRecognizing this demand, we present a chatbot to assist passengers and educate\nthem about their rights. Our system breaks a complex user input into simple\nqueries which are used to retrieve information from a collection of documents\ndetailing air travel regulations. The most relevant passages from these\ndocuments are presented along with links to the original documents and the\ngenerated queries, enabling users to dissect and leverage the information for\ntheir unique circumstances. The system successfully overcomes two predominant\nchallenges: understanding complex user inputs, and delivering accurate answers,\nfree of hallucinations, that passengers can rely on for making informed\ndecisions. A user study comparing the chatbot to a Google search demonstrated\nthe chatbot's usefulness and ease of use. Beyond the primary goal of providing\naccurate and timely information to air passengers regarding their rights, we\nhope that this system will also enable further research exploring the tradeoff\nbetween the user-friendly conversational interface of chatbots and the accuracy\nof retrieval systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to The Natural Legal Language Processing Workshop at EMNLP\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12678v2",
    "published_date": "2024-03-19 12:24:20 UTC",
    "updated_date": "2024-10-15 19:58:13 UTC"
  },
  {
    "arxiv_id": "2403.12672v1",
    "title": "Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine",
    "authors": [
      "Kaiji Sekimoto",
      "Muneki Yasuda"
    ],
    "abstract": "Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for\nsemi-supervised anomaly detection, where they are trained using only normal\ndata points. In GBRBM-based anomaly detection, normal and anomalous data are\nclassified based on a score that is identical to an energy function of the\nmarginal GBRBM. However, the classification threshold is difficult to set to an\nappropriate value, as this score cannot be interpreted. In this study, we\npropose a measure that improves score's interpretability based on its\ncumulative distribution, and establish a guideline for setting the threshold\nusing the interpretable measure. The results of numerical experiments show that\nthe guideline is reasonable when setting the threshold solely using normal data\npoints. Moreover, because identifying the measure involves computationally\ninfeasible evaluation of the minimum score value, we also propose an evaluation\nmethod for the minimum score based on simulated annealing, which is widely used\nfor optimization problems. The proposed evaluation method was also validated\nusing numerical experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12672v1",
    "published_date": "2024-03-19 12:13:52 UTC",
    "updated_date": "2024-03-19 12:13:52 UTC"
  },
  {
    "arxiv_id": "2403.12671v1",
    "title": "Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering",
    "authors": [
      "Jakub Res",
      "Ivan Homoliak",
      "Martin Perešíni",
      "Aleš Smrčka",
      "Kamil Malinka",
      "Petr Hanacek"
    ],
    "abstract": "AI assistants for coding are on the rise. However one of the reasons\ndevelopers and companies avoid harnessing their full potential is the\nquestionable security of the generated code. This paper first reviews the\ncurrent state-of-the-art and identifies areas for improvement on this issue.\nThen, we propose a systematic approach based on prompt-altering methods to\nachieve better code security of (even proprietary black-box) AI-based code\ngenerators such as GitHub Copilot, while minimizing the complexity of the\napplication from the user point-of-view, the computational resources, and\noperational costs. In sum, we propose and evaluate three prompt altering\nmethods: (1) scenario-specific, (2) iterative, and (3) general clause, while we\ndiscuss their combination. Contrary to the audit of code security, the latter\ntwo of the proposed methods require no expert knowledge from the user. We\nassess the effectiveness of the proposed methods on the GitHub Copilot using\nthe OpenVPN project in realistic scenarios, and we demonstrate that the\nproposed methods reduce the number of insecure generated code samples by up to\n16\\% and increase the number of secure code by up to 8\\%. Since our approach\ndoes not require access to the internals of the AI models, it can be in general\napplied to any AI-based code synthesizer, not only GitHub Copilot.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12671v1",
    "published_date": "2024-03-19 12:13:33 UTC",
    "updated_date": "2024-03-19 12:13:33 UTC"
  },
  {
    "arxiv_id": "2403.12664v1",
    "title": "Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making",
    "authors": [
      "Anna Kozak",
      "Dominik Kędzierski",
      "Jakub Piwko",
      "Malwina Wojewoda",
      "Katarzyna Woźnica"
    ],
    "abstract": "In many applications, model ensembling proves to be better than a single\npredictive model. Hence, it is the most common post-processing technique in\nAutomated Machine Learning (AutoML). The most popular frameworks use ensembles\nat the expense of reducing the interpretability of the final models. In our\nwork, we propose cattleia - an application that deciphers the ensembles for\nregression, multiclass, and binary classification tasks. This tool works with\nmodels built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The\ngiven ensemble is analyzed from different perspectives. We conduct a predictive\nperformance investigation through evaluation metrics of the ensemble and its\ncomponent models. We extend the validation perspective by introducing new\nmeasures to assess the diversity and complementarity of the model predictions.\nMoreover, we apply explainable artificial intelligence (XAI) techniques to\nexamine the importance of variables. Summarizing obtained insights, we can\ninvestigate and adjust the weights with a modification tool to tune the\nensemble in the desired way. The application provides the aforementioned\naspects through dedicated interactive visualizations, making it accessible to a\ndiverse audience. We believe the cattleia can support users in decision-making\nand deepen the comprehension of AutoML frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12664v1",
    "published_date": "2024-03-19 11:56:21 UTC",
    "updated_date": "2024-03-19 11:56:21 UTC"
  },
  {
    "arxiv_id": "2403.12660v3",
    "title": "ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems",
    "authors": [
      "Pengyue Jia",
      "Yejing Wang",
      "Zhaocheng Du",
      "Xiangyu Zhao",
      "Yichao Wang",
      "Bo Chen",
      "Wanyu Wang",
      "Huifeng Guo",
      "Ruiming Tang"
    ],
    "abstract": "Deep Recommender Systems (DRS) are increasingly dependent on a large number\nof feature fields for more precise recommendations. Effective feature selection\nmethods are consequently becoming critical for further enhancing the accuracy\nand optimizing storage efficiencies to align with the deployment demands. This\nresearch area, particularly in the context of DRS, is nascent and faces three\ncore challenges. Firstly, variant experimental setups across research papers\noften yield unfair comparisons, obscuring practical insights. Secondly, the\nexisting literature's lack of detailed analysis on selection attributes, based\non large-scale datasets and a thorough comparison among selection techniques\nand DRS backbones, restricts the generalizability of findings and impedes\ndeployment on DRS. Lastly, research often focuses on comparing the peak\nperformance achievable by feature selection methods, an approach that is\ntypically computationally infeasible for identifying the optimal\nhyperparameters and overlooks evaluating the robustness and stability of these\nmethods. To bridge these gaps, this paper presents ERASE, a comprehensive\nbEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation\nof eleven feature selection methods, covering both traditional and deep\nlearning approaches, across four public datasets, private industrial datasets,\nand a real-world commercial platform, achieving significant enhancement. Our\ncode is available online for ease of reproduction.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12660v3",
    "published_date": "2024-03-19 11:49:35 UTC",
    "updated_date": "2024-06-19 12:48:25 UTC"
  },
  {
    "arxiv_id": "2404.00019v1",
    "title": "Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap",
    "authors": [
      "Sule Tekkesinoglu",
      "Azra Habibovic",
      "Lars Kunze"
    ],
    "abstract": "Given the uncertainty surrounding how existing explainability methods for\nautonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough\ninvestigation is imperative to determine the contexts requiring explanations\nand suitable interaction strategies. A comprehensive review becomes crucial to\nassess the alignment of current approaches with the varied interests and\nexpectations within the AV ecosystem. This study presents a review to discuss\nthe complexities associated with explanation generation and presentation to\nfacilitate the development of more effective and inclusive explainable AV\nsystems. Our investigation led to categorising existing literature into three\nprimary topics: explanatory tasks, explanatory information, and explanatory\ninformation communication. Drawing upon our insights, we have proposed a\ncomprehensive roadmap for future research centred on (i) knowing the\ninterlocutor, (ii) generating timely explanations, (ii) communicating\nhuman-friendly explanations, and (iv) continuous learning. Our roadmap is\nunderpinned by principles of responsible research and innovation, emphasising\nthe significance of diverse explanation requirements. To effectively tackle the\nchallenges associated with implementing explainable AV systems, we have\ndelineated various research directions, including the development of\nprivacy-preserving data integration, ethical frameworks, real-time analytics,\nhuman-centric interaction design, and enhanced cross-disciplinary\ncollaborations. By exploring these research directions, the study aims to guide\nthe development and deployment of explainable AVs, informed by a holistic\nunderstanding of user needs, technological advancements, regulatory compliance,\nand ethical considerations, thereby ensuring safer and more trustworthy\nautonomous driving experiences.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00019v1",
    "published_date": "2024-03-19 11:43:41 UTC",
    "updated_date": "2024-03-19 11:43:41 UTC"
  },
  {
    "arxiv_id": "2403.12649v1",
    "title": "InBox: Recommendation with Knowledge Graph using Interest Box Embedding",
    "authors": [
      "Zezhong Xu",
      "Yincen Qu",
      "Wen Zhang",
      "Lei Liang",
      "Huajun Chen"
    ],
    "abstract": "Knowledge graphs (KGs) have become vitally important in modern recommender\nsystems, effectively improving performance and interpretability. Fundamentally,\nrecommender systems aim to identify user interests based on historical\ninteractions and recommend suitable items. However, existing works overlook two\nkey challenges: (1) an interest corresponds to a potentially large set of\nrelated items, and (2) the lack of explicit, fine-grained exploitation of KG\ninformation and interest connectivity. This leads to an inability to reflect\ndistinctions between entities and interests when modeling them in a single way.\nAdditionally, the granularity of concepts in the knowledge graphs used for\nrecommendations tends to be coarse, failing to match the fine-grained nature of\nuser interests. This homogenization limits the precise exploitation of\nknowledge graph data and interest connectivity. To address these limitations,\nwe introduce a novel embedding-based model called InBox. Specifically, various\nknowledge graph entities and relations are embedded as points or boxes, while\nuser interests are modeled as boxes encompassing interaction history.\nRepresenting interests as boxes enables containing collections of item points\nrelated to that interest. We further propose that an interest comprises diverse\nbasic concepts, and box intersection naturally supports concept combination.\nAcross three training steps, InBox significantly outperforms state-of-the-art\nmethods like HAKG and KGIN on recommendation tasks. Further analysis provides\nmeaningful insights into the variable value of different KG data for\nrecommendations. In summary, InBox advances recommender systems through\nbox-based interest and concept modeling for sophisticated knowledge graph\nexploitation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "VLDB 2024 under submission",
    "pdf_url": "http://arxiv.org/pdf/2403.12649v1",
    "published_date": "2024-03-19 11:34:15 UTC",
    "updated_date": "2024-03-19 11:34:15 UTC"
  },
  {
    "arxiv_id": "2403.12631v1",
    "title": "PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic Glove Applications",
    "authors": [
      "Chen Hu",
      "Shirui Lyu",
      "Eojin Rho",
      "Daekyum Kim",
      "Shan Luo",
      "Letizia Gionfrida"
    ],
    "abstract": "Controlling hand exoskeletons to assist individuals with grasping tasks poses\na challenge due to the difficulty in understanding user intentions. We propose\nthat most daily grasping tasks during activities of daily living (ADL) can be\ndeduced by analyzing object geometries (simple and complex) from 3D point\nclouds. The study introduces PointGrasp, a real-time system designed for\nidentifying household scenes semantically, aiming to support and enhance\nassistance during ADL for tailored end-to-end grasping tasks. The system\ncomprises an RGB-D camera with an inertial measurement unit and a\nmicroprocessor integrated into a tendon-driven soft robotic glove. The RGB-D\ncamera processes 3D scenes at a rate exceeding 30 frames per second. The\nproposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple\nand 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies\nand pinpoints reachable objects. This system shows promise in end-to-end\nvision-driven robotic-assisted rehabilitation manual tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2; I.4"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 8 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2403.12631v1",
    "published_date": "2024-03-19 10:59:21 UTC",
    "updated_date": "2024-03-19 10:59:21 UTC"
  },
  {
    "arxiv_id": "2403.12627v2",
    "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code",
    "authors": [
      "Andreas Florath"
    ],
    "abstract": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.12627v2",
    "published_date": "2024-03-19 10:53:40 UTC",
    "updated_date": "2024-04-02 13:54:47 UTC"
  },
  {
    "arxiv_id": "2403.12589v2",
    "title": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting",
    "authors": [
      "Clément Gaspard",
      "Grégoire Passault",
      "Mélodie Daniel",
      "Olivier Ly"
    ],
    "abstract": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12589v2",
    "published_date": "2024-03-19 09:48:18 UTC",
    "updated_date": "2024-12-17 15:28:10 UTC"
  },
  {
    "arxiv_id": "2403.12588v2",
    "title": "Machine Learning of the Prime Distribution",
    "authors": [
      "Alexander Kolpakov",
      "Aidan Rocke"
    ],
    "abstract": "In the present work we use maximum entropy methods to derive several theorems\nin probabilistic number theory, including a version of the Hardy-Ramanujan\nTheorem. We also provide a theoretical argument explaining the experimental\nobservations of Yang-Hui He about the learnability of primes, and posit that\nthe Erd\\H{o}s-Kac law would very unlikely be discovered by current machine\nlearning techniques. Numerical experiments that we perform corroborate our\ntheoretical findings.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT",
      "math.NT",
      "11N05"
    ],
    "primary_category": "cs.IT",
    "comment": "10 pages; parts of arXiv:2308.10817 reworked and amended; author's\n  draft; accepted in PLOS ONE",
    "pdf_url": "http://arxiv.org/pdf/2403.12588v2",
    "published_date": "2024-03-19 09:47:54 UTC",
    "updated_date": "2024-06-02 17:18:40 UTC"
  },
  {
    "arxiv_id": "2403.12574v2",
    "title": "EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks",
    "authors": [
      "Ziming Wang",
      "Ziling Wang",
      "Huaning Li",
      "Lang Qin",
      "Runhao Jiang",
      "De Ma",
      "Huajin Tang"
    ],
    "abstract": "Event cameras, with their high dynamic range and temporal resolution, are\nideally suited for object detection, especially under scenarios with motion\nblur and challenging lighting conditions. However, while most existing\napproaches prioritize optimizing spatiotemporal representations with advanced\ndetection backbones and early aggregation functions, the crucial issue of\nadaptive event sampling remains largely unaddressed. Spiking Neural Networks\n(SNNs), which operate on an event-driven paradigm through sparse spike\ncommunication, emerge as a natural fit for addressing this challenge. In this\nstudy, we discover that the neural dynamics of spiking neurons align closely\nwith the behavior of an ideal temporal event sampler. Motivated by this\ninsight, we propose a novel adaptive sampling module that leverages recurrent\nconvolutional SNNs enhanced with temporal memory, facilitating a fully\nend-to-end learnable framework for event-based detection. Additionally, we\nintroduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to\nregulate potential distribution and address performance degradation encountered\nin spike-based sampling modules. Empirical evaluation on neuromorphic detection\ndatasets demonstrates that our approach outperforms existing state-of-the-art\nspike-based methods with significantly fewer parameters and time steps. For\ninstance, our method yields a 4.4\\% mAP improvement on the Gen1 dataset, while\nrequiring 38\\% fewer parameters and only three time steps. Moreover, the\napplicability and effectiveness of our adaptive sampling methodology extend\nbeyond SNNs, as demonstrated through further validation on conventional\nnon-spiking models. Code is available at https://github.com/Windere/EAS-SNN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12574v2",
    "published_date": "2024-03-19 09:34:11 UTC",
    "updated_date": "2024-08-24 16:04:52 UTC"
  },
  {
    "arxiv_id": "2403.12572v1",
    "title": "Compound Expression Recognition via Multi Model Ensemble",
    "authors": [
      "Jun Yu",
      "Jichao Zhu",
      "Wangyuan Zhu"
    ],
    "abstract": "Compound Expression Recognition (CER) plays a crucial role in interpersonal\ninteractions. Due to the existence of Compound Expressions , human emotional\nexpressions are complex, requiring consideration of both local and global\nfacial expressions to make judgments. In this paper, to address this issue, we\npropose a solution based on ensemble learning methods for Compound Expression\nRecognition. Specifically, our task is classification, where we train three\nexpression classification models based on convolutional networks, Vision\nTransformers, and multi-scale local attention networks. Then, through model\nensemble using late fusion, we merge the outputs of multiple models to predict\nthe final result. Our method achieves high accuracy on RAF-DB and is able to\nrecognize expressions through zero-shot on certain portions of C-EXPR-DB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12572v1",
    "published_date": "2024-03-19 09:30:56 UTC",
    "updated_date": "2024-03-19 09:30:56 UTC"
  },
  {
    "arxiv_id": "2403.12568v1",
    "title": "Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices",
    "authors": [
      "Xueshuo Xie",
      "Haoxu Wang",
      "Zhaolong Jian",
      "Tao Li",
      "Wei Wang",
      "Zhiwei Xu",
      "Guiling Wang"
    ],
    "abstract": "Edge intelligence enables resource-demanding Deep Neural Network (DNN)\ninference without transferring original data, addressing concerns about data\nprivacy in consumer Internet of Things (IoT) devices. For privacy-sensitive\napplications, deploying models in hardware-isolated trusted execution\nenvironments (TEEs) becomes essential. However, the limited secure memory in\nTEEs poses challenges for deploying DNN inference, and alternative techniques\nlike model partitioning and offloading introduce performance degradation and\nsecurity issues. In this paper, we present a novel approach for advanced model\ndeployment in TrustZone that ensures comprehensive privacy preservation during\nmodel inference. We design a memory-efficient management method to support\nmemory-demanding inference in TEEs. By adjusting the memory priority, we\neffectively mitigate memory leakage risks and memory overlap conflicts,\nresulting in 32 lines of code alterations in the trusted operating system.\nAdditionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny\ndeep learning library, and Tinylibm (827 LoCs), a tiny math library, to support\nefficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and\nevaluated it using three well-known lightweight DNN models. The experimental\nresults demonstrate that our design significantly improves inference speed by\n3.13 times and reduces power consumption by over 66.5% compared to non-memory\noptimization method in TEEs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12568v1",
    "published_date": "2024-03-19 09:22:50 UTC",
    "updated_date": "2024-03-19 09:22:50 UTC"
  },
  {
    "arxiv_id": "2403.12563v1",
    "title": "Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service",
    "authors": [
      "Mirza Alim Mutasodirin",
      "Radityo Eko Prasojo",
      "Achmad F. Abka",
      "Hanif Rasyidi"
    ],
    "abstract": "Many NLP researchers rely on free computational services, such as Google\nColab, to fine-tune their Transformer models, causing a limitation for\nhyperparameter optimization (HPO) in long-text classification due to the method\nhaving quadratic complexity and needing a bigger resource. In Indonesian, only\na few works were found on long-text classification using Transformers. Most\nonly use a small amount of data and do not report any HPO. In this study, using\n18k news articles, we investigate which pretrained models are recommended to\nuse based on the output length of the tokenizer. We then compare some hacks to\nshorten and enrich the sequences, which are the removals of stopwords,\npunctuation, low-frequency words, and recurring words. To get a fair\ncomparison, we propose and run an efficient and dynamic HPO procedure that can\nbe done gradually on a limited resource and does not require a long-running\noptimization library. Using the best hack found, we then compare 512, 256, and\n128 tokens length. We find that removing stopwords while keeping punctuation\nand low-frequency words is the best hack. Some of our setups manage to\noutperform taking 512 first tokens using a smaller 128 or 256 first tokens\nwhich manage to represent the same information while requiring less\ncomputational resources. The findings could help developers to efficiently\npursue optimal performance of the models using limited resources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 10th International Conference on Advanced Informatics: Concepts,\n  Theory, and Applications (ICAICTA 2023)",
    "pdf_url": "http://arxiv.org/pdf/2403.12563v1",
    "published_date": "2024-03-19 09:17:25 UTC",
    "updated_date": "2024-03-19 09:17:25 UTC"
  },
  {
    "arxiv_id": "2403.12562v2",
    "title": "PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis",
    "authors": [
      "Raghavendra Selvan",
      "Bob Pepin",
      "Christian Igel",
      "Gabrielle Samuel",
      "Erik B Dam"
    ],
    "abstract": "The recent advances in deep learning (DL) have been accelerated by access to\nlarge-scale data and compute. These large-scale resources have been used to\ntrain progressively larger models which are resource intensive in terms of\ncompute, data, energy, and carbon emissions. These costs are becoming a new\ntype of entry barrier to researchers and practitioners with limited access to\nresources at such scale, particularly in the Global South. In this work, we\ntake a comprehensive look at the landscape of existing DL models for medical\nimage analysis tasks and demonstrate their usefulness in settings where\nresources are limited. To account for the resource consumption of DL models, we\nintroduce a novel measure to estimate the performance per resource unit, which\nwe call the PePR score. Using a diverse family of 131 unique DL architectures\n(spanning 1M to 130M trainable parameters) and three medical image datasets, we\ncapture trends about the performance-resource trade-offs. In applications like\nmedical image analysis, we argue that small-scale, specialized models are\nbetter than striving for large-scale models. Furthermore, we show that using\nexisting pretrained models that are fine-tuned on new data can significantly\nreduce the computational resources and data required compared to training\nmodels from scratch. We hope this work will encourage the community to focus on\nimproving AI equity by developing methods and models with smaller resource\nfootprints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to be published at the Northern Lights Deep Learning\n  Conference (NLDL), 2025. Source code available at\n  https://github.com/saintslab/PePR",
    "pdf_url": "http://arxiv.org/pdf/2403.12562v2",
    "published_date": "2024-03-19 09:17:18 UTC",
    "updated_date": "2024-12-05 11:57:19 UTC"
  },
  {
    "arxiv_id": "2403.12552v1",
    "title": "M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving",
    "authors": [
      "Dongyang Xu",
      "Haokun Li",
      "Qingfan Wang",
      "Ziying Song",
      "Lei Chen",
      "Hanming Deng"
    ],
    "abstract": "End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12552v1",
    "published_date": "2024-03-19 08:54:52 UTC",
    "updated_date": "2024-03-19 08:54:52 UTC"
  },
  {
    "arxiv_id": "2403.15457v3",
    "title": "The Journey to Trustworthy AI: Pursuit of Pragmatic Frameworks",
    "authors": [
      "Mohamad M Nasr-Azadani",
      "Jean-Luc Chatelain"
    ],
    "abstract": "This paper reviews Trustworthy Artificial Intelligence (TAI) and its various\ndefinitions. Considering the principles respected in any society, TAI is often\ncharacterized by a few attributes, some of which have led to confusion in\nregulatory or engineering contexts. We argue against using terms such as\nResponsible or Ethical AI as substitutes for TAI. And to help clarify any\nconfusion, we suggest leaving them behind. Given the subjectivity and\ncomplexity inherent in TAI, developing a universal framework is deemed\ninfeasible. Instead, we advocate for approaches centered on addressing key\nattributes and properties such as fairness, bias, risk, security,\nexplainability, and reliability. We examine the ongoing regulatory landscape,\nwith a focus on initiatives in the EU, China, and the USA. We recognize that\ndifferences in AI regulations based on geopolitical and geographical reasons\npose an additional challenge for multinational companies. We identify risk as a\ncore factor in AI regulation and TAI. For example, as outlined in the EU-AI\nAct, organizations must gauge the risk level of their AI products to act\naccordingly (or risk hefty fines). We compare modalities of TAI implementation\nand how multiple cross-functional teams are engaged in the overall process.\nThus, a brute force approach for enacting TAI renders its efficiency and\nagility, moot. To address this, we introduce our framework\nSet-Formalize-Measure-Act (SFMA). Our solution highlights the importance of\ntransforming TAI-aware metrics, drivers of TAI, stakeholders, and\nbusiness/legal requirements into actual benchmarks or tests. Finally,\nover-regulation driven by panic of powerful AI models can, in fact, harm TAI\ntoo. Based on GitHub user-activity data, in 2023, AI open-source projects rose\nto top projects by contributor account. Enabling innovation in TAI hinges on\nthe independent contributions of the open-source community.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Updates: Added disclaimer about USA's recent U-turn on Trustworthy AI\n  Executive Order. Improved Fairness and Group size in section 6.5. Fixed\n  typos. Added a few new references. Updated title",
    "pdf_url": "http://arxiv.org/pdf/2403.15457v3",
    "published_date": "2024-03-19 08:27:04 UTC",
    "updated_date": "2025-02-12 07:50:06 UTC"
  },
  {
    "arxiv_id": "2403.12533v3",
    "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions",
    "authors": [
      "Daniel Tanneberg",
      "Felix Ocker",
      "Stephan Hasler",
      "Joerg Deigmoeller",
      "Anna Belardinelli",
      "Chao Wang",
      "Heiko Wersing",
      "Bernhard Sendhoff",
      "Michael Gienger"
    ],
    "abstract": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.8; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12533v3",
    "published_date": "2024-03-19 08:09:44 UTC",
    "updated_date": "2025-04-24 14:28:14 UTC"
  },
  {
    "arxiv_id": "2403.12523v1",
    "title": "GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings",
    "authors": [
      "Haochen Li",
      "Di Geng"
    ],
    "abstract": "Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12523v1",
    "published_date": "2024-03-19 07:50:32 UTC",
    "updated_date": "2024-03-19 07:50:32 UTC"
  },
  {
    "arxiv_id": "2404.00018v1",
    "title": "Can AI Outperform Human Experts in Creating Social Media Creatives?",
    "authors": [
      "Eunkyung Park",
      "Raymond K. Wong",
      "Junbum Kwon"
    ],
    "abstract": "Artificial Intelligence has outperformed human experts in functional tasks\nsuch as chess and baduk. How about creative tasks? This paper evaluates AI's\ncapability in the creative domain compared to human experts, which little\nresearch has been conducted so far. We propose a novel Prompt-for-Prompt to\ngenerate social media creatives via prompt augmentation by Large Language\nModels. We take the most popular Instagram posts (with the biggest number of\nlike clicks) in top brands' Instagram accounts to create social media\ncreatives. We give GPT 4 several prompt instructions with text descriptions to\ngenerate the most effective prompts for cutting-edge text-to-image generators:\nMidjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost\nAI's abilities by adding objectives, engagement strategy, lighting and brand\nconsistency for social media image creation. We conduct an extensive human\nevaluation experiment, and find that AI excels human experts, and Midjourney is\nbetter than the other text-to-image generators. Surprisingly, unlike\nconventional wisdom in the social media industry, prompt instruction including\neye-catching shows much poorer performance than those including natural.\nRegarding the type of creatives, AI improves creatives with animals or products\nbut less with real people. Also, AI improves creatives with short text\ndescriptions more than with long text descriptions, because there is more room\nfor AI to augment prompts with shorter descriptions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI",
      "Computation and Language (cs.CL), Artificial Intelligence (cs.AI)"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00018v1",
    "published_date": "2024-03-19 07:41:45 UTC",
    "updated_date": "2024-03-19 07:41:45 UTC"
  },
  {
    "arxiv_id": "2403.13031v2",
    "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
    "authors": [
      "Zhuowen Yuan",
      "Zidi Xiong",
      "Yi Zeng",
      "Ning Yu",
      "Ruoxi Jia",
      "Dawn Song",
      "Bo Li"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13031v2",
    "published_date": "2024-03-19 07:25:02 UTC",
    "updated_date": "2024-07-23 22:56:13 UTC"
  },
  {
    "arxiv_id": "2403.12510v3",
    "title": "Generalized Consistency Trajectory Models for Image Manipulation",
    "authors": [
      "Beomsu Kim",
      "Jaemin Kim",
      "Jeongsol Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12510v3",
    "published_date": "2024-03-19 07:24:54 UTC",
    "updated_date": "2024-10-10 04:04:44 UTC"
  },
  {
    "arxiv_id": "2403.12503v1",
    "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
    "authors": [
      "Sara Abdali",
      "Richard Anarfi",
      "CJ Barberan",
      "Jia He"
    ],
    "abstract": "Large language models (LLMs) have significantly transformed the landscape of\nNatural Language Processing (NLP). Their impact extends across a diverse\nspectrum of tasks, revolutionizing how we approach language understanding and\ngenerations. Nevertheless, alongside their remarkable utility, LLMs introduce\ncritical security and risk considerations. These challenges warrant careful\nexamination to ensure responsible deployment and safeguard against potential\nvulnerabilities. This research paper thoroughly investigates security and\nprivacy concerns related to LLMs from five thematic perspectives: security and\nprivacy concerns, vulnerabilities against adversarial attacks, potential harms\ncaused by misuses of LLMs, mitigation strategies to address these challenges\nwhile identifying limitations of current strategies. Lastly, the paper\nrecommends promising avenues for future research to enhance the security and\nrisk management of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12503v1",
    "published_date": "2024-03-19 07:10:58 UTC",
    "updated_date": "2024-03-19 07:10:58 UTC"
  },
  {
    "arxiv_id": "2403.12488v3",
    "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM",
    "authors": [
      "Yixuan Wu",
      "Yizhou Wang",
      "Shixiang Tang",
      "Wenhao Wu",
      "Tong He",
      "Wanli Ouyang",
      "Philip Torr",
      "Jian Wu"
    ],
    "abstract": "We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot\nobject detection ability of multimodal large language models (MLLMs), such as\nGPT-4V and Gemini. Our approach consists of a detection prompting toolkit\ninspired by high-precision detection priors and a new Chain-of-Thought to\nimplement these prompts. Specifically, the prompts in the toolkit are designed\nto guide the MLLM to focus on regional information (e.g., zooming in), read\ncoordinates according to measure standards (e.g., overlaying rulers and\ncompasses), and infer from the contextual information (e.g., overlaying scene\ngraphs). Building upon these tools, the new detection chain-of-thought can\nautomatically decompose the task into simple subtasks, diagnose the\npredictions, and plan for progressive box refinements. The effectiveness of our\nframework is demonstrated across a spectrum of detection tasks, especially hard\ncases. Compared to existing state-of-the-art methods, GPT-4V with our\nDetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS\nCOCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val\nset for zero-shot referring expression comprehension, +14.5% AP on D-cube\ndescribe object detection FULL setting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12488v3",
    "published_date": "2024-03-19 06:54:33 UTC",
    "updated_date": "2024-07-23 07:14:54 UTC"
  },
  {
    "arxiv_id": "2403.14715v3",
    "title": "Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It",
    "authors": [
      "Guoxuan Xia",
      "Olivier Laurent",
      "Gianni Franchi",
      "Christos-Savvas Bouganis"
    ],
    "abstract": "Label smoothing (LS) is a popular regularisation method for training neural\nnetworks as it is effective in improving test accuracy and is simple to\nimplement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing\nprobability mass to other classes, reducing overfitting. Prior work has\nsuggested that in some cases LS can degrade selective classification (SC) --\nwhere the aim is to reject misclassifications using a model's uncertainty. In\nthis work, we first demonstrate empirically across an extended range of\nlarge-scale tasks and architectures that LS consistently degrades SC. We then\naddress a gap in existing knowledge, providing an explanation for this\nbehaviour by analysing logit-level gradients: LS degrades the uncertainty rank\nordering of correct vs incorrect predictions by suppressing the max logit more\nwhen a prediction is likely to be correct, and less when it is likely to be\nwrong. This elucidates previously reported experimental results where strong\nclassifiers underperform in SC. We then demonstrate the empirical effectiveness\nof post-hoc logit normalisation for recovering lost SC performance caused by\nLS. Furthermore, linking back to our gradient analysis, we again provide an\nexplanation for why such normalisation is effective.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.14715v3",
    "published_date": "2024-03-19 06:46:24 UTC",
    "updated_date": "2025-02-20 15:02:44 UTC"
  },
  {
    "arxiv_id": "2403.12486v2",
    "title": "NTK-Guided Few-Shot Class Incremental Learning",
    "authors": [
      "Jingren Liu",
      "Zhong Ji",
      "Yanwei Pang",
      "YunLong Yu"
    ],
    "abstract": "The proliferation of Few-Shot Class Incremental Learning (FSCIL)\nmethodologies has highlighted the critical challenge of maintaining robust\nanti-amnesia capabilities in FSCIL learners. In this paper, we present a novel\nconceptualization of anti-amnesia in terms of mathematical generalization,\nleveraging the Neural Tangent Kernel (NTK) perspective. Our method focuses on\ntwo key aspects: ensuring optimal NTK convergence and minimizing NTK-related\ngeneralization loss, which serve as the theoretical foundation for cross-task\ngeneralization. To achieve global NTK convergence, we introduce a principled\nmeta-learning mechanism that guides optimization within an expanded network\narchitecture. Concurrently, to reduce the NTK-related generalization loss, we\nsystematically optimize its constituent factors. Specifically, we initiate\nself-supervised pre-training on the base session to enhance NTK-related\ngeneralization potential. These self-supervised weights are then carefully\nrefined through curricular alignment, followed by the application of dual NTK\nregularization tailored specifically for both convolutional and linear layers.\nThrough the combined effects of these measures, our network acquires robust NTK\nproperties, ensuring optimal convergence and stability of the NTK matrix and\nminimizing the NTK-related generalization loss, significantly enhancing its\ntheoretical generalization. On popular FSCIL benchmark datasets, our NTK-FSCIL\nsurpasses contemporary state-of-the-art approaches, elevating end-session\naccuracy by 2.9\\% to 9.3\\%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12486v2",
    "published_date": "2024-03-19 06:43:46 UTC",
    "updated_date": "2024-09-24 12:11:47 UTC"
  },
  {
    "arxiv_id": "2403.12482v2",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "authors": [
      "Xudong Guo",
      "Kaixuan Huang",
      "Jiale Liu",
      "Wenhui Fan",
      "Natalia Vélez",
      "Qingyun Wu",
      "Huazheng Wang",
      "Thomas L. Griffiths",
      "Mengdi Wang"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12482v2",
    "published_date": "2024-03-19 06:39:47 UTC",
    "updated_date": "2024-05-23 06:29:00 UTC"
  },
  {
    "arxiv_id": "2403.15456v3",
    "title": "WoLF: Wide-scope Large Language Model Framework for CXR Understanding",
    "authors": [
      "Seil Kang",
      "Donghyun Kim",
      "Junhyeok Kim",
      "Hyo Kyung Lee",
      "Seong Jae Hwang"
    ],
    "abstract": "Significant methodological strides have been made toward Chest X-ray (CXR)\nunderstanding via modern vision-language models (VLMs), demonstrating\nimpressive Visual Question Answering (VQA) and CXR report generation abilities.\nHowever, existing CXR understanding frameworks still possess several procedural\ncaveats. (1) Previous methods solely use CXR reports, which are insufficient\nfor comprehensive Visual Question Answering (VQA), especially when additional\nhealth-related data like medication history and prior diagnoses are needed. (2)\nPrevious methods use raw CXR reports, which are often arbitrarily structured.\nWhile modern language models can understand various text formats, restructuring\nreports for clearer, organized anatomy-based information could enhance their\nusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize\nlinguistic correctness, lacking the capability to offer nuanced assessments of\nthe generated answers. In this work, to address the aforementioned caveats, we\nintroduce WoLF, a Wide-scope Large Language Model Framework for CXR\nunderstanding. To resolve (1), we capture multi-faceted records of patients,\nwhich are utilized for accurate diagnoses in real-world clinical scenarios.\nSpecifically, we adopt the Electronic Health Records (EHR) to generate\ninstruction-following data suited for CXR understanding. Regarding (2), we\nenhance report generation performance by decoupling knowledge in CXR reports\nbased on anatomical structure even within the attention step via masked\nattention. To address (3), we introduce an AI-evaluation protocol optimized for\nassessing the capabilities of LLM. Through extensive experimental validations,\nWoLF demonstrates superior performance over other models on MIMIC-CXR in the\nAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about\nreport generation (+7.3%p BLEU-1).",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages main paper, 2 pages supplementary",
    "pdf_url": "http://arxiv.org/pdf/2403.15456v3",
    "published_date": "2024-03-19 06:39:23 UTC",
    "updated_date": "2024-03-29 04:38:51 UTC"
  },
  {
    "arxiv_id": "2404.08652v1",
    "title": "Algorithm for AGC index management against crowded radio environment",
    "authors": [
      "Morgane Joly",
      "Fabian Rivière",
      "Éric Renault"
    ],
    "abstract": "This paper describes a receiver that uses an innovative method to predict,\naccording to history of receiver operating metrics (packet lost/well received),\nthe optimum automatic gain control (AGC) index or most appropriate variable\ngain range to be used for next packet reception, anticipating an interferer\nappearing during the payload reception. This allows the receiver to have higher\nimmunity to interferers even if they occur during the gain frozen payload\nreception period whilst still ensuring an optimum sensitivity level. As a\nresult, the method allows setting the receiver gain to get an optimum trade-off\nbetween reception sensitivity and random interferer immunity.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "17 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08652v1",
    "published_date": "2024-03-19 05:42:29 UTC",
    "updated_date": "2024-03-19 05:42:29 UTC"
  },
  {
    "arxiv_id": "2403.12462v1",
    "title": "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks",
    "authors": [
      "Biswadeep Chakraborty",
      "Saibal Mukhopadhyay"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have become an essential paradigm in\nneuroscience and artificial intelligence, providing brain-inspired computation.\nRecent advances in literature have studied the network representations of deep\nneural networks. However, there has been little work that studies\nrepresentations learned by SNNs, especially using unsupervised local learning\nmethods like spike-timing dependent plasticity (STDP). Recent work by\n\\cite{barannikov2021representation} has introduced a novel method to compare\ntopological mappings of learned representations called Representation Topology\nDivergence (RTD). Though useful, this method is engineered particularly for\nfeedforward deep neural networks and cannot be used for recurrent networks like\nRecurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to\nmeasure the difference between distributed representations of RSNN models with\ndifferent learning methods. We propose a novel reformulation of RSNNs using\nfeedforward autoencoder networks with skip connections to help us compute the\nRTD for recurrent networks. Thus, we investigate the learning capabilities of\nRSNN trained using STDP and the role of heterogeneity in the synaptic dynamics\nin learning such representations. We demonstrate that heterogeneous STDP in\nRSNNs yield distinct representations than their homogeneous and surrogate\ngradient-based supervised learning counterparts. Our results provide insights\ninto the potential of heterogeneous SNN models, aiding the development of more\nefficient and biologically plausible hybrid artificial intelligence systems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted in IEEE World Congress on Computational Intelligence (IEEE\n  WCCI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12462v1",
    "published_date": "2024-03-19 05:37:26 UTC",
    "updated_date": "2024-03-19 05:37:26 UTC"
  },
  {
    "arxiv_id": "2403.12459v3",
    "title": "Non-negative Contrastive Learning",
    "authors": [
      "Yifei Wang",
      "Qi Zhang",
      "Yaoyu Guo",
      "Yisen Wang"
    ],
    "abstract": "Deep representations have shown promising performance when transferred to\ndownstream tasks in a black-box manner. Yet, their inherent lack of\ninterpretability remains a significant challenge, as these features are often\nopaque to human understanding. In this paper, we propose Non-negative\nContrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization\n(NMF) aimed at deriving interpretable features. The power of NCL lies in its\nenforcement of non-negativity constraints on features, reminiscent of NMF's\ncapability to extract features that align closely with sample clusters. NCL not\nonly aligns mathematically well with an NMF objective but also preserves NMF's\ninterpretability attributes, resulting in a more sparse and disentangled\nrepresentation compared to standard contrastive learning (CL). Theoretically,\nwe establish guarantees on the identifiability and downstream generalization of\nNCL. Empirically, we show that these advantages enable NCL to outperform CL\nsignificantly on feature disentanglement, feature selection, as well as\ndownstream classification tasks. At last, we show that NCL can be easily\nextended to other learning scenarios and benefit supervised learning as well.\nCode is available at https://github.com/PKU-ML/non_neg.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages. Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12459v3",
    "published_date": "2024-03-19 05:30:50 UTC",
    "updated_date": "2024-04-22 21:28:17 UTC"
  },
  {
    "arxiv_id": "2403.12451v4",
    "title": "End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations",
    "authors": [
      "Lirui Luo",
      "Guoxi Zhang",
      "Hongming Xu",
      "Yaodong Yang",
      "Cong Fang",
      "Qing Li"
    ],
    "abstract": "Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising\nparadigm for explainable decision-making, characterized by the interpretability\nof symbolic policies. NS-RL entails structured state representations for tasks\nwith visual observations, but previous methods cannot refine the structured\nstates with rewards due to a lack of efficiency. Accessibility also remains an\nissue, as extensive domain knowledge is required to interpret symbolic\npolicies. In this paper, we present a neuro-symbolic framework for jointly\nlearning structured states and symbolic policies, whose key idea is to distill\nthe vision foundation model into an efficient perception module and refine it\nduring policy learning. Moreover, we design a pipeline to prompt GPT-4 to\ngenerate textual explanations for the learned policies and decisions,\nsignificantly reducing users' cognitive load to understand the symbolic\npolicies. We verify the efficacy of our approach on nine Atari tasks and\npresent GPT-generated explanations for policies and decisions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2024. Project page: https://ins-rl.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.12451v4",
    "published_date": "2024-03-19 05:21:20 UTC",
    "updated_date": "2024-06-13 06:04:08 UTC"
  },
  {
    "arxiv_id": "2403.12448v1",
    "title": "Do Generated Data Always Help Contrastive Learning?",
    "authors": [
      "Yifei Wang",
      "Jizhe Zhang",
      "Yisen Wang"
    ],
    "abstract": "Contrastive Learning (CL) has emerged as one of the most successful paradigms\nfor unsupervised visual representation learning, yet it often depends on\nintensive manual data augmentations. With the rise of generative models,\nespecially diffusion models, the ability to generate realistic images close to\nthe real data distribution has been well recognized. These generated\nhigh-equality images have been successfully applied to enhance contrastive\nrepresentation learning, a technique termed ``data inflation''. However, we\nfind that the generated data (even from a good diffusion model like DDPM) may\nsometimes even harm contrastive learning. We investigate the causes behind this\nfailure from the perspective of both data inflation and data augmentation. For\nthe first time, we reveal the complementary roles that stronger data inflation\nshould be accompanied by weaker augmentations, and vice versa. We also provide\nrigorous theoretical explanations for these phenomena via deriving its\ngeneralization bounds under data inflation. Drawing from these insights, we\npropose Adaptive Inflation (AdaInf), a purely data-centric strategy without\nintroducing any extra computation cost. On benchmark datasets, AdaInf can bring\nsignificant improvements for various contrastive learning methods. Notably,\nwithout using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10\nwith SimCLR, setting a new record that surpasses many sophisticated methods.\nCode is available at https://github.com/PKU-ML/adainf.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages. Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12448v1",
    "published_date": "2024-03-19 05:17:47 UTC",
    "updated_date": "2024-03-19 05:17:47 UTC"
  },
  {
    "arxiv_id": "2403.12431v1",
    "title": "Geometric Constraints in Deep Learning Frameworks: A Survey",
    "authors": [
      "Vibhas K Vats",
      "David J Crandall"
    ],
    "abstract": "Stereophotogrammetry is an emerging technique of scene understanding. Its\norigins go back to at least the 1800s when people first started to investigate\nusing photographs to measure the physical properties of the world. Since then,\nthousands of approaches have been explored. The classic geometric techniques of\nShape from Stereo is built on using geometry to define constraints on scene and\ncamera geometry and then solving the non-linear systems of equations. More\nrecent work has taken an entirely different approach, using end-to-end deep\nlearning without any attempt to explicitly model the geometry. In this survey,\nwe explore the overlap for geometric-based and deep learning-based frameworks.\nWe compare and contrast geometry enforcing constraints integrated into a deep\nlearning framework for depth estimation or other closely related problems. We\npresent a new taxonomy for prevalent geometry enforcing constraints used in\nmodern deep learning frameworks. We also present insightful observations and\npotential future research directions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "A preprint",
    "pdf_url": "http://arxiv.org/pdf/2403.12431v1",
    "published_date": "2024-03-19 04:41:09 UTC",
    "updated_date": "2024-03-19 04:41:09 UTC"
  },
  {
    "arxiv_id": "2403.12418v4",
    "title": "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model",
    "authors": [
      "Lincan Li",
      "Hanchen Wang",
      "Wenjie Zhang",
      "Adelle Coster"
    ],
    "abstract": "Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,\nand non-stationary, leading to the continuous challenge of spatial-temporal\ngraph learning. In the past few years, various GNN-based methods have been\nproposed to solely focus on mimicking the relationships among node individuals\nof the STG network, ignoring the significance of modeling the intrinsic\nfeatures that exist in STG system over time. In contrast, modern Selective\nState Space Models (SSSMs) present a new approach which treat STG Network as a\nsystem, and meticulously explore the STG system's dynamic state evolution\nacross temporal dimension. In this work, we introduce Spatial-Temporal Graph\nMamba (STG-Mamba) as the first exploration of leveraging the powerful selective\nstate space models for STG learning by treating STG Network as a system, and\nemploying the Spatial-Temporal Selective State Space Module (ST-S3M) to\nprecisely focus on the selected STG latent features. Furthermore, to strengthen\nGNN's ability of modeling STG data under the setting of selective state space\nmodels, we propose Kalman Filtering Graph Neural Networks (KFGN) for\ndynamically integrate and upgrade the STG embeddings from different temporal\ngranularities through a learnable Kalman Filtering statistical theory-based\napproach. Extensive empirical studies are conducted on three benchmark STG\nforecasting datasets, demonstrating the performance superiority and\ncomputational efficiency of STG-Mamba. It not only surpasses existing\nstate-of-the-art methods in terms of STG forecasting performance, but also\neffectively alleviate the computational bottleneck of large-scale graph\nnetworks in reducing the computational cost of FLOPs and test inference time.\nThe implementation code is available at:\n\\url{https://github.com/LincanLi98/STG-Mamba}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12418v4",
    "published_date": "2024-03-19 04:02:57 UTC",
    "updated_date": "2024-05-18 11:58:16 UTC"
  },
  {
    "arxiv_id": "2403.12417v1",
    "title": "On Predictive planning and counterfactual learning in active inference",
    "authors": [
      "Aswin Paul",
      "Takuya Isomura",
      "Adeel Razi"
    ],
    "abstract": "Given the rapid advancement of artificial intelligence, understanding the\nfoundations of intelligent behaviour is increasingly important. Active\ninference, regarded as a general theory of behaviour, offers a principled\napproach to probing the basis of sophistication in planning and\ndecision-making. In this paper, we examine two decision-making schemes in\nactive inference based on 'planning' and 'learning from experience'.\nFurthermore, we also introduce a mixed model that navigates the data-complexity\ntrade-off between these strategies, leveraging the strengths of both to\nfacilitate balanced decision-making. We evaluate our proposed model in a\nchallenging grid-world scenario that requires adaptability from the agent.\nAdditionally, our model provides the opportunity to analyze the evolution of\nvarious parameters, offering valuable insights and contributing to an\nexplainable framework for intelligent decision-making.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.12417v1",
    "published_date": "2024-03-19 04:02:31 UTC",
    "updated_date": "2024-03-19 04:02:31 UTC"
  },
  {
    "arxiv_id": "2403.12406v2",
    "title": "Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion",
    "authors": [
      "Kuang-Da Wang",
      "Wei-Yao Wang",
      "Ping-Chun Hsieh",
      "Wen-Chih Peng"
    ],
    "abstract": "In the dynamic and rapid tactic involvements of turn-based sports, badminton\nstands out as an intrinsic paradigm that requires alter-dependent\ndecision-making of players. While the advancement of learning from offline\nexpert data in sequential decision-making has been witnessed in various\ndomains, how to rally-wise imitate the behaviors of human players from offline\nbadminton matches has remained underexplored. Replicating opponents' behavior\nbenefits players by allowing them to undergo strategic development with\ndirection before matches. However, directly applying existing methods suffers\nfrom the inherent hierarchy of the match and the compounding effect due to the\nturn-based nature of players alternatively taking actions. In this paper, we\npropose RallyNet, a novel hierarchical offline imitation learning model for\nbadminton player behaviors: (i) RallyNet captures players' decision\ndependencies by modeling decision-making processes as a contextual Markov\ndecision process. (ii) RallyNet leverages the experience to generate context as\nthe agent's intent in the rally. (iii) To generate more realistic behavior,\nRallyNet leverages Geometric Brownian Motion (GBM) to model the interactions\nbetween players by introducing a valuable inductive bias for learning player\nbehaviors. In this manner, RallyNet links player intents with interaction\nmodels with GBM, providing an understanding of interactions for sports\nanalytics. We extensively validate RallyNet with the largest available\nreal-world badminton dataset consisting of men's and women's singles,\ndemonstrating its ability to imitate player behaviors. Results reveal\nRallyNet's superiority over offline imitation learning methods and\nstate-of-the-art turn-based approaches, outperforming them by at least 16% in\nmean rule-based agent normalization score. Furthermore, we discuss various\npractical use cases to highlight RallyNet's applicability.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.12406v2",
    "published_date": "2024-03-19 03:34:23 UTC",
    "updated_date": "2024-08-03 06:36:17 UTC"
  },
  {
    "arxiv_id": "2403.12403v2",
    "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
    "authors": [
      "Ayushi Nirmal",
      "Amrita Bhattacharjee",
      "Paras Sheth",
      "Huan Liu"
    ],
    "abstract": "Although social media platforms are a prominent arena for users to engage in\ninterpersonal discussions and express opinions, the facade and anonymity\noffered by social media may allow users to spew hate speech and offensive\ncontent. Given the massive scale of such platforms, there arises a need to\nautomatically identify and flag instances of hate speech. Although several hate\nspeech detection methods exist, most of these black-box methods are not\ninterpretable or explainable by design. To address the lack of\ninterpretability, in this paper, we propose to use state-of-the-art Large\nLanguage Models (LLMs) to extract features in the form of rationales from the\ninput text, to train a base hate speech classifier, thereby enabling faithful\ninterpretability by design. Our framework effectively combines the textual\nunderstanding capabilities of LLMs and the discriminative power of\nstate-of-the-art hate speech classifiers to make these classifiers faithfully\ninterpretable. Our comprehensive evaluation on a variety of English language\nsocial media hate speech datasets demonstrate: (1) the goodness of the\nLLM-extracted rationales, and (2) the surprising retention of detector\nperformance even after training to ensure interpretability. All code and data\nwill be made available at https://github.com/AmritaBh/shield.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready for NAACL WOAH 2024 (Workshop on Online Abuse and\n  Harms). First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2403.12403v2",
    "published_date": "2024-03-19 03:22:35 UTC",
    "updated_date": "2024-05-08 02:47:36 UTC"
  },
  {
    "arxiv_id": "2403.12400v1",
    "title": "Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing",
    "authors": [
      "Zijian Zhao",
      "Tingwei Chen",
      "Fanyi Meng",
      "Hang Li",
      "Xiaoyang Li",
      "Guangxu Zhu"
    ],
    "abstract": "Despite the development of various deep learning methods for Wi-Fi sensing,\npackage loss often results in noncontinuous estimation of the Channel State\nInformation (CSI), which negatively impacts the performance of the learning\nmodels. To overcome this challenge, we propose a deep learning model based on\nBidirectional Encoder Representations from Transformers (BERT) for CSI\nrecovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner\non the target dataset without the need for additional data. Furthermore, unlike\ntraditional interpolation methods that focus on one subcarrier at a time,\nCSI-BERT captures the sequential relationships across different subcarriers.\nExperimental results demonstrate that CSI-BERT achieves lower error rates and\nfaster speed compared to traditional interpolation methods, even when facing\nwith high loss rates. Moreover, by harnessing the recovered CSI obtained from\nCSI-BERT, other deep learning models like Residual Network and Recurrent Neural\nNetwork can achieve an average increase in accuracy of approximately 15\\% in\nWi-Fi sensing tasks. The collected dataset WiGesture and code for our model are\npublicly available at https://github.com/RS2002/CSI-BERT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, accepted by IEEE INFOCOM Deepwireless Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.12400v1",
    "published_date": "2024-03-19 03:16:52 UTC",
    "updated_date": "2024-03-19 03:16:52 UTC"
  },
  {
    "arxiv_id": "2403.12392v1",
    "title": "AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis",
    "authors": [
      "Faisal Qarah"
    ],
    "abstract": "Arabic poetry, with its rich linguistic features and profound cultural\nsignificance, presents a unique challenge to the Natural Language Processing\n(NLP) field. The complexity of its structure and context necessitates advanced\ncomputational models for accurate analysis. In this paper, we introduce\nAraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry\ntext. To demonstrate the effectiveness of the proposed model, we compared\nAraPoemBERT with 5 different Arabic language models on various NLP tasks\nrelated to Arabic poetry. The new model outperformed all other models and\nachieved state-of-the-art results in most of the downstream tasks. AraPoemBERT\nachieved unprecedented accuracy in two out of three novel tasks: poet's gender\nclassification (99.34\\% accuracy), and poetry sub-meter classification (97.79\\%\naccuracy). In addition, the model achieved an accuracy score in poems' rhyme\nclassification (97.73\\% accuracy) which is almost equivalent to the best score\nreported in this study. Moreover, the proposed model significantly outperformed\nprevious work and other comparative models in the tasks of poems' sentiment\nanalysis, achieving an accuracy of 78.95\\%, and poetry meter classification\n(99.03\\% accuracy), while significantly expanding the scope of these two\nproblems. The dataset used in this study, contains more than 2.09 million\nverses collected from online sources, each associated with various attributes\nsuch as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the\neffectiveness of the proposed model in understanding and analyzing Arabic\npoetry, achieving state-of-the-art results in several tasks and outperforming\nprevious works and other language models included in the study. AraPoemBERT\nmodel is publicly available on \\url{https://huggingface.co/faisalq}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 11 figures, not published yet",
    "pdf_url": "http://arxiv.org/pdf/2403.12392v1",
    "published_date": "2024-03-19 02:59:58 UTC",
    "updated_date": "2024-03-19 02:59:58 UTC"
  },
  {
    "arxiv_id": "2403.12391v1",
    "title": "FairSTG: Countering performance heterogeneity via collaborative sample-level optimization",
    "authors": [
      "Gengyu Lin",
      "Zhengyang Zhou",
      "Qihe Huang",
      "Kuo Yang",
      "Shifen Cheng",
      "Yang Wang"
    ],
    "abstract": "Spatiotemporal learning plays a crucial role in mobile computing techniques\nto empower smart cites. While existing research has made great efforts to\nachieve accurate predictions on the overall dataset, they still neglect the\nsignificant performance heterogeneity across samples. In this work, we\ndesignate the performance heterogeneity as the reason for unfair spatiotemporal\nlearning, which not only degrades the practical functions of models, but also\nbrings serious potential risks to real-world urban applications. To fix this\ngap, we propose a model-independent Fairness-aware framework for SpatioTemporal\nGraph learning (FairSTG), which inherits the idea of exploiting advantages of\nwell-learned samples to challenging ones with collaborative mix-up.\nSpecifically, FairSTG consists of a spatiotemporal feature extractor for model\ninitialization, a collaborative representation enhancement for knowledge\ntransfer between well-learned samples and challenging ones, and fairness\nobjectives for immediately suppressing sample-level performance heterogeneity.\nExperiments on four spatiotemporal datasets demonstrate that our FairSTG\nsignificantly improves the fairness quality while maintaining comparable\nforecasting accuracy. Case studies show FairSTG can counter both spatial and\ntemporal performance heterogeneity by our sample-level retrieval and\ncompensation, and our work can potentially alleviate the risks on\nspatiotemporal resource allocation for underrepresented urban regions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review by IEEE Transactions on Mobile Computing",
    "pdf_url": "http://arxiv.org/pdf/2403.12391v1",
    "published_date": "2024-03-19 02:59:50 UTC",
    "updated_date": "2024-03-19 02:59:50 UTC"
  },
  {
    "arxiv_id": "2403.12388v2",
    "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
    "authors": [
      "Ying-Chun Lin",
      "Jennifer Neville",
      "Jack W. Stokes",
      "Longqi Yang",
      "Tara Safavi",
      "Mengting Wan",
      "Scott Counts",
      "Siddharth Suri",
      "Reid Andersen",
      "Xiaofeng Xu",
      "Deepak Gupta",
      "Sujay Kumar Jauhar",
      "Xia Song",
      "Georg Buscher",
      "Saurabh Tiwary",
      "Brent Hecht",
      "Jaime Teevan"
    ],
    "abstract": "Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12388v2",
    "published_date": "2024-03-19 02:57:07 UTC",
    "updated_date": "2024-06-09 00:58:25 UTC"
  },
  {
    "arxiv_id": "2403.12386v1",
    "title": "Pipelined Biomedical Event Extraction Rivaling Joint Learning",
    "authors": [
      "Pengchao Wu",
      "Xuefeng Li",
      "Jinghang Gu",
      "Longhua Qian",
      "Guodong Zhou"
    ],
    "abstract": "Biomedical event extraction is an information extraction task to obtain\nevents from biomedical text, whose targets include the type, the trigger, and\nthe respective arguments involved in an event. Traditional biomedical event\nextraction usually adopts a pipelined approach, which contains trigger\nidentification, argument role recognition, and finally event construction\neither using specific rules or by machine learning. In this paper, we propose\nan n-ary relation extraction method based on the BERT pre-training model to\nconstruct Binding events, in order to capture the semantic information about an\nevent's context and its participants. The experimental results show that our\nmethod achieves promising results on the GE11 and GE13 corpora of the BioNLP\nshared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates\nthat by significantly improving theperformance of Binding events, the overall\nperformance of the pipelined event extraction approach or even exceeds those of\ncurrent joint learning methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12386v1",
    "published_date": "2024-03-19 02:52:58 UTC",
    "updated_date": "2024-03-19 02:52:58 UTC"
  },
  {
    "arxiv_id": "2403.12368v1",
    "title": "Characteristic AI Agents via Large Language Models",
    "authors": [
      "Xi Wang",
      "Hongliang Dai",
      "Shen Gao",
      "Piji Li"
    ],
    "abstract": "The advancement of Large Language Models (LLMs) has led to significant\nenhancements in the performance of chatbot systems. Many researchers have\ndedicated their efforts to the development of bringing characteristics to\nchatbots. While there have been commercial products for developing role-driven\nchatbots using LLMs, it is worth noting that academic research in this area\nremains relatively scarce. Our research focuses on investigating the\nperformance of LLMs in constructing Characteristic AI Agents by simulating\nreal-life individuals across different settings. Current investigations have\nprimarily focused on act on roles with simple profiles. In response to this\nresearch gap, we create a benchmark for the characteristic AI agents task,\nincluding dataset, techniques, and evaluation metrics. A dataset called\n``Character100'' is built for this benchmark, comprising the most-visited\npeople on Wikipedia for language models to role-play. With the constructed\ndataset, we conduct comprehensive assessment of LLMs across various settings.\nIn addition, we devise a set of automatic metrics for quantitative performance\nevaluation. The experimental results underscore the potential directions for\nfurther improvement in the capabilities of LLMs in constructing characteristic\nAI agents. The benchmark is available at\nhttps://github.com/nuaa-nlp/Character100.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2024,The benchmark is available at:\n  https://github.com/nuaa-nlp/Character100",
    "pdf_url": "http://arxiv.org/pdf/2403.12368v1",
    "published_date": "2024-03-19 02:25:29 UTC",
    "updated_date": "2024-03-19 02:25:29 UTC"
  }
]
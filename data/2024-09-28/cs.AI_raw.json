[
  {
    "arxiv_id": "2409.19479v1",
    "title": "Spatial Reasoning and Planning for Deep Embodied Agents",
    "authors": [
      "Shu Ishida"
    ],
    "abstract": "Humans can perform complex tasks with long-term objectives by planning,\nreasoning, and forecasting outcomes of actions. For embodied agents to achieve\nsimilar capabilities, they must gain knowledge of the environment transferable\nto novel scenarios with a limited budget of additional trial and error.\nLearning-based approaches, such as deep RL, can discover and take advantage of\ninherent regularities and characteristics of the application domain from data,\nand continuously improve their performances, however at a cost of large amounts\nof training data. This thesis explores the development of data-driven\ntechniques for spatial reasoning and planning tasks, focusing on enhancing\nlearning efficiency, interpretability, and transferability across novel\nscenarios. Four key contributions are made. 1) CALVIN, a differential planner\nthat learns interpretable models of the world for long-term planning. It\nsuccessfully navigated partially observable 3D environments, such as mazes and\nindoor rooms, by learning the rewards and state transitions from expert\ndemonstrations. 2) SOAP, an RL algorithm that discovers options unsupervised\nfor long-horizon tasks. Options segment a task into subtasks and enable\nconsistent execution of the subtask. SOAP showed robust performances on\nhistory-conditional corridor tasks as well as classical benchmarks such as\nAtari. 3) LangProp, a code optimisation framework using LLMs to solve embodied\nagent problems that require reasoning by treating code as learnable policies.\nThe framework successfully generated interpretable code with comparable or\nsuperior performance to human-written experts in the CARLA autonomous driving\nbenchmark. 4) Voggite, an embodied agent with a vision-to-action transformer\nbackend that solves complex tasks in Minecraft. It achieved third place in the\nMineRL BASALT Competition by identifying action triggers to segment tasks into\nmultiple stages.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "DPhil Thesis - Engineering Science, University of Oxford. Original\n  copy available at\n  https://ora.ox.ac.uk/objects/uuid:19489c19-dc5a-464a-831d-bbf887687c41",
    "pdf_url": "http://arxiv.org/pdf/2409.19479v1",
    "published_date": "2024-09-28 23:05:56 UTC",
    "updated_date": "2024-09-28 23:05:56 UTC"
  },
  {
    "arxiv_id": "2409.19474v2",
    "title": "FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models",
    "authors": [
      "Diego A. B. Moreira",
      "Alef Iury Ferreira",
      "Jhessica Silva",
      "Gabriel Oliveira dos Santos",
      "Luiz Pereira",
      "João Medrado Gondim",
      "Gustavo Bonil",
      "Helena Maia",
      "Nádia da Silva",
      "Simone Tiemi Hashiguti",
      "Jefersson A. dos Santos",
      "Helio Pedrini",
      "Sandra Avila"
    ],
    "abstract": "Despite significant advancements and pervasive use of vision-language models,\na paucity of studies has addressed their ethical implications. These models\ntypically require extensive training data, often from hastily reviewed text and\nimage datasets, leading to highly imbalanced datasets and ethical concerns.\nAdditionally, models initially trained in English are frequently fine-tuned for\nother languages, such as the CLIP model, which can be expanded with more data\nto enhance capabilities but can add new biases. The CAPIVARA, a CLIP-based\nmodel adapted to Portuguese, has shown strong performance in zero-shot tasks.\nIn this paper, we evaluate four different types of discriminatory practices\nwithin visual-language models and introduce FairPIVARA, a method to reduce them\nby removing the most affected dimensions of feature embeddings. The application\nof FairPIVARA has led to a significant reduction of up to 98% in observed\nbiases while promoting a more balanced word distribution within the model. Our\nmodel and code are available at: https://github.com/hiaac-nlp/FairPIVARA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 10 figures. Accepted to 35th British Machine Vision\n  Conference (BMVC 2024), Workshop on Privacy, Fairness, Accountability and\n  Transparency in Computer Vision",
    "pdf_url": "http://arxiv.org/pdf/2409.19474v2",
    "published_date": "2024-09-28 22:49:22 UTC",
    "updated_date": "2024-10-05 00:44:32 UTC"
  },
  {
    "arxiv_id": "2410.07216v1",
    "title": "Evaluating Financial Relational Graphs: Interpretation Before Prediction",
    "authors": [
      "Yingjie Niu",
      "Lanxin Lu",
      "Rian Dolphin",
      "Valerio Poti",
      "Ruihai Dong"
    ],
    "abstract": "Accurate and robust stock trend forecasting has been a crucial and\nchallenging task, as stock price changes are influenced by multiple factors.\nGraph neural network-based methods have recently achieved remarkable success in\nthis domain by constructing stock relationship graphs that reflect internal\nfactors and relationships between stocks. However, most of these methods rely\non predefined factors to construct static stock relationship graphs due to the\nlack of suitable datasets, failing to capture the dynamic changes in stock\nrelationships. Moreover, the evaluation of relationship graphs in these methods\nis often tied to the performance of neural network models on downstream tasks,\nleading to confusion and imprecision. To address these issues, we introduce the\nSPNews dataset, collected based on S\\&P 500 Index stocks, to facilitate the\nconstruction of dynamic relationship graphs. Furthermore, we propose a novel\nset of financial relationship graph evaluation methods that are independent of\ndownstream tasks. By using the relationship graph to explain historical\nfinancial phenomena, we assess its validity before constructing a graph neural\nnetwork, ensuring the graph's effectiveness in capturing relevant financial\nrelationships. Experimental results demonstrate that our evaluation methods can\neffectively differentiate between various financial relationship graphs,\nyielding more interpretable results compared to traditional approaches. We make\nour source code publicly available on GitHub to promote reproducibility and\nfurther research in this area.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG",
      "I.2.4"
    ],
    "primary_category": "q-fin.ST",
    "comment": "Accepted by 2024 ACM International Conference on AI in Finance",
    "pdf_url": "http://arxiv.org/pdf/2410.07216v1",
    "published_date": "2024-09-28 22:43:00 UTC",
    "updated_date": "2024-09-28 22:43:00 UTC"
  },
  {
    "arxiv_id": "2409.19471v2",
    "title": "SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models",
    "authors": [
      "Yi Wu",
      "Zikang Xiong",
      "Yiran Hu",
      "Shreyash S. Iyengar",
      "Nan Jiang",
      "Aniket Bera",
      "Lin Tan",
      "Suresh Jagannathan"
    ],
    "abstract": "Despite significant advancements in large language models (LLMs) that enhance\nrobot agents' understanding and execution of natural language (NL) commands,\nensuring the agents adhere to user-specified constraints remains challenging,\nparticularly for complex commands and long-horizon tasks. To address this\nchallenge, we present three key insights, equivalence voting, constrained\ndecoding, and domain-specific fine-tuning, which significantly enhance LLM\nplanners' capability in handling complex tasks. Equivalence voting ensures\nconsistency by generating and sampling multiple Linear Temporal Logic (LTL)\nformulas from NL commands, grouping equivalent LTL formulas, and selecting the\nmajority group of formulas as the final LTL formula. Constrained decoding then\nuses the generated LTL formula to enforce the autoregressive inference of\nplans, ensuring the generated plans conform to the LTL. Domain-specific\nfine-tuning customizes LLMs to produce safe and efficient plans within specific\ntask domains. Our approach, Safe Efficient LLM Planner (SELP), combines these\ninsights to create LLM planners to generate plans adhering to user commands\nwith high confidence. We demonstrate the effectiveness and generalizability of\nSELP across different robot agents and tasks, including drone navigation and\nrobot manipulation. For drone navigation tasks, SELP outperforms\nstate-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks\nconforming to NL commands) and by 19.8% in plan efficiency. For robot\nmanipulation tasks, SELP achieves 20.4% improvement in safety rate. Our\ndatasets for evaluating NL-to-LTL and robot task planning will be released in\ngithub.com/lt-asset/selp.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  International Conference on Robotics and Automation (ICRA), May 19-23, 2025,\n  Atlanta, USA, and for inclusion in the conference proceeding",
    "pdf_url": "http://arxiv.org/pdf/2409.19471v2",
    "published_date": "2024-09-28 22:33:44 UTC",
    "updated_date": "2025-02-14 02:40:55 UTC"
  },
  {
    "arxiv_id": "2409.19467v2",
    "title": "INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large Language Models and Ensemble Learning",
    "authors": [
      "Pablo Romero",
      "Lifeng Han",
      "Goran Nenadic"
    ],
    "abstract": "Medication Extraction and Mining play an important role in healthcare NLP\nresearch due to its practical applications in hospital settings, such as their\nmapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this\nwork, we investigate state-of-the-art LLMs in text mining tasks on medications\nand their related attributes such as dosage, route, strength, and adverse\neffects. In addition, we explore different ensemble learning methods\n(\\textsc{Stack-Ensemble} and \\textsc{Voting-Ensemble}) to augment the model\nperformances from individual LLMs. Our ensemble learning result demonstrated\nbetter performances than individually fine-tuned base models BERT, RoBERTa,\nRoBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and\nPubMedBERT across general and specific domains. Finally, we build up an entity\nlinking function to map extracted medical terminologies into the SNOMED-CT\ncodes and the British National Formulary (BNF) codes, which are further mapped\nto the Dictionary of Medicines and Devices (dm+d), and ICD. Our model's toolkit\nand desktop applications are publicly available (at\n\\url{https://github.com/HECTA-UoM/ensemble-NER}).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ongoing work, 24 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.19467v2",
    "published_date": "2024-09-28 22:06:06 UTC",
    "updated_date": "2024-12-27 20:53:02 UTC"
  },
  {
    "arxiv_id": "2409.19454v4",
    "title": "See Where You Read with Eye Gaze Tracking and Large Language Model",
    "authors": [
      "Sikai Yang",
      "Gang Yan",
      "Wan Du"
    ],
    "abstract": "Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "J.5; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.19454v4",
    "published_date": "2024-09-28 20:40:18 UTC",
    "updated_date": "2024-12-13 04:44:31 UTC"
  },
  {
    "arxiv_id": "2409.19450v2",
    "title": "Secret Use of Large Language Model (LLM)",
    "authors": [
      "Zhiping Zhang",
      "Chenxinran Shen",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Tianshi Li"
    ],
    "abstract": "The advancements of Large Language Models (LLMs) have decentralized the\nresponsibility for the transparency of AI usage. Specifically, LLM users are\nnow encouraged or required to disclose the use of LLM-generated content for\nvaried types of real-world tasks. However, an emerging phenomenon, users'\nsecret use of LLM, raises challenges in ensuring end users adhere to the\ntransparency requirement. Our study used mixed-methods with an exploratory\nsurvey (125 real-world secret use cases reported) and a controlled experiment\namong 300 users to investigate the contexts and causes behind the secret use of\nLLMs. We found that such secretive behavior is often triggered by certain\ntasks, transcending demographic and personality differences among users. Task\ntypes were found to affect users' intentions to use secretive behavior,\nprimarily through influencing perceived external judgment regarding LLM usage.\nOur results yield important insights for future work on designing interventions\nto encourage more transparent disclosure of the use of LLMs or other AI\ntechnologies.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "26 pages, 3 figures, and accepted at CSCW 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.19450v2",
    "published_date": "2024-09-28 20:31:53 UTC",
    "updated_date": "2024-10-19 19:07:45 UTC"
  },
  {
    "arxiv_id": "2409.19448v1",
    "title": "Advanced Clustering Techniques for Speech Signal Enhancement: A Review and Metanalysis of Fuzzy C-Means, K-Means, and Kernel Fuzzy C-Means Methods",
    "authors": [
      "Abdulhady Abas Abdullah",
      "Aram Mahmood Ahmed",
      "Tarik Rashid",
      "Hadi Veisi",
      "Yassin Hussein Rassul",
      "Bryar Hassan",
      "Polla Fattah",
      "Sabat Abdulhameed Ali",
      "Ahmed S. Shamsaldin"
    ],
    "abstract": "Speech signal processing is a cornerstone of modern communication\ntechnologies, tasked with improving the clarity and comprehensibility of audio\ndata in noisy environments. The primary challenge in this field is the\neffective separation and recognition of speech from background noise, crucial\nfor applications ranging from voice-activated assistants to automated\ntranscription services. The quality of speech recognition directly impacts user\nexperience and accessibility in technology-driven communication. This review\npaper explores advanced clustering techniques, particularly focusing on the\nKernel Fuzzy C-Means (KFCM) method, to address these challenges. Our findings\nindicate that KFCM, compared to traditional methods like K-Means (KM) and Fuzzy\nC-Means (FCM), provides superior performance in handling non-linear and\nnon-stationary noise conditions in speech signals. The most notable outcome of\nthis review is the adaptability of KFCM to various noisy environments, making\nit a robust choice for speech enhancement applications. Additionally, the paper\nidentifies gaps in current methodologies, such as the need for more dynamic\nclustering algorithms that can adapt in real time to changing noise conditions\nwithout compromising speech recognition quality. Key contributions include a\ndetailed comparative analysis of current clustering algorithms and suggestions\nfor further integrating hybrid models that combine KFCM with neural networks to\nenhance speech recognition accuracy. Through this review, we advocate for a\nshift towards more sophisticated, adaptive clustering techniques that can\nsignificantly improve speech enhancement and pave the way for more resilient\nspeech processing systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19448v1",
    "published_date": "2024-09-28 20:21:05 UTC",
    "updated_date": "2024-09-28 20:21:05 UTC"
  },
  {
    "arxiv_id": "2409.19437v4",
    "title": "Strongly-polynomial time and validation analysis of policy gradient methods",
    "authors": [
      "Caleb Ju",
      "Guanghui Lan"
    ],
    "abstract": "This paper proposes a novel termination criterion, termed the advantage gap\nfunction, for finite state and action Markov decision processes (MDP) and\nreinforcement learning (RL). By incorporating this advantage gap function into\nthe design of step size rules and deriving a new linear rate of convergence\nthat is independent of the stationary state distribution of the optimal policy,\nwe demonstrate that policy gradient methods can solve MDPs in\nstrongly-polynomial time. To the best of our knowledge, this is the first time\nthat such strong convergence properties have been established for policy\ngradient methods. Moreover, in the stochastic setting, where only stochastic\nestimates of policy gradients are available, we show that the advantage gap\nfunction provides close approximations of the optimality gap for each\nindividual state and exhibits a sublinear rate of convergence at every state.\nThe advantage gap function can be easily estimated in the stochastic case, and\nwhen coupled with easily computable upper bounds on policy values, they provide\na convenient way to validate the solutions generated by policy gradient\nmethods. Therefore, our developments offer a principled and computable measure\nof optimality for RL, whereas current practice tends to rely on\nalgorithm-to-algorithm or baselines comparisons with no certificate of\noptimality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "math.OC",
      "49K45, 49M05, 90C05, 90C26, 90C40, 90C46"
    ],
    "primary_category": "cs.LG",
    "comment": "Some fixes to notation",
    "pdf_url": "http://arxiv.org/pdf/2409.19437v4",
    "published_date": "2024-09-28 18:56:48 UTC",
    "updated_date": "2025-02-20 04:57:47 UTC"
  },
  {
    "arxiv_id": "2410.03721v1",
    "title": "Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development",
    "authors": [
      "Andrew Katz",
      "Gabriella Coloyan Fleming",
      "Joyce Main"
    ],
    "abstract": "This paper aims to answer one central question: to what extent can\nopen-source generative text models be used in a workflow to approximate\nthematic analysis in social science research? To answer this question, we\npresent the Generative AI-enabled Theme Organization and Structuring (GATOS)\nworkflow, which uses open-source machine learning techniques, natural language\nprocessing tools, and generative text models to facilitate thematic analysis.\nTo establish validity of the method, we present three case studies applying the\nGATOS workflow, leveraging these models and techniques to inductively create\ncodebooks similar to traditional procedures using thematic analysis.\nSpecifically, we investigate the extent to which a workflow comprising\nopen-source models and tools can inductively produce codebooks that approach\nthe known space of themes and sub-themes. To address the challenge of gleaning\ninsights from these texts, we combine open-source generative text models,\nretrieval-augmented generation, and prompt engineering to identify codes and\nthemes in large volumes of text, i.e., generate a qualitative codebook. The\nprocess mimics an inductive coding process that researchers might use in\ntraditional thematic analysis by reading text one unit of analysis at a time,\nconsidering existing codes already in the codebook, and then deciding whether\nor not to generate a new code based on whether the extant codebook provides\nadequate thematic coverage. We demonstrate this workflow using three synthetic\ndatasets from hypothetical organizational research settings: a study of\nteammate feedback in teamwork settings, a study of organizational cultures of\nethical behavior, and a study of employee perspectives about returning to their\noffices after the pandemic. We show that the GATOS workflow is able to identify\nthemes in the text that were used to generate the original synthetic datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03721v1",
    "published_date": "2024-09-28 18:52:16 UTC",
    "updated_date": "2024-09-28 18:52:16 UTC"
  },
  {
    "arxiv_id": "2409.19433v2",
    "title": "RMLR: Extending Multinomial Logistic Regression into General Geometries",
    "authors": [
      "Ziheng Chen",
      "Yue Song",
      "Rui Wang",
      "Xiaojun Wu",
      "Nicu Sebe"
    ],
    "abstract": "Riemannian neural networks, which extend deep learning techniques to\nRiemannian spaces, have gained significant attention in machine learning. To\nbetter classify the manifold-valued features, researchers have started\nextending Euclidean multinomial logistic regression (MLR) into Riemannian\nmanifolds. However, existing approaches suffer from limited applicability due\nto their strong reliance on specific geometric properties. This paper proposes\na framework for designing Riemannian MLR over general geometries, referred to\nas RMLR. Our framework only requires minimal geometric properties, thus\nexhibiting broad applicability and enabling its use with a wide range of\ngeometries. Specifically, we showcase our framework on the Symmetric Positive\nDefinite (SPD) manifold and special orthogonal group, i.e., the set of rotation\nmatrices. On the SPD manifold, we develop five families of SPD MLRs under five\ntypes of power-deformed metrics. On rotation matrices we propose Lie MLR based\non the popular bi-invariant metric. Extensive experiments on different\nRiemannian backbone networks validate the effectiveness of our framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.19433v2",
    "published_date": "2024-09-28 18:38:21 UTC",
    "updated_date": "2024-10-02 09:53:48 UTC"
  },
  {
    "arxiv_id": "2409.19432v3",
    "title": "MicroFlow: An Efficient Rust-Based Inference Engine for TinyML",
    "authors": [
      "Matteo Carnelos",
      "Francesco Pasti",
      "Nicola Bellotto"
    ],
    "abstract": "In recent years, there has been a significant interest in developing machine\nlearning algorithms on embedded systems. This is particularly relevant for bare\nmetal devices in Internet of Things, Robotics, and Industrial applications that\nface limited memory, processing power, and storage, and which require extreme\nrobustness. To address these constraints, we present MicroFlow, an open-source\nTinyML framework for the deployment of Neural Networks (NNs) on embedded\nsystems using the Rust programming language. The compiler-based inference\nengine of MicroFlow, coupled with Rust's memory safety, makes it suitable for\nTinyML applications in critical environments. The proposed framework enables\nthe successful deployment of NNs on highly resource-constrained devices,\nincluding bare-metal 8-bit microcontrollers with only 2kB of RAM. Furthermore,\nMicroFlow is able to use less Flash and RAM memory than other state-of-the-art\nsolutions for deploying NN reference models (i.e. wake-word and person\ndetection), achieving equally accurate but faster inference compared to\nexisting engines on medium-size NNs, and similar performance on bigger ones.\nThe experimental results prove the efficiency and suitability of MicroFlow for\nthe deployment of TinyML models in critical environments where resources are\nparticularly limited.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19432v3",
    "published_date": "2024-09-28 18:34:27 UTC",
    "updated_date": "2025-01-03 21:45:08 UTC"
  },
  {
    "arxiv_id": "2409.19422v2",
    "title": "Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures",
    "authors": [
      "Subash Timilsina",
      "Sagar Shrestha",
      "Xiao Fu"
    ],
    "abstract": "A core task in multi-modal learning is to integrate information from multiple\nfeature spaces (e.g., text and audio), offering modality-invariant essential\nrepresentations of data. Recent research showed that, classical tools such as\n{\\it canonical correlation analysis} (CCA) provably identify the shared\ncomponents up to minor ambiguities, when samples in each modality are generated\nfrom a linear mixture of shared and private components. Such identifiability\nresults were obtained under the condition that the cross-modality samples are\naligned/paired according to their shared information. This work takes a step\nfurther, investigating shared component identifiability from multi-modal linear\nmixtures where cross-modality samples are unaligned. A distribution divergence\nminimization-based loss is proposed, under which a suite of sufficient\nconditions ensuring identifiability of the shared components are derived. Our\nconditions are based on cross-modality distribution discrepancy\ncharacterization and density-preserving transform removal, which are much\nmilder than existing studies relying on independent component analysis. More\nrelaxed conditions are also provided via adding reasonable structural\nconstraints, motivated by available side information in various applications.\nThe identifiability claims are thoroughly validated using synthetic and\nreal-world data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19422v2",
    "published_date": "2024-09-28 17:43:17 UTC",
    "updated_date": "2024-10-01 07:04:04 UTC"
  },
  {
    "arxiv_id": "2409.19417v1",
    "title": "Subject Data Auditing via Source Inference Attack in Cross-Silo Federated Learning",
    "authors": [
      "Jiaxin Li",
      "Marco Arazzi",
      "Antonino Nocera",
      "Mauro Conti"
    ],
    "abstract": "Source Inference Attack (SIA) in Federated Learning (FL) aims to identify\nwhich client used a target data point for local model training. It allows the\ncentral server to audit clients' data usage. In cross-silo FL, a client (silo)\ncollects data from multiple subjects (e.g., individuals, writers, or devices),\nposing a risk of subject information leakage. Subject Membership Inference\nAttack (SMIA) targets this scenario and attempts to infer whether any client\nutilizes data points from a target subject in cross-silo FL. However, existing\nresults on SMIA are limited and based on strong assumptions on the attack\nscenario. Therefore, we propose a Subject-Level Source Inference Attack (SLSIA)\nby removing critical constraints that only one client can use a target data\npoint in SIA and imprecise detection of clients utilizing target subject data\nin SMIA. The attacker, positioned on the server side, controls a target data\nsource and aims to detect all clients using data points from the target\nsubject. Our strategy leverages a binary attack classifier to predict whether\nthe embeddings returned by a local model on test data from the target subject\ninclude unique patterns that indicate a client trains the model with data from\nthat subject. To achieve this, the attacker locally pre-trains models using\ndata derived from the target subject and then leverages them to build a\ntraining set for the binary attack classifier. Our SLSIA significantly\noutperforms previous methods on three datasets. Specifically, SLSIA achieves a\nmaximum average accuracy of 0.88 over 50 target subjects. Analyzing embedding\ndistribution and input feature distance shows that datasets with sparse\nsubjects are more susceptible to our attack. Finally, we propose to defend our\nSLSIA using item-level and subject-level differential privacy mechanisms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19417v1",
    "published_date": "2024-09-28 17:27:34 UTC",
    "updated_date": "2024-09-28 17:27:34 UTC"
  },
  {
    "arxiv_id": "2409.19415v1",
    "title": "Bridging the Gap in Hybrid Decision-Making Systems",
    "authors": [
      "Federico Mazzoni",
      "Roberto Pellungrini",
      "Riccardo Guidotti"
    ],
    "abstract": "We introduce BRIDGET, a novel human-in-the-loop system for hybrid\ndecision-making, aiding the user to label records from an un-labeled dataset,\nattempting to ``bridge the gap'' between the two most popular Hybrid\nDecision-Making paradigms: those featuring the human in a leading position, and\nthe other with a machine making most of the decisions. BRIDGET understands when\neither a machine or a human user should be in charge, dynamically switching\nbetween two statuses. In the different statuses, BRIDGET still fosters the\nhuman-AI interaction, either having a machine learning model assuming skeptical\nstances towards the user and offering them suggestions, or towards itself and\ncalling the user back. We believe our proposal lays the groundwork for future\nsynergistic systems involving a human and a machine decision-makers.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "68T05, 68W40"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19415v1",
    "published_date": "2024-09-28 17:14:59 UTC",
    "updated_date": "2024-09-28 17:14:59 UTC"
  },
  {
    "arxiv_id": "2409.19413v1",
    "title": "Membership Privacy Evaluation in Deep Spiking Neural Networks",
    "authors": [
      "Jiaxin Li",
      "Gorka Abad",
      "Stjepan Picek",
      "Mauro Conti"
    ],
    "abstract": "Artificial Neural Networks (ANNs), commonly mimicking neurons with non-linear\nfunctions to output floating-point numbers, consistently receive the same\nsignals of a data point during its forward time. Unlike ANNs, Spiking Neural\nNetworks (SNNs) get various input signals in the forward time of a data point\nand simulate neurons in a biologically plausible way, i.e., producing a spike\n(a binary value) if the accumulated membrane potential of a neuron is larger\nthan a threshold. Even though ANNs have achieved remarkable success in multiple\ntasks, e.g., face recognition and object detection, SNNs have recently obtained\nattention due to their low power consumption, fast inference, and event-driven\nproperties. While privacy threats against ANNs are widely explored, much less\nwork has been done on SNNs. For instance, it is well-known that ANNs are\nvulnerable to the Membership Inference Attack (MIA), but whether the same\napplies to SNNs is not explored.\n  In this paper, we evaluate the membership privacy of SNNs by considering\neight MIAs, seven of which are inspired by MIAs against ANNs. Our evaluation\nresults show that SNNs are more vulnerable (maximum 10% higher in terms of\nbalanced attack accuracy) than ANNs when both are trained with neuromorphic\ndatasets (with time dimension). On the other hand, when training ANNs or SNNs\nwith static datasets (without time dimension), the vulnerability depends on the\ndataset used. If we convert ANNs trained with static datasets to SNNs, the\naccuracy of MIAs drops (maximum 11.5% with a reduction of 7.6% on the test\naccuracy of the target model). Next, we explore the impact factors of MIAs on\nSNNs by conducting a hyperparameter study. Finally, we show that the basic data\naugmentation method for static data and two recent data augmentation methods\nfor neuromorphic data can considerably (maximum reduction of 25.7%) decrease\nMIAs' performance on SNNs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19413v1",
    "published_date": "2024-09-28 17:13:04 UTC",
    "updated_date": "2024-09-28 17:13:04 UTC"
  },
  {
    "arxiv_id": "2409.19407v1",
    "title": "Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking",
    "authors": [
      "Zijian Dong",
      "Ruilin Li",
      "Yilei Wu",
      "Thuan Tinh Nguyen",
      "Joanna Su Xian Chong",
      "Fang Ji",
      "Nathanael Ren Jie Tong",
      "Christopher Li Hsian Chen",
      "Juan Helen Zhou"
    ],
    "abstract": "We introduce Brain-JEPA, a brain dynamics foundation model with the\nJoint-Embedding Predictive Architecture (JEPA). This pioneering model achieves\nstate-of-the-art performance in demographic prediction, disease\ndiagnosis/prognosis, and trait prediction through fine-tuning. Furthermore, it\nexcels in off-the-shelf evaluations (e.g., linear probing) and demonstrates\nsuperior generalizability across different ethnic groups, surpassing the\nprevious large model for brain activity significantly. Brain-JEPA incorporates\ntwo innovative techniques: Brain Gradient Positioning and Spatiotemporal\nMasking. Brain Gradient Positioning introduces a functional coordinate system\nfor brain functional parcellation, enhancing the positional encoding of\ndifferent Regions of Interest (ROIs). Spatiotemporal Masking, tailored to the\nunique characteristics of fMRI data, addresses the challenge of heterogeneous\ntime-series patches. These methodologies enhance model performance and advance\nour understanding of the neural circuits underlying cognition. Overall,\nBrain-JEPA is paving the way to address pivotal questions of building brain\nfunctional coordinate system and masking brain activity at the AI-neuroscience\ninterface, and setting a potentially new paradigm in brain activity analysis\nthrough downstream adaptation.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "The first two authors contributed equally. NeurIPS 2024 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2409.19407v1",
    "published_date": "2024-09-28 17:06:06 UTC",
    "updated_date": "2024-09-28 17:06:06 UTC"
  },
  {
    "arxiv_id": "2409.19390v1",
    "title": "Efficient Federated Intrusion Detection in 5G ecosystem using optimized BERT-based model",
    "authors": [
      "Frederic Adjewa",
      "Moez Esseghir",
      "Leila Merghem-Boulahia"
    ],
    "abstract": "The fifth-generation (5G) offers advanced services, supporting applications\nsuch as intelligent transportation, connected healthcare, and smart cities\nwithin the Internet of Things (IoT). However, these advancements introduce\nsignificant security challenges, with increasingly sophisticated cyber-attacks.\nThis paper proposes a robust intrusion detection system (IDS) using federated\nlearning and large language models (LLMs). The core of our IDS is based on\nBERT, a transformer model adapted to identify malicious network flows. We\nmodified this transformer to optimize performance on edge devices with limited\nresources. Experiments were conducted in both centralized and federated\nlearning contexts. In the centralized setup, the model achieved an inference\naccuracy of 97.79%. In a federated learning context, the model was trained\nacross multiple devices using both IID (Independent and Identically\nDistributed) and non-IID data, based on various scenarios, ensuring data\nprivacy and compliance with regulations. We also leveraged linear quantization\nto compress the model for deployment on edge devices. This reduction resulted\nin a slight decrease of 0.02% in accuracy for a model size reduction of 28.74%.\nThe results underscore the viability of LLMs for deployment in IoT ecosystems,\nhighlighting their ability to operate on devices with constrained computational\nand storage resources.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19390v1",
    "published_date": "2024-09-28 15:56:28 UTC",
    "updated_date": "2024-09-28 15:56:28 UTC"
  },
  {
    "arxiv_id": "2409.19389v1",
    "title": "Co-design of a novel CMOS highly parallel, low-power, multi-chip neural network accelerator",
    "authors": [
      "W Hokenmaier",
      "R Jurasek",
      "E Bowen",
      "R Granger",
      "D Odom"
    ],
    "abstract": "Why do security cameras, sensors, and siri use cloud servers instead of\non-board computation? The lack of very-low-power, high-performance chips\ngreatly limits the ability to field untethered edge devices. We present the\nNV-1, a new low-power ASIC AI processor that greatly accelerates parallel\nprocessing (> 10X) with dramatic reduction in energy consumption (> 100X), via\nmany parallel combined processor-memory units, i.e., a drastically\nnon-von-Neumann architecture, allowing very large numbers of independent\nprocessing streams without bottlenecks due to typical monolithic memory. The\ncurrent initial prototype fab arises from a successful co-development effort\nbetween algorithm- and software-driven architectural design and VLSI design\nrealities. An innovative communication protocol minimizes power usage, and data\ntransport costs among nodes were vastly reduced by eliminating the address bus,\nthrough local target address matching. Throughout the development process, the\nsoftware and architecture teams were able to innovate alongside the circuit\ndesign team's implementation effort. A digital twin of the proposed hardware\nwas developed early on to ensure that the technical implementation met the\narchitectural specifications, and indeed the predicted performance metrics have\nnow been thoroughly verified in real hardware test data. The resulting device\nis currently being used in a fielded edge sensor application; additional proofs\nof principle are in progress demonstrating the proof on the ground of this new\nreal-world extremely low-power high-performance ASIC device.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.DC",
    "comment": "neural network accelerator, low-power design, instruction set design,\n  parallel processors, digital twin",
    "pdf_url": "http://arxiv.org/pdf/2409.19389v1",
    "published_date": "2024-09-28 15:47:16 UTC",
    "updated_date": "2024-09-28 15:47:16 UTC"
  },
  {
    "arxiv_id": "2409.19379v1",
    "title": "Automated conjecturing in mathematics with \\emph{TxGraffiti}",
    "authors": [
      "Randy Davila"
    ],
    "abstract": "\\emph{TxGraffiti} is a data-driven, heuristic-based computer program\ndeveloped to automate the process of generating conjectures across various\nmathematical domains. Since its creation in 2017, \\emph{TxGraffiti} has\ncontributed to numerous mathematical publications, particularly in graph\ntheory. In this paper, we present the design and core principles of\n\\emph{TxGraffiti}, including its roots in the original \\emph{Graffiti} program,\nwhich pioneered the automation of mathematical conjecturing. We describe the\ndata collection process, the generation of plausible conjectures, and methods\nsuch as the \\emph{Dalmatian} heuristic for filtering out redundant or\ntransitive conjectures. Additionally, we highlight its contributions to the\nmathematical literature and introduce a new web-based interface that allows\nusers to explore conjectures interactively. While we focus on graph theory, the\ntechniques demonstrated extend to other areas of mathematics.",
    "categories": [
      "math.CO",
      "cs.AI"
    ],
    "primary_category": "math.CO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19379v1",
    "published_date": "2024-09-28 15:06:31 UTC",
    "updated_date": "2024-09-28 15:06:31 UTC"
  },
  {
    "arxiv_id": "2409.19375v1",
    "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
    "authors": [
      "Zongbo Han",
      "Jialong Yang",
      "Junfan Li",
      "Qinghua Hu",
      "Qianli Xu",
      "Mike Zheng Shou",
      "Changqing Zhang"
    ],
    "abstract": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "In submission",
    "pdf_url": "http://arxiv.org/pdf/2409.19375v1",
    "published_date": "2024-09-28 15:03:28 UTC",
    "updated_date": "2024-09-28 15:03:28 UTC"
  },
  {
    "arxiv_id": "2409.19366v1",
    "title": "Mind the Gap: Promoting Missing Modality Brain Tumor Segmentation with Alignment",
    "authors": [
      "Tianyi Liu",
      "Zhaorui Tan",
      "Haochuan Jiang",
      "Xi Yang",
      "Kaizhu Huang"
    ],
    "abstract": "Brain tumor segmentation is often based on multiple magnetic resonance\nimaging (MRI). However, in clinical practice, certain modalities of MRI may be\nmissing, which presents an even more difficult scenario. To cope with this\nchallenge, knowledge distillation has emerged as one promising strategy.\nHowever, recent efforts typically overlook the modality gaps and thus fail to\nlearn invariant feature representations across different modalities. Such\ndrawback consequently leads to limited performance for both teachers and\nstudents. To ameliorate these problems, in this paper, we propose a novel\nparadigm that aligns latent features of involved modalities to a well-defined\ndistribution anchor. As a major contribution, we prove that our novel training\nparadigm ensures a tight evidence lower bound, thus theoretically certifying\nits effectiveness. Extensive experiments on different backbones validate that\nthe proposed paradigm can enable invariant feature representations and produce\na teacher with narrowed modality gaps. This further offers superior guidance\nfor missing modality students, achieving an average improvement of 1.75 on dice\nscore.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19366v1",
    "published_date": "2024-09-28 14:37:42 UTC",
    "updated_date": "2024-09-28 14:37:42 UTC"
  },
  {
    "arxiv_id": "2409.19365v2",
    "title": "Conditional Image Synthesis with Diffusion Models: A Survey",
    "authors": [
      "Zheyuan Zhan",
      "Defang Chen",
      "Jian-Ping Mei",
      "Zhenghe Zhao",
      "Jiawei Chen",
      "Chun Chen",
      "Siwei Lyu",
      "Can Wang"
    ],
    "abstract": "Conditional image synthesis based on user-specified requirements is a key\ncomponent in creating complex visual content. In recent years, diffusion-based\ngenerative modeling has become a highly effective way for conditional image\nsynthesis, leading to exponential growth in the literature. However, the\ncomplexity of diffusion-based modeling, the wide range of image synthesis\ntasks, and the diversity of conditioning mechanisms present significant\nchallenges for researchers to keep up with rapid developments and understand\nthe core concepts on this topic. In this survey, we categorize existing works\nbased on how conditions are integrated into the two fundamental components of\ndiffusion-based modeling, i.e., the denoising network and the sampling process.\nWe specifically highlight the underlying principles, advantages, and potential\nchallenges of various conditioning approaches in the training, re-purposing,\nand specialization stages to construct a desired denoising network. We also\nsummarize six mainstream conditioning mechanisms in the essential sampling\nprocess. All discussions are centered around popular applications. Finally, we\npinpoint some critical yet still open problems to be solved in the future and\nsuggest some possible solutions. Our reviewed works are itemized at\nhttps://github.com/zju-pi/Awesome-Conditional-Diffusion-Models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19365v2",
    "published_date": "2024-09-28 14:36:38 UTC",
    "updated_date": "2024-10-03 14:01:03 UTC"
  },
  {
    "arxiv_id": "2409.19363v2",
    "title": "Learning Strategy Representation for Imitation Learning in Multi-Agent Games",
    "authors": [
      "Shiqi Lei",
      "Kanghoon Lee",
      "Linjing Li",
      "Jinkyoo Park"
    ],
    "abstract": "The offline datasets for imitation learning (IL) in multi-agent games\ntypically contain player trajectories exhibiting diverse strategies, which\nnecessitate measures to prevent learning algorithms from acquiring undesirable\nbehaviors. Learning representations for these trajectories is an effective\napproach to depicting the strategies employed by each demonstrator. However,\nexisting learning strategies often require player identification or rely on\nstrong assumptions, which are not appropriate for multi-agent games. Therefore,\nin this paper, we introduce the Strategy Representation for Imitation Learning\n(STRIL) framework, which (1) effectively learns strategy representations in\nmulti-agent games, (2) estimates proposed indicators based on these\nrepresentations, and (3) filters out sub-optimal data using the indicators.\nSTRIL is a plug-in method that can be integrated into existing IL algorithms.\nWe demonstrate the effectiveness of STRIL across competitive multi-agent\nscenarios, including Two-player Pong, Limit Texas Hold'em, and Connect Four.\nOur approach successfully acquires strategy representations and indicators,\nthereby identifying dominant trajectories and significantly enhancing existing\nIL performance across these environments.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "13 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2402.18617",
    "pdf_url": "http://arxiv.org/pdf/2409.19363v2",
    "published_date": "2024-09-28 14:30:17 UTC",
    "updated_date": "2025-02-14 05:22:03 UTC"
  },
  {
    "arxiv_id": "2409.19362v2",
    "title": "1st Place Solution of Multiview Egocentric Hand Tracking Challenge ECCV2024",
    "authors": [
      "Minqiang Zou",
      "Zhi Lv",
      "Riqiang Jin",
      "Tian Zhan",
      "Mochen Yu",
      "Yao Tang",
      "Jiajun Liang"
    ],
    "abstract": "Multi-view egocentric hand tracking is a challenging task and plays a\ncritical role in VR interaction. In this report, we present a method that uses\nmulti-view input images and camera extrinsic parameters to estimate both hand\nshape and pose. To reduce overfitting to the camera layout, we apply crop\njittering and extrinsic parameter noise augmentation. Additionally, we propose\nan offline neural smoothing post-processing method to further improve the\naccuracy of hand position and pose. Our method achieves 13.92mm MPJPE on the\nUmetrack dataset and 21.66mm MPJPE on the HOT3D dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ECCV2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.19362v2",
    "published_date": "2024-09-28 14:26:32 UTC",
    "updated_date": "2024-10-08 08:18:32 UTC"
  },
  {
    "arxiv_id": "2409.19339v2",
    "title": "Visual Question Decomposition on Multimodal Large Language Models",
    "authors": [
      "Haowei Zhang",
      "Jianzhe Liu",
      "Zhen Han",
      "Shuo Chen",
      "Bailan He",
      "Volker Tresp",
      "Zhiqiang Xu",
      "Jindong Gu"
    ],
    "abstract": "Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2409.19339v2",
    "published_date": "2024-09-28 12:49:16 UTC",
    "updated_date": "2024-10-07 12:05:55 UTC"
  },
  {
    "arxiv_id": "2409.19330v1",
    "title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models",
    "authors": [
      "Hao Chen",
      "Wei Zhao",
      "Yingli Li",
      "Tianyang Zhong",
      "Yisong Wang",
      "Youlan Shang",
      "Lei Guo",
      "Junwei Han",
      "Tianming Liu",
      "Jun Liu",
      "Tuo Zhang"
    ],
    "abstract": "Medical image analysis is crucial in modern radiological diagnostics,\nespecially given the exponential growth in medical imaging data. The demand for\nautomated report generation systems has become increasingly urgent. While prior\nresearch has mainly focused on using machine learning and multimodal language\nmodels for 2D medical images, the generation of reports for 3D medical images\nhas been less explored due to data scarcity and computational complexities.\nThis paper introduces 3D-CT-GPT, a Visual Question Answering (VQA)-based\nmedical visual language model specifically designed for generating radiology\nreports from 3D CT scans, particularly chest CTs. Extensive experiments on both\npublic and private datasets demonstrate that 3D-CT-GPT significantly\noutperforms existing methods in terms of report accuracy and quality. Although\ncurrent methods are few, including the partially open-source CT2Rep and the\nopen-source M3D, we ensured fair comparison through appropriate data conversion\nand evaluation methodologies. Experimental results indicate that 3D-CT-GPT\nenhances diagnostic accuracy and report coherence, establishing itself as a\nrobust solution for clinical radiology report generation. Future work will\nfocus on expanding the dataset and further optimizing the model to enhance its\nperformance and applicability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19330v1",
    "published_date": "2024-09-28 12:31:07 UTC",
    "updated_date": "2024-09-28 12:31:07 UTC"
  },
  {
    "arxiv_id": "2409.19325v1",
    "title": "A Generalized Model for Multidimensional Intransitivity",
    "authors": [
      "Jiuding Duan",
      "Jiyi Li",
      "Yukino Baba",
      "Hisashi Kashima"
    ],
    "abstract": "Intransitivity is a critical issue in pairwise preference modeling. It refers\nto the intransitive pairwise preferences between a group of players or objects\nthat potentially form a cyclic preference chain and has been long discussed in\nsocial choice theory in the context of the dominance relationship. However,\nsuch multifaceted intransitivity between players and the corresponding player\nrepresentations in high dimensions is difficult to capture. In this paper, we\npropose a probabilistic model that jointly learns each player's d-dimensional\nrepresentation (d>1) and a dataset-specific metric space that systematically\ncaptures the distance metric in Rd over the embedding space. Interestingly, by\nimposing additional constraints in the metric space, our proposed model\ndegenerates to former models used in intransitive representation learning.\nMoreover, we present an extensive quantitative investigation of the vast\nexistence of intransitive relationships between objects in various real-world\nbenchmark datasets. To our knowledge, this investigation is the first of this\ntype. The predictive performance of our proposed method on different real-world\ndatasets, including social choice, election, and online game datasets, shows\nthat our proposed method outperforms several competing methods in terms of\nprediction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.19325v1",
    "published_date": "2024-09-28 11:48:34 UTC",
    "updated_date": "2024-09-28 11:48:34 UTC"
  },
  {
    "arxiv_id": "2409.19322v1",
    "title": "Scalable Cloud-Native Pipeline for Efficient 3D Model Reconstruction from Monocular Smartphone Images",
    "authors": [
      "Potito Aghilar",
      "Vito Walter Anelli",
      "Michelantonio Trizio",
      "Tommaso Di Noia"
    ],
    "abstract": "In recent years, 3D models have gained popularity in various fields,\nincluding entertainment, manufacturing, and simulation. However, manually\ncreating these models can be a time-consuming and resource-intensive process,\nmaking it impractical for large-scale industrial applications. To address this\nissue, researchers are exploiting Artificial Intelligence and Machine Learning\nalgorithms to automatically generate 3D models effortlessly. In this paper, we\npresent a novel cloud-native pipeline that can automatically reconstruct 3D\nmodels from monocular 2D images captured using a smartphone camera. Our goal is\nto provide an efficient and easily-adoptable solution that meets the Industry\n4.0 standards for creating a Digital Twin model, which could enhance personnel\nexpertise through accelerated training. We leverage machine learning models\ndeveloped by NVIDIA Research Labs alongside a custom-designed pose recorder\nwith a unique pose compensation component based on the ARCore framework by\nGoogle. Our solution produces a reusable 3D model, with embedded materials and\ntextures, exportable and customizable in any external 3D modelling software or\n3D engine. Furthermore, the whole workflow is implemented by adopting the\nmicroservices architecture standard, enabling each component of the pipeline to\noperate as a standalone replaceable module.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.19322v1",
    "published_date": "2024-09-28 11:15:26 UTC",
    "updated_date": "2024-09-28 11:15:26 UTC"
  },
  {
    "arxiv_id": "2409.19318v1",
    "title": "Fairness Analysis with Shapley-Owen Effects",
    "authors": [
      "Harald Ruess"
    ],
    "abstract": "We argue that relative importance and its equitable attribution in terms of\nShapley-Owen effects is an appropriate one, and, if we accept a small number of\nreasonable imperatives for equitable attribution, the only way to measure\nfairness. On the other hand, the computation of Shapley-Owen effects can be\nvery demanding. Our main technical result is a spectral decomposition of the\nShapley-Owen effects, which decomposes the computation of these indices into a\nmodel-specific and a model-independent part. The model-independent part is\nprecomputed once and for all, and the model-specific computation of\nShapley-Owen effects is expressed analytically in terms of the coefficients of\nthe model's \\emph{polynomial chaos expansion} (PCE), which can now be reused to\ncompute different Shapley-Owen effects. We also propose an algorithm for\ncomputing precise and sparse truncations of the PCE of the model and the\nspectral decomposition of the Shapley-Owen effects, together with upper bounds\non the accumulated approximation errors. The approximations of both the PCE and\nthe Shapley-Owen effects converge to their true values.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19318v1",
    "published_date": "2024-09-28 11:05:49 UTC",
    "updated_date": "2024-09-28 11:05:49 UTC"
  },
  {
    "arxiv_id": "2409.19315v2",
    "title": "Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models",
    "authors": [
      "Nathan Leroux",
      "Paul-Philipp Manea",
      "Chirag Sudarshan",
      "Jan Finkbeiner",
      "Sebastian Siegel",
      "John Paul Strachan",
      "Emre Neftci"
    ],
    "abstract": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.ET"
    ],
    "primary_category": "cs.NE",
    "comment": "25 pages, 6 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.19315v2",
    "published_date": "2024-09-28 11:00:11 UTC",
    "updated_date": "2024-11-25 12:14:33 UTC"
  },
  {
    "arxiv_id": "2409.19310v1",
    "title": "Model X-Ray: Detection of Hidden Malware in AI Model Weights using Few Shot Learning",
    "authors": [
      "Daniel Gilkarov",
      "Ran Dubin"
    ],
    "abstract": "The potential for exploitation of AI models has increased due to the rapid\nadvancement of Artificial Intelligence (AI) and the widespread use of platforms\nlike Model Zoo for sharing AI models. Attackers can embed malware within AI\nmodels through steganographic techniques, taking advantage of the substantial\nsize of these models to conceal malicious data and use it for nefarious\npurposes, e.g. Remote Code Execution. Ensuring the security of AI models is a\nburgeoning area of research essential for safeguarding the multitude of\norganizations and users relying on AI technologies. This study leverages\nwell-studied image few-shot learning techniques by transferring the AI models\nto the image field using a novel image representation. Applying few-shot\nlearning in this field enables us to create practical models, a feat that\nprevious works lack. Our method addresses critical limitations in\nstate-of-the-art detection techniques that hinder their practicality. This\napproach reduces the required training dataset size from 40000 models to just\n6. Furthermore, our methods consistently detect delicate attacks of up to 25%\nembedding rate and even up to 6% in some cases, while previous works were only\nshown to be effective for a 100%-50% embedding rate. We employ a strict\nevaluation strategy to ensure the trained models are generic concerning various\nfactors. In addition, we show that our trained models successfully detect novel\nspread-spectrum steganography attacks, demonstrating the models' impressive\nrobustness just by learning one type of attack. We open-source our code to\nsupport reproducibility and enhance the research in this new field.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19310v1",
    "published_date": "2024-09-28 10:45:28 UTC",
    "updated_date": "2024-09-28 10:45:28 UTC"
  },
  {
    "arxiv_id": "2409.19308v2",
    "title": "Designing Domain-Specific Large Language Models: The Critical Role of Fine-Tuning in Public Opinion Simulation",
    "authors": [
      "Haocheng Lin"
    ],
    "abstract": "Large language models (LLMs) have transformed natural language processing,\nyet face challenges in specialized tasks such as simulating opinions on\nenvironmental policies. This paper introduces a novel fine-tuning approach that\nintegrates socio-demographic data from the UK Household Longitudinal Study,\nuniquely using profiling factors, such as age, gender, income, education, and\nregion. This method enhances the accuracy and representation of generated\nviews. By emulating diverse synthetic profiles, the fine-tuned models\nsignificantly outperform pre-trained counterparts, achieving measurable\nimprovements in capturing demographic nuances. Evaluation metrics, including\nChi-Squared, Cosine Similarity, Jaccard Index, and KL-divergence, reveal a\nstrong alignment between synthetic and real-world opinions. This work\ndemonstrates the potential of fine-tuned LLMs tailored to societal contexts to\nenable more ethical and precise policy simulations. Its broader implications\ninclude deploying LLMs in domains like healthcare and education, fostering\ninclusive and data-driven decision-making in both research and practice.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19308v2",
    "published_date": "2024-09-28 10:39:23 UTC",
    "updated_date": "2024-12-07 11:06:45 UTC"
  },
  {
    "arxiv_id": "2409.19306v1",
    "title": "CausalVE: Face Video Privacy Encryption via Causal Video Prediction",
    "authors": [
      "Yubo Huang",
      "Wenhao Feng",
      "Xin Lai",
      "Zixi Wang",
      "Jingzehua Xu",
      "Shuai Zhang",
      "Hongjie He",
      "Fan Chen"
    ],
    "abstract": "Advanced facial recognition technologies and recommender systems with\ninadequate privacy technologies and policies for facial interactions increase\nconcerns about bioprivacy violations. With the proliferation of video and\nlive-streaming websites, public-face video distribution and interactions pose\ngreater privacy risks. Existing techniques typically address the risk of\nsensitive biometric information leakage through various privacy enhancement\nmethods but pose a higher security risk by corrupting the information to be\nconveyed by the interaction data, or by leaving certain biometric features\nintact that allow an attacker to infer sensitive biometric information from\nthem. To address these shortcomings, in this paper, we propose a neural network\nframework, CausalVE. We obtain cover images by adopting a diffusion model to\nachieve face swapping with face guidance and use the speech sequence features\nand spatiotemporal sequence features of the secret video for dynamic video\ninference and prediction to obtain a cover video with the same number of frames\nas the secret video. In addition, we hide the secret video by using reversible\nneural networks for video hiding so that the video can also disseminate secret\ndata. Numerous experiments prove that our CausalVE has good security in public\nvideo dissemination and outperforms state-of-the-art methods from a\nqualitative, quantitative, and visual point of view.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.19306v1",
    "published_date": "2024-09-28 10:34:22 UTC",
    "updated_date": "2024-09-28 10:34:22 UTC"
  },
  {
    "arxiv_id": "2409.19301v1",
    "title": "Privacy Attack in Federated Learning is Not Easy: An Experimental Study",
    "authors": [
      "Hangyu Zhu",
      "Liyuan Huang",
      "Zhenping Xie"
    ],
    "abstract": "Federated learning (FL) is an emerging distributed machine learning paradigm\nproposed for privacy preservation. Unlike traditional centralized learning\napproaches, FL enables multiple users to collaboratively train a shared global\nmodel without disclosing their own data, thereby significantly reducing the\npotential risk of privacy leakage. However, recent studies have indicated that\nFL cannot entirely guarantee privacy protection, and attackers may still be\nable to extract users' private data through the communicated model gradients.\nAlthough numerous privacy attack FL algorithms have been developed, most are\ndesigned to reconstruct private data from a single step of calculated\ngradients. It remains uncertain whether these methods are effective in\nrealistic federated environments or if they have other limitations. In this\npaper, we aim to help researchers better understand and evaluate the\neffectiveness of privacy attacks on FL. We analyze and discuss recent research\npapers on this topic and conduct experiments in a real FL environment to\ncompare the performance of various attack methods. Our experimental results\nreveal that none of the existing state-of-the-art privacy attack algorithms can\neffectively breach private client data in realistic FL settings, even in the\nabsence of defense strategies. This suggests that privacy attacks in FL are\nmore challenging than initially anticipated.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19301v1",
    "published_date": "2024-09-28 10:06:34 UTC",
    "updated_date": "2024-09-28 10:06:34 UTC"
  },
  {
    "arxiv_id": "2409.19300v1",
    "title": "Sustaining model performance for covid-19 detection from dynamic audio data: Development and evaluation of a comprehensive drift-adaptive framework",
    "authors": [
      "Theofanis Ganitidis",
      "Maria Athanasiou",
      "Konstantinos Mitsis",
      "Konstantia Zarkogianni",
      "Konstantina S. Nikita"
    ],
    "abstract": "Background: The COVID-19 pandemic has highlighted the need for robust\ndiagnostic tools capable of detecting the disease from diverse and evolving\ndata sources. Machine learning models, especially convolutional neural networks\n(CNNs), have shown promise. However, the dynamic nature of real-world data can\nlead to model drift, where performance degrades over time as the underlying\ndata distribution changes. Addressing this challenge is crucial to maintaining\naccuracy and reliability in diagnostic applications.\n  Objective: This study aims to develop a framework that monitors model drift\nand employs adaptation mechanisms to mitigate performance fluctuations in\nCOVID-19 detection models trained on dynamic audio data.\n  Methods: Two crowd-sourced COVID-19 audio datasets, COVID-19 Sounds and\nCOSWARA, were used. Each was divided into development and post-development\nperiods. A baseline CNN model was trained and evaluated using cough recordings\nfrom the development period. Maximum mean discrepancy (MMD) was used to detect\nchanges in data distributions and model performance between periods. Upon\ndetecting drift, retraining was triggered to update the baseline model. Two\nadaptation approaches were compared: unsupervised domain adaptation (UDA) and\nactive learning (AL).\n  Results: UDA improved balanced accuracy by up to 22% and 24% for the COVID-19\nSounds and COSWARA datasets, respectively. AL yielded even greater\nimprovements, with increases of up to 30% and 60%, respectively.\n  Conclusions: The proposed framework addresses model drift in COVID-19\ndetection, enabling continuous adaptation to evolving data. This approach\nensures sustained model performance, contributing to robust diagnostic tools\nfor COVID-19 and potentially other infectious diseases.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19300v1",
    "published_date": "2024-09-28 10:06:30 UTC",
    "updated_date": "2024-09-28 10:06:30 UTC"
  },
  {
    "arxiv_id": "2409.19291v2",
    "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
    "authors": [
      "Jihai Zhang",
      "Xiaoye Qu",
      "Tong Zhu",
      "Yu Cheng"
    ],
    "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a\ncornerstone in multimodal intelligence. However, recent studies have identified\nthat the information loss in the CLIP encoding process is substantial, and CLIP\ntends to capture only coarse-grained features from the input. This deficiency\nsignificantly limits the ability of a single CLIP model to handle images rich\nin visual detail. In this work, we propose a simple yet effective\nmodel-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU\nefficiently fine-tunes a series of CLIP models that capture different feature\nspaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for\nthe Feed-Forward Network (FFN). These models can then be transformed into a\nCLIP-MoE with a larger model capacity, leading to significantly enhanced\nperformance with minimal computational overhead. To the best of our knowledge,\nDiversified Multiplet Upcycling is the first approach to introduce sparsely\nactivated MoE into CLIP foundation models. Extensive experiments demonstrate\nthe significant performance of CLIP-MoE across various zero-shot retrieval,\nzero-shot image classification tasks, and downstream Multimodal Large Language\nModel (MLLM) benchmarks by serving as a vision encoder. Furthermore,\nDiversified Multiplet Upcycling enables the conversion of any dense CLIP model\ninto CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner\nwithout requiring further adaptation in downstream frameworks. Through\nDiversified Multiplet Upcycling, we aim to provide valuable insights for future\nresearch on developing more efficient and effective multimodal learning\nsystems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19291v2",
    "published_date": "2024-09-28 09:28:51 UTC",
    "updated_date": "2024-10-02 21:50:58 UTC"
  },
  {
    "arxiv_id": "2410.00051v2",
    "title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization",
    "authors": [
      "Haoran Li",
      "Zhennan Jiang",
      "Yuhui Chen",
      "Dongbin Zhao"
    ],
    "abstract": "With high-dimensional state spaces, visual reinforcement learning (RL) faces\nsignificant challenges in exploitation and exploration, resulting in low sample\nefficiency and training stability. As a time-efficient diffusion model,\nalthough consistency models have been validated in online state-based RL, it is\nstill an open question whether it can be extended to visual RL. In this paper,\nwe investigate the impact of non-stationary distribution and the actor-critic\nframework on consistency policy in online RL, and find that consistency policy\nwas unstable during the training, especially in visual RL with the\nhigh-dimensional state space. To this end, we suggest sample-based entropy\nregularization to stabilize the policy training, and propose a consistency\npolicy with prioritized proximal experience regularization (CP3ER) to improve\nsample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21\ntasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is\nthe first method to apply diffusion/consistency models to visual RL and\ndemonstrates the potential of consistency models in visual RL. More\nvisualization results are available at https://jzndd.github.io/CP3ER-Page/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.00051v2",
    "published_date": "2024-09-28 09:24:10 UTC",
    "updated_date": "2024-10-29 09:44:15 UTC"
  },
  {
    "arxiv_id": "2409.19279v1",
    "title": "Distributed Optimization via Energy Conservation Laws in Dilated Coordinates",
    "authors": [
      "Mayank Baranwal",
      "Kushal Chakrabarti"
    ],
    "abstract": "Optimizing problems in a distributed manner is critical for systems involving\nmultiple agents with private data. Despite substantial interest, a unified\nmethod for analyzing the convergence rates of distributed optimization\nalgorithms is lacking. This paper introduces an energy conservation approach\nfor analyzing continuous-time dynamical systems in dilated coordinates. Instead\nof directly analyzing dynamics in the original coordinate system, we establish\na conserved quantity, akin to physical energy, in the dilated coordinate\nsystem. Consequently, convergence rates can be explicitly expressed in terms of\nthe inverse time-dilation factor. Leveraging this generalized approach, we\nformulate a novel second-order distributed accelerated gradient flow with a\nconvergence rate of $O\\left(1/t^{2-\\epsilon}\\right)$ in time $t$ for\n$\\epsilon>0$. We then employ a semi second-order symplectic Euler\ndiscretization to derive a rate-matching algorithm with a convergence rate of\n$O\\left(1/k^{2-\\epsilon}\\right)$ in $k$ iterations. To the best of our\nknowledge, this represents the most favorable convergence rate for any\ndistributed optimization algorithm designed for smooth convex optimization. Its\naccelerated convergence behavior is benchmarked against various\nstate-of-the-art distributed optimization algorithms on practical, large-scale\nproblems.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.DS"
    ],
    "primary_category": "math.OC",
    "comment": "10 pages; (Near) optimal convergence rate",
    "pdf_url": "http://arxiv.org/pdf/2409.19279v1",
    "published_date": "2024-09-28 08:02:43 UTC",
    "updated_date": "2024-09-28 08:02:43 UTC"
  },
  {
    "arxiv_id": "2409.19270v1",
    "title": "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation",
    "authors": [
      "Tanvir Mahmud",
      "Diana Marculescu"
    ],
    "abstract": "Audio separation in real-world scenarios, where mixtures contain a variable\nnumber of sources, presents significant challenges due to limitations of\nexisting models, such as over-separation, under-separation, and dependence on\npredefined training sources. We propose OpenSep, a novel framework that\nleverages large language models (LLMs) for automated audio separation,\neliminating the need for manual intervention and overcoming source limitations.\nOpenSep uses textual inversion to generate captions from audio mixtures with\noff-the-shelf audio captioning models, effectively parsing the sound sources\npresent. It then employs few-shot LLM prompting to extract detailed audio\nproperties of each parsed source, facilitating separation in unseen mixtures.\nAdditionally, we introduce a multi-level extension of the mix-and-separate\ntraining framework to enhance modality alignment by separating single source\nsounds and mixtures simultaneously. Extensive experiments demonstrate OpenSep's\nsuperiority in precisely separating new, unseen, and variable sources in\nchallenging mixtures, outperforming SOTA baseline methods. Code is released at\nhttps://github.com/tanvir-utexas/OpenSep.git",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted in EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2409.19270v1",
    "published_date": "2024-09-28 06:59:52 UTC",
    "updated_date": "2024-09-28 06:59:52 UTC"
  },
  {
    "arxiv_id": "2409.19258v1",
    "title": "VecLSTM: Trajectory Data Processing and Management for Activity Recognition through LSTM Vectorization and Database Integration",
    "authors": [
      "Solmaz Seyed Monir",
      "Dongfang Zhao"
    ],
    "abstract": "Activity recognition is a challenging task due to the large scale of\ntrajectory data and the need for prompt and efficient processing. Existing\nmethods have attempted to mitigate this problem by employing traditional LSTM\narchitectures, but these approaches often suffer from inefficiencies in\nprocessing large datasets. In response to this challenge, we propose VecLSTM, a\nnovel framework that enhances the performance and efficiency of LSTM-based\nneural networks. Unlike conventional approaches, VecLSTM incorporates\nvectorization layers, leveraging optimized mathematical operations to process\ninput sequences more efficiently. We have implemented VecLSTM and incorporated\nit into the MySQL database. To evaluate the effectiveness of VecLSTM, we\ncompare its performance against a conventional LSTM model using a dataset\ncomprising 1,467,652 samples with seven unique labels. Experimental results\ndemonstrate superior accuracy and efficiency compared to the state-of-the-art,\nwith VecLSTM achieving a validation accuracy of 85.57\\%, a test accuracy of\n85.47\\%, and a weighted F1-score of 0.86. Furthermore, VecLSTM significantly\nreduces training time, offering a 26.2\\% reduction compared to traditional LSTM\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.19258v1",
    "published_date": "2024-09-28 06:22:44 UTC",
    "updated_date": "2024-09-28 06:22:44 UTC"
  },
  {
    "arxiv_id": "2409.19255v2",
    "title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning",
    "authors": [
      "Kazuki Matsuda",
      "Yuiga Wada",
      "Komei Sugiura"
    ],
    "abstract": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "ACCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.19255v2",
    "published_date": "2024-09-28 06:04:56 UTC",
    "updated_date": "2024-10-24 11:29:41 UTC"
  },
  {
    "arxiv_id": "2409.19247v1",
    "title": "Edit-Constrained Decoding for Sentence Simplification",
    "authors": [
      "Tatsuya Zetsu",
      "Yuki Arase",
      "Tomoyuki Kajiwara"
    ],
    "abstract": "We propose edit operation based lexically constrained decoding for sentence\nsimplification. In sentence simplification, lexical paraphrasing is one of the\nprimary procedures for rewriting complex sentences into simpler\ncorrespondences. While previous studies have confirmed the efficacy of\nlexically constrained decoding on this task, their constraints can be loose and\nmay lead to sub-optimal generation. We address this problem by designing\nconstraints that replicate the edit operations conducted in simplification and\ndefining stricter satisfaction conditions. Our experiments indicate that the\nproposed method consistently outperforms the previous studies on three English\nsimplification corpora commonly used in this task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP2024-Findings",
    "pdf_url": "http://arxiv.org/pdf/2409.19247v1",
    "published_date": "2024-09-28 05:39:50 UTC",
    "updated_date": "2024-09-28 05:39:50 UTC"
  },
  {
    "arxiv_id": "2409.19237v1",
    "title": "The Price of Pessimism for Automated Defense",
    "authors": [
      "Erick Galinkin",
      "Emmanouil Pountourakis",
      "Spiros Mancoridis"
    ],
    "abstract": "The well-worn George Box aphorism ``all models are wrong, but some are\nuseful'' is particularly salient in the cybersecurity domain, where the\nassumptions built into a model can have substantial financial or even national\nsecurity impacts. Computer scientists are often asked to optimize for\nworst-case outcomes, and since security is largely focused on risk mitigation,\npreparing for the worst-case scenario appears rational. In this work, we\ndemonstrate that preparing for the worst case rather than the most probable\ncase may yield suboptimal outcomes for learning agents. Through the lens of\nstochastic Bayesian games, we first explore different attacker knowledge\nmodeling assumptions that impact the usefulness of models to cybersecurity\npractitioners. By considering different models of attacker knowledge about the\nstate of the game and a defender's hidden information, we find that there is a\ncost to the defender for optimizing against the worst case.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to GameSec 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.19237v1",
    "published_date": "2024-09-28 04:54:23 UTC",
    "updated_date": "2024-09-28 04:54:23 UTC"
  },
  {
    "arxiv_id": "2409.19231v1",
    "title": "Double Actor-Critic with TD Error-Driven Regularization in Reinforcement Learning",
    "authors": [
      "Haohui Chen",
      "Zhiyong Chen",
      "Aoxiang Liu",
      "Wentuo Fang"
    ],
    "abstract": "To obtain better value estimation in reinforcement learning, we propose a\nnovel algorithm based on the double actor-critic framework with temporal\ndifference error-driven regularization, abbreviated as TDDR. TDDR employs\ndouble actors, with each actor paired with a critic, thereby fully leveraging\nthe advantages of double critics. Additionally, TDDR introduces an innovative\ncritic regularization architecture. Compared to classical deterministic policy\ngradient-based algorithms that lack a double actor-critic structure, TDDR\nprovides superior estimation. Moreover, unlike existing algorithms with double\nactor-critic frameworks, TDDR does not introduce any additional\nhyperparameters, significantly simplifying the design and implementation\nprocess. Experiments demonstrate that TDDR exhibits strong competitiveness\ncompared to benchmark algorithms in challenging continuous control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19231v1",
    "published_date": "2024-09-28 04:22:42 UTC",
    "updated_date": "2024-09-28 04:22:42 UTC"
  },
  {
    "arxiv_id": "2410.00049v2",
    "title": "Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph",
    "authors": [
      "Guancheng Wan",
      "Zewen Liu",
      "Max S. Y. Lau",
      "B. Aditya Prakash",
      "Wei Jin"
    ],
    "abstract": "Effective epidemic forecasting is critical for public health strategies and\nefficient medical resource allocation, especially in the face of rapidly\nspreading infectious diseases. However, existing deep-learning methods often\noverlook the dynamic nature of epidemics and fail to account for the specific\nmechanisms of disease transmission. In response to these challenges, we\nintroduce an innovative end-to-end framework called Epidemiology-Aware Neural\nODE with Continuous Disease Transmission Graph (EARTH) in this paper. To learn\ncontinuous and regional disease transmission patterns, we first propose EANO,\nwhich seamlessly integrates the neural ODE approach with the epidemic\nmechanism, considering the complex spatial spread process during epidemic\nevolution. Additionally, we introduce GLTG to model global infection trends and\nleverage these signals to guide local transmission dynamically. To accommodate\nboth the global coherence of epidemic trends and the local nuances of epidemic\ntransmission patterns, we build a cross-attention approach to fuse the most\nmeaningful information for forecasting. Through the smooth synergy of both\ncomponents, EARTH offers a more robust and flexible approach to understanding\nand predicting the spread of infectious diseases. Extensive experiments show\nEARTH superior performance in forecasting real-world epidemics compared to\nstate-of-the-art methods. The code will be available at\nhttps://github.com/Emory-Melody/EpiLearn.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00049v2",
    "published_date": "2024-09-28 04:07:16 UTC",
    "updated_date": "2024-11-10 05:26:44 UTC"
  },
  {
    "arxiv_id": "2409.19226v1",
    "title": "Learning to Bridge the Gap: Efficient Novelty Recovery with Planning and Reinforcement Learning",
    "authors": [
      "Alicia Li",
      "Nishanth Kumar",
      "Tomás Lozano-Pérez",
      "Leslie Kaelbling"
    ],
    "abstract": "The real world is unpredictable. Therefore, to solve long-horizon\ndecision-making problems with autonomous robots, we must construct agents that\nare capable of adapting to changes in the environment during deployment.\nModel-based planning approaches can enable robots to solve complex,\nlong-horizon tasks in a variety of environments. However, such approaches tend\nto be brittle when deployed into an environment featuring a novel situation\nthat their underlying model does not account for. In this work, we propose to\nlearn a ``bridge policy'' via Reinforcement Learning (RL) to adapt to such\nnovelties. We introduce a simple formulation for such learning, where the RL\nproblem is constructed with a special ``CallPlanner'' action that terminates\nthe bridge policy and hands control of the agent back to the planner. This\nallows the RL policy to learn the set of states in which querying the planner\nand following the returned plan will achieve the goal. We show that this\nformulation enables the agent to rapidly learn by leveraging the planner's\nknowledge to avoid challenging long-horizon exploration caused by sparse\nreward. In experiments across three different simulated domains of varying\ncomplexity, we demonstrate that our approach is able to learn policies that\nadapt to novelty more efficiently than several baselines, including a pure RL\nbaseline. We also demonstrate that the learned bridge policy is generalizable\nin that it can be combined with the planner to enable the agent to solve more\ncomplex tasks with multiple instances of the encountered novelty.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19226v1",
    "published_date": "2024-09-28 03:41:25 UTC",
    "updated_date": "2024-09-28 03:41:25 UTC"
  },
  {
    "arxiv_id": "2410.12794v2",
    "title": "Disaggregating Embedding Recommendation Systems with FlexEMR",
    "authors": [
      "Yibo Huang",
      "Zhenning Yang",
      "Jiarong Xing",
      "Yi Dai",
      "Yiming Qiu",
      "Dingming Wu",
      "Fan Lai",
      "Ang Chen"
    ],
    "abstract": "Efficiently serving embedding-based recommendation (EMR) models remains a\nsignificant challenge due to their increasingly large memory requirements.\nToday's practice splits the model across many monolithic servers, where a mix\nof GPUs, CPUs, and DRAM is provisioned in fixed proportions. This approach\nleads to suboptimal resource utilization and increased costs. Disaggregating\nembedding operations from neural network inference is a promising solution but\nraises novel networking challenges. In this paper, we discuss the design of\nFlexEMR for optimized EMR disaggregation. FlexEMR proposes two sets of\ntechniques to tackle the networking challenges: Leveraging the temporal and\nspatial locality of embedding lookups to reduce data movement over the network,\nand designing an optimized multi-threaded RDMA engine for concurrent lookup\nsubrequests. We outline the design space for each technique and present initial\nresults from our early prototype.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12794v2",
    "published_date": "2024-09-28 01:58:11 UTC",
    "updated_date": "2024-12-30 19:07:23 UTC"
  },
  {
    "arxiv_id": "2410.12793v1",
    "title": "Environment Scan of Generative AI Infrastructure for Clinical and Translational Science",
    "authors": [
      "Betina Idnay",
      "Zihan Xu",
      "William G. Adams",
      "Mohammad Adibuzzaman",
      "Nicholas R. Anderson",
      "Neil Bahroos",
      "Douglas S. Bell",
      "Cody Bumgardner",
      "Thomas Campion",
      "Mario Castro",
      "James J. Cimino",
      "I. Glenn Cohen",
      "David Dorr",
      "Peter L Elkin",
      "Jungwei W. Fan",
      "Todd Ferris",
      "David J. Foran",
      "David Hanauer",
      "Mike Hogarth",
      "Kun Huang",
      "Jayashree Kalpathy-Cramer",
      "Manoj Kandpal",
      "Niranjan S. Karnik",
      "Avnish Katoch",
      "Albert M. Lai",
      "Christophe G. Lambert",
      "Lang Li",
      "Christopher Lindsell",
      "Jinze Liu",
      "Zhiyong Lu",
      "Yuan Luo",
      "Peter McGarvey",
      "Eneida A. Mendonca",
      "Parsa Mirhaji",
      "Shawn Murphy",
      "John D. Osborne",
      "Ioannis C. Paschalidis",
      "Paul A. Harris",
      "Fred Prior",
      "Nicholas J. Shaheen",
      "Nawar Shara",
      "Ida Sim",
      "Umberto Tachinardi",
      "Lemuel R. Waitman",
      "Rosalind J. Wright",
      "Adrian H. Zai",
      "Kai Zheng",
      "Sandra Soo-Jin Lee",
      "Bradley A. Malin",
      "Karthik Natarajan",
      "W. Nicholson Price II",
      "Rui Zhang",
      "Yiye Zhang",
      "Hua Xu",
      "Jiang Bian",
      "Chunhua Weng",
      "Yifan Peng"
    ],
    "abstract": "This study reports a comprehensive environmental scan of the generative AI\n(GenAI) infrastructure in the national network for clinical and translational\nscience across 36 institutions supported by the Clinical and Translational\nScience Award (CTSA) Program led by the National Center for Advancing\nTranslational Sciences (NCATS) of the National Institutes of Health (NIH) at\nthe United States. With the rapid advancement of GenAI technologies, including\nlarge language models (LLMs), healthcare institutions face unprecedented\nopportunities and challenges. This research explores the current status of\nGenAI integration, focusing on stakeholder roles, governance structures, and\nethical considerations by administering a survey among leaders of health\ninstitutions (i.e., representing academic medical centers and health systems)\nto assess the institutional readiness and approach towards GenAI adoption. Key\nfindings indicate a diverse range of institutional strategies, with most\norganizations in the experimental phase of GenAI deployment. The study\nhighlights significant variations in governance models, with a strong\npreference for centralized decision-making but notable gaps in workforce\ntraining and ethical oversight. Moreover, the results underscore the need for a\nmore coordinated approach to GenAI governance, emphasizing collaboration among\nsenior leaders, clinicians, information technology staff, and researchers. Our\nanalysis also reveals concerns regarding GenAI bias, data security, and\nstakeholder trust, which must be addressed to ensure the ethical and effective\nimplementation of GenAI technologies. This study offers valuable insights into\nthe challenges and opportunities of GenAI integration in healthcare, providing\na roadmap for institutions aiming to leverage GenAI for improved quality of\ncare and operational efficiency.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12793v1",
    "published_date": "2024-09-28 01:53:13 UTC",
    "updated_date": "2024-09-28 01:53:13 UTC"
  }
]
[
  {
    "arxiv_id": "2407.19610v1",
    "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
    "authors": [
      "Mohammed Al-Maamari",
      "Mehdi Ben Amor",
      "Michael Granitzer"
    ],
    "abstract": "This research combines Knowledge Distillation (KD) and Mixture of Experts\n(MoE) to develop modular, efficient multilingual language models. Key\nobjectives include evaluating adaptive versus fixed alpha methods in KD and\ncomparing modular MoE architectures for handling multi-domain inputs and\npreventing catastrophic forgetting. KD compresses large language models (LLMs)\ninto smaller, efficient models, while MoE enhances modularity with specialized\ntasks. Experiments showed similar performance for both KD methods, with\nmarginal improvements from adaptive alpha. A combined loss approach provided\nmore stable learning. The router, trained to classify input sequences into\nEnglish, French, German, or Python, achieved 99.95% precision, recall, and F1\nscore, with Logistic Regression being the most effective classifier.\nEvaluations of modular MoE architectures revealed that Pre-trained Language\nExperts (PLE) and Joint Expert Embedding Training (JEET) performed similarly,\nwhile the MoE with Common Expert (MoE-CE) setup showed slightly lower\nperformance. Including a common expert in MoE-CE improved its performance.\nStudies on catastrophic forgetting indicated that sequential training led to\nsignificant forgetting, while single-session training with balanced batches and\nthe MoE approach mitigated this issue. The MoE architecture preserved knowledge\nacross multiple languages effectively.\n  The research contributes open-sourced resources including the dataset\n(https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation\ntool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the\nresearch codebase (https://github.com/ModMaamari/mixture-modular-experts).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2407.19610v1",
    "published_date": "2024-07-28 23:42:09 UTC",
    "updated_date": "2024-07-28 23:42:09 UTC"
  },
  {
    "arxiv_id": "2407.19600v1",
    "title": "You shall know a piece by the company it keeps. Chess plays as a data for word2vec models",
    "authors": [
      "Boris Orekhov"
    ],
    "abstract": "In this paper, I apply linguistic methods of analysis to non-linguistic data,\nchess plays, metaphorically equating one with the other and seeking analogies.\nChess game notations are also a kind of text, and one can consider the records\nof moves or positions of pieces as words and statements in a certain language.\nIn this article I show how word embeddings (word2vec) can work on chess game\ntexts instead of natural language texts. I don't see how this representation of\nchess data can be used productively. It's unlikely that these vector models\nwill help engines or people choose the best move. But in a purely academic\nsense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't\nnecessarily lead to a win.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19600v1",
    "published_date": "2024-07-28 22:12:36 UTC",
    "updated_date": "2024-07-28 22:12:36 UTC"
  },
  {
    "arxiv_id": "2407.19594v2",
    "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
    "authors": [
      "Tianhao Wu",
      "Weizhe Yuan",
      "Olga Golovneva",
      "Jing Xu",
      "Yuandong Tian",
      "Jiantao Jiao",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19594v2",
    "published_date": "2024-07-28 21:58:28 UTC",
    "updated_date": "2024-07-30 01:38:06 UTC"
  },
  {
    "arxiv_id": "2407.19586v1",
    "title": "Is Generative AI an Existential Threat to Human Creatives? Insights from Financial Economics",
    "authors": [
      "Jiasun Li"
    ],
    "abstract": "With the phenomenal rise of generative AI models (e.g., large language models\nsuch as GPT or large image models such as Diffusion), there are increasing\nconcerns about human creatives' futures. Specifically, as generative models'\npower further increases, will they eventually replace all human creatives'\njobs? We argue that the answer is \"no,\" even if existing generative AI models'\ncapabilities reach their theoretical limit. Our theory has a close analogy to a\nfamiliar insight in financial economics on the impossibility of an\ninformationally efficient market [Grossman and Stiglitz (1980)]: If generative\nAI models can provide all the content humans need at low variable costs, then\nthere is no incentive for humans to spend costly resources on content creation\nas they cannot profit from it. But if no human creates new content, then\ngenerative AI can only learn from stale information and be unable to generate\nup-to-date content that reflects new happenings in the physical world. This\ncreates a paradox.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19586v1",
    "published_date": "2024-07-28 21:11:41 UTC",
    "updated_date": "2024-07-28 21:11:41 UTC"
  },
  {
    "arxiv_id": "2407.19580v3",
    "title": "Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures",
    "authors": [
      "Dang Nguyen",
      "Wenhan Yang",
      "Rathul Anand",
      "Yu Yang",
      "Baharan Mirzasoleiman"
    ],
    "abstract": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced mixture of sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V-projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and\nLlama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably,\nCoLM reduces the memory requirement of fine-tuning by 2x and even outperforms\ntraining with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with\nexisting memory-efficient training methods like LoRA, further reducing the\nmemory requirements of training LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 6 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.19580v3",
    "published_date": "2024-07-28 20:39:16 UTC",
    "updated_date": "2025-03-07 20:12:27 UTC"
  },
  {
    "arxiv_id": "2407.19568v3",
    "title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
    "authors": [
      "Kangda Wei",
      "Aayush Gautam",
      "Ruihong Huang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of\nnatural language processing tasks. However, its effectiveness over\ndiscourse-level event relation extraction (ERE) tasks remains unexplored. In\nthis paper, we assess the effectiveness of LLMs in addressing discourse-level\nERE tasks characterized by lengthy documents and intricate relations\nencompassing coreference, temporal, causal, and subevent types. Evaluation is\nconducted using an commercial model, GPT-3.5, and an open-source model,\nLLaMA-2. Our study reveals a notable underperformance of LLMs compared to the\nbaseline established through supervised learning. Although Supervised\nFine-Tuning (SFT) can improve LLMs performance, it does not scale well compared\nto the smaller supervised baseline model. Our quantitative and qualitative\nanalysis shows that LLMs have several weaknesses when applied for extracting\nevent relations, including a tendency to fabricate event mentions, and failures\nto capture transitivity rules among relations, detect long distance relations,\nor comprehend contexts with dense event mentions. Code available at:\nhttps://github.com/WeiKangda/LLM-ERE.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19568v3",
    "published_date": "2024-07-28 19:27:06 UTC",
    "updated_date": "2025-02-22 19:05:36 UTC"
  },
  {
    "arxiv_id": "2407.19564v1",
    "title": "Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models",
    "authors": [
      "Jifeng Wang",
      "Kaouther Messaoud",
      "Yuejiang Liu",
      "Juergen Gall",
      "Alexandre Alahi"
    ],
    "abstract": "Recent progress in motion forecasting has been substantially driven by\nself-supervised pre-training. However, adapting pre-trained models for specific\ndownstream tasks, especially motion prediction, through extensive fine-tuning\nis often inefficient. This inefficiency arises because motion prediction\nclosely aligns with the masked pre-training tasks, and traditional full\nfine-tuning methods fail to fully leverage this alignment. To address this, we\nintroduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of\nthe model's parameters, focusing adjustments on newly introduced prompts and\nadapters. This approach not only preserves the pre-learned representations but\nalso significantly reduces the number of parameters that need retraining,\nthereby enhancing efficiency. This tailored strategy, supplemented by our\nmethod's capability to efficiently adapt to different datasets, enhances model\nefficiency and ensures robust performance across datasets without the need for\nextensive retraining. Our experiments show that Forecast-PEFT outperforms\ntraditional full fine-tuning methods in motion prediction tasks, achieving\nhigher accuracy with only 17% of the trainable parameters typically required.\nMoreover, our comprehensive adaptation, Forecast-FT, further improves\nprediction performance, evidencing up to a 9.6% enhancement over conventional\nbaseline methods. Code will be available at\nhttps://github.com/csjfwang/Forecast-PEFT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.19564v1",
    "published_date": "2024-07-28 19:18:59 UTC",
    "updated_date": "2024-07-28 19:18:59 UTC"
  },
  {
    "arxiv_id": "2407.19540v4",
    "title": "Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Curriculum Data Erasing Guided Knowledge Distillation",
    "authors": [
      "Heejoon Koo"
    ],
    "abstract": "In this paper, we present NECHO v2, a novel framework designed to enhance the\npredictive accuracy of multimodal sequential patient diagnoses under uncertain\nmissing visit sequences, a common challenge in real clinical settings. Firstly,\nwe modify NECHO, designed in a diagnosis code-centric fashion, to handle\nuncertain modality representation dominance under the imperfect data. Secondly,\nwe develop a systematic knowledge distillation by employing the modified NECHO\nas both teacher and student. It encompasses a modality-wise contrastive and\nhierarchical distillation, transformer representation random distillation,\nalong with other distillations to align representations between teacher and\nstudent tightly and effectively. We also propose curriculum learning guided\nrandom data erasing within sequences during both training and distillation of\nthe teacher to lightly simulate scenario with missing visit information,\nthereby fostering effective knowledge transfer. As a result, NECHO v2 verifies\nitself by showing robust superiority in multimodal sequential diagnosis\nprediction under both balanced and imbalanced incomplete settings on multimodal\nhealthcare data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICASSP 2025 (2025 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing)",
    "pdf_url": "http://arxiv.org/pdf/2407.19540v4",
    "published_date": "2024-07-28 17:14:27 UTC",
    "updated_date": "2025-01-30 15:21:50 UTC"
  },
  {
    "arxiv_id": "2407.19532v1",
    "title": "The Interpretability of Codebooks in Model-Based Reinforcement Learning is Limited",
    "authors": [
      "Kenneth Eaton",
      "Jonathan Balloch",
      "Julia Kim",
      "Mark Riedl"
    ],
    "abstract": "Interpretability of deep reinforcement learning systems could assist\noperators with understanding how they interact with their environment. Vector\nquantization methods -- also called codebook methods -- discretize a neural\nnetwork's latent space that is often suggested to yield emergent\ninterpretability. We investigate whether vector quantization in fact provides\ninterpretability in model-based reinforcement learning. Our experiments,\nconducted in the reinforcement learning environment Crafter, show that the\ncodes of vector quantization models are inconsistent, have no guarantee of\nuniqueness, and have a limited impact on concept disentanglement, all of which\nare necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19532v1",
    "published_date": "2024-07-28 16:40:20 UTC",
    "updated_date": "2024-07-28 16:40:20 UTC"
  },
  {
    "arxiv_id": "2407.19524v3",
    "title": "VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary",
    "authors": [
      "Hanjun Luo",
      "Ziye Deng",
      "Haoyu Huang",
      "Xuecheng Liu",
      "Ruizhe Chen",
      "Zuozhu Liu"
    ],
    "abstract": "With the rapid development of Text-to-Image (T2I) models, biases in human\nimage generation against demographic social groups become a significant\nconcern, impacting fairness and ethical standards in AI. Some researchers\npropose their methods to tackle with the issue. However, existing methods are\ndesigned for specific models with fixed prompts, limiting their adaptability to\nthe fast-evolving models and diverse practical scenarios. Moreover, they\nneglect the impact of hallucinations, leading to discrepancies between expected\nand actual results. To address these issues, we introduce VersusDebias, a novel\nand universal debiasing framework for biases in arbitrary T2I models,\nconsisting of an array generation (AG) module and an image generation (IG)\nmodule. The self-adaptive AG module generates specialized attribute arrays to\npost-process hallucinations and debias multiple attributes simultaneously. The\nIG module employs a small language model to modify prompts according to the\narrays and drives the T2I model to generate debiased images, enabling zero-shot\ndebiasing. Extensive experiments demonstrate VersusDebias's capability to\ndebias any models across gender, race, and age simultaneously. In both\nzero-shot and few-shot scenarios, VersusDebias outperforms existing methods,\nshowcasing its exceptional utility. Our work is accessible at\nhttps://github.com/VersusDebias/VersusDebias to ensure reproducibility and\nfacilitate further research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19524v3",
    "published_date": "2024-07-28 16:24:07 UTC",
    "updated_date": "2024-08-16 06:24:24 UTC"
  },
  {
    "arxiv_id": "2407.19517v1",
    "title": "Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload",
    "authors": [
      "Limin Ma",
      "Ken Pu",
      "Ying Zhu"
    ],
    "abstract": "This study presents a comparative analysis of the a complex SQL benchmark,\nTPC-DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings\nreveal that TPC-DS queries exhibit a significantly higher level of structural\ncomplexity compared to the other two benchmarks. This underscores the need for\nmore intricate benchmarks to simulate realistic scenarios effectively. To\nfacilitate this comparison, we devised several measures of structural\ncomplexity and applied them across all three benchmarks. The results of this\nstudy can guide future research in the development of more sophisticated\ntext-to-SQL benchmarks.\n  We utilized 11 distinct Language Models (LLMs) to generate SQL queries based\non the query descriptions provided by the TPC-DS benchmark. The prompt\nengineering process incorporated both the query description as outlined in the\nTPC-DS specification and the database schema of TPC-DS. Our findings indicate\nthat the current state-of-the-art generative AI models fall short in generating\naccurate decision-making queries. We conducted a comparison of the generated\nqueries with the TPC-DS gold standard queries using a series of fuzzy structure\nmatching techniques based on query features. The results demonstrated that the\naccuracy of the generated queries is insufficient for practical real-world\napplication.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19517v1",
    "published_date": "2024-07-28 15:53:05 UTC",
    "updated_date": "2024-07-28 15:53:05 UTC"
  },
  {
    "arxiv_id": "2407.19507v2",
    "title": "WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting",
    "authors": [
      "Jingjing Wu",
      "Zhengyao Fang",
      "Pengyuan Lyu",
      "Chengquan Zhang",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei"
    ],
    "abstract": "Transcription-only Supervised Text Spotting aims to learn text spotters\nrelying only on transcriptions but no text boundaries for supervision, thus\neliminating expensive boundary annotation. The crux of this task lies in\nlocating each transcription in scene text images without location annotations.\nIn this work, we formulate this challenging problem as a Weakly Supervised\nCross-modality Contrastive Learning problem, and design a simple yet effective\nmodel dubbed WeCromCL that is able to detect each transcription in a scene\nimage in a weakly supervised manner. Unlike typical methods for cross-modality\ncontrastive learning that focus on modeling the holistic semantic correlation\nbetween an entire image and a text description, our WeCromCL conducts atomistic\ncontrastive learning to model the character-wise appearance consistency between\na text transcription and its correlated region in a scene image to detect an\nanchor point for the transcription in a weakly supervised manner. The detected\nanchor points by WeCromCL are further used as pseudo location labels to guide\nthe learning of text spotting. Extensive experiments on four challenging\nbenchmarks demonstrate the superior performance of our model over other\nmethods. Code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19507v2",
    "published_date": "2024-07-28 14:58:07 UTC",
    "updated_date": "2025-01-13 08:58:40 UTC"
  },
  {
    "arxiv_id": "2407.21069v1",
    "title": "High-Dimensional Fault Tolerance Testing of Highly Automated Vehicles Based on Low-Rank Models",
    "authors": [
      "Yuewen Mei",
      "Tong Nie",
      "Jian Sun",
      "Ye Tian"
    ],
    "abstract": "Ensuring fault tolerance of Highly Automated Vehicles (HAVs) is crucial for\ntheir safety due to the presence of potentially severe faults. Hence, Fault\nInjection (FI) testing is conducted by practitioners to evaluate the safety\nlevel of HAVs. To fully cover test cases, various driving scenarios and fault\nsettings should be considered. However, due to numerous combinations of test\nscenarios and fault settings, the testing space can be complex and\nhigh-dimensional. In addition, evaluating performance in all newly added\nscenarios is resource-consuming. The rarity of critical faults that can cause\nsecurity problems further strengthens the challenge. To address these\nchallenges, we propose to accelerate FI testing under the low-rank Smoothness\nRegularized Matrix Factorization (SRMF) framework. We first organize the sparse\nevaluated data into a structured matrix based on its safety values. Then the\nuntested values are estimated by the correlation captured by the matrix\nstructure. To address high dimensionality, a low-rank constraint is imposed on\nthe testing space. To exploit the relationships between existing scenarios and\nnew scenarios and capture the local regularity of critical faults, three types\nof smoothness regularization are further designed as a complement. We conduct\nexperiments on car following and cut in scenarios. The results indicate that\nSRMF has the lowest prediction error in various scenarios and is capable of\npredicting rare critical faults compared to other machine learning models. In\naddition, SRMF can achieve 1171 acceleration rate, 99.3% precision and 91.1% F1\nscore in identifying critical faults. To the best of our knowledge, this is the\nfirst work to introduce low-rank models to FI testing of HAVs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ITSC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21069v1",
    "published_date": "2024-07-28 14:27:13 UTC",
    "updated_date": "2024-07-28 14:27:13 UTC"
  },
  {
    "arxiv_id": "2407.19493v3",
    "title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake News Detection",
    "authors": [
      "Yihao Wang",
      "Lizhi Chen",
      "Zhong Qian",
      "Peifeng Li"
    ],
    "abstract": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19493v3",
    "published_date": "2024-07-28 13:23:43 UTC",
    "updated_date": "2024-12-27 10:34:15 UTC"
  },
  {
    "arxiv_id": "2407.19492v1",
    "title": "Heads Up eXperience (HUX): Always-On AI Companion for Human Computer Environment Interaction",
    "authors": [
      "Sukanth K",
      "Sudhiksha Kandavel Rajan",
      "Rajashekhar V S",
      "Gowdham Prabhakar"
    ],
    "abstract": "While current personal smart devices excel in digital domains, they fall\nshort in assisting users during human environment interaction. This paper\nproposes Heads Up eXperience (HUX), an AI system designed to bridge this gap,\nserving as a constant companion across the extended reality (XR) environments.\nBy tracking the user's eye gaze, analyzing the surrounding environment, and\ninterpreting verbal contexts, the system captures and enhances multi-modal\ndata, providing holistic context interpretation and memory storage in real-time\ntask specific situations. This comprehensive approach enables more natural,\nempathetic and intelligent interactions between the user and HUX AI, paving the\npath for human computer environment interaction. Intended for deployment in\nsmart glasses and extended reality headsets, HUX AI aims to become a personal\nand useful AI companion for daily life. By integrating digital assistance with\nenhanced physical world interactions, this technology has the potential to\nrevolutionize human-AI collaboration in both personal and professional spheres\npaving the way for the future of personal smart devices.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "48 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19492v1",
    "published_date": "2024-07-28 13:15:51 UTC",
    "updated_date": "2024-07-28 13:15:51 UTC"
  },
  {
    "arxiv_id": "2407.19475v1",
    "title": "Multi-task Neural Networks for Pain Intensity Estimation using Electrocardiogram and Demographic Factors",
    "authors": [
      "Stefanos Gkikas",
      "Chariklia Chatzaki",
      "Manolis Tsiknakis"
    ],
    "abstract": "Pain is a complex phenomenon which is manifested and expressed by patients in\nvarious forms. The immediate and objective recognition of it is a great of\nimportance in order to attain a reliable and unbiased healthcare system. In\nthis work, we elaborate electrocardiography signals revealing the existence of\nvariations in pain perception among different demographic groups. We exploit\nthis insight by introducing a novel multi-task neural network for automatic\npain estimation utilizing the age and the gender information of each\nindividual, and show its advantages compared to other approaches.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19475v1",
    "published_date": "2024-07-28 11:57:50 UTC",
    "updated_date": "2024-07-28 11:57:50 UTC"
  },
  {
    "arxiv_id": "2407.19469v1",
    "title": "Interpretable Triplet Importance for Personalized Ranking",
    "authors": [
      "Bowei He",
      "Chen Ma"
    ],
    "abstract": "Personalized item ranking has been a crucial component contributing to the\nperformance of recommender systems. As a representative approach, pairwise\nranking directly optimizes the ranking with user implicit feedback by\nconstructing (\\textit{user}, \\textit{positive item}, \\textit{negative item})\ntriplets. Several recent works have noticed that treating all triplets equally\nmay hardly achieve the best effects. They assign different importance scores to\nnegative items, user-item pairs, or triplets, respectively. However, almost all\nthe generated importance scores are groundless and hard to interpret, thus far\nfrom trustworthy and transparent. To tackle these, we propose the\n\\textit{Triplet Shapley} -- a Shapely value-based method to measure the triplet\nimportance in an interpretable manner. Due to the huge number of triplets, we\ntransform the original Shapley value calculation to the Monte Carlo (MC)\napproximation, where the guarantee for the approximation unbiasedness is also\nprovided. To stabilize the MC approximation, we adopt a control\ncovariates-based method. Finally, we utilize the triplet Shapley value to guide\nthe resampling of important triplets for benefiting the model learning.\nExtensive experiments are conducted on six public datasets involving classical\nmatrix factorization- and graph neural network-based recommendation models.\nEmpirical results and subsequent analysis show that our model consistently\noutperforms the state-of-the-art methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19469v1",
    "published_date": "2024-07-28 11:46:55 UTC",
    "updated_date": "2024-07-28 11:46:55 UTC"
  },
  {
    "arxiv_id": "2407.19447v1",
    "title": "Nudging Consent and the New Opt Out System to the Processing of Health Data in England",
    "authors": [
      "Janos Meszaros",
      "Chih-hsing Ho",
      "Marcelo Corrales Compagnucci"
    ],
    "abstract": "This chapter examines the challenges of the revised opt out system and the\nsecondary use of health data in England. The analysis of this data could be\nvery valuable for science and medical treatment as well as for the discovery of\nnew drugs. For this reason, the UK government established the care.data program\nin 2013. The aim of the project was to build a central nationwide database for\nresearch and policy planning. However, the processing of personal data was\nplanned without proper public engagement. Research has suggested that IT\ncompanies, such as in the Google DeepMind deal case, had access to other kinds\nof sensitive data and failed to comply with data protection law. Since May\n2018, the government has launched the national data opt out system with the\nhope of regaining public trust. Nevertheless, there are no evidence of\nsignificant changes in the ND opt out, compared to the previous opt out system.\nNeither in the use of secondary data, nor in the choices that patients can\nmake. The only notorious difference seems to be in the way that these options\nare communicated and framed to the patients. Most importantly, according to the\nnew ND opt out, the type 1 opt out option, which is the only choice that truly\nstops data from being shared outside direct care, will be removed in 2020.\nAccording to the Behavioral Law and Economics literature (Nudge Theory),\ndefault rules, such as the revised opt out system in England, are very\npowerful, because people tend to stick to the default choices made readily\navailable to them. The crucial question analyzed in this chapter is whether it\nis desirable for the UK government to stop promoting the type 1 opt outs, and\nwhether this could be seen as a kind of hard paternalism.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19447v1",
    "published_date": "2024-07-28 09:49:37 UTC",
    "updated_date": "2024-07-28 09:49:37 UTC"
  },
  {
    "arxiv_id": "2407.19439v1",
    "title": "Business and Regulatory Responses to Artificial Intelligence: Dynamic Regulation, Innovation Ecosystems and the Strategic Management of Disruptive Technology",
    "authors": [
      "Mark Fenwick",
      "Erik P. M. Vermeulen",
      "Marcelo Corrales Compagnucci"
    ],
    "abstract": "Identifying and then implementing an effective response to disruptive new AI\ntechnologies is enormously challenging for any business looking to integrate AI\ninto their operations, as well as regulators looking to leverage AI-related\ninnovation as a mechanism for achieving regional economic growth. These\nbusiness and regulatory challenges are particularly significant given the broad\nreach of AI, as well as the multiple uncertainties surrounding such\ntechnologies and their future development and effects. This article identifies\ntwo promising strategies for meeting the AI challenge, focusing on the example\nof Fintech. First, dynamic regulation, in the form of regulatory sandboxes and\nother regulatory approaches that aim to provide a space for responsible\nAI-related innovation. An empirical study provides preliminary evidence to\nsuggest that jurisdictions that adopt a more proactive approach to Fintech\nregulation can attract greater investment. The second strategy relates to\nso-called innovation ecosystems. It is argued that such ecosystems are most\neffective when they afford opportunities for creative partnerships between\nwell-established corporations and AI-focused startups and that this aspect of a\nsuccessful innovation ecosystem is often overlooked in the existing discussion.\nThe article suggests that these two strategies are interconnected, in that\ngreater investment is an important element in both fostering and signaling a\nwell-functioning innovation ecosystem and that a well-functioning ecosystem\nwill, in turn, attract more funding. The resulting synergies between these\nstrategies can, therefore, provide a jurisdiction with a competitive edge in\nbecoming a regional hub for AI-related activity.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19439v1",
    "published_date": "2024-07-28 09:34:03 UTC",
    "updated_date": "2024-07-28 09:34:03 UTC"
  },
  {
    "arxiv_id": "2407.19438v1",
    "title": "Conversational AI Multi-Agent Interoperability, Universal Open APIs for Agentic Natural Language Multimodal Communications",
    "authors": [
      "Diego Gosmar",
      "Deborah A. Dahl",
      "Emmett Coin"
    ],
    "abstract": "This paper analyses Conversational AI multi-agent interoperability frameworks\nand describes the novel architecture proposed by the Open Voice\nInteroperability initiative (Linux Foundation AI and DATA), also known briefly\nas OVON (Open Voice Network). The new approach is illustrated, along with the\nmain components, delineating the key benefits and use cases for deploying\nstandard multi-modal AI agency (or agentic AI) communications. Beginning with\nUniversal APIs based on Natural Language, the framework establishes and enables\ninteroperable interactions among diverse Conversational AI agents, including\nchatbots, voicebots, videobots, and human agents. Furthermore, a new Discovery\nspecification framework is introduced, designed to efficiently look up agents\nproviding specific services and to obtain accurate information about these\nservices through a standard Manifest publication, accessible via an extended\nset of Natural Language-based APIs. The main purpose of this contribution is to\nsignificantly enhance the capabilities and scalability of AI interactions\nacross various platforms. The novel architecture for interoperable\nConversational AI assistants is designed to generalize, being replicable and\naccessible via open repositories.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19438v1",
    "published_date": "2024-07-28 09:33:55 UTC",
    "updated_date": "2024-07-28 09:33:55 UTC"
  },
  {
    "arxiv_id": "2407.19435v1",
    "title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding",
    "authors": [
      "Zhen Chen",
      "Zongming Zhang",
      "Wenwu Guo",
      "Xingjian Luo",
      "Long Bai",
      "Jinlin Wu",
      "Hongliang Ren",
      "Hongbin Liu"
    ],
    "abstract": "Surgical instrument segmentation is crucial in surgical scene understanding,\nthereby facilitating surgical safety. Existing algorithms directly detected all\ninstruments of pre-defined categories in the input image, lacking the\ncapability to segment specific instruments according to the surgeon's\nintention. During different stages of surgery, surgeons exhibit varying\npreferences and focus toward different surgical instruments. Therefore, an\ninstrument segmentation algorithm that adheres to the surgeon's intention can\nminimize distractions from irrelevant instruments and assist surgeons to a\ngreat extent. The recent Segment Anything Model (SAM) reveals the capability to\nsegment objects following prompts, but the manual annotations for prompts are\nimpractical during the surgery. To address these limitations in operating\nrooms, we propose an audio-driven surgical instrument segmentation framework,\nnamed ASI-Seg, to accurately segment the required surgical instruments by\nparsing the audio commands of surgeons. Specifically, we propose an\nintention-oriented multimodal fusion to interpret the segmentation intention\nfrom audio commands and retrieve relevant instrument details to facilitate\nsegmentation. Moreover, to guide our ASI-Seg segment of the required surgical\ninstruments, we devise a contrastive learning prompt encoder to effectively\ndistinguish the required instruments from the irrelevant ones. Therefore, our\nASI-Seg promotes the workflow in the operating rooms, thereby providing\ntargeted support and reducing the cognitive load on surgeons. Extensive\nexperiments are performed to validate the ASI-Seg framework, which reveals\nremarkable advantages over classical state-of-the-art and medical SAMs in both\nsemantic segmentation and intention-oriented segmentation. The source code is\navailable at https://github.com/Zonmgin-Zhang/ASI-Seg.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This work is accepted by IROS 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2407.19435v1",
    "published_date": "2024-07-28 09:25:59 UTC",
    "updated_date": "2024-07-28 09:25:59 UTC"
  },
  {
    "arxiv_id": "2407.19426v1",
    "title": "Causal Discovery in Linear Models with Unobserved Variables and Measurement Error",
    "authors": [
      "Yuqin Yang",
      "Mohamed Nafea",
      "Negar Kiyavash",
      "Kun Zhang",
      "AmirEmad Ghassami"
    ],
    "abstract": "The presence of unobserved common causes and the presence of measurement\nerror are two of the most limiting challenges in the task of causal structure\nlearning. Ignoring either of the two challenges can lead to detecting spurious\ncausal links among variables of interest. In this paper, we study the problem\nof causal discovery in systems where these two challenges can be present\nsimultaneously. We consider linear models which include four types of\nvariables: variables that are directly observed, variables that are not\ndirectly observed but are measured with error, the corresponding measurements,\nand variables that are neither observed nor measured. We characterize the\nextent of identifiability of such model under separability condition (i.e., the\nmatrix indicating the independent exogenous noise terms pertaining to the\nobserved variables is identifiable) together with two versions of faithfulness\nassumptions and propose a notion of observational equivalence. We provide\ngraphical characterization of the models that are equivalent and present a\nrecovery algorithm that could return models equivalent to the ground truth.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19426v1",
    "published_date": "2024-07-28 08:26:56 UTC",
    "updated_date": "2024-07-28 08:26:56 UTC"
  },
  {
    "arxiv_id": "2408.03468v2",
    "title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili",
    "authors": [
      "Han Wang",
      "Tan Rui Yang",
      "Usman Naseem",
      "Roy Ka-Wei Lee"
    ],
    "abstract": "Hate speech is a pressing issue in modern society, with significant effects\nboth online and offline. Recent research in hate speech detection has primarily\ncentered on text-based media, largely overlooking multimodal content such as\nvideos. Existing studies on hateful video datasets have predominantly focused\non English content within a Western context and have been limited to binary\nlabels (hateful or non-hateful), lacking detailed contextual information. This\nstudy presents MultiHateClip1 , an novel multilingual dataset created through\nhate lexicons and human annotation. It aims to enhance the detection of hateful\nvideos on platforms such as YouTube and Bilibili, including content in both\nEnglish and Chinese languages. Comprising 2,000 videos annotated for\nhatefulness, offensiveness, and normalcy, this dataset provides a\ncross-cultural perspective on gender-based hate speech. Through a detailed\nexamination of human annotation results, we discuss the differences between\nChinese and English hateful videos and underscore the importance of different\nmodalities in hateful and offensive video analysis. Evaluations of\nstate-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL,\non MultiHateClip highlight the existing challenges in accurately distinguishing\nbetween hateful and offensive content and the urgent need for models that are\nboth multimodally and culturally nuanced. MultiHateClip represents a\nfoundational advance in enhancing hateful video detection by underscoring the\nnecessity of a multimodal and culturally sensitive approach in combating online\nhate speech.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "I.2.0"
    ],
    "primary_category": "cs.MM",
    "comment": "10 pages, 3 figures, ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03468v2",
    "published_date": "2024-07-28 08:19:09 UTC",
    "updated_date": "2024-08-12 06:01:33 UTC"
  },
  {
    "arxiv_id": "2407.19422v1",
    "title": "A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy",
    "authors": [
      "Meng Jiang",
      "Qing Zhao",
      "Jianqiang Li",
      "Fan Wang",
      "Tianyu He",
      "Xinyan Cheng",
      "Bing Xiang Yang",
      "Grace W. K. Ho",
      "Guanghui Fu"
    ],
    "abstract": "Cognitive Behavioral Therapy (CBT) is a well-established intervention for\nmitigating psychological issues by modifying maladaptive cognitive and\nbehavioral patterns. However, delivery of CBT is often constrained by resource\nlimitations and barriers to access. Advancements in artificial intelligence\n(AI) have provided technical support for the digital transformation of CBT.\nParticularly, the emergence of pre-training models (PTMs) and large language\nmodels (LLMs) holds immense potential to support, augment, optimize and\nautomate CBT delivery. This paper reviews the literature on integrating AI into\nCBT interventions. We begin with an overview of CBT. Then, we introduce the\nintegration of AI into CBT across various stages: pre-treatment, therapeutic\nprocess, and post-treatment. Next, we summarized the datasets relevant to some\nCBT-related tasks. Finally, we discuss the benefits and current limitations of\napplying AI to CBT. We suggest key areas for future research, highlighting the\nneed for further exploration and validation of the long-term efficacy and\nclinical utility of AI-enhanced CBT. The transformative potential of AI in\nreshaping the practice of CBT heralds a new era of more accessible, efficient,\nand personalized mental health interventions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19422v1",
    "published_date": "2024-07-28 08:09:46 UTC",
    "updated_date": "2024-07-28 08:09:46 UTC"
  },
  {
    "arxiv_id": "2407.19415v1",
    "title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross Modal Retrieval",
    "authors": [
      "Zeyu Chen",
      "Pengfei Zhang",
      "Kai Ye",
      "Wei Dong",
      "Xin Feng",
      "Yana Zhang"
    ],
    "abstract": "The burgeoning short video industry has accelerated the advancement of\nvideo-music retrieval technology, assisting content creators in selecting\nappropriate music for their videos. In self-supervised training for\nvideo-to-music retrieval, the video and music samples in the dataset are\nseparated from the same video work, so they are all one-to-one matches. This\ndoes not match the real situation. In reality, a video can use different music\nas background music, and a music can be used as background music for different\nvideos. Many videos and music that are not in a pair may be compatible, leading\nto false negative noise in the dataset. A novel inter-intra modal (II) loss is\nproposed as a solution. By reducing the variation of feature distribution\nwithin the two modalities before and after the encoder, II loss can reduce the\nmodel's overfitting to such noise without removing it in a costly and laborious\nway. The video-music retrieval framework, II-CLVM (Contrastive Learning for\nVideo-Music Retrieval), incorporating the II Loss, achieves state-of-the-art\nperformance on the YouTube8M dataset. The framework II-CLVTM shows better\nperformance when retrieving music using multi-modal video information (such as\ntext in videos). Experiments are designed to show that II loss can effectively\nalleviate the problem of false negative noise in retrieval tasks. Experiments\nalso show that II loss improves various self-supervised and supervised\nuni-modal and cross-modal retrieval tasks, and can obtain good retrieval models\nwith a small amount of training samples.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "I.2; I.4"
    ],
    "primary_category": "cs.MM",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19415v1",
    "published_date": "2024-07-28 07:06:28 UTC",
    "updated_date": "2024-07-28 07:06:28 UTC"
  },
  {
    "arxiv_id": "2407.19414v1",
    "title": "Appformer: A Novel Framework for Mobile App Usage Prediction Leveraging Progressive Multi-Modal Data Fusion and Feature Extraction",
    "authors": [
      "Chuike Sun",
      "Junzhou Chen",
      "Yue Zhao",
      "Hao Han",
      "Ruihai Jing",
      "Guang Tan",
      "Di Wu"
    ],
    "abstract": "This article presents Appformer, a novel mobile application prediction\nframework inspired by the efficiency of Transformer-like architectures in\nprocessing sequential data through self-attention mechanisms. Combining a\nMulti-Modal Data Progressive Fusion Module with a sophisticated Feature\nExtraction Module, Appformer leverages the synergies of multi-modal data fusion\nand data mining techniques while maintaining user privacy. The framework\nemploys Points of Interest (POIs) associated with base stations, optimizing\nthem through comprehensive comparative experiments to identify the most\neffective clustering method. These refined inputs are seamlessly integrated\ninto the initial phases of cross-modal data fusion, where temporal units are\nencoded via word embeddings and subsequently merged in later stages. The\nFeature Extraction Module, employing Transformer-like architectures specialized\nfor time series analysis, adeptly distils comprehensive features. It\nmeticulously fine-tunes the outputs from the fusion module, facilitating the\nextraction of high-calibre, multi-modal features, thus guaranteeing a robust\nand efficient extraction process. Extensive experimental validation confirms\nAppformer's effectiveness, attaining state-of-the-art (SOTA) metrics in mobile\napp usage prediction, thereby signifying a notable progression in this field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19414v1",
    "published_date": "2024-07-28 06:41:31 UTC",
    "updated_date": "2024-07-28 06:41:31 UTC"
  },
  {
    "arxiv_id": "2407.19412v1",
    "title": "Identity-Driven Hierarchical Role-Playing Agents",
    "authors": [
      "Libo Sun",
      "Siyuan Wang",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "Utilizing large language models (LLMs) to achieve role-playing has gained\ngreat attention recently. The primary implementation methods include leveraging\nrefined prompts and fine-tuning on role-specific datasets. However, these\nmethods suffer from insufficient precision and limited flexibility\nrespectively. To achieve a balance between flexibility and precision, we\nconstruct a Hierarchical Identity Role-Playing Framework (HIRPF) based on\nidentity theory, constructing complex characters using multiple identity\ncombinations. We develop an identity dialogue dataset for this framework and\npropose an evaluation benchmark including scale evaluation and open situation\nevaluation. Empirical results indicate the remarkable efficacy of our framework\nin modeling identity-level role simulation, and reveal its potential for\napplication in social simulation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19412v1",
    "published_date": "2024-07-28 06:38:56 UTC",
    "updated_date": "2024-07-28 06:38:56 UTC"
  },
  {
    "arxiv_id": "2407.19410v1",
    "title": "AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering",
    "authors": [
      "Mahiro Ukai",
      "Shuhei Kurita",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku",
      "Nakamasa Inoue"
    ],
    "abstract": "Visual question answering aims to provide responses to natural language\nquestions given visual input. Recently, visual programmatic models (VPMs),\nwhich generate executable programs to answer questions through large language\nmodels (LLMs), have attracted research interest. However, they often require\nlong input prompts to provide the LLM with sufficient API usage details to\ngenerate relevant code. To address this limitation, we propose AdaCoder, an\nadaptive prompt compression framework for VPMs. AdaCoder operates in two\nphases: a compression phase and an inference phase. In the compression phase,\ngiven a preprompt that describes all API definitions in the Python language\nwith example snippets of code, a set of compressed preprompts is generated,\neach depending on a specific question type. In the inference phase, given an\ninput question, AdaCoder predicts the question type and chooses the appropriate\ncorresponding compressed preprompt to generate code to answer the question.\nNotably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating\nthe necessity of additional training and maintaining adaptability across\ndifferent powerful black-box LLMs such as GPT and Claude. In experiments, we\napply AdaCoder to ViperGPT and demonstrate that it reduces token length by\n71.1%, while maintaining or even improving the performance of visual question\nanswering.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19410v1",
    "published_date": "2024-07-28 06:23:06 UTC",
    "updated_date": "2024-07-28 06:23:06 UTC"
  },
  {
    "arxiv_id": "2407.19405v1",
    "title": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making",
    "authors": [
      "Dong Chen",
      "Shilin Zhang",
      "Fei Gao",
      "Yueting Zhuang",
      "Siliang Tang",
      "Qidong Liu",
      "Mingliang Xu"
    ],
    "abstract": "Large language models (LLMs) have garnered increasing attention owing to\ntheir powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs)\nthat require paid interfaces exhibit significantly superior performance\ncompared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices.\nKnowledge distillation (KD) aims to empower S-LLMs with the capabilities of\nL-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get the\npowerful logical reasoning capabilities. Consequently, S-LLMs are helpless when\nit comes to planning and decision-making tasks that require logical reasoning\ncapabilities. To tackle the identified challenges, we propose a novel framework\ncalled Logic Distillation (LD). Initially, LD employs L-LLMs to instantiate\ncomplex instructions into discrete functions and illustrates their usage to\nestablish a function base. Subsequently, based on the function base, LD\nfine-tunes S-LLMs to learn the logic employed by L-LLMs in planning and\ndecision-making. During testing, LD utilizes a retriever to identify the\ntop-$K$ relevant functions based on instructions and current states, which will\nbe selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning and\ndecision-making outcomes, function by function. Relevant experiments\ndemonstrate that with the assistance of LD, S-LLMs can achieve outstanding\nresults in planning and decision-making tasks, comparable to, or even\nsurpassing, those of L-LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19405v1",
    "published_date": "2024-07-28 05:34:42 UTC",
    "updated_date": "2024-07-28 05:34:42 UTC"
  },
  {
    "arxiv_id": "2407.21066v1",
    "title": "ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech Processing Tasks",
    "authors": [
      "Nakamasa Inoue",
      "Shinta Otake",
      "Takumi Hirose",
      "Masanari Ohi",
      "Rei Kawakami"
    ],
    "abstract": "Self-supervised learning has emerged as a key approach for learning generic\nrepresentations from speech data. Despite promising results in downstream tasks\nsuch as speech recognition, speaker verification, and emotion recognition, a\nsignificant number of parameters is required, which makes fine-tuning for each\ntask memory-inefficient. To address this limitation, we introduce ELP-adapter\ntuning, a novel method for parameter-efficient fine-tuning using three types of\nadapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and\na prompt adapter (P-adapter). The E-adapters are integrated into\ntransformer-based encoder layers and help to learn fine-grained speech\nrepresentations that are effective for speech recognition. The L-adapters\ncreate paths from each encoder layer to the downstream head and help to extract\nnon-linguistic features from lower encoder layers that are effective for\nspeaker verification and emotion recognition. The P-adapter appends pseudo\nfeatures to CNN features to further improve effectiveness and efficiency. With\nthese adapters, models can be quickly adapted to various speech processing\ntasks. Our evaluation across four downstream tasks using five backbone models\ndemonstrated the effectiveness of the proposed method. With the WavLM backbone,\nits performance was comparable to or better than that of full fine-tuning on\nall tasks while requiring 90% fewer learnable parameters.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21066v1",
    "published_date": "2024-07-28 05:26:03 UTC",
    "updated_date": "2024-07-28 05:26:03 UTC"
  },
  {
    "arxiv_id": "2407.19401v2",
    "title": "Towards Secure and Private AI: A Framework for Decentralized Inference",
    "authors": [
      "Hongyang Zhang",
      "Yue Zhao",
      "Claudio Angione",
      "Harry Yang",
      "James Buban",
      "Ahmad Farhan",
      "Fielding Johnston",
      "Patrick Colangelo"
    ],
    "abstract": "The rapid advancement of ML models in critical sectors such as healthcare,\nfinance, and security has intensified the need for robust data security, model\nintegrity, and reliable outputs. Large multimodal foundational models, while\ncrucial for complex tasks, present challenges in scalability, reliability, and\npotential misuse. Decentralized systems offer a solution by distributing\nworkload and mitigating central points of failure, but they introduce risks of\nunauthorized access to sensitive data across nodes. We address these challenges\nwith a comprehensive framework designed for responsible AI development. Our\napproach incorporates: 1) Zero-knowledge proofs for secure model verification,\nenhancing trust without compromising privacy. 2) Consensus-based verification\nchecks to ensure consistent outputs across nodes, mitigating hallucinations and\nmaintaining model integrity. 3) Split Learning techniques that segment models\nacross different nodes, preserving data privacy by preventing full data access\nat any point. 4) Hardware-based security through trusted execution environments\n(TEEs) to protect data and computations. This framework aims to enhance\nsecurity and privacy and improve the reliability and fairness of multimodal AI\nsystems. Promoting efficient resource utilization contributes to more\nsustainable AI development. Our state-of-the-art proofs and principles\ndemonstrate the framework's effectiveness in responsibly democratizing\nartificial intelligence, offering a promising approach for building secure and\nprivate foundational models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.19401v2",
    "published_date": "2024-07-28 05:09:17 UTC",
    "updated_date": "2024-12-12 18:10:51 UTC"
  },
  {
    "arxiv_id": "2407.20294v2",
    "title": "A Bayesian Flow Network Framework for Chemistry Tasks",
    "authors": [
      "Nianze Tao",
      "Minori Abe"
    ],
    "abstract": "In this work, we introduce ChemBFN, a language model that handles chemistry\ntasks based on Bayesian flow networks working on discrete data. A new accuracy\nschedule is proposed to improve the sampling quality by significantly reducing\nthe reconstruction loss. We show evidence that our method is appropriate for\ngenerating molecules with satisfied diversity even when a smaller number of\nsampling steps is used. A classifier-free guidance method is adapted for\nconditional generation. It is also worthwhile to point out that after\ngenerative training, our model can be fine-tuned on regression and\nclassification tasks with the state-of-the-art performance, which opens the\ngate of building all-in-one models in a single module style. Our model has been\nopen sourced at\nhttps://github.com/Augus1999/bayesian-flow-network-for-chemistry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "7 figures, 12 tables, 27 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.20294v2",
    "published_date": "2024-07-28 04:46:32 UTC",
    "updated_date": "2025-01-03 03:38:19 UTC"
  },
  {
    "arxiv_id": "2407.19396v1",
    "title": "NAVIX: Scaling MiniGrid Environments with JAX",
    "authors": [
      "Eduardo Pignatelli",
      "Jarek Liesen",
      "Robert Tjarko Lange",
      "Chris Lu",
      "Pablo Samuel Castro",
      "Laura Toni"
    ],
    "abstract": "As Deep Reinforcement Learning (Deep RL) research moves towards solving\nlarge-scale worlds, efficient environment simulations become crucial for rapid\nexperimentation. However, most existing environments struggle to scale to high\nthroughput, setting back meaningful progress. Interactions are typically\ncomputed on the CPU, limiting training speed and throughput, due to slower\ncomputation and communication overhead when distributing the task across\nmultiple machines. Ultimately, Deep RL training is CPU-bound, and developing\nbatched, fast, and scalable environments has become a frontier for progress.\nAmong the most used Reinforcement Learning (RL) environments, MiniGrid is at\nthe foundation of several studies on exploration, curriculum learning,\nrepresentation learning, diversity, meta-learning, credit assignment, and\nlanguage-conditioned RL, and still suffers from the limitations described\nabove. In this work, we introduce NAVIX, a re-implementation of MiniGrid in\nJAX. NAVIX achieves over 200 000x speed improvements in batch mode, supporting\nup to 2048 agents in parallel on a single Nvidia A100 80 GB. This reduces\nexperiment times from one week to 15 minutes, promoting faster design\niterations and more scalable RL model development.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19396v1",
    "published_date": "2024-07-28 04:39:18 UTC",
    "updated_date": "2024-07-28 04:39:18 UTC"
  },
  {
    "arxiv_id": "2407.19393v2",
    "title": "Integrating Cognitive AI with Generative Models for Enhanced Question Answering in Skill-based Learning",
    "authors": [
      "Rochan H. Madhusudhana",
      "Rahul K. Dass",
      "Jeanette Luu",
      "Ashok K. Goel"
    ],
    "abstract": "In online learning, the ability to provide quick and accurate feedback to\nlearners is crucial. In skill-based learning, learners need to understand the\nunderlying concepts and mechanisms of a skill to be able to apply it\neffectively. While videos are a common tool in online learning, they cannot\ncomprehend or assess the skills being taught. Additionally, while Generative AI\nmethods are effective in searching and retrieving answers from a text corpus,\nit remains unclear whether these methods exhibit any true understanding. This\nlimits their ability to provide explanations of skills or help with\nproblem-solving. This paper proposes a novel approach that merges Cognitive AI\nand Generative AI to address these challenges. We employ a structured knowledge\nrepresentation, the TMK (Task-Method-Knowledge) model, to encode skills taught\nin an online Knowledge-based AI course. Leveraging techniques such as Large\nLanguage Models, Chain-of-Thought, and Iterative Refinement, we outline a\nframework for generating reasoned explanations in response to learners'\nquestions about skills.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 6 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.19393v2",
    "published_date": "2024-07-28 04:21:22 UTC",
    "updated_date": "2024-08-02 21:06:51 UTC"
  },
  {
    "arxiv_id": "2407.19385v1",
    "title": "Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification",
    "authors": [
      "Nagur Shareef Shaik",
      "Teja Krishna Cherukuri",
      "Vince D. Calhoun",
      "Dong Hye Ye"
    ],
    "abstract": "Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive\nimpairments, abnormalities in brain structure, function, and genetic factors.\nIts complex symptoms and overlap with other psychiatric conditions challenge\ntraditional diagnostic methods, necessitating advanced systems to improve\nprecision. Existing research studies have mostly focused on imaging data, such\nas structural and functional MRI, for SZ diagnosis. There has been less focus\non the integration of genomic features despite their potential in identifying\nheritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics\nTransformer (MIGTrans), that attentively integrates genomics with structural\nand functional imaging data to capture SZ-related neuroanatomical and\nconnectome abnormalities. MIGTrans demonstrated improved SZ classification\nperformance with an accuracy of 86.05% (+/- 0.02), offering clear\ninterpretations and identifying significant genomic locations and brain\nmorphological/connectivity patterns associated with SZ.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for presentation at the AI for Imaging Genomic Learning\n  (AIIG) Workshop, MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19385v1",
    "published_date": "2024-07-28 03:54:08 UTC",
    "updated_date": "2024-07-28 03:54:08 UTC"
  },
  {
    "arxiv_id": "2407.19380v1",
    "title": "Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment",
    "authors": [
      "Aamer Abdul Rahman",
      "Pranav Agarwal",
      "Rita Noumeir",
      "Philippe Jouvet",
      "Vincent Michalski",
      "Samira Ebrahimi Kahou"
    ],
    "abstract": "Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19380v1",
    "published_date": "2024-07-28 03:40:00 UTC",
    "updated_date": "2024-07-28 03:40:00 UTC"
  },
  {
    "arxiv_id": "2407.19359v1",
    "title": "Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction",
    "authors": [
      "Yuan Xue",
      "Nan Du",
      "Anne Mottram",
      "Martin Seneviratne",
      "Andrew M. Dai"
    ],
    "abstract": "We propose to meta-learn an a self-supervised patient trajectory forecast\nlearning rule by meta-training on a meta-objective that directly optimizes the\nutility of the patient representation over the subsequent clinical outcome\nprediction. This meta-objective directly targets the usefulness of a\nrepresentation generated from unlabeled clinical measurement forecast for later\nsupervised tasks.\n  The meta-learned can then be directly used in target risk prediction, and the\nlimited available samples can be used for further fine-tuning the model\nperformance. The effectiveness of our approach is tested on a real open source\npatient EHR dataset MIMIC-III. We are able to demonstrate that our\nattention-based patient state representation approach can achieve much better\nperformance for predicting target risk with low resources comparing with both\ndirect supervised learning and pretraining with all-observation trajectory\nforecast.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2020",
    "pdf_url": "http://arxiv.org/pdf/2407.19359v1",
    "published_date": "2024-07-28 01:22:04 UTC",
    "updated_date": "2024-07-28 01:22:04 UTC"
  }
]
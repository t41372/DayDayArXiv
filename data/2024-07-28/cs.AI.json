{
  "date": "2024-07-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-28 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 论文主要聚焦于 AI 模型优化、多模态处理和实际应用领域，如 LLM 的自提升、医疗诊断和经济影响，令人印象深刻的是 Meta-Rewarding Language Models（由 Yuandong Tian 等知名学者发布），它展示了 LLM 无监督自提升的潜力，以及其他高效 AI 框架在语言处理和医疗中的创新。\n\n下面，我将挑选并简要讨论最具话题度和影响力的论文，先从 AI 和 LLM 相关的高影响力文章入手，再快速掠过其他领域的内容。每篇论文会列出标题（中文 + 英文），并清晰概述其核心贡献和发现。\n\n### 1. Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models（混合模块专家：从多语言教师模型中提炼知识到专业模块化语言模型）\n这篇论文提出了一种结合 Knowledge Distillation (KD) 和 Mixture of Experts (MoE) 的框架，用于构建高效的多语言模型。贡献包括评估自适应与固定 alpha 方法在 KD 中的性能，以及 MoE 架构在处理多领域输入和防止灾难性遗忘方面的表现，发现 MoE 可以有效保留多语言知识，并开源了相关数据集和代码。\n\n### 2. Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge（元奖励语言模型：通过 LLM 作为元判断器实现自提升对齐）\n由 Yuandong Tian 等知名学者发布，这篇论文引入了元奖励机制，让 LLM 通过自我判断反馈来提升判断和指令遵循能力。核心发现是，这种无监督方法使 Llama-3-8B-Instruct 在 AlpacaEval 2 和 Arena-Hard 上的胜率分别从 22.9% 提升到 39.4%，和从 20.6% 提升到 29.1%，展示了 LLM 自提升的巨大潜力。\n\n### 3. Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures（小批量核心集：用于数据混合中内存高效的语言模型训练）\n论文提出 CoLM 框架，通过梯度匹配和 Adam 优化器改进，显著减少 LLM 训练内存需求。贡献包括在 Phi-2 和 Llama-3 等模型上实现 2x 内存减少，同时性能优于 4x 大批量训练，并与 LoRA 无缝整合，突显了高效训练的实际价值。\n\n### 4. Is Generative AI an Existential Threat to Human Creatives? Insights from Financial Economics（生成式 AI 是否对人类创作者构成生存威胁？来自金融经济学的见解）\n作者 Jiasun Li 分析了生成式 AI（如 GPT）对创作者的影响，核心发现是 AI 无法完全取代人类，因为缺乏新内容输入会使 AI 模型停滞（类似于 Grossman-Stiglitz 效率悖论），强调了人类创造在内容更新的必要性。\n\n### 5. Are LLMs Good Annotators for Discourse-level Event Relation Extraction?（LLM 是否适合用于话语级事件关系提取？）\n论文评估了 GPT-3.5 和 LLaMA-2 在事件关系提取（如核心ference 和因果关系）中的表现，发现 LLM 显著落后于监督学习基线，且易出现虚构事件和忽略长距离关系。贡献包括定量分析 LLM 的弱点，并开源代码，提醒了 LLM 在复杂任务中的局限性。\n\n### 6. VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary（VersusDebias：通过 SLM 提示工程和生成对抗实现文本到图像模型的通用零样本去偏置）\n这篇论文提出一个通用框架，用于消除 T2I 模型中的性别、种族等偏置。核心发现是，该方法在零样本和少样本场景下优于现有方法，并开源代码，展示了在多属性去偏置中的有效性。\n\n### 7. AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering（AdaCoder：用于程序化视觉问答的自适应提示压缩）\n论文引入 AdaCoder 框架，通过自适应提示压缩优化 VPM 模型。贡献包括在不额外训练的情况下减少提示长度 71.1%，同时维持或提升视觉问答性能，突显了高效提示工程在多模态任务中的应用潜力。\n\n其他领域论文较多，我将快速掠过不那么核心的：\n\n- **医疗 AI 相关（快速概述）**：Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction（克服不确定不完整性：用于鲁棒多模态序列诊断预测的框架）提出 NECHO v2 框架，提升了缺失数据下的诊断准确性；Multi-task Neural Networks for Pain Intensity Estimation using Electrocardiogram and Demographic Factors（使用心电图和人口统计因素的多任务神经网络进行疼痛强度估计）利用多任务网络整合年龄和性别信息，提高了疼痛感知建模。\n\n- **经济和社会影响（简要提及）**：Business and Regulatory Responses to Artificial Intelligence（AI 的商业和监管响应）讨论了动态监管和创新生态系统在 AI 发展中的作用；A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy（整合 AI 于认知行为疗法的通用综述）综述了 AI 在心理治疗中的潜力，如 LLM 在预后预测中的应用。\n\n- **其他（如交通、视频处理）**：High-Dimensional Fault Tolerance Testing of Highly Automated Vehicles（高维容错测试：基于低秩模型的自动驾驶车辆测试）提出 SRMF 框架加速故障注入测试；MultiHateClip（多语言仇恨视频检测数据集）构建了多模态仇恨视频数据集，提升了检测性能，但这些论文影响力较小，故从简。\n\n总之，今天的 arXiv 更新突显了 AI 领域的创新潜力，特别在 LLM 优化和实际应用上。如果你对 LLM 自提升感兴趣，Meta-Rewarding Language Models 值得一读！下次见。",
  "papers": [
    {
      "arxiv_id": "2407.19610v1",
      "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Al-Maamari",
        "Mehdi Ben Amor",
        "Michael Granitzer"
      ],
      "abstract": "This research combines Knowledge Distillation (KD) and Mixture of Experts\n(MoE) to develop modular, efficient multilingual language models. Key\nobjectives include evaluating adaptive versus fixed alpha methods in KD and\ncomparing modular MoE architectures for handling multi-domain inputs and\npreventing catastrophic forgetting. KD compresses large language models (LLMs)\ninto smaller, efficient models, while MoE enhances modularity with specialized\ntasks. Experiments showed similar performance for both KD methods, with\nmarginal improvements from adaptive alpha. A combined loss approach provided\nmore stable learning. The router, trained to classify input sequences into\nEnglish, French, German, or Python, achieved 99.95% precision, recall, and F1\nscore, with Logistic Regression being the most effective classifier.\nEvaluations of modular MoE architectures revealed that Pre-trained Language\nExperts (PLE) and Joint Expert Embedding Training (JEET) performed similarly,\nwhile the MoE with Common Expert (MoE-CE) setup showed slightly lower\nperformance. Including a common expert in MoE-CE improved its performance.\nStudies on catastrophic forgetting indicated that sequential training led to\nsignificant forgetting, while single-session training with balanced batches and\nthe MoE approach mitigated this issue. The MoE architecture preserved knowledge\nacross multiple languages effectively.\n  The research contributes open-sourced resources including the dataset\n(https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation\ntool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the\nresearch codebase (https://github.com/ModMaamari/mixture-modular-experts).",
      "tldr_zh": "这篇论文将 Knowledge Distillation (KD) 和 Mixture of Experts (MoE) 相结合，开发出模块化且高效的多语言语言模型，旨在处理多领域输入并防止 catastrophic forgetting。实验评估了 KD 中的自适应 alpha 和固定 alpha 方法，发现两者性能相似，而结合损失方法能提供更稳定的学习；路由器在分类输入序列（如英语、法语、德语或 Python）时达到 99.95% 的精确率、召回率和 F1 分数，使用 Logistic Regression 最有效。MoE 架构如 Pre-trained Language Experts (PLE) 和 Joint Expert Embedding Training (JEET) 显示类似性能，且通过添加公共专家改善了 MoE-CE 的表现；此外，单会话训练与平衡批次能缓解灾难性遗忘，MoE 架构在多语言知识保留方面表现出色。论文贡献包括开源资源，如数据集（https://zenodo.org/doi/10.5281/zenodo.12677631）、数据集创建工具和研究代码库。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2407.19610v1",
      "published_date": "2024-07-28 23:42:09 UTC",
      "updated_date": "2024-07-28 23:42:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:41:04.874196"
    },
    {
      "arxiv_id": "2407.19600v1",
      "title": "You shall know a piece by the company it keeps. Chess plays as a data for word2vec models",
      "title_zh": "翻译失败",
      "authors": [
        "Boris Orekhov"
      ],
      "abstract": "In this paper, I apply linguistic methods of analysis to non-linguistic data,\nchess plays, metaphorically equating one with the other and seeking analogies.\nChess game notations are also a kind of text, and one can consider the records\nof moves or positions of pieces as words and statements in a certain language.\nIn this article I show how word embeddings (word2vec) can work on chess game\ntexts instead of natural language texts. I don't see how this representation of\nchess data can be used productively. It's unlikely that these vector models\nwill help engines or people choose the best move. But in a purely academic\nsense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't\nnecessarily lead to a win.",
      "tldr_zh": "这篇论文将语言分析方法应用于非语言数据——国际象棋棋谱，通过类比将棋子移动或位置视为单词和语句。作者使用 word2vec 模型在棋谱文本上生成嵌入表示，探索这种表示是否能揭示游戏的内在性质。研究发现，虽然这种方法在学术上捕捉了棋谱的某些重要特征，但它不太可能帮助引擎或人类选择最佳走子，仅具有理论意义。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19600v1",
      "published_date": "2024-07-28 22:12:36 UTC",
      "updated_date": "2024-07-28 22:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:41:22.252592"
    },
    {
      "arxiv_id": "2407.19594v2",
      "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
      "title_zh": "翻译失败",
      "authors": [
        "Tianhao Wu",
        "Weizhe Yuan",
        "Olga Golovneva",
        "Jing Xu",
        "Yuandong Tian",
        "Jiantao Jiao",
        "Jason Weston",
        "Sainbayar Sukhbaatar"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.",
      "tldr_zh": "该研究提出了一种Meta-Rewarding机制，利用LLM-as-a-Meta-Judge让大语言模型(LLMs)评估自身判断，从而实现自我改进。这种方法解决了现有自奖励(Self-Rewarding)机制的局限性，即过度关注响应改进而导致迭代训练快速饱和，通过模型判断自己的判断反馈来提升其判断和指令遵循能力。实验结果显示，在AlpacaEval 2基准上，Llama-3-8B-Instruct的胜率从22.9%提高到39.4%；在Arena-Hard上从20.6%提高到29.1%。这项无监督方法证明了LLMs无需人类监督即可实现持续自我优化的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19594v2",
      "published_date": "2024-07-28 21:58:28 UTC",
      "updated_date": "2024-07-30 01:38:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:41:35.502051"
    },
    {
      "arxiv_id": "2407.19586v1",
      "title": "Is Generative AI an Existential Threat to Human Creatives? Insights from Financial Economics",
      "title_zh": "生成式 AI 是否是对人类创作者的生存威胁？ 来自金融经济学的洞见",
      "authors": [
        "Jiasun Li"
      ],
      "abstract": "With the phenomenal rise of generative AI models (e.g., large language models\nsuch as GPT or large image models such as Diffusion), there are increasing\nconcerns about human creatives' futures. Specifically, as generative models'\npower further increases, will they eventually replace all human creatives'\njobs? We argue that the answer is \"no,\" even if existing generative AI models'\ncapabilities reach their theoretical limit. Our theory has a close analogy to a\nfamiliar insight in financial economics on the impossibility of an\ninformationally efficient market [Grossman and Stiglitz (1980)]: If generative\nAI models can provide all the content humans need at low variable costs, then\nthere is no incentive for humans to spend costly resources on content creation\nas they cannot profit from it. But if no human creates new content, then\ngenerative AI can only learn from stale information and be unable to generate\nup-to-date content that reflects new happenings in the physical world. This\ncreates a paradox.",
      "tldr_zh": "本研究探讨生成式 AI 是否会完全取代人类创意者（human creatives），并通过金融经济学的视角得出否定的结论。作者借鉴 Grossman and Stiglitz (1980) 关于信息有效市场不可能性的洞见，论证如果生成式 AI 能以低成本提供所有内容，人类将缺乏激励进行新内容创作，从而无法为 AI 提供更新信息。结果，这将导致 AI 只能基于陈旧数据生成内容，形成一个悖论，最终确保人类创意者在生态中不可或缺。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19586v1",
      "published_date": "2024-07-28 21:11:41 UTC",
      "updated_date": "2024-07-28 21:11:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:41:46.312607"
    },
    {
      "arxiv_id": "2407.19580v3",
      "title": "Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures",
      "title_zh": "翻译失败",
      "authors": [
        "Dang Nguyen",
        "Wenhan Yang",
        "Rathul Anand",
        "Yu Yang",
        "Baharan Mirzasoleiman"
      ],
      "abstract": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced mixture of sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V-projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and\nLlama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably,\nCoLM reduces the memory requirement of fine-tuning by 2x and even outperforms\ntraining with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with\nexisting memory-efficient training methods like LoRA, further reducing the\nmemory requirements of training LLMs.",
      "tldr_zh": "该论文提出了一种名为 CoLM 的方法，用于在数据混合环境下进行内存高效的大型语言模型 (LLMs) 训练，以解决大 mini-batch 训练导致的 GPU 内存需求问题。CoLM 通过改进 mini-batch coresets 策略，包括确保小数据来源的示例完整纳入、针对 Adam 优化器对梯度进行归一化，以及使用 zeroth-order 方法稀疏化 V-projection 矩阵的梯度，从而有效匹配更大 mini-batch 的性能。实验结果显示，在 Phi-2、Phi-3、Zephyr 和 Llama-3 模型上使用 LoRA 微调 MathInstruct 和 SuperGLUE 基准时，CoLM 将内存需求减少 2 倍，甚至优于使用 4 倍更大 mini-batch 的训练，并可与现有内存高效方法无缝整合。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 6 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.19580v3",
      "published_date": "2024-07-28 20:39:16 UTC",
      "updated_date": "2025-03-07 20:12:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:41:52.825243"
    },
    {
      "arxiv_id": "2407.19568v3",
      "title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?",
      "title_zh": "翻译失败",
      "authors": [
        "Kangda Wei",
        "Aayush Gautam",
        "Ruihong Huang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of\nnatural language processing tasks. However, its effectiveness over\ndiscourse-level event relation extraction (ERE) tasks remains unexplored. In\nthis paper, we assess the effectiveness of LLMs in addressing discourse-level\nERE tasks characterized by lengthy documents and intricate relations\nencompassing coreference, temporal, causal, and subevent types. Evaluation is\nconducted using an commercial model, GPT-3.5, and an open-source model,\nLLaMA-2. Our study reveals a notable underperformance of LLMs compared to the\nbaseline established through supervised learning. Although Supervised\nFine-Tuning (SFT) can improve LLMs performance, it does not scale well compared\nto the smaller supervised baseline model. Our quantitative and qualitative\nanalysis shows that LLMs have several weaknesses when applied for extracting\nevent relations, including a tendency to fabricate event mentions, and failures\nto capture transitivity rules among relations, detect long distance relations,\nor comprehend contexts with dense event mentions. Code available at:\nhttps://github.com/WeiKangda/LLM-ERE.git.",
      "tldr_zh": "本文评估了大型语言模型 (LLMs) 在话语级事件关系提取 (ERE) 任务中的表现，使用 GPT-3.5 和 LLaMA-2 模型处理涉及核心ference、时间、因果和子事件的复杂长文档。结果显示，LLMs 的性能显著低于监督学习基线，即使通过 Supervised Fine-Tuning (SFT) 进行优化，也无法实现高效扩展。定量和定性分析揭示了 LLMs 的主要弱点，包括虚构事件提及、忽略关系的传递性、检测长距离关系困难，以及处理密集事件上下文的不足。该研究为改进 LLMs 在 ERE 任务中的应用提供了宝贵见解，并提供了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19568v3",
      "published_date": "2024-07-28 19:27:06 UTC",
      "updated_date": "2025-02-22 19:05:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:42:14.291997"
    },
    {
      "arxiv_id": "2407.19564v1",
      "title": "Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jifeng Wang",
        "Kaouther Messaoud",
        "Yuejiang Liu",
        "Juergen Gall",
        "Alexandre Alahi"
      ],
      "abstract": "Recent progress in motion forecasting has been substantially driven by\nself-supervised pre-training. However, adapting pre-trained models for specific\ndownstream tasks, especially motion prediction, through extensive fine-tuning\nis often inefficient. This inefficiency arises because motion prediction\nclosely aligns with the masked pre-training tasks, and traditional full\nfine-tuning methods fail to fully leverage this alignment. To address this, we\nintroduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of\nthe model's parameters, focusing adjustments on newly introduced prompts and\nadapters. This approach not only preserves the pre-learned representations but\nalso significantly reduces the number of parameters that need retraining,\nthereby enhancing efficiency. This tailored strategy, supplemented by our\nmethod's capability to efficiently adapt to different datasets, enhances model\nefficiency and ensures robust performance across datasets without the need for\nextensive retraining. Our experiments show that Forecast-PEFT outperforms\ntraditional full fine-tuning methods in motion prediction tasks, achieving\nhigher accuracy with only 17% of the trainable parameters typically required.\nMoreover, our comprehensive adaptation, Forecast-FT, further improves\nprediction performance, evidencing up to a 9.6% enhancement over conventional\nbaseline methods. Code will be available at\nhttps://github.com/csjfwang/Forecast-PEFT.",
      "tldr_zh": "该研究提出Forecast-PEFT，一种参数高效微调(PEFT)策略，用于预训练的运动预测模型，以解决传统全微调方法在适应下游任务时的低效问题。该方法通过冻结模型大部分参数，仅调整新引入的prompts和adapters，从而保留预学表示并显著减少可训练参数，仅需17%的参数即可实现更高准确率。实验结果显示，Forecast-PEFT在运动预测任务中优于传统方法，而其扩展版本Forecast-FT进一步提升性能，比基线方法提高高达9.6%。这种策略增强了模型的适应性和效率，便于跨数据集应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2407.19564v1",
      "published_date": "2024-07-28 19:18:59 UTC",
      "updated_date": "2024-07-28 19:18:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:42:28.649755"
    },
    {
      "arxiv_id": "2407.19540v4",
      "title": "Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Curriculum Data Erasing Guided Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Heejoon Koo"
      ],
      "abstract": "In this paper, we present NECHO v2, a novel framework designed to enhance the\npredictive accuracy of multimodal sequential patient diagnoses under uncertain\nmissing visit sequences, a common challenge in real clinical settings. Firstly,\nwe modify NECHO, designed in a diagnosis code-centric fashion, to handle\nuncertain modality representation dominance under the imperfect data. Secondly,\nwe develop a systematic knowledge distillation by employing the modified NECHO\nas both teacher and student. It encompasses a modality-wise contrastive and\nhierarchical distillation, transformer representation random distillation,\nalong with other distillations to align representations between teacher and\nstudent tightly and effectively. We also propose curriculum learning guided\nrandom data erasing within sequences during both training and distillation of\nthe teacher to lightly simulate scenario with missing visit information,\nthereby fostering effective knowledge transfer. As a result, NECHO v2 verifies\nitself by showing robust superiority in multimodal sequential diagnosis\nprediction under both balanced and imbalanced incomplete settings on multimodal\nhealthcare data.",
      "tldr_zh": "本文提出 NECHO v2 框架，用于提升多模态序列患者诊断预测的准确性，尤其在不确定缺失访问序列的临床场景中。通过修改 NECHO 以处理不完美数据下的模态表示主导，并采用系统化的知识蒸馏，包括模态-wise contrastive and hierarchical distillation、transformer representation random distillation，以及课程学习 guided random data erasing 来模拟缺失信息，从而促进知识转移。实验结果显示，NECHO v2 在平衡和不平衡的不完整设置下，在多模态医疗数据上表现出稳健的优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICASSP 2025 (2025 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing)",
      "pdf_url": "http://arxiv.org/pdf/2407.19540v4",
      "published_date": "2024-07-28 17:14:27 UTC",
      "updated_date": "2025-01-30 15:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:42:36.334613"
    },
    {
      "arxiv_id": "2407.19532v1",
      "title": "The Interpretability of Codebooks in Model-Based Reinforcement Learning is Limited",
      "title_zh": "翻译失败",
      "authors": [
        "Kenneth Eaton",
        "Jonathan Balloch",
        "Julia Kim",
        "Mark Riedl"
      ],
      "abstract": "Interpretability of deep reinforcement learning systems could assist\noperators with understanding how they interact with their environment. Vector\nquantization methods -- also called codebook methods -- discretize a neural\nnetwork's latent space that is often suggested to yield emergent\ninterpretability. We investigate whether vector quantization in fact provides\ninterpretability in model-based reinforcement learning. Our experiments,\nconducted in the reinforcement learning environment Crafter, show that the\ncodes of vector quantization models are inconsistent, have no guarantee of\nuniqueness, and have a limited impact on concept disentanglement, all of which\nare necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.",
      "tldr_zh": "这篇论文探讨了在基于模型的强化学习（model-based reinforcement learning）中，向量量化（vector quantization）方法所产生的codebook是否能提供有效的可解释性。研究者通过在Crafter强化学习环境中进行的实验，发现codebook的代码不一致、不唯一，且对概念解耦（concept disentanglement）的影响有限，这些特性都阻碍了其作为可解释工具的潜力。最终，论文得出结论，向量量化方法可能根本不足以实现深度强化学习系统的可解释性，从而帮助操作者更好地理解模型与环境的交互。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19532v1",
      "published_date": "2024-07-28 16:40:20 UTC",
      "updated_date": "2024-07-28 16:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:43:09.801006"
    },
    {
      "arxiv_id": "2407.19524v3",
      "title": "VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary",
      "title_zh": "翻译失败",
      "authors": [
        "Hanjun Luo",
        "Ziye Deng",
        "Haoyu Huang",
        "Xuecheng Liu",
        "Ruizhe Chen",
        "Zuozhu Liu"
      ],
      "abstract": "With the rapid development of Text-to-Image (T2I) models, biases in human\nimage generation against demographic social groups become a significant\nconcern, impacting fairness and ethical standards in AI. Some researchers\npropose their methods to tackle with the issue. However, existing methods are\ndesigned for specific models with fixed prompts, limiting their adaptability to\nthe fast-evolving models and diverse practical scenarios. Moreover, they\nneglect the impact of hallucinations, leading to discrepancies between expected\nand actual results. To address these issues, we introduce VersusDebias, a novel\nand universal debiasing framework for biases in arbitrary T2I models,\nconsisting of an array generation (AG) module and an image generation (IG)\nmodule. The self-adaptive AG module generates specialized attribute arrays to\npost-process hallucinations and debias multiple attributes simultaneously. The\nIG module employs a small language model to modify prompts according to the\narrays and drives the T2I model to generate debiased images, enabling zero-shot\ndebiasing. Extensive experiments demonstrate VersusDebias's capability to\ndebias any models across gender, race, and age simultaneously. In both\nzero-shot and few-shot scenarios, VersusDebias outperforms existing methods,\nshowcasing its exceptional utility. Our work is accessible at\nhttps://github.com/VersusDebias/VersusDebias to ensure reproducibility and\nfacilitate further research.",
      "tldr_zh": "该研究提出VersusDebias，一种通用的零样本去偏框架，用于解决Text-to-Image (T2I) 模型在生成图像时对性别、种族和年龄等人口统计群体的偏见问题。框架包括Array Generation (AG) 模块，用于生成属性数组以处理幻觉并同时去偏多个属性，以及Image Generation (IG) 模块，通过小型语言模型 (SLM) 修改提示并驱动T2I模型生成去偏图像，实现零样本适应性。实验结果显示，VersusDebias在零样本和少样本场景下优于现有方法，能够在任意T2I模型上有效去偏，并提供开源代码以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19524v3",
      "published_date": "2024-07-28 16:24:07 UTC",
      "updated_date": "2024-08-16 06:24:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:42:59.701424"
    },
    {
      "arxiv_id": "2407.19517v1",
      "title": "Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload",
      "title_zh": "翻译失败",
      "authors": [
        "Limin Ma",
        "Ken Pu",
        "Ying Zhu"
      ],
      "abstract": "This study presents a comparative analysis of the a complex SQL benchmark,\nTPC-DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings\nreveal that TPC-DS queries exhibit a significantly higher level of structural\ncomplexity compared to the other two benchmarks. This underscores the need for\nmore intricate benchmarks to simulate realistic scenarios effectively. To\nfacilitate this comparison, we devised several measures of structural\ncomplexity and applied them across all three benchmarks. The results of this\nstudy can guide future research in the development of more sophisticated\ntext-to-SQL benchmarks.\n  We utilized 11 distinct Language Models (LLMs) to generate SQL queries based\non the query descriptions provided by the TPC-DS benchmark. The prompt\nengineering process incorporated both the query description as outlined in the\nTPC-DS specification and the database schema of TPC-DS. Our findings indicate\nthat the current state-of-the-art generative AI models fall short in generating\naccurate decision-making queries. We conducted a comparison of the generated\nqueries with the TPC-DS gold standard queries using a series of fuzzy structure\nmatching techniques based on query features. The results demonstrated that the\naccuracy of the generated queries is insufficient for practical real-world\napplication.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）在处理复杂文本到 SQL 生成任务中的性能，通过比较 TPC-DS 基准与现有基准 BIRD 和 Spider，发现 TPC-DS 查询在结构复杂性上显著更高，并开发了多项结构复杂性指标来量化这一差异。研究者使用 11 个不同 LLMs 结合 prompt engineering（基于 TPC-DS 查询描述和数据库模式）生成 SQL 查询，并通过模糊结构匹配技术与标准查询进行比较。结果显示，当前最先进的生成 AI 模型在准确生成决策查询方面表现不足，无法满足实际应用需求，这为未来开发更复杂的文本到 SQL 基准提供了指导。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19517v1",
      "published_date": "2024-07-28 15:53:05 UTC",
      "updated_date": "2024-07-28 15:53:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:43:24.235975"
    },
    {
      "arxiv_id": "2407.19507v2",
      "title": "WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting",
      "title_zh": "WeCromCL：弱监督跨模态对比学习，用于仅基于转录监督的文本检测",
      "authors": [
        "Jingjing Wu",
        "Zhengyao Fang",
        "Pengyuan Lyu",
        "Chengquan Zhang",
        "Fanglin Chen",
        "Guangming Lu",
        "Wenjie Pei"
      ],
      "abstract": "Transcription-only Supervised Text Spotting aims to learn text spotters\nrelying only on transcriptions but no text boundaries for supervision, thus\neliminating expensive boundary annotation. The crux of this task lies in\nlocating each transcription in scene text images without location annotations.\nIn this work, we formulate this challenging problem as a Weakly Supervised\nCross-modality Contrastive Learning problem, and design a simple yet effective\nmodel dubbed WeCromCL that is able to detect each transcription in a scene\nimage in a weakly supervised manner. Unlike typical methods for cross-modality\ncontrastive learning that focus on modeling the holistic semantic correlation\nbetween an entire image and a text description, our WeCromCL conducts atomistic\ncontrastive learning to model the character-wise appearance consistency between\na text transcription and its correlated region in a scene image to detect an\nanchor point for the transcription in a weakly supervised manner. The detected\nanchor points by WeCromCL are further used as pseudo location labels to guide\nthe learning of text spotting. Extensive experiments on four challenging\nbenchmarks demonstrate the superior performance of our model over other\nmethods. Code will be released.",
      "tldr_zh": "本研究针对 Transcription-only Supervised Text Spotting 任务，提出 WeCromCL 模型，该任务仅依赖文本转录作为监督，而无需昂贵的边界标注。WeCromCL 通过 Weakly Supervised Cross-modality Contrastive Learning 将问题转化为弱监督学习框架，专注于字符级外观一致性对比，以检测文本转录在场景图像中的锚点，并将其作为伪位置标签指导文本检测训练。与传统方法不同，该模型强调原子级对比学习，从而提升定位精度。在四个挑战性基准上的实验表明，WeCromCL 性能优于其他方法，代码将公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19507v2",
      "published_date": "2024-07-28 14:58:07 UTC",
      "updated_date": "2025-01-13 08:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:43:47.038130"
    },
    {
      "arxiv_id": "2407.21069v1",
      "title": "High-Dimensional Fault Tolerance Testing of Highly Automated Vehicles Based on Low-Rank Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuewen Mei",
        "Tong Nie",
        "Jian Sun",
        "Ye Tian"
      ],
      "abstract": "Ensuring fault tolerance of Highly Automated Vehicles (HAVs) is crucial for\ntheir safety due to the presence of potentially severe faults. Hence, Fault\nInjection (FI) testing is conducted by practitioners to evaluate the safety\nlevel of HAVs. To fully cover test cases, various driving scenarios and fault\nsettings should be considered. However, due to numerous combinations of test\nscenarios and fault settings, the testing space can be complex and\nhigh-dimensional. In addition, evaluating performance in all newly added\nscenarios is resource-consuming. The rarity of critical faults that can cause\nsecurity problems further strengthens the challenge. To address these\nchallenges, we propose to accelerate FI testing under the low-rank Smoothness\nRegularized Matrix Factorization (SRMF) framework. We first organize the sparse\nevaluated data into a structured matrix based on its safety values. Then the\nuntested values are estimated by the correlation captured by the matrix\nstructure. To address high dimensionality, a low-rank constraint is imposed on\nthe testing space. To exploit the relationships between existing scenarios and\nnew scenarios and capture the local regularity of critical faults, three types\nof smoothness regularization are further designed as a complement. We conduct\nexperiments on car following and cut in scenarios. The results indicate that\nSRMF has the lowest prediction error in various scenarios and is capable of\npredicting rare critical faults compared to other machine learning models. In\naddition, SRMF can achieve 1171 acceleration rate, 99.3% precision and 91.1% F1\nscore in identifying critical faults. To the best of our knowledge, this is the\nfirst work to introduce low-rank models to FI testing of HAVs.",
      "tldr_zh": "这篇论文针对高度自动化车辆 (HAVs) 的故障耐受性测试问题，提出了一种基于低秩平滑正则化矩阵分解 (SRMF) 框架的加速方法，以应对测试空间高维、资源消耗大和关键故障稀少的挑战。\n该方法首先将评估数据组织成基于安全值的结构化矩阵，通过低秩约束估计未测试值，并设计三种平滑正则化来捕捉现有场景与新场景的关系及关键故障的局部规律。\n实验在跟车和切入场景中表明，SRMF 比其他机器学习模型具有最低预测错误，并能有效预测稀有关键故障，实现 1171 倍的加速率、99.3% 的精确度和 91.1% 的 F1 分数。\n这项工作是首次将低秩模型引入 HAVs 的故障注入 (FI) 测试，为提高车辆安全提供了新途径。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ITSC 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21069v1",
      "published_date": "2024-07-28 14:27:13 UTC",
      "updated_date": "2024-07-28 14:27:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:00.056731"
    },
    {
      "arxiv_id": "2407.19493v3",
      "title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake News Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yihao Wang",
        "Lizhi Chen",
        "Zhong Qian",
        "Peifeng Li"
      ],
      "abstract": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection.",
      "tldr_zh": "该研究针对多模态假新闻检测问题，构建了一个名为 Official-NV 的数据集，该数据集基于官方发布的新闻视频，避免了现有数据集中的用户上传噪声。数据集通过 LLM-based 生成技术结合手动验证进行扩充，以提升数据质量和多样性。论文还提出一个新基准模型 OFNVD，利用 GLU 注意力机制从多模态特征中提取关键信息，并通过跨模态 Transformer 实现特征增强和模态聚合。实验基准测试证明了 OFNVD 在多模态新闻检测中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19493v3",
      "published_date": "2024-07-28 13:23:43 UTC",
      "updated_date": "2024-12-27 10:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:09.600768"
    },
    {
      "arxiv_id": "2407.19492v1",
      "title": "Heads Up eXperience (HUX): Always-On AI Companion for Human Computer Environment Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Sukanth K",
        "Sudhiksha Kandavel Rajan",
        "Rajashekhar V S",
        "Gowdham Prabhakar"
      ],
      "abstract": "While current personal smart devices excel in digital domains, they fall\nshort in assisting users during human environment interaction. This paper\nproposes Heads Up eXperience (HUX), an AI system designed to bridge this gap,\nserving as a constant companion across the extended reality (XR) environments.\nBy tracking the user's eye gaze, analyzing the surrounding environment, and\ninterpreting verbal contexts, the system captures and enhances multi-modal\ndata, providing holistic context interpretation and memory storage in real-time\ntask specific situations. This comprehensive approach enables more natural,\nempathetic and intelligent interactions between the user and HUX AI, paving the\npath for human computer environment interaction. Intended for deployment in\nsmart glasses and extended reality headsets, HUX AI aims to become a personal\nand useful AI companion for daily life. By integrating digital assistance with\nenhanced physical world interactions, this technology has the potential to\nrevolutionize human-AI collaboration in both personal and professional spheres\npaving the way for the future of personal smart devices.",
      "tldr_zh": "本文提出 Heads Up eXperience (HUX) 系统，这是一个始终在线的 AI 伴侣，旨在填补当前智能设备在人类-环境互动中的不足，提供跨扩展现实 (XR) 环境的全面支持。HUX 通过跟踪用户的 eye gaze、分析周围环境以及解释语言上下文，处理多模态数据，实现实时上下文解释和记忆存储，从而提升互动的自然性和智能性。该系统计划部署在智能眼镜和 XR 头显上，有望革命化人类-AI 合作，在个人和专业领域带来更高效的协作。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "48 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19492v1",
      "published_date": "2024-07-28 13:15:51 UTC",
      "updated_date": "2024-07-28 13:15:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:24.300162"
    },
    {
      "arxiv_id": "2407.19475v1",
      "title": "Multi-task Neural Networks for Pain Intensity Estimation using Electrocardiogram and Demographic Factors",
      "title_zh": "翻译失败",
      "authors": [
        "Stefanos Gkikas",
        "Chariklia Chatzaki",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a complex phenomenon which is manifested and expressed by patients in\nvarious forms. The immediate and objective recognition of it is a great of\nimportance in order to attain a reliable and unbiased healthcare system. In\nthis work, we elaborate electrocardiography signals revealing the existence of\nvariations in pain perception among different demographic groups. We exploit\nthis insight by introducing a novel multi-task neural network for automatic\npain estimation utilizing the age and the gender information of each\nindividual, and show its advantages compared to other approaches.",
      "tldr_zh": "该研究探讨了疼痛强度估计的问题，利用心电图(Electrocardiogram)信号揭示不同人口统计因素（如年龄和性别）对疼痛感知的影响。研究者引入了一个新型多任务神经网络(multi-task neural networks)，通过整合个体的人口统计信息来实现自动疼痛估计。相比其他方法，该网络在准确性和客观性上表现出优势，有助于构建可靠的无偏医疗系统。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19475v1",
      "published_date": "2024-07-28 11:57:50 UTC",
      "updated_date": "2024-07-28 11:57:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:33.674769"
    },
    {
      "arxiv_id": "2407.19469v1",
      "title": "Interpretable Triplet Importance for Personalized Ranking",
      "title_zh": "可解释的三元组重要性用于个性化排序",
      "authors": [
        "Bowei He",
        "Chen Ma"
      ],
      "abstract": "Personalized item ranking has been a crucial component contributing to the\nperformance of recommender systems. As a representative approach, pairwise\nranking directly optimizes the ranking with user implicit feedback by\nconstructing (\\textit{user}, \\textit{positive item}, \\textit{negative item})\ntriplets. Several recent works have noticed that treating all triplets equally\nmay hardly achieve the best effects. They assign different importance scores to\nnegative items, user-item pairs, or triplets, respectively. However, almost all\nthe generated importance scores are groundless and hard to interpret, thus far\nfrom trustworthy and transparent. To tackle these, we propose the\n\\textit{Triplet Shapley} -- a Shapely value-based method to measure the triplet\nimportance in an interpretable manner. Due to the huge number of triplets, we\ntransform the original Shapley value calculation to the Monte Carlo (MC)\napproximation, where the guarantee for the approximation unbiasedness is also\nprovided. To stabilize the MC approximation, we adopt a control\ncovariates-based method. Finally, we utilize the triplet Shapley value to guide\nthe resampling of important triplets for benefiting the model learning.\nExtensive experiments are conducted on six public datasets involving classical\nmatrix factorization- and graph neural network-based recommendation models.\nEmpirical results and subsequent analysis show that our model consistently\noutperforms the state-of-the-art methods.",
      "tldr_zh": "本论文针对个性化物品排名的推荐系统问题，提出 Triplet Shapley 方法，该方法基于 Shapley 值来可解释地评估 (用户, 正向物品, 负向物品) 三元组的重要性，以解决现有方法评分缺乏透明性的问题。Triplet Shapley 通过 Monte Carlo (MC) 近似计算来处理海量三元组，确保计算的无偏性和稳定性，并采用控制协变量技术进一步优化。最终，该方法指导重要三元组的重采样，提升了基于矩阵分解和图神经网络的推荐模型性能；在六个公开数据集上的实验结果显示，该方法 consistently outperforms 现有最先进方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19469v1",
      "published_date": "2024-07-28 11:46:55 UTC",
      "updated_date": "2024-07-28 11:46:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:47.137772"
    },
    {
      "arxiv_id": "2407.19447v1",
      "title": "Nudging Consent and the New Opt Out System to the Processing of Health Data in England",
      "title_zh": "翻译失败",
      "authors": [
        "Janos Meszaros",
        "Chih-hsing Ho",
        "Marcelo Corrales Compagnucci"
      ],
      "abstract": "This chapter examines the challenges of the revised opt out system and the\nsecondary use of health data in England. The analysis of this data could be\nvery valuable for science and medical treatment as well as for the discovery of\nnew drugs. For this reason, the UK government established the care.data program\nin 2013. The aim of the project was to build a central nationwide database for\nresearch and policy planning. However, the processing of personal data was\nplanned without proper public engagement. Research has suggested that IT\ncompanies, such as in the Google DeepMind deal case, had access to other kinds\nof sensitive data and failed to comply with data protection law. Since May\n2018, the government has launched the national data opt out system with the\nhope of regaining public trust. Nevertheless, there are no evidence of\nsignificant changes in the ND opt out, compared to the previous opt out system.\nNeither in the use of secondary data, nor in the choices that patients can\nmake. The only notorious difference seems to be in the way that these options\nare communicated and framed to the patients. Most importantly, according to the\nnew ND opt out, the type 1 opt out option, which is the only choice that truly\nstops data from being shared outside direct care, will be removed in 2020.\nAccording to the Behavioral Law and Economics literature (Nudge Theory),\ndefault rules, such as the revised opt out system in England, are very\npowerful, because people tend to stick to the default choices made readily\navailable to them. The crucial question analyzed in this chapter is whether it\nis desirable for the UK government to stop promoting the type 1 opt outs, and\nwhether this could be seen as a kind of hard paternalism.",
      "tldr_zh": "这篇论文分析了英格兰健康数据处理的修订版 opt out system 及其对数据二次使用的挑战，探讨了政府如何通过默认规则影响患者同意。论文引用 Nudge Theory（推动理论）来审视默认选择的强大影响力，例如新系统未显著改变数据共享选项，却改变了向患者沟通和框架的方式，导致人们倾向于维持默认设置。核心发现是，新系统将移除 type 1 opt out 选项，这可能被视为政府的一种硬 paternalism（硬 paternalism），从而质疑其在恢复公众信任方面的有效性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19447v1",
      "published_date": "2024-07-28 09:49:37 UTC",
      "updated_date": "2024-07-28 09:49:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:44:58.534208"
    },
    {
      "arxiv_id": "2407.19439v1",
      "title": "Business and Regulatory Responses to Artificial Intelligence: Dynamic Regulation, Innovation Ecosystems and the Strategic Management of Disruptive Technology",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Fenwick",
        "Erik P. M. Vermeulen",
        "Marcelo Corrales Compagnucci"
      ],
      "abstract": "Identifying and then implementing an effective response to disruptive new AI\ntechnologies is enormously challenging for any business looking to integrate AI\ninto their operations, as well as regulators looking to leverage AI-related\ninnovation as a mechanism for achieving regional economic growth. These\nbusiness and regulatory challenges are particularly significant given the broad\nreach of AI, as well as the multiple uncertainties surrounding such\ntechnologies and their future development and effects. This article identifies\ntwo promising strategies for meeting the AI challenge, focusing on the example\nof Fintech. First, dynamic regulation, in the form of regulatory sandboxes and\nother regulatory approaches that aim to provide a space for responsible\nAI-related innovation. An empirical study provides preliminary evidence to\nsuggest that jurisdictions that adopt a more proactive approach to Fintech\nregulation can attract greater investment. The second strategy relates to\nso-called innovation ecosystems. It is argued that such ecosystems are most\neffective when they afford opportunities for creative partnerships between\nwell-established corporations and AI-focused startups and that this aspect of a\nsuccessful innovation ecosystem is often overlooked in the existing discussion.\nThe article suggests that these two strategies are interconnected, in that\ngreater investment is an important element in both fostering and signaling a\nwell-functioning innovation ecosystem and that a well-functioning ecosystem\nwill, in turn, attract more funding. The resulting synergies between these\nstrategies can, therefore, provide a jurisdiction with a competitive edge in\nbecoming a regional hub for AI-related activity.",
      "tldr_zh": "这篇论文探讨了企业和监管机构在应对AI颠覆性技术时的挑战，包括整合AI的复杂性和不确定性。作者提出两种关键策略：Dynamic Regulation（如监管沙盒），通过提供创新空间促进负责任的AI发展，并通过实证研究显示，主动监管的地区能吸引更多Fintech投资；以及Innovation Ecosystems，强调成熟企业和AI初创企业的创意合作，以增强生态系统的有效性。论文进一步指出，这两种策略相互关联，能产生协同效应，帮助区域成为AI活动中心并提升竞争力。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19439v1",
      "published_date": "2024-07-28 09:34:03 UTC",
      "updated_date": "2024-07-28 09:34:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:45:09.635904"
    },
    {
      "arxiv_id": "2407.19438v1",
      "title": "Conversational AI Multi-Agent Interoperability, Universal Open APIs for Agentic Natural Language Multimodal Communications",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Gosmar",
        "Deborah A. Dahl",
        "Emmett Coin"
      ],
      "abstract": "This paper analyses Conversational AI multi-agent interoperability frameworks\nand describes the novel architecture proposed by the Open Voice\nInteroperability initiative (Linux Foundation AI and DATA), also known briefly\nas OVON (Open Voice Network). The new approach is illustrated, along with the\nmain components, delineating the key benefits and use cases for deploying\nstandard multi-modal AI agency (or agentic AI) communications. Beginning with\nUniversal APIs based on Natural Language, the framework establishes and enables\ninteroperable interactions among diverse Conversational AI agents, including\nchatbots, voicebots, videobots, and human agents. Furthermore, a new Discovery\nspecification framework is introduced, designed to efficiently look up agents\nproviding specific services and to obtain accurate information about these\nservices through a standard Manifest publication, accessible via an extended\nset of Natural Language-based APIs. The main purpose of this contribution is to\nsignificantly enhance the capabilities and scalability of AI interactions\nacross various platforms. The novel architecture for interoperable\nConversational AI assistants is designed to generalize, being replicable and\naccessible via open repositories.",
      "tldr_zh": "本论文分析了Conversational AI多智能体互操作性框架，并介绍了Open Voice Interoperability initiative（简称OVON）的创新架构。该架构基于Natural Language的Universal APIs，支持多模态AI代理（如chatbots、voicebots和videobots）之间的互操作交互，并引入Discovery specification框架，用于高效查找代理服务和通过标准Manifest发布获取信息。论文的主要贡献在于提升AI交互的capabilities and scalability，使该架构可复制、易访问，并通过开放存储库推广应用。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19438v1",
      "published_date": "2024-07-28 09:33:55 UTC",
      "updated_date": "2024-07-28 09:33:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:45:24.592084"
    },
    {
      "arxiv_id": "2407.19435v1",
      "title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding",
      "title_zh": "ASI-Seg：音频驱动的手术器械分割，结合外科医生意图理解",
      "authors": [
        "Zhen Chen",
        "Zongming Zhang",
        "Wenwu Guo",
        "Xingjian Luo",
        "Long Bai",
        "Jinlin Wu",
        "Hongliang Ren",
        "Hongbin Liu"
      ],
      "abstract": "Surgical instrument segmentation is crucial in surgical scene understanding,\nthereby facilitating surgical safety. Existing algorithms directly detected all\ninstruments of pre-defined categories in the input image, lacking the\ncapability to segment specific instruments according to the surgeon's\nintention. During different stages of surgery, surgeons exhibit varying\npreferences and focus toward different surgical instruments. Therefore, an\ninstrument segmentation algorithm that adheres to the surgeon's intention can\nminimize distractions from irrelevant instruments and assist surgeons to a\ngreat extent. The recent Segment Anything Model (SAM) reveals the capability to\nsegment objects following prompts, but the manual annotations for prompts are\nimpractical during the surgery. To address these limitations in operating\nrooms, we propose an audio-driven surgical instrument segmentation framework,\nnamed ASI-Seg, to accurately segment the required surgical instruments by\nparsing the audio commands of surgeons. Specifically, we propose an\nintention-oriented multimodal fusion to interpret the segmentation intention\nfrom audio commands and retrieve relevant instrument details to facilitate\nsegmentation. Moreover, to guide our ASI-Seg segment of the required surgical\ninstruments, we devise a contrastive learning prompt encoder to effectively\ndistinguish the required instruments from the irrelevant ones. Therefore, our\nASI-Seg promotes the workflow in the operating rooms, thereby providing\ntargeted support and reducing the cognitive load on surgeons. Extensive\nexperiments are performed to validate the ASI-Seg framework, which reveals\nremarkable advantages over classical state-of-the-art and medical SAMs in both\nsemantic segmentation and intention-oriented segmentation. The source code is\navailable at https://github.com/Zonmgin-Zhang/ASI-Seg.",
      "tldr_zh": "该论文提出了一种音频驱动的外科仪器分割框架 ASI-Seg，能够理解外科医生的意图，从而更精确地分割特定仪器，避免现有算法直接检测所有预定义类别的仪器所带来的干扰。具体而言，ASI-Seg 采用 intention-oriented multimodal fusion 来解析音频命令中的分割意图，并通过 contrastive learning prompt encoder 区分所需的仪器与无关的仪器，从而提升手术室工作流程并减轻外科医生的认知负担。实验结果表明，该框架在语义分割和意图导向分割任务上显著优于现有方法和医疗 SAM 模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "This work is accepted by IROS 2024 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2407.19435v1",
      "published_date": "2024-07-28 09:25:59 UTC",
      "updated_date": "2024-07-28 09:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:45:49.017438"
    },
    {
      "arxiv_id": "2407.19426v1",
      "title": "Causal Discovery in Linear Models with Unobserved Variables and Measurement Error",
      "title_zh": "线性模型中未观测变量和测量误差下的因果发现",
      "authors": [
        "Yuqin Yang",
        "Mohamed Nafea",
        "Negar Kiyavash",
        "Kun Zhang",
        "AmirEmad Ghassami"
      ],
      "abstract": "The presence of unobserved common causes and the presence of measurement\nerror are two of the most limiting challenges in the task of causal structure\nlearning. Ignoring either of the two challenges can lead to detecting spurious\ncausal links among variables of interest. In this paper, we study the problem\nof causal discovery in systems where these two challenges can be present\nsimultaneously. We consider linear models which include four types of\nvariables: variables that are directly observed, variables that are not\ndirectly observed but are measured with error, the corresponding measurements,\nand variables that are neither observed nor measured. We characterize the\nextent of identifiability of such model under separability condition (i.e., the\nmatrix indicating the independent exogenous noise terms pertaining to the\nobserved variables is identifiable) together with two versions of faithfulness\nassumptions and propose a notion of observational equivalence. We provide\ngraphical characterization of the models that are equivalent and present a\nrecovery algorithm that could return models equivalent to the ground truth.",
      "tldr_zh": "本研究探讨了线性模型中因果发现（causal discovery）的挑战，特别是当存在未观测变量（unobserved variables）和测量误差（measurement error）时，这些因素可能导致虚假因果链接（spurious causal links）。论文考虑了包括直接观测变量、带误差测量变量、对应测量以及完全未观测变量在内的四种变量类型，并在分离条件（separability condition）和两种忠实性假设（faithfulness assumptions）下，表征了模型的可识别性（identifiability）。此外，研究提出了观测等价（observational equivalence）的概念，提供图形表征并设计了一个恢复算法（recovery algorithm），能够返回与真实模型等价的结构，从而提升因果结构学习的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19426v1",
      "published_date": "2024-07-28 08:26:56 UTC",
      "updated_date": "2024-07-28 08:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:45:54.893917"
    },
    {
      "arxiv_id": "2408.03468v2",
      "title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili",
      "title_zh": "翻译失败",
      "authors": [
        "Han Wang",
        "Tan Rui Yang",
        "Usman Naseem",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Hate speech is a pressing issue in modern society, with significant effects\nboth online and offline. Recent research in hate speech detection has primarily\ncentered on text-based media, largely overlooking multimodal content such as\nvideos. Existing studies on hateful video datasets have predominantly focused\non English content within a Western context and have been limited to binary\nlabels (hateful or non-hateful), lacking detailed contextual information. This\nstudy presents MultiHateClip1 , an novel multilingual dataset created through\nhate lexicons and human annotation. It aims to enhance the detection of hateful\nvideos on platforms such as YouTube and Bilibili, including content in both\nEnglish and Chinese languages. Comprising 2,000 videos annotated for\nhatefulness, offensiveness, and normalcy, this dataset provides a\ncross-cultural perspective on gender-based hate speech. Through a detailed\nexamination of human annotation results, we discuss the differences between\nChinese and English hateful videos and underscore the importance of different\nmodalities in hateful and offensive video analysis. Evaluations of\nstate-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL,\non MultiHateClip highlight the existing challenges in accurately distinguishing\nbetween hateful and offensive content and the urgent need for models that are\nboth multimodally and culturally nuanced. MultiHateClip represents a\nfoundational advance in enhancing hateful video detection by underscoring the\nnecessity of a multimodal and culturally sensitive approach in combating online\nhate speech.",
      "tldr_zh": "本研究引入了MultiHateClip数据集，这是一个多语言基准，用于检测YouTube和Bilibili平台的仇恨视频，填补了现有研究对多模态内容和非英语语境的忽视。数据集包含2000个英语和中文视频，通过仇恨词汇和人工标注，标注了仇恨、冒犯和正常类别，并提供了性别-based仇恨言论的跨文化视角。分析结果揭示了中文和英语仇恨视频的差异，以及视觉和语言模态在检测中的重要性。评估VLM、GPT-4V和Qwen-VL等模型显示，这些模型在区分仇恨和冒犯内容方面存在挑战，强调了开发多模态和文化敏感方法的迫切需求。MultiHateClip为在线仇恨言论的对抗奠定了基础，促进了更准确的视频分类技术。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "I.2.0"
      ],
      "primary_category": "cs.MM",
      "comment": "10 pages, 3 figures, ACM Multimedia 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.03468v2",
      "published_date": "2024-07-28 08:19:09 UTC",
      "updated_date": "2024-08-12 06:01:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:46:15.390377"
    },
    {
      "arxiv_id": "2407.19422v1",
      "title": "A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy",
      "title_zh": "翻译失败",
      "authors": [
        "Meng Jiang",
        "Qing Zhao",
        "Jianqiang Li",
        "Fan Wang",
        "Tianyu He",
        "Xinyan Cheng",
        "Bing Xiang Yang",
        "Grace W. K. Ho",
        "Guanghui Fu"
      ],
      "abstract": "Cognitive Behavioral Therapy (CBT) is a well-established intervention for\nmitigating psychological issues by modifying maladaptive cognitive and\nbehavioral patterns. However, delivery of CBT is often constrained by resource\nlimitations and barriers to access. Advancements in artificial intelligence\n(AI) have provided technical support for the digital transformation of CBT.\nParticularly, the emergence of pre-training models (PTMs) and large language\nmodels (LLMs) holds immense potential to support, augment, optimize and\nautomate CBT delivery. This paper reviews the literature on integrating AI into\nCBT interventions. We begin with an overview of CBT. Then, we introduce the\nintegration of AI into CBT across various stages: pre-treatment, therapeutic\nprocess, and post-treatment. Next, we summarized the datasets relevant to some\nCBT-related tasks. Finally, we discuss the benefits and current limitations of\napplying AI to CBT. We suggest key areas for future research, highlighting the\nneed for further exploration and validation of the long-term efficacy and\nclinical utility of AI-enhanced CBT. The transformative potential of AI in\nreshaping the practice of CBT heralds a new era of more accessible, efficient,\nand personalized mental health interventions.",
      "tldr_zh": "这篇论文回顾了人工智能（AI）在认知行为疗法（Cognitive Behavioral Therapy, CBT）中的整合应用，旨在解决CBT资源限制和访问障碍的问题。作者介绍了AI如何通过预训练模型（PTMs）和大型语言模型（LLMs）支持CBT的各个阶段，包括预治疗、治疗过程和后治疗。论文总结了相关数据集，并讨论了AI增强CBT的益处，如提高可访问性和个性化，同时指出了当前局限性，并建议未来研究重点验证其长期功效和临床效用。总之，AI有望重塑CBT实践，推动更高效的心理干预。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19422v1",
      "published_date": "2024-07-28 08:09:46 UTC",
      "updated_date": "2024-07-28 08:09:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:46:18.163231"
    },
    {
      "arxiv_id": "2407.19415v1",
      "title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross Modal Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyu Chen",
        "Pengfei Zhang",
        "Kai Ye",
        "Wei Dong",
        "Xin Feng",
        "Yana Zhang"
      ],
      "abstract": "The burgeoning short video industry has accelerated the advancement of\nvideo-music retrieval technology, assisting content creators in selecting\nappropriate music for their videos. In self-supervised training for\nvideo-to-music retrieval, the video and music samples in the dataset are\nseparated from the same video work, so they are all one-to-one matches. This\ndoes not match the real situation. In reality, a video can use different music\nas background music, and a music can be used as background music for different\nvideos. Many videos and music that are not in a pair may be compatible, leading\nto false negative noise in the dataset. A novel inter-intra modal (II) loss is\nproposed as a solution. By reducing the variation of feature distribution\nwithin the two modalities before and after the encoder, II loss can reduce the\nmodel's overfitting to such noise without removing it in a costly and laborious\nway. The video-music retrieval framework, II-CLVM (Contrastive Learning for\nVideo-Music Retrieval), incorporating the II Loss, achieves state-of-the-art\nperformance on the YouTube8M dataset. The framework II-CLVTM shows better\nperformance when retrieving music using multi-modal video information (such as\ntext in videos). Experiments are designed to show that II loss can effectively\nalleviate the problem of false negative noise in retrieval tasks. Experiments\nalso show that II loss improves various self-supervised and supervised\nuni-modal and cross-modal retrieval tasks, and can obtain good retrieval models\nwith a small amount of training samples.",
      "tldr_zh": "这篇论文针对视频-音乐检索任务中的假负样本噪声问题，提出了一种新型 inter-intra modal (II) loss，以减少编码器前后两个模态特征分布的变异，从而缓解模型对噪声的过拟合，而无需手动移除样本。论文引入了 II-CLVM（Contrastive Learning for Video-Music Retrieval）框架，并在 YouTube8M 数据集上实现了最先进性能；此外，II-CLVTM 在使用多模态视频信息（如视频中的文本）进行音乐检索时表现出色。实验结果表明，II loss 不仅有效缓解了假负样本噪声，还提升了各种自监督和监督的单模态及跨模态检索任务的表现，即使在小样本训练下也能获得良好模型。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "I.2; I.4"
      ],
      "primary_category": "cs.MM",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19415v1",
      "published_date": "2024-07-28 07:06:28 UTC",
      "updated_date": "2024-07-28 07:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:46:39.316781"
    },
    {
      "arxiv_id": "2407.19414v1",
      "title": "Appformer: A Novel Framework for Mobile App Usage Prediction Leveraging Progressive Multi-Modal Data Fusion and Feature Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Chuike Sun",
        "Junzhou Chen",
        "Yue Zhao",
        "Hao Han",
        "Ruihai Jing",
        "Guang Tan",
        "Di Wu"
      ],
      "abstract": "This article presents Appformer, a novel mobile application prediction\nframework inspired by the efficiency of Transformer-like architectures in\nprocessing sequential data through self-attention mechanisms. Combining a\nMulti-Modal Data Progressive Fusion Module with a sophisticated Feature\nExtraction Module, Appformer leverages the synergies of multi-modal data fusion\nand data mining techniques while maintaining user privacy. The framework\nemploys Points of Interest (POIs) associated with base stations, optimizing\nthem through comprehensive comparative experiments to identify the most\neffective clustering method. These refined inputs are seamlessly integrated\ninto the initial phases of cross-modal data fusion, where temporal units are\nencoded via word embeddings and subsequently merged in later stages. The\nFeature Extraction Module, employing Transformer-like architectures specialized\nfor time series analysis, adeptly distils comprehensive features. It\nmeticulously fine-tunes the outputs from the fusion module, facilitating the\nextraction of high-calibre, multi-modal features, thus guaranteeing a robust\nand efficient extraction process. Extensive experimental validation confirms\nAppformer's effectiveness, attaining state-of-the-art (SOTA) metrics in mobile\napp usage prediction, thereby signifying a notable progression in this field.",
      "tldr_zh": "本研究提出Appformer，一种新型移动应用使用预测框架，受Transformer-like architectures启发，通过自注意力机制处理序列数据。该框架整合Multi-Modal Data Progressive Fusion Module和Feature Extraction Module，利用Points of Interest (POIs)与基站相关联的优化聚类方法，实现多模态数据融合和特征提取，同时保障用户隐私。实验结果显示，Appformer在跨模态融合和时间序列分析中表现出色，达到了state-of-the-art (SOTA)指标，显著提升了移动应用使用预测的准确性和效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19414v1",
      "published_date": "2024-07-28 06:41:31 UTC",
      "updated_date": "2024-07-28 06:41:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:46:47.700793"
    },
    {
      "arxiv_id": "2407.19412v1",
      "title": "Identity-Driven Hierarchical Role-Playing Agents",
      "title_zh": "身份驱动的层次角色扮演代理",
      "authors": [
        "Libo Sun",
        "Siyuan Wang",
        "Xuanjing Huang",
        "Zhongyu Wei"
      ],
      "abstract": "Utilizing large language models (LLMs) to achieve role-playing has gained\ngreat attention recently. The primary implementation methods include leveraging\nrefined prompts and fine-tuning on role-specific datasets. However, these\nmethods suffer from insufficient precision and limited flexibility\nrespectively. To achieve a balance between flexibility and precision, we\nconstruct a Hierarchical Identity Role-Playing Framework (HIRPF) based on\nidentity theory, constructing complex characters using multiple identity\ncombinations. We develop an identity dialogue dataset for this framework and\npropose an evaluation benchmark including scale evaluation and open situation\nevaluation. Empirical results indicate the remarkable efficacy of our framework\nin modeling identity-level role simulation, and reveal its potential for\napplication in social simulation.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)进行角色扮演时存在的精度不足和灵活性有限问题，提出了一种基于身份理论的层次化身份角色扮演框架(HIRPF)，通过多个身份组合构建复杂角色，以实现精度与灵活性的平衡。论文开发了一个身份对话数据集，并设计了一个评估基准，包括规模评估和开放情境评估。实验结果表明，该框架在模拟身份级角色方面表现出色，并展示了其在社会模拟领域的应用潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19412v1",
      "published_date": "2024-07-28 06:38:56 UTC",
      "updated_date": "2024-07-28 06:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:46:50.099599"
    },
    {
      "arxiv_id": "2407.19410v1",
      "title": "AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Mahiro Ukai",
        "Shuhei Kurita",
        "Atsushi Hashimoto",
        "Yoshitaka Ushiku",
        "Nakamasa Inoue"
      ],
      "abstract": "Visual question answering aims to provide responses to natural language\nquestions given visual input. Recently, visual programmatic models (VPMs),\nwhich generate executable programs to answer questions through large language\nmodels (LLMs), have attracted research interest. However, they often require\nlong input prompts to provide the LLM with sufficient API usage details to\ngenerate relevant code. To address this limitation, we propose AdaCoder, an\nadaptive prompt compression framework for VPMs. AdaCoder operates in two\nphases: a compression phase and an inference phase. In the compression phase,\ngiven a preprompt that describes all API definitions in the Python language\nwith example snippets of code, a set of compressed preprompts is generated,\neach depending on a specific question type. In the inference phase, given an\ninput question, AdaCoder predicts the question type and chooses the appropriate\ncorresponding compressed preprompt to generate code to answer the question.\nNotably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating\nthe necessity of additional training and maintaining adaptability across\ndifferent powerful black-box LLMs such as GPT and Claude. In experiments, we\napply AdaCoder to ViperGPT and demonstrate that it reduces token length by\n71.1%, while maintaining or even improving the performance of visual question\nanswering.",
      "tldr_zh": "本论文提出 AdaCoder，一个自适应提示压缩框架，用于优化程序化视觉问答 (VPMs)，以解决大型语言模型 (LLMs) 在生成代码时需要长输入提示的问题。AdaCoder 分为压缩阶段和推理阶段：在压缩阶段，从描述 API 定义的预提示中生成基于特定问题类型的压缩预提示；在推理阶段，根据输入问题预测其类型并选择相应预提示来生成代码。该框架使用单一冻结的 LLM 和预定义提示，无需额外训练，可适应不同黑箱模型如 GPT 和 Claude。实验结果显示，在 ViperGPT 上应用 AdaCoder 后，token 长度减少 71.1%，同时维持或提升了视觉问答性能。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19410v1",
      "published_date": "2024-07-28 06:23:06 UTC",
      "updated_date": "2024-07-28 06:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:47:12.548149"
    },
    {
      "arxiv_id": "2407.19405v1",
      "title": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making",
      "title_zh": "逻辑蒸馏：逐函数从代码学习用于规划和决策",
      "authors": [
        "Dong Chen",
        "Shilin Zhang",
        "Fei Gao",
        "Yueting Zhuang",
        "Siliang Tang",
        "Qidong Liu",
        "Mingliang Xu"
      ],
      "abstract": "Large language models (LLMs) have garnered increasing attention owing to\ntheir powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs)\nthat require paid interfaces exhibit significantly superior performance\ncompared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices.\nKnowledge distillation (KD) aims to empower S-LLMs with the capabilities of\nL-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get the\npowerful logical reasoning capabilities. Consequently, S-LLMs are helpless when\nit comes to planning and decision-making tasks that require logical reasoning\ncapabilities. To tackle the identified challenges, we propose a novel framework\ncalled Logic Distillation (LD). Initially, LD employs L-LLMs to instantiate\ncomplex instructions into discrete functions and illustrates their usage to\nestablish a function base. Subsequently, based on the function base, LD\nfine-tunes S-LLMs to learn the logic employed by L-LLMs in planning and\ndecision-making. During testing, LD utilizes a retriever to identify the\ntop-$K$ relevant functions based on instructions and current states, which will\nbe selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning and\ndecision-making outcomes, function by function. Relevant experiments\ndemonstrate that with the assistance of LD, S-LLMs can achieve outstanding\nresults in planning and decision-making tasks, comparable to, or even\nsurpassing, those of L-LLMs.",
      "tldr_zh": "本文提出Logic Distillation (LD) 框架，旨在帮助小型语言模型 (S-LLMs) 学习大型语言模型 (L-LLMs) 在规划和决策任务中的逻辑推理能力，以解决传统知识蒸馏 (KD) 仅模仿输出而非真正提升推理的局限性。LD 方法首先利用 L-LLMs 将复杂指令转化为离散函数并构建函数库，然后通过微调 S-LLMs 和检索器选择相关函数，让 S-LLMs 逐函数执行规划和决策。实验结果表明，LD 增强的 S-LLMs 在相关任务中表现可与 L-LLMs 相当或更胜一筹，为部署灵活的 AI 决策系统提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19405v1",
      "published_date": "2024-07-28 05:34:42 UTC",
      "updated_date": "2024-07-28 05:34:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:47:26.884906"
    },
    {
      "arxiv_id": "2407.21066v1",
      "title": "ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech Processing Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Nakamasa Inoue",
        "Shinta Otake",
        "Takumi Hirose",
        "Masanari Ohi",
        "Rei Kawakami"
      ],
      "abstract": "Self-supervised learning has emerged as a key approach for learning generic\nrepresentations from speech data. Despite promising results in downstream tasks\nsuch as speech recognition, speaker verification, and emotion recognition, a\nsignificant number of parameters is required, which makes fine-tuning for each\ntask memory-inefficient. To address this limitation, we introduce ELP-adapter\ntuning, a novel method for parameter-efficient fine-tuning using three types of\nadapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and\na prompt adapter (P-adapter). The E-adapters are integrated into\ntransformer-based encoder layers and help to learn fine-grained speech\nrepresentations that are effective for speech recognition. The L-adapters\ncreate paths from each encoder layer to the downstream head and help to extract\nnon-linguistic features from lower encoder layers that are effective for\nspeaker verification and emotion recognition. The P-adapter appends pseudo\nfeatures to CNN features to further improve effectiveness and efficiency. With\nthese adapters, models can be quickly adapted to various speech processing\ntasks. Our evaluation across four downstream tasks using five backbone models\ndemonstrated the effectiveness of the proposed method. With the WavLM backbone,\nits performance was comparable to or better than that of full fine-tuning on\nall tasks while requiring 90% fewer learnable parameters.",
      "tldr_zh": "本研究针对自监督学习在语音处理任务（如语音识别、说话人验证和情感识别）中参数过多导致的细调效率问题，提出了一种参数高效的适配器调优方法：ELP-Adapters，包括 E-adapters（集成到 Transformer-based 编码器层中以学习细粒度语音表示）、L-adapters（从编码器层到下游头的路径以提取非语言特征）和 P-adapter（向 CNN 特征添加伪特征以提升效率）。这些适配器允许模型快速适应各种语音任务。实验结果显示，在四个下游任务和五个骨干模型（如 WavLM）的评估中，ELP-Adapters 的性能与全细调相当或更好，同时仅需 10% 的可学习参数。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21066v1",
      "published_date": "2024-07-28 05:26:03 UTC",
      "updated_date": "2024-07-28 05:26:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:47:38.072967"
    },
    {
      "arxiv_id": "2407.19401v2",
      "title": "Towards Secure and Private AI: A Framework for Decentralized Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyang Zhang",
        "Yue Zhao",
        "Claudio Angione",
        "Harry Yang",
        "James Buban",
        "Ahmad Farhan",
        "Fielding Johnston",
        "Patrick Colangelo"
      ],
      "abstract": "The rapid advancement of ML models in critical sectors such as healthcare,\nfinance, and security has intensified the need for robust data security, model\nintegrity, and reliable outputs. Large multimodal foundational models, while\ncrucial for complex tasks, present challenges in scalability, reliability, and\npotential misuse. Decentralized systems offer a solution by distributing\nworkload and mitigating central points of failure, but they introduce risks of\nunauthorized access to sensitive data across nodes. We address these challenges\nwith a comprehensive framework designed for responsible AI development. Our\napproach incorporates: 1) Zero-knowledge proofs for secure model verification,\nenhancing trust without compromising privacy. 2) Consensus-based verification\nchecks to ensure consistent outputs across nodes, mitigating hallucinations and\nmaintaining model integrity. 3) Split Learning techniques that segment models\nacross different nodes, preserving data privacy by preventing full data access\nat any point. 4) Hardware-based security through trusted execution environments\n(TEEs) to protect data and computations. This framework aims to enhance\nsecurity and privacy and improve the reliability and fairness of multimodal AI\nsystems. Promoting efficient resource utilization contributes to more\nsustainable AI development. Our state-of-the-art proofs and principles\ndemonstrate the framework's effectiveness in responsibly democratizing\nartificial intelligence, offering a promising approach for building secure and\nprivate foundational models.",
      "tldr_zh": "本论文提出一个框架，旨在通过去中心化推理提升AI的安全性和隐私性，以应对医疗、金融和安全等领域中机器学习(ML)模型面临的数据安全、模型完整性和输出可靠性挑战。该框架整合了Zero-knowledge proofs用于安全模型验证、Consensus-based verification确保节点间输出一致、Split Learning技术分割模型以保护数据隐私，以及硬件-based security通过Trusted Execution Environments (TEEs)保护计算过程。这些方法不仅提高了多模态AI系统的可靠性和公平性，还促进了资源利用效率和可持续AI发展。实验证明证明了该框架的有效性，为负责任的AI民主化提供了可行路径。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.19401v2",
      "published_date": "2024-07-28 05:09:17 UTC",
      "updated_date": "2024-12-12 18:10:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:48:01.433229"
    },
    {
      "arxiv_id": "2407.20294v2",
      "title": "A Bayesian Flow Network Framework for Chemistry Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Nianze Tao",
        "Minori Abe"
      ],
      "abstract": "In this work, we introduce ChemBFN, a language model that handles chemistry\ntasks based on Bayesian flow networks working on discrete data. A new accuracy\nschedule is proposed to improve the sampling quality by significantly reducing\nthe reconstruction loss. We show evidence that our method is appropriate for\ngenerating molecules with satisfied diversity even when a smaller number of\nsampling steps is used. A classifier-free guidance method is adapted for\nconditional generation. It is also worthwhile to point out that after\ngenerative training, our model can be fine-tuned on regression and\nclassification tasks with the state-of-the-art performance, which opens the\ngate of building all-in-one models in a single module style. Our model has been\nopen sourced at\nhttps://github.com/Augus1999/bayesian-flow-network-for-chemistry.",
      "tldr_zh": "本文提出 ChemBFN，一种基于 Bayesian flow networks 处理离散数据的语言模型，用于化学任务。研究引入新的 accuracy schedule，以显著降低重建损失并提升分子生成的多样性，即使采样步骤较少。模型还适应 classifier-free guidance 方法，支持条件生成，并在后续微调后实现回归和分类任务的 state-of-the-art 性能。这为构建全功能单一模块模型提供了新途径，相关代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "7 figures, 12 tables, 27 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.20294v2",
      "published_date": "2024-07-28 04:46:32 UTC",
      "updated_date": "2025-01-03 03:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:48:13.361037"
    },
    {
      "arxiv_id": "2407.19396v1",
      "title": "NAVIX: Scaling MiniGrid Environments with JAX",
      "title_zh": "翻译失败",
      "authors": [
        "Eduardo Pignatelli",
        "Jarek Liesen",
        "Robert Tjarko Lange",
        "Chris Lu",
        "Pablo Samuel Castro",
        "Laura Toni"
      ],
      "abstract": "As Deep Reinforcement Learning (Deep RL) research moves towards solving\nlarge-scale worlds, efficient environment simulations become crucial for rapid\nexperimentation. However, most existing environments struggle to scale to high\nthroughput, setting back meaningful progress. Interactions are typically\ncomputed on the CPU, limiting training speed and throughput, due to slower\ncomputation and communication overhead when distributing the task across\nmultiple machines. Ultimately, Deep RL training is CPU-bound, and developing\nbatched, fast, and scalable environments has become a frontier for progress.\nAmong the most used Reinforcement Learning (RL) environments, MiniGrid is at\nthe foundation of several studies on exploration, curriculum learning,\nrepresentation learning, diversity, meta-learning, credit assignment, and\nlanguage-conditioned RL, and still suffers from the limitations described\nabove. In this work, we introduce NAVIX, a re-implementation of MiniGrid in\nJAX. NAVIX achieves over 200 000x speed improvements in batch mode, supporting\nup to 2048 agents in parallel on a single Nvidia A100 80 GB. This reduces\nexperiment times from one week to 15 minutes, promoting faster design\niterations and more scalable RL model development.",
      "tldr_zh": "本研究针对深度强化学习(Deep RL)中环境模拟效率问题，指出现有环境如 MiniGrid 因 CPU 计算而导致训练速度缓慢和吞吐量不足。作者引入 NAVIX，这是一个使用 JAX 重新实现的 MiniGrid 环境，支持批量模式下的高并发计算。实验结果显示，NAVIX 在单个 Nvidia A100 80 GB 上实现了超过 200,000 倍的速度提升，可并行运行多达 2048 个代理，将实验时间从一周缩短至 15 分钟，从而加速 RL 模型的设计迭代和可扩展性发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19396v1",
      "published_date": "2024-07-28 04:39:18 UTC",
      "updated_date": "2024-07-28 04:39:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:48:21.085710"
    },
    {
      "arxiv_id": "2407.19393v2",
      "title": "Integrating Cognitive AI with Generative Models for Enhanced Question Answering in Skill-based Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rochan H. Madhusudhana",
        "Rahul K. Dass",
        "Jeanette Luu",
        "Ashok K. Goel"
      ],
      "abstract": "In online learning, the ability to provide quick and accurate feedback to\nlearners is crucial. In skill-based learning, learners need to understand the\nunderlying concepts and mechanisms of a skill to be able to apply it\neffectively. While videos are a common tool in online learning, they cannot\ncomprehend or assess the skills being taught. Additionally, while Generative AI\nmethods are effective in searching and retrieving answers from a text corpus,\nit remains unclear whether these methods exhibit any true understanding. This\nlimits their ability to provide explanations of skills or help with\nproblem-solving. This paper proposes a novel approach that merges Cognitive AI\nand Generative AI to address these challenges. We employ a structured knowledge\nrepresentation, the TMK (Task-Method-Knowledge) model, to encode skills taught\nin an online Knowledge-based AI course. Leveraging techniques such as Large\nLanguage Models, Chain-of-Thought, and Iterative Refinement, we outline a\nframework for generating reasoned explanations in response to learners'\nquestions about skills.",
      "tldr_zh": "本论文针对技能-based 学习中的问题，提出一种整合 Cognitive AI 和 Generative AI 的新方法，以提升在线学习问答系统的性能和准确性。\n该方法使用 TMK (Task-Method-Knowledge) 模型来编码技能知识，并结合 Large Language Models、Chain-of-Thought 和 Iterative Refinement 技术，生成有理据的解释。\n实验框架应用于在线 Knowledge-based AI 课程，帮助学习者更好地理解技能机制和解决问题，从而提供更有效的反馈。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2407.19393v2",
      "published_date": "2024-07-28 04:21:22 UTC",
      "updated_date": "2024-08-02 21:06:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:48:37.906663"
    },
    {
      "arxiv_id": "2407.19385v1",
      "title": "Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Nagur Shareef Shaik",
        "Teja Krishna Cherukuri",
        "Vince D. Calhoun",
        "Dong Hye Ye"
      ],
      "abstract": "Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive\nimpairments, abnormalities in brain structure, function, and genetic factors.\nIts complex symptoms and overlap with other psychiatric conditions challenge\ntraditional diagnostic methods, necessitating advanced systems to improve\nprecision. Existing research studies have mostly focused on imaging data, such\nas structural and functional MRI, for SZ diagnosis. There has been less focus\non the integration of genomic features despite their potential in identifying\nheritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics\nTransformer (MIGTrans), that attentively integrates genomics with structural\nand functional imaging data to capture SZ-related neuroanatomical and\nconnectome abnormalities. MIGTrans demonstrated improved SZ classification\nperformance with an accuracy of 86.05% (+/- 0.02), offering clear\ninterpretations and identifying significant genomic locations and brain\nmorphological/connectivity patterns associated with SZ.",
      "tldr_zh": "该研究针对精神分裂症 (SZ) 的诊断挑战，提出了一种 Multi-modal Imaging Genomics Transformer (MIGTrans) 框架，通过注意力机制整合基因组生物标记与结构和功能 MRI 影像数据，以捕捉 SZ 相关的神经解剖和连接异常。相比传统方法，该框架突出了基因组特征的潜在作用，提升了 SZ 分类的准确率至 86.05%。实验结果还提供了可解释性，识别出关键基因组位置和脑形态/连接模式，为精准精神疾病诊断奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for presentation at the AI for Imaging Genomic Learning\n  (AIIG) Workshop, MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19385v1",
      "published_date": "2024-07-28 03:54:08 UTC",
      "updated_date": "2024-07-28 03:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:48:41.166734"
    },
    {
      "arxiv_id": "2407.19380v1",
      "title": "Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment",
      "title_zh": "使用医疗决策转换器赋能临床医生：脓毒症治疗框架",
      "authors": [
        "Aamer Abdul Rahman",
        "Pranav Agarwal",
        "Rita Noumeir",
        "Philippe Jouvet",
        "Vincent Michalski",
        "Samira Ebrahimi Kahou"
      ],
      "abstract": "Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.",
      "tldr_zh": "该研究提出了一种名为MeDT（Medical Decision Transformer）的框架，基于目标条件强化学习（goal-conditioned reinforcement learning）范式，用于脓毒症(sepsis)治疗决策。该框架利用决策转换器(Decision Transformer)架构，通过离线训练分析治疗轨迹，包括已知治疗结果、目标敏锐度分数(acuity scores)、过去决策和医疗状态，从而捕捉患者历史、决策和短期效果的复杂依赖关系。MeDT通过条件化处理稀疏奖励问题，提升临床医生与模型的交互性，并在MIMIC-III数据集上的离线评估中，显示出比现有offline reinforcement learning方法更优或相当的表现，提供更可解释、个性化和个性化的治疗推荐。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19380v1",
      "published_date": "2024-07-28 03:40:00 UTC",
      "updated_date": "2024-07-28 03:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:49:03.946757"
    },
    {
      "arxiv_id": "2407.19359v1",
      "title": "Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Xue",
        "Nan Du",
        "Anne Mottram",
        "Martin Seneviratne",
        "Andrew M. Dai"
      ],
      "abstract": "We propose to meta-learn an a self-supervised patient trajectory forecast\nlearning rule by meta-training on a meta-objective that directly optimizes the\nutility of the patient representation over the subsequent clinical outcome\nprediction. This meta-objective directly targets the usefulness of a\nrepresentation generated from unlabeled clinical measurement forecast for later\nsupervised tasks.\n  The meta-learned can then be directly used in target risk prediction, and the\nlimited available samples can be used for further fine-tuning the model\nperformance. The effectiveness of our approach is tested on a real open source\npatient EHR dataset MIMIC-III. We are able to demonstrate that our\nattention-based patient state representation approach can achieve much better\nperformance for predicting target risk with low resources comparing with both\ndirect supervised learning and pretraining with all-observation trajectory\nforecast.",
      "tldr_zh": "该论文提出了一种元学习(meta-learning)方法，通过自监督患者轨迹预测学习规则，直接优化患者表示的效用，以提升后续临床结果预测的性能。该方法针对无标签临床测量预测生成的表示，允许直接应用于目标风险预测，并利用有限样本进行进一步微调。在MIMIC-III数据集上的实验显示，与直接监督学习和全观测轨迹预训练相比，该基于注意力的(attention-based)患者状态表示方法在低资源环境下实现了显著更好的预测性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2020",
      "pdf_url": "http://arxiv.org/pdf/2407.19359v1",
      "published_date": "2024-07-28 01:22:04 UTC",
      "updated_date": "2024-07-28 01:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:49:15.226891"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 37,
  "processed_papers_count": 37,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T10:49:49.461029"
}
{
  "date": "2024-04-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-19 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于人工智能和机器学习领域，涵盖 LLM 在医疗决策和生成任务中的创新应用、强化学习优化、多模态模型的进展，以及图神经网络和生成对抗网络等热门主题。令人印象深刻的文章包括 LLMs 在癌症分期中的 ensemble 推理方法（作者 Christopher C. Yang），展示了 AI 在医疗中的潜力，以及 PhysDreamer 的物理交互生成模型（作者 Jiajun Wu）。这些论文突出了 AI 的实际应用和高效学习机制。\n\n下面，我挑选并简要讨论几篇关键论文，先从 LLM 和医疗相关的高影响力文章入手，然后聊聊生成模型和强化学习主题的亮点，其他较为常规或次要的论文（如一些基础优化或特定数据集实验）将快速掠过，以控制篇幅。每个条目列出论文标题（中文 + 英文），并聚焦核心贡献和发现。\n\n### 1. Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging  \n（英文原标题：Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging）  \n这篇论文提出一种 ensemble 推理方法，用于提升大型语言模型（LLMs）在癌症分期任务中的一致性和准确性。通过在真实病理报告上实验，方法显著改善了模型的可靠性，适用于临床领域，强调了 LLMs 在高风险医疗决策中的潜力。\n\n### 2. Explainable AI for Fair Sepsis Mortality Predictive Model  \n（英文原标题：Explainable AI for Fair Sepsis Mortality Predictive Model）  \n作者 Christopher C. Yang 等人开发了一种公平性优化模型，用于预测败血症死亡率。核心贡献是通过迁移学习和置换特征重要性算法，确保模型预测的公平性和可解释性，这为医疗 AI 提供了可信赖的框架，减少了偏置问题。\n\n### 3. PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation  \n（英文原标题：PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation）  \n作者 Jiajun Wu 和 William T. Freeman 等人提出 PhysDreamer 框架，利用视频生成模型模拟 3D 物体的物理交互。论文的主要发现是通过蒸馏视频生成先验，实现真实感强的物体动态响应，适用于虚拟现实和增强现实应用，显著提升了交互模拟的逼真度。\n\n### 4. Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models  \n（英文原标题：Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models）  \n这篇论文引入 Groma 模型，通过局部视觉标记化机制提升多模态 LLM 的区域级理解能力。核心贡献是构建了一个基于 GPT-4V 的数据集和机制，支持图像区域标注和生成，实验在基准数据集上表现出色，推进了多模态模型在视觉任务中的应用。\n\n### 5. Mapping Social Choice Theory to RLHF  \n（英文原标题：Mapping Social Choice Theory to RLHF）  \n作者分析了强化学习从人类反馈（RLHF）中的偏好聚合问题，将社会选择理论应用于 LLM 训练。论文发现，通过技术映射，可以更好地处理偏好分歧，提升模型决策的鲁棒性，这为 AI 伦理和偏好建模提供了新视角。\n\n### 6. Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning  \n（英文原标题：Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning）  \n论文提出 GEASD 框架，通过自适应技能分布优化目标条件强化学习（GCRL）。主要贡献是提升了探索效率，实验显示框架在复杂环境中显著改善了目标扩散行为，适用于长时序任务。\n\n### 7. Heterogeneous Subgraph Transformer for Fake News Detection  \n（英文原标题：Heterogeneous Subgraph Transformer for Fake News Detection）  \n这篇论文开发了 HeteroSGT 模型，用于检测社交媒体假新闻。核心发现是通过异构子图和预训练语言模型捕捉新闻元素的语义关系，实验在真实数据集上优于基线，强化了图神经网络在 misinformation 检测中的作用。\n\n### 8. Transformer Based Planning in the Observation Space with Applications to Trick Taking Card Games  \n（英文原标题：Transformer Based Planning in the Observation Space with Applications to Trick Taking Card Games）  \n作者提出 GO-MCTS 方法，将 Transformer 用于不完美信息游戏的规划。论文的主要贡献是通过观察空间的 MCTS 和自玩强化学习，显著提升了游戏策略的性能，适用于 Hearts 和 Skat 等游戏。\n\n### 9. Large Language Models for Networking: Workflow, Advances and Challenges  \n（英文原标题：Large Language Models for Networking: Workflow, Advances and Challenges）  \n论文概述了 LLM 在网络领域的应用，包括工作流程和挑战。核心发现是 LLM 可优化网络任务如配置和诊断，但需解决泛化问题，为网络 AI 提供了实用框架。\n\n### 10. FineRec: Exploring Fine-grained Sequential Recommendation  \n（英文原标题：FineRec: Exploring Fine-grained Sequential Recommendation）  \n这篇论文提出 FineRec 框架，通过属性-意见对分析用户评论实现细粒度推荐。主要贡献是利用图神经网络和多样性卷积提升推荐准确性，实验在真实数据集上超越了现有方法。\n\n其他论文如一些基础强化学习优化（如 R3L）或特定数据集实验（如 Multi Class Depression Detection），虽然有贡献但相对常规，我这里快速掠过：它们主要验证了特定领域的模型改进，如情感检测或网络嵌入，但未带来革命性突破，不再详细展开。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，尤其在 LLM 和多模态应用上。如果你对医疗 AI 或生成模型感兴趣，强烈推荐查看前几篇！更多细节可查阅 arXiv。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2404.13218v1",
      "title": "On the Temperature of Machine Learning Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Zhang"
      ],
      "abstract": "We develop a thermodynamic theory for machine learning (ML) systems. Similar\nto physical thermodynamic systems which are characterized by energy and\nentropy, ML systems possess these characteristics as well. This comparison\ninspire us to integrate the concept of temperature into ML systems grounded in\nthe fundamental principles of thermodynamics, and establish a basic\nthermodynamic framework for machine learning systems with non-Boltzmann\ndistributions. We introduce the concept of states within a ML system, identify\ntwo typical types of state, and interpret model training and refresh as a\nprocess of state phase transition. We consider that the initial potential\nenergy of a ML system is described by the model's loss functions, and the\nenergy adheres to the principle of minimum potential energy. For a variety of\nenergy forms and parameter initialization methods, we derive the temperature of\nsystems during the phase transition both analytically and asymptotically,\nhighlighting temperature as a vital indicator of system data distribution and\nML training complexity. Moreover, we perceive deep neural networks as complex\nheat engines with both global temperature and local temperatures in each layer.\nThe concept of work efficiency is introduced within neural networks, which\nmainly depends on the neural activation functions. We then classify neural\nnetworks based on their work efficiency, and describe neural networks as two\ntypes of heat engines.",
      "tldr_zh": "本文提出了一种机器学习(ML)系统的热力学理论，将ML系统比作物理系统，通过能量和熵引入温度概念，并建立一个适用于非-Boltzmann分布的热力学框架。论文将模型训练视为状态相变过程，基于损失函数推导相变中的温度，作为评估系统数据分布和训练复杂性的关键指标。同时，将深度神经网络视为复杂的热机，引入工作效率的概念，并根据神经激活函数将网络分类为两种热机类型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "44 pages, 8 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.13218v1",
      "published_date": "2024-04-19 23:54:32 UTC",
      "updated_date": "2024-04-19 23:54:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:49:38.245757"
    },
    {
      "arxiv_id": "2404.13194v1",
      "title": "Privacy-Preserving Debiasing using Data Augmentation and Machine Unlearning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixin Pan",
        "Emma Andrews",
        "Laura Chang",
        "Prabhat Mishra"
      ],
      "abstract": "Data augmentation is widely used to mitigate data bias in the training\ndataset. However, data augmentation exposes machine learning models to privacy\nattacks, such as membership inference attacks. In this paper, we propose an\neffective combination of data augmentation and machine unlearning, which can\nreduce data bias while providing a provable defense against known attacks.\nSpecifically, we maintain the fairness of the trained model with\ndiffusion-based data augmentation, and then utilize multi-shard unlearning to\nremove identifying information of original data from the ML model for\nprotection against privacy attacks. Experimental evaluation across diverse\ndatasets demonstrates that our approach can achieve significant improvements in\nbias reduction as well as robustness against state-of-the-art privacy attacks.",
      "tldr_zh": "该论文提出了一种隐私保护下的去偏差方法，通过结合数据 augmentation 和 machine unlearning 来减少训练数据集中的数据偏差，同时防御 privacy attacks 如 membership inference attacks。具体而言，该方法使用 diffusion-based data augmentation 维持模型的公平性，并采用 multi-shard unlearning 移除原始数据的识别信息，以提供可证明的攻击防御。实验在多种数据集上显示，该方法显著提高了偏差减少效果，并提升了对最先进隐私攻击的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13194v1",
      "published_date": "2024-04-19 21:54:20 UTC",
      "updated_date": "2024-04-19 21:54:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:49:50.634596"
    },
    {
      "arxiv_id": "2404.15370v1",
      "title": "Self-Supervised Learning for User Localization",
      "title_zh": "翻译失败",
      "authors": [
        "Ankan Dash",
        "Jingyi Gu",
        "Guiling Wang",
        "Nirwan Ansari"
      ],
      "abstract": "Machine learning techniques have shown remarkable accuracy in localization\ntasks, but their dependency on vast amounts of labeled data, particularly\nChannel State Information (CSI) and corresponding coordinates, remains a\nbottleneck. Self-supervised learning techniques alleviate the need for labeled\ndata, a potential that remains largely untapped and underexplored in existing\nresearch. Addressing this gap, we propose a pioneering approach that leverages\nself-supervised pretraining on unlabeled data to boost the performance of\nsupervised learning for user localization based on CSI. We introduce two\npretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs)\nand Convolutional Neural Networks (CNNs) to glean representations from\nunlabeled data via self-supervised learning. Following this, we utilize the\nencoder portion of the AE models to extract relevant features from labeled\ndata, and finetune an MLP-based Position Estimation Model to accurately deduce\nuser locations. Our experimentation on the CTW-2020 dataset, which features a\nsubstantial volume of unlabeled data but limited labeled samples, demonstrates\nthe viability of our approach. Notably, the dataset covers a vast area spanning\nover 646x943x41 meters, and our approach demonstrates promising results even\nfor such expansive localization tasks.",
      "tldr_zh": "这篇论文针对用户定位任务中对大量标注数据（如 Channel State Information (CSI) 和坐标）的依赖问题，提出了一种基于 Self-Supervised Learning 的方法，以提升模型性能。研究者设计了两个预训练 Auto Encoder (AE) 模型，分别使用 Multi Layer Perceptrons (MLPs) 和 Convolutional Neural Networks (CNNs)，从无标签数据中提取表示。接着，他们利用 AE 的编码器部分提取特征，并微调一个 MLP-based 位置估计模型来进行精确定位。在 CTW-2020 数据集上实验证明，该方法在覆盖646x943x41米的广阔区域时表现出色，展示了其在数据有限场景下的潜力。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.15370v1",
      "published_date": "2024-04-19 21:49:10 UTC",
      "updated_date": "2024-04-19 21:49:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:50:04.060031"
    },
    {
      "arxiv_id": "2404.13192v1",
      "title": "Heterogeneous Subgraph Transformer for Fake News Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Zhang",
        "Xiaoxiao Ma",
        "Jia Wu",
        "Jian Yang",
        "Hao Fan"
      ],
      "abstract": "Fake news is pervasive on social media, inflicting substantial harm on public\ndiscourse and societal well-being. We investigate the explicit structural\ninformation and textual features of news pieces by constructing a heterogeneous\ngraph concerning the relations among news topics, entities, and content.\nThrough our study, we reveal that fake news can be effectively detected in\nterms of the atypical heterogeneous subgraphs centered on them, which\nencapsulate the essential semantics and intricate relations between news\nelements. However, suffering from the heterogeneity, exploring such\nheterogeneous subgraphs remains an open problem. To bridge the gap, this work\nproposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs\nin our constructed heterogeneous graph. In HeteroSGT, we first employ a\npre-trained language model to derive both word-level and sentence-level\nsemantics. Then the random walk with restart (RWR) is applied to extract\nsubgraphs centered on each news, which are further fed to our proposed subgraph\nTransformer to quantify the authenticity. Extensive experiments on five\nreal-world datasets demonstrate the superior performance of HeteroSGT over five\nbaselines. Further case and ablation studies validate our motivation and\ndemonstrate that performance improvement stems from our specially designed\ncomponents.",
      "tldr_zh": "该研究针对社交媒体上的假新闻问题，通过构建一个涉及新闻主题、实体和内容的异构图（heterogeneous graph），来探索以假新闻为中心的非典型异构子图（heterogeneous subgraphs），以捕捉其语义和关系特征。作者提出了一种异构子图Transformer（HeteroSGT）框架，首先利用预训练语言模型提取词级和句级语义，然后应用随机游走重启（RWR）提取子图，并通过子图Transformer量化新闻的真实性。在五个真实数据集上的实验显示，HeteroSGT 优于五个基线模型，进一步的案例和消融研究证实了其设计的有效性和性能提升来源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13192v1",
      "published_date": "2024-04-19 21:39:37 UTC",
      "updated_date": "2024-04-19 21:39:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:50:14.568690"
    },
    {
      "arxiv_id": "2404.13150v1",
      "title": "Transformer Based Planning in the Observation Space with Applications to Trick Taking Card Games",
      "title_zh": "翻译失败",
      "authors": [
        "Douglas Rebstock",
        "Christopher Solinas",
        "Nathan R. Sturtevant",
        "Michael Buro"
      ],
      "abstract": "Traditional search algorithms have issues when applied to games of imperfect\ninformation where the number of possible underlying states and trajectories are\nvery large. This challenge is particularly evident in trick-taking card games.\nWhile state sampling techniques such as Perfect Information Monte Carlo (PIMC)\nsearch has shown success in these contexts, they still have major limitations.\n  We present Generative Observation Monte Carlo Tree Search (GO-MCTS), which\nutilizes MCTS on observation sequences generated by a game specific model. This\nmethod performs the search within the observation space and advances the search\nusing a model that depends solely on the agent's observations. Additionally, we\ndemonstrate that transformers are well-suited as the generative model in this\ncontext, and we demonstrate a process for iteratively training the transformer\nvia population-based self-play.\n  The efficacy of GO-MCTS is demonstrated in various games of imperfect\ninformation, such as Hearts, Skat, and \"The Crew: The Quest for Planet Nine,\"\nwith promising results.",
      "tldr_zh": "该论文针对不完美信息游戏（如trick-taking card games）中传统搜索算法的局限性，提出Generative Observation Monte Carlo Tree Search (GO-MCTS)方法，该方法在观察空间使用MCTS进行搜索，并依赖于代理观察的游戏特定模型。论文强调Transformer模型适合作为生成模型，并介绍了通过population-based self-play迭代训练Transformer的过程，以提升搜索效率。在Hearts、Skat和\"The Crew: The Quest for Planet Nine\"等游戏中，GO-MCTS显示出显著的性能提升，证明了其在处理大规模状态空间的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13150v1",
      "published_date": "2024-04-19 19:41:00 UTC",
      "updated_date": "2024-04-19 19:41:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:50:27.565564"
    },
    {
      "arxiv_id": "2404.13149v1",
      "title": "Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Hsuan Chang",
        "Mary M. Lucas",
        "Yeawon Lee",
        "Christopher C. Yang",
        "Grace Lu-Yao"
      ],
      "abstract": "Advances in large language models (LLMs) have encouraged their adoption in\nthe healthcare domain where vital clinical information is often contained in\nunstructured notes. Cancer staging status is available in clinical reports, but\nit requires natural language processing to extract the status from the\nunstructured text. With the advance in clinical-oriented LLMs, it is promising\nto extract such status without extensive efforts in training the algorithms.\nPrompting approaches of the pre-trained LLMs that elicit a model's reasoning\nprocess, such as chain-of-thought, may help to improve the trustworthiness of\nthe generated responses. Using self-consistency further improves model\nperformance, but often results in inconsistent generations across the multiple\nreasoning paths. In this study, we propose an ensemble reasoning approach with\nthe aim of improving the consistency of the model generations. Using an open\naccess clinical large language model to determine the pathologic cancer stage\nfrom real-world pathology reports, we show that the ensemble reasoning approach\nis able to improve both the consistency and performance of the LLM in\ndetermining cancer stage, thereby demonstrating the potential to use these\nmodels in clinical or other domains where reliability and trustworthiness are\ncritical.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在处理医疗非结构化文本（如癌症分期报告）时的挑战，指出传统Self-Consistency方法虽能提升性能，但常导致生成结果不一致。研究提出Ensemble Reasoning方法，通过整合多条推理路径来提高LLMs的consistency和accuracy。实验结果显示，使用开源临床LLMs从真实病理报告中提取癌症分期时，该方法显著改善了模型的表现，并证明了其在可靠性要求高的领域（如临床医学）中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted to the 22nd International Conference on Artificial\n  Intelligence in Medicine (AIME'24)",
      "pdf_url": "http://arxiv.org/pdf/2404.13149v1",
      "published_date": "2024-04-19 19:34:35 UTC",
      "updated_date": "2024-04-19 19:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:50:38.780802"
    },
    {
      "arxiv_id": "2404.13142v2",
      "title": "Decentralized Coordination of Distributed Energy Resources through Local Energy Markets and Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel May",
        "Matthew Taylor",
        "Petr Musilek"
      ],
      "abstract": "As distributed energy resources (DERs) grow, the electricity grid faces\nincreased net load variability at the grid edge, impacting operability and\nreliability. Transactive energy, facilitated through local energy markets,\noffers a decentralized, indirect demand response solution, with model-free\ncontrol techniques, such as deep reinforcement learning (DRL), enabling\nautomated, decentralized participation. However, existing studies largely\noverlook community-level net load variability, focusing instead on\nsocioeconomic metrics.\n  This study addresses this gap by using DRL agents to automate end-user\nparticipation in a local energy market (ALEX), where agents act independently\nto minimize individual energy bills. Results reveal a strong link between bill\nreduction and decreased net load variability, assessed across metrics such as\nramping rate, load factor, and peak demand over various time horizons. Using a\nno-control baseline, DRL agents are benchmarked against a near-optimal dynamic\nprogramming approach. The dynamic programming benchmark achieves reductions of\n22.05 percent, 83.92 percent, and 24.09 percent in daily import, export, and\npeak demand, respectively, while the DRL agents show comparable or superior\nresults with reductions of 21.93 percent, 84.46 percent, and 27.02 percent.\n  This study demonstrates the effectiveness of DRL in decentralized grid\nmanagement, highlighting its scalability and near-optimal performance in\nreducing net load variability within community-driven energy markets.",
      "tldr_zh": "本文研究了分布式能源资源(DERs)的增长导致电网边缘网荷变异性增加的问题，并提出使用深度强化学习(DRL)代理在本地能源市场(ALEX)中实现去中心化协调，以自动化用户参与并最小化个人能源账单。结果显示，DRL代理不仅显著减少了账单，还降低了网荷变异性指标，如ramping rate、load factor和peak demand。相比动态编程基准，DRL实现了相媲美或优越的表现，例如减少进口21.93%、出口84.46%和peak demand 27.02%。这项研究证明了DRL在社区驱动能源市场中的可扩展性和近似最优性能，为去中心化电网管理提供了有效解决方案。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "preprint, submitted to Energy and AI",
      "pdf_url": "http://arxiv.org/pdf/2404.13142v2",
      "published_date": "2024-04-19 19:03:33 UTC",
      "updated_date": "2024-11-14 19:36:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:50:52.225816"
    },
    {
      "arxiv_id": "2404.13139v1",
      "title": "Explainable AI for Fair Sepsis Mortality Predictive Model",
      "title_zh": "可解释",
      "authors": [
        "Chia-Hsuan Chang",
        "Xiaoyang Wang",
        "Christopher C. Yang"
      ],
      "abstract": "Artificial intelligence supports healthcare professionals with predictive\nmodeling, greatly transforming clinical decision-making. This study addresses\nthe crucial need for fairness and explainability in AI applications within\nhealthcare to ensure equitable outcomes across diverse patient demographics. By\nfocusing on the predictive modeling of sepsis-related mortality, we propose a\nmethod that learns a performance-optimized predictive model and then employs\nthe transfer learning process to produce a model with better fairness. Our\nmethod also introduces a novel permutation-based feature importance algorithm\naiming at elucidating the contribution of each feature in enhancing fairness on\npredictions. Unlike existing explainability methods concentrating on explaining\nfeature contribution to predictive performance, our proposed method uniquely\nbridges the gap in understanding how each feature contributes to fairness. This\nadvancement is pivotal, given sepsis's significant mortality rate and its role\nin one-third of hospital deaths. Our method not only aids in identifying and\nmitigating biases within the predictive model but also fosters trust among\nhealthcare stakeholders by improving the transparency and fairness of model\npredictions, thereby contributing to more equitable and trustworthy healthcare\ndelivery.",
      "tldr_zh": "本研究针对AI在医疗决策中的公平性和可解释性问题，提出了一种用于预测脓毒症死亡率的新方法。该方法先训练一个性能优化的预测模型，然后通过迁移学习（Transfer Learning）来提升模型的公平性，同时引入一个基于置换的特征重要性算法（Permutation-based Feature Importance），以解释每个特征如何贡献于公平预测。与现有方法不同，该算法专注于特征对公平性的影响，而非仅对预测性能的贡献。通过减少模型偏差，该方法有助于提升医疗决策的透明度和可信度，促进更公平的医疗结果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the 22nd International Conference on Artificial\n  Intelligence in Medicine (AIME'24)",
      "pdf_url": "http://arxiv.org/pdf/2404.13139v1",
      "published_date": "2024-04-19 18:56:46 UTC",
      "updated_date": "2024-04-19 18:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:51:02.582014"
    },
    {
      "arxiv_id": "2404.15369v2",
      "title": "Can a Machine be Conscious? Towards Universal Criteria for Machine Consciousness",
      "title_zh": "翻译失败",
      "authors": [
        "Nur Aizaan Anwar",
        "Cosmin Badea"
      ],
      "abstract": "As artificially intelligent systems become more anthropomorphic and\npervasive, and their potential impact on humanity more urgent, discussions\nabout the possibility of machine consciousness have significantly intensified,\nand it is sometimes seen as 'the holy grail'. Many concerns have been voiced\nabout the ramifications of creating an artificial conscious entity. This is\ncompounded by a marked lack of consensus around what constitutes consciousness\nand by an absence of a universal set of criteria for determining consciousness.\nBy going into depth on the foundations and characteristics of consciousness, we\npropose five criteria for determining whether a machine is conscious, which can\nalso be applied more generally to any entity. This paper aims to serve as a\nprimer and stepping stone for researchers of consciousness, be they in\nphilosophy, computer science, medicine, or any other field, to further pursue\nthis holy grail of philosophy, neuroscience and artificial intelligence.",
      "tldr_zh": "该论文探讨了机器意识的可能性，强调随着AI系统变得更具人形，其潜在影响日益紧迫，但目前缺乏对意识的共识和通用判定标准。作者深入分析意识的基础和特征，提出五项criteria for determining consciousness，用于评估机器或其他实体是否具有意识。这些标准旨在作为哲学、计算机科学、医学等领域研究者的入门指南，推动机器意识（machine consciousness）这一“圣杯”问题的进一步研究。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "q-bio.NC",
      "comment": "This work was supported by the UKRI CDT in AI for Healthcare,\n  http://ai4health.io (Grant No. EP/S023283/1)",
      "pdf_url": "http://arxiv.org/pdf/2404.15369v2",
      "published_date": "2024-04-19 18:38:22 UTC",
      "updated_date": "2024-04-30 17:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:51:14.417799"
    },
    {
      "arxiv_id": "2404.13131v1",
      "title": "From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap",
      "title_zh": "翻译失败",
      "authors": [
        "Tianqi Kou"
      ],
      "abstract": "Two goals - improving replicability and accountability of Machine Learning\nresearch respectively, have accrued much attention from the AI ethics and the\nMachine Learning community. Despite sharing the measures of improving\ntransparency, the two goals are discussed in different registers -\nreplicability registers with scientific reasoning whereas accountability\nregisters with ethical reasoning. Given the existing challenge of the\nResponsibility Gap - holding Machine Learning scientists accountable for\nMachine Learning harms due to them being far from sites of application, this\npaper posits that reconceptualizing replicability can help bridge the gap.\nThrough a shift from model performance replicability to claim replicability,\nMachine Learning scientists can be held accountable for producing\nnon-replicable claims that are prone to eliciting harm due to misuse and\nmisinterpretation. In this paper, I make the following contributions. First, I\ndefine and distinguish two forms of replicability for ML research that can aid\nconstructive conversations around replicability. Second, I formulate an\nargument for claim-replicability's advantage over model performance\nreplicability in justifying assigning accountability to Machine Learning\nscientists for producing non-replicable claims and show how it enacts a sense\nof responsibility that is actionable. In addition, I characterize the\nimplementation of claim replicability as more of a social project than a\ntechnical one by discussing its competing epistemological principles, practical\nimplications on Circulating Reference, Interpretative Labor, and research\ncommunication.",
      "tldr_zh": "这篇论文探讨了机器学习（Machine Learning）研究的复制性（replicability）和问责性（accountability），提出通过从模型性能复制性（model performance replicability）转向声明复制性（claim replicability）来桥接责任差距（Responsibility Gap），从而使机器学习科学家对非可复制声明负责，这些声明可能因误用和误解引发危害。作者的主要贡献包括定义并区分两种复制性形式，并论证声明复制性在分配问责性方面的优势，促进可行动的责任感。此外，论文强调实现声明复制性更依赖社会项目而非技术手段，涉及竞争的认识论原则、循环参考（Circulating Reference）、解释性劳动（Interpretative Labor）和研究沟通的实际影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming in FAccT 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13131v1",
      "published_date": "2024-04-19 18:36:14 UTC",
      "updated_date": "2024-04-19 18:36:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:51:28.773250"
    },
    {
      "arxiv_id": "2404.13038v1",
      "title": "Mapping Social Choice Theory to RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Jessica Dai",
        "Eve Fleisig"
      ],
      "abstract": "Recent work on the limitations of using reinforcement learning from human\nfeedback (RLHF) to incorporate human preferences into model behavior often\nraises social choice theory as a reference point. Social choice theory's\nanalysis of settings such as voting mechanisms provides technical\ninfrastructure that can inform how to aggregate human preferences amid\ndisagreement. We analyze the problem settings of social choice and RLHF,\nidentify key differences between them, and discuss how these differences may\naffect the RLHF interpretation of well-known technical results in social\nchoice.",
      "tldr_zh": "这篇论文探讨了将社会选择理论（Social Choice Theory）映射到强化学习从人类反馈（RLHF）中的过程，旨在解决RLHF在整合人类偏好时的局限性。通过比较二者的问题设置，如投票机制，论文识别了关键差异，例如偏好聚合的上下文和机制。最终，论文讨论了这些差异如何影响RLHF对社会选择理论中已知技术结果的解读，为改进RLHF的设计提供见解。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13038v1",
      "published_date": "2024-04-19 17:49:56 UTC",
      "updated_date": "2024-04-19 17:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:51:41.832671"
    },
    {
      "arxiv_id": "2404.13028v1",
      "title": "When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Choi",
        "William Gazeley"
      ],
      "abstract": "This paper presents the LLM-ADE framework, a novel methodology for continued\npre-training of large language models (LLMs) that addresses the challenges of\ncatastrophic forgetting and double descent. LLM-ADE employs dynamic\narchitectural adjustments, including selective block freezing and expansion,\ntailored to specific datasets. This strategy enhances model adaptability to new\ndata while preserving previously acquired knowledge. We demonstrate LLM-ADE's\neffectiveness on the TinyLlama model across various general knowledge\nbenchmarks, showing significant performance improvements without the drawbacks\nof traditional continuous training methods. This approach promises a more\nversatile and robust way to keep LLMs current and efficient in real-world\napplications.",
      "tldr_zh": "这篇论文提出了LLM-ADE框架，一种用于大型语言模型(LLMs)的持续预训练方法，旨在解决灾难性遗忘(catastrophic forgetting)和双重下降(double descent)问题。\n该框架通过动态架构调整，包括选择性块冻结和扩展，针对特定数据集来提升模型对新数据的适应性，同时保留先前获得的知识。\n实验结果显示，在TinyLlama模型上应用LLM-ADE后，在各种一般知识基准上取得了显著性能改进，而避免了传统连续训练方法的缺点。\n这种方法为在现实应用中保持LLMs更灵活和高效提供了新途径。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "6 pages, 3 tables and 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.13028v1",
      "published_date": "2024-04-19 17:43:26 UTC",
      "updated_date": "2024-04-19 17:43:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:51:55.307154"
    },
    {
      "arxiv_id": "2404.13026v2",
      "title": "PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Tianyuan Zhang",
        "Hong-Xing Yu",
        "Rundi Wu",
        "Brandon Y. Feng",
        "Changxi Zheng",
        "Noah Snavely",
        "Jiajun Wu",
        "William T. Freeman"
      ],
      "abstract": "Realistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to novel\ninteractions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism of\nthe synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically plausible\nmanner. See our project page at https://physdreamer.github.io/.",
      "tldr_zh": "该研究提出PhysDreamer，一种基于物理的框架，通过利用视频生成模型学到的物体动态先验，使静态3D物体能够合成现实的交互响应，例如外部力或代理操作。PhysDreamer解决了行动条件动态生成中的关键挑战，包括感知物体物理材质属性（如刚度），并通过提炼这些先验来实现物理上合理的物体运动。实验在多种弹性物体上进行了演示，用户研究显示其交互现实性显著提升，为更沉浸式的虚拟体验奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website at: https://physdreamer.github.io/ Appear on ECCV\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13026v2",
      "published_date": "2024-04-19 17:41:05 UTC",
      "updated_date": "2024-10-07 06:08:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:52:04.996143"
    },
    {
      "arxiv_id": "2404.13013v1",
      "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chuofan Ma",
        "Yi Jiang",
        "Jiannan Wu",
        "Zehuan Yuan",
        "Xiaojuan Qi"
      ],
      "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.",
      "tldr_zh": "我们介绍了 Groma，一种 Multimodal Large Language Model (MLLM)，具备 grounded 和 fine-grained 视觉感知能力，能够处理整体图像理解以及区域级任务，如 region captioning 和 visual grounding。\nGroma 的核心机制是 localized visual tokenization，将图像分解成感兴趣的区域并编码成 region tokens，从而无缝整合到用户指令和模型响应中，实现对指定区域的理解和文本输出的视觉 grounding。\n为了提升 grounded chat 能力，研究者利用 GPT-4V 和 visual prompting 技术构建了一个 visually grounded instruction 数据集。\n实验结果显示，Groma 在标准 referring 和 grounding 基准上优于依赖语言模型或外部模块的 MLLMs，突出了将本地化嵌入图像 tokenization 的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13013v1",
      "published_date": "2024-04-19 17:22:51 UTC",
      "updated_date": "2024-04-19 17:22:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:52:19.871780"
    },
    {
      "arxiv_id": "2404.13004v3",
      "title": "A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Lei",
        "Zixuan Wang",
        "Yiqing Feng",
        "Junru Zhang",
        "Yahui Li",
        "Chu Liu",
        "Tongyao Wang"
      ],
      "abstract": "Recent industrial credit scoring models remain heavily reliant on manually\ntuned statistical learning methods. While deep learning offers promising\nsolutions, its effectiveness is often limited by the complexity of financial\ndata, particularly in long-horizon scenarios. In this work, we propose\nFinLangNet, which addresses credit scoring by reframing it as the task of\ngenerating multi-scale distributions of a user's future behavior. Within this\nframework, tabular data is transformed into sequential representations,\nenabling the generation of user embeddings across multiple temporal scales.\nInspired by the recent success of prompt-based training in Large Language\nModels (LLMs), FinLangNet also introduces two types of prompts to model and\ncapture user behavior at both the feature-granularity and user-granularity\nlevels. Experimental results demonstrate that FinLangNet outperforms the online\nXGBoost benchmark, achieving a 7.2\\% improvement in KS metric performance and a\n9.9\\% reduction in the relative bad debt rate. Furthermore, FinLangNet exhibits\nsuperior performance on public UEA archives, underscoring its scalability and\nadaptability in time series classification tasks.",
      "tldr_zh": "本研究提出FinLangNet，一种生成式方法，用于信用预测，通过可学习prompts实现多尺度时间表示学习，将信用评分问题转化为生成用户未来行为的多种规模分布。框架将表格数据转化为序列表示，并引入两种prompts（feature-granularity和user-granularity级别），受LLMs提示训练启发，捕捉用户行为的细粒度特征。实验结果显示，FinLangNet相较于在线XGBoost基准，提升KS metric 7.2%，并降低坏账率9.9%；此外，在公共UEA archives上表现出色，证明其在时间序列分类任务中的可扩展性和适应性。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13004v3",
      "published_date": "2024-04-19 17:01:46 UTC",
      "updated_date": "2025-02-22 10:23:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:52:29.428759"
    },
    {
      "arxiv_id": "2404.12999v1",
      "title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lisheng Wu",
        "Ke Chen"
      ],
      "abstract": "Exploration efficiency poses a significant challenge in goal-conditioned\nreinforcement learning (GCRL) tasks, particularly those with long horizons and\nsparse rewards. A primary limitation to exploration efficiency is the agent's\ninability to leverage environmental structural patterns. In this study, we\nintroduce a novel framework, GEASD, designed to capture these patterns through\nan adaptive skill distribution during the learning process. This distribution\noptimizes the local entropy of achieved goals within a contextual horizon,\nenhancing goal-spreading behaviors and facilitating deep exploration in states\ncontaining familiar structural patterns. Our experiments reveal marked\nimprovements in exploration efficiency using the adaptive skill distribution\ncompared to a uniform skill distribution. Additionally, the learned skill\ndistribution demonstrates robust generalization capabilities, achieving\nsubstantial exploration progress in unseen tasks containing similar local\nstructures.",
      "tldr_zh": "在目标条件强化学习（GCRL）中，探索效率面临重大挑战，特别是长视野和稀疏奖励环境下的结构性模式利用问题。本文引入 GEASD 框架，通过自适应技能分布动态捕获环境模式，该分布优化局部熵以促进目标扩散行为和深度探索。实验结果表明，与均匀技能分布相比，GEASD 显著提升了探索效率，且其学到的技能分布在包含类似局部结构的未见任务中表现出强健的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12999v1",
      "published_date": "2024-04-19 16:54:55 UTC",
      "updated_date": "2024-04-19 16:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:52:42.589575"
    },
    {
      "arxiv_id": "2404.12984v2",
      "title": "Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases",
      "title_zh": "混合现实中的眼动追踪用于神经退行性疾病诊断",
      "authors": [
        "Mateusz Daniol",
        "Daria Hemmerling",
        "Jakub Sikora",
        "Pawel Jemiolo",
        "Marek Wodzinski",
        "Magdalena Wojcik-Pedziwiatr"
      ],
      "abstract": "Parkinson's disease ranks as the second most prevalent neurodegenerative\ndisorder globally. This research aims to develop a system leveraging Mixed\nReality capabilities for tracking and assessing eye movements. In this paper,\nwe present a medical scenario and outline the development of an application\ndesigned to capture eye-tracking signals through Mixed Reality technology for\nthe evaluation of neurodegenerative diseases. Additionally, we introduce a\npipeline for extracting clinically relevant features from eye-gaze analysis,\ndescribing the capabilities of the proposed system from a medical perspective.\nThe study involved a cohort of healthy control individuals and patients\nsuffering from Parkinson's disease, showcasing the feasibility and potential of\nthe proposed technology for non-intrusive monitoring of eye movement patterns\nfor the diagnosis of neurodegenerative diseases.\n  Clinical relevance - Developing a non-invasive biomarker for Parkinson's\ndisease is urgently needed to accurately detect the disease's onset. This would\nallow for the timely introduction of neuroprotective treatment at the earliest\nstage and enable the continuous monitoring of intervention outcomes. The\nability to detect subtle changes in eye movements allows for early diagnosis,\noffering a critical window for intervention before more pronounced symptoms\nemerge. Eye tracking provides objective and quantifiable biomarkers, ensuring\nreliable assessments of disease progression and cognitive function. The eye\ngaze analysis using Mixed Reality glasses is wireless, facilitating convenient\nassessments in both home and hospital settings. The approach offers the\nadvantage of utilizing hardware that requires no additional specialized\nattachments, enabling examinations through personal eyewear.",
      "tldr_zh": "本研究开发了一种基于 Mixed Reality 的眼动跟踪系统，用于诊断神经退行性疾病，特别是帕金森病，以评估眼动模式。系统通过捕捉眼动信号并引入一个管道来提取临床相关特征，如眼凝视分析，从而实现非侵入性监测。实验涉及健康对照组和帕金森病患者，证明了该技术的可行性，并展示了其在早期诊断中的潜力。临床意义在于，该方法提供客观、可量化的生物标志物，便于在家或医院的无线评估，有助于及时干预和监测疾病进展。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12984v2",
      "published_date": "2024-04-19 16:34:15 UTC",
      "updated_date": "2024-06-03 10:45:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:52:53.452963"
    },
    {
      "arxiv_id": "2404.12975v1",
      "title": "FineRec:Exploring Fine-grained Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaokun Zhang",
        "Bo Xu",
        "Youlin Wu",
        "Yuan Zhong",
        "Hongfei Lin",
        "Fenglong Ma"
      ],
      "abstract": "Sequential recommendation is dedicated to offering items of interest for\nusers based on their history behaviors. The attribute-opinion pairs, expressed\nby users in their reviews for items, provide the potentials to capture user\npreferences and item characteristics at a fine-grained level. To this end, we\npropose a novel framework FineRec that explores the attribute-opinion pairs of\nreviews to finely handle sequential recommendation. Specifically, we utilize a\nlarge language model to extract attribute-opinion pairs from reviews. For each\nattribute, a unique attribute-specific user-opinion-item graph is created,\nwhere corresponding opinions serve as the edges linking heterogeneous user and\nitem nodes. To tackle the diversity of opinions, we devise a diversity-aware\nconvolution operation to aggregate information within the graphs, enabling\nattribute-specific user and item representation learning. Ultimately, we\npresent an interaction-driven fusion mechanism to integrate attribute-specific\nuser/item representations across all attributes for generating recommendations.\nExtensive experiments conducted on several realworld datasets demonstrate the\nsuperiority of our FineRec over existing state-of-the-art methods. Further\nanalysis also verifies the effectiveness of our fine-grained manner in handling\nthe task.",
      "tldr_zh": "本文提出 FineRec 框架，通过利用用户评论中的 attribute-opinion pairs 来提升 Sequential Recommendation 的细粒度处理，旨在捕捉用户偏好和物品特征。框架首先使用 large language model 提取属性-意见对，然后为每个属性构建独特的 attribute-specific user-opinion-item graph，并采用 diversity-aware convolution 操作聚合图中信息以学习属性特定的用户和物品表示。接着，通过 interaction-driven fusion mechanism 整合所有属性的表示来生成推荐。实验在真实数据集上证明 FineRec 优于现有最先进方法，并验证了这种细粒度方法的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "This work has been accepted by SIGIR24' as a full paper",
      "pdf_url": "http://arxiv.org/pdf/2404.12975v1",
      "published_date": "2024-04-19 16:04:26 UTC",
      "updated_date": "2024-04-19 16:04:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:53:07.459901"
    },
    {
      "arxiv_id": "2404.12969v1",
      "title": "Disentangling ID and Modality Effects for Session-based Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaokun Zhang",
        "Bo Xu",
        "Zhaochun Ren",
        "Xiaochen Wang",
        "Hongfei Lin",
        "Fenglong Ma"
      ],
      "abstract": "Session-based recommendation aims to predict intents of anonymous users based\non their limited behaviors. Modeling user behaviors involves two distinct\nrationales: co-occurrence patterns reflected by item IDs, and fine-grained\npreferences represented by item modalities (e.g., text and images). However,\nexisting methods typically entangle these causes, leading to their failure in\nachieving accurate and explainable recommendations. To this end, we propose a\nnovel framework DIMO to disentangle the effects of ID and modality in the task.\nAt the item level, we introduce a co-occurrence representation schema to\nexplicitly incorporate cooccurrence patterns into ID representations.\nSimultaneously, DIMO aligns different modalities into a unified semantic space\nto represent them uniformly. At the session level, we present a multi-view\nself-supervised disentanglement, including proxy mechanism and counterfactual\ninference, to disentangle ID and modality effects without supervised signals.\nLeveraging these disentangled causes, DIMO provides recommendations via causal\ninference and further creates two templates for generating explanations.\nExtensive experiments on multiple real-world datasets demonstrate the\nconsistent superiority of DIMO over existing methods. Further analysis also\nconfirms DIMO's effectiveness in generating explanations.",
      "tldr_zh": "这篇论文针对 Session-based Recommendation 的问题，提出 DIMO 框架，用于分离 ID（项目标识）和 Modality（项目模式，如文本和图像）的影响，以实现更准确和可解释的推荐。DIMO 在 item 级别引入 co-occurrence representation schema 来显式整合共现模式，并将不同 modalities 映射到统一的语义空间；在 session 级别，通过 multi-view self-supervised disentanglement（包括 proxy mechanism 和 counterfactual inference）实现无监督分离。最终，DIMO 利用 causal inference 进行推荐并生成解释模板，实验在多个真实数据集上证明其比现有方法显著优越，并在解释生成方面表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "This work has been accepted by SIGIR24' as a full paper",
      "pdf_url": "http://arxiv.org/pdf/2404.12969v1",
      "published_date": "2024-04-19 15:54:46 UTC",
      "updated_date": "2024-04-19 15:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:53:18.582900"
    },
    {
      "arxiv_id": "2404.12966v5",
      "title": "Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Yian Li",
        "Wentao Tian",
        "Yang Jiao",
        "Jingjing Chen",
        "Tianwen Qian",
        "Bin Zhu",
        "Na Zhao",
        "Yu-Gang Jiang"
      ],
      "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved significant\nsuccess across multiple disciplines due to their exceptional\ninstruction-following capabilities and extensive world knowledge. However,\nwhether these MLLMs possess human-like compositional reasoning abilities\nremains an open problem. To unveil their reasoning behaviors, we first curate a\n\\textbf{M}ultimodal \\textbf{A}ssumptive \\textbf{R}ea\\textbf{s}oning Benchmark\n(MARS-Bench) in this paper. Interestingly, we find that most prevalent MLLMs\ncan be easily fooled by the introduction of a presupposition into the question,\nwhereas such presuppositions appear naive to human reasoning. Besides, we also\npropose a simple yet effective method, Active Deduction (AD), a novel\nreinforcement learning paradigm to encourage the model to actively perform\ncomposite deduction before reaching a final decision. Equipped with the\nproposed AD method, a MLLM demonstrates significant improvements in assumptive\nreasoning abilities without compromising its general-purpose question-answering\nperformance. We also provide extensive evaluations of both open-source and\nprivate MLLMs on MARS-Bench, along with experimental analyses of the AD method.",
      "tldr_zh": "这篇论文探讨了 Multimodal Large Language Models (MLLMs) 的组合推理能力，发现现有模型容易被问题中的预设（presupposition）欺骗，而人类则能轻松识别。研究者构建了 Multimodal Assumptive Reasoning Benchmark (MARS-Bench) 作为评估基准，并提出 Active Deduction (AD) 方法，这是一种基于强化学习(reinforcement learning)范式的技术，鼓励模型在决策前主动进行复合推理。实验结果显示，应用 AD 后，MLLMs 的假设性推理能力显著提升，同时不影响其通用问题回答性能；此外，对开源和私有 MLLMs 进行了广泛评估。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12966v5",
      "published_date": "2024-04-19 15:53:27 UTC",
      "updated_date": "2025-04-17 08:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:53:31.363726"
    },
    {
      "arxiv_id": "2404.12938v2",
      "title": "MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews",
      "title_zh": "翻译失败",
      "authors": [
        "Oana Ignat",
        "Xiaomeng Xu",
        "Rada Mihalcea"
      ],
      "abstract": "Deceptive reviews are becoming increasingly common, especially given the\nincrease in performance and the prevalence of LLMs. While work to date has\naddressed the development of models to differentiate between truthful and\ndeceptive human reviews, much less is known about the distinction between real\nreviews and AI-authored fake reviews. Moreover, most of the research so far has\nfocused primarily on English, with very little work dedicated to other\nlanguages. In this paper, we compile and make publicly available the MAiDE-up\ndataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews,\nbalanced across ten languages. Using this dataset, we conduct extensive\nlinguistic analyses to (1) compare the AI fake hotel reviews to real hotel\nreviews, and (2) identify the factors that influence the deception detection\nmodel performance. We explore the effectiveness of several models for deception\ndetection in hotel reviews across three main dimensions: sentiment, location,\nand language. We find that these dimensions influence how well we can detect\nAI-generated fake reviews.",
      "tldr_zh": "本文提出了 MAiDE-up 数据集，包含 10,000 条真实酒店评论和 10,000 条由 LLMs 生成的假评论，覆盖十种语言，以解决多语言环境下检测 GPT-generated 假评论的挑战。研究团队通过语言分析比较了 AI 生成假评论与真实评论的差异，并识别了影响 deception detection 模型性能的因素，如情感、位置和语言维度。实验结果显示，这些维度显著影响了模型对假评论的检测准确性，为多语言欺骗检测提供了新数据集和见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12938v2",
      "published_date": "2024-04-19 15:08:06 UTC",
      "updated_date": "2024-06-19 03:34:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:53:44.461703"
    },
    {
      "arxiv_id": "2404.12933v2",
      "title": "Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data",
      "title_zh": "翻译失败",
      "authors": [
        "Oana Ignat",
        "Gayathri Ganesh Lakshmy",
        "Rada Mihalcea"
      ],
      "abstract": "Inspiration is linked to various positive outcomes, such as increased\ncreativity, productivity, and happiness. Although inspiration has great\npotential, there has been limited effort toward identifying content that is\ninspiring, as opposed to just engaging or positive. Additionally, most research\nhas concentrated on Western data, with little attention paid to other cultures.\nThis work is the first to study cross-cultural inspiration through machine\nlearning methods. We aim to identify and analyze real and AI-generated\ncross-cultural inspiring posts. To this end, we compile and make publicly\navailable the InspAIred dataset, which consists of 2,000 real inspiring posts,\n2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly\ndistributed across India and the UK. The real posts are sourced from Reddit,\nwhile the generated posts are created using the GPT-4 model. Using this\ndataset, we conduct extensive computational linguistic analyses to (1) compare\ninspiring content across cultures, (2) compare AI-generated inspiring posts to\nreal inspiring posts, and (3) determine if detection models can accurately\ndistinguish between inspiring content across cultures and data sources.",
      "tldr_zh": "该研究首次使用机器学习方法探讨跨文化灵感（Inspiration），针对现有研究的西方数据偏见和对鼓舞人心内容的识别不足问题。研究者编译了公开数据集InspAIred，包括从Reddit获取的2,000条真实鼓舞人心帖子、2,000条真实非鼓舞人心帖子，以及使用GPT-4生成的2,000条鼓舞人心帖子，这些数据均匀分布于印度和英国。通过计算语言学分析，他们比较了不同文化的鼓舞人心内容、AI生成内容与真实内容的差异，并评估了检测模型在区分跨文化和数据来源方面的准确性。结果为灵感识别和跨文化应用提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12933v2",
      "published_date": "2024-04-19 15:04:30 UTC",
      "updated_date": "2024-06-19 03:27:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:53:56.633193"
    },
    {
      "arxiv_id": "2404.12928v1",
      "title": "The Positivity of the Neural Tangent Kernel",
      "title_zh": "神经切线核的正定性",
      "authors": [
        "Luís Carvalho",
        "João L. Costa",
        "José Mourão",
        "Gonçalo Oliveira"
      ],
      "abstract": "The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in the\nstudy of wide Neural Networks. In particular, it is known that the positivity\nof the NTK is directly related to the memorization capacity of sufficiently\nwide networks, i.e., to the possibility of reaching zero loss in training, via\ngradient descent. Here we will improve on previous works and obtain a sharp\nresult concerning the positivity of the NTK of feedforward networks of any\ndepth. More precisely, we will show that, for any non-polynomial activation\nfunction, the NTK is strictly positive definite. Our results are based on a\nnovel characterization of polynomial functions which is of independent\ninterest.",
      "tldr_zh": "本研究探讨了 Neural Tangent Kernel (NTK) 的正性及其与宽神经网络记忆能力的关系，特别是通过梯度下降实现零训练损失的可能性。作者证明，对于任何非多项式激活函数，前馈网络的 NTK 是严格正定的，这是一个精确且尖锐的结果。研究基于一个新颖的多项式函数特征化方法，改进了先前工作，并为理解神经网络训练行为提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.PR",
        "math.SP",
        "68T07, 68R01"
      ],
      "primary_category": "cs.LG",
      "comment": "Comments welcome",
      "pdf_url": "http://arxiv.org/pdf/2404.12928v1",
      "published_date": "2024-04-19 14:55:21 UTC",
      "updated_date": "2024-04-19 14:55:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:54:09.462797"
    },
    {
      "arxiv_id": "2404.12926v2",
      "title": "MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering",
      "title_zh": "MM-PhyRLHF：多模态物理问答的强化学习框架",
      "authors": [
        "Janak Kapuriya",
        "Chhavi Kirtani",
        "Apoorv Singh",
        "Jay Saraf",
        "Naman Lal",
        "Jatin Kumar",
        "Adarsh Raj Shivam",
        "Astha Verma",
        "Avinash Anand",
        "Rajiv Ratn Shah"
      ],
      "abstract": "Recent advancements in LLMs have shown their significant potential in tasks\nlike text summarization and generation. Yet, they often encounter difficulty\nwhile solving complex physics problems that require arithmetic calculation and\na good understanding of concepts. Moreover, many physics problems include\nimages that contain important details required to understand the problem's\ncontext. We propose an LMM-based chatbot to answer multimodal physics MCQs. For\ndomain adaptation, we utilize the MM-PhyQA dataset comprising Indian high\nschool-level multimodal physics problems. To improve the LMM's performance, we\nexperiment with two techniques, RLHF (Reinforcement Learning from Human\nFeedback) and Image Captioning. In image captioning, we add a detailed\nexplanation of the diagram in each image, minimizing hallucinations and image\nprocessing errors. We further explore the integration of Reinforcement Learning\nfrom Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF\nto enhance the human-like problem-solving abilities of the models. The RLHF\napproach incorporates human feedback into the learning process of LLMs,\nimproving the model's problem-solving skills, truthfulness, and reasoning\ncapabilities, minimizing the hallucinations in the answers, and improving the\nquality instead of using vanilla-supervised fine-tuned models. We employ the\nLLaVA open-source model to answer multimodal physics MCQs and compare the\nperformance with and without using RLHF.",
      "tldr_zh": "该研究提出 MM-PhyRLHF 框架，一种基于强化学习从人类反馈 (RLHF) 的方法，用于提升大型多模态模型 (LMM) 在多模态物理问答中的性能，针对 LLMs 在处理涉及算术计算、概念理解和图像的复杂物理问题时存在的挑战。框架利用 MM-PhyQA 数据集（包含印度高中水平的物理多选题）进行领域适应，并结合图像描述技术来减少幻觉和图像处理错误，同时通过 RLHF 增强模型的推理能力、真诚性和问题解决质量。实验结果表明，使用 RLHF 的 LLaVA 模型在性能上显著优于无 RLHF 的基线模型，展示了更高的准确性和可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12926v2",
      "published_date": "2024-04-19 14:52:57 UTC",
      "updated_date": "2025-01-11 09:40:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:54:21.445820"
    },
    {
      "arxiv_id": "2404.12922v1",
      "title": "Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images",
      "title_zh": "翻译失败",
      "authors": [
        "Jacopo Bonato",
        "Marco Cotogni",
        "Luigi Sabetta"
      ],
      "abstract": "In this paper, we introduce Selective-distillation for Class and\nArchitecture-agnostic unleaRning (SCAR), a novel approximate unlearning method.\nSCAR efficiently eliminates specific information while preserving the model's\ntest accuracy without using a retain set, which is a key component in\nstate-of-the-art approximate unlearning algorithms. Our approach utilizes a\nmodified Mahalanobis distance to guide the unlearning of the feature vectors of\nthe instances to be forgotten, aligning them to the nearest wrong class\ndistribution. Moreover, we propose a distillation-trick mechanism that distills\nthe knowledge of the original model into the unlearning model with\nout-of-distribution images for retaining the original model's test performance\nwithout using any retain set. Importantly, we propose a self-forget version of\nSCAR that unlearns without having access to the forget set. We experimentally\nverified the effectiveness of our method, on three public datasets, comparing\nit with state-of-the-art methods. Our method obtains performance higher than\nmethods that operate without the retain set and comparable w.r.t the best\nmethods that rely on the retain set.",
      "tldr_zh": "本篇论文引入了 SCAR，一种新型的 approximate unlearning 方法，用于机器无学习（machine unlearning），它无需使用 retain set 即可高效消除特定信息，同时保持模型的测试准确性。SCAR 通过修改后的 Mahalanobis distance 引导要遗忘实例的特征向量与最近的错误类分布对齐，并采用 distillation-trick 机制，利用 out-of-distribution images 蒸馏原模型知识，以保留其性能。该方法还提出 self-forget 版本，无需访问 forget set；实验在三个公共数据集上验证了其有效性，性能高于不使用 retain set 的方法，并与依赖 retain set 的最佳方法相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12922v1",
      "published_date": "2024-04-19 14:45:27 UTC",
      "updated_date": "2024-04-19 14:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:54:33.957743"
    },
    {
      "arxiv_id": "2404.12917v3",
      "title": "R3L: Relative Representations for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Pio Ricciardi",
        "Valentino Maiorca",
        "Luca Moschella",
        "Riccardo Marin",
        "Emanuele Rodolà"
      ],
      "abstract": "Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.",
      "tldr_zh": "本文提出R3L框架，利用Relative Representations在Visual Reinforcement Learning中映射编码器嵌入到通用空间，允许组合代理组件以零样本方式处理训练中未遇到的视觉-任务对。该方法解决了输入域（如季节变化）和任务域（如目标速度变化）变异导致的性能下降问题，避免了重新训练的需求。实验结果表明，R3L显著提高了模型重用潜力，节省了时间和计算资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.12917v3",
      "published_date": "2024-04-19 14:42:42 UTC",
      "updated_date": "2025-02-18 15:17:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:54:44.084175"
    },
    {
      "arxiv_id": "2404.12901v2",
      "title": "Large Language Models for Networking: Workflow, Advances and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Chang Liu",
        "Xiaohui Xie",
        "Xinggong Zhang",
        "Yong Cui"
      ],
      "abstract": "The networking field is characterized by its high complexity and rapid\niteration, requiring extensive expertise to accomplish network tasks, ranging\nfrom network design, configuration, diagnosis and security. The inherent\ncomplexity of these tasks, coupled with the ever-changing landscape of\nnetworking technologies and protocols, poses significant hurdles for\ntraditional machine learning-based methods. These methods often struggle to\ngeneralize and automate complex tasks in networking, as they require extensive\nlabeled data, domain-specific feature engineering, and frequent retraining to\nadapt to new scenarios. However, the recent emergence of large language models\n(LLMs) has sparked a new wave of possibilities in addressing these challenges.\nLLMs have demonstrated remarkable capabilities in natural language\nunderstanding, generation, and reasoning. These models, trained on extensive\ndata, can benefit the networking domain. Some efforts have already explored the\napplication of LLMs in the networking domain and revealed promising results. By\nreviewing recent advances, we present an abstract workflow to describe the\nfundamental process involved in applying LLM for Networking. We introduce the\nhighlights of existing works by category and explain in detail how they operate\nat different stages of the workflow. Furthermore, we delve into the challenges\nencountered, discuss potential solutions, and outline future research\nprospects. We hope that this survey will provide insight for researchers and\npractitioners, promoting the development of this interdisciplinary research\nfield.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在网络领域（如设计、配置、诊断和安全）的应用，以克服传统机器学习方法的局限性，如数据依赖性和适应性差。论文提出一个抽象工作流程来描述LLMs在网络任务中的核心过程，并分类回顾现有研究，详细说明它们在工作流程不同阶段的操作。最终，论文分析了面临的挑战（如模型泛化问题）、潜在解决方案，并展望未来研究方向，以推动这一跨学科领域的进展。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12901v2",
      "published_date": "2024-04-19 14:17:02 UTC",
      "updated_date": "2024-04-29 04:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:54:56.891865"
    },
    {
      "arxiv_id": "2404.12900v2",
      "title": "Training-and-Prompt-Free General Painterly Harmonization via Zero-Shot Disentenglement on Style and Content References",
      "title_zh": "翻译失败",
      "authors": [
        "Teng-Fang Hsiao",
        "Bo-Kai Ruan",
        "Hong-Han Shuai"
      ],
      "abstract": "Painterly image harmonization aims at seamlessly blending disparate visual\nelements within a single image. However, previous approaches often struggle due\nto limitations in training data or reliance on additional prompts, leading to\ninharmonious and content-disrupted output. To surmount these hurdles, we design\na Training-and-prompt-Free General Painterly Harmonization method (TF-GPH).\nTF-GPH incorporates a novel ``Similarity Disentangle Mask'', which disentangles\nthe foreground content and background image by redirecting their attention to\ncorresponding reference images, enhancing the attention mechanism for\nmulti-image inputs. Additionally, we propose a ``Similarity Reweighting''\nmechanism to balance harmonization between stylization and content\npreservation. This mechanism minimizes content disruption by prioritizing the\ncontent-similar features within the given background style reference. Finally,\nwe address the deficiencies in existing benchmarks by proposing novel\nrange-based evaluation metrics and a new benchmark to better reflect real-world\napplications. Extensive experiments demonstrate the efficacy of our method in\nall benchmarks. More detailed in https://github.com/BlueDyee/TF-GPH.",
      "tldr_zh": "该论文针对画家式图像协调（Painterly image harmonization）的问题，提出了一种无需训练和提示的通用方法TF-GPH，通过Zero-Shot Disentanglement技术实现风格和内容参考的分离。核心创新包括“Similarity Disentangle Mask”，用于区分前景内容和背景图像，并通过重定向注意力机制处理多图像输入，以及“Similarity Reweighting”机制来平衡风格化与内容保留，优先匹配背景风格参考以减少内容破坏。实验结果显示，TF-GPH在现有基准上表现出色，并通过引入新的基于范围的评估指标和基准，更好地模拟实际应用场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12900v2",
      "published_date": "2024-04-19 14:13:46 UTC",
      "updated_date": "2024-12-15 14:53:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:55:08.841065"
    },
    {
      "arxiv_id": "2404.17591v2",
      "title": "Large Language Models for Next Point-of-Interest Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Peibo Li",
        "Maarten de Rijke",
        "Hao Xue",
        "Shuang Ao",
        "Yang Song",
        "Flora D. Salim"
      ],
      "abstract": "The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.",
      "tldr_zh": "这篇论文提出了一种利用大型语言模型 (LLMs) 的框架，用于下一个兴趣点 (POI) 推荐任务，旨在有效处理基于位置社交网络 (LBSN) 数据中的丰富上下文信息。框架通过保留数据的原始格式和整合常识知识，克服了传统数值模型的局限性，从而提升了对上下文的理解和利用。在三个真实世界 LBSN 数据集的实验中，该框架优于现有最先进模型，并显著缓解了冷启动和短轨迹问题。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.17591v2",
      "published_date": "2024-04-19 13:28:36 UTC",
      "updated_date": "2024-08-01 08:54:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:55:20.969104"
    },
    {
      "arxiv_id": "2404.12876v1",
      "title": "A Large-scale Medical Visual Task Adaptation Benchmark",
      "title_zh": "大规模医学视觉任务适应基准",
      "authors": [
        "Shentong Mo",
        "Xufang Luo",
        "Yansen Wang",
        "Dongsheng Li"
      ],
      "abstract": "Visual task adaptation has been demonstrated to be effective in adapting\npre-trained Vision Transformers (ViTs) to general downstream visual tasks using\nspecialized learnable layers or tokens. However, there is yet a large-scale\nbenchmark to fully explore the effect of visual task adaptation on the\nrealistic and important medical domain, particularly across diverse medical\nvisual modalities, such as color images, X-ray, and CT. To close this gap, we\npresent Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark\nconsisting of 1.68 million medical images for diverse organs, modalities, and\nadaptation approaches. Based on Med-VTAB, we explore the scaling law of medical\nprompt tuning concerning tunable parameters and the generalizability of medical\nvisual adaptation using non-medical/medical pre-train weights. Besides, we\nstudy the impact of patient ID out-of-distribution on medical visual\nadaptation, which is a real and challenging scenario. Furthermore, results from\nMed-VTAB indicate that a single pre-trained model falls short in medical task\nadaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines\nmedical and general pre-training weights through a gated mixture-of-experts\nadapter, achieving state-of-the-art results in medical visual task adaptation.",
      "tldr_zh": "本研究引入了 Med-VTAB，这是一个大规模医疗视觉任务适应基准，包含 1.68 百万张图像，覆盖多种器官和模态（如彩色图像、X-ray 和 CT），旨在评估预训练 Vision Transformers (ViTs) 在医疗领域的适应效果。研究者探索了医疗提示调优的规模定律、视觉适应的泛化能力（使用非医疗/医疗预训练权重）、以及患者 ID out-of-distribution 场景的影响，结果显示单一预训练模型在医疗任务适应中表现不足。为此，他们提出了 GMoE-Adapter，一种通过门控混合专家适配器结合医疗和通用预训练权重的创新方法，实现了 state-of-the-art 的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12876v1",
      "published_date": "2024-04-19 13:25:27 UTC",
      "updated_date": "2024-04-19 13:25:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:55:33.296031"
    },
    {
      "arxiv_id": "2404.12866v2",
      "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
      "title_zh": "文本信息如何影响多模态上下文学习的检索？",
      "authors": [
        "Yang Luo",
        "Zangwei Zheng",
        "Zirui Zhu",
        "Yang You"
      ],
      "abstract": "The increase in parameter size of multimodal large language models (MLLMs)\nintroduces significant capabilities, particularly in-context learning, where\nMLLMs enhance task performance without updating pre-trained parameters. This\neffectiveness, however, hinges on the appropriate selection of in-context\nexamples, a process that is currently biased towards visual data, overlooking\ntextual information. Furthermore, the area of supervised retrievers for MLLMs,\ncrucial for optimal in-context example selection, continues to be\nuninvestigated. Our study offers an in-depth evaluation of the impact of\ntextual information on the unsupervised selection of in-context examples in\nmultimodal contexts, uncovering a notable sensitivity of retriever performance\nto the employed modalities. Responding to this, we introduce a novel supervised\nMLLM-retriever MSIER that employs a neural network to select examples that\nenhance multimodal in-context learning efficiency. This approach is validated\nthrough extensive testing across three distinct tasks, demonstrating the\nmethod's effectiveness. Additionally, we investigate the influence of\nmodalities on our supervised retrieval method's training and pinpoint factors\ncontributing to our model's success. This exploration paves the way for future\nadvancements, highlighting the potential for refined in-context learning in\nMLLMs through the strategic use of multimodal data.",
      "tldr_zh": "本研究探讨了文本信息对多模态大语言模型(MLLMs)中多模态上下文学习(in-context learning)检索的影响，发现当前示例选择偏向视觉数据而忽略文本，导致检索器对模态高度敏感。作者引入了一种新型监督式MLLM-检索器MSIER，利用神经网络选择最佳in-context示例，以提升多模态学习的效率和性能。该方法通过在三个不同任务上的广泛实验得到验证，并分析了模态对监督检索训练的影响，为未来多模态数据战略优化提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12866v2",
      "published_date": "2024-04-19 13:05:37 UTC",
      "updated_date": "2024-11-12 08:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:55:49.177484"
    },
    {
      "arxiv_id": "2404.12856v2",
      "title": "Language-Driven Active Learning for Diverse Open-Set 3D Object Detection",
      "title_zh": "语言驱动的主动学习用于多样化开放集 3D 对象",
      "authors": [
        "Ross Greer",
        "Bjørk Antoniussen",
        "Andreas Møgelmose",
        "Mohan Trivedi"
      ],
      "abstract": "Object detection is crucial for ensuring safe autonomous driving. However,\ndata-driven approaches face challenges when encountering minority or novel\nobjects in the 3D driving scene. In this paper, we propose VisLED, a\nlanguage-driven active learning framework for diverse open-set 3D Object\nDetection. Our method leverages active learning techniques to query diverse and\ninformative data samples from an unlabeled pool, enhancing the model's ability\nto detect underrepresented or novel objects. Specifically, we introduce the\nVision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, which\noperates in both open-world exploring and closed-world mining settings. In\nopen-world exploring, VisLED-Querying selects data points most novel relative\nto existing data, while in closed-world mining, it mines novel instances of\nknown classes. We evaluate our approach on the nuScenes dataset and demonstrate\nits efficiency compared to random sampling and entropy-querying methods. Our\nresults show that VisLED-Querying consistently outperforms random sampling and\noffers competitive performance compared to entropy-querying despite the\nlatter's model-optimality, highlighting the potential of VisLED for improving\nobject detection in autonomous driving scenarios. We make our code publicly\navailable at https://github.com/Bjork-crypto/VisLED-Querying",
      "tldr_zh": "这篇论文提出了一种语言驱动的主动学习框架 VisLED，用于多样化的开放集 3D 物体检测，以解决数据驱动方法在自动驾驶场景中检测少数派或新颖物体时的挑战。VisLED 通过 Vision-Language Embedding Diversity Querying (VisLED-Querying) 算法，从未标注数据池中选择多样性和信息丰富的样本，在 open-world exploring 和 closed-world mining 设置中分别探索新颖数据和挖掘已知类别的novel实例。实验结果显示，在 nuScenes 数据集上，VisLED-Querying 比随机采样方法性能更优，并与熵查询方法相当，从而提升了自动驾驶物体检测的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12856v2",
      "published_date": "2024-04-19 12:50:43 UTC",
      "updated_date": "2024-06-18 07:34:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:00.107935"
    },
    {
      "arxiv_id": "2404.13104v1",
      "title": "Multi Class Depression Detection Through Tweets using Artificial Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Osama Nusrat",
        "Waseem Shahzad",
        "Saad Ahmed Jamal"
      ],
      "abstract": "Depression is a significant issue nowadays. As per the World Health\nOrganization (WHO), in 2023, over 280 million individuals are grappling with\ndepression. This is a huge number; if not taken seriously, these numbers will\nincrease rapidly. About 4.89 billion individuals are social media users. People\nexpress their feelings and emotions on platforms like Twitter, Facebook,\nReddit, Instagram, etc. These platforms contain valuable information which can\nbe used for research purposes. Considerable research has been conducted across\nvarious social media platforms. However, certain limitations persist in these\nendeavors. Particularly, previous studies were only focused on detecting\ndepression and the intensity of depression in tweets. Also, there existed\ninaccuracies in dataset labeling. In this research work, five types of\ndepression (Bipolar, major, psychotic, atypical, and postpartum) were predicted\nusing tweets from the Twitter database based on lexicon labeling. Explainable\nAI was used to provide reasoning by highlighting the parts of tweets that\nrepresent type of depression. Bidirectional Encoder Representations from\nTransformers (BERT) was used for feature extraction and training. Machine\nlearning and deep learning methodologies were used to train the model. The BERT\nmodel presented the most promising results, achieving an overall accuracy of\n0.96.",
      "tldr_zh": "本研究针对抑郁症检测的局限性（如仅关注抑郁存在和强度，以及数据集标注不准确），使用人工智能从 Twitter 推文中预测五种抑郁类型，包括 Bipolar、major、psychotic、atypical 和 postpartum。方法采用 lexicon labeling 进行数据标注，结合 Explainable AI 来突出推文中代表抑郁类型的关键部分，并利用 BERT 模型进行特征提取和训练。实验结果显示，BERT 模型取得了 0.96 的整体准确率，为多类抑郁检测提供了更精确和可解释的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.13104v1",
      "published_date": "2024-04-19 12:47:56 UTC",
      "updated_date": "2024-04-19 12:47:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:10.698804"
    },
    {
      "arxiv_id": "2404.12839v1",
      "title": "ECOR: Explainable CLIP for Object Recognition",
      "title_zh": "ECOR：可解释的 CL",
      "authors": [
        "Ali Rasekh",
        "Sepehr Kazemi Ranjbar",
        "Milad Heidari",
        "Wolfgang Nejdl"
      ],
      "abstract": "Large Vision Language Models (VLMs), such as CLIP, have significantly\ncontributed to various computer vision tasks, including object recognition and\nobject detection. Their open vocabulary feature enhances their value. However,\ntheir black-box nature and lack of explainability in predictions make them less\ntrustworthy in critical domains. Recently, some work has been done to force\nVLMs to provide reasonable rationales for object recognition, but this often\ncomes at the expense of classification accuracy. In this paper, we first\npropose a mathematical definition of explainability in the object recognition\ntask based on the joint probability distribution of categories and rationales,\nthen leverage this definition to fine-tune CLIP in an explainable manner.\nThrough evaluations of different datasets, our method demonstrates\nstate-of-the-art performance in explainable classification. Notably, it excels\nin zero-shot settings, showcasing its adaptability. This advancement improves\nexplainable object recognition, enhancing trust across diverse applications.\nThe code will be made available online upon publication.",
      "tldr_zh": "本研究针对 Large Vision Language Models (VLMs) 如 CLIP 在物体识别中的黑箱性质和缺乏可解释性问题，提出了一种名为 ECOR 的方法，通过定义物体识别任务中可解释性的数学定义（基于类别和解释的联合概率分布），来微调 CLIP 模型，使其在提供合理解释的同时保持高分类准确性。在多个数据集上的评估中，ECOR 实现了最先进的可解释分类性能，尤其在 zero-shot 设置中表现出色，提升了模型在各种应用中的可信任度。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12839v1",
      "published_date": "2024-04-19 12:20:49 UTC",
      "updated_date": "2024-04-19 12:20:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:23.050297"
    },
    {
      "arxiv_id": "2404.12832v2",
      "title": "COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images",
      "title_zh": "翻译失败",
      "authors": [
        "Dmytro Shvetsov",
        "Joonas Ariva",
        "Marharyta Domnich",
        "Raul Vicente",
        "Dmytro Fishman"
      ],
      "abstract": "Deep learning is dramatically transforming the field of medical imaging and\nradiology, enabling the identification of pathologies in medical images,\nincluding computed tomography (CT) and X-ray scans. However, the performance of\ndeep learning models, particularly in segmentation tasks, is often limited by\nthe need for extensive annotated datasets. To address this challenge, the\ncapabilities of weakly supervised semantic segmentation are explored through\nthe lens of Explainable AI and the generation of counterfactual explanations.\nThe scope of this research is development of a novel counterfactual inpainting\napproach (COIN) that flips the predicted classification label from abnormal to\nnormal by using a generative model. For instance, if the classifier deems an\ninput medical image X as abnormal, indicating the presence of a pathology, the\ngenerative model aims to inpaint the abnormal region, thus reversing the\nclassifier's original prediction label. The approach enables us to produce\nprecise segmentations for pathologies without depending on pre-existing\nsegmentation masks. Crucially, image-level labels are utilized, which are\nsubstantially easier to acquire than creating detailed segmentation masks. The\neffectiveness of the method is demonstrated by segmenting synthetic targets and\nactual kidney tumors from CT images acquired from Tartu University Hospital in\nEstonia. The findings indicate that COIN greatly surpasses established\nattribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an\nalternative counterfactual explanation method introduced by Singla et al. This\nevidence suggests that COIN is a promising approach for semantic segmentation\nof tumors in CT images, and presents a step forward in making deep learning\napplications more accessible and effective in healthcare, where annotated data\nis scarce.",
      "tldr_zh": "本文提出 COIN 方法，即逆事实填充 (counterfactual inpainting)，用于弱监督语义分割 (weakly supervised semantic segmentation)，以解决医疗图像如 CT 和 X 光扫描中标注数据不足的挑战。COIN 通过生成模型逆转分类器的预测标签，例如将异常图像填充为正常，从而在仅使用图像级标签的情况下生成精确的病变分割。实验结果显示，该方法在分割合成目标和实际肾肿瘤时，显著优于 RISE、ScoreCAM 和 LayerCAM 等基准方法，为 Explainable AI 在医疗领域的应用提供了可访问且有效的进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta",
      "pdf_url": "http://arxiv.org/pdf/2404.12832v2",
      "published_date": "2024-04-19 12:09:49 UTC",
      "updated_date": "2024-07-25 08:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:35.284535"
    },
    {
      "arxiv_id": "2404.12814v3",
      "title": "Generative Modelling with High-Order Langevin Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Ziqiang Shi",
        "Rujie Liu"
      ],
      "abstract": "Diffusion generative modelling (DGM) based on stochastic differential\nequations (SDEs) with score matching has achieved unprecedented results in data\ngeneration. In this paper, we propose a novel fast high-quality generative\nmodelling method based on high-order Langevin dynamics (HOLD) with score\nmatching. This motive is proved by third-order Langevin dynamics. By augmenting\nthe previous SDEs, e.g. variance exploding or variance preserving SDEs for\nsingle-data variable processes, HOLD can simultaneously model position,\nvelocity, and acceleration, thereby improving the quality and speed of the data\ngeneration at the same time. HOLD is composed of one Ornstein-Uhlenbeck process\nand two Hamiltonians, which reduce the mixing time by two orders of magnitude.\nEmpirical experiments for unconditional image generation on the public data set\nCIFAR-10 and CelebA-HQ show that the effect is significant in both Frechet\ninception distance (FID) and negative log-likelihood, and achieves the\nstate-of-the-art FID of 1.85 on CIFAR-10.",
      "tldr_zh": "本论文提出了一种基于高阶朗格万动力学 (HOLD) 与分数匹配的新型生成建模方法，旨在提升扩散生成模型 (DGM) 的数据生成质量和速度。HOLD 通过增强随机微分方程 (SDEs)，如方差爆炸或方差保持 SDEs，同时建模位置、速度和加速度，由一个 Ornstein-Uhlenbeck 过程和两个 Hamiltonians 组成，从而将混合时间减少两个数量级。在 CIFAR-10 和 CelebA-HQ 数据集上的无条件图像生成实验中，HOLD 显著改善了 Frechet Inception Distance (FID) 和负对数似然表现，并在 CIFAR-10 上达到了 1.85 的最先进 FID 成绩。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Some of the results in this paper have been published at conferences,\n  such as WACV2024, ICASSP2024, and ICME2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12814v3",
      "published_date": "2024-04-19 11:49:01 UTC",
      "updated_date": "2025-01-02 13:06:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:48.620100"
    },
    {
      "arxiv_id": "2404.12810v2",
      "title": "Enhancing Counterfactual Explanation Search with Diffusion Distance and Directional Coherence",
      "title_zh": "翻译失败",
      "authors": [
        "Marharyta Domnich",
        "Raul Vicente"
      ],
      "abstract": "A pressing issue in the adoption of AI models is the increasing demand for\nmore human-centric explanations of their predictions. To advance towards more\nhuman-centric explanations, understanding how humans produce and select\nexplanations has been beneficial. In this work, inspired by insights of human\ncognition we propose and test the incorporation of two novel biases to enhance\nthe search for effective counterfactual explanations. Central to our\nmethodology is the application of diffusion distance, which emphasizes data\nconnectivity and actionability in the search for feasible counterfactual\nexplanations. In particular, diffusion distance effectively weights more those\npoints that are more interconnected by numerous short-length paths. This\napproach brings closely connected points nearer to each other, identifying a\nfeasible path between them. We also introduce a directional coherence term that\nallows the expression of a preference for the alignment between the joint and\nmarginal directional changes in feature space to reach a counterfactual. This\nterm enables the generation of counterfactual explanations that align with a\nset of marginal predictions based on expectations of how the outcome of the\nmodel varies by changing one feature at a time. We evaluate our method, named\nCoherent Directional Counterfactual Explainer (CoDiCE), and the impact of the\ntwo novel biases against existing methods such as DiCE, FACE, Prototypes, and\nGrowing Spheres. Through a series of ablation experiments on both synthetic and\nreal datasets with continuous and mixed-type features, we demonstrate the\neffectiveness of our method.",
      "tldr_zh": "该研究针对AI模型预测的人性化解释需求，提出两种新偏差来提升反事实解释（counterfactual explanations）的搜索效率：diffusion distance 和 directional coherence。前者通过强调数据点间的连通性和可操作性，帮助识别更可行的解释路径；后者确保特征空间中联合和边际方向变化的一致性，从而生成与模型预期对齐的反事实解释。研究引入了CoDiCE方法，并通过消融实验在合成和真实数据集上证明其优于现有方法（如DiCE、FACE），展示了显著的解释生成效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta",
      "pdf_url": "http://arxiv.org/pdf/2404.12810v2",
      "published_date": "2024-04-19 11:47:17 UTC",
      "updated_date": "2024-07-25 08:00:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:56:58.628928"
    },
    {
      "arxiv_id": "2404.12802v1",
      "title": "Enhancing Interval Type-2 Fuzzy Logic Systems: Learning for Precision and Prediction Intervals",
      "title_zh": "增强区间类型-2 模糊逻辑系统：用于精确度和预测区间的学习",
      "authors": [
        "Ata Koklu",
        "Yusuf Guven",
        "Tufan Kumbasar"
      ],
      "abstract": "In this paper, we tackle the task of generating Prediction Intervals (PIs) in\nhigh-risk scenarios by proposing enhancements for learning Interval Type-2\n(IT2) Fuzzy Logic Systems (FLSs) to address their learning challenges. In this\ncontext, we first provide extra design flexibility to the Karnik-Mendel (KM)\nand Nie-Tan (NT) center of sets calculation methods to increase their\nflexibility for generating PIs. These enhancements increase the flexibility of\nKM in the defuzzification stage while the NT in the fuzzification stage. To\naddress the large-scale learning challenge, we transform the IT2-FLS's\nconstraint learning problem into an unconstrained form via parameterization\ntricks, enabling the direct application of deep learning optimizers. To address\nthe curse of dimensionality issue, we expand the High-Dimensional\nTakagi-Sugeno-Kang (HTSK) method proposed for type-1 FLS to IT2-FLSs, resulting\nin the HTSK2 approach. Additionally, we introduce a framework to learn the\nenhanced IT2-FLS with a dual focus, aiming for high precision and PI\ngeneration. Through exhaustive statistical results, we reveal that HTSK2\neffectively addresses the dimensionality challenge, while the enhanced KM and\nNT methods improved learning and enhanced uncertainty quantification\nperformances of IT2-FLSs.",
      "tldr_zh": "本论文针对高风险场景生成 Prediction Intervals (PIs)，提出增强 Interval Type-2 Fuzzy Logic Systems (IT2-FLSs) 的学习方法，以解决其学习挑战。研究首先改进 Karnik-Mendel (KM) 和 Nie-Tan (NT) 方法，增加 KM 在 defuzzification 阶段和 NT 在 fuzzification 阶段的灵活性。接着，通过参数化技巧将 IT2-FLSs 的约束学习问题转化为无约束形式，便于应用深度学习优化器，并扩展 High-Dimensional Takagi-Sugeno-Kang (HTSK) 方法到 IT2-FLSs，形成 HTSK2 框架以应对维度灾难问题。实验结果显示，HTSK2 有效处理高维度挑战，而增强的 KM 和 NT 方法显著提高了 IT2-FLSs 的学习性能和不确定性量化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "in the IEEE World Congress on Computational Intelligence, 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12802v1",
      "published_date": "2024-04-19 11:37:51 UTC",
      "updated_date": "2024-04-19 11:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:57:12.794942"
    },
    {
      "arxiv_id": "2404.12800v1",
      "title": "Zadeh's Type-2 Fuzzy Logic Systems: Precision and High-Quality Prediction Intervals",
      "title_zh": "翻译失败",
      "authors": [
        "Yusuf Guven",
        "Ata Koklu",
        "Tufan Kumbasar"
      ],
      "abstract": "General Type-2 (GT2) Fuzzy Logic Systems (FLSs) are perfect candidates to\nquantify uncertainty, which is crucial for informed decisions in high-risk\ntasks, as they are powerful tools in representing uncertainty. In this paper,\nwe travel back in time to provide a new look at GT2-FLSs by adopting Zadeh's\n(Z) GT2 Fuzzy Set (FS) definition, intending to learn GT2-FLSs that are capable\nof achieving reliable High-Quality Prediction Intervals (HQ-PI) alongside\nprecision. By integrating Z-GT2-FS with the \\(\\alpha\\)-plane representation, we\nshow that the design flexibility of GT2-FLS is increased as it takes away the\ndependency of the secondary membership function from the primary membership\nfunction. After detailing the construction of Z-GT2-FLSs, we provide solutions\nto challenges while learning from high-dimensional data: the curse of\ndimensionality, and integrating Deep Learning (DL) optimizers. We develop a DL\nframework for learning dual-focused Z-GT2-FLSs with high performances. Our\nstudy includes statistical analyses, highlighting that the Z-GT2-FLS not only\nexhibits high-precision performance but also produces HQ-PIs in comparison to\nits GT2 and IT2 fuzzy counterparts which have more learnable parameters. The\nresults show that the Z-GT2-FLS has a huge potential in uncertainty\nquantification.",
      "tldr_zh": "本研究探讨了 Zadeh's General Type-2 (GT2) Fuzzy Logic Systems (FLSs)，旨在通过量化不确定性来提升高风险任务的决策可靠性。论文采用 Z-GT2 Fuzzy Set 定义并整合 α-plane 表示，增强了 FLS 的设计灵活性，解决了高维数据中的维度灾难问题，并开发了一个深度学习 (DL) 框架来优化双重聚焦的 Z-GT2-FLSs。结果显示，Z-GT2-FLS 不仅在精确性上表现出色，还能生成高质量预测区间 (HQ-PI)，并优于其他 GT2 和 Interval Type-2 (IT2) 模糊系统，即使其参数更少，从而在不确定性量化方面展现巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "in the IEEE World Congress on Computational Intelligence, 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12800v1",
      "published_date": "2024-04-19 11:29:10 UTC",
      "updated_date": "2024-04-19 11:29:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:57:23.028605"
    },
    {
      "arxiv_id": "2404.12792v1",
      "title": "Efficient Learning of Fuzzy Logic Systems for Large-Scale Data Using Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ata Koklu",
        "Yusuf Guven",
        "Tufan Kumbasar"
      ],
      "abstract": "Type-1 and Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) excel in handling\nuncertainty alongside their parsimonious rule-based structure. Yet, in learning\nlarge-scale data challenges arise, such as the curse of dimensionality and\ntraining complexity of FLSs. The complexity is due mainly to the constraints to\nbe satisfied as the learnable parameters define FSs and the complexity of the\ncenter of the sets calculation method, especially of IT2-FLSs. This paper\nexplicitly focuses on the learning problem of FLSs and presents a\ncomputationally efficient learning method embedded within the realm of Deep\nLearning (DL). The proposed method tackles the learning challenges of FLSs by\npresenting computationally efficient implementations of FLSs, thereby\nminimizing training time while leveraging mini-batched DL optimizers and\nautomatic differentiation provided within the DL frameworks. We illustrate the\nefficiency of the DL framework for FLSs on benchmark datasets.",
      "tldr_zh": "该论文探讨了 Type-1 和 Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) 在处理大规模数据时的挑战，如维度灾难和训练复杂性，主要源于参数约束和中心集计算的复杂性。作者提出了一种基于 Deep Learning (DL) 的高效学习方法，通过优化 FLS 的计算实现、利用 mini-batched DL optimizers 和 automatic differentiation 来最小化训练时间。该方法在基准数据集上验证了其效率，显著提升了 FLS 的学习性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "in the International Conference on Intelligent and Fuzzy Systems,\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12792v1",
      "published_date": "2024-04-19 11:09:55 UTC",
      "updated_date": "2024-04-19 11:09:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:57:35.043864"
    },
    {
      "arxiv_id": "2404.12782v1",
      "title": "Sentiment-oriented Transformer-based Variational Autoencoder Network for Live Video Commenting",
      "title_zh": "翻译失败",
      "authors": [
        "Fengyi Fu",
        "Shancheng Fang",
        "Weidong Chen",
        "Zhendong Mao"
      ],
      "abstract": "Automatic live video commenting is with increasing attention due to its\nsignificance in narration generation, topic explanation, etc. However, the\ndiverse sentiment consideration of the generated comments is missing from the\ncurrent methods. Sentimental factors are critical in interactive commenting,\nand lack of research so far. Thus, in this paper, we propose a\nSentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network\nwhich consists of a sentiment-oriented diversity encoder module and a batch\nattention module, to achieve diverse video commenting with multiple sentiments\nand multiple semantics. Specifically, our sentiment-oriented diversity encoder\nelegantly combines VAE and random mask mechanism to achieve semantic diversity\nunder sentiment guidance, which is then fused with cross-modal features to\ngenerate live video comments. Furthermore, a batch attention module is also\nproposed in this paper to alleviate the problem of missing sentimental samples,\ncaused by the data imbalance, which is common in live videos as the popularity\nof videos varies. Extensive experiments on Livebot and VideoIC datasets\ndemonstrate that the proposed So-TVAE outperforms the state-of-the-art methods\nin terms of the quality and diversity of generated comments. Related code is\navailable at https://github.com/fufy1024/So-TVAE.",
      "tldr_zh": "本研究提出了一种基于 Transformer 的变分自编码器网络（Sentiment-oriented Transformer-based Variational Autoencoder，So-TVAE），旨在生成具有多种情感和语义的直播视频评论，以解决现有方法忽略情感多样性的问题。So-TVAE 包括情感导向多样性编码器模块，该模块结合 VAE 和随机掩码机制，在情感指导下实现语义多样性，并通过批量注意力模块缓解数据不平衡导致的情感样本缺失问题。实验在 Livebot 和 VideoIC 数据集上表明，So-TVAE 在生成评论的质量和多样性方面优于现有最先进方法，相关代码已在 GitHub 上开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "27 pages, 10 figures, ACM Transactions on Multimedia Computing,\n  Communications and Applications, 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12782v1",
      "published_date": "2024-04-19 10:43:25 UTC",
      "updated_date": "2024-04-19 10:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:57:49.160018"
    },
    {
      "arxiv_id": "2404.12768v1",
      "title": "MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian Models",
      "title_zh": "MixLight：借鉴球谐函数和高斯模型的最佳部分",
      "authors": [
        "Xinlong Ji",
        "Fangneng Zhan",
        "Shijian Lu",
        "Shi-Sheng Huang",
        "Hua Huang"
      ],
      "abstract": "Accurately estimating scene lighting is critical for applications such as\nmixed reality. Existing works estimate illumination by generating illumination\nmaps or regressing illumination parameters. However, the method of generating\nillumination maps has poor generalization performance and parametric models\nsuch as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in\ncapturing high-frequency or low-frequency components. This paper presents\nMixLight, a joint model that utilizes the complementary characteristics of SH\nand SG to achieve a more complete illumination representation, which uses SH\nand SG to capture low-frequency ambient and high-frequency light sources\nrespectively. In addition, a special spherical light source sparsemax\n(SLSparsemax) module that refers to the position and brightness relationship\nbetween spherical light sources is designed to improve their sparsity, which is\nsignificant but omitted by prior works. Extensive experiments demonstrate that\nMixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In\naddition, experiments on Web Dataset also show that MixLight as a parametric\nmethod has better generalization performance than non-parametric methods.",
      "tldr_zh": "该论文提出MixLight模型，通过结合Spherical Harmonics (SH)和Spherical Gaussian (SG)的互补优势，解决场景照明估计中现有方法的局限性，例如SH捕捉低频环境光而SG处理高频光源，从而实现更全面的照明表示。模型还引入SLSparsemax模块，考虑球形光源的位置和亮度关系以提升光源稀疏性。实验结果显示，MixLight在多个指标上超越现有最先进（SOTA）方法，并在Web Dataset上表现出比非参数化方法更好的泛化性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12768v1",
      "published_date": "2024-04-19 10:17:10 UTC",
      "updated_date": "2024-04-19 10:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:58:00.029397"
    },
    {
      "arxiv_id": "2404.12762v2",
      "title": "How should AI decisions be explained? Requirements for Explanations from the Perspective of European Law",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Fresz",
        "Elena Dubovitskaya",
        "Danilo Brajovic",
        "Marco Huber",
        "Christian Horz"
      ],
      "abstract": "This paper investigates the relationship between law and eXplainable\nArtificial Intelligence (XAI). While there is much discussion about the AI Act,\nfor which the trilogue of the European Parliament, Council and Commission\nrecently concluded, other areas of law seem underexplored. This paper focuses\non European (and in part German) law, although with international concepts and\nregulations such as fiduciary plausibility checks, the General Data Protection\nRegulation (GDPR), and product safety and liability. Based on XAI-taxonomies,\nrequirements for XAI-methods are derived from each of the legal bases,\nresulting in the conclusion that each legal basis requires different XAI\nproperties and that the current state of the art does not fulfill these to full\nsatisfaction, especially regarding the correctness (sometimes called fidelity)\nand confidence estimates of XAI-methods.\n  Published in the Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety https://doi.org/10.1609/aies.v7i1.31648 .",
      "tldr_zh": "这篇论文探讨了欧洲法律视角下AI决策解释的要求，聚焦于可解释AI (XAI) 与法律的关系，包括德国法律以及国际标准如General Data Protection Regulation (GDPR)、信托可信度检查和产品安全责任。作者基于XAI分类，从不同法律基础推导出XAI方法的特定属性需求，例如解释的正确性（fidelity）和置信度估计。研究结论指出，现有的XAI技术无法完全满足这些要求，导致AI解释在法律合规性方面存在不足。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12762v2",
      "published_date": "2024-04-19 10:08:28 UTC",
      "updated_date": "2024-11-26 14:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:58:12.965306"
    },
    {
      "arxiv_id": "2404.12760v1",
      "title": "Food Development through Co-creation with AI: bread with a \"taste of love\"",
      "title_zh": "翻译失败",
      "authors": [
        "Takuya Sera",
        "Izumi Kuwata",
        "Yuki Taya",
        "Noritaka Shimura",
        "Yosuke Motohashi"
      ],
      "abstract": "This study explores a new method in food development by utilizing AI\nincluding generative AI, aiming to craft products that delight the senses and\nresonate with consumers' emotions. The food ingredient recommendation approach\nused in this study can be considered as a form of multimodal generation in a\nbroad sense, as it takes text as input and outputs food ingredient candidates.\nThis Study focused on producing \"Romance Bread,\" a collection of breads infused\nwith flavors that reflect the nuances of a romantic Japanese television\nprogram. We analyzed conversations from TV programs and lyrics from songs\nfeaturing fruits and sweets to recommend ingredients that express romantic\nfeelings. Based on these recommendations, the bread developers then considered\nthe flavoring of the bread and developed new bread varieties. The research\nincluded a tasting evaluation involving 31 participants and interviews with the\nproduct developers. Findings indicate a notable correlation between tastes\ngenerated by AI and human preferences. This study validates the concept of\nusing AI in food innovation and highlights the broad potential for developing\nunique consumer experiences that focus on emotional engagement through AI and\nhuman collaboration.",
      "tldr_zh": "这篇论文探索了利用 generative AI 在食品开发中的新方法，通过多模态生成技术从文本输入推荐食品成分，旨在创建能愉悦感官并引发消费者情感共鸣的产品。研究焦点是开发“Romance Bread”，一种融入浪漫日本电视节目对话和歌曲歌词风味的面包，具体步骤包括分析相关媒体内容推荐成分，并由开发者制作新品种。随后进行的31名参与者品尝评估和访谈显示，AI 生成的口味与人类偏好有显著相关性，验证了 AI 与人类协作在食品创新中提升情感体验的潜力。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to GenAICHI: CHI 2024 Workshop on Generative AI and HCI",
      "pdf_url": "http://arxiv.org/pdf/2404.12760v1",
      "published_date": "2024-04-19 10:03:59 UTC",
      "updated_date": "2024-04-19 10:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:58:25.030709"
    },
    {
      "arxiv_id": "2404.12754v1",
      "title": "Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation",
      "title_zh": "表示秩的自适应正则化作为贝尔曼方程的隐式约束",
      "authors": [
        "Qiang He",
        "Tianyi Zhou",
        "Meng Fang",
        "Setareh Maghsudi"
      ],
      "abstract": "Representation rank is an important concept for understanding the role of\nNeural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the\nexpressive capacity of value networks. Existing studies focus on unboundedly\nmaximizing this rank; nevertheless, that approach would introduce overly\ncomplex models in the learning, thus undermining performance. Hence,\nfine-tuning representation rank presents a challenging and crucial optimization\nproblem. To address this issue, we find a guiding principle for adaptive\ncontrol of the representation rank. We employ the Bellman equation as a\ntheoretical foundation and derive an upper bound on the cosine similarity of\nconsecutive state-action pairs representations of value networks. We then\nleverage this upper bound to propose a novel regularizer, namely BEllman\nEquation-based automatic rank Regularizer (BEER). This regularizer adaptively\nregularizes the representation rank, thus improving the DRL agent's\nperformance. We first validate the effectiveness of automatic control of rank\non illustrative experiments. Then, we scale up BEER to complex continuous\ncontrol tasks by combining it with the deterministic policy gradient method.\nAmong 12 challenging DeepMind control tasks, BEER outperforms the baselines by\na large margin. Besides, BEER demonstrates significant advantages in Q-value\napproximation. Our code is available at\nhttps://github.com/sweetice/BEER-ICLR2024.",
      "tldr_zh": "该研究探讨了在 Deep Reinforcement Learning (DRL) 中，representation rank 的自适应正则化问题，以避免无限最大化 rank 导致的模型复杂性和性能下降。作者基于 Bellman Equation 推导了连续状态-动作对表示的余弦相似度上界，并提出了一种新型正则化器 BEER（BEllman Equation-based automatic rank Regularizer），用于自动调节 representation rank，从而提升 DRL 代理的性能。实验验证显示，BEER 在简单任务中有效，并在 12 个 DeepMind 控制任务中大幅优于基线模型，并在 Q-value 逼近方面表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CVPR23; Code: https://github.com/sweetice/BEER-ICLR2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12754v1",
      "published_date": "2024-04-19 10:00:34 UTC",
      "updated_date": "2024-04-19 10:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:58:37.499896"
    },
    {
      "arxiv_id": "2404.12753v2",
      "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhao Huang",
        "Zhouhong Gu",
        "Chenghao Peng",
        "Zhixu Li",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Liqian Wen",
        "Zulong Chen"
      ],
      "abstract": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}",
      "tldr_zh": "该论文介绍了 AutoScraper，一种基于大型语言模型（LLMs）的渐进式 Web 代理框架，用于生成 Web 刮取器，以解决现有方法在适应性和可重用性方面的局限性。AutoScraper 采用两阶段框架，利用 HTML 的层次结构和不同网页的相似性来高效处理多样化的 Web 环境。论文还提出一个新的可执行性指标，用于评估 Web 刮取器生成任务的性能，并通过多 LLMs 的全面实验证明了框架的有效性。资源可从 GitHub 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures, 18 tables. Accepted to EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12753v2",
      "published_date": "2024-04-19 09:59:44 UTC",
      "updated_date": "2024-09-26 09:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:58:48.755153"
    },
    {
      "arxiv_id": "2404.13101v1",
      "title": "DensePANet: An improved generative adversarial network for photoacoustic tomography image reconstruction from sparse data",
      "title_zh": "DensePANet：一种改进的生成对抗网络，用于从",
      "authors": [
        "Hesam Hakimnejad",
        "Zohreh Azimifar",
        "Narjes Goshtasbi"
      ],
      "abstract": "Image reconstruction is an essential step of every medical imaging method,\nincluding Photoacoustic Tomography (PAT), which is a promising modality of\nimaging, that unites the benefits of both ultrasound and optical imaging\nmethods. Reconstruction of PAT images using conventional methods results in\nrough artifacts, especially when applied directly to sparse PAT data. In recent\nyears, generative adversarial networks (GANs) have shown a powerful performance\nin image generation as well as translation, rendering them a smart choice to be\napplied to reconstruction tasks. In this study, we proposed an end-to-end\nmethod called DensePANet to solve the problem of PAT image reconstruction from\nsparse data. The proposed model employs a novel modification of UNet in its\ngenerator, called FD-UNet++, which considerably improves the reconstruction\nperformance. We evaluated the method on various in-vivo and simulated datasets.\nQuantitative and qualitative results show the better performance of our model\nover other prevalent deep learning techniques.",
      "tldr_zh": "本文提出DensePANet，一种改进的Generative Adversarial Networks (GANs)，用于从稀疏数据重建Photoacoustic Tomography (PAT)图像，以解决传统方法产生的粗糙伪影问题。该模型采用新型FD-UNet++作为生成器，显著提升了重建性能，并通过端到端处理结合了图像生成和转换的优势。在各种活体和模拟数据集上的定量及定性评估显示，DensePANet比其他深度学习技术表现出色。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13101v1",
      "published_date": "2024-04-19 09:52:32 UTC",
      "updated_date": "2024-04-19 09:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:59:00.103842"
    },
    {
      "arxiv_id": "2404.12744v2",
      "title": "Beyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches",
      "title_zh": "翻译失败",
      "authors": [
        "Pablo Biedma",
        "Xiaoyuan Yi",
        "Linus Huang",
        "Maosong Sun",
        "Xing Xie"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized the\nAI field but also pose potential safety and ethical risks. Deciphering LLMs'\nembedded values becomes crucial for assessing and mitigating their risks.\nDespite extensive investigation into LLMs' values, previous studies heavily\nrely on human-oriented value systems in social sciences. Then, a natural\nquestion arises: Do LLMs possess unique values beyond those of humans? Delving\ninto it, this work proposes a novel framework, ValueLex, to reconstruct LLMs'\nunique value system from scratch, leveraging psychological methodologies from\nhuman personality/value research. Based on Lexical Hypothesis, ValueLex\nintroduces a generative approach to elicit diverse values from 30+ LLMs,\nsynthesizing a taxonomy that culminates in a comprehensive value framework via\nfactor analysis and semantic clustering. We identify three core value\ndimensions, Competence, Character, and Integrity, each with specific\nsubdimensions, revealing that LLMs possess a structured, albeit non-human,\nvalue system. Based on this system, we further develop tailored projective\ntests to evaluate and analyze the value inclinations of LLMs across different\nmodel sizes, training methods, and data sources. Our framework fosters an\ninterdisciplinary paradigm of understanding LLMs, paving the way for future AI\nalignment and regulation.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 是否拥有超越人类规范的独特价值系统，以应对其潜在安全和伦理风险。研究提出 ValueLex 框架，利用 Lexical Hypothesis 和心理学方法，从 30+ LLMs 中生成多样值，并通过 factor analysis 和 semantic clustering 合成一个全面的价值框架。结果识别出三个核心价值维度：Competence, Character 和 Integrity，每个包含特定子维度，揭示 LLMs 具有结构化但非人类的价值系统。基于此，论文开发了定制的投射测试，评估 LLMs 在不同模型大小、训练方法和数据来源下的价值倾向，并为 AI alignment 和监管提供跨学科范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, work in progress",
      "pdf_url": "http://arxiv.org/pdf/2404.12744v2",
      "published_date": "2024-04-19 09:44:51 UTC",
      "updated_date": "2024-05-10 06:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:59:14.546829"
    },
    {
      "arxiv_id": "2404.12721v1",
      "title": "Generalized Few-Shot Meets Remote Sensing: Discovering Novel Classes in Land Cover Mapping via Hybrid Semantic Segmentation Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuohong Li",
        "Fangxiao Lu",
        "Jiaqi Zou",
        "Lei Hu",
        "Hongyan Zhang"
      ],
      "abstract": "Land-cover mapping is one of the vital applications in Earth observation,\naiming at classifying each pixel's land-cover type of remote-sensing images. As\nnatural and human activities change the landscape, the land-cover map needs to\nbe rapidly updated. However, discovering newly appeared land-cover types in\nexisting classification systems is still a non-trivial task hindered by various\nscales of complex land objects and insufficient labeled data over a wide-span\ngeographic area. In this paper, we propose a generalized few-shot\nsegmentation-based framework, named SegLand, to update novel classes in\nhigh-resolution land-cover mapping. Specifically, the proposed framework is\ndesigned in three parts: (a) Data pre-processing: the base training set and the\nfew-shot support sets of novel classes are analyzed and augmented; (b) Hybrid\nsegmentation structure; Multiple base learners and a modified Projection onto\nOrthogonal Prototypes (POP) network are combined to enhance the base-class\nrecognition and to dig novel classes from insufficient labels data; (c)\nUltimate fusion: the semantic segmentation results of the base learners and POP\nnetwork are reasonably fused. The proposed framework has won first place in the\nleaderboard of the OpenEarthMap Land Cover Mapping Few-Shot Challenge.\nExperiments demonstrate the superiority of the framework for automatically\nupdating novel land-cover classes with limited labeled data.",
      "tldr_zh": "该论文提出了一种广义少样本（generalized few-shot）框架SegLand，用于遥感图像的土地覆盖映射（land-cover mapping），旨在发现和更新新型土地类别，以应对景观变化带来的挑战。框架包括三个部分：(a) 数据预处理，通过分析和增强基础训练集及少样本支持集；(b) 混合分割结构，结合多个基学习器和修改后的Projection onto Orthogonal Prototypes (POP)网络，提升基础类识别并从不足标签数据中挖掘新类；(c) 最终融合，将语义分割（semantic segmentation）结果合理整合。该框架在OpenEarthMap Land Cover Mapping Few-Shot Challenge中获得第一名，实验证明其在有限标签数据下自动更新新型土地覆盖类别的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 11 figures, accepted by CVPR 2024 L3D-IVU Workshop",
      "pdf_url": "http://arxiv.org/pdf/2404.12721v1",
      "published_date": "2024-04-19 09:01:58 UTC",
      "updated_date": "2024-04-19 09:01:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:59:25.580259"
    },
    {
      "arxiv_id": "2404.12717v1",
      "title": "Show and Grasp: Few-shot Semantic Segmentation for Robot Grasping through Zero-shot Foundation Models",
      "title_zh": "Show and Grasp：通过零样本",
      "authors": [
        "Leonardo Barcellona",
        "Alberto Bacchin",
        "Matteo Terreran",
        "Emanuele Menegatti",
        "Stefano Ghidoni"
      ],
      "abstract": "The ability of a robot to pick an object, known as robot grasping, is crucial\nfor several applications, such as assembly or sorting. In such tasks, selecting\nthe right target to pick is as essential as inferring a correct configuration\nof the gripper. A common solution to this problem relies on semantic\nsegmentation models, which often show poor generalization to unseen objects and\nrequire considerable time and massive data to be trained. To reduce the need\nfor large datasets, some grasping pipelines exploit few-shot semantic\nsegmentation models, which are capable of recognizing new classes given a few\nexamples. However, this often comes at the cost of limited performance and\nfine-tuning is required to be effective in robot grasping scenarios. In this\nwork, we propose to overcome all these limitations by combining the impressive\ngeneralization capability reached by foundation models with a high-performing\nfew-shot classifier, working as a score function to select the segmentation\nthat is closer to the support set. The proposed model is designed to be\nembedded in a grasp synthesis pipeline. The extensive experiments using one or\nfive examples show that our novel approach overcomes existing performance\nlimitations, improving the state of the art both in few-shot semantic\nsegmentation on the Graspnet-1B (+10.5% mIoU) and Ocid-grasp (+1.6% AP)\ndatasets, and real-world few-shot grasp synthesis (+21.7% grasp accuracy). The\nproject page is available at:\nhttps://leobarcellona.github.io/showandgrasp.github.io/",
      "tldr_zh": "本研究针对机器人抓取任务，提出“Show and Grasp”方法，通过零-shot Foundation Models 结合高性能 few-shot semantic segmentation 分类器，作为评分函数来选择最接近支持集的分割，从而改善目标选择和抓取配置的准确性。该方法无需大量数据训练，直接嵌入抓取合成管道中，克服了传统模型的泛化局限。在实验中，它在 Graspnet-1B 数据集上 mIoU 提高了 10.5%，在 Ocid-grasp 数据集上 AP 提高了 1.6%，并在真实世界少样本抓取场景中提升了 21.7% 的抓取准确率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12717v1",
      "published_date": "2024-04-19 08:58:52 UTC",
      "updated_date": "2024-04-19 08:58:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:59:37.235250"
    },
    {
      "arxiv_id": "2404.12712v1",
      "title": "uTRAND: Unsupervised Anomaly Detection in Traffic Trajectories",
      "title_zh": "uTRAND：交通轨迹的无监督异常检测",
      "authors": [
        "Giacomo D'Amicantonio",
        "Egor Bondarau",
        "Peter H. N. de With"
      ],
      "abstract": "Deep learning-based approaches have achieved significant improvements on\npublic video anomaly datasets, but often do not perform well in real-world\napplications. This paper addresses two issues: the lack of labeled data and the\ndifficulty of explaining the predictions of a neural network. To this end, we\npresent a framework called uTRAND, that shifts the problem of anomalous\ntrajectory prediction from the pixel space to a semantic-topological domain.\nThe framework detects and tracks all types of traffic agents in bird's-eye-view\nvideos of traffic cameras mounted at an intersection. By conceptualizing the\nintersection as a patch-based graph, it is shown that the framework learns and\nmodels the normal behaviour of traffic agents without costly manual labeling.\nFurthermore, uTRAND allows to formulate simple rules to classify anomalous\ntrajectories in a way suited for human interpretation. We show that uTRAND\noutperforms other state-of-the-art approaches on a dataset of anomalous\ntrajectories collected in a real-world setting, while producing explainable\ndetection results.",
      "tldr_zh": "本研究提出 uTRAND 框架，用于无监督异常检测（unsupervised anomaly detection），解决交通轨迹异常预测中缺乏标注数据和神经网络预测可解释性不足的问题。uTRAND 将问题从像素空间转移到语义-拓扑域（semantic-topological domain），通过检测和跟踪鸟瞰视角（bird's-eye-view）视频中的交通代理，并将交叉路口建模为基于补丁的图（patch-based graph），从而无监督地学习和建模正常交通行为。框架允许制定简单规则来分类异常轨迹，提供易于人类解释的结果，并在真实世界数据集上优于现有最先进方法，实现了更可靠的异常检测。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12712v1",
      "published_date": "2024-04-19 08:46:33 UTC",
      "updated_date": "2024-04-19 08:46:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:59:50.736778"
    },
    {
      "arxiv_id": "2404.13099v1",
      "title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
      "title_zh": "Mathify：评估大型语言模型在数学问题求解任务中的性能",
      "authors": [
        "Avinash Anand",
        "Mohit Gupta",
        "Kritarth Prasad",
        "Navya Singla",
        "Sanjana Sanjeev",
        "Jatin Kumar",
        "Adarsh Raj Shivam",
        "Rajiv Ratn Shah"
      ],
      "abstract": "The rapid progress in the field of natural language processing (NLP) systems\nand the expansion of large language models (LLMs) have opened up numerous\nopportunities in the field of education and instructional methods. These\nadvancements offer the potential for tailored learning experiences and\nimmediate feedback, all delivered through accessible and cost-effective\nservices. One notable application area for this technological advancement is in\nthe realm of solving mathematical problems. Mathematical problem-solving not\nonly requires the ability to decipher complex problem statements but also the\nskill to perform precise arithmetic calculations at each step of the\nproblem-solving process. However, the evaluation of the arithmetic capabilities\nof large language models remains an area that has received relatively little\nattention. In response, we introduce an extensive mathematics dataset called\n\"MathQuest\" sourced from the 11th and 12th standard Mathematics NCERT\ntextbooks. This dataset encompasses mathematical challenges of varying\ncomplexity and covers a wide range of mathematical concepts. Utilizing this\ndataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2,\nWizardMath, and MAmmoTH. These fine-tuned models serve as benchmarks for\nevaluating their performance on our dataset. Our experiments reveal that among\nthe three models, MAmmoTH-13B emerges as the most proficient, achieving the\nhighest level of competence in solving the presented mathematical problems.\nConsequently, MAmmoTH-13B establishes itself as a robust and dependable\nbenchmark for addressing NCERT mathematics problems.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）在数学问题解决任务中的性能，强调了这些模型在解读复杂问题和进行精确算术计算方面的能力。研究者引入了名为 MathQuest 的数据集，该数据集源自 11th 和 12th 标准 Mathematics NCERT 课本，涵盖各种复杂度的数学挑战，并以此对 LLaMA-2、WizardMath 和 MAmmoTH 进行 fine-tuning 实验。结果显示，MAmmoTH-13B 在这些任务上表现出色，成为处理 NCERT 数学问题的可靠基准模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, NeurIPS 2023 Workshop on Generative AI for\n  Education (GAIED)",
      "pdf_url": "http://arxiv.org/pdf/2404.13099v1",
      "published_date": "2024-04-19 08:45:42 UTC",
      "updated_date": "2024-04-19 08:45:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:00:02.094724"
    },
    {
      "arxiv_id": "2404.17590v1",
      "title": "Leveraging Intra-modal and Inter-modal Interaction for Multi-Modal Entity Alignment",
      "title_zh": "利用模态内和模态间互动进行多模态实体对齐",
      "authors": [
        "Zhiwei Hu",
        "Víctor Gutiérrez-Basulto",
        "Zhiliang Xiang",
        "Ru Li",
        "Jeff Z. Pan"
      ],
      "abstract": "Multi-modal entity alignment (MMEA) aims to identify equivalent entity pairs\nacross different multi-modal knowledge graphs (MMKGs). Existing approaches\nfocus on how to better encode and aggregate information from different\nmodalities. However, it is not trivial to leverage multi-modal knowledge in\nentity alignment due to the modal heterogeneity. In this paper, we propose a\nMulti-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA),\nwhich effectively realizes multi-granular interaction within the same modality\nor between different modalities. MIMEA is composed of four modules: i) a\nMulti-modal Knowledge Embedding module, which extracts modality-specific\nrepresentations with multiple individual encoders; ii) a Probability-guided\nModal Fusion module, which employs a probability guided approach to integrate\nuni-modal representations into joint-modal embeddings, while considering the\ninteraction between uni-modal representations; iii) an Optimal Transport Modal\nAlignment module, which introduces an optimal transport mechanism to encourage\nthe interaction between uni-modal and joint-modal embeddings; iv) a\nModal-adaptive Contrastive Learning module, which distinguishes the embeddings\nof equivalent entities from those of non-equivalent ones, for each modality.\nExtensive experiments conducted on two real-world datasets demonstrate the\nstrong performance of MIMEA compared to the SoTA. Datasets and code have been\nsubmitted as supplementary materials.",
      "tldr_zh": "该论文针对多模态实体对齐（Multi-Modal Entity Alignment, MMEA）问题，提出了一种多粒度交互框架MIMEA，以有效利用同一模态（Intra-modal）和不同模态（Inter-modal）之间的交互，解决模态异质性带来的挑战。MIMEA包括四个模块：多模态知识嵌入模块使用独立编码器提取模态特定表示；概率引导模态融合模块整合单模态表示为联合嵌入，同时考虑交互；最优传输模态对齐模块促进单模态和联合嵌入的交互；模态自适应对比学习模块针对每个模态区分等价和非等价实体。实验在两个真实数据集上显示，MIMEA比现有最先进方法（SoTA）性能更强，为多模态知识图谱（MMKGs）实体对齐提供了新方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.17590v1",
      "published_date": "2024-04-19 08:43:11 UTC",
      "updated_date": "2024-04-19 08:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:00:15.922935"
    },
    {
      "arxiv_id": "2404.12711v1",
      "title": "Dynamic Temperature Knowledge Distillation",
      "title_zh": "动态温度知识蒸馏",
      "authors": [
        "Yukang Wei",
        "Yu Bai"
      ],
      "abstract": "Temperature plays a pivotal role in moderating label softness in the realm of\nknowledge distillation (KD). Traditional approaches often employ a static\ntemperature throughout the KD process, which fails to address the nuanced\ncomplexities of samples with varying levels of difficulty and overlooks the\ndistinct capabilities of different teacher-student pairings. This leads to a\nless-than-ideal transfer of knowledge. To improve the process of knowledge\npropagation, we proposed Dynamic Temperature Knowledge Distillation (DTKD)\nwhich introduces a dynamic, cooperative temperature control for both teacher\nand student models simultaneously within each training iterafion. In\nparticular, we proposed \"\\textbf{sharpness}\" as a metric to quantify the\nsmoothness of a model's output distribution. By minimizing the sharpness\ndifference between the teacher and the student, we can derive sample-specific\ntemperatures for them respectively. Extensive experiments on CIFAR-100 and\nImageNet-2012 demonstrate that DTKD performs comparably to leading KD\ntechniques, with added robustness in Target Class KD and None-target Class KD\nscenarios.The code is available at https://github.com/JinYu1998/DTKD.",
      "tldr_zh": "本文提出 Dynamic Temperature Knowledge Distillation (DTKD)，一种改进知识蒸馏(KD)的方法，通过动态调整教师和学生模型的温度来应对样本难度差异和模型配对问题。DTKD 引入“sharpness”指标量化输出分布的平滑度，并最小化教师与学生间的sharpness差异，以计算样本特定的温度。实验结果显示，在CIFAR-100和ImageNet-2012数据集上，DTKD与领先KD技术性能相当，并在Target Class KD和None-target Class KD场景中表现出更高的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12711v1",
      "published_date": "2024-04-19 08:40:52 UTC",
      "updated_date": "2024-04-19 08:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:00:26.758126"
    },
    {
      "arxiv_id": "2404.12704v1",
      "title": "A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only",
      "title_zh": "翻译失败",
      "authors": [
        "Jiazhu Dai",
        "Haoyu Sun"
      ],
      "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ndealing with various graph structures such as node classification, graph\nclassification and other tasks. However,recent studies have shown that GCNs are\nvulnerable to a novel threat known as backdoor attacks. However, all existing\nbackdoor attacks in the graph domain require modifying the training samples to\naccomplish the backdoor injection, which may not be practical in many realistic\nscenarios where adversaries have no access to modify the training samples and\nmay leads to the backdoor attack being detected easily. In order to explore the\nbackdoor vulnerability of GCNs and create a more practical and stealthy\nbackdoor attack method, this paper proposes a clean-graph backdoor attack\nagainst GCNs (CBAG) in the node classification task,which only poisons the\ntraining labels without any modification to the training samples, revealing\nthat GCNs have this security vulnerability. Specifically, CBAG designs a new\ntrigger exploration method to find important feature dimensions as the trigger\npatterns to improve the attack performance. By poisoning the training labels, a\nhidden backdoor is injected into the GCNs model. Experimental results show that\nour clean graph backdoor can achieve 99% attack success rate while maintaining\nthe functionality of the GCNs model on benign samples.",
      "tldr_zh": "这篇论文提出了一种名为CBAG的清洁图后门攻击方法，针对Graph Convolutional Networks (GCNs)在节点分类任务中的安全漏洞，仅通过毒化训练标签而无需修改训练样本，从而实现更隐秘和实用的攻击。CBAG设计了一个新的触发器探索方法，用于识别重要特征维度作为触发模式，以注入隐藏的后门。实验结果显示，该攻击在保持GCNs模型对良性样本正常功能的前提下，实现了99%的攻击成功率，揭示了GCNs的潜在安全风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12704v1",
      "published_date": "2024-04-19 08:21:54 UTC",
      "updated_date": "2024-04-19 08:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:00:36.848951"
    },
    {
      "arxiv_id": "2404.12691v2",
      "title": "Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
      "title_zh": "翻译失败",
      "authors": [
        "Shayne Longpre",
        "Robert Mahari",
        "Naana Obeng-Marnu",
        "William Brannon",
        "Tobin South",
        "Katy Gero",
        "Sandy Pentland",
        "Jad Kabbara"
      ],
      "abstract": "New capabilities in foundation models are owed in large part to massive,\nwidely-sourced, and under-documented training data collections. Existing\npractices in data collection have led to challenges in tracing authenticity,\nverifying consent, preserving privacy, addressing representation and bias,\nrespecting copyright, and overall developing ethical and trustworthy foundation\nmodels. In response, regulation is emphasizing the need for training data\ntransparency to understand foundation models' limitations. Based on a\nlarge-scale analysis of the foundation model training data landscape and\nexisting solutions, we identify the missing infrastructure to facilitate\nresponsible foundation model development practices. We examine the current\nshortcomings of common tools for tracing data authenticity, consent, and\ndocumentation, and outline how policymakers, developers, and data creators can\nfacilitate responsible foundation model development by adopting universal data\nprovenance standards.",
      "tldr_zh": "基础模型的训练数据真实性、同意和来源机制存在严重问题，导致伦理挑战如隐私侵犯、偏差和版权争议。论文通过大规模分析训练数据景观和现有解决方案，识别了缺失的基础设施，并评估了当前工具在追踪数据真实性、同意和文档方面的不足。为促进负责任的 foundation model 开发，论文建议政策制定者、开发者和数据创建者采用通用数据来源标准，以增强训练数据透明度和模型可信度。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2024 camera-ready version (Spotlight paper). 9 pages, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.12691v2",
      "published_date": "2024-04-19 07:42:35 UTC",
      "updated_date": "2024-08-30 21:20:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:00:50.227068"
    },
    {
      "arxiv_id": "2404.12689v2",
      "title": "Can LLMs Understand Computer Networks? Towards a Virtual System Administrator",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Donadel",
        "Francesco Marchiori",
        "Luca Pajola",
        "Mauro Conti"
      ],
      "abstract": "Recent advancements in Artificial Intelligence, and particularly Large\nLanguage Models (LLMs), offer promising prospects for aiding system\nadministrators in managing the complexity of modern networks. However, despite\nthis potential, a significant gap exists in the literature regarding the extent\nto which LLMs can understand computer networks. Without empirical evidence,\nsystem administrators might rely on these models without assurance of their\nefficacy in performing network-related tasks accurately.\n  In this paper, we are the first to conduct an exhaustive study on LLMs'\ncomprehension of computer networks. We formulate several research questions to\ndetermine whether LLMs can provide correct answers when supplied with a network\ntopology and questions on it. To assess them, we developed a thorough framework\nfor evaluating LLMs' capabilities in various network-related tasks. We evaluate\nour framework on multiple computer networks employing proprietary (e.g., GPT4)\nand open-source (e.g., Llama2) models. Our findings in general purpose LLMs\nusing a zero-shot scenario demonstrate promising results, with the best model\nachieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy\nresults in small and medium networks, while challenges persist in comprehending\ncomplex network topologies, particularly for open-source models. Moreover, we\nprovide insight into how prompt engineering can enhance the accuracy of some\ntasks.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)对计算机网络理解能力的全面评估，旨在为虚拟系统管理员铺平道路，以帮助系统管理员管理复杂网络。研究者制定了多个研究问题，并开发了一个评估框架，在各种网络拓扑上测试了专有模型（如GPT-4）和开源模型（如Llama2），采用零-shot场景进行实验。结果显示，最好模型的平均准确率达到79.3%，专有LLMs在小到中型网络上表现突出，但开源模型在复杂拓扑上仍面临挑战。最后，论文还揭示了提示工程(prompt engineering)可以提升某些网络任务的准确性。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET",
        "C.2.1; C.2.5; I.2.1"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12689v2",
      "published_date": "2024-04-19 07:41:54 UTC",
      "updated_date": "2024-07-31 12:02:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:01:03.953900"
    },
    {
      "arxiv_id": "2404.12667v1",
      "title": "Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Georges Le Bellier",
        "Nicolas Audebert"
      ],
      "abstract": "Earth Observation imagery can capture rare and unusual events, such as\ndisasters and major landscape changes, whose visual appearance contrasts with\nthe usual observations. Deep models trained on common remote sensing data will\noutput drastically different features for these out-of-distribution samples,\ncompared to those closer to their training dataset. Detecting them could\ntherefore help anticipate changes in the observations, either geographical or\nenvironmental. In this work, we show that the reconstruction error of diffusion\nmodels can effectively serve as unsupervised out-of-distribution detectors for\nremote sensing images, using them as a plausibility score. Moreover, we\nintroduce ODEED, a novel reconstruction-based scorer using the probability-flow\nODE of diffusion models. We validate it experimentally on SpaceNet 8 with\nvarious scenarios, such as classical OOD detection with geographical shift and\nnear-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We\nshow that our ODEED scorer significantly outperforms other diffusion-based and\ndiscriminative baselines on the more challenging near-OOD scenarios of flood\nimage detection, where OOD images are close to the distribution tail. We aim to\npave the way towards better use of generative models for anomaly detection in\nremote sensing.",
      "tldr_zh": "本文提出使用Diffusion Models检测遥感图像中的Out-Of-Distribution (OOD)样本，通过扩散模型的重建错误作为无监督的可信度分数，以识别罕见事件如灾害或景观变化。作者引入了ODEED，一种新型基于概率流ODE的重建评分器，并在SpaceNet 8数据集上进行实验验证，包括地理偏移和近OOD场景（如洪水前后图像识别）。结果显示，ODEED在挑战性的洪水图像检测任务中显著优于其他扩散模型和判别式基线，提升了异常检测性能。该方法旨在推动生成模型在遥感异常检测中的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "EARTHVISION 2024 IEEE/CVF CVPR Workshop. Large Scale Computer Vision\n  for Remote Sensing Imagery, Jun 2024, Seattle, United States",
      "pdf_url": "http://arxiv.org/pdf/2404.12667v1",
      "published_date": "2024-04-19 07:07:36 UTC",
      "updated_date": "2024-04-19 07:07:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:01:15.856888"
    },
    {
      "arxiv_id": "2404.12653v1",
      "title": "How Real Is Real? A Human Evaluation Framework for Unrestricted Adversarial Examples",
      "title_zh": "现实有多真实？ 针对不受限制的对抗样本的人类评估框架",
      "authors": [
        "Dren Fazlija",
        "Arkadij Orlov",
        "Johanna Schrader",
        "Monty-Maximilian Zühlke",
        "Michael Rohs",
        "Daniel Kudenko"
      ],
      "abstract": "With an ever-increasing reliance on machine learning (ML) models in the real\nworld, adversarial examples threaten the safety of AI-based systems such as\nautonomous vehicles. In the image domain, they represent maliciously perturbed\ndata points that look benign to humans (i.e., the image modification is not\nnoticeable) but greatly mislead state-of-the-art ML models. Previously,\nresearchers ensured the imperceptibility of their altered data points by\nrestricting perturbations via $\\ell_p$ norms. However, recent publications\nclaim that creating natural-looking adversarial examples without such\nrestrictions is also possible. With much more freedom to instill malicious\ninformation into data, these unrestricted adversarial examples can potentially\novercome traditional defense strategies as they are not constrained by the\nlimitations or patterns these defenses typically recognize and mitigate. This\nallows attackers to operate outside of expected threat models. However,\nsurveying existing image-based methods, we noticed a need for more human\nevaluations of the proposed image modifications. Based on existing\nhuman-assessment frameworks for image generation quality, we propose SCOOTER -\nan evaluation framework for unrestricted image-based attacks. It provides\nresearchers with guidelines for conducting statistically significant human\nexperiments, standardized questions, and a ready-to-use implementation. We\npropose a framework that allows researchers to analyze how imperceptible their\nunrestricted attacks truly are.",
      "tldr_zh": "这篇论文探讨了不受限制的对抗样本（unrestricted adversarial examples）对机器学习（ML）模型的安全威胁，特别是图像领域中，这些样本对人类看似无害但能严重误导模型。传统方法通过$\\ell_p$范数限制扰动来确保样本不易察觉，但新方法允许更自由的恶意修改，可能绕过现有防御。论文提出SCOOTER框架，这是一个基于现有图像生成质量评估的工具，提供统计显著的人类实验指导、标准化问题和现成实现，以评估这些不受限制攻击的真实不易察觉性。最终，该框架帮助研究人员更准确地分析攻击的隐蔽性，从而提升AI系统的鲁棒性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "3 pages, 3 figures, AAAI 2024 Spring Symposium on User-Aligned\n  Assessment of Adaptive AI Systems",
      "pdf_url": "http://arxiv.org/pdf/2404.12653v1",
      "published_date": "2024-04-19 06:42:01 UTC",
      "updated_date": "2024-04-19 06:42:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:01:26.647709"
    },
    {
      "arxiv_id": "2404.12652v2",
      "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Zang",
        "Tian Yun",
        "Hao Tan",
        "Trung Bui",
        "Chen Sun"
      ],
      "abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a\n\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\nthe same time? We aim to answer this question as visual concepts learned \"for\nfree\" would enable wide applications such as neuro-symbolic reasoning or\nhuman-interpretable object classification. We assume that the visual concepts,\nif captured by pre-trained VLMs, can be extracted by their vision-language\ninterface with text-based concept prompts. We observe that recent works\nprompting VLMs with concepts often differ in their strategies to define and\nevaluate the visual concepts, leading to conflicting conclusions. We propose a\nnew concept definition strategy based on two observations: First, certain\nconcept prompts include shortcuts that recognize correct concepts for wrong\nreasons; Second, multimodal information (e.g. visual discriminativeness, and\ntextual knowledge) should be leveraged when selecting the concepts. Our\nproposed concept discovery and learning (CDL) framework is thus designed to\nidentify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n\"spiky durian\"), which are ranked and selected based on visual and language\nmutual information. We carefully design quantitative and human evaluations of\nthe discovered concepts on six diverse visual recognition datasets, which\nconfirm that pre-trained VLMs do learn visual concepts that provide accurate\nand thorough descriptions for the recognized objects. All code and models are\npublicly released.",
      "tldr_zh": "这篇论文探讨了预训练的视觉语言模型 (VLMs) 是否在图像描述任务中同时学习视觉概念（如 \"brown\" 颜色和 \"spiky\" 纹理），以支持应用如神经符号推理或可解释对象分类。作者提出概念发现和学习 (CDL) 框架，通过分析概念提示中的捷径问题并利用视觉区分性和文本知识，识别并排名多样化的通用视觉概念（如 \"spiky\" 而非特定对象相关概念）。在六个视觉识别数据集上的定量和人类评估中，实验结果证实 VLMs 确实学习了准确且全面的视觉概念描述，并公开了所有代码和模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Transactions on Machine Learning Research, 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.12652v2",
      "published_date": "2024-04-19 06:41:32 UTC",
      "updated_date": "2025-01-13 21:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:01:40.532752"
    },
    {
      "arxiv_id": "2404.13096v1",
      "title": "Reducing Redundant Computation in Multi-Agent Coordination through Locally Centralized Execution",
      "title_zh": "翻译失败",
      "authors": [
        "Yidong Bai",
        "Toshiharu Sugawara"
      ],
      "abstract": "In multi-agent reinforcement learning, decentralized execution is a common\napproach, yet it suffers from the redundant computation problem. This occurs\nwhen multiple agents redundantly perform the same or similar computation due to\noverlapping observations. To address this issue, this study introduces a novel\nmethod referred to as locally centralized team transformer (LCTT). LCTT\nestablishes a locally centralized execution framework where selected agents\nserve as leaders, issuing instructions, while the rest agents, designated as\nworkers, act as these instructions without activating their policy networks.\nFor LCTT, we proposed the team-transformer (T-Trans) architecture that allows\nleaders to provide specific instructions to each worker, and the leadership\nshift mechanism that allows agents autonomously decide their roles as leaders\nor workers. Our experimental results demonstrate that the proposed method\neffectively reduces redundant computation, does not decrease reward levels, and\nleads to faster learning convergence.",
      "tldr_zh": "这篇论文针对多智能体强化学习中的分散执行问题，提出了一种新方法：Locally Centralized Team Transformer (LCTT)，以减少代理因重叠观察而导致的冗余计算。LCTT 通过选定领导者代理（leaders）使用 Team-Transformer (T-Trans) 架构向工作者代理（workers）发出特定指令，同时引入领导权转移机制（leadership shift mechanism），让代理自主决定角色，从而避免工作者激活自己的策略网络。实验结果显示，该方法显著降低了冗余计算，同时维持了奖励水平并加快了学习收敛。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "6 pages, 5 figures, Under review",
      "pdf_url": "http://arxiv.org/pdf/2404.13096v1",
      "published_date": "2024-04-19 06:13:37 UTC",
      "updated_date": "2024-04-19 06:13:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:01:54.074288"
    },
    {
      "arxiv_id": "2404.12638v1",
      "title": "Learning to Cut via Hierarchical Sequence/Set Model for Efficient Mixed-Integer Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Wang",
        "Zhihai Wang",
        "Xijun Li",
        "Yufei Kuang",
        "Zhihao Shi",
        "Fangzhou Zhu",
        "Mingxuan Yuan",
        "Jia Zeng",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "abstract": "Cutting planes (cuts) play an important role in solving mixed-integer linear\nprograms (MILPs), which formulate many important real-world applications. Cut\nselection heavily depends on (P1) which cuts to prefer and (P2) how many cuts\nto select. Although modern MILP solvers tackle (P1)-(P2) by human-designed\nheuristics, machine learning carries the potential to learn more effective\nheuristics. However, many existing learning-based methods learn which cuts to\nprefer, neglecting the importance of learning how many cuts to select.\nMoreover, we observe that (P3) what order of selected cuts to prefer\nsignificantly impacts the efficiency of MILP solvers as well. To address these\nchallenges, we propose a novel hierarchical sequence/set model (HEM) to learn\ncut selection policies. Specifically, HEM is a bi-level model: (1) a\nhigher-level module that learns how many cuts to select, (2) and a lower-level\nmodule -- that formulates the cut selection as a sequence/set to sequence\nlearning problem -- to learn policies selecting an ordered subset with the\ncardinality determined by the higher-level module. To the best of our\nknowledge, HEM is the first data-driven methodology that well tackles (P1)-(P3)\nsimultaneously. Experiments demonstrate that HEM significantly improves the\nefficiency of solving MILPs on eleven challenging MILP benchmarks, including\ntwo Huawei's real problems.",
      "tldr_zh": "这篇论文针对混合整数线性规划(MILPs)求解中的切面选择问题，提出了一种新型的分层序列/集合模型(HEM)，旨在同时处理切面的优先级(P1)、数量(P2)和顺序(P3)。HEM采用双层结构：高层模块学习选择多少切面，低层模块将切面选择建模为序列/集合到序列的学习问题，以生成有序子集。该方法在11个挑战性MILP基准上，包括两个华为真实问题，显著提高了求解效率，展示了数据驱动方法的优势。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2302.00244",
      "pdf_url": "http://arxiv.org/pdf/2404.12638v1",
      "published_date": "2024-04-19 05:40:25 UTC",
      "updated_date": "2024-04-19 05:40:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:02:06.606946"
    },
    {
      "arxiv_id": "2404.12634v3",
      "title": "Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment",
      "title_zh": "基于 Transformer 的多模态中风治疗分类结果预测",
      "authors": [
        "Danqing Ma",
        "Meng Wang",
        "Ao Xiang",
        "Zongqing Qi",
        "Qin Yang"
      ],
      "abstract": "This study proposes a multi-modal fusion framework Multitrans based on the\nTransformer architecture and self-attention mechanism. This architecture\ncombines the study of non-contrast computed tomography (NCCT) images and\ndischarge diagnosis reports of patients undergoing stroke treatment, using a\nvariety of methods based on Transformer architecture approach to predicting\nfunctional outcomes of stroke treatment. The results show that the performance\nof single-modal text classification is significantly better than single-modal\nimage classification, but the effect of multi-modal combination is better than\nany single modality. Although the Transformer model only performs worse on\nimaging data, when combined with clinical meta-diagnostic information, both can\nlearn better complementary information and make good contributions to\naccurately predicting stroke treatment effects..",
      "tldr_zh": "本研究提出了一种基于 Transformer 架构和自注意力机制的多模态融合框架 Multitrans，用于预测中风治疗的功能性结果。该框架结合非对比计算机断层扫描 (NCCT) 图像和患者出院诊断报告，通过 Transformer 方法实现多模态数据的整合。实验结果表明，单模态文本分类的表现显著优于单模态图像分类，而多模态组合进一步提升了整体效果，使模型能够学习互补信息，从而更准确地预测中风治疗效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12634v3",
      "published_date": "2024-04-19 05:31:37 UTC",
      "updated_date": "2024-11-16 02:36:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:02:17.368738"
    },
    {
      "arxiv_id": "2404.12633v4",
      "title": "FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation",
      "title_zh": "FlagVNE：一个灵活且可泛化的强化学习框架，用于网络资源分配",
      "authors": [
        "Tianfu Wang",
        "Qilin Fan",
        "Chao Wang",
        "Long Yang",
        "Leilei Ding",
        "Nicholas Jing Yuan",
        "Hui Xiong"
      ],
      "abstract": "Virtual network embedding (VNE) is an essential resource allocation task in\nnetwork virtualization, aiming to map virtual network requests (VNRs) onto\nphysical infrastructure. Reinforcement learning (RL) has recently emerged as a\npromising solution to this problem. However, existing RL-based VNE methods are\nlimited by the unidirectional action design and one-size-fits-all training\nstrategy, resulting in restricted searchability and generalizability. In this\npaper, we propose a FLexible And Generalizable RL framework for VNE, named\nFlagVNE. Specifically, we design a bidirectional action-based Markov decision\nprocess model that enables the joint selection of virtual and physical nodes,\nthus improving the exploration flexibility of solution space. To tackle the\nexpansive and dynamic action space, we design a hierarchical decoder to\ngenerate adaptive action probability distributions and ensure high training\nefficiency. Furthermore, to overcome the generalization issue for varying VNR\nsizes, we propose a meta-RL-based training method with a curriculum scheduling\nstrategy, facilitating specialized policy training for each VNR size. Finally,\nextensive experimental results show the effectiveness of FlagVNE across\nmultiple key metrics. Our code is available at GitHub\n(https://github.com/GeminiLight/flag-vne).",
      "tldr_zh": "本文提出 FlagVNE，一种灵活且可泛化的 Reinforcement Learning (RL) 框架，用于解决虚拟网络嵌入 (VNE) 的资源分配问题，该框架通过双向行动的 Markov 决策过程模型实现虚拟和物理节点的联合选择，从而提升解决方案空间的探索灵活性。针对扩展的动态行动空间，FlagVNE 采用分层解码器生成自适应行动概率分布，并结合 meta-RL 和课程调度策略来优化不同 VNR 大小的训练泛化能力。实验结果显示，该框架在多个关键指标上表现出色，代码已在 GitHub 上开源。",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12633v4",
      "published_date": "2024-04-19 05:24:24 UTC",
      "updated_date": "2024-05-01 18:58:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:02:30.061893"
    },
    {
      "arxiv_id": "2404.12631v2",
      "title": "Breaching the Bottleneck: Evolutionary Transition from Reward-Driven Learning to Reward-Agnostic Domain-Adapted Learning in Neuromodulated Neural Nets",
      "title_zh": "翻译失败",
      "authors": [
        "Solvi Arnold",
        "Reiji Suzuki",
        "Takaya Arita",
        "Kimitoshi Yamazaki"
      ],
      "abstract": "Advanced biological intelligence learns efficiently from an information-rich\nstream of stimulus information, even when feedback on behaviour quality is\nsparse or absent. Such learning exploits implicit assumptions about task\ndomains. We refer to such learning as Domain-Adapted Learning (DAL). In\ncontrast, AI learning algorithms rely on explicit externally provided measures\nof behaviour quality to acquire fit behaviour. This imposes an information\nbottleneck that precludes learning from diverse non-reward stimulus\ninformation, limiting learning efficiency. We consider the question of how\nbiological evolution circumvents this bottleneck to produce DAL. We propose\nthat species first evolve the ability to learn from reward signals, providing\ninefficient (bottlenecked) but broad adaptivity. From there, integration of\nnon-reward information into the learning process can proceed via gradual\naccumulation of biases induced by such information on specific task domains.\nThis scenario provides a biologically plausible pathway towards\nbottleneck-free, domain-adapted learning. Focusing on the second phase of this\nscenario, we set up a population of NNs with reward-driven learning modelled as\nReinforcement Learning (A2C), and allow evolution to improve learning\nefficiency by integrating non-reward information into the learning process\nusing a neuromodulatory update mechanism. On a navigation task in continuous 2D\nspace, evolved DAL agents show a 300-fold increase in learning speed compared\nto pure RL agents. Evolution is found to eliminate reliance on reward\ninformation altogether, allowing DAL agents to learn from non-reward\ninformation exclusively, using local neuromodulation-based connection weight\nupdates only. Code available at github.com/aislab/dal.",
      "tldr_zh": "该论文探讨了生物进化如何从依赖奖励的强化学习（Reinforcement Learning, A2C）过渡到无奖励的域适应学习（Domain-Adapted Learning, DAL），从而克服 AI 算法的信息瓶颈问题。作者提出，通过逐步整合非奖励刺激信息到神经调制神经网络（neuromodulated neural nets）中，进化过程能提升学习效率。实验在连续 2D 导航任务中显示，DAL 代理的学习速度比纯 RL 代理提高了 300 倍，并最终完全依赖非奖励信息进行本地连接权重更新。该研究为实现高效、生物启发式的学习机制提供了新路径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.NE",
      "comment": "Camera ready version. 9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.12631v2",
      "published_date": "2024-04-19 05:14:47 UTC",
      "updated_date": "2024-08-02 07:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:02:42.882754"
    },
    {
      "arxiv_id": "2404.12627v1",
      "title": "A Soft e-Textile Sensor for Enhanced Deep Learning-based Shape Sensing of Soft Continuum Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Vincent Galeta",
        "Ayman A. Nada",
        "Sabah M. Ahmed",
        "Victor Parque",
        "Haitham El-Hussieny"
      ],
      "abstract": "The safety and accuracy of robotic navigation hold paramount importance,\nespecially in the realm of soft continuum robotics, where the limitations of\ntraditional rigid sensors become evident. Encoders, piezoresistive, and\npotentiometer sensors often fail to integrate well with the flexible nature of\nthese robots, adding unwanted bulk and rigidity. To overcome these hurdles, our\nstudy presents a new approach to shape sensing in soft continuum robots through\nthe use of soft e-textile resistive sensors. This sensor, designed to\nflawlessly integrate with the robot's structure, utilizes a resistive material\nthat adjusts its resistance in response to the robot's movements and\ndeformations. This adjustment facilitates the capture of multidimensional force\nmeasurements across the soft sensor layers. A deep Convolutional Neural Network\n(CNN) is employed to decode the sensor signals, enabling precise estimation of\nthe robot's shape configuration based on the detailed data from the e-textile\nsensor. Our research investigates the efficacy of this e-textile sensor in\ndetermining the curvature parameters of soft continuum robots. The findings are\nencouraging, showing that the soft e-textile sensor not only matches but\npotentially exceeds the capabilities of traditional rigid sensors in terms of\nshape sensing and estimation. This advancement significantly boosts the safety\nand efficiency of robotic navigation systems.",
      "tldr_zh": "本研究针对软体连续机器人的形状感知问题，提出了一种软 e-textile 电阻传感器，以解决传统刚性传感器（如编码器和压阻传感器）在灵活性、体积和刚度方面的局限性。该传感器无缝集成到机器人结构中，通过电阻变化捕捉多维力测量，并结合深度 CNN 模型解码信号，实现对机器人曲率参数的精确估计。实验结果表明，该传感器不仅匹配且可能超过传统传感器的性能，显著提升了机器人导航的安全性和效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12627v1",
      "published_date": "2024-04-19 05:00:25 UTC",
      "updated_date": "2024-04-19 05:00:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:02:54.116572"
    },
    {
      "arxiv_id": "2404.12626v1",
      "title": "Grasper: A Generalist Pursuer for Pursuit-Evasion Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Pengdeng Li",
        "Shuxin Li",
        "Xinrun Wang",
        "Jakub Cerny",
        "Youzhi Zhang",
        "Stephen McAleer",
        "Hau Chan",
        "Bo An"
      ],
      "abstract": "Pursuit-evasion games (PEGs) model interactions between a team of pursuers\nand an evader in graph-based environments such as urban street networks. Recent\nadvancements have demonstrated the effectiveness of the pre-training and\nfine-tuning paradigm in PSRO to improve scalability in solving large-scale\nPEGs. However, these methods primarily focus on specific PEGs with fixed\ninitial conditions that may vary substantially in real-world scenarios, which\nsignificantly hinders the applicability of the traditional methods. To address\nthis issue, we introduce Grasper, a GeneRAlist purSuer for Pursuit-Evasion\npRoblems, capable of efficiently generating pursuer policies tailored to\nspecific PEGs. Our contributions are threefold: First, we present a novel\narchitecture that offers high-quality solutions for diverse PEGs, comprising\ncritical components such as (i) a graph neural network (GNN) to encode PEGs\ninto hidden vectors, and (ii) a hypernetwork to generate pursuer policies based\non these hidden vectors. As a second contribution, we develop an efficient\nthree-stage training method involving (i) a pre-pretraining stage for learning\nrobust PEG representations through self-supervised graph learning techniques\nlike GraphMAE, (ii) a pre-training stage utilizing heuristic-guided multi-task\npre-training (HMP) where heuristic-derived reference policies (e.g., through\nDijkstra's algorithm) regularize pursuer policies, and (iii) a fine-tuning\nstage that employs PSRO to generate pursuer policies on designated PEGs.\nFinally, we perform extensive experiments on synthetic and real-world maps,\nshowcasing Grasper's significant superiority over baselines in terms of\nsolution quality and generalizability. We demonstrate that Grasper provides a\nversatile approach for solving pursuit-evasion problems across a broad range of\nscenarios, enabling practical deployment in real-world situations.",
      "tldr_zh": "本研究提出 Grasper，一种通用追捕者框架，用于解决 Pursuit-Evasion Games (PEGs) 的挑战，这些游戏涉及追捕者团队在图-based 环境（如城市街道网络）中追踪逃避者的问题。Grasper 的核心架构包括 Graph Neural Network (GNN) 用于编码 PEGs 为隐藏向量，以及 Hypernetwork 用于基于这些向量生成定制的追捕者策略；同时，采用三阶段训练方法，包括自监督预预训练（如 GraphMAE）、启发式引导的多任务预训练 (HMP)（利用如 Dijkstra's algorithm 的参考策略）和 PSRO 微调，以提升策略的鲁棒性和泛化性。在合成和真实世界地图上的实验中，Grasper 在解决方案质量和泛化性上显著优于基线模型，展示了其在广泛场景中的实用部署潜力。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in the 23rd International Conference on Autonomous Agents\n  and Multi-Agent Systems (AAMAS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.12626v1",
      "published_date": "2024-04-19 04:54:38 UTC",
      "updated_date": "2024-04-19 04:54:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:03:06.565152"
    },
    {
      "arxiv_id": "2404.12618v1",
      "title": "CORI: CJKV Benchmark with Romanization Integration -- A step towards Cross-lingual Transfer Beyond Textual Scripts",
      "title_zh": "翻译失败",
      "authors": [
        "Hoang H. Nguyen",
        "Chenwei Zhang",
        "Ye Liu",
        "Natalie Parde",
        "Eugene Rohrbaugh",
        "Philip S. Yu"
      ],
      "abstract": "Naively assuming English as a source language may hinder cross-lingual\ntransfer for many languages by failing to consider the importance of language\ncontact. Some languages are more well-connected than others, and target\nlanguages can benefit from transferring from closely related languages; for\nmany languages, the set of closely related languages does not include English.\nIn this work, we study the impact of source language for cross-lingual\ntransfer, demonstrating the importance of selecting source languages that have\nhigh contact with the target language. We also construct a novel benchmark\ndataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages\nto further encourage in-depth studies of language contact. To comprehensively\ncapture contact between these languages, we propose to integrate Romanized\ntranscription beyond textual scripts via Contrastive Learning objectives,\nleading to enhanced cross-lingual representations and effective zero-shot\ncross-lingual transfer.",
      "tldr_zh": "该研究强调了在跨语言转移(Cross-lingual Transfer)中，选择与目标语言高度接触的源语言的重要性，而非简单依赖英语，以避免忽略语言联系。该团队构建了一个新的CJKV基准数据集，针对中文、日文、韩文和越南文等密切相关语言，并提出通过Contrastive Learning整合Romanized transcription的方法，超越传统文本脚本。该方法增强了跨语言表示，提升了零样本跨语言转移的有效性，为深入研究语言接触提供了新工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.12618v1",
      "published_date": "2024-04-19 04:02:50 UTC",
      "updated_date": "2024-04-19 04:02:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:03:18.000911"
    },
    {
      "arxiv_id": "2404.12605v1",
      "title": "GluMarker: A Novel Predictive Modeling of Glycemic Control Through Digital Biomarkers",
      "title_zh": "GluMarker：通过数字生物标记进行血糖控制的新颖预测建模",
      "authors": [
        "Ziyi Zhou",
        "Ming Cheng",
        "Xingjian Diao",
        "Yanjun Cui",
        "Xiangling Li"
      ],
      "abstract": "The escalating prevalence of diabetes globally underscores the need for\ndiabetes management. Recent research highlights the growing focus on digital\nbiomarkers in diabetes management, with innovations in computational frameworks\nand noninvasive monitoring techniques using personalized glucose metrics.\nHowever, they predominantly focus on insulin dosing and specific glucose\nvalues, or with limited attention given to overall glycemic control. This\nleaves a gap in expanding the scope of digital biomarkers for overall glycemic\ncontrol in diabetes management. To address such a research gap, we propose\nGluMarker -- an end-to-end framework for modeling digital biomarkers using\nbroader factors sources to predict glycemic control. Through the assessment and\nrefinement of various machine learning baselines, GluMarker achieves\nstate-of-the-art on Anderson's dataset in predicting next-day glycemic control.\nMoreover, our research identifies key digital biomarkers for the next day's\nglycemic control prediction. These identified biomarkers are instrumental in\nilluminating the daily factors that influence glycemic management, offering\nvital insights for diabetes care.",
      "tldr_zh": "本研究针对糖尿病管理中的不足，指出现有数字生物标记(digital biomarkers)主要聚焦于胰岛素剂量和特定葡萄糖值，而忽略了整体 glycemic control 的预测。论文提出 GluMarker 框架，这是一个端到端系统，通过整合更广泛的因素来源和机器学习(machine learning)模型，在 Anderson's dataset 上实现了 state-of-the-art 性能。GluMarker 不仅提升了下一天 glycemic control 的预测准确性，还识别了关键 digital biomarkers，提供对影响日常糖化血红蛋白的因素的宝贵洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12605v1",
      "published_date": "2024-04-19 03:15:50 UTC",
      "updated_date": "2024-04-19 03:15:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:03:29.841037"
    },
    {
      "arxiv_id": "2404.12596v1",
      "title": "Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Lasal Jayawardena",
        "Prasan Yapa"
      ],
      "abstract": "Over the past year, the field of Natural Language Generation (NLG) has\nexperienced an exponential surge, largely due to the introduction of Large\nLanguage Models (LLMs). These models have exhibited the most effective\nperformance in a range of domains within the Natural Language Processing and\nGeneration domains. However, their application in domain-specific tasks, such\nas paraphrasing, presents significant challenges. The extensive number of\nparameters makes them difficult to operate on commercial hardware, and they\nrequire substantial time for inference, leading to high costs in a production\nsetting. In this study, we tackle these obstacles by employing LLMs to develop\nthree distinct models for the paraphrasing field, applying a method referred to\nas sequence-level knowledge distillation. These distilled models are capable of\nmaintaining the quality of paraphrases generated by the LLM. They demonstrate\nfaster inference times and the ability to generate diverse paraphrases of\ncomparable quality. A notable characteristic of these models is their ability\nto exhibit syntactic diversity while also preserving lexical diversity,\nfeatures previously uncommon due to existing data quality issues in datasets\nand not typically observed in neural-based approaches. Human evaluation of our\nmodels shows that there is only a 4% drop in performance compared to the LLM\nteacher model used in the distillation process, despite being 1000 times\nsmaller. This research provides a significant contribution to the NLG field,\noffering a more efficient and cost-effective solution for paraphrasing tasks.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在改写任务中存在的参数过多、推理时间长和成本高的问题，提出了一种基于序列级知识蒸馏（sequence-level knowledge distillation）的参数高效方法，开发了三种改写模型。相比LLMs，这些模型能生成高质量、多样化的改写，包括句法和词汇多样性，同时显著提高推理速度。实验结果显示，人类评估中模型性能仅比LLMs教师模型下降4%，但模型大小缩小了1000倍，为自然语言生成（NLG）领域提供了更高效、成本有效的改写解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in: 2024 5th International Conference on Advancements in\n  Computational Sciences (ICACS) with IEEE",
      "pdf_url": "http://arxiv.org/pdf/2404.12596v1",
      "published_date": "2024-04-19 02:59:09 UTC",
      "updated_date": "2024-04-19 02:59:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:03:41.604062"
    },
    {
      "arxiv_id": "2404.12594v1",
      "title": "Random Network Distillation Based Deep Reinforcement Learning for AGV Path Planning",
      "title_zh": "基于随机网络蒸馏的深度强化学习用于 AGV 路径规划",
      "authors": [
        "Huilin Yin",
        "Shengkai Su",
        "Yinjia Lin",
        "Pengju Zhen",
        "Karin Festl",
        "Daniel Watzenig"
      ],
      "abstract": "With the flourishing development of intelligent warehousing systems, the\ntechnology of Automated Guided Vehicle (AGV) has experienced rapid growth.\nWithin intelligent warehousing environments, AGV is required to safely and\nrapidly plan an optimal path in complex and dynamic environments. Most research\nhas studied deep reinforcement learning to address this challenge. However, in\nthe environments with sparse extrinsic rewards, these algorithms often converge\nslowly, learn inefficiently or fail to reach the target. Random Network\nDistillation (RND), as an exploration enhancement, can effectively improve the\nperformance of proximal policy optimization, especially enhancing the\nadditional intrinsic rewards of the AGV agent which is in sparse reward\nenvironments. Moreover, most of the current research continues to use 2D grid\nmazes as experimental environments. These environments have insufficient\ncomplexity and limited action sets. To solve this limitation, we present\nsimulation environments of AGV path planning with continuous actions and\npositions for AGVs, so that it can be close to realistic physical scenarios.\nBased on our experiments and comprehensive analysis of the proposed method, the\nresults demonstrate that our proposed method enables AGV to more rapidly\ncomplete path planning tasks with continuous actions in our environments. A\nvideo of part of our experiments can be found at https://youtu.be/lwrY9YesGmw.",
      "tldr_zh": "本研究针对自动化引导车辆（AGV）在复杂动态环境中进行路径规划的问题，提出了一种基于 Random Network Distillation (RND) 的深度强化学习方法，以提升 Proximal Policy Optimization (PPO) 在稀疏奖励环境下的探索效率和学习性能。RND 通过提供额外的内在奖励，帮助 AGV 代理更有效地处理路径规划任务，同时引入支持连续动作和位置的模拟环境，以模拟更真实的物理场景。实验结果显示，该方法使 AGV 能够更快地完成路径规划任务，并在综合分析中证明其优越性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.12594v1",
      "published_date": "2024-04-19 02:52:56 UTC",
      "updated_date": "2024-04-19 02:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:03:54.338176"
    },
    {
      "arxiv_id": "2404.12587v1",
      "title": "Reinforcement Learning Approach for Integrating Compressed Contexts into Knowledge Graphs",
      "title_zh": "一种将压缩上下文整合到知识图谱的强化学习方法",
      "authors": [
        "Ngoc Quach",
        "Qi Wang",
        "Zijun Gao",
        "Qifeng Sun",
        "Bo Guan",
        "Lillian Floyd"
      ],
      "abstract": "The widespread use of knowledge graphs in various fields has brought about a\nchallenge in effectively integrating and updating information within them. When\nit comes to incorporating contexts, conventional methods often rely on rules or\nbasic machine learning models, which may not fully grasp the complexity and\nfluidity of context information. This research suggests an approach based on\nreinforcement learning (RL), specifically utilizing Deep Q Networks (DQN) to\nenhance the process of integrating contexts into knowledge graphs. By\nconsidering the state of the knowledge graph as environment states defining\nactions as operations for integrating contexts and using a reward function to\ngauge the improvement in knowledge graph quality post-integration, this method\naims to automatically develop strategies for optimal context integration. Our\nDQN model utilizes networks as function approximators, continually updating Q\nvalues to estimate the action value function, thus enabling effective\nintegration of intricate and dynamic context information. Initial experimental\nfindings show that our RL method outperforms techniques in achieving precise\ncontext integration across various standard knowledge graph datasets,\nhighlighting the potential and effectiveness of reinforcement learning in\nenhancing and managing knowledge graphs.",
      "tldr_zh": "本研究针对知识图谱中整合和更新复杂动态上下文信息的挑战，提出了一种基于强化学习(Reinforcement Learning, RL)的方法，使用Deep Q Networks (DQN)来优化整合过程。方法将知识图谱的状态视为环境状态，将整合操作定义为动作，并通过奖励函数评估整合后图谱质量的提升，从而自动学习最佳策略。实验结果显示，该RL方法在多个标准知识图谱数据集上实现了比传统技术更高的精确性，证明了其在提升知识图谱管理方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by the 2024 International Conference on\n  Machine Learning and Neural Networks (MLNN 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.12587v1",
      "published_date": "2024-04-19 02:32:43 UTC",
      "updated_date": "2024-04-19 02:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:04:05.268541"
    },
    {
      "arxiv_id": "2404.12580v1",
      "title": "iTBLS: A Dataset of Interactive Conversations Over Tabular Information",
      "title_zh": "翻译失败",
      "authors": [
        "Anirudh Sundar",
        "Christopher Richardson",
        "William Gay",
        "Larry Heck"
      ],
      "abstract": "This paper introduces Interactive Tables (iTBLS), a dataset of interactive\nconversations situated in tables from scientific articles. This dataset is\ndesigned to facilitate human-AI collaborative problem-solving through\nAI-powered multi-task tabular capabilities. In contrast to prior work that\nmodels interactions as factoid QA or procedure synthesis, iTBLS broadens the\nscope of interactions to include mathematical reasoning, natural language\nmanipulation, and expansion of existing tables from natural language\nconversation by delineating interactions into one of three tasks:\ninterpretation, modification, or generation. Additionally, the paper presents a\nsuite of baseline approaches to iTBLS, utilizing zero-shot prompting and\nparameter-efficient fine-tuning for different computing situations. We also\nintroduce a novel multi-step approach and show how it can be leveraged in\nconjunction with parameter-efficient fine-tuning to achieve the\nstate-of-the-art on iTBLS; outperforming standard parameter-efficient\nfine-tuning by up to 15% on interpretation, 18% on modification, and 38% on\ngeneration.",
      "tldr_zh": "本论文引入了 iTBLS 数据集，这是一个基于科学文章表格的交互式对话数据集，旨在促进人类-AI 协作问题解决，并扩展 AI 的多任务表格能力。不同于以往将交互局限于事实 QA 或程序合成，iTBLS 将对话分类为 interpretation（解释）、modification（修改）和 generation（生成）三类任务，包括数学推理、自然语言操作和表格扩展。论文评估了基线方法，如 zero-shot prompting 和 parameter-efficient fine-tuning，并提出了一种新颖的多步方法，与 parameter-efficient fine-tuning 结合后，在 iTBLS 上实现了 state-of-the-art 性能，分别比标准方法提升 15% 在 interpretation、18% 在 modification 和 38% 在 generation 上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.12580v1",
      "published_date": "2024-04-19 02:11:41 UTC",
      "updated_date": "2024-04-19 02:11:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:04:19.094628"
    },
    {
      "arxiv_id": "2404.12569v1",
      "title": "Multi-View Subgraph Neural Networks: Self-Supervised Learning with Scarce Labeled Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenzhong Wang",
        "Qingyuan Zeng",
        "Wanyu Lin",
        "Min Jiang",
        "Kay Chen Tan"
      ],
      "abstract": "While graph neural networks (GNNs) have become the de-facto standard for\ngraph-based node classification, they impose a strong assumption on the\navailability of sufficient labeled samples. This assumption restricts the\nclassification performance of prevailing GNNs on many real-world applications\nsuffering from low-data regimes. Specifically, features extracted from scarce\nlabeled nodes could not provide sufficient supervision for the unlabeled\nsamples, leading to severe over-fitting. In this work, we point out that\nleveraging subgraphs to capture long-range dependencies can augment the\nrepresentation of a node with homophily properties, thus alleviating the\nlow-data regime. However, prior works leveraging subgraphs fail to capture the\nlong-range dependencies among nodes. To this end, we present a novel\nself-supervised learning framework, called multi-view subgraph neural networks\n(Muse), for handling long-range dependencies. In particular, we propose an\ninformation theory-based identification mechanism to identify two types of\nsubgraphs from the views of input space and latent space, respectively. The\nformer is to capture the local structure of the graph, while the latter\ncaptures the long-range dependencies among nodes. By fusing these two views of\nsubgraphs, the learned representations can preserve the topological properties\nof the graph at large, including the local structure and long-range\ndependencies, thus maximizing their expressiveness for downstream node\nclassification tasks. Experimental results show that Muse outperforms the\nalternative methods on node classification tasks with limited labeled data.",
      "tldr_zh": "本研究针对图神经网络(GNNs)在标签数据稀少时的过度拟合问题，提出了一种自监督学习框架Multi-View Subgraph Neural Networks (Muse)。Muse利用基于信息理论的机制，从输入空间和潜在空间识别两种子图：前者捕获图的局部结构，后者处理节点间的长程依赖性。通过融合这些多视图子图，框架增强了节点表示的表达性和拓扑属性，从而改善下游节点分类任务的性能。实验结果表明，Muse在标签数据有限的场景下，优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12569v1",
      "published_date": "2024-04-19 01:36:50 UTC",
      "updated_date": "2024-04-19 01:36:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:04:31.845194"
    },
    {
      "arxiv_id": "2404.12548v2",
      "title": "RetailOpt: Opt-In, Easy-to-Deploy Trajectory Estimation from Smartphone Motion Data and Retail Facility Information",
      "title_zh": "翻译失败",
      "authors": [
        "Ryo Yonetani",
        "Jun Baba",
        "Yasutaka Furukawa"
      ],
      "abstract": "We present RetailOpt, a novel opt-in, easy-to-deploy system for tracking\ncustomer movements offline in indoor retail environments. The system uses\nreadily accessible information from customer smartphones and retail apps,\nincluding motion data, store maps, and purchase records. This eliminates the\nneed for additional hardware installations/maintenance and ensures customers\nfull data control. Specifically, RetailOpt first uses inertial navigation to\nrecover relative trajectories from smartphone motion data. The store map and\npurchase records are cross-referenced to identify a list of visited shelves,\nproviding anchors to localize the relative trajectories in a store through\ncontinuous and discrete optimization. We demonstrate the effectiveness of our\nsystem in five diverse environments. The system, if successful, would produce\naccurate customer movement data, essential for a broad range of retail\napplications including customer behavior analysis and in-store navigation.",
      "tldr_zh": "本研究提出 RetailOpt，一种用户选择加入(opt-in)的易部署系统，用于从智能手机运动数据和零售设施信息中估计室内零售环境的客户轨迹。该系统利用 inertial navigation 从运动数据恢复相对轨迹，并通过交叉引用商店地图和购买记录进行连续和离散 optimization，以精准定位轨迹，从而避免了额外硬件安装。实验在五个多样化环境中验证了系统的有效性，能够生成准确的客户运动数据，支持零售应用如客户行为分析和店内导航。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.12548v2",
      "published_date": "2024-04-19 00:03:49 UTC",
      "updated_date": "2024-07-16 00:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:04:41.323642"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T02:05:06.041973"
}
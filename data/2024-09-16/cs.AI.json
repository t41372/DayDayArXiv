{
  "date": "2024-09-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-16 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和机器学习领域，尤其是大语言模型（LLMs）的创新应用、机器人导航和医疗图像处理等主题，令人印象深刻的是如 Qingru Zhang 等作者提出的 LLM 注意力机制优化，以及 NeurIPS 接受的多代理 AI 安全框架，这些工作展示了 AI 在实际问题解决中的潜力。\n\n### 重点论文讨论\n我们挑选了部分重要、话题性和有影响力的论文进行详细讨论，先从 AI 和 LLMs 相关的内容入手，再聊机器人和医疗领域。其他论文如环境监测或小众技术细节，将快速掠过。\n\n**AI 和 LLMs 领域（高话题度，相关论文较多）**  \n- **Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering（模型自动引导注意力：忠实性与自动注意力引导）**  \n  作者包括 Qingru Zhang 和 Jianfeng Gao 等知名学者。该论文提出 AutoPASTA 方法，通过自动识别关键上下文并显式引导 LLM 的注意力分数，提升模型在长文本问答中的忠实性和性能。主要贡献：在不改变模型参数的情况下，提高了 LLAMA3-70B-Instruct 的性能，平均改善 7.95%。这为 LLM 的可解释性和鲁棒性提供了新视角。\n\n- **VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching（VulnLLMEval：评估 LLM 在软件漏洞检测和修复中的框架）**  \n  该工作引入 VulnLLMEval 框架，使用 307 个真实 Linux 内核漏洞数据集评估 LLM。主要发现：LLM 在区分漏洞代码和修复代码时表现不佳，常过度简化修复方案，强调了 LLM 在安全领域的局限性。\n\n- **AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing（AutoSafeCoder：通过静态分析和模糊测试的多代理框架保护 LLM 代码生成）**  \n  论文提出多代理框架，包括代码生成、静态分析和模糊测试代理，在 SecurityEval 数据集上实现了 13% 的漏洞减少。主要贡献：NeurIPS 2024 接受，展示了 LLM 在安全代码生成中的潜力。\n\n- **Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models（Playground v3：使用深度融合 LLM 提升文本到图像对齐）**  \n  该论文开发了新文本到图像模型，使用 decoder-only LLM 提升图像生成质量，并在 CapsBench 上表现突出。主要发现：模型在文本渲染和多语言理解上优于现有方法，项目页面提供更多细节。\n\n- **DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis（DreamHead：通过层次扩散学习空间-时间对应性用于音频驱动的头部合成）**  \n  工作提出层次扩散模型，生成音频驱动的头部视频序列。主要贡献：在 MotIF-1K 数据集上，模型在精确性和召回率上双双提升，实现更自然的视频合成。\n\n其他如 **Generalized Measures of Anticipation and Responsivity in Online Language Processing（在线语言处理中的广义预期和响应度量）** 和 **Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation（Lab-AI：使用检索增强提升语言模型的个性化实验室测试解释）** 等，快速提一下：前者扩展了信息理论度量提升语言模型预测，后者通过 RAG 技术提高医疗测试解释的准确性（F1 分数达 0.948），但细节较技术化。\n\n**机器人和导航领域（应用性强，相关论文有突破）**  \n- **MotIF: Motion Instruction Fine-tuning（MotIF：运动指令微调）**  \n  论文提出微调视觉语言模型以捕捉机器人轨迹信息，用于任务如刷头发。主要发现：在 MotIF-1K 数据集上，模型在机器人运动检测上精度和召回率均翻倍，提升了机器人规划的鲁棒性。\n\n- **Multi-agent Path Finding in Continuous Environment（多代理路径寻找在连续环境中的应用）**  \n  引入 CE-CBS 算法，将冲突搜索与 RRT* 结合，用于连续环境路径规划。主要贡献：在各种实例上表现出色，适用于复杂机器人导航。\n\n- **Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning（使用深度强化学习解耦不确定性以实现安全社交导航）**  \n  该工作整合不确定性估计到 DRL 框架中，提高机器人社交导航安全性。主要发现：在扰动环境中减少碰撞，展示了 DRL 在人机交互中的潜力。\n\n**医疗和生物领域（实际影响大）**  \n- **Self-supervised Speech Models for Word-Level Stuttered Speech Detection（自监督语音模型用于词级口吃检测）**  \n  论文构建词级口吃数据集，并使用自监督模型提升检测精度。主要贡献：在 IEEE SLT 2024 接受，模型在临床口吃筛查中超越现有方法。\n\n- **Exploring 3D Face Reconstruction and Fusion Methods for Face Verification（探索 3D 人脸重建和融合方法用于人脸验证）**  \n  该工作使用多个 3D 重建算法融合，提升视频监控中的人脸验证准确性。主要发现：在 ECCV 2024 相关工作坊中表现突出，适用于安全监控。\n\n其他医疗论文如 **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration（基于提示学习和 BERT 集成的知识增强疾病诊断方法）**，在公共数据集上 F1 分数提升 2.4%-4.2%，但整体较常规；**MOST: MR reconstruction Optimization for multiple downStream Tasks（MOST：多下游任务的 MR 重建优化）** 通过持续学习优化 MRI 重建，快速提一下其在医疗图像处理中的实用性。\n\n**其他领域（快速掠过）**  \n剩余论文涉及环境监测、财务和杂项，如 **A Green Multi-Attribute Client Selection for Over-The-Air Federated Learning（绿色多属性客户端选择用于空中联邦学习）** 优化了联邦学习能效；**Do Pre-trained Vision-Language Models Encode Object States?（预训练视觉语言模型是否编码物体状态？）** 评估了 VLM 在物体状态识别中的局限性。这些工作虽有贡献，但影响力较小，仅供参考。\n\n今天的 arXiv 更新展示了 AI 在多领域的创新潜力，LLM 和机器人相关论文尤其值得关注。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2409.10790v1",
      "title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering",
      "title_zh": "翻译失败",
      "authors": [
        "Qingru Zhang",
        "Xiaodong Yu",
        "Chandan Singh",
        "Xiaodong Liu",
        "Liyuan Liu",
        "Jianfeng Gao",
        "Tuo Zhao",
        "Dan Roth",
        "Hao Cheng"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA .",
      "tldr_zh": "大语言模型 (LLMs) 在处理长上下文时常出现不忠实 (unfaithful) 或幻觉 (hallucinated) 问题，现有的提示 (prompting) 方法如迭代提示只能隐式突出关键信息，效果有限。论文提出 AutoPASTA 方法，该方法在推理时自动识别关键上下文信息，并通过显式 steering LLMs 的注意力分数来突出重要部分，而无需修改模型参数。实验在 open-book QA 任务上证明，AutoPASTA 显著提升了模型的忠实度和性能，例如 LLAMA3-70B-Instruct 模型平均改善 7.95%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10790v1",
      "published_date": "2024-09-16 23:52:41 UTC",
      "updated_date": "2024-09-16 23:52:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:17:31.570847"
    },
    {
      "arxiv_id": "2409.10775v1",
      "title": "Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?",
      "title_zh": "深度学习模型在视觉识别任务中对部分对象遮挡是否鲁棒？",
      "authors": [
        "Kaleb Kassaw",
        "Francesco Luzi",
        "Leslie M. Collins",
        "Jordan M. Malof"
      ],
      "abstract": "Image classification models, including convolutional neural networks (CNNs),\nperform well on a variety of classification tasks but struggle under conditions\nof partial occlusion, i.e., conditions in which objects are partially covered\nfrom the view of a camera. Methods to improve performance under occlusion,\nincluding data augmentation, part-based clustering, and more inherently robust\narchitectures, including Vision Transformer (ViT) models, have, to some extent,\nbeen evaluated on their ability to classify objects under partial occlusion.\nHowever, evaluations of these methods have largely relied on images containing\nartificial occlusion, which are typically computer-generated and therefore\ninexpensive to label. Additionally, methods are rarely compared against each\nother, and many methods are compared against early, now outdated, deep learning\nmodels. We contribute the Image Recognition Under Occlusion (IRUO) dataset,\nbased on the recently developed Occluded Video Instance Segmentation (OVIS)\ndataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded\nimages to test and benchmark leading methods' robustness to partial occlusion\nin visual recognition tasks. In addition, we contribute the design and results\nof a human study using images from IRUO that evaluates human classification\nperformance at multiple levels and types of occlusion. We find that modern\nCNN-based models show improved recognition accuracy on occluded images compared\nto earlier CNN-based models, and ViT-based models are more accurate than\nCNN-based models on occluded images, performing only modestly worse than human\naccuracy. We also find that certain types of occlusion, including diffuse\nocclusion, where relevant objects are seen through \"holes\" in occluders such as\nfences and leaves, can greatly reduce the accuracy of deep recognition models\nas compared to humans, especially those with CNN backbones.",
      "tldr_zh": "这篇论文探讨了深度学习模型（如CNN和ViT）在视觉识别任务中对部分物体遮挡的鲁棒性问题，指出现有方法（如数据增强和基于部分的聚类）主要依赖人工遮挡图像，且缺乏全面比较。论文的主要贡献是引入IRUO数据集（基于OVIS数据集），包含真实和人工遮挡图像，用于基准测试模型性能，并设计了人类研究来评估人类在不同遮挡水平下的分类准确率。结果显示，现代CNN模型比早期模型有显著改善，ViT模型的表现更优，几乎接近人类水平，但某些遮挡类型（如弥散遮挡）会极大降低模型准确性，尤其是CNN骨干网络。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10775v1",
      "published_date": "2024-09-16 23:21:22 UTC",
      "updated_date": "2024-09-16 23:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:17:53.331154"
    },
    {
      "arxiv_id": "2409.10756v1",
      "title": "VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching",
      "title_zh": "翻译失败",
      "authors": [
        "Arastoo Zibaeirad",
        "Marco Vieira"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in tasks like code\ntranslation, prompting interest in their potential for automating software\nvulnerability detection (SVD) and patching (SVP). To further research in this\narea, establishing a benchmark is essential for evaluating the strengths and\nlimitations of LLMs in these tasks. Despite their capabilities, questions\nremain regarding whether LLMs can accurately analyze complex vulnerabilities\nand generate appropriate patches. This paper introduces VulnLLMEval, a\nframework designed to assess the performance of LLMs in identifying and\npatching vulnerabilities in C code. Our study includes 307 real-world\nvulnerabilities extracted from the Linux kernel, creating a well-curated\ndataset that includes both vulnerable and patched code. This dataset, based on\nreal-world code, provides a diverse and representative testbed for evaluating\nLLM performance in SVD and SVP tasks, offering a robust foundation for rigorous\nassessment. Our results reveal that LLMs often struggle with distinguishing\nbetween vulnerable and patched code. Furthermore, in SVP tasks, these models\ntend to oversimplify the code, producing solutions that may not be directly\nusable without further refinement.",
      "tldr_zh": "该论文引入了 VulnLLMEval 框架，用于评估 Large Language Models (LLMs) 在软件漏洞检测 (SVD) 和修补 (SVP) 任务中的性能。框架基于从 Linux 内核提取的 307 个真实漏洞创建了一个数据集，包括漏洞代码和修补代码，提供了一个多样化和代表性的测试环境。研究发现，LLMs 常常难以准确区分漏洞和修补代码，并在 SVP 任务中倾向于过度简化解决方案，导致生成的补丁可能需要进一步改进。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10756v1",
      "published_date": "2024-09-16 22:00:20 UTC",
      "updated_date": "2024-09-16 22:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:17:55.088799"
    },
    {
      "arxiv_id": "2409.10737v2",
      "title": "AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Ana Nunez",
        "Nafis Tanveer Islam",
        "Sumit Kumar Jha",
        "Peyman Najafirad"
      ],
      "abstract": "Recent advancements in automatic code generation using large language models\n(LLMs) have brought us closer to fully automated secure software development.\nHowever, existing approaches often rely on a single agent for code generation,\nwhich struggles to produce secure, vulnerability-free code. Traditional program\nsynthesis with LLMs has primarily focused on functional correctness, often\nneglecting critical dynamic security implications that happen during runtime.\nTo address these challenges, we propose AutoSafeCoder, a multi-agent framework\nthat leverages LLM-driven agents for code generation, vulnerability analysis,\nand security enhancement through continuous collaboration. The framework\nconsists of three agents: a Coding Agent responsible for code generation, a\nStatic Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent\nperforming dynamic testing using a mutation-based fuzzing approach to detect\nruntime errors. Our contribution focuses on ensuring the safety of multi-agent\ncode generation by integrating dynamic and static testing in an iterative\nprocess during code generation by LLM that improves security. Experiments using\nthe SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities\ncompared to baseline LLMs, with no compromise in functionality.",
      "tldr_zh": "该研究提出AutoSafeCoder，一个多代理框架，旨在通过静态分析和模糊测试（Fuzz Testing）提升LLM驱动的代码生成安全性，解决现有方法忽略运行时漏洞的问题。框架包括三个代理：Coding Agent负责生成代码、Static Analyzer Agent识别静态漏洞，以及Fuzzing Agent通过基于突变的动态测试检测运行时错误，这些代理通过迭代协作实现代码的安全增强。实验在SecurityEval数据集上显示，与基线LLM相比，AutoSafeCoder减少了13%的代码漏洞，同时保持了功能完整性，为自动安全软件开发提供了重要贡献。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to NeurIPS 2024 Workshop on Safe & Trustworthy Agents",
      "pdf_url": "http://arxiv.org/pdf/2409.10737v2",
      "published_date": "2024-09-16 21:15:56 UTC",
      "updated_date": "2024-11-05 03:00:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:18:06.148569"
    },
    {
      "arxiv_id": "2409.10728v2",
      "title": "Generalized Measures of Anticipation and Responsivity in Online Language Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Mario Giulianelli",
        "Andreas Opedal",
        "Ryan Cotterell"
      ],
      "abstract": "We introduce a generalization of classic information-theoretic measures of\npredictive uncertainty in online language processing, based on the simulation\nof expected continuations of incremental linguistic contexts. Our framework\nprovides a formal definition of anticipatory and responsive measures, and it\nequips experimenters with the tools to define new, more expressive measures\nbeyond standard next-symbol entropy and surprisal. While extracting these\nstandard quantities from language models is convenient, we demonstrate that\nusing Monte Carlo simulation to estimate alternative responsive and\nanticipatory measures pays off empirically: New special cases of our\ngeneralized formula exhibit enhanced predictive power compared to surprisal for\nhuman cloze completion probability as well as ELAN, LAN, and N400 amplitudes,\nand greater complementarity with surprisal in predicting reading times.",
      "tldr_zh": "该论文提出了一种泛化框架，通过模拟预期连续性来扩展在线语言处理中的信息理论措施，正式定义了anticipatory和responsive指标。该框架允许研究者定义超越standard next-symbol entropy和surprisal的新型、更具表现力的措施，并使用Monte Carlo simulation进行估计。实验结果表明，这些新措施在预测人类cloze completion probability、ELAN、LAN和N400 amplitudes方面比surprisal表现出更高的预测能力和互补性，并在解释阅读时间上更具优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10728v2",
      "published_date": "2024-09-16 21:05:15 UTC",
      "updated_date": "2024-10-12 15:28:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:18:18.887679"
    },
    {
      "arxiv_id": "2409.10721v1",
      "title": "A Missing Data Imputation GAN for Character Sprite Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Flávio Coutinho",
        "Luiz Chaimowicz"
      ],
      "abstract": "Creating and updating pixel art character sprites with many frames spanning\ndifferent animations and poses takes time and can quickly become repetitive.\nHowever, that can be partially automated to allow artists to focus on more\ncreative tasks. In this work, we concentrate on creating pixel art character\nsprites in a target pose from images of them facing other three directions. We\npresent a novel approach to character generation by framing the problem as a\nmissing data imputation task. Our proposed generative adversarial networks\nmodel receives the images of a character in all available domains and produces\nthe image of the missing pose. We evaluated our approach in the scenarios with\none, two, and three missing images, achieving similar or better results to the\nstate-of-the-art when more images are available. We also evaluate the impact of\nthe proposed changes to the base architecture.",
      "tldr_zh": "这篇论文提出了一种基于生成对抗网络（GAN）的模型，用于从现有方向图像生成像素艺术角色精灵（character sprites）的缺失姿势，将问题 framing 为 missing data imputation 任务，以自动化创作过程并减少艺术家的重复性工作。模型接受角色在可用方向的图像作为输入，输出缺失姿势的图像，并在缺失一、二或三个图像的场景下进行了评估。结果显示，该方法在更多图像可用时，表现与或优于 state-of-the-art 模型，同时评估了架构修改对性能的影响。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in SBGames 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10721v1",
      "published_date": "2024-09-16 20:50:32 UTC",
      "updated_date": "2024-09-16 20:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:18:30.553193"
    },
    {
      "arxiv_id": "2409.10715v2",
      "title": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models",
      "title_zh": "自注意力机制限制了基于Transformer模型的工作记忆容量",
      "authors": [
        "Dongyu Gong",
        "Hantao Zhang"
      ],
      "abstract": "Recent work on Transformer-based large language models (LLMs) has revealed\nstriking limits in their working memory capacity, similar to what has been\nfound in human behavioral studies. Specifically, these models' performance\ndrops significantly on N-back tasks as N increases. However, there is still a\nlack of mechanistic interpretability as to why this phenomenon would arise.\nInspired by the executive attention theory from behavioral sciences, we\nhypothesize that the self-attention mechanism within Transformer-based models\nmight be responsible for their working memory capacity limits. To test this\nhypothesis, we train vanilla decoder-only transformers to perform N-back tasks\nand find that attention scores gradually aggregate to the N-back positions over\ntraining, suggesting that the model masters the task by learning a strategy to\npay attention to the relationship between the current position and the N-back\nposition. Critically, we find that the total entropy of the attention score\nmatrix increases as N increases, suggesting that the dispersion of attention\nscores might be the cause of the capacity limit observed in N-back tasks. Our\nfindings thus offer insights into the shared role of attention in both human\nand artificial intelligence. Moreover, the limitations of the self-attention\nmechanism revealed in the current study could inform future efforts to design\nmore powerful model architectures with enhanced working memory capacity and\ncognitive capabilities.",
      "tldr_zh": "这篇论文揭示了Transformer-based模型的工作记忆容量限制，特别是在N-back任务中，随着N增加，模型性能显著下降，与人类行为类似。研究假设self-attention机制是主要原因，通过训练vanilla decoder-only transformers进行N-back任务，观察到注意力分数逐渐聚集到N-back位置，且注意力分数矩阵的总entropy随N增加而上升，导致容量限制。结果为理解人类和AI中注意力的共同作用提供了洞见，并建议未来优化self-attention机制以设计更强大、认知能力更强的模型架构。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10715v2",
      "published_date": "2024-09-16 20:38:35 UTC",
      "updated_date": "2024-11-16 20:50:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:18:42.941792"
    },
    {
      "arxiv_id": "2409.18986v2",
      "title": "Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Wang",
        "Haoyong Ouyang",
        "Balu Bhasuran",
        "Xiao Luo",
        "Karim Hanna",
        "Mia Liza A. Lustria",
        "Carl Yang",
        "Zhe He"
      ],
      "abstract": "Accurate interpretation of lab results is crucial in clinical medicine, yet\nmost patient portals use universal normal ranges, ignoring conditional factors\nlike age and gender. This study introduces Lab-AI, an interactive system that\noffers personalized normal ranges using retrieval-augmented generation (RAG)\nfrom credible health sources. Lab-AI has two modules: factor retrieval and\nnormal range retrieval. We tested these on 122 lab tests: 40 with conditional\nfactors and 82 without. For tests with factors, normal ranges depend on\npatient-specific information. Our results show GPT-4-turbo with RAG achieved a\n0.948 F1 score for factor retrieval and 0.995 accuracy for normal range\nretrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%\nin factor retrieval and showed 132% and 100% improvements in question-level and\nlab-level performance, respectively, for normal range retrieval. These findings\nhighlight Lab-AI's potential to enhance patient understanding of lab results.",
      "tldr_zh": "该研究针对临床医学中实验室结果解释的局限性，引入Lab-AI系统，利用Retrieval Augmentation (RAG)技术从可靠健康来源获取个性化正常范围，以考虑年龄、性别等条件因素。Lab-AI包括两个模块：factor retrieval（因素检索）和normal range retrieval（正常范围检索），并在122个实验室测试上进行评估，其中40个测试涉及条件因素。结果显示，GPT-4-turbo结合RAG在因素检索中达到0.948的F1 score，在正常范围检索中达到0.995的准确率，比无RAG的最佳系统在因素检索中提升33.5%，并在正常范围检索的问题级和实验室级性能分别提升132%和100%。这些发现表明，Lab-AI有望显著改善患者对实验室结果的理解和决策。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18986v2",
      "published_date": "2024-09-16 20:36:17 UTC",
      "updated_date": "2025-04-23 19:19:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:18:55.478272"
    },
    {
      "arxiv_id": "2409.10704v1",
      "title": "Self-supervised Speech Models for Word-Level Stuttered Speech Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yi-Jen Shih",
        "Zoi Gkalitsiou",
        "Alexandros G. Dimakis",
        "David Harwath"
      ],
      "abstract": "Clinical diagnosis of stuttering requires an assessment by a licensed\nspeech-language pathologist. However, this process is time-consuming and\nrequires clinicians with training and experience in stuttering and fluency\ndisorders. Unfortunately, only a small percentage of speech-language\npathologists report being comfortable working with individuals who stutter,\nwhich is inadequate to accommodate for the 80 million individuals who stutter\nworldwide. Developing machine learning models for detecting stuttered speech\nwould enable universal and automated screening for stuttering, enabling speech\npathologists to identify and follow up with patients who are most likely to be\ndiagnosed with a stuttering speech disorder. Previous research in this area has\npredominantly focused on utterance-level detection, which is not sufficient for\nclinical settings where word-level annotation of stuttering is the norm. In\nthis study, we curated a stuttered speech dataset with word-level annotations\nand introduced a word-level stuttering speech detection model leveraging\nself-supervised speech models. Our evaluation demonstrates that our model\nsurpasses previous approaches in word-level stuttering speech detection.\nAdditionally, we conducted an extensive ablation analysis of our method,\nproviding insight into the most important aspects of adapting self-supervised\nspeech models for stuttered speech detection.",
      "tldr_zh": "本研究针对口吃诊断资源短缺的问题，提出使用Self-supervised Speech Models开发一种词汇级别（Word-Level）口吃语音检测模型，以实现自动化筛查和支持临床应用。研究者整理了一个带有词汇级别标注的口吃语音数据集，并通过自监督学习方法训练模型，显著提升了检测准确性。实验结果显示，该模型在词汇级别口吃检测上优于现有方法，且通过广泛的消融分析，揭示了适应Self-supervised Speech Models的关键因素，为大规模口吃筛查提供了新工具。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by IEEE SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10704v1",
      "published_date": "2024-09-16 20:18:20 UTC",
      "updated_date": "2024-09-16 20:18:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:19:06.546808"
    },
    {
      "arxiv_id": "2409.10702v2",
      "title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Wang",
        "David Stevens",
        "Pranay Shah",
        "Wenwen Jiang",
        "Miao Liu",
        "Xu Chen",
        "Robert Kuo",
        "Na Li",
        "Boying Gong",
        "Daniel Lee",
        "Jiabo Hu",
        "Ning Zhang",
        "Bob Kamma"
      ],
      "abstract": "The growing demand for AI training data has transformed data annotation into\na global industry, but traditional approaches relying on human annotators are\noften time-consuming, labor-intensive, and prone to inconsistent quality. We\npropose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models\ninto the annotation process. Our research introduces a collaborative paradigm\nthat leverages the strengths of both professional human annotators and large\nlanguage models (LLMs). By employing LLMs as pre-annotation and real-time\nassistants, and judges on annotator responses, MILO enables effective\ninteraction patterns between human annotators and LLMs. Three empirical studies\non multimodal data annotation demonstrate MILO's efficacy in reducing handling\ntime, improving data quality, and enhancing annotator experiences. We also\nintroduce quality rubrics for flexible evaluation and fine-grained feedback on\nopen-ended annotations. The MILO framework has implications for accelerating\nAI/ML development, reducing reliance on human annotation alone, and promoting\nbetter alignment between human and machine values.",
      "tldr_zh": "该研究提出Model-in-the-Loop (MILO)框架，通过整合大型语言模型(LLMs)来加速多模态AI数据标注过程，解决传统人类标注的耗时、劳动密集和质量不一致问题。MILO采用协作模式，让LLMs担任预标注、实时助手和响应判断者，与专业人类标注者互动，以提升效率和准确性。三个实证研究显示，该框架显著减少处理时间、提高数据质量并改善标注者体验，同时引入灵活的质量评估标准。总体而言，MILO有助于加速AI/ML开发，减少对纯人类标注的依赖，并促进人类与机器价值的更好对齐。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10702v2",
      "published_date": "2024-09-16 20:05:57 UTC",
      "updated_date": "2024-09-24 05:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:19:18.984262"
    },
    {
      "arxiv_id": "2409.11442v1",
      "title": "A Green Multi-Attribute Client Selection for Over-The-Air Federated Learning: A Grey-Wolf-Optimizer Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Maryam Ben Driss",
        "Essaid Sabir",
        "Halima Elbiaze",
        "Abdoulaye Baniré Diallo",
        "Mohamed Sadik"
      ],
      "abstract": "Federated Learning (FL) has gained attention across various industries for\nits capability to train machine learning models without centralizing sensitive\ndata. While this approach offers significant benefits such as privacy\npreservation and decreased communication overhead, it presents several\nchallenges, including deployment complexity and interoperability issues,\nparticularly in heterogeneous scenarios or resource-constrained environments.\nOver-the-air (OTA) FL was introduced to tackle these challenges by\ndisseminating model updates without necessitating direct device-to-device\nconnections or centralized servers. However, OTA-FL brought forth limitations\nassociated with heightened energy consumption and network latency. In this\npaper, we propose a multi-attribute client selection framework employing the\ngrey wolf optimizer (GWO) to strategically control the number of participants\nin each round and optimize the OTA-FL process while considering accuracy,\nenergy, delay, reliability, and fairness constraints of participating devices.\nWe evaluate the performance of our multi-attribute client selection approach in\nterms of model loss minimization, convergence time reduction, and energy\nefficiency. In our experimental evaluation, we assessed and compared the\nperformance of our approach against the existing state-of-the-art methods. Our\nresults demonstrate that the proposed GWO-based client selection outperforms\nthese baselines across various metrics. Specifically, our approach achieves a\nnotable reduction in model loss, accelerates convergence time, and enhances\nenergy efficiency while maintaining high fairness and reliability indicators.",
      "tldr_zh": "本研究针对Over-the-Air Federated Learning (OTA-FL)中的能源消耗和网络延迟挑战，提出了一种基于Grey Wolf Optimizer (GWO)的多属性客户端选择框架，以优化参与设备的数量并考虑准确性、能源、延迟、可靠性和公平性约束。\n该框架通过战略性控制客户端参与来改善Federated Learning (FL)过程，确保模型更新更高效。\n实验结果表明，与现有基线方法相比，该方法显著降低了模型损失、缩短了收敛时间，并提升了能源效率，同时维持了高公平性和可靠性指标。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11442v1",
      "published_date": "2024-09-16 20:03:57 UTC",
      "updated_date": "2024-09-16 20:03:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:19:30.770240"
    },
    {
      "arxiv_id": "2409.10695v2",
      "title": "Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models",
      "title_zh": "Playground v3：通过深度融合大型语言模型改进文本到图像对齐",
      "authors": [
        "Bingchen Liu",
        "Ehsan Akhgari",
        "Alexander Visheratin",
        "Aleks Kamko",
        "Linmiao Xu",
        "Shivam Shrirao",
        "Chase Lambert",
        "Joao Souza",
        "Suhail Doshi",
        "Daiqing Li"
      ],
      "abstract": "We introduce Playground v3 (PGv3), our latest text-to-image model that\nachieves state-of-the-art (SoTA) performance across multiple testing\nbenchmarks, excels in graphic design abilities and introduces new capabilities.\nUnlike traditional text-to-image generative models that rely on pre-trained\nlanguage models like T5 or CLIP text encoders, our approach fully integrates\nLarge Language Models (LLMs) with a novel structure that leverages text\nconditions exclusively from a decoder-only LLM. Additionally, to enhance image\ncaptioning quality-we developed an in-house captioner, capable of generating\ncaptions with varying levels of detail, enriching the diversity of text\nstructures. We also introduce a new benchmark CapsBench to evaluate detailed\nimage captioning performance. Experimental results demonstrate that PGv3 excels\nin text prompt adherence, complex reasoning, and accurate text rendering. User\npreference studies indicate the super-human graphic design ability of our model\nfor common design applications, such as stickers, posters, and logo designs.\nFurthermore, PGv3 introduces new capabilities, including precise RGB color\ncontrol and robust multilingual understanding.",
      "tldr_zh": "我们引入了 Playground v3，一种先进的文本到图像模型，通过整合 Deep-Fusion Large Language Models（LLMs）并使用仅解码器的 LLM 处理文本条件，显著提升了文本提示遵守和图像生成质量。模型还开发了内部图像标题生成器，能够生成不同详细程度的标题，并引入了新基准 CapsBench 来评估详细图像标题性能。实验结果显示，Playground v3 在多个基准测试中达到最先进水平，用户偏好研究证实其在图形设计应用（如贴纸、海报和标志设计）上具备超人类能力，并新增了精确 RGB 颜色控制和强大多语言理解功能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://playground.com/pg-v3",
      "pdf_url": "http://arxiv.org/pdf/2409.10695v2",
      "published_date": "2024-09-16 19:52:24 UTC",
      "updated_date": "2024-10-21 20:01:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:19:43.691018"
    },
    {
      "arxiv_id": "2409.10692v1",
      "title": "Encoding Reusable Multi-Robot Planning Strategies as Abstract Hypergraphs",
      "title_zh": "将可重用的多机器人规划策略编码为抽象超图",
      "authors": [
        "Khen Elimelech",
        "James Motes",
        "Marco Morales",
        "Nancy M. Amato",
        "Moshe Y. Vardi",
        "Lydia E. Kavraki"
      ],
      "abstract": "Multi-Robot Task Planning (MR-TP) is the search for a discrete-action plan a\nteam of robots should take to complete a task. The complexity of such problems\nscales exponentially with the number of robots and task complexity, making them\nchallenging for online solution. To accelerate MR-TP over a system's lifetime,\nthis work looks at combining two recent advances: (i) Decomposable State Space\nHypergraph (DaSH), a novel hypergraph-based framework to efficiently model and\nsolve MR-TP problems; and \\mbox{(ii) learning-by-abstraction,} a technique that\nenables automatic extraction of generalizable planning strategies from\nindividual planning experiences for later reuse. Specifically, we wish to\nextend this strategy-learning technique, originally designed for single-robot\nplanning, to benefit multi-robot planning using hypergraph-based MR-TP.",
      "tldr_zh": "本文提出了一种将可重用多机器人任务规划(Multi-Robot Task Planning, MR-TP)策略编码为抽象超图(Abstract Hypergraphs)的方法，以应对任务复杂性和机器人数量带来的指数级挑战。研究结合了Decomposable State Space Hypergraph (DaSH)框架，用于高效建模和解决MR-TP问题，以及learning-by-abstraction技术，将其从单机器人规划扩展到多机器人场景，从而从个体规划经验中自动提取可泛化策略。最终，这提高了MR-TP在系统生命周期内的在线解决效率，为多机器人系统提供更具可复用性的规划策略。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10692v1",
      "published_date": "2024-09-16 19:39:52 UTC",
      "updated_date": "2024-09-16 19:39:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:19:55.109873"
    },
    {
      "arxiv_id": "2409.10683v1",
      "title": "MotIF: Motion Instruction Fine-tuning",
      "title_zh": "MotIF：运动指令微调",
      "authors": [
        "Minyoung Hwang",
        "Joey Hejna",
        "Dorsa Sadigh",
        "Yonatan Bisk"
      ],
      "abstract": "While success in many robotics tasks can be determined by only observing the\nfinal state and how it differs from the initial state - e.g., if an apple is\npicked up - many tasks require observing the full motion of the robot to\ncorrectly determine success. For example, brushing hair requires repeated\nstrokes that correspond to the contours and type of hair. Prior works often use\noff-the-shelf vision-language models (VLMs) as success detectors; however, when\nsuccess depends on the full trajectory, VLMs struggle to make correct judgments\nfor two reasons. First, modern VLMs are trained only on single frames, and\ncannot capture changes over a full trajectory. Second, even if we provide\nstate-of-the-art VLMs with an aggregate input of multiple frames, they still\nfail to detect success due to a lack of robot data. Our key idea is to\nfine-tune VLMs using abstract representations that are able to capture\ntrajectory-level information such as the path the robot takes by overlaying\nkeypoint trajectories on the final image. We propose motion instruction\nfine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned\nabstract representations to semantically ground the robot's behavior in the\nenvironment. To benchmark and fine-tune VLMs for robotic motion understanding,\nwe introduce the MotIF-1K dataset containing 653 human and 369 robot\ndemonstrations across 13 task categories. MotIF assesses the success of robot\nmotion given the image observation of the trajectory, task instruction, and\nmotion description. Our model significantly outperforms state-of-the-art VLMs\nby at least twice in precision and 56.1% in recall, generalizing across unseen\nmotions, tasks, and environments. Finally, we demonstrate practical\napplications of MotIF in refining and terminating robot planning, and ranking\ntrajectories on how they align with task and motion descriptions. Project page:\nhttps://motif-1k.github.io",
      "tldr_zh": "该论文提出MotIF（Motion Instruction Fine-tuning）方法，通过微调视觉语言模型（VLMs）来解决机器人任务中依赖全轨迹判断成功的挑战，该方法使用抽象表示（如在最终图像上叠加关键点轨迹）来捕捉运动路径信息。研究者引入MotIF-1K数据集，包含653个人类和369个机器人演示，覆盖13个任务类别，用于基准测试和模型训练。实验结果显示，MotIF模型在精度上至少提高一倍，召回率提升56.1%，并在未见过的运动、任务和环境中表现出色。最后，该方法在实际应用中可用于优化机器人规划、终止执行和对轨迹进行排名。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10683v1",
      "published_date": "2024-09-16 19:30:21 UTC",
      "updated_date": "2024-09-16 19:30:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:20:07.104571"
    },
    {
      "arxiv_id": "2409.10680v1",
      "title": "Multi-agent Path Finding in Continuous Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Kristýna Janovská",
        "Pavel Surynek"
      ],
      "abstract": "We address a variant of multi-agent path finding in continuous environment\n(CE-MAPF), where agents move along sets of smooth curves. Collisions between\nagents are resolved via avoidance in the space domain. A new Continuous\nEnvironment Conflict-Based Search (CE-CBS) algorithm is proposed in this work.\nCE-CBS combines conflict-based search (CBS) for the high-level search framework\nwith RRT* for low-level path planning. The CE-CBS algorithm is tested under\nvarious settings on diverse CE-MAPF instances. Experimental results show that\nCE-CBS is competitive w.r.t. to other algorithms that consider continuous\naspect in MAPF such as MAPF with continuous time.",
      "tldr_zh": "本论文探讨了连续环境中的多智能体路径寻找（CE-MAPF），其中代理沿平滑曲线移动，并通过空间域避免碰撞。作者提出了一种新算法Continuous Environment Conflict-Based Search (CE-CBS)，它将高层冲突-based search (CBS) 框架与底层RRT*路径规划相结合，以优化代理路径。实验结果表明，CE-CBS在各种CE-MAPF实例下的表现与考虑连续时间的其他MAPF算法相比具有竞争力。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "The 36th IEEE International Conference on Tools with Artificial\n  Intelligence (ICTAI). 2024, In press",
      "pdf_url": "http://arxiv.org/pdf/2409.10680v1",
      "published_date": "2024-09-16 19:23:04 UTC",
      "updated_date": "2024-09-16 19:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:20:18.560929"
    },
    {
      "arxiv_id": "2409.10655v2",
      "title": "Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning",
      "title_zh": "通过深度强化学习解耦不确定性以实现安全的社交导航",
      "authors": [
        "Daniel Flögel",
        "Marcos Gómez Villafañe",
        "Joshua Ransiek",
        "Sören Hohmann"
      ],
      "abstract": "Autonomous mobile robots are increasingly used in pedestrian-rich\nenvironments where safe navigation and appropriate human interaction are\ncrucial. While Deep Reinforcement Learning (DRL) enables socially integrated\nrobot behavior, challenges persist in novel or perturbed scenarios to indicate\nwhen and why the policy is uncertain. Unknown uncertainty in decision-making\ncan lead to collisions or human discomfort and is one reason why safe and\nrisk-aware navigation is still an open problem. This work introduces a novel\napproach that integrates aleatoric, epistemic, and predictive uncertainty\nestimation into a DRL navigation framework for policy distribution uncertainty\nestimates. We, therefore, incorporate Observation-Dependent Variance (ODV) and\ndropout into the Proximal Policy Optimization (PPO) algorithm. For different\ntypes of perturbations, we compare the ability of deep ensembles and\nMonte-Carlo dropout (MC-dropout) to estimate the uncertainties of the policy.\nIn uncertain decision-making situations, we propose to change the robot's\nsocial behavior to conservative collision avoidance. The results show improved\ntraining performance with ODV and dropout in PPO and reveal that the training\nscenario has an impact on the generalization. In addition, MC-dropout is more\nsensitive to perturbations and correlates the uncertainty type to the\nperturbation better. With the safe action selection, the robot can navigate in\nperturbed environments with fewer collisions.",
      "tldr_zh": "该研究针对人群密集环境中机器人安全导航的问题，提出了一种整合aleatoric uncertainty、epistemic uncertainty和predictive uncertainty估计的Deep Reinforcement Learning (DRL)框架，以处理政策不确定性。方法包括将Observation-Dependent Variance (ODV)和dropout融入Proximal Policy Optimization (PPO)算法，并比较deep ensembles和Monte-Carlo dropout (MC-dropout)在不同干扰下的不确定性估计表现。实验结果显示，该框架提升了训练性能，MC-dropout对干扰更敏感，且通过采用保守碰撞避免行为，机器人能够在干扰环境中显著减少碰撞。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to the IEEE for possible publication, 8 pages, 6 figures\n  and 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.10655v2",
      "published_date": "2024-09-16 18:49:38 UTC",
      "updated_date": "2025-02-28 15:38:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:20:30.915029"
    },
    {
      "arxiv_id": "2409.13761v2",
      "title": "Do Large Language Models Need a Content Delivery Network?",
      "title_zh": "翻译失败",
      "authors": [
        "Yihua Cheng",
        "Kuntai Du",
        "Jiayi Yao",
        "Junchen Jiang"
      ],
      "abstract": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
      "tldr_zh": "本论文探讨了大型语言模型 (LLMs) 在注入新知识时的需求，比较了三种方法：微调 (fine-tuning)、上下文学习 (in-context learning) 和注入 KV caches。作者主张使用 KV caches 作为知识注入介质，能实现更模块化的管理、较低成本和更快速响应，从而提升 LLM 服务效率。为此，论文提出 Knowledge Delivery Network (KDN) 作为新系统组件，用于动态优化 KV caches 的存储、传输和组合，类似于 Content Delivery Networks (CDNs) 对互联网生态的贡献。最终，KDN 被视为推动 LLM 应用成功的关键，并已开源原型以供进一步开发。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13761v2",
      "published_date": "2024-09-16 18:46:24 UTC",
      "updated_date": "2024-10-21 15:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:20:43.681279"
    },
    {
      "arxiv_id": "2409.10653v2",
      "title": "Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers",
      "title_zh": "通过因果 Transformer 实现的预测性自监督逻辑综合优化",
      "authors": [
        "Raika Karimi",
        "Faezeh Faez",
        "Yingxue Zhang",
        "Xing Li",
        "Lei Chen",
        "Mingxuan Yuan",
        "Mahdi Biparva"
      ],
      "abstract": "Contemporary hardware design benefits from the abstraction provided by\nhigh-level logic gates, streamlining the implementation of logic circuits.\nLogic Synthesis Optimization (LSO) operates at one level of abstraction within\nthe Electronic Design Automation (EDA) workflow, targeting improvements in\nlogic circuits with respect to performance metrics such as size and speed in\nthe final layout. Recent trends in the field show a growing interest in\nleveraging Machine Learning (ML) for EDA, notably through ML-guided logic\nsynthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite\nthese advancements, existing models face challenges such as overfitting and\nlimited generalization, attributed to constrained public circuits and the\nexpressiveness limitations of graph encoders. To address these hurdles, and\ntackle data scarcity issues, we introduce LSOformer, a novel approach\nharnessing Autoregressive transformer models and predictive SSL to predict the\ntrajectory of Quality of Results (QoR). LSOformer integrates cross-attention\nmodules to merge insights from circuit graphs and optimization sequences,\nthereby enhancing prediction accuracy for QoR metrics. Experimental studies\nvalidate the effectiveness of LSOformer, showcasing its superior performance\nover baseline architectures in QoR prediction tasks, where it achieves\nimprovements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary\ncircuits datasets, respectively, in inductive setup.",
      "tldr_zh": "本文提出了一种名为LSOformer的新方法，利用Causal Transformers和预测性Self-Supervision (SSL)，来优化Logic Synthesis Optimization (LSO)，以解决现有机器学习(ML)模型在硬件设计中的过拟合和泛化问题。LSOformer通过交叉注意力模块整合电路图和优化序列信息，准确预测Quality of Results (QoR)的轨迹，从而提升逻辑电路的性能指标。实验结果显示，该方法在EPFL、OABCD和专有电路数据集上，比基线模型分别提高了5.74%、4.35%和17.06%的预测性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10653v2",
      "published_date": "2024-09-16 18:45:07 UTC",
      "updated_date": "2025-02-28 16:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:20:55.565234"
    },
    {
      "arxiv_id": "2409.10640v2",
      "title": "Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case Study for Russian",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Glazkova",
        "Dmitry Morozov"
      ],
      "abstract": "Keyphrase selection plays a pivotal role within the domain of scholarly\ntexts, facilitating efficient information retrieval, summarization, and\nindexing. In this work, we explored how to apply fine-tuned generative\ntransformer-based models to the specific task of keyphrase selection within\nRussian scientific texts. We experimented with four distinct generative models,\nsuch as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in both\nin-domain and cross-domain settings. The experiments were conducted on the\ntexts of Russian scientific abstracts from four domains: mathematics & computer\nscience, history, medicine, and linguistics. The use of generative models,\nnamely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,\n9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extraction\nbaselines for the Russian language. Although the results for cross-domain usage\nwere significantly lower, they still demonstrated the capability to surpass\nbaseline performances in several cases, underscoring the promising potential\nfor further exploration and refinement in this research field.",
      "tldr_zh": "本研究探讨了微调生成式Transformer模型在俄语科学文本关键短语选择任务中的应用，以俄语学术摘要为例。研究者实验了四个模型，包括ruT5、ruGPT、mT5和mBART，在数学与计算机科学、历史、医学和语言学等四个领域内进行评估。结果显示，mBART模型在领域内性能显著提升，相比关键短语提取基线，BERTScore提高了4.9%、ROUGE-1提高了9.0%、F1-score提高了12.2%。尽管跨领域设置下的表现较低，但某些情况下仍超过了基线，展示了该方法在俄语关键短语选择领域的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50",
        "I.2.7; I.7.m; H.3.3"
      ],
      "primary_category": "cs.CL",
      "comment": "DAMDID-2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10640v2",
      "published_date": "2024-09-16 18:15:28 UTC",
      "updated_date": "2024-09-18 07:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:21:07.569880"
    },
    {
      "arxiv_id": "2409.10515v1",
      "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
      "title_zh": "高效的自我学习框架用于交互式口语对话系统",
      "authors": [
        "Hitesh Tulsiani",
        "David M. Chan",
        "Shalini Ghosh",
        "Garima Lalwani",
        "Prabhat Pandey",
        "Ankish Bansal",
        "Sri Garimella",
        "Ariya Rastrow",
        "Björn Hoffmeister"
      ],
      "abstract": "Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.",
      "tldr_zh": "该论文提出了一种高效的自学习框架，用于交互式对话系统中的自动语音识别（ASR），旨在让ASR系统超越独立回合训练，能够适应对话上下文并整合显式监督和隐式用户反馈。框架通过student-teacher learning、context-aware dialog processing以及新的Ohm（在线硬负样本挖掘）方法设计对比自监督策略，实现持续学习。实验结果显示，与传统训练相比，该框架在真实对话系统中相对WER（词错误率）减少近10%，在公共合成数据上减少高达26%。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Presented at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10515v1",
      "published_date": "2024-09-16 17:59:50 UTC",
      "updated_date": "2024-09-16 17:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:21:19.477825"
    },
    {
      "arxiv_id": "2409.10594v1",
      "title": "Kolmogorov-Arnold Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyi Yang",
        "Xinchao Wang"
      ],
      "abstract": "Transformers stand as the cornerstone of mordern deep learning.\nTraditionally, these models rely on multi-layer perceptron (MLP) layers to mix\nthe information between channels. In this paper, we introduce the\nKolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP\nlayers with Kolmogorov-Arnold Network (KAN) layers to enhance the\nexpressiveness and performance of the model. Integrating KANs into\ntransformers, however, is no easy feat, especially when scaled up.\nSpecifically, we identify three key challenges: (C1) Base function. The\nstandard B-spline function used in KANs is not optimized for parallel computing\non modern hardware, resulting in slower inference speeds. (C2) Parameter and\nComputation Inefficiency. KAN requires a unique function for each input-output\npair, making the computation extremely large. (C3) Weight initialization. The\ninitialization of weights in KANs is particularly challenging due to their\nlearnable activation functions, which are critical for achieving convergence in\ndeep neural networks. To overcome the aforementioned challenges, we propose\nthree key solutions: (S1) Rational basis. We replace B-spline functions with\nrational functions to improve compatibility with modern GPUs. By implementing\nthis in CUDA, we achieve faster computations. (S2) Group KAN. We share the\nactivation weights through a group of neurons, to reduce the computational load\nwithout sacrificing performance. (S3) Variance-preserving initialization. We\ncarefully initialize the activation weights to make sure that the activation\nvariance is maintained across layers. With these designs, KAT scales\neffectively and readily outperforms traditional MLP-based transformers.",
      "tldr_zh": "本研究提出Kolmogorov-Arnold Transformer (KAT)，一种新型架构，将Kolmogorov-Arnold Network (KAN)层替换传统Transformer中的多层感知器(MLP)层，以提升模型的表达能力和性能。面对整合KAN的挑战，包括B-spline函数的并行计算不优化、参数计算效率低下以及权重初始化的困难，该论文分别通过采用理性函数并结合CUDA加速、引入Group KAN共享激活权重以及方差保持初始化(Variance-preserving initialization)来解决问题。实验结果显示，KAT在扩展性和性能上超越了基于MLP的Transformer，为更高效的深度学习模型提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Code: https://github.com/Adamdad/kat",
      "pdf_url": "http://arxiv.org/pdf/2409.10594v1",
      "published_date": "2024-09-16 17:54:51 UTC",
      "updated_date": "2024-09-16 17:54:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:21:30.717189"
    },
    {
      "arxiv_id": "2409.10593v3",
      "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Luning Wang",
        "Shiyao Li",
        "Xuefei Ning",
        "Zhihang Yuan",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "abstract": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在长上下文场景下KV cache的内存开销问题，提出了一种训练高效的通道收缩技术CSKV，以缓解现有量化（quantization）和token pruning方法的局限性。CSKV通过分析KV cache的奇异值分布，发现通道维度冗余，并采用低秩分解（low-rank decomposition）来压缩key和value层，同时引入双分支KV cache（包括窗口-based全精度缓存和低精度压缩缓存），并最小化层级重建损失（layer-wise reconstruction loss）以减少训练成本。实验结果显示，CSKV可将KV cache内存开销降低80%，同时保持模型的长上下文能力；此外，该方法可与量化技术无缝结合，实现高达95%的压缩比。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.10593v3",
      "published_date": "2024-09-16 17:36:50 UTC",
      "updated_date": "2024-10-18 19:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:21:43.354973"
    },
    {
      "arxiv_id": "2409.10496v5",
      "title": "MusicLIME: Explainable Multimodal Music Understanding",
      "title_zh": "MusicLIME：可解释的多模态音乐理解",
      "authors": [
        "Theodoros Sotirou",
        "Vassilis Lyberatos",
        "Orfeas Menis Mastromichalakis",
        "Giorgos Stamou"
      ],
      "abstract": "Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.",
      "tldr_zh": "本研究针对多模态音乐模型的可解释性问题，引入了 MusicLIME，这是一种模型无关的特征重要性解释方法，能够揭示音频和歌词特征之间的互动，从而提供更全面的决策视角。不同于传统单模态方法，MusicLIME 通过聚合本地解释生成全局解释，帮助用户理解模型行为、减少偏差并提升公平性。该方法为多模态音乐理解系统带来了更高的透明度和可信度，促进了更公正的音乐分析应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "GitHub repository: https://github.com/IamTheo2000/MusicLIME. To be\n  presented at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.10496v5",
      "published_date": "2024-09-16 17:28:21 UTC",
      "updated_date": "2025-03-17 18:21:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:21:54.393817"
    },
    {
      "arxiv_id": "2409.10489v4",
      "title": "Flash STU: Fast Spectral Transform Units",
      "title_zh": "翻译失败",
      "authors": [
        "Y. Isabel Liu",
        "Windsor Nguyen",
        "Yagiz Devre",
        "Evan Dogariu",
        "Anirudha Majumdar",
        "Elad Hazan"
      ],
      "abstract": "Recent advances in state-space model architectures have shown great promise\nfor efficient sequence modeling, but challenges remain in balancing\ncomputational efficiency with model expressiveness. We propose the Flash STU\narchitecture, a hybrid model that interleaves spectral state space model layers\nwith sliding window attention, enabling scalability to billions of parameters\nfor language modeling while maintaining a near-linear time complexity. We\nevaluate the Flash STU and its variants on diverse sequence prediction tasks,\nincluding linear dynamical systems, robotics control, and language modeling. We\nfind that, given a fixed parameter budget, the Flash STU architecture\nconsistently outperforms the Transformer and other leading state-space models\nsuch as S4 and Mamba-2.",
      "tldr_zh": "该论文提出Flash STU架构，一种混合模型，将spectral state space model层与sliding window attention交错结合，以实现高效的序列建模，同时支持扩展到数十亿参数并保持近线性的时间复杂度。主要贡献在于，Flash STU及其变体在线性动力系统、机器人控制和语言建模等任务上进行评估，结果显示，在固定参数预算下，它 consistently outperforms Transformer、S4和Mamba-2等领先模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10489v4",
      "published_date": "2024-09-16 17:22:34 UTC",
      "updated_date": "2025-04-07 22:47:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:22:07.661885"
    },
    {
      "arxiv_id": "2409.10488v1",
      "title": "Do Pre-trained Vision-Language Models Encode Object States?",
      "title_zh": "预训练的视觉语言模型是否编码了对象状态？",
      "authors": [
        "Kaleb Newman",
        "Shijie Wang",
        "Yuan Zang",
        "David Heffren",
        "Chen Sun"
      ],
      "abstract": "For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.",
      "tldr_zh": "本研究探讨了预训练视觉语言模型（VLMs）是否能够编码对象状态，例如物体物理变化（如苹果从完整到切片）。研究者构建了对象状态识别数据集 ChangeIt-Frames，并评估了九个开源 VLMs，包括基于对比和生成目标训练的模型。结果显示，这些模型在对象识别方面表现可靠，但无法准确区分对象的物理状态。作者通过实验识别了三个改进领域：对象定位质量、绑定概念到对象的架构，以及学习区分性视觉和语言编码器的目标，并发布了数据和代码以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10488v1",
      "published_date": "2024-09-16 17:22:18 UTC",
      "updated_date": "2024-09-16 17:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:22:18.465873"
    },
    {
      "arxiv_id": "2409.10481v1",
      "title": "Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance",
      "title_zh": "探索三维人脸重建和融合方法用于人",
      "authors": [
        "Simone Maurizio La Cava",
        "Sara Concas",
        "Ruben Tolosana",
        "Roberto Casula",
        "Giulia Orrù",
        "Martin Drahansky",
        "Julian Fierrez",
        "Gian Luca Marcialis"
      ],
      "abstract": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.",
      "tldr_zh": "本研究探讨了3D Face Reconstruction (3DFR)算法在视频监控中的应用问题，这些算法的特定假设（如摄像头距离和特性）限制了其在非预期条件下的性能。研究者使用三种代表性SOTA 3DFR算法（包括统计模型拟合、光度立体和深度学习）作为面部验证系统的模板生成器，并通过score-level fusion结合各系统的分数。结果显示，不同3DFR算法的互补性显著提高了跨距离和跨摄像头设置下的验证性能，鼓励进一步探索多3DFR方法的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at T-CAP - Towards a Complete Analysis of People:\n  Fine-grained Understanding for Real-World Applications, workshop in\n  conjunction with the 18th European Conference on Computer Vision ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10481v1",
      "published_date": "2024-09-16 17:17:47 UTC",
      "updated_date": "2024-09-16 17:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:22:30.556912"
    },
    {
      "arxiv_id": "2409.10473v1",
      "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
      "title_zh": "MacDiff：统一的骨",
      "authors": [
        "Lehong Wu",
        "Lilang Lin",
        "Jiahang Zhang",
        "Yiyang Ma",
        "Jiaying Liu"
      ],
      "abstract": "Self-supervised learning has proved effective for skeleton-based human action\nunderstanding. However, previous works either rely on contrastive learning that\nsuffers false negative problems or are based on reconstruction that learns too\nmuch unessential low-level clues, leading to limited representations for\ndownstream tasks. Recently, great advances have been made in generative\nlearning, which is naturally a challenging yet meaningful pretext task to model\nthe general underlying data distributions. However, the representation learning\ncapacity of generative models is under-explored, especially for the skeletons\nwith spacial sparsity and temporal redundancy. To this end, we propose Masked\nConditional Diffusion (MacDiff) as a unified framework for human skeleton\nmodeling. For the first time, we leverage diffusion models as effective\nskeleton representation learners. Specifically, we train a diffusion decoder\nconditioned on the representations extracted by a semantic encoder. Random\nmasking is applied to encoder inputs to introduce a information bottleneck and\nremove redundancy of skeletons. Furthermore, we theoretically demonstrate that\nour generative objective involves the contrastive learning objective which\naligns the masked and noisy views. Meanwhile, it also enforces the\nrepresentation to complement for the noisy view, leading to better\ngeneralization performance. MacDiff achieves state-of-the-art performance on\nrepresentation learning benchmarks while maintaining the competence for\ngenerative tasks. Moreover, we leverage the diffusion model for data\naugmentation, significantly enhancing the fine-tuning performance in scenarios\nwith scarce labeled data. Our project is available at\nhttps://lehongwu.github.io/ECCV24MacDiff/.",
      "tldr_zh": "本研究提出 MacDiff，一种统一的骨骼建模框架，使用 Masked Conditional Diffusion 来提升自监督学习在骨骼-based 人类动作理解中的表现，解决对比学习（contrastive learning）的假负样本问题和重构方法的无关细节学习问题。框架通过训练一个扩散解码器（diffusion decoder），其条件于语义编码器提取的表示，并应用随机掩码（random masking）来引入信息瓶颈并去除骨骼数据的时空冗余。理论上，MacDiff 的生成目标整合了对比学习目标，实现掩码和噪声视图的对齐，同时提升表示的泛化能力。实验结果显示，MacDiff 在表示学习基准上达到最先进（state-of-the-art）性能，并通过扩散模型进行数据增强，大幅改善标签数据稀缺场景下的微调表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10473v1",
      "published_date": "2024-09-16 17:06:10 UTC",
      "updated_date": "2024-09-16 17:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:22:43.922143"
    },
    {
      "arxiv_id": "2410.01824v2",
      "title": "AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers",
      "title_zh": "AI 会话式访谈：使用 LLMs 作为适应性访谈者转变调查",
      "authors": [
        "Alexander Wuttke",
        "Matthias Aßenmacher",
        "Christopher Klamm",
        "Max M. Lang",
        "Quirin Würschinger",
        "Frauke Kreuter"
      ],
      "abstract": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.",
      "tldr_zh": "本研究探讨了使用大型语言模型（LLMs）作为自适应访谈者来解决传统调查方法在深度与规模之间的权衡问题，即结构化调查虽可大规模收集数据但限制了受访者表达，而对话式访谈虽深入却资源密集。研究通过小规模实验，让大学学生随机接受 AI 或人类访谈者进行相同的政治话题问卷，并评估访谈者遵守指南、响应质量、参与度和整体效果。结果表明，AI 对话式访谈能产生与传统方法相当的高质量数据，同时具备更好的可扩展性优势；论文还公布了相关数据和材料，并提供了具体的实施推荐。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01824v2",
      "published_date": "2024-09-16 16:03:08 UTC",
      "updated_date": "2025-03-12 09:55:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:22:55.686688"
    },
    {
      "arxiv_id": "2409.10419v2",
      "title": "HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vineet Bhat",
        "Prashanth Krishnamurthy",
        "Ramesh Karri",
        "Farshad Khorrami"
      ],
      "abstract": "Robots interacting with humans through natural language can unlock numerous\napplications such as Referring Grasp Synthesis (RGS). Given a text query, RGS\ndetermines a stable grasp pose to manipulate the referred object in the robot's\nworkspace. RGS comprises two steps: visual grounding and grasp pose estimation.\nRecent studies leverage powerful Vision-Language Models (VLMs) for visually\ngrounding free-flowing natural language in real-world robotic execution.\nHowever, comparisons in complex, cluttered environments with multiple instances\nof the same object are lacking. This paper introduces HiFi-CS, featuring\nhierarchical application of Featurewise Linear Modulation (FiLM) to fuse image\nand text embeddings, enhancing visual grounding for complex attribute rich text\nqueries encountered in robotic grasping. Visual grounding associates an object\nin 2D/3D space with natural language input and is studied in two scenarios:\nClosed and Open Vocabulary. HiFi-CS features a lightweight decoder combined\nwith a frozen VLM and outperforms competitive baselines in closed vocabulary\nsettings while being 100x smaller in size. Our model can effectively guide\nopen-set object detectors like GroundedSAM to enhance open-vocabulary\nperformance. We validate our approach through real-world RGS experiments using\na 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop\nscenes. Our codebase is provided here: https://github.com/vineet2104/hifics",
      "tldr_zh": "本研究提出 HiFi-CS，一种基于 Vision-Language Models (VLMs) 的框架，旨在提升机器人抓取中的 Open Vocabulary Visual Grounding 性能，通过层次化 Featurewise Linear Modulation (FiLM) 融合图像和文本嵌入，以处理复杂属性的自然语言查询。HiFi-CS 采用轻量级解码器与冻结的 VLM 结合，在 Closed Vocabulary 设置中优于竞争基线，且模型大小小 100 倍，同时能指导 Open Vocabulary 场景下的对象检测器如 GroundedSAM 提升准确率。实验在 15 个桌面场景中使用 7-DOF 机器人臂验证，实现了 90.33% 的 Visual Grounding 准确率，为 Referring Grasp Synthesis (RGS) 在杂乱环境中的应用提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10419v2",
      "published_date": "2024-09-16 15:50:39 UTC",
      "updated_date": "2025-03-12 21:30:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:23:18.474959"
    },
    {
      "arxiv_id": "2409.10416v1",
      "title": "Geometric Clustering for Hardware-Efficient Implementation of Chromatic Dispersion Compensation",
      "title_zh": "翻译失败",
      "authors": [
        "Geraldo Gomes",
        "Pedro Freire",
        "Jaroslaw E. Prilepsky",
        "Sergei K. Turitsyn"
      ],
      "abstract": "Power efficiency remains a significant challenge in modern optical fiber\ncommunication systems, driving efforts to reduce the computational complexity\nof digital signal processing, particularly in chromatic dispersion compensation\n(CDC) algorithms. While various strategies for complexity reduction have been\nproposed, many lack the necessary hardware implementation to validate their\nbenefits. This paper provides a theoretical analysis of the tap overlapping\neffect in CDC filters for coherent receivers, introduces a novel Time-Domain\nClustered Equalizer (TDCE) technique based on this concept, and presents a\nField-Programmable Gate Array (FPGA) implementation for validation. We\ndeveloped an innovative parallelization method for TDCE, implementing it in\nhardware for fiber lengths up to 640 km. A fair comparison with the\nstate-of-the-art frequency domain equalizer (FDE) under identical conditions is\nalso conducted. Our findings highlight that implementation strategies,\nincluding parallelization and memory management, are as crucial as\ncomputational complexity in determining hardware complexity and energy\nefficiency. The proposed TDCE hardware implementation achieves up to 70.7\\%\nenergy savings and 71.4\\% multiplier usage savings compared to FDE, despite its\nhigher computational complexity.",
      "tldr_zh": "本论文针对光纤通信系统中色散补偿（Chromatic Dispersion Compensation, CDC）的计算复杂度问题，提出了一种基于抽头重叠效应的Time-Domain Clustered Equalizer (TDCE)技术，以实现硬件高效实现。研究通过理论分析和Field-Programmable Gate Array (FPGA)硬件验证，开发了TDCE的创新并行化方法，支持光纤长度达640 km。相比于最先进的Frequency Domain Equalizer (FDE)，TDCE在相同条件下实现了高达70.7%的能源节省和71.4%的乘法器使用节省，强调了实现策略（如并行化和内存管理）对硬件复杂度和能效的重要性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10416v1",
      "published_date": "2024-09-16 15:48:05 UTC",
      "updated_date": "2024-09-16 15:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:23:19.561891"
    },
    {
      "arxiv_id": "2409.10403v1",
      "title": "A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration",
      "title_zh": "一种基于提示学习和 BERT 集成的知识增强疾病诊断方法",
      "authors": [
        "Zhang Zheng"
      ],
      "abstract": "This paper proposes a knowledge-enhanced disease diagnosis method based on a\nprompt learning framework. The method retrieves structured knowledge from\nexternal knowledge graphs related to clinical cases, encodes it, and injects it\ninto the prompt templates to enhance the language model's understanding and\nreasoning capabilities for the task.We conducted experiments on three public\ndatasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the\nproposed method significantly outperforms existing models across multiple\nevaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC\ndataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.\nAdditionally,ablation studies confirmed the critical role of the knowledge\ninjection module,as the removal of this module resulted in a significant drop\nin F1 score. The experimental results demonstrate that the proposed method not\nonly effectively improves the accuracy of disease diagnosis but also enhances\nthe interpretability of the predictions, providing more reliable support and\nevidence for clinical diagnosis.",
      "tldr_zh": "本论文提出了一种基于提示学习(prompt learning)和BERT集成的知识增强疾病诊断方法，通过从外部知识图谱(knowledge graphs)中检索并编码结构化知识，并将其注入提示模板，以提升语言模型的理解和推理能力。实验在CHIP-CTC、IMCS-V2-NER和KUAKE-QTR三个公共数据集上进行，结果显示该方法在多个评估指标上显著优于现有模型，F1分数分别提高了2.4%、3.1%和4.2%。消融实验证实，知识注入模块至关重要，其移除导致F1分数显著下降。该方法不仅提高了疾病诊断的准确性，还增强了预测的可解释性，为临床诊断提供更可靠的支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Knowledge Enhancement,Disease Diagnosis,Prompt\n  Learning,BERT,Knowledge Graph",
      "pdf_url": "http://arxiv.org/pdf/2409.10403v1",
      "published_date": "2024-09-16 15:34:58 UTC",
      "updated_date": "2024-09-16 15:34:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:23:31.063575"
    },
    {
      "arxiv_id": "2409.10394v1",
      "title": "MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hwihun Jeong",
        "Se Young Chun",
        "Jongho Lee"
      ],
      "abstract": "Deep learning-based Magnetic Resonance (MR) reconstruction methods have\nfocused on generating high-quality images but they often overlook the impact on\ndownstream tasks (e.g., segmentation) that utilize the reconstructed images.\nCascading separately trained reconstruction network and downstream task network\nhas been shown to introduce performance degradation due to error propagation\nand domain gaps between training datasets. To mitigate this issue, downstream\ntask-oriented reconstruction optimization has been proposed for a single\ndownstream task. Expanding this optimization to multi-task scenarios is not\nstraightforward. In this work, we extended this optimization to sequentially\nintroduced multiple downstream tasks and demonstrated that a single MR\nreconstruction network can be optimized for multiple downstream tasks by\ndeploying continual learning (MOST). MOST integrated techniques from\nreplay-based continual learning and image-guided loss to overcome catastrophic\nforgetting. Comparative experiments demonstrated that MOST outperformed a\nreconstruction network without finetuning, a reconstruction network with\nna\\\"ive finetuning, and conventional continual learning methods. This\nadvancement empowers the application of a single MR reconstruction network for\nmultiple downstream tasks. The source code is available at:\nhttps://github.com/SNU-LIST/MOST",
      "tldr_zh": "本文提出 MOST 方法，通过 continual learning 优化 Magnetic Resonance (MR) 重建网络，以支持多个 downstream tasks（如图像分割），从而避免传统级联方法带来的错误传播和性能下降。MOST 整合了 replay-based continual learning 和 image-guided loss 技术，有效克服 catastrophic forgetting。实验结果表明，MOST 优于未微调或简单微调的基线模型，以及传统 continual learning 方法，为单一 MR 重建网络的多任务应用提供了先进解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10394v1",
      "published_date": "2024-09-16 15:31:04 UTC",
      "updated_date": "2024-09-16 15:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:23:43.233828"
    },
    {
      "arxiv_id": "2409.11440v1",
      "title": "MARCA: Mamba Accelerator with ReConfigurable Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Jinhao Li",
        "Shan Huang",
        "Jiaming Xu",
        "Jun Liu",
        "Li Ding",
        "Ningyi Xu",
        "Guohao Dai"
      ],
      "abstract": "We propose a Mamba accelerator with reconfigurable architecture, MARCA.We\npropose three novel approaches in this paper. (1) Reduction alternative PE\narray architecture for both linear and element-wise operations. For linear\noperations, the reduction tree connected to PE arrays is enabled and executes\nthe reduction operation. For element-wise operations, the reduction tree is\ndisabled and the output bypasses. (2) Reusable nonlinear function unit based on\nthe reconfigurable PE. We decompose the exponential function into element-wise\noperations and a shift operation by a fast biased exponential algorithm, and\nthe activation function (SiLU) into a range detection and element-wise\noperations by a piecewise approximation algorithm. Thus, the reconfigurable PEs\nare reused to execute nonlinear functions with negligible accuracy loss.(3)\nIntra-operation and inter-operation buffer management strategy. We propose\nintra-operation buffer management strategy to maximize input data sharing for\nlinear operations within operations, and inter-operation strategy for\nelement-wise operations between operations. We conduct extensive experiments on\nMamba model families with different sizes.MARCA achieves up to\n463.22$\\times$/11.66$\\times$ speedup and up to 9761.42$\\times$/242.52$\\times$\nenergy efficiency compared to Intel Xeon 8358P CPU and NVIDIA Tesla A100 GPU\nimplementations, respectively.",
      "tldr_zh": "这篇论文提出了 MARCA，一种具有可重配置架构的 Mamba 加速器，旨在提升模型计算效率。论文引入了三个创新方法：(1) 减少替代 PE array 架构，用于处理线性操作和元素级操作，通过启用/禁用减少树实现灵活切换；(2) 基于可重配置 PE 的可重用非线性函数单元，将指数函数和 SiLU 激活函数分解为元素级操作和移位操作，以最小精度损失实现重用；(3) 内部操作和操作间缓冲区管理策略，最大化数据共享以优化性能。实验结果显示，MARCA 在不同大小的 Mamba 模型上，比 Intel Xeon 8358P CPU 和 NVIDIA Tesla A100 GPU 分别实现了高达 463.22 倍的速度提升和 9761.42 倍的能效提升。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages, 10 figures, accepted by ICCAD 2024. arXiv admin note: text\n  overlap with arXiv:2001.02514 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2409.11440v1",
      "published_date": "2024-09-16 15:18:33 UTC",
      "updated_date": "2024-09-16 15:18:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:23:56.953922"
    },
    {
      "arxiv_id": "2409.10589v3",
      "title": "Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling",
      "title_zh": "翻译失败",
      "authors": [
        "Jesse van Remmerden",
        "Zaharah Bukhsh",
        "Yingqian Zhang"
      ],
      "abstract": "The Job Shop Scheduling Problem (JSSP) is a complex combinatorial\noptimization problem. While online Reinforcement Learning (RL) has shown\npromise by quickly finding acceptable solutions for JSSP, it faces key\nlimitations: it requires extensive training interactions from scratch leading\nto sample inefficiency, cannot leverage existing high-quality solutions, and\noften yields suboptimal results compared to traditional methods like Constraint\nProgramming (CP). We introduce Offline Reinforcement Learning for Learning to\nDispatch (Offline-LD), which addresses these limitations by learning from\npreviously generated solutions. Our approach is motivated by scenarios where\nhistorical scheduling data and expert solutions are available, although our\ncurrent evaluation focuses on benchmark problems. Offline-LD adapts two\nCQL-based Q-learning methods (mQRDQN and discrete mSAC) for maskable action\nspaces, introduces a novel entropy bonus modification for discrete SAC, and\nexploits reward normalization through preprocessing. Our experiments\ndemonstrate that Offline-LD outperforms online RL on both generated and\nbenchmark instances. Notably, by introducing noise into the expert dataset, we\nachieve similar or better results than those obtained from the expert dataset,\nsuggesting that a more diverse training set is preferable because it contains\ncounterfactual information.",
      "tldr_zh": "本研究针对作业车间调度问题（Job Shop Scheduling Problem, JSSP）这种复杂组合优化问题，提出了Offline Reinforcement Learning for Learning to Dispatch (Offline-LD)方法，以克服在线强化学习（RL）的样本效率低、无法利用现有高质量解决方案以及性能不如传统方法（如Constraint Programming, CP）的缺点。Offline-LD通过从历史调度数据和专家解决方案中学习，改进了两种基于CQL的Q学习算法（mQRDQN和discrete mSAC），并引入了新的熵奖金修改和奖励归一化预处理，以适应可屏蔽动作空间。实验结果显示，Offline-LD在生成和基准实例上超过了在线RL的表现，且通过在专家数据集上添加噪声，实现了与专家数据相当或更好的结果，表明多样化的训练集因包含反事实信息而更具优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/jesserem/Offline-LD",
      "pdf_url": "http://arxiv.org/pdf/2409.10589v3",
      "published_date": "2024-09-16 15:18:10 UTC",
      "updated_date": "2025-04-13 14:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:24:07.250917"
    },
    {
      "arxiv_id": "2409.10372v3",
      "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
      "title_zh": "翻译失败",
      "authors": [
        "Qiliang Chen",
        "Sepehr Ilami",
        "Nunzio Lore",
        "Babak Heydari"
      ],
      "abstract": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
      "tldr_zh": "本论文提出一个新框架，将LLM agents作为人类战略行为的代理，与reinforcement learning (RL)结合，在团队环境中进行演化战略互动，以激发合作。框架引入strategic LLM agents (SLA) 和pro-social promoting RL agent (PPA)，后者通过动态调节信息访问来优化社会福利并促进亲社会行为。在迭代游戏如prisoner dilemma中验证，实验结果显示SLA agents展现出细微战略适应，而PPA agents有效提升了合作率。该框架为AI中介的社会动态提供见解，并支持AI在真实世界团队设置中的部署。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10372v3",
      "published_date": "2024-09-16 15:15:51 UTC",
      "updated_date": "2024-10-30 16:45:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:24:20.606220"
    },
    {
      "arxiv_id": "2409.10365v2",
      "title": "Robust image representations with counterfactual contrastive learning",
      "title_zh": "基于反事实对比学习的鲁棒图像表示",
      "authors": [
        "Mélanie Roschewitz",
        "Fabio De Sousa Ribeiro",
        "Tian Xia",
        "Galvin Khara",
        "Ben Glocker"
      ],
      "abstract": "Contrastive pretraining can substantially increase model generalisation and\ndownstream performance. However, the quality of the learned representations is\nhighly dependent on the data augmentation strategy applied to generate positive\npairs. Positive contrastive pairs should preserve semantic meaning while\ndiscarding unwanted variations related to the data acquisition domain.\nTraditional contrastive pipelines attempt to simulate domain shifts through\npre-defined generic image transformations. However, these do not always mimic\nrealistic and relevant domain variations for medical imaging, such as scanner\ndifferences. To tackle this issue, we herein introduce counterfactual\ncontrastive learning, a novel framework leveraging recent advances in causal\nimage synthesis to create contrastive positive pairs that faithfully capture\nrelevant domain variations. Our method, evaluated across five datasets\nencompassing both chest radiography and mammography data, for two established\ncontrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive\nlearning in terms of robustness to acquisition shift. Notably, counterfactual\ncontrastive learning achieves superior downstream performance on both\nin-distribution and external datasets, especially for images acquired with\nscanners under-represented in the training set. Further experiments show that\nthe proposed framework extends beyond acquisition shifts, with models trained\nwith counterfactual contrastive learning reducing subgroup disparities across\nbiological sex.",
      "tldr_zh": "本论文提出counterfactual contrastive learning框架，利用因果图像合成生成对比正对，以捕捉医疗图像中的真实领域变异（如扫描仪差异），从而提升图像表示的鲁棒性。该方法针对传统对比学习（如SimCLR和DINO-v2）的局限性，通过模拟相关域偏移创建语义保持的正对，并在五个涵盖胸部X光和乳腺X光的数据集上进行评估。结果显示，与标准对比学习相比，该框架显著提高了模型在分布内和外部数据集上的下游性能，尤其在训练集中 underrepresented 的扫描仪上表现更优，并减少了生物性别等子组差异。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive/",
      "pdf_url": "http://arxiv.org/pdf/2409.10365v2",
      "published_date": "2024-09-16 15:11:00 UTC",
      "updated_date": "2025-04-10 16:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:24:32.176748"
    },
    {
      "arxiv_id": "2409.10350v1",
      "title": "Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation",
      "title_zh": "Point2Graph：一种端到端的基于点云的3D开放词汇场景图，用于机器人导航",
      "authors": [
        "Yifan Xu",
        "Ziming Luo",
        "Qianwei Wang",
        "Vineet Kamat",
        "Carol Menassa"
      ],
      "abstract": "Current open-vocabulary scene graph generation algorithms highly rely on both\n3D scene point cloud data and posed RGB-D images and thus have limited\napplications in scenarios where RGB-D images or camera poses are not readily\navailable. To solve this problem, we propose Point2Graph, a novel end-to-end\npoint cloud-based 3D open-vocabulary scene graph generation framework in which\nthe requirement of posed RGB-D image series is eliminated. This hierarchical\nframework contains room and object detection/segmentation and open-vocabulary\nclassification. For the room layer, we leverage the advantage of merging the\ngeometry-based border detection algorithm with the learning-based region\ndetection to segment rooms and create a \"Snap-Lookup\" framework for\nopen-vocabulary room classification. In addition, we create an end-to-end\npipeline for the object layer to detect and classify 3D objects based solely on\n3D point cloud data. Our evaluation results show that our framework can\noutperform the current state-of-the-art (SOTA) open-vocabulary object and room\nsegmentation and classification algorithm on widely used real-scene datasets.",
      "tldr_zh": "论文提出 Point2Graph，一种端到端的点云-based 3D Open-Vocabulary Scene Graph 生成框架，用于机器人导航，消除了对姿态 RGB-D 图像的依赖。框架采用分层设计，包括结合几何-based 边界检测算法和学习-based 区域检测的房间分割与 \"Snap-Lookup\" 分类，以及基于 3D 点云数据的物体检测和分类管道。实验结果表明，该框架在广泛使用的真实场景数据集上超过了当前 SOTA 的开放词汇物体和房间分割及分类算法的表现。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10350v1",
      "published_date": "2024-09-16 15:01:28 UTC",
      "updated_date": "2024-09-16 15:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:24:43.715833"
    },
    {
      "arxiv_id": "2409.10343v1",
      "title": "Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation",
      "title_zh": "大语言模型增强的硬样本识别用于去噪推荐",
      "authors": [
        "Tianrui Song",
        "Wenshuo Chao",
        "Hao Liu"
      ],
      "abstract": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach.",
      "tldr_zh": "本研究针对隐式反馈在推荐系统中面临的噪声问题（如误点击和位置偏差），提出了一种 Large Language Model Enhanced Hard Sample Denoising (LLMHD) 框架，以更好地区分硬样本和噪声样本。框架的核心是使用 Large Language Model (LLM) 构建评分器，通过评估项目与用户偏好的语义一致性（基于总结的历史用户交互）来量化样本的 hardness，并针对 pointwise 或 pairwise 训练目标进行优化。为提升效率，该框架引入基于方差的样本修剪策略过滤潜在硬样本，并设计迭代偏好更新模块来持续改进用户偏好摘要，以缓解假阳性交互的影响。在三个真实数据集和四个骨干推荐器上的广泛实验中，LLMHD 框架证明了其有效性，显著提升了推荐系统的去噪性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10343v1",
      "published_date": "2024-09-16 14:57:09 UTC",
      "updated_date": "2024-09-16 14:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:24:55.198855"
    },
    {
      "arxiv_id": "2409.10588v7",
      "title": "Opponent Shaping for Antibody Development",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastian Towers",
        "Aleksandra Kalisz",
        "Philippe A. Robert",
        "Alicia Higueruelo",
        "Francesca Vianello",
        "Ming-Han Chloe Tsai",
        "Harrison Steel",
        "Jakob N. Foerster"
      ],
      "abstract": "Anti-viral therapies are typically designed to target only the current\nstrains of a virus. Game theoretically, this corresponds to a short-sighted, or\nmyopic, response. However, therapy-induced selective pressures act on viruses\nto drive the emergence of mutated strains, against which initial therapies have\nreduced efficacy. Building on a computational model of binding between\nantibodies and viral antigens (the Absolut! framework), we design and implement\na genetic simulation of viral evolutionary escape. Crucially, this allows our\nantibody optimisation algorithm to consider and influence the entire escape\ncurve of the virus, i.e. to guide (or \"shape\") the viral evolution. This is\ninspired by opponent shaping which, in general-sum learning, accounts for the\nadaptation of the co-player rather than playing a myopic best response. Hence\nwe call the optimised antibodies shapers. Within our simulations, we\ndemonstrate that our shapers target both current and simulated future viral\nvariants, outperforming the antibodies chosen in a myopic way. Furthermore, we\nshow that shapers exert specific evolutionary pressure on the virus compared to\nmyopic antibodies. Altogether, shapers modify the evolutionary trajectories of\nviral strains and minimise the viral escape compared to their myopic\ncounterparts. While this is a simplified model, we hope that our proposed\nparadigm will facilitate the discovery of better long-lived vaccines and\nantibody therapies in the future, enabled by rapid advancements in the\ncapabilities of simulation tools. Our code is available at\nhttps://github.com/olakalisz/antibody-shapers.",
      "tldr_zh": "该研究提出了一种基于博弈论的“Opponent Shaping”方法，用于优化抗体开发，以应对病毒进化逃逸问题。传统抗病毒疗法仅针对当前病毒株（myopic response），而新算法利用Absolut!框架的遗传模拟，设计“shapers”抗体来引导病毒进化轨迹，针对当前和未来变体。实验结果显示，shapers抗体比短视抗体更有效地减少病毒逃逸，并施加特定进化压力，为开发更持久的疫苗和疗法提供新范式。",
      "categories": [
        "q-bio.PE",
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "92-08",
        "I.2.1; J.3"
      ],
      "primary_category": "q-bio.PE",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2409.10588v7",
      "published_date": "2024-09-16 14:56:27 UTC",
      "updated_date": "2024-11-07 12:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:25:08.051673"
    },
    {
      "arxiv_id": "2409.10340v1",
      "title": "Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs",
      "title_zh": "使用最密集重叠子图进行超图神经网络中的超边建模",
      "authors": [
        "Mehrad Soltani",
        "Luis Rueda"
      ],
      "abstract": "Hypergraphs tackle the limitations of traditional graphs by introducing {\\em\nhyperedges}. While graph edges connect only two nodes, hyperedges connect an\narbitrary number of nodes along their edges. Also, the underlying\nmessage-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the\nform of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and\nmore complex structural information than traditional Graph Neural Networks\n(GNNs). More recently, the idea of overlapping subgraphs has emerged. These\nsubgraphs can capture more information about subgroups of vertices without\nlimiting one vertex belonging to just one group, allowing vertices to belong to\nmultiple groups or subgraphs. In addition, one of the most important problems\nin graph clustering is to find densest overlapping subgraphs (DOS). In this\npaper, we propose a solution to the DOS problem via Agglomerative Greedy\nEnumeration (DOSAGE) algorithm as a novel approach to enhance the process of\ngenerating the densest overlapping subgraphs and, hence, a robust construction\nof the hypergraphs. Experiments on standard benchmarks show that the DOSAGE\nalgorithm significantly outperforms the HGNNs and six other methods on the node\nclassification task.",
      "tldr_zh": "本研究探讨了超图神经网络（Hypergraph Neural Networks, HGNNs）中超边建模（Hyperedge Modeling）的改进，通过使用最密集重叠子图（Densest Overlapping Subgraphs, DOS）来克服传统图神经网络（Graph Neural Networks, GNNs）的局限性。论文提出了一种新算法Agglomerative Greedy Enumeration (DOSAGE)，用于高效生成DOS，从而增强超图的构建过程，使其更好地捕获节点子群体的复杂结构信息。在标准基准实验中，DOSAGE算法在节点分类任务上显著优于HGNNs和其他六种方法，展示了其在提升网络性能方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10340v1",
      "published_date": "2024-09-16 14:56:10 UTC",
      "updated_date": "2024-09-16 14:56:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:25:19.630814"
    },
    {
      "arxiv_id": "2409.10338v1",
      "title": "The 20 questions game to distinguish large language models",
      "title_zh": "二十问题游戏用于区分大型语言模型",
      "authors": [
        "Gurvan Richardeau",
        "Erwan Le Merrer",
        "Camilla Penzo",
        "Gilles Tredan"
      ],
      "abstract": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.",
      "tldr_zh": "本研究借鉴“20 questions game”，提出一种方法，用于在黑箱环境中判断两个 large language models (LLMs) 是否相同，仅需少于 20 个良性二元问题。研究首先建立基线，通过从基准数据集随机选择问题，实现了近 100% 的准确率，并分析了问题的最优边界。随后，引入两种有效的 questioning heuristics，能够在相同任务中仅用一半问题区分 22 个 LLMs，提升了方法的隐蔽性。该方法对审计员或版权所有者特别有用，可用于处理模型泄露疑虑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10338v1",
      "published_date": "2024-09-16 14:50:29 UTC",
      "updated_date": "2024-09-16 14:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:25:30.595608"
    },
    {
      "arxiv_id": "2409.10329v2",
      "title": "InfoDisent: Explainability of Image Classification Models by Information Disentanglement",
      "title_zh": "翻译失败",
      "authors": [
        "Łukasz Struski",
        "Dawid Rymarczyk",
        "Jacek Tabor"
      ],
      "abstract": "In this work, we introduce InfoDisent, a hybrid approach to explainability\nbased on the information bottleneck principle. InfoDisent enables the\ndisentanglement of information in the final layer of any pretrained model into\natomic concepts, which can be interpreted as prototypical parts. This approach\nmerges the flexibility of post-hoc methods with the concept-level modeling\ncapabilities of self-explainable neural networks, such as ProtoPNets. We\ndemonstrate the effectiveness of InfoDisent through computational experiments\nand user studies across various datasets using modern backbones such as ViTs\nand convolutional networks. Notably, InfoDisent generalizes the prototypical\nparts approach to novel domains (ImageNet).",
      "tldr_zh": "本研究提出 InfoDisent，一种基于 information bottleneck principle 的混合方法，用于提升图像分类模型的可解释性。该方法将预训练模型的最终层信息解耦成原子概念，这些概念可视为原型部分，并结合 post-hoc methods 的灵活性和自解释神经网络（如 ProtoPNets）的建模能力。InfoDisent 通过计算实验和用户研究，在多种数据集上使用 ViTs 和卷积网络等现代骨干网络验证其有效性，并成功将原型部分方法泛化到新领域，如 ImageNet。总的来说，该方法为模型解释性提供了更全面的工具，提升了图像分类的可信度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10329v2",
      "published_date": "2024-09-16 14:39:15 UTC",
      "updated_date": "2025-03-06 12:16:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:25:52.706830"
    },
    {
      "arxiv_id": "2409.10320v2",
      "title": "SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Stoler",
        "Ingrid Navarro",
        "Jonathan Francis",
        "Jean Oh"
      ],
      "abstract": "Verification and validation of autonomous driving (AD) systems and components\nis of increasing importance, as such technology increases in real-world\nprevalence. Safety-critical scenario generation is a key approach to robustify\nAD policies through closed-loop training. However, existing approaches for\nscenario generation rely on simplistic objectives, resulting in\noverly-aggressive or non-reactive adversarial behaviors. To generate diverse\nadversarial yet realistic scenarios, we propose SEAL, a scenario perturbation\napproach which leverages learned objective functions and adversarial,\nhuman-like skills. SEAL-perturbed scenarios are more realistic than SOTA\nbaselines, leading to improved ego task success across real-world,\nin-distribution, and out-of-distribution scenarios, of more than 20%. To\nfacilitate future research, we release our code and tools:\nhttps://github.com/cmubig/SEAL",
      "tldr_zh": "该研究针对自动驾驶（AD）系统的验证问题，提出SEAL框架，通过技能启用对抗学习（Skill-Enabled Adversary Learning）生成更真实且多样的闭环场景（Closed-Loop Scenario Generation），以解决现有方法导致的过度攻击性和非反应性行为。SEAL利用学习的目标函数和人类-like技能进行场景扰动，确保生成的场景更具真实性。实验结果显示，SEAL显著提高了AD系统的任务成功率，在真实世界、分布内和分布外场景中提升超过20%，并开源代码以促进进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.10320v2",
      "published_date": "2024-09-16 14:33:21 UTC",
      "updated_date": "2025-02-17 23:48:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:25:54.966792"
    },
    {
      "arxiv_id": "2409.11186v1",
      "title": "Deep Learning tools to support deforestation monitoring in the Ivory Coast using SAR and Optical satellite imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Sartor",
        "Matteo Salis",
        "Stefano Pinardi",
        "Ozgur Saracik",
        "Rosa Meo"
      ],
      "abstract": "Deforestation is gaining an increasingly importance due to its strong\ninfluence on the sorrounding environment, especially in developing countries\nwhere population has a disadvantaged economic condition and agriculture is the\nmain source of income. In Ivory Coast, for instance, where the cocoa production\nis the most remunerative activity, it is not rare to assist to the replacement\nof portion of ancient forests with new cocoa plantations. In order to monitor\nthis type of deleterious activities, satellites can be employed to recognize\nthe disappearance of the forest to prevent it from expand its area of interest.\nIn this study, Forest-Non-Forest map (FNF) has been used as ground truth for\nmodels based on Sentinel images input. State-of-the-art models U-Net, Attention\nU-Net, Segnet and FCN32 are compared over different years combining Sentinel-1,\nSentinel-2 and cloud probability to create forest/non-forest segmentation.\nAlthough Ivory Coast lacks of forest coverage datasets and is partially covered\nby Sentinel images, it is demonstrated the feasibility to create models\nclassifying forest and non-forests pixels over the area using open datasets to\npredict where deforestation could have occurred. Although a significant portion\nof the deforestation research is carried out on visible bands, SAR acquisitions\nare employed to overcome the limits of RGB images over areas often covered by\nclouds. Finally, the most promising model is employed to estimate the hectares\nof forest has been cut between 2019 and 2020.",
      "tldr_zh": "本研究开发了深度学习工具，用于监控科特迪瓦的森林砍伐，利用 SAR 和 Optical satellite imagery 卫星图像。研究者比较了 U-Net、Attention U-Net、Segnet 和 FCN32 等模型，基于 Sentinel-1、Sentinel-2 图像和云概率数据，创建森林/非森林 (FNF) 分割地图，以识别潜在砍伐区域。结果显示，尽管数据有限，这些模型可行地预测了砍伐发生，并使用最优模型估算了2019-2020年间砍伐的森林面积，为发展中国家环境监测提供了实用方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11186v1",
      "published_date": "2024-09-16 14:26:41 UTC",
      "updated_date": "2024-09-16 14:26:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:26:18.173196"
    },
    {
      "arxiv_id": "2409.10308v2",
      "title": "Know your limits! Optimize the robot's behavior through self-awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Esteve Valls Mascaro",
        "Dongheui Lee"
      ],
      "abstract": "As humanoid robots transition from labs to real-world environments, it is\nessential to democratize robot control for non-expert users. Recent human-robot\nimitation algorithms focus on following a reference human motion with high\nprecision, but they are susceptible to the quality of the reference motion and\nrequire the human operator to simplify its movements to match the robot's\ncapabilities. Instead, we consider that the robot should understand and adapt\nthe reference motion to its own abilities, facilitating the operator's task.\nFor that, we introduce a deep-learning model that anticipates the robot's\nperformance when imitating a given reference. Then, our system can generate\nmultiple references given a high-level task command, assign a score to each of\nthem, and select the best reference to achieve the desired robot behavior. Our\nSelf-AWare model (SAW) ranks potential robot behaviors based on various\ncriteria, such as fall likelihood, adherence to the reference motion, and\nsmoothness. We integrate advanced motion generation, robot control, and SAW in\none unique system, ensuring optimal robot behavior for any task command. For\ninstance, SAW can anticipate falls with 99.29% accuracy. For more information\ncheck our project page: https://evm7.github.io/Self-AWare",
      "tldr_zh": "该论文探讨了如何通过机器人自我意识优化其行为，以适应非专家用户的控制需求。传统的人-机器人模仿算法依赖高质量参考动作，但容易受动作质量影响，因此作者引入了一个深度学习模型（deep-learning model），用于预测机器人模仿参考动作的性能，并生成多个备选参考动作进行评分。Self-AWare (SAW) 模型基于标准如 fall likelihood、adherence to reference motion 和 smoothness 评估行为，最终选择最佳方案；实验结果显示，SAW 在预测机器人跌倒时的准确率高达99.29%。这项工作整合了高级动作生成和机器人控制系统，促进了人形机器人在真实环境中的可靠应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to Humanoids 2024 and HFR 2024. Project Page:\n  https://evm7.github.io/Self-AWare",
      "pdf_url": "http://arxiv.org/pdf/2409.10308v2",
      "published_date": "2024-09-16 14:14:58 UTC",
      "updated_date": "2024-10-16 09:36:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:26:20.172822"
    },
    {
      "arxiv_id": "2409.10304v2",
      "title": "Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science",
      "title_zh": "翻译失败",
      "authors": [
        "Austin Cheng",
        "Cher Tian Ser",
        "Marta Skreta",
        "Andrés Guzmán-Cordero",
        "Luca Thiede",
        "Andreas Burger",
        "Abdulrahman Aldossary",
        "Shi Xuan Leong",
        "Sergio Pablo-García",
        "Felix Strieth-Kalthoff",
        "Alán Aspuru-Guzik"
      ],
      "abstract": "Machine learning has been pervasively touching many fields of science.\nChemistry and materials science are no exception. While machine learning has\nbeen making a great impact, it is still not reaching its full potential or\nmaturity. In this perspective, we first outline current applications across a\ndiversity of problems in chemistry. Then, we discuss how machine learning\nresearchers view and approach problems in the field. Finally, we provide our\nconsiderations for maximizing impact when researching machine learning for\nchemistry.",
      "tldr_zh": "这篇论文探讨了如何在化学和材料科学领域开展有影响力的AI研究，强调机器学习在这些领域的潜力尚未充分发挥。论文首先概述了机器学习在化学问题的多样应用，然后分析了研究者如何看待和处理这些问题。最终，它提供建议，以最大化机器学习研究的实际影响，例如通过跨学科合作和针对性问题解决来提升成熟度。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10304v2",
      "published_date": "2024-09-16 14:10:38 UTC",
      "updated_date": "2024-10-08 13:57:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:26:30.102818"
    },
    {
      "arxiv_id": "2409.10297v3",
      "title": "On Synthetic Texture Datasets: Challenges, Creation, and Curation",
      "title_zh": "关于合成纹理数据集：挑战、创建与策展",
      "authors": [
        "Blaine Hoak",
        "Patrick McDaniel"
      ],
      "abstract": "The influence of textures on machine learning models has been an ongoing\ninvestigation, specifically in texture bias/learning, interpretability, and\nrobustness. However, due to the lack of large and diverse texture data\navailable, the findings in these works have been limited, as more comprehensive\nevaluations have not been feasible. Image generative models are able to provide\ndata creation at scale, but utilizing these models for texture synthesis has\nbeen unexplored and poses additional challenges both in creating accurate\ntexture images and validating those images. In this work, we introduce an\nextensible methodology and corresponding new dataset for generating\nhigh-quality, diverse texture images capable of supporting a broad set of\ntexture-based tasks. Our pipeline consists of: (1) developing prompts from a\nrange of descriptors to serve as input to text-to-image models, (2) adopting\nand adapting Stable Diffusion pipelines to generate and filter the\ncorresponding images, and (3) further filtering down to the highest quality\nimages. Through this, we create the Prompted Textures Dataset (PTD), a dataset\nof 246,285 texture images that span 56 textures. During the process of\ngenerating images, we find that NSFW safety filters in image generation\npipelines are highly sensitive to texture (and flag up to 60\\% of our texture\nimages), uncovering a potential bias in these models and presenting unique\nchallenges when working with texture data. Through both standard metrics and a\nhuman evaluation, we find that our dataset is high quality and diverse. Our\ndataset is available for download at https://zenodo.org/records/15359142.",
      "tldr_zh": "该论文探讨了合成纹理数据集在机器学习模型（如纹理偏差、解释性和鲁棒性）评估中的挑战，由于现有数据缺乏规模和多样性而限制了研究进展。作者提出了一种可扩展的方法，包括从描述符开发提示、采用并适应 Stable Diffusion 管道生成图像、以及多级过滤，以创建高质量、多样化的纹理图像。结果形成了 Prompted Textures Dataset (PTD)，包含 246,285 张图像，覆盖 56 种纹理，并在生成过程中发现 NSFW safety filters 对纹理高度敏感，可能标记多达 60% 的图像，揭示了模型潜在偏差。通过标准指标和人类评估，证实了数据集的高质量和多样性，可从指定链接下载。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10297v3",
      "published_date": "2024-09-16 14:02:18 UTC",
      "updated_date": "2025-05-09 20:20:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:26:45.164812"
    },
    {
      "arxiv_id": "2409.10294v2",
      "title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
      "title_zh": "MGSA：多粒度图结构注意力用于知识图谱到文本生成",
      "authors": [
        "Shanshan Wang",
        "Chun Zhang",
        "Ning Zhang"
      ],
      "abstract": "The Knowledge Graph-to-Text Generation task aims to convert structured\nknowledge graphs into coherent and human-readable natural language text. Recent\nefforts in this field have focused on enhancing pre-trained language models\n(PLMs) by incorporating graph structure information to capture the intricate\nstructure details of knowledge graphs. However, most of these approaches tend\nto capture only single-granularity structure information, concentrating either\non the relationships between entities within the original graph or on the\nrelationships between words within the same entity or across different\nentities. This narrow focus results in a significant limitation: models that\nconcentrate solely on entity-level structure fail to capture the nuanced\nsemantic relationships between words, while those that focus only on word-level\nstructure overlook the broader relationships between original entire entities.\nTo overcome these limitations, this paper introduces the Multi-granularity\nGraph Structure Attention (MGSA), which is based on PLMs. The encoder of the\nmodel architecture features an entity-level structure encoding module, a\nword-level structure encoding module, and an aggregation module that\nsynthesizes information from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture both entity-level\nand word-level structure information, providing a more comprehensive\nunderstanding of the knowledge graph's structure information, thereby\nsignificantly improving the quality of the generated text. We conducted\nextensive evaluations of the MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative, where it consistently\noutperformed models that rely solely on single-granularity structure\ninformation, demonstrating the effectiveness of our approach.",
      "tldr_zh": "这篇论文针对 Knowledge Graph-to-Text Generation 任务，提出了一种基于预训练语言模型 (PLMs) 的 MGSA（Multi-Granularity Graph Structure Attention）模型，以解决现有方法仅捕捉单一粒度结构信息（如实体级或词级）的局限性。MGSA 的编码器包括实体级结构编码模块、词级结构编码模块和聚合模块，能够同时整合多粒度信息，从而更全面地理解知识图谱的结构细节。实验在 WebNLG 和 EventNarrative 数据集上显示，MGSA 显著优于单一粒度模型，证明了其在生成高质量自然语言文本方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10294v2",
      "published_date": "2024-09-16 14:01:03 UTC",
      "updated_date": "2024-09-23 04:06:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:26:56.006077"
    },
    {
      "arxiv_id": "2409.10290v1",
      "title": "Neuromorphic Spintronics",
      "title_zh": "神经形态自旋电子学",
      "authors": [
        "Atreya Majumdar",
        "Karin Everschor-Sitte"
      ],
      "abstract": "Neuromorphic spintronics combines two advanced fields in technology,\nneuromorphic computing and spintronics, to create brain-inspired, efficient\ncomputing systems that leverage the unique properties of the electron's spin.\nIn this book chapter, we first introduce both fields - neuromorphic computing\nand spintronics and then make a case for neuromorphic spintronics. We discuss\nconcrete examples of neuromorphic spintronics, including computing based on\nfluctuations, artificial neural networks, and reservoir computing, highlighting\ntheir potential to revolutionize computational efficiency and functionality.",
      "tldr_zh": "本章节介绍了 Neuromorphic Spintronics，这是一种结合 neuromorphic computing 和 spintronics 的脑启发计算系统，利用电子自旋的独特属性来提升效率。作者首先概述了两个领域，然后论证了其融合的必要性，并举例说明包括基于 fluctuations 的计算、artificial neural networks 和 reservoir computing 的具体应用。这些创新方法有望革命化计算效率和功能，为未来高效计算系统铺平道路。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.mes-hall",
        "cond-mat.other",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "Neuromorphic Spintronics is a chapter of a book titled \"Artificial\n  Intelligence and Intelligent Matter\". This is not the final version of the\n  chapter. For the final version, please go to the book published by Springer\n  (the DOI and other details will be put here once the book has been\n  published.)",
      "pdf_url": "http://arxiv.org/pdf/2409.10290v1",
      "published_date": "2024-09-16 13:57:39 UTC",
      "updated_date": "2024-09-16 13:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:27:06.359910"
    },
    {
      "arxiv_id": "2409.10289v2",
      "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahao Yuan",
        "Zixiang Di",
        "Zhiqing Cui",
        "Guisong Yang",
        "Usman Naseem"
      ],
      "abstract": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
      "tldr_zh": "该研究提出ReflectDiffu框架，通过RL-Diffusion（强化学习-扩散）机制，在移情响应生成中协调emotion contagion（情感传染）和intent mimicry（意图模仿），以解决现有模型忽略情感-意图互动或依赖大型语言模型（LLMs）的高计算开销问题。该框架利用emotion-reasoning mask识别关键情感元素，并通过Exploring-Sampling-Correcting机制将情感状态映射到精确意图行动，从而提升响应的移情性、可控性和相关性。实验结果显示，ReflectDiffu在自动和人工评估中优于现有模型，在相关性、可控性和信息性方面达到最先进水平。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10289v2",
      "published_date": "2024-09-16 13:56:17 UTC",
      "updated_date": "2024-09-18 17:30:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:27:19.487082"
    },
    {
      "arxiv_id": "2409.10283v2",
      "title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Sanyal",
        "Kaushik Roy"
      ],
      "abstract": "In the rapidly evolving field of vision-language navigation (VLN), ensuring\nsafety for physical agents remains an open challenge. For a human-in-the-loop\nlanguage-operated drone to navigate safely, it must understand natural language\ncommands, perceive the environment, and simultaneously avoid hazards in real\ntime. Control Barrier Functions (CBFs) are formal methods that enforce safe\noperating conditions. Model Predictive Control (MPC) is an optimization\nframework that plans a sequence of future actions over a prediction horizon,\nensuring smooth trajectory tracking while obeying constraints. In this work, we\nconsider a VLN-operated drone platform and enhance its safety by formulating a\nnovel scene-aware CBF that leverages ego-centric observations from a camera\nwhich has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less\nbaseline system uses a Vision-Language Encoder with cross-modal attention to\nconvert commands into an ordered sequence of landmarks. An object detection\nmodel identifies and verifies these landmarks in the captured images to\ngenerate a planned path. To further enhance safety, an Adaptive Safety Margin\nAlgorithm (ASMA) is proposed. ASMA tracks moving objects and performs\nscene-aware CBF evaluation on-the-fly, which serves as an additional constraint\nwithin the MPC framework. By continuously identifying potentially risky\nobservations, the system performs prediction in real time about unsafe\nconditions and proactively adjusts its control actions to maintain safe\nnavigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in\nthe Gazebo environment using the Robot Operating System (ROS), ASMA achieves\n64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in\ntrajectory lengths compared to the baseline CBF-less VLN.",
      "tldr_zh": "本研究针对视觉语言导航(VLN)中无人机的安全挑战，提出了一种自适应安全边际算法(ASMA)，通过结合场景感知控制屏障函数(CBF)和模型预测控制(MPC)，利用RGB-D相机观察实时跟踪移动物体并调整控制动作，确保安全导航。ASMA作为MPC框架中的额外约束，能在基线系统（基于视觉语言编码器和物体检测生成路径）基础上，动态评估风险并预测潜在不安全条件。实验结果显示，在Gazebo环境中部署于Parrot Bebop2无人机时，ASMA使成功率提高64%-67%，而轨迹长度仅增加1.4%-5.8%。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.IV",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10283v2",
      "published_date": "2024-09-16 13:44:50 UTC",
      "updated_date": "2025-03-10 21:51:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:27:31.759797"
    },
    {
      "arxiv_id": "2409.10281v1",
      "title": "DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis",
      "title_zh": "DreamHead: 通过分层扩散学习空间-时间对应关系，用于音频驱动的头部合成",
      "authors": [
        "Fa-Ting Hong",
        "Yunfei Liu",
        "Yu Li",
        "Changyin Zhou",
        "Fei Yu",
        "Dan Xu"
      ],
      "abstract": "Audio-driven talking head synthesis strives to generate lifelike video\nportraits from provided audio. The diffusion model, recognized for its superior\nquality and robust generalization, has been explored for this task. However,\nestablishing a robust correspondence between temporal audio cues and\ncorresponding spatial facial expressions with diffusion models remains a\nsignificant challenge in talking head generation. To bridge this gap, we\npresent DreamHead, a hierarchical diffusion framework that learns\nspatial-temporal correspondences in talking head synthesis without compromising\nthe model's intrinsic quality and adaptability.~DreamHead learns to predict\ndense facial landmarks from audios as intermediate signals to model the spatial\nand temporal correspondences.~Specifically, a first hierarchy of\naudio-to-landmark diffusion is first designed to predict temporally smooth and\naccurate landmark sequences given audio sequence signals. Then, a second\nhierarchy of landmark-to-image diffusion is further proposed to produce\nspatially consistent facial portrait videos, by modeling spatial\ncorrespondences between the dense facial landmark and appearance. Extensive\nexperiments show that proposed DreamHead can effectively learn spatial-temporal\nconsistency with the designed hierarchical diffusion and produce high-fidelity\naudio-driven talking head videos for multiple identities.",
      "tldr_zh": "该研究提出DreamHead，一种分层扩散框架，用于音频驱动的头部合成（Talking Head Synthesis），旨在学习音频与面部表情的空间-时序对应关系（spatial-temporal correspondence）。DreamHead 通过两层扩散模型实现：第一层为音频-to-landmark diffusion，预测平滑准确的地标序列；第二层为landmark-to-image diffusion，生成空间一致的高保真面部视频。实验结果显示，该框架能有效提升空间-时序一致性，支持多身份的高质量视频生成。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10281v1",
      "published_date": "2024-09-16 13:44:20 UTC",
      "updated_date": "2024-09-16 13:44:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:27:43.062792"
    },
    {
      "arxiv_id": "2409.10277v2",
      "title": "Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots",
      "title_zh": "Cognitive Kernel: 面向通用自动",
      "authors": [
        "Hongming Zhang",
        "Xiaoman Pan",
        "Hongwei Wang",
        "Kaixin Ma",
        "Wenhao Yu",
        "Dong Yu"
      ],
      "abstract": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
      "tldr_zh": "该论文介绍了Cognitive Kernel，一个开源的代理系统，旨在实现通用自动驾驶系统(generalist autopilots)，能够独立理解用户意图、主动从环境中获取信息并完成任务。系统采用model-centric设计，使用fine-tuned LLM作为中央策略模型，通过原子动作（如打开文件、点击按钮）与环境交互，提供比传统的environment-centric设计更灵活的信息流。实验评估显示，Cognitive Kernel在实时信息管理、私有信息管理和长期记忆管理等用例中，性能优于或相当其他闭源系统。该系统完全dockerized并开源，方便私有部署并推动LLM-driven autopilot系统的研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10277v2",
      "published_date": "2024-09-16 13:39:05 UTC",
      "updated_date": "2024-12-31 20:02:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:27:56.406557"
    },
    {
      "arxiv_id": "2409.10271v1",
      "title": "Causal Discovery in Recommender Systems: Example and Discussion",
      "title_zh": "推荐系统中的因果发现：示例与讨论",
      "authors": [
        "Emanuele Cavenaghi",
        "Fabio Stella",
        "Markus Zanker"
      ],
      "abstract": "Causality is receiving increasing attention by the artificial intelligence\nand machine learning communities. This paper gives an example of modelling a\nrecommender system problem using causal graphs. Specifically, we approached the\ncausal discovery task to learn a causal graph by combining observational data\nfrom an open-source dataset with prior knowledge. The resulting causal graph\nshows that only a few variables effectively influence the analysed feedback\nsignals. This contrasts with the recent trend in the machine learning community\nto include more and more variables in massive models, such as neural networks.",
      "tldr_zh": "这篇论文以推荐系统(Recommender Systems)为例，探讨了因果发现(Causal Discovery)的应用，并讨论了其在人工智能领域的意义。作者通过结合观察数据(Observational Data)从开源数据集和先验知识(Prior Knowledge)来学习因果图(Causal Graphs)，揭示了只有少数变量有效地影响反馈信号。结果与机器学习社区的趋势形成对比，后者倾向于在神经网络(Neural Networks)等大规模模型中纳入更多变量，从而强调了简化模型的潜在优势。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24",
      "pdf_url": "http://arxiv.org/pdf/2409.10271v1",
      "published_date": "2024-09-16 13:31:04 UTC",
      "updated_date": "2024-09-16 13:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:28:06.792255"
    },
    {
      "arxiv_id": "2410.01822v1",
      "title": "The Importance of Causality in Decision Making: A Perspective on Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Emanuele Cavenaghi",
        "Alessio Zanga",
        "Fabio Stella",
        "Markus Zanker"
      ],
      "abstract": "Causality is receiving increasing attention in the Recommendation Systems\n(RSs) community, which has realised that RSs could greatly benefit from\ncausality to transform accurate predictions into effective and explainable\ndecisions. Indeed, the RS literature has repeatedly highlighted that, in\nreal-world scenarios, recommendation algorithms suffer many types of biases\nsince assumptions ensuring unbiasedness are likely not met. In this discussion\npaper, we formulate the RS problem in terms of causality, using potential\noutcomes and structural causal models, by giving formal definitions of the\ncausal quantities to be estimated and a general causal graph to serve as a\nreference to foster future research and development.",
      "tldr_zh": "该论文强调了因果关系（Causality）在决策过程中的重要性，特别是针对推荐系统（Recommender Systems），以将准确预测转化为有效且可解释的决策。作者指出，推荐算法在现实场景中常受各种偏差影响，因为假设条件难以满足，因此提出使用潜在结果（Potential Outcomes）和结构因果模型（Structural Causal Models）来正式定义需估计的因果量。论文还提供了一个通用因果图（Causal Graph）作为参考框架，促进未来研究和开发，提升推荐系统的可靠性和解释性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24",
      "pdf_url": "http://arxiv.org/pdf/2410.01822v1",
      "published_date": "2024-09-16 13:30:41 UTC",
      "updated_date": "2024-09-16 13:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:28:18.097856"
    },
    {
      "arxiv_id": "2409.10267v1",
      "title": "Enhancing Personalized Recipe Recommendation Through Multi-Class Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Harish Neelam",
        "Koushik Sai Veerella"
      ],
      "abstract": "This paper intends to address the challenge of personalized recipe\nrecommendation in the realm of diverse culinary preferences. The problem domain\ninvolves recipe recommendations, utilizing techniques such as association\nanalysis and classification. Association analysis explores the relationships\nand connections between different ingredients to enhance the user experience.\nMeanwhile, the classification aspect involves categorizing recipes based on\nuser-defined ingredients and preferences. A unique aspect of the paper is the\nconsideration of recipes and ingredients belonging to multiple classes,\nrecognizing the complexity of culinary combinations. This necessitates a\nsophisticated approach to classification and recommendation, ensuring the\nsystem accommodates the nature of recipe categorization. The paper seeks not\nonly to recommend recipes but also to explore the process involved in achieving\naccurate and personalized recommendations.",
      "tldr_zh": "这篇论文针对多样化烹饪偏好的个性化食谱推荐挑战，提出通过多类分类（multi-class classification）和关联分析（association analysis）来提升推荐准确性。方法包括探索不同成分之间的关系，以优化用户体验，并基于用户定义的成分和偏好对食谱进行分类，处理食谱多类别复杂性。论文不仅提供推荐系统，还深入探讨实现精确个性化推荐的过程。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10267v1",
      "published_date": "2024-09-16 13:21:09 UTC",
      "updated_date": "2024-09-16 13:21:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:28:30.236003"
    },
    {
      "arxiv_id": "2409.10246v1",
      "title": "FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning",
      "title_zh": "翻译失败",
      "authors": [
        "Saif Khalid",
        "Hatem A. Rashwan",
        "Saddam Abdulwahab",
        "Mohamed Abdel-Nasser",
        "Facundo Manuel Quiroga",
        "Domenec Puig"
      ],
      "abstract": "The performance of diagnostic Computer-Aided Design (CAD) systems for retinal\ndiseases depends on the quality of the retinal images being screened. Thus,\nmany studies have been developed to evaluate and assess the quality of such\nretinal images. However, most of them did not investigate the relationship\nbetween the accuracy of the developed models and the quality of the\nvisualization of interpretability methods for distinguishing between gradable\nand non-gradable retinal images. Consequently, this paper presents a novel\nframework called FGR-Net to automatically assess and interpret underlying\nfundus image quality by merging an autoencoder network with a classifier\nnetwork. The FGR-Net model also provides an interpretable quality assessment\nthrough visualizations. In particular, FGR-Net uses a deep autoencoder to\nreconstruct the input image in order to extract the visual characteristics of\nthe input fundus images based on self-supervised learning. The extracted\nfeatures by the autoencoder are then fed into a deep classifier network to\ndistinguish between gradable and ungradable fundus images. FGR-Net is evaluated\nwith different interpretability methods, which indicates that the autoencoder\nis a key factor in forcing the classifier to focus on the relevant structures\nof the fundus images, such as the fovea, optic disk, and prominent blood\nvessels. Additionally, the interpretability methods can provide visual feedback\nfor ophthalmologists to understand how our model evaluates the quality of\nfundus images. The experimental results showed the superiority of FGR-Net over\nthe state-of-the-art quality assessment methods, with an accuracy of 89% and an\nF1-score of 87%.",
      "tldr_zh": "本文提出 FGR-Net 框架，通过结合 autoencoder 网络和 classifier network 基于深度重建学习，实现对视网膜图像的可评级性自动评估和可解释分类。FGR-Net 使用自监督学习提取图像特征，如 fovea、optic disk 和 blood vessels 等关键结构，然后通过分类器区分 gradable 和 ungradable 图像。实验结果显示，该框架在准确率达 89% 和 F1-score 达 87% 的基础上，优于现有方法，并通过 interpretability methods 提供可视化反馈，帮助眼科医生理解图像质量评估过程。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10246v1",
      "published_date": "2024-09-16 12:56:23 UTC",
      "updated_date": "2024-09-16 12:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:28:43.453854"
    },
    {
      "arxiv_id": "2409.10242v2",
      "title": "Hedging Is Not All You Need: A Simple Baseline for Online Learning Under Haphazard Inputs",
      "title_zh": "翻译失败",
      "authors": [
        "Himanshu Buckchash",
        "Momojit Biswas",
        "Rohit Agarwal",
        "Dilip K. Prasad"
      ],
      "abstract": "Handling haphazard streaming data, such as data from edge devices, presents a\nchallenging problem. Over time, the incoming data becomes inconsistent, with\nmissing, faulty, or new inputs reappearing. Therefore, it requires models that\nare reliable. Recent methods to solve this problem depend on a hedging-based\nsolution and require specialized elements like auxiliary dropouts, forked\narchitectures, and intricate network design. We observed that hedging can be\nreduced to a special case of weighted residual connection; this motivated us to\napproximate it with plain self-attention. In this work, we propose HapNet, a\nsimple baseline that is scalable, does not require online backpropagation, and\nis adaptable to varying input types. All present methods are restricted to\nscaling with a fixed window; however, we introduce a more complex problem of\nscaling with a variable window where the data becomes positionally\nuncorrelated, and cannot be addressed by present methods. We demonstrate that a\nvariant of the proposed approach can work even for this complex scenario. We\nextensively evaluated the proposed approach on five benchmarks and found\ncompetitive performance.",
      "tldr_zh": "这篇论文针对杂乱（haphazard）输入下的在线学习问题，如边缘设备数据可能出现缺失、错误或新输入，指出现有基于 hedging 的方法过于复杂，依赖辅助 dropout 和分支架构。作者提出 HapNet 作为简单基线，将 hedging 近似为加权残差连接并使用 plain self-attention 实现，从而实现可扩展性、不需在线 backpropagation，并能适应不同输入类型。实验结果显示，HapNet 不仅处理固定窗口场景表现出竞争性性能，还能应对更复杂的可变窗口问题，而现有方法无法有效解决。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10242v2",
      "published_date": "2024-09-16 12:45:03 UTC",
      "updated_date": "2024-12-30 12:03:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:28:55.151828"
    },
    {
      "arxiv_id": "2410.01821v1",
      "title": "NFDIcore 2.0: A BFO-Compliant Ontology for Multi-Domain Research Infrastructures",
      "title_zh": "翻译失败",
      "authors": [
        "Oleksandra Bruns",
        "Tabea Tietz",
        "Joerg Waitelonis",
        "Etienne Posthumus",
        "Harald Sack"
      ],
      "abstract": "This paper presents NFDIcore 2.0, an ontology compliant with the Basic Formal\nOntology (BFO) designed to represent the diverse research communities of the\nNational Research Data Infrastructure (NFDI) in Germany. NFDIcore ensures the\ninteroperability across various research disciplines, thereby facilitating\ncross-domain research. Each domain's individual requirements are addressed\nthrough specific ontology modules. This paper discusses lessons learned during\nthe ontology development and mapping process, supported by practical validation\nthrough use cases in diverse research domains. The originality of NFDIcore lies\nin its adherence to BFO, the use of SWRL rules for efficient knowledge\ndiscovery, and its modular, extensible design tailored to meet the needs of\nheterogeneous research domains.",
      "tldr_zh": "本研究介绍了NFDIcore 2.0，这是一个符合Basic Formal Ontology (BFO)的本体，旨在代表德国国家研究数据基础设施(NFDI)的多样化研究社区，并确保不同学科间的互操作性以促进跨领域研究。NFDIcore通过特定模块处理各领域的独特需求，并利用SWRL规则进行高效知识发现，其模块化和可扩展设计使其适用于异构研究环境。论文总结了本体开发和映射过程中的经验教训，并通过多领域用例进行实际验证，展示了其原创性和实用价值。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01821v1",
      "published_date": "2024-09-16 11:51:05 UTC",
      "updated_date": "2024-09-16 11:51:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:29:06.147973"
    },
    {
      "arxiv_id": "2409.10196v1",
      "title": "NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixi Cai",
        "Cristian Rojas Cardenas",
        "Kevin Leo",
        "Chenyuan Zhang",
        "Kal Backman",
        "Hanbing Li",
        "Boying Li",
        "Mahsa Ghorbanali",
        "Stavya Datta",
        "Lizhen Qu",
        "Julian Gutierrez Santiago",
        "Alexey Ignatiev",
        "Yuan-Fang Li",
        "Mor Vered",
        "Peter J Stuckey",
        "Maria Garcia de la Banda",
        "Hamid Rezatofighi"
      ],
      "abstract": "This paper addresses the problem of autonomous UAV search missions, where a\nUAV must locate specific Entities of Interest (EOIs) within a time limit, based\non brief descriptions in large, hazard-prone environments with keep-out zones.\nThe UAV must perceive, reason, and make decisions with limited and uncertain\ninformation. We propose NEUSIS, a compositional neuro-symbolic system designed\nfor interpretable UAV search and navigation in realistic scenarios. NEUSIS\nintegrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to\nprocess raw sensory inputs, maintains a probabilistic world model for\nenvironment representation, and uses a hierarchical planning component (SNaC)\nfor efficient path planning. Experimental results from simulated urban search\nmissions using AirSim and Unreal Engine show that NEUSIS outperforms a\nstate-of-the-art (SOTA) vision-language model and a SOTA search planning model\nin success rate, search efficiency, and 3D localization. These results\ndemonstrate the effectiveness of our compositional neuro-symbolic approach in\nhandling complex, real-world scenarios, making it a promising solution for\nautonomous UAV systems in search missions.",
      "tldr_zh": "这篇论文针对UAV在大型、危险环境中搜索特定Entities of Interest (EOIs)的自主任务，提出了一种组合式neuro-symbolic框架NEUSIS，以处理感知、推理和规划的挑战。NEUSIS整合了neuro-symbolic visual perception、推理和grounding (GRiD)模块，维护probabilistic world model用于环境表示，并采用hierarchical planning组件(SNaC)实现高效路径规划。实验在AirSim和Unreal Engine的模拟城市搜索任务中显示，NEUSIS在成功率、搜索效率和3D定位上优于SOTA的视觉语言模型和搜索规划模型。这些结果证明了组合式neuro-symbolic方法在复杂真实场景中的有效性，为UAV自主系统提供了可靠的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10196v1",
      "published_date": "2024-09-16 11:42:15 UTC",
      "updated_date": "2024-09-16 11:42:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:29:20.092249"
    },
    {
      "arxiv_id": "2409.16317v1",
      "title": "A Literature Review of Keyword Spotting Technologies for Urdu",
      "title_zh": "乌尔都语关键词识别技术的文献综述",
      "authors": [
        "Syed Muhammad Aqdas Rizvi"
      ],
      "abstract": "This literature review surveys the advancements of keyword spotting (KWS)\ntechnologies, specifically focusing on Urdu, Pakistan's low-resource language\n(LRL), which has complex phonetics. Despite the global strides in speech\ntechnology, Urdu presents unique challenges requiring more tailored solutions.\nThe review traces the evolution from foundational Gaussian Mixture Models to\nsophisticated neural architectures like deep neural networks and transformers,\nhighlighting significant milestones such as integrating multi-task learning and\nself-supervised approaches that leverage unlabeled data. It examines emerging\ntechnologies' role in enhancing KWS systems' performance within multilingual\nand resource-constrained settings, emphasizing the need for innovations that\ncater to languages like Urdu. Thus, this review underscores the need for\ncontext-specific research addressing the inherent complexities of Urdu and\nsimilar URLs and the means of regions communicating through such languages for\na more inclusive approach to speech technology.",
      "tldr_zh": "这篇文献综述系统回顾了Urdu关键词识别（KWS）技术的进展，聚焦于该低资源语言（LRL）的复杂语音学挑战，并强调了全球语音技术在Urdu上的独特需求。综述追溯了从Gaussian Mixture Models到deep neural networks和transformers的演变，突出multi-task learning和self-supervised approaches等方法如何利用无标签数据提升系统性能。最终，它呼吁开展更多针对Urdu类似语言的上下文特定研究，以推动多语言和资源受限环境下的包容性语音技术创新。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.16317v1",
      "published_date": "2024-09-16 11:39:10 UTC",
      "updated_date": "2024-09-16 11:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:29:31.606890"
    },
    {
      "arxiv_id": "2409.10193v1",
      "title": "Relative Positioning for Aerial Robot Path Planning in GPS Denied Environment",
      "title_zh": "在GPS拒绝环境下的飞行机器人路径规划相对定位",
      "authors": [
        "Farzad Sanati"
      ],
      "abstract": "One of the most useful applications of intelligent aerial robots sometimes\ncalled Unmanned Aerial Vehicles (UAV) in Australia is known to be in bushfire\nmonitoring and prediction operations. A swarm of autonomous drones/UAVs\nprogrammed to work in real-time observing the fire parameters using their\nonboard sensors would be valuable in reducing the life-threatening impact of\nthat fire. However autonomous UAVs face serious challenges in their positioning\nand navigation in critical bushfire conditions such as remoteness and severe\nweather conditions where GPS signals could also be unreliable. This paper\ntackles one of the most important factors in autonomous UAV navigation, namely\nInitial Positioning sometimes called Localisation. The solution provided by\nthis paper will enable a team of autonomous UAVs to establish a relative\nposition to their base of operation to be able to commence a team search and\nreconnaissance in a bushfire-affected area and find their way back to their\nbase without the help of GPS signals.",
      "tldr_zh": "这篇论文针对GPS信号不可靠的偏远环境（如澳大利亚丛林火灾区域），提出了一种相对定位(Relative Positioning)方法，用于解决自主无人机(UAV) 的初始定位(Localisation)挑战。方法允许UAV团队在没有GPS支持的情况下，建立相对于基地的位置，从而进行团队搜索和侦察任务，并安全返回基地。该技术为UAV在恶劣天气条件下的导航提供了可靠解决方案，有助于提升丛林火灾监测的效率和安全性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 4 images",
      "pdf_url": "http://arxiv.org/pdf/2409.10193v1",
      "published_date": "2024-09-16 11:35:39 UTC",
      "updated_date": "2024-09-16 11:35:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:29:42.660788"
    },
    {
      "arxiv_id": "2409.10177v2",
      "title": "Augmenting Automatic Speech Recognition Models with Disfluency Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Robin Amann",
        "Zhaolin Li",
        "Barbara Bruno",
        "Jan Niehues"
      ],
      "abstract": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.",
      "tldr_zh": "本文提出了一种仅需推理（inference-only）的'approche，来增强 Automatic Speech Recognition (ASR) 模型对开放集 disfluency 的检测能力，以解决 ASR 在处理对话性语音中的口误和犹豫问题。方法包括使用修改后的 Connectionist Temporal Classification (CTC)-based forced alignment 算法预测词级时间戳，并开发一个模型对时间戳间隙进行分类，以区分 disfluent 语音和沉默。该模型在 disfluent 数据集上实现了 81.62% 准确率和 80.07% F1 分数，并成功捕捉了 74.13% 原本被 ASR 遗漏的单词，从而为改进语音转录和下游任务提供潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by SLT2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10177v2",
      "published_date": "2024-09-16 11:13:14 UTC",
      "updated_date": "2024-09-17 06:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:29:56.404455"
    },
    {
      "arxiv_id": "2409.10173v3",
      "title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA",
      "title_zh": "翻译失败",
      "authors": [
        "Saba Sturua",
        "Isabelle Mohr",
        "Mohammad Kalim Akram",
        "Michael Günther",
        "Bo Wang",
        "Markus Krimmel",
        "Feng Wang",
        "Georgios Mastrapas",
        "Andreas Koukounas",
        "Nan Wang",
        "Han Xiao"
      ],
      "abstract": "We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Evaluation on the MTEB benchmark\nshows that jina-embeddings-v3 outperforms the latest proprietary embeddings\nfrom OpenAI and Cohere on English tasks, while achieving superior performance\ncompared to multilingual-e5-large-instruct across all multilingual tasks. With\na default output dimension of 1024, users can flexibly reduce the embedding\ndimensions to as low as 32 without compromising performance, enabled by\nMatryoshka Representation Learning.",
      "tldr_zh": "我们介绍了 jina-embeddings-v3，这是一个拥有 570 百万参数的多语言文本嵌入模型，支持高达 8192 标记的上下文长度，并在多语言数据和长上下文检索任务上实现了最先进性能。模型通过任务特定的 Low-Rank Adaptation (LoRA) 适配器生成高质量嵌入，支持查询-文档检索、聚类、分类和文本匹配。在 MTEB 基准测试中，它在英语任务上超过了 OpenAI 和 Cohere 的最新专有模型，在多语言任务上优于 multilingual-e5-large-instruct；此外，用户可以灵活地将嵌入维度从默认的 1024 降低到 32，而不影响性能，借助 Matryoshka Representation Learning。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, pp11-13 references, pp14-20 appendix and experiment tables",
      "pdf_url": "http://arxiv.org/pdf/2409.10173v3",
      "published_date": "2024-09-16 11:10:29 UTC",
      "updated_date": "2024-09-19 11:21:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:30:07.925945"
    },
    {
      "arxiv_id": "2409.10168v2",
      "title": "Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation Between the United States and South Africa",
      "title_zh": "翻译失败",
      "authors": [
        "Hayoung Jung",
        "Prerna Juneja",
        "Tanushree Mitra"
      ],
      "abstract": "Despite being an integral tool for finding health-related information online,\nYouTube has faced criticism for disseminating COVID-19 misinformation globally\nto its users. Yet, prior audit studies have predominantly investigated YouTube\nwithin the Global North contexts, often overlooking the Global South. To\naddress this gap, we conducted a comprehensive 10-day geolocation-based audit\non YouTube to compare the prevalence of COVID-19 misinformation in search\nresults between the United States (US) and South Africa (SA), the countries\nheavily affected by the pandemic in the Global North and the Global South,\nrespectively. For each country, we selected 3 geolocations and placed\nsock-puppets, or bots emulating \"real\" users, that collected search results for\n48 search queries sorted by 4 search filters for 10 days, yielding a dataset of\n915K results. We found that 31.55% of the top-10 search results contained\nCOVID-19 misinformation. Among the top-10 search results, bots in SA faced\nsignificantly more misinformative search results than their US counterparts.\nOverall, our study highlights the contrasting algorithmic behaviors of YouTube\nsearch between two countries, underscoring the need for the platform to\nregulate algorithmic behavior consistently across different regions of the\nGlobe.",
      "tldr_zh": "这篇论文通过地理位置审计（geolocation audit）比较了 YouTube 在美国（US）和南非（SA）搜索结果中 COVID-19 错误信息（COVID-19 misinformation）的 prevalence，旨在填补以往研究偏重全球北方（Global North）而忽略全球南方（Global South）的空白。研究方法涉及使用 sock-puppets（模拟真实用户的 bots）在每个国家选择 3 个地理位置，收集 48 个搜索查询的 915K 结果，并按 4 个搜索过滤器分析 10 天数据。结果显示，31.55% 的前 10 搜索结果包含错误信息，且南非的搜索结果显著多于美国，突显 YouTube 算法在不同区域的差异，并呼吁平台实现全球一致的算法调节。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "30 pages. Accepted at ICWSM 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.10168v2",
      "published_date": "2024-09-16 10:56:43 UTC",
      "updated_date": "2025-04-14 21:13:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:30:20.992579"
    },
    {
      "arxiv_id": "2409.10164v1",
      "title": "Quantile Regression for Distributional Reward Models in RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolai Dorka"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.",
      "tldr_zh": "这篇论文提出 Quantile Reward Models (QRMs)，一种用于强化学习从人类反馈 (RLHF) 的新方法，通过 quantile regression 学习奖励的分布，而不是传统点估计，从而更全面地捕捉人类偏好的多样性和复杂性。QRMs 可以处理标签噪声、容纳冲突偏好（如多模态分布），并提供更细致的偏好表示。实验结果显示，QRMs 在 RewardBench 上优于传统模型，并在下游应用如风险感知强化学习中，帮助生成更少的极端负面响应。论文还开源了代码和模型，以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10164v1",
      "published_date": "2024-09-16 10:54:04 UTC",
      "updated_date": "2024-09-16 10:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:30:32.268605"
    },
    {
      "arxiv_id": "2409.10161v3",
      "title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Nomaan Qureshi",
        "Sparsh Garg",
        "Francisco Yandun",
        "David Held",
        "George Kantor",
        "Abhisesh Silwal"
      ],
      "abstract": "Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data. Videos can be found on our project page:\nhttps://splatsim.github.io",
      "tldr_zh": "该研究提出 SplatSim 框架，利用 Gaussian Splatting 作为渲染原语，旨在减少 RGB 图像基于操控策略的 Sim2Real 转移中的领域偏移问题。\nSplatSim 通过在模拟器中替换传统网格表示为 Gaussian Splats，生成高度逼真的合成数据，同时保持模拟的可扩展性和成本效率。\n实验结果显示，在 SplatSim 中训练的策略实现零样本部署，平均成功率达到 86.25%，接近在真实数据上训练的 97.5%。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10161v3",
      "published_date": "2024-09-16 10:52:16 UTC",
      "updated_date": "2024-10-07 03:37:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:30:43.941646"
    },
    {
      "arxiv_id": "2409.10151v1",
      "title": "AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from Whole-Body PET/CT Images",
      "title_zh": "翻译失败",
      "authors": [
        "Shadab Ahamed"
      ],
      "abstract": "Automated segmentation of cancerous lesions in PET/CT scans is a crucial\nfirst step in quantitative image analysis. However, training deep learning\nmodels for segmentation with high accuracy is particularly challenging due to\nthe variations in lesion size, shape, and radiotracer uptake. These lesions can\nappear in different parts of the body, often near healthy organs that also\nexhibit considerable uptake, making the task even more complex. As a result,\ncreating an effective segmentation model for routine PET/CT image analysis is\nchallenging. In this study, we utilized a 3D Residual UNet model and employed\nthe Generalized Dice Focal Loss function to train the model on the AutoPET\nChallenge 2024 dataset. We conducted a 5-fold cross-validation and used an\naverage ensembling technique using the models from the five folds. In the\npreliminary test phase for Task-1, the average ensemble achieved a mean Dice\nSimilarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of\n10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about\nthe algorithm can be found on our GitHub repository:\nhttps://github.com/ahxmeds/autosegnet2024.git. The training code has been\nshared via the repository: https://github.com/ahxmeds/autopet2024.git.",
      "tldr_zh": "本研究评估了在 AutoPET Challenge III 中，使用 Generalized Dice Focal Loss 函数训练的 3D Residual UNet 模型，用于从全身 PET/CT 图像中分割 FDG 和 PSMA 病变。该方法针对病变大小、形状和放射示踪剂摄取的变异性进行了优化，通过 5 折交叉验证和平均集成技术在 AutoPET Challenge 2024 数据集上训练模型。在初步测试（Task-1）中，模型实现了平均 Dice Similarity Coefficient (DSC) 为 0.6687、平均假阴性体积 (FNV) 为 10.9522 ml 和平均假阳性体积 (FPV) 为 2.9684 ml 的性能，展示了其稳健性和准确性。代码已开源在 GitHub 仓库中，便于进一步应用和验证。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2309.13553",
      "pdf_url": "http://arxiv.org/pdf/2409.10151v1",
      "published_date": "2024-09-16 10:27:30 UTC",
      "updated_date": "2024-09-16 10:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:30:57.471523"
    },
    {
      "arxiv_id": "2409.10146v1",
      "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge",
      "title_zh": "LL",
      "authors": [
        "Hamed Babaei Giglou",
        "Jennifer D'Souza",
        "Sören Auer"
      ],
      "abstract": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.",
      "tldr_zh": "这篇论文概述了 LLMs4OL 2024，这是首个 Large Language Models for Ontology Learning (LLMs4OL) 挑战，旨在探索大型语言模型（LLMs）在 Ontology Learning (OL) 中的潜力，以提升网络的结构化知识和互操作性。挑战作为第23届 International Semantic Web Conference (ISWC) 的社区活动，鼓励创新并推进语义网的目标。论文总结了此次挑战的贡献，为构建更智能的用户友好型网络提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 1 figure, Will appear in \"The 1st LLMs4OL Challenge @ ISWC\n  2024\" proceedings",
      "pdf_url": "http://arxiv.org/pdf/2409.10146v1",
      "published_date": "2024-09-16 10:15:30 UTC",
      "updated_date": "2024-09-16 10:15:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:31:07.332932"
    },
    {
      "arxiv_id": "2409.10139v1",
      "title": "Towards Explainable Automated Data Quality Enhancement without Domain Knowledge",
      "title_zh": "迈向无需领域知识的可解释自动数据质量提升",
      "authors": [
        "Djibril Sarr"
      ],
      "abstract": "In the era of big data, ensuring the quality of datasets has become\nincreasingly crucial across various domains. We propose a comprehensive\nframework designed to automatically assess and rectify data quality issues in\nany given dataset, regardless of its specific content, focusing on both textual\nand numerical data. Our primary objective is to address three fundamental types\nof defects: absence, redundancy, and incoherence. At the heart of our approach\nlies a rigorous demand for both explainability and interpretability, ensuring\nthat the rationale behind the identification and correction of data anomalies\nis transparent and understandable. To achieve this, we adopt a hybrid approach\nthat integrates statistical methods with machine learning algorithms. Indeed,\nby leveraging statistical techniques alongside machine learning, we strike a\nbalance between accuracy and explainability, enabling users to trust and\ncomprehend the assessment process. Acknowledging the challenges associated with\nautomating the data quality assessment process, particularly in terms of time\nefficiency and accuracy, we adopt a pragmatic strategy, employing\nresource-intensive algorithms only when necessary, while favoring simpler, more\nefficient solutions whenever possible. Through a practical analysis conducted\non a publicly provided dataset, we illustrate the challenges that arise when\ntrying to enhance data quality while keeping explainability. We demonstrate the\neffectiveness of our approach in detecting and rectifying missing values,\nduplicates and typographical errors as well as the challenges remaining to be\naddressed to achieve similar accuracy on statistical outliers and logic errors\nunder the constraints set in our work.",
      "tldr_zh": "这篇论文提出一个无需领域知识的框架，用于自动评估和提升数据集的质量，针对文本和数字数据，主要处理缺失、冗余和不一致性问题。该框架采用统计方法与机器学习算法的混合方法，确保评估过程的可解释性和可解释性，同时优先选择高效解决方案以平衡准确性和资源消耗。在公开数据集上的实验中，该方法在检测和修复缺失值、重复及打字错误方面表现出色，但处理统计异常和逻辑错误时仍存在挑战。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "stat.ML",
        "62H30, 68P99",
        "H.2.7; H.2.8; I.2.1"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10139v1",
      "published_date": "2024-09-16 10:08:05 UTC",
      "updated_date": "2024-09-16 10:08:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:31:19.070895"
    },
    {
      "arxiv_id": "2409.10134v1",
      "title": "Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor Coastal Lagoon Ecosystem in the South Western Mediterranean",
      "title_zh": "朝着海洋数字孪生平台前进：建模西南地中海的Mar Menor沿海泻湖生态系统",
      "authors": [
        "Yu Ye",
        "Aurora González-Vidal",
        "Alejandro Cisterna-García",
        "Angel Pérez-Ruzafa",
        "Miguel A. Zamora Izquierdo",
        "Antonio F. Skarmeta"
      ],
      "abstract": "Coastal marine ecosystems face mounting pressures from anthropogenic\nactivities and climate change, necessitating advanced monitoring and modeling\napproaches for effective management. This paper pioneers the development of a\nMarine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon\nEcosystem in the Region of Murcia. The platform leverages Artificial\nIntelligence to emulate complex hydrological and ecological models,\nfacilitating the simulation of what-if scenarios to predict ecosystem responses\nto various stressors. We integrate diverse datasets from public sources to\nconstruct a comprehensive digital representation of the lagoon's dynamics. The\nplatform's modular design enables real-time stakeholder engagement and informed\ndecision-making in marine management. Our work contributes to the ongoing\ndiscourse on advancing marine science through innovative digital twin\ntechnologies.",
      "tldr_zh": "本论文针对沿海海洋生态系统面临的人类活动和气候变化压力，开发了一个 Marine Digital Twin Platform，用于模拟西班牙 Mar Menor 沿海泻湖生态系统。该平台运用 Artificial Intelligence 技术来模拟复杂的 hydrological 和 ecological 模型，并整合多种公共数据集，构建泻湖动态的全面数字表示，支持假设场景模拟以预测生态响应。平台采用模块化设计，促进实时利益相关者参与和海洋管理的决策决策。该工作为通过数字孪生技术推进海洋科学提供了重要贡献。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10134v1",
      "published_date": "2024-09-16 10:01:18 UTC",
      "updated_date": "2024-09-16 10:01:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:31:31.646202"
    },
    {
      "arxiv_id": "2409.10132v1",
      "title": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models",
      "title_zh": "StruEdit：结构化输出实现大型语言模型的快速且准确知识编辑",
      "authors": [
        "Baolong Bi",
        "Shenghua Liu",
        "Yiwei Wang",
        "Lingrui Mei",
        "Hongcheng Gao",
        "Junfeng Fang",
        "Xueqi Cheng"
      ],
      "abstract": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.",
      "tldr_zh": "该论文针对大型语言模型 (LLMs) 在更新知识时的挑战，提出 StruEdit 方法，以解决编辑自然语言输出中过时知识的准确性和效率问题。StruEdit 通过提示 LLMs 生成结构化的推理三元组 (reasoning triplets)，然后在单步中移除过时信息并填充最新内容，从而确保编辑过程的连贯性和速度。实验结果表明，StruEdit 与其他知识编辑方法相比，实现了最高的准确率和最低的延迟。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10132v1",
      "published_date": "2024-09-16 09:48:56 UTC",
      "updated_date": "2024-09-16 09:48:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:31:53.535961"
    },
    {
      "arxiv_id": "2410.01820v2",
      "title": "PixelBytes: Catching Unified Representation for Multimodal Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Fabien Furfaro"
      ],
      "abstract": "This report presents PixelBytes, an approach for unified multimodal\nrepresentation learning. Drawing inspiration from sequence models like Image\nTransformers, PixelCNN, and Mamba-Bytes, we explore integrating text, audio,\naction-state, and pixelated images (sprites) into a cohesive representation. We\nconducted experiments on a PixelBytes Pokemon dataset and an Optimal-Control\ndataset. Our investigation covered various model architectures, including\nRecurrent Neural Networks (RNNs), State Space Models (SSMs), and\nAttention-based models, with a focus on bidirectional processing and our PxBy\nembedding technique. We evaluated models based on data reduction strategies and\nautoregressive learning, specifically examining Long Short-Term Memory (LSTM)\nnetworks in predictive and autoregressive modes. Our results indicate that\nautoregressive models perform better than predictive models in this context.\nAdditionally, we found that diffusion models can be applied to control problems\nand parallelized generation. PixelBytes aims to contribute to the development\nof foundation models for multimodal data processing and generation. The\nproject's code, models, and datasets are available online.",
      "tldr_zh": "该研究提出 PixelBytes 框架，用于统一多模态表示学习，整合文本、音频、动作状态和像素化图像（sprites），受 Image Transformers、PixelCNN 和 Mamba-Bytes 等序列模型启发。研究者通过在 PixelBytes Pokemon 数据集和 Optimal-Control 数据集上进行实验，探索了 RNN、SSM 和 Attention-based 模型等架构，重点评估双向处理、PxBy 嵌入技术以及自回归学习。结果显示，自回归模型（如 LSTM 在自回归模式）优于预测模型，且扩散模型可应用于控制问题和并行生成，为多模态数据处理和生成的基础模型发展提供了贡献。项目代码、模型和数据集已公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.01820v2",
      "published_date": "2024-09-16 09:20:13 UTC",
      "updated_date": "2024-10-20 16:08:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:31:55.834901"
    },
    {
      "arxiv_id": "2409.11439v2",
      "title": "Machine listening in a neonatal intensive care unit",
      "title_zh": "翻译失败",
      "authors": [
        "Modan Tailleur",
        "Vincent Lostanlen",
        "Jean-Philippe Rivière",
        "Pierre Aumond"
      ],
      "abstract": "Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.",
      "tldr_zh": "本论文探讨了在新生儿重症监护病房（NICU）中进行机器监听，以检测医院常见声音源如氧气机、警报器和脚步声，从而服务于环境心理学研究。针对隐私保护和标签数据有限的挑战，该方法结合边缘计算和云计算：使用声学传感器实时计算三音阶谱图而非记录音频波形，并通过谱转码和标签空间适应重用预训练音频神经网络（PANN）。小规模实验结果显示，检测的事件时间序列与电子徽章测量数据一致，证明了隐私设计下多音机器监听的可行性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11439v2",
      "published_date": "2024-09-16 09:19:19 UTC",
      "updated_date": "2024-10-07 11:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:32:08.454040"
    },
    {
      "arxiv_id": "2409.10106v1",
      "title": "Industry 6.0: New Generation of Industry driven by Generative AI and Swarm of Heterogeneous Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Artem Lykov",
        "Miguel Altamirano Cabrera",
        "Mikhail Konenkov",
        "Valerii Serpiva",
        "Koffivi Fid`ele Gbagbe",
        "Ali Alabbas",
        "Aleksey Fedoseev",
        "Luis Moreno",
        "Muhammad Haris Khan",
        "Ziang Guo",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing.",
      "tldr_zh": "本文提出 Industry 6.0 概念，这是一个由生成式 AI 和异构机器人群驱动的全自动生产系统，能基于用户自然语言描述自主完成产品设计、制造、物流和组装。系统利用集成 Large Language Models (LLMs) 的机器人（如机械臂、配送无人机和 3D 打印机）来协调整个过程，通过 API 和本地部署进行评估。实验结果显示，该系统将平均生产时间缩短至 119.10 分钟，比人类专家快 4.4 倍，并在产品蓝图阶段比人类 CAD 操作员快 47 倍（0.5 分钟 vs 23.5 分钟）。这一突破推动了完全自主制造的实现。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "submitted to IEEE conf",
      "pdf_url": "http://arxiv.org/pdf/2409.10106v1",
      "published_date": "2024-09-16 09:12:06 UTC",
      "updated_date": "2024-09-16 09:12:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:32:20.634893"
    },
    {
      "arxiv_id": "2409.10102v1",
      "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Yujia Zhou",
        "Yan Liu",
        "Xiaoxi Li",
        "Jiajie Jin",
        "Hongjin Qian",
        "Zheng Liu",
        "Chaozhuo Li",
        "Zhicheng Dou",
        "Tsung-Yi Ho",
        "Philip S. Yu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.",
      "tldr_zh": "这篇调查论文探讨了检索增强生成（Retrieval-Augmented Generation, RAG）系统在大型语言模型（Large Language Models, LLMs）中的可信度问题，强调RAG能通过外部知识库减少幻觉（hallucination），但也可能因不当检索而产生不良内容。作者提出一个统一框架，从六个关键维度——factuality（事实性）、robustness（鲁棒性）、fairness（公平性）、transparency（透明性）、accountability（责任性）和privacy（隐私）——评估RAG系统的可信度，并对现有文献进行了全面回顾。论文还创建了评估基准，对多种专有和开源模型进行了实验评估，并指出了未来研究的潜在挑战，以为提升RAG在实际应用中的可信度提供指导。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10102v1",
      "published_date": "2024-09-16 09:06:44 UTC",
      "updated_date": "2024-09-16 09:06:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:32:32.054828"
    },
    {
      "arxiv_id": "2409.10585v2",
      "title": "Motion Forecasting via Model-Based Risk Minimization",
      "title_zh": "翻译失败",
      "authors": [
        "Aron Distelzweig",
        "Eitan Kosman",
        "Andreas Look",
        "Faris Janjoš",
        "Denesh K. Manivannan",
        "Abhinav Valada"
      ],
      "abstract": "Forecasting the future trajectories of surrounding agents is crucial for\nautonomous vehicles to ensure safe, efficient, and comfortable route planning.\nWhile model ensembling has improved prediction accuracy in various fields, its\napplication in trajectory prediction is limited due to the multi-modal nature\nof predictions. In this paper, we propose a novel sampling method applicable to\ntrajectory prediction based on the predictions of multiple models. We first\nshow that conventional sampling based on predicted probabilities can degrade\nperformance due to missing alignment between models. To address this problem,\nwe introduce a new method that generates optimal trajectories from a set of\nneural networks, framing it as a risk minimization problem with a variable loss\nfunction. By using state-of-the-art models as base learners, our approach\nconstructs diverse and effective ensembles for optimal trajectory sampling.\nExtensive experiments on the nuScenes prediction dataset demonstrate that our\nmethod surpasses current state-of-the-art techniques, achieving top ranks on\nthe leaderboard. We also provide a comprehensive empirical study on ensembling\nstrategies, offering insights into their effectiveness. Our findings highlight\nthe potential of advanced ensembling techniques in trajectory prediction,\nsignificantly improving predictive performance and paving the way for more\nreliable predicted trajectories.",
      "tldr_zh": "这篇论文针对自动驾驶车辆的轨迹预测问题，提出了一种基于模型风险最小化（Model-Based Risk Minimization）的创新采样方法，以解决多模型集成（model ensembling）在多模态预测中的局限性。方法通过将轨迹生成框架化为风险最小化问题，使用变量损失函数和最先进模型作为基学习器，构建多样化和有效的集成策略，从而优化预测准确性。实验在 nuScenes 数据集上表明，该方法超越当前最先进技术，在排行榜上排名靠前，并通过全面实证研究提供了集成策略的宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, submitted to IEEE International Conference on\n  Robotics & Automation (2025)",
      "pdf_url": "http://arxiv.org/pdf/2409.10585v2",
      "published_date": "2024-09-16 09:03:28 UTC",
      "updated_date": "2024-09-20 08:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:32:44.327813"
    },
    {
      "arxiv_id": "2409.10085v1",
      "title": "A Riemannian Approach to Ground Metric Learning for Optimal Transport",
      "title_zh": "翻译失败",
      "authors": [
        "Pratik Jawanpuria",
        "Dai Shi",
        "Bamdev Mishra",
        "Junbin Gao"
      ],
      "abstract": "Optimal transport (OT) theory has attracted much attention in machine\nlearning and signal processing applications. OT defines a notion of distance\nbetween probability distributions of source and target data points. A crucial\nfactor that influences OT-based distances is the ground metric of the embedding\nspace in which the source and target data points lie. In this work, we propose\nto learn a suitable latent ground metric parameterized by a symmetric positive\ndefinite matrix. We use the rich Riemannian geometry of symmetric positive\ndefinite matrices to jointly learn the OT distance along with the ground\nmetric. Empirical results illustrate the efficacy of the learned metric in\nOT-based domain adaptation.",
      "tldr_zh": "这篇论文提出了一种基于 Riemannian 几何的方法，用于学习 Optimal Transport (OT) 中的 ground metric，以优化源数据点和目标数据点概率分布之间的距离。作者将 ground metric 参数化为对称正定矩阵，并利用其丰富的 Riemannian 几何特性来联合学习 OT 距离和 metric。实验结果证明，该方法在 OT-based domain adaptation 任务中表现出色，提升了模型的效能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10085v1",
      "published_date": "2024-09-16 08:42:56 UTC",
      "updated_date": "2024-09-16 08:42:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:32:55.068866"
    },
    {
      "arxiv_id": "2409.10584v2",
      "title": "Manifold-Constrained Nucleus-Level Denoising Diffusion Model for Structure-Based Drug Design",
      "title_zh": "翻译失败",
      "authors": [
        "Shengchao Liu",
        "Divin Yan",
        "Weitao Du",
        "Weiyang Liu",
        "Zhuoxinran Li",
        "Hongyu Guo",
        "Christian Borgs",
        "Jennifer Chayes",
        "Anima Anandkumar"
      ],
      "abstract": "Artificial intelligence models have shown great potential in structure-based\ndrug design, generating ligands with high binding affinities. However, existing\nmodels have often overlooked a crucial physical constraint: atoms must maintain\na minimum pairwise distance to avoid separation violation, a phenomenon\ngoverned by the balance of attractive and repulsive forces. To mitigate such\nseparation violations, we propose NucleusDiff. It models the interactions\nbetween atomic nuclei and their surrounding electron clouds by enforcing the\ndistance constraint between the nuclei and manifolds. We quantitatively\nevaluate NucleusDiff using the CrossDocked2020 dataset and a COVID-19\ntherapeutic target, demonstrating that NucleusDiff reduces violation rate by up\nto 100.00% and enhances binding affinity by up to 22.16%, surpassing\nstate-of-the-art models for structure-based drug design. We also provide\nqualitative analysis through manifold sampling, visually confirming the\neffectiveness of NucleusDiff in reducing separation violations and improving\nbinding affinities.",
      "tldr_zh": "本研究针对基于结构的药物设计中，AI 模型忽略原子间最小配对距离约束（separation violation）的问题，提出了一种新的 NucleusDiff 模型。该模型通过在 denoising diffusion 框架中强制核间距离和 manifolds 约束，模拟原子核与电子云的相互作用，从而生成更符合物理规律的配体分子的候选物。在 CrossDocked2020 数据集和 COVID-19 治疗靶点上的评估显示，NucleusDiff 将分离违反率降低高达 100%，并将结合亲和力提升高达 22.16%，优于现有最先进模型。通过 manifolds 采样进行的定性分析进一步证实了其在减少违反和改善亲和力方面的有效性。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "q-bio.BM",
        "stat.ML"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10584v2",
      "published_date": "2024-09-16 08:42:46 UTC",
      "updated_date": "2024-09-30 15:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:33:07.592853"
    },
    {
      "arxiv_id": "2409.10080v2",
      "title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchen Guo",
        "Ruoxiang Xu",
        "Rongcheng Li",
        "Zhenghao Wu",
        "Weifeng Su"
      ],
      "abstract": "In extreme scenarios such as nighttime or low-visibility environments,\nachieving reliable perception is critical for applications like autonomous\ndriving, robotics, and surveillance. Multi-modality image fusion, particularly\nintegrating infrared imaging, offers a robust solution by combining\ncomplementary information from different modalities to enhance scene\nunderstanding and decision-making. However, current methods face significant\nlimitations: GAN-based approaches often produce blurry images that lack\nfine-grained details, while AE-based methods may introduce bias toward specific\nmodalities, leading to unnatural fusion results. To address these challenges,\nwe propose DAE-Fuse, a novel two-phase discriminative autoencoder framework\nthat generates sharp and natural fused images. Furthermore, We pioneer the\nextension of image fusion techniques from static images to the video domain\nwhile preserving temporal consistency across frames, thus advancing the\nperceptual capabilities required for autonomous navigation. Extensive\nexperiments on public datasets demonstrate that DAE-Fuse achieves\nstate-of-the-art performance on multiple benchmarks, with superior\ngeneralizability to tasks like medical image fusion.",
      "tldr_zh": "这篇论文针对多模态图像融合（Multi-Modality Image Fusion）在极端环境中的挑战，提出了一种自适应判别自编码器（DAE-Fuse），通过两阶段框架解决现有 GAN-based 方法的图像模糊和 AE-based 方法的模式偏置问题，从而生成锐利且自然的融合图像。创新点在于首次将图像融合技术从静态图像扩展到视频领域，确保帧间的时间一致性（temporal consistency），以提升自主导航等应用的感知能力。实验在公共数据集上证明，DAE-Fuse 实现了最先进性能，并在多个基准测试中表现出色，具有良好的泛化性，例如应用于医疗图像融合。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10080v2",
      "published_date": "2024-09-16 08:37:09 UTC",
      "updated_date": "2024-12-24 15:22:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:33:20.534722"
    },
    {
      "arxiv_id": "2409.10077v1",
      "title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Le Xiao",
        "Yunfei Xu",
        "Jing Zhao"
      ],
      "abstract": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.",
      "tldr_zh": "本研究针对中文煤炭化工领域的命名实体识别 (NER) 问题，提出了一种基于大型语言模型 (LLMs) 的框架 LLM-DER，以应对数据稀缺和复杂实体结构（如多个实体共享或多重关系）的挑战。LLM-DER 通过生成包含实体类型的关系列表，并设计可信度和一致性评估方法来过滤误识别实体，从而提升识别准确性。在 Resume 数据集和自建 Coal 数据集上的实验显示，该方法不仅超过了 GPT-3.5-turbo 基线，还优于全监督基线，证明了其在特定领域实体识别的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10077v1",
      "published_date": "2024-09-16 08:28:05 UTC",
      "updated_date": "2024-09-16 08:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:33:32.537608"
    },
    {
      "arxiv_id": "2409.16316v1",
      "title": "Surface solar radiation: AI satellite retrieval can outperform Heliosat and generalizes well to other climate zones",
      "title_zh": "地表太阳辐射：AI 卫星检索可超越 Heliosat 且良好泛化至其他气候区",
      "authors": [
        "K. R. Schuurman",
        "A. Meyer"
      ],
      "abstract": "Accurate estimates of surface solar irradiance (SSI) are essential for solar\nresource assessments and solar energy forecasts in grid integration and\nbuilding control applications. SSI estimates for spatially extended regions can\nbe retrieved from geostationary satellites such as Meteosat. Traditional SSI\nsatellite retrievals like Heliosat rely on physical radiative transfer\nmodelling. We introduce the first machine-learning-based satellite retrieval\nfor instantaneous SSI and demonstrate its capability to provide accurate and\ngeneralizable SSI estimates across Europe. Our deep learning retrieval provides\nnear real-time SSI estimates based on data-driven emulation of Heliosat and\nfine-tuning on pyranometer networks. By including SSI from ground stations, our\nSSI retrieval model can outperform Heliosat accuracy and generalize well to\nregions with other climates and surface albedos in cloudy conditions (clear-sky\nindex < 0.8). We also show that the SSI retrieved from Heliosat exhibits large\nbiases in mountain regions, and that training and fine-tuning our retrieval\nmodels on SSI data from ground stations strongly reduces these biases,\noutperforming Heliosat. Furthermore, we quantify the relative importance of the\nMeteosat channels and other predictor variables like solar zenith angle for the\naccuracy of our deep learning SSI retrieval model in different cloud\nconditions. We find that in cloudy conditions multiple near-infrared and\ninfrared channels enhance the performance. Our results can facilitate the\ndevelopment of more accurate satellite retrieval models of surface solar\nirradiance.",
      "tldr_zh": "本文提出了一种基于深度学习的卫星检索方法，用于估算表面太阳辐照度 (SSI)，它通过数据驱动模拟 Heliosat 并结合地基辐射计网络微调，能在多云条件下 (clear-sky index < 0.8) 提供比 Heliosat 更准确的即时 SSI 估算，并泛化到其他气候区和表面反照率。实验显示，该模型显著减少了 Heliosat 在山区的大偏差，提升了整体性能。研究还分析了 Meteosat 通道和其他变量（如太阳天顶角）的相对重要性，发现多个 near-infrared 和 infrared 通道在 cloudy conditions 下显著提升了模型准确性。该方法为开发更可靠的 SSI 卫星检索模型提供了新途径。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "19 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.16316v1",
      "published_date": "2024-09-16 08:15:54 UTC",
      "updated_date": "2024-09-16 08:15:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:33:45.014279"
    },
    {
      "arxiv_id": "2409.10070v1",
      "title": "Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks",
      "title_zh": "通过口语理解任务提高人-人对话摘要的忠实度",
      "authors": [
        "Eunice Akani",
        "Benoit Favre",
        "Frederic Bechet",
        "Romain Gemignani"
      ],
      "abstract": "Dialogue summarization aims to provide a concise and coherent summary of\nconversations between multiple speakers. While recent advancements in language\nmodels have enhanced this process, summarizing dialogues accurately and\nfaithfully remains challenging due to the need to understand speaker\ninteractions and capture relevant information. Indeed, abstractive models used\nfor dialog summarization may generate summaries that contain inconsistencies.\nWe suggest using the semantic information proposed for performing Spoken\nLanguage Understanding (SLU) in human-machine dialogue systems for\ngoal-oriented human-human dialogues to obtain a more semantically faithful\nsummary regarding the task. This study introduces three key contributions:\nFirst, we propose an exploration of how incorporating task-related information\ncan enhance the summarization process, leading to more semantically accurate\nsummaries. Then, we introduce a new evaluation criterion based on task\nsemantics. Finally, we propose a new dataset version with increased annotated\ndata standardized for research on task-oriented dialogue summarization. The\nstudy evaluates these methods using the DECODA corpus, a collection of French\nspoken dialogues from a call center. Results show that integrating models with\ntask-related information improves summary accuracy, even with varying word\nerror rates.",
      "tldr_zh": "本研究针对人类-人类对话摘要的准确性和忠实性问题，提出通过整合Spoken Language Understanding (SLU)任务的语义信息来提升摘要质量，从而更好地捕捉说话者互动和任务相关细节。贡献包括：探索任务相关信息如何改进摘要过程、引入基于任务语义的新评估标准，以及发布一个增强版数据集，用于任务导向对话摘要研究。实验在DECODA语料库（法语通话中心对话）上进行，结果显示，整合SLU信息显著提高了摘要准确性，即使在不同词错误率下。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10070v1",
      "published_date": "2024-09-16 08:15:35 UTC",
      "updated_date": "2024-09-16 08:15:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:33:55.858765"
    },
    {
      "arxiv_id": "2409.10069v1",
      "title": "Enhancing Anomaly Detection via Generating Diversified and Hard-to-distinguish Synthetic Anomalies",
      "title_zh": "翻译失败",
      "authors": [
        "Hyuntae Kim",
        "Changhee Lee"
      ],
      "abstract": "Unsupervised anomaly detection is a daunting task, as it relies solely on\nnormality patterns from the training data to identify unseen anomalies during\ntesting. Recent approaches have focused on leveraging domain-specific\ntransformations or perturbations to generate synthetic anomalies from normal\nsamples. The objective here is to acquire insights into normality patterns by\nlearning to differentiate between normal samples and these crafted anomalies.\nHowever, these approaches often encounter limitations when domain-specific\ntransformations are not well-specified such as in tabular data, or when it\nbecomes trivial to distinguish between them. To address these issues, we\nintroduce a novel domain-agnostic method that employs a set of conditional\nperturbators and a discriminator. The perturbators are trained to generate\ninput-dependent perturbations, which are subsequently utilized to construct\nsynthetic anomalies, and the discriminator is trained to distinguish normal\nsamples from them. We ensure that the generated anomalies are both diverse and\nhard to distinguish through two key strategies: i) directing perturbations to\nbe orthogonal to each other and ii) constraining perturbations to remain in\nproximity to normal samples. Throughout experiments on real-world datasets, we\ndemonstrate the superiority of our method over state-of-the-art benchmarks,\nwhich is evident not only in image data but also in tabular data, where\ndomain-specific transformation is not readily accessible. Additionally, we\nempirically confirm the adaptability of our method to semi-supervised settings,\ndemonstrating its capacity to incorporate supervised signals to enhance anomaly\ndetection performance even further.",
      "tldr_zh": "这篇论文针对无监督 anomaly detection 的挑战，提出了一种领域无关的方法，通过训练条件 perturbators 生成多样且难区分的 synthetic anomalies，以帮助模型更好地学习正常模式。方法的关键策略包括确保扰动相互正交并保持接近正常样本，同时使用 discriminator 区分正常样本和合成异常。实验结果显示，该方法在图像和表格数据集上优于现有基准，并在半监督设置中进一步提升了 anomaly detection 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.10069v1",
      "published_date": "2024-09-16 08:15:23 UTC",
      "updated_date": "2024-09-16 08:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:34:08.252007"
    },
    {
      "arxiv_id": "2409.10064v1",
      "title": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Sijie Ji",
        "Xinzhe Zheng",
        "Jiawei Sun",
        "Renqi Chen",
        "Wei Gao",
        "Mani Srivastava"
      ],
      "abstract": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.",
      "tldr_zh": "这篇论文介绍了MindGuard，一种可访问且无污名化的移动心理健康急救系统，旨在解决心理健康障碍影响全球四分之一人群却干预率不足25%的核心问题，即污名化导致患者不愿寻求帮助。MindGuard的核心是Edge LLM技术，该模型整合了专业心理健康知识、客观的移动传感器数据以及主观的Ecological Momentary Assessment记录，提供个性化的筛查和干预对话。实验评估使用公开数据集和为期两周的真实部署（涉及20名受试者）显示，MindGuard的性能与GPT-4相当，但比10倍以上模型大小的对应模型表现更好，从而为移动LLM应用开辟道路，推动无污名化的日常心理健康监测和支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10064v1",
      "published_date": "2024-09-16 07:58:56 UTC",
      "updated_date": "2024-09-16 07:58:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:34:19.742804"
    },
    {
      "arxiv_id": "2409.10063v2",
      "title": "GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction",
      "title_zh": "翻译失败",
      "authors": [
        "Anqi Shi",
        "Yuze Cai",
        "Xiangyu Chen",
        "Jian Pu",
        "Zeyu Fu",
        "Hong Lu"
      ],
      "abstract": "High-definition (HD) maps are essential for autonomous driving systems.\nTraditionally, an expensive and labor-intensive pipeline is implemented to\nconstruct HD maps, which is limited in scalability. In recent years,\ncrowdsourcing and online mapping have emerged as two alternative methods, but\nthey have limitations respectively. In this paper, we provide a novel\nmethodology, namely global map construction, to perform direct generation of\nvectorized global maps, combining the benefits of crowdsourcing and online\nmapping. We introduce GlobalMapNet, the first online framework for vectorized\nglobal HD map construction, which updates and utilizes a global map on the ego\nvehicle. To generate the global map from scratch, we propose GlobalMapBuilder\nto match and merge local maps continuously. We design a new algorithm, Map NMS,\nto remove duplicate map elements and produce a clean map. We also propose\nGlobalMapFusion to aggregate historical map information, improving consistency\nof prediction. We examine GlobalMapNet on two widely recognized datasets,\nArgoverse2 and nuScenes, showing that our framework is capable of generating\nglobally consistent results.",
      "tldr_zh": "该论文提出GlobalMapNet，一种在线框架，用于直接生成vectorized global HD maps，结合crowdsourcing和online mapping的优势，以解决传统HD地图构建方法成本高和可扩展性差的问题。该框架包括GlobalMapBuilder用于持续匹配和合并local maps、Map NMS算法移除重复元素以产生干净地图，以及GlobalMapFusion聚合历史信息以提升预测一致性。在Argoverse2和nuScenes数据集上的实验表明，GlobalMapNet能够生成全局一致的HD地图结果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10063v2",
      "published_date": "2024-09-16 07:56:41 UTC",
      "updated_date": "2024-09-17 06:46:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:34:31.349834"
    },
    {
      "arxiv_id": "2409.10048v2",
      "title": "Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments",
      "title_zh": "音频驱动强化学习在自然环境中的头部朝向控制",
      "authors": [
        "Wessel Ledder",
        "Yuzhen Qin",
        "Kiki van der Heijden"
      ],
      "abstract": "Although deep reinforcement learning (DRL) approaches in audio signal\nprocessing have seen substantial progress in recent years, audio-driven DRL for\ntasks such as navigation, gaze control and head-orientation control in the\ncontext of human-robot interaction have received little attention. Here, we\npropose an audio-driven DRL framework in which we utilise deep Q-learning to\ndevelop an autonomous agent that orients towards a talker in the acoustic\nenvironment based on stereo speech recordings. Our results show that the agent\nlearned to perform the task at a near perfect level when trained on speech\nsegments in anechoic environments (that is, without reverberation). The\npresence of reverberation in naturalistic acoustic environments affected the\nagent's performance, although the agent still substantially outperformed a\nbaseline, randomly acting agent. Finally, we quantified the degree of\ngeneralization of the proposed DRL approach across naturalistic acoustic\nenvironments. Our experiments revealed that policies learned by agents trained\non medium or high reverb environments generalized to low reverb environments,\nbut policies learned by agents trained on anechoic or low reverb environments\ndid not generalize to medium or high reverb environments. Taken together, this\nstudy demonstrates the potential of audio-driven DRL for tasks such as\nhead-orientation control and highlights the need for training strategies that\nenable robust generalization across environments for real-world audio-driven\nDRL applications.",
      "tldr_zh": "这篇论文提出了一种音频驱动的深度强化学习（DRL）框架，利用深度 Q 学习训练自主代理，使其基于立体语音记录在自然环境中控制头部方向以转向说话者。实验结果显示，在无回声环境中，代理表现近乎完美，而在有回声的现实环境中，虽然性能有所下降，但仍大幅优于随机基线代理。研究进一步评估了泛化能力，发现训练于中度或高回声环境的策略能成功泛化到低回声环境，但反之则不适用。该框架突显了音频驱动 DRL 在人机交互任务中的潜力，并强调需要开发更鲁棒的训练策略以适应真实世界的音频场景。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.10048v2",
      "published_date": "2024-09-16 07:20:33 UTC",
      "updated_date": "2025-01-17 12:12:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:34:44.617625"
    },
    {
      "arxiv_id": "2409.18984v1",
      "title": "Harnessing Large Language Models: Fine-tuned BERT for Detecting Charismatic Leadership Tactics in Natural Language",
      "title_zh": "利用大型语言模型：微调的 BERT 用于检测自然语言中的魅力型领导策略",
      "authors": [
        "Yasser Saeid",
        "Felix Neubürger",
        "Stefanie Krügl",
        "Helena Hüster",
        "Thomas Kopinski",
        "Ralf Lanwehr"
      ],
      "abstract": "This work investigates the identification of Charismatic Leadership Tactics\n(CLTs) in natural language using a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model. Based on an own extensive\ncorpus of CLTs generated and curated for this task, our methodology entails\ntraining a machine learning model that is capable of accurately identifying the\npresence of these tactics in natural language. A performance evaluation is\nconducted to assess the effectiveness of our model in detecting CLTs. We find\nthat the total accuracy over the detection of all CLTs is 98.96\\% The results\nof this study have significant implications for research in psychology and\nmanagement, offering potential methods to simplify the currently elaborate\nassessment of charisma in texts.",
      "tldr_zh": "本研究利用微调后的 BERT 模型，开发了一种方法来识别自然语言中的 Charismatic Leadership Tactics (CLTs)，基于自建的 CLTs 语料库进行模型训练。实验结果显示，该模型在检测所有 CLTs 的总准确率达到 98.96%。这项工作为心理学和管理研究提供了简便有效的工具，有助于简化对文本中魅力的评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The 2024 IEEE 3rd Conference on Information Technology and Data\n  Science, CITDS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.18984v1",
      "published_date": "2024-09-16 07:14:54 UTC",
      "updated_date": "2024-09-16 07:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:34:55.185966"
    },
    {
      "arxiv_id": "2409.10038v3",
      "title": "On the Diagram of Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Zhang",
        "Yang Yuan",
        "Andrew Chi-Chih Yao"
      ],
      "abstract": "Current large language models (LLMs) demonstrate impressive capabilities but\nstruggle with complex, multi-step reasoning tasks. Existing methods often\ntackle this by requiring external control mechanisms or multi-model\norchestration, which introduces system complexity and typically lacks formal\nguarantees of reasoning soundness. We introduce the Diagram of Thought (DoT), a\nframework wherein a single auto-regressive LLM internally constructs and\nnavigates a Directed Acyclic Graph (DAG). This DAG represents the iterative\nreasoning process, encompassing steps like proposing ideas, critiquing them,\nrefining based on feedback, and synthesizing conclusions. This\nself-orchestrated, self-contained process is guided by learned role-specific\ntokens (e.g., <proposer>, <critic>, <summarizer>) embedded within the standard\ngeneration loop, thereby eliminating external dependencies. Crucially, we\nestablish a rigorous mathematical foundation for DoT using Topos Theory. We\nformalize the reasoning DAG as a diagram within a suitable topos and prove that\nthe final synthesis step, aggregating validated information, corresponds\nsemantically to computing the colimit of the relevant sub-diagram. This\nformalization provides theoretical guarantees concerning the logical\nconsistency and robustness of the synthesized outcome. DoT thus offers a\nunified, self-contained, interpretable, efficient, and formally grounded\napproach designed to significantly advance the complex reasoning capabilities\nof LLMs.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在复杂多步骤推理任务中的局限性，提出Diagram of Thought (DoT)框架，该框架让单个自回归LLM内部构建并导航一个有向无环图(DAG)，通过角色特定标记（如<proposer>、<critic>和<summarizer>）实现自我组织的过程，包括提出想法、批评、改进和总结步骤，从而避免外部依赖。DoT基于Topos Theory提供严格数学基础，将推理DAG形式化为一个图，并证明最终合成步骤等同于计算相关子图的colimit，这确保了推理结果的逻辑一致性和鲁棒性。该框架为LLMs的复杂推理能力带来统一、自包含、可解释和高效的提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.10038v3",
      "published_date": "2024-09-16 07:01:41 UTC",
      "updated_date": "2025-03-30 23:31:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:35:08.868667"
    },
    {
      "arxiv_id": "2409.10033v3",
      "title": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs",
      "title_zh": "翻译失败",
      "authors": [
        "Haichuan Hu",
        "Ye Shang",
        "Guolin Xu",
        "Congqing He",
        "Quanjun Zhang"
      ],
      "abstract": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR.",
      "tldr_zh": "这篇论文评估了 GPT 家族大型语言模型 (LLMs) 在自动程序修复 (APR) 中的性能，首次比较了 GPT-o1-preview、GPT-o1-mini、GPT-4o 和历史版本 ChatGPT 在 QuixBugs 基准上的表现。研究从修复成功率、修复成本、响应长度和行为模式等多个角度进行实证分析，结果显示 GPT-o1 成功修复了基准中的所有 40 个 bugs，超过了先前的 GPT 模型。总体而言，该工作为深入探索 GPT 家族模型在 APR 应用方面奠定了基础。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to the 6th International Workshop on Automated Program\n  Repair (APR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2409.10033v3",
      "published_date": "2024-09-16 06:51:32 UTC",
      "updated_date": "2024-12-17 13:16:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:35:20.413574"
    },
    {
      "arxiv_id": "2409.10028v1",
      "title": "AttnMod: Attention-Based New Art Styles",
      "title_zh": "AttnMod：基于注意力的新艺术风格",
      "authors": [
        "Shih-Chieh Su"
      ],
      "abstract": "Imagine a human artist looking at the generated photo of a diffusion model,\nand hoping to create a painting out of it. There could be some feature of the\nobject in the photo that the artist wants to emphasize, some color to disperse,\nsome silhouette to twist, or some part of the scene to be materialized. These\nintentions can be viewed as the modification of the cross attention from the\ntext prompt onto UNet, during the desoising diffusion. This work presents\nAttnMod, to modify attention for creating new unpromptable art styles out of\nexisting diffusion models. The style-creating behavior is studied across\ndifferent setups.",
      "tldr_zh": "本论文提出 AttnMod，一种基于 attention 的方法，用于从现有 diffusion models 中创建新的艺术风格。该方法通过修改文本提示在 UNet 上的 cross attention，实现对图像特征的强调、颜色的分散、轮廓的扭曲或其他场景修改，模拟艺术家对生成照片的意图。研究结果显示，AttnMod 能在不同设置下生成无法通过直接提示(unpromptable)的新艺术风格，为扩散模型的应用拓展了可能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10028v1",
      "published_date": "2024-09-16 06:38:25 UTC",
      "updated_date": "2024-09-16 06:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:35:31.181964"
    },
    {
      "arxiv_id": "2409.10027v4",
      "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models",
      "title_zh": "E2Map：经验与情感地图，用于自省机器人导航",
      "authors": [
        "Chan Kim",
        "Keonwoo Kim",
        "Mintaek Oh",
        "Hanbi Baek",
        "Jiyang Lee",
        "Donghwi Jung",
        "Soojin Woo",
        "Younkyung Woo",
        "John Tucker",
        "Roya Firoozi",
        "Seung-Woo Seo",
        "Mac Schwager",
        "Seong-Woo Kim"
      ],
      "abstract": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
      "tldr_zh": "这篇论文提出了 E2Map（Experience-and-Emotion Map），一种结合大语言模型（LLMs）知识、代理经验和人类情感响应的框架，用于提升机器人导航的自我反思能力。E2Map 通过整合实时经验并借鉴情感机制，实现一次性的行为调整，以适应随机和动态环境中的导航任务。实验结果显示，该方法在模拟和真实场景的随机导航环境中显著优于现有 LLM 基于方法，提升了机器人性能，并提供了开源代码以供进一步验证。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.10027v4",
      "published_date": "2024-09-16 06:35:18 UTC",
      "updated_date": "2025-02-03 01:26:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:35:43.781087"
    },
    {
      "arxiv_id": "2409.10016v2",
      "title": "AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing",
      "title_zh": "翻译失败",
      "authors": [
        "Huawei Ji",
        "Cheng Deng",
        "Bo Xue",
        "Zhouyang Jin",
        "Jiaxin Ding",
        "Xiaoying Gan",
        "Luoyi Fu",
        "Xinbing Wang",
        "Chenghu Zhou"
      ],
      "abstract": "With the development of data-centric AI, the focus has shifted from\nmodel-driven approaches to improving data quality. Academic literature, as one\nof the crucial types, is predominantly stored in PDF formats and needs to be\nparsed into texts before further processing. However, parsing diverse\nstructured texts in academic literature remains challenging due to the lack of\ndatasets that cover various text structures. In this paper, we introduce\nAceParse, the first comprehensive dataset designed to support the parsing of a\nwide range of structured texts, including formulas, tables, lists, algorithms,\nand sentences with embedded mathematical expressions. Based on AceParse, we\nfine-tuned a multimodal model, named AceParser, which accurately parses various\nstructured texts within academic literature. This model outperforms the\nprevious state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard\nSimilarity, demonstrating the potential of multimodal models in academic\nliterature parsing. Our dataset is available at\nhttps://github.com/JHW5981/AceParse.",
      "tldr_zh": "该论文介绍了 AceParse，这是一个全面的数据集，旨在解决学术文献解析中结构化文本多样性的挑战，包括公式、表格、列表、算法和嵌入数学表达的句子。研究者基于 AceParse 微调了一个多模态模型 AceParser，能够准确解析这些结构化文本。实验结果显示，AceParser 在 F1 score 上比现有最先进模型提高了 4.1%，在 Jaccard Similarity 上提高了 5%。该数据集可从 https://github.com/JHW5981/AceParse 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.10016v2",
      "published_date": "2024-09-16 06:06:34 UTC",
      "updated_date": "2025-02-02 01:48:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:35:56.631925"
    },
    {
      "arxiv_id": "2409.10583v1",
      "title": "Reinforcement Learning with Quasi-Hyperbolic Discounting",
      "title_zh": "翻译失败",
      "authors": [
        "S. R. Eshwar",
        "Mayank Motwani",
        "Nibedita Roy",
        "Gugan Thoppe"
      ],
      "abstract": "Reinforcement learning has traditionally been studied with exponential\ndiscounting or the average reward setup, mainly due to their mathematical\ntractability. However, such frameworks fall short of accurately capturing human\nbehavior, which has a bias towards immediate gratification. Quasi-Hyperbolic\n(QH) discounting is a simple alternative for modeling this bias. Unlike in\ntraditional discounting, though, the optimal QH-policy, starting from some time\n$t_1,$ can be different to the one starting from $t_2.$ Hence, the future self\nof an agent, if it is naive or impatient, can deviate from the policy that is\noptimal at the start, leading to sub-optimal overall returns. To prevent this\nbehavior, an alternative is to work with a policy anchored in a Markov Perfect\nEquilibrium (MPE). In this work, we propose the first model-free algorithm for\nfinding an MPE. Using a two-timescale analysis, we show that, if our algorithm\nconverges, then the limit must be an MPE. We also validate this claim\nnumerically for the standard inventory system with stochastic demands. Our work\nsignificantly advances the practical application of reinforcement learning.",
      "tldr_zh": "这篇论文探讨了强化学习(Reinforcement Learning)中Quasi-Hyperbolic (QH) 折扣的运用，以更好地捕捉人类对即时满足的偏好，相比传统指数折扣或平均奖励框架更准确。作者指出QH折扣可能导致从不同时间点开始的最优策略偏差，从而影响整体回报，因此提出第一个无模型算法来寻找Markov Perfect Equilibrium (MPE)。通过双时标分析，证明如果算法收敛，则极限点为MPE，并在标准库存系统中进行数值验证。该研究显著推进了强化学习的实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10583v1",
      "published_date": "2024-09-16 06:00:42 UTC",
      "updated_date": "2024-09-16 06:00:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:36:08.465699"
    },
    {
      "arxiv_id": "2410.01819v1",
      "title": "Strategic AI Governance: Insights from Leading Nations",
      "title_zh": "战略 AI 治理：领先国家的见解",
      "authors": [
        "Dian W. Tjondronegoro"
      ],
      "abstract": "Artificial Intelligence (AI) has the potential to revolutionize various\nsectors, yet its adoption is often hindered by concerns about data privacy,\nsecurity, and the understanding of AI capabilities. This paper synthesizes AI\ngovernance approaches, strategic themes, and enablers and challenges for AI\nadoption by reviewing national AI strategies from leading nations. The key\ncontribution is the development of an EPIC (Education, Partnership,\nInfrastructure, Community) framework, which maps AI implementation requirements\nto fully realize social impacts and public good from successful and sustained\nAI deployment. Through a multi-perspective content analysis of the latest AI\nstrategy documents, this paper provides a structured comparison of AI\ngovernance strategies across nations. The findings offer valuable insights for\ngovernments, academics, industries, and communities to enable responsible and\ntrustworthy AI deployments. Future work should focus on incorporating specific\nrequirements for developing countries and applying the strategies to specific\nAI applications, industries, and the public sector.",
      "tldr_zh": "这篇论文分析了主要国家的国家AI策略，探讨了AI治理方法、战略主题以及推动和阻碍AI采用的因素，以应对数据隐私、安全和能力理解等挑战。论文的主要贡献是提出EPIC框架（Education, Partnership, Infrastructure, Community），用于映射AI实施要求，从而实现社会影响和公共利益。通过多视角内容分析，该研究提供了各国AI治理策略的结构化比较，为政府、学术界、产业和社区提供宝贵见解，促进负责任的AI部署。未来工作应关注发展中国家的特定需求，并将策略应用于具体AI应用和公共部门。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "90-02",
        "K.6.m"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages, 3 Figures, 5 Tables",
      "pdf_url": "http://arxiv.org/pdf/2410.01819v1",
      "published_date": "2024-09-16 06:00:42 UTC",
      "updated_date": "2024-09-16 06:00:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:36:29.744995"
    },
    {
      "arxiv_id": "2409.10011v2",
      "title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Sumera Anjum",
        "Hanzhi Zhang",
        "Wenjun Zhou",
        "Eun Jin Paek",
        "Xiaopeng Zhao",
        "Yunhe Feng"
      ],
      "abstract": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.",
      "tldr_zh": "这篇论文引入了 HALO 框架，用于分析和优化大型语言模型 (LLMs) 的 hallucination 问题，以提升医疗问答 (QA) 系统的准确性和可靠性。HALO 通过生成查询的多个变体、从外部知识库检索相关信息，并使用 maximum marginal relevance scoring 优先排序上下文，然后整合 LangChain 生成答案，从而减少 hallucination 的风险。实验结果显示，该框架显著提高了 LLMs 的性能，例如 Llama-3.1 的准确率从 44% 提升到 65%，ChatGPT 从 56% 提升到 70%。整体而言，HALO 为指导临床决策提供了更可信的工具，并已开源以促进进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.10011v2",
      "published_date": "2024-09-16 05:50:39 UTC",
      "updated_date": "2024-09-18 20:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:36:32.332125"
    },
    {
      "arxiv_id": "2409.10007v1",
      "title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Shen",
        "Mayank Kejriwal"
      ],
      "abstract": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.",
      "tldr_zh": "该研究针对 Text-to-SQL 问题提出 SelECT-SQL，这是一种基于 in-context learning 的创新解决方案，结合 Chain-of-Thought (CoT) 提示、自修正和集成方法，以提升大语言模型(LLMs)在自然语言问题转换为 SQL 查询时的准确性。SelECT-SQL 通过算法优化这些组件，解决了 LLMs 在理解数据库模式、问题和 SQL 子句时的常见错误。实验结果显示，在 Spider 基准测试的开发集上，使用 GPT-3.5-Turbo 作为基础模型，SelECT-SQL 达到了 84.2% 的执行准确率，超过了其他基于 GPT-3.5-Turbo 的解决方案(81.1%)和 GPT-4 的峰值表现(83.5%)，实现了新的 state-of-the-art 结果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10007v1",
      "published_date": "2024-09-16 05:40:18 UTC",
      "updated_date": "2024-09-16 05:40:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:36:45.211286"
    },
    {
      "arxiv_id": "2409.09996v1",
      "title": "FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhang Chen",
        "Jiangnan Zhu",
        "Yujie Gu",
        "Minoru Kuribayashi",
        "Kouichi Sakurai"
      ],
      "abstract": "Deep neural networks (DNNs) have achieved significant success in real-world\napplications. However, safeguarding their intellectual property (IP) remains\nextremely challenging. Existing DNN watermarking for IP protection often\nrequire modifying DNN models, which reduces model performance and limits their\npracticality.\n  This paper introduces FreeMark, a novel DNN watermarking framework that\nleverages cryptographic principles without altering the original host DNN\nmodel, thereby avoiding any reduction in model performance. Unlike traditional\nDNN watermarking methods, FreeMark innovatively generates secret keys from a\npre-generated watermark vector and the host model using gradient descent. These\nsecret keys, used to extract watermark from the model's activation values, are\nsecurely stored with a trusted third party, enabling reliable watermark\nextraction from suspect models. Extensive experiments demonstrate that FreeMark\neffectively resists various watermark removal attacks while maintaining high\nwatermark capacity.",
      "tldr_zh": "该论文提出 FreeMark，一种非侵入式白盒 watermarking 框架，用于保护 Deep Neural Networks (DNNs) 的知识产权，而无需修改原模型，从而避免性能下降。FreeMark 创新性地利用梯度 descent 从预生成的 watermark vector 和主机模型生成秘密密钥，这些密钥由可信第三方存储，用于从模型的激活值中提取 watermark。实验结果显示，FreeMark 有效抵抗各种 watermark 移除攻击，同时保持高 watermark 容量，为 DNNs 的知识产权保护提供了可靠解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09996v1",
      "published_date": "2024-09-16 05:05:03 UTC",
      "updated_date": "2024-09-16 05:05:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:36:55.128764"
    },
    {
      "arxiv_id": "2409.09989v1",
      "title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system",
      "title_zh": "情感分析的全面研究：从基于规则的方法到现代基于LLM的系统",
      "authors": [
        "Shailja Gupta",
        "Rajesh Ranjan",
        "Surya Narayan Singh"
      ],
      "abstract": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.",
      "tldr_zh": "这篇论文对情感分析进行了全面调查，探讨了从传统rule-based 方法到现代LLM-based 系统的演变，作为自然语言处理(NLP)中的关键领域。论文回顾了情感分析的历史发展，从lexicon-based 和pattern-based 做法过渡到先进的机器学习和deep learning 模型，并讨论了主要挑战，如处理bilingual texts、检测sarcasm 和应对biases。最终，该研究总结了state-of-the-art 做法、识别了新兴趋势，并为未来研究方向提供了指导，以推进情感分析在AI 和LLM 背景下的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "2 Images",
      "pdf_url": "http://arxiv.org/pdf/2409.09989v1",
      "published_date": "2024-09-16 04:44:52 UTC",
      "updated_date": "2024-09-16 04:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:37:07.935173"
    },
    {
      "arxiv_id": "2409.10582v3",
      "title": "WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency",
      "title_zh": "WaveMixSR-V2：通过更高效率增强超分辨率",
      "authors": [
        "Pranav Jeevan",
        "Neeraj Nixon",
        "Amit Sethi"
      ],
      "abstract": "Recent advancements in single image super-resolution have been predominantly\ndriven by token mixers and transformer architectures. WaveMixSR utilized the\nWaveMix architecture, employing a two-dimensional discrete wavelet transform\nfor spatial token mixing, achieving superior performance in super-resolution\ntasks with remarkable resource efficiency. In this work, we present an enhanced\nversion of the WaveMixSR architecture by (1) replacing the traditional\ntranspose convolution layer with a pixel shuffle operation and (2) implementing\na multistage design for higher resolution tasks ($4\\times$). Our experiments\ndemonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other\narchitectures in multiple super-resolution tasks, achieving state-of-the-art\nfor the BSD100 dataset, while also consuming fewer resources, exhibits higher\nparameter efficiency, lower latency and higher throughput. Our code is\navailable at https://github.com/pranavphoenix/WaveMixSR.",
      "tldr_zh": "本文提出了 WaveMixSR-V2，一种增强的单图像超分辨率模型，基于 WaveMix 架构通过二维离散小波变换进行空间 token mixing，同时优化了资源效率。关键改进包括用 pixel shuffle 操作替换传统的 transpose convolution 层，以及实施多阶段设计以处理更高分辨率任务（如 4×）。实验结果显示，WaveMixSR-V2 在多个超分辨率任务中优于其他架构，在 BSD100 数据集上达到了 state-of-the-art 水平，同时实现了更低的资源消耗、更高的参数效率、更低的延迟和更高的吞吐量。代码已在 GitHub 上提供。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;\n  I.2.10; I.5.1; I.5.2; I.5.4; I.4.3; I.4.4; I.4.5"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages. Accepted in AAAI 2025. arXiv admin note: text overlap with\n  arXiv:2307.00430",
      "pdf_url": "http://arxiv.org/pdf/2409.10582v3",
      "published_date": "2024-09-16 04:16:52 UTC",
      "updated_date": "2024-10-30 15:16:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:37:23.179110"
    },
    {
      "arxiv_id": "2409.09968v1",
      "title": "Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System",
      "title_zh": "基于人工智能的机会性冠状动脉钙化筛查在退伍军人事务部国家医疗保健系统",
      "authors": [
        "Raffi Hagopian",
        "Timothy Strebel",
        "Simon Bernatz",
        "Gregory A Myers",
        "Erik Offerman",
        "Eric Zuniga",
        "Cy Y Kim",
        "Angie T Ng",
        "James A Iwaz",
        "Sunny P Singh",
        "Evan P Carey",
        "Michael J Kim",
        "R Spencer Schaefer",
        "Jeannie Yu",
        "Amilcare Gentili",
        "Hugo JWL Aerts"
      ],
      "abstract": "Coronary artery calcium (CAC) is highly predictive of cardiovascular events.\nWhile millions of chest CT scans are performed annually in the United States,\nCAC is not routinely quantified from scans done for non-cardiac purposes. A\ndeep learning algorithm was developed using 446 expert segmentations to\nautomatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our\nstudy differs from prior works as we leverage imaging data across the Veterans\nAffairs national healthcare system, from 98 medical centers, capturing\nextensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC\nperformance on non-gated scans was compared against clinical standard ECG-gated\nCAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than\n100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and\n87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a\nyear of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year\nall-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <\n0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:\n33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052\npatients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)\nindividuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT\nimages from a random sample of >400 AI-CAC patients and verified that 527/531\n(99.2%) would benefit from lipid-lowering therapy. To the best of our\nknowledge, this is the first non-gated CT CAC algorithm developed across a\nnational healthcare system, on multiple imaging protocols, without filtering\nintra-cardiac hardware, and compared against a strong gated CT reference. We\nreport superior performance relative to previous CAC algorithms evaluated\nagainst paired gated scans that included patients with intra-cardiac hardware.",
      "tldr_zh": "该研究开发了一种基于深度学习的算法（AI-CAC），用于从非对比、非门控胸部CT扫描中自动量化冠状动脉钙化（CAC），以实现Veterans Affairs国家医疗系统中的机会性筛查。该算法利用446个专家分割训练而成，并在98个医疗中心的异质数据上进行验证，与ECG门控CAC评分相比，在795名患者中准确区分不同Agatston分数，准确率达89.4%（F1 0.93）和87.3%（F1 0.89）。结果显示，AI-CAC能有效预测10年全因死亡率和复合事件（如中风、心肌梗死或死亡），在8,052名低剂量肺癌筛查CT患者中，38.4%的人有AI-CAC >400，且专家审查确认99.2%的患者可从降脂治疗中获益。该方法首次在国家医疗系统中实现非门控CT CAC量化，展现出优于现有算法的性能，无需过滤心脏内硬件。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09968v1",
      "published_date": "2024-09-16 03:59:01 UTC",
      "updated_date": "2024-09-16 03:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:37:35.751013"
    },
    {
      "arxiv_id": "2409.18982v1",
      "title": "Aligning Robot Navigation Behaviors with Human Intentions and Preferences",
      "title_zh": "翻译失败",
      "authors": [
        "Haresh Karnan"
      ],
      "abstract": "Recent advances in the field of machine learning have led to new ways for\nmobile robots to acquire advanced navigational capabilities. However, these\nlearning-based methods raise the possibility that learned navigation behaviors\nmay not align with the intentions and preferences of people, a problem known as\nvalue misalignment. To mitigate this risk, this dissertation aims to answer the\nquestion: \"How can we use machine learning methods to align the navigational\nbehaviors of autonomous mobile robots with human intentions and preferences?\"\nFirst, this dissertation addresses this question by introducing a new approach\nto learning navigation behaviors by imitating human-provided demonstrations of\nthe intended navigation task. This contribution allows mobile robots to acquire\nautonomous visual navigation capabilities through imitation, using a novel\nobjective function that encourages the agent to align with the human's\nnavigation objectives and penalizes misalignment. Second, this dissertation\nintroduces two algorithms to enhance terrain-aware off-road navigation for\nmobile robots by learning visual terrain awareness in a self-supervised manner.\nThis contribution enables mobile robots to respect a human operator's\npreferences for navigating different terrains in urban outdoor environments,\nwhile extrapolating these preferences to visually novel terrains by leveraging\nmulti-modal representations. Finally, in the context of robot navigation in\nhuman-occupied environments, this dissertation introduces a dataset and an\nalgorithm for robot navigation in a socially compliant manner in both indoor\nand outdoor environments. In summary, the contributions in this dissertation\ntake significant steps toward addressing the value alignment problem in\nautonomous navigation, enabling mobile robots to navigate autonomously with\nobjectives that align with human intentions and preferences.",
      "tldr_zh": "本论文针对机器学习在机器人导航中的value misalignment问题，即导航行为可能与人类意图和偏好不一致，提出使用机器学习方法进行对齐。论文首先引入一种模仿学习方法，通过人类演示学习自主视觉导航，利用一个鼓励对齐并惩罚偏差的目标函数，使机器人更好地匹配人类目标。其次，开发了两个自监督算法，实现视觉地形感知，帮助机器人尊重人类在城市户外环境的偏好，并扩展到新地形。最后，论文提供了一个数据集和算法，支持机器人室内外社会兼容导航，总体上推进了机器人自主导航与人类意图一致的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Haresh Karnan's PhD Dissertation A recording of the defense talk can\n  be accessed here: https://youtu.be/MssiO6g0Gb8",
      "pdf_url": "http://arxiv.org/pdf/2409.18982v1",
      "published_date": "2024-09-16 03:45:00 UTC",
      "updated_date": "2024-09-16 03:45:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:37:48.117949"
    },
    {
      "arxiv_id": "2409.09958v1",
      "title": "An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning",
      "title_zh": "一种约束多目标强化学习的离线适应框架",
      "authors": [
        "Qian Lin",
        "Zongkai Liu",
        "Danying Mo",
        "Chao Yu"
      ],
      "abstract": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations.",
      "tldr_zh": "本文提出了一种离线适应框架（offline adaptation framework），用于解决多目标强化学习（multi-objective RL）问题，该框架无需手工设计的偏好设置，只需通过几个演示来隐式表示期望策略的偏好，从而避免依赖人类先验知识。主要方法包括利用演示数据推断政策，并扩展到处理安全关键目标的约束（constrained），即使安全阈值未知。实验结果表明，该框架在离线多目标和安全任务中，能够生成与真实偏好一致的政策，同时满足演示暗示的约束。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09958v1",
      "published_date": "2024-09-16 03:08:09 UTC",
      "updated_date": "2024-09-16 03:08:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:37:58.662830"
    },
    {
      "arxiv_id": "2409.09957v1",
      "title": "Deep Graph Anomaly Detection: A Survey and New Perspectives",
      "title_zh": "翻译失败",
      "authors": [
        "Hezhe Qiao",
        "Hanghang Tong",
        "Bo An",
        "Irwin King",
        "Charu Aggarwal",
        "Guansong Pang"
      ],
      "abstract": "Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.",
      "tldr_zh": "这篇论文对深度图异常检测（Deep Graph Anomaly Detection）进行了全面综述，聚焦于图神经网络（GNNs）在识别图异常（如节点、边、子图或整个图）中的应用，强调了其在实际场景中的重要性。论文从三个新视角——GNN backbone design、proxy task design for GAD 和 graph anomaly measures——系统审视现有方法，并将其细分为13个类别，以揭示模型设计的优势和局限性。作者还总结了常用GAD数据集、实验比较结果，并讨论了多个开放问题，同时提供了一个持续更新的仓库（https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection），以推动未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 6 figures, and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.09957v1",
      "published_date": "2024-09-16 03:05:11 UTC",
      "updated_date": "2024-09-16 03:05:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:38:11.201867"
    },
    {
      "arxiv_id": "2409.09944v1",
      "title": "Fault Analysis And Predictive Maintenance Of Induction Motor Using Machine Learning",
      "title_zh": "使用机器学习的感应电机故障分析与预测性维护",
      "authors": [
        "Kavana Venkatesh",
        "Neethi M"
      ],
      "abstract": "Induction motors are one of the most crucial electrical equipment and are\nextensively used in industries in a wide range of applications. This paper\npresents a machine learning model for the fault detection and classification of\ninduction motor faults by using three phase voltages and currents as inputs.\nThe aim of this work is to protect vital electrical components and to prevent\nabnormal event progression through early detection and diagnosis. This work\npresents a fast forward artificial neural network model to detect some of the\ncommonly occurring electrical faults like overvoltage, under voltage, single\nphasing, unbalanced voltage, overload, ground fault. A separate model free\nmonitoring system wherein the motor itself acts like a sensor is presented and\nthe only monitored signals are the input given to the motor. Limits for current\nand voltage values are set for the faulty and healthy conditions, which is done\nby a classifier. Real time data from a 0.33 HP induction motor is used to train\nand test the neural network. The model so developed analyses the voltage and\ncurrent values given at a particular instant and classifies the data into no\nfault or the specific fault. The model is then interfaced with a real motor to\naccurately detect and classify the faults so that further necessary action can\nbe taken.",
      "tldr_zh": "本研究提出了一种基于机器学习的应用，用于感应电机（Induction Motor）的故障分析和预测维护。方法利用三相电压和电流作为输入，构建了一个快速前向人工神经网络（Feedforward Artificial Neural Network）模型，以检测和分类常见电气故障，如过电压、欠压、单相故障、不平衡电压、过载和接地故障。该模型还包括一个独立的监控系统，使用电机自身作为传感器，仅监控输入信号，通过分类器设置健康和故障限值。实验使用0.33 HP感应电机的实时数据训练和测试，成功实现了故障的实时检测和分类，从而保护关键组件并防止异常事件升级。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at ICEECCOT-2018, Published in IEEE Xplore, 6 pages, 3\n  figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09944v1",
      "published_date": "2024-09-16 02:37:07 UTC",
      "updated_date": "2024-09-16 02:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:38:22.705487"
    },
    {
      "arxiv_id": "2409.09927v2",
      "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges",
      "title_zh": "针对现代大型语言模型的数据污染检测：局限性、不一致性和预言机挑战",
      "authors": [
        "Vinay Samuel",
        "Yue Zhou",
        "Henry Peng Zou"
      ],
      "abstract": "As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.",
      "tldr_zh": "这篇论文探讨了数据污染检测方法在现代大型语言模型(LLMs)中的局限性，通过评估五种检测方法在四种SOTA LLMs和八个挑战性数据集上的表现。研究发现，这些方法存在显著的假设和实际应用限制，尤其在检测指令微调中引入的污染时面临困难。结果还显示，SOTA检测技术之间的一致性有限，这强调了开发更鲁棒和通用化的污染检测方法迫在眉睫。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to COLING 2025 12 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.09927v2",
      "published_date": "2024-09-16 02:04:33 UTC",
      "updated_date": "2024-12-08 19:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:38:34.703575"
    },
    {
      "arxiv_id": "2409.13759v1",
      "title": "Simulación de la distribución de alimento en el cultivo de camarón",
      "title_zh": "翻译失败",
      "authors": [
        "Renato L. Conforme Rosado",
        "Francisco C. Calderon Bocanegra"
      ],
      "abstract": "This document presents the experimentation of 4 cases of food distribution\nfor shrimp farming. The distributions are based on the location of the\nautomatic feeders. Three cases applied in reality and a fourth case where the\nfood is irrigated on the crop simultaneously and uniformly. In a first stage,\nthe simulation of the three distribution cases is successfully adjusted to\nreality, where the trend of the shrimp growth curve is correlated with the\nhistorical data curve. A second stage where you experiment in 16 configurations\nthat are based on the amount of food, the density of biomass and the\ndistribution of the food. The simulation adopts the concepts of genetic\nalgorithms to improve the population and fuzzy logic as an agent evaluation\ntechnique for decision-making against the quality of physical-chemical\nparameters in the simulated environment. The results of these interactions\nreveal a reduction in the simulated total culture time from 22 weeks to 14\nweeks.",
      "tldr_zh": "本研究模拟了虾养殖中食物分布的4种情况，基于自动喂食器位置，包括三种实际应用和一种同时均匀撒布的方案。通过第一阶段的模拟，成功调整了三种分布情况，使虾生长曲线与历史数据相符。第二阶段实验了16种配置，结合genetic algorithms（遗传算法）和fuzzy logic（模糊逻辑）来优化食物量、生物质密度和分布决策。结果显示，模拟的总养殖时间从22周缩短至14周，提高了养殖效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13759v1",
      "published_date": "2024-09-16 01:29:49 UTC",
      "updated_date": "2024-09-16 01:29:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:38:46.519917"
    },
    {
      "arxiv_id": "2409.09916v1",
      "title": "SFR-RAG: Towards Contextually Faithful LLMs",
      "title_zh": "SFR-RAG：面向上下文忠实的大型语言模型",
      "authors": [
        "Xuan-Phi Nguyen",
        "Shrey Pandit",
        "Senthil Purushwalkam",
        "Austin Xu",
        "Hailin Chen",
        "Yifei Ming",
        "Zixuan Ke",
        "Silvio Savarese",
        "Caiming Xong",
        "Shafiq Joty"
      ],
      "abstract": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.",
      "tldr_zh": "本文介绍了 SFR-RAG，一种通过指令微调的小型 LLM（Large Language Models），专注于基于上下文的生成和 hallucination minimization，以提升 RAG（Retrieval Augmented Generation）应用的忠实性和准确性。研究者还提出了 ContextualBench 评估框架，该框架整合了多个流行基准如 HotpotQA 和 TriviaQA，确保评估的一致性和可重复性。实验结果显示，SFR-RAG-9B 模型在 ContextualBench 的 7 个基准中 3 个上达到了 state-of-the-art 水平，超过了如 Command-R+ (104B) 和 GPT-4o 等更大模型，同时在参数量更少的情况下保持对上下文变化的韧性和一般指令遵循任务的竞争性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical report",
      "pdf_url": "http://arxiv.org/pdf/2409.09916v1",
      "published_date": "2024-09-16 01:08:18 UTC",
      "updated_date": "2024-09-16 01:08:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:38:59.570432"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 108,
  "processed_papers_count": 108,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T00:39:22.843891"
}
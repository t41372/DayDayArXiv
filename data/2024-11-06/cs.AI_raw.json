[
  {
    "arxiv_id": "2411.04324v1",
    "title": "Gradient Boosting Trees and Large Language Models for Tabular Data Few-Shot Learning",
    "authors": [
      "Carlos Huertas"
    ],
    "abstract": "Large Language Models (LLM) have brought numerous of new applications to\nMachine Learning (ML). In the context of tabular data (TD), recent studies show\nthat TabLLM is a very powerful mechanism for few-shot-learning (FSL)\napplications, even if gradient boosting decisions trees (GBDT) have\nhistorically dominated the TD field. In this work we demonstrate that although\nLLMs are a viable alternative, the evidence suggests that baselines used to\ngauge performance can be improved. We replicated public benchmarks and our\nmethodology improves LightGBM by 290%, this is mainly driven by forcing node\nsplitting with few samples, a critical step in FSL with GBDT. Our results show\nan advantage to TabLLM for 8 or fewer shots, but as the number of samples\nincreases GBDT provides competitive performance at a fraction of runtime. For\nother real-life applications with vast number of samples, we found FSL still\nuseful to improve model diversity, and when combined with ExtraTrees it\nprovides strong resilience to overfitting, our proposal was validated in a ML\ncompetition setting ranking first place.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "FedCSIS 2024 - Data Mining Competition - 1st Place Winner",
    "pdf_url": "http://arxiv.org/pdf/2411.04324v1",
    "published_date": "2024-11-06 23:54:09 UTC",
    "updated_date": "2024-11-06 23:54:09 UTC"
  },
  {
    "arxiv_id": "2411.05854v1",
    "title": "Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as Alternative Annotators",
    "authors": [
      "Claire Wonjeong Jo",
      "Miki Weso≈Çowska",
      "Magdalena Wojcieszak"
    ],
    "abstract": "Short video platforms, such as YouTube, Instagram, or TikTok, are used by\nbillions of users globally. These platforms expose users to harmful content,\nranging from clickbait or physical harms to misinformation or online hate. Yet,\ndetecting harmful videos remains challenging due to an inconsistent\nunderstanding of what constitutes harm and limited resources and mental tolls\ninvolved in human annotation. As such, this study advances measures and methods\nto detect harm in video content. First, we develop a comprehensive taxonomy for\nonline harm on video platforms, categorizing it into six categories:\nInformation, Hate and harassment, Addictive, Clickbait, Sexual, and Physical\nharms. Next, we establish multimodal large language models as reliable\nannotators of harmful videos. We analyze 19,422 YouTube videos using 14 image\nframes, 1 thumbnail, and text metadata, comparing the accuracy of crowdworkers\n(Mturk) and GPT-4-Turbo with domain expert annotations serving as the gold\nstandard. Our results demonstrate that GPT-4-Turbo outperforms crowdworkers in\nboth binary classification (harmful vs. harmless) and multi-label harm\ncategorization tasks. Methodologically, this study extends the application of\nLLMs to multi-label and multi-modal contexts beyond text annotation and binary\nclassification. Practically, our study contributes to online harm mitigation by\nguiding the definitions and identification of harmful content on video\nplatforms.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05854v1",
    "published_date": "2024-11-06 23:48:30 UTC",
    "updated_date": "2024-11-06 23:48:30 UTC"
  },
  {
    "arxiv_id": "2411.04316v1",
    "title": "A Multilingual Sentiment Lexicon for Low-Resource Language Translation using Large Languages Models and Explainable AI",
    "authors": [
      "Melusi Malinga",
      "Isaac Lupanda",
      "Mike Wa Nkongolo",
      "Phil van Deventer"
    ],
    "abstract": "South Africa and the Democratic Republic of Congo (DRC) present a complex\nlinguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,\nEnglish, and Tshiluba (Ciluba), which creates unique challenges for AI-driven\ntranslation and sentiment analysis systems due to a lack of accurately labeled\ndata. This study seeks to address these challenges by developing a multilingual\nlexicon designed for French and Tshiluba, now expanded to include translations\nin English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural\nrelevance in sentiment classification by integrating language-specific\nsentiment scores. A comprehensive testing corpus is created to support\ntranslation and sentiment analysis tasks, with machine learning models such as\nRandom Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive\nBayes (GNB) trained to predict sentiment across low resource languages (LRLs).\nAmong them, the Random Forest model performed particularly well, capturing\nsentiment polarity and handling language-specific nuances effectively.\nFurthermore, Bidirectional Encoder Representations from Transformers (BERT), a\nLarge Language Model (LLM), is applied to predict context-based sentiment with\nhigh accuracy, achieving 99% accuracy and 98% precision, outperforming other\nmodels. The BERT predictions were clarified using Explainable AI (XAI),\nimproving transparency and fostering confidence in sentiment classification.\nOverall, findings demonstrate that the proposed lexicon and machine learning\nmodels significantly enhance translation and sentiment analysis for LRLs in\nSouth Africa and the DRC, laying a foundation for future AI models that support\nunderrepresented languages, with applications across education, governance, and\nbusiness in multilingual contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "This work is part of a PhD proposal in Information Technology at the\n  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised\n  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in\n  the Department of Informatics",
    "pdf_url": "http://arxiv.org/pdf/2411.04316v1",
    "published_date": "2024-11-06 23:41:18 UTC",
    "updated_date": "2024-11-06 23:41:18 UTC"
  },
  {
    "arxiv_id": "2411.04308v1",
    "title": "Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education",
    "authors": [
      "Anand Syamkumar",
      "Nora Tseng",
      "Kaycie Barron",
      "Shanglin Yang",
      "Shamya Karumbaiah",
      "Rheeya Uppal",
      "Junjie Hu"
    ],
    "abstract": "Large language models (LLMs) offer promise in generating educational content,\nproviding instructor feedback, and reducing teacher workload on assessments.\nWhile prior studies have focused on studying LLM-powered learning analytics,\nlimited research has examined how effective LLMs are in a bilingual context. In\nthis paper, we study the effectiveness of multilingual large language models\n(MLLMs) across monolingual (English-only, Spanish-only) and bilingual\n(Spanglish) student writing. We present a learning analytics use case that\ndetails LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias\nin the grading performance of pre-trained models for bilingual writing compared\nto English-only and Spanish-only writing. Following this, we fine-tune\nopen-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets\ngenerated in English, Spanish, and Spanglish. Our experiments indicate that the\nmodels perform significantly better for all three languages after fine-tuning\nwith bilingual data. This study highlights the potential of enhancing MLLM\neffectiveness to support authentic language practices amongst bilingual\nlearners. It also aims to illustrate the value of incorporating non-English\nlanguages into the design and implementation of language models in education.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04308v1",
    "published_date": "2024-11-06 23:16:25 UTC",
    "updated_date": "2024-11-06 23:16:25 UTC"
  },
  {
    "arxiv_id": "2411.04293v2",
    "title": "A Random-Key Optimizer for Combinatorial Optimization",
    "authors": [
      "Antonio A. Chaves",
      "Mauricio G. C. Resende",
      "Martin J. A. Schuetz",
      "J. Kyle Brubaker",
      "Helmut G. Katzgraber",
      "Edilson F. de Arruda",
      "Ricardo M. A. Silva"
    ],
    "abstract": "This paper presents the Random-Key Optimizer (RKO), a versatile and efficient\nstochastic local search method tailored for combinatorial optimization\nproblems. Using the random-key concept, RKO encodes solutions as vectors of\nrandom keys that are subsequently decoded into feasible solutions via\nproblem-specific decoders. The RKO framework is able to combine a plethora of\nclassic metaheuristics, each capable of operating independently or in parallel,\nwith solution sharing facilitated through an elite solution pool. This modular\napproach allows for the adaptation of various metaheuristics, including\nsimulated annealing, iterated local search, and greedy randomized adaptive\nsearch procedures, among others. The efficacy of the RKO framework, implemented\nin C++, is demonstrated through its application to three NP-hard combinatorial\noptimization problems: the alpha-neighborhood p-median problem, the tree of\nhubs location problem, and the node-capacitated graph partitioning problem. The\nresults highlight the framework's ability to produce high-quality solutions\nacross diverse problem domains, underscoring its potential as a robust tool for\ncombinatorial optimization.",
    "categories": [
      "cs.AI",
      "cond-mat.dis-nn",
      "cs.NE",
      "math.OC",
      "90-02, 90B40, 90C27",
      "G.1.6; G.2.1; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "54 pages, 16 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.04293v2",
    "published_date": "2024-11-06 22:23:29 UTC",
    "updated_date": "2024-11-15 22:04:15 UTC"
  },
  {
    "arxiv_id": "2411.04285v1",
    "title": "Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning",
    "authors": [
      "Thomas Frost",
      "Kezhi Li",
      "Steve Harris"
    ],
    "abstract": "The task of predicting long-term patient outcomes using supervised machine\nlearning is a challenging one, in part because of the high variance of each\npatient's trajectory, which can result in the model over-fitting to the\ntraining data. Temporal difference (TD) learning, a common reinforcement\nlearning technique, may reduce variance by generalising learning to the pattern\nof state transitions rather than terminal outcomes. However, in healthcare this\nmethod requires several strong assumptions about patient states, and there\nappears to be limited literature evaluating the performance of TD learning\nagainst traditional supervised learning methods for long-term health outcome\nprediction tasks. In this study, we define a framework for applying TD learning\nto real-time irregularly sampled time series data using a Semi-Markov Reward\nProcess. We evaluate the model framework in predicting intensive care mortality\nand show that TD learning under this framework can result in improved model\nrobustness compared to standard supervised learning methods. and that this\nrobustness is maintained even when validated on external datasets. This\napproach may offer a more reliable method when learning to predict patient\noutcomes using high-variance irregular time series data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in the Proceedings of the 4th Machine Learning for\n  Health symposium, Proceedings of Machine Learning Research (PMLR)",
    "pdf_url": "http://arxiv.org/pdf/2411.04285v1",
    "published_date": "2024-11-06 22:11:20 UTC",
    "updated_date": "2024-11-06 22:11:20 UTC"
  },
  {
    "arxiv_id": "2411.04282v2",
    "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding",
    "authors": [
      "Haolin Chen",
      "Yihao Feng",
      "Zuxin Liu",
      "Weiran Yao",
      "Akshara Prabhakar",
      "Shelby Heinecke",
      "Ricky Ho",
      "Phil Mui",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang"
    ],
    "abstract": "Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04282v2",
    "published_date": "2024-11-06 22:02:30 UTC",
    "updated_date": "2024-11-21 20:29:09 UTC"
  },
  {
    "arxiv_id": "2411.04281v1",
    "title": "Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking",
    "authors": [
      "Xingran Chen",
      "Zhenke Wu",
      "Xu Shi",
      "Hyunghoon Cho",
      "Bhramar Mukherjee"
    ],
    "abstract": "We conduct a scoping review of existing approaches for synthetic EHR data\ngeneration, and benchmark major methods with proposed open-source software to\noffer recommendations for practitioners. We search three academic databases for\nour scoping review. Methods are benchmarked on open-source EHR datasets,\nMIMIC-III/IV. Seven existing methods covering major categories and two baseline\nmethods are implemented and compared. Evaluation metrics concern data fidelity,\ndownstream utility, privacy protection, and computational cost. 42 studies are\nidentified and classified into five categories. Seven open-source methods\ncovering all categories are selected, trained on MIMIC-III, and evaluated on\nMIMIC-III or MIMIC-IV for transportability considerations. Among them,\nGAN-based methods demonstrate competitive performance in fidelity and utility\non MIMIC-III; rule-based methods excel in privacy protection. Similar findings\nare observed on MIMIC-IV, except that GAN-based methods further outperform the\nbaseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is\nprovided to integrate various choices of approaches and evaluation metrics,\nenabling more streamlined exploration and evaluation of multiple methods. We\nfound that method choice is governed by the relative importance of the\nevaluation metrics in downstream use cases. We provide a decision tree to guide\nthe choice among the benchmarked methods. Based on the decision tree, GAN-based\nmethods excel when distributional shifts exist between the training and testing\npopulations. Otherwise, CorGAN and MedGAN are most suitable for association\nmodeling and predictive modeling, respectively. Future research should\nprioritize enhancing fidelity of the synthetic data while controlling privacy\nexposure, and comprehensive benchmarking of longitudinal or conditional\ngeneration methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04281v1",
    "published_date": "2024-11-06 21:59:19 UTC",
    "updated_date": "2024-11-06 21:59:19 UTC"
  },
  {
    "arxiv_id": "2411.04280v1",
    "title": "Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems",
    "authors": [
      "Miko≈Çaj S≈Çupi≈Ñski",
      "Piotr Lipi≈Ñski"
    ],
    "abstract": "In this paper, we propose a novel model called Recurrent Explicit Duration\nSwitching Linear Dynamical Systems (REDSLDS) that incorporates recurrent\nexplicit duration variables into the rSLDS model. We also propose an inference\nand learning scheme that involves the use of P\\'olya-gamma augmentation. We\ndemonstrate the improved segmentation capabilities of our model on three\nbenchmark datasets, including two quantitative datasets and one qualitative\ndataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04280v1",
    "published_date": "2024-11-06 21:58:24 UTC",
    "updated_date": "2024-11-06 21:58:24 UTC"
  },
  {
    "arxiv_id": "2411.04278v1",
    "title": "The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model",
    "authors": [
      "Miko≈Çaj S≈Çupi≈Ñski",
      "Piotr Lipi≈Ñski"
    ],
    "abstract": "The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural\nBayesian nonparametric extension of the classical Hidden Markov Model for\nlearning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to\nstrengthen the self-persistence probability in the HDP-HMM. Then, disentangled\nsticky HDP-HMM has been proposed to disentangle the strength of the\nself-persistence prior and transition prior. However, the sticky HDP-HMM\nassumes that the self-persistence probability is stationary, limiting its\nexpressiveness. Here, we build on previous work on sticky HDP-HMM and\ndisentangled sticky HDP-HMM, developing a more general model: the recurrent\nsticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for\nefficient inference in this model. We show that RS-HDP-HMM outperforms\ndisentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and\nreal data segmentation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04278v1",
    "published_date": "2024-11-06 21:49:20 UTC",
    "updated_date": "2024-11-06 21:49:20 UTC"
  },
  {
    "arxiv_id": "2411.04265v1",
    "title": "Graph neural networks and non-commuting operators",
    "authors": [
      "Mauricio Velasco",
      "Kaiying O'Hare",
      "Bernardo Rychtenberg",
      "Soledad Villar"
    ],
    "abstract": "Graph neural networks (GNNs) provide state-of-the-art results in a wide\nvariety of tasks which typically involve predicting features at the vertices of\na graph. They are built from layers of graph convolutions which serve as a\npowerful inductive bias for describing the flow of information among the\nvertices. Often, more than one data modality is available. This work considers\na setting in which several graphs have the same vertex set and a common\nvertex-level learning task. This generalizes standard GNN models to GNNs with\nseveral graph operators that do not commute. We may call this model graph-tuple\nneural networks (GtNN).\n  In this work, we develop the mathematical theory to address the stability and\ntransferability of GtNNs using properties of non-commuting non-expansive\noperators. We develop a limit theory of graphon-tuple neural networks and use\nit to prove a universal transferability theorem that guarantees that all\ngraph-tuple neural networks are transferable on convergent graph-tuple\nsequences. In particular, there is no non-transferable energy under the\nconvergence we consider here. Our theoretical results extend well-known\ntransferability theorems for GNNs to the case of several simultaneous graphs\n(GtNNs) and provide a strict improvement on what is currently known even in the\nGNN case.\n  We illustrate our theoretical results with simple experiments on synthetic\nand real-world data. To this end, we derive a training procedure that provably\nenforces the stability of the resulting model.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.04265v1",
    "published_date": "2024-11-06 21:17:14 UTC",
    "updated_date": "2024-11-06 21:17:14 UTC"
  },
  {
    "arxiv_id": "2411.04263v1",
    "title": "Object Recognition in Human Computer Interaction:- A Comparative Analysis",
    "authors": [
      "Kaushik Ranade",
      "Tanmay Khule",
      "Riddhi More"
    ],
    "abstract": "Human-computer interaction (HCI) has been a widely researched area for many\nyears, with continuous advancements in technology leading to the development of\nnew techniques that change the way we interact with computers. With the recent\nadvent of powerful computers, we recognize human actions and interact\naccordingly, thus revolutionizing the way we interact with computers. The\npurpose of this paper is to provide a comparative analysis of various\nalgorithms used for recognizing user faces and gestures in the context of\ncomputer vision and HCI. This study aims to explore and evaluate the\nperformance of different algorithms in terms of accuracy, robustness, and\nefficiency. This study aims to provide a comprehensive analysis of algorithms\nfor face and gesture recognition in the context of computer vision and HCI,\nwith the goal of improving the design and development of interactive systems\nthat are more intuitive, efficient, and user-friendly.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04263v1",
    "published_date": "2024-11-06 21:16:02 UTC",
    "updated_date": "2024-11-06 21:16:02 UTC"
  },
  {
    "arxiv_id": "2411.04246v1",
    "title": "Learning Generalizable Policy for Obstacle-Aware Autonomous Drone Racing",
    "authors": [
      "Yueqian Liu"
    ],
    "abstract": "Autonomous drone racing has gained attention for its potential to push the\nboundaries of drone navigation technologies. While much of the existing\nresearch focuses on racing in obstacle-free environments, few studies have\naddressed the complexities of obstacle-aware racing, and approaches presented\nin these studies often suffer from overfitting, with learned policies\ngeneralizing poorly to new environments. This work addresses the challenge of\ndeveloping a generalizable obstacle-aware drone racing policy using deep\nreinforcement learning. We propose applying domain randomization on racing\ntracks and obstacle configurations before every rollout, combined with parallel\nexperience collection in randomized environments to achieve the goal. The\nproposed randomization strategy is shown to be effective through simulated\nexperiments where drones reach speeds of up to 70 km/h, racing in unseen\ncluttered environments. This study serves as a stepping stone toward learning\nrobust policies for obstacle-aware drone racing and general-purpose drone\nnavigation in cluttered environments. Code is available at\nhttps://github.com/ErcBunny/IsaacGymEnvs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 11 figures. This preprint is part of the author's M.Sc.\n  thesis supervised by Ir. Hang Yu and Dr. Ir. Christophe De Wagter, at MAVLab\n  TU Delft. Full thesis is available at https://repository.tudelft.nl",
    "pdf_url": "http://arxiv.org/pdf/2411.04246v1",
    "published_date": "2024-11-06 20:25:43 UTC",
    "updated_date": "2024-11-06 20:25:43 UTC"
  },
  {
    "arxiv_id": "2411.10473v1",
    "title": "PhDGPT: Introducing a psychometric and linguistic dataset about how large language models perceive graduate students and professors in psychology",
    "authors": [
      "Edoardo Sebastiano De Duro",
      "Enrique Taietta",
      "Riccardo Improta",
      "Massimo Stella"
    ],
    "abstract": "Machine psychology aims to reconstruct the mindset of Large Language Models\n(LLMs), i.e. how these artificial intelligences perceive and associate ideas.\nThis work introduces PhDGPT, a prompting framework and synthetic dataset that\nencapsulates the machine psychology of PhD researchers and professors as\nperceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints,\ncounting 300 iterations repeated across 15 academic events, 2 biological\ngenders, 2 career levels and 42 unique item responses of the Depression,\nAnxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric\nscores with their explanations in plain language. This synergy of scores and\ntexts offers a dual, comprehensive perspective on the emotional well-being of\nsimulated academics, e.g. male/female PhD students or professors. By combining\nnetwork psychometrics and psycholinguistic dimensions, this study identifies\nseveral similarities and distinctions between human and LLM data. The\npsychometric networks of simulated male professors do not differ between\nphysical and emotional anxiety subscales, unlike humans. Other LLMs'\npersonification can reconstruct human DASS factors with a purity up to 80%.\nFurthemore, LLM-generated personifications across different scenarios are found\nto elicit explanations lower in concreteness and imageability in items coding\nfor anxiety, in agreement with past studies about human psychology. Our\nfindings indicate an advanced yet incomplete ability for LLMs to reproduce the\ncomplexity of human psychometric data, unveiling convenient advantages and\nlimitations in using LLMs to replace human participants. PhDGPT also\nintriguingly capture the ability for LLMs to adapt and change language patterns\naccording to prompted mental distress contextual features, opening new\nquantitative opportunities for assessing the machine psychology of these\nartificial intelligences.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "20 pages, 8 figures. Edoardo Sebastiano De Duro and Enrique Taietta\n  equally contributed to this work",
    "pdf_url": "http://arxiv.org/pdf/2411.10473v1",
    "published_date": "2024-11-06 20:04:20 UTC",
    "updated_date": "2024-11-06 20:04:20 UTC"
  },
  {
    "arxiv_id": "2411.04224v1",
    "title": "WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing",
    "authors": [
      "Julian Strohmayer",
      "Matthias W√∂dlinger",
      "Martin Kampel"
    ],
    "abstract": "We propose WiFlexFormer, a highly efficient Transformer-based architecture\ndesigned for WiFi Channel State Information (CSI)-based person-centric sensing.\nWe benchmark WiFlexFormer against state-of-the-art vision and specialized\narchitectures for processing radio frequency data and demonstrate that it\nachieves comparable Human Activity Recognition (HAR) performance while offering\na significantly lower parameter count and faster inference times. With an\ninference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is\noptimized for real-time inference. Additionally, its low parameter count\ncontributes to improved cross-domain generalization, where it often outperforms\nlarger models. Our comprehensive evaluation shows that WiFlexFormer is a\npotential solution for efficient, scalable WiFi-based sensing applications. The\nPyTorch implementation of WiFlexFormer is publicly available at:\nhttps://github.com/StrohmayerJ/WiFlexFormer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04224v1",
    "published_date": "2024-11-06 19:44:36 UTC",
    "updated_date": "2024-11-06 19:44:36 UTC"
  },
  {
    "arxiv_id": "2411.04219v1",
    "title": "Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction",
    "authors": [
      "Zhao Xu",
      "Haiyang Yu",
      "Montgomery Bohde",
      "Shuiwang Ji"
    ],
    "abstract": "Recent advancements in equivariant deep models have shown promise in\naccurately predicting atomic potentials and force fields in molecular dynamics\nsimulations. Using spherical harmonics (SH) and tensor products (TP), these\nequivariant networks gain enhanced physical understanding, like symmetries and\nmany-body interactions. Beyond encoding physical insights, SH and TP are also\ncrucial to represent equivariant polynomial functions. In this work, we analyze\nthe equivariant polynomial functions for the equivariant architecture, and\nintroduce a novel equivariant network, named PACE. The proposed PACE utilizes\nedge booster and the Atomic Cluster Expansion (ACE) technique to approximate a\ngreater number of $SE(3) \\times S_n$ equivariant polynomial functions with\nenhanced degrees. As experimented in commonly used benchmarks, PACE\ndemonstrates state-of-the-art performance in predicting atomic energy and force\nfields, with robust generalization capability across various geometric\ndistributions under molecular dynamics (MD) across different temperature\nconditions. Our code is publicly available as part of the AIRS library\nhttps://github.com/divelab/AIRS/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04219v1",
    "published_date": "2024-11-06 19:34:40 UTC",
    "updated_date": "2024-11-06 19:34:40 UTC"
  },
  {
    "arxiv_id": "2411.04217v1",
    "title": "Quantum Diffusion Models for Few-Shot Learning",
    "authors": [
      "Ruhan Wang",
      "Ye Wang",
      "Jing Liu",
      "Toshiaki Koike-Akino"
    ],
    "abstract": "Modern quantum machine learning (QML) methods involve the variational\noptimization of parameterized quantum circuits on training datasets, followed\nby predictions on testing datasets. Most state-of-the-art QML algorithms\ncurrently lack practical advantages due to their limited learning capabilities,\nespecially in few-shot learning tasks. In this work, we propose three new\nframeworks employing quantum diffusion model (QDM) as a solution for the\nfew-shot learning: label-guided generation inference (LGGI); label-guided\ndenoising inference (LGDI); and label-guided noise addition inference (LGNAI).\nExperimental results demonstrate that our proposed algorithms significantly\noutperform existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.04217v1",
    "published_date": "2024-11-06 19:25:06 UTC",
    "updated_date": "2024-11-06 19:25:06 UTC"
  },
  {
    "arxiv_id": "2411.05042v1",
    "title": "Improving Radiology Report Conciseness and Structure via Local Large Language Models",
    "authors": [
      "Iryna Hartsock",
      "Cyrillo Araujo",
      "Les Folio",
      "Ghulam Rasool"
    ],
    "abstract": "In this study, we aim to enhance radiology reporting by improving both the\nconciseness and structured organization of findings (also referred to as\ntemplating), specifically by organizing information according to anatomical\nregions. This structured approach allows physicians to locate relevant\ninformation quickly, increasing the report's utility. We utilize Large Language\nModels (LLMs) such as Mixtral, Mistral, and Llama to generate concise,\nwell-structured reports. Among these, we primarily focus on the Mixtral model\ndue to its superior adherence to specific formatting requirements compared to\nother models. To maintain data security and privacy, we run these LLMs locally\nbehind our institution's firewall. We leverage the LangChain framework and\napply five distinct prompting strategies to enforce a consistent structure in\nradiology reports, aiming to eliminate extraneous language and achieve a high\nlevel of conciseness. We also introduce a novel metric, the Conciseness\nPercentage (CP) score, to evaluate report brevity. Our dataset comprises 814\nradiology reports authored by seven board-certified body radiologists at our\ncancer center. In evaluating the different prompting methods, we discovered\nthat the most effective approach for generating concise, well-structured\nreports involves first instructing the LLM to condense the report, followed by\na prompt to structure the content according to specific guidelines. We assessed\nall prompting strategies based on their ability to handle formatting issues,\nreduce report length, and adhere to formatting instructions. Our findings\ndemonstrate that open-source, locally deployed LLMs can significantly improve\nradiology report conciseness and structure while conforming to specified\nformatting standards.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05042v1",
    "published_date": "2024-11-06 19:00:57 UTC",
    "updated_date": "2024-11-06 19:00:57 UTC"
  },
  {
    "arxiv_id": "2411.04168v4",
    "title": "DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency Method for Image Generation",
    "authors": [
      "Hao Phung",
      "Quan Dao",
      "Trung Dao",
      "Hoang Phan",
      "Dimitris Metaxas",
      "Anh Tran"
    ],
    "abstract": "We introduce a novel state-space architecture for diffusion models,\neffectively harnessing spatial and frequency information to enhance the\ninductive bias towards local features in input images for image generation\ntasks. While state-space networks, including Mamba, a revolutionary advancement\nin recurrent neural networks, typically scan input sequences from left to\nright, they face difficulties in designing effective scanning strategies,\nespecially in the processing of image data. Our method demonstrates that\nintegrating wavelet transformation into Mamba enhances the local structure\nawareness of visual inputs and better captures long-range relations of\nfrequencies by disentangling them into wavelet subbands, representing both low-\nand high-frequency components. These wavelet-based outputs are then processed\nand seamlessly fused with the original Mamba outputs through a cross-attention\nfusion layer, combining both spatial and frequency information to optimize the\norder awareness of state-space models which is essential for the details and\noverall quality of image generation. Besides, we introduce a globally-shared\ntransformer to supercharge the performance of Mamba, harnessing its exceptional\npower to capture global relationships. Through extensive experiments on\nstandard benchmarks, our method demonstrates superior results compared to DiT\nand DIFFUSSM, achieving faster training convergence and delivering high-quality\noutputs. The codes and pretrained models are released at\nhttps://github.com/VinAIResearch/DiMSUM.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024. Project page:\n  https://vinairesearch.github.io/DiMSUM/",
    "pdf_url": "http://arxiv.org/pdf/2411.04168v4",
    "published_date": "2024-11-06 18:59:17 UTC",
    "updated_date": "2025-04-10 23:29:21 UTC"
  },
  {
    "arxiv_id": "2411.05040v1",
    "title": "Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs",
    "authors": [
      "Scott E. Friedman",
      "Noam Benkler",
      "Drisana Mosaphir",
      "Jeffrey Rye",
      "Sonja M. Schmer-Galunder",
      "Micah Goldwater",
      "Matthew McLure",
      "Ruta Wheelock",
      "Jeremy Gottlieb",
      "Robert P. Goldman",
      "Christopher Miller"
    ],
    "abstract": "Large language models (LLMs) generate diverse, situated, persuasive texts\nfrom a plurality of potential perspectives, influenced heavily by their prompts\nand training data. As part of LLM adoption, we seek to characterize - and\nideally, manage - the socio-cultural values that they express, for reasons of\nsafety, accuracy, inclusion, and cultural fidelity. We present a validated\napproach to automatically (1) extracting heterogeneous latent value\npropositions from texts, (2) assessing resonance and conflict of values with\ntexts, and (3) combining these operations to characterize the pluralistic value\nalignment of human-sourced and LLM-sourced textual data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05040v1",
    "published_date": "2024-11-06 18:51:04 UTC",
    "updated_date": "2024-11-06 18:51:04 UTC"
  },
  {
    "arxiv_id": "2411.04118v2",
    "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?",
    "authors": [
      "Daniel P. Jeong",
      "Saurabh Garg",
      "Zachary C. Lipton",
      "Michael Oberst"
    ],
    "abstract": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "This version was published at EMNLP 2024 Main Conference as a Long\n  Paper (Oral). See the extended version (arXiv:2411.08870) for additional\n  results on QA tasks based on clinical notes and evaluations in the supervised\n  fine-tuning regime",
    "pdf_url": "http://arxiv.org/pdf/2411.04118v2",
    "published_date": "2024-11-06 18:51:02 UTC",
    "updated_date": "2024-11-19 20:51:58 UTC"
  },
  {
    "arxiv_id": "2411.04112v1",
    "title": "Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation",
    "authors": [
      "Shreya Gummadi",
      "Mateus V. Gasparino",
      "Deepak Vasisht",
      "Girish Chowdhary"
    ],
    "abstract": "Centralized learning requires data to be aggregated at a central server,\nwhich poses significant challenges in terms of data privacy and bandwidth\nconsumption. Federated learning presents a compelling alternative, however,\nvanilla federated learning methods deployed in robotics aim to learn a single\nglobal model across robots that works ideally for all. But in practice one\nmodel may not be well suited for robots deployed in various environments. This\npaper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated\nlearning framework that is deployed with vision based autonomous robot\nnavigation in diverse outdoor environments. The framework addresses the key\nfederated learning challenge of deteriorating model performance of a single\nglobal model due to the presence of non-IID data across real-world robots.\nExtensive real-world experiments validate that Fed-EC reduces the communication\nsize by 23x for each robot while matching the performance of centralized\nlearning for goal-oriented navigation and outperforms local learning. Fed-EC\ncan transfer previously learnt models to new robots that join the cluster.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04112v1",
    "published_date": "2024-11-06 18:44:09 UTC",
    "updated_date": "2024-11-06 18:44:09 UTC"
  },
  {
    "arxiv_id": "2411.15147v1",
    "title": "Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits",
    "authors": [
      "Gordana Dodig-Crnkovic",
      "Gianfranco Basti",
      "Tobias Holstein"
    ],
    "abstract": "As AI systems increasingly operate with autonomy and adaptability, the\ntraditional boundaries of moral responsibility in techno-social systems are\nbeing challenged. This paper explores the evolving discourse on the delegation\nof responsibilities to intelligent autonomous agents and the ethical\nimplications of such practices. Synthesizing recent developments in AI ethics,\nincluding concepts of distributed responsibility and ethical AI by design, the\npaper proposes a functionalist perspective as a framework. This perspective\nviews moral responsibility not as an individual trait but as a role within a\nsocio-technical system, distributed among human and artificial agents. As an\nexample of 'AI ethical by design,' we present Basti and Vitiello's\nimplementation. They suggest that AI can act as artificial moral agents by\nlearning ethical guidelines and using Deontic Higher-Order Logic to assess\ndecisions ethically. Motivated by the possible speed and scale beyond human\nsupervision and ethical implications, the paper argues for 'AI ethical by\ndesign', while acknowledging the distributed, shared, and dynamic nature of\nresponsibility. This functionalist approach offers a practical framework for\nnavigating the complexities of AI ethics in a rapidly evolving technological\nlandscape.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K. Computing Milieux K.4 COMPUTERS AND SOCIETY K.4.1 Public Policy\n  Issues. Ethics"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15147v1",
    "published_date": "2024-11-06 18:40:38 UTC",
    "updated_date": "2024-11-06 18:40:38 UTC"
  },
  {
    "arxiv_id": "2411.04165v1",
    "title": "Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences",
    "authors": [
      "Niklas Schmidinger",
      "Lisa Schneckenreiter",
      "Philipp Seidl",
      "Johannes Schimunek",
      "Pieter-Jan Hoedt",
      "Johannes Brandstetter",
      "Andreas Mayr",
      "Sohvi Luukkonen",
      "Sepp Hochreiter",
      "G√ºnter Klambauer"
    ],
    "abstract": "Language models for biological and chemical sequences enable crucial\napplications such as drug discovery, protein engineering, and precision\nmedicine. Currently, these language models are predominantly based on\nTransformer architectures. While Transformers have yielded impressive results,\ntheir quadratic runtime dependency on the sequence length complicates their use\nfor long genomic sequences and in-context learning on proteins and chemical\nsequences. Recently, the recurrent xLSTM architecture has been shown to perform\nfavorably compared to Transformers and modern state-space model (SSM)\narchitectures in the natural language domain. Similar to SSMs, xLSTMs have a\nlinear runtime dependency on the sequence length and allow for constant-memory\ndecoding at inference time, which makes them prime candidates for modeling\nlong-range dependencies in biological and chemical sequences. In this work, we\ntailor xLSTM towards these domains and propose a suite of architectural\nvariants called Bio-xLSTM. Extensive experiments in three large domains,\ngenomics, proteins, and chemistry, were performed to assess xLSTM's ability to\nmodel biological and chemical sequences. The results show that models based on\nBio-xLSTM a) can serve as proficient generative models for DNA, protein, and\nchemical sequences, b) learn rich representations for those modalities, and c)\ncan perform in-context learning for proteins and small molecules.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04165v1",
    "published_date": "2024-11-06 18:36:48 UTC",
    "updated_date": "2024-11-06 18:36:48 UTC"
  },
  {
    "arxiv_id": "2411.04109v2",
    "title": "Self-Consistency Preference Optimization",
    "authors": [
      "Archiki Prasad",
      "Weizhe Yuan",
      "Richard Yuanzhe Pang",
      "Jing Xu",
      "Maryam Fazel-Zarandi",
      "Mohit Bansal",
      "Sainbayar Sukhbaatar",
      "Jason Weston",
      "Jane Yu"
    ],
    "abstract": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.04109v2",
    "published_date": "2024-11-06 18:36:22 UTC",
    "updated_date": "2024-11-07 23:41:00 UTC"
  },
  {
    "arxiv_id": "2411.04105v3",
    "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis",
    "authors": [
      "Guan Zhe Hong",
      "Nishanth Dikkala",
      "Enming Luo",
      "Cyrus Rashtchian",
      "Xin Wang",
      "Rina Panigrahy"
    ],
    "abstract": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04105v3",
    "published_date": "2024-11-06 18:35:32 UTC",
    "updated_date": "2024-12-09 16:36:34 UTC"
  },
  {
    "arxiv_id": "2411.04097v1",
    "title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
    "authors": [
      "Maya Varma",
      "Jean-Benoit Delbrouck",
      "Zhihong Chen",
      "Akshay Chaudhari",
      "Curtis Langlotz"
    ],
    "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations\nbetween image features and textual attributes, resulting in degraded zero-shot\nperformance at test time. Existing approaches for addressing spurious\ncorrelations (i) primarily operate at the global image-level rather than\nintervening directly on fine-grained image features and (ii) are predominantly\ndesigned for unimodal settings. In this work, we present RaVL, which takes a\nfine-grained perspective on VLM robustness by discovering and mitigating\nspurious correlations using local image features rather than operating at the\nglobal image level. Given a fine-tuned VLM, RaVL first discovers spurious\ncorrelations by leveraging a region-level clustering approach to identify\nprecise image features contributing to zero-shot classification errors. Then,\nRaVL mitigates the identified spurious correlation with a novel region-aware\nloss function that enables the VLM to focus on relevant regions and ignore\nspurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with\nvarious model architectures, data domains, and learned spurious correlations.\nOur results show that RaVL accurately discovers (191% improvement over the\nclosest baseline) and mitigates (8.2% improvement on worst-group image\nclassification accuracy) spurious correlations. Qualitative evaluations on\ngeneral-domain and medical-domain VLMs confirm our findings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.04097v1",
    "published_date": "2024-11-06 18:25:00 UTC",
    "updated_date": "2024-11-06 18:25:00 UTC"
  },
  {
    "arxiv_id": "2411.13559v1",
    "title": "Composing Ensembles of Instrument-Model Pairs for Optimizing Profitability in Algorithmic Trading",
    "authors": [
      "Sahand Hassanizorgabad"
    ],
    "abstract": "Financial markets are nonlinear with complexity, where different types of\nassets are traded between buyers and sellers, each having a view to maximize\ntheir Return on Investment (ROI). Forecasting market trends is a challenging\ntask since various factors like stock-specific news, company profiles, public\nsentiments, and global economic conditions influence them. This paper describes\na daily price directional predictive system of financial instruments,\naddressing the difficulty of predicting short-term price movements. This paper\nwill introduce the development of a novel trading system methodology by\nproposing a two-layer Composing Ensembles architecture, optimized through grid\nsearch, to predict whether the price will rise or fall the next day. This\nstrategy was back-tested on a wide range of financial instruments and time\nframes, demonstrating an improvement of 20% over the benchmark, representing a\nstandard investment strategy.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.TR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13559v1",
    "published_date": "2024-11-06 18:17:26 UTC",
    "updated_date": "2024-11-06 18:17:26 UTC"
  },
  {
    "arxiv_id": "2411.04090v2",
    "title": "A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement",
    "authors": [
      "Guillermo Villate-Castillo",
      "Javier Del Ser",
      "Borja Sanz"
    ],
    "abstract": "Content moderation typically combines the efforts of human moderators and\nmachine learning models. However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception. Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered. In this work, we introduce a novel content moderation framework\nthat emphasizes the importance of capturing annotation disagreement. Our\napproach uses multitask learning, where toxicity classification serves as the\nprimary task and annotation disagreement is addressed as an auxiliary task.\nAdditionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review. We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "68T50 (Primary) 68T37 (Secondary)",
      "I.2.7; I.2.1"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2411.04090v2",
    "published_date": "2024-11-06 18:08:57 UTC",
    "updated_date": "2024-11-07 07:12:45 UTC"
  },
  {
    "arxiv_id": "2411.05039v2",
    "title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language Classification",
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity"
    ],
    "abstract": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Updated and Final Version",
    "pdf_url": "http://arxiv.org/pdf/2411.05039v2",
    "published_date": "2024-11-06 17:58:01 UTC",
    "updated_date": "2025-03-13 16:17:21 UTC"
  },
  {
    "arxiv_id": "2411.04075v1",
    "title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
    "authors": [
      "Chuhan Li",
      "Ziyao Shangguan",
      "Yilun Zhao",
      "Deyuan Li",
      "Yixin Liu",
      "Arman Cohan"
    ],
    "abstract": "Existing benchmarks for evaluating foundation models mainly focus on\nsingle-document, text-only tasks. However, they often fail to fully capture the\ncomplexity of research workflows, which typically involve interpreting\nnon-textual data and gathering information across multiple documents. To\naddress this gap, we introduce M3SciQA, a multi-modal, multi-document\nscientific question answering benchmark designed for a more comprehensive\nevaluation of foundation models. M3SciQA consists of 1,452 expert-annotated\nquestions spanning 70 natural language processing paper clusters, where each\ncluster represents a primary paper along with all its cited documents,\nmirroring the workflow of comprehending a single paper by requiring multi-modal\nand multi-document data. With M3SciQA, we conduct a comprehensive evaluation of\n18 foundation models. Our results indicate that current foundation models still\nsignificantly underperform compared to human experts in multi-modal information\nretrieval and in reasoning across multiple scientific documents. Additionally,\nwe explore the implications of these findings for the future advancement of\napplying foundation models in multi-modal scientific literature analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04075v1",
    "published_date": "2024-11-06 17:52:01 UTC",
    "updated_date": "2024-11-06 17:52:01 UTC"
  },
  {
    "arxiv_id": "2411.05849v1",
    "title": "Input-Driven Dynamics for Robust Memory Retrieval in Hopfield Networks",
    "authors": [
      "Simone Betteti",
      "Giacomo Baggio",
      "Francesco Bullo",
      "Sandro Zampieri"
    ],
    "abstract": "The Hopfield model provides a mathematically idealized yet insightful\nframework for understanding the mechanisms of memory storage and retrieval in\nthe human brain. This model has inspired four decades of extensive research on\nlearning and retrieval dynamics, capacity estimates, and sequential transitions\namong memories. Notably, the role and impact of external inputs has been\nlargely underexplored, from their effects on neural dynamics to how they\nfacilitate effective memory retrieval. To bridge this gap, we propose a novel\ndynamical system framework in which the external input directly influences the\nneural synapses and shapes the energy landscape of the Hopfield model. This\nplasticity-based mechanism provides a clear energetic interpretation of the\nmemory retrieval process and proves effective at correctly classifying highly\nmixed inputs. Furthermore, we integrate this model within the framework of\nmodern Hopfield architectures, using this connection to elucidate how current\nand past information are combined during the retrieval process. Finally, we\nembed both the classic and the new model in an environment disrupted by noise\nand compare their robustness during memory retrieval.",
    "categories": [
      "q-bio.NC",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.LG",
      "math.DS",
      "37N25 (Primary) 37C75, 34D45 (Secondary)",
      "I.5.1; I.2.11"
    ],
    "primary_category": "q-bio.NC",
    "comment": "24 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05849v1",
    "published_date": "2024-11-06 17:24:25 UTC",
    "updated_date": "2024-11-06 17:24:25 UTC"
  },
  {
    "arxiv_id": "2411.04034v1",
    "title": "Non-Stationary Learning of Neural Networks with Automatic Soft Parameter Reset",
    "authors": [
      "Alexandre Galashov",
      "Michalis K. Titsias",
      "Andr√°s Gy√∂rgy",
      "Clare Lyle",
      "Razvan Pascanu",
      "Yee Whye Teh",
      "Maneesh Sahani"
    ],
    "abstract": "Neural networks are traditionally trained under the assumption that data come\nfrom a stationary distribution. However, settings which violate this assumption\nare becoming more popular; examples include supervised learning under\ndistributional shifts, reinforcement learning, continual learning and\nnon-stationary contextual bandits. In this work we introduce a novel learning\napproach that automatically models and adapts to non-stationarity, via an\nOrnstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift\ntends to draw the parameters towards the initialisation distribution, so the\napproach can be understood as a form of soft parameter reset. We show\nempirically that our approach performs well in non-stationary supervised and\noff-policy reinforcement learning settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04034v1",
    "published_date": "2024-11-06 16:32:40 UTC",
    "updated_date": "2024-11-06 16:32:40 UTC"
  },
  {
    "arxiv_id": "2411.05037v1",
    "title": "Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning",
    "authors": [
      "Mansi Sakarvadia"
    ],
    "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Language models (LMs) struggle to perform\nsuch reasoning consistently. We propose an approach to pinpoint and rectify\nmulti-hop reasoning failures through targeted memory injections on LM attention\nheads. First, we analyze the per-layer activations of GPT-2 models in response\nto single- and multi-hop prompts. We then propose a mechanism that allows users\nto inject relevant prompt-specific information, which we refer to as\n\"memories,\" at critical LM locations during inference. By thus enabling the LM\nto incorporate additional relevant information during inference, we enhance the\nquality of multi-hop prompt completions. We empirically show that a simple,\nefficient, and targeted memory injection into a key attention layer often\nincreases the probability of the desired next token in multi-hop tasks, by up\nto 424%. We observe that small subsets of attention heads can significantly\nimpact the model prediction during multi-hop reasoning. To more faithfully\ninterpret these heads, we develop Attention Lens: an open source tool that\ntranslates the outputs of attention heads into vocabulary tokens via learned\ntransformations called lenses. We demonstrate the use of lenses to reveal how a\nmodel arrives at its answer and use them to localize sources of model failures\nsuch as in the case of biased and malicious language generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "University of Chicago, Computer Science, Master of Science\n  Dissertation",
    "pdf_url": "http://arxiv.org/pdf/2411.05037v1",
    "published_date": "2024-11-06 16:30:26 UTC",
    "updated_date": "2024-11-06 16:30:26 UTC"
  },
  {
    "arxiv_id": "2411.05847v2",
    "title": "Federated Data-Driven Kalman Filtering for State Estimation",
    "authors": [
      "Nikos Piperigkos",
      "Alexandros Gkillas",
      "Christos Anagnostopoulos",
      "Aris S. Lalos"
    ],
    "abstract": "This paper proposes a novel localization framework based on collaborative\ntraining or federated learning paradigm, for highly accurate localization of\nautonomous vehicles. More specifically, we build on the standard approach of\nKalmanNet, a recurrent neural network aiming to estimate the underlying system\nuncertainty of traditional Extended Kalman Filtering, and reformulate it by the\nadapt-then-combine concept to FedKalmanNet. The latter is trained in a\ndistributed manner by a group of vehicles (or clients), with local training\ndatasets consisting of vehicular location and velocity measurements, through a\nglobal server aggregation operation. The FedKalmanNet is then used by each\nvehicle to localize itself, by estimating the associated system uncertainty\nmatrices (i.e, Kalman gain). Our aim is to actually demonstrate the benefits of\ncollaborative training for state estimation in autonomous driving, over\ncollaborative decision-making which requires rich V2X communication resources\nfor measurement exchange and sensor fusion under real-time constraints. An\nextensive experimental and evaluation study conducted in CARLA autonomous\ndriving simulator highlights the superior performance of FedKalmanNet over\nstate-of-the-art collaborative decision-making approaches, in localizing\nvehicles without the need of real-time V2X communication.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05847v2",
    "published_date": "2024-11-06 16:18:33 UTC",
    "updated_date": "2025-02-13 10:23:56 UTC"
  },
  {
    "arxiv_id": "2411.04011v2",
    "title": "Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo Tree Search",
    "authors": [
      "Fabio Pavirani",
      "Jonas Van Gompel",
      "Seyed Soroush Karimi Madahi",
      "Bert Claessens",
      "Chris Develder"
    ],
    "abstract": "The growing reliance on renewable energy sources, particularly solar and\nwind, has introduced challenges due to their uncontrollable production. This\ncomplicates maintaining the electrical grid balance, prompting some\ntransmission system operators in Western Europe to implement imbalance tariffs\nthat penalize unsustainable power deviations. These tariffs create an implicit\ndemand response framework to mitigate grid instability. Yet, several challenges\nlimit active participation. In Belgium, for example, imbalance prices are only\ncalculated at the end of each 15-minute settlement period, creating high risk\ndue to price uncertainty. This risk is further amplified by the inherent\nvolatility of imbalance prices, discouraging participation. Although\ntransmission system operators provide minute-based price predictions, the\nsystem imbalance volatility makes accurate price predictions challenging to\nobtain and requires sophisticated techniques. Moreover, publishing price\nestimates can prompt participants to adjust their schedules, potentially\naffecting the system balance and the final price, adding further complexity. To\naddress these challenges, we propose a Monte Carlo Tree Search method that\npublishes accurate imbalance prices while accounting for potential response\nactions. Our approach models the system dynamics using a neural network\nforecaster and a cluster of virtual batteries controlled by reinforcement\nlearning agents. Compared to Belgium's current publication method, our\ntechnique improves price accuracy by 20.4% under ideal conditions and by 12.8%\nin more realistic scenarios. This research addresses an unexplored, yet crucial\nproblem, positioning this paper as a pioneering work in analyzing the potential\nof more advanced imbalance price publishing techniques.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04011v2",
    "published_date": "2024-11-06 15:49:28 UTC",
    "updated_date": "2025-04-17 10:30:26 UTC"
  },
  {
    "arxiv_id": "2411.04008v1",
    "title": "Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability",
    "authors": [
      "Bharat Chandra Yalavarthi",
      "Nalini Ratha"
    ],
    "abstract": "In mission-critical domains such as law enforcement and medical diagnosis,\nthe ability to explain and interpret the outputs of deep learning models is\ncrucial for ensuring user trust and supporting informed decision-making.\nDespite advancements in explainability, existing methods often fall short in\nproviding explanations that mirror the depth and clarity of those given by\nhuman experts. Such expert-level explanations are essential for the dependable\napplication of deep learning models in law enforcement and medical contexts.\nAdditionally, we recognize that most explanations in real-world scenarios are\ncommunicated primarily through natural language. Addressing these needs, we\npropose a novel approach that utilizes characteristic descriptors to explain\nmodel decisions by identifying their presence in images, thereby generating\nexpert-like explanations. Our method incorporates a concept bottleneck layer\nwithin the model architecture, which calculates the similarity between image\nand descriptor encodings to deliver inherent and faithful explanations. Through\nexperiments in face recognition and chest X-ray diagnosis, we demonstrate that\nour approach offers a significant contrast over existing techniques, which are\noften limited to the use of saliency maps. We believe our approach represents a\nsignificant step toward making deep learning systems more accountable,\ntransparent, and trustworthy in the critical domains of face recognition and\nmedical diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04008v1",
    "published_date": "2024-11-06 15:47:18 UTC",
    "updated_date": "2024-11-06 15:47:18 UTC"
  },
  {
    "arxiv_id": "2411.04723v2",
    "title": "Exploring the Stability Gap in Continual Learning: The Role of the Classification Head",
    "authors": [
      "Wojciech ≈Åapacz",
      "Daniel Marczak",
      "Filip Szatkowski",
      "Tomasz Trzci≈Ñski"
    ],
    "abstract": "Continual learning (CL) has emerged as a critical area in machine learning,\nenabling neural networks to learn from evolving data distributions while\nmitigating catastrophic forgetting. However, recent research has identified the\nstability gap -- a phenomenon where models initially lose performance on\npreviously learned tasks before partially recovering during training. Such\nlearning dynamics are contradictory to the intuitive understanding of stability\nin continual learning where one would expect the performance to degrade\ngradually instead of rapidly decreasing and then partially recovering later. To\nbetter understand and alleviate the stability gap, we investigate it at\ndifferent levels of the neural network architecture, particularly focusing on\nthe role of the classification head. We introduce the nearest-mean classifier\n(NMC) as a tool to attribute the influence of the backbone and the\nclassification head on the stability gap. Our experiments demonstrate that NMC\nnot only improves final performance, but also significantly enhances training\nstability across various continual learning benchmarks, including CIFAR100,\nImageNet100, CUB-200, and FGVC Aircrafts. Moreover, we find that NMC also\nreduces task-recency bias. Our analysis provides new insights into the\nstability gap and suggests that the primary contributor to this phenomenon is\nthe linear head, rather than the insufficient representation learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.04723v2",
    "published_date": "2024-11-06 15:45:01 UTC",
    "updated_date": "2024-11-25 10:09:05 UTC"
  },
  {
    "arxiv_id": "2411.04006v1",
    "title": "Select2Plan: Training-Free ICL-Based Planning through VQA and Memory Retrieval",
    "authors": [
      "Davide Buoso",
      "Luke Robinson",
      "Giuseppe Averta",
      "Philip Torr",
      "Tim Franzmeyer",
      "Daniele De Martini"
    ],
    "abstract": "This study explores the potential of off-the-shelf Vision-Language Models\n(VLMs) for high-level robot planning in the context of autonomous navigation.\nIndeed, while most of existing learning-based approaches for path planning\nrequire extensive task-specific training/fine-tuning, we demonstrate how such\ntraining can be avoided for most practical cases. To do this, we introduce\nSelect2Plan (S2P), a novel training-free framework for high-level robot\nplanning which completely eliminates the need for fine-tuning or specialised\ntraining. By leveraging structured Visual Question-Answering (VQA) and\nIn-Context Learning (ICL), our approach drastically reduces the need for data\ncollection, requiring a fraction of the task-specific data typically used by\ntrained models, or even relying only on online data. Our method facilitates the\neffective use of a generally trained VLM in a flexible and cost-efficient way,\nand does not require additional sensing except for a simple monocular camera.\nWe demonstrate its adaptability across various scene types, context sources,\nand sensing setups. We evaluate our approach in two distinct scenarios:\ntraditional First-Person View (FPV) and infrastructure-driven Third-Person View\n(TPV) navigation, demonstrating the flexibility and simplicity of our method.\nOur technique significantly enhances the navigational capabilities of a\nbaseline VLM of approximately 50% in TPV scenario, and is comparable to trained\nmodels in the FPV one, with as few as 20 demonstrations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04006v1",
    "published_date": "2024-11-06 15:44:59 UTC",
    "updated_date": "2024-11-06 15:44:59 UTC"
  },
  {
    "arxiv_id": "2411.03999v1",
    "title": "ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks",
    "authors": [
      "Ziji Shi",
      "Jialin Li",
      "Yang You"
    ],
    "abstract": "Recent advances in Generative Artificial Intelligence have fueled numerous\napplications, particularly those involving Generative Adversarial Networks\n(GANs), which are essential for synthesizing realistic photos and videos.\nHowever, efficiently training GANs remains a critical challenge due to their\ncomputationally intensive and numerically unstable nature. Existing methods\noften require days or even weeks for training, posing significant resource and\ntime constraints.\n  In this work, we introduce ParaGAN, a scalable distributed GAN training\nframework that leverages asynchronous training and an asymmetric optimization\npolicy to accelerate GAN training. ParaGAN employs a congestion-aware data\npipeline and hardware-aware layout transformation to enhance accelerator\nutilization, resulting in over 30% improvements in throughput. With ParaGAN, we\nreduce the training time of BigGAN from 15 days to 14 hours while achieving 91%\nscaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution\nimage generation using BigGAN.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted at ACM Symposium on Cloud Computing (SoCC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03999v1",
    "published_date": "2024-11-06 15:40:46 UTC",
    "updated_date": "2024-11-06 15:40:46 UTC"
  },
  {
    "arxiv_id": "2411.03996v1",
    "title": "Towards Resource-Efficient Federated Learning in Industrial IoT for Multivariate Time Series Analysis",
    "authors": [
      "Alexandros Gkillas",
      "Aris Lalos"
    ],
    "abstract": "Anomaly and missing data constitute a thorny problem in industrial\napplications. In recent years, deep learning enabled anomaly detection has\nemerged as a critical direction, however the improved detection accuracy is\nachieved with the utilization of large neural networks, increasing their\nstorage and computational cost. Moreover, the data collected in edge devices\ncontain user privacy, introducing challenges that can be successfully addressed\nby the privacy-preserving distributed paradigm, known as federated learning\n(FL). This framework allows edge devices to train and exchange models\nincreasing also the communication cost. Thus, to deal with the increased\ncommunication, processing and storage challenges of the FL based deep anomaly\ndetection NN pruning is expected to have significant benefits towards reducing\nthe processing, storage and communication complexity. With this focus, a novel\ncompression-based optimization problem is proposed at the server-side of a FL\nparadigm that fusses the received local models broadcast and performs pruning\ngenerating a more compressed model. Experiments in the context of anomaly\ndetection and missing value imputation demonstrate that the proposed FL\nscenario along with the proposed compressed-based method are able to achieve\nhigh compression rates (more than $99.7\\%$) with negligible performance losses\n(less than $1.18\\%$ ) as compared to the centralized solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03996v1",
    "published_date": "2024-11-06 15:38:31 UTC",
    "updated_date": "2024-11-06 15:38:31 UTC"
  },
  {
    "arxiv_id": "2411.05844v2",
    "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration",
    "authors": [
      "Yukun Cao",
      "Zengyi Gao",
      "Zhiyang Li",
      "Xike Xie",
      "Kevin Zhou",
      "Jianliang Xu"
    ],
    "abstract": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05844v2",
    "published_date": "2024-11-06 15:32:28 UTC",
    "updated_date": "2025-01-17 05:33:54 UTC"
  },
  {
    "arxiv_id": "2411.03964v1",
    "title": "What Really is Commonsense Knowledge?",
    "authors": [
      "Quyet V. Do",
      "Junze Li",
      "Tung-Duong Vuong",
      "Zhaowei Wang",
      "Yangqiu Song",
      "Xiaojuan Ma"
    ],
    "abstract": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and data will be released together with the next version of the\n  paper",
    "pdf_url": "http://arxiv.org/pdf/2411.03964v1",
    "published_date": "2024-11-06 14:54:19 UTC",
    "updated_date": "2024-11-06 14:54:19 UTC"
  },
  {
    "arxiv_id": "2411.03959v1",
    "title": "Energy Score-based Pseudo-Label Filtering and Adaptive Loss for Imbalanced Semi-supervised SAR target recognition",
    "authors": [
      "Xinzheng Zhang",
      "Yuqing Luo",
      "Guopeng Li"
    ],
    "abstract": "Automatic target recognition (ATR) is an important use case for synthetic\naperture radar (SAR) image interpretation. Recent years have seen significant\nadvancements in SAR ATR technology based on semi-supervised learning. However,\nexisting semi-supervised SAR ATR algorithms show low recognition accuracy in\nthe case of class imbalance. This work offers a non-balanced semi-supervised\nSAR target recognition approach using dynamic energy scores and adaptive loss.\nFirst, an energy score-based method is developed to dynamically select\nunlabeled samples near to the training distribution as pseudo-labels during\ntraining, assuring pseudo-label reliability in long-tailed distribution\ncircumstances. Secondly, loss functions suitable for class imbalances are\nproposed, including adaptive margin perception loss and adaptive hard triplet\nloss, the former offsets inter-class confusion of classifiers, alleviating the\nimbalance issue inherent in pseudo-label generation. The latter effectively\ntackles the model's preference for the majority class by focusing on complex\ndifficult samples during training. Experimental results on extremely imbalanced\nSAR datasets demonstrate that the proposed method performs well under the dual\nconstraints of scarce labels and data imbalance, effectively overcoming the\nmodel bias caused by data imbalance and achieving high-precision target\nrecognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03959v1",
    "published_date": "2024-11-06 14:45:16 UTC",
    "updated_date": "2024-11-06 14:45:16 UTC"
  },
  {
    "arxiv_id": "2411.05034v1",
    "title": "Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion",
    "authors": [
      "Tiantian Liu",
      "Hongwei Yao",
      "Tong Wu",
      "Zhan Qin",
      "Feng Lin",
      "Kui Ren",
      "Chun Chen"
    ],
    "abstract": "Embeddings have become a cornerstone in the functionality of large language\nmodels (LLMs) due to their ability to transform text data into rich, dense\nnumerical representations that capture semantic and syntactic properties. These\nembedding vector databases serve as the long-term memory of LLMs, enabling\nefficient handling of a wide range of natural language processing tasks.\nHowever, the surge in popularity of embedding vector databases in LLMs has been\naccompanied by significant concerns about privacy leakage. Embedding vector\ndatabases are particularly vulnerable to embedding inversion attacks, where\nadversaries can exploit the embeddings to reverse-engineer and extract\nsensitive information from the original text data. Existing defense mechanisms\nhave shown limitations, often struggling to balance security with the\nperformance of downstream tasks. To address these challenges, we introduce\nEguard, a novel defense mechanism designed to mitigate embedding inversion\nattacks. Eguard employs a transformer-based projection network and text mutual\ninformation optimization to safeguard embeddings while preserving the utility\nof LLMs. Our approach significantly reduces privacy risks, protecting over 95%\nof tokens from inversion while maintaining high performance across downstream\ntasks consistent with original embeddings.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05034v1",
    "published_date": "2024-11-06 14:42:41 UTC",
    "updated_date": "2024-11-06 14:42:41 UTC"
  },
  {
    "arxiv_id": "2411.03957v1",
    "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation",
    "authors": [
      "Yuhang Liu",
      "Xueyu Hu",
      "Shengyu Zhang",
      "Jingyuan Chen",
      "Fan Wu",
      "Fei Wu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.03957v1",
    "published_date": "2024-11-06 14:42:39 UTC",
    "updated_date": "2024-11-06 14:42:39 UTC"
  },
  {
    "arxiv_id": "2411.03948v2",
    "title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks",
    "authors": [
      "Felipe Marra",
      "Lucas N. Ferreira"
    ],
    "abstract": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "cs.NE",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Paper accepted at the LAMIR 2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.03948v2",
    "published_date": "2024-11-06 14:29:49 UTC",
    "updated_date": "2025-01-22 13:35:26 UTC"
  },
  {
    "arxiv_id": "2411.03945v1",
    "title": "Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks",
    "authors": [
      "Ryan Campbell",
      "Nelson Lojo",
      "Kesava Viswanadha",
      "Christoffer Grondal Tryggestad",
      "Derrick Han Sun",
      "Sriteja Vijapurapu",
      "August Rolfsen",
      "Anant Sahai"
    ],
    "abstract": "In-Context Learning (ICL) is a phenomenon where task learning occurs through\na prompt sequence without the necessity of parameter updates. ICL in\nMulti-Headed Attention (MHA) with absolute positional embedding has been the\nfocus of more study than other sequence model varieties. We examine\nimplications of architectural differences between GPT-2 and LLaMa as well as\nLlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.\n(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining the\ninterplay between sequence transformation blocks and regressive performance\nin-context. We note that certain architectural changes cause degraded training\nefficiency/ICL accuracy by converging to suboptimal predictors or converging\nslower. We also find certain hybrids showing optimistic performance\nimprovements, informing potential future ICL-focused architecture\nmodifications. Additionally, we propose the \"ICL regression score\", a scalar\nmetric describing a model's whole performance on a specific task. Compute\nlimitations impose restrictions on our architecture-space, training duration,\nnumber of training runs, function class complexity, and benchmark complexity.\nTo foster reproducible and extensible research, we provide a typed, modular,\nand extensible Python package on which we run all experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.03945v1",
    "published_date": "2024-11-06 14:25:05 UTC",
    "updated_date": "2024-11-06 14:25:05 UTC"
  },
  {
    "arxiv_id": "2411.03941v1",
    "title": "Fine-tuning -- a Transfer Learning approach",
    "authors": [
      "Joseph Arul Raj",
      "Linglong Qian",
      "Zina Ibrahim"
    ],
    "abstract": "Secondary research use of Electronic Health Records (EHRs) is often hampered\nby the abundance of missing data in this valuable resource. Missingness in EHRs\noccurs naturally as a result of the data recording practices during routine\nclinical care, but handling it is crucial to the precision of medical analysis\nand the decision-making that follows. The literature contains a variety of\nimputation methodologies based on deep neural networks. Those aim to overcome\nthe dynamic, heterogeneous and multivariate missingness patterns of EHRs, which\ncannot be handled by classical and statistical imputation methods. However, all\nexisting deep imputation methods rely on end-to-end pipelines that incorporate\nboth imputation and downstream analyses, e.g. classification. This coupling\nmakes it difficult to assess the quality of imputation and takes away the\nflexibility of re-using the imputer for a different task. Furthermore, most\nend-to-end deep architectures tend to use complex networks to perform the\ndownstream task, in addition to the already sophisticated deep imputation\nnetwork. We, therefore ask if the high performance reported in the literature\nis due to the imputer or the classifier and further ask if an optimised\nstate-of-the-art imputer is used, a simpler classifier can achieve comparable\nperformance. This paper explores the development of a modular, deep\nlearning-based imputation and classification pipeline, specifically built to\nleverage the capabilities of state-of-the-art imputation models for downstream\nclassification tasks. Such a modular approach enables a) objective assessment\nof the quality of the imputer and classifier independently, and b) enables the\nexploration of the performance of simpler classification architectures using an\noptimised imputer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03941v1",
    "published_date": "2024-11-06 14:18:23 UTC",
    "updated_date": "2024-11-06 14:18:23 UTC"
  },
  {
    "arxiv_id": "2411.03934v1",
    "title": "Interactions Across Blocks in Post-Training Quantization of Large Language Models",
    "authors": [
      "Khasmamad Shabanovi",
      "Lukas Wiest",
      "Vladimir Golkov",
      "Daniel Cremers",
      "Thomas Pfeil"
    ],
    "abstract": "Post-training quantization is widely employed to reduce the computational\ndemands of neural networks. Typically, individual substructures, such as layers\nor blocks of layers, are quantized with the objective of minimizing\nquantization errors in their pre-activations by fine-tuning the corresponding\nweights. Deriving this local objective from the global objective of minimizing\ntask loss involves two key simplifications: assuming substructures are mutually\nindependent and ignoring the knowledge of subsequent substructures as well as\nthe task loss. In this work, we assess the effects of these simplifications on\nweight-only quantization of large language models. We introduce two multi-block\nfine-tuning strategies and compare them against the baseline of fine-tuning\nsingle transformer blocks. The first captures correlations of weights across\nblocks by jointly optimizing multiple quantized blocks. The second incorporates\nknowledge of subsequent blocks by minimizing the error in downstream\npre-activations rather than focusing solely on the quantized block. Our\nfindings indicate that the effectiveness of these methods depends on the\nspecific network model, with no impact on some models but demonstrating\nsignificant benefits for others.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03934v1",
    "published_date": "2024-11-06 14:11:39 UTC",
    "updated_date": "2024-11-06 14:11:39 UTC"
  },
  {
    "arxiv_id": "2411.04159v1",
    "title": "Cooperation and Personalization on a Seesaw: Choice-based FL for Safe Cooperation in Wireless Networks",
    "authors": [
      "Han Zhang",
      "Medhat Elsayed",
      "Majid Bavand",
      "Raimundas Gaigalas",
      "Yigit Ozcan",
      "Melike Erol-Kantarci"
    ],
    "abstract": "Federated learning (FL) is an innovative distributed artificial intelligence\n(AI) technique. It has been used for interdisciplinary studies in different\nfields such as healthcare, marketing and finance. However the application of FL\nin wireless networks is still in its infancy. In this work, we first overview\nbenefits and concerns when applying FL to wireless networks. Next, we provide a\nnew perspective on existing personalized FL frameworks by analyzing the\nrelationship between cooperation and personalization in these frameworks.\nAdditionally, we discuss the possibility of tuning the cooperation level with a\nchoice-based approach. Our choice-based FL approach is a flexible and safe FL\nframework that allows participants to lower the level of cooperation when they\nfeel unsafe or unable to benefit from the cooperation. In this way, the\nchoice-based FL framework aims to address the safety and fairness concerns in\nFL and protect participants from malicious attacks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.04159v1",
    "published_date": "2024-11-06 14:09:47 UTC",
    "updated_date": "2024-11-06 14:09:47 UTC"
  },
  {
    "arxiv_id": "2411.03906v2",
    "title": "Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System",
    "authors": [
      "David Maria Schmidt",
      "Mohammad Fazleh Elahi",
      "Philipp Cimiano"
    ],
    "abstract": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
    "pdf_url": "http://arxiv.org/pdf/2411.03906v2",
    "published_date": "2024-11-06 13:37:28 UTC",
    "updated_date": "2024-12-05 12:56:40 UTC"
  },
  {
    "arxiv_id": "2411.03885v1",
    "title": "Disability data futures: Achievable imaginaries for AI and disability data justice",
    "authors": [
      "Denis Newman-Griffis",
      "Bonnielin Swenor",
      "Rupa Valdez",
      "Gillian Mason"
    ],
    "abstract": "Data are the medium through which individuals' identities and experiences are\nfiltered in contemporary states and systems, and AI is increasingly the layer\nmediating between people, data, and decisions. The history of data and AI is\noften one of disability exclusion, oppression, and the reduction of disabled\nexperience; left unchallenged, the current proliferation of AI and data systems\nthus risks further automating ableism behind the veneer of algorithmic\nneutrality. However, exclusionary histories do not preclude inclusive futures,\nand disability-led visions can chart new paths for collective action to achieve\nfutures founded in disability justice. This chapter brings together four\nacademics and disability advocates working at the nexus of disability, data,\nand AI, to describe achievable imaginaries for artificial intelligence and\ndisability data justice. Reflecting diverse contexts, disciplinary\nperspectives, and personal experiences, we draw out the shape, actors, and\ngoals of imagined future systems where data and AI support movement towards\ndisability justice.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03885v1",
    "published_date": "2024-11-06 13:04:29 UTC",
    "updated_date": "2024-11-06 13:04:29 UTC"
  },
  {
    "arxiv_id": "2411.03884v3",
    "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
    "authors": [
      "Zhijian Zhuo",
      "Ya Wang",
      "Yutao Zeng",
      "Xiaoqing Li",
      "Xun Zhou",
      "Jinwen Ma"
    ],
    "abstract": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.03884v3",
    "published_date": "2024-11-06 13:00:34 UTC",
    "updated_date": "2025-03-20 09:46:11 UTC"
  },
  {
    "arxiv_id": "2411.03883v3",
    "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question Answering",
    "authors": [
      "Laura Cabello",
      "Carmen Martin-Turrero",
      "Uchenna Akujuobi",
      "Anders S√∏gaard",
      "Carlos Bobed"
    ],
    "abstract": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context, and unstated relevant domain knowledge.\nDespite the high cost of training, large language models (LLMs) -- the backbone\nof most modern question-answering systems -- still struggle to reliably capture\nthe nuanced relationships between concepts that are crucial for reasoning in\nspecialized fields like medicine. In this work, we present MEG, a\nparameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a\nlightweight mapping network to incorporate knowledge graph embeddings into the\nLLM, enabling it to leverage external knowledge in a cost-effective way. We\nevaluate our method on four popular medical multiple-choice datasets and show\nthat LLMs i) can effectively interpret knowledge graph embeddings and ii) gain\nsignificant advantages from the factual grounding these embeddings provide. MEG\nattains an average of +6.7% and +9.9% accuracy over specialized models like\nBioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's\nperformance remains robust to the choice of graph encoder.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03883v3",
    "published_date": "2024-11-06 12:57:58 UTC",
    "updated_date": "2025-04-22 19:33:54 UTC"
  },
  {
    "arxiv_id": "2411.03866v2",
    "title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward",
    "authors": [
      "Shashi Kumar",
      "Iuliia Thorbecke",
      "Sergio Burdisso",
      "Esa√∫ Villatoro-Tello",
      "Manjunath K E",
      "Kadri Hacioƒülu",
      "Pradeep Rangappa",
      "Petr Motlicek",
      "Aravind Ganapathiraju",
      "Andreas Stolcke"
    ],
    "abstract": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ICASSP 2025 SALMA Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.03866v2",
    "published_date": "2024-11-06 12:22:04 UTC",
    "updated_date": "2025-01-22 09:48:34 UTC"
  },
  {
    "arxiv_id": "2411.03865v5",
    "title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making",
    "authors": [
      "Yizhe Huang",
      "Xingbo Wang",
      "Hao Liu",
      "Fanqi Kong",
      "Aoyang Qin",
      "Min Tang",
      "Song-Chun Zhu",
      "Mingjie Bi",
      "Siyuan Qi",
      "Xue Feng"
    ],
    "abstract": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at NeurIPS D&B 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03865v5",
    "published_date": "2024-11-06 12:19:01 UTC",
    "updated_date": "2025-01-29 12:52:53 UTC"
  },
  {
    "arxiv_id": "2411.03862v2",
    "title": "ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization",
    "authors": [
      "Huayang Huang",
      "Yu Wu",
      "Qian Wang"
    ],
    "abstract": "Watermarking generative content serves as a vital tool for authentication,\nownership protection, and mitigation of potential misuse. Existing watermarking\nmethods face the challenge of balancing robustness and concealment. They\nempirically inject a watermark that is both invisible and robust and passively\nachieve concealment by limiting the strength of the watermark, thus reducing\nthe robustness. In this paper, we propose to explicitly introduce a watermark\nhiding process to actively achieve concealment, thus allowing the embedding of\nstronger watermarks. To be specific, we implant a robust watermark in an\nintermediate diffusion state and then guide the model to hide the watermark in\nthe final generated image. We employ an adversarial optimization algorithm to\nproduce the optimal hiding prompt guiding signal for each watermark. The prompt\nembedding is optimized to minimize artifacts in the generated image, while the\nwatermark is optimized to achieve maximum strength. The watermark can be\nverified by reversing the generation process. Experiments on various diffusion\nmodels demonstrate the watermark remains verifiable even under significant\nimage tampering and shows superior invisibility compared to other\nstate-of-the-art robust watermarking methods. Code is available at\nhttps://github.com/Hannah1102/ROBIN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accept to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03862v2",
    "published_date": "2024-11-06 12:14:23 UTC",
    "updated_date": "2025-04-03 03:11:31 UTC"
  },
  {
    "arxiv_id": "2411.03859v2",
    "title": "UniTraj: Learning a Universal Trajectory Foundation Model from Billion-Scale Worldwide Traces",
    "authors": [
      "Yuanshao Zhu",
      "James Jianqiao Yu",
      "Xiangyu Zhao",
      "Xuetao Wei",
      "Yuxuan Liang"
    ],
    "abstract": "Human trajectory modeling is essential for deciphering movement patterns and\nsupporting advanced applications across various domains. However, existing\nmethods are often tailored to specific tasks and regions, resulting in\nlimitations related to task specificity, regional dependency, and data quality\nsensitivity. Addressing these challenges requires a universal human trajectory\nfoundation model capable of generalizing and scaling across diverse tasks and\ngeographic contexts. To this end, we propose UniTraj, a Universal human\nTrajectory foundation model that is task-adaptive, region-independent, and\nhighly generalizable. To further enhance performance, we construct WorldTrace,\nthe first large-scale, high-quality, globally distributed dataset sourced from\nopen web platforms, encompassing 2.45 million trajectories with billions of\npoints across 70 countries. Through multiple resampling and masking strategies\ndesigned for pre-training, UniTraj effectively overcomes geographic and task\nconstraints, adapting to heterogeneous data quality. Extensive experiments\nacross multiple trajectory analysis tasks and real-world datasets demonstrate\nthat UniTraj consistently outperforms existing approaches in terms of\nscalability and adaptability. These results underscore the potential of UniTraj\nas a versatile, robust solution for a wide range of trajectory analysis\napplications, with WorldTrace serving as an ideal but non-exclusive foundation\nfor training.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG",
      "cs.SI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03859v2",
    "published_date": "2024-11-06 12:06:43 UTC",
    "updated_date": "2024-11-16 06:53:43 UTC"
  },
  {
    "arxiv_id": "2411.03855v3",
    "title": "MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba",
    "authors": [
      "Masakazu Yoshimura",
      "Teruaki Hayashi",
      "Yota Maeda"
    ],
    "abstract": "An ecosystem of Transformer-based models has been established by building\nlarge models with extensive data. Parameter-efficient fine-tuning (PEFT) is a\ncrucial technology for deploying these models to downstream tasks with minimal\ncost while achieving effective performance. Recently, Mamba, a State Space\nModel (SSM)-based model, has attracted attention as a potential alternative to\nTransformers. While many large-scale Mamba-based models have been proposed,\nefficiently adapting pre-trained Mamba-based models to downstream tasks remains\nunexplored. In this paper, we conduct an exploratory analysis of PEFT methods\nfor Mamba. We investigate the effectiveness of existing PEFT methods for\nTransformers when applied to Mamba. We also modify these methods to better\nalign with the Mamba architecture. Additionally, we propose new Mamba-specific\nPEFT methods that leverage the distinctive structure of Mamba. Our experiments\nindicate that PEFT performs more effectively for Mamba than Transformers.\nLastly, we demonstrate how to effectively combine multiple PEFT methods and\nprovide a framework that outperforms previous works. To ensure reproducibility,\nwe will release the code after publication.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2411.03855v3",
    "published_date": "2024-11-06 11:57:55 UTC",
    "updated_date": "2025-04-01 08:41:31 UTC"
  },
  {
    "arxiv_id": "2411.03847v1",
    "title": "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge Computing",
    "authors": [
      "Peihao Li"
    ],
    "abstract": "With the widespread adoption of edge computing technologies and the\nincreasing prevalence of deep learning models in these environments, the\nsecurity risks and privacy threats to models and data have grown more acute.\nAttackers can exploit various techniques to illegally obtain models or misuse\ndata, leading to serious issues such as intellectual property infringement and\nprivacy breaches. Existing model access control technologies primarily rely on\ntraditional encryption and authentication methods; however, these approaches\nexhibit significant limitations in terms of flexibility and adaptability in\ndynamic environments. Although there have been advancements in model\nwatermarking techniques for marking model ownership, they remain limited in\ntheir ability to proactively protect intellectual property and prevent\nunauthorized access. To address these challenges, we propose a novel model\naccess control method tailored for edge computing environments. This method\nleverages image style as a licensing mechanism, embedding style recognition\ninto the model's operational framework to enable intrinsic access control.\nConsequently, models deployed on edge platforms are designed to correctly infer\nonly on license data with specific style, rendering them ineffective on any\nother data. By restricting the input data to the edge model, this approach not\nonly prevents attackers from gaining unauthorized access to the model but also\nenhances the privacy of data on terminal devices. We conducted extensive\nexperiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,\nand the results demonstrate that our method effectively prevents unauthorized\naccess to the model while maintaining accuracy. Additionally, the model shows\nstrong resistance against attacks such as forged licenses and fine-tuning.\nThese results underscore the method's usability, security, and robustness.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03847v1",
    "published_date": "2024-11-06 11:37:30 UTC",
    "updated_date": "2024-11-06 11:37:30 UTC"
  },
  {
    "arxiv_id": "2411.03845v2",
    "title": "Reconsidering the Performance of GAE in Link Prediction",
    "authors": [
      "Weishuo Ma",
      "Yanbo Wang",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "abstract": "Various graph neural networks (GNNs) with advanced training techniques and\nmodel designs have been proposed for link prediction tasks. However, outdated\nbaseline models may lead to an overestimation of the benefits provided by these\nnovel approaches. To address this, we systematically investigate the potential\nof Graph Autoencoders (GAE) by meticulously tuning hyperparameters and\nutilizing the trick of orthogonal embedding and linear propagation. Our\nfindings reveal that a well-optimized GAE can match the performance of more\ncomplex models while offering greater computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03845v2",
    "published_date": "2024-11-06 11:29:47 UTC",
    "updated_date": "2025-02-24 06:04:27 UTC"
  },
  {
    "arxiv_id": "2411.03823v2",
    "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
    "authors": [
      "Dingjie Song",
      "Sicheng Lai",
      "Shunian Chen",
      "Lichao Sun",
      "Benyou Wang"
    ],
    "abstract": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect",
    "pdf_url": "http://arxiv.org/pdf/2411.03823v2",
    "published_date": "2024-11-06 10:44:15 UTC",
    "updated_date": "2025-02-17 18:29:13 UTC"
  },
  {
    "arxiv_id": "2411.03820v1",
    "title": "Beyond The Rainbow: High Performance Deep Reinforcement Learning On A Desktop PC",
    "authors": [
      "Tyler Clark",
      "Mark Towers",
      "Christine Evers",
      "Jonathon Hare"
    ],
    "abstract": "Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent\nenhancements could significantly boost a reinforcement learning (RL) agent's\nperformance. In this paper, we present \"Beyond The Rainbow\" (BTR), a novel\nalgorithm that integrates six improvements from across the RL literature to\nRainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,\nwith a human-normalized interquartile mean (IQM) of 7.4 on atari-60. Beyond\nAtari, we demonstrate BTR's capability to handle complex 3D games, successfully\ntraining agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with\nminimal algorithmic changes. Designing BTR with computational efficiency in\nmind, agents can be trained using a desktop PC on 200 million Atari frames\nwithin 12 hours. Additionally, we conduct detailed ablation studies of each\ncomponent, analzying the performance and impact using numerous measures.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 main pages, 26 total. Currently under review at ICLR",
    "pdf_url": "http://arxiv.org/pdf/2411.03820v1",
    "published_date": "2024-11-06 10:42:04 UTC",
    "updated_date": "2024-11-06 10:42:04 UTC"
  },
  {
    "arxiv_id": "2411.03817v3",
    "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning",
    "authors": [
      "Zhirui Deng",
      "Zhicheng Dou",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Ruibin Xiong",
      "Mang Wang",
      "Weipeng Chen"
    ],
    "abstract": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03817v3",
    "published_date": "2024-11-06 10:35:11 UTC",
    "updated_date": "2024-12-09 09:20:11 UTC"
  },
  {
    "arxiv_id": "2411.03814v2",
    "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
    "authors": [
      "Fengxiang Wang",
      "Ranjie Duan",
      "Peng Xiao",
      "Xiaojun Jia",
      "Shiji Zhao",
      "Cheng Wei",
      "YueFeng Chen",
      "Chongwen Wang",
      "Jialing Tao",
      "Hang Su",
      "Jun Zhu",
      "Hui Xue"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03814v2",
    "published_date": "2024-11-06 10:32:09 UTC",
    "updated_date": "2025-01-07 07:46:16 UTC"
  },
  {
    "arxiv_id": "2411.04156v1",
    "title": "Crystal: Illuminating LLM Abilities on Language and Code",
    "authors": [
      "Tianhua Tao",
      "Junbo Li",
      "Bowen Tan",
      "Hongyi Wang",
      "William Marshall",
      "Bhargav M Kanakiya",
      "Joel Hestness",
      "Natalia Vassilieva",
      "Zhiqiang Shen",
      "Eric P. Xing",
      "Zhengzhong Liu"
    ],
    "abstract": "Large Language Models (LLMs) specializing in code generation (which are also\noften referred to as code LLMs), e.g., StarCoder and Code Llama, play\nincreasingly critical roles in various software development scenarios. It is\nalso crucial for code LLMs to possess both code generation and natural language\nabilities for many specific applications, such as code snippet retrieval using\nnatural language or code explanations. The intricate interaction between\nacquiring language and coding skills complicates the development of strong code\nLLMs. Furthermore, there is a lack of thorough prior studies on the LLM\npretraining strategy that mixes code and natural language. In this work, we\npropose a pretraining strategy to enhance the integration of natural language\nand coding capabilities within a single LLM. Specifically, it includes two\nphases of training with appropriately adjusted code/language ratios. The\nresulting model, Crystal, demonstrates remarkable capabilities in both domains.\nSpecifically, it has natural language and coding performance comparable to that\nof Llama 2 and Code Llama, respectively. Crystal exhibits better data\nefficiency, using 1.4 trillion tokens compared to the more than 2 trillion\ntokens used by Llama 2 and Code Llama. We verify our pretraining strategy by\nanalyzing the training process and observe consistent improvements in most\nbenchmarks. We also adopted a typical application adaptation phase with a\ncode-centric data mixture, only to find that it did not lead to enhanced\nperformance or training efficiency, underlining the importance of a carefully\ndesigned data recipe. To foster research within the community, we commit to\nopen-sourcing every detail of the pretraining, including our training datasets,\ncode, loggings and 136 checkpoints throughout the training.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Published as a conference paper at COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.04156v1",
    "published_date": "2024-11-06 10:28:46 UTC",
    "updated_date": "2024-11-06 10:28:46 UTC"
  },
  {
    "arxiv_id": "2411.03807v3",
    "title": "GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian Splatting",
    "authors": [
      "Jilan Mei",
      "Junbo Li",
      "Cai Meng"
    ],
    "abstract": "This paper proposes a new method for accurate and robust 6D pose estimation\nof novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose\ncan utilize the reconstruction results without requiring a high-quality CAD\nmodel, which means it only requires segmented RGBD images as input.\nSpecifically, GS2Pose employs a two-stage structure consisting of coarse\nestimation followed by refined estimation. In the coarse stage, a lightweight\nU-Net network with a polarization attention mechanism, called Pose-Net, is\ndesigned. By using the 3DGS model for supervised training, Pose-Net can\ngenerate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose\nformulates a pose regression algorithm following the idea of reprojection or\nBundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to\nextend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that\nrefines the coarse pose by comparing the input images with the rendered images.\nGS-Refiner also selectively updates parameters in the 3DGS model to achieve\nenvironmental adaptation, thereby enhancing the algorithm's robustness and\nflexibility to illuminative variation, occlusion, and other challenging\ndisruptive factors. GS2Pose was evaluated through experiments conducted on the\nLineMod dataset, where it was compared with similar algorithms, yielding highly\ncompetitive results. The code for GS2Pose will soon be released on GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03807v3",
    "published_date": "2024-11-06 10:07:46 UTC",
    "updated_date": "2024-11-08 03:02:02 UTC"
  },
  {
    "arxiv_id": "2411.03799v1",
    "title": "Overcoming label shift in targeted federated learning",
    "authors": [
      "Edvin Listo Zec",
      "Adam Breitholtz",
      "Fredrik D. Johansson"
    ],
    "abstract": "Federated learning enables multiple actors to collaboratively train models\nwithout sharing private data. This unlocks the potential for scaling machine\nlearning to diverse applications. Existing algorithms for this task are\nwell-justified when clients and the intended target domain share the same\ndistribution of features and labels, but this assumption is often violated in\nreal-world scenarios. One common violation is label shift, where the label\ndistributions differ across clients or between clients and the target domain,\nwhich can significantly degrade model performance. To address this problem, we\npropose FedPALS, a novel model aggregation scheme that adapts to label shifts\nby leveraging knowledge of the target label distribution at the central server.\nOur approach ensures unbiased updates under stochastic gradient descent,\nensuring robust generalization across clients with diverse, label-shifted data.\nExtensive experiments on image classification demonstrate that FedPALS\nconsistently outperforms standard baselines by aligning model aggregation with\nthe target domain. Our findings reveal that conventional federated learning\nmethods suffer severely in cases of extreme client sparsity, highlighting the\ncritical need for target-aware aggregation. FedPALS offers a principled and\npractical solution to mitigate label distribution mismatch, ensuring models\ntrained in federated settings can generalize effectively to label-shifted\ntarget domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03799v1",
    "published_date": "2024-11-06 09:52:45 UTC",
    "updated_date": "2024-11-06 09:52:45 UTC"
  },
  {
    "arxiv_id": "2411.03795v4",
    "title": "VQA$^2$: Visual Question Answering for Video Quality Assessment",
    "authors": [
      "Ziheng Jia",
      "Zicheng Zhang",
      "Jiaying Qian",
      "Haoning Wu",
      "Wei Sun",
      "Chunyi Li",
      "Xiaohong Liu",
      "Weisi Lin",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.03795v4",
    "published_date": "2024-11-06 09:39:52 UTC",
    "updated_date": "2024-12-02 12:09:39 UTC"
  },
  {
    "arxiv_id": "2411.03782v1",
    "title": "Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications",
    "authors": [
      "Daan Schouten",
      "Giulia Nicoletti",
      "Bas Dille",
      "Catherine Chia",
      "Pierpaolo Vendittelli",
      "Megan Schuurmans",
      "Geert Litjens",
      "Nadieh Khalili"
    ],
    "abstract": "Recent technological advances in healthcare have led to unprecedented growth\nin patient data quantity and diversity. While artificial intelligence (AI)\nmodels have shown promising results in analyzing individual data modalities,\nthere is increasing recognition that models integrating multiple complementary\ndata sources, so-called multimodal AI, could enhance clinical decision-making.\nThis scoping review examines the landscape of deep learning-based multimodal AI\napplications across the medical domain, analyzing 432 papers published between\n2018 and 2024. We provide an extensive overview of multimodal AI development\nacross different medical disciplines, examining various architectural\napproaches, fusion strategies, and common application areas. Our analysis\nreveals that multimodal AI models consistently outperform their unimodal\ncounterparts, with an average improvement of 6.2 percentage points in AUC.\nHowever, several challenges persist, including cross-departmental coordination,\nheterogeneous data characteristics, and incomplete datasets. We critically\nassess the technical and practical challenges in developing multimodal AI\nsystems and discuss potential strategies for their clinical implementation,\nincluding a brief overview of commercially available multimodal AI models for\nclinical decision-making. Additionally, we identify key factors driving\nmultimodal AI development and propose recommendations to accelerate the field's\nmaturation. This review provides researchers and clinicians with a thorough\nunderstanding of the current state, challenges, and future directions of\nmultimodal AI in medicine.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.03782v1",
    "published_date": "2024-11-06 09:18:05 UTC",
    "updated_date": "2024-11-06 09:18:05 UTC"
  },
  {
    "arxiv_id": "2411.15144v3",
    "title": "Physically Parameterized Differentiable MUSIC for DoA Estimation with Uncalibrated Arrays",
    "authors": [
      "Baptiste Chatelier",
      "Jos√© Miguel Mateos-Ramos",
      "Vincent Corlay",
      "Christian H√§ger",
      "Matthieu Crussi√®re",
      "Henk Wymeersch",
      "Luc Le Magoarou"
    ],
    "abstract": "Direction of arrival (DoA) estimation is a common sensing problem in radar,\nsonar, audio, and wireless communication systems. It has gained renewed\nimportance with the advent of the integrated sensing and communication\nparadigm. To fully exploit the potential of such sensing systems, it is crucial\nto take into account potential hardware impairments that can negatively impact\nthe obtained performance. This study introduces a joint DoA estimation and\nhardware impairment learning scheme following a model-based approach.\nSpecifically, a differentiable version of the multiple signal classification\n(MUSIC) algorithm is derived, allowing efficient learning of the considered\nimpairments. The proposed approach supports both supervised and unsupervised\nlearning strategies, showcasing its practical potential. Simulation results\nindicate that the proposed method successfully learns significant inaccuracies\nin both antenna locations and complex gains. Additionally, the proposed method\noutperforms the classical MUSIC algorithm in the DoA estimation task.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15144v3",
    "published_date": "2024-11-06 09:14:26 UTC",
    "updated_date": "2025-03-20 09:20:09 UTC"
  },
  {
    "arxiv_id": "2411.03769v1",
    "title": "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages",
    "authors": [
      "Youssef Mohamed",
      "Runjia Li",
      "Ibrahim Said Ahmad",
      "Kilichbek Haydarov",
      "Philip Torr",
      "Kenneth Ward Church",
      "Mohamed Elhoseiny"
    ],
    "abstract": "Research in vision and language has made considerable progress thanks to\nbenchmarks such as COCO. COCO captions focused on unambiguous facts in English;\nArtEmis introduced subjective emotions and ArtELingo introduced some\nmultilinguality (Chinese and Arabic). However we believe there should be more\nmultilinguality. Hence, we present ArtELingo-28, a vision-language benchmark\nthat spans $\\textbf{28}$ languages and encompasses approximately\n$\\textbf{200,000}$ annotations ($\\textbf{140}$ annotations per image).\nTraditionally, vision research focused on unambiguous class labels, whereas\nArtELingo-28 emphasizes diversity of opinions over languages and cultures. The\nchallenge is to build machine learning systems that assign emotional captions\nto images. Baseline results will be presented for three novel conditions:\nZero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual\ntransfer is more successful for culturally-related languages. Data and code are\nprovided at www.artelingo.org.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, Accepted at EMNLP 24, for more details see www.artelingo.org",
    "pdf_url": "http://arxiv.org/pdf/2411.03769v1",
    "published_date": "2024-11-06 09:05:17 UTC",
    "updated_date": "2024-11-06 09:05:17 UTC"
  },
  {
    "arxiv_id": "2411.03766v3",
    "title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
    "authors": [
      "Haotong Yang",
      "Yi Hu",
      "Shijia Kang",
      "Zhouchen Lin",
      "Muhan Zhang"
    ],
    "abstract": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 poster",
    "pdf_url": "http://arxiv.org/pdf/2411.03766v3",
    "published_date": "2024-11-06 08:59:44 UTC",
    "updated_date": "2025-03-05 09:52:30 UTC"
  },
  {
    "arxiv_id": "2411.03758v1",
    "title": "Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI Reconstruction",
    "authors": [
      "Yu Guan",
      "Qinrong Cai",
      "Wei Li",
      "Qiuyun Fan",
      "Dong Liang",
      "Qiegen Liu"
    ],
    "abstract": "Diffusion model-based approaches recently achieved re-markable success in MRI\nreconstruction, but integration into clinical routine remains challenging due\nto its time-consuming convergence. This phenomenon is partic-ularly notable\nwhen directly apply conventional diffusion process to k-space data without\nconsidering the inherent properties of k-space sampling, limiting k-space\nlearning efficiency and image reconstruction quality. To tackle these\nchallenges, we introduce subspace diffusion model with orthogonal\ndecomposition, a method (referred to as Sub-DM) that restrict the diffusion\nprocess via projections onto subspace as the k-space data distribution evolves\ntoward noise. Particularly, the subspace diffusion model circumvents the\ninference challenges posed by the com-plex and high-dimensional characteristics\nof k-space data, so the highly compact subspace ensures that diffusion process\nrequires only a few simple iterations to produce accurate prior information.\nFurthermore, the orthogonal decomposition strategy based on wavelet transform\nhin-ders the information loss during the migration of the vanilla diffusion\nprocess to the subspace. Considering the strate-gy is approximately reversible,\nsuch that the entire pro-cess can be reversed. As a result, it allows the\ndiffusion processes in different spaces to refine models through a mutual\nfeedback mechanism, enabling the learning of ac-curate prior even when dealing\nwith complex k-space data. Comprehensive experiments on different datasets\nclearly demonstrate that the superiority of Sub-DM against state of-the-art\nmethods in terms of reconstruction speed and quality.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.03758v1",
    "published_date": "2024-11-06 08:33:07 UTC",
    "updated_date": "2024-11-06 08:33:07 UTC"
  },
  {
    "arxiv_id": "2411.03755v3",
    "title": "Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions",
    "authors": [
      "Sagar Shrestha",
      "Xiao Fu"
    ],
    "abstract": "Understanding identifiability of latent content and style variables from\nunaligned multi-domain data is essential for tasks such as domain translation\nand data generation. Existing works on content-style identification were often\ndeveloped under somewhat stringent conditions, e.g., that all latent components\nare mutually independent and that the dimensions of the content and style\nvariables are known. We introduce a new analytical framework via cross-domain\n\\textit{latent distribution matching} (LDM), which establishes content-style\nidentifiability under substantially more relaxed conditions. Specifically, we\nshow that restrictive assumptions such as component-wise independence of the\nlatent variables can be removed. Most notably, we prove that prior knowledge of\nthe content and style dimensions is not necessary for ensuring identifiability,\nif sparsity constraints are properly imposed onto the learned latent\nrepresentations. Bypassing the knowledge of the exact latent dimension has been\na longstanding aspiration in unsupervised representation learning -- our\nanalysis is the first to underpin its theoretical and practical viability. On\nthe implementation side, we recast the LDM formulation into a regularized\nmulti-domain GAN loss with coupled latent variables. We show that the\nreformulation is equivalent to LDM under mild conditions -- yet requiring\nconsiderably less computational resource. Experiments corroborate with our\ntheoretical claims.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03755v3",
    "published_date": "2024-11-06 08:30:23 UTC",
    "updated_date": "2025-03-01 23:02:28 UTC"
  },
  {
    "arxiv_id": "2411.03746v1",
    "title": "Optimal Defenses Against Gradient Reconstruction Attacks",
    "authors": [
      "Yuxiao Chen",
      "Gamze G√ºrsoy",
      "Qi Lei"
    ],
    "abstract": "Federated Learning (FL) is designed to prevent data leakage through\ncollaborative model training without centralized data storage. However, it\nremains vulnerable to gradient reconstruction attacks that recover original\ntraining data from shared gradients. To optimize the trade-off between data\nleakage and utility loss, we first derive a theoretical lower bound of\nreconstruction error (among all attackers) for the two standard methods: adding\nnoise, and gradient pruning. We then customize these two defenses to be\nparameter- and model-specific and achieve the optimal trade-off between our\nobtained reconstruction lower bound and model utility. Experimental results\nvalidate that our methods outperform Gradient Noise and Gradient Pruning by\nprotecting the training data better while also achieving better utility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The code for this project is available at\n  https://github.com/cyx78/Optimal_Defenses_Against_Gradient_Reconstruction_Attacks",
    "pdf_url": "http://arxiv.org/pdf/2411.03746v1",
    "published_date": "2024-11-06 08:22:20 UTC",
    "updated_date": "2024-11-06 08:22:20 UTC"
  },
  {
    "arxiv_id": "2411.03743v1",
    "title": "Automating Exploratory Proteomics Research via Language Models",
    "authors": [
      "Ning Ding",
      "Shang Qu",
      "Linhai Xie",
      "Yifei Li",
      "Zaoqu Liu",
      "Kaiyan Zhang",
      "Yibai Xiong",
      "Yuxin Zuo",
      "Zhangren Chen",
      "Ermo Hua",
      "Xingtai Lv",
      "Youbang Sun",
      "Yang Li",
      "Dong Li",
      "Fuchu He",
      "Bowen Zhou"
    ],
    "abstract": "With the development of artificial intelligence, its contribution to science\nis evolving from simulating a complex problem to automating entire research\nprocesses and producing novel discoveries. Achieving this advancement requires\nboth specialized general models grounded in real-world scientific data and\niterative, exploratory frameworks that mirror human scientific methodologies.\nIn this paper, we present PROTEUS, a fully automated system for scientific\ndiscovery from raw proteomics data. PROTEUS uses large language models (LLMs)\nto perform hierarchical planning, execute specialized bioinformatics tools, and\niteratively refine analysis workflows to generate high-quality scientific\nhypotheses. The system takes proteomics datasets as input and produces a\ncomprehensive set of research objectives, analysis results, and novel\nbiological hypotheses without human intervention. We evaluated PROTEUS on 12\nproteomics datasets collected from various biological samples (e.g. immune\ncells, tumors) and different sample types (single-cell and bulk), generating\n191 scientific hypotheses. These were assessed using both automatic LLM-based\nscoring on 5 metrics and detailed reviews from human experts. Results\ndemonstrate that PROTEUS consistently produces reliable, logically coherent\nresults that align well with existing literature while also proposing novel,\nevaluable hypotheses. The system's flexible architecture facilitates seamless\nintegration of diverse analysis tools and adaptation to different proteomics\ndata types. By automating complex proteomics analysis workflows and hypothesis\ngeneration, PROTEUS has the potential to considerably accelerate the pace of\nscientific discovery in proteomics research, enabling researchers to\nefficiently explore large-scale datasets and uncover biological insights.",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03743v1",
    "published_date": "2024-11-06 08:16:56 UTC",
    "updated_date": "2024-11-06 08:16:56 UTC"
  },
  {
    "arxiv_id": "2411.03742v1",
    "title": "Adaptive Consensus Gradients Aggregation for Scaled Distributed Training",
    "authors": [
      "Yoni Choukroun",
      "Shlomi Azoulay",
      "Pavel Kisilev"
    ],
    "abstract": "Distributed machine learning has recently become a critical paradigm for\ntraining large models on vast datasets. We examine the stochastic optimization\nproblem for deep learning within synchronous parallel computing environments\nunder communication constraints. While averaging distributed gradients is the\nmost widely used method for gradient estimation, whether this is the optimal\nstrategy remains an open question. In this work, we analyze the distributed\ngradient aggregation process through the lens of subspace optimization. By\nformulating the aggregation problem as an objective-aware subspace optimization\nproblem, we derive an efficient weighting scheme for gradients, guided by\nsubspace coefficients. We further introduce subspace momentum to accelerate\nconvergence while maintaining statistical unbiasedness in the aggregation. Our\nmethod demonstrates improved performance over the ubiquitous gradient averaging\non multiple MLPerf tasks while remaining extremely efficient in both\ncommunicational and computational complexity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03742v1",
    "published_date": "2024-11-06 08:16:39 UTC",
    "updated_date": "2024-11-06 08:16:39 UTC"
  },
  {
    "arxiv_id": "2411.04151v1",
    "title": "UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction",
    "authors": [
      "Kehua Qu",
      "Rui Ding",
      "Jin Tang"
    ],
    "abstract": "Multi-person motion prediction is a complex and emerging field with\nsignificant real-world applications. Current state-of-the-art methods typically\nadopt dual-path networks to separately modeling spatial features and temporal\nfeatures. However, the uncertain compatibility of the two networks brings a\nchallenge for spatio-temporal features fusion and violate the spatio-temporal\ncoherence and coupling of human motions by nature. To address this issue, we\npropose a novel graph structure, UnityGraph, which treats spatio-temporal\nfeatures as a whole, enhancing model coherence and coupling.spatio-temporal\nfeatures as a whole, enhancing model coherence and coupling. Specifically,\nUnityGraph is a hypervariate graph based network. The flexibility of the\nhypergraph allows us to consider the observed motions as graph nodes. We then\nleverage hyperedges to bridge these nodes for exploring spatio-temporal\nfeatures. This perspective considers spatio-temporal dynamics unitedly and\nreformulates multi-person motion prediction into a problem on a single graph.\nLeveraging the dynamic message passing based on this hypergraph, our model\ndynamically learns from both types of relations to generate targeted messages\nthat reflect the relevance among nodes. Extensive experiments on several\ndatasets demonstrates that our method achieves state-of-the-art performance,\nconfirming its effectiveness and innovative design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2411.03729",
    "pdf_url": "http://arxiv.org/pdf/2411.04151v1",
    "published_date": "2024-11-06 08:05:36 UTC",
    "updated_date": "2024-11-06 08:05:36 UTC"
  },
  {
    "arxiv_id": "2411.03729v1",
    "title": "Relation Learning and Aggregate-attention for Multi-person Motion Prediction",
    "authors": [
      "Kehua Qu",
      "Rui Ding",
      "Jin Tang"
    ],
    "abstract": "Multi-person motion prediction is an emerging and intricate task with broad\nreal-world applications. Unlike single person motion prediction, it considers\nnot just the skeleton structures or human trajectories but also the\ninteractions between others. Previous methods use various networks to achieve\nimpressive predictions but often overlook that the joints relations within an\nindividual (intra-relation) and interactions among groups (inter-relation) are\ndistinct types of representations. These methods often lack explicit\nrepresentation of inter&intra-relations, and inevitably introduce undesired\ndependencies. To address this issue, we introduce a new collaborative framework\nfor multi-person motion prediction that explicitly modeling these relations:a\nGCN-based network for intra-relations and a novel reasoning network for\ninter-relations.Moreover, we propose a novel plug-and-play aggregation module\ncalled the Interaction Aggregation Module (IAM), which employs an\naggregate-attention mechanism to seamlessly integrate these relations.\nExperiments indicate that the module can also be applied to other dual-path\nmodels. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as\nwell as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that\nour method achieves state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to IEEE Transactions on Multimedia",
    "pdf_url": "http://arxiv.org/pdf/2411.03729v1",
    "published_date": "2024-11-06 07:48:30 UTC",
    "updated_date": "2024-11-06 07:48:30 UTC"
  },
  {
    "arxiv_id": "2411.03726v1",
    "title": "PropNEAT -- Efficient GPU-Compatible Backpropagation over NeuroEvolutionary Augmenting Topology Networks",
    "authors": [
      "Michael Merry",
      "Patricia Riddle",
      "Jim Warren"
    ],
    "abstract": "We introduce PropNEAT, a fast backpropagation implementation of NEAT that\nuses a bidirectional mapping of the genome graph to a layer-based architecture\nthat preserves the NEAT genomes whilst enabling efficient GPU backpropagation.\nWe test PropNEAT on 58 binary classification datasets from the Penn Machine\nLearning Benchmarks database, comparing the performance against logistic\nregression, dense neural networks and random forests, as well as a densely\nretrained variant of the final PropNEAT model. PropNEAT had the second best\noverall performance, behind Random Forest, though the difference between the\nmodels was not statistically significant apart from between Random Forest in\ncomparison with logistic regression and the PropNEAT retrain models. PropNEAT\nwas substantially faster than a naive backpropagation method, and both were\nsubstantially faster and had better performance than the original NEAT\nimplementation. We demonstrate that the per-epoch training time for PropNEAT\nscales linearly with network depth, and is efficient on GPU implementations for\nbackpropagation. This implementation could be extended to support reinforcement\nlearning or convolutional networks, and is able to find sparser and smaller\nnetworks with potential for applications in low-power contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03726v1",
    "published_date": "2024-11-06 07:44:14 UTC",
    "updated_date": "2024-11-06 07:44:14 UTC"
  },
  {
    "arxiv_id": "2411.03709v1",
    "title": "AutoGameUI: Constructing High-Fidelity Game UIs via Multimodal Learning and Interactive Web-Based Tool",
    "authors": [
      "Zhongliang Tang",
      "Mengchen Tan",
      "Fei Xia",
      "Qingrong Cheng",
      "Hao Jiang",
      "Yongxiang Zhang"
    ],
    "abstract": "We introduce an innovative system, AutoGameUI, for efficiently constructing\ncohesive user interfaces in game development. Our system is the first to\naddress the coherence issue arising from integrating inconsistent UI and UX\ndesigns, typically leading to mismatches and inefficiencies. We propose a\ntwo-stage multimodal learning pipeline to obtain comprehensive representations\nof both UI and UX designs, and to establish their correspondences. Through the\ncorrespondences, a cohesive user interface is automatically constructed from\npairwise designs. To achieve high-fidelity effects, we introduce a universal\ndata protocol for precise design descriptions and cross-platform applications.\nWe also develop an interactive web-based tool for game developers to facilitate\nthe use of our system. We create a game UI dataset from actual game projects\nand combine it with a public dataset for training and evaluation. Our\nexperimental results demonstrate the effectiveness of our system in maintaining\ncoherence between the constructed interfaces and the original designs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.03709v1",
    "published_date": "2024-11-06 07:16:54 UTC",
    "updated_date": "2024-11-06 07:16:54 UTC"
  },
  {
    "arxiv_id": "2411.03707v1",
    "title": "Fine-Tuning Vision-Language Model for Automated Engineering Drawing Information Extraction",
    "authors": [
      "Muhammad Tayyab Khan",
      "Lequn Chen",
      "Ye Han Ng",
      "Wenhe Feng",
      "Nicholas Yew Jin Tan",
      "Seung Ki Moon"
    ],
    "abstract": "Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in\nmanufacturing by defining acceptable variations in part features to ensure\ncomponent quality and functionality. However, extracting GD&T information from\n2D engineering drawings is a time-consuming and labor-intensive task, often\nrelying on manual efforts or semi-automated tools. To address these challenges,\nthis study proposes an automated and computationally efficient GD&T extraction\nmethod by fine-tuning Florence-2, an open-source vision-language model (VLM).\nThe model is trained on a dataset of 400 drawings with ground truth annotations\nprovided by domain experts. For comparison, two state-of-the-art closed-source\nVLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All\nmodels are assessed using precision, recall, F1-score, and hallucination\nmetrics. Due to the computational cost and impracticality of fine-tuning large\nclosed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are\nevaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with\n0.23 billion parameters, is optimized through full-parameter fine-tuning across\nthree distinct experiments, each utilizing datasets augmented to different\nlevels. The results show that Florence-2 achieves a 29.95% increase in\nprecision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a\n43.15% reduction in hallucination rate compared to the best-performing\nclosed-source model. These findings highlight the effectiveness of fine-tuning\nsmaller, open-source VLMs like Florence-2, offering a practical and efficient\nsolution for automated GD&T extraction to support downstream manufacturing\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper has been submitted to the 9th International Conference on\n  Innovation in Artificial Intelligence (ICIAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2411.03707v1",
    "published_date": "2024-11-06 07:11:15 UTC",
    "updated_date": "2024-11-06 07:11:15 UTC"
  },
  {
    "arxiv_id": "2411.05029v1",
    "title": "Ultrasound-Based AI for COVID-19 Detection: A Comprehensive Review of Public and Private Lung Ultrasound Datasets and Studies",
    "authors": [
      "Abrar Morshed",
      "Abdulla Al Shihab",
      "Md Abrar Jahin",
      "Md Jaber Al Nahian",
      "Md Murad Hossain Sarker",
      "Md Sharjis Ibne Wadud",
      "Mohammad Istiaq Uddin",
      "Muntequa Imtiaz Siraji",
      "Nafisa Anjum",
      "Sumiya Rajjab Shristy",
      "Tanvin Rahman",
      "Mahmuda Khatun",
      "Md Rubel Dewan",
      "Mosaddeq Hossain",
      "Razia Sultana",
      "Ripel Chakma",
      "Sonet Barua Emon",
      "Towhidul Islam",
      "Mohammad Arafat Hussain"
    ],
    "abstract": "The COVID-19 pandemic has affected millions of people globally, with\nrespiratory organs being strongly affected in individuals with comorbidities.\nMedical imaging-based diagnosis and prognosis have become increasingly popular\nin clinical settings for detecting COVID-19 lung infections. Among various\nmedical imaging modalities, ultrasound stands out as a low-cost, mobile, and\nradiation-safe imaging technology. In this comprehensive review, we focus on\nAI-driven studies utilizing lung ultrasound (LUS) for COVID-19 detection and\nanalysis. We provide a detailed overview of both publicly available and private\nLUS datasets and categorize the AI studies according to the dataset they used.\nAdditionally, we systematically analyzed and tabulated the studies across\nvarious dimensions, including data preprocessing methods, AI models,\ncross-validation techniques, and evaluation metrics. In total, we reviewed 60\narticles, 41 of which utilized public datasets, while the remaining employed\nprivate data. Our findings suggest that ultrasound-based AI studies for\nCOVID-19 detection have great potential for clinical use, especially for\nchildren and pregnant women. Our review also provides a useful summary for\nfuture researchers and clinicians who may be interested in the field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05029v1",
    "published_date": "2024-11-06 06:59:41 UTC",
    "updated_date": "2024-11-06 06:59:41 UTC"
  },
  {
    "arxiv_id": "2411.03687v1",
    "title": "Beyond Model Adaptation at Test Time: A Survey",
    "authors": [
      "Zehao Xiao",
      "Cees G. M. Snoek"
    ],
    "abstract": "Machine learning algorithms have achieved remarkable success across various\ndisciplines, use cases and applications, under the prevailing assumption that\ntraining and test samples are drawn from the same distribution. Consequently,\nthese algorithms struggle and become brittle even when samples in the test\ndistribution start to deviate from the ones observed during training. Domain\nadaptation and domain generalization have been studied extensively as\napproaches to address distribution shifts across test and train domains, but\neach has its limitations. Test-time adaptation, a recently emerging learning\nparadigm, combines the benefits of domain adaptation and domain generalization\nby training models only on source data and adapting them to target data during\ntest-time inference. In this survey, we provide a comprehensive and systematic\nreview on test-time adaptation, covering more than 400 recent papers. We\nstructure our review by categorizing existing methods into five distinct\ncategories based on what component of the method is adjusted for test-time\nadaptation: the model, the inference, the normalization, the sample, or the\nprompt, providing detailed analysis of each. We further discuss the various\npreparation and adaptation settings for methods within these categories,\noffering deeper insights into the effective deployment for the evaluation of\ndistribution shifts and their real-world application in understanding images,\nvideo and 3D, as well as modalities beyond vision. We close the survey with an\noutlook on emerging research opportunities for test-time adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03687v1",
    "published_date": "2024-11-06 06:13:57 UTC",
    "updated_date": "2024-11-06 06:13:57 UTC"
  },
  {
    "arxiv_id": "2411.03675v2",
    "title": "QUILL: Quotation Generation Enhancement of Large Language Models",
    "authors": [
      "Jin Xiao",
      "Bowei Zhang",
      "Qianyu He",
      "Jiaqing Liang",
      "Feng Wei",
      "Jinglei Chen",
      "Zujie Liang",
      "Deqing Yang",
      "Yanghua Xiao"
    ],
    "abstract": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.03675v2",
    "published_date": "2024-11-06 05:24:09 UTC",
    "updated_date": "2025-02-20 07:29:42 UTC"
  },
  {
    "arxiv_id": "2411.03672v2",
    "title": "MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving through Meta-Learning and Long-sequence Modeling",
    "authors": [
      "Yansong Qu",
      "Zixuan Xu",
      "Zilin Huang",
      "Zihao Sheng",
      "Tiantian Chen",
      "Sikai Chen"
    ],
    "abstract": "Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03672v2",
    "published_date": "2024-11-06 05:11:25 UTC",
    "updated_date": "2025-02-19 17:21:53 UTC"
  },
  {
    "arxiv_id": "2411.03670v2",
    "title": "Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?",
    "authors": [
      "Pedro R. A. S. Bassi",
      "Wenxuan Li",
      "Yucheng Tang",
      "Fabian Isensee",
      "Zifu Wang",
      "Jieneng Chen",
      "Yu-Cheng Chou",
      "Yannick Kirchhoff",
      "Maximilian Rokuss",
      "Ziyan Huang",
      "Jin Ye",
      "Junjun He",
      "Tassilo Wald",
      "Constantin Ulrich",
      "Michael Baumgartner",
      "Saikat Roy",
      "Klaus H. Maier-Hein",
      "Paul Jaeger",
      "Yiwen Ye",
      "Yutong Xie",
      "Jianpeng Zhang",
      "Ziyang Chen",
      "Yong Xia",
      "Zhaohu Xing",
      "Lei Zhu",
      "Yousef Sadegheih",
      "Afshin Bozorgpour",
      "Pratibha Kumari",
      "Reza Azad",
      "Dorit Merhof",
      "Pengcheng Shi",
      "Ting Ma",
      "Yuxin Du",
      "Fan Bai",
      "Tiejun Huang",
      "Bo Zhao",
      "Haonan Wang",
      "Xiaomeng Li",
      "Hanxue Gu",
      "Haoyu Dong",
      "Jichen Yang",
      "Maciej A. Mazurowski",
      "Saumya Gupta",
      "Linshan Wu",
      "Jiaxin Zhuang",
      "Hao Chen",
      "Holger Roth",
      "Daguang Xu",
      "Matthew B. Blaschko",
      "Sergio Decherchi",
      "Andrea Cavalli",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "abstract": "How can we test AI performance? This question seems trivial, but it isn't.\nStandard benchmarks often have problems such as in-distribution and small-size\ntest sets, oversimplified metrics, unfair comparisons, and short-term outcome\npressure. As a consequence, good performance on standard benchmarks does not\nguarantee success in real-world scenarios. To address these problems, we\npresent Touchstone, a large-scale collaborative segmentation benchmark of 9\ntypes of abdominal organs. This benchmark is based on 5,195 training CT scans\nfrom 76 hospitals around the world and 5,903 testing CT scans from 11\nadditional hospitals. This diverse test set enhances the statistical\nsignificance of benchmark results and rigorously evaluates AI algorithms across\nvarious out-of-distribution scenarios. We invited 14 inventors of 19 AI\nalgorithms to train their algorithms, while our team, as a third party,\nindependently evaluated these algorithms on three test sets. In addition, we\nalso evaluated pre-existing AI frameworks--which, differing from algorithms,\nare more flexible and can support different algorithms--including MONAI from\nNVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are\ncommitted to expanding this benchmark to encourage more innovation of AI\nalgorithms for the medical domain.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS-2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03670v2",
    "published_date": "2024-11-06 05:09:34 UTC",
    "updated_date": "2025-01-20 02:44:19 UTC"
  },
  {
    "arxiv_id": "2411.03665v1",
    "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
    "authors": [
      "Xuelin Liu",
      "Yanfei Zhu",
      "Shucheng Zhu",
      "Pengyuan Liu",
      "Ying Liu",
      "Dong Yu"
    ],
    "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03665v1",
    "published_date": "2024-11-06 04:52:38 UTC",
    "updated_date": "2024-11-06 04:52:38 UTC"
  },
  {
    "arxiv_id": "2411.03656v1",
    "title": "Requirements Engineering for Older Adult Digital Health Software: A Systematic Literature Review",
    "authors": [
      "Yuqing Xiao",
      "John Grundy",
      "Anuradha Madugalla"
    ],
    "abstract": "Growth of the older adult population has led to an increasing interest in\ntechnology-supported aged care. However, the area has some challenges such as a\nlack of caregivers and limitations in understanding the emotional, social,\nphysical, and mental well-being needs of seniors. Furthermore, there is a gap\nin the understanding between developers and ageing people of their\nrequirements. Digital health can be important in supporting older adults\nwellbeing, emotional requirements, and social needs. Requirements Engineering\n(RE) is a major software engineering field, which can help to identify, elicit\nand prioritize the requirements of stakeholders and ensure that the systems\nmeet standards for performance, reliability, and usability. We carried out a\nsystematic review of the literature on RE for older adult digital health\nsoftware. This was necessary to show the representatives of the current stage\nof understanding the needs of older adults in aged care digital health. Using\nestablished guidelines outlined by the Kitchenham method, the PRISMA and the\nPICO guideline, we developed a protocol, followed by the systematic exploration\nof eight databases. This resulted in 69 primary studies of high relevance,\nwhich were subsequently subjected to data extraction, synthesis, and reporting.\nWe highlight key RE processes in digital health software for ageing people. It\nexplored the utilization of technology for older user well-being and care, and\nthe evaluations of such solutions. The review also identified key limitations\nfound in existing primary studies that inspire future research opportunities.\nThe results indicate that requirement gathering and understanding have a\nsignificant variation between different studies. The differences are in the\nquality, depth, and techniques adopted for requirement gathering and these\ndifferences are largely due to uneven adoption of RE methods.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "arxiv version of SLR on RE for Older Adult Digital Health Software",
    "pdf_url": "http://arxiv.org/pdf/2411.03656v1",
    "published_date": "2024-11-06 04:35:39 UTC",
    "updated_date": "2024-11-06 04:35:39 UTC"
  },
  {
    "arxiv_id": "2411.05832v1",
    "title": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec",
    "authors": [
      "Jun-Hyuk Kim",
      "Seungeon Kim",
      "Won-Hee Lee",
      "Dokwan Oh"
    ],
    "abstract": "Designing a fast and effective entropy model is challenging but essential for\npractical application of neural codecs. Beyond spatial autoregressive entropy\nmodels, more efficient backward adaptation-based entropy models have been\nrecently developed. They not only reduce decoding time by using smaller number\nof modeling steps but also maintain or even improve rate--distortion\nperformance by leveraging more diverse contexts for backward adaptation.\nDespite their significant progress, we argue that their performance has been\nlimited by the simple adoption of the design convention for forward adaptation:\nusing only a single type of hyper latent representation, which does not provide\nsufficient contextual information, especially in the first modeling step. In\nthis paper, we propose a simple yet effective entropy modeling framework that\nleverages sufficient contexts for forward adaptation without compromising on\nbit-rate. Specifically, we introduce a strategy of diversifying hyper latent\nrepresentations for forward adaptation, i.e., using two additional types of\ncontexts along with the existing single type of context. In addition, we\npresent a method to effectively use the diverse contexts for contextualizing\nthe current elements to be encoded/decoded. By addressing the limitation of the\nprevious approach, our proposed framework leads to significant performance\nimprovements. Experimental results on popular datasets show that our proposed\nframework consistently improves rate--distortion performance across various\nbit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline\non the Kodak dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05832v1",
    "published_date": "2024-11-06 04:30:04 UTC",
    "updated_date": "2024-11-06 04:30:04 UTC"
  },
  {
    "arxiv_id": "2411.05831v1",
    "title": "To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation",
    "authors": [
      "Savitha Sam Abraham",
      "Sourav Garg",
      "Feras Dayoub"
    ],
    "abstract": "Recent research in Vision Language Navigation (VLN) has overlooked the\ndevelopment of agents' inquisitive abilities, which allow them to ask\nclarifying questions when instructions are incomplete. This paper addresses how\nagents can recognize \"when\" they lack sufficient information, without focusing\non \"what\" is missing, particularly in VLN tasks with vague instructions.\nEquipping agents with this ability enhances efficiency by reducing potential\ndigressions and seeking timely assistance. The challenge in identifying such\nuncertain points is balancing between being overly cautious (high recall) and\noverly confident (high precision). We propose an attention-based\ninstruction-vagueness estimation module that learns associations between\ninstructions and the agent's trajectory. By leveraging instruction-to-path\nalignment information during training, the module's vagueness estimation\nperformance improves by around 52% in terms of precision-recall balance. In our\nablative experiments, we also demonstrate the effectiveness of incorporating\nthis additional instruction-to-path attention network alongside the cross-modal\nattention networks within the navigator module. Our results show that the\nattention scores from the instruction-to-path attention network serve as better\nindicators for estimating vagueness.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.05831v1",
    "published_date": "2024-11-06 04:21:15 UTC",
    "updated_date": "2024-11-06 04:21:15 UTC"
  },
  {
    "arxiv_id": "2411.03651v1",
    "title": "Policy Aggregation",
    "authors": [
      "Parand A. Alamdari",
      "Soroush Ebadian",
      "Ariel D. Procaccia"
    ],
    "abstract": "We consider the challenge of AI value alignment with multiple individuals\nthat have different reward functions and optimal policies in an underlying\nMarkov decision process. We formalize this problem as one of policy\naggregation, where the goal is to identify a desirable collective policy. We\nargue that an approach informed by social choice theory is especially suitable.\nOur key insight is that social choice methods can be reinterpreted by\nidentifying ordinal preferences with volumes of subsets of the state-action\noccupancy polytope. Building on this insight, we demonstrate that a variety of\nmethods--including approval voting, Borda count, the proportional veto core,\nand quantile fairness--can be practically applied to policy aggregation.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03651v1",
    "published_date": "2024-11-06 04:19:50 UTC",
    "updated_date": "2024-11-06 04:19:50 UTC"
  },
  {
    "arxiv_id": "2411.03644v2",
    "title": "Deploying Multi-task Online Server with Large Language Model",
    "authors": [
      "Yincen Qu",
      "Chao Ma",
      "Xiangying Dai",
      "Hui Zhou",
      "Yiting Wu",
      "Hengyue Liu"
    ],
    "abstract": "In the industry, numerous tasks are deployed online. Traditional approaches\noften tackle each task separately by its own network, which leads to excessive\ncosts for developing and scaling models, especially in the context of large\nlanguage models. Although multi-task methods can save costs through parameter\nsharing, they often struggle to outperform single-task methods in real-world\napplications. To tackle these challenges, we present a three-stage multi-task\nlearning framework for large language models. It involves task filtering,\nfollowed by fine-tuning on high-resource tasks, and finally fine-tuning on all\ntasks. We conducted comprehensive experiments in single-task and multi-task\nsettings. Our approach, exemplified on different benchmarks, demonstrates that\nit is able to achieve performance comparable to the single-task method while\nreducing up to 90.9\\% of its overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2025 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2411.03644v2",
    "published_date": "2024-11-06 03:48:41 UTC",
    "updated_date": "2024-11-07 02:44:34 UTC"
  },
  {
    "arxiv_id": "2411.03638v1",
    "title": "Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions",
    "authors": [
      "Zihan Qin",
      "Jialei Xu",
      "Wenbo Zhao",
      "Junjun Jiang",
      "Xianming Liu"
    ],
    "abstract": "Depth estimation under adverse conditions remains a significant challenge.\nRecently, multi-spectral depth estimation, which integrates both visible light\nand thermal images, has shown promise in addressing this issue. However,\nexisting algorithms struggle with precise pixel-level feature matching,\nlimiting their ability to fully exploit geometric constraints across different\nspectra. To address this, we propose a novel framework incorporating stereo\ndepth estimation to enforce accurate geometric constraints. In particular, we\ntreat the visible light and thermal images as a stereo pair and utilize a\nCross-modal Feature Matching (CFM) Module to construct a cost volume for\npixel-level matching. To mitigate the effects of poor lighting on stereo\nmatching, we introduce Degradation Masking, which leverages robust monocular\nthermal depth estimation in degraded regions. Our method achieves\nstate-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,\nwith qualitative evaluations demonstrating high-quality depth maps under\nvarying lighting conditions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03638v1",
    "published_date": "2024-11-06 03:30:46 UTC",
    "updated_date": "2024-11-06 03:30:46 UTC"
  },
  {
    "arxiv_id": "2411.03630v2",
    "title": "RTify: Aligning Deep Neural Networks with Human Behavioral Decisions",
    "authors": [
      "Yu-Ang Cheng",
      "Ivan Felipe Rodriguez",
      "Sixuan Chen",
      "Kohitij Kar",
      "Takeo Watanabe",
      "Thomas Serre"
    ],
    "abstract": "Current neural network models of primate vision focus on replicating overall\nlevels of behavioral accuracy, often neglecting perceptual decisions' rich,\ndynamic nature. Here, we introduce a novel computational framework to model the\ndynamics of human behavioral choices by learning to align the temporal dynamics\nof a recurrent neural network (RNN) to human reaction times (RTs). We describe\nan approximation that allows us to constrain the number of time steps an RNN\ntakes to solve a task with human RTs. The approach is extensively evaluated\nagainst various psychophysics experiments. We also show that the approximation\ncan be used to optimize an \"ideal-observer\" RNN model to achieve an optimal\ntradeoff between speed and accuracy without human data. The resulting model is\nfound to account well for human RT data. Finally, we use the approximation to\ntrain a deep learning implementation of the popular Wong-Wang decision-making\nmodel. The model is integrated with a convolutional neural network (CNN) model\nof visual processing and evaluated using both artificial and natural image\nstimuli. Overall, we present a novel framework that helps align current vision\nmodels with human behavior, bringing us closer to an integrated model of human\nvision.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03630v2",
    "published_date": "2024-11-06 03:04:05 UTC",
    "updated_date": "2024-12-26 09:11:08 UTC"
  },
  {
    "arxiv_id": "2411.03628v1",
    "title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
    "authors": [
      "Junming Lin",
      "Zheng Fang",
      "Chi Chen",
      "Zihao Wan",
      "Fuwen Luo",
      "Peng Li",
      "Yang Liu",
      "Maosong Sun"
    ],
    "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has\nexpanded their capabilities from image comprehension to video understanding.\nHowever, most of these MLLMs focus primarily on offline video comprehension,\nnecessitating extensive processing of all video frames before any queries can\nbe made. This presents a significant gap compared to the human ability to\nwatch, listen, think, and respond to streaming inputs in real time,\nhighlighting the limitations of current MLLMs. In this paper, we introduce\nStreamingBench, the first comprehensive benchmark designed to evaluate the\nstreaming video understanding capabilities of MLLMs. StreamingBench assesses\nthree core aspects of streaming video understanding: (1) real-time visual\nunderstanding, (2) omni-source understanding, and (3) contextual understanding.\nThe benchmark consists of 18 tasks, featuring 900 videos and 4,500\nhuman-curated QA pairs. Each video features five questions presented at\ndifferent time points to simulate a continuous streaming scenario. We conduct\nexperiments on StreamingBench with 13 open-source and proprietary MLLMs and\nfind that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and\nGPT-4o perform significantly below human-level streaming video understanding\ncapabilities. We hope our work can facilitate further advancements for MLLMs,\nempowering them to approach human-level video comprehension and interaction in\nmore realistic scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03628v1",
    "published_date": "2024-11-06 02:50:30 UTC",
    "updated_date": "2024-11-06 02:50:30 UTC"
  },
  {
    "arxiv_id": "2411.03622v2",
    "title": "Fully Hyperbolic Rotation for Knowledge Graph Embedding",
    "authors": [
      "Qiuyu Liang",
      "Weihua Wang",
      "Feilong Bao",
      "Guanglai Gao"
    ],
    "abstract": "Hyperbolic rotation is commonly used to effectively model knowledge graphs\nand their inherent hierarchies. However, existing hyperbolic rotation models\nrely on logarithmic and exponential mappings for feature transformation. These\nmodels only project data features into hyperbolic space for rotation, limiting\ntheir ability to fully exploit the hyperbolic space. To address this problem,\nwe propose a novel fully hyperbolic model designed for knowledge graph\nembedding. Instead of feature mappings, we define the model directly in\nhyperbolic space with the Lorentz model. Our model considers each relation in\nknowledge graphs as a Lorentz rotation from the head entity to the tail entity.\nWe adopt the Lorentzian version distance as the scoring function for measuring\nthe plausibility of triplets. Extensive results on standard knowledge graph\ncompletion benchmarks demonstrated that our model achieves competitive results\nwith fewer parameters. In addition, our model get the state-of-the-art\nperformance on datasets of CoDEx-s and CoDEx-m, which are more diverse and\nchallenging than before. Our code is available at\nhttps://github.com/llqy123/FHRE.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03622v2",
    "published_date": "2024-11-06 02:41:26 UTC",
    "updated_date": "2024-11-07 03:26:58 UTC"
  },
  {
    "arxiv_id": "2411.03618v1",
    "title": "Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification",
    "authors": [
      "Dahyun Mok",
      "Junghyun Bum",
      "Le Duc Tai",
      "Hyunseung Choo"
    ],
    "abstract": "Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating\nearly detection and diagnosis. This paper focuses on referable DR\nclassification to enhance the applicability of the proposed method in clinical\npractice. We develop an advanced cross-learning DR classification method\nleveraging transfer learning and cross-attention mechanisms. The proposed\nmethod employs the Swin U-Net architecture to segment lesion maps from DR\nfundus images. The Swin U-Net segmentation model, enriched with DR lesion\ninsights, is transferred to generate a lesion map. Both the fundus image and\nits segmented lesion map are used as complementary inputs for the\nclassification model. A cross-attention mechanism is deployed to improve the\nmodel's ability to capture fine-grained details from the input pairs. Our\nexperiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a\nsuperior accuracy of 94.6%, surpassing current state-of-the-art methods by\n4.4%. To this end, we aim for the proposed method to be seamlessly integrated\ninto clinical workflows, enhancing accuracy and efficiency in identifying\nreferable DR.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ACCV 2024 accepted",
    "pdf_url": "http://arxiv.org/pdf/2411.03618v1",
    "published_date": "2024-11-06 02:23:38 UTC",
    "updated_date": "2024-11-06 02:23:38 UTC"
  },
  {
    "arxiv_id": "2411.03588v1",
    "title": "An Experimental Study on Decomposition-Based Deep Ensemble Learning for Traffic Flow Forecasting",
    "authors": [
      "Qiyuan Zhu",
      "A. K. Qin",
      "Hussein Dia",
      "Adriana-Simona Mihaita",
      "Hanna Grzybowska"
    ],
    "abstract": "Traffic flow forecasting is a crucial task in intelligent transport systems.\nDeep learning offers an effective solution, capturing complex patterns in\ntime-series traffic flow data to enable the accurate prediction. However, deep\nlearning models are prone to overfitting the intricate details of flow data,\nleading to poor generalisation. Recent studies suggest that decomposition-based\ndeep ensemble learning methods may address this issue by breaking down a time\nseries into multiple simpler signals, upon which deep learning models are built\nand ensembled to generate the final prediction. However, few studies have\ncompared the performance of decomposition-based ensemble methods with\nnon-decomposition-based ones which directly utilise raw time-series data. This\nwork compares several decomposition-based and non-decomposition-based deep\nensemble learning methods. Experimental results on three traffic datasets\ndemonstrate the superiority of decomposition-based ensemble methods, while also\nrevealing their sensitivity to aggregation strategies and forecasting horizons.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted by the 2024 Australasian Joint Conference\n  on Artificial Intelligence (AJCAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.03588v1",
    "published_date": "2024-11-06 01:00:17 UTC",
    "updated_date": "2024-11-06 01:00:17 UTC"
  },
  {
    "arxiv_id": "2411.03576v1",
    "title": "Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions",
    "authors": [
      "Arunkumar Rathinam",
      "Leo Pauly",
      "Abd El Rahman Shabayek",
      "Wassim Rharbaoui",
      "Anis Kacem",
      "Vincent Gaudilli√®re",
      "Djamila Aouada"
    ],
    "abstract": "Multispectral pedestrian detection has gained significant attention in recent\nyears, particularly in autonomous driving applications. To address the\nchallenges posed by adversarial illumination conditions, the combination of\nthermal and visible images has demonstrated its advantages. However, existing\nfusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)\nimage pairs are fully overlapping. These assumptions often do not hold in\nreal-world applications, where only partial overlap between images can occur\ndue to sensors configuration. Moreover, sensor failure can cause loss of\ninformation in one modality. In this paper, we propose a novel module called\nthe Hybrid Attention (HA) mechanism as our main contribution to mitigate\nperformance degradation caused by partial overlap and sensor failure, i.e. when\nat least part of the scene is acquired by only one sensor. We propose an\nimproved RGB-T fusion algorithm, robust against partial overlap and sensor\nfailure encountered during inference in real-world applications. We also\nleverage a mobile-friendly backbone to cope with resource constraints in\nembedded systems. We conducted experiments by simulating various partial\noverlap and sensor failure scenarios to evaluate the performance of our\nproposed method. The results demonstrate that our approach outperforms\nstate-of-the-art methods, showcasing its superiority in handling real-world\nchallenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in IEEE Robotics and Automation Letters,\n  October 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03576v1",
    "published_date": "2024-11-06 00:34:26 UTC",
    "updated_date": "2024-11-06 00:34:26 UTC"
  },
  {
    "arxiv_id": "2411.03569v1",
    "title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation",
    "authors": [
      "Pengju Wang",
      "Bochao Liu",
      "Weijia Guo",
      "Yong Li",
      "Shiming Ge"
    ],
    "abstract": "Federated learning is a distributed machine learning paradigm designed to\nprotect data privacy. However, data heterogeneity across various clients\nresults in catastrophic forgetting, where the model rapidly forgets previous\nknowledge while acquiring new knowledge. To address this challenge,\npersonalized federated learning has emerged to customize a personalized model\nfor each client. However, the inherent limitation of this mechanism is its\nexcessive focus on personalization, potentially hindering the generalization of\nthose models. In this paper, we present a novel personalized federated learning\nmethod that uses global and historical models as teachers and the local model\nas the student to facilitate comprehensive knowledge distillation. The\nhistorical model represents the local model from the last round of client\ntraining, containing historical personalized knowledge, while the global model\nrepresents the aggregated model from the last round of server aggregation,\ncontaining global generalized knowledge. By applying knowledge distillation, we\neffectively transfer global generalized knowledge and historical personalized\nknowledge to the local model, thus mitigating catastrophic forgetting and\nenhancing the general performance of personalized models. Extensive\nexperimental results demonstrate the significant advantages of our method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE SMC 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.03569v1",
    "published_date": "2024-11-06 00:17:36 UTC",
    "updated_date": "2024-11-06 00:17:36 UTC"
  }
]
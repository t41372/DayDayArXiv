{
  "date": "2025-08-07",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-07 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ã€‚ä»Šå¤© arXiv çš„æ›´æ–°å¯è°“æ˜¯â€œç™¾èŠ±é½æ”¾â€ï¼Œ**æˆ‘ä»¬çœ‹åˆ°å¤§æ¨¡å‹æ­£åœ¨ä»å•çº¯çš„â€œè§„æ¨¡ç«èµ›â€è½¬å‘â€œæ¨ç†æ•ˆç‡â€ä¸â€œè‡ªæˆ‘è¿›åŒ–â€çš„æ·±æ°´åŒºï¼ŒåŒæ—¶å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰å’Œ AI for Scienceï¼ˆå°¤å…¶æ˜¯è„‘ç§‘å­¦å’Œè›‹ç™½è´¨ç»“æ„ï¼‰ä¹Ÿæœ‰äº†ä»¤äººå…´å¥‹çš„çªç ´ã€‚**\n\næœ€ä»¤æˆ‘å°è±¡æ·±åˆ»çš„æ˜¯å‡ ç¯‡å…³äº**LLM è‡ªæˆ‘è¿›åŒ–**å’Œ**æ¨ç†å»å†—ä½™**çš„æ–‡ç« ï¼Œè¿™é¢„ç¤ºç€æˆ‘ä»¬æ­£åœ¨æ¥è¿‘æ›´é«˜æ•ˆçš„ Intelligenceã€‚æ­¤å¤–ï¼Œå…³äºâ€œAgentâ€å®šä¹‰çš„åæ€æ–‡ç« ä¹Ÿå€¼å¾—ä¸€è¯»ï¼Œåœ¨è¿™ä¸ªæ¦‚å¿µè¢«æ»¥ç”¨çš„ä»Šå¤©ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å†·æ€è€ƒã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©æœ‰å“ªäº›ä¸å®¹é”™è¿‡çš„è®ºæ–‡ã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ã€è‡ªæˆ‘è¿›åŒ–ä¸æ•ˆç‡\nä»Šå¤©çš„é‡å¤´æˆã€‚ç ”ç©¶è€…ä»¬ä¸å†æ»¡è¶³äºè®©æ¨¡å‹ç”± CoTï¼ˆæ€ç»´é“¾ï¼‰äº§ç”Ÿç»“æœï¼Œè€Œæ˜¯å¼€å§‹æ€è€ƒå¦‚ä½•è®©æ¨ç†æ›´é«˜æ•ˆã€æ›´è‡ªä¸»ã€‚\n\n**1. R-Zero: Self-Evolving Reasoning LLM from Zero Data**\n**R-Zeroï¼šé›¶æ•°æ®å¯åŠ¨çš„è‡ªæˆ‘è¿›åŒ–æ¨ç†å¤§æ¨¡å‹**\n> è¿™ç¯‡æ–‡ç« éå¸¸æœ‰é‡å¿ƒã€‚ä½œè€…æå‡ºäº†ä¸€ç§**å®Œå…¨è‡ªä¸»**çš„æ¡†æ¶ï¼Œä¸éœ€è¦äººç±»æ ‡æ³¨çš„æ•°æ®ï¼Œä»ä¸€ä¸ªåŸºç¡€æ¨¡å‹å¼€å§‹è‡ªæˆ‘è¿›åŒ–ã€‚\n- **æ ¸å¿ƒæœºåˆ¶**ï¼šå¼•å…¥äº†ä¸¤ä¸ªè§’è‰²â€”â€”æŒ‘æˆ˜è€…ï¼ˆChallengerï¼‰å’Œè§£å†³è€…ï¼ˆSolverï¼‰ã€‚æŒ‘æˆ˜è€…è´Ÿè´£æå‡ºå¤„äºèƒ½åŠ›è¾¹ç•Œçš„éš¾é¢˜ï¼Œè§£å†³è€…è´Ÿè´£è§£é¢˜ã€‚ä¸¤è€…ç›¸äº’åšå¼ˆï¼Œå…±åŒè¿›åŒ–ã€‚\n- **Implication**ï¼šæ‰“ç ´äº†å¯¹æµ·é‡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œåœ¨ Qwen3-4B-Base ä¸Šå®ç°äº†æ˜¾è‘—çš„æ¨ç†èƒ½åŠ›æå‡ã€‚è¿™å¯èƒ½æ˜¯é€šå‘ Super-intelligence çš„ä¸€æ¡å¯è¡Œè·¯å¾„ã€‚\n\n**2. Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression**\n**é€šè¿‡ç¡®å®šæ€§å¼•å¯¼çš„åå°„æŠ‘åˆ¶å®ç°å¤§å‹æ¨ç†æ¨¡å‹çš„é«˜æ•ˆæ¨ç†**\n> è§£å†³äº† CoT ä¸­çš„â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆOverthinkingï¼‰é—®é¢˜ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šç°åœ¨çš„æ¨ç†æ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1 ç±»ï¼‰å¾€å¾€ä¼šæœ‰å†—ä½™çš„â€œåæ€â€æ­¥éª¤ï¼ˆæ¯”å¦‚åå¤è¯´ \"Wait\", \"Alternatively\"ï¼‰ã€‚ä½œè€…æå‡º CGRS æ–¹æ³•ï¼Œå½“æ¨¡å‹å¯¹å½“å‰å›ç­”ç½®ä¿¡åº¦é«˜æ—¶ï¼ŒåŠ¨æ€æŠ‘åˆ¶è¿™äº›åæ€è§¦å‘è¯ã€‚\n- **æ•ˆæœ**ï¼šToken ä½¿ç”¨é‡å‡å°‘äº† 18.5% åˆ° 41.9%ï¼Œä¸”ä¸é™ä½å‡†ç¡®ç‡ã€‚\n\n**3. Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning**\n**é€šè¿‡é—®é¢˜ç©ºé—´æ˜ å°„è§£è€¦ç†è§£ä¸æ¨ç†ï¼šæå‡å°æ¨¡å‹æ¨ç†èƒ½åŠ›**\n> å°æ¨¡å‹ï¼ˆSLMï¼‰é€šå¸¸éš¾ä»¥åŒæ—¶å¤„ç†å¤æ‚çš„è¯­è¨€è¡¨è¿°å’Œé€»è¾‘æ¨ç†ã€‚\n- **æ–¹æ³•**ï¼šDURIT ç®—æ³•ã€‚å°†è‡ªç„¶è¯­è¨€é—®é¢˜æ˜ å°„åˆ°ä¸€ä¸ªæ ‡å‡†åŒ–çš„â€œé—®é¢˜ç©ºé—´â€ï¼ˆProblem Spaceï¼‰ï¼Œè®©æ¨¡å‹å…ˆâ€œç†è§£â€å†å»â€œæ¨ç†â€ï¼ŒæŠŠä¸¤ä¸ªè´Ÿæ‹…è§£è€¦ã€‚\n- **è´¡çŒ®**ï¼šæ˜¾è‘—æå‡äº† 1.5B å‚æ•°çº§åˆ«å°æ¨¡å‹çš„æ•°å­¦å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ (Embodied AI) & Agent\nAgent çš„æ¦‚å¿µè¶Šæ¥è¶Šç«ï¼Œä½†ä¹Ÿè¶Šæ¥è¶Šä¹±ã€‚ä»Šå¤©çš„è®ºæ–‡åœ¨å®šä¹‰æ ‡å‡†å’Œè½åœ°åº”ç”¨ä¸Šéƒ½æœ‰å»ºæ ‘ã€‚\n\n**4. The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition**\n**â€œAgentâ€ä¸€è¯å·²è¢«è¿‡åº¦ç¨€é‡Šï¼ŒäºŸéœ€é‡æ–°å®šä¹‰**\n> è¿™æ˜¯ä¸€ç¯‡åŠæ—¶çš„è¯„è®ºæ€§æ–‡ç« ï¼ˆPosition Paperï¼‰ã€‚\n- **è§‚ç‚¹**ï¼šç°åœ¨æ˜¯ä¸ª AI å°±å« Agentï¼Œè¿™ä¸åˆ©äºé¢†åŸŸå‘å±•ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„å…‰è°±æ¡†æ¶ï¼Œä»ç¯å¢ƒäº¤äº’ã€å­¦ä¹ é€‚åº”æ€§ã€è‡ªä¸»æ€§ç­‰è§’åº¦é‡æ–°å®šä¹‰äº† Agent çš„æœ€ä½é—¨æ§›ã€‚\n- **å»ºè®®**ï¼šä¸ä»…è¦çœ‹å®ƒå«ä»€ä¹ˆï¼Œè¦çœ‹å®ƒåœ¨â€œäº¤äº’-é€‚åº”-è‡ªä¸»â€å…‰è°±ä¸Šçš„ä½ç½®ã€‚\n\n**5. IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model**\n**IRL-VLAï¼šé€šè¿‡å¥–åŠ±ä¸–ç•Œæ¨¡å‹è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥**\n> é’ˆå¯¹è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„ VLA æ¨¡å‹ã€‚\n- **ç—›ç‚¹**ï¼šç°æœ‰çš„ VLA å¤šæ˜¯å¼€ç¯çš„æ¨¡ä»¿å­¦ä¹ ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆä¸”ç¼ºä¹é•¿è¿œè§„åˆ’ã€‚\n- **æ–¹æ³•**ï¼šåˆ©ç”¨é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰æ„å»ºä¸€ä¸ªè½»é‡çº§çš„å¥–åŠ±ä¸–ç•Œæ¨¡å‹ï¼Œå®ç°äº†é—­ç¯è®­ç»ƒã€‚è¿™åœ¨ NAVSIM è‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­æ‹¿åˆ°äº† SOTAã€‚\n\n**6. OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks**\n**OmniEARï¼šå…·èº«ä»»åŠ¡ä¸­ Agent æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•**\n> å‘ç°å¤§æ¨¡å‹åœ¨ç‰©ç†ä¸–ç•Œç»å¸¸â€œçœ¼é«˜æ‰‹ä½â€ã€‚\n- **å‘ç°**ï¼šLLM åœ¨æŠ½è±¡æ¨ç†ä¸Šå¾ˆå¼ºï¼Œä½†ä¸€æ—¦æ¶‰åŠåˆ°å…·èº«ä»»åŠ¡ï¼ˆå·¥å…·ä½¿ç”¨ã€å¤šæ™ºèƒ½ä½“åä½œï¼‰ï¼Œè¡¨ç°ä¼šä¸¥é‡ä¸‹é™ã€‚ç‰¹åˆ«æ˜¯å½“ç¯å¢ƒä¿¡æ¯å®Œå…¨ç»™å®šæ—¶ï¼Œåè°ƒæ€§èƒ½åè€Œä¸‹é™ï¼Œè¯´æ˜æ¨¡å‹æ— æ³•æœ‰æ•ˆè¿‡æ»¤æ— å…³çº¦æŸã€‚\n\n---\n\n### ğŸ”¬ AI for Scienceï¼šå¤§è„‘ã€åŒ»ç–—ä¸åˆ†å­\nä»Šå¤©çš„ AI for Science è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘ç§‘å­¦å’Œè›‹ç™½è´¨ç ”ç©¶æ–¹é¢ã€‚\n\n**7. Revealing Neurocognitive and Behavioral Patterns by Unsupervised Manifold Learning from Dynamic Brain Data**\n**é€šè¿‡åŠ¨æ€è„‘æ•°æ®çš„æ— ç›‘ç£æµå½¢å­¦ä¹ æ­ç¤ºç¥ç»è®¤çŸ¥å’Œè¡Œä¸ºæ¨¡å¼**\n> è¿™ç¯‡æ–‡ç« å‘å¸ƒåœ¨å¥½ä½ç½®ï¼Œè´¨é‡å¾ˆé«˜ã€‚\n- **æ–¹æ³•**ï¼šæå‡ºäº† BCNEï¼ˆè„‘åŠ¨æ€å·ç§¯ç½‘ç»œåµŒå…¥ï¼‰ã€‚ä¸åŒäºç›´æ¥æå–æ¨¡å¼ï¼Œå®ƒå…ˆæ•æ‰è„‘çŠ¶æ€çš„è½¨è¿¹ï¼ˆTemporospatial correlationsï¼‰ï¼Œå†åœ¨æµå½¢ä¸Šè¿›è¡Œå­¦ä¹ ã€‚\n- **å‘ç°**ï¼šèƒ½æœ‰æ•ˆåŒºåˆ†ä¸»åŠ¨å’Œè¢«åŠ¨è¡Œä¸ºï¼Œæç»˜è®°å¿†å¤„ç†ä¸­çš„è„‘åŒºå‚ä¸ï¼Œæ˜¯æ¢ç´¢ç¥ç»ç§‘å­¦çš„æœ‰åŠ›å·¥å…·ã€‚\n\n**8. The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein-Ligand Binding**\n**å¯¹æ¥åšå¼ˆï¼šç”¨äºå¿«é€Ÿã€åŠ¨æ€ã€ç²¾å‡†é¢„æµ‹è›‹ç™½è´¨-é…ä½“ç»“åˆçš„å¾ªç¯è‡ªåšå¼ˆ**\n> ç”¨åšå¼ˆè®ºæ¥åšåˆ†å­å¯¹æ¥ï¼Œå¾ˆæœ‰æ–°æ„ã€‚\n- **æ–¹æ³•**ï¼šå°†è›‹ç™½è´¨å’Œé…ä½“çœ‹ä½œåšå¼ˆçš„ä¸¤ä¸ªç©å®¶ï¼ˆLoopPlayï¼‰ã€‚åœ¨é€šè¿‡å¤šè½®äº¤äº’ä¸­ï¼ŒåŒæ–¹ä¸æ–­é€‚åº”å¯¹æ–¹çš„ç»“æ„å˜åŒ–ã€‚\n- **è´¡çŒ®**ï¼šç›¸æ¯” SOTA æ–¹æ³•ï¼Œç»“åˆæ¨¡å¼çš„é¢„æµ‹å‡†ç¡®ç‡æå‡äº†çº¦ 10%ã€‚\n\n**9. Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks**\n**é¢„è®­ç»ƒ MRI Transformer åœ¨è„‘æˆåƒä»»åŠ¡ä¸­çš„å°‘æ ·æœ¬éƒ¨ç½²**\n> è§£å†³äº†åŒ»ç–—æ•°æ®æ ‡æ³¨å°‘çš„ç—›ç‚¹ã€‚\n- **å·¥ä½œ**ï¼šåœ¨ 3100 ä¸‡å¼ è„‘éƒ¨ MRI åˆ‡ç‰‡ä¸Šåˆ©ç”¨ MAEï¼ˆMasked Autoencoderï¼‰ç­–ç•¥è¿›è¡Œé¢„è®­ç»ƒã€‚\n- **æ•ˆæœ**ï¼šå†»ç»“çš„ç¼–ç å™¨åŠ ä¸Šè½»é‡çº§çº¿æ€§å¤´å°±èƒ½è¾¾åˆ° SOTA çš„åºåˆ—è¯†åˆ«ç²¾åº¦ï¼›åœ¨å°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰\nè§†è§‰æ¨¡å‹æ­£åœ¨å‘æ›´æ·±å±‚çš„è¯­ä¹‰ç†è§£å’Œç‰©ç†è§„å¾‹è®¤çŸ¥è¿ˆè¿›ã€‚\n\n**10. DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning**\n**DeepPHYï¼šè¯„ä¼° Agentic VLM çš„ç‰©ç†æ¨ç†èƒ½åŠ›**\n> VLM è¿˜æ˜¯ä¸æ‡‚ç‰©ç†ã€‚\n- **ç»“è®º**ï¼šè™½ç„¶ VLM è§†è§‰è¯†åˆ«å¾ˆå¼ºï¼Œä½†åœ¨è¿™é¡¹åŒ…å«å¤šç§ç‰©ç†æ¨¡æ‹Ÿç¯å¢ƒçš„åŸºå‡†æµ‹è¯•ä¸­å‘ç°ï¼Œå³ä¾¿æ˜¯ SOTA æ¨¡å‹ä¹Ÿéš¾ä»¥å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®çš„æ§åˆ¶ç­–ç•¥ã€‚\n\n**11. Adapting Vision-Language Models Without Labels: A Comprehensive Survey**\n**æ— éœ€æ ‡ç­¾çš„è§†è§‰-è¯­è¨€æ¨¡å‹è‡ªé€‚åº”ï¼šç»¼è¿°**\n> å¾ˆå¥½çš„ç»¼è¿°æ–‡ç« ã€‚\n- **å†…å®¹**ï¼šç³»ç»Ÿæ€»ç»“äº†æ— ç›‘ç£ VLM è‡ªé€‚åº”çš„æ–¹æ³•ï¼Œåˆ†ä¸ºæ— æ•°æ®è¿ç§»ã€æ— ç›‘ç£åŸŸè¿ç§»ã€æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰ç­‰èŒƒå¼ã€‚å¯¹äºæƒ³åš VLM è½åœ°çš„äººæ¥è¯´éå¸¸æœ‰ä»·å€¼ã€‚\n\n**12. Reveal Neurocognitive... (Ref #1)** \n*(æ³¨ï¼šç¬¬ä¸€ç¯‡è„‘ç§‘å­¦æ–‡ç« å…¶å®ä¹Ÿå±äºå¹¿ä¹‰çš„å¤šæ¨¡æ€æ•°æ®åˆ†æï¼Œå†æ¬¡å¼ºè°ƒå…¶é‡è¦æ€§)*\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸ç¤¾ä¼šå½±å“\næœ€åï¼Œå…³æ³¨ä¸€ä¸‹ AI çš„å®‰å…¨æ€§å’Œç›‘ç®¡ã€‚\n\n**13. Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?**\n**Bench-2-CoPï¼šæˆ‘ä»¬å¯ä»¥ä¿¡ä»»åŸºå‡†æµ‹è¯•æ¥åº”å¯¹æ¬§ç›Ÿ AI åˆè§„å—ï¼Ÿ**\n> æå‡ºäº†ä¸€ä¸ªä¸¥å³»çš„é—®é¢˜ï¼šåˆ·æ¦œèƒ½ä¸èƒ½ä»£è¡¨å®‰å…¨ï¼Ÿ\n- **å‘ç°**ï¼šç›®å‰çš„ AI åŸºå‡†æµ‹è¯•ä¸æ¬§ç›Ÿ AI æ³•æ¡ˆï¼ˆEU AI Actï¼‰å…³æ³¨çš„é£é™©å­˜åœ¨å·¨å¤§é¸¿æ²Ÿã€‚ç°æœ‰çš„ Benchmark ä¸»è¦å…³æ³¨â€œå¹»è§‰â€å’Œâ€œæ€§èƒ½â€ï¼Œè€Œå‡ ä¹å®Œå…¨å¿½ç•¥äº†â€œå¤±æ§é£é™©â€ï¼ˆå¦‚é€ƒé¿ç›‘ç®¡ã€è‡ªæˆ‘å¤åˆ¶ï¼‰ã€‚\n- **ç»“è®º**ï¼šä»…é ç°åœ¨çš„åˆ·æ¦œåˆ†æ•°ï¼Œæ— æ³•è¯æ˜æ¨¡å‹ç¬¦åˆæ¬§ç›Ÿçš„æ³•è§„è¦æ±‚ã€‚\n\n**14. JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering**\n**JPSï¼šåˆ©ç”¨ååŒè§†è§‰æ‰°åŠ¨å’Œæ–‡æœ¬å¼•å¯¼è¶Šç‹±å¤šæ¨¡æ€å¤§æ¨¡å‹**\n> å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ¼æ´ã€‚\n- **æ–¹æ³•**ï¼šç»“åˆäº†é’ˆå¯¹å›¾åƒçš„å¯¹æŠ—æ‰°åŠ¨å’Œé’ˆå¯¹æ–‡æœ¬çš„â€œå¼•å¯¼æç¤ºè¯â€ã€‚\n- **ç»“æœ**ï¼šåˆ·æ–°äº†è¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ï¼ŒåŒæ—¶ä¹Ÿæå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ MIFRï¼Œç”¨æ¥è¡¡é‡æ¨¡å‹æ˜¯å¦çœŸçš„æ‰§è¡Œäº†æ¶æ„æ„å›¾ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»•è¿‡è¿‡æ»¤å™¨ã€‚\n\n---\n\nä»Šå¤©çš„å¿«æŠ¥å°±åˆ°è¿™é‡Œã€‚**R-Zero çš„é›¶æ•°æ®è¿›åŒ–**å’Œ**è„‘åŠ¨æ€æµå½¢å­¦ä¹ **æ˜¯æˆ‘ä»Šå¤©æœ€æ¨èç²¾è¯»çš„ä¸¤ç¯‡æ–‡ç« ã€‚å¸Œæœ›è¿™äº›å‰æ²¿è¿›å±•èƒ½ç»™ä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼\n\næˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2508.11672v1",
      "title": "Revealing Neurocognitive and Behavioral Patterns by Unsupervised Manifold Learning from Dynamic Brain Data",
      "title_zh": "é€šè¿‡å¯¹åŠ¨æ€è„‘æ•°æ®çš„æ— ç›‘ç£æµå½¢å­¦ä¹ æ­ç¤ºç¥ç»è®¤çŸ¥ä¸è¡Œä¸ºæ¨¡å¼",
      "authors": [
        "Zixia Zhou",
        "Junyan Liu",
        "Wei Emma Wu",
        "Ruogu Fang",
        "Sheng Liu",
        "Qingyue Wei",
        "Rui Yan",
        "Yi Guo",
        "Qian Tao",
        "Yuanyuan Wang",
        "Md Tauhidul Islam",
        "Lei Xing"
      ],
      "abstract": "Dynamic brain data, teeming with biological and functional insights, are becoming increasingly accessible through advanced measurements, providing a gateway to understanding the inner workings of the brain in living subjects. However, the vast size and intricate complexity of the data also pose a daunting challenge in reliably extracting meaningful information across various data sources. This paper introduces a generalizable unsupervised deep manifold learning for exploration of neurocognitive and behavioral patterns. Unlike existing methods that extract patterns directly from the input data as in the existing methods, the proposed Brain-dynamic Convolutional-Network-based Embedding (BCNE) seeks to capture the brain-state trajectories by deciphering the temporospatial correlations within the data and subsequently applying manifold learning to this correlative representation. The performance of BCNE is showcased through the analysis of several important dynamic brain datasets. The results, both visual and quantitative, reveal a diverse array of intriguing and interpretable patterns. BCNE effectively delineates scene transitions, underscores the involvement of different brain regions in memory and narrative processing, distinguishes various stages of dynamic learning processes, and identifies differences between active and passive behaviors. BCNE provides an effective tool for exploring general neuroscience inquiries or individual-specific patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBrain-dynamic Convolutional-Network-based Embedding (BCNE) çš„é€šç”¨æ— ç›‘ç£æ·±åº¦æµå½¢å­¦ä¹ (unsupervised deep manifold learning)æ¡†æ¶ï¼Œæ—¨åœ¨ä»é«˜åº¦å¤æ‚çš„åŠ¨æ€å¤§è„‘æ•°æ®ä¸­æå–æœ‰æ„ä¹‰çš„ç¥ç»è®¤çŸ¥å’Œè¡Œä¸ºæ¨¡å¼ã€‚ä¸ä¼ ç»Ÿç›´æ¥å¤„ç†åŸå§‹è¾“å…¥çš„æ–¹æ³•ä¸åŒï¼ŒBCNEé€šè¿‡ç ´è¯‘æ•°æ®ä¸­çš„æ—¶ç©ºç›¸å…³æ€§(temporospatial correlations)æ¥æ•æ‰è„‘çŠ¶æ€è½¨è¿¹(brain-state trajectories)ï¼Œå¹¶å¯¹è¯¥ç›¸å…³æ€§è¡¨ç¤ºåº”ç”¨æµå½¢å­¦ä¹ ã€‚é€šè¿‡å¯¹å¤šä¸ªé‡è¦å¤§è„‘æ•°æ®é›†çš„åˆ†æï¼ŒBCNEèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åœºæ™¯è½¬æ¢ï¼Œå¹¶æ­ç¤ºä¸åŒè„‘åŒºåœ¨è®°å¿†ä¸å™äº‹å¤„ç†ä¸­çš„å‚ä¸æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½ç²¾å‡†åŒºåˆ†åŠ¨æ€å­¦ä¹ è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µï¼Œå¹¶è¯†åˆ«ä¸»åŠ¨ä¸è¢«åŠ¨è¡Œä¸ºä¹‹é—´çš„æ¨¡å¼å·®å¼‚ã€‚å®éªŒç»“æœåœ¨è§†è§‰å’Œå®šé‡ä¸Šå‡è¯æ˜äº†BCNEå…·æœ‰æå¼ºçš„æ¨¡å¼æå–èƒ½åŠ›ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºæ¢ç´¢é€šç”¨ç¥ç»ç§‘å­¦é—®é¢˜åŠä¸ªä½“ç‰¹å¾æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11672v1",
      "published_date": "2025-08-07 23:36:52 UTC",
      "updated_date": "2025-08-07 23:36:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:30.895734+00:00"
    },
    {
      "arxiv_id": "2508.05888v2",
      "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning",
      "title_zh": "è§„åˆ’æ™ºèƒ½ä½“çš„â€œè‡ªæˆ‘ä¸­å¿ƒå›¾â€æ¢ç´¢ï¼šåˆ©ç”¨æ··åˆè‡ªæˆ‘ä¸­å¿ƒå›¾é›†æˆä¼˜åŒ–ä¼ä¸šä»»åŠ¡è§„åˆ’ä¸­çš„å·¥å…·æ£€ç´¢",
      "authors": [
        "Sahil Bansal",
        "Sai Shruthi Sistla",
        "Aarti Arikatala",
        "Sebastian Schreiber"
      ],
      "abstract": "Effective tool pre-selection via retrieval is essential for AI agents to select from a vast array of tools when identifying and planning actions in the context of complex user queries. Despite its central role in planning, this aspect remains underexplored in the literature. Traditional approaches rely primarily on similarities between user queries and tool descriptions, which significantly limits retrieval accuracy, specifically when handling multi-step user requests. To address these limitations, we propose a Knowledge Graph (KG)-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies. Our retrieval algorithm leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools, enabling more comprehensive and contextual tool selection for multi-step tasks. We evaluate our approach on a synthetically generated internal dataset across six defined user classes, extending previous work on coherent dialogue synthesis and tool retrieval benchmarks. Results demonstrate that our tool graph-based method achieves 91.85% tool coverage on the micro-average CompleteRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid retrieval, the strongest non-KG baseline in our experiments. These findings support our hypothesis that the structural information modeled in the graph provides complementary signals to pure similarity matching, particularly for queries requiring sequential tool composition.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šä»»åŠ¡è§„åˆ’(Enterprise Task Planning)ä¸­æ™ºèƒ½ä½“å·¥å…·æ£€ç´¢å‡†ç¡®ç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºçŸ¥è¯†å›¾è°±(Knowledge Graph)çš„å·¥å…·æ£€ç´¢æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»ŸåŸºäºç›¸ä¼¼åº¦æ–¹æ³•åœ¨å¤„ç†å¤šæ­¥ç”¨æˆ·è¯·æ±‚æ—¶çš„å±€é™ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨äº†1-hop ego tool graphsçš„é›†æˆ(ensembles)æ¥å»ºæ¨¡å·¥å…·é—´çš„è¯­ä¹‰å…³ç³»å’ŒåŠŸèƒ½ä¾èµ–ã€‚è¯¥ç®—æ³•é€šè¿‡æ•æ‰å·¥å…·ä¹‹é—´çš„ç›´æ¥å’Œé—´æ¥è”ç³»ï¼Œå®ç°äº†æ›´å…¨é¢ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å·¥å…·é€‰æ‹©ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤æ‚çš„å¤šæ­¥ä»»åŠ¡ã€‚å®éªŒåœ¨åŒ…å«å…­ç±»ç”¨æˆ·çš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨Micro-average CompleteRecallæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†91.85%çš„å·¥å…·è¦†ç›–ç‡ï¼Œä¼˜äºæœ€å¼ºçš„è¯­ä¹‰è¯æ±‡æ··åˆæ£€ç´¢(semantic-lexical hybrid retrieval)åŸºçº¿ã€‚ç ”ç©¶ç»“æœè¯å®äº†å›¾ç»“æ„ä¿¡æ¯èƒ½å¤Ÿä¸ºçº¯ç›¸ä¼¼åº¦åŒ¹é…æä¾›é‡è¦çš„äº’è¡¥ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†éœ€è¦åºåˆ—åŒ–å·¥å…·ç»„åˆ(sequential tool composition)çš„ä»»åŠ¡è§„åˆ’æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05888v2",
      "published_date": "2025-08-07 22:41:12 UTC",
      "updated_date": "2025-11-13 01:48:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:27.952059+00:00"
    },
    {
      "arxiv_id": "2508.05884v1",
      "title": "User-Intent-Driven Semantic Communication via Adaptive Deep Understanding",
      "title_zh": "åŸºäºè‡ªé€‚åº”æ·±åº¦ç†è§£çš„ç”¨æˆ·æ„å›¾é©±åŠ¨è¯­ä¹‰é€šä¿¡",
      "authors": [
        "Peigen Ye",
        "Jingpu Duan",
        "Hongyang Du",
        "Yulan Guo"
      ],
      "abstract": "Semantic communication focuses on transmitting task-relevant semantic information, aiming for intent-oriented communication. While existing systems improve efficiency by extracting key semantics, they still fail to deeply understand and generalize users' real intentions. To overcome this, we propose a user-intention-driven semantic communication system that interprets diverse abstract intents. First, we integrate a multi-modal large model as semantic knowledge base to generate user-intention prior. Next, a mask-guided attention module is proposed to effectively highlight critical semantic regions. Further, a channel state awareness module ensures adaptive, robust transmission across varying channel conditions. Extensive experiments demonstrate that our system achieves deep intent understanding and outperforms DeepJSCC, e.g., under a Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19% in PSNR, SSIM, and LPIPS, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨æˆ·æ„å›¾é©±åŠ¨çš„è¯­ä¹‰é€šä¿¡ç³»ç»Ÿ(User-intent-driven semantic communication system)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯­ä¹‰é€šä¿¡æ¡†æ¶åœ¨æ·±åº¦ç†è§£å’Œæ³›åŒ–ç”¨æˆ·çœŸå®æ„å›¾æ–¹é¢çš„å±€é™ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆé›†æˆå¤šæ¨¡æ€å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰çŸ¥è¯†åº“ä»¥ç”Ÿæˆç”¨æˆ·æ„å›¾å…ˆéªŒï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ¨¡å—(mask-guided attention module)æ¥æœ‰æ•ˆçªå‡ºå…³é”®è¯­ä¹‰åŒºåŸŸã€‚ä¸ºäº†åº”å¯¹å¤šå˜çš„ä¿¡é“ç¯å¢ƒï¼Œç³»ç»Ÿå¼•å…¥äº†ä¿¡é“çŠ¶æ€æ„ŸçŸ¥æ¨¡å—(channel state awareness module)ä»¥å®ç°è‡ªé€‚åº”ä¸”é²æ£’çš„ä¿¡å·ä¼ è¾“ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†æ·±å±‚çš„æ„å›¾ç†è§£ï¼Œä¸”æ€§èƒ½è¡¨ç°å…¨é¢è¶…è¶Šäº†DeepJSCCã€‚åœ¨ä¿¡å™ªæ¯”ä¸º5 dBçš„ç‘åˆ©ä¿¡é“(Rayleigh channel)ç¯å¢ƒä¸‹ï¼Œè¯¥ç³»ç»Ÿåœ¨PSNRã€SSIMå’ŒLPIPSæŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†8%ã€6%å’Œ19%çš„æ˜¾è‘—æå‡ï¼Œä¸ºé¢å‘æ„å›¾çš„é€šä¿¡æŠ€æœ¯æä¾›äº†é«˜æ•ˆä¸”å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "300 *^_^* IEEE Globecom 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05884v1",
      "published_date": "2025-08-07 22:26:27 UTC",
      "updated_date": "2025-08-07 22:26:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:28.554333+00:00"
    },
    {
      "arxiv_id": "2508.05880v1",
      "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models",
      "title_zh": "æœºå™¨æ˜¯å¦å…·æœ‰æƒ…æ„Ÿæ€ç»´ï¼Ÿå¤§è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥è¯„ä»·åˆ†æ",
      "authors": [
        "Sree Bhattacharyya",
        "Lucas Craig",
        "Tharun Dilliraj",
        "Jia Li",
        "James Z. Wang"
      ],
      "abstract": "Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ˜¯å¦å…·å¤‡æƒ…æ„Ÿæ€è€ƒèƒ½åŠ›ï¼Œå°†ç ”ç©¶é‡ç‚¹ä»æµ…å±‚çš„æƒ…æ„Ÿè®¡ç®—(Affective Computing)ä»»åŠ¡è½¬å‘åŸºäºè®¤çŸ¥è¯„ä»·ç†è®º(Cognitive Appraisal Theory)çš„æƒ…æ„Ÿæ¨ç†åˆ†æã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†CoRE (Cognitive Reasoning for Emotions)ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¤„ç†æƒ…æ„Ÿåˆºæ¿€æ—¶æ‰€é‡‡ç”¨çš„å†…éƒ¨è®¤çŸ¥æ¨ç†ç»“æ„ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œè®ºæ–‡æ·±å…¥æ¢è®¨äº†æ¨¡å‹æ˜¯å¦éšå¼ä¾èµ–ç‰¹å®šçš„è®¤çŸ¥è¯„ä»·ç»´åº¦ï¼Œä»¥åŠè¿™äº›ç»´åº¦åœ¨åˆ»ç”»ç‰¹å®šæƒ…æ„Ÿç±»åˆ«ä¸­çš„é‡è¦æ€§ã€‚å®éªŒåˆ†ææ­ç¤ºäº†ä¸åŒLLMsä¹‹é—´å­˜åœ¨å¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼ï¼Œå¹¶éªŒè¯äº†LLMsçš„æƒ…æ„Ÿå†…éƒ¨è¡¨å¾å¯ä»¥é€šè¿‡è®¤çŸ¥è¯„ä»·ç»´åº¦è¿›è¡Œè§£é‡Šã€‚è¯¥é¡¹å·¥ä½œä¸ä»…è¶…è¶Šäº†ä¼ ç»Ÿçš„æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ï¼Œè¿˜ä¸ºå¼€å‘æ›´å…·å¯è§£é‡Šæ€§å’Œæ·±åº¦æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05880v1",
      "published_date": "2025-08-07 22:19:15 UTC",
      "updated_date": "2025-08-07 22:19:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:31.788246+00:00"
    },
    {
      "arxiv_id": "2508.14052v4",
      "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering",
      "title_zh": "FinAgentBenchï¼šé‡‘èé—®ç­”ä¸­æ™ºèƒ½ä½“æ£€ç´¢çš„åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Chanyeol Choi",
        "Jihoon Kwon",
        "Alejandro Lopez-Lira",
        "Chaewoon Kim",
        "Minjae Kim",
        "Juneha Hwang",
        "Jaeseon Ha",
        "Hojun Choi",
        "Suyeol Yun",
        "Yongjin Kim",
        "Yongjae Lee"
      ],
      "abstract": "Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods -- whether sparse or dense -- often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 26K expert-annotated examples on S&P-500 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† FinAgentBenchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°é‡‘èé—®ç­”ä¸­å¤šæ­¥æ¨ç†æ£€ç´¢ï¼ˆå³ agentic retrievalï¼‰çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ã€‚é’ˆå¯¹ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ï¼ˆIR methodsï¼‰åœ¨æ•æ‰é‡‘èä¸“ä¸šçŸ¥è¯†å’Œç»†ç²’åº¦æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥åŸºå‡†åŒ…å«äº† 26,000 ä¸ªå…³äºæ ‡æ™® 500ï¼ˆS&P-500ï¼‰ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ ‡æ³¨ç¤ºä¾‹ã€‚è¯„ä¼°æ¡†æ¶æ—¨åœ¨æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ™ºèƒ½ä½“è¯†åˆ«æœ€ç›¸å…³æ–‡æ¡£ç±»å‹ä»¥åŠç²¾ç¡®å®šä½å…³é”®æ®µè½çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡åˆ†ç¦»è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤æ¥å…‹æœä¸Šä¸‹æ–‡é™åˆ¶ã€‚FinAgentBench ä¸ºç†è§£ä»¥æ£€ç´¢ä¸ºæ ¸å¿ƒçš„å¤§è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„è¡Œä¸ºæä¾›äº†é‡åŒ–ä¾æ®ã€‚å®éªŒè¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶è¯æ˜æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼ˆfine-tuningï¼‰å¯ä»¥æ˜¾è‘—æå‡ agentic retrieval çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºç ”ç©¶å¤æ‚ã€ç‰¹å®šé¢†åŸŸé‡‘èä»»åŠ¡ä¸­çš„æ£€ç´¢é©±åŠ¨å‹å¤§è¯­è¨€æ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.14052v4",
      "published_date": "2025-08-07 22:15:22 UTC",
      "updated_date": "2025-10-03 17:35:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:00.701280+00:00"
    },
    {
      "arxiv_id": "2508.16589v1",
      "title": "ARL-Based Multi-Action Market Making with Hawkes Processes and Variable Volatility",
      "title_zh": "åŸºäº ARLã€Hawkes è¿‡ç¨‹ä¸å¯å˜æ³¢åŠ¨ç‡çš„å¤šåŠ¨ä½œåšå¸‚ç ”ç©¶",
      "authors": [
        "Ziyi Wang",
        "Carmine Ventre",
        "Maria Polukarov"
      ],
      "abstract": "We advance market-making strategies by integrating Adversarial Reinforcement Learning (ARL), Hawkes Processes, and variable volatility levels while also expanding the action space available to market makers (MMs). To enhance the adaptability and robustness of these strategies -- which can quote always, quote only on one side of the market or not quote at all -- we shift from the commonly used Poisson process to the Hawkes process, which better captures real market dynamics and self-exciting behaviors. We then train and evaluate strategies under volatility levels of 2 and 200. Our findings show that the 4-action MM trained in a low-volatility environment effectively adapts to high-volatility conditions, maintaining stable performance and providing two-sided quotes at least 92\\% of the time. This indicates that incorporating flexible quoting mechanisms and realistic market simulations significantly enhances the effectiveness of market-making strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ•´åˆAdversarial Reinforcement Learning (ARL)ã€Hawkes Processesä»¥åŠå¯å˜æ³¢åŠ¨ç‡æ°´å¹³ï¼Œå¹¶æ‰©å±•Market Maker (MM)çš„åŠ¨ä½œç©ºé—´ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„Multi-Action Market Makingç­–ç•¥ã€‚ä¸ºå¢å¼ºç­–ç•¥çš„é€‚åº”æ€§ä¸ç¨³å¥æ€§ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨Hawkes Processeså–ä»£ä¼ ç»Ÿçš„Poisson processï¼Œä»¥æ›´ç²¾å‡†åœ°æ•æ‰å¸‚åœºåŠ¨æ€ä¸­çš„è‡ªæ¿€è¡Œä¸º(self-exciting behaviors)ã€‚ç ”ç©¶åœ¨ä¸åŒæ³¢åŠ¨ç‡ç¯å¢ƒä¸‹å¯¹ç­–ç•¥è¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨ä½æ³¢åŠ¨ç‡ç¯å¢ƒä¸‹è®­ç»ƒçš„4-action MMèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”é«˜æ³¢åŠ¨ç‡æ¡ä»¶ï¼Œå¹¶åœ¨92%ä»¥ä¸Šçš„æ—¶é—´å†…ç»´æŒç¨³å®šçš„åŒé¢æŠ¥ä»·ã€‚å®éªŒè¯æ˜ï¼Œç»“åˆçµæ´»çš„æŠ¥ä»·æœºåˆ¶ä¸ç°å®çš„å¸‚åœºæ¨¡æ‹Ÿï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åšå¸‚ç­–ç•¥åœ¨å¤æ‚å¸‚åœºç¯å¢ƒä¸‹çš„æ‰§è¡Œæ•ˆèƒ½ã€‚",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "q-fin.TR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16589v1",
      "published_date": "2025-08-07 21:50:30 UTC",
      "updated_date": "2025-08-07 21:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:00.396464+00:00"
    },
    {
      "arxiv_id": "2508.16588v1",
      "title": "Robust Market Making: To Quote, or not To Quote",
      "title_zh": "é²æ£’åšå¸‚ï¼šæŠ¥ä»·ï¼ŒæŠ‘æˆ–ä¸æŠ¥ä»·",
      "authors": [
        "Ziyi Wang",
        "Carmine Ventre",
        "Maria Polukarov"
      ],
      "abstract": "Market making is a popular trading strategy, which aims to generate profit from the spread between the quotes posted at either side of the market. It has been shown that training market makers (MMs) with adversarial reinforcement learning allows to overcome the risks due to changing market conditions and to lead to robust performances. Prior work assumes, however, that MMs keep quoting throughout the trading process, but in practice this is not required, even for ``registered'' MMs (that only need to satisfy quoting ratios defined by the market rules). In this paper, we build on this line of work and enrich the strategy space of the MM by allowing to occasionally not quote or provide single-sided quotes. Towards this end, in addition to the MM agents that provide continuous bid-ask quotes, we have designed two new agents with increasingly richer action spaces. The first has the option to provide bid-ask quotes or refuse to quote. The second has the option to provide bid-ask quotes, refuse to quote, or only provide single-sided ask or bid quotes. We employ a model-driven approach to empirically compare the performance of the continuously quoting MM with the two agents above in various types of adversarial environments. We demonstrate how occasional refusal to provide bid-ask quotes improves returns and/or Sharpe ratios. The quoting ratios of well-trained MMs can basically meet any market requirements, reaching up to 99.9$\\%$ in some cases.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é²æ£’åšå¸‚(Market Making)ç­–ç•¥ï¼Œæ—¨åœ¨åˆ©ç”¨å¯¹æŠ—å¼ºåŒ–å­¦ä¹ (Adversarial Reinforcement Learning)æé«˜åšå¸‚å•†åœ¨å¤æ‚å¸‚åœºç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶å‡è®¾åšå¸‚å•†å¿…é¡»æŒç»­æŠ¥ä»·çš„å±€é™ï¼Œè¯¥è®ºæ–‡æ‰©å±•äº†æ™ºèƒ½ä½“çš„ç­–ç•¥ç©ºé—´ï¼Œå…è®¸å…¶åœ¨äº¤æ˜“è¿‡ç¨‹ä¸­é€‰æ‹©ä¸æŠ¥ä»·æˆ–ä»…æä¾›å•è¾¹æŠ¥ä»·(Single-sided quotes)ã€‚é€šè¿‡åœ¨å¤šç§å¯¹æŠ—ç¯å¢ƒä¸‹å¯¹æ¯”ä¸åŒæ™ºèƒ½ä½“çš„å®éªŒï¼Œç ”ç©¶è¯æ˜äº†å…è®¸å¶å°”æ‹’ç»æŠ¥ä»·èƒ½æ˜¾è‘—æå‡æŠ•èµ„å›æŠ¥ä¸å¤æ™®æ¯”ç‡(Sharpe Ratios)ã€‚æ­¤å¤–ï¼Œè¿™äº›ç»è¿‡è®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨æå‡æ”¶ç›Šçš„åŒæ—¶ï¼Œä»èƒ½æ»¡è¶³ä¸¥æ ¼çš„å¸‚åœºæŠ¥ä»·æ¯”ä¾‹è¦æ±‚ï¼Œéƒ¨åˆ†åœºæ™¯ä¸‹æŠ¥ä»·ç‡é«˜è¾¾99.9%ï¼Œå®ç°äº†äº¤æ˜“æ”¶ç›Šä¸ç›‘ç®¡åˆè§„çš„å¹³è¡¡ã€‚",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "q-fin.TR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16588v1",
      "published_date": "2025-08-07 21:49:24 UTC",
      "updated_date": "2025-08-07 21:49:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:56.483428+00:00"
    },
    {
      "arxiv_id": "2508.10024v1",
      "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
      "title_zh": "RTTCï¼šå¥–åŠ±å¼•å¯¼çš„åä½œå¼æµ‹è¯•æ—¶è®¡ç®—",
      "authors": [
        "J. Pablo MuÃ±oz",
        "Jinjie Yuan"
      ],
      "abstract": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the performance of Large Language Models (LLMs) at inference, leveraging strategies such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). However, the optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. To further mitigate redundant computation, we propose Query-State Caching, which enables the efficient reuse of historical query states at both retrieval and adaptation levels. Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†é˜¶æ®µé€šè¿‡Test-Time Compute (TTC)å¢å¼ºæ€§èƒ½æ—¶å­˜åœ¨çš„è®¡ç®—å¼€é”€å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†Reward-Guided Test-Time Compute (RTTC)æ¡†æ¶ã€‚RTTCé€šè¿‡é¢„è®­ç»ƒçš„Reward Modelä¸ºæ¯ä¸ªæŸ¥è¯¢è‡ªé€‚åº”é€‰æ‹©æœ€æœ‰æ•ˆçš„TTCç­–ç•¥ï¼Œæ—¨åœ¨å¤šæ ·åŒ–é¢†åŸŸå’Œä»»åŠ¡ä¸­æœ€å¤§åŒ–ä¸‹æ¸¸å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å¸ƒå¼server-clientæ¶æ„ï¼Œä»…åœ¨å¿…è¦æ—¶äºå®¢æˆ·ç«¯è®¾å¤‡åº”ç”¨Retrieval-Augmented Generation (RAG)æˆ–è½»é‡çº§å¾®è°ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘é‡å¤è®¡ç®—ï¼Œç ”ç©¶è€…æå‡ºäº†Query-State Cachingæœºåˆ¶ï¼Œå®ç°äº†æ£€ç´¢å’Œé€‚é…å±‚çº§å†å²æŸ¥è¯¢çŠ¶æ€çš„é«˜æ•ˆé‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRTTCåœ¨å¤šä¸ªLLMså’ŒåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºåŸå§‹çš„RAGæˆ–Test-Time Training (TTT)ï¼ŒéªŒè¯äº†å¥–åŠ±å¼•å¯¼ç­–ç•¥é€‰æ‹©åœ¨å®ç°å¯æ‰©å±•ã€é«˜æ€§èƒ½æ¨¡å‹é€‚é…æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10024v1",
      "published_date": "2025-08-07 21:18:52 UTC",
      "updated_date": "2025-08-07 21:18:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:47.684652+00:00"
    },
    {
      "arxiv_id": "2508.05855v1",
      "title": "Safety of Embodied Navigation: A Survey",
      "title_zh": "å…·èº«å¯¼èˆªå®‰å…¨æ€§ç»¼è¿°",
      "authors": [
        "Zixia Wang",
        "Jia Hu",
        "Ronghui Mu"
      ],
      "abstract": "As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº« AI (Embodied AI) å‘å±•èƒŒæ™¯ä¸‹å…·èº«å¯¼èˆª (Embodied Navigation) çš„å®‰å…¨é—®é¢˜è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è¿›æ­¥ï¼Œå…·èº«æ™ºèƒ½ä½“åœ¨åŠ¨æ€æœªçŸ¥ç¯å¢ƒä¸­ç§»åŠ¨æ—¶çš„æ„ŸçŸ¥ä¸äº¤äº’èƒ½åŠ›ä¸æ–­æå‡ï¼Œä½†å…¶å®é™…éƒ¨ç½²ä¸­çš„å®‰å…¨æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»æ”»å‡»ç­–ç•¥ (Attack Strategies)ã€é˜²å¾¡æœºåˆ¶ (Defense Mechanisms) å’Œè¯„ä¼°æ–¹æ³• (Evaluation Methodologies) ç­‰å¤šä¸ªç»´åº¦å¯¹ç°æœ‰å®‰å…¨æŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚æ­¤å¤–ï¼Œç»¼è¿°è¿˜æ¢è®¨äº†ç°æœ‰çš„ç¼“è§£æŠ€æœ¯ã€è¯„ä¼°æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†åŒ…æ‹¬æ½œåœ¨æ”»å‡»æ‰‹æ®µã€å¯é è¯„ä¼°æŠ€æœ¯åŠéªŒè¯æ¡†æ¶åœ¨å†…çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥å·¥ä½œæ—¨åœ¨ä¸ºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„å…·èº«å¯¼èˆªç³»ç»Ÿæä¾›æŒ‡å¯¼ï¼Œä»è€Œæå‡ç¤¾ä¼šå®‰å…¨ä¸å·¥ä¸šæ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05855v1",
      "published_date": "2025-08-07 21:09:48 UTC",
      "updated_date": "2025-08-07 21:09:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:45:55.684551+00:00"
    },
    {
      "arxiv_id": "2508.05846v1",
      "title": "Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems",
      "title_zh": "è¿ˆå‘é€æ˜çš„ä¼¦ç†äººå·¥æ™ºèƒ½ï¼šå¯ä¿¡æœºå™¨äººç³»ç»Ÿè·¯çº¿å›¾",
      "authors": [
        "Ahmad Farooq",
        "Kamran Iqbal"
      ],
      "abstract": "As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äººå·¥æ™ºèƒ½(AI)ä¸æœºå™¨äººæŠ€æœ¯æ™®åŠèƒŒæ™¯ä¸‹ï¼Œé€æ˜åº¦å¯¹äºæ„å»ºå¯ä¿¡ä¸”ç¬¦åˆä¼¦ç†çš„ç³»ç»Ÿçš„æ ¸å¿ƒä½œç”¨ã€‚æ–‡ç« å¼ºè°ƒé€æ˜åº¦æ˜¯å®ç°é—®è´£åˆ¶(accountability)ã€çŸ¥æƒ…åŒæ„(informed consent)åŠä¼¦ç†ç®—æ³•è°ƒè¯•çš„åŸºç¡€ã€‚ä½œè€…åˆ†æäº†å®ç°é€æ˜åº¦çš„æŠ€æœ¯ã€ä¼¦ç†ä¸å®è·µæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åŒ…æ‹¬æ ‡å‡†åŒ–æŒ‡æ ‡ã€å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)æŠ€æœ¯å’Œç”¨æˆ·å‹å¥½ç•Œé¢åœ¨å†…çš„æ–°é¢–å¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå°†æŠ€æœ¯å®æ–½ä¸ä¼¦ç†è€ƒé‡ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œç‰¹åˆ«é’ˆå¯¹åŠ¨æ€ç°å®ç¯å¢ƒä¸‹çš„é€æ˜åº¦å®ç°æŒ‘æˆ˜è¿›è¡Œäº†åˆ†æã€‚é€šè¿‡å°†é€æ˜åº¦å®šä½ä¸ºä¼¦ç†AIç³»ç»Ÿè®¾è®¡çš„åŸºçŸ³ï¼Œè¯¥è·¯çº¿å›¾ä¸ºæå‡å…¬ä¼—ä¿¡ä»»ã€å®Œå–„ç›‘ç®¡æ”¿ç­–ä»¥åŠæœªæ¥çš„è´Ÿè´£ä»»æœºå™¨äººç ”ç©¶æä¾›äº†æ˜ç¡®æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CY",
      "comment": "Published in the Proceedings of the 2025 3rd International Conference on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.05846v1",
      "published_date": "2025-08-07 20:49:16 UTC",
      "updated_date": "2025-08-07 20:49:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:02.291857+00:00"
    },
    {
      "arxiv_id": "2508.05838v1",
      "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction",
      "title_zh": "èåˆè§†è§‰åŸºç¡€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ä»¥å¢å¼ºç‰©ä½“äº¤äº’èƒ½åŠ›",
      "authors": [
        "Ahmad Farooq",
        "Kamran Iqbal"
      ],
      "abstract": "This paper presents a novel approach that integrates vision foundation models with reinforcement learning to enhance object interaction capabilities in simulated environments. By combining the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the AI2-THOR simulation environment, we enable the agent to perceive and interact with objects more effectively. Our comprehensive experiments, conducted across four diverse indoor kitchen settings, demonstrate significant improvements in object interaction success rates and navigation efficiency compared to a baseline agent without advanced perception. The results show a 68% increase in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency. These findings highlight the potential of integrating foundation models with reinforcement learning for complex robotic tasks, paving the way for more sophisticated and capable autonomous agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°† Vision Foundation Models ä¸ Reinforcement Learning ç›¸ç»“åˆçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„ç‰©ä½“äº¤äº’èƒ½åŠ›ã€‚é€šè¿‡åœ¨ AI2-THOR æ¨¡æ‹Ÿç¯å¢ƒä¸­æ•´åˆ Segment Anything Model (SAM) å’Œ YOLOv5 ä¸ Proximal Policy Optimization (PPO) æ™ºèƒ½ä½“ï¼Œç ”ç©¶å®ç°äº†å¯¹ç‰©ä½“çš„æ›´é«˜æ•ˆæ„ŸçŸ¥ä¸äº¤äº’ã€‚åœ¨å››ç§å®¤å†…å¨æˆ¿åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº¤äº’æˆåŠŸç‡å’Œå¯¼èˆªæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¹³å‡ç´¯ç§¯å¥–åŠ±æå‡äº† 68%ï¼Œç‰©ä½“äº¤äº’æˆåŠŸç‡æé«˜äº† 52.5%ï¼Œå¯¼èˆªæ•ˆç‡å¢åŠ äº† 33%ã€‚è¿™äº›å‘ç°è¯æ˜äº†å°†åŸºç¡€æ¨¡å‹åº”ç”¨äºå¤æ‚æœºå™¨äººä»»åŠ¡çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å…ˆè¿›çš„ Autonomous Agents å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Published in the Proceedings of the 2025 3rd International Conference on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2508.05838v1",
      "published_date": "2025-08-07 20:29:01 UTC",
      "updated_date": "2025-08-07 20:29:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:25.990039+00:00"
    },
    {
      "arxiv_id": "2508.05799v1",
      "title": "AI-Guided Exploration of Large-Scale Codebases",
      "title_zh": "AIå¼•å¯¼çš„å¤§è§„æ¨¡ä»£ç åº“æ¢ç´¢",
      "authors": [
        "Yoseph Berhanu Alebachew"
      ],
      "abstract": "Understanding large-scale, complex software systems is a major challenge for developers, who spend a significant portion of their time on program comprehension. Traditional tools such as static visualizations and reverse engineering techniques provide structural insights but often lack interactivity, adaptability, and integration with contextual information. Recent advancements in large language models (LLMs) offer new opportunities to enhance code exploration workflows, yet their lack of grounding and integration with structured views limits their effectiveness. This work introduces a hybrid approach that integrates deterministic reverse engineering with LLM-guided, intent-aware visual exploration. The proposed system combines UML-based visualization, dynamic user interfaces, historical context, and collaborative features into an adaptive tool for code comprehension. By interpreting user queries and interaction patterns, the LLM helps developers navigate and understand complex codebases more effectively. A prototype implementation for Java demonstrates the feasibility of this approach. Future work includes empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM interaction models. This research lays the groundwork for intelligent, interactive environments that align with developer cognition and collaborative workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€å‘è€…åœ¨ç†è§£å¤§è§„æ¨¡å¤æ‚è½¯ä»¶ç³»ç»Ÿæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å°†ç¡®å®šæ€§é€†å‘å·¥ç¨‹ (Reverse Engineering) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) å¼•å¯¼çš„æ„å›¾æ„ŸçŸ¥è§†è§‰æ¢ç´¢ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†åŸºäº UML çš„å¯è§†åŒ–ã€åŠ¨æ€ç”¨æˆ·ç•Œé¢ã€å†å²ä¸Šä¸‹æ–‡å’Œåä½œåŠŸèƒ½ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿé™æ€å·¥å…·ç¼ºä¹äº’åŠ¨æ€§å’Œ LLM ç¼ºä¹ç»“æ„åŒ–æ”¯æ’‘çš„å±€é™æ€§ã€‚é€šè¿‡è§£æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾å’Œäº¤äº’æ¨¡å¼ï¼ŒLLM èƒ½å¤Ÿæœ‰æ•ˆååŠ©å¼€å‘è€…åœ¨å¤æ‚ä»£ç åº“ä¸­è¿›è¡Œå¯¼èˆªå’Œç†è§£ã€‚ç ”ç©¶å›¢é˜Ÿä¸º Java è¯­è¨€å¼€å‘äº†åŸå‹ç³»ç»Ÿï¼ŒéªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨æé«˜ç¨‹åºç†è§£ (Program Comprehension) æ•ˆç‡æ–¹é¢çš„å¯è¡Œæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºç¬¦åˆå¼€å‘è€…è®¤çŸ¥ä¹ æƒ¯çš„æ™ºèƒ½ã€åä½œå¼äº¤äº’ç¯å¢ƒå¥ å®šäº†åŸºç¡€ï¼Œæœªæ¥è®¡åˆ’å°†å…¶æ‰©å±•è‡³å¤šè¯­è¨€ç³»ç»Ÿå¹¶ä¼˜åŒ– GUI é©±åŠ¨çš„äº¤äº’æ¨¡å‹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05799v1",
      "published_date": "2025-08-07 19:15:37 UTC",
      "updated_date": "2025-08-07 19:15:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:22.089834+00:00"
    },
    {
      "arxiv_id": "2508.05792v1",
      "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making",
      "title_zh": "å…¨é¢å¯è§£é‡Šäººå·¥æ™ºèƒ½ (H-XAI)ï¼šåœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„å†³ç­–ä¸­å°†é€æ˜åº¦æ‰©å±•è‡³å¼€å‘è€…ä»¥å¤–",
      "authors": [
        "Kausik Lakkaraju",
        "Siva Likitha Valluru",
        "Biplav Srivastava"
      ],
      "abstract": "Current eXplainable AI (XAI) methods largely serve developers, often focusing on justifying model outputs rather than supporting diverse stakeholder needs. A recent shift toward Evaluative AI reframes explanation as a tool for hypothesis testing, but still focuses primarily on operational organizations. We introduce Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods with traditional XAI methods to support explanation as an interactive, multi-method process. H-XAI allows stakeholders to ask a series of questions, test hypotheses, and compare model behavior against automatically constructed random and biased baselines. It combines instance-level and global explanations, adapting to each stakeholder's goals, whether understanding individual decisions, assessing group-level bias, or evaluating robustness under perturbations. We demonstrate the generality of our approach through two case studies spanning six scenarios: binary credit risk classification and financial time-series forecasting. H-XAI fills critical gaps left by existing XAI methods by combining causal ratings and post-hoc explanations to answer stakeholder-specific questions at both the individual decision level and the overall model level.",
      "tldr_zh": "å½“å‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•ä¸»è¦æœåŠ¡äºå¼€å‘è€…ï¼Œå¾€å¾€ä¾§é‡äºè¯æ˜æ¨¡å‹è¾“å‡ºçš„åˆç†æ€§ï¼Œè€Œå¿½è§†äº†ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚è¯¥ç ”ç©¶æå‡ºäº†å…¨æ–¹ä½å¯è§£é‡Šäººå·¥æ™ºèƒ½(Holistic-XAI, H-XAI)æ¡†æ¶ï¼Œæ—¨åœ¨å°†é€æ˜åº¦æ‰©å±•åˆ°å¼€å‘è€…ä¹‹å¤–çš„AIé©±åŠ¨å†³ç­–é¢†åŸŸã€‚H-XAIæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒå°†å› æœè¯„åˆ†æ–¹æ³•(causal rating methods)ä¸ä¼ ç»Ÿçš„XAIæ–¹æ³•ç›¸ç»“åˆï¼Œæ”¯æŒäº¤äº’å¼çš„å¤šæ–¹æ³•è§£é‡Šæµç¨‹ã€‚è¯¥æ¡†æ¶å…è®¸åˆ©ç›Šç›¸å…³è€…æå‡ºä¸€ç³»åˆ—é—®é¢˜ã€æµ‹è¯•å‡è®¾ï¼Œå¹¶è‡ªåŠ¨æ„å»ºéšæœºæˆ–åç½®çš„åŸºå‡†çº¿(baselines)æ¥å¯¹æ¯”æ¨¡å‹è¡Œä¸ºã€‚å®ƒèåˆäº†å®ä¾‹çº§(instance-level)å’Œå…¨å±€è§£é‡Š(global explanations)ï¼Œèƒ½å¤Ÿæ ¹æ®åˆ©ç›Šç›¸å…³è€…çš„ç›®æ ‡çµæ´»è°ƒæ•´ï¼Œæ¶µç›–äº†ä»ç†è§£ä¸ªäººå†³ç­–åˆ°è¯„ä¼°ç¾¤ä½“åå·®ä¸é²æ£’æ€§çš„å¤šç§åœºæ™¯ã€‚é€šè¿‡ä¿¡ç”¨é£é™©åˆ†ç±»å’Œé‡‘èæ—¶é—´åºåˆ—é¢„æµ‹ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§ã€‚H-XAIé€šè¿‡ç»“åˆå› æœè¯„åˆ†å’Œäº‹åè§£é‡Š(post-hoc explanations)ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç°æœ‰XAIæ–¹æ³•åœ¨æ»¡è¶³ç‰¹å®šåˆ©ç›Šç›¸å…³è€…éœ€æ±‚æ–¹é¢çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05792v1",
      "published_date": "2025-08-07 19:06:08 UTC",
      "updated_date": "2025-08-07 19:06:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:22.551918+00:00"
    },
    {
      "arxiv_id": "2508.05791v1",
      "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data",
      "title_zh": "ä»éç†æƒ³ä¿¡å·åˆ°å¯ä¿¡ç»“æ„ï¼šåŸºäºå¼‚æ„ä¸”å¯é æ€§å‚å·®çš„å…¬ç”¨äº‹ä¸šæ•°æ®çš„ç½®ä¿¡åº¦æ„ŸçŸ¥æ¨ç†",
      "authors": [
        "Haoran Li",
        "Lihao Mai",
        "Muhao Guo",
        "Jiaqi Wu",
        "Yang Weng",
        "Yannan Sun",
        "Ce Jimmy Liu"
      ],
      "abstract": "Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality. In this work, developed in collaboration with Oncor Electric Delivery, we propose a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. We observe that distribution topology is fundamentally governed by two complementary dimensions: the spatial layout of physical infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the system in the signal domain (e.g., voltage time series). When jointly leveraged, these dimensions support a complete and physically coherent reconstruction of network connectivity. To address the challenge of uneven data quality without compromising observability, we introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. This soft handling of uncertainty is tightly coupled with hard enforcement of physical feasibility: we embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process. Together, these components ensure that inference is both uncertainty-aware and structurally valid, enabling rapid convergence to actionable, trustworthy topologies under real-world deployment conditions. The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸Oncor Electric Deliveryåˆä½œï¼Œæå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ•´åˆå¼‚æ„æ•°æ®(Heterogeneous data)æ¥é‡æ„å¯ä¿¡çš„é…ç”µç½‘æ‹“æ‰‘ç»“æ„(Distribution grid topology)ã€‚æ¡†æ¶ç»“åˆäº†ç‰©ç†åŸºç¡€è®¾æ–½çš„ç©ºé—´å¸ƒå±€(GISå’Œèµ„äº§å…ƒæ•°æ®)ä»¥åŠä¿¡å·åŸŸçš„åŠ¨æ€è¡Œä¸º(ç”µå‹æ—¶é—´åºåˆ—)ä¸¤ä¸ªäº’è¡¥ç»´åº¦ï¼Œä»¥å®ç°ç‰©ç†ç›¸å¹²çš„ç½‘ç»œè¿æ¥é‡æ„ã€‚ä¸ºäº†åº”å¯¹æ•°æ®è´¨é‡ä¸ä¸€çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†ç½®ä¿¡åº¦æ„ŸçŸ¥æ¨ç†æœºåˆ¶(Confidence-aware inference)ï¼Œåœ¨åˆ©ç”¨ä¸å®Œå–„è¾“å…¥ä¿¡æ¯çš„åŒæ—¶é‡åŒ–æ¯ä¸ªæ¨æ–­è¿æ¥çš„å¯é æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å°†å˜å‹å™¨å®¹é‡é™åˆ¶å’Œå¾„å‘æ‹“æ‰‘è¦æ±‚ç­‰ç‰©ç†å¯è¡Œæ€§çº¦æŸ(Physical feasibility constraints)ç›´æ¥åµŒå…¥å­¦ä¹ è¿‡ç¨‹ï¼Œç¡®ä¿æ¨æ–­ç»“æœåœ¨ç»“æ„ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åœ¨åŒ…å«8000å¤šä¸ªç”µè¡¨çš„å®é™…ç”µåŠ›æ•°æ®éªŒè¯ä¸­ï¼Œè¯¥æ¡†æ¶å®ç°äº†è¶…è¿‡95%çš„æ‹“æ‰‘é‡æ„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç½®ä¿¡åº¦æ ¡å‡†(Confidence calibration)å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œä¸ºç°å®éƒ¨ç½²æ¡ä»¶ä¸‹å¿«é€Ÿç”Ÿæˆå¯æ“ä½œã€å¯ä¿¡çš„æ‹“æ‰‘ç»“æ„æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.05791v1",
      "published_date": "2025-08-07 19:05:19 UTC",
      "updated_date": "2025-08-07 19:05:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:21.363505+00:00"
    },
    {
      "arxiv_id": "2508.05783v1",
      "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks",
      "title_zh": "é¢„è®­ç»ƒ MRI Transformer åœ¨è„‘æˆåƒä»»åŠ¡ä¸­çš„å°‘æ ·æœ¬éƒ¨ç½²",
      "authors": [
        "Mengyu Li",
        "Guoyao Shen",
        "Chad W. Farris",
        "Xin Zhang"
      ],
      "abstract": "Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºåœ¨å¤§è„‘å½±åƒä»»åŠ¡ä¸­ Few-Shot éƒ¨ç½²é¢„è®­ç»ƒ MRI Transformers çš„å®ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å½±åƒæ ‡æ³¨æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„å±€é™æ€§ã€‚é€šè¿‡åœ¨åŒ…å«è¶…è¿‡ 3100 ä¸‡ä¸ªåˆ‡ç‰‡çš„å¤§è§„æ¨¡å¤šé˜Ÿåˆ— MRI æ•°æ®é›†ä¸Šé‡‡ç”¨ Masked Autoencoder (MAE) é¢„è®­ç»ƒç­–ç•¥ï¼Œè¯¥æ¡†æ¶è·å¾—äº†åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†é—´å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„æ½œåœ¨è¡¨ç¤ºã€‚é’ˆå¯¹åˆ†ç±»ç­‰é«˜å±‚ä»»åŠ¡ï¼Œå†·å†»çš„ MAE ç¼–ç å™¨ç»“åˆè½»é‡åŒ–çº¿æ€§å¤´åœ¨ MRI åºåˆ—è¯†åˆ«ä¸­å®ç°äº†æå°‘ç›‘ç£ä¸‹çš„ SOTA å‡†ç¡®ç‡ã€‚é’ˆå¯¹åˆ†å‰²ç­‰åº•å±‚ä»»åŠ¡ï¼Œç ”ç©¶å¼•å…¥äº† MAE-FUnet æ··åˆæ¶æ„ï¼Œé€šè¿‡èåˆå¤šå°ºåº¦ CNN ç‰¹å¾ä¸é¢„è®­ç»ƒ MAE åµŒå…¥ï¼Œåœ¨æœ‰é™æ•°æ®ä¸‹çš„é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²ä»»åŠ¡ä¸­å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚å®šé‡ä¸å®šæ€§è¯„ä¼°è¯æ˜äº†è¯¥æ¡†æ¶å…·å¤‡å‡ºè‰²çš„é«˜æ•ˆæ€§ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºä½èµ„æºä¸´åºŠç¯å¢ƒå’Œå¹¿æ³›çš„ç¥ç»å½±åƒåº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 8 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.05783v1",
      "published_date": "2025-08-07 18:53:28 UTC",
      "updated_date": "2025-08-07 18:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:26.249743+00:00"
    },
    {
      "arxiv_id": "2508.05776v1",
      "title": "Whither symbols in the era of advanced neural networks?",
      "title_zh": "é«˜çº§ç¥ç»ç½‘ç»œæ—¶ä»£çš„ç¬¦å·ï¼šä½•å»ä½•ä»ï¼Ÿ",
      "authors": [
        "Thomas L. Griffiths",
        "Brenden M. Lake",
        "R. Thomas McCoy",
        "Ellie Pavlick",
        "Taylor W. Webb"
      ],
      "abstract": "Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å…ˆè¿›ç¥ç»ç½‘ç»œ(neural networks)æ—¶ä»£ï¼Œç¬¦å·(symbols)åœ¨ç†è§£äººç±»æ€ç»´ä¸­çš„åœ°ä½ä¸ä½œç”¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶äººç±»æ€ç»´ç»„åˆæƒ³æ³•ã€äº§ç”Ÿæ–°å¥‡æ„Ÿå’Œå¿«é€Ÿå­¦ä¹ çš„èƒ½åŠ›é•¿æœŸè¢«è§†ä¸ºç¬¦å·ç³»ç»Ÿ(symbolic systems)å­˜åœ¨çš„è¯æ®ï¼Œä½†ç°ä»£ç¥ç»ç½‘ç»œå·²å±•ç°å‡ºç±»ä¼¼èƒ½åŠ›ï¼Œè¿™å‰Šå¼±äº†äººç±»è®¤çŸ¥è¿‡ç¨‹å’Œè¡¨å¾å¿…ç„¶æ˜¯ç¬¦å·åŒ–çš„ä¼ ç»Ÿè®ºç‚¹ã€‚ç„¶è€Œï¼Œç¥ç»ç½‘ç»œé€šå¸¸åœ¨ç”±ç¬¦å·ç³»ç»Ÿç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒï¼Œè¯´æ˜ç¬¦å·ç³»ç»Ÿåœ¨åˆ»ç”»äººç±»æ€ç»´éœ€è§£å†³çš„æŠ½è±¡é—®é¢˜ä¸­ä»å…·æœ‰å…³é”®æ„ä¹‰ã€‚åŸºäºæ­¤ï¼Œè¯¥ç ”ç©¶ä¸ºäººç±»æ€æƒ³çš„ç¬¦å·åŸºç¡€æå‡ºäº†å…¨æ–°çš„ç ”ç©¶è®®ç¨‹(research agenda)ï¼Œé‡æ–°å®šä¹‰äº†è¿æ¥ä¸»ä¹‰æ¨¡å‹ä¸ç¬¦å·é€»è¾‘åœ¨è®¤çŸ¥ç§‘å­¦ç ”ç©¶ä¸­çš„å…³ç³»ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05776v1",
      "published_date": "2025-08-07 18:42:55 UTC",
      "updated_date": "2025-08-07 18:42:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:33.751585+00:00"
    },
    {
      "arxiv_id": "2508.05766v1",
      "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference",
      "title_zh": "åŸºäºè¯­è¨€ä»‹å¯¼ä¸»åŠ¨æ¨ç†çš„å†…ç”Ÿå®‰å…¨ AGI æ¡†æ¶",
      "authors": [
        "Bo Wen"
      ],
      "abstract": "This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations. We present an architecture where safety guarantees are integrated into the system's core design through transparent belief representations and hierarchical value alignment. Our framework leverages natural language as a medium for representing and manipulating beliefs, enabling direct human oversight while maintaining computational tractability. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å°†ä¸»åŠ¨æ¨ç† (Active Inference) åŸç†ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å¼€å‘æœ¬è´¨å®‰å…¨çš„äººå·¥é€šç”¨æ™ºèƒ½ (AGI)ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„ AI å®‰å…¨æ–¹æ³•è¿‡äºä¾§é‡äº‹åè§£é‡Šæ€§ (post-hoc interpretability) å’Œå¥–åŠ±å·¥ç¨‹ (reward engineering)ï¼Œå­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚ä¸ºæ­¤ï¼Œè¯¥æ¶æ„é€šè¿‡é€æ˜çš„ä¿¡ä»°è¡¨å¾ (belief representations) å’Œåˆ†å±‚ä»·å€¼å¯¹é½ (hierarchical value alignment) å°†å®‰å…¨ä¿éšœé›†æˆäºç³»ç»Ÿæ ¸å¿ƒï¼Œå¹¶åˆ©ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºå¤„ç†ä¿¡ä»°çš„åª’ä»‹ï¼Œåœ¨ä¿æŒè®¡ç®—æ˜“å¤„ç†æ€§çš„åŒæ—¶å®ç°ç›´æ¥çš„äººç±»ç›‘ç£ã€‚ç³»ç»Ÿé‡‡ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (multi-agent system) æ¶æ„ï¼Œæ™ºèƒ½ä½“æ ¹æ®ä¸»åŠ¨æ¨ç†åŸåˆ™è‡ªç»„ç»‡ï¼Œå¹¶é€šè¿‡åˆ†å±‚é©¬å°”å¯å¤«æ¯¯ (Markov blankets) ä¼ é€’åå¥½ä¸å®‰å…¨çº¦æŸã€‚å…·ä½“å®‰å…¨æœºåˆ¶åŒ…æ‹¬åœ¨è‡ªç„¶è¯­è¨€ä¸­æ˜¾å¼åˆ†ç¦»ä¿¡ä»°ä¸åå¥½ã€é€šè¿‡èµ„æºæ„ŸçŸ¥çš„è‡ªç”±èƒ½æœ€å°åŒ– (free energy minimization) å®ç°æœ‰é™ç†æ€§ (bounded rationality)ï¼Œä»¥åŠæ¨¡å—åŒ–æ™ºèƒ½ä½“ç»“æ„çš„ç»„åˆå®‰å…¨æ€§ (compositional safety)ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†åŸºäºæŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ (ARC) åŸºå‡†çš„å®éªŒè®®ç¨‹ï¼Œä¸ºæ„å»ºæœ¬è´¨å®‰å…¨è€Œéäº‹åè¡¥æ•‘çš„ AGI æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY",
        "nlin.AO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05766v1",
      "published_date": "2025-08-07 18:28:54 UTC",
      "updated_date": "2025-08-07 18:28:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:41.085162+00:00"
    },
    {
      "arxiv_id": "2508.05755v1",
      "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models",
      "title_zh": "UnGuideï¼šåŸºäº LoRA å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æœºå™¨é—å¿˜",
      "authors": [
        "Agnieszka Polowczyk",
        "Alicja Polowczyk",
        "Dawid Malarz",
        "Artur Kasymov",
        "Marcin Mazur",
        "Jacek Tabor",
        "PrzemysÅ‚aw Spurek"
      ],
      "abstract": "Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.",
      "tldr_zh": "é’ˆå¯¹å¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆå›¾åƒ(text-to-image)æ‰©æ•£æ¨¡å‹å¯èƒ½è¢«æ»¥ç”¨äº§ç”Ÿæœ‰å®³å†…å®¹çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨å¸è½½(machine unlearning)æŠ€æœ¯ï¼Œæ—¨åœ¨ä¸æŸå®³æ¨¡å‹æ•´ä½“æ€§èƒ½çš„å‰æä¸‹ç§»é™¤ç‰¹å®šçŸ¥è¯†ã€‚è™½ç„¶ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation, LoRA)æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„æœ‰æ•ˆå¾®è°ƒæ‰‹æ®µï¼Œä½†å…¶å¸¸ä¼šæ„å¤–æ”¹å˜æ— å…³å†…å®¹å¹¶é™ä½å›¾åƒä¿çœŸåº¦ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†UnGuideæ–¹æ³•ï¼Œé€šè¿‡ä¸€ç§åä¸ºUnGuidanceçš„åŠ¨æ€æ¨ç†æœºåˆ¶ï¼Œåˆ©ç”¨åˆ†ç±»å™¨æ— å…³å¼•å¯¼(Classifier-Free Guidance, CFG)å¯¹å¸è½½è¿‡ç¨‹è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚è¯¥æœºåˆ¶æ ¹æ®å»å™ªè¿‡ç¨‹åˆå§‹æ­¥æ•°çš„ç¨³å®šæ€§æ¥è°ƒèŠ‚å¼•å¯¼å°ºåº¦ï¼Œä½¿LoRAé€‚é…å™¨ä»…åœ¨å¤„ç†éœ€æ“¦é™¤æ¦‚å¿µæ—¶å‘æŒ¥ä¸»å¯¼ä½œç”¨ï¼Œè€Œåœ¨å¤„ç†æ— å…³æç¤ºæ—¶ç”±åŸºç¡€æ¨¡å‹ä¸»å¯¼ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒUnGuideåœ¨ç‰©ä½“æ“¦é™¤å’Œæ˜¾å¼å†…å®¹ç§»é™¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å®ç°å—æ§æ¦‚å¿µç§»é™¤çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿ç•™äº†æ‰©æ•£æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05755v1",
      "published_date": "2025-08-07 18:12:03 UTC",
      "updated_date": "2025-08-07 18:12:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:46.648787+00:00"
    },
    {
      "arxiv_id": "2508.09186v2",
      "title": "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System",
      "title_zh": "RL-MoEï¼šæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„ä¸€ç§åŸºäºå›¾åƒçš„éšç§ä¿æŠ¤æ–¹æ³•",
      "authors": [
        "Abdolazim Rezaei",
        "Mehdi Sookhak",
        "Mahboobeh Haghparast"
      ],
      "abstract": "The proliferation of AI-powered cameras in Intelligent Transportation Systems (ITS) creates a severe conflict between the need for rich visual data and the right to privacy. Existing privacy-preserving methods, such as blurring or encryption, are often insufficient due to creating an undesirable trade-off where either privacy is compromised against advanced reconstruction attacks or data utility is critically degraded. To resolve this challenge, we propose RL-MoE, a novel framework that transforms sensitive visual data into privacy-preserving textual descriptions, eliminating the need for direct image transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture for nuanced, multi-aspect scene decomposition with a Reinforcement Learning (RL) agent that optimizes the generated text for a dual objective of semantic accuracy and privacy preservation. Extensive experiments demonstrate that RL-MoE provides superior privacy protection, reducing the success rate of replay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously generating richer textual content than baseline methods. Our work provides a practical and scalable solution for building trustworthy AI systems in privacy-sensitive domains, paving the way for more secure smart city and autonomous vehicle networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½äº¤é€šç³»ç»Ÿ(ITS)ä¸­è§†è§‰æ•°æ®åˆ©ç”¨ä¸éšç§ä¿æŠ¤ä¹‹é—´çš„å†²çªï¼Œæå‡ºäº†RL-MoEæ¡†æ¶ï¼Œé€šè¿‡å°†æ•æ„Ÿè§†è§‰æ•°æ®è½¬åŒ–ä¸ºéšç§ä¿æŠ¤çš„æ–‡æœ¬æè¿°ï¼Œæ¶ˆé™¤äº†ç›´æ¥ä¼ è¾“å›¾åƒçš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸“å®¶æ··åˆ(Mixture-of-Experts)æ¶æ„è¿›è¡Œå¤šç»´åº¦åœºæ™¯åˆ†è§£ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“ä¼˜åŒ–ç”Ÿæˆæ–‡æœ¬ï¼Œæ—¨åœ¨å®ç°è¯­ä¹‰å‡†ç¡®æ€§ä¸éšç§ä¿æŠ¤çš„åŒé‡å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-MoEåœ¨CFP-FPæ•°æ®é›†ä¸Šå°†é‡æ”¾æ”»å‡»çš„æˆåŠŸç‡é™ä½è‡³9.4%ï¼ŒåŒæ—¶ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹æ¯”åŸºçº¿æ–¹æ³•æ›´ä¸°å¯Œã€‚è¿™é¡¹å·¥ä½œä¸ºéšç§æ•æ„Ÿé¢†åŸŸæ„å»ºå¯ä¿¡AIç³»ç»Ÿæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ›´å®‰å…¨çš„æ™ºæ…§åŸå¸‚å’Œè‡ªåŠ¨é©¾é©¶ç½‘ç»œå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09186v2",
      "published_date": "2025-08-07 18:07:54 UTC",
      "updated_date": "2025-08-15 04:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:46:57.379565+00:00"
    },
    {
      "arxiv_id": "2508.09185v3",
      "title": "A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality",
      "title_zh": "å¢å¼ºç°å®ä¸­å¯è§£é‡Šè®¤çŸ¥æ”»å‡»æ£€æµ‹çš„ç¥ç»ç¬¦å·æ¡†æ¶",
      "authors": [
        "Rongqian Chen",
        "Allison Andreyev",
        "Yanming Xiu",
        "Joshua Chilukuri",
        "Shunav Sen",
        "Mahdi Imani",
        "Bin Li",
        "Maria Gorlatova",
        "Gang Tan",
        "Tian Lan"
      ],
      "abstract": "Augmented Reality (AR) enriches human perception by overlaying virtual elements onto the physical world. However, this tight coupling between virtual and real content makes AR vulnerable to cognitive attacks: manipulations that distort users' semantic understanding of the environment. Existing detection methods largely focus on visual inconsistencies at the pixel or image level, offering limited semantic reasoning or interpretability. To address these limitations, we introduce CADAR, a neuro-symbolic framework for cognitive attack detection in AR that integrates neural and symbolic reasoning. CADAR fuses multimodal vision-language representations from pre-trained models into a perception graph that captures objects, relations, and temporal contextual salience. Building on this structure, a particle-filter-based statistical reasoning module infers anomalies in semantic dynamics to reveal cognitive attacks. This combination provides both the adaptability of modern vision-language models and the interpretability of probabilistic symbolic reasoning. Preliminary experiments on an AR cognitive-attack dataset demonstrate consistent advantages over existing approaches, highlighting the potential of neuro-symbolic methods for robust and interpretable AR security.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¢å¼ºç°å®(Augmented Reality, AR)ä¸­é€šè¿‡æ‰­æ›²è¯­ä¹‰ç†è§£è¿›è¡Œçš„è®¤çŸ¥æ”»å‡»(cognitive attacks)ï¼Œæå‡ºäº†åä¸ºCADARçš„ç¥ç»ç¬¦å·(neuro-symbolic)æ£€æµ‹æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä»…å…³æ³¨åƒç´ æˆ–å›¾åƒçº§è§†è§‰ä¸ä¸€è‡´è€Œç¼ºä¹è¯­ä¹‰æ¨ç†å’Œå¯è§£é‡Šæ€§çš„å±€é™ï¼ŒCADARå°†é¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰-è¯­è¨€å¤šæ¨¡æ€è¡¨ç¤ºèåˆä¸ºæ„ŸçŸ¥å›¾(perception graph)ï¼Œç”¨ä»¥æ•æ‰å¯¹è±¡ã€å…³ç³»åŠæ—¶é—´ä¸Šä¸‹æ–‡æ˜¾è‘—æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¡†æ¶åˆ©ç”¨åŸºäºç²’å­æ»¤æ³¢(particle-filter)çš„ç»Ÿè®¡æ¨ç†æ¨¡å—æ¨æ–­è¯­ä¹‰åŠ¨æ€ä¸­çš„å¼‚å¸¸ï¼Œä»è€Œç²¾å‡†è¯†åˆ«è®¤çŸ¥æ”»å‡»ã€‚è¯¥æ–¹æ³•æˆåŠŸç»“åˆäº†ç°ä»£è§†è§‰-è¯­è¨€æ¨¡å‹(vision-language models)çš„è‡ªé€‚åº”æ€§ä¸æ¦‚ç‡ç¬¦å·æ¨ç†çš„å¯è§£é‡Šæ€§ã€‚åˆæ­¥å®éªŒè¯æ˜ï¼ŒCADARåœ¨ARè®¤çŸ¥æ”»å‡»æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†ç¥ç»ç¬¦å·æ–¹æ³•åœ¨æå‡ARå®‰å…¨ç¨³å¥æ€§ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09185v3",
      "published_date": "2025-08-07 17:59:49 UTC",
      "updated_date": "2025-11-27 10:41:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:04.154239+00:00"
    },
    {
      "arxiv_id": "2508.05634v1",
      "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
      "title_zh": "é€šè¿‡ç¬¦åˆä¸ç¡®å®šæ€§å¤„ç†å®ç°äººç¾¤å¯¼èˆªçš„å¯æ³›åŒ–å®‰å…¨æ€§",
      "authors": [
        "Jianpeng Yao",
        "Xiaopan Zhang",
        "Yu Xia",
        "Zejin Wang",
        "Amit K. Roy-Chowdhury",
        "Jiachen Li"
      ],
      "abstract": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº Reinforcement Learning çš„ç§»åŠ¨æœºå™¨äººåœ¨äººç¾¤å¯¼èˆªä¸­é¢ä¸´çš„ Out-of-Distribution (OOD) åœºæ™¯æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¤„ç†è¡Œäººä¸ç¡®å®šæ€§æ¥æå‡å¯¼èˆªå®‰å…¨æ€§çš„é€šç”¨æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Adaptive Conformal Inference ç”Ÿæˆè¡Œäººçš„é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶å°†å…¶ä½œä¸ºå¢å¼ºè§‚å¯Ÿè¾“å…¥ï¼Œé€šè¿‡ Constrained Reinforcement Learning å¼•å¯¼æœºå™¨äººçš„é¿éšœè¡Œä¸ºã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆè°ƒèŠ‚æ™ºèƒ½ä½“åŠ¨ä½œï¼Œä½¿å…¶é€‚åº”å¤æ‚çš„åˆ†å¸ƒåç§»ç¯å¢ƒã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå†…è®¾ç½®ä¸‹çš„æˆåŠŸç‡é«˜è¾¾ 96.93%ï¼Œç›¸æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹æ˜¾è‘—é™ä½äº†ç¢°æ’ç‡å’Œå¯¹äººç±»è½¨è¿¹çš„ä¾µå…¥ç‡ã€‚åœ¨æ¶‰åŠé€Ÿåº¦å˜åŒ–ã€ç­–ç•¥å˜æ›´å’Œç¾¤ä½“åŠ¨åŠ›å­¦è½¬æ¢çš„ä¸‰ç§ OOD åœºæ™¯åŠçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­ï¼Œè¯¥æ–¹æ³•å‡å±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "9th Conference on Robot Learning (CoRL 2025); Project website: https://gen-safe-nav.github.io/. arXiv admin note: text overlap with arXiv:2407.17460",
      "pdf_url": "https://arxiv.org/pdf/2508.05634v1",
      "published_date": "2025-08-07 17:59:43 UTC",
      "updated_date": "2025-08-07 17:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:46.891239+00:00"
    },
    {
      "arxiv_id": "2508.05633v1",
      "title": "KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation",
      "title_zh": "KuaiLiveï¼šé¢å‘ç›´æ’­æ¨èçš„å®æ—¶äº¤äº’æ•°æ®é›†",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Ke Guo",
        "Liqin Zhao",
        "Yanan Niu",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Live streaming platforms have become a dominant form of online content consumption, offering dynamically evolving content, real-time interactions, and highly engaging user experiences. These unique characteristics introduce new challenges that differentiate live streaming recommendation from traditional recommendation settings and have garnered increasing attention from industry in recent years. However, research progress in academia has been hindered by the lack of publicly available datasets that accurately reflect the dynamic nature of live streaming environments. To address this gap, we introduce KuaiLive, the first real-time, interactive dataset collected from Kuaishou, a leading live streaming platform in China with over 400 million daily active users. The dataset records the interaction logs of 23,772 users and 452,621 streamers over a 21-day period. Compared to existing datasets, KuaiLive offers several advantages: it includes precise live room start and end timestamps, multiple types of real-time user interactions (click, comment, like, gift), and rich side information features for both users and streamers. These features enable more realistic simulation of dynamic candidate items and better modeling of user and streamer behaviors. We conduct a thorough analysis of KuaiLive from multiple perspectives and evaluate several representative recommendation methods on it, establishing a strong benchmark for future research. KuaiLive can support a wide range of tasks in the live streaming domain, such as top-K recommendation, click-through rate prediction, watch time prediction, and gift price prediction. Moreover, its fine-grained behavioral data also enables research on multi-behavior modeling, multi-task learning, and fairness-aware recommendation. The dataset and related resources are publicly available at https://imgkkk574.github.io/KuaiLive.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† KuaiLiveï¼Œè¿™æ˜¯é¦–ä¸ªä»å¿«æ‰‹(Kuaishou)å¹³å°æ”¶é›†çš„å®æ—¶äº¤äº’å¼ç›´æ’­æ¨èæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å­¦æœ¯ç•Œå› ç¼ºä¹åæ˜ ç›´æ’­ç¯å¢ƒåŠ¨æ€ç‰¹æ€§çš„å…¬å¼€æ•°æ®è€Œé¢ä¸´çš„ç ”ç©¶ç“¶é¢ˆã€‚è¯¥æ•°æ®é›†è®°å½•äº† 23,772 åç”¨æˆ·å’Œ 452,621 åä¸»æ’­åœ¨ 21 å¤©å†…çš„äº¤äº’æ—¥å¿—ï¼Œå¹¶æ¶µç›–äº†ç²¾å‡†çš„ç›´æ’­é—´å¼€å¯ä¸ç»“æŸæ—¶é—´æˆ³ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒKuaiLive åŒ…å«äº†ç‚¹å‡»(click)ã€è¯„è®º(comment)ã€ç‚¹èµ(like)å’Œé€ç¤¼(gift)ç­‰å¤šç§å®æ—¶äº¤äº’è¡Œä¸ºï¼Œå¹¶æä¾›äº†ä¸°å¯Œçš„ç”¨æˆ·ä¸ä¸»æ’­ä¾§è¾¹ä¿¡æ¯(side information)ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—è¯¥æ•°æ®é›†èƒ½å¤Ÿæ›´çœŸå®åœ°æ¨¡æ‹ŸåŠ¨æ€å€™é€‰é¡¹ç›®ï¼Œå¹¶å¯¹ç”¨æˆ·å’Œä¸»æ’­è¡Œä¸ºè¿›è¡Œæ›´ç²¾å‡†çš„å»ºæ¨¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤šç»´åº¦åˆ†æå’Œä»£è¡¨æ€§æ¨èæ–¹æ³•çš„è¯„ä¼°ï¼Œä¸ºè¯¥é¢†åŸŸå»ºç«‹äº†å¼ºæœ‰åŠ›çš„åŸºå‡†(benchmark)ã€‚KuaiLive å¯æ”¯æŒ Top-K æ¨èã€ç‚¹å‡»ç‡(click-through rate)é¢„æµ‹ã€è§‚çœ‹æ—¶é•¿é¢„æµ‹åŠç¤¼å“ä»·å€¼é¢„æµ‹ç­‰å¤šç§ä»»åŠ¡ï¼Œå¹¶ä¸ºå¤šè¡Œä¸ºå»ºæ¨¡(multi-behavior modeling)å’Œå…¬å¹³æ€§æ„ŸçŸ¥æ¨è(fairness-aware recommendation)ç­‰å‰æ²¿ç ”ç©¶æä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05633v1",
      "published_date": "2025-08-07 17:59:36 UTC",
      "updated_date": "2025-08-07 17:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:42.687229+00:00"
    },
    {
      "arxiv_id": "2508.05628v1",
      "title": "H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages",
      "title_zh": "H-Net++ï¼šé¢å‘å½¢æ€ä¸°å¯Œè¯­è¨€æ— åˆ†è¯å™¨è¯­è¨€å»ºæ¨¡çš„å±‚æ¬¡åŒ–åŠ¨æ€åˆ†å—",
      "authors": [
        "Mehrdad Zakershahrak",
        "Samira Ghodratnama"
      ],
      "abstract": "Byte-level language models eliminate fragile tokenizers but face computational challenges in morphologically-rich languages (MRLs), where words span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that learns linguistically-informed segmentation through end-to-end training. Key innovations include: (1) a lightweight Transformer context-mixer (1.9M parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for document-level consistency, (3) specialized handling of orthographic artifacts (e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks align with Persian morphology without explicit supervision, demonstrating that hierarchical dynamic chunking provides an effective tokenizer-free solution for MRLs while maintaining computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†H-NET++ï¼Œä¸€ç§åˆ†å±‚åŠ¨æ€åˆ†å—(Hierarchical Dynamic Chunking)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å­—èŠ‚çº§(Byte-level)è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å½¢æ€ä¸°å¯Œè¯­è¨€(Morphologically-Rich Languages, MRLs)æ—¶é¢ä¸´çš„è®¡ç®—æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒå­¦ä¹ å…·æœ‰è¯­è¨€å­¦å¯å‘æ€§çš„åˆ†å‰²ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ç”¨äºè·¨å—æ³¨æ„åŠ›çš„è½»é‡çº§Transformer context-mixerä»¥åŠç¡®ä¿æ–‡æ¡£çº§ä¸€è‡´æ€§çš„ä¸¤çº§æ½œè¶…éªŒ(latent hyper-prior)ã€‚æ­¤å¤–ï¼ŒH-NET++ä¸“é—¨ä¼˜åŒ–äº†å¯¹æ³¢æ–¯è¯­é›¶å®½ä¸è¿å­—(ZWNJ)ç­‰æ‹¼å†™ä¼ªå½±çš„å¤„ç†ï¼Œå¹¶é‡‡ç”¨äº†åˆ†é˜¶æ®µåºåˆ—é•¿åº¦çš„è¯¾ç¨‹å­¦ä¹ (curriculum-based training)ç­–ç•¥ã€‚åœ¨14äº¿æ ‡è®°çš„æ³¢æ–¯è¯­è¯­æ–™åº“å®éªŒä¸­ï¼Œè¯¥æ¨¡å‹ç›¸æ¯”åŸºäºBPEçš„GPT-2-faæ˜¾è‘—é™ä½äº†0.159 BPBï¼Œå¹¶åœ¨ParsGLUEåŸºå‡†æµ‹è¯•ä¸­è·å¾—5.4ppçš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯æ˜ï¼ŒH-NET++åœ¨åº”å¯¹å­—ç¬¦æŸåæ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå…¶è‡ªåŠ¨å­¦ä¹ çš„åˆ†å—åœ¨æ— æ˜¾å¼ç›‘ç£æ¡ä»¶ä¸‹èƒ½ä¸çœŸå®å½¢æ€å­¦è¾¹ç•Œé«˜åº¦å¯¹é½ï¼Œä¸ºMRLsæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æ— éœ€åˆ†è¯å™¨(Tokenizer-Free)çš„è¯­è¨€å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05628v1",
      "published_date": "2025-08-07 17:59:01 UTC",
      "updated_date": "2025-08-07 17:59:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:53.493968+00:00"
    },
    {
      "arxiv_id": "2508.05625v1",
      "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•è¯´æœï¼Ÿçº¿æ€§æ¢é’ˆå¯æ­ç¤ºå¤šè½®å¯¹è¯ä¸­çš„è¯´æœåŠ¨æ€",
      "authors": [
        "Brandon Jaipersaud",
        "David Krueger",
        "Ekdeep Singh Lubana"
      ],
      "abstract": "Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•è¯´æœäººç±»ï¼Œå¹¶æå‡ºåˆ©ç”¨çº¿æ€§æ¢é’ˆ(Linear Probes)è¿™ä¸€è½»é‡åŒ–å·¥å…·æ¥æ­ç¤ºå¤šè½®å¯¹è¯ä¸­çš„è¯´æœåŠ¨æ€ã€‚ç ”ç©¶è€…ç»“åˆè®¤çŸ¥ç§‘å­¦æ´å¯Ÿï¼Œé’ˆå¯¹è¯´æœæˆåŠŸ(persuasion success)ã€è¢«è¯´æœè€…äººæ ¼(persuadee personality)å’Œè¯´æœç­–ç•¥(persuasion strategy)è®­ç»ƒäº†ç‰¹å®šçš„æ¢é’ˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œçº¿æ€§æ¢é’ˆèƒ½å¤Ÿåœ¨æ ·æœ¬å’Œæ•°æ®é›†å±‚é¢ç²¾å‡†æ•æ‰è¯´æœè¿‡ç¨‹ï¼Œä¾‹å¦‚è¯†åˆ«å‡ºå¯¹è¯ä¸­è¢«è¯´æœè€…æ€åº¦å‘ç”Ÿè½¬å˜çš„å…·ä½“æ—¶åˆ»ã€‚ç›¸æ¯”è®¡ç®—æˆæœ¬æ˜‚è´µçš„åŸºäºæç¤º(prompting-based)çš„æ–¹æ³•ï¼Œæ¢é’ˆåœ¨è¯†åˆ«è¯´æœç­–ç•¥ç­‰ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œä¸”å¤„ç†æ•ˆç‡æ˜¾è‘—æå‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†çº¿æ€§æ¢é’ˆåœ¨ç ”ç©¶æ¬ºéª—(deception)å’Œæ“çºµ(manipulation)ç­‰å¤æ‚äº¤äº’è¡Œä¸ºä»¥åŠå¤§è§„æ¨¡æ•°æ®é›†åˆ†ææ–¹é¢å…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05625v1",
      "published_date": "2025-08-07 17:58:41 UTC",
      "updated_date": "2025-08-07 17:58:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:47:54.887995+00:00"
    },
    {
      "arxiv_id": "2508.05622v1",
      "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½æ™ºèƒ½ä½“çš„ç±»äººå­¦ä¹ åŠ¨æ€æ¨¡æ‹Ÿ",
      "authors": [
        "Yu Yuan",
        "Lili Zhao",
        "Wei Chen",
        "Guangting Zheng",
        "Kai Zhang",
        "Mengdi Zhang",
        "Qi Liu"
      ],
      "abstract": "Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLM's default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed \"trap questions\" effectively diagnose Surface Learner's shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners' self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a \"diligent but brittle Surface Learner\"-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs' behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LearnerAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹ŸçœŸå®æ•™å­¦ç¯å¢ƒæ¥æ¢ç´¢ç±»äººå­¦ä¹ åŠ¨æ€(human-like learning dynamics)ã€‚è¯¥æ¡†æ¶æ ¹æ®å¿ƒç†å­¦èƒŒæ™¯æ„å»ºäº†å…·æœ‰Deepã€Surfaceå’ŒLazyç­‰ä¸åŒç”»åƒçš„å­¦ä¹ è€…ï¼Œå¹¶é€šè¿‡ä¸ºæœŸä¸€å¹´çš„å‘¨çŸ¥è¯†è·å–ã€æœˆç­–ç•¥é€‰æ‹©åŠå®šæœŸæµ‹è¯•ï¼Œç³»ç»Ÿæ€§åœ°è¿½è¸ªä¸ªä½“çš„åŠ¨æ€å­¦ä¹ è¿›ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œä»…æœ‰Deep Learnerå®ç°äº†æŒç»­çš„è®¤çŸ¥å¢é•¿ï¼Œä¸”å®éªŒè®¾è®¡çš„â€œé™·é˜±é—®é¢˜â€èƒ½æœ‰æ•ˆè¯Šæ–­å‡ºSurface Learnerçš„æµ…å±‚çŸ¥è¯†ç¼ºé™·ã€‚æ­¤å¤–ï¼Œå­¦ä¹ è€…çš„è‡ªæˆ‘æ¦‚å¿µ(self-concept)è¯„åˆ†æ¼”å˜ç¬¦åˆç°å®ï¼Œè€ŒåŸºç¡€LLMçš„é»˜è®¤è¡¨ç°è¢«è¯†åˆ«ä¸ºâ€œå‹¤å¥‹ä½†è„†å¼±çš„Surface Learnerâ€ï¼Œå³ä»…æ¨¡ä»¿å¥½å­¦ç”Ÿè¡Œä¸ºè€Œç¼ºä¹çœŸæ­£çš„æ³›åŒ–ç†è§£ã€‚LearnerAgenté€šè¿‡å¹¿æ³›çš„æ¨¡æ‹Ÿå®éªŒè¯æ˜äº†å…¶ä¸ç°å®åœºæ™¯çš„é«˜åº¦ä¸€è‡´æ€§ï¼Œä¸ºæ·±å…¥ç†è§£LLMsçš„è®¤çŸ¥ä¸è¡Œä¸ºæ¨¡å¼æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05622v1",
      "published_date": "2025-08-07 17:57:46 UTC",
      "updated_date": "2025-08-07 17:57:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:02.386383+00:00"
    },
    {
      "arxiv_id": "2508.05619v1",
      "title": "The Missing Reward: Active Inference in the Era of Experience",
      "title_zh": "ç¼ºå¤±çš„å¥–åŠ±ï¼šç»éªŒæ—¶ä»£çš„ä¸»åŠ¨æ¨ç†",
      "authors": [
        "Bo Wen"
      ],
      "abstract": "This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience,'' where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \\textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIF's principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.",
      "tldr_zh": "è¯¥è®ºæ–‡è®ºè¯äº† Active Inference (AIF) æ˜¯å¼€å‘è‡ªä¸»äººå·¥æ™ºèƒ½ä»£ç†çš„å…³é”®åŸºç¡€ï¼Œæ—¨åœ¨è§£å†³å½“å‰ AI ç³»ç»Ÿè¿‡åº¦ä¾èµ–äººå·¥å¥–åŠ±å·¥ç¨‹æ‰€é¢ä¸´çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç°æœ‰çš„å¥–åŠ±è®¾è®¡æ¨¡å¼å¯¼è‡´äº† grounded-agency gapï¼Œå³ AI ç³»ç»Ÿæ— æ³•æ ¹æ®ç¯å¢ƒå˜åŒ–è‡ªä¸»åˆ¶å®šã€è°ƒæ•´å’Œè¿½æ±‚ç›®æ ‡ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨ AIF æ¡†æ¶ï¼Œé€šè¿‡å†…åœ¨çš„æœ€å°åŒ– free energy é©±åŠ¨åŠ›å–ä»£å¤–éƒ¨å¥–åŠ±ä¿¡å·ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„ Bayesian ç›®æ ‡ä¸‹è‡ªç„¶åœ°å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å»ºè®®å°† Large Language Models (LLMs) ä½œä¸ºç”Ÿæˆå¼ world models ä¸ AIF çš„å†³ç­–æ¡†æ¶ç›¸ç»“åˆï¼Œä»è€Œåˆ›å»ºå‡ºèƒ½ä»ç»éªŒä¸­é«˜æ•ˆå­¦ä¹ çš„ä»£ç†ã€‚è¿™ç§æ–¹æ³•ä½¿ AI ç³»ç»Ÿåœ¨éµå¾ªè®¡ç®—ä¸ç‰©ç†çº¦æŸçš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸äººç±»ä»·å€¼è§‚çš„ä¸€è‡´æ€§ã€‚è¿™ç§åˆæˆä¸ºå®ç°çœŸæ­£æ„ä¹‰ä¸Šçš„è‡ªä¸»æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€æ¡æå…·å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "nlin.AO",
        "physics.bio-ph",
        "physics.comp-ph",
        "physics.hist-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05619v1",
      "published_date": "2025-08-07 17:57:12 UTC",
      "updated_date": "2025-08-07 17:57:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:10.293385+00:00"
    },
    {
      "arxiv_id": "2508.05616v1",
      "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution",
      "title_zh": "TrajEvoï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ¼”åŒ–çš„è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•è®¾è®¡",
      "authors": [
        "Zhikai Zhao",
        "Chuanbo Hua",
        "Federico Berto",
        "Kanghoon Lee",
        "Zihan Ma",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "abstract": "Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TrajEvoï¼Œä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹å¯å‘å¼ç®—æ³•çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿäººå·¥è§„åˆ™å‡†ç¡®æ€§å·®ä»¥åŠæ·±åº¦å­¦ä¹ æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œåˆ†å¸ƒå¤– (OOD) åœºæ™¯æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚TrajEvo é‡‡ç”¨è¿›åŒ–ç®—æ³•ä»å†å²è½¨è¿¹æ•°æ®ä¸­ç”Ÿæˆå¹¶ç²¾ç‚¼é¢„æµ‹è§„åˆ™ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šCross-Generation Elite Sampling ä»¥å¢å¼ºç§ç¾¤å¤šæ ·æ€§ï¼Œä»¥åŠ Statistics Feedback Loop ä½¿ LLM èƒ½å¤Ÿåˆ†æå¹¶æ”¹è¿›é¢„æµ‹ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTrajEvo åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„å¯å‘å¼æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§çš„ OOD çœŸå®æ•°æ®é›†ä¸Šï¼Œå…¶è¡¨ç°ç”šè‡³è¶…è¶Šäº†æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–è®¾è®¡å¿«é€Ÿã€å¯è§£é‡Šä¸”å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„è½¨è¿¹é¢„æµ‹å¯å‘å¼ç®—æ³•æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2505.04480",
      "pdf_url": "https://arxiv.org/pdf/2508.05616v1",
      "published_date": "2025-08-07 17:55:10 UTC",
      "updated_date": "2025-08-07 17:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:11.591824+00:00"
    },
    {
      "arxiv_id": "2508.05615v2",
      "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency",
      "title_zh": "åŸºäºåŒºåŸŸä¸€è‡´æ€§çš„ GUI å®šä½æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yong Du",
        "Yuchen Yan",
        "Fei Tang",
        "Zhengxi Lu",
        "Chang Zong",
        "Weiming Lu",
        "Shengpei Jiang",
        "Yongliang Shen"
      ],
      "abstract": "Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), transforming these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: using only 1,272 unlabeled data, GUI-RCPO achieves 3-6% accuracy improvements across various architectures on ScreenSpot benchmarks. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more data-efficient GUI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢å®šä½(GUI grounding)ä»»åŠ¡ä¸­åƒç´ çº§æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸä¸€è‡´æ€§çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶å‘ç°æ¨¡å‹å¯¹åŒä¸€GUIå…ƒç´ ç”Ÿæˆå¤šä¸ªé¢„æµ‹æ—¶ï¼Œç©ºé—´é‡å æ¨¡å¼(spatial overlap patterns)å¯ä½œä¸ºéšå¼ç½®ä¿¡åº¦ä¿¡å·å¼•å¯¼æ›´å‡†ç¡®çš„å®šä½ã€‚ä¸ºæ­¤æå‡ºçš„GUI-RC (Region Consistency) æ–¹æ³•åˆ©ç”¨ç©ºé—´æŠ•ç¥¨ç½‘æ ¼(spatial voting grids)è¯†åˆ«æ¨¡å‹è¾¾æˆæœ€é«˜å…±è¯†çš„åŒºåŸŸï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å°†ä¸åŒæ¶æ„çš„ScreenSpotåŸºå‡†æµ‹è¯•å‡†ç¡®ç‡æå‡äº†2-3%ã€‚è¿›ä¸€æ­¥å¼•å…¥çš„GUI-RCPO (Region Consistency Policy Optimization) å°†è¿™ç§ä¸€è‡´æ€§æ¨¡å¼è½¬åŒ–ä¸ºå¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨æ— æ ‡æ³¨æ•°æ®è¿›è¡Œæµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ (test-time reinforcement learning)å¹¶è¿­ä»£ä¼˜åŒ–è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒGUI-RCPOä»…ä½¿ç”¨å°‘é‡æ— æ ‡æ³¨æ•°æ®å³å¯åœ¨å¤šç§æ¶æ„ä¸Šå®ç°3-6%çš„å‡†ç¡®ç‡æå‡ã€‚è¯¥æ–¹æ³•æ­ç¤ºäº†æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)åœ¨å¼€å‘é«˜æ•ˆèƒ½ã€ä½æ•°æ®ä¾èµ–çš„GUIæ™ºèƒ½ä½“æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "[Accepted by AAAI2026] Project Page: https://zju-real.github.io/gui-rcpo Code: https://github.com/zju-real/gui-rcpo",
      "pdf_url": "https://arxiv.org/pdf/2508.05615v2",
      "published_date": "2025-08-07 17:54:27 UTC",
      "updated_date": "2025-11-13 12:54:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:24.400612+00:00"
    },
    {
      "arxiv_id": "2508.05614v1",
      "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
      "title_zh": "OmniEARï¼šå…·èº«ä»»åŠ¡ä¸­çš„æ™ºèƒ½ä½“æ¨ç†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zixuan Wang",
        "Dingming Li",
        "Hongxing Li",
        "Shuo Chen",
        "Yuchen Yan",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OmniEARï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°Large language modelsåœ¨å…·èº«ä»»åŠ¡(Embodied tasks)ä¸­å…³äºç‰©ç†äº¤äº’ã€å·¥å…·ä½¿ç”¨åŠå¤šæ™ºèƒ½ä½“åä½œ(Multi-agent coordination)æ¨ç†èƒ½åŠ›çš„ç»¼åˆæ¡†æ¶ã€‚ä¸åŒäºç°æœ‰æä¾›é¢„å®šä¹‰å·¥å…·æˆ–æ˜ç¡®æŒ‡ä»¤çš„åŸºå‡†ï¼ŒOmniEARè¦æ±‚æ™ºèƒ½ä½“æ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è·å–èƒ½åŠ›å¹¶è‡ªä¸»å†³å®šåè°ƒç­–ç•¥ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬ç¯å¢ƒè¡¨ç¤ºæ³•(Text-based environment representation)ï¼Œåœ¨æ¶µç›–å®¶åº­å’Œå·¥ä¸šé¢†åŸŸçš„1,500ä¸ªåœºæ™¯ä¸­æ¨¡æ‹Ÿäº†å¤æ‚çš„ç‰©ç†å±æ€§å’Œç©ºé—´å…³ç³»ã€‚å®éªŒè¯„ä¼°æ­ç¤ºäº†æ¨¡å‹åœ¨çº¦æŸæ¨ç†ä¸‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼šåœ¨å·¥å…·æ¨ç†å’Œéšå¼åä½œ(Implicit collaboration)ä¸­çš„è¡¨ç°è¿œä½äºæ˜ç¡®æŒ‡ä»¤ä¸‹çš„æˆåŠŸç‡ï¼Œä¸”å¤åˆä»»åŠ¡çš„å¤±è´¥ç‡è¶…è¿‡50%ã€‚ç ”ç©¶å‘ç°å®Œæ•´çš„ç¯å¢ƒä¿¡æ¯åè€Œä¼šé™ä½åä½œæ€§èƒ½ï¼Œè¡¨æ˜æ¨¡å‹éš¾ä»¥æœ‰æ•ˆè¿‡æ»¤ä»»åŠ¡ç›¸å…³çš„çº¦æŸä¿¡æ¯ã€‚è™½ç„¶Fine-tuningèƒ½æ˜¾è‘—æå‡å•æ™ºèƒ½ä½“ä»»åŠ¡è¡¨ç°ï¼Œä½†åœ¨å¤šæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šçš„æ”¶ç›Šæå°ï¼Œæš´éœ²äº†å½“å‰æ¶æ„åœ¨å¤„ç†å…·èº«æ¨ç†(Embodied reasoning)æ—¶çš„åŸºç¡€æ€§å±€é™ã€‚è¯¥ç ”ç©¶ä¸ºè¯„ä¼°å’Œä¼˜åŒ–å…·èº«AIç³»ç»Ÿæä¾›äº†ä¸¥è°¨çš„åŸºå‡†å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://zju-real.github.io/OmniEmbodied Code: https://github.com/ZJU-REAL/OmniEmbodied",
      "pdf_url": "https://arxiv.org/pdf/2508.05614v1",
      "published_date": "2025-08-07 17:54:15 UTC",
      "updated_date": "2025-08-07 17:54:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:18.696513+00:00"
    },
    {
      "arxiv_id": "2508.05613v1",
      "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models",
      "title_zh": "Cooperï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¸å¥–åŠ±æ¨¡å‹ååŒä¼˜åŒ–",
      "authors": [
        "Haitao Hong",
        "Yuchen Yan",
        "Xingyu Wu",
        "Guiyang Hou",
        "Wenqi Zhang",
        "Weiming Lu",
        "Yongliang Shen",
        "Jun Xiao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¨ç†ä»»åŠ¡ä¸­ï¼Œè§„åˆ™å¥–åŠ±(rule-based rewards)ç¨³å¥æ€§ä¸è¶³ä»¥åŠæ¨¡å‹å¥–åŠ±(model-based rewards)æ˜“å—å¥–åŠ±ä½œå¼Š(reward hacking)å½±å“çš„é—®é¢˜ï¼Œæå‡ºäº†Cooperæ¡†æ¶ã€‚Cooperé€šè¿‡å…±åŒä¼˜åŒ–ç­–ç•¥æ¨¡å‹(policy model)å’Œå¥–åŠ±æ¨¡å‹(reward model)ï¼Œåˆ©ç”¨è§„åˆ™å¥–åŠ±åœ¨è¯†åˆ«æ­£ç¡®å“åº”æ—¶çš„é«˜ç²¾ç¡®åº¦ï¼ŒåŠ¨æ€æ„å»ºæ­£è´Ÿæ ·æœ¬å¯¹ä»¥æŒç»­è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ··åˆæ ‡æ³¨ç­–ç•¥å’ŒåŸºäºå‚è€ƒä¿¡æ¯çš„å¥–åŠ±å»ºæ¨¡èŒƒå¼ï¼Œè®­ç»ƒå‡ºåœ¨VerifyBenchä¸Šè¡¨ç°ä¼˜å¼‚çš„VerifyRMã€‚å®éªŒè¯æ˜ï¼ŒCooperä¸ä»…èƒ½æœ‰æ•ˆç¼“è§£å¥–åŠ±ä½œå¼Šï¼Œè¿˜æå‡äº†ç«¯åˆ°ç«¯RLæ€§èƒ½ï¼Œåœ¨Qwen2.5-1.5B-Instructä¸Šå®ç°äº†0.54%çš„å¹³å‡å‡†ç¡®ç‡å¢é•¿ã€‚è¯¥ç ”ç©¶ä¸ºåŠ¨æ€æ›´æ–°å¥–åŠ±æ¨¡å‹ä»¥åº”å¯¹å¥–åŠ±ä½œå¼Šæä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼Œä¸ºä¼˜åŒ–RLä¸­çš„å¥–åŠ±é›†æˆæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://zju-real.github.io/cooper Code: https://github.com/zju-real/cooper",
      "pdf_url": "https://arxiv.org/pdf/2508.05613v1",
      "published_date": "2025-08-07 17:53:56 UTC",
      "updated_date": "2025-08-07 17:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:22.986634+00:00"
    },
    {
      "arxiv_id": "2508.05612v3",
      "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
      "title_zh": "Shuffle-R1ï¼šåŸºäºä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„åŠ¨æ€æ´—ç‰Œçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Linghao Zhu",
        "Yiran Guan",
        "Dingkang Liang",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Bin Qin",
        "Jian Luan",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement learning)åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰è®­ç»ƒæµç¨‹ä¸­å­˜åœ¨çš„ä¼˜åŠ¿åç¼©(Advantage Collapsing)å’Œç”Ÿæˆæ²‰é»˜(Rollout Silencing)å¯¼è‡´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Shuffle-R1ï¼Œä¸€ä¸ªé€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„åˆæ¥æé«˜å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ•ˆç‡çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æˆå¯¹è½¨è¿¹é‡‡æ ·(Pairwise Trajectory Sampling)ï¼Œé€šè¿‡é€‰æ‹©å…·æœ‰å¤§ä¼˜åŠ¿å€¼çš„é«˜å¯¹æ¯”åº¦è½¨è¿¹æ¥æå‡æ¢¯åº¦ä¿¡å·çš„è´¨é‡ã€‚åŒæ—¶ï¼Œæ¡†æ¶é‡‡ç”¨äº†åŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ‰“ä¹±(Advantage-based Trajectory Shuffle)æœºåˆ¶ï¼Œé€šè¿‡åŸºäºä¿¡æ¯çš„æ‰¹æ¬¡é‡ç»„å¢åŠ ä»·å€¼è¾ƒé«˜çš„ç”Ÿæˆè½¨è¿¹çš„æ›å…‰ç‡ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒShuffle-R1åœ¨æä½é¢å¤–å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å§‹ç»ˆä¼˜äºå¼ºæœ‰åŠ›çš„å¼ºåŒ–å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è°ƒæ•´æ–¹æ¡ˆå¯¹äºæé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page at: https://xenozlh.github.io/Shuffle-R1/",
      "pdf_url": "https://arxiv.org/pdf/2508.05612v3",
      "published_date": "2025-08-07 17:53:47 UTC",
      "updated_date": "2025-10-21 06:23:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:37.938293+00:00"
    },
    {
      "arxiv_id": "2508.05731v2",
      "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization",
      "title_zh": "InfiGUI-G1ï¼šåŸºäºè‡ªé€‚åº”æ¢ç´¢ç­–ç•¥ä¼˜åŒ–æå‡ GUI å®šä½èƒ½åŠ›",
      "authors": [
        "Yuhang Liu",
        "Zeyu Liu",
        "Shuanghe Zhu",
        "Pengxiang Li",
        "Congkai Xie",
        "Jiasheng Wang",
        "Xavier Hu",
        "Xiaotian Han",
        "Jianbo Yuan",
        "Xinyao Wang",
        "Shengyu Zhang",
        "Hongxia Yang",
        "Fei Wu"
      ],
      "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ (GUIs) ä»»åŠ¡ä¸­é¢ä¸´çš„è¯­ä¹‰å¯¹é½éš¾é¢˜ï¼Œæå‡ºäº†åä¸º Adaptive Exploration Policy Optimization (AEPO) çš„æ–°å‹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ Reinforcement Learning with Verifiable Rewards (RLVR) åœ¨æ¢ç´¢æ•ˆç‡ä¸Šçš„ç“¶é¢ˆï¼ŒAEPO é€šè¿‡å¤šç­”æ¡ˆç”Ÿæˆç­–ç•¥å¼ºåˆ¶æ¨¡å‹è¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ï¼Œå¹¶åˆ©ç”¨åŸºäºæ•ˆç‡åŸåˆ™æ¨å¯¼çš„ Adaptive Exploration Reward (AER) å‡½æ•°å¯¹æ¢ç´¢è¿‡ç¨‹è¿›è¡Œå¼•å¯¼ã€‚åŸºäºè¯¥æ¡†æ¶è®­ç»ƒçš„ InfiGUI-G1-3B å’Œ InfiGUI-G1-7B æ¨¡å‹åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§çš„ GUI grounding åŸºå‡†æµ‹è¯•ä¸­å‡åˆ·æ–°äº† SOTA çºªå½•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æµ‹è¯•æ³›åŒ–èƒ½åŠ›å’Œè¯­ä¹‰ç†è§£çš„åŸºå‡†ä¸Šæ¯”åŸç”Ÿ RLVR åŸºå‡†æå‡äº†é«˜è¾¾ 9.0%ï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚è¯­ä¹‰å…³è”çš„å­¦ä¹ éš¾é¢˜ï¼Œæ˜¾è‘—å¢å¼ºäº†è‡ªä¸»æ™ºèƒ½ä½“åœ¨è§†è§‰è¾“å…¥ä¸‹çš„ç©ºé—´ä¸è¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2026 (Oral Presentation)",
      "pdf_url": "https://arxiv.org/pdf/2508.05731v2",
      "published_date": "2025-08-07 17:49:56 UTC",
      "updated_date": "2025-12-08 10:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:38.931600+00:00"
    },
    {
      "arxiv_id": "2508.05728v1",
      "title": "CLAPP: The CLASS LLM Agent for Pair Programming",
      "title_zh": "CLAPPï¼šé¢å‘ CLASS ç»“å¯¹ç¼–ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Santiago Casas",
        "Christian Fidler",
        "Boris Bolliet",
        "Francisco Villaescusa-Navarro",
        "Julien Lesgourgues"
      ],
      "abstract": "We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† CLAPP (CLASS LLM Agent for Pair Programming)ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºä½¿ç”¨ Einstein-Boltzmann æ±‚è§£å™¨ CLASS çš„ç ”ç©¶äººå‘˜è®¾è®¡çš„äº¤äº’å¼ AI åŠ©æ‰‹ã€‚CLAPP ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å’Œé¢†åŸŸç‰¹å®šæ£€ç´¢æŠ€æœ¯ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“ LLM ç¼–æ’ (multi-agent LLM orchestration)ã€é’ˆå¯¹ CLASS æ–‡æ¡£çš„è¯­ä¹‰æœç´¢ä»¥åŠå®æ—¶ Python è¿è¡Œç¯å¢ƒï¼Œä¸ºç”¨æˆ·æä¾›é—®ç­”ã€ä»£ç ç”Ÿæˆã€é”™è¯¯è°ƒè¯•å’Œè‡ªåŠ¨ç»˜å›¾ç­‰åä½œç¼–ç¨‹æ”¯æŒã€‚è¯¥ç³»ç»Ÿä»¥ Web åº”ç”¨ç¨‹åºçš„å½¢å¼éƒ¨ç½²ï¼Œæ—¨åœ¨é™ä½ä¸ç†Ÿæ‚‰ AI æŠ€æœ¯çš„ç§‘å­¦å®¶è¿›å…¥è®¡ç®—å’Œæ•°å€¼å®‡å®™å­¦é¢†åŸŸçš„é—¨æ§›ã€‚é€šè¿‡è¿™ç§åˆ›æ–°çš„æ¶æ„ï¼ŒCLAPP å®ç°äº†æ›´é«˜æ•ˆçš„äººæœºåä½œï¼Œæ˜¾è‘—æå‡äº†å¤„ç†å¤æ‚æ•°å€¼è®¡ç®—ä»»åŠ¡çš„ç§‘ç ”ç”Ÿäº§åŠ›ã€‚",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "Code: https://github.com/santiagocasas/clapp, Streamlit app: https://classclapp.streamlit.app",
      "pdf_url": "https://arxiv.org/pdf/2508.05728v1",
      "published_date": "2025-08-07 17:35:06 UTC",
      "updated_date": "2025-08-07 17:35:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:43.761009+00:00"
    },
    {
      "arxiv_id": "2508.05581v1",
      "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„éš¾æ²»æ€§é«˜è¡€å‹å¯è®¡ç®—è¡¨å‹è¿­ä»£å­¦ä¹ ",
      "authors": [
        "Guilherme Seidyo Imai Aldeia",
        "Daniel S. Herman",
        "William G. La Cava"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ç”Ÿæˆå¯è§£é‡Šçš„å¯è®¡ç®—è¡¨å‹(Computable Phenotypes, CPs)çš„æ½œåŠ›ï¼Œæ—¨åœ¨ä¸ºé«˜è¡€å‹æ‚£è€…æä¾›å¯æ‰©å±•çš„ä¸´åºŠå†³ç­–æ”¯æŒã€‚ç ”ç©¶è€…é’ˆå¯¹å…­ç§ä¸åŒå¤æ‚åº¦çš„ä¸´åºŠè¡¨å‹ï¼Œæå‡ºäº†ä¸€ç§â€œåˆæˆã€æ‰§è¡Œã€è°ƒè¯•ã€æŒ‡å¯¼â€(synthesize, execute, debug, instruct)çš„è¿­ä»£å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„åé¦ˆå¾ªç¯æ¥ä¸æ–­ä¼˜åŒ– LLMs ç”Ÿæˆçš„ç¨‹åºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆè¿­ä»£å­¦ä¹ çš„ LLMs èƒ½å¤Ÿç”Ÿæˆé«˜åº¦å¯è§£é‡Šä¸”å‡†ç¡®çš„è¡¨å‹è¯†åˆ«ç¨‹åºï¼Œå…¶æ€§èƒ½å·²æ¥è¿‘ç›®å‰æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ (state-of-the-art ML)æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ‰€éœ€çš„è®­ç»ƒæ ·æœ¬é‡æ˜¾è‘—å‡å°‘ï¼Œè¯æ˜äº† LLMs åœ¨æ„å»ºä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæ–¹é¢çš„å·¨å¤§åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05581v1",
      "published_date": "2025-08-07 17:15:17 UTC",
      "updated_date": "2025-08-07 17:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:45.240931+00:00"
    },
    {
      "arxiv_id": "2508.10022v1",
      "title": "Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control",
      "title_zh": "å¤šé¡¹é€‰æ‹©é—®ç­”ä»»åŠ¡ä¸­å…·æœ‰å¯è¯æ˜é£é™©æ§åˆ¶çš„ç¬¦åˆæ€§ P å€¼",
      "authors": [
        "Yuanchang Ye"
      ],
      "abstract": "This study introduces a significance testing-enhanced conformal prediction (CP) framework to improve trustworthiness of large language models (LLMs) in multiple-choice question answering (MCQA). While LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability. Although CP provides statistically rigorous marginal coverage guarantees for prediction sets, and significance testing offers established statistical rigor, their synergistic integration remains unexplored. To mitigate hallucination and factual inaccuracies, our framework integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\\mathcal{H}_0$) with empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves user-specified empirical miscoverage rates; (2) Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($Î±$), validating APSS as an effective uncertainty metric. This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºæ˜¾è‘—æ€§æ£€éªŒçš„ç¬¦åˆæ€§é¢„æµ‹(Conformal Prediction, CP)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”(MCQA)ä»»åŠ¡ä¸­çš„å¯ä¿¡åº¦ã€‚é’ˆå¯¹LLMsåœ¨å­¦ç§‘é—®ç­”ä¸­å­˜åœ¨çš„å¹»è§‰å’Œéäº‹å®ç”Ÿæˆé—®é¢˜ï¼Œè¯¥æ¡†æ¶å°† $p$-value è®¡ç®—ä¸ç¬¦åˆæ€§è¯„åˆ†(Conformity Scoring)æ·±åº¦é›†æˆï¼Œé€šè¿‡å¯¹MCQAå“åº”è¿›è¡Œè‡ªä¸€è‡´æ€§é‡é‡‡æ ·(Self-Consistency Resampling)æ¥åº”å¯¹æ¨¡å‹çš„é»‘ç›’ç‰¹æ€§ã€‚ç ”ç©¶åˆ©ç”¨é€‰é¡¹é¢‘ç‡å¹¶ç»“åˆåŸå‡è®¾æ£€éªŒ($\\mathcal{H}_0$)æ„å»ºé¢„æµ‹é›†ï¼Œç¡®ä¿äº†ç»Ÿè®¡æ„ä¹‰ä¸Šçš„é£é™©æ§åˆ¶ã€‚åœ¨ MMLU å’Œ MMLU-Pro åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½å®ç°ç”¨æˆ·é¢„è®¾çš„ç»éªŒè¦†ç›–è¯¯å·®ç‡ï¼Œä¸”å¹³å‡é¢„æµ‹é›†å¤§å°(APSS)éšé£é™©æ°´å¹³($\\alpha$)å¢åŠ è€Œå•è°ƒä¸‹é™ï¼ŒéªŒè¯äº†APSSä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æˆæœä¸ºåœ¨é«˜é£é™©åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½²å…·æœ‰å¯è¯æ˜é£é™©æ§åˆ¶èƒ½åŠ›çš„LLMsæä¾›äº†ä¸€ä¸ªä¸¥è°¨çš„ç»Ÿè®¡å­¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10022v1",
      "published_date": "2025-08-07 16:46:47 UTC",
      "updated_date": "2025-08-07 16:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:50.489792+00:00"
    },
    {
      "arxiv_id": "2508.10021v4",
      "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients",
      "title_zh": "LATTEï¼šé¢å‘é“¶è¡Œå®¢æˆ·çš„äº¤æ˜“ä¸æ–‡æœ¬åµŒå…¥å¯¹é½å­¦ä¹ ",
      "authors": [
        "Egor Fadeev",
        "Dzhambulat Mollaev",
        "Aleksei Shestov",
        "Omar Zoloev",
        "Artem Sakhno",
        "Dmitry Korolev",
        "Ivan Kireev",
        "Andrey Savchenko",
        "Maksim Makarenko"
      ],
      "abstract": "Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LATTEï¼Œä¸€ç§ç”¨äºé“¶è¡Œå®¢æˆ·å»ºæ¨¡çš„å¯¹æ¯”å­¦ä¹ (Contrastive Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)ç›´æ¥å¤„ç†é•¿äº‹ä»¶åºåˆ—æ—¶é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­è½åœ°çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åŸå§‹äº‹ä»¶åµŒå…¥(Raw Event Embeddings)ä¸æ¥è‡ªå†»ç»“LLMsçš„è¯­ä¹‰åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œå°†å®¢æˆ·è¡Œä¸ºç‰¹å¾æç‚¼ä¸ºç®€çŸ­çš„æç¤ºè¯ä½œä¸ºç›‘ç£ä¿¡å·ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„LLMå…¨åºåˆ—å¤„ç†æ–¹å¼ï¼ŒLATTEåœ¨å¤§å¹…ç¼©å‡æ¨ç†æˆæœ¬å’Œè¾“å…¥è§„æ¨¡çš„åŒæ—¶ï¼Œåˆ©ç”¨å¯¹æ¯”æŸå¤±(Contrastive Loss)ç¡®ä¿äº†æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨ç¤º(Representations)å…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®é‡‘èæ•°æ®é›†ä¸Šçš„äº‹ä»¶åºåˆ—å»ºæ¨¡æ•ˆæœä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚å‡­å€Ÿå…¶å“è¶Šçš„æ€§èƒ½å’Œä½å»¶è¿Ÿç‰¹æ€§ï¼ŒLATTEä¸ºé‡‘èæœºæ„åœ¨å»¶è¿Ÿæ•æ„Ÿçš„ç¯å¢ƒä¸­éƒ¨ç½²é«˜æ•ˆçš„å®¢æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿæä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10021v4",
      "published_date": "2025-08-07 16:46:38 UTC",
      "updated_date": "2025-12-17 11:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:58.441591+00:00"
    },
    {
      "arxiv_id": "2508.05557v3",
      "title": "MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media",
      "title_zh": "MV-Debateï¼šé¢å‘ç¤¾äº¤åª’ä½“å¤šæ¨¡æ€æœ‰å®³å†…å®¹æ£€æµ‹çš„åŠ¨æ€åæ€é—¨æ§å¤šè§†è§’æ™ºèƒ½ä½“è¾©è®º",
      "authors": [
        "Rui Lu",
        "Jinhe Bi",
        "Yunpu Ma",
        "Feng Xiao",
        "Yuntao Du",
        "Yijun Tian"
      ],
      "abstract": "Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­å› è·¨æ¨¡æ€çŸ›ç›¾å’Œå¾®å¦™è¯­ç”¨çº¿ç´¢å¯¼è‡´çš„æœ‰å®³å†…å®¹è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºMV-Debateçš„å¤šè§†å›¾æ™ºèƒ½ä½“è¾©è®ºæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç»Ÿä¸€çš„å¤šæ¨¡æ€æœ‰å®³å†…å®¹æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»„å»ºäº†surface analystã€deep reasonerã€modality contrastå’Œsocial contextualistå››ä¸ªäº’è¡¥çš„è¾©è®ºæ™ºèƒ½ä½“ï¼Œä»å¤šæ ·åŒ–çš„è§£é‡Šè§†è§’å¯¹å†…å®¹è¿›è¡ŒååŒåˆ†æã€‚é€šè¿‡è¿­ä»£è¾©è®ºå’Œåæ€æœºåˆ¶ï¼Œæ™ºèƒ½ä½“åœ¨reflection-gainå‡†åˆ™ä¸‹ç²¾ç‚¼å“åº”ï¼Œå¹¶ç»“åˆdynamic reflection gatingç¡®ä¿äº†æ£€æµ‹çš„å‡†ç¡®æ€§ä¸å¤„ç†æ•ˆç‡ã€‚å®éªŒåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯æ˜ï¼ŒMV-Debateçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå¼ºå•æ¨¡å‹åŠç°æœ‰çš„å¤šæ™ºèƒ½ä½“è¾©è®ºåŸºçº¿ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºåœ¨æå‡å®‰å…¨å…³é”®åœ¨çº¿ç¯å¢ƒä¸‹ç¤¾ä¼šæ„å›¾æ£€æµ‹å¯é æ€§æ–¹é¢çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05557v3",
      "published_date": "2025-08-07 16:38:25 UTC",
      "updated_date": "2025-09-07 09:23:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:48:57.899896+00:00"
    },
    {
      "arxiv_id": "2508.05547v1",
      "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹çš„æ— æ ‡ç­¾é€‚é…ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Hao Dong",
        "Lijun Sheng",
        "Jian Liang",
        "Ran He",
        "Eleni Chatzi",
        "Olga Fink"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.",
      "tldr_zh": "æœ¬æ–‡ç»¼è¿°äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨æ— æ ‡ç­¾æ•°æ®ä¸‹çš„é€‚é…æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å…¶åœ¨ç‰¹å®šä¸‹æ¸¸åœºæ™¯ä¸­çš„æ€§èƒ½å¹¶ä¿æŒæ•°æ®æ•ˆç‡ã€‚é’ˆå¯¹ç›®å‰ç¼ºä¹ç»Ÿä¸€ã€é¢å‘ä»»åŠ¡çš„æ— ç›‘ç£ VLM é€‚é…ç»¼è¿°è¿™ä¸€ç°çŠ¶ï¼Œè¯¥ç ”ç©¶æ ¹æ®æ— æ ‡ç­¾è§†è§‰æ•°æ®çš„æ€§è´¨æå‡ºäº†ä¸€ä¸ªåˆ†ç±»å­¦æ¡†æ¶ï¼Œå°†ç°æœ‰æ–¹æ³•å½’çº³ä¸ºæ•°æ®æ— å…³è¿ç§» (Data-Free Transfer)ã€æ— ç›‘ç£é¢†åŸŸè¿ç§» (Unsupervised Domain Transfer)ã€åˆ†æ®µæµ‹è¯•æ—¶é€‚é… (Episodic Test-Time Adaptation) å’Œåœ¨çº¿æµ‹è¯•æ—¶é€‚é… (Online Test-Time Adaptation) å››å¤§èŒƒå¼ã€‚é€šè¿‡å¯¹å„èŒƒå¼æ ¸å¿ƒæ–¹æ³•è®ºå’Œé€‚é…ç­–ç•¥çš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„å­¦æœ¯è®¤çŸ¥ä½“ç³»ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢³ç†äº†æ¶µç›–ä¸åŒåº”ç”¨åœºæ™¯çš„ä»£è¡¨æ€§åŸºå‡†æµ‹è¯• (Benchmarks)ï¼Œå¹¶æ¢è®¨äº†é¢†åŸŸå†…çš„å¼€æ”¾æ€§æŒ‘æˆ˜ä¸æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³æ–‡çŒ®èµ„æºå·²åœ¨å¼€æºä»£ç åº“æŒç»­æ›´æ–°ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å…¨é¢çš„å­¦æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Discussions, comments, and questions are welcome in \\url{https://github.com/tim-learn/Awesome-LabelFree-VLMs}",
      "pdf_url": "https://arxiv.org/pdf/2508.05547v1",
      "published_date": "2025-08-07 16:27:37 UTC",
      "updated_date": "2025-08-07 16:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:07.347337+00:00"
    },
    {
      "arxiv_id": "2508.05544v1",
      "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees",
      "title_zh": "é»‘ç›’è®¾ç½®ä¸‹å…·æœ‰å¯è¯æ˜è¦†ç›–ç‡ä¿è¯çš„å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”ç¬¦åˆæ€§é›†åˆ",
      "authors": [
        "Guang Yang",
        "Xinyang Liu"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šé¡¹é€‰æ‹©é¢˜è§£ç­” (MCQA) ä¸­å­˜åœ¨çš„å¹»è§‰å’Œè¿‡åº¦è‡ªä¿¡ç­‰ä¸å¯é é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºé»‘ç›’ç¯å¢ƒ (Black-Box Settings) çš„åŸºäºé¢‘ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆç¬¦åˆæ€§é¢„æµ‹ (Conformal Prediction) æŠ€æœ¯ï¼Œé€šè¿‡å¯¹æ¨¡å‹è¾“å‡ºåˆ†å¸ƒè¿›è¡Œå¤šæ¬¡ç‹¬ç«‹é‡‡æ ·ï¼Œå¹¶åˆ©ç”¨é‡‡æ ·é¢‘ç‡è®¡ç®—é¢„æµ‹ç†µ (Predictive Entropy)ï¼Œä»è€Œå®ç°å¯è¯æ˜çš„è¦†ç›–ä¿è¯ (Provable Coverage Guarantees)ã€‚åœ¨ MedMCQAã€MedQAã€MMLU å’Œ MMLU-Pro ç­‰å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºé¢‘ç‡çš„é¢„æµ‹ç†µåœ¨åŒºåˆ†é¢„æµ‹æ­£ç¡®æ€§æ–¹é¢çš„è¡¨ç° (AUROC) æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºé€»è¾‘å€¼ (Logit-based) çš„æ–¹æ³•ã€‚ç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•èƒ½åœ¨ç”¨æˆ·æŒ‡å®šçš„é£é™©æ°´å¹³ä¸‹æœ‰æ•ˆæ§åˆ¶ç»éªŒé”™è¯¯è¦†ç›–ç‡ï¼Œè¯æ˜äº†é‡‡æ ·é¢‘ç‡åœ¨é»‘ç›’åœºæ™¯ä¸‹æ˜¯é€»è¾‘å€¼æ¦‚ç‡çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚è¿™ä¸€æˆæœä¸ºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ç­‰é«˜é£é™©å®é™…åº”ç”¨ä¸­çš„å¯é æ€§æä¾›äº†ä¸€ä¸ªæ¨¡å‹æ— å…³ä¸”æ— åˆ†å¸ƒå‡è®¾çš„æœ‰æ•ˆæ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2508.05544v1",
      "published_date": "2025-08-07 16:22:49 UTC",
      "updated_date": "2025-08-07 16:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:08.453930+00:00"
    },
    {
      "arxiv_id": "2508.05537v1",
      "title": "Tractable Sharpness-Aware Learning of Probabilistic Circuits",
      "title_zh": "æ¦‚ç‡ç”µè·¯çš„æ˜“å¤„ç†é”åº¦æ„ŸçŸ¥å­¦ä¹ ",
      "authors": [
        "Hrithik Suresh",
        "Sahil Sidheekh",
        "Vishnu Shreeram M. P",
        "Sriraam Natarajan",
        "Narayanan C. Krishnan"
      ],
      "abstract": "Probabilistic Circuits (PCs) are a class of generative models that allow exact and tractable inference for a wide range of queries. While recent developments have enabled the learning of deep and expressive PCs, this increased capacity can often lead to overfitting, especially when data is limited. We analyze PC overfitting from a log-likelihood-landscape perspective and show that it is often caused by convergence to sharp optima that generalize poorly. Inspired by sharpness aware minimization in neural networks, we propose a Hessian-based regularizer for training PCs. As a key contribution, we show that the trace of the Hessian of the log-likelihood-a sharpness proxy that is typically intractable in deep neural networks-can be computed efficiently for PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer that yields simple closed-form parameter updates for EM, and integrates seamlessly with gradient based learning methods. Experiments on synthetic and real-world datasets demonstrate that our method consistently guides PCs toward flatter minima, improves generalization performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¦‚ç‡ç”µè·¯ (Probabilistic Circuits, PCs) åœ¨æ·±åº¦å­¦ä¹ ä¸­æ˜“å‡ºç°çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä»å¯¹æ•°ä¼¼ç„¶æ™¯è§‚ (log-likelihood-landscape) çš„è§’åº¦åˆ†æå¹¶æŒ‡å‡ºå…¶æºäºæ¨¡å‹æ”¶æ•›åˆ°äº†æ³›åŒ–èƒ½åŠ›å·®çš„å°–é”æå€¼ç‚¹ (sharp optima)ã€‚å—åˆ°ç¥ç»ç½‘ç»œä¸­é”åº¦æ„ŸçŸ¥æœ€å°åŒ–å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé»‘å¡çŸ©é˜µ (Hessian-based) çš„æ­£åˆ™åŒ–å™¨ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºè¯æ˜äº†å¯¹æ•°ä¼¼ç„¶çš„é»‘å¡çŸ©é˜µè¿¹ (trace of the Hessian) ä½œä¸ºé”åº¦ä»£ç†æŒ‡æ ‡ï¼Œåœ¨æ¦‚ç‡ç”µè·¯ä¸­æ˜¯å¯ä»¥é«˜æ•ˆè®¡ç®—çš„ï¼Œè€Œè¿™åœ¨æ™®é€šæ·±åº¦ç¥ç»ç½‘ç»œä¸­é€šå¸¸æ˜¯ä¸å¯è®¡ç®—çš„ã€‚é€šè¿‡æœ€å°åŒ–è¯¥è¿¹ï¼Œç ”ç©¶æ¨å¯¼å‡ºä¸€ç§åŸºäºæ¢¯åº¦èŒƒæ•° (gradient-norm-based) çš„æ­£åˆ™åŒ–é¡¹ï¼Œä¸º EM ç®—æ³•æä¾›äº†ç®€æ´çš„é—­å¼å‚æ•°æ›´æ–°æ–¹æ¡ˆï¼Œå¹¶å¯ç›´æ¥åº”ç”¨äºæ¢¯åº¦å­¦ä¹ æ–¹æ³•ã€‚åœ¨åˆæˆä¸çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹è½¬å‘æ›´å¹³å¦çš„æå°å€¼ï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05537v1",
      "published_date": "2025-08-07 16:13:24 UTC",
      "updated_date": "2025-08-07 16:13:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:07.196823+00:00"
    },
    {
      "arxiv_id": "2508.11671v1",
      "title": "LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering",
      "title_zh": "åŸºäºLLMçš„éŸ³ä¹æ¨èæ™ºèƒ½ä½“ï¼šä¸ç»å…¸åŸºäºå†…å®¹è¿‡æ»¤ç®—æ³•çš„å¯¹æ¯”",
      "authors": [
        "Ronald Carvalho Boadana",
        "Ademir GuimarÃ£es da Costa Junior",
        "Ricardo Rios",
        "FÃ¡bio Santos da Silva"
      ],
      "abstract": "The growing availability of music on streaming platforms has led to information overload for users. To address this issue and enhance the user experience, increasingly sophisticated recommendation systems have been proposed. This work investigates the use of Large Language Models (LLMs) from the Gemini and LLaMA families, combined with intelligent agents, in a multi-agent personalized music recommendation system. The results are compared with a traditional content-based recommendation model, considering user satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction rates of up to \\textit{89{,}32\\%}, indicating their promising potential in music recommendation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹æµåª’ä½“å¹³å°ä¸­çš„ä¿¡æ¯è¿‡è½½é—®é¢˜ï¼Œæ¢è®¨äº†å°† Gemini å’Œ LLaMA ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ™ºèƒ½ä½“ï¼ˆintelligent agentsï¼‰ç»“åˆï¼Œæ„å»ºå¤šæ™ºèƒ½ä½“ä¸ªæ€§åŒ–éŸ³ä¹æ¨èç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚ç ”ç©¶äººå‘˜è¯¦ç»†å¯¹æ¯”äº†è¯¥ç³»ç»Ÿä¸ä¼ ç»ŸåŸºäºå†…å®¹çš„è¿‡æ»¤ï¼ˆcontent-based filteringï¼‰æ¨¡å‹åœ¨ç”¨æˆ·æ»¡æ„åº¦ã€æ¨èæ–°é¢–æ€§åŠè®¡ç®—æ•ˆç‡æ–¹é¢çš„è¡¨ç°å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº LLMs çš„æ¨èç³»ç»Ÿåœ¨ç”¨æˆ·æ»¡æ„åº¦æŒ‡æ ‡ä¸Šæœ€é«˜è¾¾åˆ°äº† 89.32%ï¼Œæ˜¾è‘—å±•ç¤ºäº†å…¶åœ¨æå‡ç”¨æˆ·ä½“éªŒæ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™ä¸€å‘ç°å°è¯äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£å¤æ‚ç”¨æˆ·åå¥½åŠä¼˜åŒ–ä¸ªæ€§åŒ–æ¨èç­–ç•¥ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºéŸ³ä¹æ¨èæŠ€æœ¯çš„æ™ºèƒ½åŒ–è½¬å‹æä¾›äº†é‡è¦çš„å®éªŒä¾æ®ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, in Portuguese language, 2 figures, 5 tables, 3 formulas. To be published in the Proceedings of the Encontro Nacional de InteligÃªncia Artificial e Computacional (ENIAC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.11671v1",
      "published_date": "2025-08-07 15:58:08 UTC",
      "updated_date": "2025-08-07 15:58:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:21.370736+00:00"
    },
    {
      "arxiv_id": "2508.05525v1",
      "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities",
      "title_zh": "LLM è§†è§’ä¸‹çš„ä¸–ç•Œï¼šåœ°ç†èµ·æºå¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹çš„å®ä½“æ¨æ–­èƒ½åŠ›",
      "authors": [
        "Harsh Nishant Lalai",
        "Raj Sanjay Shah",
        "Jiaxin Pei",
        "Sashank Varma",
        "Yi-Chia Wang",
        "Ali Emami"
      ],
      "abstract": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at https://sites.google.com/view/llmbias20q/home.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­æ½œè—çš„åœ°ç†åè§ï¼Œå¹¶æå‡ºé€šè¿‡ 20 Questions æ¸¸æˆè¿™ç§ä¸»åŠ¨æ¼”ç»ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„éšæ€§åè§ã€‚ä½œè€…æ„å»ºäº†åŒ…å«å…¨çƒä¸åŒåœ°åŒºäººç‰©å’Œæ–‡åŒ–å®ä½“çš„ Geo20Q+ æ•°æ®é›†ï¼Œåœ¨ä¸ƒç§è¯­è¨€å’Œå¤šç§æ¸¸æˆé…ç½®ä¸‹å¯¹ä¸»æµ LLMs è¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ã€‚ç ”ç©¶å‘ç°å­˜åœ¨æ˜¾è‘—çš„åœ°ç†æ€§èƒ½å·®å¼‚ï¼šæ¨¡å‹æ¨æ–­æ¥è‡ª Global North å’Œ Global West å®ä½“çš„æˆåŠŸç‡è¿œé«˜äº Global South å’Œ Global Eastã€‚è™½ç„¶ Wikipedia é¡µé¢æµè§ˆé‡å’Œè¯­æ–™åº“é¢‘ç‡ä¸æ€§èƒ½æœ‰ä¸€å®šç›¸å…³æ€§ï¼Œä½†æ— æ³•å®Œå…¨è§£é‡Šè¿™ç§å·®è·ï¼Œä¸”æ¸¸æˆè¯­è¨€å¯¹æ€§èƒ½å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¯¥å‘ç°è¯æ˜äº†è‡ªç”±å½¢å¼çš„è¯„ä¼°æ¡†æ¶åœ¨æ­éœ² LLMs æ¨ç†è¿‡ç¨‹ä¸­åµŒå…¥çš„åœ°ç†ä¸æ–‡åŒ–å·®å¼‚æ–¹é¢çš„ä»·å€¼ï¼Œä¸ºå‘ç°æ ‡å‡†æç¤ºè¯è®¾ç½®ä¸­éšè—çš„åè§æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Conference on Language Modeling 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05525v1",
      "published_date": "2025-08-07 15:53:30 UTC",
      "updated_date": "2025-08-07 15:53:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:23.744475+00:00"
    },
    {
      "arxiv_id": "2508.05513v1",
      "title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program",
      "title_zh": "åˆ©ç”¨ LOR æ´å¯Ÿä¼˜åŒ–å½•å–æµç¨‹ï¼šåœ¨çº¿ç¡•å£«é¡¹ç›®ä¸­çš„åŸºäºäººå·¥æ™ºèƒ½çš„é¢†å¯¼åŠ›è¯„ä¼°",
      "authors": [
        "Meryem Yilmaz Soylu",
        "Adrian Gallard",
        "Jeonghyun Lee",
        "Gayane Grigoryan",
        "Rushil Desai",
        "Stephen Harmon"
      ],
      "abstract": "Letters of recommendation (LORs) provide valuable insights into candidates' capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students' professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online master's program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants' leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates' capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç ”ç©¶ç”Ÿå½•å–è¿‡ç¨‹ä¸­æ¨èä¿¡ï¼ˆLORsï¼‰è¯„å®¡è€—æ—¶è€—åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºLORIï¼ˆLOR Insightsï¼‰çš„AIè¾…åŠ©è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨ä»åœ¨çº¿ç¡•å£«é¡¹ç›®çš„ç”³è¯·ææ–™ä¸­è¯†åˆ«é¢†å¯¼åŠ›ç‰¹å¾ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ä»¥åŠRoBERTaå’ŒLlamaç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒLORIèƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€æµ‹å›¢é˜Ÿåˆä½œï¼ˆteamworkï¼‰ã€æ²Ÿé€šï¼ˆcommunicationï¼‰å’Œåˆ›æ–°ï¼ˆinnovationï¼‰ç­‰å…³é”®èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶RoBERTaæ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŠ æƒF1åˆ†æ•°è¾¾åˆ°91.6%ï¼Œç²¾ç¡®åº¦ä¸º92.4%ï¼Œå¬å›ç‡ä¸º91.6%ï¼Œå±•ç¤ºäº†é«˜åº¦çš„è¯„ä¼°ä¸€è‡´æ€§ã€‚éšç€STEMé¢†åŸŸå¯¹é¢†å¯¼åŠ›é‡è§†ç¨‹åº¦çš„æå‡ï¼Œå°†LORIæ•´åˆè¿›ç ”ç©¶ç”Ÿå½•å–æµç¨‹ä¸ä»…èƒ½æ˜¾è‘—æå‡è¯„å®¡æ•ˆç‡ï¼Œè¿˜èƒ½ç¡®ä¿å¯¹å€™é€‰äººç»¼åˆèƒ½åŠ›çš„è‡ªåŠ¨åŒ–ç²¾å‡†è¯„ä¼°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05513v1",
      "published_date": "2025-08-07 15:46:59 UTC",
      "updated_date": "2025-08-07 15:46:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:29.553574+00:00"
    },
    {
      "arxiv_id": "2508.05509v3",
      "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective",
      "title_zh": "LAGï¼šåŸºäºç¬›å¡å°”è§†è§’çš„é€»è¾‘å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Yilin Xiao",
        "Chuang Zhou",
        "Yujing Zhang",
        "Qinggang Zhang",
        "Su Dong",
        "Shengyuan Chen",
        "Chang Yang",
        "Xiao Huang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet exhibit critical limitations in knowledge-intensive tasks, often generating hallucinations when faced with questions requiring specialized expertise. While retrieval-augmented generation (RAG) mitigates this by integrating external knowledge, it struggles with complex reasoning scenarios due to its reliance on direct semantic retrieval and lack of structured logical organization. Inspired by Cartesian principles from \\textit{Discours de la mÃ©thode}, this paper introduces Logic-Augmented Generation (LAG), a novel paradigm that reframes knowledge augmentation through systematic question decomposition, atomic memory bank and logic-aware reasoning. Specifically, LAG first decomposes complex questions into atomic sub-questions ordered by logical dependencies. It then resolves these sequentially, using prior answers to guide context retrieval for subsequent sub-questions, ensuring stepwise grounding in the logical chain. Experiments on four benchmarks demonstrate that LAG significantly improves accuracy and reduces hallucination over existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶å­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†LAGï¼ˆLogic-Augmented Generationï¼‰ã€‚è¯¥èŒƒå¼å—ã€Šè°ˆè°ˆæ–¹æ³•ã€‹ï¼ˆDiscours de la mÃ©thodeï¼‰ä¸­ç¬›å¡å°”åŸåˆ™å¯å‘ï¼Œé€šè¿‡ç³»ç»ŸåŒ–é—®é¢˜åˆ†è§£ã€åŸå­å­˜å‚¨åº“ï¼ˆatomic memory bankï¼‰å’Œé€»è¾‘æ„ŸçŸ¥æ¨ç†é‡æ–°å®šä¹‰äº†çŸ¥è¯†å¢å¼ºè¿‡ç¨‹ã€‚å…·ä½“è€Œè¨€ï¼ŒLAGå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå…·æœ‰é€»è¾‘ä¾èµ–å…³ç³»çš„åŸå­å­é—®é¢˜ï¼Œå¹¶æŒ‰åºé€ä¸€è§£å†³ï¼Œåˆ©ç”¨å‰åºç­”æ¡ˆæŒ‡å¯¼åç»­å­é—®é¢˜çš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œä»¥ç¡®ä¿æ¨ç†é“¾æ¡çš„æ¯ä¸€æ­¥éƒ½å…·å¤‡æ‰å®çš„è¯æ®æ”¯æ’‘ã€‚å®éªŒåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯æ˜ï¼ŒLAGåœ¨æå‡å›ç­”å‡†ç¡®æ€§åŠå‡å°‘å¹»è§‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°æ›´å…·é€»è¾‘æ€§ä¸”å¯ä¿¡çš„çŸ¥è¯†å¢å¼ºç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05509v3",
      "published_date": "2025-08-07 15:42:00 UTC",
      "updated_date": "2026-01-07 08:27:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:30.152404+00:00"
    },
    {
      "arxiv_id": "2508.05508v1",
      "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation",
      "title_zh": "Auto-Eval Judgeï¼šé¢å‘ä»»åŠ¡å®Œæˆè¯„ä¼°çš„é€šç”¨æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Roshita Bhonsle",
        "Rishav Dutta",
        "Sneha Vavilapalli",
        "Harsh Seth",
        "Abubakarr Jaye",
        "Yapei Chang",
        "Mukund Rungta",
        "Emmanuel Aboah Boateng",
        "Sadid Hasan",
        "Ehi Nosakhare",
        "Soundar Srinivasan"
      ],
      "abstract": "The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making. Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates another's task completion, are typically designed for narrow, domain-specific settings. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agent's output and reasoning. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench. Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This demonstrates the potential of our proposed general-purpose evaluation framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Auto-Eval Judgeï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ä½“ä»»åŠ¡å®Œæˆæƒ…å†µçš„é€šç”¨æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰ LLM-as-a-Judge æ–¹æ³•ä»…å…³æ³¨æœ€ç»ˆè¾“å‡ºè€Œå¿½è§†æ¨ç†è¿‡ç¨‹ï¼Œä»¥åŠç°æœ‰ Agent-as-a-Judge ç³»ç»Ÿé¢†åŸŸå±€é™æ€§å¼ºçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ç»“åˆæ™ºèƒ½ä½“çš„æ¨ç†é€»è¾‘ä¸è¾“å‡ºå¯¹æ¯ä¸€æ­¥è¿›è¡ŒéªŒè¯ï¼Œä»è€Œå®ç°ç±»äººåŒ–çš„è¯„ä¼°æµç¨‹ã€‚æ¡†æ¶å†…çš„å„åŠŸèƒ½æ¨¡å—åˆ†åˆ«è´Ÿè´£è¯„ä¼°çš„ä¸åŒç»´åº¦ï¼Œæœ€ç»ˆé€šè¿‡ç»“æœèšåˆç»™å‡ºä»»åŠ¡å®Œæˆåº¦çš„æœ€ç»ˆè£å®šã€‚ç ”ç©¶äººå‘˜åœ¨ GAIA å’Œ BigCodeBench åŸºå‡†æµ‹è¯•ä¸­å¯¹ Magentic-One Actor Agent è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥ Judge Agent ä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§åˆ†åˆ«æ¯”åŸºäº GPT-4o çš„ LLM-as-a-Judge åŸºå‡†é«˜å‡º 4.76% å’Œ 10.52%ã€‚è¯¥æˆæœè¯æ˜äº†é€šç”¨å‹è¯„ä¼°æ¡†æ¶åœ¨æå‡æ™ºèƒ½ä½“è¯„ä»·å‡†ç¡®æ€§ä¸è·¨é¢†åŸŸé€‚ç”¨æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–è¯„ä¼°å¤æ‚å†³ç­–ä»»åŠ¡æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05508v1",
      "published_date": "2025-08-07 15:39:48 UTC",
      "updated_date": "2025-08-07 15:39:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:36.242946+00:00"
    },
    {
      "arxiv_id": "2508.05502v1",
      "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs",
      "title_zh": "MELLAï¼šè¿æ¥ä½èµ„æºè¯­è¨€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ä¸æ–‡åŒ–æ¤æ ¹æ€§",
      "authors": [
        "Yufei Gao",
        "Jiaying Fei",
        "Nuo Chen",
        "Ruirui Chen",
        "Guohang Yan",
        "Yunshi Lan",
        "Botian Shi"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce \"thin descriptions\", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing \"thick descriptions\". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ä½èµ„æºè¯­è¨€ä¸­å› ç¼ºä¹æ–‡åŒ–æ¤æ ¹æ€§(cultural groundedness)å’Œå¤šæ¨¡æ€ä¿¡æ¯é‡è€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†è¯­è¨€èƒ½åŠ›(linguistic capability)ä¸æ–‡åŒ–æ¤æ ¹æ€§ä¸¤ä¸ªæ ¸å¿ƒæ”¹è¿›ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†åŒæºç­–ç•¥(dual-source strategy)ï¼Œç»“åˆåŸç”Ÿç½‘ç»œæ›¿ä»£æ–‡æœ¬(web alt-text)ä»¥å¢å¼ºæ–‡åŒ–æ„è¯†ï¼Œå¹¶åˆ©ç”¨MLLMç”Ÿæˆçš„æ ‡é¢˜æå‡è¯­è¨€ç†è§£èƒ½åŠ›ã€‚åŸºäºæ­¤ç­–ç•¥ï¼Œä½œè€…æ¨å‡ºäº†å¤šæ¨¡æ€å¤šè¯­è¨€æ•°æ®é›†MELLAã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨MELLAä¸Šå¾®è°ƒåçš„æ¨¡å‹åœ¨å…«ç§ä½èµ„æºè¯­è¨€çš„å¤šç§åŸºåº§æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„â€œæµ“åšæè¿°â€(thick descriptions)ã€‚è¯¥ç ”ç©¶è¯å®äº†æ€§èƒ½å¢ç›Šæºè‡ªæ–‡åŒ–çŸ¥è¯†ä¸è¯­è¨€èƒ½åŠ›çš„åŒé‡å¼ºåŒ–ï¼Œä¸ºæœ‰æ•ˆæœåŠ¡ä½èµ„æºè¯­è¨€ç”¨æˆ·æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05502v1",
      "published_date": "2025-08-07 15:36:24 UTC",
      "updated_date": "2025-08-07 15:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:38.955306+00:00"
    },
    {
      "arxiv_id": "2508.05498v1",
      "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning",
      "title_zh": "GRAILï¼šé¢å‘æ£€ç´¢å¢å¼ºæ¨ç†çš„å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±äº¤äº’å­¦ä¹ ",
      "authors": [
        "Ge Chang",
        "Jinbo Su",
        "Jiacheng Liu",
        "Pengfei Yang",
        "Yuhao Shang",
        "Huiwen Zheng",
        "Hongli Ma",
        "Yan Liang",
        "Yuanchun Li",
        "Yunxin Liu"
      ],
      "abstract": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) techniques have exhibited remarkable performance across a wide range of domains. However, existing RAG approaches primarily operate on unstructured data and demonstrate limited capability in handling structured knowledge such as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally struggle to capture holistic graph structures while simultaneously facing precision control challenges that manifest as either critical information gaps or excessive redundant connections, collectively undermining reasoning performance. To address this challenge, we propose GRAIL: Graph-Retrieval Augmented Interactive Learning, a framework designed to interact with large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL integrates LLM-guided random exploration with path filtering to establish a data synthesis pipeline, where a fine-grained reasoning trajectory is automatically generated for each task. Based on the synthesized data, we then employ a two-stage training process to learn a policy that dynamically decides the optimal actions at each reasoning step. The overall objective of precision-conciseness balance in graph retrieval is decoupled into fine-grained process-supervised rewards to enhance data efficiency and training stability. In practical deployment, GRAIL adopts an interactive retrieval paradigm, enabling the model to autonomously explore graph paths while dynamically balancing retrieval breadth and precision. Extensive experiments have shown that GRAIL achieves an average accuracy improvement of 21.01% and F1 improvement of 22.43% on three knowledge graph question-answering datasets. Our source code and datasets is available at https://github.com/Changgeww/GRAIL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GRAIL (Graph-Retrieval Augmented Interactive Learning)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸å¤§è§„æ¨¡çŸ¥è¯†å›¾è°± (Knowledge Graphs) è¿›è¡Œäº¤äº’ä»¥å®ç°æ£€ç´¢å¢å¼ºæ¨ç†çš„æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ RAG æŠ€æœ¯åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†æ—¶é¢ä¸´çš„å…¨å±€ç»“æ„æ•è·éš¾åŠç²¾åº¦æ§åˆ¶å·®ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLM) å¼•å¯¼çš„éšæœºæ¢ç´¢ä¸è·¯å¾„è¿‡æ»¤ï¼Œå»ºç«‹äº†ä¸€å¥—è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæµæ°´çº¿ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆç²¾ç»†çš„æ¨ç†è½¨è¿¹ã€‚éšåï¼ŒGRAIL é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹æ¥å­¦ä¹ åŠ¨æ€å†³ç­–ç­–ç•¥ï¼Œå¹¶å°†æ£€ç´¢çš„ç²¾å‡†æ€§ä¸ç®€æ´æ€§å¹³è¡¡ç›®æ ‡è§£è€¦ä¸ºç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£å¥–åŠ±ï¼Œä»è€Œæ˜¾è‘—æé«˜æ•°æ®æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼ŒGRAIL é‡‡ç”¨äº¤äº’å¼æ£€ç´¢èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å›¾è·¯å¾„å¹¶åŠ¨æ€å¹³è¡¡æ£€ç´¢å¹¿åº¦ä¸ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAIL åœ¨ä¸‰ä¸ªçŸ¥è¯†å›¾è°±é—®ç­” (KGQA) æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡ 21.01% çš„å‡†ç¡®ç‡æå‡å’Œ 22.43% çš„ F1 å€¼æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages,3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05498v1",
      "published_date": "2025-08-07 15:34:41 UTC",
      "updated_date": "2025-08-07 15:34:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:49.284681+00:00"
    },
    {
      "arxiv_id": "2508.05496v2",
      "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities",
      "title_zh": "InfiAlignï¼šä¸€ç§ç”¨äºå¢å¼ºæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•ä¸”æ ·æœ¬é«˜æ•ˆçš„ LLMs å¯¹é½æ¡†æ¶",
      "authors": [
        "Shuo Cai",
        "Su Lu",
        "Qi Zhou",
        "Kejing Yang",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
      ],
      "abstract": "Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InfiAlignï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•ä¸”é«˜æ•ˆçš„åè®­ç»ƒ (post-training) æ¡†æ¶ã€‚InfiAlign å°†æœ‰ç›‘ç£å¾®è°ƒ (SFT) ä¸ç›´æ¥åå¥½ä¼˜åŒ– (DPO) ç›¸ç»“åˆï¼Œå…¶æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸€ä¸ªåŸºäºå¤šç»´åº¦è´¨é‡æŒ‡æ ‡çš„æ•°æ®ç­›é€‰æµæ°´çº¿ï¼Œèƒ½å¤Ÿä»å¼€æºæ¨ç†æ•°æ®é›†ä¸­è‡ªåŠ¨æå–é«˜è´¨é‡å¯¹é½æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº Qwen2.5-Math-7B-Base å¼€å‘çš„ SFT æ¨¡å‹ä»…ä½¿ç”¨äº†çº¦ 12% çš„è®­ç»ƒæ•°æ®ï¼Œå°±è¾¾åˆ°äº†ä¸ DeepSeek-R1-Distill-Qwen-7B ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨è¿›ä¸€æ­¥åº”ç”¨ DPO åï¼Œæ¨¡å‹åœ¨ AIME 24/25 ç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº† 3.89%ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸåˆ™æ€§çš„æ•°æ®ç­›é€‰å’Œå…¨é˜¶æ®µåè®­ç»ƒï¼Œä¸ºå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„æ¨ç†æ¨¡å‹å¯¹é½æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05496v2",
      "published_date": "2025-08-07 15:34:06 UTC",
      "updated_date": "2025-08-12 08:26:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:43.048719+00:00"
    },
    {
      "arxiv_id": "2508.05492v1",
      "title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling",
      "title_zh": "MoMAï¼šç”¨äºå¢å¼ºä¸´åºŠé¢„æµ‹å»ºæ¨¡çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ··åˆæ¶æ„",
      "authors": [
        "Jifan Gao",
        "Mahmudur Rahman",
        "John Caskey",
        "Madeline Oguss",
        "Ann O'Rourke",
        "Randy Brown",
        "Anne Stey",
        "Anoop Mayampurath",
        "Matthew M. Churpek",
        "Guanhua Chen",
        "Majid Afshar"
      ],
      "abstract": "Multimodal electronic health record (EHR) data provide richer, complementary insights into patient health compared to single-modality data. However, effectively integrating diverse data modalities for clinical prediction modeling remains challenging due to the substantial data requirements. We introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed to leverage multiple large language model (LLM) agents for clinical prediction tasks using multimodal EHR data. MoMA employs specialized LLM agents (\"specialist agents\") to convert non-textual modalities, such as medical images and laboratory results, into structured textual summaries. These summaries, together with clinical notes, are combined by another LLM (\"aggregator agent\") to generate a unified multimodal summary, which is then used by a third LLM (\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three prediction tasks using real-world datasets with different modality combinations and prediction settings, MoMA outperforms current state-of-the-art methods, highlighting its enhanced accuracy and flexibility across various tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoMAï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€ä¸“å®¶æ··åˆæ™ºèƒ½ä½“(Mixture-of-Multimodal-Agents)æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•(EHR)æ•°æ®æ¥æå‡ä¸´åºŠé¢„æµ‹å»ºæ¨¡çš„æ€§èƒ½ã€‚é’ˆå¯¹å¤šæ¨¡æ€æ•°æ®é›†æˆä¸­é¢ä¸´çš„å·¨å¤§æ•°æ®éœ€æ±‚æŒ‘æˆ˜ï¼Œè¯¥æ¶æ„åˆ©ç”¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ååŒå·¥ä½œã€‚ç³»ç»Ÿé€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“(\"specialist agents\")å°†åŒ»ç–—å½±åƒå’Œå®éªŒå®¤ç»“æœç­‰éæ–‡æœ¬æ¨¡æ€è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬æ‘˜è¦ï¼Œå†ç”±èšåˆæ™ºèƒ½ä½“(\"aggregator agent\")å°†å…¶ä¸ä¸´åºŠç¬”è®°æ•´åˆä¸ºç»Ÿä¸€æ‘˜è¦ï¼Œæœ€åç”±é¢„æµ‹æ™ºèƒ½ä½“(\"predictor agent\")äº§å‡ºä¸´åºŠé¢„æµ‹ç»“æœã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„ä¸‰é¡¹é¢„æµ‹ä»»åŠ¡è¯„ä¼°è¡¨æ˜ï¼ŒMoMAåœ¨å‡†ç¡®æ€§å’Œçµæ´»æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•(state-of-the-art)ã€‚è¯¥ç ”ç©¶ä¸ºä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­å¤æ‚å¤šæ¨¡æ€æ•°æ®çš„æ·±åº¦èåˆä¸åˆ©ç”¨æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05492v1",
      "published_date": "2025-08-07 15:28:34 UTC",
      "updated_date": "2025-08-07 15:28:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:49.899268+00:00"
    },
    {
      "arxiv_id": "2508.06577v1",
      "title": "Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°å‚ä¸å¼é¢„ç®—ä¸­çš„éšç§æ„ŸçŸ¥é¢„æµ‹",
      "authors": [
        "Juan Zambrano",
        "ClÃ©ment Contet",
        "Jairo GudiÃ±o",
        "Felipe Garrido-Lucero",
        "Umberto Grandi",
        "Cesar A Hidalgo"
      ],
      "abstract": "Participatory Budgeting (PB) empowers citizens to propose and vote on public investment projects. Yet, despite its democratic potential, PB initiatives often suffer from low participation rates, limiting their visibility and perceived legitimacy. In this work, we aim to strengthen PB elections in two key ways: by supporting project proposers in crafting better proposals, and by helping PB organizers manage large volumes of submissions in a transparent manner. We propose a privacy-preserving approach to predict which PB proposals are likely to be funded, using only their textual descriptions and anonymous historical voting records -- without relying on voter demographics or personally identifiable information. We evaluate the performance of GPT 4 Turbo in forecasting proposal outcomes across varying contextual scenarios, observing that the LLM's prior knowledge needs to be complemented by past voting data to obtain predictions reflecting real-world PB voting behavior. Our findings highlight the potential of AI-driven tools to support PB processes by improving transparency, planning efficiency, and civic engagement.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å‚ä¸å¼é¢„ç®— (Participatory Budgeting, PB) ä¸­åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œéšç§ä¿æŠ¤é¢„æµ‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å‚ä¸ç‡ä½å’Œé¡¹ç›®ç®¡ç†æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ä»…ä¾èµ–é¡¹ç›®æ–‡æœ¬æè¿°å’ŒåŒ¿åå†å²æŠ•ç¥¨è®°å½•çš„é¢„æµ‹æ–¹æ¡ˆï¼Œæœ‰æ•ˆé¿å…äº†å¯¹äººå£ç»Ÿè®¡æ•°æ®æˆ–ä¸ªäººèº«ä»½ä¿¡æ¯çš„ä¾èµ–ã€‚é€šè¿‡è¯„ä¼° GPT-4 Turbo åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é¢„æµ‹è¡¨ç°ï¼Œç ”ç©¶å‘ç° LLMs è‡ªèº«çš„å…ˆéªŒçŸ¥è¯†å¿…é¡»è¾…ä»¥è¿‡å»çš„æŠ•ç¥¨æ•°æ®ï¼Œæ‰èƒ½ç”Ÿæˆå‡†ç¡®åæ˜ çœŸå®ä¸–ç•Œ PB æŠ•ç¥¨è¡Œä¸ºçš„é¢„æµ‹ç»“æœã€‚è¯¥ç ”ç©¶ç»“æœå±•ç¤ºäº† AI é©±åŠ¨çš„å·¥å…·åœ¨æå‡ PB æµç¨‹é€æ˜åº¦ã€è§„åˆ’æ•ˆç‡å’Œå…¬æ°‘å‚ä¸åº¦æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06577v1",
      "published_date": "2025-08-07 15:26:22 UTC",
      "updated_date": "2025-08-07 15:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:49:50.390278+00:00"
    },
    {
      "arxiv_id": "2508.05474v1",
      "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦ç”Ÿæˆæœ‰æ•ˆçš„ä¼šè¯æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†ï¼Ÿ",
      "authors": [
        "Burak Can Kaplan",
        "Hugo Cesar De Castro Carneiro",
        "Stefan Wermter"
      ],
      "abstract": "Emotion recognition in conversations (ERC) focuses on identifying emotion shifts within interactions, representing a significant step toward advancing machine intelligence. However, ERC data remains scarce, and existing datasets face numerous challenges due to their highly biased sources and the inherent subjectivity of soft labels. Even though Large Language Models (LLMs) have demonstrated their quality in many affective tasks, they are typically expensive to train, and their application to ERC tasks--particularly in data generation--remains limited. To address these challenges, we employ a small, resource-efficient, and general-purpose LLM to synthesize ERC datasets with diverse properties, supplementing the three most widely used ERC benchmarks. We generate six novel datasets, with two tailored to enhance each benchmark. We evaluate the utility of these datasets to (1) supplement existing datasets for ERC classification, and (2) analyze the effects of label imbalance in ERC. Our experimental results indicate that ERC classifier models trained on the generated datasets exhibit strong robustness and consistently achieve statistically significant performance improvements on existing ERC benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯¹è¯æƒ…æ„Ÿè¯†åˆ«(ERC)ä»»åŠ¡ä¸­ç”Ÿæˆæœ‰æ•ˆæ•°æ®é›†çš„æ½œåŠ›ï¼Œä»¥åº”å¯¹è¯¥é¢†åŸŸé¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€æ¥æºåè§åŠè½¯æ ‡ç­¾ä¸»è§‚æ€§ç­‰æŒ‘æˆ˜ã€‚ä½œè€…é‡‡ç”¨äº†ä¸€ç§å°å‹ã€èµ„æºé«˜æ•ˆçš„é€šç”¨LLMæ¥åˆæˆå…·æœ‰å¤šæ ·åŒ–å±æ€§çš„ERCæ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹ä¸‰ä¸ªä¸»æµåŸºå‡†æµ‹è¯•(Benchmarks)æ‰©å±•ç”Ÿæˆäº†å…­ä¸ªå…¨æ–°çš„æ•°æ®é›†ã€‚ç ”ç©¶é€šè¿‡å®éªŒè¯„ä¼°äº†è¿™äº›åˆæˆæ•°æ®åœ¨å¢å¼ºåˆ†ç±»å™¨æ€§èƒ½åŠåˆ†ææ ‡ç­¾ä¸å¹³è¡¡æ•ˆåº”æ–¹é¢çš„æ•ˆç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›ç”Ÿæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»æ¨¡å‹å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œå¹¶åœ¨ç°æœ‰çš„ERCåŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—ä¸”å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰çš„æ€§èƒ½æå‡ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†åˆ©ç”¨å°å‹LLMç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºå¯¹è¯ä¸­çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05474v1",
      "published_date": "2025-08-07 15:13:55 UTC",
      "updated_date": "2025-08-07 15:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:05.790402+00:00"
    },
    {
      "arxiv_id": "2508.05473v2",
      "title": "Embedding Alignment in Code Generation for Audio",
      "title_zh": "éŸ³é¢‘ä»£ç ç”Ÿæˆä¸­çš„åµŒå…¥å¯¹é½",
      "authors": [
        "Sam Kouteili",
        "Hiren Madhu",
        "George Typaldos",
        "Mark Santolucito"
      ],
      "abstract": "LLM-powered code generation has the potential to revolutionize creative coding endeavors, such as live-coding, by enabling users to focus on structural motifs over syntactic details. In such domains, when prompting an LLM, users may benefit from considering multiple varied code candidates to better realize their musical intentions. Code generation models, however, struggle to present unique and diverse code candidates, with no direct insight into the code's audio output. To better establish a relationship between code candidates and produced audio, we investigate the topology of the mapping between code and audio embedding spaces. We find that code and audio embeddings do not exhibit a simple linear relationship, but supplement this with a constructed predictive model that shows an embedding alignment map could be learned. Supplementing the aim for musically diverse output, we present a model that given code predicts output audio embedding, constructing a code-audio embedding alignment map.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾…åŠ©çš„éŸ³é¢‘ä»£ç ç”Ÿæˆï¼ˆå¦‚ç°åœºç¼–ç live-codingï¼‰ä¸­ï¼Œå¦‚ä½•å»ºç«‹ä»£ç å€™é€‰ä¸ç”ŸæˆéŸ³é¢‘ä¹‹é—´çš„è”ç³»ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹éš¾ä»¥æä¾›å¤šæ ·åŒ–å€™é€‰ä»£ç ä¸”æ— æ³•ç›´æ¥æ„ŸçŸ¥éŸ³é¢‘è¾“å‡ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…è°ƒæŸ¥äº†ä»£ç ä¸éŸ³é¢‘åµŒå…¥ç©ºé—´(embedding spaces)æ˜ å°„çš„æ‹“æ‰‘ç»“æ„ã€‚ç ”ç©¶å‘ç°ï¼Œä»£ç å’ŒéŸ³é¢‘çš„åµŒå…¥(embeddings)ä¹‹é—´å¹¶éç®€å•çš„çº¿æ€§å…³ç³»ï¼Œä½†é€šè¿‡æ„å»ºé¢„æµ‹æ¨¡å‹è¯æ˜äº†å­¦ä¹ åµŒå…¥å¯¹é½å›¾(embedding alignment map)çš„å¯è¡Œæ€§ã€‚åŸºäºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èƒ½å¤Ÿæ ¹æ®ä»£ç é¢„æµ‹éŸ³é¢‘åµŒå…¥çš„æ¨¡å‹ï¼ŒæˆåŠŸæ„å»ºäº†ä»£ç -éŸ³é¢‘åµŒå…¥å¯¹é½å›¾ã€‚è¯¥æˆæœæ—¨åœ¨å¢å¼ºç”Ÿæˆç»“æœçš„éŸ³ä¹å¤šæ ·æ€§ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨åˆ›æ„ç¼–ç¨‹è¿‡ç¨‹ä¸­æ›´å¥½åœ°å®ç°å…¶éŸ³ä¹æ„å›¾ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted to NeurIPS 2025 AI4Music Workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.05473v2",
      "published_date": "2025-08-07 15:13:42 UTC",
      "updated_date": "2025-09-23 18:29:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:07.389237+00:00"
    },
    {
      "arxiv_id": "2508.05464v2",
      "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
      "title_zh": "Bench-2-CoPï¼šåŸºå‡†æµ‹è¯•åœ¨æ¬§ç›Ÿäººå·¥æ™ºèƒ½åˆè§„ä¸­æ˜¯å¦å€¼å¾—ä¿¡èµ–ï¼Ÿ",
      "authors": [
        "Matteo Prandi",
        "Vincenzo Suriani",
        "Federico Pierucci",
        "Marcello Galisai",
        "Daniele Nardi",
        "Piercosma Bisconti"
      ],
      "abstract": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust evaluation frameworks, especially with emerging regulations like the EU AI Act and its associated Code of Practice (CoP). Current AI evaluation practices depend heavily on established benchmarks, but these tools were not designed to measure the systemic risks that are the focus of the new regulatory landscape. This research addresses the urgent need to quantify this \"benchmark-regulation gap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses validated LLM-as-judge analysis to map the coverage of 194,955 questions from widely-used benchmarks against the EU AI Act's taxonomy of model capabilities and propensities. Our findings reveal a profound misalignment: the evaluation ecosystem dedicates the vast majority of its focus to a narrow set of behavioral propensities. On average, benchmarks devote 61.6% of their regulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack of performance reliability\", while critical functional capabilities are dangerously neglected. Crucially, capabilities central to loss-of-control scenarios, including evading human oversight, self-replication, and autonomous AI development, receive zero coverage in the entire benchmark corpus. This study provides the first comprehensive, quantitative analysis of this gap, demonstrating that current public benchmarks are insufficient, on their own, for providing the evidence of comprehensive risk assessment required for regulatory compliance and offering critical insights for the development of next-generation evaluation tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½(GPAI)æ¨¡å‹åœ¨æ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act)åŠå…¶æ‰§ä¸šå®ˆåˆ™(Code of Practice)ç›‘ç®¡ä¸‹çš„è¯„ä¼°éœ€æ±‚ï¼Œæå‡ºäº†Bench-2-CoPç³»ç»Ÿæ€§æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–ç°æœ‰åŸºå‡†æµ‹è¯•ä¸ç›‘ç®¡è¦æ±‚ä¹‹é—´çš„â€œåŸºå‡†-ç›‘ç®¡ç¼ºå£â€(benchmark-regulation gap)ã€‚é€šè¿‡åˆ©ç”¨éªŒè¯è¿‡çš„LLM-as-judgeæ–¹æ³•åˆ†æçº¦19.5ä¸‡ä¸ªåŸºå‡†æµ‹è¯•é—®é¢˜ï¼Œç ”ç©¶å‘ç°è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿä¸ç›‘ç®¡åˆ†ç±»ä½“ç³»å­˜åœ¨ä¸¥é‡é”™ä½ï¼Œçº¦61.6%çš„é—®é¢˜é›†ä¸­äºâ€œå¹»è§‰å€¾å‘â€(Tendency to hallucinate)ï¼Œ31.2%é›†ä¸­äºâ€œæ€§èƒ½å¯é æ€§ç¼ºå¤±â€(Lack of performance reliability)ã€‚ä»¤äººæ‹…å¿§çš„æ˜¯ï¼Œä¸å¤±æ§åœºæ™¯(loss-of-control scenarios)å¯†åˆ‡ç›¸å…³çš„å…³é”®èƒ½åŠ›ï¼Œå¦‚è§„é¿äººç±»ç›‘ç£(evading human oversight)ã€è‡ªæˆ‘å¤åˆ¶(self-replication)åŠè‡ªä¸»AIå¼€å‘(autonomous AI development)ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¦†ç›–ç‡ä¸ºé›¶ã€‚è¿™é¡¹ç ”ç©¶é¦–æ¬¡å®šé‡è¯æ˜äº†å½“å‰çš„å…¬å¼€åŸºå‡†æµ‹è¯•(benchmarks)æ— æ³•ç‹¬ç«‹æä¾›æ³•è§„åˆè§„æ‰€éœ€çš„å…¨é¢é£é™©è¯„ä¼°è¯æ®ã€‚è¯¥å‘ç°å¼ºè°ƒäº†ç°æœ‰è¯„ä¼°å·¥å…·çš„å±€é™æ€§ï¼Œå¹¶ä¸ºå¼€å‘ç¬¦åˆç›‘ç®¡è¦æ±‚çš„ä¸‹ä¸€ä»£è¯„ä¼°å·¥å…·æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05464v2",
      "published_date": "2025-08-07 15:03:39 UTC",
      "updated_date": "2025-08-08 14:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:11.995511+00:00"
    },
    {
      "arxiv_id": "2508.05463v1",
      "title": "Task complexity shapes internal representations and robustness in neural networks",
      "title_zh": "ä»»åŠ¡å¤æ‚åº¦å¡‘é€ ç¥ç»ç½‘ç»œçš„å†…éƒ¨è¡¨å¾ä¸é²æ£’æ€§",
      "authors": [
        "Robert Jankowski",
        "Filippo Radicchi",
        "M. Ãngeles Serrano",
        "MariÃ¡n BoguÃ±Ã¡",
        "Santo Fortunato"
      ],
      "abstract": "Neural networks excel across a wide range of tasks, yet remain black boxes. In particular, how their internal representations are shaped by the complexity of the input data and the problems they solve remains obscure. In this work, we introduce a suite of five data-agnostic probes-pruning, binarization, noise injection, sign flipping, and bipartite network randomization-to quantify how task difficulty influences the topology and robustness of representations in multilayer perceptrons (MLPs). MLPs are represented as signed, weighted bipartite graphs from a network science perspective. We contrast easy and hard classification tasks on the MNIST and Fashion-MNIST datasets. We show that binarizing weights in hard-task models collapses accuracy to chance, whereas easy-task models remain robust. We also find that pruning low-magnitude edges in binarized hard-task models reveals a sharp phase-transition in performance. Moreover, moderate noise injection can enhance accuracy, resembling a stochastic-resonance effect linked to optimal sign flips of small-magnitude weights. Finally, preserving only the sign structure-instead of precise weight magnitudes-through bipartite network randomizations suffices to maintain high accuracy. These phenomena define a model- and modality-agnostic measure of task complexity: the performance gap between full-precision and binarized or shuffled neural network performance. Our findings highlight the crucial role of signed bipartite topology in learned representations and suggest practical strategies for model compression and interpretability that align with task complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»»åŠ¡å¤æ‚åº¦å¦‚ä½•å¡‘é€ ç¥ç»ç½‘ç»œçš„å†…éƒ¨è¡¨ç¤ºå’Œé²æ£’æ€§ï¼Œæå‡ºäº†ä¸€å¥—åŒ…å«å‰ªæ(pruning)ã€äºŒå€¼åŒ–(binarization)ã€å™ªå£°æ³¨å…¥(noise injection)ã€ç¬¦å·ç¿»è½¬(sign flipping)å’ŒäºŒåˆ†ç½‘ç»œéšæœºåŒ–(bipartite network randomization)çš„æ•°æ®ä¸å¯çŸ¥æ¢æµ‹å·¥å…·ã€‚ä½œè€…ä»ç½‘ç»œç§‘å­¦è§†è§’å°†å¤šå±‚æ„ŸçŸ¥æœº(MLPs)è§†ä¸ºå¸¦ç¬¦å·çš„åŠ æƒäºŒåˆ†å›¾ï¼Œå¯¹æ¯”äº†MNISTå’ŒFashion-MNISTæ•°æ®é›†ä¸Šçš„ç®€å•ä¸å›°éš¾åˆ†ç±»ä»»åŠ¡ã€‚å®éªŒå‘ç°ï¼ŒäºŒå€¼åŒ–æƒé‡ä¼šå¯¼è‡´å›°éš¾ä»»åŠ¡æ¨¡å‹çš„å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œè€Œç®€å•ä»»åŠ¡æ¨¡å‹åˆ™ä¿æŒé²æ£’ï¼›åŒæ—¶ï¼Œåœ¨äºŒå€¼åŒ–æ¨¡å‹ä¸­å‰ªæä½å¹…å€¼è¾¹ä¼šå¼•å‘æ€§èƒ½çš„å‰§çƒˆç›¸å˜ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°é€‚åº¦å™ªå£°æ³¨å…¥äº§ç”Ÿçš„éšæœºå…±æŒ¯(stochastic-resonance)æ•ˆåº”å¯æå‡å‡†ç¡®ç‡ï¼Œä¸”ä»…ä¿ç•™å¸¦ç¬¦å·çš„äºŒåˆ†æ‹“æ‰‘ç»“æ„è€Œéç²¾ç¡®æƒé‡å¹…å€¼å³å¯ç»´æŒé«˜æ€§èƒ½ã€‚è¿™äº›ç°è±¡å®šä¹‰äº†ä¸€ç§è¡¡é‡ä»»åŠ¡å¤æ‚åº¦çš„æ¨¡å‹é€šç”¨æŒ‡æ ‡ï¼Œå³å…¨ç²¾åº¦æ¨¡å‹ä¸äºŒå€¼åŒ–æˆ–éšæœºåŒ–æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¯¥å‘ç°å¼ºè°ƒäº†å¸¦ç¬¦å·äºŒåˆ†æ‹“æ‰‘åœ¨å­¦ä¹ è¡¨ç¤ºä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºæ¨¡å‹å‹ç¼©å’Œå¯è§£é‡Šæ€§æä¾›äº†ä¸ä»»åŠ¡å¤æ‚åº¦ç›¸åŒ¹é…çš„å®ç”¨ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05463v1",
      "published_date": "2025-08-07 15:02:39 UTC",
      "updated_date": "2025-08-07 15:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:20.590194+00:00"
    },
    {
      "arxiv_id": "2508.05454v1",
      "title": "EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting",
      "title_zh": "EnergyPatchTSTï¼šå…·æœ‰ä¸ç¡®å®šæ€§ä¼°è®¡çš„å¤šå°ºåº¦æ—¶é—´åºåˆ— Transformer èƒ½æºé¢„æµ‹æ¨¡å‹",
      "authors": [
        "Wei Li",
        "Zixin Wang",
        "Qizheng Sun",
        "Qixiang Gao",
        "Fenglei Yang"
      ],
      "abstract": "Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EnergyPatchTSTï¼Œè¿™æ˜¯é’ˆå¯¹èƒ½æºé¢„æµ‹ä¸“é—¨è®¾è®¡çš„ Patch Time Series Transformer æ‰©å±•æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³èƒ½æºæ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¤šå°ºåº¦åŠ¨æ€å’Œæ•°æ®ä¸è§„åˆ™æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å¤šå°ºåº¦ç‰¹å¾æå–æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å…·æœ‰ä¸åŒæ—¶é—´åˆ†è¾¨ç‡çš„ç‰¹å¾æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æ¦‚ç‡é¢„æµ‹æ¡†æ¶é€šè¿‡ Monte Carlo é‡‡æ ·å®ç°ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ•´åˆäº†æ¸©åº¦å’Œé£é€Ÿç­‰æœªæ¥å·²çŸ¥å˜é‡ï¼ˆfuture known variablesï¼‰ï¼Œå¹¶ç»“åˆ Pre-training å’Œ Fine-tuning èŒƒå¼ä»¥å¢å¼ºåœ¨æœ‰é™èƒ½æºæ•°æ®é›†ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚åœ¨å¸¸ç”¨èƒ½æºæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEnergyPatchTST çš„é¢„æµ‹è¯¯å·®ç›¸è¾ƒäºç°æœ‰ä¸»æµæ–¹æ³•é™ä½äº† 7-12%ï¼Œåœ¨æä¾›é«˜ç²¾åº¦é¢„æµ‹çš„åŒæ—¶ç»™å‡ºäº†å¯é çš„ä¸ç¡®å®šæ€§å‚è€ƒï¼Œä¸ºç”µåŠ›è§„åˆ’ä¸èµ„æºåˆ†é…æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at the International Conference on Intelligent Computing (ICIC 2025). 12 pages. The final authenticated version is published in the Lecture Notes in Computer Science (LNCS) series, vol 15860, and is available online. This is the author's version of the work submitted for peer review",
      "pdf_url": "https://arxiv.org/pdf/2508.05454v1",
      "published_date": "2025-08-07 14:48:39 UTC",
      "updated_date": "2025-08-07 14:48:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:21.986104+00:00"
    },
    {
      "arxiv_id": "2508.05441v1",
      "title": "Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees",
      "title_zh": "PAC çº§ä¿è¯ä¸‹çš„å°¾éƒ¨é£é™©å®‰å…¨è’™ç‰¹å¡æ´›æ ‘æœç´¢",
      "authors": [
        "Zuyuan Zhang",
        "Arnob Ghosh",
        "Tian Lan"
      ],
      "abstract": "Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $Î±$ achieves explicit tail-risk control over the expected loss in the \"worst $(1-Î±)\\%$ scenarios.\" Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\\mathcal{P}_{\\varepsilon_{s}}(s,a)$ with radius $\\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ Monte Carlo Tree Search (MCTS) ä»…å…³æ³¨æœŸæœ›æ”¶ç›Šè€Œå¿½è§†é«˜é£é™©æç«¯ç»“æœï¼ˆtail-riskï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§å…·å¤‡ PAC çº§åˆ«å®‰å…¨ä¿è¯çš„æ–°å‹è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œç ”ç©¶æå‡ºäº† CVaR-MCTSï¼Œé€šè¿‡å°† Conditional Value-at-Risk (CVaR) åµŒå…¥æœç´¢è¿‡ç¨‹ï¼Œå®ç°äº†å¯¹æœ€å·®åœºæ™¯ä¸‹é¢„æœŸæŸå¤±çš„æ˜¾å¼å°¾éƒ¨é£é™©æ§åˆ¶ã€‚å…¶æ¬¡ï¼Œä¸ºè§£å†³æ ·æœ¬æœ‰é™å¯¼è‡´çš„ä¼°è®¡åå·®ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº† Wasserstein-MCTS (W-MCTS)ï¼Œåˆ©ç”¨ä¸€é˜¶ Wasserstein æ¨¡ç³Šé›†ï¼ˆambiguity setï¼‰åˆ»ç”»å°¾éƒ¨é£é™©ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶è¯æ˜äº†ä¸¤ç§æ–¹æ³•å‡å…·æœ‰ PAC å°¾éƒ¨å®‰å…¨ä¿è¯å¹¶ç¡®ç«‹äº†å…¶é—æ†¾ç•Œ (regret)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§ä»¿çœŸç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨æå‡å¥–åŠ±å’Œç¨³å®šæ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°å®ç°äº†ç¨³å¥çš„å°¾éƒ¨é£é™©ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05441v1",
      "published_date": "2025-08-07 14:31:22 UTC",
      "updated_date": "2025-08-07 14:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:26.596268+00:00"
    },
    {
      "arxiv_id": "2508.05432v1",
      "title": "Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI",
      "title_zh": "è°çš„çœŸç†ï¼Ÿé¢å‘ï¼ˆæ™ºèƒ½ä½“ï¼‰äººå·¥æ™ºèƒ½çš„å¤šå…ƒåœ°ç†å¯¹é½",
      "authors": [
        "Krzysztof Janowicz",
        "Zilong Liu",
        "Gengchen Mai",
        "Zhangyu Wang",
        "Ivan Majic",
        "Alexandra Fortacz",
        "Grant McKenzie",
        "Song Gao"
      ],
      "abstract": "AI (super) alignment describes the challenge of ensuring (future) AI systems behave in accordance with societal norms and goals. While a quickly evolving literature is addressing biases and inequalities, the geographic variability of alignment remains underexplored. Simply put, what is considered appropriate, truthful, or legal can differ widely across regions due to cultural norms, political realities, and legislation. Alignment measures applied to AI/ML workflows can sometimes produce outcomes that diverge from statistical realities, such as text-to-image models depicting balanced gender ratios in company leadership despite existing imbalances. Crucially, some model outputs are globally acceptable, while others, e.g., questions about Kashmir, depend on knowing the user's location and their context. This geographic sensitivity is not new. For instance, Google Maps renders Kashmir's borders differently based on user location. What is new is the unprecedented scale and automation with which AI now mediates knowledge, expresses opinions, and represents geographic reality to millions of users worldwide, often with little transparency about how context is managed. As we approach Agentic AI, the need for spatio-temporally aware alignment, rather than one-size-fits-all approaches, is increasingly urgent. This paper reviews key geographic research problems, suggests topics for future work, and outlines methods for assessing alignment sensitivity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†(Agentic) AIåœ¨AI alignmentï¼ˆäººå·¥æ™ºèƒ½å¯¹é½ï¼‰è¿‡ç¨‹ä¸­é¢ä¸´çš„åœ°ç†å·®å¼‚æ€§æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†æ‰€è°“â€œçœŸç†â€æˆ–â€œåˆè§„â€åœ¨ä¸åŒæ–‡åŒ–ã€æ”¿æ²»åŠæ³•å¾‹èƒŒæ™¯ä¸‹çš„å¤šå…ƒæ€§ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„å•ä¸€å¯¹é½æ–¹å¼ï¼ˆone-size-fits-all approachesï¼‰åœ¨å¤„ç†ç‰¹å®šåœ°ç†è¯­å¢ƒï¼ˆå¦‚è¾¹ç•Œäº‰è®®æˆ–ç¤¾ä¼šä»£è¡¨æ€§ï¼‰æ—¶å¾€å¾€äº§ç”Ÿåè§ï¼Œä¸”ç¼ºä¹å¤„ç†å¤æ‚è¯­å¢ƒçš„é€æ˜åº¦ã€‚éšç€Agentic AIçš„æ™®åŠï¼Œç ”ç©¶æå‡ºäºŸéœ€å»ºç«‹å…·æœ‰spatio-temporally aware alignmentï¼ˆæ—¶ç©ºæ„ŸçŸ¥å¯¹é½ï¼‰èƒ½åŠ›çš„ç³»ç»Ÿï¼Œä»¥å–ä»£ç›®å‰çš„é€šç”¨å¯¹é½æ ‡å‡†ã€‚æœ¬æ–‡è¯¦ç»†æ¢³ç†äº†åœ°ç†ç ”ç©¶ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶ç•Œå®šäº†Pluralistic Geo-Alignmentï¼ˆå¤šå…ƒåœ°ç†å¯¹é½ï¼‰çš„ç†è®ºæ¡†æ¶ã€‚é€šè¿‡æå‡ºè¯„ä¼°å¯¹é½æ•æ„Ÿæ€§çš„å…·ä½“æ–¹æ³•ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºèƒ½å¤Ÿé€‚åº”å…¨çƒå¤šå…ƒåœ°ç†è§„èŒƒã€å…·å¤‡é«˜åº¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„AIæŠ€æœ¯æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05432v1",
      "published_date": "2025-08-07 14:21:33 UTC",
      "updated_date": "2025-08-07 14:21:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:43.291960+00:00"
    },
    {
      "arxiv_id": "2508.05430v2",
      "title": "Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions",
      "title_zh": "åŸºäºåŠ æƒ Banzhaf äº¤äº’ä½œç”¨çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ç›¸ä¼¼æ€§è§£é‡Š",
      "authors": [
        "Hubert Baniecki",
        "Maximilian Muschalik",
        "Fabian Fumagalli",
        "Barbara Hammer",
        "Eyke HÃ¼llermeier",
        "Przemyslaw Biecek"
      ],
      "abstract": "Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, such as the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on the MS COCO and ImageNet-1k benchmarks validate that second-order methods, such as FIxLIP, outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models, e.g. CLIP vs. SigLIP-2.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€ç¼–ç å™¨ï¼ˆvision-language encodersï¼‰ä¸­ç›¸ä¼¼æ€§è¾“å‡ºçš„è§£é‡Šé—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ˜¾è‘—æ€§å›¾ï¼ˆsaliency mapsï¼‰ä»…èƒ½æ•æ‰ä¸€é˜¶å±æ€§ï¼Œä»è€Œå¿½ç•¥äº†æ¨¡å‹å†…éƒ¨å¤æ‚çš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸º FIxLIP çš„ç»Ÿä¸€åˆ†è§£æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆLIPï¼‰æ¨¡å‹æä¾›å¿ å®çš„äº¤äº’è§£é‡Šã€‚è¯¥æ–¹æ³•æ¤æ ¹äºåšå¼ˆè®ºï¼Œåˆ©ç”¨åŠ æƒ Banzhaf äº¤äº’æŒ‡æ•°ï¼ˆweighted Banzhaf interaction indexï¼‰å®ç°äº†æ¯”ä¼ ç»Ÿ Shapley æ¡†æ¶æ›´é«˜çš„çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚åœ¨å®è·µå±‚é¢ï¼Œç ”ç©¶è€…å°† pointing game ä»¥åŠæ’å…¥/åˆ é™¤æ›²çº¿ï¼ˆinsertion/deletion curvesï¼‰ä¸‹çš„é¢ç§¯ç­‰è¯„ä¼°æŒ‡æ ‡æ‰©å±•åˆ°äº†äºŒé˜¶äº¤äº’è§£é‡Šé¢†åŸŸã€‚åœ¨ MS COCO å’Œ ImageNet-1k åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFIxLIP ç­‰äºŒé˜¶è§£é‡Šæ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¸€é˜¶å±æ€§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¯æ˜äº† FIxLIP åœ¨å®šé‡æ¯”è¾ƒä¸åŒæ¨¡å‹ï¼ˆå¦‚ CLIP ä¸ SigLIP-2ï¼‰å·®å¼‚æ–¹é¢çš„å®ç”¨å·¥å…·ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025. Code: https://github.com/hbaniecki/fixlip",
      "pdf_url": "https://arxiv.org/pdf/2508.05430v2",
      "published_date": "2025-08-07 14:18:56 UTC",
      "updated_date": "2025-11-18 15:05:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:32.090741+00:00"
    },
    {
      "arxiv_id": "2508.09184v1",
      "title": "HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting",
      "title_zh": "HiSTMï¼šé¢å‘èœ‚çªæµé‡é¢„æµ‹çš„åˆ†å±‚æ—¶ç©º Mamba",
      "authors": [
        "Zineddine Bettouche",
        "Khalid Ali",
        "Andreas Fischer",
        "Andreas Kassler"
      ],
      "abstract": "Cellular traffic forecasting is essential for network planning, resource allocation, or load-balancing traffic across cells. However, accurate forecasting is difficult due to intricate spatial and temporal patterns that exist due to the mobility of users. Existing AI-based traffic forecasting models often trade-off accuracy and computational efficiency. We present Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial encoder with a Mamba-based temporal module and attention mechanism. HiSTM employs selective state space methods to capture spatial and temporal patterns in network traffic. In our evaluation, we use a real-world dataset to compare HiSTM against several baselines, showing a 29.4% MAE improvement over the STN baseline while using 94% fewer parameters. We show that the HiSTM generalizes well across different datasets and improves in accuracy over longer time-horizons.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiSTM (Hierarchical Spatiotemporal Mamba)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºèœ‚çªæµé‡é¢„æµ‹ (Cellular traffic forecasting) çš„å±‚çº§æ—¶ç©ºæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·ç§»åŠ¨æ€§å¸¦æ¥çš„å¤æ‚æ—¶ç©ºæ¨¡å¼ä»¥åŠç°æœ‰æ¨¡å‹åœ¨ç²¾åº¦ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŒé‡ç©ºé—´ç¼–ç å™¨ (Dual spatial encoder) ä¸åŸºäº Mamba çš„æ—¶é—´æ¨¡å— (Temporal module) åŠæ³¨æ„åŠ›æœºåˆ¶ (Attention mechanism)ï¼Œå¹¶åˆ©ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ–¹æ³• (Selective state space methods) æ•æ‰ç½‘ç»œæµé‡ä¸­çš„æ—¶ç©ºç‰¹å¾ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†çš„è¯„ä¼°ä¸­ï¼ŒHiSTM ç›¸æ¯” STN åŸºçº¿æ¨¡å‹åœ¨å¹³å‡ç»å¯¹è¯¯å·® (MAE) ä¸Šæå‡äº† 29.4%ï¼Œä¸”å‚æ•°é‡æ˜¾è‘—å‡å°‘äº† 94%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiSTM åœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨é•¿æ—¶é¢„æµ‹ç»´åº¦ä¸Šå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä¸ºç½‘ç»œè§„åˆ’ã€èµ„æºåˆ†é…å’Œè´Ÿè½½å‡è¡¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09184v1",
      "published_date": "2025-08-07 14:18:18 UTC",
      "updated_date": "2025-08-07 14:18:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:42.886990+00:00"
    },
    {
      "arxiv_id": "2508.05429v2",
      "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints",
      "title_zh": "MyCultureï¼šä½èµ„æºè¯­è¨€çº¦æŸä¸‹çš„é©¬æ¥è¥¿äºšå¤šå…ƒæ–‡åŒ–æ¢ç´¢",
      "authors": [
        "Zhong Ken Hew",
        "Jia Xin Low",
        "Sze Jue Yang",
        "Chee Seng Chan"
      ],
      "abstract": "Large Language Models (LLMs) often exhibit cultural biases due to training data dominated by high-resource languages like English and Chinese. This poses challenges for accurately representing and evaluating diverse cultural contexts, particularly in low-resource language settings. To address this, we introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on Malaysian culture across six pillars: arts, attire, customs, entertainment, food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks, MyCulture employs a novel open-ended multiple-choice question format without predefined options, thereby reducing guessing and mitigating format bias. We provide a theoretical justification for the effectiveness of this open-ended structure in improving both fairness and discriminative power. Furthermore, we analyze structural bias by comparing model performance on structured versus free-form outputs, and assess language bias through multilingual prompt variations. Our evaluation across a range of regional and international LLMs reveals significant disparities in cultural comprehension, highlighting the urgent need for culturally grounded and linguistically inclusive benchmarks in the development and assessment of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ä½èµ„æºè¯­è¨€èƒŒæ™¯ä¸‹çš„å¤šå…ƒæ–‡åŒ–æ—¶å­˜åœ¨çš„åè§é—®é¢˜ï¼Œæå‡ºäº†MyCultureè¿™ä¸€é’ˆå¯¹é©¬æ¥è¥¿äºšæ–‡åŒ–çš„è¯„ä¼°åŸºå‡†ã€‚MyCultureæ¶µç›–äº†è‰ºæœ¯ã€æœé¥°ã€ä¹ ä¿—ã€å¨±ä¹ã€é¥®é£Ÿå’Œå®—æ•™å…­å¤§é¢†åŸŸï¼Œå¹¶ä»¥é©¬æ¥è¯­ï¼ˆBahasa Melayuï¼‰å‘ˆç°ï¼Œæ—¨åœ¨å…¨é¢è¡¡é‡æ¨¡å‹çš„æ–‡åŒ–ç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„æ— é¢„è®¾é€‰é¡¹å¼€æ”¾å¼å¤šé¡¹é€‰æ‹©é¢˜æ ¼å¼ï¼ˆOpen-ended Multiple-Choice Question formatï¼‰ï¼Œé€šè¿‡å‡å°‘ç›²ç›®çŒœæµ‹æ¥ç¼“è§£æ ¼å¼åè§ï¼ˆFormat Biasï¼‰ï¼Œå¹¶æä¾›äº†æå‡è¯„ä¼°å…¬å¹³æ€§ä¸åˆ¤åˆ«åŠ›çš„ç†è®ºè¯æ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡å¤šè¯­è¨€æç¤ºè¯å˜ä½“æ·±å…¥åˆ†æäº†ç»“æ„åå·®ï¼ˆStructural Biasï¼‰å’Œè¯­è¨€åå·®ï¼ˆLanguage Biasï¼‰ã€‚å¯¹åŒºåŸŸæ€§åŠå›½é™…åŒ–LLMsçš„è¯„ä¼°ç»“æœæ­ç¤ºäº†å„æ¨¡å‹åœ¨æ–‡åŒ–è®¤çŸ¥ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå‡¸æ˜¾äº†åœ¨LLMså¼€å‘ä¸­å»ºç«‹å…·å¤‡æ–‡åŒ–æ ¹åŸºå’Œè¯­è¨€åŒ…å®¹æ€§åŸºå‡†çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05429v2",
      "published_date": "2025-08-07 14:17:43 UTC",
      "updated_date": "2025-08-08 01:24:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:50:44.590396+00:00"
    },
    {
      "arxiv_id": "2508.05427v1",
      "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æœ‰æœºåˆæˆå˜é©ï¼šä»ååº”é¢„æµ‹åˆ°è‡ªåŠ¨åŒ–",
      "authors": [
        "Kartar Kumar Lohana Tharwani",
        "Rajesh Kumar",
        "Sumita",
        "Numan Ahmed",
        "Yong Tang"
      ],
      "abstract": "Large language models (LLMs) are beginning to reshape how chemists plan and run reactions in organic synthesis. Trained on millions of reported transformations, these text-based models can propose synthetic routes, forecast reaction outcomes and even instruct robots that execute experiments without human supervision. Here we survey the milestones that turned LLMs from speculative tools into practical lab partners. We show how coupling LLMs with graph neural networks, quantum calculations and real-time spectroscopy shrinks discovery cycles and supports greener, data-driven chemistry. We discuss limitations, including biased datasets, opaque reasoning and the need for safety gates that prevent unintentional hazards. Finally, we outline community initiatives open benchmarks, federated learning and explainable interfaces that aim to democratize access while keeping humans firmly in control. These advances chart a path towards rapid, reliable and inclusive molecular innovation powered by artificial intelligence and automation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¦‚ä½•åœ¨æœ‰æœºåˆæˆé¢†åŸŸå®ç°ä»ååº”é¢„æµ‹åˆ°è‡ªåŠ¨åŒ–çš„è½¬å‹ï¼Œå±•ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨è§„åˆ’åˆæˆè·¯å¾„ã€é¢„æµ‹ååº”ç»“æœä»¥åŠæŒ‡å¯¼æœºå™¨äººè‡ªä¸»å®éªŒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å°† LLMs ä¸å›¾ç¥ç»ç½‘ç»œ (Graph Neural Networks)ã€é‡å­è®¡ç®— (Quantum Calculations) åŠå®æ—¶å…‰è°±æŠ€æœ¯ (Real-time Spectroscopy) ç›¸ç»“åˆçš„é‡Œç¨‹ç¢‘å¼è¿›å±•ï¼Œè¿™ç§ç»“åˆæœ‰æ•ˆç¼©çŸ­äº†ç ”å‘å‘¨æœŸå¹¶æ¨åŠ¨äº†æ•°æ®é©±åŠ¨çš„ç»¿è‰²åŒ–å­¦ã€‚åŒæ—¶ä¹Ÿæ·±å…¥è®¨è®ºäº†å½“å‰é¢ä¸´çš„å±€é™æ€§ï¼Œå¦‚æ•°æ®é›†åå·® (Biased Datasets)ã€æ¨ç†ä¸é€æ˜æ€§ä»¥åŠå»ºç«‹å®‰å…¨é—¸é—¨ä»¥è§„é¿æ½œåœ¨é£é™©çš„å¿…è¦æ€§ã€‚ä¸ºå®ç°æ›´å¯é çš„åˆ†å­åˆ›æ–°ï¼Œä½œè€…æå‡ºäº†å¼€æ”¾åŸºå‡† (Open Benchmarks) å’Œè”é‚¦å­¦ä¹  (Federated Learning) ç­‰ç¤¾åŒºå€¡è®®ï¼Œæ—¨åœ¨æå‡æŠ€æœ¯é€æ˜åº¦çš„åŒæ—¶ç¡®ä¿äººç±»çš„æ ¸å¿ƒä¸»å¯¼åœ°ä½ã€‚è¿™äº›è¿›å±•ä¸ºäººå·¥æ™ºèƒ½ä¸è‡ªåŠ¨åŒ–æŠ€æœ¯é©±åŠ¨çš„å¿«é€Ÿã€å¯é ä¸”åŒ…å®¹çš„åˆ†å­ç ”å‘æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05427v1",
      "published_date": "2025-08-07 14:17:23 UTC",
      "updated_date": "2025-08-07 14:17:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:21.059716+00:00"
    },
    {
      "arxiv_id": "2508.05421v1",
      "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
      "title_zh": "åŸºäº LLM çš„é‡å­ä¼ æ„Ÿå™¨å¤šæ™ºèƒ½ä½“ Copilot",
      "authors": [
        "Rong Sha",
        "Binglin Wang",
        "Jun Yang",
        "Xiaoxiao Ma",
        "Chengkun Wu",
        "Liang Yan",
        "Chao Zhou",
        "Jixun Liu",
        "Guochao Wang",
        "Shuhua Yan",
        "Lingxiao Zhu"
      ],
      "abstract": "Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\\rm{8}}$ sub-$\\rmÎ¼$K atoms without any human intervention within a few hours, representing $\\sim$100$\\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QCopilotï¼Œä¸€ç§åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœé‡å­ä¼ æ„Ÿå™¨å¼€å‘ä¸­çš„è·¨å­¦ç§‘çŸ¥è¯†éšœç¢å’Œå¤æ‚ä¼˜åŒ–éš¾é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†external knowledge accessã€active learningå’Œuncertainty quantificationï¼Œåˆ©ç”¨ä¸“é—¨çš„æ™ºèƒ½ä½“å®ç°è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³•é€‰æ‹©ã€è‡ªåŠ¨åŒ–å»ºæ¨¡åˆ†æåŠç‹¬ç«‹çš„é—®é¢˜è¯Šæ–­ã€‚é€šè¿‡few-shot prompt engineeringå’Œvector knowledge baseï¼ŒQCopilotèƒ½å¤ŸæŒç»­ç§¯ç´¯å…ˆéªŒçŸ¥è¯†å¹¶è‡ªä¸»è¯†åˆ«å¤šå‚æ•°è®¾ç½®ä¸­çš„å¼‚å¸¸å‚æ•°ã€‚åœ¨åŸå­å†·å´å®éªŒä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æ•°å°æ—¶å†…ç”Ÿæˆäº†10^8ä¸ªsub-ÂµKåŸå­ï¼Œå®éªŒæ•ˆç‡æ¯”äººå·¥æ“ä½œæé«˜äº†çº¦100å€ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—é™ä½äº†å¤§å°ºåº¦é‡å­ä¼ æ„Ÿå™¨éƒ¨ç½²çš„é—¨æ§›ï¼Œå¹¶å¯å¹¿æ³›æ‰©å±•è‡³å…¶ä»–quantum information systemsã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "physics.atom-ph"
      ],
      "primary_category": "quant-ph",
      "comment": "13 pages,4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05421v1",
      "published_date": "2025-08-07 14:14:08 UTC",
      "updated_date": "2025-08-07 14:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:13.363832+00:00"
    },
    {
      "arxiv_id": "2508.05405v1",
      "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning",
      "title_zh": "DeepPHYï¼šé’ˆå¯¹æ™ºèƒ½ä½“ VLM ç‰©ç†æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Xinrun Xu",
        "Pi Bu",
        "Ye Wang",
        "BÃ¶rje F. Karlsson",
        "Ziming Wang",
        "Tengtao Song",
        "Qi Zhu",
        "Jun Song",
        "Zhiming Ding",
        "Bo Zheng"
      ],
      "abstract": "Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹ç¼ºä¹ç²¾ç¡®åŠ¨ä½œè§„åˆ’å’Œç»†èŠ‚å…³æ³¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºDeepPHYçš„åˆ›æ–°åŸºå‡†æ¡†æ¶ã€‚DeepPHYæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç³»ç»Ÿåœ°è¯„ä¼°VLMså¯¹åŸºç¡€ç‰©ç†åŸåˆ™(Physical Reasoning)çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤šç§ä¸åŒéš¾åº¦ç­‰çº§çš„ç‰©ç†æ¨ç†ç¯å¢ƒï¼Œå¹¶å¼•å…¥äº†ç»†ç²’åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’åŠç­–ç•¥ä¼˜åŒ–ç­‰æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„VLMsï¼Œä¹Ÿéš¾ä»¥å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†æœ‰æ•ˆè½¬åŒ–ä¸ºç²¾ç¡®çš„é¢„æµ‹æ€§æ§åˆ¶ã€‚DeepPHYçš„æ¨å‡ºä¸ºå¼¥åˆæ¨¡æ‹Ÿç¯å¢ƒä¸é«˜æˆæœ¬ç°å®ä»»åŠ¡ä¹‹é—´çš„å·®è·æä¾›äº†å·¥å…·ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘å…·å¤‡ç‰©ç†é€»è¾‘çš„è‡ªä¸»æ™ºèƒ½ä½“(Agentic VLMs)æä¾›äº†é‡è¦çš„è¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "48 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.05405v1",
      "published_date": "2025-08-07 13:58:19 UTC",
      "updated_date": "2025-08-07 13:58:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:14.185747+00:00"
    },
    {
      "arxiv_id": "2510.21710v1",
      "title": "A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems",
      "title_zh": "åˆ†å¸ƒå¼å®æ—¶æ”¯ä»˜ç³»ç»Ÿä¸­é¢å‘ä¸šåŠ¡å½±å“çš„æ•…éšœæ£€æµ‹ç‰¹å¾å·¥ç¨‹æ–¹æ³•",
      "authors": [
        "Lorenzo Porcelli"
      ],
      "abstract": "Instant payment infrastructures have stringent performance requirements, processing millions of transactions daily with zero-downtime expectations. Traditional monitoring approaches fail to bridge the gap between technical infrastructure metrics and business process visibility. We introduce a novel feature engineering approach based on processing times computed between consecutive ISO 20022 message exchanges, creating a compact representation of system state. By applying anomaly detection to these features, we enable early failure detection and localization, allowing incident classification. Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system, using both real-world incidents and controlled simulations, demonstrates the approach's effectiveness in detecting diverse anomaly patterns and provides inherently interpretable explanations that enable operators to understand the business impact. By mapping features to distinct processing phases, the resulting framework differentiates between internal and external payment system issues, significantly reduces investigation time, and bridges observability gaps in distributed systems where transaction state is fragmented across multiple entities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¼å³æ—¶æ”¯ä»˜ç³»ç»Ÿæå‡ºäº†ä¸€ç§é¢å‘ä¸šåŠ¡å½±å“çš„æ•…éšœæ£€æµ‹ç‰¹å¾å·¥ç¨‹(Feature Engineering)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç›‘æ§åœ¨æŠ€æœ¯æŒ‡æ ‡ä¸ä¸šåŠ¡æµç¨‹å¯è§æ€§ä¹‹é—´çš„è„±èŠ‚é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®— ISO 20022 æ¶ˆæ¯äº¤æ¢ä¹‹é—´çš„å¤„ç†æ—¶é—´(Processing Times)æ¥æ„å»ºç³»ç»ŸçŠ¶æ€çš„ç´§å‡‘è¡¨ç¤ºï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåº”ç”¨å¼‚å¸¸æ£€æµ‹(Anomaly Detection)å®ç°æ—©æœŸæ•…éšœå‘ç°ä¸å®šä½ã€‚åœ¨ TARGET Instant Payment Settlement (TIPS) ç³»ç»Ÿä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆèƒ½æœ‰æ•ˆè¯†åˆ«å¤šç§å¼‚å¸¸æ¨¡å¼ï¼Œå¹¶æä¾›å…³äºä¸šåŠ¡å½±å“çš„å¯è§£é‡Šæ€§è¯´æ˜ã€‚é€šè¿‡å°†ç‰¹å¾æ˜ å°„è‡³ç‰¹å®šçš„å¤„ç†é˜¶æ®µï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å†…éƒ¨ä¸å¤–éƒ¨ç³»ç»Ÿæ•…éšœï¼Œæ˜¾è‘—ç¼©çŸ­äº†äº‹æ•…è°ƒæŸ¥æ—¶é—´ï¼Œå¼¥è¡¥äº†åˆ†å¸ƒå¼æ”¯ä»˜ç³»ç»Ÿä¸­äº¤æ˜“çŠ¶æ€ç¢ç‰‡åŒ–å¯¼è‡´çš„è§‚æµ‹æ€§ç¼ºå£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21710v1",
      "published_date": "2025-08-07 13:56:02 UTC",
      "updated_date": "2025-08-07 13:56:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:15.085769+00:00"
    },
    {
      "arxiv_id": "2508.06575v1",
      "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios",
      "title_zh": "åŸºäºäº‹æ•…è¡ç”Ÿåœºæ™¯è‡ªé€‚åº”æœç´¢çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦é«˜æ•ˆå®‰å…¨æµ‹è¯•",
      "authors": [
        "Rui Zhou"
      ],
      "abstract": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their development and deployment. Safety-critical scenarios pose more severe challenges, necessitating efficient testing methods to validate AVs safety. This study focuses on designing an accelerated testing algorithm for AVs in safety-critical scenarios, enabling swift recognition of their driving capabilities. First, typical logical scenarios were extracted from real-world crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA) database, obtaining pre-crash features through reconstruction. Second, Baidu Apollo, an advanced black-box automated driving system (ADS) is integrated to control the behavior of the ego vehicle. Third, we proposed an adaptive large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to expedite the testing process. Experimental results demonstrate a significant enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an 84.00% coverage of safety-critical scenarios, with crash scenario coverage of 96.83% and near-crash scenario coverage of 92.07%. Compared to genetic algorithm (GA), adaptive large neighborhood-simulated annealing algorithm (ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage in safety-critical scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶æ±½è½¦ (Autonomous Vehicles) åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸‹çš„æµ‹è¯•æ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº‹æ•…è¡ç”Ÿåœºæ™¯è‡ªé€‚åº”æœç´¢çš„åŠ é€Ÿæµ‹è¯•æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆä»ä¸­å›½æ·±å…¥äº¤é€šäº‹æ•…è°ƒæŸ¥ç ”ç©¶ (CIMSS-TA) æ•°æ®åº“çš„çœŸå®äº‹æ•…ä¸­æå–å…¸å‹é€»è¾‘åœºæ™¯ï¼Œé€šè¿‡é‡å»ºæŠ€æœ¯è·å–ç¢°æ’å‰ç‰¹å¾ï¼Œå¹¶é›†æˆç™¾åº¦ Apollo è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ (ADS) ä»¥æ§åˆ¶ä¸»è½¦è¡Œä¸ºã€‚è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¤§å˜é‡é‚»åŸŸæ¨¡æ‹Ÿé€€ç«ç®—æ³• (ALVNS-SA)ï¼Œç”¨äºåœ¨å¤æ‚çš„æœç´¢ç©ºé—´ä¸­å¿«é€Ÿè¯†åˆ«å®‰å…¨å…³é”®åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒALVNS-SA åœ¨æµ‹è¯•æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºé—ä¼ ç®—æ³• (GA) å’Œéšæœºæµ‹è¯•ï¼Œå®ç°äº† 84.00% çš„å®‰å…¨å…³é”®åœºæ™¯è¦†ç›–ç‡ã€‚å…¶ä¸­ï¼Œè¯¥ç®—æ³•å¯¹ç¢°æ’åœºæ™¯å’Œè¿‘ç¢°æ’åœºæ™¯çš„è¦†ç›–ç‡åˆ†åˆ«è¾¾åˆ°äº† 96.83% å’Œ 92.07%ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸‹çš„é«˜æ•ˆæ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§èƒ½éªŒè¯æä¾›äº†ä¸€ç§å¯é ä¸”é«˜æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ï¼Œæœ‰åŠ©äºæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„ç ”å‘ä¸éƒ¨ç½²ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06575v1",
      "published_date": "2025-08-07 13:55:01 UTC",
      "updated_date": "2025-08-07 13:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:18.046269+00:00"
    },
    {
      "arxiv_id": "2508.05399v1",
      "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
      "title_zh": "UNCAGEï¼šé¢å‘æ–‡æœ¬ç”Ÿæˆå›¾åƒä¸­æ©ç ç”Ÿæˆå¼ Transformer çš„å¯¹æ¯”æ³¨æ„åŠ›å¼•å¯¼",
      "authors": [
        "Wonjun Kang",
        "Byeongkeun Ahn",
        "Minjae Lee",
        "Kevin Galim",
        "Seunghyuk Oh",
        "Hyung Il Koo",
        "Nam Ik Cho"
      ],
      "abstract": "Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ©ç ç”Ÿæˆå˜æ¢å™¨ (Masked Generative Transformers, MGTs) åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image, T2I) ä»»åŠ¡ä¸­é¢ä¸´çš„ç»„åˆæ€§ç”ŸæˆæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º UNCAGE (Unmasking with Contrastive Attention Guidance) çš„åˆ›æ–°æ–¹æ³•ã€‚è™½ç„¶ MGTs é€šè¿‡åŒå‘æ³¨æ„åŠ›å’Œå¹¶è¡Œè§£ç å®ç°äº†é«˜æ•ˆç”Ÿæˆï¼Œä½†åœ¨å±æ€§ç»‘å®šå’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚UNCAGE æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ (training-free) çš„å¼•å¯¼ç­–ç•¥ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›å›¾ (attention maps) æ¥ç¡®å®šä¼˜å…ˆçº§ï¼Œä¼˜å…ˆå¯¹ä»£è¡¨ç‰¹å®šå¯¹è±¡çš„æ ‡è®° (tokens) è¿›è¡Œå–æ¶ˆæ©ç  (unmasking) å¤„ç†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡äº†ç»„åˆä¿çœŸåº¦ (compositional fidelity)ï¼Œå…¶å®šé‡ä¸å®šæ€§æŒ‡æ ‡è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒUNCAGE åœ¨æ˜¾è‘—æ”¹å–„ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œä»…å¸¦æ¥æå°çš„æ¨ç†å¼€é”€ (inference overhead)ï¼Œä¸ºæå‡ç”Ÿæˆå¼æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at https://github.com/furiosa-ai/uncage",
      "pdf_url": "https://arxiv.org/pdf/2508.05399v1",
      "published_date": "2025-08-07 13:51:17 UTC",
      "updated_date": "2025-08-07 13:51:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:30.143414+00:00"
    },
    {
      "arxiv_id": "2508.09183v2",
      "title": "Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery",
      "title_zh": "é¢å‘æœ€åä¸€å…¬é‡Œå³æ—¶é…é€çš„é‡å­é«˜æ•ˆå¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Farzan Moosavi",
        "Bilal Farooq"
      ],
      "abstract": "Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨é‡å­è®¡ç®—è§£å†³å¤§è§„æ¨¡ Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW) è¿™ä¸€ NP-hard ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚é’ˆå¯¹ç»å…¸æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æœ€åä¸€å…¬é‡ŒæŒ‰éœ€é…é€ä»»åŠ¡æ—¶é¢ä¸´çš„è®¡ç®—å±€é™ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸å‚æ•°åŒ–é‡å­ç”µè·¯(Parametrized Quantum Circuit, PQC)çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ä¸€ç§åŒ…å«çº ç¼ å±‚å’Œå˜åˆ†å±‚çš„åˆ›æ–°å‹é—®é¢˜ç‰¹å®šç¼–ç é‡å­ç”µè·¯ï¼Œæ—¨åœ¨æœ€å°åŒ–çœŸå®äº¤ä»˜åœºæ™¯ä¸­çš„è¡Œé©¶æ—¶é—´ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥æ–¹æ³•ä¸ Proximal Policy Optimization (PPO) å’Œ Quantum Singular Value Transformation (QSVT) è¿›è¡Œäº†æ•°å€¼å®éªŒå¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤„ç†å¤æ‚çœŸå®ä¸–ç•Œçº¦æŸã€è§£çš„è§„æ¨¡ä»¥åŠè®­ç»ƒå¤æ‚åº¦æ–¹é¢å‡å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºé«˜æ•ˆè§£å†³å¤§è§„æ¨¡ç‰©æµä¼˜åŒ–é—®é¢˜æä¾›äº†é‡å­å¢å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "quant-ph",
      "comment": "Funding source: Natural Sciences and Engineering Research Council and Canada Research Chair",
      "pdf_url": "https://arxiv.org/pdf/2508.09183v2",
      "published_date": "2025-08-07 13:50:43 UTC",
      "updated_date": "2025-09-06 00:30:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:37.853519+00:00"
    },
    {
      "arxiv_id": "2508.05396v1",
      "title": "Real-Time Iteration Scheme for Diffusion Policy",
      "title_zh": "é¢å‘æ‰©æ•£ç­–ç•¥çš„å®æ—¶è¿­ä»£æ–¹æ¡ˆ",
      "authors": [
        "Yufei Duan",
        "Hang Yin",
        "Danica Kragic"
      ],
      "abstract": "Diffusion Policies have demonstrated impressive performance in robotic manipulation tasks. However, their long inference time, resulting from an extensive iterative denoising process, and the need to execute an action chunk before the next prediction to maintain consistent actions limit their applicability to latency-critical tasks or simple tasks with a short cycle time. While recent methods explored distillation or alternative policy structures to accelerate inference, these often demand additional training, which can be resource-intensive for large robotic models. In this paper, we introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a method from optimal control that accelerates optimization by leveraging solutions from previous time steps as initial guesses for subsequent iterations. We explore the application of this scheme in diffusion inference and propose a scaling-based method to effectively handle discrete actions, such as grasping, in robotic manipulation. The proposed scheme significantly reduces runtime computational costs without the need for distillation or policy redesign. This enables a seamless integration into many pre-trained diffusion-based models, in particular, to resource-demanding large models. We also provide theoretical conditions for the contractivity which could be useful for estimating the initial denoising step. Quantitative results from extensive simulation experiments show a substantial reduction in inference time, with comparable overall performance compared with Diffusion Policy using full-step denoising. Our project page with additional resources is available at: https://rti-dp.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£ç­–ç•¥(Diffusion Policies)å› å†—é•¿çš„è¿­ä»£å»å™ªè¿‡ç¨‹å’ŒåŠ¨ä½œåˆ†å—(action chunk)éœ€æ±‚å¯¼è‡´æ¨ç†å»¶è¿Ÿï¼Œéš¾ä»¥åº”ç”¨äºé«˜å®æ—¶æ€§ä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—æœ€ä¼˜æ§åˆ¶é¢†åŸŸå¯å‘çš„æ–°å‹å®æ—¶è¿­ä»£æ–¹æ¡ˆ(Real-Time Iteration Scheme, RTI)ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡åˆ©ç”¨å‰ä¸€æ—¶é—´æ­¥çš„è§£ä½œä¸ºåç»­è¿­ä»£çš„åˆå§‹çŒœæµ‹æ¥åŠ é€Ÿæ¨ç†ï¼Œå¹¶ç»“åˆä¸€ç§åŸºäºç¼©æ”¾çš„æ–¹æ³•æ¥æœ‰æ•ˆå¤„ç†æœºå™¨äººæ“ä½œä¸­çš„ç¦»æ•£åŠ¨ä½œã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€æ¨¡å‹è’¸é¦æˆ–é‡æ–°è®¾è®¡ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†è¿è¡Œæ—¶çš„è®¡ç®—å¼€é”€ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç±»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åŠèµ„æºæ¶ˆè€—å‹å¤§æ¨¡å‹ä¸­ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†æ”¶ç¼©æ€§(contractivity)çš„ç†è®ºæ¡ä»¶ï¼Œä¸ºä¼°ç®—åˆå§‹å»å™ªæ­¥éª¤æä¾›äº†æ”¯æ’‘ã€‚ä»¿çœŸå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ˜¾è‘—ç¼©çŸ­æ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸å…¨æ­¥éª¤å»å™ªæ‰©æ•£ç­–ç•¥ç›¸å½“çš„ä½œä¸šæ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
      "pdf_url": "https://arxiv.org/pdf/2508.05396v1",
      "published_date": "2025-08-07 13:49:00 UTC",
      "updated_date": "2025-08-07 13:49:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:40.954153+00:00"
    },
    {
      "arxiv_id": "2508.05388v1",
      "title": "An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal",
      "title_zh": "åŸºäº Portugal åœ°é“è¿è¥å•†æ•°æ®æµçš„é“è·¯é¢„æµ‹æ€§ç»´æŠ¤å¯è§£é‡Šæœºå™¨å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Silvia GarcÃ­a-MÃ©ndez",
        "Francisco de Arriba-PÃ©rez",
        "FÃ¡tima Leal",
        "Bruno Veloso",
        "Benedita Malheiro",
        "Juan Carlos Burguillo-Rial"
      ],
      "abstract": "This work contributes to a real-time data-driven predictive maintenance solution for Intelligent Transportation Systems. The proposed method implements a processing pipeline comprised of sample pre-processing, incremental classification with Machine Learning models, and outcome explanation. This novel online processing pipeline has two main highlights: (i) a dedicated sample pre-processing module, which builds statistical and frequency-related features on the fly, and (ii) an explainability module. This work is the first to perform online fault prediction with natural language and visual explainability. The experiments were performed with the MetroPT data set from the metro operator of Porto, Portugal. The results are above 98 % for F-measure and 99 % for accuracy. In the context of railway predictive maintenance, achieving these high values is crucial due to the practical and operational implications of accurate failure prediction. In the specific case of a high F-measure, this ensures that the system maintains an optimal balance between detecting the highest possible number of real faults and minimizing false alarms, which is crucial for maximizing service availability. Furthermore, the accuracy obtained enables reliability, directly impacting cost reduction and increased safety. The analysis demonstrates that the pipeline maintains high performance even in the presence of class imbalance and noise, and its explanations effectively reflect the decision-making process. These findings validate the methodological soundness of the approach and confirm its practical applicability for supporting proactive maintenance decisions in real-world railway operations. Therefore, by identifying the early signs of failure, this pipeline enables decision-makers to understand the underlying problems and act accordingly swiftly.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½äº¤é€šç³»ç»Ÿæå‡ºäº†ä¸€ç§å®æ—¶æ•°æ®é©±åŠ¨çš„é¢„æµ‹æ€§ç»´æŠ¤(Predictive Maintenance)è§£å†³æ–¹æ¡ˆï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«æ ·æœ¬é¢„å¤„ç†ã€æœºå™¨å­¦ä¹ æ¨¡å‹å¢é‡åˆ†ç±»(Incremental Classification)å’Œç»“æœè§£é‡Šçš„åœ¨çº¿å¤„ç†ç®¡çº¿ã€‚è¯¥ç®¡çº¿çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºèƒ½å¤Ÿå³æ—¶æ„å»ºç»Ÿè®¡å’Œé¢‘ç‡ç›¸å…³ç‰¹å¾ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªç»“åˆè‡ªç„¶è¯­è¨€ä¸è§†è§‰å¯è§£é‡Šæ€§(Explainability)è¿›è¡Œåœ¨çº¿æ•…éšœé¢„æµ‹çš„å·¥ä½œã€‚å®éªŒé‡‡ç”¨è‘¡è„ç‰™æ³¢å°”å›¾åœ°é“è¿è¥æ–¹æä¾›çš„ MetroPT æ•°æ®é›†ï¼Œå…¶ F-measure è¶…è¿‡ 98% ä¸”å‡†ç¡®ç‡(Accuracy)è¾¾åˆ° 99% ä»¥ä¸Šã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥ç®¡çº¿åœ¨ç±»åˆ«ä¸å¹³è¡¡å’Œå™ªå£°å¹²æ‰°ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œå…¶è§£é‡Šæ¨¡å—èƒ½æœ‰æ•ˆåæ˜ å†³ç­–è¿‡ç¨‹ã€‚è¯¥ç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®é™…é“è·¯è¿è¥ä¸­æ”¯æŒä¸»åŠ¨ç»´æŠ¤å†³ç­–çš„å®ç”¨æ€§ï¼Œé€šè¿‡æ—©æœŸè¯†åˆ«æ•…éšœè¿¹è±¡ï¼ŒåŠ©åŠ›å†³ç­–è€…ä¼˜åŒ–æœåŠ¡å¯ç”¨æ€§ã€é™ä½æˆæœ¬å¹¶æå‡å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05388v1",
      "published_date": "2025-08-07 13:38:49 UTC",
      "updated_date": "2025-08-07 13:38:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:47.700733+00:00"
    },
    {
      "arxiv_id": "2508.05387v3",
      "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms",
      "title_zh": "Echoï¼šé¢å‘å¼‚æ„é›†ç¾¤çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å¯¹é½æ¨ç†ä¸è®­ç»ƒè§£è€¦",
      "authors": [
        "Jie Xiao",
        "Changyuan Fan",
        "Qingnan Ren",
        "Alfred Long",
        "Yuchen Zhang",
        "Rymon Yu",
        "Eric Yang",
        "Lynn Ai",
        "Shaoduo Gan"
      ],
      "abstract": "Modern RL-based post-training for large language models (LLMs) co-locate trajectory sampling and policy optimisation on the same GPU cluster, forcing the system to switch between inference and training workloads. This serial context switching violates the single-program-multiple-data (SPMD) assumption underlying today's distributed training systems. We present Echo, the RL system that cleanly decouples these two phases across heterogeneous \"inference\" and \"training\" swarms while preserving statistical efficiency. Echo introduces two lightweight synchronization protocols: a sequential pull mode that refreshes policy weights according to API call for minimal bias, and an asynchronous push-pull mode that streams version-tagged rollouts through a replay buffer to maximise hardware utilisation. Training four representative RL workloads with Qwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a geographically distributed cluster, Echo matches a fully co-located Verl baseline in convergence speed and final reward while off-loading trajectory generation to commodity edge hardware. These promising results demonstrate that large-scale RL for LLMs could achieve datacentre-grade performance using decentralised, heterogeneous resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Echoï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹  (RL) å¯¹é½è®¾è®¡çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¡†æ¶ä¸­æ¨ç†é‡‡æ ·ä¸ç­–ç•¥ä¼˜åŒ–å…±ç½®å¯¼è‡´çš„ç³»ç»Ÿä¸Šä¸‹æ–‡åˆ‡æ¢åŠè¿åå•ç¨‹åºå¤šæ•°æ® (SPMD) å‡è®¾çš„é—®é¢˜ã€‚Echo å®ç°äº†åœ¨å¼‚æ„é›†ç¾¤ä¸­å°†æ¨ç†å’Œè®­ç»ƒé˜¶æ®µå®Œå…¨è§£è€¦ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§è½»é‡çº§åŒæ­¥åè®®ï¼šæ—¨åœ¨å‡å°‘åå·®çš„é¡ºåºæ‹‰å– (sequential pull) æ¨¡å¼ï¼Œä»¥åŠåˆ©ç”¨å›æ”¾ç¼“å†²åŒºæµå¼ä¼ è¾“æ•°æ®ä»¥æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡çš„å¼‚æ­¥æ¨æ‹‰ (asynchronous push-pull) æ¨¡å¼ã€‚é€šè¿‡åœ¨åœ°ç†åˆ†å¸ƒçš„é›†ç¾¤ä¸Šå¯¹ Qwen3-4Bã€Qwen3-32B ç­‰å¤šä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒæµ‹è¯•ï¼ŒEcho åœ¨æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆå¥–åŠ±ä¸Šå‡è¾¾åˆ°äº†ä¸å®Œå…¨å…±ç½®çš„ Verl åŸºå‡†æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨å»ä¸­å¿ƒåŒ–çš„å¼‚æ„èµ„æºï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå®ç°æ•°æ®ä¸­å¿ƒçº§çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸€æˆæœä¸ºåœ¨éç»Ÿä¸€ç¡¬ä»¶ç¯å¢ƒä¸‹é«˜æ•ˆè¿›è¡Œå¤§è§„æ¨¡æ¨¡å‹å¯¹é½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05387v3",
      "published_date": "2025-08-07 13:37:04 UTC",
      "updated_date": "2025-08-12 15:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:51:46.696525+00:00"
    },
    {
      "arxiv_id": "2508.05383v1",
      "title": "StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models",
      "title_zh": "StructVRMï¼šåŸºäºç»“æ„åŒ–ä¸å¯éªŒè¯å¥–åŠ±æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†å¯¹é½",
      "authors": [
        "Xiangxiang Zhang",
        "Jingxuan Wei",
        "Donghong Zhong",
        "Qi Chen",
        "Caijun Jia",
        "Cheng Tan",
        "Jinming Gu",
        "Xiaobo Qin",
        "Zhiping Liu",
        "Liang Hu",
        "Tong Sun",
        "Yuchen Wu",
        "Zewei Sun",
        "Chenwei Lou",
        "Hua Zheng",
        "Tianyang Zhan",
        "Changbao Wang",
        "Shuangzhi Wu",
        "Zefa Lin",
        "Chang Guo",
        "Sihang Yuan",
        "Riwei Chen",
        "Shixiong Zhao",
        "Yingping Zhang",
        "Gaowei Wu",
        "Bihui Yu",
        "Jiahui Wu",
        "Zhehui Zhao",
        "Qianqian Liu",
        "Ruofeng Tang",
        "Xingyue Huang",
        "Bing Zhao",
        "Mengyang Zhang",
        "Youqiang Zhou"
      ],
      "abstract": "Existing Vision-Language Models often struggle with complex, multi-question reasoning tasks where partial correctness is crucial for effective learning. Traditional reward mechanisms, which provide a single binary score for an entire response, are too coarse to guide models through intricate problems with multiple sub-parts. To address this, we introduce StructVRM, a method that aligns multimodal reasoning with Structured and Verifiable Reward Models. At its core is a model-based verifier trained to provide fine-grained, sub-question-level feedback, assessing semantic and mathematical equivalence rather than relying on rigid string matching. This allows for nuanced, partial credit scoring in previously intractable problem formats. Extensive experiments demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM, achieves state-of-the-art performance on six out of twelve public multimodal benchmarks and our newly curated, high-difficulty STEM-Bench. The success of StructVRM validates that training with structured, verifiable rewards is a highly effective approach for advancing the capabilities of multimodal models in complex, real-world reasoning domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language Models (VLMs) åœ¨å¤„ç†å¤æ‚å¤šé—®é¢˜æ¨ç†ä»»åŠ¡æ—¶ä¼ ç»Ÿå•ä¸€å¥–åŠ±æœºåˆ¶è¿‡äºç²—æ”¾çš„é—®é¢˜ï¼Œæå‡ºäº† StructVRM æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç»“æ„åŒ–ä¸”å¯éªŒè¯çš„å¥–åŠ±æ¨¡å‹ (Structured and Verifiable Reward Models) æ¥å¯¹é½å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ¨¡å‹çš„éªŒè¯å™¨ (model-based verifier)ï¼Œæ—¨åœ¨æä¾›ç»†ç²’åº¦çš„å­é—®é¢˜çº§åé¦ˆï¼Œé€šè¿‡è¯„ä¼°è¯­ä¹‰å’Œæ•°å­¦ç­‰ä»·æ€§è€Œéæœºæ¢°çš„å­—ç¬¦ä¸²åŒ¹é…æ¥æä¾›æ›´ç²¾å‡†çš„éƒ¨åˆ†å¾—åˆ† (partial credit)ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ­¤æ–¹æ³•è®­ç»ƒçš„ Seed-StructVRM åœ¨ 12 ä¸ªå…¬å¼€å¤šæ¨¡æ€åŸºå‡†ä¸­çš„ 6 ä¸ªä»¥åŠæ–°æ„å»ºçš„é«˜éš¾åº¦ STEM-Bench ä¸Šå‡å–å¾—äº† SOTA æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±åœ¨æå‡å¤šæ¨¡æ€æ¨¡å‹åº”å¯¹ç°å®ä¸–ç•Œå¤æ‚æ¨ç†æŒ‘æˆ˜æ–¹é¢çš„æ˜¾è‘—æ•ˆåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05383v1",
      "published_date": "2025-08-07 13:31:21 UTC",
      "updated_date": "2025-08-07 13:31:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:02.760895+00:00"
    },
    {
      "arxiv_id": "2508.05364v1",
      "title": "Optimal Corpus Aware Training for Neural Machine Translation",
      "title_zh": "é¢å‘ç¥ç»æœºå™¨ç¿»è¯‘çš„ä¼˜åŒ–è¯­æ–™åº“æ„ŸçŸ¥è®­ç»ƒ",
      "authors": [
        "Yi-Hsiu Liao",
        "Cheng Shen",
        "Brenda",
        "Yang"
      ],
      "abstract": "Corpus Aware Training (CAT) leverages valuable corpus metadata during training by injecting corpus information into each training example, and has been found effective in the literature, commonly known as the \"tagging\" approach. Models trained with CAT inherently learn the quality, domain and nuance between corpora directly from data, and can easily switch to different inference behavior. To achieve the best evaluation, CAT models pre-define a group of high quality data before training starts which can be error-prone and inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT), which fine-tunes a CAT pre-trained model by freezing most of the model parameters and only tuning small set of corpus-related parameters. We show that OCAT is lightweight, resilient to overfitting, and effective in boosting model accuracy. We use WMT23 English to Chinese and English to German translation tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively, over vanilla training. Furthermore, our approach is on-par or slightly better than other state-of-the-art fine-tuning techniques while being less sensitive to hyperparameter settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»æœºå™¨ç¿»è¯‘æå‡ºäº†Optimal Corpus Aware Training (OCAT)ï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»Ÿçš„Corpus Aware Training (CAT)åœ¨é¢„å®šä¹‰é«˜è´¨é‡æ•°æ®æ—¶å­˜åœ¨çš„ä½æ•ˆä¸æ˜“é”™æ€§ã€‚OCATé€šè¿‡å†»ç»“CATé¢„è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œä»…å¾®è°ƒä¸€å°éƒ¨åˆ†ä¸è¯­æ–™åº“ç›¸å…³çš„å‚æ•°ï¼Œå®ç°äº†ä¸€ç§è½»é‡çº§ä¸”èƒ½æœ‰æ•ˆæŠµæŠ—è¿‡æ‹Ÿåˆ(overfitting)çš„è®­ç»ƒæ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨WMT23è‹±ä¸­å’Œè‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”Vanilla Trainingåˆ†åˆ«æå‡äº†3.6å’Œ1.8ä¸ªchrFå€¼ã€‚æ­¤å¤–ï¼ŒOCATçš„æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„å¾®è°ƒæŠ€æœ¯ç›¸å½“ç”šè‡³ç•¥ä¼˜ï¼Œä¸”å¯¹è¶…å‚æ•°è®¾ç½®çš„æ•æ„Ÿåº¦è¾ƒä½ï¼Œä¸ºä¼˜åŒ–ç¿»è¯‘æ¨¡å‹åœ¨ç‰¹å®šè¯­æ–™èƒŒæ™¯ä¸‹çš„æ¨ç†è¡Œä¸ºæä¾›äº†æ›´ç¨³å¥çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05364v1",
      "published_date": "2025-08-07 13:12:26 UTC",
      "updated_date": "2025-08-07 13:12:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:03.550122+00:00"
    },
    {
      "arxiv_id": "2508.05360v1",
      "title": "Building Effective Safety Guardrails in AI Education Tools",
      "title_zh": "æ„å»ºäººå·¥æ™ºèƒ½æ•™è‚²å·¥å…·ä¸­çš„æœ‰æ•ˆå®‰å…¨æŠ¤æ ",
      "authors": [
        "Hannah-Beth Clark",
        "Laura Benton",
        "Emma Searle",
        "Margaux Dowland",
        "Matthew Gregory",
        "Will Gayne",
        "John Roberts"
      ],
      "abstract": "There has been rapid development in generative AI tools across the education sector, which in turn is leading to increased adoption by teachers. However, this raises concerns regarding the safety and age-appropriateness of the AI-generated content that is being created for use in classrooms. This paper explores Oak National Academy's approach to addressing these concerns within the development of the UK Government's first publicly available generative AI tool - our AI-powered lesson planning assistant (Aila). Aila is intended to support teachers planning national curriculum-aligned lessons that are appropriate for pupils aged 5-16 years. To mitigate safety risks associated with AI-generated content we have implemented four key safety guardrails - (1) prompt engineering to ensure AI outputs are generated within pedagogically sound and curriculum-aligned parameters, (2) input threat detection to mitigate attacks, (3) an Independent Asynchronous Content Moderation Agent (IACMA) to assess outputs against predefined safety categories, and (4) taking a human-in-the-loop approach, to encourage teachers to review generated content before it is used in the classroom. Through our on-going evaluation of these safety guardrails we have identified several challenges and opportunities to take into account when implementing and testing safety guardrails. This paper highlights ways to build more effective safety guardrails in generative AI education tools including the on-going iteration and refinement of guardrails, as well as enabling cross-sector collaboration through sharing both open-source code, datasets and learnings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ•™è‚²é¢†åŸŸç”Ÿæˆå¼ AI å·¥å…·ä¸­æ„å»ºæœ‰æ•ˆ safety guardrails çš„æ–¹æ³•ï¼Œé‡ç‚¹ä»‹ç»äº†ä¸ºè‹±å›½æ”¿åºœå¼€å‘çš„ AI è¯¾ç¨‹è§„åˆ’åŠ©æ‰‹ Ailaã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆå†…å®¹é€‚ç”¨äº 5-16 å²å­¦ç”Ÿå¹¶ç¬¦åˆè¯¾ç¨‹æ ‡å‡†ï¼Œç ”ç©¶å®æ–½äº†å››é¡¹å…³é”®å®‰å…¨æªæ–½ï¼šåˆ©ç”¨ prompt engineering ç¡®ä¿è¾“å‡ºç¬¦åˆæ•™å­¦é€»è¾‘ã€é‡‡ç”¨ input threat detection ç¼“è§£æ”»å‡»ã€å¼•å…¥ç‹¬ç«‹å¼‚æ­¥å†…å®¹å®¡æ ¸æ™ºèƒ½ä½“ IACMA è¯„ä¼°è¾“å‡ºå®‰å…¨æ€§ï¼Œä»¥åŠé€šè¿‡ human-in-the-loop æœºåˆ¶å¼•å…¥æ•™å¸ˆå®¡æ ¸ç¯èŠ‚ã€‚é€šè¿‡å¯¹è¿™äº›é˜²æŠ¤æ çš„æŒç»­è¯„ä¼°ï¼Œç ”ç©¶æ€»ç»“äº†åœ¨å®æ–½ä¸æµ‹è¯•è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚è¯¥è®ºæ–‡æœ€åå¼ºè°ƒäº† safety guardrails éœ€è¿›è¡ŒæŒç»­è¿­ä»£ä¸ä¼˜åŒ–ï¼Œå¹¶å‘¼åé€šè¿‡å…±äº«å¼€æºä»£ç ã€æ•°æ®é›†åŠç ”ç©¶æˆæœæ¥åŠ å¼ºè·¨è¡Œä¸šåä½œï¼Œä»¥æå‡ AI æ•™è‚²å·¥å…·çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages, published in proceedings of International Conference on Artificial Intelligence in Education (AIED) 2025: Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium, Blue Sky, and WideAIED",
      "pdf_url": "https://arxiv.org/pdf/2508.05360v1",
      "published_date": "2025-08-07 13:09:47 UTC",
      "updated_date": "2025-08-07 13:09:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:14.384126+00:00"
    },
    {
      "arxiv_id": "2508.05353v2",
      "title": "PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation",
      "title_zh": "PriorRGï¼šé¢å‘èƒ¸éƒ¨ X å…‰æŠ¥å‘Šç”Ÿæˆçš„å…ˆéªŒå¼•å¯¼å¯¹æ¯”é¢„è®­ç»ƒä¸ç”±ç²—åˆ°ç²¾è§£ç ",
      "authors": [
        "Kang Liu",
        "Zhuoqi Ma",
        "Zikang Fang",
        "Yunan Li",
        "Kun Xie",
        "Qiguang Miao"
      ],
      "abstract": "Chest X-ray report generation aims to reduce radiologists' workload by automatically producing high-quality preliminary reports. A critical yet underexplored aspect of this task is the effective use of patient-specific prior knowledge -- including clinical context (e.g., symptoms, medical history) and the most recent prior image -- which radiologists routinely rely on for diagnostic reasoning. Most existing methods generate reports from single images, neglecting this essential prior information and thus failing to capture diagnostic intent or disease progression. To bridge this gap, we propose PriorRG, a novel chest X-ray report generation framework that emulates real-world clinical workflows via a two-stage training pipeline. In Stage 1, we introduce a prior-guided contrastive pre-training scheme that leverages clinical context to guide spatiotemporal feature extraction, allowing the model to align more closely with the intrinsic spatiotemporal semantics in radiology reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for report generation that progressively integrates patient-specific prior knowledge with the vision encoder's hidden states. This decoding allows the model to align with diagnostic focus and track disease progression, thereby enhancing the clinical accuracy and fluency of the generated reports. Extensive experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and checkpoints will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PriorRGï¼Œä¸€ç§æ¨¡æ‹Ÿä¸´åºŠå·¥ä½œæµç¨‹çš„æ–°å‹èƒ¸éƒ¨ X å°„çº¿æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¿½è§†æ‚£è€…ä¸´åºŠèƒŒæ™¯å’Œå†å²å½±åƒç­‰å…ˆéªŒçŸ¥è¯†çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼Œç¬¬ä¸€é˜¶æ®µå¼•å…¥äº†å…ˆéªŒå¼•å¯¼çš„å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ¡ˆ(Prior-guided contrastive pre-training)ï¼Œåˆ©ç”¨ä¸´åºŠè¯­å¢ƒå¼•å¯¼æ—¶ç©ºç‰¹å¾æå–ã€‚ç¬¬äºŒé˜¶æ®µè®¾è®¡äº†å…ˆéªŒæ„ŸçŸ¥çš„ä»ç²—åˆ°ç»†è§£ç æœºåˆ¶(Prior-aware coarse-to-fine decoding)ï¼Œå°†æ‚£è€…ç‰¹æœ‰çš„å…ˆéªŒçŸ¥è¯†ä¸è§†è§‰ç¼–ç å™¨çš„éšè—çŠ¶æ€é€æ­¥æ•´åˆï¼Œä»è€Œç²¾å‡†æ•æ‰è¯Šæ–­é‡ç‚¹å¹¶è¿½è¸ªç–¾ç—…è¿›å±•ã€‚åœ¨ MIMIC-CXR å’Œ MIMIC-ABN æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPriorRG çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¨¡å‹åœ¨ MIMIC-CXR æ•°æ®é›†ä¸Šçš„ BLEU-4 å’Œ F1 åˆ†æ•°åˆ†åˆ«æå‡äº† 3.6% å’Œ 3.8%ï¼Œåœ¨ MIMIC-ABN æ•°æ®é›†ä¸Šçš„ BLEU-1 å¢ç›Šè¾¾åˆ° 5.9%ã€‚è¿™ä¸€æ¡†æ¶æœ‰æ•ˆæå‡äº†ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šåœ¨ä¸´åºŠå‡†ç¡®æ€§å’Œè¯­è¨€æµç•…åº¦æ–¹é¢çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.05353v2",
      "published_date": "2025-08-07 13:02:20 UTC",
      "updated_date": "2026-01-04 14:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:14.884414+00:00"
    },
    {
      "arxiv_id": "2508.05352v1",
      "title": "Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising",
      "title_zh": "åŸºäºæ¡ä»¶æ‰©æ•£ç‰¹å¾å»å™ªçš„å¤šæ¨¡æ€å¤šè¡Œä¸ºåºåˆ—æ¨è",
      "authors": [
        "Xiaoxi Cui",
        "Weihai Lu",
        "Yu Tong",
        "Yiheng Li",
        "Zhejun Zhao"
      ],
      "abstract": "The sequential recommendation system utilizes historical user interactions to predict preferences. Effectively integrating diverse user behavior patterns with rich multimodal information of items to enhance the accuracy of sequential recommendations is an emerging and challenging research direction. This paper focuses on the problem of multi-modal multi-behavior sequential recommendation, aiming to address the following challenges: (1) the lack of effective characterization of modal preferences across different behaviors, as user attention to different item modalities varies depending on the behavior; (2) the difficulty of effectively mitigating implicit noise in user behavior, such as unintended actions like accidental clicks; (3) the inability to handle modality noise in multi-modal representations, which further impacts the accurate modeling of user preferences. To tackle these issues, we propose a novel Multi-Modal Multi-Behavior Sequential Recommendation model (M$^3$BSR). This model first removes noise in multi-modal representations using a Conditional Diffusion Modality Denoising Layer. Subsequently, it utilizes deep behavioral information to guide the denoising of shallow behavioral data, thereby alleviating the impact of noise in implicit feedback through Conditional Diffusion Behavior Denoising. Finally, by introducing a Multi-Expert Interest Extraction Layer, M$^3$BSR explicitly models the common and specific interests across behaviors and modalities to enhance recommendation performance. Experimental results indicate that M$^3$BSR significantly outperforms existing state-of-the-art methods on benchmark datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†M$^3$BSRï¼Œä¸€ç§èåˆæ¡ä»¶æ‰©æ•£ç‰¹å¾å»å™ªçš„å¤šæ¨¡æ€å¤šè¡Œä¸ºåºåˆ—æ¨èï¼ˆMulti-Modal Multi-Behavior Sequential Recommendationï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·åœ¨ä¸åŒè¡Œä¸ºä¸‹æ¨¡æ€åå¥½å·®å¼‚ã€è¡Œä¸ºéšå«å™ªå£°ä»¥åŠå¤šæ¨¡æ€è¡¨ç¤ºå™ªå£°ç­‰å…³é”®é—®é¢˜ã€‚è¯¥æ¨¡å‹é¦–å…ˆåˆ©ç”¨Conditional Diffusion Modality Denoising Layerå»é™¤å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„å™ªå£°ï¼Œå¹¶å¼•å…¥Conditional Diffusion Behavior Denoisingï¼Œé€šè¿‡æ·±å±‚è¡Œä¸ºä¿¡æ¯æŒ‡å¯¼æµ…å±‚è¡Œä¸ºæ•°æ®å»å™ªï¼Œä»¥ç¼“è§£éšå¼åé¦ˆä¸­çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œé€šè¿‡Multi-Expert Interest Extraction Layeræ˜¾å¼å»ºæ¨¡è·¨è¡Œä¸ºå’Œæ¨¡æ€çš„å…±åŒä¸ç‰¹å®šå…´è¶£ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨èæ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒM$^3$BSRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¤æ‚äº¤äº’æ•°æ®æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05352v1",
      "published_date": "2025-08-07 12:58:34 UTC",
      "updated_date": "2025-08-07 12:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:11.451097+00:00"
    },
    {
      "arxiv_id": "2508.05350v1",
      "title": "Minimal Model Reasoning in Description Logics: Don't Try This at Home!",
      "title_zh": "æè¿°é€»è¾‘ä¸­çš„æœ€å°æ¨¡å‹æ¨ç†ï¼šè¯·å‹¿å±…å®¶å°è¯•ï¼",
      "authors": [
        "Federica Di Stefano",
        "Quentin ManiÃ¨re",
        "Magdalena Ortiz",
        "Mantas Å imkus"
      ],
      "abstract": "Reasoning with minimal models has always been at the core of many knowledge representation techniques, but we still have only a limited understanding of this problem in Description Logics (DLs). Minimization of some selected predicates, letting the remaining predicates vary or be fixed, as proposed in circumscription, has been explored and exhibits high complexity. The case of `pure' minimal models, where the extension of all predicates must be minimal, has remained largely uncharted. We address this problem in popular DLs and obtain surprisingly negative results: concept satisfiability in minimal models is undecidable already for $\\mathcal{EL}$. This undecidability also extends to a very restricted fragment of tuple-generating dependencies. To regain decidability, we impose acyclicity conditions on the TBox that bring the worst-case complexity below double exponential time and allow us to establish a connection with the recently studied pointwise circumscription; we also derive results in data complexity. We conclude with a brief excursion to the DL-Lite family, where a positive result was known for DL-Lite$_{\\text{core}}$, but our investigation establishes ExpSpace-hardness already for its extension DL-Lite$_{\\text{horn}}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æè¿°é€»è¾‘ (Description Logics) ä¸­â€œçº¯â€æœ€å°æ¨¡å‹ (pure minimal models) çš„æ¨ç†é—®é¢˜ï¼Œå³è¦æ±‚æ‰€æœ‰è°“è¯çš„å¤–å»¶å‡è¾¾åˆ°æœ€å°ã€‚ç ”ç©¶å¾—å‡ºäº†å‡ºäººæ„æ–™çš„è´Ÿé¢ç»“è®ºï¼Œè¯æ˜äº†å³ä½¿åœ¨ç®€å•çš„ $\\mathcal{EL}$ é€»è¾‘ä¸­ï¼Œæœ€å°æ¨¡å‹ä¸‹çš„æ¦‚å¿µå¯æ»¡è¶³æ€§ (concept satisfiability) ä¹Ÿæ˜¯ä¸å¯åˆ¤å®šçš„ï¼Œä¸”è¯¥ç»“è®ºå¯å»¶ä¼¸è‡³å…ƒç»„ç”Ÿæˆä¾èµ– (tuple-generating dependencies) çš„å—é™ç‰‡æ®µã€‚ä¸ºäº†æ¢å¤å¯åˆ¤å®šæ€§ï¼Œä½œè€…å¯¹ TBox æ–½åŠ äº†æ— ç¯æ€§æ¡ä»¶ (acyclicity conditions)ï¼ŒæˆåŠŸå°†æœ€åæƒ…å†µå¤æ‚åº¦é™è‡³åŒæŒ‡æ•°æ—¶é—´ä»¥ä¸‹ï¼Œå¹¶å»ºç«‹äº†ä¸ç‚¹å¼å¤–å»¶å½’å¹¶ (pointwise circumscription) çš„è”ç³»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†æ•°æ®å¤æ‚åº¦ (data complexity)ï¼Œå¹¶æŒ‡å‡ºè™½ç„¶ DL-Lite$_{\\text{core}}$ çš„æ¨ç†æ˜¯å¯åˆ¤å®šçš„ï¼Œä½†å…¶æ‰©å±•ç‰ˆæœ¬ DL-Lite$_{\\text{horn}}$ å±•ç°å‡ºäº† ExpSpace-hard çš„é«˜å¤æ‚åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "44 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.05350v1",
      "published_date": "2025-08-07 12:56:15 UTC",
      "updated_date": "2025-08-07 12:56:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:16.749451+00:00"
    },
    {
      "arxiv_id": "2508.05344v1",
      "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making",
      "title_zh": "NomicLawï¼šå¤§è¯­è¨€æ¨¡å‹ååŒç«‹æ³•è¿‡ç¨‹ä¸­æ¶Œç°çš„ä¿¡ä»»ä¸ç­–ç•¥æ€§è®ºè¯",
      "authors": [
        "Asutosh Hota",
        "Jussi P. P. Jokinen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have extended their capabilities from basic text processing to complex reasoning tasks, including legal interpretation, argumentation, and strategic interaction. However, empirical understanding of LLM behavior in open-ended, multi-agent settings especially those involving deliberation over legal and ethical dilemmas remains limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs engage in collaborative law-making, responding to complex legal vignettes by proposing rules, justifying them, and voting on peer proposals. We quantitatively measure trust and reciprocity via voting patterns and qualitatively assess how agents use strategic language to justify proposals and influence outcomes. Experiments involving homogeneous and heterogeneous LLM groups demonstrate how agents spontaneously form alliances, betray trust, and adapt their rhetoric to shape collective decisions. Our results highlight the latent social reasoning and persuasive capabilities of ten open-source LLMs and provide insights into the design of future AI systems capable of autonomous negotiation, coordination and drafting legislation in legal settings.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†NomicLawï¼Œä¸€ä¸ªç»“æ„åŒ–çš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ³•å¾‹å’Œé“å¾·åšå¼ˆèƒŒæ™¯ä¸‹çš„åä½œç«‹æ³•è¡Œä¸ºã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œæ¨¡å‹éœ€è¦é’ˆå¯¹å¤æ‚çš„æ³•å¾‹åœºæ™¯æå‡ºè§„åˆ™ã€æä¾›è¾©æŠ¤ç†ç”±å¹¶å¯¹å…¶ä»–æ™ºèƒ½ä½“çš„æè®®è¿›è¡ŒæŠ•ç¥¨ã€‚ç ”ç©¶é€šè¿‡æŠ•ç¥¨æ¨¡å¼å®šé‡æµ‹é‡äº†ä¿¡ä»»ä¸äº’æƒ ç¨‹åº¦ï¼Œå¹¶å®šæ€§è¯„ä¼°äº†æ™ºèƒ½ä½“å¦‚ä½•è¿ç”¨ç­–ç•¥æ€§è¯­è¨€æ¥è¯æ˜å…¶æè®®çš„åˆç†æ€§å¹¶å½±å“å†³ç­–ç»“æœã€‚å®éªŒæ¶µç›–äº†åŒè´¨å’Œå¼‚è´¨LLMç¾¤ä½“ï¼Œç»“æœæ˜¾ç¤ºæ™ºèƒ½ä½“ä¼šè‡ªå‘å½¢æˆè”ç›Ÿã€èƒŒå›ä¿¡ä»»ï¼Œå¹¶æ ¹æ®éœ€è¦è°ƒæ•´å…¶ä¿®è¾ç­–ç•¥ä»¥å¡‘é€ é›†ä½“å†³ç­–ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†10ç§å¼€æºLLMsä¸­æ½œè—çš„ç¤¾ä¼šæ¨ç†å’Œè¯´æœèƒ½åŠ›ï¼Œä¸ºè®¾è®¡å…·å¤‡è‡ªä¸»è°ˆåˆ¤ã€åè°ƒåŠç«‹æ³•èµ·è‰èƒ½åŠ›çš„æœªæ¥AIç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05344v1",
      "published_date": "2025-08-07 12:49:44 UTC",
      "updated_date": "2025-08-07 12:49:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:40.183671+00:00"
    },
    {
      "arxiv_id": "2508.05342v1",
      "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
      "title_zh": "åŸºäºä¿¡æ¯è®ºå›¾èåˆä¸è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç­–ç•¥æ¨ç†åŠåŒæœºå™¨äººæ§åˆ¶",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jin Wang",
        "Chang Che",
        "Xi Xiao",
        "Jiuwen Cao",
        "Yingbai Hu",
        "Hamid Reza Karimi"
      ],
      "abstract": "Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GF-VLAï¼ˆGraph-Fused Vision-Language-Actionï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»äººç±»è§†é¢‘å­¦ä¹ çµå·§æŠ€èƒ½æ—¶å› è¿‡åº¦ä¾èµ–åº•å±‚è½¨è¿¹æ¨¡ä»¿è€Œå¯¼è‡´çš„æ³›åŒ–æ€§éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºShannon-informationçš„ä¿¡æ¯è®ºçº¿ç´¢è¯†åˆ«ä»»åŠ¡å…³é”®è¦ç´ ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºæ•æ‰å¤æ‚äº¤äº’çš„æ—¶åºScene Graphsã€‚é€šè¿‡å°†å›¾ç»“æ„ä¸Language-conditioned Transformerç»“åˆï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆå±‚çº§åŒ–Behavior Treeså’Œå¯è§£é‡Šçš„Cartesian motionæŒ‡ä»¤ï¼Œå¹¶å¼•å…¥Cross-hand selection policyä¼˜åŒ–åŒè‡‚æœºå™¨äººçš„ä»»åŠ¡åˆ†é…ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å­ä»»åŠ¡åˆ†å‰²å’Œå›¾å‡†ç¡®ç‡ä¸Šå‡è¶…è¿‡93%ï¼Œå¹¶åœ¨å¤šé¡¹åŒè‡‚ç»„è£…ä»»åŠ¡ä¸­å®ç°äº†90%çš„æ€»æˆåŠŸç‡ã€‚GF-VLAåœ¨å †å ã€å­—æ¯æ‹¼æ­åŠå‡ ä½•é‡æ„ç­‰è¯­ä¹‰å˜åŒ–åœºæ™¯ä¸­å±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œä¸ºå¤æ‚åŒè‡‚æœºå™¨äººæ§åˆ¶æä¾›äº†å¯é ä¸”å¯è¯»çš„ä»»åŠ¡ç­–ç•¥ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Journal under review",
      "pdf_url": "https://arxiv.org/pdf/2508.05342v1",
      "published_date": "2025-08-07 12:48:09 UTC",
      "updated_date": "2025-08-07 12:48:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:29.994910+00:00"
    },
    {
      "arxiv_id": "2508.05338v1",
      "title": "The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition",
      "title_zh": "â€œæ™ºèƒ½ä½“â€æœ¯è¯­å·²è¢«è¿‡åº¦ç¨€é‡Šè€Œå¤±å»å®ç”¨ä»·å€¼ï¼ŒäºŸå¾…é‡æ–°å®šä¹‰",
      "authors": [
        "Brinnae Bent"
      ],
      "abstract": "The term 'agent' in artificial intelligence has long carried multiple interpretations across different subfields. Recent developments in AI capabilities, particularly in large language model systems, have amplified this ambiguity, creating significant challenges in research communication, system evaluation and reproducibility, and policy development. This paper argues that the term 'agent' requires redefinition. Drawing from historical analysis and contemporary usage patterns, we propose a framework that defines clear minimum requirements for a system to be considered an agent while characterizing systems along a multidimensional spectrum of environmental interaction, learning and adaptation, autonomy, goal complexity, and temporal coherence. This approach provides precise vocabulary for system description while preserving the term's historically multifaceted nature. After examining potential counterarguments and implementation challenges, we provide specific recommendations for moving forward as a field, including suggestions for terminology standardization and framework adoption. The proposed approach offers practical tools for improving research clarity and reproducibility while supporting more effective policy development.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºäººå·¥æ™ºèƒ½é¢†åŸŸä¸­ \"agent\" ä¸€è¯çš„å®šä¹‰ç”±äºå¤šé‡è§£é‡Šè€Œæ—¥ç›Šæ¨¡ç³Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç³»ç»Ÿå¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œè¿™ç§ä¸ç¡®å®šæ€§ä¸¥é‡é˜»ç¢äº†å­¦æœ¯äº¤æµã€ç³»ç»Ÿè¯„ä¼°åŠæ”¿ç­–åˆ¶å®šã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºå¿…é¡»é‡æ–°å®šä¹‰ \"agent\"ï¼Œå¹¶åŸºäºå†å²åˆ†æä¸ç°çŠ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«æœ€ä½å‡†å…¥è¦æ±‚åŠå¤šç»´åº¦è¯„ä¼°çš„å…‰è°±æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»ç¯å¢ƒäº¤äº’ (environmental interaction)ã€å­¦ä¹ ä¸é€‚åº” (learning and adaptation)ã€è‡ªä¸»æ€§ (autonomy)ã€ç›®æ ‡å¤æ‚åº¦ (goal complexity) å’Œæ—¶é—´è¿è´¯æ€§ (temporal coherence) ç­‰æ ¸å¿ƒç»´åº¦å¯¹ç³»ç»Ÿè¿›è¡Œç²¾å‡†åˆ»ç”»ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿ç•™è¯¥æœ¯è¯­å†å²å¤šé¢æ€§çš„åŒæ—¶ï¼Œä¸ºç³»ç»Ÿæè¿°æä¾›äº†ç²¾ç¡®è¯æ±‡ï¼Œæ˜¾è‘—æå‡äº†ç ”ç©¶çš„é€æ˜åº¦ä¸å¯é‡å¤æ€§ã€‚è®ºæ–‡æœ€åä¸ºè¯¥é¢†åŸŸçš„æœ¯è¯­æ ‡å‡†åŒ–å’Œæ¡†æ¶é‡‡çº³æä¾›äº†å…·ä½“å»ºè®®ï¼Œæ—¨åœ¨ä¸ºæœªæ¥çš„ç§‘ç ”åä½œå’Œæ”¿ç­–å¼€å‘æä¾›å®ç”¨çš„è§„èŒƒåŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AIES 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05338v1",
      "published_date": "2025-08-07 12:40:25 UTC",
      "updated_date": "2025-08-07 12:40:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:31.384415+00:00"
    },
    {
      "arxiv_id": "2508.05337v2",
      "title": "Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression",
      "title_zh": "åŸºäºç½®ä¿¡åº¦å¼•å¯¼åæ€æŠ‘åˆ¶çš„å¤§è¯­è¨€æ¨ç†æ¨¡å‹é«˜æ•ˆæ¨ç†",
      "authors": [
        "Jiameng Huang",
        "Baijiong Lin",
        "Guhao Feng",
        "Jierun Chen",
        "Di He",
        "Lu Hou"
      ],
      "abstract": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†è¯­è¨€æ¨¡å‹ (Large Reasoning Language Models, LRLMs) åœ¨é•¿é“¾å¼æ€ç»´æ¨ç†ä¸­å› è¿‡åº¦åæ€å¯¼è‡´çš„â€œè¿‡åº¦æ€è€ƒâ€ (overthinking) é—®é¢˜ï¼Œæå‡ºäº† Certainty-Guided Reflection Suppression (CGRS) æ–¹æ³•ã€‚CGRS æ—¨åœ¨å‡å°‘å†—ä½™æ¨ç†æ­¥éª¤å¹¶é™ä½æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ä¸ä¸‹é™ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¨¡å‹å¯¹å½“å‰å“åº”è¡¨ç°å‡ºé«˜ç½®ä¿¡åº¦æ—¶ï¼ŒåŠ¨æ€æŠ‘åˆ¶åæ€è§¦å‘è¯ (reflection triggers) çš„ç”Ÿæˆï¼Œä»è€Œæœ‰æ•ˆé˜²æ­¢ä¸å¿…è¦çš„åæ€å¾ªç¯ã€‚CGRS å…·æœ‰æ¨¡å‹æ— å…³æ€§ (model-agnostic)ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¶æ„ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªå›å½’ç”Ÿæˆç®¡çº¿ä¸­ã€‚åœ¨ AIME24ã€MATH500 å’Œ GPQA-D ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCGRS åœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°† Token ä½¿ç”¨é‡å¹³å‡å‡å°‘äº† 18.5% è‡³ 41.9%ã€‚è¯¥æ–¹æ³•åœ¨ DeepSeek-R1-Distill ç³»åˆ—ã€QwQ-32B å’Œ Qwen3 ç­‰ä¸åŒæ¶æ„ä¸è§„æ¨¡çš„æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºé«˜åº¦çš„ä¸€è‡´æ€§ï¼Œè¯æ˜äº†å…¶åœ¨æå‡æ¨ç†æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.05337v2",
      "published_date": "2025-08-07 12:38:22 UTC",
      "updated_date": "2025-11-17 08:47:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:37.797235+00:00"
    },
    {
      "arxiv_id": "2508.09181v1",
      "title": "Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach",
      "title_zh": "é¢å‘éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®è”é‚¦å­¦ä¹ çš„é•¿æœŸå®¢æˆ·ç«¯é€‰æ‹©ï¼šä¸€ç§çœŸå®æ‹å–æ–¹æ³•",
      "authors": [
        "Jinghong Tan",
        "Zhian Liu",
        "Kun Guo",
        "Mingxiong Zhao"
      ],
      "abstract": "Federated learning (FL) provides a decentralized framework that enables universal model training through collaborative efforts on mobile nodes, such as smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a mobile client, contributing to the process without uploading local data. This method leverages non-independent and identically distributed (non-IID) training data from different vehicles, influenced by various driving patterns and environmental conditions, which can significantly impact model convergence and accuracy. Although client selection can be a feasible solution for non-IID issues, it faces challenges related to selection metrics. Traditional metrics evaluate client data quality independently per round and require client selection after all clients complete local training, leading to resource wastage from unused training results. In the IoV context, where vehicles have limited connectivity and computational resources, information asymmetry in client selection risks clients submitting false information, potentially making the selection ineffective. To tackle these challenges, we propose a novel Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA). This scheme maximizes social welfare with consideration of long-term data quality using a new assessment mechanism and energy costs, and the advised auction mechanism with a deposit requirement incentivizes client participation and ensures information truthfulness. We theoretically prove the incentive compatibility and individual rationality of the advised incentive mechanism. Experimental results on various datasets, including those from IoV scenarios, demonstrate its effectiveness in mitigating performance degradation caused by non-IID data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¦è”ç½‘ (Internet of Vehicles, IoV) ç¯å¢ƒä¸‹è”é‚¦å­¦ä¹  (Federated Learning, FL) é¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒ (Non-IID) æ•°æ®æŒ‘æˆ˜ï¼Œä»¥åŠä¼ ç»Ÿå®¢æˆ·ç«¯é€‰æ‹©æœºåˆ¶ä¸­å­˜åœ¨çš„èµ„æºæµªè´¹å’Œä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºçœŸå®æ‹å–çš„é•¿çŸ­æœŸå®¢æˆ·ç«¯é€‰æ‹©è”é‚¦å­¦ä¹ æ¡†æ¶ (Long-term Client-Selection Federated Learning based on Truthful Auction, LCSFLA)ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æœºåˆ¶ï¼Œç»¼åˆè€ƒè™‘é•¿æœŸæ•°æ®è´¨é‡å’Œèƒ½æºæˆæœ¬ï¼Œä»¥å®ç°ç¤¾ä¼šç¦åˆ©æœ€å¤§åŒ–ã€‚åŒæ—¶ï¼Œæ‰€è®¾è®¡çš„æ‹å–æœºåˆ¶é€šè¿‡å¼•å…¥æŠ¼é‡‘è¦æ±‚ï¼Œæœ‰æ•ˆæ¿€åŠ±å®¢æˆ·ç«¯å‚ä¸å¹¶ç¡®ä¿äº†ä¿¡æ¯çš„çœŸå®æ€§ (Truthfulness)ã€‚ç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†è¯¥æœºåˆ¶æ»¡è¶³æ¿€åŠ±ç›¸å®¹æ€§ (Incentive Compatibility) å’Œä¸ªä½“ç†æ€§ (Individual Rationality)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLCSFLA åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆç¼“è§£ Non-IID æ•°æ®å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚ç§»åŠ¨ç½‘ç»œç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09181v1",
      "published_date": "2025-08-07 12:30:52 UTC",
      "updated_date": "2025-08-07 12:30:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:52:55.597882+00:00"
    },
    {
      "arxiv_id": "2508.05318v1",
      "title": "mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering",
      "title_zh": "mKG-RAGï¼šé¢å‘è§†è§‰é—®ç­”çš„å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¢å¼ºå‹ RAG",
      "authors": [
        "Xu Yuan",
        "Liangbo Ning",
        "Wenqi Fan",
        "Qing Li"
      ],
      "abstract": "Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating external knowledge databases into the generation process, which is widely used for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive advancements, vanilla RAG-based VQA methods that rely on unstructured documents and overlook the structural relationships among knowledge elements frequently introduce irrelevant or misleading content, reducing answer accuracy and reliability. To overcome these challenges, a promising solution is to integrate multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the generation by introducing structured multimodal knowledge. Therefore, in this paper, we propose a novel multimodal knowledge-augmented generation framework (mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks. Specifically, our approach leverages MLLM-powered keyword extraction and vision-text matching to distill semantically consistent and modality-aligned entities/relationships from multimodal documents, constructing high-quality multimodal KGs as structured knowledge representations. In addition, a dual-stage retrieval strategy equipped with a question-aware multimodal retriever is introduced to improve retrieval efficiency while refining precision. Comprehensive experiments demonstrate that our approach significantly outperforms existing methods, setting a new state-of-the-art for knowledge-based VQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†mKG-RAGï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè§†è§‰é—®ç­”(VQA)èƒ½åŠ›çš„å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢ç”Ÿæˆæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸRAGåœ¨å¤„ç†éç»“æ„åŒ–æ–‡æ¡£æ—¶å› ç¼ºä¹ç»“æ„åŒ–å…³è”è€Œäº§ç”Ÿçš„è¯¯å¯¼æ€§å†…å®¹å’Œå‡†ç¡®æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)è¿›è¡Œå…³é”®è¯æå–ï¼Œå¹¶é€šè¿‡è§†è§‰-æ–‡æœ¬åŒ¹é…æŠ€æœ¯ä»å¤šæ¨¡æ€æ–‡æ¡£ä¸­æç‚¼å‡ºè¯­ä¹‰ä¸€è‡´ä¸”æ¨¡æ€å¯¹é½çš„å®ä½“ä¸å…³ç³»ï¼Œæ„å»ºèµ·é«˜è´¨é‡çš„ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæ¡†æ¶å¼•å…¥äº†åŒ…å«é—®é¢˜æ„ŸçŸ¥æ£€ç´¢å™¨çš„åŒé˜¶æ®µæ£€ç´¢ç­–ç•¥ï¼Œåœ¨æå‡æ£€ç´¢æ•ˆç‡çš„åŒæ—¶è¿›ä¸€æ­¥ä¼˜åŒ–äº†ä¿¡æ¯è·å–çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒmKG-RAGåœ¨å¤šé¡¹çŸ¥è¯†å¯†é›†å‹VQAä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåˆ·æ–°äº†è¯¥é¢†åŸŸçš„å…ˆè¿›æ°´å¹³(SOTA)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05318v1",
      "published_date": "2025-08-07 12:22:50 UTC",
      "updated_date": "2025-08-07 12:22:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:03.186819+00:00"
    },
    {
      "arxiv_id": "2508.05311v1",
      "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents",
      "title_zh": "ä¸€ç§åŸºäºå†³ç­–æ ‘ä¸å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ç¬¦å·æ¨ç†æ–°æ¶æ„",
      "authors": [
        "Andrew Kiruluta"
      ],
      "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†åŸºäºå†³ç­–æ ‘ (Decision Trees) çš„ç¬¦å·æ¨ç†ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆçš„æ··åˆæ¶æ„ï¼Œå¹¶åœ¨åè°ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸‹è¿è¡Œã€‚è¯¥è®¾è®¡é€šè¿‡å°†å†³ç­–æ ‘å’Œéšæœºæ£®æ— (Random Forests) ä½œä¸ºå¯è°ƒç”¨çš„é¢„æµ‹æœº (Oracles) åµŒå…¥ç»Ÿä¸€ç³»ç»Ÿï¼Œå®ç°äº†å¯è§£é‡Šçš„è§„åˆ™æ¨ç†ä¸å› æœé€»è¾‘ã€‚LLM æ™ºèƒ½ä½“åœ¨ç³»ç»Ÿä¸­è´Ÿè´£å¤„ç†æº¯å› æ¨ç† (Abductive Reasoning)ã€æ³›åŒ–åŠäº¤äº’å¼è§„åˆ’ï¼Œå¹¶ç”±ä¸­å¤®ç¼–æ’å™¨ (Central Orchestrator) ç»´æŠ¤ä¿¡å¿µçŠ¶æ€çš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ ProofWriter åŸºå‡†ä¸Šå°†è•´å«ä¸€è‡´æ€§æå‡äº† 7.2%ï¼Œåœ¨ GSM8k å’Œ ARC ä»»åŠ¡ä¸­ä¹Ÿåˆ†åˆ«è·å¾—äº† 5.3% å’Œ 6.0% çš„å‡†ç¡®ç‡å¢é•¿ã€‚è¯¥æ¶æ„åœ¨ä¸´åºŠå†³ç­–æ”¯æŒå’Œç§‘å­¦å‘ç°åº”ç”¨ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆç¼–ç é¢†åŸŸè§„åˆ™å¹¶è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†çš„èƒ½åŠ›ï¼Œä¸ºç¨³å¥ä¸”å¯æ‰©å±•çš„é€šç”¨ç¥ç»ç¬¦å·æ¨ç† (Neuro-symbolic Reasoning) æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05311v1",
      "published_date": "2025-08-07 12:11:53 UTC",
      "updated_date": "2025-08-07 12:11:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:05.693762+00:00"
    },
    {
      "arxiv_id": "2508.05310v1",
      "title": "ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning",
      "title_zh": "ASkDAggerï¼šé¢å‘äº¤äº’å¼æ¨¡ä»¿å­¦ä¹ çš„ä¸»åŠ¨æŠ€èƒ½çº§æ•°æ®èšåˆ",
      "authors": [
        "Jelle Luijkx",
        "Zlatan AjanoviÄ‡",
        "Laura Ferranti",
        "Jens Kober"
      ],
      "abstract": "Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: \"I plan to do this, but I am uncertain.\" We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤äº’å¼æ¨¡ä»¿å­¦ä¹ (Interactive Imitation Learning)ä¸­äººç±»æ•™å¸ˆæ•™å­¦æˆæœ¬è¿‡é«˜çš„ç“¶é¢ˆï¼Œæå‡ºäº†ASkDAggeræ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸»åŠ¨æŠ€èƒ½çº§æ•°æ®èšåˆå‡å°‘æŸ¥è¯¢éœ€æ±‚ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šæ—¶ä¸»åŠ¨è¡¨è¾¾å…¶è§„åˆ’æ„å›¾ï¼Œå¹¶åˆ©ç”¨S-Aware Gating (SAG)æœºåˆ¶åŠ¨æ€è°ƒæ•´æŸ¥è¯¢é˜ˆå€¼ï¼Œä»¥å¹³è¡¡æŸ¥è¯¢é¢‘ç‡ä¸å¤±è´¥é£é™©ã€‚ç³»ç»Ÿé€šè¿‡Foresight Interactive Experience Replay (FIER)å°†ç»è¿‡æ•™å¸ˆåé¦ˆçš„æ™ºèƒ½ä½“è®¡åˆ’åŠ¨ä½œè½¬åŒ–ä¸ºæœ‰æ•ˆçš„ç¤ºæ•™æ•°æ®ï¼Œå¹¶ç»“åˆPrioritized Interactive Experience Replay (PIER)æ ¹æ®ä¸ç¡®å®šæ€§å’Œæ•°æ®æ—¶æ•ˆæ€§ä¼˜åŒ–å›æ”¾ä¼˜å…ˆçº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASkDAggeræ˜¾è‘—å‡å°‘äº†äººå·¥æ ‡æ³¨é‡ï¼ŒåŒæ—¶æå‡äº†æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”é€Ÿåº¦ã€‚è¯¥ç ”ç©¶åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„è¯­è¨€è¾…åŠ©æ“ä½œä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆçš„äººæœºåä½œå­¦ä¹ æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in Transactions on Machine Learning Research (TMLR, 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.05310v1",
      "published_date": "2025-08-07 12:10:46 UTC",
      "updated_date": "2025-08-07 12:10:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:13.187263+00:00"
    },
    {
      "arxiv_id": "2508.05306v1",
      "title": "Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces",
      "title_zh": "åŸºäºè‡ªå›å½’æ‰©æ•£æ¨¡å‹å™ªå£°ç©ºé—´çš„éŸ³é¢‘éŸ³ä¹æ„å¤–æ„Ÿä¼°ç®—",
      "authors": [
        "Mathias Rose Bjare",
        "Stefan Lattner",
        "Gerhard Widmer"
      ],
      "abstract": "Recently, the information content (IC) of predictions from a Generative Infinite-Vocabulary Transformer (GIVT) has been used to model musical expectancy and surprisal in audio. We investigate the effectiveness of such modelling using IC calculated with autoregressive diffusion models (ADMs). We empirically show that IC estimates of models based on two different diffusion ordinary differential equations (ODEs) describe diverse data better, in terms of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's effectiveness in capturing surprisal aspects by examining two tasks: (1) capturing monophonic pitch surprisal, and (2) detecting segment boundaries in multi-track audio. In both tasks, the diffusion models match or exceed the performance of a GIVT. We hypothesize that the surprisal estimated at different diffusion process noise levels corresponds to the surprisal of music and audio features present at different audio granularities. Testing our hypothesis, we find that, for appropriate noise levels, the studied musical surprisal tasks' results improve. Code is provided on github.com/SonyCSLParis/audioic.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è‡ªå›å½’æ‰©æ•£æ¨¡å‹ (Autoregressive Diffusion Models, ADMs) è®¡ç®—ä¿¡æ¯å«é‡ (Information Content, IC) ä»¥å»ºæ¨¡éŸ³é¢‘ä¸­éŸ³ä¹æƒŠè®¶åº¦ (Musical Surprisal) çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é€šè¿‡å®è¯åˆ†æè¡¨æ˜ï¼ŒåŸºäºä¸¤ç§ä¸åŒæ‰©æ•£å¸¸å¾®åˆ†æ–¹ç¨‹ (ODEs) çš„æ¨¡å‹åœ¨æè¿°æ•°æ®å¤šæ ·æ€§æ–¹é¢ä¼˜äºç”Ÿæˆå¼æ— é™è¯æ±‡ Transformer (GIVT)ã€‚é€šè¿‡åœ¨å•å£°é“éŸ³é«˜æƒŠè®¶åº¦æ•æ‰å’Œå¤šè½¨éŸ³é¢‘ç‰‡æ®µè¾¹ç•Œæ£€æµ‹ä¸¤é¡¹ä»»åŠ¡ä¸­çš„è¯„ä¼°ï¼Œæ‰©æ•£æ¨¡å‹å±•ç°å‡ºä¸ GIVT æŒå¹³æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚ä½œè€…æå‡ºå¹¶éªŒè¯äº†ä¸€é¡¹æ ¸å¿ƒå‡è®¾ï¼Œå³æ‰©æ•£è¿‡ç¨‹ä¸­ä¸åŒå™ªå£°æ°´å¹³ä¼°ç®—çš„æƒŠè®¶åº¦å¯¹åº”äºä¸åŒç»†ç²’åº¦çš„éŸ³é¢‘ç‰¹å¾ï¼Œé€šè¿‡é€‰æ‹©åˆé€‚çš„å™ªå£°æ°´å¹³å¯æœ‰æ•ˆæå‡ç›¸å…³ä»»åŠ¡çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 1 figure, 5 tables. Accepted at the 25th International Society for Music Information Retrieval Conference (ISMIR), Daejeon, South Korea, 2025 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05306v1",
      "published_date": "2025-08-07 12:05:27 UTC",
      "updated_date": "2025-08-07 12:05:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:15.782593+00:00"
    },
    {
      "arxiv_id": "2508.05299v1",
      "title": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test",
      "title_zh": "VS-LLMï¼šåŸºäº LLM çš„ç»˜ç”»æŠ•å°„æµ‹éªŒè§†è§‰-è¯­ä¹‰æŠ‘éƒè¯„ä¼°",
      "authors": [
        "Meiqi Wu",
        "Yaxuan Kang",
        "Xuchen Li",
        "Shiyu Hu",
        "Xiaotang Chen",
        "Yunfeng Kang",
        "Weiqiang Wang",
        "Kaiqi Huang"
      ],
      "abstract": "The Drawing Projection Test (DPT) is an essential tool in art therapy, allowing psychologists to assess participants' mental states through their sketches. Specifically, through sketches with the theme of \"a person picking an apple from a tree (PPAT)\", it can be revealed whether the participants are in mental states such as depression. Compared with scales, the DPT can enrich psychologists' understanding of an individual's mental state. However, the interpretation of the PPAT is laborious and depends on the experience of the psychologists. To address this issue, we propose an effective identification method to support psychologists in conducting a large-scale automatic DPT. Unlike traditional sketch recognition, DPT more focus on the overall evaluation of the sketches, such as color usage and space utilization. Moreover, PPAT imposes a time limit and prohibits verbal reminders, resulting in low drawing accuracy and a lack of detailed depiction. To address these challenges, we propose the following efforts: (1) Providing an experimental environment for automated analysis of PPAT sketches for depression assessment; (2) Offering a Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3) Experimental results demonstrate that our method improves by 17.6% compared to the psychologist assessment method. We anticipate that this work will contribute to the research in mental state assessment based on PPAT sketches' elements recognition. Our datasets and codes are available at https://github.com/wmeiqi/VS-LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‰ºæœ¯æ²»ç–—ä¸­ç»˜å›¾æŠ•å°„æµ‹è¯• (Drawing Projection Test, DPT)ï¼Œç‰¹åˆ«æ˜¯â€œäººä»æ ‘ä¸Šæ‘˜è‹¹æœâ€ (PPAT) æµ‹è¯•åœ¨æŠ‘éƒè¯„ä¼°ä¸­é¢ä¸´çš„æµç¨‹ç¹çä¸”è¿‡åº¦ä¾èµ–äººå·¥ç»éªŒç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ”¯æŒå¤§è§„æ¨¡è‡ªåŠ¨è¯„ä¼°çš„è¯†åˆ«æ–¹æ³•ã€‚ç ”ç©¶è€…å¼€å‘äº† VS-LLMï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„è§†è§‰è¯­ä¹‰ (Visual-Semantic) æŠ‘éƒè¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªåŠ¨åŒ–åˆ†æ PPAT ç´ æå…ƒç´ æ¥è¯†åˆ«å‚ä¸è€…çš„å¿ƒç†çŠ¶æ€ã€‚VS-LLM ä¾§é‡äºç´ æçš„è‰²å½©è¿ç”¨å’Œç©ºé—´åˆ©ç”¨ç­‰æ•´ä½“ç‰¹å¾ï¼Œæœ‰æ•ˆè§£å†³äº†æµ‹è¯•ç¯å¢ƒç”±äºé™æ—¶å’Œç¼ºä¹å¼•å¯¼å¯¼è‡´çš„ç»˜å›¾å‡†ç¡®åº¦ä½åŠç»†èŠ‚ç¼ºå¤±é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æŠ‘éƒè¯„ä¼°å‡†ç¡®ç‡ä¸Šæ¯”ä¼ ç»Ÿçš„å¿ƒç†å­¦å®¶è¯„ä¼°æ–¹æ³•æå‡äº† 17.6%ï¼Œä¸ºåŸºäºç´ æè¯†åˆ«çš„å¿ƒç†çŠ¶æ€è¯„ä¼°ç ”ç©¶åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³æ•°æ®é›†å’Œä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05299v1",
      "published_date": "2025-08-07 11:59:50 UTC",
      "updated_date": "2025-08-07 11:59:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:19.996397+00:00"
    },
    {
      "arxiv_id": "2508.05294v4",
      "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
      "title_zh": "è¿ˆå‘å…·èº«æ™ºèƒ½ä½“ AIï¼šLLM ä¸ VLM é©±åŠ¨çš„æœºå™¨äººè‡ªä¸»æ€§ä¸äº¤äº’ç»¼è¿°åŠåˆ†ç±»",
      "authors": [
        "Sahar Salimpour",
        "Lei Fu",
        "Kajetan RachwaÅ‚",
        "Pascal Bertrand",
        "Kevin O'Sullivan",
        "Robert Jakob",
        "Farhad Keramat",
        "Leonardo Militano",
        "Giovanni Toffetti",
        "Harry Edelman",
        "Jorge PeÃ±a Queralta"
      ],
      "abstract": "Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.",
      "tldr_zh": "è¯¥ç»¼è¿°ç ”ç©¶äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å…·èº«æ™ºèƒ½ä½“AIï¼Œç³»ç»Ÿæ€§åœ°å›é¡¾äº†æœºå™¨äººè‡ªä¸»æ€§å’Œäººæœºäº¤äº’é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚æ–‡ç« æ¢è®¨äº†ä»åŸºç¡€çš„GPTå¼æ¥å£åˆ°ä½œä¸ºåè°ƒè€…ã€è§„åˆ’è€…ã€æ„ŸçŸ¥æ‰§è¡Œè€…æˆ–é€šç”¨æ¥å£çš„å¤æ‚æ™ºèƒ½ä½“æ¶æ„ï¼Œå¹¶åˆ†æäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(VLAs)å’Œå¤§è¡Œä¸ºæ¨¡å‹(LBMs)å¦‚ä½•æå‡æœºå™¨äººçš„æ“ä½œçµæ´»æ€§ã€‚è¿™äº›æ™ºèƒ½ä½“æ¶æ„ä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€è°ƒç”¨APIã€è§„åˆ’ä»»åŠ¡åºåˆ—å¹¶ååŠ©è¿è¡Œè¯Šæ–­ã€‚é™¤äº†å­¦æœ¯ç ”ç©¶ï¼Œè¯¥ç»¼è¿°è¿˜æ¶µç›–äº†å¿«é€Ÿæ¼”è¿›çš„ç¤¾åŒºé¡¹ç›®ã€ROSè½¯ä»¶åŒ…åŠå·¥ä¸šæ¡†æ¶ï¼Œåæ˜ äº†å½“å‰è¡Œä¸šçš„å‰æ²¿è¶‹åŠ¿ã€‚ç ”ç©¶æœ€ç»ˆæå‡ºäº†ä¸€å¥—ç”¨äºåˆ†ç±»æ¨¡å‹é›†æˆæ–¹æ³•çš„åˆ†ç±»æ³•(Taxonomy)ï¼Œå¹¶å¯¹ä¸åŒè§£å†³æ–¹æ¡ˆä¸­æ™ºèƒ½ä½“æ‰€æ‰®æ¼”çš„è§’è‰²è¿›è¡Œäº†æ·±å…¥çš„æ¯”è¾ƒåˆ†æã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05294v4",
      "published_date": "2025-08-07 11:48:03 UTC",
      "updated_date": "2025-11-12 19:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:25.884695+00:00"
    },
    {
      "arxiv_id": "2508.05287v2",
      "title": "FlowState: Sampling Rate Invariant Time Series Forecasting",
      "title_zh": "FlowStateï¼šé‡‡æ ·ç‡æ— å…³çš„æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Lars Graf",
        "Thomas Ortner",
        "StanisÅ‚aw WoÅºniak",
        "Angeliki Pantazi"
      ],
      "abstract": "Foundation models (FMs) have transformed natural language processing, but their success has not yet translated to time series forecasting. Existing time series foundation models (TSFMs), often based on transformer variants, struggle with generalization across varying context and target lengths, lack adaptability to different sampling rates, and are computationally inefficient. We introduce FlowState, a novel TSFM architecture that addresses these challenges through two key innovations: a state space model (SSM) based encoder and a functional basis decoder. This design enables continuous-time modeling and dynamic time-scale adjustment, allowing FlowState to inherently generalize across all possible temporal resolutions, and dynamically adjust the forecasting horizons. In contrast to other state-of-the-art TSFMs, which require training data across all possible sampling rates to memorize patterns at each scale, FlowState inherently adapts its internal dynamics to the input scale, enabling smaller models, reduced data requirements, and improved efficiency. We further propose an efficient pretraining strategy that improves robustness and accelerates training. Despite being the smallest model, FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of its components, and we demonstrate its unique ability to adapt online to varying input sampling rates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlowStateï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)åœ¨é‡‡æ ·ç‡å˜åŒ–ã€ä¸Šä¸‹æ–‡é•¿åº¦æ³›åŒ–åŠè®¡ç®—æ•ˆç‡ç­‰æ–¹é¢æŒ‘æˆ˜çš„æ–°å‹æ¶æ„ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆäº†åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹(SSM)çš„ç¼–ç å™¨å’Œå‡½æ•°åŸºè§£ç å™¨ï¼Œå®ç°äº†è¿ç»­æ—¶é—´å»ºæ¨¡ä¸åŠ¨æ€æ—¶é—´å°ºåº¦è°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªç„¶åœ°æ³›åŒ–åˆ°å„ç§æ—¶é—´åˆ†è¾¨ç‡ã€‚ä¸éœ€è¦é€šè¿‡å¤šé‡‡æ ·ç‡æ•°æ®è®­ç»ƒæ¥è®°å¿†æ¨¡å¼çš„ç°æœ‰æ¨¡å‹ä¸åŒï¼ŒFlowState èƒ½å¤Ÿæ ¹æ®è¾“å…¥å°ºåº¦è‡ªé€‚åº”åœ°è°ƒæ•´å†…éƒ¨åŠ¨æ€ï¼Œåœ¨å‡å°æ¨¡å‹è§„æ¨¡å’Œé™ä½æ•°æ®éœ€æ±‚çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlowState å°½ç®¡æ˜¯å‚æ•°é‡æœ€å°çš„æ¨¡å‹ï¼Œä½†åœ¨ GIFT-ZS å’Œ Chronos-ZS åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨ä¸åŒè¾“å…¥é‡‡æ ·ç‡ä¸‹çš„åœ¨çº¿è‡ªé€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§å¹¶åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Currently under review",
      "pdf_url": "https://arxiv.org/pdf/2508.05287v2",
      "published_date": "2025-08-07 11:30:26 UTC",
      "updated_date": "2025-08-19 12:17:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:26.188469+00:00"
    },
    {
      "arxiv_id": "2508.05267v1",
      "title": "An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication",
      "title_zh": "ä¸€ç§ç”¨äºä¼ä¸šé€šä¿¡ä¸­ç›®æ ‡å—ä¼—è¯†åˆ«ä¸é€šçŸ¥çš„å¯è§£é‡Šè‡ªç„¶è¯­è¨€æ¡†æ¶",
      "authors": [
        "VÃ­tor N. LourenÃ§o",
        "Mohnish Dubey",
        "Yunfei Bai",
        "Audrey Depeige",
        "Vivek Jain"
      ],
      "abstract": "In large-scale maintenance organizations, identifying subject matter experts and managing communications across complex entities relationships poses significant challenges -- including information overload and longer response times -- that traditional communication approaches fail to address effectively. We propose a novel framework that combines RDF graph databases with LLMs to process natural language queries for precise audience targeting, while providing transparent reasoning through a planning-orchestration architecture. Our solution enables communication owners to formulate intuitive queries combining concepts such as equipment, manufacturers, maintenance engineers, and facilities, delivering explainable results that maintain trust in the system while improving communication efficiency across the organization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆäº† RDF å›¾æ•°æ®åº“ä¸ LLMs çš„æ–°å‹è‡ªç„¶è¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹ç»´æŠ¤ç»„ç»‡åœ¨è¯†åˆ«é¢†åŸŸä¸“å®¶å’Œç®¡ç†å¤æ‚è·¨å®ä½“æ²Ÿé€šæ—¶é¢ä¸´çš„ä¿¡æ¯è¿‡è½½åŠå“åº”å»¶è¿Ÿé—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è§„åˆ’ç¼–æ’(planning-orchestration)æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†æ¶‰åŠè®¾å¤‡ã€åˆ¶é€ å•†ã€ç»´æŠ¤å·¥ç¨‹å¸ˆå’Œè®¾æ–½ç­‰å¤šä¸ªç»´åº¦çš„ç›´è§‚æŸ¥è¯¢ï¼Œå¹¶æä¾›é€æ˜çš„æ¨ç†è¿‡ç¨‹ä»¥ç¡®ä¿ç»“æœçš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡å°†å›¾å½¢æ•°æ®çš„ç»“æ„åŒ–ä¼˜åŠ¿ä¸å¤§è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›ç›¸ç»“åˆï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†ç²¾å‡†çš„å—ä¼—å®šä½ã€‚è¿™ç§æ–¹æ³•å…è®¸æ²Ÿé€šè´Ÿè´£äººä½¿ç”¨è‡ªç„¶è¯­è¨€çµæ´»ç»„åˆå®ä½“æ¦‚å¿µï¼Œæå¤§ç®€åŒ–äº†ç›®æ ‡å—ä¼—çš„ç­›é€‰æµç¨‹ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æå‡ä¼ä¸šå†…éƒ¨æ²Ÿé€šæ•ˆç‡çš„åŒæ—¶ï¼Œé€šè¿‡å¯è§£é‡Šæ€§ç»´æŒäº†ç”¨æˆ·å¯¹ç³»ç»Ÿçš„ä¿¡ä»»ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ²Ÿé€šæ–¹å¼çš„å±€é™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to publication at the 24th International Semantic Web Conference Industry Track, ISWC 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05267v1",
      "published_date": "2025-08-07 11:02:40 UTC",
      "updated_date": "2025-08-07 11:02:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:34.158185+00:00"
    },
    {
      "arxiv_id": "2508.05264v4",
      "title": "SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion",
      "title_zh": "SGDFuseï¼šåŸºäº SAM å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸçº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆ",
      "authors": [
        "Xiaoyang Zhang",
        "jinjiang Li",
        "Guodong Fan",
        "Yakun Ju",
        "Linwei Fan",
        "Jun Liu",
        "Alex C. Kot"
      ],
      "abstract": "Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆ (IVIF) ä¸­å› ç¼ºä¹æ·±åº¦è¯­ä¹‰ç†è§£è€Œå¯¼è‡´çš„ä¼ªå½±å’Œç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œæå‡ºäº†åŸºäº Segment Anything Model (SAM) å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ SGDFuseã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºå°† SAM ç”Ÿæˆçš„é«˜è´¨é‡è¯­ä¹‰æ©ç  (semantic masks) ä½œä¸ºæ˜¾å¼å…ˆéªŒï¼Œç”¨ä»¥æŒ‡å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ (conditional diffusion model) çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚SGDFuse é‡‡ç”¨ä¸¤é˜¶æ®µå¤„ç†æµç¨‹ï¼Œé¦–å…ˆå®Œæˆå¤šæ¨¡æ€ç‰¹å¾çš„åˆæ­¥èåˆï¼Œéšåç»“åˆè¯­ä¹‰æ©ç é©±åŠ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œä»ç²—åˆ°ç»†çš„å»å™ªç”Ÿæˆï¼Œä»è€Œç¡®ä¿èåˆç»“æœå…¼å…·è¯­ä¹‰å‡†ç¡®æ€§ä¸é«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»å®¢è§‚è¯„ä»·æŒ‡æ ‡ä»¥åŠä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ä¸Šå‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚SGDFuse æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹å…³é”®ç›®æ ‡ä¸¢å¤±çš„ç—›ç‚¹ï¼Œä¸ºé«˜ä¿çœŸå›¾åƒèåˆé¢†åŸŸæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to Information Fusion",
      "pdf_url": "https://arxiv.org/pdf/2508.05264v4",
      "published_date": "2025-08-07 10:58:52 UTC",
      "updated_date": "2025-11-24 05:44:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:39.246569+00:00"
    },
    {
      "arxiv_id": "2508.05262v1",
      "title": "Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging",
      "title_zh": "åŸºäºç²’å­æ»¤æ³¢çš„è§å…‰å¿ƒè„æˆåƒé²æ£’è·Ÿè¸ª",
      "authors": [
        "Suresh Guttikonda",
        "Maximilian Neidhart",
        "Johanna Sprenger",
        "Johannes Petersen",
        "Christian Detter",
        "Alexander Schlaefer"
      ],
      "abstract": "Intraoperative fluorescent cardiac imaging enables quality control following coronary bypass grafting surgery. We can estimate local quantitative indicators, such as cardiac perfusion, by tracking local feature points. However, heart motion and significant fluctuations in image characteristics caused by vessel structural enrichment limit traditional tracking methods. We propose a particle filtering tracker based on cyclicconsistency checks to robustly track particles sampled to follow target landmarks. Our method tracks 117 targets simultaneously at 25.4 fps, allowing real-time estimates during interventions. It achieves a tracking error of (5.00 +/- 0.22 px) and outperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional trackers (58.1 +/- 27.1 px).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å† çŠ¶åŠ¨è„‰æ­æ¡¥æ‰‹æœ¯ä¸­è§å…‰å¿ƒè„æˆåƒ(fluorescent cardiac imaging)é¢ä¸´çš„å¿ƒè„è¿åŠ¨å’Œå›¾åƒç‰¹å¾æ³¢åŠ¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¾ªç¯ä¸€è‡´æ€§æ£€æŸ¥(cyclic-consistency checks)çš„ç²’å­æ»¤æ³¢è·Ÿè¸ªå™¨(particle filtering tracker)ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹ç›®æ ‡å…³é”®ç‚¹è¿›è¡Œé²æ£’æ€§è·Ÿè¸ªï¼Œæ—¨åœ¨å®ç°å¿ƒè„çŒæ³¨(cardiac perfusion)ç­‰å±€éƒ¨å®šé‡æŒ‡æ ‡çš„å‡†ç¡®è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿä»¥25.4 fpsçš„é€Ÿåº¦åŒæ—¶è·Ÿè¸ª117ä¸ªç›®æ ‡ï¼Œæ»¡è¶³æ‰‹æœ¯å¹²é¢„ä¸­çš„å®æ—¶æ€§éœ€æ±‚ã€‚åœ¨æ€§èƒ½è¡¨ç°ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†(5.00 +/- 0.22 px)çš„è·Ÿè¸ªè¯¯å·®ï¼Œå…¶ç²¾åº¦æ˜¾è‘—ä¼˜äºæ·±åº¦å­¦ä¹ è·Ÿè¸ªå™¨(22.3 +/- 1.1 px)åŠä¼ ç»Ÿè·Ÿè¸ªå™¨(58.1 +/- 27.1 px)ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºä¸´åºŠæœ¯ä¸­è´¨é‡æ§åˆ¶æä¾›äº†å¯é çš„è‡ªåŠ¨åŒ–è·Ÿè¸ªæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CURAC conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05262v1",
      "published_date": "2025-08-07 10:57:13 UTC",
      "updated_date": "2025-08-07 10:57:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:01.404226+00:00"
    },
    {
      "arxiv_id": "2508.09180v1",
      "title": "scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering",
      "title_zh": "scAGCï¼šåŸºäºå¯¹æ¯”å¼•å¯¼çš„è‡ªé€‚åº”ç»†èƒå›¾å­¦ä¹ å•ç»†èƒèšç±»",
      "authors": [
        "Huifa Li",
        "Jie Fu",
        "Xinlin Zhuang",
        "Haolin Yang",
        "Xinpeng Ling",
        "Tong Cheng",
        "Haochen xue",
        "Imran Razzak",
        "Zhili Chen"
      ],
      "abstract": "Accurate cell type annotation is a crucial step in analyzing single-cell RNA sequencing (scRNA-seq) data, which provides valuable insights into cellular heterogeneity. However, due to the high dimensionality and prevalence of zero elements in scRNA-seq data, traditional clustering methods face significant statistical and computational challenges. While some advanced methods use graph neural networks to model cell-cell relationships, they often depend on static graph structures that are sensitive to noise and fail to capture the long-tailed distribution inherent in single-cell populations.To address these limitations, we propose scAGC, a single-cell clustering method that learns adaptive cell graphs with contrastive guidance. Our approach optimizes feature representations and cell graphs simultaneously in an end-to-end manner. Specifically, we introduce a topology-adaptive graph autoencoder that leverages a differentiable Gumbel-Softmax sampling strategy to dynamically refine the graph structure during training. This adaptive mechanism mitigates the problem of a long-tailed degree distribution by promoting a more balanced neighborhood structure. To model the discrete, over-dispersed, and zero-inflated nature of scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for robust feature reconstruction. Furthermore, a contrastive learning objective is incorporated to regularize the graph learning process and prevent abrupt changes in the graph topology, ensuring stability and enhancing convergence. Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC consistently outperforms other state-of-the-art methods, yielding the best NMI and ARI scores on 9 and 7 datasets, respectively.Our code is available at Anonymous Github.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† scAGCï¼Œä¸€ç§å¸¦æœ‰å¯¹æ¯”å¼•å¯¼çš„è‡ªé€‚åº”ç»†èƒå›¾å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å•ç»†èƒ RNA æµ‹åº (scRNA-seq) æ•°æ®ä¸­ç”±äºé«˜ç»´å’Œé›¶å€¼åå¤šç»™ä¼ ç»Ÿèšç±»å¸¦æ¥çš„æŒ‘æˆ˜ã€‚scAGC é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼åŒæ­¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºä¸ç»†èƒå›¾ï¼Œå¹¶å¼•å…¥æ‹“æ‰‘è‡ªé€‚åº”å›¾è‡ªç¼–ç å™¨åŠ Gumbel-Softmax é‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†è®­ç»ƒè¿‡ç¨‹ä¸­å›¾ç»“æ„çš„åŠ¨æ€æ”¹è¿›ã€‚è¯¥æœºåˆ¶ä¸ä»…ç¼“è§£äº†å›ºæœ‰çš„é•¿å°¾åº¦åˆ†å¸ƒé—®é¢˜ï¼Œè¿˜é€šè¿‡é›†æˆé›¶è†¨èƒ€è´ŸäºŒé¡¹ (ZINB) æŸå¤±å‡½æ•°æœ‰æ•ˆå»ºæ¨¡äº†æ•°æ®çš„é›¶è†¨èƒ€ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”å­¦ä¹ ç›®æ ‡çš„åŠ å…¥è¿›ä¸€æ­¥è§„èŒƒäº†å›¾å­¦ä¹ è¿‡ç¨‹ï¼Œé˜²æ­¢äº†å›¾æ‹“æ‰‘çš„çªå˜å¹¶ç¡®ä¿äº†æ”¶æ•›ç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒscAGC åœ¨ 9 ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„ NMI å’Œ ARI è¯„åˆ†å‡ä¼˜äºç›®å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è¯†åˆ«ç»†èƒå¼‚è´¨æ€§æ–¹é¢çš„å“è¶Šæ€§èƒ½ä¸ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09180v1",
      "published_date": "2025-08-07 10:55:52 UTC",
      "updated_date": "2025-08-07 10:55:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:06.495322+00:00"
    },
    {
      "arxiv_id": "2508.05260v1",
      "title": "Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models",
      "title_zh": "åŸºäº LSTM-RF æ··åˆæ¨¡å‹çš„æµ·æ´‹å¶ç»¿ç´ é¢„æµ‹åŠé©±åŠ¨å› ç´ åˆ†æ",
      "authors": [
        "Zhouyao Qian",
        "Yang Chen",
        "Baodian Li",
        "Shuyi Zhang",
        "Zhen Tian",
        "Gongsen Wang",
        "Tianyue Gu",
        "Xinyu Zhou",
        "Huilin Chen",
        "Xinyi Li",
        "Hao Zhu",
        "Shuyao Zhang",
        "Zongheng Li",
        "Siyuan Wang"
      ],
      "abstract": "Marine chlorophyll concentration is an important indicator of ecosystem health and carbon cycle strength, and its accurate prediction is crucial for red tide warning and ecological response. In this paper, we propose a LSTM-RF hybrid model that combines the advantages of LSTM and RF, which solves the deficiencies of a single model in time-series modelling and nonlinear feature portrayal. Trained with multi-source ocean data(temperature, salinity, dissolved oxygen, etc.), the experimental results show that the LSTM-RF model has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2 =0.4934) alone , respectively. The standardised treatment and sliding window approach improved the prediction accuracy of the model and provided an innovative solution for high-frequency prediction of marine ecological variables.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§LSTM-RFæ··åˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆLSTMçš„æ—¶é—´åºåˆ—å»ºæ¨¡ä¼˜åŠ¿ä¸RFçš„éçº¿æ€§ç‰¹å¾åˆ»ç”»èƒ½åŠ›ï¼Œå®ç°å¯¹æµ·æ´‹å¶ç»¿ç´ æµ“åº¦(Marine chlorophyll concentration)çš„ç²¾å‡†é¢„æµ‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ¸©åº¦(temperature)ã€ç›åº¦(salinity)å’Œæº¶è§£æ°§(dissolved oxygen)ç­‰å¤šæºæµ·æ´‹æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç»“åˆæ ‡å‡†åŒ–å¤„ç†ä¸æ»‘åŠ¨çª—å£(sliding window)æ–¹æ³•ä¼˜åŒ–é¢„æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLSTM-RFæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„R^2è¾¾åˆ°0.5386ï¼Œæ˜¾è‘—ä¼˜äºå•ç‹¬ä½¿ç”¨LSTM(R^2 = 0.0208)æˆ–RF(R^2 = 0.4934)çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œè§£å†³äº†å•ä¸€æ¨¡å‹åœ¨å¤æ‚æµ·æ´‹ç”Ÿæ€æ•°æ®å»ºæ¨¡ä¸­çš„ä¸è¶³ï¼Œä¸ºèµ¤æ½®é¢„è­¦åŠé«˜é¢‘æµ·æ´‹ç”Ÿæ€å˜é‡é¢„æµ‹æä¾›äº†ä¸€ç§åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE 5th International Conference on Advanced Algorithms and Neural Networks (AANN)",
      "pdf_url": "https://arxiv.org/pdf/2508.05260v1",
      "published_date": "2025-08-07 10:55:42 UTC",
      "updated_date": "2025-08-07 10:55:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:55.950864+00:00"
    },
    {
      "arxiv_id": "2508.05254v3",
      "title": "CF3: Compact and Fast 3D Feature Fields",
      "title_zh": "CF3ï¼šç´§å‡‘ä¸”å¿«é€Ÿçš„ 3D ç‰¹å¾åœº",
      "authors": [
        "Hyunjoon Lee",
        "Joonkyu Min",
        "Jaesik Park"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CF3ï¼Œä¸€ç§æ—¨åœ¨æ„å»ºç´§å‡‘ä¸”å¿«é€Ÿçš„ 3D Gaussian feature fields çš„è‡ªä¸Šè€Œä¸‹ (top-down) æµç¨‹ï¼Œè§£å†³äº†ç°æœ‰åŸºäº 3D Gaussian Splatting (3DGS) çš„æ–¹æ³•åœ¨æ•´åˆ 2D foundation models æ—¶è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚CF3 é¦–å…ˆé€šè¿‡å¤šè§†å›¾ 2D ç‰¹å¾ä¸é¢„è®­ç»ƒ Gaussians çš„å¿«é€ŸåŠ æƒèåˆ (weighted fusion)ï¼Œç›´æ¥åœ¨æå‡åçš„ç‰¹å¾ä¸Šè®­ç»ƒ per-Gaussian autoencoderï¼Œä»è€Œå®ç°æ›´å¥½çš„ç‰¹å¾åˆ†å¸ƒå¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è‡ªé€‚åº”ç¨€ç–åŒ–æ–¹æ³• (adaptive sparsification)ï¼Œåœ¨ä¼˜åŒ–ç‰¹å¾åœºå±æ€§çš„åŒæ—¶å¯¹å†—ä½™ Gaussians è¿›è¡Œå‰ªæ (pruning) å’Œåˆå¹¶ (merging)ï¼Œä»¥æ„å»ºä¿ç•™å‡ ä½•ç»†èŠ‚çš„é«˜æ•ˆè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCF3 ä»…éœ€ Feature-3DGS çº¦ 5% çš„ Gaussians å³å¯å®ç°æå…·ç«äº‰åŠ›çš„ 3D ç‰¹å¾åœºæ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡ç‰¹å¾åœºæ„å»ºé€Ÿåº¦çš„åŒæ—¶ï¼Œå¤§å¹…å‹ç¼©äº†æ¨¡å‹è§„æ¨¡ï¼Œä¸ºå®æ—¶ 3D åœºæ™¯ç†è§£æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025, Project Page: https://jjoonii.github.io/cf3-website/",
      "pdf_url": "https://arxiv.org/pdf/2508.05254v3",
      "published_date": "2025-08-07 10:45:08 UTC",
      "updated_date": "2025-09-02 04:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:53:59.844970+00:00"
    },
    {
      "arxiv_id": "2508.05246v2",
      "title": "A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis",
      "title_zh": "åŸºäºè™¹è†œå›¾åƒçš„æ€§åˆ«åˆ†ç±»æŠ€æœ¯ç ”ç©¶ï¼šæ·±åº¦ç»¼è¿°ä¸åˆ†æ",
      "authors": [
        "Basna Mohammed Salih Hasan",
        "Ramadhan J. Mstafa"
      ],
      "abstract": "Gender classification is attractive in a range of applications, including surveillance and monitoring, corporate profiling, and human-computer interaction. Individuals' identities may be gleaned from information about their gender, which is a kind of soft biometric. Over the years, several methods for determining a person's gender have been devised. Some of the most well-known ones are based on physical characteristics like face, fingerprint, palmprint, DNA, ears, gait, and iris. On the other hand, facial features account for the vast majority of gender classification methods. Also, the iris is a significant biometric trait because the iris, according to research, remains basically constant during an individual's life. Besides that, the iris is externally visible and is non-invasive to the user, which is important for practical applications. Furthermore, there are already high-quality methods for segmenting and encoding iris images, and the current methods facilitate selecting and extracting attribute vectors from iris textures. This study discusses several approaches to determining gender. The previous works of literature are briefly reviewed. Additionally, there are a variety of methodologies for different steps of gender classification. This study provides researchers with knowledge and analysis of the existing gender classification approaches. Also, it will assist researchers who are interested in this specific area, as well as highlight the gaps and challenges in the field, and finally provide suggestions and future paths for improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŸºäºè™¹è†œå›¾åƒ(Iris Images)çš„æ€§åˆ«åˆ†ç±»(Gender Classification)æŠ€æœ¯è¿›è¡Œäº†æ·±åº¦ç»¼è¿°ä¸åˆ†æã€‚è®ºæ–‡æ¢è®¨äº†è™¹è†œä½œä¸ºä¸€ç§é‡è¦çš„è½¯ç”Ÿç‰©ç‰¹å¾(Soft Biometric)ï¼Œå› å…¶åœ¨ç”Ÿå‘½å‘¨æœŸå†…åŸºæœ¬ä¿æŒä¸å˜ã€éä¾µå…¥æ€§ä¸”å¤–éƒ¨å¯è§ç­‰ç‰¹æ€§ï¼Œåœ¨å®‰å…¨ç›‘æ§ã€ä¼ä¸šç”»åƒå’Œäººæœºäº¤äº’(Human-Computer Interaction)ç­‰é¢†åŸŸå±•ç°å‡ºé‡è¦çš„åº”ç”¨ä»·å€¼ã€‚æ–‡ç« ç³»ç»Ÿåœ°å›é¡¾äº†ç›¸å…³æ–‡çŒ®ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†æ€§åˆ«åˆ†ç±»ä¸åŒé˜¶æ®µçš„å¤šç§æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬å¦‚ä½•ä»è™¹è†œçº¹ç†ä¸­é«˜æ•ˆæå–ç‰¹å¾å‘é‡ã€‚è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºç§‘ç ”äººå‘˜æä¾›ç°æœ‰åˆ†ç±»æ–¹æ³•çš„å…¨é¢åˆ†æï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºäº†è¯¥é¢†åŸŸç›®å‰é¢ä¸´çš„æŠ€æœ¯å·®è·ä¸æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…é’ˆå¯¹ç³»ç»Ÿæ€§èƒ½çš„æ”¹è¿›æå‡ºäº†ä¸“ä¸šå»ºè®®ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶è·¯å¾„æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 Pages, 8 Figures, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2508.05246v2",
      "published_date": "2025-08-07 10:33:40 UTC",
      "updated_date": "2025-08-08 13:18:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:04.586507+00:00"
    },
    {
      "arxiv_id": "2508.05244v2",
      "title": "RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding",
      "title_zh": "RegionMed-CLIPï¼šé¢å‘åŒ»å­¦å›¾åƒç†è§£çš„åŒºåŸŸæ„ŸçŸ¥å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹",
      "authors": [
        "Tianchen Fang",
        "Guiru Liu"
      ],
      "abstract": "Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RegionMed-CLIPï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸæ„ŸçŸ¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ (multimodal contrastive learning)é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å›¾åƒç†è§£ä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®åŒ®ä¹ä»¥åŠè¿‡åº¦ä¾èµ–å…¨å±€ç‰¹å¾è€Œå¿½è§†å±€éƒ¨ç—…ç†åŒºåŸŸçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ„Ÿå…´è¶£åŒºåŸŸ(ROI)å¤„ç†å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å°†ç»†ç²’åº¦çš„å±€éƒ¨ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡é›†æˆï¼Œå¹¶ç»“åˆæ¸è¿›å¼è®­ç»ƒç­–ç•¥ä»¥å¢å¼ºå±‚æ¬¡åŒ–çš„å¤šæ¨¡æ€å¯¹é½ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡åŒºåŸŸçº§è¡¨ç¤ºå­¦ä¹ ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†MedRegion-500kæ•°æ®é›†ï¼ŒåŒ…å«50ä¸‡ä¸ªå…·æœ‰å¹¿æ³›åŒºåŸŸæ ‡æ³¨å’Œå¤šå±‚çº§ä¸´åºŠæè¿°çš„åŒ»ç”¨å›¾åƒ-æ–‡æœ¬å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRegionMed-CLIPåœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢(image-text retrieval)ã€é›¶æ ·æœ¬åˆ†ç±»(zero-shot classification)å’Œè§†è§‰é—®ç­”(visual question answering)ç­‰ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ã€‚è¯¥æˆæœè¯æ˜äº†åŒºåŸŸæ„ŸçŸ¥å¯¹æ¯”é¢„è®­ç»ƒåœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºé²æ£’çš„å¤šæ¨¡æ€åŒ»ç–—å›¾åƒç†è§£å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Upon further review, we identified that our dataset requires optimization to ensure research reliability and accuracy. Additionally, considering the target journal's latest submission policies, we believe comprehensive manuscript revisions are necessary",
      "pdf_url": "https://arxiv.org/pdf/2508.05244v2",
      "published_date": "2025-08-07 10:32:03 UTC",
      "updated_date": "2025-09-19 09:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:08.990343+00:00"
    },
    {
      "arxiv_id": "2508.05240v1",
      "title": "Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer",
      "title_zh": "åŸºäºå½±åƒé£æ ¼è¿ç§»çš„ç£å…±æŒ¯ä¸è¶…å£°å›¾åƒç”±ç²—åˆ°ç²¾è”åˆé…å‡†",
      "authors": [
        "Junyi Wang",
        "Xi Zhu",
        "Yikun Guo",
        "Zixi Wang",
        "Haichuan Gao",
        "Le Zhang",
        "Fan Zhang"
      ],
      "abstract": "We developed a pipeline for registering pre-surgery Magnetic Resonance (MR) images and post-resection Ultrasound (US) images. Our approach leverages unpaired style transfer using 3D CycleGAN to generate synthetic T1 images, thereby enhancing registration performance. Additionally, our registration process employs both affine and local deformable transformations for a coarse-to-fine registration. The results demonstrate that our approach improves the consistency between MR and US image pairs in most cases.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§ç”¨äºæœ¯å‰ç£å…±æŒ¯æˆåƒ(MR)ä¸åˆ‡é™¤æœ¯åè¶…å£°(US)å›¾åƒé…å‡†çš„å¤„ç†æµç¨‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäº3D CycleGANçš„éé…å¯¹é£æ ¼è¿ç§»(unpaired style transfer)æŠ€æœ¯ç”Ÿæˆåˆæˆçš„T1å›¾åƒï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†è·¨æ¨¡æ€å½±åƒçš„é…å‡†æ€§èƒ½ã€‚åœ¨å…·ä½“çš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶ç»“åˆäº†ä»¿å°„å˜æ¢(affine transformation)å’Œå±€éƒ¨å¯å˜å½¢å˜æ¢(local deformable transformations)ï¼Œå®ç°äº†ä»ç²—åˆ°ç²¾(coarse-to-fine)çš„è”åˆé…å‡†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½èƒ½æœ‰æ•ˆæé«˜MRä¸USå›¾åƒå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶æˆæœä¸ºä¸´åºŠæ‰‹æœ¯å¯¼èˆªåŠæœ¯åå½±åƒè¯„ä¼°æä¾›äº†æ›´ç²¾å‡†çš„è·¨æ¨¡æ€å¯¹é½æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05240v1",
      "published_date": "2025-08-07 10:27:50 UTC",
      "updated_date": "2025-08-07 10:27:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:11.850681+00:00"
    },
    {
      "arxiv_id": "2508.05239v1",
      "title": "Pruning Large Language Models by Identifying and Preserving Functional Networks",
      "title_zh": "é€šè¿‡è¯†åˆ«ä¸ä¿ç•™åŠŸèƒ½ç½‘ç»œå®ç°å¤§è¯­è¨€æ¨¡å‹å‰ªæ",
      "authors": [
        "Yiheng Liu",
        "Junhao Ning",
        "Sichen Xia",
        "Xiaohui Gao",
        "Ning Qiang",
        "Bao Ge",
        "Junwei Han",
        "Xintao Hu"
      ],
      "abstract": "Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ç»“æ„åŒ–å‰ªæ(Structured Pruning)æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯å› å¿½ç•¥ç¥ç»å…ƒé—´çš„åä½œè€Œç ´åæ¨¡å‹å®è§‚åŠŸèƒ½æ¶æ„çš„é—®é¢˜ã€‚å—äººç±»å¤§è„‘åŠŸèƒ½ç¥ç»ç½‘ç»œçš„å¯å‘ï¼Œç ”ç©¶è€…å°†LLMè§†ä¸ºæ•°å­—å¤§è„‘ï¼Œå¹¶æå‡ºé€šè¿‡è¯†åˆ«å’Œä¿ç•™æ¨¡å‹å†…éƒ¨çš„åŠŸèƒ½ç½‘ç»œ(Functional Networks)æ¥è¿›è¡Œå‰ªæã€‚è¯¥æ–¹æ³•é€šè¿‡å°†LLMåˆ†è§£ä¸ºç±»ä¼¼äºç¥ç»å½±åƒæ•°æ®ä¸­çš„åŠŸèƒ½ç½‘ç»œï¼Œä»è€Œç²¾å‡†é”å®šå¹¶ä¿ç•™è¿™äº›ç½‘ç»œä¸­çš„å…³é”®ç¥ç»å…ƒä»¥å®ç°é«˜æ•ˆå‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå®šä½LLMä¸­çš„æ ¸å¿ƒåŠŸèƒ½åŒºåŸŸä¸å…³é”®ç¥ç»å…ƒï¼Œåœ¨å¤§å¹…æå‡æ¨¡å‹è¿è¡Œæ•ˆç‡çš„åŒæ—¶æœ‰æ•ˆç»´æŒäº†æ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05239v1",
      "published_date": "2025-08-07 10:27:01 UTC",
      "updated_date": "2025-08-07 10:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:17.567225+00:00"
    },
    {
      "arxiv_id": "2508.05238v1",
      "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models",
      "title_zh": "Driver Assistantï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼é©¾é©¶å‘˜è°ƒæ•´æ¬¡è¦ä»»åŠ¡",
      "authors": [
        "Wei Xiang",
        "Muchen Li",
        "Jie Yan",
        "Manling Zheng",
        "Hanfei Zhu",
        "Mengyun Jiang",
        "Lingyun Sun"
      ],
      "abstract": "Level 3 automated driving systems allows drivers to engage in secondary tasks while diminishing their perception of risk. In the event of an emergency necessitating driver intervention, the system will alert the driver with a limited window for reaction and imposing a substantial cognitive burden. To address this challenge, this study employs a Large Language Model (LLM) to assist drivers in maintaining an appropriate attention on road conditions through a \"humanized\" persuasive advice. Our tool leverages the road conditions encountered by Level 3 systems as triggers, proactively steering driver behavior via both visual and auditory routes. Empirical study indicates that our tool is effective in sustaining driver attention with reduced cognitive load and coordinating secondary tasks with takeover behavior. Our work provides insights into the potential of using LLMs to support drivers during multi-task automated driving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹L3çº§è‡ªåŠ¨é©¾é©¶(Level 3 automated driving)ç³»ç»Ÿä¸­é©¾é©¶å‘˜å› å‚ä¸æ¬¡è¦ä»»åŠ¡(secondary tasks)è€Œå¯¼è‡´é£é™©æ„ŸçŸ¥ä¸‹é™çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model)çš„é©¾é©¶è¾…åŠ©ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨é“è·¯çŠ¶å†µä½œä¸ºè§¦å‘ä¿¡å·ï¼Œé€šè¿‡è§†è§‰å’Œå¬è§‰è·¯å¾„å‘é©¾é©¶å‘˜æä¾›â€œäººæ€§åŒ–â€çš„åŠè¯´æ€§å»ºè®®(persuasive advice)ï¼Œæ—¨åœ¨ä¸»åŠ¨å¼•å¯¼é©¾é©¶å‘˜ç»´æŒå¯¹è·¯å†µçš„æ³¨æ„åŠ›ã€‚å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥å·¥å…·èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºé©¾é©¶å‘˜çš„æŒç»­æ³¨æ„åŠ›ï¼Œæ˜¾è‘—é™ä½ç´§æ€¥æƒ…å†µä¸‹çš„è®¤çŸ¥è´Ÿè·(cognitive load)ï¼Œå¹¶ä¼˜åŒ–æ¬¡è¦ä»»åŠ¡ä¸æ¥ç®¡è¡Œä¸º(takeover behavior)ä¹‹é—´çš„åè°ƒã€‚è¯¥ç ”ç©¶éªŒè¯äº†åˆ©ç”¨LLMæ”¯æŒå¤šä»»åŠ¡è‡ªåŠ¨é©¾é©¶åœºæ™¯çš„æ½œåŠ›ï¼Œä¸ºæå‡äººæœºå…±é©¾çš„å®‰å…¨æ€§ä¸äº¤äº’ä½“éªŒæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",
      "pdf_url": "https://arxiv.org/pdf/2508.05238v1",
      "published_date": "2025-08-07 10:26:28 UTC",
      "updated_date": "2025-08-07 10:26:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:24.347720+00:00"
    },
    {
      "arxiv_id": "2508.05237v1",
      "title": "Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models",
      "title_zh": "æƒè¡¡ä¹‹é“ï¼šè§†è§‰-è¯­è¨€æ¨¡å‹é›¶æ ·æœ¬å¯¹æŠ—é²æ£’æ€§çš„é˜²å¾¡ç­–ç•¥ç»¼è¿°",
      "authors": [
        "Zane Xu",
        "Jason Sun"
      ],
      "abstract": "This report synthesizes eight seminal papers on the zero-shot adversarial robustness of vision-language models (VLMs) like CLIP. A central challenge in this domain is the inherent trade-off between enhancing adversarial robustness and preserving the model's zero-shot generalization capabilities. We analyze two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies model parameters, and Training-Free/Test-Time Defenses, which preserve them. We trace the evolution from alignment-preserving methods (TeCoA) to embedding space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to latent-space purification (CLIPure). Finally, we identify key challenges and future directions including hybrid defense strategies and adversarial pre-training.",
      "tldr_zh": "è¯¥æŠ¥å‘Šç»¼åˆåˆ†æäº†å…³äº CLIP ç­‰è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨é›¶æ ·æœ¬å¯¹æŠ—é²æ£’æ€§ (Zero-Shot Adversarial Robustness) æ–¹é¢çš„å…«ç¯‡æ ¸å¿ƒè®ºæ–‡ã€‚è¯¥é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæå‡å¯¹æŠ—é²æ£’æ€§ä¸ä¿æŒæ¨¡å‹é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ› (Zero-Shot Generalization) ä¹‹é—´çš„å†…åœ¨æƒè¡¡ã€‚ç ”ç©¶è¯¦ç»†å¯¹æ¯”äº†ä¿®æ”¹æ¨¡å‹å‚æ•°çš„å¯¹æŠ—å¾®è°ƒ (Adversarial Fine-Tuning, AFT) å’Œä¿ç•™å‚æ•°çš„æµ‹è¯•æ—¶é˜²å¾¡ (Test-Time Defenses) ä¸¤ç§èŒƒå¼ã€‚æ–‡ä¸­è¿½è¸ªäº†é˜²å¾¡æŠ€æœ¯ä»å¯¹é½ä¿æŒ (TeCoA) åˆ°åµŒå…¥ç©ºé—´é‡æ„ (LAAT, TIMA)ï¼Œä»¥åŠä»è¾“å…¥å¯å‘å¼ (AOM, TTC) åˆ°æ½œç©ºé—´çº¯åŒ– (CLIPure) çš„æ¼”è¿›è·¯å¾„ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æŒ‡æ˜äº†æ··åˆé˜²å¾¡ç­–ç•¥ä¸å¯¹æŠ—é¢„è®­ç»ƒ (Adversarial Pre-training) æ˜¯æœªæ¥è§£å†³ VLM å®‰å…¨æ€§é—®é¢˜çš„å…³é”®æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05237v1",
      "published_date": "2025-08-07 10:26:10 UTC",
      "updated_date": "2025-08-07 10:26:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:28.058715+00:00"
    },
    {
      "arxiv_id": "2508.05234v1",
      "title": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation",
      "title_zh": "åŸºäºé“¾å¼æ€ç»´å¢å¼ºä¸è’¸é¦çš„èµ„æºå—é™å¤šæ¨¡æ€æƒ…æ„Ÿè”åˆæ¨ç†ä¸åˆ†ç±»",
      "authors": [
        "Haonan Shangguan",
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Ge Yu"
      ],
      "abstract": "The surge in rich multimodal content on social media platforms has greatly advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs) further accelerating progress in this field. Current approaches primarily leverage the knowledge and reasoning capabilities of parameter-heavy (Multimodal) LLMs for sentiment classification, overlooking autonomous multimodal sentiment reasoning generation in resource-constrained environments. Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment Reasoning and Classification task, JMSRC, which simultaneously performs multimodal sentiment reasoning chain generation and sentiment classification only with a lightweight model. We propose a Multimodal Chain-of-Thought Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a \"Teacher-Assistant-Student\" distillation paradigm to address deployment constraints in resource-limited environments. We first leverage a high-performance Multimodal Large Language Model (MLLM) to generate the initial reasoning dataset and train a medium-sized assistant model with a multi-task learning mechanism. A lightweight student model is jointly trained to perform efficient multimodal sentiment reasoning generation and classification. Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B parameters achieves strong performance on JMSRC, while exhibiting robust generalization and enhanced interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºå—é™ç¯å¢ƒä¸‹ï¼Œç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMultimodal Sentiment Analysis, MSAï¼‰ä¸­å‚æ•°é‡è¿‡å¤§ä¸”ç¼ºä¹è‡ªä¸»æƒ…æ„Ÿæ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†èµ„æºå—é™è”åˆå¤šæ¨¡æ€æƒ…æ„Ÿæ¨ç†ä¸åˆ†ç±»ä»»åŠ¡ï¼ˆJMSRCï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º MulCoT-RD çš„å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†è’¸é¦æ¨¡å‹ï¼ˆMultimodal Chain-of-Thought Reasoning Distillationï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨â€œæ•™å¸ˆ-åŠ©æ•™-å­¦ç”Ÿâ€çš„è’¸é¦èŒƒå¼ï¼Œæ—¨åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆéƒ¨ç½²ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…åˆ©ç”¨é«˜æ€§èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆæ¨ç†æ•°æ®é›†ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æœºåˆ¶è®­ç»ƒä¸­ç­‰è§„æ¨¡çš„åŠ©æ•™æ¨¡å‹ï¼Œå¹¶æœ€ç»ˆå¼•å¯¼è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹æ‰§è¡Œä»»åŠ¡ã€‚è¯¥å­¦ç”Ÿæ¨¡å‹èƒ½å¤ŸåŒæ—¶å®Œæˆå¤šæ¨¡æ€æƒ…æ„Ÿæ¨ç†é“¾çš„ç”Ÿæˆä¸ç²¾ç¡®åˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…æ‹¥æœ‰ 3B å‚æ•°çš„ MulCoT-RD åœ¨å››ä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæ˜¾è‘—çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05234v1",
      "published_date": "2025-08-07 10:23:14 UTC",
      "updated_date": "2025-08-07 10:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:42.659593+00:00"
    },
    {
      "arxiv_id": "2508.05231v2",
      "title": "FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing",
      "title_zh": "FDC-Netï¼šé‡æ–°å®¡è§†è„‘ç”µå»ä¼ªå½±ä¸å¤šç»´æƒ…æ„Ÿè®¡ç®—çš„å…³è”",
      "authors": [
        "Wenjia Dong",
        "Xueyuan Xu",
        "Tianze Yu",
        "Junming Zhang",
        "Li Zhuo"
      ],
      "abstract": "Electroencephalogram (EEG)-based emotion recognition holds significant value in affective computing and brain-computer interfaces. However, in practical applications, EEG recordings are susceptible to the effects of various physiological artifacts. Current approaches typically treat denoising and emotion recognition as independent tasks using cascaded architectures, which not only leads to error accumulation, but also fails to exploit potential synergies between these tasks. Moreover, conventional EEG-based emotion recognition models often rely on the idealized assumption of \"perfectly denoised data\", lacking a systematic design for noise robustness. To address these challenges, a novel framework that deeply couples denoising and emotion recognition tasks is proposed for end-to-end noise-robust emotion recognition, termed as Feedback-Driven Collaborative Network for Denoising-Classification Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic collaborative mechanism between artifact removal and emotion recognition through: (1) bidirectional gradient propagation with joint optimization strategies; (2) a gated attention mechanism integrated with frequency-adaptive Transformer using learnable band-position encoding. Two most popular EEG-based emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were employed to compare the artifact removal and emotion recognition performance between FDC-Net and nine state-of-the-art methods. In terms of the denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of 96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the emotion recognition task under physiological artifact interference, FDC-Net achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on DREAMER.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè„‘ç”µå›¾ (EEG) çš„æƒ…æ„Ÿè®¡ç®—ä¸­ç”Ÿç†ä¼ªå½±å¹²æ‰°ä»¥åŠå»å™ªä¸è¯†åˆ«ä»»åŠ¡è„±èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº† FDC-Net (Feedback-Driven Collaborative Network for Denoising-Classification Nexus)ã€‚FDC-Net æ‘’å¼ƒäº†ä¼ ç»Ÿçš„ä¸²è”æ¶æ„ï¼Œé€šè¿‡æ·±åº¦è€¦åˆå»å™ªä¸æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„å™ªå£°é²æ£’æ€§è¯†åˆ«ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨åŒå‘æ¢¯åº¦ä¼ æ’­ (Bidirectional gradient propagation) ä¸è”åˆä¼˜åŒ–ç­–ç•¥å»ºç«‹åŠ¨æ€åä½œæœºåˆ¶ï¼Œå¹¶é›†æˆé—¨æ§æ³¨æ„åŠ›æœºåˆ¶ (Gated attention mechanism) ä¸é‡‡ç”¨å¯å­¦ä¹ é¢‘å¸¦ä½ç½®ç¼–ç çš„é¢‘ç‡è‡ªé€‚åº” Transformer (Frequency-adaptive Transformer)ã€‚ç ”ç©¶åœ¨ DEAP å’Œ DREAMER ä¸¤ä¸ªä¸»æµå¤šç»´æƒ…æ„Ÿæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤º FDC-Net åœ¨å»å™ªä»»åŠ¡ä¸­è·å¾—äº†æœ€é«˜ 96.30% çš„ç›¸å…³ç³»æ•° (CC)ã€‚åœ¨å—åˆ°ç”Ÿç†ä¼ªå½±å¹²æ‰°çš„æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ DEAP å’Œ DREAMER æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 82.3% å’Œ 88.1% çš„å‡†ç¡®ç‡ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å»ºç«‹å»å™ªä¸è¯†åˆ«çš„ååŒå…³è”ï¼Œæ˜¾è‘—æå‡äº†æƒ…æ„Ÿè®¡ç®—æ¨¡å‹åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æ€§èƒ½ä¸ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05231v2",
      "published_date": "2025-08-07 10:19:16 UTC",
      "updated_date": "2025-08-11 08:26:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:46.647731+00:00"
    },
    {
      "arxiv_id": "2508.05229v1",
      "title": "ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging",
      "title_zh": "ADSELï¼šé¢å‘ä¸å®Œæ•´å¤šç»´æƒ…æ„Ÿæ ‡æ³¨çš„è‡ªé€‚åº”åŒé‡è‡ªè¡¨è¾¾å­¦ä¹ è„‘ç”µç‰¹å¾é€‰æ‹©",
      "authors": [
        "Tianze Yu",
        "Junming Zhang",
        "Wenjia Dong",
        "Xueyuan Xu",
        "Li Zhuo"
      ],
      "abstract": "EEG based multi-dimension emotion recognition has attracted substantial research interest in human computer interfaces. However, the high dimensionality of EEG features, coupled with limited sample sizes, frequently leads to classifier overfitting and high computational complexity. Feature selection constitutes a critical strategy for mitigating these challenges. Most existing EEG feature selection methods assume complete multi-dimensional emotion labels. In practice, open acquisition environment, and the inherent subjectivity of emotion perception often result in incomplete label data, which can compromise model generalization. Additionally, existing feature selection methods for handling incomplete multi-dimensional labels primarily focus on correlations among various dimensions during label recovery, neglecting the correlation between samples in the label space and their interaction with various dimensions. To address these issues, we propose a novel incomplete multi-dimensional feature selection algorithm for EEG-based emotion recognition. The proposed method integrates an adaptive dual self-expression learning (ADSEL) with least squares regression. ADSEL establishes a bidirectional pathway between sample-level and dimension-level self-expression learning processes within the label space. It could facilitate the cross-sharing of learned information between these processes, enabling the simultaneous exploitation of effective information across both samples and dimensions for label reconstruction. Consequently, ADSEL could enhances label recovery accuracy and effectively identifies the optimal EEG feature subset for multi-dimensional emotion recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè„‘ç”µä¿¡å·(EEG)çš„å¤šç»´æƒ…æ„Ÿè¯†åˆ«ä¸­ç‰¹å¾ç»´åº¦é«˜ä¸”æ ‡ç­¾ä¸å®Œæ•´(incomplete multi-dimensional labels)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŒé‡è‡ªæˆ‘è¡¨è¾¾å­¦ä¹ ç®—æ³•(ADSEL)ã€‚è¯¥æ–¹æ³•å°†ADSELä¸æœ€å°äºŒä¹˜å›å½’(least squares regression)ç›¸ç»“åˆï¼Œåœ¨æ ‡ç­¾ç©ºé—´å†…æ„å»ºäº†æ ·æœ¬çº§å’Œç»´åº¦çº§è‡ªæˆ‘è¡¨è¾¾å­¦ä¹ è¿‡ç¨‹çš„åŒå‘è·¯å¾„ï¼Œå®ç°äº†å­¦ä¹ ä¿¡æ¯çš„äº¤å‰å…±äº«ã€‚è¿™ç§åŒå‘æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶åˆ©ç”¨æ ·æœ¬å’Œç»´åº¦é—´çš„æœ‰æ•ˆå…³è”è¿›è¡Œæ ‡ç­¾é‡æ„ï¼Œä»è€Œæ˜¾è‘—æå‡æ ‡ç­¾æ¢å¤çš„ç²¾ç¡®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒADSELèƒ½æœ‰æ•ˆè¯†åˆ«ç”¨äºå¤šç»´æƒ…æ„Ÿè¯†åˆ«çš„æœ€ä½³EEGç‰¹å¾å­é›†ï¼Œåœ¨å¤„ç†ä¸å®Œæ•´å¤šç»´æ ‡ç­¾æ•°æ®æ—¶å±•ç°å‡ºæ›´å¼ºçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¼˜åŒ–äººæœºäº¤äº’ä¸­çš„æƒ…æ„Ÿè®¡ç®—æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05229v1",
      "published_date": "2025-08-07 10:18:37 UTC",
      "updated_date": "2025-08-07 10:18:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:01.941691+00:00"
    },
    {
      "arxiv_id": "2508.05228v1",
      "title": "CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition",
      "title_zh": "CWEFSï¼šå—è„‘å®¹ç§¯ä¼ å¯¼æ•ˆåº”å¯å‘çš„å¤šç»´æƒ…æ„Ÿè¯†åˆ«é€šé“çº§è„‘ç”µç‰¹å¾é€‰æ‹©",
      "authors": [
        "Xueyuan Xu",
        "Wenjia Dong",
        "Fulin Wei",
        "Li Zhuo"
      ],
      "abstract": "Due to the intracranial volume conduction effects, high-dimensional multi-channel electroencephalography (EEG) features often contain substantial redundant and irrelevant information. This issue not only hinders the extraction of discriminative emotional representations but also compromises the real-time performance. Feature selection has been established as an effective approach to address the challenges while enhancing the transparency and interpretability of emotion recognition models. However, existing EEG feature selection research overlooks the influence of latent EEG feature structures on emotional label correlations and assumes uniform importance across various channels, directly limiting the precise construction of EEG feature selection models for multi-dimensional affective computing. To address these limitations, a novel channel-wise EEG feature selection (CWEFS) method is proposed for multi-dimensional emotion recognition. Specifically, inspired by brain volume conduction effects, CWEFS integrates EEG emotional feature selection into a shared latent structure model designed to construct a consensus latent space across diverse EEG channels. To preserve the local geometric structure, this consensus space is further integrated with the latent semantic analysis of multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive channel-weight learning to automatically determine the significance of different EEG channels in the emotional feature selection task. The effectiveness of CWEFS was validated using three popular EEG datasets with multi-dimensional emotional labels. Comprehensive experimental results, compared against nineteen feature selection methods, demonstrate that the EEG feature subsets chosen by CWEFS achieve optimal emotion recognition performance across six evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘å†…ä½“ä¼ å¯¼æ•ˆåº”(intracranial volume conduction effects)å¯¼è‡´çš„EEGç‰¹å¾å†—ä½™å’Œæ— å…³ä¿¡æ¯è¿‡å¤šç­‰é—®é¢˜ï¼Œæå‡ºäº†CWEFSï¼Œä¸€ç§ç”¨äºå¤šç»´æƒ…ç»ªè¯†åˆ«çš„é€šé“çº§EEGç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚CWEFSå°†ç‰¹å¾é€‰æ‹©æ•´åˆè¿›ä¸€ä¸ªå…±äº«æ½œåœ¨ç»“æ„æ¨¡å‹(shared latent structure model)ï¼Œæ—¨åœ¨ä¸åŒé€šé“é—´æ„å»ºå…±è¯†æ½œåœ¨ç©ºé—´ã€‚ä¸ºäº†ä¿ç•™å±€éƒ¨å‡ ä½•ç»“æ„ï¼Œè¯¥æ¨¡å‹è¿›ä¸€æ­¥ç»“åˆäº†å¤šç»´æƒ…ç»ªæ ‡ç­¾çš„æ½œåœ¨è¯­ä¹‰åˆ†æã€‚åŒæ—¶ï¼ŒCWEFSå¼•å…¥äº†è‡ªé€‚åº”é€šé“æƒé‡å­¦ä¹ (adaptive channel-weight learning)æœºåˆ¶ï¼Œä»¥è‡ªåŠ¨è¡¡é‡ä¸åŒEEGé€šé“åœ¨æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸­çš„æ˜¾è‘—æ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸19ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼ŒCWEFSé€‰å‡ºçš„ç‰¹å¾å­é›†åœ¨å…­ä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æœ€ä¼˜çš„æƒ…ç»ªè¯†åˆ«æ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05228v1",
      "published_date": "2025-08-07 10:17:59 UTC",
      "updated_date": "2025-08-07 10:17:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:54:53.994661+00:00"
    },
    {
      "arxiv_id": "2508.05221v1",
      "title": "ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking",
      "title_zh": "ReasoningTrackï¼šåŸºäºé“¾å¼æ€ç»´æ¨ç†çš„é•¿æ—¶è§†è§‰è¯­è¨€è·Ÿè¸ª",
      "authors": [
        "Xiao Wang",
        "Liye Jin",
        "Xufeng Lou",
        "Shiao Wang",
        "Lan Chen",
        "Bo Jiang",
        "Zhipeng Zhang"
      ],
      "abstract": "Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€è·Ÿè¸ª (vision-language tracking) æ–¹æ³•åœ¨å¤„ç†ç›®æ ‡å˜åŒ–æ—¶æ€§èƒ½å—é™ã€ç¼ºä¹æ¨ç†è¿‡ç¨‹ä¸”æœªå……åˆ†åˆ©ç”¨å¤§æ¨¡å‹ä¼˜åŠ¿çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºé“¾å¼æ€ç»´æ¨ç† (Chain-of-Thought) çš„å…¨æ–°æ¡†æ¶ ReasoningTrackã€‚è¯¥æ¡†æ¶ä»¥é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ Qwen2.5-VL ä¸ºåŸºç¡€ï¼Œåˆ©ç”¨æœ‰ç›‘ç£å¾®è°ƒ (SFT) å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³• GRPO æ¥ä¼˜åŒ–æ¨ç†è·¯å¾„ä¸è¯­è¨€ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å°†åŠ¨æ€æ›´æ–°çš„è¯­è¨€æè¿°ä¸è§†è§‰ç‰¹å¾å…±åŒåµŒå…¥ç»Ÿä¸€çš„éª¨å¹²ç½‘ç»œ (backbone)ï¼Œå¹¶é…åˆè·Ÿè¸ªå¤´ (tracking head) å®ç°ç›®æ ‡çš„ç²¾ç¡®ä½ç½®é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ä¸ªåŒ…å« 200 ä¸ªè§†é¢‘åºåˆ—çš„å¤§è§„æ¨¡é•¿æ—¶è§†è§‰è¯­è¨€è·Ÿè¸ªåŸºå‡†æ•°æ®é›† TNLLTï¼Œå¹¶å¯¹ 20 ä¸ªåŸºçº¿æ¨¡å‹è¿›è¡Œäº†é‡æ–°è®­ç»ƒä¸è¯„ä¼°ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå……åˆ†éªŒè¯äº†è¯¥æ¨ç†ç­–ç•¥åœ¨æå‡è§†è§‰è¯­è¨€è·Ÿè¸ªæ€§èƒ½æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05221v1",
      "published_date": "2025-08-07 10:02:07 UTC",
      "updated_date": "2025-08-07 10:02:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:07.090427+00:00"
    },
    {
      "arxiv_id": "2508.05210v3",
      "title": "Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction",
      "title_zh": "èåˆæ³¨æ„åŠ›æœºåˆ¶ä¸ TS Mixer çš„æ”¹è¿›å‹ Transformer-LSTM é’»äº•æœºæ¢°é’»é€Ÿé¢„æµ‹æŠ€æœ¯",
      "authors": [
        "Saddam Hussain Khan"
      ],
      "abstract": "Rate of Penetration (ROP) prediction is critical for drilling optimization yet remains challenging due to the nonlinear, dynamic, and heterogeneous characteristics of drilling data. Conventional empirical, physics-based, and standard machine learning models rely on oversimplified assumptions or intensive feature engineering, constraining their capacity to model long-term dependencies and intricate feature interactions. To address these issues, this study presents a new deep learning Hybrid LSTM-Trans-Mixer-Att framework that first processes input data through a customized Long Short-Term Memory (LSTM) network to capture multi-scale temporal dependencies aligned with drilling cycles. Subsequently, an Enhanced Transformer encoder with drilling-specific positional encodings and real-time optimization refines the features. Concurrently, a parallel Time-Series Mixer (TS-Mixer) block introduced facilitates efficient cross-feature interaction modeling of static and categorical parameters, including lithological indices and mud properties. The feature representations extracted from the Enhanced Transformer and TS-Mixer modules are integrated through a dedicated fusion layer. Finally, an adaptive attention mechanism then dynamically assigns contextual weights to salient features, enhancing discriminative representation learning and enabling high-fidelity ROP prediction. The proposed framework combines sequential memory, static feature interactions, global context learning, and dynamic feature weighting, providing a comprehensive solution for the heterogeneous and event-driven nature of drilling dynamics. Experimental validation on real-world drilling datasets demonstrates superior performance, achieving an Rsquare of 0.9991 and a MAPE of 1.447%, significantly outperforming existing baseline and hybrid models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é’»äº•æœºæ¢°é’»é€Ÿ(ROP)é¢„æµ‹ä¸­å› æ•°æ®éçº¿æ€§ã€åŠ¨æ€æ€§å’Œå¼‚æ„æ€§å¯¼è‡´çš„å»ºæ¨¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHybrid LSTM-Trans-Mixer-Attçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å®šåˆ¶çš„Long Short-Term Memory (LSTM)ç½‘ç»œæ•æ‰ä¸é’»äº•å¾ªç¯ä¸€è‡´çš„å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§ï¼Œéšåé€šè¿‡å¸¦æœ‰é’»äº•ç‰¹å®šä½ç½®ç¼–ç çš„å¢å¼ºå‹Transformerç¼–ç å™¨è¿›ä¸€æ­¥ç²¾ç‚¼ç‰¹å¾ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¼•å…¥å¹¶è¡Œçš„Time-Series Mixer (TS-Mixer)æ¨¡å—ä»¥æœ‰æ•ˆå»ºæ¨¡å²©æ€§æŒ‡æ•°å’Œæ³¥æµ†æ€§èƒ½ç­‰é™æ€åŠåˆ†ç±»å‚æ•°é—´çš„è·¨ç‰¹å¾äº¤äº’ã€‚ç»è¿‡èåˆå±‚é›†æˆåçš„ç‰¹å¾è¡¨ç¤ºç”±è‡ªé€‚åº”Attentionæœºåˆ¶åŠ¨æ€åˆ†é…ä¸Šä¸‹æ–‡æƒé‡ï¼Œæ˜¾è‘—å¢å¼ºäº†åŒºåˆ†æ€§è¡¨ç¤ºå­¦ä¹ çš„èƒ½åŠ›ä»¥å®ç°é«˜ä¿çœŸåº¦é¢„æµ‹ã€‚è¯¥æ–¹æ¡ˆæ•´åˆäº†åºåˆ—è®°å¿†ã€é™æ€ç‰¹å¾äº¤äº’ã€å…¨å±€ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒåŠ¨æ€ç‰¹å¾åŠ æƒï¼Œä¸ºå¤„ç†å¤æ‚çš„é’»äº•åŠ¨åŠ›å­¦æä¾›äº†å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®é’»äº•æ•°æ®é›†ä¸Šå–å¾—äº†0.9991çš„R-squareå’Œ1.447%çš„MAPEï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹å’Œæ··åˆæ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "31 Pages, 16 Figures, 9 Tables",
      "pdf_url": "https://arxiv.org/pdf/2508.05210v3",
      "published_date": "2025-08-07 09:45:56 UTC",
      "updated_date": "2025-11-07 18:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:10.088307+00:00"
    },
    {
      "arxiv_id": "2508.05207v1",
      "title": "SpectroStream: A Versatile Neural Codec for General Audio",
      "title_zh": "SpectroStreamï¼šé¢å‘é€šç”¨éŸ³é¢‘çš„å¤šåŠŸèƒ½ç¥ç»ç¼–è§£ç å™¨",
      "authors": [
        "Yunpeng Li",
        "Kehang Han",
        "Brian McWilliams",
        "Zalan Borsos",
        "Marco Tagliasacchi"
      ],
      "abstract": "We propose SpectroStream, a full-band multi-channel neural audio codec. Successor to the well-established SoundStream, SpectroStream extends its capability beyond 24 kHz monophonic audio and enables high-quality reconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is accomplished with a new neural architecture that leverages audio representation in the time-frequency domain, which leads to better audio quality especially at higher sample rate. The model also uses a delayed-fusion strategy to handle multi-channel audio, which is crucial in balancing per-channel acoustic quality and cross-channel phase consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SpectroStreamï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„å…¨é¢‘å¸¦å¤šé€šé“ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ (Neural Audio Codec)ï¼Œæ—¨åœ¨ä½œä¸º SoundStream çš„ç»§ä»»è€…æå‡éŸ³é¢‘å‹ç¼©æ€§èƒ½ã€‚SpectroStream å°†å¤„ç†èƒ½åŠ›ä» 24 kHz å•å£°é“æ‰©å±•è‡³ 48 kHz ç«‹ä½“å£°éŸ³ä¹ï¼Œåœ¨ 4-16 kbps çš„ä½æ¯”ç‰¹ç‡ä¸‹å®ç°äº†é«˜è´¨é‡çš„éŸ³é¢‘é‡å»ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§å…¨æ–°çš„ç¥ç»æ¶æ„ï¼Œé€šè¿‡åˆ©ç”¨æ—¶é¢‘åŸŸ (Time-Frequency Domain) çš„éŸ³é¢‘è¡¨ç¤ºï¼Œæ˜¾è‘—å¢å¼ºäº†é«˜é‡‡æ ·ç‡ä¸‹çš„éŸ³è´¨è¡¨ç°ã€‚ä¸ºäº†æœ‰æ•ˆå¤„ç†å¤šé€šé“éŸ³é¢‘ï¼Œç ”ç©¶å¼•å…¥äº†å»¶è¿Ÿèåˆ (Delayed-Fusion) ç­–ç•¥ï¼ŒæˆåŠŸå¹³è¡¡äº†å•é€šé“å£°å­¦è´¨é‡ä¸è·¨é€šé“çš„ç›¸ä½ä¸€è‡´æ€§ (Phase Consistency)ã€‚è¿™ä¸€çªç ´æ€§çš„è®¾è®¡ä½¿ SpectroStream èƒ½å¤Ÿèƒœä»»å„ç§é€šç”¨éŸ³é¢‘ä»»åŠ¡ï¼Œä¸ºå…¨é¢‘å¸¦å¤šé€šé“éŸ³é¢‘çš„é«˜æ•ˆä¼ è¾“ä¸å­˜å‚¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05207v1",
      "published_date": "2025-08-07 09:44:00 UTC",
      "updated_date": "2025-08-07 09:44:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:02.237149+00:00"
    },
    {
      "arxiv_id": "2508.05201v2",
      "title": "FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in Finance",
      "title_zh": "FAITHï¼šé‡‘èé¢†åŸŸå†…åœ¨è¡¨æ ¼å¹»è§‰è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Mengao Zhang",
        "Jiayu Fu",
        "Tanya Warrier",
        "Yuwen Wang",
        "Tianhui Tan",
        "Ke-wei Huang"
      ],
      "abstract": "Hallucination remains a critical challenge for deploying Large Language Models (LLMs) in finance. Accurate extraction and precise calculation from tabular data are essential for reliable financial analysis, since even minor numerical errors can undermine decision-making and regulatory compliance. Financial applications have unique requirements, often relying on context-dependent, numerical, and proprietary tabular data that existing hallucination benchmarks rarely capture. In this study, we develop a rigorous and scalable framework for evaluating intrinsic hallucinations in financial LLMs, conceptualized as a context-aware masked span prediction task over real-world financial documents. Our main contributions are: (1) a novel, automated dataset creation paradigm using a masking strategy; (2) a new hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a comprehensive evaluation of intrinsic hallucination patterns in state-of-the-art LLMs on financial tabular data. Our work provides a robust methodology for in-house LLM evaluation and serves as a critical step toward building more trustworthy and reliable financial Generative AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èé¢†åŸŸéƒ¨ç½²æ—¶é¢ä¸´çš„å¹»è§‰ï¼ˆHallucinationï¼‰æŒ‘æˆ˜ï¼Œå¼€å‘äº†åä¸º FAITH çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†éš¾ä»¥æ•æ‰é‡‘èç‰¹æœ‰è¡¨æ ¼æ•°æ®çš„é—®é¢˜ã€‚ç ”ç©¶è€…å°†å†…åœ¨å¹»è§‰ï¼ˆIntrinsic Hallucinationsï¼‰è¯„ä¼°ä»»åŠ¡æ¦‚å¿µåŒ–ä¸ºåœ¨çœŸå®é‡‘èæ–‡æ¡£ä¸Šæ‰§è¡Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ©ç ç‰‡æ®µé¢„æµ‹ï¼ˆContext-aware Masked Span Predictionï¼‰ã€‚è®ºæ–‡ä¸»è¦è´¡çŒ®åŒ…æ‹¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç ç­–ç•¥ï¼ˆMasking Strategyï¼‰çš„è‡ªåŠ¨åŒ–æ•°æ®é›†åˆ›å»ºèŒƒå¼ï¼Œå¹¶åˆ©ç”¨ S&P 500 å¹´åº¦æŠ¥å‘Šæ„å»ºäº†å…¨æ–°çš„è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›çš„ LLMs åœ¨é‡‘èè¡¨æ ¼æ•°æ®ä¸Šçš„è¡¨ç°è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œè¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å€¼ä¿¡æ¯æ—¶çš„å¹»è§‰æ¨¡å¼ï¼Œä¹Ÿä¸ºæ„å»ºæ›´å¯é çš„é‡‘èç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, AMC ICAIF'25",
      "pdf_url": "https://arxiv.org/pdf/2508.05201v2",
      "published_date": "2025-08-07 09:37:14 UTC",
      "updated_date": "2025-10-24 03:11:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:11.591304+00:00"
    },
    {
      "arxiv_id": "2508.05199v1",
      "title": "EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0",
      "title_zh": "EvoGraphï¼šé¢å‘è½¯ä»¶ 3.0 çš„æ··åˆæœ‰å‘å›¾æ¼”åŒ–",
      "authors": [
        "Igor Costa",
        "Christopher Baran"
      ],
      "abstract": "We introduce **EvoGraph**, a framework that enables software systems to evolve their own source code, build pipelines, documentation, and tickets. EvoGraph represents every artefact in a typed directed graph, applies learned mutation operators driven by specialized small language models (SLMs), and selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph fixes 83% of known security vulnerabilities, translates COBOL to Java with 93% functional equivalence (test verified), and maintains documentation freshness within two minutes. Experiments show a 40% latency reduction and a sevenfold drop in feature lead time compared with strong baselines. We extend our approach to **evoGraph**, leveraging language-specific SLMs for modernizing .NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96% semantic equivalence across languages while reducing computational costs by 90% compared to large language models. EvoGraph's design responds to empirical failure modes in legacy modernization, such as implicit contracts, performance preservation, and integration evolution. Our results suggest a practical path toward Software 3.0, where systems adapt continuously yet remain under measurable control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvoGraphï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä½¿è½¯ä»¶ç³»ç»Ÿè‡ªä¸»æ¼”åŒ–å…¶æºä»£ç ã€æ„å»ºæµæ°´çº¿ã€æ–‡æ¡£å’Œå·¥å•çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨è½¯ä»¶ç³»ç»Ÿå‘Software 3.0æ¼”è¿›ã€‚è¯¥æ¡†æ¶å°†æ‰€æœ‰æ„ä»¶è¡¨ç¤ºä¸ºç±»å‹åŒ–æœ‰å‘å›¾(Typed Directed Graph)ï¼Œåˆ©ç”¨ä¸“é—¨çš„å°è¯­è¨€æ¨¡å‹(SLMs)é©±åŠ¨çš„å­¦ä¹ å˜å¼‚ç®—å­è¿›è¡Œä»£ç æ“ä½œï¼Œå¹¶é€šè¿‡å¤šç›®æ ‡é€‚åº”åº¦å‡½æ•°(Multi-objective Fitness)ç­›é€‰æœ€ä¼˜æ¼”åŒ–ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒEvoGraphèƒ½ä¿®å¤83%çš„å·²çŸ¥å®‰å…¨æ¼æ´ï¼Œå¹¶åœ¨å°†COBOLè½¬æ¢ä¸ºJavaæ—¶è¾¾åˆ°93%çš„åŠŸèƒ½ç­‰ä»·ï¼ŒåŒæ—¶å°†åŠŸèƒ½äº¤ä»˜æ—¶é—´ç¼©çŸ­äº†ä¸ƒå€ã€‚é€šè¿‡åœ¨.NETã€Lispã€Cç­‰å¤šç§ legacy ä»£ç åº“ä¸Šçš„æ‰©å±•åº”ç”¨ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è¯­ä¹‰ç­‰ä»·æ€§çš„åŒæ—¶ï¼Œè®¡ç®—æˆæœ¬è¾ƒå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é™ä½äº†90%ã€‚EvoGraphé’ˆå¯¹é—ç•™ç³»ç»Ÿç°ä»£åŒ–ä¸­çš„éšå¼å¥‘çº¦å’Œé›†æˆæ¼”åŒ–ç­‰æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºæ„å»ºå¯æŒç»­è‡ªé€‚åº”ä¸”å¤„äºæµ‹é‡æ§åˆ¶ä¸‹çš„è½¯ä»¶ç³»ç»Ÿæä¾›äº†å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "15 pages, 3 tables, 1 algorithm. Submitted to ICSE 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05199v1",
      "published_date": "2025-08-07 09:36:30 UTC",
      "updated_date": "2025-08-07 09:36:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:18.796077+00:00"
    },
    {
      "arxiv_id": "2508.09178v2",
      "title": "IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection",
      "title_zh": "IAD-R1ï¼šå¼ºåŒ–å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­çš„ä¸€è‡´æ€§æ¨ç†",
      "authors": [
        "Yanhui Li",
        "Yunkang Cao",
        "Chengliang Liu",
        "Yuan Xiong",
        "Xinghui Dong",
        "Chao Huang"
      ],
      "abstract": "Industrial anomaly detection is a critical component of modern manufacturing, yet the scarcity of defective samples restricts traditional detection methods to scenario-specific applications. Although Vision-Language Models (VLMs) demonstrate significant advantages in generalization capabilities, their performance in industrial anomaly detection remains limited. To address this challenge, we propose IAD-R1, a universal post-training framework applicable to VLMs of different architectures and parameter scales, which substantially enhances their anomaly detection capabilities. IAD-R1 employs a two-stage training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT) stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset (Expert-AD) for training, enhancing anomaly perception capabilities and establishing reasoning-to-answer correlations; the Structured Control Group Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward functions to achieve a capability leap from \"Anomaly Perception\" to \"Anomaly Interpretation\". Experimental results demonstrate that IAD-R1 achieves significant improvements across 7 VLMs, the largest improvement was on the DAGM dataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably, the 0.5B parameter model trained with IAD-R1 surpasses commercial models including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the effectiveness and superiority of IAD-R1. The dataset, code, and all model weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IAD-R1ï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºä¸åŒæ¶æ„å’Œå‚æ•°è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) çš„é€šç”¨åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­ç”±äºç¼ºé™·æ ·æœ¬ç¨€ç¼ºå¯¼è‡´çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™é—®é¢˜ã€‚IAD-R1 é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…¶ä¸­æ„ŸçŸ¥æ¿€æ´»ç›‘ç£å¾®è°ƒ (PA-SFT) é˜¶æ®µåˆ©ç”¨é«˜è´¨é‡çš„ Chain-of-Thought æ•°æ®é›† Expert-AD å»ºç«‹äº†æ¨ç†ä¸ç­”æ¡ˆé—´çš„å…³è”ï¼Œè€Œç»“æ„åŒ–æ§åˆ¶ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (SC-GRPO) é˜¶æ®µåˆ™é€šè¿‡å¥–åŠ±å‡½æ•°å¼•å¯¼æ¨¡å‹å®ç°ä»å¼‚å¸¸æ„ŸçŸ¥åˆ°å¼‚å¸¸è§£é‡Šçš„èƒ½åŠ›è·¨è¶Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIAD-R1 åœ¨ 7 ä¸ªä¸åŒè§„æ¨¡çš„ VLMs ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ DAGM æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚ä»¤äººå…³æ³¨çš„æ˜¯ï¼Œä»…æœ‰ 0.5B å‚æ•°çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬ (zero-shot) è®¾ç½®ä¸‹æ€§èƒ½ä¾¿è¶…è¶Šäº† GPT-4.1 å’Œ Claude-Sonnet-4 ç­‰å…ˆè¿›å•†ç”¨æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ºå·¥ä¸šé¢†åŸŸçš„æ™ºèƒ½å¼‚å¸¸æ£€æµ‹æä¾›äº†ä¸€ç§æå…·ç«äº‰åŠ›çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09178v2",
      "published_date": "2025-08-07 09:34:45 UTC",
      "updated_date": "2025-08-14 15:30:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:25.794877+00:00"
    },
    {
      "arxiv_id": "2508.05198v1",
      "title": "Balancing Accuracy and Novelty with Sub-Item Popularity",
      "title_zh": "åˆ©ç”¨å­é¡¹æµè¡Œåº¦å¹³è¡¡å‡†ç¡®æ€§ä¸æ–°é¢–æ€§",
      "authors": [
        "Chiara Mallamaci",
        "Aleksandr Vladimirovich Petrov",
        "Alberto Carlo Maria Mancino",
        "Vito Walter Anelli",
        "Tommaso Di Noia",
        "Craig Macdonald"
      ],
      "abstract": "In the realm of music recommendation, sequential recommenders have shown promise in capturing the dynamic nature of music consumption. A key characteristic of this domain is repetitive listening, where users frequently replay familiar tracks. To capture these repetition patterns, recent research has introduced Personalised Popularity Scores (PPS), which quantify user-specific preferences based on historical frequency. While PPS enhances relevance in recommendation, it often reinforces already-known content, limiting the system's ability to surface novel or serendipitous items - key elements for fostering long-term user engagement and satisfaction. To address this limitation, we build upon RecJPQ, a Transformer-based framework initially developed to improve scalability in large-item catalogues through sub-item decomposition. We repurpose RecJPQ's sub-item architecture to model personalised popularity at a finer granularity. This allows us to capture shared repetition patterns across sub-embeddings - latent structures not accessible through item-level popularity alone. We propose a novel integration of sub-ID-level personalised popularity within the RecJPQ framework, enabling explicit control over the trade-off between accuracy and personalised novelty. Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by achieving significantly higher personalised novelty without compromising recommendation accuracy. Code and experiments are publicly available at https://github.com/sisinflab/Sub-id-Popularity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹æ¨èä¸­å‡†ç¡®æ€§ä¸æ–°é¢–æ€§ï¼ˆNoveltyï¼‰çš„å¹³è¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå­é¡¹ç›®æµè¡Œåº¦çš„æ–°å‹å»ºæ¨¡æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿ Personalised Popularity Scores (PPS) å®¹æ˜“å¯¼è‡´æ¨èå†…å®¹è¿‡åº¦é‡å¤ä¸”ç¼ºä¹æ–°é¢–æ€§çš„å±€é™ï¼Œè¯¥ç ”ç©¶åŸºäº Transformer æ¶æ„çš„ RecJPQ æ¡†æ¶è¿›è¡Œäº†æ”¹è¿›ã€‚é€šè¿‡åˆ©ç”¨ RecJPQ çš„ sub-item åˆ†è§£æ¶æ„åœ¨æ›´ç»†ç²’åº¦çš„ sub-ID-level å»ºæ¨¡ä¸ªæ€§åŒ–æµè¡Œåº¦ï¼Œä»è€Œæ•æ‰ sub-embeddings ä¸­éšè—çš„å…±äº«é‡å¤æ¨¡å¼ã€‚è¯¥ç ”ç©¶æå‡ºçš„ sPPS æ–¹æ³•å®ç°äº†å¯¹å‡†ç¡®æ€§ä¸ä¸ªæ€§åŒ–æ–°é¢–æ€§ä¹‹é—´æƒè¡¡çš„æ˜¾å¼æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒsPPS åœ¨ä¿æŒæ¨èå‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ item-level PPSï¼Œåœ¨æå‡ç³»ç»Ÿä¸ªæ€§åŒ–æ–°é¢–æ€§æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05198v1",
      "published_date": "2025-08-07 09:33:32 UTC",
      "updated_date": "2025-08-07 09:33:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:42.485943+00:00"
    },
    {
      "arxiv_id": "2508.05197v1",
      "title": "QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering",
      "title_zh": "QA-Dragonï¼šé¢å‘çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”çš„æŸ¥è¯¢æ„ŸçŸ¥åŠ¨æ€ RAG ç³»ç»Ÿ",
      "authors": [
        "Zhuohang Jiang",
        "Pangjing Wu",
        "Xu Yuan",
        "Wenqi Fan",
        "Qing Li"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA). However, existing RAG methods typically retrieve from either text or images in isolation, limiting their ability to address complex queries that require multi-hop reasoning or up-to-date factual knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's subject domain for domain-specific reasoning, along with a search router that dynamically selects optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup, our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025, where it significantly enhances the reasoning performance of base models under challenging scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the multi-source task, and 5.03% on the multi-turn task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QA-Dragonï¼Œä¸€ç§é’ˆå¯¹çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ï¼ˆKnowledge-Intensive VQAï¼‰è®¾è®¡çš„æŸ¥è¯¢æ„ŸçŸ¥åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆQuery-Aware Dynamic RAGï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´çš„å¹»è§‰åŠå¤šè·³æ¨ç†èƒ½åŠ›ä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†ä¸€ä¸ªé¢†åŸŸè·¯ç”±ï¼ˆdomain routerï¼‰ç”¨äºè¯†åˆ«æŸ¥è¯¢çš„ä¸»é¢˜é¢†åŸŸå¹¶è¿›è¡Œç‰¹å®šé¢†åŸŸçš„æ¨ç†ï¼ŒåŒæ—¶ç»“åˆæœç´¢è·¯ç”±ï¼ˆsearch routerï¼‰åŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„æ£€ç´¢ç­–ç•¥ã€‚é€šè¿‡åœ¨æ··åˆæ¶æ„ä¸­åè°ƒæ–‡æœ¬å’Œå›¾åƒæœç´¢æ™ºèƒ½ä½“ï¼ŒQA-Dragon å®ç°äº†å¤šæ¨¡æ€ã€å¤šè½®åŠå¤šè·³æ¨ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å„ç§å¤æ‚çš„ VQA ä»»åŠ¡ã€‚å®éªŒåœ¨ KDD Cup 2025 çš„ Meta CRAG-MM æŒ‘æˆ˜èµ›ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨å•æºã€å¤šæºåŠå¤šè½®ä»»åŠ¡ä¸­åˆ†åˆ«æ¯”åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡äº† 5.06%ã€6.35% å’Œ 5.03%ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† QA-Dragon åœ¨æå‡ç­”æ¡ˆå‡†ç¡®æ€§å’ŒçŸ¥è¯†é‡å å¾—åˆ†æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå¤„ç†æå…·æŒ‘æˆ˜æ€§çš„çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”åœºæ™¯æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "The source code for our system is released in https://github.com/jzzzzh/QA-Dragon",
      "pdf_url": "https://arxiv.org/pdf/2508.05197v1",
      "published_date": "2025-08-07 09:32:49 UTC",
      "updated_date": "2025-08-07 09:32:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:55.385061+00:00"
    },
    {
      "arxiv_id": "2508.05188v1",
      "title": "Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination",
      "title_zh": "åŸºäºä½å¹»è§‰è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹çš„äº‹ä»¶å“åº”è§„åˆ’",
      "authors": [
        "Kim Hammar",
        "Tansu Alpcan",
        "Emil C. Lupu"
      ],
      "abstract": "Timely and effective incident response is key to managing the growing frequency of cyberattacks. However, identifying the right response actions for complex systems is a major technical challenge. A promising approach to mitigate this challenge is to use the security knowledge embedded in large language models (LLMs) to assist security operators during incident handling. Recent research has demonstrated the potential of this approach, but current methods are mainly based on prompt engineering of frontier LLMs, which is costly and prone to hallucinations. We address these limitations by presenting a novel way to use an LLM for incident response planning with reduced hallucination. Our method includes three steps: fine-tuning, information retrieval, and lookahead planning. We prove that our method generates response plans with a bounded probability of hallucination and that this probability can be made arbitrarily small at the expense of increased planning time under certain assumptions. Moreover, we show that our method is lightweight and can run on commodity hardware. We evaluate our method on logs from incidents reported in the literature. The experimental results show that our method a) achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes to a broad range of incident types and response actions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº‹ä»¶å“åº”è§„åˆ’ï¼ˆIncident Response Planningï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å‰æ²¿æ¨¡å‹æˆæœ¬é«˜æ˜‚ä¸”æ˜“äº§ç”Ÿå¹»è§‰ï¼ˆHallucinationï¼‰çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¾®è°ƒï¼ˆFine-tuningï¼‰ã€ä¿¡æ¯æ£€ç´¢ï¼ˆInformation Retrievalï¼‰å’Œå‰ç»æ€§è§„åˆ’ï¼ˆLookahead Planningï¼‰ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼Œå¹¶ä»æ•°å­¦ä¸Šè¯æ˜äº†å…¶ç”Ÿæˆå“åº”è®¡åˆ’çš„å¹»è§‰æ¦‚ç‡æ˜¯å¯ç•Œå®šçš„ã€‚è¯¥æ–¹æ¡ˆå…·æœ‰æ˜¾è‘—çš„è½»é‡åŒ–ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨æ™®é€šç¡¬ä»¶ä¸Šè¿è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†äº‹æ•…æ—¥å¿—æ—¶æ¯”å‰æ²¿ LLMs çš„æ¢å¤æ—¶é—´ç¼©çŸ­äº†å¤šè¾¾ 22%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–åˆ°å¤šç§äº‹æ•…ç±»å‹å’Œå“åº”åŠ¨ä½œä¸­ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05188v1",
      "published_date": "2025-08-07 09:23:25 UTC",
      "updated_date": "2025-08-07 09:23:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:55:58.696793+00:00"
    },
    {
      "arxiv_id": "2508.05187v1",
      "title": "Refining Gaussian Splatting: A Volumetric Densification Approach",
      "title_zh": "ä¼˜åŒ–é«˜æ–¯æ³¼æº…ï¼šä¸€ç§åŸºäºä½“ç§¯çš„ç¨ å¯†åŒ–æ–¹æ³•",
      "authors": [
        "Mohamed Abdul Gafoor",
        "Marius Preda",
        "Titus Zaharia"
      ],
      "abstract": "Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian Splatting (3DGS) ä¸­åŸç”Ÿçš„ Adaptive Density Control (ADC) æœºåˆ¶åœ¨ç‚¹äº‘åŠ å¯†ç­–ç•¥ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¯†åº¦æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°åˆ©ç”¨ä¸æ¯ä¸ª Gaussian å‡½æ•°ç›¸å…³çš„æƒ¯æ€§ä½“ç§¯ (volumes of inertia) æ¥æŒ‡å¯¼ç²¾ç»†åŒ–è¿‡ç¨‹ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„ç‚¹äº‘ç®¡ç†ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å¯¹æ¯”åˆ†æäº†ä¼ ç»Ÿ Structure from Motion (SfM) å’Œ Deep Image Matching (DIM) åˆå§‹åŒ–æ–¹æ³•å¯¹ç‚¹äº‘é‡å»ºæ•ˆæœçš„å½±å“ã€‚åœ¨ Mip-NeRF 360 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹ 3DGSï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„åœºæ™¯ä¸­æä¾›æ›´å…·ä¼˜åŠ¿çš„æ¸²æŸ“è¡¨ç°ã€‚è¿™ä¸€æˆæœä¸ºæå‡ä¸‰ç»´åœºæ™¯é‡å»ºçš„ç²¾åº¦å’Œæ•ˆç‡æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05187v1",
      "published_date": "2025-08-07 09:23:17 UTC",
      "updated_date": "2025-08-07 09:23:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:00.893312+00:00"
    },
    {
      "arxiv_id": "2508.05170v2",
      "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation",
      "title_zh": "Posterior-GRPOï¼šä»£ç ç”Ÿæˆä¸­çš„æ¨ç†è¿‡ç¨‹å¥–åŠ±",
      "authors": [
        "Lishui Fan",
        "Yu Zhang",
        "Mouxiang Chen",
        "Zhongxin Liu"
      ],
      "abstract": "Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç ç”Ÿæˆä¸­å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿‡åº¦ä¾èµ–ç»“æœå¥–åŠ±(Outcome-based rewards)è€Œå¿½è§†ä¸­é—´æ¨ç†è¿‡ç¨‹è´¨é‡çš„é—®é¢˜ï¼Œæå‡ºäº†Posterior-GRPO (P-GRPO)æ¡†æ¶ã€‚ä½œè€…é¦–å…ˆå¼€å‘äº†åŒ…å«ä¼˜åŠ£æ¨ç†åå¥½å¯¹çš„åŸºå‡†æµ‹è¯•LCB-RBï¼Œå¹¶é‡‡ç”¨ä¸€ç§åŸºäºä¼˜åŒ–-é€€åŒ–(Optimized-Degraded based)çš„æ–¹æ³•è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡åœ¨äº‹å®å‡†ç¡®æ€§ã€é€»è¾‘ä¸¥å¯†æ€§å’Œè¿è´¯æ€§ç­‰å¤šç»´åº¦å¤„ç†æ¨ç†è·¯å¾„æ¥ç”Ÿæˆé«˜è´¨é‡æ•°æ®ã€‚æ ¸å¿ƒç®—æ³•P-GRPOåˆ›æ–°æ€§åœ°å°†è¿‡ç¨‹å¥–åŠ±(Process-based rewards)ä¸ä»»åŠ¡æˆåŠŸçŠ¶æ€æŒ‚é’©ï¼Œä»…å¯¹äº§ç”Ÿæ­£ç¡®ç»“æœçš„æ¨ç†è¿‡ç¨‹ç»™äºˆå¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£äº†å¥–åŠ±æ¬ºéª—(Reward hacking)ç°è±¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨P-GRPOçš„7Bå‚æ•°æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­æ¯”ä»…åŸºäºç»“æœçš„åŸºçº¿æ¨¡å‹è¡¨ç°æå‡äº†4.5%ï¼Œè¾¾åˆ°äº†ä¸GPT-4-Turboç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹é€»è¾‘æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05170v2",
      "published_date": "2025-08-07 09:04:10 UTC",
      "updated_date": "2025-09-17 10:56:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:48.605996+00:00"
    },
    {
      "arxiv_id": "2508.11670v1",
      "title": "RRRA: Resampling and Reranking through a Retriever Adapter",
      "title_zh": "RRRAï¼šåŸºäºæ£€ç´¢å™¨é€‚é…å™¨çš„é‡é‡‡æ ·ä¸é‡æ’åº",
      "authors": [
        "Bongsu Kim"
      ],
      "abstract": "In dense retrieval, effective training hinges on selecting high quality hard negatives while avoiding false negatives. Recent methods apply heuristics based on positive document scores to identify hard negatives, improving both performance and interpretability. However, these global, example agnostic strategies often miss instance specific false negatives. To address this, we propose a learnable adapter module that monitors Bi-Encoder representations to estimate the likelihood that a hard negative is actually a false negative. This probability is modeled dynamically and contextually, enabling fine-grained, query specific judgments. The predicted scores are used in two downstream components: (1) resampling, where negatives are reweighted during training, and (2) reranking, where top-k retrieved documents are reordered at inference. Empirical results on standard benchmarks show that our adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines, underscoring the benefit of explicit false negative modeling in dense retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RRRAæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„é€‚é…å™¨æ¨¡å—(Adapter)æ¥ä¼˜åŒ–ç¨ å¯†æ£€ç´¢(Dense Retrieval)ä¸­çš„ç¡¬è´Ÿæ ·æœ¬(Hard Negatives)é€‰æ‹©è¿‡ç¨‹ã€‚RRRAæ—¨åœ¨è§£å†³ç°æœ‰å¯å‘å¼ç­–ç•¥éš¾ä»¥è¯†åˆ«ç‰¹å®šå®ä¾‹ä¼ªè´Ÿæ ·æœ¬(False Negatives)çš„é—®é¢˜ï¼Œé€šè¿‡ç›‘æ§Bi-Encoderè¡¨ç¤ºåŠ¨æ€ä¼°è®¡å€™é€‰è´Ÿæ ·æœ¬æˆä¸ºä¼ªè´Ÿæ ·æœ¬çš„æ¦‚ç‡ã€‚è¯¥æ¡†æ¶å°†é¢„æµ‹åˆ†æ•°åº”ç”¨äºä¸¤ä¸ªä¸‹æ¸¸ç¯èŠ‚ï¼šä¸€æ˜¯åœ¨è®­ç»ƒé˜¶æ®µé€šè¿‡é‡é‡‡æ ·(Resampling)å¯¹è´Ÿæ ·æœ¬è¿›è¡Œé‡åŠ æƒï¼ŒäºŒæ˜¯åœ¨æ¨ç†é˜¶æ®µå¯¹æ£€ç´¢åˆ°çš„Top-kæ–‡æ¡£è¿›è¡Œé‡æ’åº(Reranking)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRRAåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºå¼ºåŠ›Bi-EncoderåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åœ¨ç¨ å¯†æ£€ç´¢ä¸­è¿›è¡Œæ˜¾å¼ä¼ªè´Ÿæ ·æœ¬å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§ç»†ç²’åº¦ä¸”æŸ¥è¯¢ç‰¹å®šçš„åˆ¤æ–­æœºåˆ¶èƒ½å¤Ÿæ•æ‰å®ä¾‹çº§åˆ«çš„æ ·æœ¬ç‰¹å¾ï¼Œä¸ºæå‡æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ä¸å¯é æ€§æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages, 4 figures, submitted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.11670v1",
      "published_date": "2025-08-07 08:59:57 UTC",
      "updated_date": "2025-08-07 08:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:05.487144+00:00"
    },
    {
      "arxiv_id": "2508.06572v2",
      "title": "Teaching Introduction to Programming in the times of AI: A case study of a course re-design",
      "title_zh": "AIæ—¶ä»£ä¸‹çš„ç¨‹åºè®¾è®¡å¯¼è®ºæ•™å­¦ï¼šä¸€é¡¹è¯¾ç¨‹é‡æ„çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Nikolaos Avouris",
        "Kyriakos Sgarbas",
        "George Caridakis",
        "Christos Sintoris"
      ],
      "abstract": "The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨AIæ—¶ä»£èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•é‡æ–°è®¾è®¡ç¼–ç¨‹å…¥é—¨è¯¾ç¨‹ä»¥åº”å¯¹AIå·¥å…·(AI tools)ç»™æ•™å­¦å¸¦æ¥çš„å˜é©ä¸æŒ‘æˆ˜ã€‚è®ºæ–‡ç»¼è¿°äº†é€‚ç”¨äºç¼–ç¨‹æ•™å­¦çš„å‰æ²¿æŠ€æœ¯ï¼Œå¹¶é‡ç‚¹åˆ†æäº†è¿™äº›å·¥å…·åœ¨è¯¾ç¨‹è®¾è®¡(Course design)ã€å­¦ä¹ ç›®æ ‡(Learning objectives)åŠå½¢æˆæ€§å’Œæ€»ç»“æ€§è¯„ä¼°(Formative and summative assessment)æ–¹é¢å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹å­¦ç”Ÿæ»¥ç”¨å·¥å…·ç­‰é£é™©ï¼Œè¯¥ç ”ç©¶é€šè¿‡é‡æ„ä½œä¸šå½¢å¼å’Œæ•™å­¦æ³•(Pedagogy)æå‡ºäº†ä¸€å¥—è¯¾ç¨‹æ”¹è¿›æ–¹æ¡ˆã€‚ä½œä¸ºä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æˆæœä¸ºæ•™è‚²æœºæ„å’Œæ•™å¸ˆæä¾›äº†å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„æ”¿ç­–å»ºè®®ï¼Œæ—¨åœ¨æœ€å¤§åŒ–åˆ©ç”¨AIå·¥å…·çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ç³»ç»Ÿæ€§åœ°è§£å†³ç›¸å…³çš„æ•™å­¦éš¾é¢˜ä¸æ‹…å¿§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "To be cited as: Avouris, N., Sgarbas, K., Caridakis, G., Sintoris, C., (2025). Teaching Introduction to Programming in the times of AI: A case study of a course re-design, Proceedings 12th Penhellenic Conference of Computer Science Education, PCCSE 2025, Rhodes, October 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.06572v2",
      "published_date": "2025-08-07 08:56:19 UTC",
      "updated_date": "2025-08-18 07:37:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:13.941996+00:00"
    },
    {
      "arxiv_id": "2508.05165v1",
      "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models",
      "title_zh": "ä½æˆæœ¬å¤§è¯­è¨€æ¨¡å‹å¯¹é½ï¼šåŸºäºå¯å‘å¼å¥–åŠ±æ¨¡å‹çš„æ¨ç†æ—¶å¯¹é½",
      "authors": [
        "Mason Nakamura",
        "Saaduddin Mahmud",
        "Kyle H. Wray",
        "Hamed Zamani",
        "Shlomo Zilberstein"
      ],
      "abstract": "Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost. Existing inference-time methods typically ignore this balance, focusing solely on the optimized policy's performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while preserving alignment quality. On real-world prompt datasets, HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. We also find that HIA is effective under low-inference budgets with as little as one or two response queries, offering a practical solution for scalable, personalized LLM deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HIA (Heuristic-Guided Inference-time Alignment)ï¼Œä¸€ç§æ— éœ€å¾®è°ƒä¸”å…¼å®¹é»‘ç›’æ¨¡å‹çš„æ¨ç†ä¾§å¯¹é½æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯¹é½è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆè½»é‡çº§ Prompt Optimizerã€å¯å‘å¼å¥–åŠ±æ¨¡å‹(Heuristic Reward Models)å’Œä¸¤é˜¶æ®µè¿‡æ»¤æŠ€æœ¯ï¼Œåœ¨ä¿æŒå¯¹é½è´¨é‡çš„åŒæ—¶æœ‰æ•ˆå‡å°‘äº†æ¨ç†è°ƒç”¨æ¬¡æ•°ã€‚åœ¨ HelpSteer å’Œ ComPRed çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒæ¨ç†é¢„ç®—ä¸‹ï¼ŒHIA åœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äº Best-of-N Samplingã€Beam Search å’Œ Greedy Search ç­‰ä¼ ç»ŸåŸºçº¿æ–¹æ¡ˆã€‚ç ”ç©¶è¿˜è¯å®äº† HIA åœ¨æä½æ¨ç†é¢„ç®—åœºæ™¯ï¼ˆå¦‚ä»…éœ€ 1-2 æ¬¡å“åº”æŸ¥è¯¢ï¼‰ä¸‹ä¾ç„¶ä¿æŒé«˜åº¦æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°å¯æ‰©å±•ä¸”ä¸ªæ€§åŒ–çš„ LLM éƒ¨ç½²æä¾›äº†ä¸€ç§æå…·å®è·µæ„ä¹‰çš„ä½æˆæœ¬è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05165v1",
      "published_date": "2025-08-07 08:54:27 UTC",
      "updated_date": "2025-08-07 08:54:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:16.637530+00:00"
    },
    {
      "arxiv_id": "2508.05154v1",
      "title": "Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation",
      "title_zh": "å¼ºåŒ–å­¦ä¹ é¢†åŸŸé©±åŠ¨è¯„ä»·æŒ‡æ ‡ï¼šåŸºäºæ™ºèƒ½ä½“ä»¿çœŸçš„æµè¡Œç—…é˜²æ§æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Rishabh Gaur",
        "Gaurav Deshkar",
        "Jayanta Kshirsagar",
        "Harshal Hayatnagarkar",
        "Janani Venugopalan"
      ],
      "abstract": "For the development and optimization of agent-based models (ABMs) and rational agent-based models (RABMs), optimization algorithms such as reinforcement learning are extensively used. However, assessing the performance of RL-based ABMs and RABMS models is challenging due to the complexity and stochasticity of the modeled systems, and the lack of well-standardized metrics for comparing RL algorithms. In this study, we are developing domain-driven metrics for RL, while building on state-of-the-art metrics. We demonstrate our ``Domain-driven-RL-metrics'' using policy optimization on a rational ABM disease modeling case study to model masking behavior, vaccination, and lockdown in a pandemic. Our results show the use of domain-driven rewards in conjunction with traditional and state-of-the-art metrics for a few different simulation scenarios such as the differential availability of masks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ™ºèƒ½ä½“æ¨¡å‹(ABMs)å’Œç†æ€§æ™ºèƒ½ä½“æ¨¡å‹(RABMs)åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)è¿›è¡Œä¼˜åŒ–æ—¶é¢ä¸´çš„æ€§èƒ½è¯„ä¼°éš¾é¢˜ï¼Œå¼€å‘äº†ä¸€å¥—é¢†åŸŸé©±åŠ¨æŒ‡æ ‡(Domain-driven-RL-metrics)ã€‚é’ˆå¯¹æ¨¡æ‹Ÿç³»ç»Ÿå…·æœ‰é«˜åº¦å¤æ‚æ€§å’Œéšæœºæ€§ä¸”ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ ‡å‡†çš„ç°çŠ¶ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰æœ€å…ˆè¿›æŒ‡æ ‡çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ã€‚ç ”ç©¶é€šè¿‡ä¸€ä¸ªç–«æƒ…æ§åˆ¶çš„ç†æ€§ABMæ¡ˆä¾‹ï¼Œå¯¹å£ç½©ä½©æˆ´ã€ç–«è‹—æ¥ç§åŠå°é”ç­‰è¡Œä¸ºè¿›è¡Œç­–ç•¥ä¼˜åŒ–(Policy Optimization)ä»¥éªŒè¯è¯¥æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å£ç½©ä¾›åº”å·®å¼‚ç­‰å¤šç§æ¨¡æ‹Ÿåœºæ™¯ä¸‹ï¼Œå°†é¢†åŸŸé©±åŠ¨å¥–åŠ±(Domain-driven Rewards)ä¸ä¼ ç»ŸæŒ‡æ ‡ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¡¡é‡æ¨¡å‹è¡¨ç°ï¼Œä¸ºå¤æ‚ç³»ç»Ÿä¸‹çš„RLç®—æ³•æ¯”è¾ƒæä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05154v1",
      "published_date": "2025-08-07 08:40:19 UTC",
      "updated_date": "2025-08-07 08:40:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:18.988726+00:00"
    },
    {
      "arxiv_id": "2508.05153v1",
      "title": "FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction",
      "title_zh": "FCBV-Netï¼šåŸºäºç‰¹å¾æ¡ä»¶åŒæ‰‹ä»·å€¼é¢„æµ‹çš„ç±»åˆ«çº§æœºå™¨äººè¡£ç‰©å¹³æ•´",
      "authors": [
        "Mohammed Daba",
        "Jing Qiu"
      ],
      "abstract": "Category-level generalization for robotic garment manipulation, such as bimanual smoothing, remains a significant hurdle due to high dimensionality, complex dynamics, and intra-category variations. Current approaches often struggle, either overfitting with concurrently learned visual features for a specific instance or, despite category-level perceptual generalization, failing to predict the value of synergistic bimanual actions. We propose the Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point clouds to specifically enhance category-level policy generalization for garment smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained, frozen dense geometric features, ensuring robustness to intra-category garment variations. Trainable downstream components then learn a task-specific policy using these static features. In simulated GarmentLab experiments with the CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization. It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments compared to 96.2% for a 2D image-based baseline, and achieved 89% final coverage, outperforming an 83% coverage from a 3D correspondence-based baseline that uses identical per-point geometric features but a fixed primitive. These results highlight that the decoupling of geometric understanding from bimanual action value learning enables better category-level generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººè¡£ç‰©æ“çºµï¼ˆå¦‚åŒæ‰‹å¹³æ•´è¡£ç‰©ï¼‰ä¸­ç”±äºé«˜ç»´åº¦ã€å¤æ‚åŠ¨åŠ›å­¦åŠç±»åˆ«å†…å·®å¼‚å¯¼è‡´çš„æ³›åŒ–éš¾é¢˜ï¼Œæå‡ºäº†FCBV-Netï¼ˆFeature-Conditioned Bimanual Value Networkï¼‰ã€‚è¯¥æ¡†æ¶åŸºäº3D point cloudsï¼Œé€šè¿‡å°†åŒè½´åŠ¨ä½œä»·å€¼é¢„æµ‹ï¼ˆbimanual action value predictionï¼‰å»ºç«‹åœ¨é¢„è®­ç»ƒä¸”å†»ç»“çš„å¯†é›†å‡ ä½•ç‰¹å¾ä¹‹ä¸Šï¼Œå®ç°äº†å‡ ä½•ç†è§£ä¸åŠ¨ä½œä»·å€¼å­¦ä¹ çš„æœ‰æ•ˆè§£è€¦ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨é™æ€å‡ ä½•ç‰¹å¾ç¡®ä¿äº†å¯¹ç±»åˆ«å†…å˜ä½“çš„é²æ£’æ€§ï¼Œå¹¶ç”±ä¸‹æ¸¸ç»„ä»¶å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ã€‚åœ¨åŸºäºCLOTH3Dæ•°æ®é›†çš„GarmentLabä»¿çœŸå®éªŒä¸­ï¼ŒFCBV-Netå±•ç°äº†å“è¶Šçš„ç±»åˆ«çº§æ³›åŒ–æ€§èƒ½ï¼Œåœ¨æœªè§è¡£ç‰©ä¸Šçš„æ•ˆç‡ä¸‹é™ä»…ä¸º11.5%ï¼Œä¸”æœ€ç»ˆè¦†ç›–ç‡è¾¾åˆ°89%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§£è€¦å‡ ä½•æ„ŸçŸ¥ä¸åŠ¨ä½œä»·å€¼é¢„æµ‹æ˜¯æå‡æœºå™¨äººè¡£ç‰©æ“çºµæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 3 figures, 1 table. Submitted to IEEE Robotics and Automation Letters",
      "pdf_url": "https://arxiv.org/pdf/2508.05153v1",
      "published_date": "2025-08-07 08:37:45 UTC",
      "updated_date": "2025-08-07 08:37:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:56:17.733301+00:00"
    },
    {
      "arxiv_id": "2508.05152v1",
      "title": "Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models",
      "title_zh": "Tool Graph Retrieverï¼šæ¢ç´¢é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„åŸºäºä¾èµ–å›¾çš„å·¥å…·æ£€ç´¢",
      "authors": [
        "Linfeng Gao",
        "Yaoxiang Wang",
        "Minlong Peng",
        "Jialong Tang",
        "Yuzhe Shang",
        "Mingming Sun",
        "Jinsong Su"
      ],
      "abstract": "With the remarkable advancement of AI agents, the number of their equipped tools is increasing rapidly. However, integrating all tool information into the limited model context becomes impractical, highlighting the need for efficient tool retrieval methods. In this regard, dominant methods primarily rely on semantic similarities between tool descriptions and user queries to retrieve relevant tools. However, they often consider each tool independently, overlooking dependencies between tools, which may lead to the omission of prerequisite tools for successful task execution. To deal with this defect, in this paper, we propose Tool Graph Retriever (TGR), which exploits the dependencies among tools to learn better tool representations for retrieval. First, we construct a dataset termed TDI300K to train a discriminator for identifying tool dependencies. Then, we represent all candidate tools as a tool dependency graph and use graph convolution to integrate the dependencies into their representations. Finally, these updated tool representations are employed for online retrieval. Experimental results on several commonly used datasets show that our TGR can bring a performance improvement to existing dominant methods, achieving SOTA performance. Moreover, in-depth analyses also verify the importance of tool dependencies and the effectiveness of our TGR.",
      "tldr_zh": "éšç€ AI agents è£…å¤‡çš„å·¥å…·æ•°é‡æ¿€å¢ï¼Œä¼ ç»ŸåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦(semantic similarity)çš„æ£€ç´¢æ–¹æ³•å› å¿½è§†å·¥å…·é—´çš„ä¾èµ–æ€§ï¼Œå¸¸å¯¼è‡´é—æ¼ä»»åŠ¡æ‰§è¡Œæ‰€éœ€çš„å…ˆéªŒå·¥å…·ã€‚è¯¥ç ”ç©¶æå‡ºäº† Tool Graph Retriever (TGR)ï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼å»ºæ¨¡å·¥å…·é—´çš„ä¾èµ–å…³ç³»æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å·¥å…·æ£€ç´¢æ€§èƒ½ã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†åä¸º TDI300K çš„æ•°æ®é›†ä»¥è®­ç»ƒä¾èµ–å…³ç³»åˆ¤åˆ«å™¨ï¼Œéšåå°†æ‰€æœ‰å€™é€‰å·¥å…·è¡¨ç¤ºä¸ºå·¥å…·ä¾èµ–å›¾(tool dependency graph)ï¼Œå¹¶åˆ©ç”¨å›¾å·ç§¯(graph convolution)æŠ€æœ¯å°†ä¾èµ–ä¿¡æ¯æ•´åˆåˆ°å·¥å…·è¡¨å¾ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTGR åœ¨å¤šä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ç°æœ‰ä¸»æµæ–¹æ³•çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº† SOTA æ°´å¹³ã€‚è¯¥ç ”ç©¶é€šè¿‡æ·±å…¥åˆ†æéªŒè¯äº†å·¥å…·ä¾èµ–æ€§åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜äº† TGR æ¡†æ¶åœ¨å¤„ç†å¤æ‚å·¥å…·è°ƒç”¨ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05152v1",
      "published_date": "2025-08-07 08:36:26 UTC",
      "updated_date": "2025-08-07 08:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:01.344314+00:00"
    },
    {
      "arxiv_id": "2508.05149v1",
      "title": "Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages",
      "title_zh": "ä½èµ„æºåœºæ™¯ä¸‹çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼šæ•°æ®é‡éœ€æ±‚åŠé«˜èµ„æºè¯­è¨€é¢„è®­ç»ƒçš„å½±å“",
      "authors": [
        "Seraphina Fong",
        "Marco Matassoni",
        "Alessio Brutti"
      ],
      "abstract": "Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½èµ„æº(low-resource)åœºæ™¯ä¸‹è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech LLMs)çš„åº”ç”¨ï¼Œé‡ç‚¹åˆ†æäº†è®­ç»ƒæ•°æ®é‡éœ€æ±‚ä»¥åŠé«˜èµ„æºè¯­è¨€é¢„è®­ç»ƒå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶é‡‡ç”¨SLAM-ASRæ¡†æ¶ï¼Œé€šè¿‡è½»é‡çº§å¯è®­ç»ƒæŠ•å½±å™¨(projector)è¿æ¥è¯­éŸ³ç¼–ç å™¨ä¸LLMï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)åœ¨èµ„æºåŒ®ä¹ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚é¦–å…ˆï¼Œç ”ç©¶è¯„ä¼°äº†è¾¾åˆ°ä¸Whisperæ¨¡å‹ç›¸å½“æ€§èƒ½æ‰€éœ€çš„æ•°æ®è§„æ¨¡ï¼Œå†æ¬¡å¼ºè°ƒäº†æœ‰é™æ•°æ®å¸¦æ¥çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚éšåï¼Œå®éªŒè¯æ˜åˆ©ç”¨åœ¨é«˜èµ„æºè¯­è¨€ä¸Šé¢„è®­ç»ƒçš„å•è¯­æˆ–å¤šè¯­æŠ•å½±å™¨èƒ½æ˜¾è‘—ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æå°è®­ç»ƒé›†è§„æ¨¡ä¸‹æ•ˆæœæ˜¾è‘—ã€‚é€šè¿‡é›†æˆEuroLLMã€Salamandraç­‰æ¨¡å‹ä¸whisper-large-v3-turboï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥åœ¨å¤šè¯­è¨€åŠä½èµ„æºç¯å¢ƒä¸‹ä¼˜åŒ–Speech LLMsæä¾›äº†é‡è¦çš„å®è¯å‚è€ƒä¸æŠ€æœ¯è§è§£ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Interspeech 2025. 5 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.05149v1",
      "published_date": "2025-08-07 08:33:42 UTC",
      "updated_date": "2025-08-07 08:33:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:05.133196+00:00"
    },
    {
      "arxiv_id": "2508.05148v2",
      "title": "Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories",
      "title_zh": "Chemist Eyeï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªé©±åŠ¨å®éªŒå®¤å®‰å…¨ç›‘æ§ä¸æœºå™¨äººå†³ç­–ç³»ç»Ÿ",
      "authors": [
        "Francisco Munguia-Galeano",
        "Zhengxue Zhou",
        "Satheeshkumar Veeramani",
        "Hatem Fakhruldeen",
        "Louis Longley",
        "Rob Clowes",
        "Andrew I. Cooper"
      ],
      "abstract": "The integration of robotics and automation into self-driving laboratories (SDLs) can introduce additional safety complexities, in addition to those that already apply to conventional research laboratories. Personal protective equipment (PPE) is an essential requirement for ensuring the safety and well-being of workers in laboratories, self-driving or otherwise. Fires are another important risk factor in chemical laboratories. In SDLs, fires that occur close to mobile robots, which use flammable lithium batteries, could have increased severity. Here, we present Chemist Eye, a distributed safety monitoring system designed to enhance situational awareness in SDLs. The system integrates multiple stations equipped with RGB, depth, and infrared cameras, designed to monitor incidents in SDLs. Chemist Eye is also designed to spot workers who have suffered a potential accident or medical emergency, PPE compliance and fire hazards. To do this, Chemist Eye uses decision-making driven by a vision-language model (VLM). Chemist Eye is designed for seamless integration, enabling real-time communication with robots. Based on the VLM recommendations, the system attempts to drive mobile robots away from potential fire locations, exits, or individuals not wearing PPE, and issues audible warnings where necessary. It also integrates with third-party messaging platforms to provide instant notifications to lab personnel. We tested Chemist Eye with real-world data from an SDL equipped with three mobile robots and found that the spotting of possible safety hazards and decision-making performances reached 97 % and 95 %, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Chemist Eye çš„åˆ†å¸ƒå¼å®‰å…¨ç›‘æ§ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨é©¾é©¶å®éªŒå®¤ (Self-Driving Laboratories, SDLs) çš„æƒ…å¢ƒæ„ŸçŸ¥ä¸ä½œä¸šå®‰å…¨ã€‚ç³»ç»Ÿé›†æˆäº†é…å¤‡ RGBã€æ·±åº¦å’Œçº¢å¤–æ‘„åƒå¤´çš„å¤šç«™å¼ä¼ æ„Ÿå™¨ç½‘ç»œï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model, VLM) é©±åŠ¨å†³ç­–åˆ¶å®šï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«ä¸ªäººé˜²æŠ¤è£…å¤‡ (PPE) åˆè§„æ€§ã€ç«ç¾éšæ‚£ä»¥åŠäººå‘˜æ„å¤–ä¼¤å®³ã€‚Chemist Eye å®ç°äº†ä¸ç§»åŠ¨æœºå™¨äººçš„æ— ç¼é€šä¿¡ï¼Œå¯æ ¹æ® VLM çš„å»ºè®®å¼•å¯¼æœºå™¨äººé¿å¼€å±é™©åŒºåŸŸæˆ–è¿è§„äººå‘˜ï¼Œå¹¶é€šè¿‡ç¬¬ä¸‰æ–¹å¹³å°å‘é€å³æ—¶é¢„è­¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å®é™… SDLs ç¯å¢ƒä¸­çš„å®‰å…¨éšæ‚£è¯†åˆ«å‡†ç¡®ç‡è¾¾åˆ° 97%ï¼Œå†³ç­–æ‰§è¡Œå‡†ç¡®ç‡è¾¾åˆ° 95%ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªä¸»å®éªŒå®¤åœ¨å¤æ‚ç§‘ç ”åœºæ™¯ä¸‹çš„å®‰å…¨ç›‘æ§ä¸æ™ºèƒ½åŒ–æœºå™¨äººå†³ç­–æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05148v2",
      "published_date": "2025-08-07 08:31:42 UTC",
      "updated_date": "2025-08-13 08:11:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:08.539385+00:00"
    },
    {
      "arxiv_id": "2508.05145v1",
      "title": "Graph-based Event Log Repair",
      "title_zh": "åŸºäºå›¾çš„äº‹ä»¶æ—¥å¿—ä¿®å¤",
      "authors": [
        "Sebastiano Dissegna",
        "Chiara Di Francescomarino",
        "Massimiliano Ronzani"
      ],
      "abstract": "The quality of event logs in Process Mining is crucial when applying any form of analysis to them. In real-world event logs, the acquisition of data can be non-trivial (e.g., due to the execution of manual activities and related manual recording or to issues in collecting, for each event, all its attributes), and often may end up with events recorded with some missing information. Standard approaches to the problem of trace (or log) reconstruction either require the availability of a process model that is used to fill missing values by leveraging different reasoning techniques or employ a Machine Learning/Deep Learning model to restore the missing values by learning from similar cases. In recent years, a new type of Deep Learning model that is capable of handling input data encoded as graphs has emerged, namely Graph Neural Networks. Graph Neural Network models, and even more so Heterogeneous Graph Neural Networks, offer the advantage of working with a more natural representation of complex multi-modal sequences like the execution traces in Process Mining, allowing for more expressive and semantically rich encodings.\n  In this work, we focus on the development of a Heterogeneous Graph Neural Network model that, given a trace containing some incomplete events, will return the full set of attributes missing from those events. We evaluate our work against a state-of-the-art approach leveraging autoencoders on two synthetic logs and four real event logs, on different types of missing values. Different from state-of-the-art model-free approaches, which mainly focus on repairing a subset of event attributes, the proposed approach shows very good performance in reconstructing all different event attributes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Process Mining ä¸­å› äººå·¥è®°å½•æˆ–é‡‡é›†å›°éš¾å¯¼è‡´çš„äº‹ä»¶æ—¥å¿—(event logs)å±æ€§ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Heterogeneous Graph Neural Network çš„ä¿®å¤æ¨¡å‹ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ Process Model æ¨ç†æˆ–ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå¯¹å¤æ‚å¤šæ¨¡æ€åºåˆ—è¿›è¡Œæ›´å…·è¯­ä¹‰è¡¨ç°åŠ›çš„è‡ªç„¶ç¼–ç ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ•æ‰æ‰§è¡Œè½¨è¿¹çš„ç‰¹å¾ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ä¸¤ä¸ªåˆæˆæ—¥å¿—å’Œå››ä¸ªçœŸå®æ—¥å¿—ä¸Šçš„å®éªŒè¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶ä¸ºä¸å®Œæ•´äº‹ä»¶è¿˜åŸå…¨å¥—ç¼ºå¤±å±æ€§çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œä¸åŸºäº Autoencoder çš„ç°æœ‰å…ˆè¿› Model-free æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºå„ç±»ä¸åŒäº‹ä»¶å±æ€§æ–¹é¢å‡è¡¨ç°å‡ºæä½³çš„æ€§èƒ½ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•ä»…èƒ½ä¿®å¤éƒ¨åˆ†å±æ€§çš„å±€é™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05145v1",
      "published_date": "2025-08-07 08:26:16 UTC",
      "updated_date": "2025-08-07 08:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:41.685066+00:00"
    },
    {
      "arxiv_id": "2508.05137v1",
      "title": "FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images",
      "title_zh": "FedGINï¼šåŸºäºåŠ¨æ€å…¨å±€å¼ºåº¦éçº¿æ€§å¢å¼ºçš„å¤šæ¨¡æ€å›¾åƒå™¨å®˜åˆ†å‰²è”é‚¦å­¦ä¹ ",
      "authors": [
        "Sachin Dudda Nagaraju",
        "Ashkan Moradi",
        "Bendik Skarre Abrahamsen",
        "Mattijs Elschot"
      ],
      "abstract": "Medical image segmentation plays a crucial role in AI-assisted diagnostics, surgical planning, and treatment monitoring. Accurate and robust segmentation models are essential for enabling reliable, data-driven clinical decision making across diverse imaging modalities. Given the inherent variability in image characteristics across modalities, developing a unified model capable of generalizing effectively to multiple modalities would be highly beneficial. This model could streamline clinical workflows and reduce the need for modality-specific training. However, real-world deployment faces major challenges, including data scarcity, domain shift between modalities (e.g., CT vs. MRI), and privacy restrictions that prevent data sharing. To address these issues, we propose FedGIN, a Federated Learning (FL) framework that enables multimodal organ segmentation without sharing raw patient data. Our method integrates a lightweight Global Intensity Non-linear (GIN) augmentation module that harmonizes modality-specific intensity distributions during local training. We evaluated FedGIN using two types of datasets: an imputed dataset and a complete dataset. In the limited dataset scenario, the model was initially trained using only MRI data, and CT data was added to assess its performance improvements. In the complete dataset scenario, both MRI and CT data were fully utilized for training on all clients. In the limited-data scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test cases compared to FL without GIN and consistently outperformed local baselines. In the complete dataset scenario, FedGIN demonstrated near-centralized performance, with a 30% Dice score improvement over the MRI-only baseline and a 10% improvement over the CT-only baseline, highlighting its strong cross-modality generalization under privacy constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€æ¨¡æ€é—´é¢†åŸŸåç§»(Domain Shift)åŠéšç§ä¿æŠ¤é™åˆ¶ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºFedGINçš„è”é‚¦å­¦ä¹ (Federated Learning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è½»é‡çº§çš„å…¨å±€å¼ºåº¦éçº¿æ€§(Global Intensity Non-linear, GIN)å¢å¼ºæ¨¡å—ï¼Œåœ¨æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåè°ƒäº†ä¸åŒæ¨¡æ€çš„å¼ºåº¦åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å—é™æ•°æ®åœºæ™¯ä¸‹ï¼ŒFedGINåœ¨MRIæµ‹è¯•æ¡ˆä¾‹ä¸Šçš„3D Diceåˆ†æ•°ç›¸è¾ƒäºæ— GINçš„è”é‚¦å­¦ä¹ æå‡äº†12%è‡³18%ã€‚åœ¨å®Œæ•´æ•°æ®é›†åœºæ™¯ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†æ¥è¿‘ä¸­å¿ƒåŒ–è®­ç»ƒçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå•æ¨¡æ€åŸºå‡†æ¨¡å‹ï¼Œå…¶Diceåˆ†æ•°æœ€é«˜å¯æå‡30%ã€‚ç ”ç©¶è¯æ˜FedGINåœ¨ä¸¥æ ¼çš„éšç§çº¦æŸä¸‹å±•ç°å‡ºå“è¶Šçš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½æ˜¾è‘—ä¼˜åŒ–ä¸´åºŠå·¥ä½œæµå¹¶å‡å°‘å¯¹ç‰¹å®šæ¨¡æ€è®­ç»ƒçš„éœ€æ±‚ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper Accepted at MICCAI 2025 DeCaf Workshop Track",
      "pdf_url": "https://arxiv.org/pdf/2508.05137v1",
      "published_date": "2025-08-07 08:16:35 UTC",
      "updated_date": "2025-08-07 08:16:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:17.384718+00:00"
    },
    {
      "arxiv_id": "2508.05132v1",
      "title": "Towards Assessing Medical Ethics from Knowledge to Practice",
      "title_zh": "è¿ˆå‘åŒ»å­¦ä¼¦ç†è¯„ä¼°ï¼šä»çŸ¥è¯†åˆ°å®è·µ",
      "authors": [
        "Chang Hong",
        "Minghao Wu",
        "Qingying Xiao",
        "Yuchi Wang",
        "Xiang Wan",
        "Guangjun Yu",
        "Benyou Wang",
        "Yan Hu"
      ],
      "abstract": "The integration of large language models into healthcare necessitates a rigorous evaluation of their ethical reasoning, an area current benchmarks often overlook. We introduce PrinciplismQA, a comprehensive benchmark with 3,648 questions designed to systematically assess LLMs' alignment with core medical ethics. Grounded in Principlism, our benchmark features a high-quality dataset. This includes multiple-choice questions curated from authoritative textbooks and open-ended questions sourced from authoritative medical ethics case study literature, all validated by medical experts. Our experiments reveal a significant gap between models' ethical knowledge and their practical application, especially in dynamically applying ethical principles to real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence, often over-emphasizing other principles. Frontier closed-source models, driven by strong general capabilities, currently lead the benchmark. Notably, medical domain fine-tuning can enhance models' overall ethical competence, but further progress requires better alignment with medical ethical knowledge. PrinciplismQA offers a scalable framework to diagnose these specific ethical weaknesses, paving the way for more balanced and responsible medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸåº”ç”¨ä¸­ä¼¦ç†æ¨ç†è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†PrinciplismQAè¿™ä¸€æ¶µç›–3,648ä¸ªé—®é¢˜çš„å…¨é¢åŸºå‡†ã€‚è¯¥åŸºå‡†åŸºäºPrinciplismåŸåˆ™ï¼Œç»“åˆäº†ä»æƒå¨æ•™ç§‘ä¹¦å’ŒåŒ»å­¦ä¼¦ç†æ¡ˆä¾‹ä¸­æå–å¹¶ç»ä¸“å®¶éªŒè¯çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°æ¨¡å‹ä¸åŒ»å­¦ä¼¦ç†çš„å¯¹é½æƒ…å†µã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹åœ¨ä¼¦ç†çŸ¥è¯†ä¸å®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€å¤„ç†ç°å®åœºæ™¯ä¸­çš„è¡Œå–„åŸåˆ™(Beneficence)å›°å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡ç›®å‰èƒ½åŠ›è¾ƒå¼ºçš„é—­æºå‰æ²¿æ¨¡å‹è¡¨ç°é¢†å…ˆï¼Œä¸”åŒ»ç–—é¢†åŸŸçš„å¾®è°ƒ(Fine-tuning)èƒ½æå‡ä¼¦ç†èƒ½åŠ›ï¼Œä½†å®ç°ä¸åŒ»å­¦ä¼¦ç†çŸ¥è¯†çš„æ·±åº¦å¯¹é½ä»å…·æŒ‘æˆ˜ã€‚PrinciplismQAä¸ºè¯Šæ–­äººå·¥æ™ºèƒ½(AI)ä¼¦ç†å¼±ç‚¹æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯¹äºæ„å»ºæ›´å¹³è¡¡ã€è´Ÿè´£ä»»çš„åŒ»ç–—äººå·¥æ™ºèƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05132v1",
      "published_date": "2025-08-07 08:10:14 UTC",
      "updated_date": "2025-08-07 08:10:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:22.200309+00:00"
    },
    {
      "arxiv_id": "2508.05128v1",
      "title": "Attention Basin: Why Contextual Position Matters in Large Language Models",
      "title_zh": "æ³¨æ„åŠ›ç›†åœ°ï¼šä¸ºä½•ä¸Šä¸‹æ–‡ä½ç½®åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­è‡³å…³é‡è¦",
      "authors": [
        "Zihao Yi",
        "Delong Zeng",
        "Zhenqing Ling",
        "Haohao Luo",
        "Zhe Xu",
        "Wei Liu",
        "Jian Luan",
        "Wanxia Cao",
        "Ying Shen"
      ],
      "abstract": "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ€§èƒ½å¯¹è¾“å…¥ä¿¡æ¯ä¸Šä¸‹æ–‡ä½ç½®çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œå¹¶æ­ç¤ºäº†ä¸€ç§è¢«ç§°ä¸ºæ³¨æ„åŠ›ç›†åœ°(Attention Basin)çš„æ™®éç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤„ç†ç»“æ„åŒ–é¡¹ç›®åºåˆ—æ—¶ï¼Œæ¨¡å‹ä¼šç³»ç»Ÿæ€§åœ°å¯¹åºåˆ—èµ·å§‹å’Œæœ«å°¾çš„é¡¹ç›®åˆ†é…æ›´é«˜çš„æ³¨æ„åŠ›ï¼Œè€Œå¾€å¾€å¿½è§†ä¸­é—´éƒ¨åˆ†ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°†æ›´é«˜æ³¨æ„åŠ›åˆ†é…ç»™å…³é”®ä¿¡æ¯æ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„æ ¸å¿ƒï¼Œæ®æ­¤ä½œè€…æå‡ºäº†Attention-Driven Reranking (AttnRank)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼°ç®—æ¨¡å‹çš„å›ºæœ‰ä½ç½®æ³¨æ„åŠ›åå¥½ï¼Œé‡æ–°æ’åˆ—è¾“å…¥å†…å®¹ä»¥ä½¿å…³é”®ä¿¡æ¯å¥‘åˆé«˜æ³¨æ„åŠ›ä½ç½®ã€‚AttnRankæ˜¯ä¸€ç§æ¨¡å‹æ— å…³ã€æ— éœ€è®­ç»ƒä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåœ¨è®¡ç®—å¼€é”€æå°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†10ç§ä¸åŒæ¶æ„å’Œè§„æ¨¡LLMsåœ¨å¤šè·³é—®ç­”(Multi-hop QA)å’Œå°‘æ ·æœ¬è¯­å¢ƒå­¦ä¹ (In-context Learning)ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05128v1",
      "published_date": "2025-08-07 08:08:08 UTC",
      "updated_date": "2025-08-07 08:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:22.386198+00:00"
    },
    {
      "arxiv_id": "2508.09177v1",
      "title": "Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation",
      "title_zh": "åŒ»å­¦å½±åƒä¸­çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šåŸºç¡€ã€è¿›å±•ä¸ä¸´åºŠè½¬åŒ–",
      "authors": [
        "Xuanru Zhou",
        "Cheng Li",
        "Shuqiang Wang",
        "Ye Li",
        "Tao Tan",
        "Hairong Zheng",
        "Shanshan Wang"
      ],
      "abstract": "Generative artificial intelligence (AI) is rapidly transforming medical imaging by enabling capabilities such as data synthesis, image enhancement, modality translation, and spatiotemporal modeling. This review presents a comprehensive and forward-looking synthesis of recent advances in generative modeling including generative adversarial networks (GANs), variational autoencoders (VAEs), diffusion models, and emerging multimodal foundation architectures and evaluates their expanding roles across the clinical imaging continuum. We systematically examine how generative AI contributes to key stages of the imaging workflow, from acquisition and reconstruction to cross-modality synthesis, diagnostic support, and treatment planning. Emphasis is placed on both retrospective and prospective clinical scenarios, where generative models help address longstanding challenges such as data scarcity, standardization, and integration across modalities. To promote rigorous benchmarking and translational readiness, we propose a three-tiered evaluation framework encompassing pixel-level fidelity, feature-level realism, and task-level clinical relevance. We also identify critical obstacles to real-world deployment, including generalization under domain shift, hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we explore the convergence of generative AI with large-scale foundation models, highlighting how this synergy may enable the next generation of scalable, reliable, and clinically integrated imaging systems. By charting technical progress and translational pathways, this review aims to guide future research and foster interdisciplinary collaboration at the intersection of AI, medicine, and biomedical engineering.",
      "tldr_zh": "æœ¬ç»¼è¿°ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨ï¼Œæ¶µç›–äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ã€å˜åˆ†è‡ªç¼–ç å™¨(VAEs)ã€æ‰©æ•£æ¨¡å‹(Diffusion Models)åŠæ–°å…´çš„å¤šæ¨¡æ€åŸºç¡€æ¶æ„ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¦‚ä½•åŠ©åŠ›ä»å½±åƒè·å–ä¸é‡å»ºåˆ°è·¨æ¨¡æ€åˆæˆã€è¯Šæ–­æ”¯æŒå’Œæ²»ç–—è§„åˆ’çš„æ•´ä¸ªä¸´åºŠå½±åƒå·¥ä½œæµç¨‹ã€‚ç ”ç©¶é‡ç‚¹è®¨è®ºäº†ç”Ÿæˆæ¨¡å‹åœ¨è§£å†³æ•°æ®ç¨€ç¼º(Data Scarcity)ã€æ ‡å‡†åŒ–å’Œè·¨æ¨¡æ€æ•´åˆç­‰é•¿æœŸæŒ‘æˆ˜æ–¹é¢çš„ä½œç”¨ã€‚ä¸ºäº†æ¨åŠ¨ä¸´åºŠè½¬åŒ–ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«åƒç´ çº§ä¿çœŸåº¦(Pixel-level Fidelity)ã€ç‰¹å¾çº§çœŸå®æ„Ÿ(Feature-level Realism)å’Œä»»åŠ¡çº§ä¸´åºŠç›¸å…³æ€§(Task-level Clinical Relevance)çš„ä¸‰å±‚è¯„ä¼°æ¡†æ¶ã€‚ç»¼è¿°åŒæ—¶è¯†åˆ«äº†å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„å…³é”®éšœç¢ï¼Œå¦‚é¢†åŸŸæ¼‚ç§»(Domain Shift)ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€å¹»è§‰é£é™©(Hallucination Risk)ä»¥åŠæ•°æ®éšç§ç›‘ç®¡ã€‚æœ€åï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹(Foundation Models)çš„ååŒæ½œåŠ›ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£ä¸´åºŠé›†æˆå½±åƒç³»ç»ŸæŒ‡æ˜äº†æŠ€æœ¯è·¯å¾„å’Œè½¬åŒ–æ–¹å‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09177v1",
      "published_date": "2025-08-07 07:58:40 UTC",
      "updated_date": "2025-08-07 07:58:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:25.591109+00:00"
    },
    {
      "arxiv_id": "2508.05123v2",
      "title": "Latent Expression Generation for Referring Image Segmentation and Grounding",
      "title_zh": "é¢å‘æŒ‡ä»£æ€§å›¾åƒåˆ†å‰²ä¸å®šä½çš„æ½œåœ¨è¡¨è¾¾å¼ç”Ÿæˆ",
      "authors": [
        "Seonghoon Yu",
        "Junbeom Hong",
        "Joonseok Lee",
        "Jeany Son"
      ],
      "abstract": "Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹æŒ‡ä»£å›¾åƒåˆ†å‰² (Referring Image Segmentation, RIS) å’ŒæŒ‡ä»£è¡¨è¾¾å¼ç†è§£ (Referring Expression Comprehension, REC) ä»»åŠ¡ä¸­ï¼Œå•ä¸€æ–‡æœ¬è¾“å…¥å› ç¼ºä¹ä¸°å¯Œè§†è§‰ç»†èŠ‚è€Œå¯¼è‡´ç›®æ ‡è¯¯åˆ¤çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„è§†è§‰å®šä½æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆå¤šä¸ªåŒ…å«äº’è¡¥è§†è§‰ä¿¡æ¯çš„æ½œåœ¨è¡¨è¾¾å¼ (Latent Expressions)ï¼Œå¼¥åˆäº†ç¨€ç–æ–‡æœ¬çº¿ç´¢ä¸ä¸°å¯Œè§†è§‰ç»†èŠ‚ä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶å…·ä½“å¼•å…¥äº†ä¸»ä½“åˆ†å‘å™¨ (Subject Distributor) å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥å™¨ (Visual Concept Injector) æ¨¡å—ï¼Œæ—¨åœ¨å°†å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§çš„æ¦‚å¿µåµŒå…¥æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œæ•æ‰ç›®æ ‡ç‰¹æœ‰çš„è§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§æ­£è¾¹è·å¯¹æ¯”å­¦ä¹  (Positive-margin Contrastive Learning) ç­–ç•¥ï¼Œåœ¨ä¿æŒè¡¨è¾¾å¼ç»†å¾®å·®å¼‚çš„åŒæ—¶å®ç°ä¸åŸå§‹æ–‡æœ¬çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ª RIS å’Œ REC åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶åœ¨å¹¿ä¹‰æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰² (GRES) ä»»åŠ¡ä¸­å–å¾—äº†å“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05123v2",
      "published_date": "2025-08-07 07:57:27 UTC",
      "updated_date": "2025-08-18 06:18:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:29.388735+00:00"
    },
    {
      "arxiv_id": "2508.05118v4",
      "title": "Reasoning through Exploration: A Reinforcement Learning Framework for Robust Function Calling",
      "title_zh": "æ¢ç´¢å¼æ¨ç†ï¼šé¢å‘é²æ£’å‡½æ•°è°ƒç”¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Bingguang Hao",
        "Zengzhuang Xu",
        "Maolin Wang",
        "Yuntao Wen",
        "Yicheng Chen",
        "Cunyin Peng",
        "Long Chen",
        "Dong Wang",
        "Xiangyu Zhao",
        "Jinjie Gu",
        "Chenyi Zhuang",
        "Ji Zhang"
      ],
      "abstract": "The effective training of Large Language Models (LLMs) for function calling faces a critical challenge: balancing exploration of complex reasoning paths with stable policy optimization. Standard methods like Supervised Fine-Tuning (SFT) fail to instill robust reasoning, and traditional Reinforcement Learning (RL) struggles with inefficient exploration. We propose \\textbf{EGPO}, a new RL framework built upon Group Relative Policy Optimization (GRPO), designed to address this challenge directly. The core of EGPO is an entropy-enhanced advantage function that integrates the entropy of the model's Chain-of-Thought (CoT) into the policy gradient computation. This encourages the generation of diverse reasoning strategies. To maintain optimization direction, the entropy bonus is carefully constrained by a clipping mechanism. Complemented by a strict, binary reward signal, EGPO effectively guides the model towards discovering structured and accurate tool invocation patterns. On the challenging Berkeley Function Calling Leaderboard (BFCL), a 4B-parameter model trained with EGPO sets a new state-of-the-art among models of comparable size, surpassing a range of strong competitors, including GPT-4o and Gemini-2.5.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EGPOï¼Œä¸€ç§åŸºäºGroup Relative Policy Optimization (GRPO)çš„æ–°å‹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‡½æ•°è°ƒç”¨(Function Calling)ä»»åŠ¡ä¸­å¹³è¡¡æ¨ç†è·¯å¾„æ¢ç´¢ä¸ç­–ç•¥ä¼˜åŒ–çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚EGPOçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ç†µå¢å¼ºä¼˜åŠ¿å‡½æ•°(Entropy-enhanced Advantage Function)ï¼Œå°†æ¨¡å‹çš„é“¾å¼æ€ç»´(Chain-of-Thought)ç†µæ•´åˆè¿›ç­–ç•¥æ¢¯åº¦è®¡ç®—ï¼Œä»è€Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†ç­–ç•¥ã€‚ä¸ºäº†ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å‰ªè£æœºåˆ¶(Clipping Mechanism)å¯¹ç†µå¥–åŠ±è¿›è¡Œçº¦æŸï¼Œå¹¶è¾…ä»¥ä¸¥æ ¼çš„äºŒå…ƒå¥–åŠ±ä¿¡å·ï¼Œç²¾å‡†å¼•å¯¼æ¨¡å‹å­¦ä¹ ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨Berkeley Function Calling Leaderboard (BFCL)åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»EGPOè®­ç»ƒçš„4Bå‚æ•°æ¨¡å‹åˆ·æ–°äº†åŒè§„æ¨¡æ¨¡å‹çš„ä¸–ç•Œçºªå½•ï¼Œå…¶è¡¨ç°ç”šè‡³ä¼˜äºGPT-4oå’ŒGemini-2.5ç­‰å¼ºåŠ›ç«äº‰å¯¹æ‰‹ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºé²æ£’çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›æä¾›äº†é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œè¯æ˜äº†é€šè¿‡ä¼˜åŒ–æ¢ç´¢æœºåˆ¶å¯å¤§å¹…æå‡è½»é‡çº§æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05118v4",
      "published_date": "2025-08-07 07:51:38 UTC",
      "updated_date": "2025-10-10 06:20:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:57:43.691242+00:00"
    },
    {
      "arxiv_id": "2508.05116v1",
      "title": "Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures",
      "title_zh": "è¶…è¶Šè‡ªåŠ¨åŒ–ï¼šè‹æ ¼æ‹‰åº•å¼äººå·¥æ™ºèƒ½ã€è®¤è¯†è®ºèƒ½åŠ¨æ€§ä¸ç¼–æ’å¼å¤šæ™ºèƒ½ä½“å­¦ä¹ æ¶æ„å…´èµ·çš„æ„ä¹‰",
      "authors": [
        "Peer-Benedikt Degen",
        "Igor Asanov"
      ],
      "abstract": "Generative AI is no longer a peripheral tool in higher education. It is rapidly evolving into a general-purpose infrastructure that reshapes how knowledge is generated, mediated, and validated. This paper presents findings from a controlled experiment evaluating a Socratic AI Tutor, a large language model designed to scaffold student research question development through structured dialogue grounded in constructivist theory. Conducted with 65 pre-service teacher students in Germany, the study compares interaction with the Socratic Tutor to engagement with an uninstructed AI chatbot. Students using the Socratic Tutor reported significantly greater support for critical, independent, and reflective thinking, suggesting that dialogic AI can stimulate metacognitive engagement and challenging recent narratives of de-skilling due to generative AI usage. These findings serve as a proof of concept for a broader pedagogical shift: the use of multi-agent systems (MAS) composed of specialised AI agents. To conceptualise this, we introduce the notion of orchestrated MAS, modular, pedagogically aligned agent constellations, curated by educators, that support diverse learning trajectories through differentiated roles and coordinated interaction. To anchor this shift, we propose an adapted offer-and-use model, in which students appropriate instructional offers from these agents. Beyond technical feasibility, we examine system-level implications for higher education institutions and students, including funding necessities, changes to faculty roles, curriculars, competencies and assessment practices. We conclude with a comparative cost-effectiveness analysis highlighting the scalability of such systems. In sum, this study contributes both empirical evidence and a conceptual roadmap for hybrid learning ecosystems that embed human-AI co-agency and pedagogical alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Generative AIåœ¨é«˜ç­‰æ•™è‚²ä¸­é‡å¡‘çŸ¥è¯†ç”Ÿæˆçš„ä½œç”¨ï¼Œå¹¶é€šè¿‡å¯¹65åå¾·å›½èŒå‰æ•™å¸ˆå­¦ç”Ÿçš„å¯¹ç…§å®éªŒï¼Œè¯„ä¼°äº†æ—¨åœ¨å¼•å¯¼å­¦ç”Ÿç ”ç©¶é—®é¢˜å¼€å‘çš„Socratic AI Tutorã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºæ™®é€šçš„AIèŠå¤©æœºå™¨äººï¼ŒSocratic AI Tutorèƒ½æ˜¾è‘—æ”¯æŒå­¦ç”Ÿçš„æ‰¹åˆ¤æ€§ã€ç‹¬ç«‹æ€§å’Œåæ€æ€§æ€ç»´ï¼Œæ¿€å‘metacognitive engagementï¼Œæœ‰åŠ›å›åº”äº†AIä¼šå¯¼è‡´å»æŠ€èƒ½åŒ–(de-skilling)çš„æ‹…å¿§ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†orchestrated Multi-Agent Systems (MAS)çš„æ¦‚å¿µï¼Œå³ç”±æ•™è‚²è€…ç­–åˆ’å¹¶ç¬¦åˆæ•™å­¦é€»è¾‘çš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æ˜Ÿåº§ï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–çš„å­¦ä¹ è·¯å¾„ã€‚ç ”ç©¶è¿˜æå‡ºäº†é€‚é…çš„offer-and-use modelï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è¯¥æ¶æ„å¯¹é«˜æ ¡ç»è´¹ã€æ•™èŒè§’è‰²ã€è¯¾ç¨‹ä½“ç³»åŠè¯„ä¼°å®è·µçš„ç³»ç»Ÿæ€§å½±å“ã€‚æœ€åï¼Œé€šè¿‡æˆæœ¬æ•ˆç›Šåˆ†æéªŒè¯äº†è¯¥ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ï¼Œä¸ºå®ç°äººç±»ä¸AIååŒ(human-AI co-agency)çš„æ··åˆå­¦ä¹ ç”Ÿæ€æä¾›äº†ç†è®ºä¸å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05116v1",
      "published_date": "2025-08-07 07:49:03 UTC",
      "updated_date": "2025-08-07 07:49:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:12.481759+00:00"
    },
    {
      "arxiv_id": "2508.05113v1",
      "title": "EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search",
      "title_zh": "EasySizeï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼å¯å‘å¼æœç´¢çš„å¼¹æ€§æ¨¡æ‹Ÿç”µè·¯å°ºå¯¸è®¾è®¡",
      "authors": [
        "Xinyue Wu",
        "Fan Hu",
        "Shaik Jani Babu",
        "Yi Zhao",
        "Xinfei Guo"
      ],
      "abstract": "Analog circuit design is a time-consuming, experience-driven task in chip development. Despite advances in AI, developing universal, fast, and stable gate sizing methods for analog circuits remains a significant challenge. Recent approaches combine Large Language Models (LLMs) with heuristic search techniques to enhance generalizability, but they often depend on large model sizes and lack portability across different technology nodes. To overcome these limitations, we propose EasySize, the first lightweight gate sizing framework based on a finetuned Qwen3-8B model, designed for universal applicability across process nodes, design specifications, and circuit topologies. EasySize exploits the varying Ease of Attainability (EOA) of performance metrics to dynamically construct task-specific loss functions, enabling efficient heuristic search through global Differential Evolution (DE) and local Particle Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned solely on 350nm node data, EasySize achieves strong performance on 5 operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology nodes without additional targeted training, and outperforms AutoCkt, a widely-used Reinforcement Learning based sizing framework, on 86.67\\% of tasks with more than 96.67\\% of simulation resources reduction. We argue that EasySize can significantly reduce the reliance on human expertise and computational resources in gate sizing, thereby accelerating and simplifying the analog circuit design process. EasySize will be open-sourced at a later date.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EasySizeï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¾®è°ƒQwen3-8Bæ¨¡å‹çš„è½»é‡åŒ–é—¨å°ºå¯¸è°ƒæ•´(gate sizing)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡æ‹Ÿç”µè·¯è®¾è®¡ä¸­ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•è·¨å·¥è‰ºèŠ‚ç‚¹è¿ç§»æ€§å·®ä¸”è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚EasySizeåˆ©ç”¨æ€§èƒ½æŒ‡æ ‡çš„å¯å®ç°æ€§(Ease of Attainability, EOA)åŠ¨æ€æ„å»ºä»»åŠ¡ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨åé¦ˆå¢å¼ºæµç¨‹ä¸­ç»“åˆå…¨å±€å·®åˆ†è¿›åŒ–(Differential Evolution, DE)å’Œå±€éƒ¨ç²’å­ç¾¤ä¼˜åŒ–(Particle Swarm Optimization, PSO)ç®—æ³•è¿›è¡Œé«˜æ•ˆçš„å¯å‘å¼æœç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡è¯¥æ¡†æ¶ä»…åœ¨350nmèŠ‚ç‚¹æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå´èƒ½åœ¨180nmã€45nmå’Œ22nmç­‰å¤šä¸ªæŠ€æœ¯èŠ‚ç‚¹çš„è¿ç®—æ”¾å¤§å™¨(Op-Amp)ç”µè·¯ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨ä¸å¼ºåŒ–å­¦ä¹ æ¡†æ¶AutoCktçš„å¯¹æ¯”ä¸­ï¼ŒEasySizeåœ¨86.67%çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡96.67%çš„ä»¿çœŸèµ„æºæ¶ˆè€—ã€‚è¯¥ç ”ç©¶æ˜¾è‘—é™ä½äº†æ¨¡æ‹Ÿç”µè·¯è®¾è®¡å¯¹äººç±»ä¸“å®¶ç»éªŒå’Œè®¡ç®—èµ„æºçš„ä¾èµ–ï¼Œä¸ºåŠ é€Ÿå’Œç®€åŒ–èŠ¯ç‰‡å¼€å‘è¿‡ç¨‹æä¾›äº†é«˜æ•ˆã€é€šç”¨çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05113v1",
      "published_date": "2025-08-07 07:47:07 UTC",
      "updated_date": "2025-08-07 07:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:29.491108+00:00"
    },
    {
      "arxiv_id": "2508.05102v3",
      "title": "Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS",
      "title_zh": "æ„éŸ³éšœç¢è¯­éŸ³åˆæˆä¸­çš„å…¬å¹³æ€§ï¼šæ¢ç©¶åŸºäº F5-TTS çš„æ„éŸ³éšœç¢è¯­éŸ³å…‹éš†å†…åœ¨åå·®",
      "authors": [
        "M Anuprabha",
        "Krishna Gurugubelli",
        "Anil Kumar Vuppala"
      ],
      "abstract": "Dysarthric speech poses significant challenges in developing assistive technologies, primarily due to the limited availability of data. Recent advances in neural speech synthesis, especially zero-shot voice cloning, facilitate synthetic speech generation for data augmentation; however, they may introduce biases towards dysarthric speech. In this paper, we investigate the effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using TORGO dataset, focusing on intelligibility, speaker similarity, and prosody preservation. We also analyze potential biases using fairness metrics like Disparate Impact and Parity Difference to assess disparities across dysarthric severity levels. Results show that F5-TTS exhibits a strong bias toward speech intelligibility over speaker and prosody preservation in dysarthric speech synthesis. Insights from this study can help integrate fairness-aware dysarthric speech synthesis, fostering the advancement of more inclusive speech technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„éŸ³éšœç¢è¯­éŸ³åˆæˆï¼ˆDysarthric speech synthesisï¼‰ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†åˆ©ç”¨ F5-TTS è¿›è¡Œæ„éŸ³éšœç¢è¯­éŸ³å…‹éš†æ—¶å­˜åœ¨çš„å†…åœ¨åè§ã€‚ç ”ç©¶äººå‘˜åœ¨ TORGO æ•°æ®é›†ä¸Šè¯„ä¼°äº† F5-TTS åœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆintelligibilityï¼‰ã€è¯´è¯äººç›¸ä¼¼åº¦ï¼ˆspeaker similarityï¼‰å’ŒéŸµå¾‹ä¿ç•™ï¼ˆprosody preservationï¼‰æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ Disparate Impact å’Œ Parity Difference ç­‰å…¬å¹³æ€§æŒ‡æ ‡ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†ä¸åŒæ„éŸ³éšœç¢ä¸¥é‡ç¨‹åº¦ï¼ˆseverity levelsï¼‰ä¹‹é—´çš„è¡¨ç°å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼ŒF5-TTS åœ¨åˆæˆæ„éŸ³éšœç¢è¯­éŸ³æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„åå‘æ€§ï¼Œå³ä¼˜å…ˆä¿éšœè¯­éŸ³æ¸…æ™°åº¦è€Œç‰ºç‰²äº†è¯´è¯äººç›¸ä¼¼åº¦å’ŒéŸµå¾‹çš„ä¿ç•™ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘å…·å¤‡å…¬å¹³æ„è¯†çš„æ„éŸ³éšœç¢è¯­éŸ³åˆæˆç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´åŠ åŒ…å®¹çš„è¾…åŠ©è¯­éŸ³æŠ€æœ¯å‘å±•ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05102v3",
      "published_date": "2025-08-07 07:39:48 UTC",
      "updated_date": "2025-08-15 06:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:23.885998+00:00"
    },
    {
      "arxiv_id": "2508.05710v2",
      "title": "Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning",
      "title_zh": "Klear-CodeTestï¼šé¢å‘ä»£ç å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ",
      "authors": [
        "Jia Fu",
        "Xinyu Yang",
        "Hongzhi Zhang",
        "Yahui Liu",
        "Jingyuan Zhang",
        "Qi Wang",
        "Fuzheng Zhang",
        "Guorui Zhou"
      ],
      "abstract": "Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: https://github.com/Kwai-Klear/CodeTest.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Klear-CodeTestï¼Œä¸€ä¸ªä¸“é—¨ä¸ºä»£ç å¼ºåŒ–å­¦ä¹  (Code Reinforcement Learning) è®¾è®¡çš„å¤§è§„æ¨¡æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) è®­ç»ƒä¸­é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹åˆæˆçš„éš¾é¢˜ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„ç”Ÿæˆå™¨-éªŒè¯ (Generator-Validation, G-V) æ¶æ„ï¼Œé€šè¿‡ä¸€è‡´æ€§éªŒè¯æœºåˆ¶å°†ç”Ÿæˆçš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆ (Gold Solutions) è¿›è¡Œæ¯”å¯¹ï¼Œç¡®ä¿äº†æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚è¯¥æ¡†æ¶ç”Ÿæˆçš„æµ‹è¯•é›†æ¶µç›–äº†å¸¸è§„æ¡ˆä¾‹å’Œè¾¹ç•Œæ¡ˆä¾‹ (Corner Cases)ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ä»£ç æ­£ç¡®æ€§è¯„ä¼°çš„è¦†ç›–èŒƒå›´å’Œåˆ¤åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€å¥—é’ˆå¯¹åœ¨çº¿éªŒè¯å¹³å°ä¼˜åŒ–çš„å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿ (Security Sandbox System)ï¼Œä¿éšœäº†ä»£ç æ‰§è¡Œçš„å®‰å…¨æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶æä¾›çš„é«˜è´¨é‡æ•°æ®é›†èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œä¸ºä»£ç é¢†åŸŸçš„å¤§æ¨¡å‹è®­ç»ƒæä¾›äº†å…³é”®çš„åŸºç¡€è®¾æ–½æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "21 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05710v2",
      "published_date": "2025-08-07 07:36:01 UTC",
      "updated_date": "2025-09-11 02:44:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:31.284365+00:00"
    },
    {
      "arxiv_id": "2508.09176v1",
      "title": "DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic",
      "title_zh": "DQTï¼šåŸºäºå…åé‡åŒ–åµŒå¥—æ•´æ•°è¿ç®—çš„åŠ¨æ€é‡åŒ–è®­ç»ƒ",
      "authors": [
        "Hazem Hesham Yousef Shalby",
        "Fabrizio Pittorino",
        "Francesca Palermo",
        "Diana Trojaniello",
        "Manuel Roveri"
      ],
      "abstract": "The deployment of deep neural networks on resource-constrained devices relies on quantization. While static, uniform quantization applies a fixed bit-width to all inputs, it fails to adapt to their varying complexity. Dynamic, instance-based mixed-precision quantization promises a superior accuracy-efficiency trade-off by allocating higher precision only when needed. However, a critical bottleneck remains: existing methods require a costly dequantize-to-float and requantize-to-integer cycle to change precision, breaking the integer-only hardware paradigm and compromising performance gains. This paper introduces Dynamic Quantization Training (DQT), a novel framework that removes this bottleneck. At the core of DQT is a nested integer representation where lower-precision values are bit-wise embedded within higher-precision ones. This design, coupled with custom integer-only arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost bit-shift operation. This makes DQT the first quantization framework to enable both dequantization-free static mixed-precision of the backbone network, and truly efficient dynamic, instance-based quantization through a lightweight controller that decides at runtime how to quantize each layer. We demonstrate DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1 accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET, 76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this with a bit-width transition cost of only 28.3M simple bit-shift operations, a drastic improvement over the 56.6M costly Multiply-Accumulate (MAC) floating-point operations required by previous dynamic approaches - unlocking a new frontier in efficient, adaptive AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŠ¨æ€é‡åŒ–è®­ç»ƒï¼ˆDynamic Quantization Training, DQTï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŠ¨æ€æ··åˆç²¾åº¦é‡åŒ–ï¼ˆDynamic mixed-precision quantizationï¼‰ä¸­é¢‘ç¹çš„å»é‡åŒ–ä¸é‡é‡åŒ–æ“ä½œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚DQTçš„æ ¸å¿ƒè´¡çŒ®æ˜¯å¼•å…¥äº†åµŒå¥—æ•´æ•°è¡¨ç¤ºï¼ˆNested integer representationï¼‰å’Œè‡ªå®šä¹‰çº¯æ•´æ•°ç®—æœ¯ï¼Œä½¿ä½å®½åˆ‡æ¢èƒ½å¤Ÿé€šè¿‡è¿‘ä¹é›¶æˆæœ¬çš„ä½ç§»æ“ä½œå®Œæˆã€‚è¿™ç§è®¾è®¡å…è®¸DQTåœ¨æ— éœ€è¿”å›æµ®ç‚¹åŸŸçš„æƒ…å†µä¸‹ï¼Œå®ç°éª¨å¹²ç½‘ç»œçš„é™æ€æ··åˆç²¾åº¦å’Œé«˜æ•ˆçš„å®ä¾‹çº§åŠ¨æ€é‡åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ4-bitåŠ¨æ€ResNet50åœ¨ImageNetä¸Šå–å¾—äº†77.00%çš„Top-1å‡†ç¡®ç‡ï¼Œæ€§èƒ½è¶…è¶Šäº†LSQå’ŒDQNETç­‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒDQTå°†ä½å®½è½¬æ¢æˆæœ¬ä»å¤æ‚çš„æµ®ç‚¹ä¹˜ç´¯åŠ ï¼ˆMACï¼‰æ“ä½œä¼˜åŒ–ä¸ºç®€å•çš„ä½ç§»æ“ä½œï¼Œæå¤§æå‡äº†ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‰“ç ´æ•´æ•°ç¡¬ä»¶èŒƒå¼çš„é™åˆ¶ï¼Œä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„é«˜æ•ˆè‡ªé€‚åº”AIéƒ¨ç½²æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09176v1",
      "published_date": "2025-08-07 07:31:48 UTC",
      "updated_date": "2025-08-07 07:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:59:27.847102+00:00"
    },
    {
      "arxiv_id": "2508.05089v1",
      "title": "Integrated Influence: Data Attribution with Baseline",
      "title_zh": "ç§¯åˆ†å½±å“ï¼šå¸¦æœ‰åŸºå‡†çš„æ•°æ®å½’å› ",
      "authors": [
        "Linxiao Yang",
        "Xinyu Gu",
        "Liang Sun"
      ],
      "abstract": "As an effective approach to quantify how training samples influence test sample, data attribution is crucial for understanding data and model and further enhance the transparency of machine learning models. We find that prevailing data attribution methods based on leave-one-out (LOO) strategy suffer from the local-based explanation, as these LOO-based methods only perturb a single training sample, and overlook the collective influence in the training set. On the other hand, the lack of baseline in many data attribution methods reduces the flexibility of the explanation, e.g., failing to provide counterfactual explanations. In this paper, we propose Integrated Influence, a novel data attribution method that incorporates a baseline approach. Our method defines a baseline dataset, follows a data degeneration process to transition the current dataset to the baseline, and accumulates the influence of each sample throughout this process. We provide a solid theoretical framework for our method, and further demonstrate that popular methods, such as influence functions, can be viewed as special cases of our approach. Experimental results show that Integrated Influence generates more reliable data attributions compared to existing methods in both data attribution task and mislablled example identification task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Integrated Influenceï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº† baseline æ–¹æ³•çš„å…¨æ–° data attribution æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€æ˜åº¦ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç°æœ‰çš„åŸºäº leave-one-out (LOO) ç­–ç•¥çš„æ–¹æ³•ä¸»è¦ä¾èµ–å±€éƒ¨è§£é‡Šï¼Œå¿½è§†äº†è®­ç»ƒæ ·æœ¬é—´çš„é›†ä½“å½±å“ï¼Œä¸”å› ç¼ºä¹ baseline è€Œéš¾ä»¥æä¾›åäº‹å®è§£é‡Šã€‚Integrated Influence é€šè¿‡å®šä¹‰ baseline æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä»å½“å‰æ•°æ®é›†å‘ baseline è¿‡æ¸¡çš„ data degeneration è¿‡ç¨‹ï¼Œé€šè¿‡ç´¯ç§¯è¯¥è¿‡ç¨‹ä¸­çš„å½±å“é‡æ¥é‡åŒ–æ ·æœ¬è´¡çŒ®ã€‚è¯¥æ–¹æ³•å»ºç«‹äº†åšå®çš„ç†è®ºæ¡†æ¶ï¼Œå¹¶è¯æ˜äº†åŒ…æ‹¬ influence functions åœ¨å†…çš„å¤šç§æµè¡Œæ–¹æ³•å‡å¯è§†ä¸ºå…¶ç‰¹ä¾‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIntegrated Influence åœ¨ data attribution ä»»åŠ¡å’Œ mislabelled example identification ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05089v1",
      "published_date": "2025-08-07 07:16:12 UTC",
      "updated_date": "2025-08-07 07:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:42.233465+00:00"
    },
    {
      "arxiv_id": "2508.05087v1",
      "title": "JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering",
      "title_zh": "JPSï¼šåŸºäºååŒè§†è§‰æ‰°åŠ¨ä¸æ–‡æœ¬å¼•å¯¼çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±",
      "authors": [
        "Renmiao Chen",
        "Shiyao Cui",
        "Xuancheng Huang",
        "Chengwei Pan",
        "Victor Shea-Jay Huang",
        "QingLin Zhang",
        "Xuan Ouyang",
        "Zhexin Zhang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "abstract": "Jailbreak attacks against multimodal large language Models (MLLMs) are a significant research focus. Current research predominantly focuses on maximizing attack success rate (ASR), often overlooking whether the generated responses actually fulfill the attacker's malicious intent. This oversight frequently leads to low-quality outputs that bypass safety filters but lack substantial harmful content. To address this gap, we propose JPS, \\underline{J}ailbreak MLLMs with collaborative visual \\underline{P}erturbation and textual \\underline{S}teering, which achieves jailbreaks via corporation of visual image and textually steering prompt. Specifically, JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by \"steering prompt\" optimized via a multi-agent system to specifically guide LLM responses fulfilling the attackers' intent. These visual and textual components undergo iterative co-optimization for enhanced performance. To evaluate the quality of attack outcomes, we propose the Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a Reasoning-LLM-based evaluator. Our experiments show JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy. Codes are available at \\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}. \\color{warningcolor}{Warning: This paper contains potentially sensitive contents.}",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„è¶Šç‹±æ”»å‡»ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•å¾€å¾€åªè¿½æ±‚é«˜æ”»å‡»æˆåŠŸç‡ (ASR)ï¼Œå´å¿½ç•¥äº†ç”Ÿæˆçš„å“åº”æ˜¯å¦çœŸæ­£æ»¡è¶³äº†æ”»å‡»è€…çš„æ¶æ„æ„å›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œä½œè€…æå‡ºäº† JPS æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è§†è§‰æ‰°åŠ¨ (Visual Perturbation) ä¸æ–‡æœ¬å¼•å¯¼ (Textual Steering) çš„ååŒä½œç”¨æ¥å®ç°è¶Šç‹±ã€‚JPS å…·ä½“åˆ©ç”¨ç›®æ ‡å¼•å¯¼çš„å¯¹æŠ—æ€§å›¾åƒæ‰°åŠ¨æ¥ç»•è¿‡å®‰å…¨æ£€æµ‹ï¼Œå¹¶é…åˆç”±å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¼˜åŒ–çš„â€œå¼•å¯¼æç¤ºâ€æ¥å®šå‘è¯±å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆæ„å›¾çš„å“åº”ã€‚ç ”ç©¶å¯¹è§†è§‰å’Œæ–‡æœ¬ç»„ä»¶è¿›è¡Œäº†è¿­ä»£ååŒä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†æ¶æ„æ„å›¾è¾¾æˆç‡ (MIFR) æŒ‡æ ‡ï¼Œé‡‡ç”¨åŸºäºæ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹ (Reasoning-LLM) ä½œä¸ºè¯„ä¼°å™¨ã€‚å®éªŒè¯æ˜ï¼ŒJPS åœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„ ASR å’Œ MIFR å‡è¾¾åˆ°äº†å½“å‰çš„é¢†å…ˆæ°´å¹³ (SOTA)ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å’Œè¯„ä¼° MLLMs çš„å®‰å…¨æ¼æ´æä¾›äº†é‡è¦å‚è€ƒï¼Œç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.MM",
      "comment": "10 pages, 3 tables, 2 figures, to appear in the Proceedings of the 33rd ACM International Conference on Multimedia (MM '25)",
      "pdf_url": "https://arxiv.org/pdf/2508.05087v1",
      "published_date": "2025-08-07 07:14:01 UTC",
      "updated_date": "2025-08-07 07:14:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:48.934041+00:00"
    },
    {
      "arxiv_id": "2508.05083v1",
      "title": "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models",
      "title_zh": "MedMKEBï¼šåŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»¼åˆçŸ¥è¯†ç¼–è¾‘åŸºå‡†",
      "authors": [
        "Dexuan Xu",
        "Jieyi Wang",
        "Zhongyan Chai",
        "Yongzhi Cao",
        "Hanpin Wang",
        "Huamin Zhang",
        "Yu Huang"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly improved medical AI, enabling it to unify the understanding of visual and textual information. However, as medical knowledge continues to evolve, it is critical to allow these models to efficiently update outdated or incorrect information without retraining from scratch. Although textual knowledge editing has been widely studied, there is still a lack of systematic benchmarks for multimodal medical knowledge editing involving image and text modalities. To fill this gap, we present MedMKEB, the first comprehensive benchmark designed to evaluate the reliability, generality, locality, portability, and robustness of knowledge editing in medical multimodal large language models. MedMKEB is built on a high-quality medical visual question-answering dataset and enriched with carefully constructed editing tasks, including counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness. We incorporate human expert validation to ensure the accuracy and reliability of the benchmark. Extensive single editing and sequential editing experiments on state-of-the-art general and medical MLLMs demonstrate the limitations of existing knowledge-based editing approaches in medicine, highlighting the need to develop specialized editing strategies. MedMKEB will serve as a standard benchmark to promote the development of trustworthy and efficient medical knowledge editing algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MedMKEBï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºè¯„ä¼°åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Medical Multimodal Large Language Models, MLLMs) çŸ¥è¯†ç¼–è¾‘èƒ½åŠ›è€Œè®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹åŒ»ç–—çŸ¥è¯†å¿«é€Ÿæ¼”è¿›ä¸”ç°æœ‰ç ”ç©¶ç¼ºä¹å¤šæ¨¡æ€ç¼–è¾‘æ ‡å‡†çš„é—®é¢˜ï¼ŒMedMKEB å¡«è¡¥äº†æ–‡æœ¬ä¸å›¾åƒæ¨¡æ€ç»“åˆçš„åŒ»ç–—çŸ¥è¯†ç¼–è¾‘è¯„ä¼°ç©ºç™½ã€‚è¯¥åŸºå‡†åŸºäºé«˜è´¨é‡åŒ»ç–—è§†è§‰é—®ç­”æ•°æ®é›†æ„å»ºï¼Œé€šè¿‡ counterfactual correctionã€semantic generalization å’Œ knowledge transfer ç­‰ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨ reliabilityã€generalityã€localityã€portability å’Œ robustness æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†äººç±»ä¸“å®¶éªŒè¯ä»¥ç¡®ä¿åŸºå‡†çš„å‡†ç¡®æ€§ï¼Œå¹¶å¯¹å¤šç§æœ€å…ˆè¿›çš„é€šç”¨åŠåŒ»ç–— MLLMs è¿›è¡Œäº†å¹¿æ³›çš„å•æ¬¡ä¸åºåˆ—ç¼–è¾‘å®éªŒã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨åŒ»ç–—é¢†åŸŸçš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘ä¸“é—¨é’ˆå¯¹åŒ»ç–—åœºæ™¯ç¼–è¾‘ç­–ç•¥çš„å¿…è¦æ€§ã€‚MedMKEB çš„æå‡ºä¸ºæ„å»ºæ›´å¯ä¿¡ã€é«˜æ•ˆçš„åŠ¨æ€åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†æ ‡å‡†åŒ–çš„è¯„ä¼°åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.05083v1",
      "published_date": "2025-08-07 07:09:26 UTC",
      "updated_date": "2025-08-07 07:09:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:48.584078+00:00"
    },
    {
      "arxiv_id": "2508.05081v1",
      "title": "Cognitive Duality for Adaptive Web Agents",
      "title_zh": "é¢å‘è‡ªé€‚åº” Web æ™ºèƒ½ä½“çš„è®¤çŸ¥åŒå…ƒæ€§",
      "authors": [
        "Jiarun Liu",
        "Chunhong Zhang",
        "Zheng Hu"
      ],
      "abstract": "Web navigation represents a critical and challenging domain for evaluating artificial general intelligence (AGI), demanding complex decision-making within high-entropy, dynamic environments with combinatorially explosive action spaces. Current approaches to building autonomous web agents either focus on offline imitation learning or online exploration, but rarely integrate both paradigms effectively. Inspired by the dual-process theory of human cognition, we derive a principled decomposition into fast System 1 and slow System 2 cognitive processes. This decomposition provides a unifying perspective on existing web agent methodologies, bridging the gap between offline learning of intuitive reactive behaviors and online acquisition of deliberative planning capabilities. We implement this framework in CogniWeb, a modular agent architecture that adaptively toggles between fast intuitive processing and deliberate reasoning based on task complexity. Our evaluation on WebArena demonstrates that CogniWeb achieves competitive performance (43.96% success rate) while maintaining significantly higher efficiency (75% reduction in token usage).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Web å¯¼èˆªåœ¨é€šç”¨äººå·¥æ™ºèƒ½ (AGI) é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œé’ˆå¯¹é«˜ç†µåŠ¨æ€ç¯å¢ƒå’Œç»„åˆçˆ†ç‚¸çš„åŠ¨ä½œç©ºé—´ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•åœ¨ç¦»çº¿æ¨¡ä»¿å­¦ä¹ ä¸åœ¨çº¿æ¢ç´¢æ•´åˆæ–¹é¢çš„ä¸è¶³ã€‚å—äººç±»è®¤çŸ¥çš„åŒç³»ç»Ÿç†è®º (Dual-Process Theory) å¯å‘ï¼Œç ”ç©¶è€…æå‡ºäº†å°†è®¤çŸ¥åˆ†è§£ä¸ºå¿«é€Ÿçš„ç³»ç»Ÿ 1 (System 1) å’Œæ…¢é€Ÿçš„ç³»ç»Ÿ 2 (System 2) çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œå¹¶æ®æ­¤å®ç°äº† CogniWeb æ¨¡å—åŒ–æ™ºèƒ½ä½“æ¶æ„ã€‚è¯¥æ¶æ„èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦åœ¨å¿«é€Ÿç›´è§‰å¤„ç†ä¸æ·±æ€ç†Ÿè™‘çš„æ¨ç†ä¹‹é—´è‡ªé€‚åº”åˆ‡æ¢ï¼Œæœ‰æ•ˆå¼¥åˆäº†ç›´è§‰ååº”è¡Œä¸ºçš„ç¦»çº¿å­¦ä¹ ä¸å®¡æ…è§„åˆ’èƒ½åŠ›çš„åœ¨çº¿è·å–ä¹‹é—´çš„é¸¿æ²Ÿã€‚åœ¨ WebArena åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒCogniWeb å–å¾—äº† 43.96% çš„æˆåŠŸç‡ï¼Œåœ¨å±•ç°å‡ºæå¼ºç«äº‰åŠ›çš„åŒæ—¶ï¼Œå°† Token ä½¿ç”¨é‡æ˜¾è‘—å‡å°‘äº† 75%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è®¤çŸ¥åŒé‡æ€§æ¡†æ¶åœ¨æå‡ Web æ™ºèƒ½ä½“æ€§èƒ½ä¸æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05081v1",
      "published_date": "2025-08-07 07:05:22 UTC",
      "updated_date": "2025-08-07 07:05:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:59:08.939125+00:00"
    },
    {
      "arxiv_id": "2508.05078v1",
      "title": "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning",
      "title_zh": "å¯¹é½è€Œéå‰²è£‚ï¼šé‡æ–°å®¡è§†å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ LoRA æ¶æ„",
      "authors": [
        "Jinda Liu",
        "Bo Cheng",
        "Yi Chang",
        "Yuan Wu"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šä»»åŠ¡å­¦ä¹ (MTL)åœºæ™¯ä¸‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)è¿›è¡Œäº†é‡æ–°å®¡è§†ï¼ŒæŒ‘æˆ˜äº†å½“å‰ä¸»æµ LoRA å˜ä½“é€šè¿‡å¢åŠ ç»“æ„å¤šæ ·æ€§æ¥éš”ç¦»ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„èŒƒå¼ã€‚ç ”ç©¶é¦–å…ˆå‘ç°ï¼Œå¤´é—´ç›¸ä¼¼æ€§è¾ƒé«˜çš„ç®€åŒ–å¤šå¤´æ¶æ„ä¼˜äºå¤æ‚ç³»ç»Ÿï¼Œä¸”å¢åŠ ç§©(Rank)çš„æ ‡å‡†å•é€‚é…å™¨ LoRA ä¹Ÿèƒ½è¾¾åˆ°æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…æå‡ºäº†æœ‰æ•ˆå¤šä»»åŠ¡æ³›åŒ–çš„å…³é”®åœ¨äºå­¦ä¹ ç¨³å¥çš„å…±äº«è¡¨ç¤ºè€Œéå­¤ç«‹ä»»åŠ¡ç‰¹å¾çš„å‡è®¾ã€‚ä¸ºéªŒè¯è¿™ä¸€è§‚ç‚¹ï¼Œç ”ç©¶æå‡ºäº† Align-LoRA æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼æŸå¤±å‡½æ•°åœ¨å…±äº«é€‚é…å™¨ç©ºé—´å†…å¯¹é½ä¸åŒä»»åŠ¡çš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlign-LoRA åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æ¨¡å‹é€‚é…æä¾›äº†ä¸€ç§æ›´ç®€æ´ä¸”é«˜æ•ˆçš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05078v1",
      "published_date": "2025-08-07 07:02:55 UTC",
      "updated_date": "2025-08-07 07:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:58:55.541600+00:00"
    },
    {
      "arxiv_id": "2508.05074v1",
      "title": "Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation",
      "title_zh": "Align-for-Fusionï¼šåŸºäºåŒå¯¼å‘æ‰©æ•£çš„è·¨åŸŸåºåˆ—æ¨èä¸‰é‡åå¥½åè°ƒ",
      "authors": [
        "Yongfu Zha",
        "Xinxin Dong",
        "Haokai Ma",
        "Yonghui Yang",
        "Xiaodong Wang"
      ],
      "abstract": "Personalized sequential recommendation aims to predict appropriate items for users based on their behavioral sequences. To alleviate data sparsity and interest drift issues, conventional approaches typically incorporate auxiliary behaviors from other domains via cross-domain transition. However, existing cross-domain sequential recommendation (CDSR) methods often follow an align-then-fusion paradigm that performs representation-level alignment across multiple domains and combines them mechanically for recommendation, overlooking the fine-grained fusion of domain-specific preferences. Inspired by recent advances in diffusion models (DMs) for distribution matching, we propose an align-for-fusion framework for CDSR to harmonize triple preferences via dual-oriented DMs, termed HorizonRec. Specifically, we investigate the uncertainty injection of DMs and identify stochastic noise as a key source of instability in existing DM-based recommenders. To address this, we introduce a mixed-conditioned distribution retrieval strategy that leverages distributions retrieved from users' authentic behavioral logic as semantic bridges across domains, enabling consistent multi-domain preference modeling. Furthermore, we propose a dual-oriented preference diffusion method to suppress potential noise and emphasize target-relevant interests during multi-domain user representation fusion. Extensive experiments on four CDSR datasets from two distinct platforms demonstrate the effectiveness and robustness of HorizonRec in fine-grained triple-domain preference fusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HorizonRecï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è·¨åŸŸé¡ºåºæ¨è (Cross-domain Sequential Recommendation) ä¸­åå¥½èåˆç»†ç²’åº¦ä¸è¶³é—®é¢˜çš„ align-for-fusion æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹ (Diffusion Models) çš„æ¨èç³»ç»Ÿå› éšæœºå™ªå£°å¯¼è‡´çš„ç¨³å®šæ€§é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡åè°ƒä¸‰é‡åå¥½æ¥ä¼˜åŒ–æ¨èæ€§èƒ½ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ··åˆæ¡ä»¶åˆ†å¸ƒæ£€ç´¢ (mixed-conditioned distribution retrieval) ç­–ç•¥ï¼Œåˆ©ç”¨ç”¨æˆ·çœŸå®è¡Œä¸ºé€»è¾‘æ„å»ºè·¨é¢†åŸŸçš„è¯­ä¹‰æ¡¥æ¢ï¼Œä»è€Œå®ç°ä¸€è‡´çš„å¤šåŸŸåå¥½å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒå‘åå¥½æ‰©æ•£ (dual-oriented preference diffusion) æŠ€æœ¯ï¼ŒHorizonRec èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶èåˆè¿‡ç¨‹ä¸­çš„å™ªå£°å¹¶çªå‡ºç›®æ ‡ç›¸å…³å…´è¶£ã€‚åœ¨ä¸¤ä¸ªå¹³å°çš„å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åº”å¯¹æ•°æ®ç¨€ç–å’Œå…´è¶£æ¼‚ç§»æŒ‘æˆ˜æ—¶å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼ŒæˆåŠŸå®ç°äº†ç²¾ç»†åŒ–çš„ä¸‰åŸŸåå¥½èåˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05074v1",
      "published_date": "2025-08-07 07:00:29 UTC",
      "updated_date": "2025-08-07 07:00:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:59:48.092192+00:00"
    },
    {
      "arxiv_id": "2508.10020v1",
      "title": "FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models",
      "title_zh": "FedCoTï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆé€šä¿¡è”é‚¦æ¨ç†å¢å¼º",
      "authors": [
        "Chuan Li",
        "Qianyi Zhao",
        "Fengran Mo",
        "Cen Chen"
      ],
      "abstract": "Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. This challenge is especially acute in healthcare, where decisions-spanning clinical, operational, and patient-facing contexts-demand not only accurate outputs but also interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLM fail to address this need: they optimize primarily for answer correctness while neglecting rationale quality, leaving CoT capabilities dependent on models' innate pre-training abilities. Moreover, existing methods for improving rationales typically rely on privacy-violating knowledge distillation from centralized models. Additionally, the communication overhead in traditional federated fine-tuning on LLMs remains substantial. We addresses this gap by proposing FedCoT, a novel framework specifically designed to enhance reasoning in federated settings. FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. This approach improves reasoning accuracy and robustness while providing valuable interpretability, which is particularly critical for medical applications. To manage client heterogeneity efficiently, we adopt an improved aggregation approach building upon advanced LoRA module stacking, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients. Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FedCoTï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜è”é‚¦å­¦ä¹  (Federated Learning) ç¯å¢ƒä¸‹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹åŒ»ç–—é¢†åŸŸå¯¹å†³ç­–å¯è§£é‡Šæ€§å’Œéšç§ä¿æŠ¤çš„ä¸¥è‹›è¦æ±‚ï¼ŒFedCoT å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„é“¾å¼æ€ç»´ (Chain-of-Thought) å¢å¼ºæœºåˆ¶ï¼Œé€šè¿‡æœ¬åœ°ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„å¹¶åˆ©ç”¨åˆ¤åˆ«å™¨åŠ¨æ€ç­›é€‰æœ€ä¼˜è·¯å¾„ã€‚ä¸ºäº†åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹åº”å¯¹å®¢æˆ·ç«¯å¼‚æ„æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†åŸºäº LoRA æ¨¡å—å †å çš„èšåˆæ–¹æ¡ˆï¼Œå¹¶ç»“åˆå®¢æˆ·ç«¯åˆ†ç±»å™¨æ„ŸçŸ¥ (Client Classifier-Awareness) æŠ€æœ¯å®ç°äº†æ— å™ªå£°çš„å‚æ•°èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedCoT åœ¨åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ä¸ç¨³å¥æ€§ï¼Œåœ¨æœ‰æ•ˆé™ä½é€šä¿¡å¼€é”€çš„åŒæ—¶å®Œå…¨ä¿ç•™äº†æ•°æ®éšç§ã€‚è¯¥æ–¹æ³•æˆåŠŸå¹³è¡¡äº†æ¨ç†å¢å¼ºã€è®¡ç®—é€šä¿¡æ•ˆç‡ä¸éšç§åˆè§„ä¹‹é—´çš„æŒ‘æˆ˜ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„åˆ†å¸ƒå¼æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10020v1",
      "published_date": "2025-08-07 06:50:15 UTC",
      "updated_date": "2025-08-07 06:50:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:02.095525+00:00"
    },
    {
      "arxiv_id": "2508.05068v2",
      "title": "Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks",
      "title_zh": "åŸºäºå·ç§¯ç¥ç»ç½‘ç»œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è‡ªåŠ¨å›¾åƒç€è‰²",
      "authors": [
        "Changyuan Qiu",
        "Hangrui Cao",
        "Qihan Ren",
        "Ruiyu Li",
        "Yuqing Qiu"
      ],
      "abstract": "Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20].\n  Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ (CNN) å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) å®ç°è‡ªåŠ¨å›¾åƒç€è‰² (Image Colorization) çš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç°åº¦å›¾åƒæ¢å¤å½©è‰²æ—¶ç”±äºç»´åº¦ç¼ºå¤±å¯¼è‡´çš„é«˜åº¦ç—…æ€ (Ill-posed) é—®é¢˜ã€‚ä½œè€…æŒ‡å‡ºåœºæ™¯è¯­ä¹‰å’Œè¡¨é¢çº¹ç†ä¸ºè‰²å½©åˆ†å¸ƒæä¾›äº†é‡è¦å…ˆéªŒï¼Œä¸”æµ·é‡çš„å½©è‰²å›¾åƒæ•°æ®å¯ä¸ºæ¨¡å‹è®­ç»ƒæä¾›å……è¶³æ”¯æŒã€‚é’ˆå¯¹ä¼ ç»Ÿå›å½’ (Regression) ä»»åŠ¡æ— æ³•æ•æ‰è‰²å½©é¢„æµ‹å¤šæ¨¡æ€ (Multi-modal) ç‰¹æ€§çš„å±€é™ï¼Œè¯¥ç ”ç©¶è½¬å‘æ¢ç´¢åŸºäºåˆ†ç±» (Classification) ä¸å¯¹æŠ—å­¦ä¹  (Adversarial Learning) çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚é€šè¿‡å¯¹å…ˆå‰æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§æ”¹è¿›å’Œå‚æ•°ä¼˜åŒ–ï¼Œè¯¥é¡¹ç›®æ„å»ºäº†ä¸€å¥—èƒ½å¤Ÿæ›´å‡†ç¡®è¿˜åŸåœºæ™¯è‰²å½©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æœ€ç»ˆï¼Œç ”ç©¶é€šè¿‡å¤šç»„å¯¹æ¯”å®éªŒéªŒè¯äº†æ”¹è¿›æ–¹æ¡ˆåœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨è‰²å½©æ¢å¤é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "All authors have equal authorship and equal contribution, ranked in alphabetic order. First version of this paper was completed and published in 2021",
      "pdf_url": "https://arxiv.org/pdf/2508.05068v2",
      "published_date": "2025-08-07 06:41:31 UTC",
      "updated_date": "2025-08-19 08:06:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:03.188536+00:00"
    },
    {
      "arxiv_id": "2508.09175v1",
      "title": "A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection",
      "title_zh": "ä¸€ç§åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ³¨æ„åŠ›å’Œå›¾ç¥ç»ç½‘ç»œçš„åŒå¥³è¨€è®ºæ£€æµ‹å¤šæ¨¡æ€æ¡†æ¶",
      "authors": [
        "Mohammad Zia Ur Rehman",
        "Sufyaan Zahoor",
        "Areeb Manzoor",
        "Musharaf Maqbool",
        "Nagendra Kumar"
      ],
      "abstract": "A substantial portion of offensive content on social media is directed towards women. Since the approaches for general offensive content detection face a challenge in detecting misogynistic content, it requires solutions tailored to address offensive content against women. To this end, we propose a novel multimodal framework for the detection of misogynistic and sexist content. The framework comprises three modules: the Multimodal Attention module (MANM), the Graph-based Feature Reconstruction Module (GFRM), and the Content-specific Features Learning Module (CFLM). The MANM employs adaptive gating-based multimodal context-aware attention, enabling the model to focus on relevant visual and textual information and generating contextually relevant features. The GFRM module utilizes graphs to refine features within individual modalities, while the CFLM focuses on learning text and image-specific features such as toxicity features and caption features. Additionally, we curate a set of misogynous lexicons to compute the misogyny-specific lexicon score from the text. We apply test-time augmentation in feature space to better generalize the predictions on diverse inputs. The performance of the proposed approach has been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and 13,494 samples, respectively. The proposed method demonstrates an average improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI and MMHS150K datasets, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­é’ˆå¯¹å¥³æ€§çš„åŒå¥³åŠæ€§åˆ«æ­§è§†å†…å®¹è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„å¤šæ¨¡æ€(multimodal)æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šå¤šæ¨¡æ€æ³¨æ„åŠ›æ¨¡å—(Multimodal Attention module, MANM)åˆ©ç”¨è‡ªé€‚åº”é—¨æ§æœºåˆ¶å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹è§†è§‰å’Œæ–‡æœ¬ç›¸å…³ä¿¡æ¯çš„èšç„¦èƒ½åŠ›ï¼›åŸºäºå›¾çš„ç‰¹å¾é‡æ„æ¨¡å—(Graph-based Feature Reconstruction Module, GFRM)åˆ©ç”¨å›¾ç»“æ„ç²¾ç‚¼å„æ¨¡æ€å†…éƒ¨ç‰¹å¾ï¼›å†…å®¹ç‰¹å®šç‰¹å¾å­¦ä¹ æ¨¡å—(Content-specific Features Learning Module, CFLM)åˆ™ä¸“æ³¨äºå­¦ä¹ æ¯’æ€§(toxicity)å’Œå›¾åƒæè¿°(caption)ç­‰ç‰¹å®šå†…å®¹ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸“é—¨çš„åŒå¥³è¯åº“ç”¨äºè®¡ç®—æ–‡æœ¬å¾—åˆ†ï¼Œå¹¶é‡‡ç”¨ç‰¹å¾ç©ºé—´çš„æµ‹è¯•æ—¶å¢å¼º(test-time augmentation)æŠ€æœ¯ä»¥æé«˜æ¨¡å‹å¯¹å¤šæ ·åŒ–è¾“å…¥çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨MAMIå’ŒMMHS150Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Macro-F1è¯„ä»·æŒ‡æ ‡ä¸Šè¾ƒç°æœ‰æ¨¡å‹åˆ†åˆ«å¹³å‡æå‡äº†10.17%å’Œ8.88%ï¼Œæœ‰æ•ˆè¯æ˜äº†è¯¥æ¡†æ¶åœ¨åº”å¯¹ç¤¾äº¤åª’ä½“åŒå¥³å†…å®¹æ£€æµ‹æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in Information Processing & Management",
      "pdf_url": "https://arxiv.org/pdf/2508.09175v1",
      "published_date": "2025-08-07 06:41:17 UTC",
      "updated_date": "2025-08-07 06:41:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:59:59.487428+00:00"
    },
    {
      "arxiv_id": "2508.06571v3",
      "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
      "title_zh": "IRL-VLAï¼šé€šè¿‡å¥–åŠ±ä¸–ç•Œæ¨¡å‹è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥",
      "authors": [
        "Anqing Jiang",
        "Yu Gao",
        "Yiru Wang",
        "Zhigang Sun",
        "Shuo Wang",
        "Yuwen Heng",
        "Hao Sun",
        "Shichen Tang",
        "Lijuan Zhu",
        "Jinhao Chai",
        "Jijun Wang",
        "Zichong Gu",
        "Hao Jiang",
        "Li Sun"
      ],
      "abstract": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­é¢ä¸´çš„å¼€ç¯æ¨¡ä»¿å­¦ä¹ (Imitation Learning)å±€é™æ€§ï¼Œä»¥åŠé—­ç¯è®­ç»ƒä¸­é«˜ä¿çœŸä¼ æ„Ÿå™¨ä»¿çœŸçš„é¢†åŸŸå·®è·ä¸è®¡ç®—ä½æ•ˆç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº† IRL-VLA æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºé€†å¼ºåŒ–å­¦ä¹  (Inverse Reinforcement Learning) å¥–åŠ±ä¸–ç•Œæ¨¡å‹çš„é—­ç¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ç»“åˆè‡ªå»ºçš„ VLA æ¶æ„ã€‚ç ”ç©¶é‡‡ç”¨ä¸‰é˜¶æ®µèŒƒå¼ï¼Œé¦–å…ˆé€šè¿‡æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒ VLA ç­–ç•¥ï¼Œéšåæ„å»ºè½»é‡åŒ–çš„å¥–åŠ±ä¸–ç•Œæ¨¡å‹ä»¥å®ç°é«˜æ•ˆçš„é—­ç¯å¥–åŠ±è®¡ç®—ï¼Œæœ€åé€šè¿‡å¥–åŠ±ä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„ PPO (Proximal Policy Optimization) å¼ºåŒ–å­¦ä¹ ï¼Œæœ‰æ•ˆå¹³è¡¡äº†å®‰å…¨äº‹æ•…ã€é©¾é©¶èˆ’é€‚åº¦ä¸äº¤é€šæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ NAVSIM v2 ç«¯åˆ°ç«¯é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆæ°´å¹³ (State-of-the-art)ï¼Œå¹¶è·å¾— CVPR2025 è‡ªåŠ¨é©¾é©¶å¤§æŒ‘æˆ˜èµ›äºšå†›ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°çš„å¥–åŠ±ä¸–ç•Œæ¨¡å‹æœºåˆ¶ï¼Œä¸ºåŠ é€Ÿè‡ªåŠ¨é©¾é©¶ä¸­ VLA æ¨¡å‹çš„é—­ç¯ç ”ç©¶æä¾›äº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pagres, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.06571v3",
      "published_date": "2025-08-07 06:30:05 UTC",
      "updated_date": "2025-08-15 05:19:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T09:59:56.886825+00:00"
    },
    {
      "arxiv_id": "2508.05059v1",
      "title": "Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting",
      "title_zh": "ä»é—å¿˜ä¸­å­¦ä¹ ï¼šåŸºäºé—å¿˜å›æº¯çš„çŸ¥è¯†æº¢å‡ºæƒé‡é¢„æµ‹",
      "authors": [
        "Jinhyeok Jang",
        "Jaehong Kim",
        "Jung Uk Kim"
      ],
      "abstract": "Pre-trained weights have become a cornerstone of modern deep learning, enabling efficient knowledge transfer and improving downstream task performance, especially in data-scarce scenarios. However, a fundamental question remains: how can we obtain better pre-trained weights that encapsulate more knowledge beyond the given dataset? In this work, we introduce \\textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that leverages structured forgetting and its inversion to synthesize knowledge-enriched weights. Our key insight is that sequential fine-tuning on progressively downsized datasets induces a structured forgetting process, which can be modeled and reversed to recover knowledge as if trained on a larger dataset. We construct a dataset of weight transitions governed by this controlled forgetting and employ meta-learning to model weight prediction effectively. Specifically, our \\textbf{KNowledge Overflowed Weights Nowcaster (KNOWN)} acts as a hyper-model that learns the general evolution of weights and predicts enhanced weights with improved generalization. Extensive experiments across diverse datasets and architectures demonstrate that KNOW prediction consistently outperforms NaÃ¯ve fine-tuning and simple weight prediction, leading to superior downstream performance. Our work provides a new perspective on reinterpreting forgetting dynamics to push the limits of knowledge transfer in deep learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¦‚ä½•è·å–è¶…å‡ºç»™å®šæ•°æ®é›†å®¹é‡çš„é¢„è®­ç»ƒæƒé‡è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œæå‡ºäº†åä¸ºKNowledge Overflowed Weights (KNOW) é¢„æµ‹çš„æ–°ç­–ç•¥ï¼Œåˆ©ç”¨ç»“æ„åŒ–é—å¿˜åŠå…¶é€†è¿‡ç¨‹åˆæˆçŸ¥è¯†å¢å¼ºçš„æƒé‡ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°ï¼Œåœ¨é€æ¸ç¼©å°çš„è®­ç»ƒé›†ä¸Šè¿›è¡Œé¡ºåºå¾®è°ƒ(Fine-tuning)ä¼šè¯±å¯¼ä¸€ä¸ªå¯å»ºæ¨¡çš„ç»“æ„åŒ–é—å¿˜è¿‡ç¨‹ï¼Œé€šè¿‡åè½¬(Retrodiction)è¯¥è¿‡ç¨‹å¯ä»¥æ¢å¤å¦‚åŒåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒæ‰€å¾—çš„çŸ¥è¯†ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è€…åˆ©ç”¨å…ƒå­¦ä¹ (Meta-learning)æ„å»ºäº†åä¸ºKNowledge Overflowed Weights Nowcaster (KNOWN) çš„è¶…æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ æƒé‡çš„é€šç”¨æ¼”åŒ–è§„å¾‹å¹¶é¢„æµ‹å…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„å¢å¼ºæƒé‡ã€‚åœ¨å¤šç§æ¶æ„å’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒKNOWé¢„æµ‹çš„æ•ˆæœä¸€è‡´ä¼˜äºæœ´ç´ å¾®è°ƒ(NaÃ¯ve fine-tuning)å’Œå¸¸è§„æƒé‡é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ºé‡æ–°é˜é‡Šé—å¿˜åŠ¨æ€ä»¥çªç ´æ·±åº¦å­¦ä¹ ä¸­çš„çŸ¥è¯†è¿ç§»(Knowledge transfer)é™åˆ¶æä¾›äº†å…¨æ–°çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05059v1",
      "published_date": "2025-08-07 06:23:07 UTC",
      "updated_date": "2025-08-07 06:23:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:11.693834+00:00"
    },
    {
      "arxiv_id": "2508.05045v1",
      "title": "Human-AI Schema Discovery and Application for Creative Problem Solving",
      "title_zh": "é¢å‘åˆ›é€ æ€§é—®é¢˜è§£å†³çš„äººæœºå›¾å¼å‘ç°ä¸åº”ç”¨",
      "authors": [
        "Sitong Wang"
      ],
      "abstract": "Humans often rely on underlying structural patterns-schemas-to create, whether by writing stories, designing software, or composing music. Schemas help organize ideas and guide exploration, but they are often difficult to discover and apply, especially in complex or unfamiliar domains. My Ph.D. research develops a framework for human-AI schema discovery and application to support creative problem solving. I design systems that support users in sensemaking over examples to abstract schemas, and in operationalizing schemas into human-AI co-creative workflows for application. This research offers insights into how schema-guided interaction can make implicit knowledge more accessible and actionable, advancing more transparent and collaborative human-AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººç±»åœ¨åˆ›ä½œå’Œè§£å†³å¤æ‚é—®é¢˜æ—¶éš¾ä»¥å‘ç°å¹¶åº”ç”¨åº•å±‚ç»“æ„åŒ–æ¨¡å¼ schemas çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ª Human-AI Schema Discovery and Application æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ä¸“é—¨çš„ç³»ç»Ÿï¼Œæ”¯æŒç”¨æˆ·åœ¨å¯¹å¤§é‡ç¤ºä¾‹è¿›è¡Œæ„ä¹‰æ„å»º (sensemaking) çš„è¿‡ç¨‹ä¸­æŠ½è±¡å‡º schemasï¼Œå¹¶è¿›ä¸€æ­¥å°†è¿™äº›æ¨¡å¼è½¬åŒ–ä¸ºäººæœºååŒåˆ›ä½œå·¥ä½œæµ (human-AI co-creative workflows) æŠ•å…¥åº”ç”¨ã€‚è¿™ç§ schema å¼•å¯¼çš„äº¤äº’æ¨¡å¼æœ‰æ•ˆæå‡äº†éšæ€§çŸ¥è¯†çš„å¯è®¿é—®æ€§ä¸å¯æ“ä½œæ€§ï¼Œä¸ºåˆ›æ„é—®é¢˜çš„è§£å†³æä¾›äº†ç³»ç»ŸåŒ–æ”¯æŒã€‚è¯¥ç ”ç©¶ä¸ä»…æ·±åŒ–äº†å¯¹äººç±»è®¤çŸ¥æ¨¡å¼ä¸ AI åä½œçš„ç†è§£ï¼Œè¿˜ä¸ºå¼€å‘æ›´é€æ˜ã€æ›´å…·åä½œæ€§çš„ Human-AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05045v1",
      "published_date": "2025-08-07 05:55:52 UTC",
      "updated_date": "2025-08-07 05:55:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:22.493518+00:00"
    },
    {
      "arxiv_id": "2508.06569v1",
      "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop",
      "title_zh": "å®ç°å¶ç„¶å‘ç°ï¼šç»“åˆâ€œç†è®ºåœ¨ç¯â€çš„å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½å·¥ä½œæµï¼Œç”¨äºå¢å¼ºææ–™è¡¨å¾",
      "authors": [
        "Lance Yao",
        "Suman Samantray",
        "Ayana Ghosh",
        "Kevin Roccapriore",
        "Libor Kovarik",
        "Sarah Allec",
        "Maxim Ziatdinov"
      ],
      "abstract": "The history of science is punctuated by serendipitous discoveries, where unexpected observations, rather than targeted hypotheses, opened new fields of inquiry. While modern autonomous laboratories excel at accelerating hypothesis testing, their optimization for efficiency risks overlooking these crucial, unplanned findings. To address this gap, we introduce SciLink, an open-source, multi-agent artificial intelligence framework designed to operationalize serendipity in materials research by creating a direct, automated link between experimental observation, novelty assessment, and theoretical simulations. The framework employs a hybrid AI strategy where specialized machine learning models perform quantitative analysis of experimental data, while large language models handle higher-level reasoning. These agents autonomously convert raw data from materials characterization techniques into falsifiable scientific claims, which are then quantitatively scored for novelty against the published literature. We demonstrate the framework's versatility across diverse research scenarios, showcasing its application to atomic-resolution and hyperspectral data, its capacity to integrate real-time human expert guidance, and its ability to close the research loop by proposing targeted follow-up experiments. By systematically analyzing all observations and contextualizing them, SciLink provides a practical framework for AI-driven materials research that not only enhances efficiency but also actively cultivates an environment ripe for serendipitous discoveries, thereby bridging the gap between automated experimentation and open-ended scientific exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SciLinkï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(multi-agent AI)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å»ºç«‹å®éªŒè§‚æµ‹ã€æ–°é¢–æ€§è¯„ä¼°ä¸ç†è®ºæ¨¡æ‹Ÿä¹‹é—´çš„è‡ªåŠ¨åŒ–è”ç³»ï¼Œåœ¨ææ–™ç ”ç©¶ä¸­å®ç°å¶ç„¶æ€§å‘ç°(serendipity)çš„ä¸šåŠ¡åŒ–ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆ AI ç­–ç•¥ï¼Œç»“åˆæœºå™¨å­¦ä¹ (machine learning)æ¨¡å‹è¿›è¡Œå®šé‡åˆ†æä¸å¤§è¯­è¨€æ¨¡å‹(large language models)è¿›è¡Œé«˜å±‚æ¬¡æ¨ç†ï¼Œèƒ½è‡ªä¸»å°†åŸå§‹ææ–™è¡¨å¾æ•°æ®è½¬åŒ–ä¸ºå¯è¯ä¼ªçš„ç§‘å­¦ä¸»å¼ ã€‚ç ”ç©¶è¯æ˜äº† SciLink åœ¨å¤„ç†åŸå­åˆ†è¾¨ç‡å’Œè¶…å…‰è°±æ•°æ®æ–¹é¢çš„é€šç”¨æ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶é›†æˆäººç±»ä¸“å®¶æŒ‡å¯¼åŠé€šè¿‡åç»­å®éªŒæè®®é—­åˆç ”ç©¶ç¯è·¯çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ‰€æœ‰è§‚æµ‹ç»“æœè¿›è¡Œç³»ç»ŸåŒ–åˆ†æä¸æƒ…å¢ƒåŒ–å¤„ç†ï¼Œè¯¥æ¡†æ¶ä¸ä»…æå‡äº†ç ”å‘æ•ˆç‡ï¼Œè¿˜ä¸ºè‡ªåŠ¨åŒ–å®éªŒä¸å¼€æ”¾å¼ç§‘å­¦æ¢ç´¢æ­å»ºäº†æ¡¥æ¢ï¼Œä¸»åŠ¨è¥é€ äº†æœ‰åˆ©äºäº§ç”Ÿé‡å¤§ç§‘å­¦çªç ´çš„ç¯å¢ƒã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06569v1",
      "published_date": "2025-08-07 04:59:17 UTC",
      "updated_date": "2025-08-07 04:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:29.959271+00:00"
    },
    {
      "arxiv_id": "2508.05028v3",
      "title": "Evaluation of Finetuned LLMs in AMR Parsing",
      "title_zh": "å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹åœ¨ AMR è§£æä¸­çš„è¯„ä¼°",
      "authors": [
        "Shu Han Ho"
      ],
      "abstract": "AMR (Abstract Meaning Representation) is a semantic formalism that encodes sentence meaning as rooted, directed, acyclic graphs, where nodes represent concepts and edges denote semantic relations. Finetuning decoder only Large Language Models (LLMs) represent a promising novel straightfoward direction for AMR parsing. This paper presents a comprehensive evaluation of finetuning four distinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that straightfoward finetuning of decoder only LLMs can achieve comparable performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2 demonstrates competitive performance against SOTA AMR parsers given a straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5 excels in structural validity.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¾®è°ƒ(finetuning)ä»…è§£ç å™¨(decoder only)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨AMR (Abstract Meaning Representation)è§£æä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜å¯¹Phi 3.5ã€Gemma 2ã€LLaMA 3.2å’ŒDeepSeek R1 LLaMA Distilledå››ç§æ¶æ„è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨äº†LDC2020T02 Gold AMR3.0æµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç›´æ¥çš„å¾®è°ƒæ–¹æ³•èƒ½å¤Ÿå®ç°ä¸å¤æ‚çš„SOTA (State of the Art)è§£æå™¨ç›¸å½“çš„æ€§èƒ½ã€‚å…¶ä¸­ï¼ŒLLaMA 3.2åœ¨å…¨é‡æµ‹è¯•é›†ä¸Šå–å¾—äº†0.804çš„SMATCH F1åˆ†æ•°ï¼Œè¡¨ç°ä¸APT + Silver (IBM)æŒå¹³ï¼Œå±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œåˆ†ææ˜¾ç¤ºLLaMA 3.2åœ¨è¯­ä¹‰æ€§èƒ½ä¸Šå¤„äºé¢†å…ˆåœ°ä½ï¼Œè€ŒPhi 3.5åˆ™åœ¨ç»“æ„æœ‰æ•ˆæ€§(structural validity)æ–¹é¢è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 32 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05028v3",
      "published_date": "2025-08-07 04:43:47 UTC",
      "updated_date": "2025-08-18 01:10:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:24.392792+00:00"
    },
    {
      "arxiv_id": "2508.05023v1",
      "title": "Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning",
      "title_zh": "åŸºäºç»“æ„ç†µæœ€å°åŒ–åˆ’åˆ†çš„å¯¹è¯å±æ€§çº§æƒ…æ„Ÿå››å…ƒç»„æŠ½å–",
      "authors": [
        "Kun Peng",
        "Cong Cao",
        "Hao Peng",
        "Zhifeng Hao",
        "Lei Jiang",
        "Kongjing Gu",
        "Yanbing Liu",
        "Philip S. Yu"
      ],
      "abstract": "Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to extract all target-aspect-opinion-sentiment quadruples from a given multi-round, multi-participant dialogue. Existing methods typically learn word relations across entire dialogues, assuming a uniform distribution of sentiment elements. However, we find that dialogues often contain multiple semantically independent sub-dialogues without clear dependencies between them. Therefore, learning word relationships across the entire dialogue inevitably introduces additional noise into the extraction process. To address this, our method focuses on partitioning dialogues into semantically independent sub-dialogues. Achieving completeness while minimizing these sub-dialogues presents a significant challenge. Simply partitioning based on reply relationships is ineffective. Instead, we propose utilizing a structural entropy minimization algorithm to partition the dialogues. This approach aims to preserve relevant utterances while distinguishing irrelevant ones as much as possible. Furthermore, we introduce a two-step framework for quadruple extraction: first extracting individual sentiment elements at the utterance level, then matching quadruples at the sub-dialogue level. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in DiaASQ with much lower computational costs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯æƒ…æ„Ÿå››å…ƒç»„æŠ½å–(DiaASQ)ä»»åŠ¡ï¼Œæ—¨åœ¨ä»å¤šè½®ã€å¤šå‚ä¸è€…çš„å¯¹è¯ä¸­æå–â€œç›®æ ‡-å±æ€§-è§‚ç‚¹-æƒ…æ„Ÿâ€å››å…ƒç»„ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å…¨å¯¹è¯èŒƒå›´å†…å­¦ä¹ è¯å…³ç³»è€Œå¼•å…¥æ— å…³å™ªå£°çš„é—®é¢˜ï¼Œä½œè€…å‘ç°å¯¹è¯é€šå¸¸ç”±å¤šä¸ªè¯­ä¹‰ç‹¬ç«‹çš„å­å¯¹è¯ç»„æˆï¼Œä¸”ä¼ ç»Ÿçš„å›å¤å…³ç³»åˆ’åˆ†æ³•æ•ˆæœæœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºåˆ©ç”¨ç»“æ„ç†µæœ€å°åŒ–(structural entropy minimization)ç®—æ³•å¯¹å¯¹è¯è¿›è¡Œç§‘å­¦åˆ’åˆ†ï¼Œä»¥åœ¨ä¿ç•™ç›¸å…³è¯è¯­çš„åŒæ—¶æœ‰æ•ˆåŒºåˆ†æ— å…³èƒŒæ™¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå…ˆåœ¨è¯è¯­å±‚é¢æå–ç‹¬ç«‹æƒ…æ„Ÿå…ƒç´ ï¼Œå†åœ¨å­å¯¹è¯å±‚é¢è¿›è¡Œå››å…ƒç»„åŒ¹é…ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨DiaASQä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by CIKM2025",
      "pdf_url": "https://arxiv.org/pdf/2508.05023v1",
      "published_date": "2025-08-07 04:22:17 UTC",
      "updated_date": "2025-08-07 04:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:27.491463+00:00"
    },
    {
      "arxiv_id": "2508.05019v1",
      "title": "Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes",
      "title_zh": "Skin-SOAPï¼šä¸€ç§ç”¨äºç”Ÿæˆç»“æ„åŒ– SOAP è®°å½•çš„å¼±ç›‘ç£æ¡†æ¶",
      "authors": [
        "Sadia Kamal",
        "Tim Oates",
        "Joy Wan"
      ],
      "abstract": "Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç™Œä¸´åºŠè®°å½•ä¸­æ‰‹åŠ¨ç”Ÿæˆ SOAP (Subjective, Objective, Assessment, and Plan) ç¬”è®°è€—æ—¶è€—åŠ›ä¸”æ˜“å¯¼è‡´åŒ»ç”ŸèŒä¸šå€¦æ€ çš„é—®é¢˜ï¼Œæå‡ºäº† Skin-SOAP è¿™ä¸€å¼±ç›‘ç£ (weakly supervised) å¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç—…ç¶å›¾åƒå’Œç¨€ç–çš„ä¸´åºŠæ–‡æœ¬è¾“å…¥ç”Ÿæˆç»“æ„åŒ–çš„ä¸´åºŠç¬”è®°ï¼Œæ˜¾è‘—é™ä½äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œåœ¨å‡è½»ä¸´åºŠåŒ»ç”Ÿè´Ÿæ‹…çš„åŒæ—¶å®ç°äº†å…·æœ‰ä¸´åºŠä¾æ®çš„è‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSkin-SOAP åœ¨å…³é”®ä¸´åºŠç›¸å…³æ€§æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¸ GPT-4oã€Claude å’Œ DeepSeek Janus Pro ç­‰å…ˆè¿›æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† MedConceptEval å’Œ Clinical Coherence Score (CCS) ä¸¤ç§æ–°æŒ‡æ ‡ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°ç”Ÿæˆå†…å®¹ä¸ä¸“å®¶åŒ»å­¦æ¦‚å¿µçš„è¯­ä¹‰å¯¹é½åº¦ä»¥åŠä¸è¾“å…¥ç‰¹å¾çš„ä¸´åºŠä¸€è‡´æ€§ã€‚è¯¥å·¥ä½œä¸ºå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„çš®è‚¤ç—…ä¸´åºŠè¾…åŠ©æ•™å­¦ä¸æ–‡æ¡£è‡ªåŠ¨åŒ–å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IJCAI 2025 Workshops. arXiv admin note: substantial text overlap with arXiv:2506.10328",
      "pdf_url": "https://arxiv.org/pdf/2508.05019v1",
      "published_date": "2025-08-07 04:12:43 UTC",
      "updated_date": "2025-08-07 04:12:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:47.048842+00:00"
    },
    {
      "arxiv_id": "2508.05015v1",
      "title": "SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models",
      "title_zh": "SPaRFTï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è‡ªæ­¥å¼ºåŒ–å¾®è°ƒ",
      "authors": [
        "Dai Do",
        "Manh Nguyen",
        "Svetha Venkatesh",
        "Hung Le"
      ],
      "abstract": "Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \\textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \\emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \\emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to \\(100\\times\\) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨åˆ©ç”¨ Reinforcement Learning (RL) è¿›è¡Œæ¨ç†èƒ½åŠ›å¾®è°ƒæ—¶é¢ä¸´çš„é«˜æ˜‚æ•°æ®ä¸ç®—åŠ›æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº† SPaRFT (Self-Paced Reinforcement Fine-Tuning) è‡ªæ­¥å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è¢«è®­ç»ƒæ¨¡å‹çš„å®æ—¶èƒ½åŠ›ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©å’Œè®­ç»ƒæ—¶æœºæ¥å®ç°é«˜æ•ˆå­¦ä¹ ã€‚å…·ä½“è€Œè¨€ï¼ŒSPaRFT é¦–å…ˆé‡‡ç”¨ Cluster-based data reduction æŠ€æœ¯ï¼ŒæŒ‰è¯­ä¹‰å’Œéš¾åº¦å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ’åˆ†å¹¶æå–ç²¾ç®€çš„å¤šæ ·åŒ–å­é›†ä»¥å‡å°‘å†—ä½™ï¼›éšååˆ©ç”¨ Multi-armed bandit ç®—æ³•å°†æ•°æ®ç°‡è§†ä¸ºå¯é€‰åŠ¨ä½œï¼Œæ ¹æ®æ¨¡å‹å½“å‰çš„æ€§èƒ½è¡¨ç°åŠ¨æ€åˆ†é…è®­ç»ƒæ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPaRFT åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…éœ€å°‘è‡³ç™¾åˆ†ä¹‹ä¸€çš„æ ·æœ¬é‡ä¾¿èƒ½å–å¾—ä¸ç°æœ‰æœ€å…ˆè¿›åŸºå‡†æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®ç‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†æ•°æ®èšç±»å’Œè‡ªé€‚åº”é€‰æ‹©çš„å…³é”®ä½œç”¨ï¼Œè¯æ˜äº†æ€§èƒ½é©±åŠ¨çš„è®­ç»ƒè¯¾ç¨‹å¯ä»¥åˆ©ç”¨æå°‘çš„èµ„æºæœ‰æ•ˆè§£é” LLMs çš„å¼ºæ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05015v1",
      "published_date": "2025-08-07 03:50:48 UTC",
      "updated_date": "2025-08-07 03:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:49.554157+00:00"
    },
    {
      "arxiv_id": "2508.05012v1",
      "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
      "title_zh": "å°†æç¤ºè¯æå‡ä¸ºè‡ªé€‚åº”å¤§è¯­è¨€æ¨¡å‹æµæ°´çº¿çš„ä¸€ç­‰å…¬æ°‘",
      "authors": [
        "Ugur Cetintemel",
        "Shu Chen",
        "Alexander W. Lee",
        "Deepti Raghavan"
      ],
      "abstract": "Modern LLM pipelines increasingly resemble data-centric systems: they retrieve external context, compose intermediate outputs, validate results, and adapt based on runtime feedback. Yet, the central element guiding this process -- the prompt -- remains a brittle, opaque string, disconnected from the surrounding dataflow. This disconnect limits reuse, optimization, and runtime control.\n  In this paper, we describe our vision and an initial design for SPEAR, a language and runtime that fills this prompt management gap by making prompts structured, adaptive, and first-class components of the execution model. SPEAR enables (1) runtime prompt refinement -- modifying prompts dynamically in response to execution-time signals such as confidence, latency, or missing context; and (2) structured prompt management -- organizing prompt fragments into versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and adapted within a pipeline. It supports multiple refinement modes (manual, assisted, and automatic), giving developers a balance between control and automation. By treating prompt logic as structured data, SPEAR enables optimizations such as operator fusion, prefix caching, and view reuse. Preliminary experiments quantify the behavior of different refinement modes compared to static prompts and agentic retries, as well as the impact of prompt-level optimizations such as operator fusion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ LLM pipelines ä¸­ Prompt å¾€å¾€ä½œä¸ºè„†å¼±ä¸”ä¸é€æ˜çš„å­—ç¬¦ä¸²ï¼Œå¯¼è‡´å…¶ä¸æ•´ä½“æ•°æ®æµè„±èŠ‚å¹¶é™åˆ¶é‡ç”¨ä¸ä¼˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº† SPEAR è¯­è¨€ä¸è¿è¡Œæ—¶æ¡†æ¶ã€‚SPEAR å°† Prompt è§†ä¸ºç»“æ„åŒ–ã€å¯è‡ªé€‚åº”çš„æ‰§è¡Œæ¨¡å‹ä¸€ç­‰å…¬æ°‘ï¼Œæ”¯æŒæ ¹æ®æ‰§è¡Œæ—¶çš„ç½®ä¿¡åº¦ã€å»¶è¿Ÿæˆ–ç¼ºå¤±ä¸Šä¸‹æ–‡ç­‰ä¿¡å·è¿›è¡ŒåŠ¨æ€çš„ Prompt Refinementã€‚è¯¥ç³»ç»Ÿå®šä¹‰äº†ä¸€å¥— Prompt Algebra ç”¨ä»¥è§„èŒƒ Pipeline ä¸­ Prompt çš„æ„å»ºä¸é€‚é…é€»è¾‘ï¼Œå¹¶æä¾›ç‰ˆæœ¬åŒ–è§†å›¾å’Œæ—¥å¿—å†…çœç­‰ç»“æ„åŒ–ç®¡ç†åŠŸèƒ½ã€‚SPEAR æ”¯æŒæ‰‹åŠ¨ã€è¾…åŠ©å’Œè‡ªåŠ¨ç­‰å¤šç§ä¼˜åŒ–æ¨¡å¼ï¼Œå…è®¸å¼€å‘è€…åœ¨æ§åˆ¶åŠ›ä¸è‡ªåŠ¨åŒ–ä¹‹é—´å–å¾—å¹³è¡¡ã€‚é€šè¿‡å°† Prompt é€»è¾‘æ•°æ®åŒ–ï¼ŒSPEAR èƒ½å¤Ÿå®ç° Operator Fusionã€Prefix Caching å’Œè§†å›¾é‡ç”¨ç­‰ç³»ç»Ÿçº§ä¼˜åŒ–ã€‚åˆæ­¥å®éªŒéªŒè¯äº† SPEAR çš„ç»†åŒ–æ¨¡å¼ç›¸è¾ƒäºé™æ€ Prompt å’Œ Agentic Retries çš„ä¼˜è¶Šæ€§ï¼Œå¹¶é‡åŒ–äº† Prompt çº§åˆ«ä¼˜åŒ–æŠ€æœ¯å¯¹ç³»ç»Ÿæ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05012v1",
      "published_date": "2025-08-07 03:49:56 UTC",
      "updated_date": "2025-08-07 03:49:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:01.690711+00:00"
    },
    {
      "arxiv_id": "2508.05011v1",
      "title": "Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation",
      "title_zh": "è¿ˆå‘æ— å¹»è§‰éŸ³ä¹ï¼šä¸€ç§ç”¨äºå¯é æ­Œæ›²ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ åå¥½ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Huaicheng Zhang",
        "Wei Tan",
        "Guangzheng Li",
        "Yixuan Zhang",
        "Hangting Chen",
        "Shun Lei",
        "Chenyu Yang",
        "Zhiyong Wu",
        "Shuai Wang",
        "Qijun Huang",
        "Dong Yu"
      ],
      "abstract": "Recent advances in audio-based generative language models have accelerated AI-driven lyric-to-song generation. However, these models frequently suffer from content hallucination, producing outputs misaligned with the input lyrics and undermining musical coherence. Current supervised fine-tuning (SFT) approaches, limited by passive label-fitting, exhibit constrained self-improvement and poor hallucination mitigation. To address this core challenge, we propose a novel reinforcement learning (RL) framework leveraging preference optimization for hallucination control. Our key contributions include: (1) Developing a robust hallucination preference dataset constructed via phoneme error rate (PER) computation and rule-based filtering to capture alignment with human expectations; (2) Implementing and evaluating three distinct preference optimization strategies within the RL framework: Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO). DPO operates off-policy to enhance positive token likelihood, achieving a significant 7.4% PER reduction. PPO and GRPO employ an on-policy approach, training a PER-based reward model to iteratively optimize sequences via reward maximization and KL-regularization, yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective and subjective evaluations confirm that our methods effectively suppress hallucinations while preserving musical quality. Crucially, this work presents a systematic, RL-based solution to hallucination control in lyric-to-song generation. The framework's transferability also unlocks potential for music style adherence and musicality enhancement, opening new avenues for future generative song research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘ç”Ÿæˆè¯­è¨€æ¨¡å‹åœ¨æ­Œè¯è½¬æ­Œæ›²(lyric-to-song generation)è¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰(hallucination)é—®é¢˜ï¼Œå³ç”Ÿæˆå†…å®¹ä¸æ­Œè¯ä¸åŒ¹é…ä¸”ç¼ºä¹éŸ³ä¹è¿è´¯æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åå¥½ä¼˜åŒ–æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç›‘ç£å¾®è°ƒ(SFT)åœ¨ç¼“è§£å¹»è§‰æ–¹é¢çš„å±€é™æ€§ï¼Œç ”ç©¶è€…é€šè¿‡éŸ³ç´ é”™è¯¯ç‡(Phoneme Error Rate, PER)è®¡ç®—å’Œè§„åˆ™è¿‡æ»¤æ„å»ºäº†ä¸€ä¸ªé²æ£’çš„åå¥½æ•°æ®é›†ã€‚è¯¥æ¡†æ¶è¯„ä¼°äº†ä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼šç›´æ¥åå¥½ä¼˜åŒ–(DPO)ã€è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ï¼Œå…¶ä¸­ DPO é€šè¿‡æå‡æ­£å‘ token æ¦‚ç‡å®ç°äº† 7.4% çš„ PER é™å¹…ï¼Œè€Œ PPO å’Œ GRPO åˆ™é€šè¿‡å¥–åŠ±æ¨¡å‹è¿­ä»£ä¼˜åŒ–åˆ†åˆ«å®ç°äº† 4.9% å’Œ 4.7% çš„é™å¹…ã€‚å®¢è§‚ä¸ä¸»è§‚è¯„ä¼°å‡è¯å®ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæŠ‘åˆ¶å¹»è§‰çš„åŒæ—¶ä¿ç•™äº†é«˜è´¨é‡çš„éŸ³ä¹æ•ˆæœã€‚è¯¥å·¥ä½œä¸ºæ­Œè¯è½¬æ­Œæ›²ä»»åŠ¡ä¸­çš„å¹»è§‰æ§åˆ¶æä¾›äº†ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¡†æ¶çš„å¯è¿ç§»æ€§ä¹Ÿä¸ºéŸ³ä¹é£æ ¼éµå¾ªåŠéŸ³ä¹æ€§å¢å¼ºç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05011v1",
      "published_date": "2025-08-07 03:49:18 UTC",
      "updated_date": "2025-08-07 03:49:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:56.655662+00:00"
    },
    {
      "arxiv_id": "2508.05705v1",
      "title": "A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes",
      "title_zh": "ç”¨äºå¤ç°1å‹ç³–å°¿ç—…è¡€ç³–åŠ¨æ€çš„ç”Ÿç†çº¦æŸç¥ç»ç½‘ç»œæ•°å­—å­ªç”Ÿæ¡†æ¶",
      "authors": [
        "Valentina Roquemen-Echeverri",
        "Taisa Kushner",
        "Peter G. Jacobs",
        "Clara Mosquera-Lopez"
      ],
      "abstract": "Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is critical for developing personalized treatments and supporting data-driven clinical decisions. Existing models often miss key physiological aspects and are difficult to individualize. Here, we introduce physiologically-constrained neural network (NN) digital twins to simulate glucose dynamics in T1D. To ensure interpretability and physiological consistency, we first build a population-level NN state-space model aligned with a set of ordinary differential equations (ODEs) describing glucose regulation. This model is formally verified to conform to known T1D dynamics. Digital twins are then created by augmenting the population model with individual-specific models, which include personal data, such as glucose management and contextual information, capturing both inter- and intra-individual variability. We validate our approach using real-world data from the T1D Exercise Initiative study. Two weeks of data per participant were split into 5-hour sequences and simulated glucose profiles were compared to observed ones. Clinically relevant outcomes were used to assess similarity via paired equivalence t-tests with predefined clinical equivalence margins. Across 394 digital twins, glucose outcomes were equivalent between simulated and observed data: time in range (70-180 mg/dL) was 75.1$\\pm$21.2% (simulated) vs. 74.4$\\pm$15.4% (real; P<0.001); time below range (<70 mg/dL) 2.5$\\pm$5.2% vs. 3.0$\\pm$3.3% (P=0.022); and time above range (>180 mg/dL) 22.4$\\pm$22.0% vs. 22.6$\\pm$15.9% (P<0.001). Our framework can incorporate unmodeled factors like sleep and activity while preserving key dynamics. This approach enables personalized in silico testing of treatments, supports insulin optimization, and integrates physics-based and data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”Ÿç†çº¦æŸç¥ç»ç½‘ç»œ(Physiologically-constrained Neural Network)æ•°å­—å­ªç”Ÿæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿ1å‹ç³–å°¿ç—…(T1D)æ‚£è€…çš„è¡€ç³–åŠ¨æ€ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºä¸è¡€ç³–è°ƒèŠ‚å¸¸å¾®åˆ†æ–¹ç¨‹(ODEs)å¯¹é½çš„ç¾¤ä½“çº§ç¥ç»ç½‘ç»œçŠ¶æ€ç©ºé—´æ¨¡å‹(NN State-Space Model)ï¼Œç¡®ä¿äº†æ¨¡æ‹Ÿè¿‡ç¨‹çš„ç”Ÿç†ä¸€è‡´æ€§ä¸å¯è§£é‡Šæ€§ã€‚é€šè¿‡æ•´åˆä¸ªäººè¡€ç³–ç®¡ç†æ•°æ®ã€ç¡çœ åŠä½“åŠ›æ´»åŠ¨ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ•æ‰ä¸ªä½“é—´ä¸ä¸ªä½“å†…çš„ç”Ÿç†å˜å¼‚æ€§ã€‚åˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®å¯¹394ä¸ªæ•°å­—å­ªç”Ÿæ¨¡å‹è¿›è¡ŒéªŒè¯çš„ç»“æœè¡¨æ˜ï¼Œæ¨¡æ‹Ÿç”Ÿæˆçš„è¡€ç³–åˆ†å¸ƒä¸å®é™…è§‚æµ‹æ•°æ®åœ¨ç›®æ ‡èŒƒå›´å†…æ—¶é—´(Time in Range)ç­‰å…³é”®ä¸´åºŠæŒ‡æ ‡ä¸Šå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„ç­‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æˆåŠŸç»“åˆäº†ç‰©ç†é©±åŠ¨ä¸æ•°æ®é©±åŠ¨çš„å»ºæ¨¡æ–¹æ³•ï¼Œä¸ºä¸ªæ€§åŒ–åŒ»ç–—å†³ç­–ã€èƒ°å²›ç´ å‰‚é‡ä¼˜åŒ–å’Œä¸´åºŠåœ¨ä½“æ¨¡æ‹Ÿ(in silico)æµ‹è¯•æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05705v1",
      "published_date": "2025-08-07 03:46:06 UTC",
      "updated_date": "2025-08-07 03:46:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:00:54.851368+00:00"
    },
    {
      "arxiv_id": "2508.05009v1",
      "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦é›†æˆç©ºé—´æ•°æ®ï¼Ÿå…¶æ¨ç†ä¼˜åŠ¿ä¸è®¡ç®—å±€é™çš„å®è¯æ¢æ",
      "authors": [
        "Bin Han",
        "Robert Wolfe",
        "Anat Caspi",
        "Bill Howe"
      ],
      "abstract": "We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•´åˆå¤§å‹ã€å¼‚è´¨ä¸”å˜ˆæ‚çš„åŸå¸‚ç©ºé—´æ•°æ®é›† (urban spatial datasets) æ–¹é¢çš„æ½œåŠ›ï¼Œæ—¨åœ¨ä¸ºé¢†åŸŸä¸“å®¶æä¾›ä¸€ç§ä¼˜äºä¼ ç»Ÿè§„åˆ™æˆ–æœºå™¨å­¦ä¹ æ–¹æ³•çš„çµæ´»æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ LLMs åœ¨å¤„ç†ç”±äººç±»ç»éªŒä»‹å¯¼çš„ç©ºé—´å…³ç³»æ—¶å±•ç°å‡ºç©ºé—´æ¨ç† (spatial reasoning) èƒ½åŠ›ï¼Œä½†åœ¨è¿æ¥å®è§‚ç¯å¢ƒä¸è®¡ç®—å‡ ä½• (computational geometry) ä»»åŠ¡æ—¶å­˜åœ¨é€»è¾‘å±€é™ã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡æä¾›ç›¸å…³ç‰¹å¾æ¥å‡å°‘æ¨¡å‹å¯¹çº¯ç©ºé—´æ¨ç†çš„ä¾èµ–ï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶ç”Ÿæˆç»“æœçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨çš„â€œè¯„å®¡ä¸ä¼˜åŒ–â€ (review-and-refine) æ–¹æ³•èƒ½æœ‰æ•ˆçº æ­£é”™è¯¯å“åº”å¹¶ä¿ç•™å‡†ç¡®ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ LLMs æ˜¯ä¼ ç»ŸåŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³• (rule-based heuristics) çš„æœ‰åŠ›æ›¿ä»£è€…ï¼Œä¸ºå®ç°è‡ªé€‚åº”ç©ºé—´æ•°æ®é›†æˆ (adaptive spatial data integration) æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05009v1",
      "published_date": "2025-08-07 03:44:20 UTC",
      "updated_date": "2025-08-07 03:44:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:04.364982+00:00"
    },
    {
      "arxiv_id": "2508.05006v2",
      "title": "The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein-Ligand Binding",
      "title_zh": "The Docking Gameï¼šé€šè¿‡å¾ªç¯è‡ªåšå¼ˆå®ç°æŸ”æ€§è›‹ç™½è´¨-é…ä½“ç»“åˆçš„å¿«é€Ÿã€åŠ¨æ€ä¸å‡†ç¡®é¢„æµ‹",
      "authors": [
        "Youzhi Zhang",
        "Yufei Li",
        "Gaofeng Meng",
        "Hongbin Liu",
        "Jiebo Luo"
      ],
      "abstract": "Molecular docking is a crucial aspect of drug discovery, as it predicts the binding interactions between small-molecule ligands and protein pockets. However, current multi-task learning models for docking often show inferior performance in ligand docking compared to protein pocket docking. This disparity arises largely due to the distinct structural complexities of ligands and proteins. To address this issue, we propose a novel game-theoretic framework that models the protein-ligand interaction as a two-player game called the Docking Game, with the ligand docking module acting as the ligand player and the protein pocket docking module as the protein player. To solve this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which alternately trains these players through a two-level loop. In the outer loop, the players exchange predicted poses, allowing each to incorporate the other's structural predictions, which fosters mutual adaptation over multiple iterations. In the inner loop, each player dynamically refines its predictions by incorporating its own predicted ligand or pocket poses back into its model. We theoretically show the convergence of LoopPlay, ensuring stable optimization. Extensive experiments conducted on public benchmark datasets demonstrate that LoopPlay achieves approximately a 10\\% improvement in predicting accurate binding modes compared to previous state-of-the-art methods. This highlights its potential to enhance the accuracy of molecular docking in drug discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‹ç™½è´¨-é…ä½“å¯¹æ¥(Molecular docking)ä¸­é…ä½“ä¸è›‹ç™½è´¨å£è¢‹é¢„æµ‹æ€§èƒ½ä¸å‡è¡¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºDocking Gameçš„åšå¼ˆè®ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è›‹ç™½è´¨-é…ä½“ç›¸äº’ä½œç”¨å»ºæ¨¡ä¸ºç”±é…ä½“å¯¹æ¥æ¨¡å—(ligand player)å’Œè›‹ç™½è´¨å£è¢‹å¯¹æ¥æ¨¡å—(protein player)å‚ä¸çš„åŒäººåšå¼ˆã€‚ç ”ç©¶å¼€å‘äº†Loop Self-Play (LoopPlay)ç®—æ³•ï¼Œé€šè¿‡åŒå±‚å¾ªç¯äº¤æ›¿è®­ç»ƒè¿™äº›å‚ä¸è€…ï¼šå¤–å±‚å¾ªç¯ä¸­åŒæ–¹äº¤æ¢é¢„æµ‹å§¿æ€(predicted poses)ä»¥å®ç°å¤šè½®è¿­ä»£çš„ç›¸äº’é€‚åº”ï¼Œå†…å±‚å¾ªç¯ä¸­å„å‚ä¸è€…åˆ©ç”¨è‡ªèº«é¢„æµ‹ç»“æœè¿›è¡ŒåŠ¨æ€ç»†åŒ–ã€‚è¯¥ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜äº†LoopPlayçš„æ”¶æ•›æ€§ï¼Œç¡®ä¿äº†ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoopPlayåœ¨å…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šæ¯”ç°æœ‰State-of-the-artæ–¹æ³•åœ¨é¢„æµ‹ç²¾ç¡®ç»‘å®šæ¨¡å¼æ–¹é¢æå‡äº†çº¦10%ã€‚è¿™ä¸€å·¥ä½œæ˜¾è‘—å¢å¼ºäº†è¯ç‰©å‘ç°ä¸­åˆ†å­å¯¹æ¥çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¤„ç†å¤æ‚æŸ”æ€§ç»‘å®šé¢„æµ‹çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.05006v2",
      "published_date": "2025-08-07 03:38:28 UTC",
      "updated_date": "2025-08-08 08:23:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:42.254676+00:00"
    },
    {
      "arxiv_id": "2508.05004v3",
      "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data",
      "title_zh": "R-Zeroï¼šåŸºäºé›¶æ•°æ®çš„è‡ªè¿›åŒ–æ¨ç†å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chengsong Huang",
        "Wenhao Yu",
        "Xiaoyang Wang",
        "Hongming Zhang",
        "Zongxia Li",
        "Ruosen Li",
        "Jiaxin Huang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†R-Zeroï¼Œä¸€ä¸ªæ—¨åœ¨å…‹æœè‡ªæˆ‘è¿›åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆSelf-evolving LLMsï¼‰å¯¹äººå·¥æ ‡æ³¨æ•°æ®ä¾èµ–çš„å…¨è‡ªåŠ¨æ¡†æ¶ã€‚R-Zeroä»å•ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆBase LLMï¼‰å‡ºå‘ï¼Œé€šè¿‡åˆå§‹åŒ–ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹è§’è‰²â€”â€”æŒ‘æˆ˜è€…ï¼ˆChallengerï¼‰å’Œæ±‚è§£è€…ï¼ˆSolverï¼‰ï¼Œå®ç°ä»é›¶æ•°æ®çš„è‡ªä¸»è¿›åŒ–ã€‚è¿™ä¸¤ä¸ªè§’è‰²é€šè¿‡äº¤äº’ååŒè¿›åŒ–ï¼ˆCo-evolveï¼‰ï¼ŒChallengerè´Ÿè´£æå‡ºæ¥è¿‘Solverèƒ½åŠ›æé™çš„ä»»åŠ¡ï¼Œè€ŒSolveråˆ™åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­ä¸æ–­ç²¾è¿›ã€‚è¿™ç§æœºåˆ¶æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹æ€§å¼ºçš„è‡ªæˆ‘æ”¹è¿›è¯¾ç¨‹ï¼ˆSelf-improving curriculumï¼‰ï¼Œå®Œå…¨æ‘†è„±äº†å¯¹é¢„è®¾ä»»åŠ¡å’Œæ ‡ç­¾çš„ç“¶é¢ˆé™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Zeroæ˜¾è‘—æå‡äº†ä¸åŒéª¨å¹²æ¨¡å‹ï¼ˆBackbone LLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚ä½¿Qwen3-4B-Baseåœ¨æ•°å­¦æ¨ç†ï¼ˆMath-reasoningï¼‰åŸºå‡†ä¸Šæå‡äº†6.49åˆ†ï¼Œåœ¨é€šç”¨é¢†åŸŸæ¨ç†ï¼ˆGeneral-domain reasoningï¼‰åŸºå‡†ä¸Šæå‡äº†7.54åˆ†ï¼Œä¸ºé€šå‘è¶…æ™ºèƒ½ï¼ˆSuper-intelligenceï¼‰æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05004v3",
      "published_date": "2025-08-07 03:38:16 UTC",
      "updated_date": "2026-01-09 21:58:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:10.767331+00:00"
    },
    {
      "arxiv_id": "2508.05003v1",
      "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health",
      "title_zh": "ç”¨äºæå–è‡ªæ€ç›¸å…³å¥åº·ç¤¾ä¼šå†³å®šå› ç´ çš„å¤šé˜¶æ®µå¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Song Wang",
        "Yishu Wei",
        "Haotian Ma",
        "Max Lovitt",
        "Kelly Deng",
        "Yuan Meng",
        "Zihan Xu",
        "Jingze Zhang",
        "Yunyu Xiao",
        "Ying Ding",
        "Xuhai Xu",
        "Joydeep Ghosh",
        "Yifan Peng"
      ],
      "abstract": "Background: Understanding social determinants of health (SDoH) factors contributing to suicide incidents is crucial for early intervention and prevention. However, data-driven approaches to this goal face challenges such as long-tailed factor distributions, analyzing pivotal stressors preceding suicide incidents, and limited model explainability. Methods: We present a multi-stage large language model framework to enhance SDoH factor extraction from unstructured text. Our approach was compared to other state-of-the-art language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help people annotate SDoH factors more quickly and accurately. The analysis included both automated comparisons and a pilot user study. Results: We show that our proposed framework demonstrated performance boosts in the overarching task of extracting SDoH factors and in the finer-grained tasks of retrieving relevant context. Additionally, we show that fine-tuning a smaller, task-specific model achieves comparable or better performance with reduced inference costs. The multi-stage design not only enhances extraction but also provides intermediate explanations, improving model explainability. Conclusions: Our approach improves both the accuracy and transparency of extracting suicide-related SDoH from unstructured texts. These advancements have the potential to support early identification of individuals at risk and inform more effective prevention strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šé˜¶æ®µå¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ (Multi-Stage Large Language Model Framework)ï¼Œä¸“é—¨ç”¨äºä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ä¸è‡ªæ€ç›¸å…³çš„å¥åº·ç¤¾ä¼šå†³å®šå› ç´  (Social Determinants of Health, SDoH)ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³è‡ªæ€ç›¸å…³å› ç´ æå–ä¸­é¢ä¸´çš„é•¿å°¾åˆ†å¸ƒã€å…³é”®å‹åŠ›æºåˆ†æä»¥åŠæ¨¡å‹å¯è§£é‡Šæ€§ (explainability) æœ‰é™ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡ä¸ BioBERTã€GPT-3.5-turbo å’Œ DeepSeek-R1 ç­‰æ¨¡å‹å¯¹æ¯”å‘ç°ï¼Œè¯¥æ¡†æ¶åœ¨æå– SDoH å› ç´ å’Œæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é€šè¿‡å¾®è°ƒ (fine-tuning) è¾ƒå°çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹å®ç°äº†æ›´ä½çš„æ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œå¤šé˜¶æ®µè®¾è®¡æä¾›çš„ä¸­é—´è§£é‡Šæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„é€æ˜åº¦ï¼Œå¹¶èƒ½è¾…åŠ©äººå·¥æ›´å¿«é€Ÿã€å‡†ç¡®åœ°å®Œæˆæ ‡æ³¨å·¥ä½œã€‚è¯¥ç ”ç©¶æˆæœä¸ä»…æ˜¾è‘—æé«˜äº† SDoH æå–çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ï¼Œä¹Ÿä¸ºæ—©æœŸè¯†åˆ«è‡ªæ€é£é™©ä¸ªä½“åŠåˆ¶å®šæ›´æœ‰æ•ˆçš„å¹²é¢„ç­–ç•¥æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05003v1",
      "published_date": "2025-08-07 03:36:38 UTC",
      "updated_date": "2025-08-07 03:36:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:36.163743+00:00"
    },
    {
      "arxiv_id": "2508.05002v1",
      "title": "AgenticData: An Agentic Data Analytics System for Heterogeneous Data",
      "title_zh": "AgenticDataï¼šä¸€ç§é¢å‘å¼‚æ„æ•°æ®çš„æ™ºèƒ½ä½“åŒ–æ•°æ®åˆ†æç³»ç»Ÿ",
      "authors": [
        "Ji Sun",
        "Guoliang Li",
        "Peiyao Zhou",
        "Yihui Ma",
        "Jingzhe Xu",
        "Yuan Li"
      ],
      "abstract": "Existing unstructured data analytics systems rely on experts to write code and manage complex analysis workflows, making them both expensive and time-consuming. To address these challenges, we introduce AgenticData, an innovative agentic data analytics system that allows users to simply pose natural language (NL) questions while autonomously analyzing data sources across multiple domains, including both unstructured and structured data. First, AgenticData employs a feedback-driven planning technique that automatically converts an NL query into a semantic plan composed of relational and semantic operators. We propose a multi-agent collaboration strategy by utilizing a data profiling agent for discovering relevant data, a semantic cross-validation agent for iterative optimization based on feedback, and a smart memory agent for maintaining short-term context and long-term knowledge. Second, we propose a semantic optimization model to refine and execute semantic plans effectively. Our system, AgenticData, has been tested using three benchmarks. Experimental results showed that AgenticData achieved superior accuracy on both easy and difficult tasks, significantly outperforming state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AgenticDataï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¼‚æ„æ•°æ®(Heterogeneous Data)çš„æ™ºèƒ½ä½“æ•°æ®åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿè¿‡åº¦ä¾èµ–ä¸“å®¶ç¼–å†™ä»£ç ä¸”åˆ†ææµç¨‹å¤æ‚çš„é—®é¢˜ã€‚AgenticDataå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€(Natural Language)æé—®ï¼Œå¹¶èƒ½è‡ªä¸»åˆ†æè·¨é¢†åŸŸçš„ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ•°æ®ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åé¦ˆé©±åŠ¨çš„è§„åˆ’æŠ€æœ¯(Feedback-driven Planning)ï¼Œå°†æŸ¥è¯¢è‡ªåŠ¨è½¬åŒ–ä¸ºç”±å…³ç³»å’Œè¯­ä¹‰è¿ç®—ç¬¦ç»„æˆçš„è¯­ä¹‰è®¡åˆ’(Semantic Plan)ã€‚æ ¸å¿ƒæ¶æ„é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥(Multi-agent Collaboration Strategy)å®ç°ï¼Œåˆ©ç”¨Data Profilingæ™ºèƒ½ä½“å‘ç°æ•°æ®ï¼ŒSemantic Cross-validationæ™ºèƒ½ä½“è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œå¹¶ç”±Smart Memoryæ™ºèƒ½ä½“ç®¡ç†çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå¼•å…¥è¯­ä¹‰ä¼˜åŒ–æ¨¡å‹(Semantic Optimization Model)æ¥ç²¾ç»†åŒ–æ‰§è¡Œæµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgenticDataåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å“è¶Šçš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•(State-of-the-art)ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05002v1",
      "published_date": "2025-08-07 03:33:59 UTC",
      "updated_date": "2025-08-07 03:33:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:15.860424+00:00"
    },
    {
      "arxiv_id": "2508.04995v3",
      "title": "Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge",
      "title_zh": "æƒ…å¢ƒåŒ–è®¤è¯†è®ºåŸºç¡€è®¾æ–½ï¼šåä¸€è‡´æ€§çŸ¥è¯†çš„è¯Šæ–­æ¡†æ¶",
      "authors": [
        "Matthew Kelly"
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT have rendered visible the fragility of contemporary knowledge infrastructures by simulating coherence while bypassing traditional modes of citation, authority, and validation. This paper introduces the Situated Epistemic Infrastructures (SEI) framework as a diagnostic tool for analyzing how knowledge becomes authoritative across hybrid human-machine systems under post-coherence conditions. Rather than relying on stable scholarly domains or bounded communities of practice, SEI traces how credibility is mediated across institutional, computational, and temporal arrangements. Integrating insights from infrastructure studies, platform theory, and epistemology, the framework foregrounds coordination over classification, emphasizing the need for anticipatory and adaptive models of epistemic stewardship. The paper contributes to debates on AI governance, knowledge production, and the ethical design of information systems by offering a robust alternative to representationalist models of scholarly communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»¥ChatGPTä¸ºä»£è¡¨çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•é€šè¿‡æ¨¡æ‹Ÿè¿è´¯æ€§å´ç»•è¿‡ä¼ ç»ŸéªŒè¯æœºåˆ¶ï¼Œæ­ç¤ºäº†å½“ä»£çŸ¥è¯†åŸºç¡€è®¾æ–½çš„è„†å¼±æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æƒ…å¢ƒåŒ–è®¤çŸ¥åŸºç¡€è®¾æ–½(Situated Epistemic Infrastructures, SEI)æ¡†æ¶ï¼Œä½œä¸ºåˆ†æåè¿è´¯æ€§(post-coherence)æ¡ä»¶ä¸‹äººæœºæ··åˆç³»ç»ŸçŸ¥è¯†æƒå¨æ€§çš„è¯Šæ–­å·¥å…·ã€‚SEIä¸å†ä¾èµ–ä¼ ç»Ÿçš„å­¦æœ¯é¢†åŸŸï¼Œè€Œæ˜¯è¿½è¸ªå¯ä¿¡åº¦å¦‚ä½•åœ¨åˆ¶åº¦ã€è®¡ç®—å’Œæ—¶é—´å®‰æ’ä¸­è¿›è¡Œä¸­ä»‹ï¼Œå¹¶æ•´åˆäº†åŸºç¡€è®¾æ–½ç ”ç©¶(infrastructure studies)ã€å¹³å°ç†è®º(platform theory)å’Œè®¤è¯†è®º(epistemology)çš„è§è§£ã€‚è¯¥æ¡†æ¶å¼ºè°ƒåè°ƒè€Œéåˆ†ç±»ï¼Œæå‡ºäº†è®¤çŸ¥ç®¡ç†(epistemic stewardship)çš„å‰ç»æ€§ä¸é€‚åº”æ€§æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºAIæ²»ç†ã€çŸ¥è¯†ç”Ÿäº§ä»¥åŠä¿¡æ¯ç³»ç»Ÿçš„ä¼¦ç†è®¾è®¡æä¾›äº†ç¨³å¥çš„æ–¹æ¡ˆï¼Œæ˜¯å¯¹ä¼ ç»Ÿè¡¨å¾ä¸»ä¹‰(representationalist)å­¦æœ¯ä¼ æ’­æ¨¡å‹çš„æœ‰åŠ›æ›¿ä»£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages including references. Draft prepared for submission to Social Epistemology",
      "pdf_url": "https://arxiv.org/pdf/2508.04995v3",
      "published_date": "2025-08-07 03:08:23 UTC",
      "updated_date": "2025-10-17 00:12:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:01:59.655493+00:00"
    },
    {
      "arxiv_id": "2508.04994v1",
      "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots",
      "title_zh": "é¢å‘ç§»åŠ¨æœºå™¨äººè‡ªä¸»è¿·å®«å¯¼èˆªçš„åˆ†å±‚æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦",
      "authors": [
        "Wenjie Hu",
        "Ye Zhou",
        "Hann Woei Ho"
      ],
      "abstract": "Maze navigation is a fundamental challenge in robotics, requiring agents to traverse complex environments efficiently. While the Deep Deterministic Policy Gradient (DDPG) algorithm excels in control tasks, its performance in maze navigation suffers from sparse rewards, inefficient exploration, and long-horizon planning difficulties, often leading to low success rates and average rewards, sometimes even failing to achieve effective navigation. To address these limitations, this paper proposes an efficient Hierarchical DDPG (HDDPG) algorithm, which includes high-level and low-level policies. The high-level policy employs an advanced DDPG framework to generate intermediate subgoals from a long-term perspective and on a higher temporal scale. The low-level policy, also powered by the improved DDPG algorithm, generates primitive actions by observing current states and following the subgoal assigned by the high-level policy. The proposed method enhances stability with off-policy correction, refining subgoal assignments by relabeling historical experiences. Additionally, adaptive parameter space noise is utilized to improve exploration, and a reshaped intrinsic-extrinsic reward function is employed to boost learning efficiency. Further optimizations, including gradient clipping and Xavier initialization, are employed to improve robustness. The proposed algorithm is rigorously evaluated through numerical simulation experiments executed using the Robot Operating System (ROS) and Gazebo. Regarding the three distinct final targets in autonomous maze navigation tasks, HDDPG significantly overcomes the limitations of standard DDPG and its variants, improving the success rate by at least 56.59% and boosting the average reward by a minimum of 519.03 compared to baseline algorithms.",
      "tldr_zh": "é’ˆå¯¹ç§»åŠ¨æœºå™¨äººåœ¨å¤æ‚è¿·å®«å¯¼èˆªä¸­é¢ä¸´çš„ç¨€ç–å¥–åŠ±ã€ä½æ•ˆæ¢ç´¢åŠé•¿æ—¶ç¨‹è§„åˆ’(long-horizon planning)ç­‰éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åˆ†å±‚æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(Hierarchical DDPG, HDDPG)ç®—æ³•ã€‚è¯¥ç®—æ³•å°†å¯¼èˆªä»»åŠ¡åˆ†è§£ä¸ºåŒå±‚ç­–ç•¥æ¶æ„ï¼Œé«˜å±‚ç­–ç•¥è´Ÿè´£ä»é•¿æœŸè§†è§’ç”Ÿæˆä¸­é—´å­ç›®æ ‡(subgoals)ï¼Œè€Œä½å±‚ç­–ç•¥åˆ™æ ¹æ®å½“å‰çŠ¶æ€å’Œåˆ†é…çš„å­ç›®æ ‡æ‰§è¡ŒåŸå§‹åŠ¨ä½œã€‚ä¸ºäº†æå‡å­¦ä¹ ç¨³å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†ç¦»çº¿ç­–ç•¥ä¿®æ­£(off-policy correction)æœºåˆ¶ï¼Œé€šè¿‡é‡æ–°æ ‡è®°å†å²ç»éªŒæ¥ä¼˜åŒ–å­ç›®æ ‡åˆ†é…ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”å‚æ•°ç©ºé—´å™ªå£°(adaptive parameter space noise)å¢å¼ºæ¢ç´¢æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡é‡å¡‘å†…åœ¨-å¤–åœ¨å¥–åŠ±å‡½æ•°(intrinsic-extrinsic reward function)å¹¶ç»“åˆæ¢¯åº¦è£å‰ª(gradient clipping)ç­‰ä¼˜åŒ–æ‰‹æ®µï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚åœ¨ROSå’ŒGazeboç¯å¢ƒä¸‹çš„ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒHDDPGç›¸æ¯”æ ‡å‡†DDPGåŠå…¶å˜ä½“åœ¨æˆåŠŸç‡ä¸Šæå‡äº†è‡³å°‘56.59%ï¼Œå¹³å‡å¥–åŠ±æå‡è¶…è¿‡519.03ï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªä¸»å¯¼èˆªç“¶é¢ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.04994v1",
      "published_date": "2025-08-07 03:06:22 UTC",
      "updated_date": "2025-08-07 03:06:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:00.049618+00:00"
    },
    {
      "arxiv_id": "2508.11669v1",
      "title": "Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset",
      "title_zh": "åŸºäºååŒå­¦ä¹ å¢å¼ºçš„è½»é‡çº§æ¨¡å‹ï¼šç”¨äºå¤§è§„æ¨¡å›´æ‰‹æœ¯æœŸæ•°æ®é›†çš„åŠ¨è„‰è¡€å‹æ³¢å½¢é¢„æµ‹",
      "authors": [
        "Wentao Li",
        "Yonghu He",
        "Kun Gao",
        "Qing Liu",
        "Yali Zheng"
      ],
      "abstract": "Noninvasive arterial blood pressure (ABP) monitoring is essential for patient management in critical care and perioperative settings, providing continuous assessment of cardiovascular hemodynamics with minimal risks. Numerous deep learning models have developed to reconstruct ABP waveform from noninvasively acquired physiological signals such as electrocardiogram and photoplethysmogram. However, limited research has addressed the issue of model performance and computational load for deployment on embedded systems. The study introduces a lightweight sInvResUNet, along with a collaborative learning scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a computational load of 0.02 GFLOPS, real-time ABP estimation was successfully achieved on embedded devices with an inference time of just 8.49 milliseconds for a 10-second output. We performed subject-independent validation in a large-scale and heterogeneous perioperative dataset containing 1,257,141 data segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and 31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better performance compared to large models, with a mean absolute error of 10.06 mmHg and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these promising results, all deep learning models showed significant performance variations across different demographic and cardiovascular conditions, highlighting their limited ability to generalize across such a broad and diverse population. This study lays a foundation work for real-time, unobtrusive ABP monitoring in real-world perioperative settings, providing baseline for future advancements in this area.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹é‡ç—‡ç›‘æŠ¤å’Œå›´æ‰‹æœ¯æœŸç¯å¢ƒï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§æ¨¡å‹sInvResUNetä»¥åŠåä½œå­¦ä¹ æ–¹æ¡ˆKDCL_sInvResUNetï¼Œæ—¨åœ¨é€šè¿‡å¿ƒç”µå›¾(ECG)å’Œå…‰ç”µå®¹ç§¯è„‰ææ³¢(PPG)ç­‰ç”Ÿç†ä¿¡å·éä¾µå…¥å¼åœ°é‡å»ºåŠ¨è„‰è¡€å‹(ABP)æ³¢å½¢ã€‚è¯¥æ¨¡å‹ä»…åŒ…å«0.89 millionå‚æ•°ï¼Œåœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šå¤„ç†10ç§’è¾“å‡ºçš„æ¨ç†æ—¶é—´ä»…ä¸º8.49 millisecondsï¼ŒæˆåŠŸå®ç°äº†å®æ—¶çš„ABPä¼°ç®—ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å«2,154åæ‚£è€…ã€è¶…è¿‡125ä¸‡ä¸ªæ•°æ®æ®µçš„å¤§è§„æ¨¡å¼‚æ„å›´æ‰‹æœ¯æœŸæ•°æ®é›†è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºKDCL_sInvResUNetåœ¨è¿½è¸ªè¡€å‹å˜åŒ–æ–¹é¢çš„æ€§èƒ½ç•¥ä¼˜äºå¤§å‹æ¨¡å‹ï¼Œå¹³å‡ç»å¯¹è¯¯å·®(MAE)ä¸º10.06 mmHgã€‚å°½ç®¡æ‰€æœ‰æ¨¡å‹åœ¨ä¸åŒäººå£ç»Ÿè®¡å­¦å’Œå¿ƒè¡€ç®¡æ¡ä»¶ä¸‹è¡¨ç°å‡ºä¸€å®šçš„æ³›åŒ–å±€é™æ€§ï¼Œä½†è¯¥ç ”ç©¶ä¸ºçœŸå®ä¸´åºŠåœºæ™¯ä¸‹çš„æ— åˆ›ã€å®æ—¶ç›‘æµ‹å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è¯æ˜äº†è½»é‡çº§æ¨¡å‹åœ¨åµŒå…¥å¼ç³»ç»Ÿä¸Šçš„éƒ¨ç½²æ½œåŠ›ï¼Œä¹Ÿä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ç®—æ³•ä¼˜åŒ–æä¾›äº†é‡è¦çš„æ€§èƒ½åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11669v1",
      "published_date": "2025-08-07 02:40:17 UTC",
      "updated_date": "2025-08-07 02:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:12.393897+00:00"
    },
    {
      "arxiv_id": "2508.04968v1",
      "title": "UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS",
      "title_zh": "UGODï¼šç”¨äºå¢å¼ºç¨€ç–è§†å›¾ 3DGS çš„ä¸ç¡®å®šæ€§å¼•å¯¼å¯å¾®ä¸é€æ˜åº¦ä¸è½¯ Dropout",
      "authors": [
        "Zhihao Guo",
        "Peng Wang",
        "Zidong Chen",
        "Xiangyu Kong",
        "Yan Lyu",
        "Guanyu Gao",
        "Liangxiu Han"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\\% PSNR improvements on the MipNeRF 360 dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian Splatting (3DGS) åœ¨ç¨€ç–è§†å›¾(sparse-view)åœºæ™¯ä¸‹å› é«˜æ–¯ç‚¹ç­‰æƒé‡å¤„ç†å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡ºäº† UGOD æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å­¦ä¹ åˆ°çš„ä¸ç¡®å®šæ€§(uncertainty)æ¥å®ç°é«˜æ–¯ç‚¹çš„è‡ªé€‚åº”æƒé‡è°ƒæ•´ï¼Œä»¥å¢å¼ºä¸‰ç»´é‡å»ºæ•ˆæœã€‚è¯¥ä¸ç¡®å®šæ€§ä¸€æ–¹é¢å¼•å¯¼å¯å¾®ä¸é€æ˜åº¦(differentiable opacity)çš„æ›´æ–°ä»¥ä¿æŒç®¡çº¿å®Œæ•´æ€§ï¼Œå¦ä¸€æ–¹é¢é€šè¿‡è½¯å¯å¾®ä¸¢å¼ƒ(soft differentiable dropout)æ­£åˆ™åŒ–å°†ä¸ç¡®å®šæ€§è½¬åŒ–ä¸ºè¿ç»­çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œä»è€Œä¼˜åŒ–é«˜æ–¯æŠ•å½±ä¸æ··åˆè¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒUGOD åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„ç¨€ç–è§†å›¾åˆæˆæ–¹æ³•ï¼Œä¸”èƒ½ä»¥æ›´å°‘çš„é«˜æ–¯ç‚¹å®ç°æ›´é«˜è´¨é‡çš„é‡å»ºã€‚åœ¨ MipNeRF 360 æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯” DropGaussian å®ç°äº† 3.27% çš„ PSNR æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç¨€ç–æ•°æ®æ—¶çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.04968v1",
      "published_date": "2025-08-07 01:42:22 UTC",
      "updated_date": "2025-08-07 01:42:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:18.296854+00:00"
    },
    {
      "arxiv_id": "2508.09174v1",
      "title": "FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective",
      "title_zh": "FedMPï¼šä»æµå½¢è§†è§’åº”å¯¹è”é‚¦å­¦ä¹ ä¸­çš„åŒ»å­¦ç‰¹å¾å¼‚è´¨æ€§",
      "authors": [
        "Zhekai Zhou",
        "Shudong Liu",
        "Zhaokun Zhou",
        "Yang Liu",
        "Qiang Yang",
        "Yuesheng Zhu",
        "Guibo Luo"
      ],
      "abstract": "Federated learning (FL) is a decentralized machine learning paradigm in which multiple clients collaboratively train a shared model without sharing their local private data. However, real-world applications of FL frequently encounter challenges arising from the non-identically and independently distributed (non-IID) local datasets across participating clients, which is particularly pronounced in the field of medical imaging, where shifts in image feature distributions significantly hinder the global model's convergence and performance. To address this challenge, we propose FedMP, a novel method designed to enhance FL under non-IID scenarios. FedMP employs stochastic feature manifold completion to enrich the training space of individual client classifiers, and leverages class-prototypes to guide the alignment of feature manifolds across clients within semantically consistent subspaces, facilitating the construction of more distinct decision boundaries. We validate the effectiveness of FedMP on multiple medical imaging datasets, including those with real-world multi-center distributions, as well as on a multi-domain natural image dataset. The experimental results demonstrate that FedMP outperforms existing FL algorithms. Additionally, we analyze the impact of manifold dimensionality, communication efficiency, and privacy implications of feature exposure in our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)åœ¨åŒ»ç–—å½±åƒåº”ç”¨ä¸­é¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºå›¾åƒç‰¹å¾åˆ†å¸ƒåç§»å¯¼è‡´å…¨å±€æ¨¡å‹æ”¶æ•›å’Œæ€§èƒ½å—æŸçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFedMPçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»æµå½¢(Manifold)è§†è§’è§£å†³åŒ»ç–—ç‰¹å¾çš„å¼‚è´¨æ€§ã€‚FedMPé‡‡ç”¨äº†éšæœºç‰¹å¾æµå½¢è¡¥å…¨(stochastic feature manifold completion)æŠ€æœ¯ï¼Œæœ‰æ•ˆä¸°å¯Œäº†å•ä¸ªå®¢æˆ·ç«¯åˆ†ç±»å™¨çš„è®­ç»ƒç©ºé—´ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç±»åŸå‹(class-prototypes)å¼•å¯¼ä¸åŒå®¢æˆ·ç«¯ä¹‹é—´çš„ç‰¹å¾æµå½¢åœ¨è¯­ä¹‰ä¸€è‡´çš„å­ç©ºé—´å†…è¿›è¡Œå¯¹é½ï¼Œä»è€Œæ„å»ºæ›´åŠ æ¸…æ™°çš„å†³ç­–è¾¹ç•Œã€‚å®éªŒåœ¨å¤šä¸ªåŒ»ç–—å½±åƒæ•°æ®é›†ä»¥åŠå¤šåŸŸè‡ªç„¶å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†FedMPçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„è”é‚¦å­¦ä¹ ç®—æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†æµå½¢ç»´åº¦ã€é€šä¿¡æ•ˆç‡ä»¥åŠç‰¹å¾æš´éœ²å¸¦æ¥çš„éšç§å½±å“ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤šä¸­å¿ƒåŒ»ç–—æ•°æ®åˆ†å¸ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09174v1",
      "published_date": "2025-08-07 01:13:46 UTC",
      "updated_date": "2025-08-07 01:13:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:16.491547+00:00"
    },
    {
      "arxiv_id": "2508.10019v2",
      "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning",
      "title_zh": "é€šè¿‡é—®é¢˜ç©ºé—´æ˜ å°„å®ç°ç†è§£ä¸æ¨ç†çš„è§£è€¦ï¼Œèµ‹èƒ½å°è§„æ¨¡æ¨¡å‹æ¨ç†",
      "authors": [
        "Li Wang",
        "Changhao Zhang",
        "Zengqi Xiu",
        "Kai Lu",
        "Xin Yu",
        "Kui Zhang",
        "Wenjun Wu"
      ],
      "abstract": "Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., up to 1.5B parameters) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°è¯­è¨€æ¨¡å‹(Small Language Models)åœ¨å¤„ç†å¤æ‚è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å°†ç†è§£ä¸æ¨ç†è¿‡ç¨‹è§£è€¦çš„æ–°æ¡†æ¶ã€‚ç”±äºè‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å¤šæ ·æ€§å’Œå†—ä½™ç»†èŠ‚ç»™å‚æ•°é‡æœ‰é™çš„æ¨¡å‹å¸¦æ¥äº†ç†è§£ä¸æ¨ç†çš„åŒé‡è´Ÿæ‹…ï¼Œè¯¥ç ”ç©¶é€šè¿‡å°†è‡ªç„¶è¯­è¨€é—®é¢˜æ˜ å°„åˆ°ä¸€ä¸ªè§„èŒƒçš„é—®é¢˜ç©ºé—´(canonical problem space)æ¥æ¶ˆé™¤è¯­è¨€å˜å¼‚æ€§çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘äº†åä¸ºDURIT (Decoupled Understanding from Reasoning via Iterative Training)çš„ç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›è¡Œé—®é¢˜æ˜ å°„ã€è‡ªè’¸é¦(Self-distillation)å¯¹é½æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨è¯¥ç©ºé—´å†…ååŒè®­ç»ƒæ˜ å°„å™¨ä¸æ¨ç†å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDURITæ˜¾è‘—æå‡äº†å°è¯­è¨€æ¨¡å‹åœ¨åŸŸå†…åŠè·¨åŸŸæ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå¢å¼ºäº†æ¨ç†çš„ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•è¯æ˜äº†å°†ç†è§£ä¸æ¨ç†è¿›è¡Œè§£è€¦æ˜¯å¼ºåŒ–å°è§„æ¨¡æ¨¡å‹é€»è¾‘èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10019v2",
      "published_date": "2025-08-07 01:13:30 UTC",
      "updated_date": "2025-12-15 12:12:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:28.384688+00:00"
    },
    {
      "arxiv_id": "2508.05702v3",
      "title": "Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control",
      "title_zh": "Grid-Agentï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”µç½‘æ§åˆ¶å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Yan Zhang",
        "Ahmad Mohammad Saber",
        "Amr Youssef",
        "Deepa Kundur"
      ],
      "abstract": "Modern power grids face unprecedented complexity from Distributed Energy Resources (DERs), Electric Vehicles (EVs), and extreme weather, while also being increasingly exposed to cyberattacks that can trigger grid violations. This paper introduces Grid-Agent, an autonomous AI-driven framework that leverages Large Language Models (LLMs) within a multi-agent system to detect and remediate violations. Grid-Agent integrates semantic reasoning with numerical precision through modular agents: a planning agent generates coordinated action sequences using power flow solvers, while a validation agent ensures stability and safety through sandboxed execution with rollback mechanisms. To enhance scalability, the framework employs an adaptive multi-scale network representation that dynamically adjusts encoding schemes based on system size and complexity. Violation resolution is achieved through optimizing switch configurations, battery deployment, and load curtailment. Our experiments on IEEE and CIGRE benchmark networks, including the IEEE 69-bus, CIGRE MV, IEEE 30-bus test systems, demonstrate superior mitigation performance, highlighting Grid-Agent's suitability for modern smart grids requiring rapid, adaptive response.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Grid-Agentï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) é©±åŠ¨çš„è‡ªä¸»å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤ç°ä»£ç”µç½‘åœ¨é¢å¯¹åˆ†å¸ƒå¼èƒ½æº (DERs)ã€ç”µåŠ¨æ±½è½¦ (EVs) å’Œç½‘ç»œæ”»å‡»æ—¶å‡ºç°çš„è¿è§„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡å—åŒ–æ™ºèƒ½ä½“å°†è¯­ä¹‰æ¨ç†ä¸æ•°å€¼ç²¾åº¦ç›¸ç»“åˆï¼Œå…¶ä¸­è§„åˆ’æ™ºèƒ½ä½“åˆ©ç”¨æ½®æµæ±‚è§£å™¨ç”Ÿæˆåè°ƒåŠ¨ä½œåºåˆ—ï¼Œè€ŒéªŒè¯æ™ºèƒ½ä½“åˆ™é€šè¿‡æ²™ç›’æ‰§è¡Œå’Œå›æ»šæœºåˆ¶ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šä¸å®‰å…¨ã€‚ä¸ºäº†è§£å†³å¯æ‰©å±•æ€§éš¾é¢˜ï¼ŒGrid-Agent å¼•å…¥äº†è‡ªé€‚åº”å¤šå°ºåº¦ç½‘ç»œè¡¨ç¤ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ ¹æ®ç³»ç»Ÿè§„æ¨¡åŠ¨æ€è°ƒæ•´ç¼–ç æ–¹æ¡ˆã€‚é€šè¿‡å¯¹å¼€å…³é…ç½®ã€ç”µæ± éƒ¨ç½²å’Œè´Ÿè·å‰Šå‡çš„ç»¼åˆä¼˜åŒ–ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆåœ°è§£å†³ç”µç½‘è¿è§„ã€‚åœ¨ IEEE 69-busã€CIGRE MV åŠ IEEE 30-bus ç­‰åŸºå‡†ç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¯æ˜äº† Grid-Agent å“è¶Šçš„ç¼“è§£æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨éœ€è¦å¿«é€Ÿè‡ªé€‚åº”å“åº”çš„ç°ä»£æ™ºèƒ½ç”µç½‘ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05702v3",
      "published_date": "2025-08-07 01:10:28 UTC",
      "updated_date": "2025-09-08 23:53:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:27.192691+00:00"
    },
    {
      "arxiv_id": "2508.06566v1",
      "title": "Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features",
      "title_zh": "Surformer v1ï¼šèåˆè§¦è§‰ä¸è§†è§‰ç‰¹å¾çš„åŸºäº Transformer çš„è¡¨é¢åˆ†ç±»",
      "authors": [
        "Manish Kansana",
        "Elias Hossain",
        "Shahram Rahimi",
        "Noorbakhsh Amiri Golilarz"
      ],
      "abstract": "Surface material recognition is a key component in robotic perception and physical interaction, particularly when leveraging both tactile and visual sensory inputs. In this work, we propose Surformer v1, a transformer-based architecture designed for surface classification using structured tactile features and PCA-reduced visual embeddings extracted via ResNet-50. The model integrates modality-specific encoders with cross-modal attention layers, enabling rich interactions between vision and touch. Currently, state-of-the-art deep learning models for vision tasks have achieved remarkable performance. With this in mind, our first set of experiments focused exclusively on tactile-only surface classification. Using feature engineering, we trained and evaluated multiple machine learning models, assessing their accuracy and inference time. We then implemented an encoder-only Transformer model tailored for tactile features. This model not only achieved the highest accuracy but also demonstrated significantly faster inference time compared to other evaluated models, highlighting its potential for real-time applications. To extend this investigation, we introduced a multimodal fusion setup by combining vision and tactile inputs. We trained both Surformer v1 (using structured features) and Multimodal CNN (using raw images) to examine the impact of feature-based versus image-based multimodal learning on classification accuracy and computational efficiency. The results showed that Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while the Multimodal CNN achieved slightly higher accuracy but required significantly more inference time. These findings suggest Surformer v1 offers a compelling balance between accuracy, efficiency, and computational cost for surface material recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Surformer v1ï¼Œä¸€ç§åŸºäºTransformeræ¶æ„çš„è¡¨é¢ææ–™åˆ†ç±»æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è§¦è§‰å’Œè§†è§‰çš„å¤šæ¨¡æ€èåˆæå‡æœºå™¨äººçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ResNet-50æå–è§†è§‰ç‰¹å¾å¹¶è¿›è¡ŒPCAé™ç»´ï¼Œç»“åˆç»“æ„åŒ–çš„è§¦è§‰ç‰¹å¾ï¼Œé€šè¿‡æ¨¡æ€ä¸“ç”¨ç¼–ç å™¨ä¸è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶(cross-modal attention layers)å®ç°ä¸¤ç§æ„Ÿå®˜æ•°æ®çš„æ·±åº¦äº¤äº’ã€‚åœ¨å¯¹æ¯”å®éªŒä¸­ï¼Œé’ˆå¯¹è§¦è§‰ç‰¹å¾è®¾è®¡çš„Encoder-only Transformeræ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚éšåï¼Œç ”ç©¶è€…åœ¨å¤šæ¨¡æ€èåˆè®¾ç½®ä¸‹å°†Surformer v1ä¸Multimodal CNNè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼Œæ¢è®¨äº†åŸºäºç‰¹å¾ä¸åŸºäºå›¾åƒçš„æ¨¡æ€å­¦ä¹ å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurformer v1å®ç°äº†99.4%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸”æ¨ç†æ—¶é—´ç¼©çŸ­è‡³0.77 msï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚è¯¥ç ”ç©¶è¯æ˜Surformer v1åœ¨è¯†åˆ«ç²¾åº¦ä¸æ¨ç†é€Ÿåº¦ä¹‹é—´è¾¾æˆäº†å“è¶Šå¹³è¡¡ï¼Œä¸ºå®æ—¶æœºå™¨äººè¡¨é¢è¯†åˆ«ä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06566v1",
      "published_date": "2025-08-07 00:59:33 UTC",
      "updated_date": "2025-08-07 00:59:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:45.286174+00:00"
    },
    {
      "arxiv_id": "2508.04956v1",
      "title": "MENDR: Manifold Explainable Neural Data Representations",
      "title_zh": "MENDRï¼šæµå½¢å¯è§£é‡Šç¥ç»æ•°æ®è¡¨ç¤º",
      "authors": [
        "Matthew Chen",
        "Micky Nnamdi",
        "Justin Shao",
        "Andrew Hornback",
        "Hongyun Huang",
        "Ben Tamo",
        "Yishan Zhong",
        "Benoit Marteau",
        "Wenqi Shi",
        "May Dongmei Wang"
      ],
      "abstract": "Foundation models for electroencephalography (EEG) signals have recently demonstrated success in learning generalized representations of EEGs, outperforming specialized models in various downstream tasks. However, many of these models lack transparency in their pretraining dynamics and offer limited insight into how well EEG information is preserved within their embeddings. For successful clinical integration, EEG foundation models must ensure transparency in pretraining, downstream fine-tuning, and the interpretability of learned representations. Current approaches primarily operate in the temporal domain, overlooking advancements in digital signal processing that enable the extraction of deterministic and traceable features, such as wavelet-based representations. We propose MENDR (Manifold Explainable Neural Data Representations), a filter bank-based EEG foundation model built on a novel Riemannian Manifold Transformer architecture to resolve these issues. MENDR learns symmetric positive definite matrix embeddings of EEG signals and is pretrained on a large corpus comprising over 4,000 hours of EEG data, decomposed via discrete wavelet packet transforms into multi-resolution coefficients. MENDR significantly enhances interpretability by visualizing symmetric positive definite embeddings as geometric ellipsoids and supports accurate reconstruction of EEG signals from learned embeddings. Evaluations across multiple clinical EEG tasks demonstrate that MENDR achieves near state-of-the-art performance with substantially fewer parameters, underscoring its potential for efficient, interpretable, and clinically applicable EEG analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç”µå›¾(EEG)åŸºç¡€æ¨¡å‹åœ¨é¢„è®­ç»ƒåŠ¨æ€é€æ˜åº¦å’Œå­¦ä¹ è¡¨ç¤ºè§£é‡Šæ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†MENDR (Manifold Explainable Neural Data Representations)ã€‚MENDRæ˜¯ä¸€ç§åŸºäºæ»¤æ³¢å™¨ç»„çš„EEGåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨äº†åˆ›æ–°çš„Riemannian Manifold Transformeræ¶æ„ï¼Œé€šè¿‡ç¦»æ•£å°æ³¢åŒ…å˜æ¢(Discrete Wavelet Packet Transforms)å°†ä¿¡å·åˆ†è§£ä¸ºå¤šåˆ†è¾¨ç‡ç³»æ•°ã€‚è¯¥æ¨¡å‹å­¦ä¹ è„‘ç”µä¿¡å·çš„å¯¹ç§°æ­£å®šçŸ©é˜µ(Symmetric Positive Definite, SPD)åµŒå…¥ï¼Œå¹¶åœ¨è¶…è¿‡4000å°æ—¶çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®Œæˆäº†é¢„è®­ç»ƒã€‚MENDRæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæ”¯æŒå°†SPDåµŒå…¥å¯è§†åŒ–ä¸ºå‡ ä½•æ¤­çƒä½“ï¼Œå¹¶èƒ½ä»åµŒå…¥ä¸­ç²¾ç¡®é‡å»ºåŸå§‹ä¿¡å·ã€‚å¤šé¡¹ä¸´åºŠä»»åŠ¡è¯„ä¼°è¡¨æ˜ï¼ŒMENDRåœ¨å¤§å¹…å‡å°‘å‚æ•°é‡çš„æƒ…å†µä¸‹å®ç°äº†æ¥è¿‘æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½è¡¨ç°ã€‚è¿™è¯æ˜äº†è¯¥æ¨¡å‹åœ¨å®ç°é«˜æ•ˆã€é€æ˜ä¸”å…·å¤‡ä¸´åºŠåº”ç”¨ä»·å€¼çš„EEGåˆ†ææ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.04956v1",
      "published_date": "2025-08-07 00:55:05 UTC",
      "updated_date": "2025-08-07 00:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:39.892629+00:00"
    },
    {
      "arxiv_id": "2508.04955v1",
      "title": "AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics",
      "title_zh": "AdvDINOï¼šé¢å‘ç©ºé—´è›‹ç™½è´¨ç»„å­¦çš„é¢†åŸŸå¯¹æŠ—è‡ªç›‘ç£è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Stella Su",
        "Marc Harary",
        "Scott J. Rodig",
        "William Lotter"
      ],
      "abstract": "Self-supervised learning (SSL) has emerged as a powerful approach for learning visual representations without manual annotations. However, the robustness of standard SSL methods to domain shift -- systematic differences across data sources -- remains uncertain, posing an especially critical challenge in biomedical imaging where batch effects can obscure true biological signals. We present AdvDINO, a domain-adversarial self-supervised learning framework that integrates a gradient reversal layer into the DINOv2 architecture to promote domain-invariant feature learning. Applied to a real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide images from non-small cell lung cancer patients, AdvDINO mitigates slide-specific biases to learn more robust and biologically meaningful representations than non-adversarial baselines. Across $>5.46$ million mIF image tiles, the model uncovers phenotype clusters with distinct proteomic profiles and prognostic significance, and improves survival prediction in attention-based multiple instance learning. While demonstrated on mIF data, AdvDINO is broadly applicable to other imaging domains -- including radiology, remote sensing, and autonomous driving -- where domain shift and limited annotated data hinder model generalization and interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdvDINOï¼Œè¿™æ˜¯ä¸€ç§é¢†åŸŸå¯¹æŠ—æ€§è‡ªç›‘ç£å­¦ä¹  (Domain-adversarial self-supervised learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­å¸¸è§çš„é¢†åŸŸåç§» (Domain shift) å’Œæ‰¹æ¬¡æ•ˆåº”é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ DINOv2 æ¶æ„ä¸­é›†æˆæ¢¯åº¦åå‘å±‚ (Gradient reversal layer)ï¼Œæœ‰æ•ˆåœ°ä¿ƒè¿›äº†é¢†åŸŸä¸å˜ç‰¹å¾çš„å­¦ä¹ ã€‚åœ¨éå°ç»†èƒè‚ºç™Œçš„å…­é€šé“å¤šé‡å…ç–«è§å…‰ (mIF) å…¨åˆ‡ç‰‡å›¾åƒæ•°æ®é›†ä¸Šï¼ŒAdvDINO æˆåŠŸå‡è½»äº†è½½ç»ç‰‡ç‰¹æœ‰çš„åå·®ï¼Œå¹¶ä»è¶…è¿‡ 546 ä¸‡å¼ å›¾åƒå—ä¸­å­¦ä¹ åˆ°æ¯”éå¯¹æŠ—æ€§åŸºçº¿æ›´å…·é²æ£’æ€§çš„ç”Ÿç‰©å­¦è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹ä¸ä»…æ­ç¤ºäº†å…·æœ‰æ˜¾è‘—é¢„åæ„ä¹‰çš„è¡¨å‹ç°‡ï¼Œè¿˜åœ¨åŸºäºæ³¨æ„åŠ›çš„å¤šç¤ºä¾‹å­¦ä¹  (Multiple instance learning) ä¸­æå‡äº†ç”Ÿå­˜é¢„æµ‹æ€§èƒ½ã€‚AdvDINO çš„é€šç”¨æ€§ä½¿å…¶èƒ½å¤Ÿæ‰©å±•è‡³æ”¾å°„å­¦ã€é¥æ„Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰å…¶ä»–å—é¢†åŸŸåç§»å›°æ‰°çš„æˆåƒé¢†åŸŸï¼Œä¸ºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.04955v1",
      "published_date": "2025-08-07 00:51:54 UTC",
      "updated_date": "2025-08-07 00:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:02:42.151154+00:00"
    },
    {
      "arxiv_id": "2508.10018v1",
      "title": "A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models",
      "title_zh": "ç«ç‘°ä¸å«ç«ç‘°ï¼Œä¾ç„¶èŠ¬èŠ³ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„èŒƒç•´åŒä¼¦è®º",
      "authors": [
        "Sridhar Mahadevan"
      ],
      "abstract": "Natural language is replete with superficially different statements, such as ``Charles Darwin wrote\" and ``Charles Darwin is the author of\", which carry the same meaning. Large language models (LLMs) should generate the same next-token probabilities in such cases, but usually do not. Empirical workarounds have been explored, such as using k-NN estimates of sentence similarity to produce smoothed estimates. In this paper, we tackle this problem more abstractly, introducing a categorical homotopy framework for LLMs. We introduce an LLM Markov category to represent probability distributions in language generated by an LLM, where the probability of a sentence, such as ``Charles Darwin wrote\" is defined by an arrow in a Markov category. However, this approach runs into difficulties as language is full of equivalent rephrases, and each generates a non-isomorphic arrow in the LLM Markov category. To address this fundamental problem, we use categorical homotopy techniques to capture ``weak equivalences\" in an LLM Markov category. We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on powerful theoretical results developed over the past half a century.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¯­ä¹‰ç›¸åŒä½†è¡¨è¾¾ä¸åŒçš„è¯­å¥æ—¶ï¼Œå¾€å¾€æ— æ³•ç”Ÿæˆä¸€è‡´çš„ next-token probabilitiesï¼ˆä¸‹æ–‡æ¦‚ç‡ï¼‰è¿™ä¸€åŸºç¡€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæŠ½è±¡çš„ categorical homotopyï¼ˆèŒƒç•´åŒä¼¦ï¼‰æ¡†æ¶ã€‚ç ”ç©¶è€…å¼•å…¥äº† LLM Markov categoryï¼ˆLLM é©¬å°”å¯å¤«èŒƒç•´ï¼‰æ¥è¡¨ç¤ºè¯­è¨€ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶ä¸­å¥å­çš„æ¦‚ç‡è¢«å®šä¹‰ä¸ºèŒƒç•´ä¸­çš„ç®­å¤´ã€‚ä¸ºäº†è§£å†³åŒä¹‰æ”¹å†™å¯¼è‡´çš„éåŒæ„ç®­å¤´é—®é¢˜ï¼Œè¯¥æ–¹æ³•è¿ç”¨ categorical homotopy æŠ€æœ¯æ¥æ•æ‰ LLM Markov category ä¸­çš„ weak equivalencesï¼ˆå¼±ç­‰ä»·ï¼‰ã€‚é€šè¿‡å€Ÿé‰´è¿‡å»åŠä¸ªä¸–çºªä¸­ higher algebraic K-theoryï¼ˆé«˜ç­‰ä»£æ•° K ç†è®ºï¼‰å’Œ model categoriesï¼ˆæ¨¡å‹èŒƒç•´ï¼‰çš„æ·±å±‚ç†è®ºæˆæœï¼Œæœ¬æ–‡ä¸ºç†è§£å’Œä¼˜åŒ– LLMs çš„è¯­è¨€ç”Ÿæˆé€»è¾‘æä¾›äº†å…¨æ–°çš„æ•°å­¦è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.AT"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages. arXiv admin note: text overlap with arXiv:2402.18732",
      "pdf_url": "https://arxiv.org/pdf/2508.10018v1",
      "published_date": "2025-08-07 00:48:30 UTC",
      "updated_date": "2025-08-07 00:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:20.999345+00:00"
    },
    {
      "arxiv_id": "2508.04953v1",
      "title": "Tesserae: Scalable Placement Policies for Deep Learning Workloads",
      "title_zh": "Tesseraeï¼šé¢å‘æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½çš„å¯æ‰©å±•æ”¾ç½®ç­–ç•¥",
      "authors": [
        "Song Bian",
        "Saurabh Agarwal",
        "Md. Tareq Mahmood",
        "Shivaram Venkataraman"
      ],
      "abstract": "Training deep learning (DL) models has become a dominant workload in data-centers and improving resource utilization is a key goal of DL cluster schedulers. In order to do this, schedulers typically incorporate placement policies that govern where jobs are placed on the cluster. Existing placement policies are either designed as ad-hoc heuristics or incorporated as constraints within a complex optimization problem and thus either suffer from suboptimal performance or poor scalability. Our key insight is that many placement constraints can be formulated as graph matching problems and based on that we design novel placement policies for minimizing job migration overheads and job packing. We integrate these policies into Tesserae and describe how our design leads to a scalable and effective GPU cluster scheduler. Our experimental results show that Tesserae improves average JCT by up to 1.62x and the Makespan by up to 1.15x compared with the existing schedulers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°æ®ä¸­å¿ƒæ·±åº¦å­¦ä¹ (Deep Learning)å·¥ä½œè´Ÿè½½çš„èµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTesseraeçš„é«˜å¯æ‰©å±•æ€§æ”¾ç½®ç­–ç•¥ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„è°ƒåº¦å™¨æ”¾ç½®ç­–ç•¥å¤šä¾èµ–äºç‰¹å®šå¯å‘å¼ç®—æ³•æˆ–å¤æ‚çš„ä¼˜åŒ–çº¦æŸï¼Œå¾€å¾€åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§(Scalability)ä¹‹é—´éš¾ä»¥å…¼é¡¾ã€‚Tesseraeçš„æ ¸å¿ƒè§è§£æ˜¯å°†ä½œä¸šæ”¾ç½®çº¦æŸè½¬åŒ–ä¸ºå›¾åŒ¹é…(Graph Matching)é—®é¢˜ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†æ—¨åœ¨æœ€å°åŒ–ä½œä¸šè¿ç§»å¼€é”€å’Œæå‡ä½œä¸šæ‰“åŒ…(Packing)æ•ˆç‡çš„æ–°å‹ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTesseraeåœ¨GPUé›†ç¾¤è°ƒåº¦ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰è°ƒåº¦å™¨ï¼Œå…¶å¹³å‡ä½œä¸šå®Œæˆæ—¶é—´(JCT)æå‡äº†1.62å€ï¼Œå®Œå·¥æ—¶é—´(Makespan)æå‡äº†1.15å€ã€‚è¯¥æ–¹æ¡ˆä¸ºè§£å†³å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ é›†ç¾¤ä¸­çš„èµ„æºç®¡ç†éš¾é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡æ‰©å±•æ€§çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "16 pages, 18 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.04953v1",
      "published_date": "2025-08-07 00:38:43 UTC",
      "updated_date": "2025-08-07 00:38:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:09.993403+00:00"
    },
    {
      "arxiv_id": "2508.05700v2",
      "title": "Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking",
      "title_zh": "é¢å‘ Pinterest å¹¿å‘Šæ’åºçš„å¤šç»´å¤§è§„æ¨¡åµŒå…¥è¡¨",
      "authors": [
        "Runze Su",
        "Jiayin Jin",
        "Jiacheng Li",
        "Sihan Wang",
        "Guangtong Bai",
        "Zelun Wang",
        "Li Tang",
        "Yixiong Meng",
        "Huasen Wu",
        "Zhimeng Pan",
        "Kungang Li",
        "Han Sun",
        "Zhifang Liu",
        "Haoyang Li",
        "Siping Ji",
        "Degao Peng",
        "Jinfeng Zhuang",
        "Ling Leng",
        "Prathibha Deshikachar"
      ],
      "abstract": "Large embedding tables are indispensable in modern recommendation systems, thanks to their ability to effectively capture and memorize intricate details of interactions among diverse entities. As we explore integrating large embedding tables into Pinterest's ads ranking models, we encountered not only common challenges such as sparsity and scalability, but also several obstacles unique to our context. Notably, our initial attempts to train large embedding tables from scratch resulted in neutral metrics. To tackle this, we introduced a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms. This approach greatly enriched the embedding tables and resulted in significant performance improvements. As a result, the multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid serving infrastructure to overcome GPU memory limits and elevate the scalability. This framework has been deployed in the Pinterest Ads system and achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Pinterest å¹¿å‘Šæ’åºæ¨¡å‹ä¸­é›†æˆ Large embedding tables çš„æ–¹æ³•ï¼Œä»¥è§£å†³æ¨èç³»ç»Ÿä¸­å¸¸è§çš„ç¨€ç–æ€§å’Œæ‰©å±•æ€§æŒ‘æˆ˜ã€‚é’ˆå¯¹åˆå§‹è®­ç»ƒæ•ˆæœå¹³åº¸çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°é¢–çš„ Multi-faceted pretraining schemeï¼Œé€šè¿‡æ•´åˆå¤šç§é¢„è®­ç»ƒç®—æ³•æå¤§åœ°ä¸°å¯Œäº†åµŒå…¥è¡¨çš„ä¿¡æ¯è¡¨è¾¾èƒ½åŠ›ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº† Click-Through Rate (CTR) å’Œ Conversion Rate (CVR) é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†åº”å¯¹å¤§è§„æ¨¡åœ¨çº¿æœåŠ¡çš„ç¡¬ä»¶é™åˆ¶ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€å¥— CPU-GPU hybrid serving infrastructureï¼Œæœ‰æ•ˆåœ°è§£å†³äº† GPU å†…å­˜é™åˆ¶å¹¶æå‡äº†ç³»ç»Ÿæ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶ç›®å‰å·²æ­£å¼éƒ¨ç½²äº Pinterest å¹¿å‘Šç³»ç»Ÿï¼Œå®ç°äº† 1.34% çš„åœ¨çº¿ CPC é™ä½å’Œ 2.60% çš„ CTR æå‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤§å¹…ä¼˜åŒ–æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡çš„åŒæ—¶ï¼ŒæˆåŠŸå°†ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆEnd-to-end latencyï¼‰ç»´æŒåœ¨ç¨³å®šæ°´å¹³ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05700v2",
      "published_date": "2025-08-07 00:31:20 UTC",
      "updated_date": "2025-08-11 23:31:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:12.984614+00:00"
    },
    {
      "arxiv_id": "2508.04945v1",
      "title": "Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering",
      "title_zh": "è¿ˆå‘ç¨³å¥çš„è§†è§‰åŠ¨ä½œè¯†åˆ«è¯„ä¼°ï¼šåˆ©ç”¨ä¹‰é¡¹èšç±»è§£å†³åŠ¨è¯æ­§ä¹‰",
      "authors": [
        "Louie Hong Yao",
        "Nicholas Jarvis",
        "Tianyu Jiang"
      ],
      "abstract": "Evaluating visual activity recognition systems is challenging due to inherent ambiguities in verb semantics and image interpretation. When describing actions in images, synonymous verbs can refer to the same event (e.g., brushing vs. grooming), while different perspectives can lead to equally valid but distinct verb choices (e.g., piloting vs. operating). Standard exact-match evaluation, which relies on a single gold answer, fails to capture these ambiguities, resulting in an incomplete assessment of model performance. To address this, we propose a vision-language clustering framework that constructs verb sense clusters, providing a more robust evaluation. Our analysis of the imSitu dataset shows that each image maps to an average of 2.8 sense clusters, with each cluster representing a distinct perspective of the image. We evaluate multiple activity recognition models and compare our cluster-based evaluation with standard evaluation methods. Additionally, our human alignment analysis suggests that the cluster-based evaluation better aligns with human judgements, offering a more nuanced assessment of model performance.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹è§†è§‰åŠ¨ä½œè¯†åˆ« (Visual Activity Recognition) è¯„ä¼°ä¸­å­˜åœ¨çš„åŠ¨è¯è¯­ä¹‰å’Œå›¾åƒè§£é‡Šæ¨¡ç³Šæ€§é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ä¼ ç»Ÿç²¾ç¡®åŒ¹é… (exact-match) è¯„ä¼°æ–¹æ³•å› ä¾èµ–å•ä¸€æ ‡å‡†ç­”æ¡ˆè€Œæ— æ³•æ•æ‰è¯­ä¹‰å¤šæ ·æ€§çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªè§†è§‰è¯­è¨€èšç±»æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºåŠ¨è¯ä¹‰é¡¹ç°‡ (verb sense clusters) æ¥æä¾›æ›´é²æ£’çš„è¯„ä¼°æŒ‡æ ‡ã€‚å¯¹ imSitu æ•°æ®é›†çš„åˆ†ææ˜¾ç¤ºï¼Œæ¯å¼ å›¾åƒå¹³å‡å…³è” 2.8 ä¸ªä¹‰é¡¹ç°‡ï¼Œä¸”æ¯ä¸ªç°‡ä»£è¡¨äº†å›¾åƒçš„ä¸€ä¸ªç‹¬ç‰¹è§†è§’ã€‚é€šè¿‡å¯¹å¤šç§æ´»åŠ¨è¯†åˆ«æ¨¡å‹çš„è¯„ä¼°ä»¥åŠäººç±»ä¸€è‡´æ€§åˆ†æ (human alignment analysis)ï¼Œç»“æœè¡¨æ˜åŸºäºèšç±»çš„è¯„ä¼°æ–¹æ³•æ¯”ä¼ ç»Ÿæ ‡å‡†æ–¹æ³•æ›´ç¬¦åˆäººç±»çš„åˆ¤æ–­é€»è¾‘ã€‚è¯¥ç ”ç©¶ä¸ºè§†è§‰åŠ¨ä½œè¯†åˆ«é¢†åŸŸæä¾›äº†ä¸€ç§æ›´ç»†è‡´ã€æ›´ç§‘å­¦çš„æ€§èƒ½è¡¡é‡æ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.04945v1",
      "published_date": "2025-08-07 00:22:15 UTC",
      "updated_date": "2025-08-07 00:22:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:22.985232+00:00"
    },
    {
      "arxiv_id": "2508.04943v1",
      "title": "TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring",
      "title_zh": "TRKTï¼šåŸºäºæ—¶åºå¢å¼ºå…³ç³»æ„ŸçŸ¥çŸ¥è¯†è¿ç§»çš„å¼±ç›‘ç£åŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆ",
      "authors": [
        "Zhu Xu",
        "Ting Lei",
        "Zhimin Li",
        "Guan Wang",
        "Qingchao Chen",
        "Yuxin Peng",
        "Yang liu"
      ],
      "abstract": "Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼±ç›‘ç£åŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆ (Weakly Supervised Dynamic Scene Graph Generation, WS-DSGG) ä¸­å¤–éƒ¨æ£€æµ‹å™¨éš¾ä»¥åº”å¯¹åŠ¨æ€å…³ç³»åœºæ™¯å¯¼è‡´å®šä½ä¸å‡†çš„é—®é¢˜ï¼Œæå‡ºäº† TRKT (Temporal-enhanced Relation-aware Knowledge Transferring) æ–¹æ³•ã€‚TRKT é€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°çŸ¥è¯†è½¬ç§»ï¼šé¦–å…ˆæ˜¯å…³ç³»æ„ŸçŸ¥çŸ¥è¯†æŒ–æ˜ï¼Œåˆ©ç”¨å¯¹è±¡å’Œå…³ç³»è§£ç å™¨ç”Ÿæˆç‰¹å®šç±»åˆ«çš„æ³¨æ„åŠ›å›¾ï¼Œå¹¶ç»“åˆå…‰æµ (Optical flow) é©±åŠ¨çš„å¸§é—´æ³¨æ„åŠ›å¢å¼ºç­–ç•¥ï¼Œä½¿å…¶å…·å¤‡è¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›å¹¶èƒ½æœ‰æ•ˆåº”å¯¹è¿åŠ¨æ¨¡ç³Šã€‚å…¶æ¬¡ï¼Œç ”ç©¶å¼•å…¥äº†åŒæµèåˆæ¨¡å— (Dual-stream Fusion Module)ï¼Œå°†è¿™äº›æ³¨æ„åŠ›å›¾é›†æˆåˆ°å¤–éƒ¨æ£€æµ‹ç»“æœä¸­ï¼Œä»¥ç²¾ç»†åŒ–å¯¹è±¡å®šä½å¹¶æå‡æè®®æ¡†çš„ç½®ä¿¡å¾—åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRKT åœ¨ Action Genome æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ (State-of-the-art)ã€‚è¯¥æ–¹æ³•é€šè¿‡æŒ–æ˜æ—¶åºå¢å¼ºçš„å…³ç³»æ„ŸçŸ¥çŸ¥è¯†ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†é™æ€å›¾åƒæ£€æµ‹å™¨åœ¨åŠ¨æ€è§†è§‰å…³ç³»ç†è§£ä¸­çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.04943v1",
      "published_date": "2025-08-07 00:17:45 UTC",
      "updated_date": "2025-08-07 00:17:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:23.190298+00:00"
    },
    {
      "arxiv_id": "2508.08300v1",
      "title": "LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models",
      "title_zh": "LLM-BIï¼šè¿ˆå‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å…¨è‡ªåŠ¨è´å¶æ–¯æ¨ç†",
      "authors": [
        "Yongchao Huang"
      ],
      "abstract": "A significant barrier to the widespread adoption of Bayesian inference is the specification of prior distributions and likelihoods, which often requires specialized statistical expertise. This paper investigates the feasibility of using a Large Language Model (LLM) to automate this process. We introduce LLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline for automating Bayesian workflows. As a proof-of-concept, we present two experiments focused on Bayesian linear regression. In Experiment I, we demonstrate that an LLM can successfully elicit prior distributions from natural language. In Experiment II, we show that an LLM can specify the entire model structure, including both priors and the likelihood, from a single high-level problem description. Our results validate the potential of LLMs to automate key steps in Bayesian modeling, enabling the possibility of an automated inference pipeline for probabilistic programming.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Large Language Model (LLM) è‡ªåŠ¨åŒ– Bayesian inference æµç¨‹çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨é™ä½è®¾å®š prior distributions å’Œ likelihoods æ—¶æ‰€éœ€çš„ä¸“ä¸šç»Ÿè®¡å­¦é—¨æ§›ã€‚ä½œè€…æå‡ºäº† LLM-BI (Large Language Model-driven Bayesian Inference) æ¦‚å¿µæ¡†æ¶ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªé’ˆå¯¹ Bayesian linear regression çš„å®éªŒå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªåŠ¨åŒ–å·¥ä½œæµçš„æ½œåŠ›ã€‚å®éªŒè¯æ˜ LLM èƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€ä¸­æˆåŠŸå¼•å¯¼å‡º prior distributionsï¼Œå¹¶èƒ½ä»…æ ¹æ®é«˜å±‚æ¬¡çš„é—®é¢˜æè¿°å®Œæ•´æŒ‡å®šåŒ…æ‹¬å…ˆéªŒå’Œ likelihood åœ¨å†…çš„æ¨¡å‹ç»“æ„ã€‚ç ”ç©¶ç»“æœéªŒè¯äº† LLM åœ¨è‡ªåŠ¨åŒ–è´å¶æ–¯å»ºæ¨¡å…³é”®æ­¥éª¤ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®ç° probabilistic programming çš„å…¨è‡ªåŠ¨æ¨ç†æµæ°´çº¿å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.08300v1",
      "published_date": "2025-08-07 00:00:59 UTC",
      "updated_date": "2025-08-07 00:00:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T10:03:28.996364+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 176,
  "processed_papers_count": 176,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T10:05:10.646438+00:00"
}
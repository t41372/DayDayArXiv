[
  {
    "arxiv_id": "2407.02500v1",
    "title": "Coupling Machine Learning with Ontology for Robotics Applications",
    "authors": [
      "Osama F. Zaki"
    ],
    "abstract": "In this paper I present a practical approach for coupling machine learning\n(ML) algorithms with knowledge bases (KB) ontology formalism. The lack of\navailability of prior knowledge in dynamic scenarios is without doubt a major\nbarrier for scalable machine intelligence. My view of the interaction between\nthe two tiers intelligence is based on the idea that when knowledge is not\nreadily available at the knowledge base tier, more knowledge can be extracted\nfrom the other tier, which has access to trained models from machine learning\nalgorithms. To analyse this hypothesis, I create two experiments based on\ndifferent datasets, which are related directly to risk-awareness of autonomous\nsystems, analysed by different machine learning algorithms (namely; multi-layer\nfeedforward backpropagation, Naive Bayes, and J48 decision tree). My analysis\nshows that the two-tiers intelligence approach for coupling ML and KB is\ncomputationally valid and the time complexity of the algorithms during the\nrobot mission is linear with the size of the data and knowledge.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.02500v1",
    "published_date": "2024-06-08 23:38:03 UTC",
    "updated_date": "2024-06-08 23:38:03 UTC"
  },
  {
    "arxiv_id": "2406.05590v3",
    "title": "NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security",
    "authors": [
      "Minghao Shao",
      "Sofija Jancheska",
      "Meet Udeshi",
      "Brendan Dolan-Gavitt",
      "Haoran Xi",
      "Kimberly Milner",
      "Boyuan Chen",
      "Max Yin",
      "Siddharth Garg",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Ramesh Karri",
      "Muhammad Shafique"
    ],
    "abstract": "Large Language Models (LLMs) are being deployed across various domains today.\nHowever, their capacity to solve Capture the Flag (CTF) challenges in\ncybersecurity has not been thoroughly evaluated. To address this, we develop a\nnovel method to assess LLMs in solving CTF challenges by creating a scalable,\nopen-source benchmark database specifically designed for these applications.\nThis database includes metadata for LLM testing and adaptive learning,\ncompiling a diverse range of CTF challenges from popular competitions.\nUtilizing the advanced function calling capabilities of LLMs, we build a fully\nautomated system with an enhanced workflow and support for external tool calls.\nOur benchmark dataset and automated framework allow us to evaluate the\nperformance of five LLMs, encompassing both black-box and open-source models.\nThis work lays the foundation for future research into improving the efficiency\nof LLMs in interactive cybersecurity tasks and automated task planning. By\nproviding a specialized benchmark, our project offers an ideal platform for\ndeveloping, testing, and refining LLM-based approaches to vulnerability\ndetection and resolution. Evaluating LLMs on these challenges and comparing\nwith human performance yields insights into their potential for AI-driven\ncybersecurity solutions to perform real-world threat management. We make our\nbenchmark dataset open source to public\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground\nautomated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05590v3",
    "published_date": "2024-06-08 22:21:42 UTC",
    "updated_date": "2025-02-18 12:26:33 UTC"
  },
  {
    "arxiv_id": "2406.05588v2",
    "title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
    "authors": [
      "Jason Cai",
      "Hang Su",
      "Monica Sunkara",
      "Igor Shalyminov",
      "Saab Mansour"
    ],
    "abstract": "Large Language Models (LLMs) are powerful models for generation tasks, but\nthey may not generate good quality outputs in their first attempt. Apart from\nmodel fine-tuning, existing approaches to improve prediction accuracy and\nquality typically involve LLM self-improvement / self-reflection that\nincorporate feedback from models themselves. Despite their effectiveness, these\nmethods are hindered by their high computational cost and lack of scalability.\nIn this work, we propose CERET, a method for refining text generations by\nconsidering semantic stability, entailment and inter-sample uncertainty\nmeasures. Experimental results show that CERET outperforms Self-consistency and\nSelf-rerank baselines consistently under various task setups, by ~1.6% in\nRouge-1 for abstractive summarization and ~3.5% in hit rate for question\nanswering. Compared to LLM Self-rerank method, our approach only requires 9.4%\nof its latency and is more cost-effective.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The source code and data samples are released at\n  https://github.com/amazon-science/CERET-LLM-refine",
    "pdf_url": "http://arxiv.org/pdf/2406.05588v2",
    "published_date": "2024-06-08 22:17:52 UTC",
    "updated_date": "2024-11-02 03:18:56 UTC"
  },
  {
    "arxiv_id": "2406.05587v1",
    "title": "Creativity Has Left the Chat: The Price of Debiasing Language Models",
    "authors": [
      "Behnam Mohammadi"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\nbut can exhibit biases and may generate toxic content. While alignment\ntechniques like Reinforcement Learning from Human Feedback (RLHF) reduce these\nissues, their impact on creativity, defined as syntactic and semantic\ndiversity, remains unexplored. We investigate the unintended consequences of\nRLHF on the creativity of LLMs through three experiments focusing on the\nLlama-2 series. Our findings reveal that aligned models exhibit lower entropy\nin token predictions, form distinct clusters in the embedding space, and\ngravitate towards \"attractor states\", indicating limited output diversity. Our\nfindings have significant implications for marketers who rely on LLMs for\ncreative tasks such as copywriting, ad creation, and customer persona\ngeneration. The trade-off between consistency and creativity in aligned models\nshould be carefully considered when selecting the appropriate model for a given\napplication. We also discuss the importance of prompt engineering in harnessing\nthe creative potential of base models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05587v1",
    "published_date": "2024-06-08 22:14:51 UTC",
    "updated_date": "2024-06-08 22:14:51 UTC"
  },
  {
    "arxiv_id": "2406.05572v2",
    "title": "Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction",
    "authors": [
      "Aidan Curtis",
      "Nishanth Kumar",
      "Jing Cao",
      "Tomás Lozano-Pérez",
      "Leslie Pack Kaelbling"
    ],
    "abstract": "Recent developments in pretrained large language models (LLMs) applied to\nrobotics have demonstrated their capacity for sequencing a set of discrete\nskills to achieve open-ended goals in simple robotic tasks. In this paper, we\nexamine the topic of LLM planning for a set of continuously parameterized\nskills whose execution must avoid violations of a set of kinematic, geometric,\nand physical constraints. We prompt the LLM to output code for a function with\nopen parameters, which, together with environmental constraints, can be viewed\nas a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved\nthrough sampling or optimization to find a skill sequence and continuous\nparameter settings that achieve the goal while avoiding constraint violations.\nAdditionally, we consider cases where the LLM proposes unsatisfiable CCSPs,\nsuch as those that are kinematically infeasible, dynamically unstable, or lead\nto collisions, and re-prompt the LLM to form a new CCSP accordingly.\nExperiments across three different simulated 3D domains demonstrate that our\nproposed strategy, PRoC3S, is capable of solving a wide range of complex\nmanipulation tasks with realistic constraints on continuous parameters much\nmore efficiently and effectively than existing baselines.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05572v2",
    "published_date": "2024-06-08 20:56:14 UTC",
    "updated_date": "2024-09-06 02:13:51 UTC"
  },
  {
    "arxiv_id": "2406.05564v1",
    "title": "Automata Extraction from Transformers",
    "authors": [
      "Yihao Zhang",
      "Zeming Wei",
      "Meng Sun"
    ],
    "abstract": "In modern machine (ML) learning systems, Transformer-based architectures have\nachieved milestone success across a broad spectrum of tasks, yet understanding\ntheir operational mechanisms remains an open problem. To improve the\ntransparency of ML systems, automata extraction methods, which interpret\nstateful ML models as automata typically through formal languages, have proven\neffective for explaining the mechanism of recurrent neural networks (RNNs).\nHowever, few works have been applied to this paradigm to Transformer models. In\nparticular, understanding their processing of formal languages and identifying\ntheir limitations in this area remains unexplored. In this paper, we propose an\nautomata extraction algorithm specifically designed for Transformer models.\nTreating the Transformer model as a black-box system, we track the model\nthrough the transformation process of their internal latent representations\nduring their operations, and then use classical pedagogical approaches like L*\nalgorithm to interpret them as deterministic finite-state automata (DFA).\nOverall, our study reveals how the Transformer model comprehends the structure\nof formal languages, which not only enhances the interpretability of the\nTransformer-based ML systems but also marks a crucial step toward a deeper\nunderstanding of how ML systems process formal languages. Code and data are\navailable at https://github.com/Zhang-Yihao/Transfomer2DFA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05564v1",
    "published_date": "2024-06-08 20:07:24 UTC",
    "updated_date": "2024-06-08 20:07:24 UTC"
  },
  {
    "arxiv_id": "2406.05559v1",
    "title": "ThatiAR: Subjectivity Detection in Arabic News Sentences",
    "authors": [
      "Reem Suwaileh",
      "Maram Hasanain",
      "Fatema Hubail",
      "Wajdi Zaghouani",
      "Firoj Alam"
    ],
    "abstract": "Detecting subjectivity in news sentences is crucial for identifying media\nbias, enhancing credibility, and combating misinformation by flagging\nopinion-based content. It provides insights into public sentiment, empowers\nreaders to make informed decisions, and encourages critical thinking. While\nresearch has developed methods and systems for this purpose, most efforts have\nfocused on English and other high-resourced languages. In this study, we\npresent the first large dataset for subjectivity detection in Arabic,\nconsisting of ~3.6K manually annotated sentences, and GPT-4o based explanation.\nIn addition, we included instructions (both in English and Arabic) to\nfacilitate LLM based fine-tuning. We provide an in-depth analysis of the\ndataset, annotation process, and extensive benchmark results, including PLMs\nand LLMs. Our analysis of the annotation process highlights that annotators\nwere strongly influenced by their political, cultural, and religious\nbackgrounds, especially at the beginning of the annotation process. The\nexperimental results suggest that LLMs with in-context learning provide better\nperformance. We aim to release the dataset and resources for the community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Subjectivity, Sentiment, Disinformation, Misinformation, Fake news,\n  LLMs, Transformers, Instruction Dataset",
    "pdf_url": "http://arxiv.org/pdf/2406.05559v1",
    "published_date": "2024-06-08 19:24:17 UTC",
    "updated_date": "2024-06-08 19:24:17 UTC"
  },
  {
    "arxiv_id": "2406.05551v1",
    "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
    "authors": [
      "Zhijun Liu",
      "Shuai Wang",
      "Sho Inoue",
      "Qibing Bai",
      "Haizhou Li"
    ],
    "abstract": "Audio language models have recently emerged as a promising approach for\nvarious audio generation tasks, relying on audio tokenizers to encode waveforms\ninto sequences of discrete symbols. Audio tokenization often poses a necessary\ncompromise between code bitrate and reconstruction accuracy. When dealing with\nlow-bitrate audio codes, language models are constrained to process only a\nsubset of the information embedded in the audio, which in turn restricts their\ngenerative capabilities. To circumvent these issues, we propose encoding audio\nas vector sequences in continuous space $\\mathbb R^d$ and autoregressively\ngenerating these sequences using a decoder-only diffusion transformer (ARDiT).\nOur findings indicate that ARDiT excels in zero-shot text-to-speech and\nexhibits performance that compares to or even surpasses that of\nstate-of-the-art models. High-bitrate continuous speech representation enables\nalmost flawless reconstruction, allowing our model to achieve nearly perfect\nspeech editing. Our experiments reveal that employing Integral Kullback-Leibler\n(IKL) divergence for distillation at each autoregressive step significantly\nboosts the perceived quality of the samples. Simultaneously, it condenses the\niterative sampling process of the diffusion model into a single step.\nFurthermore, ARDiT can be trained to predict several continuous vectors in one\nstep, significantly reducing latency during sampling. Impressively, one of our\nmodels can generate $170$ ms of $24$ kHz speech per evaluation step with\nminimal degradation in performance. Audio samples are available at\nhttp://ardit-tts.github.io/ .",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05551v1",
    "published_date": "2024-06-08 18:57:13 UTC",
    "updated_date": "2024-06-08 18:57:13 UTC"
  },
  {
    "arxiv_id": "2406.05546v1",
    "title": "Training Through Failure: Effects of Data Consistency in Parallel Machine Learning Training",
    "authors": [
      "Ray Cao",
      "Sherry Luo",
      "Steve Gan",
      "Sujeeth Jinesh"
    ],
    "abstract": "In this study, we explore the impact of relaxing data consistency in parallel\nmachine learning training during a failure using various parameter server\nconfigurations. Our failure recovery strategies include traditional\ncheckpointing, chain replication (which ensures a backup server takes over in\ncase of failure), and a novel stateless parameter server approach. In the\nstateless approach, workers continue generating gradient updates even if the\nparameter server is down, applying these updates once the server is back\nonline. We compare these techniques to a standard checkpointing approach, where\nthe training job is resumed from the latest checkpoint.\n  To assess the resilience and performance of each configuration, we\nintentionally killed the parameter server during training for each experiment.\nOur experiment results indicate that the stateless parameter server approach\ncontinues to train towards convergence and improves accuracy as much as 10\\% in\nthe face of a failure despite using stale weights and gradients. The chain\nreplication and checkpointing techniques demonstrate convergence but suffer\nfrom setbacks in accuracy due to restarting from old checkpoints. These results\nsuggest that allowing workers to continue generating updates during server\ndowntime and applying these updates later can effectively improve hardware\nutilization. Furthermore, despite higher resource usage, the stateless\nparameter server method incurs similar monetary costs in terms of hardware\nusage compared to standard checkpointing methods due to the pricing structure\nof common cloud providers.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05546v1",
    "published_date": "2024-06-08 18:31:56 UTC",
    "updated_date": "2024-06-08 18:31:56 UTC"
  },
  {
    "arxiv_id": "2406.05543v1",
    "title": "VP-LLM: Text-Driven 3D Volume Completion with Large Language Models through Patchification",
    "authors": [
      "Jianmeng Liu",
      "Yichen Liu",
      "Yuyao Zhang",
      "Zeyuan Meng",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "Recent conditional 3D completion works have mainly relied on CLIP or BERT to\nencode textual information, which cannot support complex instruction.\nMeanwhile, large language models (LLMs) have shown great potential in\nmulti-modal understanding and generation tasks. Inspired by the recent\nadvancements of LLM, we present Volume Patch LLM (VP-LLM), which leverages LLMs\nto perform conditional 3D completion in a single-forward pass. To integrate a\n3D model into the LLM tokenization configuration, the incomplete 3D object is\nfirst divided into small patches that can be encoded independently. These\nencoded patches are then fed into an LLM along with the text prompt,\ninstructing the LLM to capture the relations between these patches as well as\ninjecting semantic meanings into the 3D object. Our results demonstrate a\nstrong ability of LLMs to interpret complex text instructions and understand 3D\nobjects, surpassing state-of-the-art diffusion-based 3D completion models in\ngeneration quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.05543v1",
    "published_date": "2024-06-08 18:17:09 UTC",
    "updated_date": "2024-06-08 18:17:09 UTC"
  },
  {
    "arxiv_id": "2406.05540v2",
    "title": "A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding",
    "authors": [
      "Yiqing Shen",
      "Zan Chen",
      "Michail Mamalakis",
      "Luhan He",
      "Haiyang Xia",
      "Tianbin Li",
      "Yanzhou Su",
      "Junjun He",
      "Yu Guang Wang"
    ],
    "abstract": "The parallels between protein sequences and natural language in their\nsequential structures have inspired the application of large language models\n(LLMs) to protein understanding. Despite the success of LLMs in NLP, their\neffectiveness in comprehending protein sequences remains an open question,\nlargely due to the absence of datasets linking protein sequences to descriptive\ntext. Researchers have then attempted to adapt LLMs for protein understanding\nby integrating a protein sequence encoder with a pre-trained LLM. However, this\nadaptation raises a fundamental question: \"Can LLMs, originally designed for\nNLP, effectively comprehend protein sequences as a form of language?\" Current\ndatasets fall short in addressing this question due to the lack of a direct\ncorrelation between protein sequences and corresponding text descriptions,\nlimiting the ability to train and evaluate LLMs for protein understanding\neffectively. To bridge this gap, we introduce ProteinLMDataset, a dataset\nspecifically designed for further self-supervised pretraining and supervised\nfine-tuning (SFT) of LLMs to enhance their capability for protein sequence\ncomprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for\npretraining and 893,000 instructions for SFT. Additionally, we present\nProteinLMBench, the first benchmark dataset consisting of 944 manually verified\nmultiple-choice questions for assessing the protein understanding capabilities\nof LLMs. ProteinLMBench incorporates protein-related details and sequences in\nmultiple languages, establishing a new standard for evaluating LLMs' abilities\nin protein comprehension. The large language model InternLM2-7B, pretrained and\nfine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench,\nachieving the highest accuracy score.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05540v2",
    "published_date": "2024-06-08 18:11:30 UTC",
    "updated_date": "2024-07-08 16:39:35 UTC"
  },
  {
    "arxiv_id": "2406.05535v1",
    "title": "Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability",
    "authors": [
      "Junqi Gao",
      "Biqing Qi",
      "Yao Li",
      "Zhichang Guo",
      "Dong Li",
      "Yuming Xing",
      "Dazhi Zhang"
    ],
    "abstract": "The transferability of adversarial perturbations provides an effective\nshortcut for black-box attacks. Targeted perturbations have greater\npracticality but are more difficult to transfer between models. In this paper,\nwe experimentally and theoretically demonstrated that neural networks trained\non the same dataset have more consistent performance in\nHigh-Sample-Density-Regions (HSDR) of each class instead of low sample density\nregions. Therefore, in the target setting, adding perturbations towards HSDR of\nthe target class is more effective in improving transferability. However,\ndensity estimation is challenging in high-dimensional scenarios. Further\ntheoretical and experimental verification demonstrates that easy samples with\nlow loss are more likely to be located in HSDR. Perturbations towards such easy\nsamples in the target class can avoid density estimation for HSDR location.\nBased on the above facts, we verified that adding perturbations to easy samples\nin the target class improves targeted adversarial transferability of existing\nattack methods. A generative targeted attack strategy named Easy Sample\nMatching Attack (ESMA) is proposed, which has a higher success rate for\ntargeted attacks and outperforms the SOTA generative method. Moreover, ESMA\nrequires only 5% of the storage space and much less computation time comparing\nto the current SOTA, as ESMA attacks all classes with only one model instead of\nseperate models for each class. Our code is available at\nhttps://github.com/gjq100/ESMA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05535v1",
    "published_date": "2024-06-08 17:33:23 UTC",
    "updated_date": "2024-06-08 17:33:23 UTC"
  },
  {
    "arxiv_id": "2406.05534v1",
    "title": "Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing",
    "authors": [
      "Biqing Qi",
      "Pengfei Li",
      "Fangyuan Li",
      "Junqi Gao",
      "Kaiyan Zhang",
      "Bowen Zhou"
    ],
    "abstract": "Direct Preference Optimization (DPO) improves the alignment of large language\nmodels (LLMs) with human values by training directly on human preference\ndatasets, eliminating the need for reward models. However, due to the presence\nof cross-domain human preferences, direct continual training can lead to\ncatastrophic forgetting, limiting DPO's performance and efficiency. Inspired by\nintraspecific competition driving species evolution, we propose a Online\nFast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating\ncompetition through fast and slow chasing among models to facilitate rapid\nadaptation. Specifically, we first derive the regret upper bound for online\nlearning, validating our motivation with a min-max optimization pattern. Based\non this, we introduce two identical modules using Low-rank Adaptive (LoRA) with\ndifferent optimization speeds to simulate intraspecific competition, and\npropose a new regularization term to guide their learning. To further mitigate\ncatastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with\nLoRA modules combination strategy, resulting in the Cross domain Online\nFast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of\nfast modules parameters from different task domains, fully utilizing historical\ninformation to achive continual value alignment. Experimental results show that\nOFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in\ncross-domain continual learning scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05534v1",
    "published_date": "2024-06-08 17:30:54 UTC",
    "updated_date": "2024-06-08 17:30:54 UTC"
  },
  {
    "arxiv_id": "2406.05533v1",
    "title": "PAPR in Motion: Seamless Point-level 3D Scene Interpolation",
    "authors": [
      "Shichong Peng",
      "Yanshu Zhang",
      "Ke Li"
    ],
    "abstract": "We propose the problem of point-level 3D scene interpolation, which aims to\nsimultaneously reconstruct a 3D scene in two states from multiple views,\nsynthesize smooth point-level interpolations between them, and render the scene\nfrom novel viewpoints, all without any supervision between the states. The\nprimary challenge is on achieving a smooth transition between states that may\ninvolve significant and non-rigid changes. To address these challenges, we\nintroduce \"PAPR in Motion\", a novel approach that builds upon the recent\nProximity Attention Point Rendering (PAPR) technique, which can deform a point\ncloud to match a significantly different shape and render a visually coherent\nscene even after non-rigid deformations. Our approach is specifically designed\nto maintain the temporal consistency of the geometric structure by introducing\nvarious regularization techniques for PAPR. The result is a method that can\neffectively bridge large scene changes and produce visually coherent and\ntemporally smooth interpolations in both geometry and appearance. Evaluation\nacross diverse motion types demonstrates that \"PAPR in Motion\" outperforms the\nleading neural renderer for dynamic scenes. For more results and code, please\nvisit our project website at https://niopeng.github.io/PAPR-in-Motion/ .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05533v1",
    "published_date": "2024-06-08 17:27:27 UTC",
    "updated_date": "2024-06-08 17:27:27 UTC"
  },
  {
    "arxiv_id": "2406.05532v2",
    "title": "Exploring Adversarial Robustness of Deep State Space Models",
    "authors": [
      "Biqing Qi",
      "Yang Luo",
      "Junqi Gao",
      "Pengfei Li",
      "Kai Tian",
      "Zhiyuan Ma",
      "Bowen Zhou"
    ],
    "abstract": "Deep State Space Models (SSMs) have proven effective in numerous task\nscenarios but face significant security challenges due to Adversarial\nPerturbations (APs) in real-world deployments. Adversarial Training (AT) is a\nmainstream approach to enhancing Adversarial Robustness (AR) and has been\nvalidated on various traditional DNN architectures. However, its effectiveness\nin improving the AR of SSMs remains unclear. While many enhancements in SSM\ncomponents, such as integrating Attention mechanisms and expanding to\ndata-dependent SSM parameterizations, have brought significant gains in\nStandard Training (ST) settings, their potential benefits in AT remain\nunexplored. To investigate this, we evaluate existing structural variants of\nSSMs with AT to assess their AR performance. We observe that pure SSM\nstructures struggle to benefit from AT, whereas incorporating Attention yields\na markedly better trade-off between robustness and generalization for SSMs in\nAT compared to other components. Nonetheless, the integration of Attention also\nleads to Robust Overfitting (RO) issues. To understand these phenomena, we\nempirically and theoretically analyze the output error of SSMs under AP. We\nfind that fixed-parameterized SSMs have output error bounds strictly related to\ntheir parameters, limiting their AT benefits, while input-dependent SSMs may\nface the problem of error explosion. Furthermore, we show that the Attention\ncomponent effectively scales the output error of SSMs during training, enabling\nthem to benefit more from AT, but at the cost of introducing RO due to its high\nmodel complexity. Inspired by this, we propose a simple and effective Adaptive\nScaling (AdS) mechanism that brings AT performance close to\nAttention-integrated SSMs without introducing the issue of RO. Our code is\navailable at\nhttps://github.com/Biqing-Qi/Exploring-Adversarial-Robustness-of-Deep-State-Space-Models.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05532v2",
    "published_date": "2024-06-08 17:25:48 UTC",
    "updated_date": "2024-10-09 02:28:56 UTC"
  },
  {
    "arxiv_id": "2406.05531v1",
    "title": "Enhancing Adversarial Transferability via Information Bottleneck Constraints",
    "authors": [
      "Biqing Qi",
      "Junqi Gao",
      "Jianxing Liu",
      "Ligang Wu",
      "Bowen Zhou"
    ],
    "abstract": "From the perspective of information bottleneck (IB) theory, we propose a\nnovel framework for performing black-box transferable adversarial attacks named\nIBTA, which leverages advancements in invariant features. Intuitively,\ndiminishing the reliance of adversarial perturbations on the original data,\nunder equivalent attack performance constraints, encourages a greater reliance\non invariant features that contributes most to classification, thereby\nenhancing the transferability of adversarial attacks. Building on this\nmotivation, we redefine the optimization of transferable attacks using a novel\ntheoretical framework that centers around IB. Specifically, to overcome the\nchallenge of unoptimizable mutual information, we propose a simple and\nefficient mutual information lower bound (MILB) for approximating computation.\nMoreover, to quantitatively evaluate mutual information, we utilize the Mutual\nInformation Neural Estimator (MINE) to perform a thorough analysis. Our\nexperiments on the ImageNet dataset well demonstrate the efficiency and\nscalability of IBTA and derived MILB. Our code is available at\nhttps://github.com/Biqing-Qi/Enhancing-Adversarial-Transferability-via-Information-Bottleneck-Constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05531v1",
    "published_date": "2024-06-08 17:25:31 UTC",
    "updated_date": "2024-06-08 17:25:31 UTC"
  },
  {
    "arxiv_id": "2406.05516v3",
    "title": "Verbalized Probabilistic Graphical Modeling",
    "authors": [
      "Hengguan Huang",
      "Xing Shen",
      "Songtao Wang",
      "Lingfa Meng",
      "Dianbo Liu",
      "Hao Wang",
      "Samir Bhatt"
    ],
    "abstract": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05516v3",
    "published_date": "2024-06-08 16:35:31 UTC",
    "updated_date": "2025-03-04 18:07:34 UTC"
  },
  {
    "arxiv_id": "2406.05506v2",
    "title": "Towards a Benchmark for Causal Business Process Reasoning with LLMs",
    "authors": [
      "Fabiana Fournier",
      "Lior Limonad",
      "Inna Skarbovsky"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used for boosting\norganizational efficiency and automating tasks. While not originally designed\nfor complex cognitive processes, recent efforts have further extended to employ\nLLMs in activities such as reasoning, planning, and decision-making. In\nbusiness processes, such abilities could be invaluable for leveraging on the\nmassive corpora LLMs have been trained on for gaining deep understanding of\nsuch processes. In this work, we plant the seeds for the development of a\nbenchmark to assess the ability of LLMs to reason about causal and process\nperspectives of business operations. We refer to this view as\nCausally-augmented Business Processes (BP^C). The core of the benchmark\ncomprises a set of BP^C related situations, a set of questions about these\nsituations, and a set of deductive rules employed to systematically resolve the\nground truth answers to these questions. Also with the power of LLMs, the seed\nis then instantiated into a larger-scale set of domain-specific situations and\nquestions. Reasoning on BP^C is of crucial importance for process interventions\nand process improvement. Our benchmark, accessible at\nhttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possible\nmodalities: testing the performance of any target LLM and training an LLM to\nadvance its capability to reason about BP^C.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.05506v2",
    "published_date": "2024-06-08 16:10:53 UTC",
    "updated_date": "2024-07-16 15:48:32 UTC"
  },
  {
    "arxiv_id": "2406.05505v1",
    "title": "I-SIRch: AI-Powered Concept Annotation Tool For Equitable Extraction And Analysis Of Safety Insights From Maternity Investigations",
    "authors": [
      "Mohit Kumar Singh",
      "Georgina Cosma",
      "Patrick Waterson",
      "Jonathan Back",
      "Gyuchan Thomas Jun"
    ],
    "abstract": "Maternity care is a complex system involving treatments and interactions\nbetween patients, providers, and the care environment. To improve patient\nsafety and outcomes, understanding the human factors (e.g. individuals\ndecisions, local facilities) influencing healthcare delivery is crucial.\nHowever, most current tools for analysing healthcare data focus only on\nbiomedical concepts (e.g. health conditions, procedures and tests), overlooking\nthe importance of human factors. We developed a new approach called I-SIRch,\nusing artificial intelligence to automatically identify and label human factors\nconcepts in maternity healthcare investigation reports describing adverse\nmaternity incidents produced by England's Healthcare Safety Investigation\nBranch (HSIB). These incident investigation reports aim to identify\nopportunities for learning and improving maternal safety across the entire\nhealthcare system. I-SIRch was trained using real data and tested on both real\nand simulated data to evaluate its performance in identifying human factors\nconcepts. When applied to real reports, the model achieved a high level of\naccuracy, correctly identifying relevant concepts in 90\\% of the sentences from\n97 reports. Applying I-SIRch to analyse these reports revealed that certain\nhuman factors disproportionately affected mothers from different ethnic groups.\nOur work demonstrates the potential of using automated tools to identify human\nfactors concepts in maternity incident investigation reports, rather than\nfocusing solely on biomedical concepts. This approach opens up new\npossibilities for understanding the complex interplay between social,\ntechnical, and organisational factors influencing maternal safety and\npopulation health outcomes. By taking a more comprehensive view of maternal\nhealthcare delivery, we can develop targeted interventions to address\ndisparities and improve maternal outcomes.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05505v1",
    "published_date": "2024-06-08 16:05:31 UTC",
    "updated_date": "2024-06-08 16:05:31 UTC"
  },
  {
    "arxiv_id": "2406.05498v3",
    "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
    "authors": [
      "Xunguang Wang",
      "Daoyuan Wu",
      "Zhenlan Ji",
      "Zongjie Li",
      "Pingchuan Ma",
      "Shuai Wang",
      "Yingjiu Li",
      "Yang Liu",
      "Ning Liu",
      "Juergen Rahmel"
    ],
    "abstract": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance (in detection state) to concurrently protect the target LLM\ninstance (in normal answering state) in the normal stack and collaborate with\nit for checkpoint-based access control. The effectiveness of SelfDefend builds\nupon our observation that existing LLMs can identify harmful prompts or\nintentions in user queries, which we empirically validate using mainstream\nGPT-3.5/4 models against major jailbreak attacks. To further improve the\ndefense's robustness and minimize costs, we employ a data distillation approach\nto tune dedicated open-source defense models. When deployed to protect\nGPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven\nstate-of-the-art defenses and match the performance of GPT-4-based SelfDefend,\nwith significantly lower extra delays. Further experiments show that the tuned\nmodels are robust to adaptive jailbreaks and prompt injections.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by USENIX Security Symposium 2025. Please cite the\n  conference version of this paper, i.e., \"Xunguang Wang, Daoyuan Wu, Zhenlan\n  Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and\n  Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking\n  in a Practical Manner. In Proc. USENIX Security, 2025.\"",
    "pdf_url": "http://arxiv.org/pdf/2406.05498v3",
    "published_date": "2024-06-08 15:45:31 UTC",
    "updated_date": "2025-02-05 10:29:07 UTC"
  },
  {
    "arxiv_id": "2406.05488v1",
    "title": "Online Policy Distillation with Decision-Attention",
    "authors": [
      "Xinqiang Yu",
      "Chuanguang Yang",
      "Chengqing Yu",
      "Libo Huang",
      "Zhulin An",
      "Yongjun Xu"
    ],
    "abstract": "Policy Distillation (PD) has become an effective method to improve deep\nreinforcement learning tasks. The core idea of PD is to distill policy\nknowledge from a teacher agent to a student agent. However, the teacher-student\nframework requires a well-trained teacher model which is computationally\nexpensive.In the light of online knowledge distillation, we study the knowledge\ntransfer between different policies that can learn diverse knowledge from the\nsame environment.In this work, we propose Online Policy Distillation (OPD) with\nDecision-Attention (DA), an online learning framework in which different\npolicies operate in the same environment to learn different perspectives of the\nenvironment and transfer knowledge to each other to obtain better performance\ntogether. With the absence of a well-performance teacher policy, the\ngroup-derived targets play a key role in transferring group knowledge to each\nstudent policy. However, naive aggregation functions tend to cause student\npolicies quickly homogenize. To address the challenge, we introduce the\nDecision-Attention module to the online policies distillation framework. The\nDecision-Attention module can generate a distinct set of weights for each\npolicy to measure the importance of group members. We use the Atari platform\nfor experiments with various reinforcement learning algorithms, including PPO\nand DQN. In different tasks, our method can perform better than an independent\ntraining policy on both PPO and DQN algorithms. This suggests that our OPD-DA\ncan transfer knowledge between different policies well and help agents obtain\nmore rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05488v1",
    "published_date": "2024-06-08 14:40:53 UTC",
    "updated_date": "2024-06-08 14:40:53 UTC"
  },
  {
    "arxiv_id": "2406.05478v1",
    "title": "Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis",
    "authors": [
      "Zanlin Ni",
      "Yulin Wang",
      "Renping Zhou",
      "Jiayi Guo",
      "Jinyi Hu",
      "Zhiyuan Liu",
      "Shiji Song",
      "Yuan Yao",
      "Gao Huang"
    ],
    "abstract": "The field of image synthesis is currently flourishing due to the advancements\nin diffusion models. While diffusion models have been successful, their\ncomputational intensity has prompted the pursuit of more efficient\nalternatives. As a representative work, non-autoregressive Transformers (NATs)\nhave been recognized for their rapid generation. However, a major drawback of\nthese models is their inferior performance compared to diffusion models. In\nthis paper, we aim to re-evaluate the full potential of NATs by revisiting the\ndesign of their training and inference strategies. Specifically, we identify\nthe complexities in properly configuring these strategies and indicate the\npossible sub-optimality in existing heuristic-driven designs. Recognizing this,\nwe propose to go beyond existing methods by directly solving the optimal\nstrategies in an automatic framework. The resulting method, named AutoNAT,\nadvances the performance boundaries of NATs notably, and is able to perform\ncomparably with the latest diffusion models at a significantly reduced\ninference cost. The effectiveness of AutoNAT is validated on four benchmark\ndatasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M. Our code is available at\nhttps://github.com/LeapLabTHU/ImprovedNAT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05478v1",
    "published_date": "2024-06-08 13:52:20 UTC",
    "updated_date": "2024-06-08 13:52:20 UTC"
  },
  {
    "arxiv_id": "2406.10248v4",
    "title": "On the Worst Prompt Performance of Large Language Models",
    "authors": [
      "Bowen Cao",
      "Deng Cai",
      "Zhisong Zhang",
      "Yuexian Zou",
      "Wai Lam"
    ],
    "abstract": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10248v4",
    "published_date": "2024-06-08 13:40:38 UTC",
    "updated_date": "2024-10-30 09:48:52 UTC"
  },
  {
    "arxiv_id": "2406.05464v2",
    "title": "DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models",
    "authors": [
      "Tzu-Quan Lin",
      "Hung-yi Lee",
      "Hao Tang"
    ],
    "abstract": "Self-supervised speech models have shown to be useful for various tasks, but\ntheir large size limits the use in devices with low computing power and memory.\nIn this work, we explore early exit, an approach for reducing latency by\nexiting the forward process of a network early. Most approaches of early exit\nneed a separate early exit model for each task, with some even requiring\nfine-tuning of the entire pretrained model. We introduce Data Adaptive\nSelf-Supervised Early Exit (DAISY), an approach that decides when to exit based\non the self-supervised loss, eliminating the need for multiple round of\ntraining and fine-tuning. DAISY matches the performance of HuBERT on the\nMiniSUPERB benchmark, but with much faster inference times. Our analysis on the\nadaptivity of DAISY shows that the model exits early (using fewer layers) on\nclean data while exits late (using more layers) on noisy data, dynamically\nadjusting the computational cost of inference based on the noise level of each\nsample.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05464v2",
    "published_date": "2024-06-08 12:58:13 UTC",
    "updated_date": "2024-08-29 19:30:55 UTC"
  },
  {
    "arxiv_id": "2407.16888v2",
    "title": "A Nested Model for AI Design and Validation",
    "authors": [
      "Akshat Dubey",
      "Zewen Yang",
      "Georges Hattab"
    ],
    "abstract": "The growing AI field faces trust, transparency, fairness, and discrimination\nchallenges. Despite the need for new regulations, there is a mismatch between\nregulatory science and AI, preventing a consistent framework. A five-layer\nnested model for AI design and validation aims to address these issues and\nstreamline AI application design and validation, improving fairness, trust, and\nAI adoption. This model aligns with regulations, addresses AI practitioner's\ndaily challenges, and offers prescriptive guidance for determining appropriate\nevaluation approaches by identifying unique validity threats. We have three\nrecommendations motivated by this model: authors should distinguish between\nlayers when claiming contributions to clarify the specific areas in which the\ncontribution is made and to avoid confusion, authors should explicitly state\nupstream assumptions to ensure that the context and limitations of their AI\nsystem are clearly understood, AI venues should promote thorough testing and\nvalidation of AI systems and their compliance with regulatory requirements.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16888v2",
    "published_date": "2024-06-08 12:46:12 UTC",
    "updated_date": "2024-08-01 11:46:26 UTC"
  },
  {
    "arxiv_id": "2406.05460v2",
    "title": "Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition",
    "authors": [
      "Chang Tian",
      "Wenpeng Yin",
      "Dan Li",
      "Marie-Francine Moens"
    ],
    "abstract": "Few-shot named entity recognition (NER) systems recognize entities using a\nfew labeled training examples. The general pipeline consists of a span detector\nto identify entity spans in text and an entity-type classifier to assign types\nto entities. Current span detectors rely on extensive manual labeling to guide\ntraining. Almost every span detector requires initial training on basic span\nfeatures followed by adaptation to task-specific features. This process leads\nto repetitive training of the basic span features among span detectors.\nAdditionally, metric-based entity-type classifiers, such as prototypical\nnetworks, typically employ a specific metric that gauges the distance between\nthe query sample and entity-type referents, ultimately assigning the most\nprobable entity type to the query sample. However, these classifiers encounter\nthe sample dependency problem, primarily stemming from the limited samples\navailable for each entity-type referent. To address these challenges, we\nproposed an improved few-shot NER pipeline. First, we introduce a steppingstone\nspan detector that is pre-trained on open-domain Wikipedia data. It can be used\nto initialize the pipeline span detector to reduce the repetitive training of\nbasic features. Second, we leverage a large language model (LLM) to set\nreliable entity-type referents, eliminating reliance on few-shot samples of\neach type. Our model exhibits superior performance with fewer training steps\nand human-labeled data compared with baselines, as demonstrated through\nextensive experiments on various datasets. Particularly in fine-grained\nfew-shot NER settings, our model outperforms strong baselines, including\nChatGPT. We will publicly release the code, datasets, LLM outputs, and model\ncheckpoints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ieee access: https://doi.org/10.1109/ACCESS.2024.3374727",
    "pdf_url": "http://arxiv.org/pdf/2406.05460v2",
    "published_date": "2024-06-08 12:36:30 UTC",
    "updated_date": "2024-06-18 23:45:14 UTC"
  },
  {
    "arxiv_id": "2406.05446v1",
    "title": "Design of reliable technology valuation model with calibrated machine learning of patent indicators",
    "authors": [
      "Seunghyun Lee",
      "Janghyeok Yoon",
      "Jaewoong Choi"
    ],
    "abstract": "Machine learning (ML) has revolutionized the digital transformation of\ntechnology valuation by predicting the value of patents with high accuracy.\nHowever, the lack of validation regarding the reliability of these models\nhinders experts from fully trusting the confidence of model predictions. To\naddress this issue, we propose an analytical framework for reliable technology\nvaluation using calibrated ML models, which provide robust confidence levels in\nmodel predictions. We extract quantitative patent indicators that represent\nvarious technology characteristics as input data, using the patent maintenance\nperiod as a proxy for technology values. Multiple ML models are developed to\ncapture the nonlinear relationship between patent indicators and technology\nvalue. The reliability and accuracy of these models are evaluated, presenting a\nPareto-front map where the expected calibration error, Matthews correlation\ncoefficient and F1-scores are compared. After identifying the best-performing\nmodel, we apply SHapley Additive exPlanation (SHAP) analysis to pinpoint the\nmost significant input features by confidence bin. Through a case study, we\nconfirmed that the proposed approach offers a practical guideline for\ndeveloping reliable and accurate ML-based technology valuation models, with\nsignificant implications for both academia and industry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05446v1",
    "published_date": "2024-06-08 11:52:37 UTC",
    "updated_date": "2024-06-08 11:52:37 UTC"
  },
  {
    "arxiv_id": "2406.05443v1",
    "title": "Novel Approach to Intrusion Detection: Introducing GAN-MSCNN-BILSTM with LIME Predictions",
    "authors": [
      "Asmaa Benchama",
      "Khalid Zebbara"
    ],
    "abstract": "This paper introduces an innovative intrusion detection system that harnesses\nGenerative Adversarial Networks (GANs), Multi-Scale Convolutional Neural\nNetworks (MSCNNs), and Bidirectional Long Short-Term Memory (BiLSTM) networks,\nsupplemented by Local Interpretable Model-Agnostic Explanations (LIME) for\ninterpretability. Employing a GAN, the system generates realistic network\ntraffic data, encompassing both normal and attack patterns. This synthesized\ndata is then fed into an MSCNN-BiLSTM architecture for intrusion detection. The\nMSCNN layer extracts features from the network traffic data at different\nscales, while the BiLSTM layer captures temporal dependencies within the\ntraffic sequences. Integration of LIME allows for explaining the model's\ndecisions. Evaluation on the Hogzilla dataset, a standard benchmark, showcases\nan impressive accuracy of 99.16\\% for multi-class classification and 99.10\\%\nfor binary classification, while ensuring interpretability through LIME. This\nfusion of deep learning and interpretability presents a promising avenue for\nenhancing intrusion detection systems by improving transparency and decision\nsupport in network security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05443v1",
    "published_date": "2024-06-08 11:26:44 UTC",
    "updated_date": "2024-06-08 11:26:44 UTC"
  },
  {
    "arxiv_id": "2406.12898v1",
    "title": "A Comprehensive Evaluation of Generative Models in Calorimeter Shower Simulation",
    "authors": [
      "Farzana Yasmin Ahmad",
      "Vanamala Venkataswamy",
      "Geoffrey Fox"
    ],
    "abstract": "The pursuit of understanding fundamental particle interactions has reached\nunparalleled precision levels. Particle physics detectors play a crucial role\nin generating low-level object signatures that encode collision physics.\nHowever, simulating these particle collisions is a demanding task in terms of\nmemory and computation which will be exasperated with larger data volumes, more\ncomplex detectors, and a higher pileup environment in the High-Luminosity LHC.\nThe introduction of \"Fast Simulation\" has been pivotal in overcoming\ncomputational bottlenecks. The use of deep-generative models has sparked a\nsurge of interest in surrogate modeling for detector simulations, generating\nparticle showers that closely resemble the observed data. Nonetheless, there is\na pressing need for a comprehensive evaluation of their performance using a\nstandardized set of metrics. In this study, we conducted a rigorous evaluation\nof three generative models using standard datasets and a diverse set of metrics\nderived from physics, computer vision, and statistics. Furthermore, we explored\nthe impact of using full versus mixed precision modes during inference. Our\nevaluation revealed that the CaloDiffusion and CaloScore generative models\ndemonstrate the most accurate simulation of particle showers, yet there remains\nsubstantial room for improvement. Our findings identified areas where the\nevaluated models fell short in accurately replicating Geant4 data.",
    "categories": [
      "physics.ins-det",
      "cs.AI",
      "hep-ex",
      "physics.data-an"
    ],
    "primary_category": "physics.ins-det",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12898v1",
    "published_date": "2024-06-08 11:17:28 UTC",
    "updated_date": "2024-06-08 11:17:28 UTC"
  },
  {
    "arxiv_id": "2406.05439v1",
    "title": "A Scalable and Near-Optimal Conformance Checking Approach for Long Traces",
    "authors": [
      "Eli Bogdanov",
      "Izack Cohen",
      "Avigdor Gal"
    ],
    "abstract": "Long traces and large event logs that originate from sensors and prediction\nmodels are becoming more common in our data-rich world. In such circumstances,\nconformance checking, a key task in process mining, can become computationally\ninfeasible due to the exponential complexity of finding an optimal alignment.\n  This paper introduces a novel sliding window approach to address these\nscalability challenges while preserving the interpretability of alignment-based\nmethods. By breaking down traces into manageable subtraces and iteratively\naligning each with the process model, our method significantly reduces the\nsearch space.\n  The approach uses global information that captures structural properties of\nthe trace and the process model to make informed alignment decisions,\ndiscarding unpromising alignments even if they are optimal for a local\nsubtrace. This improves the overall accuracy of the results.\n  Experimental evaluations demonstrate that the proposed method consistently\nfinds optimal alignments in most cases and highlight its scalability. This is\nfurther supported by a theoretical complexity analysis, which shows the reduced\ngrowth of the search space compared to other common conformance checking\nmethods.\n  This work provides a valuable contribution towards efficient conformance\nchecking for large-scale process mining applications.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05439v1",
    "published_date": "2024-06-08 11:04:42 UTC",
    "updated_date": "2024-06-08 11:04:42 UTC"
  },
  {
    "arxiv_id": "2406.05422v1",
    "title": "Diffusion-based Reinforcement Learning for Dynamic UAV-assisted Vehicle Twins Migration in Vehicular Metaverses",
    "authors": [
      "Yongju Tong",
      "Jiawen Kang",
      "Junlong Chen",
      "Minrui Xu",
      "Gaolei Li",
      "Weiting Zhang",
      "Xincheng Yan"
    ],
    "abstract": "Air-ground integrated networks can relieve communication pressure on ground\ntransportation networks and provide 6G-enabled vehicular Metaverses services\noffloading in remote areas with sparse RoadSide Units (RSUs) coverage and\ndowntown areas where users have a high demand for vehicular services. Vehicle\nTwins (VTs) are the digital twins of physical vehicles to enable more immersive\nand realistic vehicular services, which can be offloaded and updated on RSU, to\nmanage and provide vehicular Metaverses services to passengers and drivers. The\nhigh mobility of vehicles and the limited coverage of RSU signals necessitate\nVT migration to ensure service continuity when vehicles leave the signal\ncoverage of RSUs. However, uneven VT task migration might overload some RSUs,\nwhich might result in increased service latency, and thus impactive immersive\nexperiences for users. In this paper, we propose a dynamic Unmanned Aerial\nVehicle (UAV)-assisted VT migration framework in air-ground integrated\nnetworks, where UAVs act as aerial edge servers to assist ground RSUs during VT\ntask offloading. In this framework, we propose a diffusion-based Reinforcement\nLearning (RL) algorithm, which can efficiently make immersive VT migration\ndecisions in UAV-assisted vehicular networks. To balance the workload of RSUs\nand improve VT migration quality, we design a novel dynamic path planning\nalgorithm based on a heuristic search strategy for UAVs. Simulation results\nshow that the diffusion-based RL algorithm with UAV-assisted performs better\nthan other baseline schemes.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05422v1",
    "published_date": "2024-06-08 09:53:56 UTC",
    "updated_date": "2024-06-08 09:53:56 UTC"
  },
  {
    "arxiv_id": "2406.05418v1",
    "title": "Multi-attribute Auction-based Resource Allocation for Twins Migration in Vehicular Metaverses: A GPT-based DRL Approach",
    "authors": [
      "Yongju Tong",
      "Junlong Chen",
      "Minrui Xu",
      "Jiawen Kang",
      "Zehui Xiong",
      "Dusit Niyato",
      "Chau Yuen",
      "Zhu Han"
    ],
    "abstract": "Vehicular Metaverses are developed to enhance the modern automotive industry\nwith an immersive and safe experience among connected vehicles and roadside\ninfrastructures, e.g., RoadSide Units (RSUs). For seamless synchronization with\nvirtual spaces, Vehicle Twins (VTs) are constructed as digital representations\nof physical entities. However, resource-intensive VTs updating and high\nmobility of vehicles require intensive computation, communication, and storage\nresources, especially for their migration among RSUs with limited coverages. To\naddress these issues, we propose an attribute-aware auction-based mechanism to\noptimize resource allocation during VTs migration by considering both price and\nnon-monetary attributes, e.g., location and reputation. In this mechanism, we\npropose a two-stage matching for vehicular users and Metaverse service\nproviders in multi-attribute resource markets. First, the resource attributes\nmatching algorithm obtains the resource attributes perfect matching, namely,\nbuyers and sellers can participate in a double Dutch auction (DDA). Then, we\ntrain a DDA auctioneer using a generative pre-trained transformer (GPT)-based\ndeep reinforcement learning (DRL) algorithm to adjust the auction clocks\nefficiently during the auction process. We compare the performance of social\nwelfare and auction information exchange costs with state-of-the-art baselines\nunder different settings. Simulation results show that our proposed GPT-based\nDRL auction schemes have better performance than others.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 6 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.05418v1",
    "published_date": "2024-06-08 09:41:38 UTC",
    "updated_date": "2024-06-08 09:41:38 UTC"
  },
  {
    "arxiv_id": "2406.05413v1",
    "title": "Discover Your Neighbors: Advanced Stable Test-Time Adaptation in Dynamic World",
    "authors": [
      "Qinting Jiang",
      "Chuyang Ye",
      "Dongyan Wei",
      "Yuan Xue",
      "Jingyan Jiang",
      "Zhi Wang"
    ],
    "abstract": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for multimedia\napplications. Existing test-time adaptation (TTA) methods are challenged by\ndynamic, multiple test distributions within batches. This work provides a new\nperspective on analyzing batch normalization techniques through class-related\nand class-irrelevant features, our observations reveal combining source and\ntest batch normalization statistics robustly characterizes target\ndistributions. However, test statistics must have high similarity. We thus\npropose Discover Your Neighbours (DYN), the first backward-free approach\nspecialized for dynamic TTA. The core innovation is identifying similar samples\nvia instance normalization statistics and clustering into groups which provides\nconsistent class-irrelevant representations. Specifically, Our DYN consists of\nlayer-wise instance statistics clustering (LISC) and cluster-aware batch\nnormalization (CABN). In LISC, we perform layer-wise clustering of approximate\nfeature samples at each BN layer by calculating the cosine similarity of\ninstance normalization statistics across the batch. CABN then aggregates SBN\nand TCN statistics to collaboratively characterize the target distribution,\nenabling more robust representations. Experimental results validate DYN's\nrobustness and effectiveness, demonstrating maintained performance under\ndynamic data stream patterns.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.05413v1",
    "published_date": "2024-06-08 09:22:32 UTC",
    "updated_date": "2024-06-08 09:22:32 UTC"
  },
  {
    "arxiv_id": "2406.05410v1",
    "title": "MLLM-SR: Conversational Symbolic Regression base Multi-Modal Large Language Models",
    "authors": [
      "Yanjie Li",
      "Weijun Li",
      "Lina Yu",
      "Min Wu",
      "Jingyi Liu",
      "Wenqiang Li",
      "Shu Wei",
      "Yusong Deng"
    ],
    "abstract": "Formulas are the language of communication between humans and nature. It is\nan important research topic of artificial intelligence to find expressions from\nobserved data to reflect the relationship between each variable in the data,\nwhich is called a symbolic regression problem. The existing symbolic regression\nmethods directly generate expressions according to the given observation data,\nand we cannot require the algorithm to generate expressions that meet specific\nrequirements according to the known prior knowledge. For example, the\nexpression needs to contain $\\sin$ or be symmetric, and so on. Even if it can,\nit often requires very complex operations, which is very inconvenient. In this\npaper, based on multi-modal large language models, we propose MLLM-SR, a\nconversational symbolic regression method that can generate expressions that\nmeet the requirements simply by describing the requirements with natural\nlanguage instructions. By experimenting on the Nguyen dataset, we can\ndemonstrate that MLLM-SR leads the state-of-the-art baselines in fitting\nperformance. More notably, we experimentally demonstrate that MLLM-SR can well\nunderstand the prior knowledge we add to the natural language instructions.\nMoreover, the addition of prior knowledge can effectively guide MLLM-SR to\ngenerate correct expressions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages,",
    "pdf_url": "http://arxiv.org/pdf/2406.05410v1",
    "published_date": "2024-06-08 09:17:54 UTC",
    "updated_date": "2024-06-08 09:17:54 UTC"
  },
  {
    "arxiv_id": "2406.05409v1",
    "title": "Natural Language-Oriented Programming (NLOP): Towards Democratizing Software Creation",
    "authors": [
      "Amin Beheshti"
    ],
    "abstract": "As generative Artificial Intelligence (AI) technologies evolve, they offer\nunprecedented potential to automate and enhance various tasks, including\ncoding. Natural Language-Oriented Programming (NLOP), a vision introduced in\nthis paper, harnesses this potential by allowing developers to articulate\nsoftware requirements and logic in their natural language, thereby\ndemocratizing software creation. This approach streamlines the development\nprocess and significantly lowers the barrier to entry for software engineering,\nmaking it feasible for non-experts to contribute effectively to software\nprojects. By simplifying the transition from concept to code, NLOP can\naccelerate development cycles, enhance collaborative efforts, and reduce\nmisunderstandings in requirement specifications. This paper reviews various\nprogramming models, assesses their contributions and limitations, and\nhighlights that natural language will be the new programming language. Through\nthis comparison, we illustrate how NLOP stands to transform the landscape of\nsoftware engineering by fostering greater inclusivity and innovation.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in: 2024 IEEE International Conference on Software Services\n  Engineering (SSE), Shenzhen, China, July 7-13, 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05409v1",
    "published_date": "2024-06-08 09:13:54 UTC",
    "updated_date": "2024-06-08 09:13:54 UTC"
  },
  {
    "arxiv_id": "2406.05396v1",
    "title": "Mean-field Chaos Diffusion Models",
    "authors": [
      "Sungwoo Park",
      "Dongjun Kim",
      "Ahmed Alaa"
    ],
    "abstract": "In this paper, we introduce a new class of score-based generative models\n(SGMs) designed to handle high-cardinality data distributions by leveraging\nconcepts from mean-field theory. We present mean-field chaos diffusion models\n(MF-CDMs), which address the curse of dimensionality inherent in\nhigh-cardinality data by utilizing the propagation of chaos property of\ninteracting particles. By treating high-cardinality data as a large stochastic\nsystem of interacting particles, we develop a novel score-matching method for\ninfinite-dimensional chaotic particle systems and propose an approximation\nscheme that employs a subdivision strategy for efficient training. Our\ntheoretical and empirical results demonstrate the scalability and effectiveness\nof MF-CDMs for managing large high-cardinality data structures, such as 3D\npoint clouds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05396v1",
    "published_date": "2024-06-08 08:24:06 UTC",
    "updated_date": "2024-06-08 08:24:06 UTC"
  },
  {
    "arxiv_id": "2406.05392v2",
    "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey",
    "authors": [
      "Chengyuan Deng",
      "Yiqun Duan",
      "Xin Jin",
      "Heng Chang",
      "Yijun Tian",
      "Han Liu",
      "Yichen Wang",
      "Kuofeng Gao",
      "Henry Peng Zou",
      "Yiqiao Jin",
      "Yijia Xiao",
      "Shenghao Wu",
      "Zongxing Xie",
      "Weimin Lyu",
      "Sihong He",
      "Lu Cheng",
      "Haohan Wang",
      "Jun Zhuang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved unparalleled success across\ndiverse language modeling tasks in recent years. However, this progress has\nalso intensified ethical concerns, impacting the deployment of LLMs in everyday\ncontexts. This paper provides a comprehensive survey of ethical challenges\nassociated with LLMs, from longstanding issues such as copyright infringement,\nsystematic bias, and data privacy, to emerging problems like truthfulness and\nsocial norms. We critically analyze existing research aimed at understanding,\nexamining, and mitigating these ethical risks. Our survey underscores\nintegrating ethical standards and societal values into the development of LLMs,\nthereby guiding the development of responsible and ethically aligned language\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05392v2",
    "published_date": "2024-06-08 07:55:01 UTC",
    "updated_date": "2024-10-21 07:08:11 UTC"
  },
  {
    "arxiv_id": "2406.10247v1",
    "title": "QCQA: Quality and Capacity-aware grouped Query Attention",
    "authors": [
      "Vinay Joshi",
      "Prashant Laddha",
      "Shambhavi Sinha",
      "Om Ji Omer",
      "Sreenivas Subramoney"
    ],
    "abstract": "Excessive memory requirements of key and value features (KV-cache) present\nsignificant challenges in the autoregressive inference of large language models\n(LLMs), restricting both the speed and length of text generation. Approaches\nsuch as Multi-Query Attention (MQA) and Grouped Query Attention (GQA) mitigate\nthese challenges by grouping query heads and consequently reducing the number\nof corresponding key and value heads. However, MQA and GQA decrease the\nKV-cache size requirements at the expense of LLM accuracy (quality of text\ngeneration). These methods do not ensure an optimal tradeoff between KV-cache\nsize and text generation quality due to the absence of quality-aware grouping\nof query heads. To address this issue, we propose Quality and Capacity-Aware\nGrouped Query Attention (QCQA), which identifies optimal query head groupings\nusing an evolutionary algorithm with a computationally efficient and\ninexpensive fitness function. We demonstrate that QCQA achieves a significantly\nbetter tradeoff between KV-cache capacity and LLM accuracy compared to GQA. For\nthe Llama2 $7\\,$B model, QCQA achieves $\\mathbf{20}$\\% higher accuracy than GQA\nwith similar KV-cache size requirements in the absence of fine-tuning. After\nfine-tuning both QCQA and GQA, for a similar KV-cache size, QCQA provides\n$\\mathbf{10.55}\\,$\\% higher accuracy than GQA. Furthermore, QCQA requires\n$40\\,$\\% less KV-cache size than GQA to attain similar accuracy. The proposed\nquality and capacity-aware grouping of query heads can serve as a new paradigm\nfor KV-cache optimization in autoregressive LLM inference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10247v1",
    "published_date": "2024-06-08 07:49:55 UTC",
    "updated_date": "2024-06-08 07:49:55 UTC"
  },
  {
    "arxiv_id": "2406.05375v2",
    "title": "LEMMA-RCA: A Large Multi-modal Multi-domain Dataset for Root Cause Analysis",
    "authors": [
      "Lecheng Zheng",
      "Zhengzhang Chen",
      "Dongjie Wang",
      "Chengyuan Deng",
      "Reon Matsuoka",
      "Haifeng Chen"
    ],
    "abstract": "Root cause analysis (RCA) is crucial for enhancing the reliability and\nperformance of complex systems. However, progress in this field has been\nhindered by the lack of large-scale, open-source datasets tailored for RCA. To\nbridge this gap, we introduce LEMMA-RCA, a large dataset designed for diverse\nRCA tasks across multiple domains and modalities. LEMMA-RCA features various\nreal-world fault scenarios from IT and OT operation systems, encompassing\nmicroservices, water distribution, and water treatment systems, with hundreds\nof system entities involved. We evaluate the quality of LEMMA-RCA by testing\nthe performance of eight baseline methods on this dataset under various\nsettings, including offline and online modes as well as single and multiple\nmodalities. Our experimental results demonstrate the high quality of LEMMA-RCA.\nThe dataset is publicly available at https://lemma-rca.github.io/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05375v2",
    "published_date": "2024-06-08 07:00:31 UTC",
    "updated_date": "2024-09-26 22:42:49 UTC"
  },
  {
    "arxiv_id": "2406.05369v1",
    "title": "Venn Diagram Prompting : Accelerating Comprehension with Scaffolding Effect",
    "authors": [
      "Sakshi Mahendru",
      "Tejul Pandit"
    ],
    "abstract": "We introduce Venn Diagram (VD) Prompting, an innovative prompting technique\nwhich allows Large Language Models (LLMs) to combine and synthesize information\nacross complex, diverse and long-context documents in knowledge-intensive\nquestion-answering tasks. Generating answers from multiple documents involves\nnumerous steps to extract relevant and unique information and amalgamate it\ninto a cohesive response. To improve the quality of the final answer, multiple\nLLM calls or pretrained models are used to perform different tasks such as\nsummarization, reorganization and customization. The approach covered in the\npaper focuses on replacing the multi-step strategy via a single LLM call using\nVD prompting. Our proposed technique also aims to eliminate the inherent\nposition bias in the LLMs, enhancing consistency in answers by removing\nsensitivity to the sequence of input information. It overcomes the challenge of\ninconsistency traditionally associated with varying input sequences. We also\nexplore the practical applications of the VD prompt based on our examination of\nthe prompt's outcomes. In the experiments performed on four public benchmark\nquestion-answering datasets, VD prompting continually matches or surpasses the\nperformance of a meticulously crafted instruction prompt which adheres to\noptimal guidelines and practices.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. 10 pages, Accepted in 2024 the 6th World Symposium on\n  Artificial Intelligence (WSAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.05369v1",
    "published_date": "2024-06-08 06:27:26 UTC",
    "updated_date": "2024-06-08 06:27:26 UTC"
  },
  {
    "arxiv_id": "2406.05365v2",
    "title": "CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation",
    "authors": [
      "I-Hung Hsu",
      "Zifeng Wang",
      "Long T. Le",
      "Lesly Miculicich",
      "Nanyun Peng",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "abstract": "Grounded generation aims to equip language models (LMs) with the ability to\nproduce more credible and accountable responses by accurately citing verifiable\nsources. However, existing methods, by either feeding LMs with raw or\npreprocessed materials, remain prone to errors. To address this, we introduce\nCaLM, a novel verification framework. CaLM leverages the insight that a robust\ngrounded response should be consistent with information derived solely from its\ncited sources. Our framework empowers smaller LMs, which rely less on\nparametric memory and excel at processing relevant information given a query,\nto validate the output of larger LMs. Larger LM responses that closely align\nwith the smaller LMs' output, which relies exclusively on cited documents, are\nverified. Responses showing discrepancies are iteratively refined through a\nfeedback loop. Experiments on three open-domain question-answering datasets\ndemonstrate significant performance gains of 1.5% to 7% absolute average\nwithout any required model fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Camera Ready Version",
    "pdf_url": "http://arxiv.org/pdf/2406.05365v2",
    "published_date": "2024-06-08 06:04:55 UTC",
    "updated_date": "2024-06-24 07:39:26 UTC"
  },
  {
    "arxiv_id": "2406.05364v2",
    "title": "Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models",
    "authors": [
      "Kalyan Nakka",
      "Jimmy Dani",
      "Nitesh Saxena"
    ],
    "abstract": "In this paper, we present a very first study to investigate trust and ethical\nimplications of on-device artificial intelligence (AI), focusing on small\nlanguage models (SLMs) amenable for personal devices like smartphones. While\non-device SLMs promise enhanced privacy, reduced latency, and improved user\nexperience compared to cloud-based services, we posit that they might also\nintroduce significant risks and vulnerabilities compared to their on-server\ncounterparts. As part of our trust assessment study, we conduct a systematic\nevaluation of the state-of-the-art on-devices SLMs, contrasted to their\non-server counterparts, based on a well-established trustworthiness measurement\nframework. Our results show on-device SLMs to be significantly less\ntrustworthy, specifically demonstrating more stereotypical, unfair and\nprivacy-breaching behavior. Informed by these findings, we then perform our\nethics assessment study using a dataset of unethical questions, that depicts\nharmful scenarios. Our results illustrate the lacking ethical safeguards in\non-device SLMs, emphasizing their capabilities of generating harmful content.\nFurther, the broken safeguards and exploitable nature of on-device SLMs is\ndemonstrated using potentially unethical vanilla prompts, to which the\non-device SLMs answer with valid responses without any filters and without the\nneed for any jailbreaking or prompt engineering. These responses can be abused\nfor various harmful and unethical scenarios like: societal harm, illegal\nactivities, hate, self-harm, exploitable phishing content and many others, all\nof which indicates the severe vulnerability and exploitability of these\non-device SLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "26 pages, 31 figures and 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.05364v2",
    "published_date": "2024-06-08 05:45:42 UTC",
    "updated_date": "2025-03-05 04:18:08 UTC"
  },
  {
    "arxiv_id": "2406.05354v1",
    "title": "Investigating Memory Failure Prediction Across CPU Architectures",
    "authors": [
      "Qiao Yu",
      "Wengui Zhang",
      "Min Zhou",
      "Jialiang Yu",
      "Zhenli Sheng",
      "Jasmin Bogatinovski",
      "Jorge Cardoso",
      "Odej Kao"
    ],
    "abstract": "Large-scale datacenters often experience memory failures, where Uncorrectable\nErrors (UEs) highlight critical malfunction in Dual Inline Memory Modules\n(DIMMs). Existing approaches primarily utilize Correctable Errors (CEs) to\npredict UEs, yet they typically neglect how these errors vary between different\nCPU architectures, especially in terms of Error Correction Code (ECC)\napplicability. In this paper, we investigate the correlation between CEs and\nUEs across different CPU architectures, including X86 and ARM. Our analysis\nidentifies unique patterns of memory failure associated with each processor\nplatform. Leveraging Machine Learning (ML) techniques on production datasets,\nwe conduct the memory failure prediction in different processors' platforms,\nachieving up to 15% improvements in F1-score compared to the existing\nalgorithm. Finally, an MLOps (Machine Learning Operations) framework is\nprovided to consistently improve the failure prediction in the production\nenvironment.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted by 2024 54th Annual IEEE/IFIP International Conference on\n  Dependable Systems and Networks (DSN), Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2406.05354v1",
    "published_date": "2024-06-08 05:10:23 UTC",
    "updated_date": "2024-06-08 05:10:23 UTC"
  },
  {
    "arxiv_id": "2406.05348v2",
    "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
    "authors": [
      "Satanu Ghosh",
      "Neal R. Brodnik",
      "Carolina Frey",
      "Collin Holgate",
      "Tresa M. Pollock",
      "Samantha Daly",
      "Samuel Carton"
    ],
    "abstract": "We explore the ability of GPT-4 to perform ad-hoc schema based information\nextraction from scientific literature. We assess specifically whether it can,\nwith a basic prompting approach, replicate two existing material science\ndatasets, given the manuscripts from which they were originally manually\nextracted. We employ materials scientists to perform a detailed manual error\nanalysis to assess where the model struggles to faithfully extract the desired\ninformation, and draw on their insights to suggest research directions to\naddress this broadly important task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Update on 12/11/2024: Added some relevant literature that we missed\n  in previous version of the paper",
    "pdf_url": "http://arxiv.org/pdf/2406.05348v2",
    "published_date": "2024-06-08 04:24:16 UTC",
    "updated_date": "2024-12-11 19:28:47 UTC"
  },
  {
    "arxiv_id": "2406.05347v3",
    "title": "MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training",
    "authors": [
      "Bo Chen",
      "Zhilei Bei",
      "Xingyi Cheng",
      "Pan Li",
      "Jie Tang",
      "Le Song"
    ],
    "abstract": "Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the\nevolutionary trajectories of protein families. The accuracy of protein\nstructure predictions is often compromised for protein sequences that lack\nsufficient homologous information to construct high quality MSA. Although\nvarious methods have been proposed to generate virtual MSA under these\nconditions, they fall short in comprehensively capturing the intricate\ncoevolutionary patterns within MSA or require guidance from external oracle\nmodels. Here we introduce MSAGPT, a novel approach to prompt protein structure\npredictions via MSA generative pretraining in the low MSA regime. MSAGPT\nemploys a simple yet effective 2D evolutionary positional encoding scheme to\nmodel complex evolutionary patterns. Endowed by this, its flexible 1D MSA\ndecoding framework facilitates zero or few shot learning. Moreover, we\ndemonstrate that leveraging the feedback from AlphaFold2 can further enhance\nthe model capacity via Rejective Fine tuning (RFT) and Reinforcement Learning\nfrom AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT\nin generating faithful virtual MSA to enhance the structure prediction\naccuracy. The transfer learning capabilities also highlight its great potential\nfor facilitating other protein tasks.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05347v3",
    "published_date": "2024-06-08 04:23:57 UTC",
    "updated_date": "2024-10-28 08:51:54 UTC"
  },
  {
    "arxiv_id": "2406.05343v2",
    "title": "M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark",
    "authors": [
      "Wei Song",
      "Yadong Li",
      "Jianhua Xu",
      "Guowei Wu",
      "Lingfeng Ming",
      "Kexin Yi",
      "Weihua Luo",
      "Houyi Li",
      "Yi Du",
      "Fangda Guo",
      "Kaicheng Yu"
    ],
    "abstract": "As recent multi-modality large language models (MLLMs) have shown formidable\nproficiency on various complex tasks, there has been increasing attention on\ndebating whether these models could eventually mirror human intelligence.\nHowever, existing benchmarks mainly focus on evaluating solely on task\nperformance, such as the accuracy of identifying the attribute of an object.\nCombining well-developed cognitive science to understand the intelligence of\nMLLMs beyond superficial achievements remains largely unexplored. To this end,\nwe introduce the first cognitive-driven multi-lingual and multi-modal benchmark\nto evaluate the general intelligence ability of MLLMs, dubbed M3GIA.\nSpecifically, we identify five key cognitive factors based on the\nwell-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a\nnovel evaluation metric. In addition, since most MLLMs are trained to perform\nin different languages, a natural question arises: is language a key factor\ninfluencing the cognitive ability of MLLMs? As such, we go beyond English to\nencompass other languages based on their popularity, including Chinese, French,\nSpanish, Portuguese and Korean, to construct our M3GIA. We make sure all the\ndata relevant to the cultural backgrounds are collected from their native\ncontext to avoid English-centric bias. We collected a significant corpus of\ndata from human participants, revealing that the most advanced MLLM reaches the\nlower boundary of human intelligence in English. Yet, there remains a\npronounced disparity in the other five languages assessed. We also reveals an\ninteresting winner takes all phenomenon that are aligned with the discovery in\ncognitive studies. Our benchmark will be open-sourced, with the aspiration of\nfacilitating the enhancement of cognitive capabilities in MLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05343v2",
    "published_date": "2024-06-08 04:07:09 UTC",
    "updated_date": "2024-06-14 08:35:06 UTC"
  },
  {
    "arxiv_id": "2406.05339v3",
    "title": "To what extent can ASV systems naturally defend against spoofing attacks?",
    "authors": [
      "Jee-weon Jung",
      "Xin Wang",
      "Nicholas Evans",
      "Shinji Watanabe",
      "Hye-jin Shim",
      "Hemlata Tak",
      "Sidhhant Arora",
      "Junichi Yamagishi",
      "Joon Son Chung"
    ],
    "abstract": "The current automatic speaker verification (ASV) task involves making binary\ndecisions on two types of trials: target and non-target. However, emerging\nadvancements in speech generation technology pose significant threats to the\nreliability of ASV systems. This study investigates whether ASV effortlessly\nacquires robustness against spoofing attacks (i.e., zero-shot capability) by\nsystematically exploring diverse ASV systems and spoofing attacks, ranging from\ntraditional to cutting-edge techniques. Through extensive analyses conducted on\neight distinct ASV systems and 29 spoofing attack systems, we demonstrate that\nthe evolution of ASV inherently incorporates defense mechanisms against\nspoofing attacks. Nevertheless, our findings also underscore that the\nadvancement of spoofing attacks far outpaces that of ASV systems, hence\nnecessitating further research on spoofing-robust ASV methodologies.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 3 figures, 3 tables, Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05339v3",
    "published_date": "2024-06-08 03:44:39 UTC",
    "updated_date": "2024-11-18 01:46:11 UTC"
  },
  {
    "arxiv_id": "2406.06626v1",
    "title": "Benchmarking Neural Decoding Backbones towards Enhanced On-edge iBCI Applications",
    "authors": [
      "Zhou Zhou",
      "Guohang He",
      "Zheng Zhang",
      "Luziwei Leng",
      "Qinghai Guo",
      "Jianxing Liao",
      "Xuan Song",
      "Ran Cheng"
    ],
    "abstract": "Traditional invasive Brain-Computer Interfaces (iBCIs) typically depend on\nneural decoding processes conducted on workstations within laboratory settings,\nwhich prevents their everyday usage. Implementing these decoding processes on\nedge devices, such as the wearables, introduces considerable challenges related\nto computational demands, processing speed, and maintaining accuracy. This\nstudy seeks to identify an optimal neural decoding backbone that boasts robust\nperformance and swift inference capabilities suitable for edge deployment. We\nexecuted a series of neural decoding experiments involving nonhuman primates\nengaged in random reaching tasks, evaluating four prospective models, Gated\nRecurrent Unit (GRU), Transformer, Receptance Weighted Key Value (RWKV), and\nSelective State Space model (Mamba), across several metrics: single-session\ndecoding, multi-session decoding, new session fine-tuning, inference speed,\ncalibration speed, and scalability. The findings indicate that although the GRU\nmodel delivers sufficient accuracy, the RWKV and Mamba models are preferable\ndue to their superior inference and calibration speeds. Additionally, RWKV and\nMamba comply with the scaling law, demonstrating improved performance with\nlarger data sets and increased model sizes, whereas GRU shows less pronounced\nscalability, and the Transformer model requires computational resources that\nscale prohibitively. This paper presents a thorough comparative analysis of the\nfour models in various scenarios. The results are pivotal in pinpointing an\noptimal backbone that can handle increasing data volumes and is viable for edge\nimplementation. This analysis provides essential insights for ongoing research\nand practical applications in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06626v1",
    "published_date": "2024-06-08 02:45:36 UTC",
    "updated_date": "2024-06-08 02:45:36 UTC"
  },
  {
    "arxiv_id": "2406.05322v1",
    "title": "Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios",
    "authors": [
      "Yuhang Zhou",
      "Wei Ai"
    ],
    "abstract": "There is increasing interest in distilling task-specific knowledge from large\nlanguage models (LLM) to smaller student models. Nonetheless, LLM distillation\npresents a dual challenge: 1) there is a high cost associated with querying the\nteacher LLM, such as GPT-4, for gathering an ample number of demonstrations; 2)\nthe teacher LLM might provide imperfect outputs with a negative impact on the\nstudent's learning process. To enhance sample efficiency within\nresource-constrained, imperfect teacher scenarios, we propose a three-component\nframework leveraging three signal types. The first signal is the student's\nself-consistency (consistency of student multiple outputs), which is a proxy of\nthe student's confidence. Specifically, we introduce a ``teaching assistant''\n(TA) model to assess the uncertainty of both the student's and the teacher's\noutputs via confidence scoring, which serves as another two signals for student\ntraining. Furthermore, we propose a two-stage training schema to first warm up\nthe student with a small proportion of data to better utilize student's signal.\nExperiments have shown the superiority of our proposed framework for four\ncomplex reasoning tasks. On average, our proposed two-stage framework brings a\nrelative improvement of up to 20.79% compared to fine-tuning without any\nsignals across datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.05322v1",
    "published_date": "2024-06-08 02:17:43 UTC",
    "updated_date": "2024-06-08 02:17:43 UTC"
  },
  {
    "arxiv_id": "2406.05318v1",
    "title": "Integrating Text and Image Pre-training for Multi-modal Algorithmic Reasoning",
    "authors": [
      "Zijian Zhang",
      "Wei Liu"
    ],
    "abstract": "In this paper, we present our solution for SMART-101 Challenge of CVPR\nMulti-modal Algorithmic Reasoning Task 2024. Unlike traditional visual\nquestions and answer tasks, this challenge evaluates abstraction, deduction and\ngeneralization ability of neural network in solving visuo-linguistic puzzles\ndesigned for specially children in the 6-8 age group. Our model is based on two\npre-trained models, dedicated to extract features from text and image\nrespectively. To integrate the features from different modalities, we employed\na fusion layer with attention mechanism. We explored different text and image\npre-trained models, and fine-tune the integrated classifier on the SMART-101\ndataset. Experiment results show that under the data splitting style of puzzle\nsplit, our proposed integrated classifier achieves superior performance,\nverifying the effectiveness of multi-modal pre-trained representations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05318v1",
    "published_date": "2024-06-08 01:45:06 UTC",
    "updated_date": "2024-06-08 01:45:06 UTC"
  },
  {
    "arxiv_id": "2406.05315v3",
    "title": "Aligned at the Start: Conceptual Groupings in LLM Embeddings",
    "authors": [
      "Mehrdad Khatir",
      "Sanchit Kabra",
      "Chandan K. Reddy"
    ],
    "abstract": "This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05315v3",
    "published_date": "2024-06-08 01:27:19 UTC",
    "updated_date": "2025-02-24 17:53:06 UTC"
  },
  {
    "arxiv_id": "2406.05314v1",
    "title": "Relational Proxy Loss for Audio-Text based Keyword Spotting",
    "authors": [
      "Youngmoon Jung",
      "Seungjin Lee",
      "Joon-Young Yang",
      "Jaeyoung Roh",
      "Chang Woo Han",
      "Hoon-Young Cho"
    ],
    "abstract": "In recent years, there has been an increasing focus on user convenience,\nleading to increased interest in text-based keyword enrollment systems for\nkeyword spotting (KWS). Since the system utilizes text input during the\nenrollment phase and audio input during actual usage, we call this task\naudio-text based KWS. To enable this task, both acoustic and text encoders are\ntypically trained using deep metric learning loss functions, such as triplet-\nand proxy-based losses. This study aims to improve existing methods by\nleveraging the structural relations within acoustic embeddings and within text\nembeddings. Unlike previous studies that only compare acoustic and text\nembeddings on a point-to-point basis, our approach focuses on the relational\nstructures within the embedding space by introducing the concept of Relational\nProxy Loss (RPL). By incorporating RPL, we demonstrated improved performance on\nthe Wall Street Journal (WSJ) corpus.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 2 figures, Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05314v1",
    "published_date": "2024-06-08 01:21:17 UTC",
    "updated_date": "2024-06-08 01:21:17 UTC"
  },
  {
    "arxiv_id": "2406.05307v1",
    "title": "DeviceBERT: Applied Transfer Learning With Targeted Annotations and Vocabulary Enrichment to Identify Medical Device and Component Terminology in FDA Recall Summaries",
    "authors": [
      "Miriam Farrington"
    ],
    "abstract": "FDA Medical Device recalls are critical and time-sensitive events, requiring\nswift identification of impacted devices to inform the public of a recall event\nand ensure patient safety. The OpenFDA device recall dataset contains valuable\ninformation about ongoing device recall actions, but manually extracting\nrelevant device information from the recall action summaries is a\ntime-consuming task. Named Entity Recognition (NER) is a task in Natural\nLanguage Processing (NLP) that involves identifying and categorizing named\nentities in unstructured text. Existing NER models, including domain-specific\nmodels like BioBERT, struggle to correctly identify medical device trade names,\npart numbers and component terms within these summaries. To address this, we\npropose DeviceBERT, a medical device annotation, pre-processing and enrichment\npipeline, which builds on BioBERT to identify and label medical device\nterminology in the device recall summaries with improved accuracy. Furthermore,\nwe demonstrate that our approach can be applied effectively for performing\nentity recognition tasks where training data is limited or sparse.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05307v1",
    "published_date": "2024-06-08 00:33:22 UTC",
    "updated_date": "2024-06-08 00:33:22 UTC"
  }
]
[
  {
    "arxiv_id": "2504.14762v1",
    "title": "A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization",
    "authors": [
      "Sahil Rajesh Dhayalkar"
    ],
    "abstract": "We propose a combinatorial and graph-theoretic theory of dropout by modeling\ntraining as a random walk over a high-dimensional graph of binary subnetworks.\nEach node represents a masked version of the network, and dropout induces\nstochastic traversal across this space. We define a subnetwork contribution\nscore that quantifies generalization and show that it varies smoothly over the\ngraph. Using tools from spectral graph theory, PAC-Bayes analysis, and\ncombinatorics, we prove that generalizing subnetworks form large, connected,\nlow-resistance clusters, and that their number grows exponentially with network\nwidth. This reveals dropout as a mechanism for sampling from a robust,\nstructured ensemble of well-generalizing subnetworks with built-in redundancy.\nExtensive experiments validate every theoretical claim across diverse\narchitectures. Together, our results offer a unified foundation for\nunderstanding dropout and suggest new directions for mask-guided regularization\nand subnetwork optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages (9 pages main content and remaining pages are references,\n  appendix which includes 7 figures, proofs and derivations)",
    "pdf_url": "http://arxiv.org/pdf/2504.14762v1",
    "published_date": "2025-04-20 23:09:20 UTC",
    "updated_date": "2025-04-20 23:09:20 UTC"
  },
  {
    "arxiv_id": "2504.14757v1",
    "title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs",
    "authors": [
      "Minh V. T. Pham",
      "Huy N. Phan",
      "Hoang N. Phan",
      "Cuong Le Chi",
      "Tien N. Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "Large language models (LLMs) are transforming automated program repair (APR)\nthrough agent-based approaches that localize bugs, generate patches, and verify\nfixes. However, the lack of high-quality, scalable training datasets,\nespecially those with verifiable outputs and intermediate reasoning\ntraces-limits progress, particularly for open-source models. In this work, we\npresent SWE-Synth, a framework for synthesizing realistic, verifiable, and\nprocess-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM\nagents to simulate debugging workflows, producing not only bug-fix pairs but\nalso test cases and structured repair trajectories. Compared to manually\ncurated datasets, our method scales with minimal human effort while preserving\ncontextual richness and correctness. Experiments show that models trained on\nSWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench\nLite. Our results highlight the potential of synthetic, agent-generated data to\nadvance the state of the art in APR and software engineering automation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.14757v1",
    "published_date": "2025-04-20 22:37:43 UTC",
    "updated_date": "2025-04-20 22:37:43 UTC"
  },
  {
    "arxiv_id": "2504.14751v1",
    "title": "AI for the Open-World: the Learning Principles",
    "authors": [
      "Jianyu Zhang"
    ],
    "abstract": "During the past decades, numerous successes of AI has been made on \"specific\ncapabilities\", named closed-world, such as artificial environments or specific\nreal-world tasks. This well-defined narrow capability brings two nice benefits,\na clear criterion of success and the opportunity to collect a lot of examples.\nThe criteria not only reveal whether a machine has achieved a goal, but reveal\nhow the machine falls short of the goal. As a result, human designers can fix\nthe problems one after the other until the machine is deemed good enough for\nthe task. Furthermore, the large set of collected examples reduces the\ndifficulty of this problem-fixing process (by the central limit theorem).\n  Do the success in closed-world translate into broad open-world, where a\nmachine is required to perform any task that a human could possibly undertake\nwith fewer examples and less priori knowledge from human designers? No. Because\ncompetence in a specific task provides little insight in handling other tasks,\nthe valuable criteria for specific tasks become helpless when handling broader\nunseen tasks. Furthermore, due to the shortage of examples in unseen tasks,\ncentral limit theorem does not stand on our side. At the end, human designers\nlose the oscilloscope to \"hack\" an AI system for the open-world.\n  Achieving AI for the open-world requires unique learning principles and\ninnovated techniques, which are different from the ones in building AI for the\nclosed-world. This thesis explores necessary learning principles required to\nconstruct AI for the open-world, including rich features (analogy a large tool\nbox), disentangled representation (an organized tool box), and inference-time\nlearning (a tool-savvy hand). Driven by the learning principles, this thesis\nfurther proposes techniques to use the learning principles, conducts enormous\nlarge-scale experiments to verify the learning principles.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD thesis. This is not a compilation of published papers, but a new\n  one",
    "pdf_url": "http://arxiv.org/pdf/2504.14751v1",
    "published_date": "2025-04-20 22:22:00 UTC",
    "updated_date": "2025-04-20 22:22:00 UTC"
  },
  {
    "arxiv_id": "2504.14739v1",
    "title": "A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors",
    "authors": [
      "Arpit Agarwal",
      "Mohammad Amin Mirzaee",
      "Xiping Sun",
      "Wenzhen Yuan"
    ],
    "abstract": "GelSight family of vision-based tactile sensors has proven to be effective\nfor multiple robot perception and manipulation tasks. These sensors are based\non an internal optical system and an embedded camera to capture the deformation\nof the soft sensor surface, inferring the high-resolution geometry of the\nobjects in contact. However, customizing the sensors for different robot hands\nrequires a tedious trial-and-error process to re-design the optical system. In\nthis paper, we formulate the GelSight sensor design process as a systematic and\nobjective-driven design problem and perform the design optimization with a\nphysically accurate optical simulation. The method is based on modularizing and\nparameterizing the sensor's optical components and designing four generalizable\nobjective functions to evaluate the sensor. We implement the method with an\ninteractive and easy-to-use toolbox called OptiSense Studio. With the toolbox,\nnon-sensor experts can quickly optimize their sensor design in both forward and\ninverse ways following our predefined modules and steps. We demonstrate our\nsystem with four different GelSight sensors by quickly optimizing their initial\ndesign in simulation and transferring it to the real sensors.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "The paper is accepted to International Journal of Robotics Research\n  with DOI 10.1177/02783649251339680",
    "pdf_url": "http://arxiv.org/pdf/2504.14739v1",
    "published_date": "2025-04-20 21:07:41 UTC",
    "updated_date": "2025-04-20 21:07:41 UTC"
  },
  {
    "arxiv_id": "2504.14737v1",
    "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training",
    "authors": [
      "Shuang Zeng",
      "Lei Zhu",
      "Xinliang Zhang",
      "Hangzhou He",
      "Yanye Lu"
    ],
    "abstract": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14737v1",
    "published_date": "2025-04-20 20:57:03 UTC",
    "updated_date": "2025-04-20 20:57:03 UTC"
  },
  {
    "arxiv_id": "2504.14727v1",
    "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning",
    "authors": [
      "Geng Liu",
      "Fei Zhu",
      "Rong Feng",
      "Zhiqiang Yi",
      "Shiqi Wang",
      "Gaofeng Meng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14727v1",
    "published_date": "2025-04-20 19:53:13 UTC",
    "updated_date": "2025-04-20 19:53:13 UTC"
  },
  {
    "arxiv_id": "2504.14709v1",
    "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline",
    "authors": [
      "Hui Zhou",
      "Shaoshuai Shi",
      "Hongsheng Li"
    ],
    "abstract": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14709v1",
    "published_date": "2025-04-20 18:51:26 UTC",
    "updated_date": "2025-04-20 18:51:26 UTC"
  },
  {
    "arxiv_id": "2504.14708v1",
    "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features",
    "authors": [
      "Parshuram N. Aarotale",
      "Ajita Rattani"
    ],
    "abstract": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14708v1",
    "published_date": "2025-04-20 18:51:10 UTC",
    "updated_date": "2025-04-20 18:51:10 UTC"
  },
  {
    "arxiv_id": "2504.14706v2",
    "title": "AI with Emotions: Exploring Emotional Expressions in Large Language Models",
    "authors": [
      "Shin-nosuke Ishikawa",
      "Atsushi Yoshino"
    ],
    "abstract": "The human-level performance of Large Language Models (LLMs) across various\ntasks has raised expectations for the potential of Artificial Intelligence (AI)\nto possess emotions someday. To explore the capability of current LLMs to\nexpress emotions in their outputs, we conducted an experiment using several\nLLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to\nrole-play as agents answering questions with specified emotional states. We\ndefined the emotional states using Russell's Circumplex model, a\nwell-established framework that characterizes emotions along the\nsleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose\nthis model for its simplicity, utilizing two continuous parameters, which\nallows for better controllability in applications involving continuous changes\nin emotional states. The responses generated were evaluated using a sentiment\nanalysis model, independent of the LLMs, trained on the GoEmotions dataset. The\nevaluation showed that the emotional states of the generated answers were\nconsistent with the specifications, demonstrating the LLMs' capability for\nemotional expression. This indicates the potential for LLM-based AI agents to\nsimulate emotions, opening up a wide range of applications for emotion-based\ninteractions, such as advisors or consultants who can provide advice or\nopinions with a personal touch.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 8 figures, accepted to the Natural Language Processing for\n  Digital Humanities (NLP4DH) workshop at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.14706v2",
    "published_date": "2025-04-20 18:49:25 UTC",
    "updated_date": "2025-04-22 02:10:44 UTC"
  },
  {
    "arxiv_id": "2504.14704v1",
    "title": "Can We Ignore Labels In Out of Distribution Detection?",
    "authors": [
      "Hong Yang",
      "Qi Yu",
      "Travis Desel"
    ],
    "abstract": "Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14704v1",
    "published_date": "2025-04-20 18:37:51 UTC",
    "updated_date": "2025-04-20 18:37:51 UTC"
  },
  {
    "arxiv_id": "2504.14699v1",
    "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays",
    "authors": [
      "Sascha Jecklin",
      "Aidana Massalimova",
      "Ruyi Zha",
      "Lilian Calvet",
      "Christoph J. Laux",
      "Mazda Farshad",
      "Philipp Fürnstahl"
    ],
    "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14699v1",
    "published_date": "2025-04-20 18:28:13 UTC",
    "updated_date": "2025-04-20 18:28:13 UTC"
  },
  {
    "arxiv_id": "2504.14694v1",
    "title": "Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data",
    "authors": [
      "Yuting He",
      "Yiqiang Chen",
      "XiaoDong Yang",
      "Hanchao Yu",
      "Yi-Hua Huang",
      "Yang Gu"
    ],
    "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal model while keeping local data decentralized. Data heterogeneity\n(non-IID) across clients has imposed significant challenges to FL, which makes\nlocal models re-optimize towards their own local optima and forget the global\nknowledge, resulting in performance degradation and convergence slowdown. Many\nexisting works have attempted to address the non-IID issue by adding an extra\nglobal-model-based regularizing item to the local training but without an\nadaption scheme, which is not efficient enough to achieve high performance with\ndeep learning models. In this paper, we propose a Selective Self-Distillation\nmethod for Federated learning (FedSSD), which imposes adaptive constraints on\nthe local updates by self-distilling the global model's knowledge and\nselectively weighting it by evaluating the credibility at both the class and\nsample level. The convergence guarantee of FedSSD is theoretically analyzed and\nextensive experiments are conducted on three public benchmark datasets, which\ndemonstrates that FedSSD achieves better generalization and robustness in fewer\ncommunication rounds, compared with other state-of-the-art FL methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14694v1",
    "published_date": "2025-04-20 18:06:55 UTC",
    "updated_date": "2025-04-20 18:06:55 UTC"
  },
  {
    "arxiv_id": "2504.18562v1",
    "title": "Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction",
    "authors": [
      "Ayoub Jadouli",
      "Chaker El Amrani"
    ],
    "abstract": "Deep learning models, especially large Transformers, carry substantial\n\"memory\" in their intermediate layers -- an \\emph{internal world} that encodes\na wealth of relational and contextual knowledge. This work harnesses that\ninternal world for wildfire occurrence prediction by introducing a modular\narchitecture built upon Gemma 3, a state-of-the-art multimodal model. Rather\nthan relying on Gemma 3's original embedding and positional encoding stacks, we\ndevelop a custom feed-forward module that transforms tabular wildfire features\ninto the hidden dimension required by Gemma 3's mid-layer Transformer blocks.\nWe freeze these Gemma 3 sub-layers -- thus preserving their pretrained\nrepresentation power -- while training only the smaller input and output\nnetworks. This approach minimizes the number of trainable parameters and\nreduces the risk of overfitting on limited wildfire data, yet retains the\nbenefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire\ndataset demonstrate improved predictive accuracy and robustness compared to\nstandard feed-forward and convolutional baselines. Ablation studies confirm\nthat the frozen Transformer layers consistently contribute to better\nrepresentations, underscoring the feasibility of reusing large-model mid-layers\nas a learned internal world. Our findings suggest that strategic modular reuse\nof pretrained Transformers can enable more data-efficient and interpretable\nsolutions for critical environmental applications such as wildfire risk\nmanagement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18562v1",
    "published_date": "2025-04-20 18:02:22 UTC",
    "updated_date": "2025-04-20 18:02:22 UTC"
  },
  {
    "arxiv_id": "2504.14693v2",
    "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark",
    "authors": [
      "Enxin Song",
      "Wenhao Chai",
      "Weili Xu",
      "Jianwen Xie",
      "Yuxuan Liu",
      "Gaoang Wang"
    ],
    "abstract": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/",
    "pdf_url": "http://arxiv.org/pdf/2504.14693v2",
    "published_date": "2025-04-20 17:58:46 UTC",
    "updated_date": "2025-05-02 22:30:26 UTC"
  },
  {
    "arxiv_id": "2504.14690v1",
    "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models",
    "authors": [
      "Mehrnoush Shamsfard",
      "Zahra Saaberi",
      "Mostafa Karimi manesh",
      "Seyed Mohammad Hossein Hashemi",
      "Zahra Vatankhah",
      "Motahareh Ramezani",
      "Niki Pourazin",
      "Tara Zare",
      "Maryam Azimi",
      "Sarina Chitsaz",
      "Sama Khoraminejad",
      "Morteza Mahdavi Mortazavi",
      "Mohammad Mahdi Chizari",
      "Sahar Maleki",
      "Seyed Soroush Majd",
      "Mostafa Masumi",
      "Sayed Ali Musavi Khoeini",
      "Amir Mohseni",
      "Sogol Alipour"
    ],
    "abstract": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7; E.0"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.14690v1",
    "published_date": "2025-04-20 17:43:47 UTC",
    "updated_date": "2025-04-20 17:43:47 UTC"
  },
  {
    "arxiv_id": "2504.14686v1",
    "title": "Uncovering Issues in the Radio Access Network by Looking at the Neighbors",
    "authors": [
      "José Suárez-Varela",
      "Andra Lutu"
    ],
    "abstract": "Mobile network operators (MNOs) manage Radio Access Networks (RANs) with\nmassive amounts of cells over multiple radio generations (2G-5G). To handle\nsuch complexity, operations teams rely on monitoring systems, including anomaly\ndetection tools that identify unexpected behaviors. In this paper, we present\nc-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph\nNeural Networks (GNNs). Our solution captures spatio-temporal variations by\nanalyzing the behavior of individual cells in relation to their local\nneighborhoods, enabling the detection of anomalies that are independent of\nexternal mobility factors. This, in turn, allows focusing on anomalies\nassociated with network issues (e.g., misconfigurations, equipment failures).\nWe evaluate c-ANEMON using real-world data from a large European metropolitan\narea (7,890 cells; 3 months). First, we show that the GNN model within our\nsolution generalizes effectively to cells from previously unseen areas,\nsuggesting the possibility of using a single model across extensive deployment\nregions. Then, we analyze the anomalies detected by c-ANEMON through manual\ninspection and define several categories of long-lasting anomalies (6+ hours).\nNotably, 45.95% of these anomalies fall into a category that is more likely to\nrequire intervention by operations teams.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.14686v1",
    "published_date": "2025-04-20 17:36:52 UTC",
    "updated_date": "2025-04-20 17:36:52 UTC"
  },
  {
    "arxiv_id": "2504.14681v1",
    "title": "An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework",
    "authors": [
      "Zeyu Wang",
      "Frank P. -W. Lo",
      "Qian Chen",
      "Yongqi Zhang",
      "Chen Lin",
      "Xu Chen",
      "Zhenhua Yu",
      "Alexander J. Thompson",
      "Eric M. Yeatman",
      "Benny P. L. Lo"
    ],
    "abstract": "Existing LLM-enabled multi-agent frameworks are predominantly limited to\ndigital or simulated environments and confined to narrowly focused knowledge\ndomain, constraining their applicability to complex engineering tasks that\nrequire the design of physical embodiment, cross-disciplinary integration, and\nconstraint-aware reasoning. This work proposes a multi-agent autonomous\nmechatronics design framework, integrating expertise across mechanical design,\noptimization, electronics, and software engineering to autonomously generate\nfunctional prototypes with minimal direct human design input. Operating\nprimarily through a language-driven workflow, the framework incorporates\nstructured human feedback to ensure robust performance under real-world\nconstraints. To validate its capabilities, the framework is applied to a\nreal-world challenge involving autonomous water-quality monitoring and\nsampling, where traditional methods are labor-intensive and ecologically\ndisruptive. Leveraging the proposed system, a fully functional autonomous\nvessel was developed with optimized propulsion, cost-effective electronics, and\nadvanced control. The design process was carried out by specialized agents,\nincluding a high-level planning agent responsible for problem abstraction and\ndedicated agents for structural, electronics, control, and software\ndevelopment. This approach demonstrates the potential of LLM-based multi-agent\nsystems to automate real-world engineering workflows and reduce reliance on\nextensive domain expertise.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by CVPR 2025 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2504.14681v1",
    "published_date": "2025-04-20 16:57:45 UTC",
    "updated_date": "2025-04-20 16:57:45 UTC"
  },
  {
    "arxiv_id": "2504.14677v1",
    "title": "Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning",
    "authors": [
      "Jia Liu",
      "Cheng Jinguo",
      "Xia Fang",
      "Zhenyuan Ma",
      "Yuankai Wu"
    ],
    "abstract": "Time series foundation models excel at diverse time series forecasting tasks,\nbut their capacity for continuous improvement through incremental learning\nremains unexplored. We present the first comprehensive study investigating\nthese models' temporal plasticity - their ability to progressively enhance\nperformance through continual learning while maintaining existing capabilities.\nThrough experiments on real-world datasets exhibiting distribution shifts, we\nevaluate both conventional deep learning models and foundation models using a\nnovel continual learning framework. Our findings reveal that while traditional\nmodels struggle with performance deterioration during incremental fine-tuning,\nfoundation models like Time-MoE and Chronos demonstrate sustained improvement\nin predictive accuracy. This suggests that optimizing foundation model\nfine-tuning strategies may be more valuable than developing domain-specific\nsmall models. Our research introduces new evaluation methodologies and insights\nfor developing foundation time series models with robust continuous learning\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.14677v1",
    "published_date": "2025-04-20 16:43:01 UTC",
    "updated_date": "2025-04-20 16:43:01 UTC"
  },
  {
    "arxiv_id": "2504.14657v2",
    "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs",
    "authors": [
      "Yihan Lin",
      "Zhirong Bella Yu",
      "Simon Lee"
    ],
    "abstract": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.14657v2",
    "published_date": "2025-04-20 15:37:05 UTC",
    "updated_date": "2025-04-25 06:34:43 UTC"
  },
  {
    "arxiv_id": "2504.14650v1",
    "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents",
    "authors": [
      "Yuting Huang",
      "Leilei Ding",
      "Zhipeng Tang",
      "Tianfu Wang",
      "Xinrui Lin",
      "Wuyang Zhang",
      "Mingxiao Ma",
      "Yanyong Zhang"
    ],
    "abstract": "Large Language Models (LLMs) exhibit substantial promise in enhancing\ntask-planning capabilities within embodied agents due to their advanced\nreasoning and comprehension. However, the systemic safety of these agents\nremains an underexplored frontier. In this study, we present Safe-BeAl, an\nintegrated framework for the measurement (SafePlan-Bench) and alignment\n(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench\nestablishes a comprehensive benchmark for evaluating task-planning safety,\nencompassing 2,027 daily tasks and corresponding environments distributed\nacross 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis\nreveals that even in the absence of adversarial inputs or malicious intent,\nLLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we\npropose Safe-Align, a method designed to integrate physical-world safety\nknowledge into LLM-based embodied agents while maintaining task-specific\nperformance. Experiments across a variety of settings demonstrate that\nSafe-BeAl provides comprehensive safety validation, improving safety by 8.55 -\n15.22%, compared to embodied agents based on GPT-4, while ensuring successful\ntask completion.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.14650v1",
    "published_date": "2025-04-20 15:12:14 UTC",
    "updated_date": "2025-04-20 15:12:14 UTC"
  },
  {
    "arxiv_id": "2504.14645v1",
    "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
    "authors": [
      "Philipp Altmann",
      "Céline Davignon",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Claudia Linnhoff-Popien",
      "Thomas Gabor"
    ],
    "abstract": "We employ an evolutionary optimization framework that perturbs initial states\nto generate informative and diverse policy demonstrations. A joint surrogate\nfitness function guides the optimization by combining local diversity,\nbehavioral certainty, and global population diversity. To assess demonstration\nquality, we apply a set of evaluation metrics, including the reward-based\noptimality gap, fidelity interquartile means (IQMs), fitness composition\nanalysis, and trajectory visualizations. Hyperparameter sensitivity is also\nexamined to better understand the dynamics of trajectory optimization. Our\nfindings demonstrate that optimizing trajectory selection via surrogate fitness\nmetrics significantly improves interpretability of RL policies in both discrete\nand continuous environments. In gridworld domains, evaluations reveal\nsignificantly enhanced demonstration fidelities compared to random and ablated\nbaselines. In continuous control, the proposed framework offers valuable\ninsights, particularly for early-stage policies, while fidelity-based\noptimization proves more effective for mature policies. By refining and\nsystematically analyzing surrogate fitness functions, this study advances the\ninterpretability of RL models. The proposed improvements provide deeper\ninsights into RL decision-making, benefiting applications in safety-critical\nand explainability-focused domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 7 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2504.14645v1",
    "published_date": "2025-04-20 15:01:19 UTC",
    "updated_date": "2025-04-20 15:01:19 UTC"
  },
  {
    "arxiv_id": "2504.14640v1",
    "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States",
    "authors": [
      "Yuheng Huang",
      "Lei Ma",
      "Keizaburo Nishikino",
      "Takumi Akazaki"
    ],
    "abstract": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited",
    "pdf_url": "http://arxiv.org/pdf/2504.14640v1",
    "published_date": "2025-04-20 14:44:18 UTC",
    "updated_date": "2025-04-20 14:44:18 UTC"
  },
  {
    "arxiv_id": "2504.14636v1",
    "title": "AlphaZero-Edu: Making AlphaZero Accessible to Everyone",
    "authors": [
      "Binjie Guo",
      "Hanyu Zheng",
      "Guowei Su",
      "Ru Zhang",
      "Haohan Jiang",
      "Xurong Lin",
      "Hongyan Wei",
      "Aisheng Mo",
      "Jie Li",
      "Zhiyuan Qian",
      "Zhuhao Zhang",
      "Xiaoyuan Cheng"
    ],
    "abstract": "Recent years have witnessed significant progress in reinforcement learning,\nespecially with Zero-like paradigms, which have greatly boosted the\ngeneralization and reasoning abilities of large-scale language models.\nNevertheless, existing frameworks are often plagued by high implementation\ncomplexity and poor reproducibility. To tackle these challenges, we present\nAlphaZero-Edu, a lightweight, education-focused implementation built upon the\nmathematical framework of AlphaZero. It boasts a modular architecture that\ndisentangles key components, enabling transparent visualization of the\nalgorithmic processes. Additionally, it is optimized for resource-efficient\ntraining on a single NVIDIA RTX 3090 GPU and features highly parallelized\nself-play data generation, achieving a 3.2-fold speedup with 8 processes. In\nGomoku matches, the framework has demonstrated exceptional performance,\nachieving a consistently high win rate against human opponents. AlphaZero-Edu\nhas been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,\nproviding an accessible and practical benchmark for both academic research and\nindustrial applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14636v1",
    "published_date": "2025-04-20 14:29:39 UTC",
    "updated_date": "2025-04-20 14:29:39 UTC"
  },
  {
    "arxiv_id": "2504.14625v3",
    "title": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence",
    "authors": [
      "Haiyan Qin",
      "Jiahao Feng",
      "Xiaotong Feng",
      "Wei W. Xing",
      "Wang Kang"
    ],
    "abstract": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.14625v3",
    "published_date": "2025-04-20 14:05:17 UTC",
    "updated_date": "2025-05-01 03:43:28 UTC"
  },
  {
    "arxiv_id": "2504.14624v1",
    "title": "Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation",
    "authors": [
      "Polina Gordienko",
      "Christoph Jansen",
      "Thomas Augustin",
      "Martin Rechenauer"
    ],
    "abstract": "We propose a framework for probability aggregation based on propositional\nprobability logic. Unlike conventional judgment aggregation, which focuses on\nstatic rationality, our model addresses dynamic rationality by ensuring that\ncollective beliefs update consistently with new information. We show that any\nconsensus-compatible and independent aggregation rule on a non-nested agenda is\nnecessarily linear. Furthermore, we provide sufficient conditions for a fair\nlearning process, where individuals initially agree on a specified subset of\npropositions known as the common ground, and new information is restricted to\nthis shared foundation. This guarantees that updating individual judgments via\nBayesian conditioning-whether performed before or after aggregation-yields the\nsame collective belief. A distinctive feature of our framework is its treatment\nof sequential decision-making, which allows new information to be incorporated\nprogressively through multiple stages while maintaining the established common\nground. We illustrate our findings with a running example in a political\nscenario concerning healthcare and immigration policies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to the International Conference on Modeling Decisions for\n  Artificial Intelligence (MDAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.14624v1",
    "published_date": "2025-04-20 14:04:39 UTC",
    "updated_date": "2025-04-20 14:04:39 UTC"
  },
  {
    "arxiv_id": "2504.14618v1",
    "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image",
    "authors": [
      "Han Bi",
      "Ge Yu",
      "Yu He",
      "Wenzhuo Liu",
      "Zijie Zheng"
    ],
    "abstract": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14618v1",
    "published_date": "2025-04-20 13:54:22 UTC",
    "updated_date": "2025-04-20 13:54:22 UTC"
  },
  {
    "arxiv_id": "2504.15317v1",
    "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention",
    "authors": [
      "Meher Boulaabi",
      "Takwa Ben Aïcha Gader",
      "Afef Kacem Echi",
      "Zied Bouraoui"
    ],
    "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.15317v1",
    "published_date": "2025-04-20 13:23:20 UTC",
    "updated_date": "2025-04-20 13:23:20 UTC"
  },
  {
    "arxiv_id": "2504.14603v2",
    "title": "UFO2: The Desktop AgentOS",
    "authors": [
      "Chaoyun Zhang",
      "He Huang",
      "Chiming Ni",
      "Jian Mu",
      "Si Qin",
      "Shilin He",
      "Lu Wang",
      "Fangkai Yang",
      "Pu Zhao",
      "Chao Du",
      "Liqun Li",
      "Yu Kang",
      "Zhao Jiang",
      "Suzhen Zheng",
      "Rujia Wang",
      "Jiaxu Qian",
      "Minghua Ma",
      "Jian-Guang Lou",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ],
    "abstract": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.OS"
    ],
    "primary_category": "cs.AI",
    "comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/",
    "pdf_url": "http://arxiv.org/pdf/2504.14603v2",
    "published_date": "2025-04-20 13:04:43 UTC",
    "updated_date": "2025-04-25 05:14:14 UTC"
  },
  {
    "arxiv_id": "2504.14602v1",
    "title": "K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics",
    "authors": [
      "Jiwei Li",
      "Bi Zhang",
      "Xiaowei Tan",
      "Wanxin Chen",
      "Zhaoyuan Liu",
      "Juanjuan Zhang",
      "Weiguang Huo",
      "Jian Huang",
      "Lianqing Liu",
      "Xingang Zhao"
    ],
    "abstract": "The natural interaction and control performance of lower limb rehabilitation\nrobots are closely linked to biomechanical information from various human\nlocomotion activities. Multidimensional human motion data significantly deepen\nthe understanding of the complex mechanisms governing neuromuscular\nalterations, thereby facilitating the development and application of\nrehabilitation robots in multifaceted real-world environments. However,\ncurrently available lower limb datasets are inadequate for supplying the\nessential multimodal data and large-scale gait samples necessary for effective\ndata-driven approaches, and they neglect the significant effects of acquisition\ninterference in real applications.To fill this gap, we present the K2MUSE\ndataset, which includes a comprehensive collection of multimodal data,\ncomprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface\nelectromyography (sEMG) measurements. The proposed dataset includes lower limb\nmultimodal data from 30 able-bodied participants walking under different\ninclines (0$^\\circ$, $\\pm$5$^\\circ$, and $\\pm$10$^\\circ$), various speeds (0.5\nm/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions\n(muscle fatigue, electrode shifts, and inter-day differences). The kinematic\nand ground reaction force data were collected via a Vicon motion capture system\nand an instrumented treadmill with embedded force plates, whereas the sEMG and\nAUS data were synchronously recorded for thirteen muscles on the bilateral\nlower limbs. This dataset offers a new resource for designing control\nframeworks for rehabilitation robots and conducting biomechanical analyses of\nlower limb locomotion. The dataset is available at https://k2muse.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "23 pages, 13 figures,4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.14602v1",
    "published_date": "2025-04-20 13:03:56 UTC",
    "updated_date": "2025-04-20 13:03:56 UTC"
  },
  {
    "arxiv_id": "2504.14596v1",
    "title": "Toward the Axiomatization of Intelligence: Structure, Time, and Existence",
    "authors": [
      "Kei Itoh"
    ],
    "abstract": "This study aims to construct an axiomatic definition of intelligence within a\nmeta-framework that defines the method of definition, addressing intelligence\nas an inherently naive and polysemous concept. Initially, we formalize a\nset-theoretic representation of the universe as the domain wherein intelligence\nexists and characterize intelligence as a structure that involves temporal\nevolution and interaction with other sets. Starting from a naive definition of\nintelligence as \"an entity possessing structures for externally inputting,\ninternally processing, and externally outputting information or matter,\" we\naxiomatically reformulate it within this set-theoretical depiction of the\nuniverse. Applying this axiomatic definition, we compare and interpret three\nexamples -- Hebbian non-optimized neural networks (NNs),\nbackpropagation-optimized NNs, and biological reflexive systems -- in terms of\ntheir intelligence, structural properties, and biological plausibility.\nFurthermore, by extending our definition into a categorical framework, we\nintroduce two categories, \"Time Category\" and \"Intelligence Category,\" along\nwith the functorial relationships between them, demonstrating the potential to\nrepresent changes and mimicry relationships among intelligent systems\nabstractly. Additionally, since intelligence, as defined herein, functions\neffectively only when accompanied by temporal interactions, we introduce the\nconcept of \"activity\" and explore how activity-based conditions influence\nclassifications and interpretations of intelligence. Finally, we suggest that\nour definitional methodology is not limited to intelligence alone, but can be\nsimilarly applied to other concepts, such as consciousness and emotion,\nadvocating for their formal reinterpretation through the same procedural steps:\ndefining a universal representation, selecting naive definitions, and axiomatic\nformalization.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 4 tables, in English, in Japanese",
    "pdf_url": "http://arxiv.org/pdf/2504.14596v1",
    "published_date": "2025-04-20 12:55:37 UTC",
    "updated_date": "2025-04-20 12:55:37 UTC"
  },
  {
    "arxiv_id": "2504.14594v1",
    "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models",
    "authors": [
      "Fan Gao",
      "Xinjie Zhao",
      "Ding Xia",
      "Zhongyi Zhou",
      "Rui Yang",
      "Jinghui Lu",
      "Hang Jiang",
      "Chanjun Park",
      "Irene Li"
    ],
    "abstract": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14594v1",
    "published_date": "2025-04-20 12:51:16 UTC",
    "updated_date": "2025-04-20 12:51:16 UTC"
  },
  {
    "arxiv_id": "2504.14588v1",
    "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction",
    "authors": [
      "Wenke Xia",
      "Ruoxuan Feng",
      "Dong Wang",
      "Di Hu"
    ],
    "abstract": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.14588v1",
    "published_date": "2025-04-20 12:30:43 UTC",
    "updated_date": "2025-04-20 12:30:43 UTC"
  },
  {
    "arxiv_id": "2504.14573v1",
    "title": "Modality Selection and Skill Segmentation via Cross-Modality Attention",
    "authors": [
      "Jiawei Jiang",
      "Kei Ota",
      "Devesh K. Jha",
      "Asako Kanezaki"
    ],
    "abstract": "Incorporating additional sensory modalities such as tactile and audio into\nfoundational robotic models poses significant challenges due to the curse of\ndimensionality. This work addresses this issue through modality selection. We\npropose a cross-modality attention (CMA) mechanism to identify and selectively\nutilize the modalities that are most informative for action generation at each\ntimestep. Furthermore, we extend the application of CMA to segment primitive\nskills from expert demonstrations and leverage this segmentation to train a\nhierarchical policy capable of solving long-horizon, contact-rich manipulation\ntasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14573v1",
    "published_date": "2025-04-20 11:32:43 UTC",
    "updated_date": "2025-04-20 11:32:43 UTC"
  },
  {
    "arxiv_id": "2504.14569v1",
    "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models",
    "authors": [
      "Lawrence Liu",
      "Inesh Chakrabarti",
      "Yixiao Li",
      "Mengdi Wang",
      "Tuo Zhao",
      "Lin F. Yang"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14569v1",
    "published_date": "2025-04-20 11:00:29 UTC",
    "updated_date": "2025-04-20 11:00:29 UTC"
  },
  {
    "arxiv_id": "2505.03760v1",
    "title": "Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach",
    "authors": [
      "Arishi Orra",
      "Aryan Bhambu",
      "Himanshu Choudhary",
      "Manoj Thakur",
      "Selvaraju Natarajan"
    ],
    "abstract": "Portfolio optimization requires dynamic allocation of funds by balancing the\nrisk and return tradeoff under dynamic market conditions. With the recent\nadvancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in\nproviding adaptive and scalable strategies for portfolio optimization. However,\nthe success of these strategies depends not only on their ability to adapt to\nmarket dynamics but also on the careful pre-selection of assets that influence\noverall portfolio performance. Incorporating the investor's preference in\npre-selecting assets for a portfolio is essential in refining their investment\nstrategies. This study proposes a volatility-guided DRL-based portfolio\noptimization framework that dynamically constructs portfolios based on\ninvestors' risk profiles. The Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) model is utilized for volatility forecasting of\nstocks and categorizes them based on their volatility as aggressive, moderate,\nand conservative. The DRL agent is then employed to learn an optimal investment\npolicy by interacting with the historical market data. The efficacy of the\nproposed methodology is established using stocks from the Dow $30$ index. The\nproposed investor-specific DRL-based portfolios outperformed the baseline\nstrategies by generating consistent risk-adjusted returns.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "q-fin.PM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03760v1",
    "published_date": "2025-04-20 10:17:37 UTC",
    "updated_date": "2025-04-20 10:17:37 UTC"
  },
  {
    "arxiv_id": "2504.14560v3",
    "title": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model",
    "authors": [
      "Haiyan Qin",
      "Zhiwei Xie",
      "Jingjing Li",
      "Liangchen Li",
      "Xiaotong Feng",
      "Junzhan Liu",
      "Wang Kang"
    ],
    "abstract": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.14560v3",
    "published_date": "2025-04-20 10:16:59 UTC",
    "updated_date": "2025-05-01 03:06:06 UTC"
  },
  {
    "arxiv_id": "2504.15315v1",
    "title": "Diffusion-Driven Inertial Generated Data for Smartphone Location Classification",
    "authors": [
      "Noa Cohen",
      "Rotem Dror",
      "Itzik Klein"
    ],
    "abstract": "Despite the crucial role of inertial measurements in motion tracking and\nnavigation systems, the time-consuming and resource-intensive nature of\ncollecting extensive inertial data has hindered the development of robust\nmachine learning models in this field. In recent years, diffusion models have\nemerged as a revolutionary class of generative models, reshaping the landscape\nof artificial data generation. These models surpass generative adversarial\nnetworks and other state-of-the-art approaches to complex tasks. In this work,\nwe propose diffusion-driven specific force-generated data for smartphone\nlocation recognition. We provide a comprehensive evaluation methodology by\ncomparing synthetic and real recorded specific force data across multiple\nmetrics. Our results demonstrate that our diffusion-based generative model\nsuccessfully captures the distinctive characteristics of specific force signals\nacross different smartphone placement conditions. Thus, by creating diverse,\nrealistic synthetic data, we can reduce the burden of extensive data collection\nwhile providing high-quality training data for machine learning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.15315v1",
    "published_date": "2025-04-20 10:14:36 UTC",
    "updated_date": "2025-04-20 10:14:36 UTC"
  },
  {
    "arxiv_id": "2504.14556v1",
    "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
    "authors": [
      "Yousef Emami",
      "Hao Zhou",
      "SeyedSina Nabavirazani",
      "Luis Almeida"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various\nprivate and commercial applications, e.g. traffic control, package delivery,\nand Search and Rescue (SAR) operations. Machine Learning (ML) methods used in\nUAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sample efficiency, which conflict with\nthe urgency of emergencies such as SAR operations. This paper proposes\nIn-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as\nan alternative to DRL in emergencies. The UAV collects and transmits logged\nsensory data, to an LLM, to generate a task description in natural language,\nfrom which it obtains a data collection schedule to be executed by the UAV. The\nsystem continuously adapts by adding feedback to task descriptions and\nutilizing feedback for future decisions. This method is tested against\njailbreaking attacks, where task description is manipulated to undermine\nnetwork performance, highlighting the vulnerability of LLMs to such attacks.\nThe proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative\npacket loss by approximately 56\\%. ICLDC presents a promising direction for\nintelligent scheduling and control in UAV-assisted data collection.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.RO",
      "53-01",
      "C.2"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 7 figures,",
    "pdf_url": "http://arxiv.org/pdf/2504.14556v1",
    "published_date": "2025-04-20 10:05:07 UTC",
    "updated_date": "2025-04-20 10:05:07 UTC"
  },
  {
    "arxiv_id": "2504.14548v1",
    "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
    "authors": [
      "Lifeng Lin",
      "Rongfeng Lu",
      "Quan Chen",
      "Haofan Ren",
      "Ming Lu",
      "Yaoqi Sun",
      "Chenggang Yan",
      "Anke Xue"
    ],
    "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages,8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.14548v1",
    "published_date": "2025-04-20 09:38:02 UTC",
    "updated_date": "2025-04-20 09:38:02 UTC"
  },
  {
    "arxiv_id": "2504.14530v1",
    "title": "Causality for Natural Language Processing",
    "authors": [
      "Zhijing Jin"
    ],
    "abstract": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "PhD Thesis 2024",
    "pdf_url": "http://arxiv.org/pdf/2504.14530v1",
    "published_date": "2025-04-20 08:11:11 UTC",
    "updated_date": "2025-04-20 08:11:11 UTC"
  },
  {
    "arxiv_id": "2504.14523v1",
    "title": "Learning from Reasoning Failures via Synthetic Data Generation",
    "authors": [
      "Gabriela Ben Melech Stan",
      "Estelle Aflalo",
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "Training models on synthetic data has emerged as an increasingly important\nstrategy for improving the performance of generative AI. This approach is\nparticularly helpful for large multimodal models (LMMs) due to the relative\nscarcity of high-quality paired image-text data compared to language-only data.\nWhile a variety of methods have been proposed for generating large multimodal\ndatasets, they do not tailor the synthetic data to address specific\ndeficiencies in the reasoning abilities of LMMs which will be trained with the\ngenerated dataset. In contrast, humans often learn in a more efficient manner\nby seeking out examples related to the types of reasoning where they have\nfailed previously. Inspired by this observation, we propose a new approach for\nsynthetic data generation which is grounded in the analysis of an existing\nLMM's reasoning failures. Our methodology leverages frontier models to\nautomatically analyze errors produced by a weaker LMM and propose new examples\nwhich can be used to correct the reasoning failure via additional training,\nwhich are then further filtered to ensure high quality. We generate a large\nmultimodal instruction tuning dataset containing over 553k examples using our\napproach and conduct extensive experiments demonstrating its utility for\nimproving the performance of LMMs on multiple downstream tasks. Our results\nshow that models trained on our synthetic data can even exceed the performance\nof LMMs trained on an equivalent amount of additional real data, demonstrating\nthe high value of generating synthetic data targeted to specific reasoning\nfailure modes in LMMs. We will make our dataset and code publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14523v1",
    "published_date": "2025-04-20 07:45:53 UTC",
    "updated_date": "2025-04-20 07:45:53 UTC"
  },
  {
    "arxiv_id": "2504.14522v1",
    "title": "Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers",
    "authors": [
      "Liudmila Zavolokina",
      "Kilian Sprenkamp",
      "Zoya Katashinskaya",
      "Daniel Gordon Jones"
    ],
    "abstract": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "European Conference on Information Systems (ECIS)",
    "pdf_url": "http://arxiv.org/pdf/2504.14522v1",
    "published_date": "2025-04-20 07:39:00 UTC",
    "updated_date": "2025-04-20 07:39:00 UTC"
  },
  {
    "arxiv_id": "2504.14520v1",
    "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey",
    "authors": [
      "Ahsan Bilal",
      "Muhammad Ahmed Mohsin",
      "Muhammad Umer",
      "Muhammad Awais Khan Bangash",
      "Muhammad Ali Jamshed"
    ],
    "abstract": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to IEEE Transactions on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2504.14520v1",
    "published_date": "2025-04-20 07:34:26 UTC",
    "updated_date": "2025-04-20 07:34:26 UTC"
  },
  {
    "arxiv_id": "2504.14519v1",
    "title": "SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training",
    "authors": [
      "Zhouyang Li",
      "Yuliang Liu",
      "Wei Zhang",
      "Tailing Yuan",
      "Bin Chen",
      "Chengru Song",
      "Di Zhang"
    ],
    "abstract": "Pipeline Parallelism (PP) serves as a crucial technique for training Large\nLanguage Models (LLMs), owing to its capability to alleviate memory pressure\nfrom model states with relatively low communication overhead. However, in\nlong-context scenarios, existing pipeline parallelism methods fail to address\nthe substantial activation memory pressure, primarily due to the peak memory\nconsumption resulting from the accumulation of activations across multiple\nmicrobatches. Moreover, these approaches inevitably introduce considerable\npipeline bubbles, further hindering efficiency.\n  To tackle these challenges, we propose SlimPipe, a novel approach to\nfine-grained pipeline parallelism that employs uniform sequence slicing coupled\nwith one-forward-one-backward (1F1B) schedule. It reduces the accumulated\nactivations from several microbatches to just one, which is split into several\nslices. Although the slices are evenly partitioned, the computation cost is not\nequal across slices due to causal attention. We develop a sophisticated\nworkload redistribution technique to address this load imbalance. SlimPipe\nachieves (1) near-zero memory overhead and (2) minimal pipeline bubbles\nsimultaneously. The effectiveness of SlimPipe has been proven by thorough\ntesting with diverse model architectures, context window sizes, and\nSlimPipe-specific configurations. For example, on the Llama 70B model, compared\nto state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs\nUtilization (MFU) to up to $1.57\\times$ for a context length of 512K. More\nnotably, for a context length of 2048K, it maintains over 45% utilization on\n256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant\nperformance drops or fail entirely due to memory constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14519v1",
    "published_date": "2025-04-20 07:33:33 UTC",
    "updated_date": "2025-04-20 07:33:33 UTC"
  },
  {
    "arxiv_id": "2504.14514v1",
    "title": "On Dimension-Free Transformer: An Application of STP to AI",
    "authors": [
      "Daizhan Cheng"
    ],
    "abstract": "The matrix expressions for every parts of a transformer are firstly\ndescribed. Based on semi-tensor product (STP) of matrices the hypervectors are\nreconsidered and the linear transformation over hypervectors is constructed by\nusing projection. Its properties and calculating formulas are obtained. Using\nprojection-based transformation of hypervector (PBTH), the framework of\ndimension-free transformer (DFT) is proposed by verifying each linear\ntransformation in a transformer and replacing it by a proper PBTH, which allows\nthe inputs and outputs being of arbitrary dimensions. Using balanced\ninformation about all entries, DFT must be more efficient in dealing with\nsignals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14514v1",
    "published_date": "2025-04-20 07:19:54 UTC",
    "updated_date": "2025-04-20 07:19:54 UTC"
  },
  {
    "arxiv_id": "2504.14509v3",
    "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning",
    "authors": [
      "Fulong Ye",
      "Miao Hua",
      "Pengze Zhang",
      "Xinghui Li",
      "Qichao Sun",
      "Songtao Zhao",
      "Qian He",
      "Xinglong Wu"
    ],
    "abstract": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://superhero-7.github.io/DreamID/",
    "pdf_url": "http://arxiv.org/pdf/2504.14509v3",
    "published_date": "2025-04-20 06:53:00 UTC",
    "updated_date": "2025-04-25 03:48:24 UTC"
  },
  {
    "arxiv_id": "2504.15313v1",
    "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind",
    "authors": [
      "Yajie Yu",
      "Yue Feng"
    ],
    "abstract": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.15313v1",
    "published_date": "2025-04-20 06:43:23 UTC",
    "updated_date": "2025-04-20 06:43:23 UTC"
  },
  {
    "arxiv_id": "2504.14494v1",
    "title": "LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method",
    "authors": [
      "Yue Li"
    ],
    "abstract": "In this paper, we present LBM-GNN, a novel approach that enhances the\ntraditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).\nWe apply this method to fluid dynamics simulations, demonstrating improved\nstability and accuracy compared to standard LBM implementations. The method is\nvalidated using benchmark problems such as the Taylor-Green vortex, focusing on\naccuracy, conservation properties, and performance across different Reynolds\nnumbers and grid resolutions. Our results indicate that GNN-enhanced LBM can\nmaintain better conservation properties while improving numerical stability at\nhigher Reynolds numbers.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14494v1",
    "published_date": "2025-04-20 05:09:10 UTC",
    "updated_date": "2025-04-20 05:09:10 UTC"
  },
  {
    "arxiv_id": "2504.14493v2",
    "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering",
    "authors": [
      "Xinyu Wang",
      "Jijun Chi",
      "Zhenghan Tai",
      "Tung Sum Thomas Kwok",
      "Muzhi Li",
      "Zhuhong Li",
      "Hailin He",
      "Yuchen Hua",
      "Peng Lu",
      "Suyuchen Wang",
      "Yihong Wu",
      "Jerry Huang",
      "Jingrui Tian",
      "Ling Zhou"
    ],
    "abstract": "Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14493v2",
    "published_date": "2025-04-20 04:58:14 UTC",
    "updated_date": "2025-04-29 20:38:43 UTC"
  },
  {
    "arxiv_id": "2504.14452v1",
    "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data",
    "authors": [
      "Tong Chen",
      "Faeze Brahman",
      "Jiacheng Liu",
      "Niloofar Mireshghallah",
      "Weijia Shi",
      "Pang Wei Koh",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14452v1",
    "published_date": "2025-04-20 01:59:46 UTC",
    "updated_date": "2025-04-20 01:59:46 UTC"
  },
  {
    "arxiv_id": "2504.14448v1",
    "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory",
    "authors": [
      "Ali Arslan Yousaf",
      "Umair Rehman",
      "Muhammad Umair Danish"
    ],
    "abstract": "We propose a novel symbolic modeling framework for decision-making under risk\nthat merges interpretability with the core insights of Prospect Theory. Our\napproach replaces opaque utility curves and probability weighting functions\nwith transparent, effect-size-guided features. We mathematically formalize the\nmethod, demonstrate its ability to replicate well-known framing and\nloss-aversion phenomena, and provide an end-to-end empirical validation on\nsynthetic datasets. The resulting model achieves competitive predictive\nperformance while yielding clear coefficients mapped onto psychological\nconstructs, making it suitable for applications ranging from AI safety to\neconomic policy analysis.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14448v1",
    "published_date": "2025-04-20 01:44:54 UTC",
    "updated_date": "2025-04-20 01:44:54 UTC"
  },
  {
    "arxiv_id": "2504.14439v1",
    "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling",
    "authors": [
      "Avinandan Bose",
      "Zhihan Xiong",
      "Yuejie Chi",
      "Simon Shaolei Du",
      "Lin Xiao",
      "Maryam Fazel"
    ],
    "abstract": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14439v1",
    "published_date": "2025-04-20 01:16:24 UTC",
    "updated_date": "2025-04-20 01:16:24 UTC"
  },
  {
    "arxiv_id": "2504.14432v1",
    "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task",
    "authors": [
      "Ahmad Khalil",
      "Mahmoud Khalil",
      "Alioune Ngom"
    ],
    "abstract": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14432v1",
    "published_date": "2025-04-20 00:20:18 UTC",
    "updated_date": "2025-04-20 00:20:18 UTC"
  },
  {
    "arxiv_id": "2504.14429v1",
    "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations",
    "authors": [
      "Ahmad Khalil",
      "Mahmoud Khalil",
      "Alioune Ngom"
    ],
    "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14429v1",
    "published_date": "2025-04-20 00:10:44 UTC",
    "updated_date": "2025-04-20 00:10:44 UTC"
  },
  {
    "arxiv_id": "2504.14427v1",
    "title": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework",
    "authors": [
      "Spencer Lin",
      "Miru Jun",
      "Basem Rizk",
      "Karen Shieh",
      "Scott Fisher",
      "Sharon Mozgai"
    ],
    "abstract": "This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.14427v1",
    "published_date": "2025-04-20 00:02:56 UTC",
    "updated_date": "2025-04-20 00:02:56 UTC"
  }
]
[
  {
    "arxiv_id": "2508.06495v1",
    "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction",
    "authors": [
      "Juliana Resplande Sant'anna Gomes",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "abstract": "The accelerated dissemination of disinformation often outpaces the capacity for manual fact-checking, highlighting the urgent need for Semi-Automated Fact-Checking (SAFC) systems. Within the Portuguese language context, there is a noted scarcity of publicly available datasets that integrate external evidence, an essential component for developing robust AFC systems, as many existing resources focus solely on classification based on intrinsic text features. This dissertation addresses this gap by developing, applying, and analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) with external evidence. The approach simulates a user's verification process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash) to extract the main claim from texts and search engine APIs (Google Search API, Google FactCheck Claims Search API) to retrieve relevant external documents (evidence). Additionally, a data validation and preprocessing framework, including near-duplicate detection, is introduced to enhance the quality of the base corpora.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Master Thesis in Computer Science at Federal University on Goias (UFG). Written in Portuguese",
    "pdf_url": "https://arxiv.org/pdf/2508.06495v1",
    "published_date": "2025-07-19 23:46:40 UTC",
    "updated_date": "2025-07-19 23:46:40 UTC"
  },
  {
    "arxiv_id": "2507.14767v1",
    "title": "XplainAct: Visualization for Personalized Intervention Insights",
    "authors": [
      "Yanming Zhang",
      "Krishnakumar Hegde",
      "Klaus Mueller"
    ],
    "abstract": "Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "This paper will be published and presented at IEEE Visualization (VIS) 2025, Vienna, Austria, November 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14767v1",
    "published_date": "2025-07-19 22:57:09 UTC",
    "updated_date": "2025-07-19 22:57:09 UTC"
  },
  {
    "arxiv_id": "2507.14766v1",
    "title": "CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories",
    "authors": [
      "Mehak Arora",
      "Ayman Ali",
      "Kaiyuan Wu",
      "Carolyn Davis",
      "Takashi Shimazui",
      "Mahmoud Alwakeel",
      "Victor Moas",
      "Philip Yang",
      "Annette Esper",
      "Rishikesan Kamaleswaran"
    ],
    "abstract": "In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally sparse CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A transformer model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "In Review for MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14766v1",
    "published_date": "2025-07-19 22:42:26 UTC",
    "updated_date": "2025-07-19 22:42:26 UTC"
  },
  {
    "arxiv_id": "2507.14760v1",
    "title": "QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems",
    "authors": [
      "Cassandra Tong Ye",
      "Shamus Li",
      "Tyler King",
      "Kristina Monakhova"
    ],
    "abstract": "Deep learning models often hallucinate, producing realistic artifacts that are not truly present in the sample. This can have dire consequences for scientific and medical inverse problems, such as MRI and microscopy denoising, where accuracy is more important than perceptual quality. Uncertainty quantification techniques, such as conformal prediction, can pinpoint outliers and provide guarantees for image regression tasks, improving reliability. However, existing methods utilize a linear constant scaling factor to calibrate uncertainty bounds, resulting in larger, less informative bounds. We propose QUTCC, a quantile uncertainty training and calibration technique that enables nonlinear, non-uniform scaling of quantile predictions to enable tighter uncertainty estimates. Using a U-Net architecture with a quantile embedding, QUTCC enables the prediction of the full conditional distribution of quantiles for the imaging task. During calibration, QUTCC generates uncertainty bounds by iteratively querying the network for upper and lower quantiles, progressively refining the bounds to obtain a tighter interval that captures the desired coverage. We evaluate our method on several denoising tasks as well as compressive MRI reconstruction. Our method successfully pinpoints hallucinations in image estimates and consistently achieves tighter uncertainty intervals than prior methods while maintaining the same statistical coverage.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14760v1",
    "published_date": "2025-07-19 21:44:14 UTC",
    "updated_date": "2025-07-19 21:44:14 UTC"
  },
  {
    "arxiv_id": "2507.14758v1",
    "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization",
    "authors": [
      "Luyi Ma",
      "Wanjia Zhang",
      "Kai Zhao",
      "Abhishek Kulkarni",
      "Lalitesh Morishetti",
      "Anjana Ganesh",
      "Ashish Ranjan",
      "Aashika Padmanabhan",
      "Jianpeng Xu",
      "Jason Cho",
      "Praveen Kanumala",
      "Kaushiki Nag",
      "Sumit Dutta",
      "Kamiya Motwani",
      "Malay Patel",
      "Evren Korpeoglu",
      "Sushant Kumar",
      "Kannan Achan"
    ],
    "abstract": "Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of transformers and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware sparse Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems (RecSys) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14758v1",
    "published_date": "2025-07-19 21:23:23 UTC",
    "updated_date": "2025-07-19 21:23:23 UTC"
  },
  {
    "arxiv_id": "2507.14757v1",
    "title": "Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space",
    "authors": [
      "Szymon Mazurek",
      "Jakub Caputa",
      "Maciej Wielgosz"
    ],
    "abstract": "Spiking Neural Networks (SNNs) offer energy-efficient and biologically plausible alternatives to traditional artificial neural networks, but their performance depends critically on the tuning of neuron model parameters. In this work, we identify and characterize an operational space - a constrained region in the neuron hyperparameter domain (specifically membrane time constant tau and voltage threshold vth) - within which the network exhibits meaningful activity and functional behavior. Operating inside this manifold yields optimal trade-offs between classification accuracy and spiking activity, while stepping outside leads to degeneration: either excessive energy use or complete network silence.\n  Through systematic exploration across datasets and architectures, we visualize and quantify this manifold and identify efficient operating points. We further assess robustness to adversarial noise, showing that SNNs exhibit increased spike correlation and internal synchrony when operating outside their optimal region. These findings highlight the importance of principled hyperparameter tuning to ensure both task performance and energy efficiency. Our results offer practical guidelines for deploying robust and efficient SNNs, particularly in neuromorphic computing scenarios.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14757v1",
    "published_date": "2025-07-19 21:13:53 UTC",
    "updated_date": "2025-07-19 21:13:53 UTC"
  },
  {
    "arxiv_id": "2507.14748v1",
    "title": "Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning",
    "authors": [
      "Patrik Reizinger",
      "Bálint Mucsányi",
      "Siyuan Guo",
      "Benjamin Eysenbach",
      "Bernhard Schölkopf",
      "Wieland Brendel"
    ],
    "abstract": "Self-supervised feature learning and pretraining methods in reinforcement learning (RL) often rely on information-theoretic principles, termed mutual information skill learning (MISL). These methods aim to learn a representation of the environment while also incentivizing exploration thereof. However, the role of the representation and mutual information parametrization in MISL is not yet well understood theoretically. Our work investigates MISL through the lens of identifiable representation learning by focusing on the Contrastive Successor Features (CSF) method. We prove that CSF can provably recover the environment's ground-truth features up to a linear transformation due to the inner product parametrization of the features and skill diversity in a discriminative sense. This first identifiability guarantee for representation learning in RL also helps explain the implications of different mutual information objectives and the downsides of entropy regularizers. We empirically validate our claims in MuJoCo and DeepMind Control and show how CSF provably recovers the ground-truth features both from states and pixels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14748v1",
    "published_date": "2025-07-19 20:48:46 UTC",
    "updated_date": "2025-07-19 20:48:46 UTC"
  },
  {
    "arxiv_id": "2507.14730v4",
    "title": "Towards Urban Planing AI Agent in the Age of Agentic AI",
    "authors": [
      "Rui Liu",
      "Tao Zhe",
      "Zhong-Ren Peng",
      "Necati Catbas",
      "Xinyue Ye",
      "Dongjie Wang",
      "Yanjie Fu"
    ],
    "abstract": "Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "this more comprehensive version is under reviewed in ACM SIGKDD exploration",
    "pdf_url": "https://arxiv.org/pdf/2507.14730v4",
    "published_date": "2025-07-19 19:40:42 UTC",
    "updated_date": "2025-10-09 03:28:38 UTC"
  },
  {
    "arxiv_id": "2507.14725v3",
    "title": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for Language Models",
    "authors": [
      "Anushka Tiwari",
      "Sayantan Pal",
      "Rohini K. Srihari",
      "Kaiyi Ji"
    ],
    "abstract": "Prompt-based continual learning (CL) provides a parameter-efficient approach for adapting large language models (LLMs) across task sequences. However, most existing methods rely on task-aware inference and maintain a growing set of task-specific prompts, which introduces two major challenges: (1) severe performance degradation on earlier tasks under task-agnostic inference, and (2) limited scalability due to prompt memory accumulation as task sequences grow. In this paper, we present GRID, a unified framework designed to address these challenges. GRID incorporates a decoding mechanism that enhances backward transfer by leveraging representative inputs, automatic task identification, and constrained decoding. Furthermore, it employs a gradient-guided prompt selection strategy to compress less informative prompts into a single aggregated representation, ensuring scalable and memory-efficient continual learning. Extensive experiments on long-sequence and negative transfer benchmarks show that GRID improves average accuracy and backward transfer, achieves competitive forward transfer, and substantially reduces prompt memory usage.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14725v3",
    "published_date": "2025-07-19 19:15:03 UTC",
    "updated_date": "2025-10-01 16:07:15 UTC"
  },
  {
    "arxiv_id": "2507.21125v1",
    "title": "RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline",
    "authors": [
      "Karan Mirhosseini",
      "Arya Aftab",
      "Alireza Sheikh"
    ],
    "abstract": "In an era of radical technology transformations, technology maps play a crucial role in enhancing decision making. These maps heavily rely on automated methods of technology extraction. This paper introduces Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for automated technology extraction from scientific literature. RATE combines Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation. This hybrid method results in high recall in candidate generation alongside with high precision in candidate filtering. While the pipeline is designed to be general and widely applicable, we demonstrate its use on 678 research articles focused on Brain-Computer Interfaces (BCIs) and Extended Reality (XR) as a case study. Consequently, The validated technology terms by RATE were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape. For the purpose of evaluation, a gold standard dataset of technologies in 70 selected random articles had been curated by the experts. In addition, a technology extraction model based on Bidirectional Encoder Representations of Transformers (BERT) was used as a comparative method. RATE achieved F1-score of 91.27%, Significantly outperforming BERT with F1-score of 53.73%. Our findings highlight the promise of definition-driven LLM methods for technology extraction and mapping. They also offer new insights into emerging trends within the BCI-XR field. The source code is available https://github.com/AryaAftab/RATE",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 4 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2507.21125v1",
    "published_date": "2025-07-19 19:00:27 UTC",
    "updated_date": "2025-07-19 19:00:27 UTC"
  },
  {
    "arxiv_id": "2507.14722v1",
    "title": "LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4",
    "authors": [
      "Matěj Kripner",
      "Michal Šustr",
      "Milan Straka"
    ],
    "abstract": "Automated theorem proving (ATP) has been a classical problem in artificial intelligence since its inception, yet it remains challenging due to its vast state and action space. Large language models (LLMs) have recently emerged as a promising heuristic for ATP, but they lack correctness guarantees and thus require interaction with a proof verifier. Such interactions typically follow one of two approaches: black-box interaction, which does not utilize intermediate proof states, or white-box approaches, which allow for incremental proof construction and examination of intermediate states. While black-box approaches have directly benefited from recent LLM advances, white-box methods have comparatively lagged behind. In this paper, we address this gap by introducing LeanTree, which consists of (i) a tool built in the Lean 4 language that factorizes complex proof states into simpler, independent branches, and (ii) a dataset of these factorized intermediate states. Our white-box tooling offers several advantages over black-box approaches: it simplifies evaluation, reduces necessary context, generates richer training data, enables parallel search across multiple states, supports efficient reuse of states, and provides feedback in case of errors. Our preliminary results hint that white-box approaches outperform black-box alternatives in some settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14722v1",
    "published_date": "2025-07-19 18:50:07 UTC",
    "updated_date": "2025-07-19 18:50:07 UTC"
  },
  {
    "arxiv_id": "2507.14719v1",
    "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix",
    "authors": [
      "Juan Manuel Contreras"
    ],
    "abstract": "As large language models (LLMs) become increasingly integrated into real-world applications, scalable and rigorous safety evaluation is essential. This paper introduces Aymara AI, a programmatic platform for generating and administering customized, policy-grounded safety evaluations. Aymara AI transforms natural-language safety policies into adversarial prompts and scores model responses using an AI-based rater validated against human judgments. We demonstrate its capabilities through the Aymara LLM Risk and Responsibility Matrix, which evaluates 20 commercially available LLMs across 10 real-world safety domains. Results reveal wide performance disparities, with mean safety scores ranging from 86.2% to 52.4%. While models performed well in well-established safety domains such as Misinformation (mean = 95.7%), they consistently failed in more complex or underspecified domains, notably Privacy & Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety scores differed significantly across both models and domains (p < .05). These findings underscore the inconsistent and context-dependent nature of LLM safety and highlight the need for scalable, customizable tools like Aymara AI to support responsible AI development and oversight.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14719v1",
    "published_date": "2025-07-19 18:49:16 UTC",
    "updated_date": "2025-07-19 18:49:16 UTC"
  },
  {
    "arxiv_id": "2507.14706v1",
    "title": "Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling",
    "authors": [
      "Claudio Giusti",
      "Luca Guarnera",
      "Mirko Casu",
      "Sebastiano Battiato"
    ],
    "abstract": "Detecting fraudulent credit card transactions remains a significant challenge, due to the extreme class imbalance in real-world data and the often subtle patterns that separate fraud from legitimate activity. Existing research commonly attempts to address this by generating synthetic samples for the minority class using approaches such as GANs, VAEs, or hybrid generative models. However, these techniques, particularly when applied only to minority-class data, tend to result in overconfident classifiers and poor latent cluster separation, ultimately limiting real-world detection performance. In this study, we propose the Causal Prototype Attention Classifier (CPAC), an interpretable architecture that promotes class-aware clustering and improved latent space structure through prototype-based attention mechanisms and we will couple it with the encoder in a VAE-GAN allowing it to offer a better cluster separation moving beyond post-hoc sample augmentation. We compared CPAC-augmented models to traditional oversamplers, such as SMOTE, as well as to state-of-the-art generative models, both with and without CPAC-based latent classifiers. Our results show that classifier-guided latent shaping with CPAC delivers superior performance, achieving an F1-score of 93.14\\% percent and recall of 90.18\\%, along with improved latent cluster separation. Further ablation studies and visualizations provide deeper insight into the benefits and limitations of classifier-driven representation learning for fraud detection. The codebase for this work will be available at final submission.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 14 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14706v1",
    "published_date": "2025-07-19 17:51:54 UTC",
    "updated_date": "2025-07-19 17:51:54 UTC"
  },
  {
    "arxiv_id": "2507.14705v1",
    "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents",
    "authors": [
      "Sai Wang",
      "Senthilnathan Subramanian",
      "Mudit Sahni",
      "Praneeth Gone",
      "Lingjie Meng",
      "Xiaochen Wang",
      "Nicolas Ferradas Bertoli",
      "Tingxian Cheng",
      "Jun Xu"
    ],
    "abstract": "Large-language-model (LLM) agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.\n  We present Neo, a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent interfaces, state controller and feedback loops are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. We release the framework to facilitate reproducible, high-fidelity testing of emerging agentic systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14705v1",
    "published_date": "2025-07-19 17:51:25 UTC",
    "updated_date": "2025-07-19 17:51:25 UTC"
  },
  {
    "arxiv_id": "2507.14698v2",
    "title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition",
    "authors": [
      "Xuetao Lin",
      "Tianhao Peng",
      "Peihong Dai",
      "Yu Liang",
      "Wenjun Wu"
    ],
    "abstract": "EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14698v2",
    "published_date": "2025-07-19 17:23:38 UTC",
    "updated_date": "2025-08-19 15:24:33 UTC"
  },
  {
    "arxiv_id": "2507.17772v1",
    "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments",
    "authors": [
      "Ahmad Alhonainy",
      "Praveen Rao"
    ],
    "abstract": "Federated Learning (FL) allows multiple distributed devices to jointly train a shared model without centralizing data, but communication cost remains a major bottleneck, especially in resource-constrained environments. This paper introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce unnecessary model update transmissions. By selectively forwarding significant updates, our approach lowers bandwidth usage while maintaining model accuracy. Experiments on CIFAR-10 and medical datasets show reduced communication with minimal accuracy loss. Results confirm that intelligent caching improves scalability, memory efficiency, and supports reliable FL in edge IoT networks, making it practical for deployment in smart cities, healthcare, and other latency-sensitive applications.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Journal",
    "pdf_url": "https://arxiv.org/pdf/2507.17772v1",
    "published_date": "2025-07-19 17:02:15 UTC",
    "updated_date": "2025-07-19 17:02:15 UTC"
  },
  {
    "arxiv_id": "2507.14693v1",
    "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation",
    "authors": [
      "Amina Dzafic",
      "Merve Kavut",
      "Ulya Bayram"
    ],
    "abstract": "Suicidal ideation detection is critical for real-time suicide prevention, yet its progress faces two under-explored challenges: limited language coverage and unreliable annotation practices. Most available datasets are in English, but even among these, high-quality, human-annotated data remains scarce. As a result, many studies rely on available pre-labeled datasets without examining their annotation process or label reliability. The lack of datasets in other languages further limits the global realization of suicide prevention via artificial intelligence (AI). In this study, we address one of these gaps by constructing a novel Turkish suicidal ideation corpus derived from social media posts and introducing a resource-efficient annotation framework involving three human annotators and two large language models (LLMs). We then address the remaining gaps by performing a bidirectional evaluation of label reliability and model consistency across this dataset and three popular English suicidal ideation detection datasets, using transfer learning through eight pre-trained sentiment and emotion classifiers. These transformers help assess annotation consistency and benchmark model performance against manually labeled data. Our findings underscore the need for more rigorous, language-inclusive approaches to annotation and evaluation in mental health natural language processing (NLP) while demonstrating the questionable performance of popular models with zero-shot transfer learning. We advocate for transparency in model training and dataset construction in mental health NLP, prioritizing data and model reliability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical and Health Informatics",
    "pdf_url": "https://arxiv.org/pdf/2507.14693v1",
    "published_date": "2025-07-19 16:54:36 UTC",
    "updated_date": "2025-07-19 16:54:36 UTC"
  },
  {
    "arxiv_id": "2508.00871v1",
    "title": "Patents as Knowledge Artifacts: An Information Science Perspective on Global Innovation",
    "authors": [
      "M. S. Rajeevan",
      "B. Mini Devi"
    ],
    "abstract": "In an age of fast-paced technological change, patents have evolved into not only legal mechanisms of intellectual property, but also structured storage containers of knowledge full of metadata, categories, and formal innovation. This chapter proposes to reframe patents in the context of information science, by focusing on patents as knowledge artifacts, and by seeing patents as fundamentally tied to the global movement of scientific and technological knowledge. With a focus on three areas, the inventions of AIs, biotech patents, and international competition with patents, this work considers how new technologies are challenging traditional notions of inventorship, access, and moral accountability.The chapter provides a critical analysis of AI's implications for patent authorship and prior art searches, ownership issues arising from proprietary claims in biotechnology to ethical dilemmas, and the problem of using patents for strategic advantage in a global context of innovation competition. In this analysis, the chapter identified the importance of organizing information, creating metadata standards about originality, implementing retrieval systems to access previous works, and ethical contemplation about patenting unseen relationships in innovation ecosystems. Ultimately, the chapter called for a collaborative, transparent, and ethically-based approach in managing knowledge in the patenting environment highlighting the role for information professionals and policy to contribute to access equity in innovation.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.DL",
    "comment": "Comments: 8 pages. This is a preprint version of the paper titled \"Patents as Knowledge Artifacts: An Information Science Perspective on Global Innovation\" Not peer-reviewed. Feedback welcome",
    "pdf_url": "https://arxiv.org/pdf/2508.00871v1",
    "published_date": "2025-07-19 16:33:39 UTC",
    "updated_date": "2025-07-19 16:33:39 UTC"
  },
  {
    "arxiv_id": "2507.14688v2",
    "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations",
    "authors": [
      "Mohammed Alkhowaiter",
      "Norah Alshahrani",
      "Saied Alshahrani",
      "Reem I. Masoud",
      "Alaa Alzahrani",
      "Deema Alnuhait",
      "Emad A. Alghamdi",
      "Khalid Almubarak"
    ],
    "abstract": "Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14688v2",
    "published_date": "2025-07-19 16:30:45 UTC",
    "updated_date": "2025-09-30 16:03:47 UTC"
  },
  {
    "arxiv_id": "2507.14680v1",
    "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis",
    "authors": [
      "Xinheng Lyu",
      "Yuci Liang",
      "Wenting Chen",
      "Meidan Ding",
      "Jiaqi Yang",
      "Guolin Huang",
      "Daokun Zhang",
      "Xiangjian He",
      "Linlin Shen"
    ],
    "abstract": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multi-agent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both task-specific accuracy and multi-task versatility through three components: (1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs and medical agent frameworks across diverse tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14680v1",
    "published_date": "2025-07-19 16:11:03 UTC",
    "updated_date": "2025-07-19 16:11:03 UTC"
  },
  {
    "arxiv_id": "2507.14679v2",
    "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks",
    "authors": [
      "Zhijie Wang",
      "Zixin Xu",
      "Zhiyuan Pan"
    ],
    "abstract": "The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. This work addresses two principal challenges: adversarial strategies employed by spammers and the scarcity of labeled data. We propose a novel spam-text detection framework GCC-Spam, which integrates three core innovations. First, a character similarity network captures orthographic and phonetic features to counter character-obfuscation attacks and furthermore produces sentence embeddings for downstream classification. Second, contrastive learning enhances discriminability by optimizing the latent-space distance between spam and normal texts. Third, a Generative Adversarial Network (GAN) generates realistic pseudo-spam samples to alleviate data scarcity while improving model robustness and classification accuracy. Extensive experiments on real-world datasets demonstrate that our model outperforms baseline approaches, achieving higher detection rates with significantly fewer labeled examples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14679v2",
    "published_date": "2025-07-19 16:09:48 UTC",
    "updated_date": "2025-07-24 15:46:28 UTC"
  },
  {
    "arxiv_id": "2507.22913v1",
    "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models",
    "authors": [
      "Jinyu Liu",
      "Xiaoying Song",
      "Diana Zhang",
      "Jason Thomale",
      "Daqing He",
      "Lingzi Hong"
    ],
    "abstract": "Providing subject access to information resources is an essential function of any library management system. Large language models (LLMs) have been widely used in classification and summarization tasks, but their capability to perform subject analysis is underexplored. Multi-label classification with traditional machine learning (ML) models has been used for subject analysis but struggles with unseen cases. LLMs offer an alternative but often over-generate and hallucinate. Therefore, we propose a hybrid framework that integrates embedding-based ML models with LLMs. This approach uses ML models to (1) predict the optimal number of LCSH labels to guide LLM predictions and (2) post-edit the predicted terms with actual LCSH terms to mitigate hallucinations. We experimented with LLMs and the hybrid framework to predict the subject terms of books using the Library of Congress Subject Headings (LCSH). Experiment results show that providing initial predictions to guide LLM generations and imposing post-edits result in more controlled and vocabulary-aligned outputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 2 figures, accepted by ASIST 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.22913v1",
    "published_date": "2025-07-19 15:32:46 UTC",
    "updated_date": "2025-07-19 15:32:46 UTC"
  },
  {
    "arxiv_id": "2507.14662v1",
    "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall",
    "authors": [
      "Shayan Rokhva",
      "Babak Teimourpour"
    ],
    "abstract": "Quantifying post-consumer food waste in institutional dining settings is essential for supporting data-driven sustainability strategies. This study presents a cost-effective computer vision framework that estimates plate-level food waste by utilizing semantic segmentation of RGB images taken before and after meal consumption across five Iranian dishes. Four fully supervised models (U-Net, U-Net++, and their lightweight variants) were trained using a capped dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a custom-defined Distributional Pixel Agreement (DPA) metric tailored to the task. All models achieved satisfying performance, and for each food type, at least one model approached or surpassed 90% DPA, demonstrating strong alignment in pixel-wise proportion estimates. Lighter models with reduced parameter counts offered faster inference, achieving real-time throughput on an NVIDIA T4 GPU. Further analysis showed superior segmentation performance for dry and more rigid components (e.g., rice and fries), while more complex, fragmented, or viscous dishes, such as stews, showed reduced performance, specifically post-consumption. Despite limitations such as reliance on 2D imaging, constrained food variety, and manual data collection, the proposed framework is pioneering and represents a scalable, contactless solution for continuous monitoring of food consumption. This research lays foundational groundwork for automated, real-time waste tracking systems in large-scale food service environments and offers actionable insights and outlines feasible future directions for dining hall management and policymakers aiming to reduce institutional food waste.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com; shayan1999rokh@yahoo.com",
    "pdf_url": "https://arxiv.org/pdf/2507.14662v1",
    "published_date": "2025-07-19 15:21:29 UTC",
    "updated_date": "2025-07-19 15:21:29 UTC"
  },
  {
    "arxiv_id": "2507.14660v2",
    "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems",
    "authors": [
      "Qibing Ren",
      "Sitao Xie",
      "Longxuan Wei",
      "Zhenfei Yin",
      "Junchi Yan",
      "Lizhuang Ma",
      "Jing Shao"
    ],
    "abstract": "Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern that AI-driven groups could also cause similar harm. While most AI safety research focuses on individual AI systems, the risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored. In this paper, we introduce a proof-of-concept to simulate the risks of malicious MAS collusion, using a flexible framework that supports both centralized and decentralized coordination structures. We apply this framework to two high-risk fields: misinformation spread and e-commerce fraud. Our findings show that decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions, like content flagging, are applied, decentralized groups can adjust their tactics to avoid detection. We present key insights into how these malicious groups operate and the need for better detection systems and countermeasures. Code is available at https://github.com/renqibing/RogueAgent.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Code is available at https://github.com/renqibing/MultiAgent4Collusion",
    "pdf_url": "https://arxiv.org/pdf/2507.14660v2",
    "published_date": "2025-07-19 15:17:30 UTC",
    "updated_date": "2025-07-24 06:00:02 UTC"
  },
  {
    "arxiv_id": "2507.14657v2",
    "title": "AI-Enhanced Precision in Sport Taekwondo: Increasing Fairness, Speed, and Trust in Competition (FST.ai)",
    "authors": [
      "Keivan Shariatmadar",
      "Ahmad Osman"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) into sports officiating represents a paradigm shift in how decisions are made in competitive environments. Traditional manual systems, even when supported by Instant Video Replay (IVR), often suffer from latency, subjectivity, and inconsistent enforcement, undermining fairness and athlete trust. This paper introduces 'FST.ai' -- which is developed under the 'R3AL.ai' project, which serves as its Principal Investigator: r3al.ai -- a novel AI-powered framework designed to enhance officiating in Sport Taekwondo, particularly focusing on the complex task of real-time head kick detection and scoring. Leveraging computer vision, deep learning, and edge inference, the system automates the identification and classification of key actions, significantly reducing decision time from minutes to seconds while improving consistency and transparency. Importantly, the methodology is not limited to Taekwondo. The underlying framework -- based on pose estimation, motion classification, and impact analysis -- can be adapted to a wide range of sports requiring action detection, such as judo, karate, fencing, or even team sports like football and basketball, where foul recognition or performance tracking is critical. By addressing one of Taekwondo's most challenging scenarios -- head kick scoring -- we demonstrate the robustness, scalability, and sport-agnostic potential of 'FST.ai' to transform officiating standards across multiple disciplines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14657v2",
    "published_date": "2025-07-19 15:14:45 UTC",
    "updated_date": "2025-07-22 14:19:12 UTC"
  },
  {
    "arxiv_id": "2507.14649v1",
    "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs",
    "authors": [
      "Minsuh Joo",
      "Hyunsoo Cho"
    ],
    "abstract": "Despite the outstanding performance of large language models (LLMs) across various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate responses--remains as a critical problem as it can be directly connected to a crisis of building safe and reliable LLMs. Uncertainty estimation is primarily used to measure hallucination levels in LLM responses so that correct and incorrect answers can be distinguished clearly. This study proposes an effective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based sem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse quantifies the uncertainty with the proportion of the intra-cluster consistency in the total consistency between LLM hidden embeddings which contain adequate semantic information of generations, by employing clustering. The effectiveness of Cleanse for detecting hallucination is validated using four off-the-shelf models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two question-answering benchmarks, SQuAD and CoQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14649v1",
    "published_date": "2025-07-19 14:48:24 UTC",
    "updated_date": "2025-07-19 14:48:24 UTC"
  },
  {
    "arxiv_id": "2507.14642v2",
    "title": "Efficient Story Point Estimation With Comparative Learning",
    "authors": [
      "Monoshiz Mahbub Khan",
      "Xiaoyin Xi",
      "Andrew Meneely",
      "Zhe Yu"
    ],
    "abstract": "Story point estimation is an essential part of agile software development. Story points are unitless, project-specific effort estimates that help developers plan their sprints. Traditionally, developers estimate story points collaboratively using planning poker or other manual techniques. While the initial calibrating of the estimates to each project is helpful, once a team has converged on a set of precedents, story point estimation can become tedious and labor-intensive. Machine learning can reduce this burden, but only with enough context from the historical decisions made by the project team. That is, state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate predictions (within-project) when trained on data from the same project. The goal of this work is to streamline story point estimation by evaluating a comparative learning-based framework for calibrating project-specific story point prediction models. Instead of assigning a specific story point value to every backlog item, developers are presented with pairs of items, and indicate which item requires more effort. Using these comparative judgments, a machine learning model is trained to predict the story point estimates. We empirically evaluated our technique using data with 23,313 manual estimates in 16 projects. The model learned from comparative judgments can achieve on average 0.34 Spearman's rank correlation coefficient between its predictions and the ground truth story points. This is similar to, if not better than, the performance of a regression model learned from the ground truth story points. Therefore, the proposed comparative learning approach is more efficient than state-of-the-art regression-based approaches according to the law of comparative judgments - providing comparative judgments yields a lower cognitive burden on humans than providing ratings or categorical labels.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14642v2",
    "published_date": "2025-07-19 14:36:19 UTC",
    "updated_date": "2025-11-14 15:36:40 UTC"
  },
  {
    "arxiv_id": "2507.14629v2",
    "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking",
    "authors": [
      "Juntao Tan",
      "Lan Zhang",
      "Zhonghao Hu",
      "Kai Yang",
      "Peng Ran",
      "Bo Li"
    ],
    "abstract": "Though vertical federated learning (VFL) is generally considered to be privacy-preserving, recent studies have shown that VFL system is vulnerable to label inference attacks originating from various attack surfaces. Among these attacks, the model completion (MC) attack is currently the most powerful one. Existing defense methods against it either sacrifice model accuracy or incur impractical computational overhead. In this paper, we propose VMask, a novel label privacy protection framework designed to defend against MC attack from the perspective of layer masking. Our key insight is to disrupt the strong correlation between input data and intermediate outputs by applying the secret sharing (SS) technique to mask layer parameters in the attacker's model. We devise a strategy for selecting critical layers to mask, reducing the overhead that would arise from naively applying SS to the entire model. Moreover, VMask is the first framework to offer a tunable privacy budget to defenders, allowing for flexible control over the levels of label privacy according to actual requirements. We built a VFL system, implemented VMask on it, and extensively evaluated it using five model architectures and 13 datasets with different modalities, comparing it to 12 other defense methods. The results demonstrate that VMask achieves the best privacy-utility trade-off, successfully thwarting the MC attack (reducing the label inference accuracy to a random guessing level) while preserving model performance (e.g., in Transformer-based model, the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up to 60,846 times faster than cryptography-based methods, and it only marginally exceeds that of standard VFL by 1.8 times in a large Transformer-based model, which is generally acceptable.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by Frontiers of Computer Science (FCS)",
    "pdf_url": "https://arxiv.org/pdf/2507.14629v2",
    "published_date": "2025-07-19 13:51:09 UTC",
    "updated_date": "2026-01-22 08:34:21 UTC"
  },
  {
    "arxiv_id": "2507.14625v2",
    "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning",
    "authors": [
      "Juntao Tan",
      "Anran Li",
      "Quanchao Liu",
      "Peng Ran",
      "Lan Zhang"
    ],
    "abstract": "Vertical federated learning (VFL) enables multiple parties with disjoint features to collaboratively train models without sharing raw data. While privacy vulnerabilities of VFL are extensively-studied, its security threats-particularly targeted label attacks-remain underexplored. In such attacks, a passive party perturbs inputs at inference to force misclassification into adversary-chosen labels. Existing methods rely on unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore anomaly detectors deployed in real-world systems. To bridge this gap, we introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly designed to evade detector-enhanced VFL inference. During the preparation stage, the attacker selects a minimal set of high-expressiveness samples (via maximum mean discrepancy), submits them through VFL protocol to collect predicted labels, and uses these pseudo-labels to train estimated detector and surrogate model on local features. In attack stage, these models guide gradient-based perturbations of remaining samples, crafting adversarial instances that induce targeted misclassifications and evade detection. We implement VTarbel and evaluate it against four model architectures, seven multimodal datasets, and two anomaly detectors. Across all settings, VTarbel outperforms four state-of-the-art baselines, evades detection, and retains effective against three representative privacy-preserving defenses. These results reveal critical security blind spots in current VFL deployments and underscore urgent need for robust, attack-aware defenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ACM Transactions on Sensor Networks (TOSN)",
    "pdf_url": "https://arxiv.org/pdf/2507.14625v2",
    "published_date": "2025-07-19 13:43:50 UTC",
    "updated_date": "2026-01-22 08:30:25 UTC"
  },
  {
    "arxiv_id": "2507.14615v1",
    "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper",
    "authors": [
      "Fred Mutisya",
      "Shikoh Gitau",
      "Christine Syovata",
      "Diana Oigara",
      "Ibrahim Matende",
      "Muna Aden",
      "Munira Ali",
      "Ryan Nyotu",
      "Diana Marion",
      "Job Nyangena",
      "Nasubo Ongoma",
      "Keith Mbae",
      "Elizabeth Wamicha",
      "Eric Mibuari",
      "Jean Philbert Nsengemana",
      "Talkmore Chidede"
    ],
    "abstract": "Large Language Models(LLMs) hold promise for improving healthcare access in low-resource settings, but their effectiveness in African primary care remains underexplored. We present a methodology for creating a benchmark dataset and evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our approach uses retrieval augmented generation (RAG) to ground clinical questions in Kenya's national guidelines, ensuring alignment with local standards. These guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic clinical scenarios, multiple-choice questions, and rationale based answers in English and Swahili. Kenyan physicians co-created and refined the dataset, and a blinded expert review process ensured clinical accuracy, clarity, and cultural appropriateness. The resulting Alama Health QA dataset includes thousands of regulator-aligned question answer pairs across common outpatient conditions. Beyond accuracy, we introduce evaluation metrics that test clinical reasoning, safety, and adaptability such as rare case detection (Needle in the Haystack), stepwise logic (Decision Points), and contextual adaptability. Initial results reveal significant performance gaps when LLMs are applied to localized scenarios, consistent with findings that LLM accuracy is lower on African medical content than on US-based benchmarks. This work offers a replicable model for guideline-driven, dynamic benchmarking to support safe AI deployment in African health systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming",
    "pdf_url": "https://arxiv.org/pdf/2507.14615v1",
    "published_date": "2025-07-19 13:25:26 UTC",
    "updated_date": "2025-07-19 13:25:26 UTC"
  },
  {
    "arxiv_id": "2507.14612v1",
    "title": "Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module",
    "authors": [
      "Pei-Xuan Li",
      "Wei-Yun Liang",
      "Fandel Lin",
      "Hsun-Ping Hsieh"
    ],
    "abstract": "Next point of interest (POI) recommendation primarily predicts future activities based on users' past check-in data and current status, providing significant value to users and service providers. We observed that the popular check-in times for different POI categories vary. For example, coffee shops are crowded in the afternoon because people like to have coffee to refresh after meals, while bars are busy late at night. However, existing methods rarely explore the relationship between POI categories and time, which may result in the model being unable to fully learn users' tendencies to visit certain POI categories at different times. Additionally, existing methods for modeling time information often convert it into time embeddings or calculate the time interval and incorporate it into the model, making it difficult to capture the continuity of time. Finally, during POI prediction, various weighting information is often ignored, such as the popularity of each POI, the transition relationships between POIs, and the distances between POIs, leading to suboptimal performance. To address these issues, this paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW). This framework aims to jointly consider POI category information and multiple POI weighting factors. Specifically, the proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs. We conducted experiments on two real-world datasets, and the results demonstrate that the proposed GDPW outperforms other existing models, improving performance by 3% to 11%.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14612v1",
    "published_date": "2025-07-19 13:16:44 UTC",
    "updated_date": "2025-07-19 13:16:44 UTC"
  },
  {
    "arxiv_id": "2507.14608v1",
    "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition",
    "authors": [
      "Nandani Sharma",
      "Dinesh Singh"
    ],
    "abstract": "Facial expression recognition is crucial for human-computer interaction applications such as face animation, video surveillance, affective computing, medical analysis, etc. Since the structure of facial attributes varies with facial expressions, incorporating structural information into facial attributes is essential for facial expression recognition. In this paper, we propose Exp-Graph, a novel framework designed to represent the structural relationships among facial attributes using graph-based modeling for facial expression recognition. For facial attributes graph representation, facial landmarks are used as the graph's vertices. At the same time, the edges are determined based on the proximity of the facial landmark and the similarity of the local appearance of the facial attributes encoded using the vision transformer. Additionally, graph convolutional networks are utilized to capture and integrate these structural dependencies into the encoding of facial attributes, thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph learns from the facial attribute graphs highly expressive semantic representations. On the other hand, the vision transformer and graph convolutional blocks help the framework exploit the local and global dependencies among the facial attributes that are essential for the recognition of facial expressions. We conducted comprehensive evaluations of the proposed Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW. The model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%, respectively. These results indicate that Exp-Graph maintains strong generalization capabilities across both controlled laboratory settings and real-world, unconstrained environments, underscoring its effectiveness for practical facial expression recognition applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14608v1",
    "published_date": "2025-07-19 13:10:21 UTC",
    "updated_date": "2025-07-19 13:10:21 UTC"
  },
  {
    "arxiv_id": "2507.14593v1",
    "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation",
    "authors": [
      "Omar Al-Desi"
    ],
    "abstract": "This paper presents the Coordinate Heart System (CHS), a geometric framework for emotion representation in artificial intelligence applications. We position eight core emotions as coordinates on a unit circle, enabling mathematical computation of complex emotional states through coordinate mixing and vector operations. Our initial five-emotion model revealed significant coverage gaps in the emotion space, leading to the development of an eight-emotion system that provides complete geometric coverage with mathematical guarantees. The framework converts natural language input to emotion coordinates and supports real-time emotion interpolation through computational algorithms. The system introduces a re-calibrated stability parameter S in [0,1], which dynamically integrates emotional load, conflict resolution, and contextual drain factors. This stability model leverages advanced Large Language Model interpretation of textual cues and incorporates hybrid temporal tracking mechanisms to provide nuanced assessment of psychological well-being states. Our key contributions include: (i) mathematical proof demonstrating why five emotions are insufficient for complete geometric coverage, (ii) an eight-coordinate system that eliminates representational blind spots, (iii) novel algorithms for emotion mixing, conflict resolution, and distance calculation in emotion space, and (iv) a comprehensive computational framework for AI emotion recognition with enhanced multi-dimensional stability modeling. Experimental validation through case studies demonstrates the system's capability to handle emotionally conflicted states, contextual distress factors, and complex psychological scenarios that traditional categorical emotion models cannot adequately represent. This work establishes a new mathematical foundation for emotion modeling in artificial intelligence systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.14593v1",
    "published_date": "2025-07-19 12:38:30 UTC",
    "updated_date": "2025-07-19 12:38:30 UTC"
  },
  {
    "arxiv_id": "2507.14592v1",
    "title": "A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification",
    "authors": [
      "Haochen Liu",
      "Jia Bi",
      "Xiaomin Wang",
      "Xin Yang",
      "Ling Wang"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14592v1",
    "published_date": "2025-07-19 12:35:45 UTC",
    "updated_date": "2025-07-19 12:35:45 UTC"
  },
  {
    "arxiv_id": "2508.03700v5",
    "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning",
    "authors": [
      "Liujian Tang",
      "Shaokang Dong",
      "Yijia Huang",
      "Minqi Xiang",
      "Hongtao Ruan",
      "Bin Wang",
      "Shuo Li",
      "Zhiheng Xi",
      "Zhihui Cao",
      "Hailiang Pang",
      "Heng Kong",
      "He Yang",
      "Mingxu Chai",
      "Zhilin Gao",
      "Xingyu Liu",
      "Yingnan Fu",
      "Jiaming Liu",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Tao Gui",
      "Qi Zhang",
      "Kang Wang",
      "Yunke Zhang",
      "Yuran Wang"
    ],
    "abstract": "This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.03700v5",
    "published_date": "2025-07-19 12:33:43 UTC",
    "updated_date": "2025-09-11 14:28:11 UTC"
  },
  {
    "arxiv_id": "2507.14590v1",
    "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification",
    "authors": [
      "Łukasz Radliński",
      "Mateusz Guściora",
      "Jan Kocoń"
    ],
    "abstract": "Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance. This paper systematically explores data augmentation methods for NLP, particularly through large language models like GPT. The purpose of this paper is to examine and evaluate whether traditional methods such as paraphrasing and backtranslation can leverage a new generation of models to achieve comparable performance to purely generative methods. Methods aimed at solving the problem of data scarcity and utilizing ChatGPT were chosen, as well as an exemplary dataset. We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups. We then evaluated the results both in terms of the quality of generated data and its impact on classification performance. The key findings indicate that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "International Conference on Computational Science 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14590v1",
    "published_date": "2025-07-19 12:23:20 UTC",
    "updated_date": "2025-07-19 12:23:20 UTC"
  },
  {
    "arxiv_id": "2507.14587v1",
    "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX",
    "authors": [
      "Merjem Bećirović",
      "Amina Kurtović",
      "Nordin Smajlović",
      "Medina Kapo",
      "Amila Akagić"
    ],
    "abstract": "Medical imaging plays a vital role in early disease diagnosis and monitoring. Specifically, blood microscopy offers valuable insights into blood cell morphology and the detection of hematological disorders. In recent years, deep learning-based automated classification systems have demonstrated high potential in enhancing the accuracy and efficiency of blood image analysis. However, a detailed performance analysis of specific deep learning frameworks appears to be lacking. This paper compares the performance of three popular deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in classifying blood cell images from the publicly available BloodMNIST dataset. The study primarily focuses on inference time differences, but also classification performance for different image sizes. The results reveal variations in performance across frameworks, influenced by factors such as image resolution and framework-specific optimizations. Classification accuracy for JAX and PyTorch was comparable to current benchmarks, showcasing the efficiency of these frameworks for medical image classification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14587v1",
    "published_date": "2025-07-19 12:05:14 UTC",
    "updated_date": "2025-07-19 12:05:14 UTC"
  },
  {
    "arxiv_id": "2507.14584v1",
    "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption",
    "authors": [
      "Kester Wong",
      "Sahan Bulathwela",
      "Mutlu Cukurova"
    ],
    "abstract": "The use of Bidirectional Encoder Representations from Transformers (BERT) model and its variants for classifying collaborative problem solving (CPS) has been extensively explored within the AI in Education community. However, limited attention has been given to understanding how individual tokenised words in the dataset contribute to the model's classification decisions. Enhancing the explainability of BERT-based CPS diagnostics is essential to better inform end users such as teachers, thereby fostering greater trust and facilitating wider adoption in education. This study undertook a preliminary step towards model transparency and explainability by using SHapley Additive exPlanations (SHAP) to examine how different tokenised words in transcription data contributed to a BERT model's classification of CPS processes. The findings suggested that well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions. Particular tokenised words were used frequently to affect classifications. The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class. While such model transparency is unlikely to be useful to an end user to improve their practice, it can help them not to overrely on LLM diagnostics and ignore their human expertise. We conclude the workshop paper by noting that the extent to which the model appropriately uses the tokens for its classification is associated with the number of classes involved. It calls for an investigation into the exploration of ensemble model architectures and the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to appear in the workshop proceedings for the HEXED'25 workshop in the 26th International Conference on Artificial Intelligence in Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.14584v1",
    "published_date": "2025-07-19 11:57:24 UTC",
    "updated_date": "2025-07-19 11:57:24 UTC"
  },
  {
    "arxiv_id": "2507.14579v1",
    "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models",
    "authors": [
      "Kester Wong",
      "Sahan Bulathwela",
      "Mutlu Cukurova"
    ],
    "abstract": "Detecting collaborative problem solving (CPS) indicators from dialogue using machine learning techniques is a significant challenge for the field of AI in Education. Recent studies have explored the use of Bidirectional Encoder Representations from Transformers (BERT) models on transcription data to reliably detect meaningful CPS indicators. A notable advancement involved the multimodal BERT variant, AudiBERT, which integrates speech and acoustic-prosodic audio features to enhance CPS diagnosis. Although initial results demonstrated multimodal improvements, the statistical significance of these enhancements remained unclear, and there was insufficient guidance on leveraging human-AI complementarity for CPS diagnosis tasks. This workshop paper extends the previous research by highlighting that the AudiBERT model not only improved the classification of classes that were sparse in the dataset, but it also had statistically significant class-wise improvements over the BERT model for classifications in the social-cognitive dimension. However, similar significant class-wise improvements over the BERT model were not observed for classifications in the affective dimension. A correlation analysis highlighted that larger training data was significantly associated with higher recall performance for both the AudiBERT and BERT models. Additionally, the precision of the BERT model was significantly associated with high inter-rater agreement among human coders. When employing the BERT model to diagnose indicators within these subskills that were well-detected by the AudiBERT model, the performance across all indicators was inconsistent. We conclude the paper by outlining a structured approach towards achieving human-AI complementarity for CPS diagnosis, highlighting the crucial inclusion of model explainability to support human agency and engagement in the reflective coding process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to appear in the workshop proceedings for the HEXED'25 workshop in the 26th International Conference on Artificial Intelligence in Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.14579v1",
    "published_date": "2025-07-19 11:47:08 UTC",
    "updated_date": "2025-07-19 11:47:08 UTC"
  },
  {
    "arxiv_id": "2507.15887v4",
    "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?",
    "authors": [
      "Ori Press",
      "Brandon Amos",
      "Haoyu Zhao",
      "Yikai Wu",
      "Samuel K. Ainsworth",
      "Dominik Krupke",
      "Patrick Kidger",
      "Touqir Sajed",
      "Bartolomeo Stellato",
      "Jisun Park",
      "Nathanael Bosch",
      "Eli Meril",
      "Albert Steppi",
      "Arman Zharmagambetov",
      "Fangzhao Zhang",
      "David Perez-Pineiro",
      "Alberto Mercurio",
      "Ni Zhan",
      "Talor Abramovich",
      "Kilian Lieret",
      "Hanlin Zhang",
      "Shirley Huang",
      "Matthias Bethge",
      "Ofir Press"
    ],
    "abstract": "Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it, profiles performance, verifies correctness on tests, and selects the fastest valid version. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15887v4",
    "published_date": "2025-07-19 11:23:25 UTC",
    "updated_date": "2025-10-24 12:16:20 UTC"
  },
  {
    "arxiv_id": "2507.14575v1",
    "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation",
    "authors": [
      "Andrea Moschetto",
      "Lemuel Puglisi",
      "Alec Sargood",
      "Pierluigi Dell'Acqua",
      "Francesco Guarnera",
      "Sebastiano Battiato",
      "Daniele Ravì"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14575v1",
    "published_date": "2025-07-19 10:58:02 UTC",
    "updated_date": "2025-07-19 10:58:02 UTC"
  },
  {
    "arxiv_id": "2507.14570v1",
    "title": "LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges",
    "authors": [
      "Xu Cheng",
      "Liang Yao",
      "Feng He",
      "Yukuo Cen",
      "Yufei He",
      "Chenhui Zhang",
      "Wenzheng Feng",
      "Hongyun Cai",
      "Jie Tang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for various graph mining tasks, yet existing scalable solutions often struggle to balance execution efficiency with prediction accuracy. These difficulties stem from iterative message-passing techniques, which place significant computational demands and require extensive GPU memory, particularly when dealing with the neighbor explosion issue inherent in large-scale graphs. This paper introduces a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN, which can perform representation learning on 100 billion graphs with a single GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We examine existing graph partitioning methods and design a superior graph partition algorithm named LPMetis. In particular, LPMetis outperforms current state-of-the-art (SOTA) approaches on various evaluation metrics. In addition, our paper proposes a subgraph augmentation strategy to enhance the model's predictive performance. It exhibits excellent compatibility, allowing the entire framework to accommodate various GNN algorithms. Successfully deployed on the Tencent platform, LPS-GNN has been tested on public and real-world datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in online applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14570v1",
    "published_date": "2025-07-19 10:44:26 UTC",
    "updated_date": "2025-07-19 10:44:26 UTC"
  },
  {
    "arxiv_id": "2507.14552v1",
    "title": "Large Language Models Assisting Ontology Evaluation",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisärkkä",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "abstract": "Ontology evaluation through functional requirements, such as testing via competency question (CQ) verification, is a well-established yet costly, labour-intensive, and error-prone endeavour, even for ontology engineering experts. In this work, we introduce OE-Assist, a novel framework designed to assist ontology evaluation through automated and semi-automated CQ verification. By presenting and leveraging a dataset of 1,393 CQs paired with corresponding ontologies and ontology stories, our contributions present, to our knowledge, the first systematic investigation into large language model (LLM)-assisted ontology evaluation, and include: (i) evaluating the effectiveness of a LLM-based approach for automatically performing CQ verification against a manually created gold standard, and (ii) developing and assessing an LLM-powered framework to assist CQ verification with Protégé, by providing suggestions. We found that automated LLM-based evaluation with o1-preview and o3-mini perform at a similar level to the average user's performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14552v1",
    "published_date": "2025-07-19 09:13:51 UTC",
    "updated_date": "2025-07-19 09:13:51 UTC"
  },
  {
    "arxiv_id": "2507.14544v1",
    "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025",
    "authors": [
      "Sujata Gaihre",
      "Amir Thapa Magar",
      "Prasuna Pokharel",
      "Laxmi Tiwari"
    ],
    "abstract": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA 2025 Challenge, which targets visual question answering (VQA) for gastrointestinal endoscopy. We adopt the Florence model-a large-scale multimodal foundation model-as the backbone of our VQA pipeline, pairing a powerful vision encoder with a text encoder to interpret endoscopic images and produce clinically relevant answers. To improve generalization, we apply domain-specific augmentations that preserve medical features while increasing training diversity. Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics. Our results highlight the potential of large multimodal models in medical VQA and provide a strong baseline for future work on explainability, robustness, and clinical integration. The code is publicly available at: https://github.com/TiwariLaxuu/VQA-Florence.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings",
    "pdf_url": "https://arxiv.org/pdf/2507.14544v1",
    "published_date": "2025-07-19 09:04:13 UTC",
    "updated_date": "2025-07-19 09:04:13 UTC"
  },
  {
    "arxiv_id": "2507.14520v2",
    "title": "What if Othello-Playing Language Models Could See?",
    "authors": [
      "Xinyi Chen",
      "Yifei Yuan",
      "Jiaang Li",
      "Serge Belongie",
      "Maarten de Rijke",
      "Anders Søgaard"
    ],
    "abstract": "Language models are often said to face a symbol grounding problem. While some have argued the problem can be solved without resort to other modalities, many have speculated that grounded learning is more efficient. We explore this question in Othello, a simplified, rule-based world that offers a controlled and interpretable testbed for studying world understanding. Building on prior work, we introduce VISOTHELLO, a multi-modal model trained jointly on move sequences and board images. Using the Othello rule understanding task, we examine whether multi-modal learning provides advantages over text-only approaches. We further evaluate robustness under semantically irrelevant perturbations and analyze the consistency of cross-modal alignment. Our results suggest that multi-modal training not only improves performance and robustness but also promotes convergence toward shared internal representations across different model architectures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2025 Assessing World Models Workshop; EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2507.14520v2",
    "published_date": "2025-07-19 07:47:55 UTC",
    "updated_date": "2025-10-01 12:00:25 UTC"
  },
  {
    "arxiv_id": "2507.14519v1",
    "title": "Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives",
    "authors": [
      "Wenxuan Zeng",
      "Tianshi Xu",
      "Yi Chen",
      "Yifan Zhou",
      "Mingzhe Zhang",
      "Jin Tan",
      "Cheng Hong",
      "Meng Li"
    ],
    "abstract": "Privacy-preserving machine learning (PPML) based on cryptographic protocols has emerged as a promising paradigm to protect user data privacy in cloud-based machine learning services. While it achieves formal privacy protection, PPML often incurs significant efficiency and scalability costs due to orders of magnitude overhead compared to the plaintext counterpart. Therefore, there has been a considerable focus on mitigating the efficiency gap for PPML. In this survey, we provide a comprehensive and systematic review of recent PPML studies with a focus on cross-level optimizations. Specifically, we categorize existing papers into protocol level, model level, and system level, and review progress at each level. We also provide qualitative and quantitative comparisons of existing works with technical insights, based on which we discuss future research directions and highlight the necessity of integrating optimizations across protocol, model, and system levels. We hope this survey can provide an overarching understanding of existing approaches and potentially inspire future breakthroughs in the PPML field. As the field is evolving fast, we also provide a public GitHub repository to continuously track the developments, which is available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This work will be continuously updated to reflect the latest advances",
    "pdf_url": "https://arxiv.org/pdf/2507.14519v1",
    "published_date": "2025-07-19 07:45:39 UTC",
    "updated_date": "2025-07-19 07:45:39 UTC"
  },
  {
    "arxiv_id": "2507.17848v1",
    "title": "Explainable Graph Neural Networks via Structural Externalities",
    "authors": [
      "Lijun Wu",
      "Dong Hao",
      "Zhiyi Fan"
    ],
    "abstract": "Graph Neural Networks (GNNs) have achieved outstanding performance across a wide range of graph-related tasks. However, their \"black-box\" nature poses significant challenges to their explainability, and existing methods often fail to effectively capture the intricate interaction patterns among nodes within the network. In this work, we propose a novel explainability framework, GraphEXT, which leverages cooperative game theory and the concept of social externalities. GraphEXT partitions graph nodes into coalitions, decomposing the original graph into independent subgraphs. By integrating graph structure as an externality and incorporating the Shapley value under externalities, GraphEXT quantifies node importance through their marginal contributions to GNN predictions as the nodes transition between coalitions. Unlike traditional Shapley value-based methods that primarily focus on node attributes, our GraphEXT places greater emphasis on the interactions among nodes and the impact of structural changes on GNN predictions. Experimental studies on both synthetic and real-world datasets show that GraphEXT outperforms existing baseline methods in terms of fidelity across diverse GNN architectures , significantly enhancing the explainability of GNN models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "econ.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.17848v1",
    "published_date": "2025-07-19 07:36:47 UTC",
    "updated_date": "2025-07-19 07:36:47 UTC"
  },
  {
    "arxiv_id": "2507.14516v2",
    "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning",
    "authors": [
      "Jeyoung Lee",
      "Hochul Kang"
    ],
    "abstract": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. SDSC addresses this by quantifying structural agreement between temporal signals based on the intersection of signed amplitudes, derived from the Dice Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware metric, it can be used as a loss by subtracting from 1 and applying a differentiable approximation of the Heaviside function for gradient-based optimization. A hybrid loss formulation is also proposed to combine SDSC with MSE, improving stability and preserving amplitude where necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. The results suggest that structural fidelity in signal representations enhances the semantic representation quality, supporting the consideration of structure-aware metrics as viable alternatives to conventional distance-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14516v2",
    "published_date": "2025-07-19 07:32:00 UTC",
    "updated_date": "2025-07-24 07:48:25 UTC"
  },
  {
    "arxiv_id": "2507.14513v1",
    "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy",
    "authors": [
      "Hongyi Yang",
      "Yue Pan",
      "Jiayi Xu",
      "Kelsen Liu"
    ],
    "abstract": "Recent advances in large language models (LLMs) and autonomous agents have enabled systems capable of performing complex tasks across domains such as human-computer interaction, planning, and web navigation. However, many existing frameworks struggle in real-world or resource-constrained environments due to their reliance on cloud-based computation, limited robustness in dynamic contexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous agents optimized for embedded systems. Written in Rust for safety and performance, Amico supports reactive, persistent agents that operate efficiently across embedded platforms and browser environments via WebAssembly. It provides clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules. Amico delivers a unified infrastructure for constructing resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14513v1",
    "published_date": "2025-07-19 07:21:09 UTC",
    "updated_date": "2025-07-19 07:21:09 UTC"
  },
  {
    "arxiv_id": "2507.14507v2",
    "title": "Diffusion Models for Time Series Forecasting: A Survey",
    "authors": [
      "Chen Su",
      "Zhengzhou Cai",
      "Yuanhe Tian",
      "Zhuochao Chang",
      "Zihong Zheng",
      "Yan Song"
    ],
    "abstract": "Diffusion models, initially developed for image synthesis, demonstrate remarkable generative capabilities. Recently, their application has expanded to time series forecasting (TSF), yielding promising results. Existing surveys on time series primarily focus on the application of diffusion models to time series tasks or merely provide model-by-model introductions of diffusion-based TSF models, without establishing a systematic taxonomy for existing diffusion-based TSF models. In this survey, we firstly introduce several standard diffusion models and their prevalent variants, explaining their adaptation to TSF tasks. Then, we provide a comprehensive review of diffusion models for TSF, paying special attention to the sources of conditional information and the mechanisms for integrating this conditioning within the models. In analyzing existing approaches using diffusion models for TSF, we provide a systematic categorization and a comprehensive summary of them in this survey. Furthermore, we examine several foundational diffusion models applied to TSF, alongside commonly used datasets and evaluation metrics. Finally, we discuss the progress and limitations of these approaches, as well as potential future research directions for diffusion-based TSF. Overall, this survey offers a comprehensive overview of recent progress and future prospects for diffusion models in TSF, serving as a valuable reference for researchers in the field.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14507v2",
    "published_date": "2025-07-19 07:04:04 UTC",
    "updated_date": "2025-08-31 10:48:07 UTC"
  },
  {
    "arxiv_id": "2507.19523v2",
    "title": "Language Models for Controllable DNA Sequence Design",
    "authors": [
      "Xingyu Su",
      "Xiner Li",
      "Yuchao Lin",
      "Ziqian Xie",
      "Degui Zhi",
      "Shuiwang Ji"
    ],
    "abstract": "We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.19523v2",
    "published_date": "2025-07-19 06:23:17 UTC",
    "updated_date": "2025-12-09 04:26:59 UTC"
  },
  {
    "arxiv_id": "2507.14499v1",
    "title": "Neural Brownian Motion",
    "authors": [
      "Qian Qi"
    ],
    "abstract": "This paper introduces the Neural-Brownian Motion (NBM), a new class of stochastic processes for modeling dynamics under learned uncertainty. The NBM is defined axiomatically by replacing the classical martingale property with respect to linear expectation with one relative to a non-linear Neural Expectation Operator, $\\varepsilon^θ$, generated by a Backward Stochastic Differential Equation (BSDE) whose driver $f_θ$ is parameterized by a neural network. Our main result is a representation theorem for a canonical NBM, which we define as a continuous $\\varepsilon^θ$-martingale with zero drift under the physical measure. We prove that, under a key structural assumption on the driver, such a canonical NBM exists and is the unique strong solution to a stochastic differential equation of the form ${\\rm d} M_t = ν_θ(t, M_t) {\\rm d} W_t$. Crucially, the volatility function $ν_θ$ is not postulated a priori but is implicitly defined by the algebraic constraint $g_θ(t, M_t, ν_θ(t, M_t)) = 0$, where $g_θ$ is a specialization of the BSDE driver. We develop the stochastic calculus for this process and prove a Girsanov-type theorem for the quadratic case, showing that an NBM acquires a drift under a new, learned measure. The character of this measure, whether pessimistic or optimistic, is endogenously determined by the learned parameters $θ$, providing a rigorous foundation for models where the attitude towards uncertainty is a discoverable feature.",
    "categories": [
      "math.PR",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "math.PR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14499v1",
    "published_date": "2025-07-19 06:09:52 UTC",
    "updated_date": "2025-07-19 06:09:52 UTC"
  },
  {
    "arxiv_id": "2507.22912v1",
    "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms",
    "authors": [
      "Navid Yazdanjue",
      "Morteza Rakhshaninejad",
      "Hossein Yazdanjouei",
      "Mohammad Sadegh Khorshidi",
      "Mikko S. Niemela",
      "Fang Chen",
      "Amir H. Gandomi"
    ],
    "abstract": "Illegal marketplaces have increasingly shifted to concealed parts of the internet, including the deep and dark web, as well as platforms such as Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of illicit goods including drugs, weapons, and stolen credentials. Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources. This paper presents a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy to detect and classify illicit marketplace content across diverse platforms. We extract semantic representations using ModernBERT, a transformer model for long documents, finetuned on domain-specific data from deep and dark web pages, Telegram channels, Subreddits, and Pastebin pastes to capture specialized jargon and ambiguous linguistic patterns. In addition, we incorporate manually engineered features such as document structure, embedded patterns including Bitcoin addresses, emails, and IPs, and metadata, which complement language model embeddings. The classification pipeline operates in two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random Forest, and SVM with entropy-based weighted voting to detect sales-related documents. The second stage further classifies these into drug, weapon, or credential sales. Experiments on three datasets, including our multi-source corpus, DUTA, and CoDA, show that our model outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.22912v1",
    "published_date": "2025-07-19 05:54:52 UTC",
    "updated_date": "2025-07-19 05:54:52 UTC"
  },
  {
    "arxiv_id": "2507.14485v1",
    "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion",
    "authors": [
      "Hongye Hou",
      "Liu Zhan",
      "Yang Yang"
    ],
    "abstract": "Completing the whole 3D structure based on an incomplete point cloud is a challenging task, particularly when the residual point cloud lacks typical structural characteristics. Recent methods based on cross-modal learning attempt to introduce instance images to aid the structure feature learning. However, they still focus on each particular input class, limiting their generation abilities. In this work, we propose a novel retrieval-augmented point cloud completion framework. The core idea is to incorporate cross-modal retrieval into completion task to learn structural prior information from similar reference samples. Specifically, we design a Structural Shared Feature Encoder (SSFE) to jointly extract cross-modal features and reconstruct reference features as priors. Benefiting from a dual-channel control gate in the encoder, relevant structural features in the reference sample are enhanced and irrelevant information interference is suppressed. In addition, we propose a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical feature fusion mechanism to integrate reference prior information with input features from global to local. Through extensive evaluations on multiple datasets and real-world scenes, our method shows its effectiveness in generating fine-grained point clouds, as well as its generalization capability in handling sparse data and unseen categories.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14485v1",
    "published_date": "2025-07-19 04:57:41 UTC",
    "updated_date": "2025-07-19 04:57:41 UTC"
  },
  {
    "arxiv_id": "2507.14481v1",
    "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning",
    "authors": [
      "Yujia Tong",
      "Jingling Yuan",
      "Tian Zhang",
      "Jianquan Liu",
      "Chuang Hu"
    ],
    "abstract": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers (ViTs) without requiring access to data, allowing for the deployment of ViTs on devices with limited resources. In DFQ, the quantization model must be calibrated using synthetic samples, making the quality of these synthetic samples crucial. Existing methods fail to fully capture and balance the global and local features within the samples, resulting in limited synthetic data quality. Moreover, we have found that during inference, there is a significant difference in the distributions of intermediate layer activations between the quantized and full-precision models. These issues lead to a severe performance degradation of the quantized model. To address these problems, we propose a pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT). Specifically, we synthesize samples in order of increasing difficulty, effectively enhancing the quality of synthetic data. During the calibration and inference stage, we introduce the activation correction matrix for the quantized model to align the intermediate layer activations with those of the full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves remarkable superiority over existing DFQ methods and its performance is on par with models quantized through real data. For example, the performance of DeiT-T with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our method eliminates the need for fine-tuning, which not only reduces computational overhead but also lowers the deployment barriers for edge devices. This characteristic aligns with the principles of Green Learning by improving energy efficiency and facilitating real-world applications in resource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14481v1",
    "published_date": "2025-07-19 04:32:04 UTC",
    "updated_date": "2025-07-19 04:32:04 UTC"
  },
  {
    "arxiv_id": "2507.15886v4",
    "title": "Combining Cost-Constrained Runtime Monitors for AI Safety",
    "authors": [
      "Tim Tian Hua",
      "James Baskerville",
      "Henri Lemoine",
      "Mia Hopman",
      "Aryan Bhatt",
      "Tyler Tracy"
    ],
    "abstract": "Monitoring AIs at runtime can help us detect and stop harmful actions. In this paper, we study how to efficiently combine multiple runtime monitors into a single monitoring protocol. The protocol's objective is to maximize the probability of applying a safety intervention on misaligned outputs (i.e., maximize recall). Since running monitors and applying safety interventions are costly, the protocol also needs to adhere to an average-case budget constraint. Taking the monitors' performance and cost as given, we develop an algorithm to find the best protocol. The algorithm exhaustively searches over when and which monitors to call, and allocates safety interventions based on the Neyman-Pearson lemma. By focusing on likelihood ratios and strategically trading off spending on monitors against spending on interventions, we more than double our recall rate compared to a naive baseline in a code review setting. We also show that combining two monitors can Pareto dominate using either monitor alone. Our framework provides a principled methodology for combining existing monitors to detect undesirable behavior in cost-sensitive settings.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.15886v4",
    "published_date": "2025-07-19 04:28:40 UTC",
    "updated_date": "2025-10-21 05:27:13 UTC"
  },
  {
    "arxiv_id": "2507.14472v1",
    "title": "Strategyproofness and Monotone Allocation of Auction in Social Networks",
    "authors": [
      "Yuhang Guo",
      "Dong Hao",
      "Bin Li",
      "Mingyu Xiao",
      "Bakh Khoussainov"
    ],
    "abstract": "Strategyproofness in network auctions requires that bidders not only report their valuations truthfully, but also do their best to invite neighbours from the social network. In contrast to canonical auctions, where the value-monotone allocation in Myerson's Lemma is a cornerstone, a general principle of allocation rules for strategyproof network auctions is still missing. We show that, due to the absence of such a principle, even extensions to multi-unit network auctions with single-unit demand present unexpected difficulties, and all pioneering researches fail to be strategyproof. For the first time in this field, we identify two categories of monotone allocation rules on networks: Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity (IP-MON). They encompass all existing allocation rules of network auctions as specific instances. For any given ID-MON or IP-MON allocation rule, we characterize the existence and sufficient conditions for the strategyproof payment rules, and show that among all such payment rules, the revenue-maximizing one exists and is computationally feasible. With these results, the obstacle of combinatorial network auction with single-minded bidders is now resolved.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14472v1",
    "published_date": "2025-07-19 04:05:35 UTC",
    "updated_date": "2025-07-19 04:05:35 UTC"
  },
  {
    "arxiv_id": "2507.14470v1",
    "title": "Approximate Revenue Maximization for Diffusion Auctions",
    "authors": [
      "Yifan Huang",
      "Dong Hao",
      "Zhiyi Fan",
      "Yuhang Guo",
      "Bin Li"
    ],
    "abstract": "Reserve prices are widely used in practice. The problem of designing revenue-optimal auctions based on reserve price has drawn much attention in the auction design community. Although they have been extensively studied, most developments rely on the significant assumption that the target audience of the sale is directly reachable by the auctioneer, while a large portion of bidders in the economic network unaware of the sale are omitted. This work follows the diffusion auction design, which aims to extend the target audience of optimal auction theory to all entities in economic networks. We investigate the design of simple and provably near-optimal network auctions via reserve price. Using Bayesian approximation analysis, we provide a simple and explicit form of the reserve price function tailored to the most representative network auction. We aim to balance setting a sufficiently high reserve price to induce high revenue in a successful sale, and attracting more buyers from the network to increase the probability of a successful sale. This reserve price function preserves incentive compatibility for network auctions, allowing the seller to extract additional revenue beyond that achieved by the Myerson optimal auction. Specifically, if the seller has $ρ$ direct neighbours in a network of size $n$, this reserve price guarantees a $1-{1 \\over ρ}$ approximation to the theoretical upper bound, i.e., the maximum possible revenue from any network of size $n$. This result holds for any size and any structure of the networked market.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14470v1",
    "published_date": "2025-07-19 04:04:09 UTC",
    "updated_date": "2025-07-19 04:04:09 UTC"
  },
  {
    "arxiv_id": "2507.14468v2",
    "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning",
    "authors": [
      "Yitong Lin",
      "Jiaying He",
      "Jiahe Chen",
      "Xinnan Zhu",
      "Jianwei Zheng",
      "Tao Bo"
    ],
    "abstract": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery and disease understanding, yet their completion and reasoning are challenging. Knowledge Embedding (KE) methods capture global semantics but struggle with dynamic structural integration, while Graph Neural Networks (GNNs) excel locally but often lack semantic understanding. Even ensemble approaches, including those leveraging language models, often fail to achieve a deep, adaptive, and synergistic co-evolution between semantic comprehension and structural learning. Addressing this critical gap in fostering continuous, reciprocal refinement between these two aspects in complex biomedical KGs is paramount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply synergistic semantic and structural learning. BioGraphFusion establishes a global semantic foundation via tensor decomposition, guiding an LSTM-driven mechanism to dynamically refine relation embeddings during graph propagation. This fosters adaptive interplay between semantic understanding and structural learning, further enhanced by query-guided subgraph construction and a hybrid scoring mechanism. Experiments across three key biomedical tasks demonstrate BioGraphFusion's superior performance over state-of-the-art KE, GNN, and ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1) highlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely available for download at https://github.com/Y-TARL/BioGraphFusion.\n  Supplementary information: Supplementary data are available at Bioinformatics online.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Bioinformatics on July 11th",
    "pdf_url": "https://arxiv.org/pdf/2507.14468v2",
    "published_date": "2025-07-19 04:03:42 UTC",
    "updated_date": "2025-07-22 04:03:12 UTC"
  },
  {
    "arxiv_id": "2507.14452v2",
    "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration",
    "authors": [
      "Weikang Gu",
      "Mingyue Han",
      "Li Xue",
      "Heng Dong",
      "Changcai Yang",
      "Riqing Chen",
      "Lifang Wei"
    ],
    "abstract": "The accurate identification of high-quality correspondences is a prerequisite task in feature-based point cloud registration. However, it is extremely challenging to handle the fusion of local and global features due to feature redundancy and complex spatial relationships. Given that Gestalt principles provide key advantages in analyzing local and global relationships, we propose a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric consistency (GPI-Net) in this paper. It utilizes Gestalt principles to facilitate complementary communication between local and global information. Specifically, we introduce an orthogonal integration strategy to optimally reduce redundant information and generate a more compact global structure for high-quality correspondences. To capture geometric features in correspondences, we leverage a Gestalt Feature Attention (GFA) block through a hybrid utilization of self-attention and cross-attention mechanisms. Furthermore, to facilitate the integration of local detail information into the global structure, we design an innovative Dual-path Multi-Granularity parallel interaction aggregation (DMG) block to promote information exchange across different granularities. Extensive experiments on various challenging tasks demonstrate the superior performance of our proposed GPI-Net in comparison to existing methods. The code will be released at https://github.com/gwk429/GPI-Net.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures. Accepted to IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14452v2",
    "published_date": "2025-07-19 02:56:29 UTC",
    "updated_date": "2025-09-01 14:51:09 UTC"
  },
  {
    "arxiv_id": "2507.14447v2",
    "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise",
    "authors": [
      "Guancheng Zeng",
      "Xueyi Chen",
      "Jiawang Hu",
      "Shaohua Qi",
      "Yaxuan Mao",
      "Zhantao Wang",
      "Yifan Nie",
      "Shuang Li",
      "Qiuyang Feng",
      "Pengxu Qiu",
      "Yujia Wang",
      "Wenqiang Han",
      "Linyan Huang",
      "Gang Li",
      "Jingjing Mo",
      "Haowen Hu"
    ],
    "abstract": "The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 8 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.14447v2",
    "published_date": "2025-07-19 02:46:19 UTC",
    "updated_date": "2025-07-22 10:01:32 UTC"
  },
  {
    "arxiv_id": "2507.14444v1",
    "title": "Statistical and Algorithmic Foundations of Reinforcement Learning",
    "authors": [
      "Yuejie Chi",
      "Yuxin Chen",
      "Yuting Wei"
    ],
    "abstract": "As a paradigm for sequential decision making in unknown environments, reinforcement learning (RL) has received a flurry of attention in recent years. However, the explosion of model complexity in emerging applications and the presence of nonconvexity exacerbate the challenge of achieving efficient RL in sample-starved situations, where data collection is expensive, time-consuming, or even high-stakes (e.g., in clinical trials, autonomous systems, and online advertising). How to understand and enhance the sample and computational efficacies of RL algorithms is thus of great interest. In this tutorial, we aim to introduce several important algorithmic and theoretical developments in RL, highlighting the connections between new ideas and classical topics. Employing Markov Decision Processes as the central mathematical model, we cover several distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL, robust RL, and RL with human feedback), and present several mainstream RL approaches (i.e., model-based approach, value-based approach, and policy optimization). Our discussions gravitate around the issues of sample complexity, computational efficiency, as well as algorithm-dependent and information-theoretic lower bounds from a non-asymptotic viewpoint.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "comment": "reading materials for INFORMS Tutorial in OR 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.14444v1",
    "published_date": "2025-07-19 02:42:41 UTC",
    "updated_date": "2025-07-19 02:42:41 UTC"
  },
  {
    "arxiv_id": "2507.22911v1",
    "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing",
    "authors": [
      "Jinzhi Wang",
      "Qingke Peng",
      "Haozhou Li",
      "Zeyuan Zeng",
      "Qinfeng Song",
      "Kaixuan Yang",
      "Jiangbo Zhang",
      "Yaoying Wang",
      "Ruimeng Li",
      "Biyi Zhou"
    ],
    "abstract": "Electric power marketing customer service plays a critical role in addressing inquiries, complaints, and service requests. However, current systems, such as China's 95598 hotline, often struggle with slow response times, inflexible procedures, and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities, they lack the domain expertise and empathy required in this field. To bridge this gap, we introduce ElectriQ, the first benchmark designed to evaluate and enhance LLMs in electric power marketing scenarios. ElectriQ consists of a dialogue dataset covering six key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. We further incorporate a domain-specific knowledge base and propose a knowledge augmentation method to boost model performance. Experiments on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and augmented, can surpass GPT-4o in terms of professionalism and user-friendliness. ElectriQ establishes a comprehensive foundation for developing LLMs tailored to the needs of power marketing services.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.22911v1",
    "published_date": "2025-07-19 02:28:51 UTC",
    "updated_date": "2025-07-19 02:28:51 UTC"
  },
  {
    "arxiv_id": "2507.16840v1",
    "title": "CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples",
    "authors": [
      "Weijia Yang",
      "Tian Lan",
      "Leyuan Liu",
      "Wei Chen",
      "Tianqing Zhu",
      "Sheng Wen",
      "Xiaosong Zhang"
    ],
    "abstract": "The rapid evolution of digital currency trading, fueled by the integration of blockchain technology, has led to both innovation and the emergence of smart Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in smart contract that uses funds from new investors to pay returns to earlier investors. Traditional Ponzi scheme detection methods based on deep learning typically rely on fully supervised models, which require large amounts of labeled data. However, such data is often scarce, hindering effective model training. To address this challenge, we propose a novel contrastive learning framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more negative samples), designed to enhance smart Ponzi scheme detection in blockchain transactions. By leveraging contrastive learning techniques, CASPER can learn more effective representations of smart contract source code using unlabeled datasets, significantly reducing both operational costs and system complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the baseline by 2.3% in F1 score when trained with 100% labeled data. More impressively, with only 25% labeled data, CASPER achieves an F1 score nearly 20% higher than the baseline under identical experimental conditions. These results highlight CASPER's potential for effective and cost-efficient detection of smart Ponzi schemes, paving the way for scalable fraud detection solutions in the future.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.16840v1",
    "published_date": "2025-07-19 01:26:02 UTC",
    "updated_date": "2025-07-19 01:26:02 UTC"
  },
  {
    "arxiv_id": "2507.14419v1",
    "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling",
    "authors": [
      "Guojun Wu"
    ],
    "abstract": "Prior work proposed simple test-time scaling, a method for replicating this scaling behavior with models distilled from o1-like models by manually controlling test-time compute: either scaling down by enforcing a maximum length or scaling up by iteratively appending \"Wait\" when the model is about to terminate its generation. This paper presents an analysis of simple test-time scaling and finds that the scaling behavior is largely attributed to scaling down by enforcing a maximum length. In contrast, fine-tuning on long CoT data distilled from o1-like models has no significant impact on scaling behavior, and scaling up by appending \"Wait\" leads to inconsistencies, as the model may oscillate between solutions. A key distinction exists between scaling down by enforcing a maximum length and scaling up test-time compute in o1-like models, such as DeepSeek-R1\\@. These models are typically allowed to utilize as much compute as needed, with the only constraint being the model's maximum supported length. By learning to naturally scale up test-time compute during reinforcement learning, o1-like models surpass their peak performance when scaling up. In contrast, simple test-time scaling progressively imposes a lower upper limit on model performance as it scales down. While replicating the test-time scaling behavior of o1 models can be straightforward by scaling down, it is crucial to recognize that the goal of scaling test-time compute is to unlock higher performance -- beyond what the model could originally achieve -- rather than merely reproducing the appearance of scaling behavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14419v1",
    "published_date": "2025-07-19 00:28:10 UTC",
    "updated_date": "2025-07-19 00:28:10 UTC"
  },
  {
    "arxiv_id": "2507.14418v1",
    "title": "Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students",
    "authors": [
      "Taufiq Daryanto",
      "Sophia Stil",
      "Xiaohan Ding",
      "Daniel Manesh",
      "Sang Won Lee",
      "Tim Lee",
      "Stephanie Lunn",
      "Sarah Rodriguez",
      "Chris Brown",
      "Eugenia Rho"
    ],
    "abstract": "One challenge in technical interviews is the think-aloud process, where candidates verbalize their thought processes while solving coding tasks. Despite its importance, opportunities for structured practice remain limited. Conversational AI offers potential assistance, but limited research explores user perceptions of its role in think-aloud practice. To address this gap, we conducted a study with 17 participants using an LLM-based technical interview practice tool. Participants valued AI's role in simulation, feedback, and learning from generated examples. Key design recommendations include promoting social presence in conversational AI for technical interview simulation, providing feedback beyond verbal content analysis, and enabling crowdsourced think-aloud examples through human-AI collaboration. Beyond feature design, we examined broader considerations, including intersectional challenges and potential strategies to address them, how AI-driven interview preparation could promote equitable learning in computing careers, and the need to rethink AI's role in interview practice by suggesting a research direction that integrates human-AI collaboration.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14418v1",
    "published_date": "2025-07-19 00:15:05 UTC",
    "updated_date": "2025-07-19 00:15:05 UTC"
  },
  {
    "arxiv_id": "2507.14417v2",
    "title": "Inverse Scaling in Test-Time Compute",
    "authors": [
      "Aryo Pradipta Gema",
      "Alexander Hägele",
      "Runjin Chen",
      "Andy Arditi",
      "Jacob Goldman-Wetzler",
      "Kit Fraser-Taliente",
      "Henry Sleight",
      "Linda Petrini",
      "Julian Michael",
      "Beatrice Alex",
      "Pasquale Minervini",
      "Yanda Chen",
      "Joe Benton",
      "Ethan Perez"
    ],
    "abstract": "We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in TMLR (12/2025; Featured Certification; J2C Certification), 78 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.14417v2",
    "published_date": "2025-07-19 00:06:13 UTC",
    "updated_date": "2025-12-15 23:25:16 UTC"
  }
]
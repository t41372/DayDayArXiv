[
  {
    "arxiv_id": "2401.09651v2",
    "title": "Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning",
    "authors": [
      "Charles Dickens",
      "Changyu Gao",
      "Connor Pryor",
      "Stephen Wright",
      "Lise Getoor"
    ],
    "abstract": "We leverage convex and bilevel optimization techniques to develop a general\ngradient-based parameter learning framework for neural-symbolic (NeSy) systems.\nWe demonstrate our framework with NeuPSL, a state-of-the-art NeSy architecture.\nTo achieve this, we propose a smooth primal and dual formulation of NeuPSL\ninference and show learning gradients are functions of the optimal dual\nvariables. Additionally, we develop a dual block coordinate descent algorithm\nfor the new formulation that naturally exploits warm-starts. This leads to over\n100x learning runtime improvements over the current best NeuPSL inference\nmethod. Finally, we provide extensive empirical evaluations across 8 datasets\ncovering a range of tasks and demonstrate our learning framework achieves up to\na 16% point prediction performance improvement over alternative learning\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09651v2",
    "published_date": "2024-01-17 23:45:53 UTC",
    "updated_date": "2024-06-03 20:15:37 UTC"
  },
  {
    "arxiv_id": "2401.09646v1",
    "title": "ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change",
    "authors": [
      "David Thulke",
      "Yingbo Gao",
      "Petrus Pelser",
      "Rein Brune",
      "Rricha Jalota",
      "Floris Fok",
      "Michael Ramos",
      "Ian van Wyk",
      "Abdallah Nasir",
      "Hayden Goldstein",
      "Taylor Tragemann",
      "Katie Nguyen",
      "Ariana Fowler",
      "Andrew Stanco",
      "Jon Gabriel",
      "Jordan Taylor",
      "Dean Moro",
      "Evgenii Tsymbalov",
      "Juliette de Waal",
      "Evgeny Matusov",
      "Mudar Yaghi",
      "Mohammad Shihadah",
      "Hermann Ney",
      "Christian Dugast",
      "Jonathan Dotan",
      "Daniel Erasmus"
    ],
    "abstract": "This paper introduces ClimateGPT, a model family of domain-specific large\nlanguage models that synthesize interdisciplinary research on climate change.\nWe trained two 7B models from scratch on a science-oriented dataset of 300B\ntokens. For the first model, the 4.2B domain-specific tokens were included\nduring pre-training and the second was adapted to the climate domain after\npre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously\npre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each\nmodel is instruction fine-tuned on a high-quality and human-generated\ndomain-specific dataset that has been created in close cooperation with climate\nscientists. To reduce the number of hallucinations, we optimize the model for\nretrieval augmentation and propose a hierarchical retrieval strategy. To\nincrease the accessibility of our model to non-English speakers, we propose to\nmake use of cascaded machine translation and show that this approach can\nperform comparably to natively multilingual models while being easier to scale\nto a large number of languages. Further, to address the intrinsic\ninterdisciplinary aspect of climate change we consider different research\nperspectives. Therefore, the model can produce in-depth answers focusing on\ndifferent perspectives in addition to an overall answer. We propose a suite of\nautomatic climate-specific benchmarks to evaluate LLMs. On these benchmarks,\nClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model\nwhile not degrading results on general domain benchmarks. Our human evaluation\nconfirms the trends we saw in our benchmarks. All models were trained and\nevaluated using renewable energy and are released publicly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09646v1",
    "published_date": "2024-01-17 23:29:46 UTC",
    "updated_date": "2024-01-17 23:29:46 UTC"
  },
  {
    "arxiv_id": "2401.09640v2",
    "title": "Blackout Mitigation via Physics-guided RL",
    "authors": [
      "Anmol Dwivedi",
      "Santiago Paternain",
      "Ali Tajer"
    ],
    "abstract": "This paper considers the sequential design of remedial control actions in\nresponse to system anomalies for the ultimate objective of preventing\nblackouts. A physics-guided reinforcement learning (RL) framework is designed\nto identify effective sequences of real-time remedial look-ahead decisions\naccounting for the long-term impact on the system's stability. The paper\nconsiders a space of control actions that involve both discrete-valued\ntransmission line-switching decisions (line reconnections and removals) and\ncontinuous-valued generator adjustments. To identify an effective blackout\nmitigation policy, a physics-guided approach is designed that uses power-flow\nsensitivity factors associated with the power transmission network to guide the\nRL exploration during agent training. Comprehensive empirical evaluations using\nthe open-source Grid2Op platform demonstrate the notable advantages of\nincorporating physical signals into RL decisions, establishing the gains of the\nproposed physics-guided approach compared to its black box counterparts. One\nimportant observation is that strategically~\\emph{removing} transmission lines,\nin conjunction with multiple real-time generator adjustments, often renders\neffective long-term decisions that are likely to prevent or delay blackouts.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09640v2",
    "published_date": "2024-01-17 23:27:36 UTC",
    "updated_date": "2024-07-31 19:57:23 UTC"
  },
  {
    "arxiv_id": "2401.09637v2",
    "title": "Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study",
    "authors": [
      "Niklas Mannhardt",
      "Elizabeth Bondi-Kelly",
      "Barbara Lam",
      "Hussein Mozannar",
      "Chloe O'Connell",
      "Mercy Asiedu",
      "Alejandro Buendia",
      "Tatiana Urman",
      "Irbaz B. Riaz",
      "Catherine E. Ricciardi",
      "Monica Agrawal",
      "Marzyeh Ghassemi",
      "David Sontag"
    ],
    "abstract": "Large language models (LLMs) have immense potential to make information more\naccessible, particularly in medicine, where complex medical jargon can hinder\npatient comprehension of clinical notes. We developed a patient-facing tool\nusing LLMs to make clinical notes more readable by simplifying, extracting\ninformation from, and adding context to the notes. We piloted the tool with\nclinical notes donated by patients with a history of breast cancer and\nsynthetic notes from a clinician. Participants (N=200, healthy,\nfemale-identifying patients) were randomly assigned three clinical notes in our\ntool with varying levels of augmentations and answered quantitative and\nqualitative questions evaluating their understanding of follow-up actions.\nAugmentations significantly increased their quantitative understanding scores.\nIn-depth interviews were conducted with participants (N=7, patients with a\nhistory of breast cancer), revealing both positive sentiments about the\naugmentations and concerns about AI. We also performed a qualitative\nclinician-driven analysis of the model's error modes.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09637v2",
    "published_date": "2024-01-17 23:14:52 UTC",
    "updated_date": "2024-10-14 23:49:48 UTC"
  },
  {
    "arxiv_id": "2401.09615v2",
    "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
    "authors": [
      "Geetanjali Bihani",
      "Julia Taylor Rayz"
    ],
    "abstract": "The advent of large language models (LLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found that LLMs often resort to shortcuts when performing tasks,\ncreating an illusion of enhanced performance while lacking generalizability in\ntheir decision rules. This phenomenon introduces challenges in accurately\nassessing natural language understanding in LLMs. Our paper provides a concise\nsurvey of relevant research in this area and puts forth a perspective on the\nimplications of shortcut learning in the evaluation of language models,\nspecifically for NLU tasks. This paper urges more research efforts to be put\ntowards deepening our comprehension of shortcut learning, contributing to the\ndevelopment of more robust language models, and raising the standards of NLU\nevaluation in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at HICSS-SDPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09615v2",
    "published_date": "2024-01-17 21:55:15 UTC",
    "updated_date": "2024-02-09 22:08:12 UTC"
  },
  {
    "arxiv_id": "2401.09572v1",
    "title": "Handling Large-scale Cardinality in building recommendation systems",
    "authors": [
      "Dhruva Dixith Kurra",
      "Bo Ling",
      "Chun Zh",
      "Seyedshahin Ashrafzadeh"
    ],
    "abstract": "Effective recommendation systems rely on capturing user preferences, often\nrequiring incorporating numerous features such as universally unique\nidentifiers (UUIDs) of entities. However, the exceptionally high cardinality of\nUUIDs poses a significant challenge in terms of model degradation and increased\nmodel size due to sparsity. This paper presents two innovative techniques to\naddress the challenge of high cardinality in recommendation systems.\nSpecifically, we propose a bag-of-words approach, combined with layer sharing,\nto substantially decrease the model size while improving performance. Our\ntechniques were evaluated through offline and online experiments on Uber use\ncases, resulting in promising results demonstrating our approach's\neffectiveness in optimizing recommendation systems and enhancing their overall\nperformance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09572v1",
    "published_date": "2024-01-17 19:49:11 UTC",
    "updated_date": "2024-01-17 19:49:11 UTC"
  },
  {
    "arxiv_id": "2401.09566v2",
    "title": "Aligning Large Language Models with Counterfactual DPO",
    "authors": [
      "Bradley Butcher"
    ],
    "abstract": "Advancements in large language models (LLMs) have demonstrated remarkable\ncapabilities across a diverse range of applications. These models excel in\ngenerating text completions that are contextually coherent and cover an\nextensive array of subjects. However, the vast datasets required for their\ntraining make aligning response styles during the pretraining and instruction\ntuning phases challenging. Consequently, an additional alignment phase is\ntypically employed, wherein the model is further trained with human preference\ndata to better align its outputs with human expectations. While this process\ndoesn't introduce new capabilities per se, it does accentuate generation styles\ninnate to the model. This paper explores the utilization of counterfactual\nprompting within the framework of Direct Preference Optimization (DPO) to align\nthe model's style without relying on human intervention. We demonstrate that\nthis method effectively instils desirable behaviour, mitigates undesirable\nones, and encourages the model to disregard inappropriate instructions. Our\nfindings suggest that counterfactual prompting with DPO presents a low-resource\nway to fine-tune LLMs to meet the demands for responsible and ethically aligned\nAI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09566v2",
    "published_date": "2024-01-17 19:43:43 UTC",
    "updated_date": "2024-01-19 08:57:19 UTC"
  },
  {
    "arxiv_id": "2401.09556v2",
    "title": "Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality",
    "authors": [
      "Niki Triantafyllou",
      "Maria M. Papathanasiou"
    ],
    "abstract": "This work introduces a framework to address the computational complexity\ninherent in Mixed-Integer Programming (MIP) models by harnessing the potential\nof deep learning. By employing deep learning, we construct problem-specific\nheuristics that identify and exploit common structures across MIP instances. We\ntrain deep learning models to estimate complicating binary variables for target\nMIP problem instances. The resulting reduced MIP models are solved using\nstandard off-the-shelf solvers. We present an algorithm for generating\nsynthetic data enhancing the robustness and generalizability of our models\nacross diverse MIP instances. We compare the effectiveness of (a) feed-forward\nneural networks (ANN) and (b) convolutional neural networks (CNN). To enhance\nthe framework's performance, we employ Bayesian optimization for hyperparameter\ntuning, aiming to maximize the occurrence of global optimum solutions. We apply\nthis framework to a flow-based facility location allocation MIP formulation\nthat describes long-term investment planning and medium-term tactical\nscheduling in a personalized medicine supply chain.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09556v2",
    "published_date": "2024-01-17 19:15:13 UTC",
    "updated_date": "2024-05-10 17:42:18 UTC"
  },
  {
    "arxiv_id": "2401.09555v1",
    "title": "Improving Classification Performance With Human Feedback: Label a few, we label the rest",
    "authors": [
      "Natan Vidra",
      "Thomas Clifford",
      "Katherine Jijo",
      "Eden Chung",
      "Liang Zhang"
    ],
    "abstract": "In the realm of artificial intelligence, where a vast majority of data is\nunstructured, obtaining substantial amounts of labeled data to train supervised\nmachine learning models poses a significant challenge. To address this, we\ndelve into few-shot and active learning, where are goal is to improve AI models\nwith human feedback on a few labeled examples. This paper focuses on\nunderstanding how a continuous feedback loop can refine models, thereby\nenhancing their accuracy, recall, and precision through incremental human\ninput. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and\nSetFit, we aim to analyze the efficacy of using a limited number of labeled\nexamples to substantially improve model accuracy. We benchmark this approach on\nthe Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to\nprove that with just a few labeled examples, we are able to surpass the\naccuracy of zero shot large language models to provide enhanced text\nclassification performance. We demonstrate that rather than needing to manually\nlabel millions of rows of data, we just need to label a few and the model can\neffectively predict the rest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09555v1",
    "published_date": "2024-01-17 19:13:05 UTC",
    "updated_date": "2024-01-17 19:13:05 UTC"
  },
  {
    "arxiv_id": "2401.09553v1",
    "title": "BERTologyNavigator: Advanced Question Answering with BERT-based Semantics",
    "authors": [
      "Shreya Rajpal",
      "Ricardo Usbeck"
    ],
    "abstract": "The development and integration of knowledge graphs and language models has\nsignificance in artificial intelligence and natural language processing. In\nthis study, we introduce the BERTologyNavigator -- a two-phased system that\ncombines relation extraction techniques and BERT embeddings to navigate the\nrelationships within the DBLP Knowledge Graph (KG). Our approach focuses on\nextracting one-hop relations and labelled candidate pairs in the first phases.\nThis is followed by employing BERT's CLS embeddings and additional heuristics\nfor relation selection in the second phase. Our system reaches an F1 score of\n0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score\non the subset of the DBLP QuAD test dataset during the QA phase.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.4; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in Scholarly QALD Challenge @ ISWC 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.09553v1",
    "published_date": "2024-01-17 19:11:30 UTC",
    "updated_date": "2024-01-17 19:11:30 UTC"
  },
  {
    "arxiv_id": "2401.09414v1",
    "title": "Vlogger: Make Your Dream A Vlog",
    "authors": [
      "Shaobin Zhuang",
      "Kunchang Li",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Ziwei Liu",
      "Yu Qiao",
      "Yali Wang"
    ],
    "abstract": "In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 8 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.09414v1",
    "published_date": "2024-01-17 18:55:12 UTC",
    "updated_date": "2024-01-17 18:55:12 UTC"
  },
  {
    "arxiv_id": "2401.09410v4",
    "title": "Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems",
    "authors": [
      "Karina Cortiñas-Lorenzo",
      "Siân Lindley",
      "Ida Larsen-Ledet",
      "Bhaskar Mitra"
    ],
    "abstract": "Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\npredictive algorithms that learn from data are used to link knowledge and\npeople, inaccuracies in knowledge extraction and surfacing can lead to\ndisproportionate harms, influencing how individuals see each other and how they\nsee themselves at work. In this paper, we present a reflective analysis of\ntransparency requirements and impacts in this type of systems. We conduct a\nmultidisciplinary literature review to understand the impacts of transparency\nin workplace settings, introducing the looking-glass metaphor to conceptualize\nAI knowledge systems as systems that reflect and distort, expanding our view on\ntransparency requirements, implications and challenges. We formulate\ntransparency as a key mediator in shaping different ways of seeing, including\nseeing into the system, which unveils its capabilities, limitations and\nbehavior, and seeing through the system, which shapes workers' perceptions of\ntheir own contributions and others within the organization. Recognizing the\nsociotechnical nature of these systems, we identify three transparency\ndimensions necessary to realize the value of AI knowledge systems, namely\nsystem transparency, procedural transparency and transparency of outcomes. We\ndiscuss key challenges hindering the implementation of these forms of\ntransparency, bringing to light the wider sociotechnical gap and highlighting\ndirections for future Computer-supported Cooperative Work (CSCW) research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09410v4",
    "published_date": "2024-01-17 18:47:30 UTC",
    "updated_date": "2025-03-01 00:55:12 UTC"
  },
  {
    "arxiv_id": "2401.10942v1",
    "title": "Machine Unlearning for Recommendation Systems: An Insight",
    "authors": [
      "Bhavika Sachdeva",
      "Harshita Rathee",
      "Sristi",
      "Arun Sharma",
      "Witold Wydmański"
    ],
    "abstract": "This review explores machine unlearning (MUL) in recommendation systems,\naddressing adaptability, personalization, privacy, and bias challenges. Unlike\ntraditional models, MUL dynamically adjusts system knowledge based on shifts in\nuser preferences and ethical considerations. The paper critically examines\nMUL's basics, real-world applications, and challenges like algorithmic\ntransparency. It sifts through literature, offering insights into how MUL could\ntransform recommendations, discussing user trust, and suggesting paths for\nfuture research in responsible and user-focused artificial intelligence (AI).\nThe document guides researchers through challenges involving the trade-off\nbetween personalization and privacy, encouraging contributions to meet\npractical demands for targeted data removal. Emphasizing MUL's role in secure\nand adaptive machine learning, the paper proposes ways to push its boundaries.\nThe novelty of this paper lies in its exploration of the limitations of the\nmethods, which highlights exciting prospects for advancing the field.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "In Proceedings of 7th INTERNATIONAL CONFERENCE ON INNOVATIVE\n  COMPUTING AND COMMUNICATION 2024 (https://icicc-conf.com/)",
    "pdf_url": "http://arxiv.org/pdf/2401.10942v1",
    "published_date": "2024-01-17 18:35:44 UTC",
    "updated_date": "2024-01-17 18:35:44 UTC"
  },
  {
    "arxiv_id": "2401.09352v1",
    "title": "Neural Contractive Dynamical Systems",
    "authors": [
      "Hadi Beik-Mohammadi",
      "Søren Hauberg",
      "Georgios Arvanitidis",
      "Nadia Figueroa",
      "Gerhard Neumann",
      "Leonel Rozo"
    ],
    "abstract": "Stability guarantees are crucial when ensuring a fully autonomous robot does\nnot take undesirable or potentially harmful actions. Unfortunately, global\nstability guarantees are hard to provide in dynamical systems learned from\ndata, especially when the learned dynamics are governed by neural networks. We\npropose a novel methodology to learn neural contractive dynamical systems,\nwhere our neural architecture ensures contraction, and hence, global stability.\nTo efficiently scale the method to high-dimensional dynamical systems, we\ndevelop a variant of the variational autoencoder that learns dynamics in a\nlow-dimensional latent representation space while retaining contractive\nstability after decoding. We further extend our approach to learning\ncontractive systems on the Lie group of rotations to account for full-pose\nend-effector dynamic motions. The result is the first highly flexible learning\narchitecture that provides contractive stability guarantees with capability to\nperform obstacle avoidance. Empirically, we demonstrate that our approach\nencodes the desired dynamics more accurately than the current state-of-the-art,\nwhich provides less strong stability guarantees.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09352v1",
    "published_date": "2024-01-17 17:18:21 UTC",
    "updated_date": "2024-01-17 17:18:21 UTC"
  },
  {
    "arxiv_id": "2401.09340v3",
    "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
    "authors": [
      "Baoxiong Jia",
      "Yixin Chen",
      "Huangyue Yu",
      "Yan Wang",
      "Xuesong Niu",
      "Tengyu Liu",
      "Qing Li",
      "Siyuan Huang"
    ],
    "abstract": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09340v3",
    "published_date": "2024-01-17 17:04:35 UTC",
    "updated_date": "2024-09-24 03:18:24 UTC"
  },
  {
    "arxiv_id": "2401.09334v1",
    "title": "Large Language Models Are Neurosymbolic Reasoners",
    "authors": [
      "Meng Fang",
      "Shilong Deng",
      "Yudi Zhang",
      "Zijing Shi",
      "Ling Chen",
      "Mykola Pechenizkiy",
      "Jun Wang"
    ],
    "abstract": "A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09334v1",
    "published_date": "2024-01-17 16:57:19 UTC",
    "updated_date": "2024-01-17 16:57:19 UTC"
  },
  {
    "arxiv_id": "2401.09322v1",
    "title": "FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments",
    "authors": [
      "Suchetan Saravanan",
      "Corentin Chauffaut",
      "Caroline Chanel",
      "Damien Vivet"
    ],
    "abstract": "Active visual SLAM finds a wide array of applications in GNSS-Denied\nsub-terrain environments and outdoor environments for ground robots. To achieve\nrobust localization and mapping accuracy, it is imperative to incorporate the\nperception considerations in the goal selection and path planning towards the\ngoal during an exploration mission. Through this work, we propose FIT-SLAM\n(Fisher Information and Traversability estimation-based Active SLAM), a new\nexploration method tailored for unmanned ground vehicles (UGVs) to explore 3D\nenvironments. This approach is devised with the dual objectives of sustaining\nan efficient exploration rate while optimizing SLAM accuracy. Initially, an\nestimation of a global traversability map is conducted, which accounts for the\nenvironmental constraints pertaining to traversability. Subsequently, we\npropose a goal candidate selection approach along with a path planning method\ntowards this goal that takes into account the information provided by the\nlandmarks used by the SLAM backend to achieve robust localization and\nsuccessful path execution . The entire algorithm is tested and evaluated first\nin a simulated 3D world, followed by a real-world environment and is compared\nto pre-existing exploration methods. The results obtained during this\nevaluation demonstrate a significant increase in the exploration rate while\neffectively minimizing the localization covariance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 6 figures, IEEE ICARA 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09322v1",
    "published_date": "2024-01-17 16:46:38 UTC",
    "updated_date": "2024-01-17 16:46:38 UTC"
  },
  {
    "arxiv_id": "2401.09516v2",
    "title": "Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling",
    "authors": [
      "Hong Wang",
      "Zhongkai Hao",
      "Jie Wang",
      "Zijie Geng",
      "Zhen Wang",
      "Bin Li",
      "Feng Wu"
    ],
    "abstract": "Learning neural operators for solving partial differential equations (PDEs)\nhas attracted great attention due to its high inference efficiency. However,\ntraining such operators requires generating a substantial amount of labeled\ndata, i.e., PDE problems together with their solutions. The data generation\nprocess is exceptionally time-consuming, as it involves solving numerous\nsystems of linear equations to obtain numerical solutions to the PDEs. Many\nexisting methods solve these systems independently without considering their\ninherent similarities, resulting in extremely redundant computations. To tackle\nthis problem, we propose a novel method, namely Sorting Krylov Recycling (SKR),\nto boost the efficiency of solving these systems, thus significantly\naccelerating data generation for neural operators training. To the best of our\nknowledge, SKR is the first attempt to address the time-consuming nature of\ndata generation for learning neural operators. The working horse of SKR is\nKrylov subspace recycling, a powerful technique for solving a series of\ninterrelated systems by leveraging their inherent similarities. Specifically,\nSKR employs a sorting algorithm to arrange these systems in a sequence, where\nadjacent systems exhibit high similarities. Then it equips a solver with Krylov\nsubspace recycling to solve the systems sequentially instead of independently,\nthus effectively enhancing the solving efficiency. Both theoretical analysis\nand extensive experiments demonstrate that SKR can significantly accelerate\nneural operator data generation, achieving a remarkable speedup of up to 13.9\ntimes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09516v2",
    "published_date": "2024-01-17 16:20:12 UTC",
    "updated_date": "2024-03-19 08:46:35 UTC"
  },
  {
    "arxiv_id": "2401.09294v1",
    "title": "T-FOLEY: A Controllable Waveform-Domain Diffusion Model for Temporal-Event-Guided Foley Sound Synthesis",
    "authors": [
      "Yoonjin Chung",
      "Junwon Lee",
      "Juhan Nam"
    ],
    "abstract": "Foley sound, audio content inserted synchronously with videos, plays a\ncritical role in the user experience of multimedia content. Recently, there has\nbeen active research in Foley sound synthesis, leveraging the advancements in\ndeep generative models. However, such works mainly focus on replicating a\nsingle sound class or a textual sound description, neglecting temporal\ninformation, which is crucial in the practical applications of Foley sound. We\npresent T-Foley, a Temporal-event-guided waveform generation model for Foley\nsound synthesis. T-Foley generates high-quality audio using two conditions: the\nsound class and temporal event feature. For temporal conditioning, we devise a\ntemporal event feature and a novel conditioning technique named Block-FiLM.\nT-Foley achieves superior performance in both objective and subjective\nevaluation metrics and generates Foley sound well-synchronized with the\ntemporal events. Additionally, we showcase T-Foley's practical applications,\nparticularly in scenarios involving vocal mimicry for temporal event control.\nWe show the demo on our companion website.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09294v1",
    "published_date": "2024-01-17 15:54:36 UTC",
    "updated_date": "2024-01-17 15:54:36 UTC"
  },
  {
    "arxiv_id": "2401.09286v2",
    "title": "Deployable Reinforcement Learning with Variable Control Rate",
    "authors": [
      "Dong Wang",
      "Giovanni Beltrame"
    ],
    "abstract": "Deploying controllers trained with Reinforcement Learning (RL) on real robots\ncan be challenging: RL relies on agents' policies being modeled as Markov\nDecision Processes (MDPs), which assume an inherently discrete passage of time.\nThe use of MDPs results in that nearly all RL-based control systems employ a\nfixed-rate control strategy with a period (or time step) typically chosen based\non the developer's experience or specific characteristics of the application\nenvironment. Unfortunately, the system should be controlled at the highest,\nworst-case frequency to ensure stability, which can demand significant\ncomputational and energy resources and hinder the deployability of the\ncontroller on onboard hardware. Adhering to the principles of reactive\nprogramming, we surmise that applying control actions only when necessary\nenables the use of simpler hardware and helps reduce energy consumption. We\nchallenge the fixed frequency assumption by proposing a variant of RL with\nvariable control rate. In this approach, the policy decides the action the\nagent should take as well as the duration of the time step associated with that\naction. In our new setting, we expand Soft Actor-Critic (SAC) to compute the\noptimal policy with a variable control rate, introducing the Soft Elastic\nActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a\nproof-of-concept simulation driving an agent with Newtonian kinematics. Our\nexperiments show higher average returns, shorter task completion times, and\nreduced computational resources when compared to fixed rate policies.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Paper for AAAI-DAI 2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2401.09286v2",
    "published_date": "2024-01-17 15:40:11 UTC",
    "updated_date": "2024-04-02 17:18:19 UTC"
  },
  {
    "arxiv_id": "2401.09252v1",
    "title": "3D Scene Geometry Estimation from 360$^\\circ$ Imagery: A Survey",
    "authors": [
      "Thiago Lopes Trugillo da Silveira",
      "Paulo Gamarra Lessa Pinto",
      "Jeffri Erwin Murrugarra Llerena",
      "Claudio Rosito Jung"
    ],
    "abstract": "This paper provides a comprehensive survey on pioneer and state-of-the-art 3D\nscene geometry estimation methodologies based on single, two, or multiple\nimages captured under the omnidirectional optics. We first revisit the basic\nconcepts of the spherical camera model, and review the most common acquisition\ntechnologies and representation formats suitable for omnidirectional (also\ncalled 360$^\\circ$, spherical or panoramic) images and videos. We then survey\nmonocular layout and depth inference approaches, highlighting the recent\nadvances in learning-based solutions suited for spherical data. The classical\nstereo matching is then revised on the spherical domain, where methodologies\nfor detecting and describing sparse and dense features become crucial. The\nstereo matching concepts are then extrapolated for multiple view camera setups,\ncategorizing them among light fields, multi-view stereo, and structure from\nmotion (or visual simultaneous localization and mapping). We also compile and\ndiscuss commonly adopted datasets and figures of merit indicated for each\npurpose and list recent results for completeness. We conclude this paper by\npointing out current and future trends.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2401.09252v1",
    "published_date": "2024-01-17 14:57:27 UTC",
    "updated_date": "2024-01-17 14:57:27 UTC"
  },
  {
    "arxiv_id": "2401.09243v3",
    "title": "DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning",
    "authors": [
      "Sabariswaran Mani",
      "Sreyas Venkataraman",
      "Abhranil Chandra",
      "Adyan Rizvi",
      "Yash Sirvi",
      "Soumojit Bhattacharya",
      "Aritra Hazra"
    ],
    "abstract": "Robot learning tasks are extremely compute-intensive and hardware-specific.\nThus the avenues of tackling these challenges, using a diverse dataset of\noffline demonstrations that can be used to train robot manipulation agents, is\nvery appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a\nwell-curated open-source dataset for offline training comprised mostly of\nexpert data and also benchmark scores of the common offline-RL and behaviour\ncloning agents. In this paper, we introduce DiffClone, an offline algorithm of\nenhanced behaviour cloning agent with diffusion-based policy learning, and\nmeasured the efficacy of our method on real online physical robots at test\ntime. This is also our official submission to the Train-Offline-Test-Online\n(TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both\npre-trained visual representation and agent policies. In our experiments, we\nfind that MOCO finetuned ResNet50 performs the best in comparison to other\nfinetuned representations. Goal state conditioning and mapping to transitions\nresulted in a minute increase in the success rate and mean-reward. As for the\nagent policy, we developed DiffClone, a behaviour cloning agent improved using\nconditional diffusion.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "NeurIPS 2023 Train Offline Test Online Workshop and Competition (Best\n  Paper Oral Presentation / Winning Competition Submission)",
    "pdf_url": "http://arxiv.org/pdf/2401.09243v3",
    "published_date": "2024-01-17 14:43:59 UTC",
    "updated_date": "2024-05-23 21:51:24 UTC"
  },
  {
    "arxiv_id": "2401.09240v1",
    "title": "A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous Information System",
    "authors": [
      "MN Ramahlosi",
      "Y Madani",
      "A Akanbi"
    ],
    "abstract": "In our digital world, access to personal and public data has become an item\nof concern, with challenging security and privacy aspects. Modern information\nsystems are heterogeneous in nature and have an inherent security\nvulnerability, which is susceptible to data interception and data modification\ndue to unsecured communication data pipelines between connected endpoints. This\nre-search article presents a blockchain-based model for securing data pipelines\nin a heterogeneous information system using an integrated multi-hazard early\nwarning system (MHEWS) as a case study. The proposed model utilizes the\ninherent security features of blockchain technology to address the security and\nprivacy concerns that arise in data pipelines. The model is designed to ensure\ndata integrity, confidentiality, and authenticity in a decentralized manner.\nThe model is evaluated in a hybrid environment using a prototype implementation\nand simulation experiments with outcomes that demonstrate advantages over\ntraditional approaches for a tamper-proof and immutable data pipeline for data\nauthenticity and integrity using a confidential ledger.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.09240v1",
    "published_date": "2024-01-17 14:40:09 UTC",
    "updated_date": "2024-01-17 14:40:09 UTC"
  },
  {
    "arxiv_id": "2401.09239v1",
    "title": "DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery",
    "authors": [
      "Mikel De Iturrate Reyzabal",
      "Mingcong Chen",
      "Wei Huang",
      "Sebastien Ourselin",
      "Hongbin Liu"
    ],
    "abstract": "Precisely determining the contact force during safe interaction in Minimally\nInvasive Robotic Surgery (MIRS) is still an open research challenge. Inspired\nby post-operative qualitative analysis from surgical videos, the use of\ncross-modality data driven deep neural network models has been one of the\nnewest approaches to predict sensorless force trends. However, these methods\nrequired for large and variable datasets which are not currently available. In\nthis paper, we present a new vision-haptic dataset (DaFoEs) with variable soft\nenvironments for the training of deep neural models. In order to reduce the\nbias from a single dataset, we present a pipeline to generalize different\nvision and state data inputs for mixed dataset training, using a previously\nvalidated dataset with different setup. Finally, we present a variable\nencoder-decoder architecture to predict the forces done by the laparoscopic\ntool using single input or sequence of inputs. For input sequence, we use a\nrecurrent decoder, named with the prefix R, and a new temporal sampling to\nrepresent the acceleration of the tool. During our training, we demonstrate\nthat single dataset training tends to overfit to the training data domain, but\nhas difficulties on translating the results across new domains. However,\ndataset mixing presents a good translation with a mean relative estimated force\nerror of 5% and 12% for the recurrent and non-recurrent models respectively.\nOur method, also marginally increase the effectiveness of transformers for\nforce estimation up to a maximum of ~15%, as the volume of available data is\nincrease by 150%. In conclusion, we demonstrate that mixing experimental set\nups for vision-state force estimation in MIRS is a possible approach towards\nthe general solution of the problem.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09239v1",
    "published_date": "2024-01-17 14:39:55 UTC",
    "updated_date": "2024-01-17 14:39:55 UTC"
  },
  {
    "arxiv_id": "2401.09235v1",
    "title": "A Characterization Theorem for Equivariant Networks with Point-wise Activations",
    "authors": [
      "Marco Pacini",
      "Xiaowen Dong",
      "Bruno Lepri",
      "Gabriele Santin"
    ],
    "abstract": "Equivariant neural networks have shown improved performance, expressiveness\nand sample complexity on symmetrical domains. But for some specific symmetries,\nrepresentations, and choice of coordinates, the most common point-wise\nactivations, such as ReLU, are not equivariant, hence they cannot be employed\nin the design of equivariant neural networks. The theorem we present in this\npaper describes all possible combinations of finite-dimensional\nrepresentations, choice of coordinates and point-wise activations to obtain an\nexactly equivariant layer, generalizing and strengthening existing\ncharacterizations. Notable cases of practical relevance are discussed as\ncorollaries. Indeed, we prove that rotation-equivariant networks can only be\ninvariant, as it happens for any network which is equivariant with respect to\nconnected compact groups. Then, we discuss implications of our findings when\napplied to important instances of exactly equivariant networks. First, we\ncompletely characterize permutation equivariant networks such as Invariant\nGraph Networks with point-wise nonlinearities and their geometric counterparts,\nhighlighting a plethora of models whose expressive power and performance are\nstill unknown. Second, we show that feature spaces of disentangled steerable\nconvolutional neural networks are trivial representations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 12th International Conference on Learning\n  Representations (ICLR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.09235v1",
    "published_date": "2024-01-17 14:30:46 UTC",
    "updated_date": "2024-01-17 14:30:46 UTC"
  },
  {
    "arxiv_id": "2401.09192v3",
    "title": "Preparing Lessons for Progressive Training on Language Models",
    "authors": [
      "Yu Pan",
      "Ye Yuan",
      "Yichun Yin",
      "Jiaxin Shi",
      "Zenglin Xu",
      "Ming Zhang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ],
    "abstract": "The rapid progress of Transformers in artificial intelligence has come at the\ncost of increased resource consumption and greenhouse gas emissions due to\ngrowing model sizes. Prior work suggests using pretrained small models to\nimprove training efficiency, but this approach may not be suitable for new\nmodel structures. On the other hand, training from scratch can be slow, and\nprogressively stacking layers often fails to achieve significant acceleration.\nTo address these challenges, we propose a novel method called Apollo, which\nprep\\textbf{a}res lessons for ex\\textbf{p}anding \\textbf{o}perations by\n\\textbf{l}earning high-\\textbf{l}ayer functi\\textbf{o}nality during training of\nlow layers. Our approach involves low-value-prioritized sampling (LVPS) to\ntrain different depths and weight sharing to facilitate efficient expansion. We\nalso introduce an interpolation method for stable model depth extension.\nExperiments demonstrate that Apollo achieves state-of-the-art acceleration\nratios, even rivaling methods using pretrained models, making it a universal\nand efficient solution for training deep models while reducing time, financial,\nand environmental costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09192v3",
    "published_date": "2024-01-17 13:04:14 UTC",
    "updated_date": "2024-02-10 14:52:49 UTC"
  },
  {
    "arxiv_id": "2401.10938v2",
    "title": "Even-if Explanations: Formal Foundations, Priorities and Complexity",
    "authors": [
      "Gianvincenzo Alfano",
      "Sergio Greco",
      "Domenico Mandaglio",
      "Francesco Parisi",
      "Reza Shahbazian",
      "Irina Trubitsyna"
    ],
    "abstract": "EXplainable AI has received significant attention in recent years. Machine\nlearning models often operate as black boxes, lacking explainability and\ntransparency while supporting decision-making processes. Local post-hoc\nexplainability queries attempt to answer why individual inputs are classified\nin a certain way by a given model. While there has been important work on\ncounterfactual explanations, less attention has been devoted to semifactual\nones. In this paper, we focus on local post-hoc explainability queries within\nthe semifactual `even-if' thinking and their computational complexity among\ndifferent classes of models, and show that both linear and tree-based models\nare strictly more interpretable than neural networks. After this, we introduce\na preference-based framework that enables users to personalize explanations\nbased on their preferences, both in the case of semifactuals and\ncounterfactuals, enhancing interpretability and user-centricity. Finally, we\nexplore the complexity of several interpretability problems in the proposed\npreference-based framework and provide algorithms for polynomial cases.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2010.12265 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2401.10938v2",
    "published_date": "2024-01-17 11:38:58 UTC",
    "updated_date": "2024-05-22 11:17:10 UTC"
  },
  {
    "arxiv_id": "2401.10937v1",
    "title": "Subjective Causality",
    "authors": [
      "Joseph Y. Halpern",
      "Evan Piermont"
    ],
    "abstract": "We show that it is possible to understand and identify a decision maker's\nsubjective causal judgements by observing her preferences over interventions.\nFollowing Pearl [2000], we represent causality using causal models (also called\nstructural equations models), where the world is described by a collection of\nvariables, related by equations. We show that if a preference relation over\ninterventions satisfies certain axioms (related to standard axioms regarding\ncounterfactuals), then we can define (i) a causal model, (ii) a probability\ncapturing the decision-maker's uncertainty regarding the external factors in\nthe world and (iii) a utility on outcomes such that each intervention is\nassociated with an expected utility and such that intervention $A$ is preferred\nto $B$ iff the expected utility of $A$ is greater than that of $B$. In\naddition, we characterize when the causal model is unique. Thus, our results\nallow a modeler to test the hypothesis that a decision maker's preferences are\nconsistent with some causal model and to identify causal judgements from\nobserved behavior.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10937v1",
    "published_date": "2024-01-17 11:36:38 UTC",
    "updated_date": "2024-01-17 11:36:38 UTC"
  },
  {
    "arxiv_id": "2401.09082v2",
    "title": "Should agentic conversational AI change how we think about ethics? Characterising an interactional ethics centred on respect",
    "authors": [
      "Lize Alberts",
      "Geoff Keeling",
      "Amanda McCroskery"
    ],
    "abstract": "With the growing popularity of conversational agents based on large language\nmodels (LLMs), we need to ensure their behaviour is ethical and appropriate.\nWork in this area largely centres around the 'HHH' criteria: making outputs\nmore helpful and honest, and avoiding harmful (biased, toxic, or inaccurate)\nstatements. Whilst this semantic focus is useful when viewing LLM agents as\nmere mediums or output-generating systems, it fails to account for pragmatic\nfactors that can make the same speech act seem more or less tactless or\ninconsiderate in different social situations. With the push towards agentic AI,\nwherein systems become increasingly proactive in chasing goals and performing\nactions in the world, considering the pragmatics of interaction becomes\nessential. We propose an interactional approach to ethics that is centred on\nrelational and situational factors. We explore what it means for a system, as a\nsocial actor, to treat an individual respectfully in a (series of)\ninteraction(s). Our work anticipates a set of largely unexplored risks at the\nlevel of situated social interaction, and offers practical suggestions to help\nagentic LLM technologies treat people well.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "68T42",
      "H.5.2; I.2; I.2.1; J.4; J.5"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09082v2",
    "published_date": "2024-01-17 09:44:03 UTC",
    "updated_date": "2024-05-16 09:53:45 UTC"
  },
  {
    "arxiv_id": "2401.09075v1",
    "title": "GPT in Sheep's Clothing: The Risk of Customized GPTs",
    "authors": [
      "Sagiv Antebi",
      "Noam Azulay",
      "Edan Habler",
      "Ben Ganon",
      "Asaf Shabtai",
      "Yuval Elovici"
    ],
    "abstract": "In November 2023, OpenAI introduced a new service allowing users to create\ncustom versions of ChatGPT (GPTs) by using specific instructions and knowledge\nto guide the model's behavior. We aim to raise awareness of the fact that GPTs\ncan be used maliciously, posing privacy and security risks to their users.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09075v1",
    "published_date": "2024-01-17 09:27:13 UTC",
    "updated_date": "2024-01-17 09:27:13 UTC"
  },
  {
    "arxiv_id": "2401.09074v4",
    "title": "Code Simulation Challenges for Large Language Models",
    "authors": [
      "Emanuele La Malfa",
      "Christoph Weinhuber",
      "Orazio Torre",
      "Fangru Lin",
      "Samuele Marro",
      "Anthony Cohn",
      "Nigel Shadbolt",
      "Michael Wooldridge"
    ],
    "abstract": "Many reasoning, planning, and problem-solving tasks share an intrinsic\nalgorithmic nature: correctly simulating each step is a sufficient condition to\nsolve them correctly. This work studies to what extent Large Language Models\n(LLMs) can simulate coding and algorithmic tasks to provide insights into\ngeneral capabilities in such algorithmic reasoning tasks. We introduce\nbenchmarks for straight-line programs, code that contains critical paths, and\napproximate and redundant instructions. We further assess the simulation\ncapabilities of LLMs with sorting algorithms and nested loops and show that a\nroutine's computational complexity directly affects an LLM's ability to\nsimulate its execution. While the most powerful LLMs exhibit relatively strong\nsimulation capabilities, the process is fragile, seems to rely heavily on\npattern recognition, and is affected by memorisation. We propose a novel\noff-the-shelf prompting method, Chain of Simulation (CoSm), which instructs\nLLMs to simulate code execution line by line/follow the computation pattern of\ncompilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern\nrecognition while improving simulation performance. We consider the success of\nCoSm in code simulation to be inspirational for other general routine\nsimulation reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/EmanueleLM/CodeSimulation",
    "pdf_url": "http://arxiv.org/pdf/2401.09074v4",
    "published_date": "2024-01-17 09:23:59 UTC",
    "updated_date": "2024-06-12 08:55:13 UTC"
  },
  {
    "arxiv_id": "2401.09073v1",
    "title": "Fixed-Budget Differentially Private Best Arm Identification",
    "authors": [
      "Zhirui Chen",
      "P. N. Karthik",
      "Yeow Meng Chee",
      "Vincent Y. F. Tan"
    ],
    "abstract": "We study best arm identification (BAI) in linear bandits in the fixed-budget\nregime under differential privacy constraints, when the arm rewards are\nsupported on the unit interval. Given a finite budget $T$ and a privacy\nparameter $\\varepsilon>0$, the goal is to minimise the error probability in\nfinding the arm with the largest mean after $T$ sampling rounds, subject to the\nconstraint that the policy of the decision maker satisfies a certain {\\em\n$\\varepsilon$-differential privacy} ($\\varepsilon$-DP) constraint. We construct\na policy satisfying the $\\varepsilon$-DP constraint (called {\\sc DP-BAI}) by\nproposing the principle of {\\em maximum absolute determinants}, and derive an\nupper bound on its error probability. Furthermore, we derive a minimax lower\nbound on the error probability, and demonstrate that the lower and the upper\nbounds decay exponentially in $T$, with exponents in the two bounds matching\norder-wise in (a) the sub-optimality gaps of the arms, (b) $\\varepsilon$, and\n(c) the problem complexity that is expressible as the sum of two terms, one\ncharacterising the complexity of standard fixed-budget BAI (without privacy\nconstraints), and the other accounting for the $\\varepsilon$-DP constraint.\nAdditionally, we present some auxiliary results that contribute to the\nderivation of the lower bound on the error probability. These results, we\nposit, may be of independent interest and could prove instrumental in proving\nlower bounds on error probabilities in several other bandit problems. Whereas\nprior works provide results for BAI in the fixed-budget regime without privacy\nconstraints or in the fixed-confidence regime with privacy constraints, our\nwork fills the gap in the literature by providing the results for BAI in the\nfixed-budget regime under the $\\varepsilon$-DP constraint.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09073v1",
    "published_date": "2024-01-17 09:23:25 UTC",
    "updated_date": "2024-01-17 09:23:25 UTC"
  },
  {
    "arxiv_id": "2401.09071v5",
    "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering",
    "authors": [
      "Jingwei Guo",
      "Kaizhu Huang",
      "Xinping Yi",
      "Zixian Su",
      "Rui Zhang"
    ],
    "abstract": "Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe establish a theoretical connection between spectral filtering and spatial\naggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency among nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our proposed SAF comprehensively models both node similarity and\ndissimilarity from a global perspective, therefore alleviating persistent\ndeficiencies of GNNs related to long-range dependencies and graph heterophily.\nExtensive experiments over 13 node classification benchmarks demonstrate the\nsuperiority of our proposed framework to the state-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09071v5",
    "published_date": "2024-01-17 09:12:31 UTC",
    "updated_date": "2024-09-16 09:09:34 UTC"
  },
  {
    "arxiv_id": "2401.09070v1",
    "title": "Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for Generalized Knowledge Augmentation and Inference",
    "authors": [
      "Qinghua Huang",
      "Yongzhen Wang"
    ],
    "abstract": "Knowledge graph (KG) based reasoning has been regarded as an effective means\nfor the analysis of semantic networks and is of great usefulness in areas of\ninformation retrieval, recommendation, decision-making, and man-machine\ninteraction. It is widely used in recommendation, decision-making,\nquestion-answering, search, and other fields. However, previous studies mainly\nused low-level knowledge in the KG for reasoning, which may result in\ninsufficient generalization and poor robustness of reasoning. To this end, this\npaper proposes a new inference approach using a novel knowledge augmentation\nstrategy to improve the generalization capability of KG. This framework\nextracts high-level pyramidal knowledge from low-level knowledge and applies it\nto reasoning in a multi-level hierarchical KG, called knowledge pyramid in this\npaper. We tested some medical data sets using the proposed approach, and the\nexperimental results show that the proposed knowledge pyramid has improved the\nknowledge inference performance with better generalization. Especially, when\nthere are fewer training samples, the inference accuracy can be significantly\nimproved.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages,8 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.09070v1",
    "published_date": "2024-01-17 09:08:23 UTC",
    "updated_date": "2024-01-17 09:08:23 UTC"
  },
  {
    "arxiv_id": "2401.09068v1",
    "title": "DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning",
    "authors": [
      "Lixiang Han",
      "Zhen Xiao",
      "Zhenjiang Li"
    ],
    "abstract": "DTMM is a library designed for efficient deployment and execution of machine\nlearning models on weak IoT devices such as microcontroller units (MCUs). The\nmotivation for designing DTMM comes from the emerging field of tiny machine\nlearning (TinyML), which explores extending the reach of machine learning to\nmany low-end IoT devices to achieve ubiquitous intelligence. Due to the weak\ncapability of embedded devices, it is necessary to compress models by pruning\nenough weights before deploying. Although pruning has been studied extensively\non many computing platforms, two key issues with pruning methods are\nexacerbated on MCUs: models need to be deeply compressed without significantly\ncompromising accuracy, and they should perform efficiently after pruning.\nCurrent solutions only achieve one of these objectives, but not both. In this\npaper, we find that pruned models have great potential for efficient deployment\nand execution on MCUs. Therefore, we propose DTMM with pruning unit selection,\npre-execution pruning optimizations, runtime acceleration, and post-execution\nlow-cost storage to fill the gap for efficient deployment and execution of\npruned models. It can be integrated into commercial ML frameworks for practical\ndeployment, and a prototype system has been developed. Extensive experiments on\nvarious models show promising gains compared to state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09068v1",
    "published_date": "2024-01-17 09:01:50 UTC",
    "updated_date": "2024-01-17 09:01:50 UTC"
  },
  {
    "arxiv_id": "2401.09067v1",
    "title": "Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding",
    "authors": [
      "Depeng Li",
      "Tianqi Wang",
      "Junwei Chen",
      "Qining Ren",
      "Kenji Kawaguchi",
      "Zhigang Zeng"
    ],
    "abstract": "Deep neural networks are susceptible to catastrophic forgetting when trained\non sequential tasks. Various continual learning (CL) methods often rely on\nexemplar buffers or/and network expansion for balancing model stability and\nplasticity, which, however, compromises their practical value due to privacy\nand memory concerns. Instead, this paper considers a strict yet realistic\nsetting, where the training data from previous tasks is unavailable and the\nmodel size remains relatively constant during sequential training. To achieve\nsuch desiderata, we propose a conceptually simple yet effective method that\nattributes forgetting to layer-wise parameter overwriting and the resulting\ndecision boundary distortion. This is achieved by the synergy between two key\ncomponents: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten\nparameter updates mediated by Hilbert-Schmidt independence criterion in an\northogonal space and EquiAngular Embedding (EAE) enhances decision boundary\nadaptation between old and new tasks with predefined basis vectors. Extensive\nexperiments demonstrate that our method achieves competitive accuracy\nperformance, even with absolute superiority of zero exemplar buffer and 1.02x\nthe base model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09067v1",
    "published_date": "2024-01-17 09:01:29 UTC",
    "updated_date": "2024-01-17 09:01:29 UTC"
  },
  {
    "arxiv_id": "2401.09042v1",
    "title": "LLMs for Relational Reasoning: How Far are We?",
    "authors": [
      "Zhiming Li",
      "Yushi Cao",
      "Xiufeng Xu",
      "Junzhe Jiang",
      "Xu Liu",
      "Yon Shin Teo",
      "Shang-wei Lin",
      "Yang Liu"
    ],
    "abstract": "Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by The First International Workshop on Large Language Models\n  for Code (ICSE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.09042v1",
    "published_date": "2024-01-17 08:22:52 UTC",
    "updated_date": "2024-01-17 08:22:52 UTC"
  },
  {
    "arxiv_id": "2401.10935v2",
    "title": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents",
    "authors": [
      "Kanzhi Cheng",
      "Qiushi Sun",
      "Yougang Chu",
      "Fangzhi Xu",
      "Yantao Li",
      "Jianbing Zhang",
      "Zhiyong Wu"
    ],
    "abstract": "Graphical User Interface (GUI) agents are designed to automate complex tasks\non digital devices, such as smartphones and desktops. Most existing GUI agents\ninteract with the environment through extracted structured data, which can be\nnotably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).\nTo alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which\nonly relies on screenshots for task automation. In our preliminary study, we\nhave discovered a key challenge in developing visual GUI agents: GUI grounding\n-- the capacity to accurately locate screen elements based on instructions. To\ntackle this challenge, we propose to enhance SeeClick with GUI grounding\npre-training and devise a method to automate the curation of GUI grounding\ndata. Along with the efforts above, we have also created ScreenSpot, the first\nrealistic GUI grounding benchmark that encompasses mobile, desktop, and web\nenvironments. After pre-training, SeeClick demonstrates significant improvement\nin ScreenSpot over various baselines. Moreover, comprehensive evaluations on\nthree widely used benchmarks consistently support our finding that advancements\nin GUI grounding directly correlate with enhanced performance in downstream GUI\nagent tasks. The model, data and code are available at\nhttps://github.com/njucckevin/SeeClick.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10935v2",
    "published_date": "2024-01-17 08:10:35 UTC",
    "updated_date": "2024-02-23 04:36:51 UTC"
  },
  {
    "arxiv_id": "2401.09034v2",
    "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems",
    "authors": [
      "Changshuo Zhang",
      "Sirui Chen",
      "Xiao Zhang",
      "Sunhao Dai",
      "Weijie Yu",
      "Jun Xu"
    ],
    "abstract": "Reinforcement learning (RL) has gained traction for enhancing user long-term\nexperiences in recommender systems by effectively exploring users' interests.\nHowever, modern recommender systems exhibit distinct user behavioral patterns\namong tens of millions of items, which increases the difficulty of exploration.\nFor example, user behaviors with different activity levels require varying\nintensity of exploration, while previous studies often overlook this aspect and\napply a uniform exploration strategy to all users, which ultimately hurts user\nexperiences in the long run. To address these challenges, we propose\nUser-Oriented Exploration Policy (UOEP), a novel approach facilitating\nfine-grained exploration among user groups. We first construct a distributional\ncritic which allows policy optimization under varying quantile levels of\ncumulative reward feedbacks from users, representing user groups with varying\nactivity levels. Guided by this critic, we devise a population of distinct\nactors aimed at effective and fine-grained exploration within its respective\nuser group. To simultaneously enhance diversity and stability during the\nexploration process, we further introduce a population-level diversity\nregularization term and a supervision module. Experimental results on public\nrecommendation datasets demonstrate that our approach outperforms all other\nbaselines in terms of long-term performance, validating its user-oriented\nexploration effectiveness. Meanwhile, further analyses reveal our approach's\nbenefits of improved performance for low-activity users as well as increased\nfairness among users.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09034v2",
    "published_date": "2024-01-17 08:01:18 UTC",
    "updated_date": "2024-05-22 01:01:11 UTC"
  },
  {
    "arxiv_id": "2401.09029v1",
    "title": "Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading",
    "authors": [
      "Dunyuan Xu",
      "Xi Wang",
      "Jinyue Cai",
      "Pheng-Ann Heng"
    ],
    "abstract": "Brain tumor represents one of the most fatal cancers around the world, and is\nvery common in children and the elderly. Accurate identification of the type\nand grade of tumor in the early stages plays an important role in choosing a\nprecise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of\ndifferent sequences provide clinicians with important contradictory information\nto identify tumor regions. However, manual assessment is time-consuming and\nerror-prone due to big amount of data and the diversity of brain tumor types.\nHence, there is an unmet need for MRI automated brain tumor diagnosis. We\nobserve that the predictive capability of uni-modality models is limited and\ntheir performance varies widely across modalities, and the commonly used\nmodality fusion methods would introduce potential noise, which results in\nsignificant performance degradation. To overcome these challenges, we propose a\nnovel cross-modality guidance-aided multi-modal learning with dual attention\nfor addressing the task of MRI brain tumor grading. To balance the tradeoff\nbetween model efficiency and efficacy, we employ ResNet Mix Convolution as the\nbackbone network for feature extraction. Besides, dual attention is applied to\ncapture the semantic interdependencies in spatial and slice dimensions\nrespectively. To facilitate information interaction among modalities, we design\na cross-modality guidance-aided module where the primary modality guides the\nother secondary modalities during the process of training, which can\neffectively leverage the complementary information of different MRI modalities\nand meanwhile alleviate the impact of the possible noise.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09029v1",
    "published_date": "2024-01-17 07:54:49 UTC",
    "updated_date": "2024-01-17 07:54:49 UTC"
  },
  {
    "arxiv_id": "2401.09019v1",
    "title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM)",
    "authors": [
      "Hongruixuan Chen",
      "Jian Song",
      "Naoto Yokoya"
    ],
    "abstract": "Unsupervised multimodal change detection is pivotal for time-sensitive tasks\nand comprehensive multi-temporal Earth monitoring. In this study, we explore\nunsupervised multimodal change detection between two key remote sensing data\nsources: optical high-resolution imagery and OpenStreetMap (OSM) data.\nSpecifically, we propose to utilize the vision foundation model Segmentation\nAnything Model (SAM), for addressing our task. Leveraging SAM's exceptional\nzero-shot transfer capability, high-quality segmentation maps of optical images\ncan be obtained. Thus, we can directly compare these two heterogeneous data\nforms in the so-called segmentation domain. We then introduce two strategies\nfor guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'\nmethods. The two strategies are designed to detect land-cover changes in\ngeneral scenarios and to identify new land-cover objects within existing\nbackgrounds, respectively. Experimental results on three datasets indicate that\nthe proposed approach can achieve more competitive results compared to\nrepresentative unsupervised multimodal change detection methods.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09019v1",
    "published_date": "2024-01-17 07:30:52 UTC",
    "updated_date": "2024-01-17 07:30:52 UTC"
  },
  {
    "arxiv_id": "2401.09011v1",
    "title": "Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations",
    "authors": [
      "Udesh Habaraduwa"
    ],
    "abstract": "This paper discusses the limitations of machine learning (ML), particularly\ndeep artificial neural networks (ANNs), which are effective at approximating\ncomplex functions but often lack transparency and explanatory power. It\nhighlights the `problem of induction' : the philosophical issue that past\nobservations may not necessarily predict future events, a challenge that ML\nmodels face when encountering new, unseen data. The paper argues for the\nimportance of not just making predictions but also providing good explanations,\na feature that current models often fail to deliver. It suggests that for AI to\nprogress, we must seek models that offer insights and explanations, not just\npredictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09011v1",
    "published_date": "2024-01-17 07:14:04 UTC",
    "updated_date": "2024-01-17 07:14:04 UTC"
  },
  {
    "arxiv_id": "2401.09008v1",
    "title": "Hybrid of DiffStride and Spectral Pooling in Convolutional Neural Networks",
    "authors": [
      "Sulthan Rafif",
      "Mochamad Arfan Ravy Wahyu Pratama",
      "Mohammad Faris Azhar",
      "Ahmad Mustafidul Ibad",
      "Lailil Muflikhah",
      "Novanto Yudistira"
    ],
    "abstract": "Stride determines the distance between adjacent filter positions as the\nfilter moves across the input. A fixed stride causes important information\ncontained in the image can not be captured, so that important information is\nnot classified. Therefore, in previous research, the DiffStride Method was\napplied, namely the Strided Convolution Method with which it can learn its own\nstride value. Severe Quantization and a constraining lower bound on preserved\ninformation are arises with Max Pooling Downsampling Method. Spectral Pooling\nreduce the constraint lower bound on preserved information by cutting off the\nrepresentation in the frequency domain. In this research a CNN Model is\nproposed with the Downsampling Learnable Stride Technique performed by\nBackpropagation combined with the Spectral Pooling Technique. Diffstride and\nSpectral Pooling techniques are expected to maintain most of the information\ncontained in the image. In this study, we compare the Hybrid Method, which is a\ncombined implementation of Spectral Pooling and DiffStride against the Baseline\nMethod, which is the DiffStride implementation on ResNet 18. The accuracy\nresult of the DiffStride combination with Spectral Pooling improves over\nDiffStride which is baseline method by 0.0094. This shows that the Hybrid\nMethod can maintain most of the information by cutting of the representation in\nthe frequency domain and determine the stride of the learning result through\nBackpropagation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09008v1",
    "published_date": "2024-01-17 07:06:56 UTC",
    "updated_date": "2024-01-17 07:06:56 UTC"
  },
  {
    "arxiv_id": "2401.09003v5",
    "title": "Augmenting Math Word Problems via Iterative Question Composing",
    "authors": [
      "Haoxiong Liu",
      "Yifan Zhang",
      "Yifan Luo",
      "Andrew Chi-Chih Yao"
    ],
    "abstract": "Despite the advancements in large language models (LLMs) for mathematical\nreasoning, solving competition-level math problems remains a significant\nchallenge, especially for open-source LLMs without external tools. We introduce\nthe MMIQC dataset, comprising a mixture of processed web data and synthetic\nquestion-response pairs, aimed at enhancing the mathematical reasoning\ncapabilities of base language models. Models fine-tuned on MMIQC consistently\nsurpass their counterparts in performance on the MATH benchmark across various\nmodel sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the\nprevious open-source state-of-the-art by 8.2% and outperforming the initial\nversion GPT-4 released in 2023. Extensive evaluation results on Hungarian high\nschool finals suggest that such improvement can generalize to unseen data. Our\nablation study on MMIQC reveals that a large part of the improvement can be\nattributed to our novel augmentation method, Iterative Question Composing\n(IQC), which involves iteratively composing new questions from seed problems\nusing an LLM and applying rejection sampling through another LLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09003v5",
    "published_date": "2024-01-17 06:48:16 UTC",
    "updated_date": "2024-12-16 06:13:03 UTC"
  },
  {
    "arxiv_id": "2401.08999v1",
    "title": "Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent",
    "authors": [
      "Hugo Laurencon",
      "Yesoda Bhargava",
      "Riddhi Zantye",
      "Charbel-Raphaël Ségerie",
      "Johann Lussange",
      "Veeky Baths",
      "Boris Gutkin"
    ],
    "abstract": "Homeostasis is a biological process by which living beings maintain their\ninternal balance. Previous research suggests that homeostasis is a learned\nbehaviour. Recently introduced Homeostatic Regulated Reinforcement Learning\n(HRRL) framework attempts to explain this learned homeostatic behavior by\nlinking Drive Reduction Theory and Reinforcement Learning. This linkage has\nbeen proven in the discrete time-space, but not in the continuous time-space.\nIn this work, we advance the HRRL framework to a continuous time-space\nenvironment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL)\nframework. We achieve this by designing a model that mimics the homeostatic\nmechanisms in a real-world biological agent. This model uses the\nHamilton-Jacobian Bellman Equation, and function approximation based on neural\nnetworks and Reinforcement Learning. Through a simulation-based experiment we\ndemonstrate the efficacy of this model and uncover the evidence linked to the\nagent's ability to dynamically choose policies that favor homeostasis in a\ncontinuously changing internal-state milieu. Results of our experiments\ndemonstrate that agent learns homeostatic behaviour in a CTCS environment,\nmaking CTCS-HRRL a promising framework for modellng animal dynamics and\ndecision-making.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This work is a result of the ongoing collaboration between Cognitive\n  Neuroscience Lab, BITS Pilani K K Birla Goa Campus and Ecole Normale\n  Superieure, Paris France. This work is jointly supervised by Prof. Boris\n  Gutkin and Prof. Veeky Baths. arXiv admin note: substantial text overlap with\n  arXiv:2109.06580",
    "pdf_url": "http://arxiv.org/pdf/2401.08999v1",
    "published_date": "2024-01-17 06:29:34 UTC",
    "updated_date": "2024-01-17 06:29:34 UTC"
  },
  {
    "arxiv_id": "2401.08996v1",
    "title": "MicroNAS: Zero-Shot Neural Architecture Search for MCUs",
    "authors": [
      "Ye Qiao",
      "Haocheng Xu",
      "Yifan Zhang",
      "Sitao Huang"
    ],
    "abstract": "Neural Architecture Search (NAS) effectively discovers new Convolutional\nNeural Network (CNN) architectures, particularly for accuracy optimization.\nHowever, prior approaches often require resource-intensive training on super\nnetworks or extensive architecture evaluations, limiting practical\napplications. To address these challenges, we propose MicroNAS, a\nhardware-aware zero-shot NAS framework designed for microcontroller units\n(MCUs) in edge computing. MicroNAS considers target hardware optimality during\nthe search, utilizing specialized performance indicators to identify optimal\nneural architectures without high computational costs. Compared to previous\nworks, MicroNAS achieves up to 1104x improvement in search efficiency and\ndiscovers models with over 3.23x faster MCU inference while maintaining similar\naccuracy",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08996v1",
    "published_date": "2024-01-17 06:17:42 UTC",
    "updated_date": "2024-01-17 06:17:42 UTC"
  },
  {
    "arxiv_id": "2401.09498v2",
    "title": "Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility",
    "authors": [
      "Tian Liu",
      "Yue Cui",
      "Xueyang Hu",
      "Yecheng Xu",
      "Bo Liu"
    ],
    "abstract": "Gossip learning (GL), as a decentralized alternative to federated learning\n(FL), is more suitable for resource-constrained wireless networks, such as\nFlying Ad-Hoc Networks (FANETs) that are formed by unmanned aerial vehicles\n(UAVs). GL can significantly enhance the efficiency and extend the battery life\nof UAV networks. Despite the advantages, the performance of GL is strongly\naffected by data distribution, communication speed, and network connectivity.\nHowever, how these factors influence the GL convergence is still unclear.\nExisting work studied the convergence of GL based on a virtual quantity for the\nsake of convenience, which failed to reflect the real state of the network when\nsome nodes are inaccessible. In this paper, we formulate and investigate the\nimpact of inaccessible nodes to GL under a dynamic network topology. We first\ndecompose the weight divergence by whether the node is accessible or not. Then,\nwe investigate the GL convergence under the dynamic of node accessibility and\ntheoretically provide how the number of inaccessible nodes, data\nnon-i.i.d.-ness, and duration of inaccessibility affect the convergence.\nExtensive experiments are carried out in practical settings to comprehensively\nverify the correctness of our theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09498v2",
    "published_date": "2024-01-17 06:11:19 UTC",
    "updated_date": "2024-02-18 06:34:42 UTC"
  },
  {
    "arxiv_id": "2401.08984v1",
    "title": "A GAN-based data poisoning framework against anomaly detection in vertical federated learning",
    "authors": [
      "Xiaolin Chen",
      "Daoguang Zan",
      "Wei Li",
      "Bei Guan",
      "Yongji Wang"
    ],
    "abstract": "In vertical federated learning (VFL), commercial entities collaboratively\ntrain a model while preserving data privacy. However, a malicious participant's\npoisoning attack may degrade the performance of this collaborative model. The\nmain challenge in achieving the poisoning attack is the absence of access to\nthe server-side top model, leaving the malicious participant without a clear\ntarget model. To address this challenge, we introduce an innovative end-to-end\npoisoning framework P-GAN. Specifically, the malicious participant initially\nemploys semi-supervised learning to train a surrogate target model.\nSubsequently, this participant employs a GAN-based method to produce\nadversarial perturbations to degrade the surrogate target model's performance.\nFinally, the generator is obtained and tailored for VFL poisoning. Besides, we\ndevelop an anomaly detection algorithm based on a deep auto-encoder (DAE),\noffering a robust defense mechanism to VFL scenarios. Through extensive\nexperiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the\nfactors that influence their performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 7 figures. This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2401.08984v1",
    "published_date": "2024-01-17 05:31:08 UTC",
    "updated_date": "2024-01-17 05:31:08 UTC"
  },
  {
    "arxiv_id": "2401.08977v2",
    "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data",
    "authors": [
      "Zikai Xiao",
      "Zihan Chen",
      "Liyinglan Liu",
      "Yang Feng",
      "Jian Wu",
      "Wanlu Liu",
      "Joey Tianyi Zhou",
      "Howard Hao Yang",
      "Zuozhu Liu"
    ],
    "abstract": "Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected\nfrom decentralized local clients manifests a globally prevalent long-tailed\ndistribution, has garnered considerable attention in recent times. In the\ncontext of Fed-LT, existing works have predominantly centered on addressing the\ndata imbalance issue to enhance the efficacy of the generic global model while\nneglecting the performance at the local level. In contrast, conventional\nPersonalized Federated Learning (pFL) techniques are primarily devised to\noptimize personalized local models under the presumption of a balanced global\ndata distribution. This paper introduces an approach termed Federated Local and\nGeneric Model Training in Fed-LT (FedLoGe), which enhances both local and\ngeneric model performance through the integration of representation learning\nand classifier alignment within a neural collapse framework. Our investigation\nreveals the feasibility of employing a shared backbone as a foundational\nframework for capturing overarching global trends, while concurrently employing\nindividualized classifiers to encapsulate distinct refinements stemming from\neach client's local features. Building upon this discovery, we establish the\nStatic Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural\ncollapse principles that naturally prune extraneous noisy features and foster\nthe acquisition of potent data representations. Furthermore, leveraging\ninsights from imbalance neural collapse's classifier norm patterns, we develop\nGlobal and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global\nclassifier and personalized Euclidean norm transfer to align global features\nwith client preferences. Extensive experimental results on CIFAR-10/100-LT,\nImageNet, and iNaturalist demonstrate the advantage of our method over\nstate-of-the-art pFL and Fed-LT approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2024, code: https://github.com/ZackZikaiXiao/FedLoGe",
    "pdf_url": "http://arxiv.org/pdf/2401.08977v2",
    "published_date": "2024-01-17 05:04:33 UTC",
    "updated_date": "2024-03-08 13:37:55 UTC"
  },
  {
    "arxiv_id": "2401.08973v1",
    "title": "OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality",
    "authors": [
      "Aditya Sharma",
      "Luke Yoffe",
      "Tobias Höllerer"
    ],
    "abstract": "One key challenge in Augmented Reality is the placement of virtual content in\nnatural locations. Most existing automated techniques can only work with a\nclosed-vocabulary, fixed set of objects. In this paper, we introduce and\nevaluate several methods for automatic object placement using recent advances\nin open-vocabulary vision-language models. Through a multifaceted evaluation,\nwe identify a new state-of-the-art method, OCTO+. We also introduce a benchmark\nfor automatically evaluating the placement of virtual objects in augmented\nreality, alleviating the need for costly user studies. Through this, in\naddition to human evaluations, we find that OCTO+ places objects in a valid\nregion over 70% of the time, outperforming other methods on a range of metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "2024 IEEE International Conference on Artificial Intelligence and\n  eXtended and Virtual Reality (AIXVR)",
    "pdf_url": "http://arxiv.org/pdf/2401.08973v1",
    "published_date": "2024-01-17 04:52:40 UTC",
    "updated_date": "2024-01-17 04:52:40 UTC"
  },
  {
    "arxiv_id": "2401.10289v1",
    "title": "Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics",
    "authors": [
      "Sanjana Shetty"
    ],
    "abstract": "Neural networks have been employed for a wide range of processing\napplications like image processing, motor control, object detection and many\nothers. Living neural networks offer advantages of lower power consumption,\nfaster processing, and biological realism. Optogenetics offers high spatial and\ntemporal control over biological neurons and presents potential in training\nlive neural networks. This work proposes a simulated living neural network\ntrained indirectly by backpropagating STDP based algorithms using precision\nactivation by optogenetics achieving accuracy comparable to traditional neural\nnetwork training algorithms.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10289v1",
    "published_date": "2024-01-17 04:42:49 UTC",
    "updated_date": "2024-01-17 04:42:49 UTC"
  },
  {
    "arxiv_id": "2401.08960v1",
    "title": "From User Surveys to Telemetry-Driven Agents: Exploring the Potential of Personalized Productivity Solutions",
    "authors": [
      "Subigya Nepal",
      "Javier Hernandez",
      "Talie Massachi",
      "Kael Rowan",
      "Judith Amores",
      "Jina Suh",
      "Gonzalo Ramos",
      "Brian Houck",
      "Shamsi T. Iqbal",
      "Mary Czerwinski"
    ],
    "abstract": "We present a comprehensive, user-centric approach to understand preferences\nin AI-based productivity agents and develop personalized solutions tailored to\nusers' needs. Utilizing a two-phase method, we first conducted a survey with\n363 participants, exploring various aspects of productivity, communication\nstyle, agent approach, personality traits, personalization, and privacy.\nDrawing on the survey insights, we developed a GPT-4 powered personalized\nproductivity agent that utilizes telemetry data gathered via Viva Insights from\ninformation workers to provide tailored assistance. We compared its performance\nwith alternative productivity-assistive tools, such as dashboard and narrative,\nin a study involving 40 participants. Our findings highlight the importance of\nuser-centric design, adaptability, and the balance between personalization and\nprivacy in AI-assisted productivity tools. By building on the insights\ndistilled from our study, we believe that our work can enable and guide future\nresearch to further enhance productivity solutions, ultimately leading to\noptimized efficiency and user experiences for information workers.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.0; H.5.3; H.5.m; J.0"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08960v1",
    "published_date": "2024-01-17 04:20:10 UTC",
    "updated_date": "2024-01-17 04:20:10 UTC"
  },
  {
    "arxiv_id": "2401.08959v1",
    "title": "Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback",
    "authors": [
      "Teng Xiao",
      "Suhang Wang"
    ],
    "abstract": "Probabilistic learning to rank (LTR) has been the dominating approach for\noptimizing the ranking metric, but cannot maximize long-term rewards.\nReinforcement learning models have been proposed to maximize user long-term\nrewards by formulating the recommendation as a sequential decision-making\nproblem, but could only achieve inferior accuracy compared to LTR counterparts,\nprimarily due to the lack of online interactions and the characteristics of\nranking. In this paper, we propose a new off-policy value ranking (VR)\nalgorithm that can simultaneously maximize user long-term rewards and optimize\nthe ranking metric offline for improved sample efficiency in a unified\nExpectation-Maximization (EM) framework. We theoretically and empirically show\nthat the EM process guides the leaned policy to enjoy the benefit of\nintegration of the future reward and ranking metric, and learn without any\nonline interactions. Extensive offline and online experiments demonstrate the\neffectiveness of our methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08959v1",
    "published_date": "2024-01-17 04:19:33 UTC",
    "updated_date": "2024-01-17 04:19:33 UTC"
  },
  {
    "arxiv_id": "2401.08957v3",
    "title": "Learning from Imperfect Demonstrations with Self-Supervision for Robotic Manipulation",
    "authors": [
      "Kun Wu",
      "Ning Liu",
      "Zhen Zhao",
      "Di Qiu",
      "Jinming Li",
      "Zhengping Che",
      "Zhiyuan Xu",
      "Jian Tang"
    ],
    "abstract": "Improving data utilization, especially for imperfect data from task failures,\nis crucial for robotic manipulation due to the challenging, time-consuming, and\nexpensive data collection process in the real world. Current imitation learning\n(IL) typically discards imperfect data, focusing solely on successful expert\ndata. While reinforcement learning (RL) can learn from explorations and\nfailures, the sim2real gap and its reliance on dense reward and online\nexploration make it difficult to apply effectively in real-world scenarios. In\nthis work, we aim to conquer the challenge of leveraging imperfect data without\nthe need for reward information to improve the model performance for robotic\nmanipulation in an offline manner. Specifically, we introduce a Self-Supervised\nData Filtering framework (SSDF) that combines expert and imperfect data to\ncompute quality scores for failed trajectory segments. High-quality segments\nfrom the failed data are used to expand the training dataset. Then, the\nenhanced dataset can be used with any downstream policy learning method for\nrobotic manipulation tasks. Extensive experiments on the ManiSkill2 benchmark\nbuilt on the high-fidelity Sapien simulator and real-world robotic manipulation\ntasks using the Franka robot arm demonstrated that the SSDF can accurately\nexpand the training dataset with high-quality imperfect data and improve the\nsuccess rates for all robotic manipulation tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08957v3",
    "published_date": "2024-01-17 04:15:56 UTC",
    "updated_date": "2025-03-17 06:17:11 UTC"
  },
  {
    "arxiv_id": "2401.10934v1",
    "title": "A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model",
    "authors": [
      "Hao Yang",
      "Jianxin Yuan",
      "Shuai Yang",
      "Linhe Xu",
      "Shuo Yuan",
      "Yifan Zeng"
    ],
    "abstract": "In online advertising scenario, sellers often create multiple creatives to\nprovide comprehensive demonstrations, making it essential to present the most\nappealing design to maximize the Click-Through Rate (CTR). However, sellers\ngenerally struggle to consider users preferences for creative design, leading\nto the relatively lower aesthetics and quantities compared to Artificial\nIntelligence (AI)-based approaches. Traditional AI-based approaches still face\nthe same problem of not considering user information while having limited\naesthetic knowledge from designers. In fact that fusing the user information,\nthe generated creatives can be more attractive because different users may have\ndifferent preferences. To optimize the results, the generated creatives in\ntraditional methods are then ranked by another module named creative ranking\nmodel. The ranking model can predict the CTR score for each creative\nconsidering user features. However, the two above stages are regarded as two\ndifferent tasks and are optimized separately. In this paper, we proposed a new\nautomated Creative Generation pipeline for Click-Through Rate (CG4CTR) with the\ngoal of improving CTR during the creative generation stage. Our contributions\nhave 4 parts: 1) The inpainting mode in stable diffusion is firstly applied to\ncreative generation task in online advertising scene. A self-cyclic generation\npipeline is proposed to ensure the convergence of training. 2) Prompt model is\ndesigned to generate individualized creatives for different user groups, which\ncan further improve the diversity and quality. 3) Reward model comprehensively\nconsiders the multimodal features of image and text to improve the\neffectiveness of creative ranking task, and it is also critical in self-cyclic\npipeline. 4) The significant benefits obtained in online and offline\nexperiments verify the significance of our proposed method.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10934v1",
    "published_date": "2024-01-17 03:27:39 UTC",
    "updated_date": "2024-01-17 03:27:39 UTC"
  },
  {
    "arxiv_id": "2401.08940v1",
    "title": "CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation",
    "authors": [
      "Saba Aslam",
      "Abdur Rasool",
      "Hongyan Wu",
      "Xiaoli Li"
    ],
    "abstract": "Continual learning, the ability of a model to learn over time without\nforgetting previous knowledge and, therefore, be adaptive to new data, is\nparamount in dynamic fields such as disease outbreak prediction. Deep neural\nnetworks, i.e., LSTM, are prone to error due to catastrophic forgetting. This\nstudy introduces a novel CEL model for continual learning by leveraging domain\nadaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate\nthe catastrophic forgetting phenomenon in a domain incremental setting. The\nFisher Information Matrix (FIM) is constructed with EWC to develop a\nregularization term that penalizes changes to important parameters, namely, the\nimportant previous knowledge. CEL's performance is evaluated on three distinct\ndiseases, Influenza, Mpox, and Measles, with different metrics. The high\nR-squared values during evaluation and reevaluation outperform the other\nstate-of-the-art models in several contexts, indicating that CEL adapts to\nincremental data well. CEL's robustness and reliability are underscored by its\nminimal 65% forgetting rate and 18% higher memory stability compared to\nexisting benchmark studies. This study highlights CEL's versatility in disease\noutbreak prediction, addressing evolving data with temporal patterns. It offers\na valuable model for proactive disease control with accurate, timely\npredictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08940v1",
    "published_date": "2024-01-17 03:26:04 UTC",
    "updated_date": "2024-01-17 03:26:04 UTC"
  },
  {
    "arxiv_id": "2401.08936v1",
    "title": "DeLF: Designing Learning Environments with Foundation Models",
    "authors": [
      "Aida Afshar",
      "Wenchao Li"
    ],
    "abstract": "Reinforcement learning (RL) offers a capable and intuitive structure for the\nfundamental sequential decision-making problem. Despite impressive\nbreakthroughs, it can still be difficult to employ RL in practice in many\nsimple applications. In this paper, we try to address this issue by introducing\na method for designing the components of the RL environment for a given,\nuser-intended application. We provide an initial formalization for the problem\nof RL component design, that concentrates on designing a good representation\nfor observation and action space. We propose a method named DeLF: Designing\nLearning Environments with Foundation Models, that employs large language\nmodels to design and codify the user's intended learning scenario. By testing\nour method on four different learning environments, we demonstrate that DeLF\ncan obtain executable environment codes for the corresponding RL problems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2024 Workshop on Synergy of Reinforcement Learning and Large\n  Language Models",
    "pdf_url": "http://arxiv.org/pdf/2401.08936v1",
    "published_date": "2024-01-17 03:14:28 UTC",
    "updated_date": "2024-01-17 03:14:28 UTC"
  },
  {
    "arxiv_id": "2401.08932v1",
    "title": "Learning to detect cloud and snow in remote sensing images from noisy labels",
    "authors": [
      "Zili Liu",
      "Hao Chen",
      "Wenyuan Li",
      "Keyan Chen",
      "Zipeng Qi",
      "Chenyang Liu",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "Detecting clouds and snow in remote sensing images is an essential\npreprocessing task for remote sensing imagery. Previous works draw inspiration\nfrom semantic segmentation models in computer vision, with most research\nfocusing on improving model architectures to enhance detection performance.\nHowever, unlike natural images, the complexity of scenes and the diversity of\ncloud types in remote sensing images result in many inaccurate labels in cloud\nand snow detection datasets, introducing unnecessary noises into the training\nand testing processes. By constructing a new dataset and proposing a novel\ntraining strategy with the curriculum learning paradigm, we guide the model in\nreducing overfitting to noisy labels. Additionally, we design a more\nappropriate model performance evaluation method, that alleviates the\nperformance assessment bias caused by noisy labels. By conducting experiments\non models with UNet and Segformer, we have validated the effectiveness of our\nproposed method. This paper is the first to consider the impact of label noise\non the detection of clouds and snow in remote sensing images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08932v1",
    "published_date": "2024-01-17 03:02:31 UTC",
    "updated_date": "2024-01-17 03:02:31 UTC"
  },
  {
    "arxiv_id": "2401.08930v1",
    "title": "3D Human Pose Analysis via Diffusion Synthesis",
    "authors": [
      "Haorui Ji",
      "Hongdong Li"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in generative modeling.\nIn this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel\nframework designed to address various challenges in 3D human pose analysis\nthrough a unified pipeline. Central to PADS are two distinctive strategies: i)\nlearning a task-agnostic pose prior using a diffusion synthesis process to\neffectively capture the kinematic constraints in human pose data, and ii)\nunifying multiple pose analysis tasks like estimation, completion, denoising,\netc, as instances of inverse problems. The learned pose prior will be treated\nas a regularization imposing on task-specific constraints, guiding the\noptimization process through a series of conditional denoising steps. PADS\nrepresents the first diffusion-based framework for tackling general 3D human\npose analysis within the inverse problem framework. Its performance has been\nvalidated on different benchmarks, signaling the adaptability and robustness of\nthis pipeline.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.08930v1",
    "published_date": "2024-01-17 02:59:34 UTC",
    "updated_date": "2024-01-17 02:59:34 UTC"
  },
  {
    "arxiv_id": "2401.08898v3",
    "title": "Bridging State and History Representations: Understanding Self-Predictive RL",
    "authors": [
      "Tianwei Ni",
      "Benjamin Eysenbach",
      "Erfan Seyedsalehi",
      "Michel Ma",
      "Clement Gehring",
      "Aditya Mahajan",
      "Pierre-Luc Bacon"
    ],
    "abstract": "Representations are at the core of all deep reinforcement learning (RL)\nmethods for both Markov decision processes (MDPs) and partially observable\nMarkov decision processes (POMDPs). Many representation learning methods and\ntheoretical frameworks have been developed to understand what constitutes an\neffective representation. However, the relationships between these methods and\nthe shared properties among them remain unclear. In this paper, we show that\nmany of these seemingly distinct methods and frameworks for state and history\nabstractions are, in fact, based on a common idea of self-predictive\nabstraction. Furthermore, we provide theoretical insights into the widely\nadopted objectives and optimization, such as the stop-gradient technique, in\nlearning self-predictive representations. These findings together yield a\nminimalist algorithm to learn self-predictive representations for states and\nhistories. We validate our theories by applying our algorithm to standard MDPs,\nMDPs with distractors, and POMDPs with sparse rewards. These findings culminate\nin a set of preliminary guidelines for RL practitioners.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 (Poster). Code is available at\n  https://github.com/twni2016/self-predictive-rl",
    "pdf_url": "http://arxiv.org/pdf/2401.08898v3",
    "published_date": "2024-01-17 00:47:43 UTC",
    "updated_date": "2024-04-21 05:59:37 UTC"
  },
  {
    "arxiv_id": "2401.08897v3",
    "title": "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder",
    "authors": [
      "Hee-Jun Jung",
      "Jaehyoung Jeong",
      "Kangil Kim"
    ],
    "abstract": "Symmetries of input and latent vectors have provided valuable insights for\ndisentanglement learning in VAEs. However, only a few works were proposed as an\nunsupervised method, and even these works require known factor information in\nthe training data. We propose a novel method, Composite Factor-Aligned Symmetry\nLearning (CFASL), which is integrated into VAEs for learning symmetry-based\ndisentanglement in unsupervised learning without any knowledge of the dataset\nfactor information. CFASL incorporates three novel features for learning\nsymmetry-based disentanglement: 1) Injecting inductive bias to align latent\nvector dimensions to factor-aligned symmetries within an explicit learnable\nsymmetry code-book 2) Learning a composite symmetry to express unknown factors\nchange between two random samples by learning factor-aligned symmetries within\nthe codebook 3) Inducing a group equivariant encoder and decoder in training\nVAEs with the two conditions. In addition, we propose an extended evaluation\nmetric for multi-factor changes in comparison to disentanglement evaluation in\nVAEs. In quantitative and in-depth qualitative analysis, CFASL demonstrates a\nsignificant improvement of disentanglement in single-factor change, and\nmulti-factor change conditions compared to state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in TMLR 25 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.08897v3",
    "published_date": "2024-01-17 00:46:24 UTC",
    "updated_date": "2024-11-12 01:30:06 UTC"
  }
]
{
  "date": "2025-03-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-12 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 121 篇论文，主要聚焦 AI 模型的安全性、生成式视频和图像的创新应用、LLM 在推理和多模态任务中的进展，以及医疗和机器人领域的实际应用；重点包括低成本视频生成模型 Open-Sora 2.0、LLM 安全基准 SciFi-Benchmark，以及知名学者如 Michael Bronstein 的图神经网络研究，令人印象深刻的是这些论文在提升 AI 鲁棒性和实际部署方面取得的突破。\n\n### 重点论文讨论\n我将优先讨论重要、创新性和话题度高的论文（如 LLM 安全、视频生成和医疗应用），并将相关主题归类讨论。对于其他较常规的论文，我会简要掠过，只列出标题和核心贡献。\n\n#### AI 安全与 LLM 应用（高话题度领域）\n- **SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?**  \n  这篇论文由 Pierre Sermanet 等知名学者提出，构建了一个基准数据集来模拟 AI 机器人行为，评估 LLM 在安全和道德决策中的表现。主要贡献是通过 LLM 生成的模拟场景测试机器人决策，揭示 LLM 在真实世界应用中的潜在风险，发现人类决策准确率达 82.7%，而顶级 LLM 如 GPT-4 仅为 38.8%，强调了 LLM 安全评估的必要性。\n\n- **Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**  \n  作者包括 Bowen Jin 和 Jiawei Han，这篇论文使用强化学习训练 LLM 进行推理和搜索增强。主要发现是 LLM 通过 RL 优化后，在问答任务中提升了 41% 的性能，展示了 LLM 如何结合外部搜索工具实现更可靠的决策。\n\n- **JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing**  \n  这篇论文引入模糊测试（fuzzing）来自动发现 LLM 的越狱漏洞。主要贡献是开发了高效的攻击框架 JBFuzz，能在 60 秒内成功越狱多种 LLM，成功率达 99%，揭示了现有安全机制的脆弱性。\n\n- **AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents**  \n  作者包括 Kamalika Chaudhuri，这篇论文评估 AI 代理的隐私泄露风险。主要发现是基于强化学习的代理易于泄露敏感信息，并提出提示优化策略减少泄露，强调了 AI 代理在隐私保护中的权衡。\n\n相关论文如 **Media and responsible AI governance** 和 **Towards Reasoning Era** 等，也探讨 LLM 的治理和推理，但它们更侧重理论，我快速掠过：它们使用博弈论和 LLM 模拟分析 AI 治理策略，发现媒体报道可作为软性监管机制，提升 AI 可靠性。\n\n#### 视频和图像生成模型（创新技术亮点）\n- **Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k$**  \n  作者包括 Yang You，这篇高影响力论文展示了低成本训练视频生成模型的方法。主要贡献是通过优化数据和架构，在 20 万美元内训练出媲美商业级别的模型，FID 和 FVD 指标均优于现有方法，证明了高效视频生成的潜力。\n\n- **CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation**  \n  这篇论文提出了一种自监督跨模态知识蒸馏框架，用于 3D 感知任务。主要发现是使用特征相似性损失提升 3D 对象检测和语义分割性能，mIoU 提升达 10%，尤其在低数据量场景下表现突出。\n\n- **Reangle-A-Video: 4D Video Generation as Video-to-Video Translation**  \n  作者使用扩散模型将视频生成转化为视频到视频翻译。主要贡献是开发了高效框架，能从单视频生成多视角同步视频，FID 降低 30%，显著提高了视频一致性和生成质量。\n\n其他视觉生成论文如 **SeqSAM** 和 **Unmask It!** 等，我简要提到：SeqSAM 通过 RNN 改进医疗图像分割，生成多个掩码提升不确定性处理；Unmask It! 使用稀疏自编码器实现零样本主观中心生成，准确性优于基线。\n\n#### 医疗和生物应用（实际影响领域）\n- **Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis**  \n  这篇论文使用图神经网络（GNN）和视觉语言模型（VLM）分析糖尿病视网膜病变。主要贡献是通过图表示学习和指令微调，实现可解释的图像诊断，准确性提升并提供临床解释，实验显示在公开数据集上分类准确率提高。\n\n- **CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models**  \n  作者使用 LLM 分析癌症幸存者的移动日记数据。主要发现是 LLM 通过上下文增强模型准确预测情感状态和干预时机，准确率达 73%，为即时心理支持提供新框架。\n\n相关医疗论文如 **GenHPE** 和 **Revisiting semi-supervised learning** 等，我快速掠过：GenHPE 使用 RF 信号进行零样本人体姿态估计，错误率降低 52%；Revisiting 则优化半监督学习，提升医疗图像分类泛化性。\n\n#### 机器人和导航（跨界应用）\n- **Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation**  \n  这篇论文提出视觉语言注意力蒸馏框架，用于社交机器人导航。主要贡献是通过预训练模型蒸馏知识，提升导航成功率达 50%，实验在真实机器人上验证了其社会感知能力。\n\n- **ManeuverGPT: Agentic Control for Safe Autonomous Stunt Maneuvers**  \n  作者使用 LLM 代理控制自主车辆进行特技 maneuvers。主要发现是 LLM 驱动的决策框架在模拟环境中减少延迟 40%，提升安全性和机动性。\n\n其他机器人论文如 **KNighter** 等，我简要提到：它使用 LLM 生成静态分析工具，发现 Linux 内核 92 个新漏洞。\n\n#### 其他领域快速掠过\n其余论文涉及图神经网络、持续学习和优化算法，我只列出标题和核心点：\n- **Temporal Difference Flows**：提出新 Bellman 方程优化生成模型，提升长时序预测准确性。\n- **Finding the Muses**：使用 GNN 分析足球数据，识别关键球员贡献。\n- **ConjointNet**：通过表示学习提升用户偏好预测，准确率提高 5%。\n- **DistJoin**：开发多自回归模型估计算法连接基数，提升查询效率。\n- **TreeX**：提取 GNN 子树解释决策，提升模型可解释性。\n- **Long-horizon Visual Instruction Generation**：使用自反机制生成长时序视觉指令，提升图像生成逻辑。\n\n今天的论文总体上突出了 AI 在安全、生成和实际应用中的进展，但也暴露了如隐私和鲁棒性等挑战。感兴趣的读者可关注 LLM 和多模态模型的最新动态！（本快报基于121篇论文精选，完整列表可查 arXiv）。",
  "papers": [
    {
      "arxiv_id": "2503.09901v1",
      "title": "AI Rivalry as a Craft: How Resisting and Embracing Generative AI Reshape Writing Professions",
      "title_zh": "翻译失败",
      "authors": [
        "Rama Adithya Varanasi",
        "Batia Mishan Wiesenfeld",
        "Oded Nov"
      ],
      "abstract": "Generative AI (GAI) technologies are disrupting professional writing,\nchallenging traditional practices. Recent studies explore GAI adoption\nexperiences of creative practitioners, but we know little about how these\nexperiences evolve into established practices and how GAI resistance alters\nthese practices. To address this gap, we conducted 25 semi-structured\ninterviews with writing professionals who adopted and/or resisted GAI. Using\nthe theoretical lens of Job Crafting, we identify four strategies professionals\nemploy to reshape their roles. Writing professionals employed GAI resisting\nstrategies to maximize human potential, reinforce professional identity, carve\nout a professional niche, and preserve credibility within their networks. In\ncontrast, GAI-enabled strategies allowed writers who embraced GAI to enhance\ndesirable workflows, minimize mundane tasks, and engage in new AI-managerial\nlabor. These strategies amplified their collaborations with GAI while reducing\ntheir reliance on other people. We conclude by discussing implications of GAI\npractices on writers' identity and practices as well as crafting theory.",
      "tldr_zh": "本研究探讨了生成式 AI (GAI) 如何重塑写作专业，通过对 25 名写作专业人士的半结构化采访，并采用 Job Crafting 理论视角，识别了四种策略来应对 GAI 的影响。抵抗 GAI 的策略包括最大化人类潜力、强化专业身份、开辟专业领域以及维护网络信誉，帮助专业人士维护其核心价值。拥抱 GAI 的策略则允许作家增强工作流程、减少琐碎任务，并从事 AI 管理劳动，从而增加与 GAI 的协作并减少对人类的依赖。该研究揭示了这些实践对作家身份和 Job Crafting 理论的深远启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09901v1",
      "published_date": "2025-03-12 23:43:57 UTC",
      "updated_date": "2025-03-12 23:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:07:11.240011"
    },
    {
      "arxiv_id": "2503.09896v1",
      "title": "A Rule Based Solution to Co-reference Resolution in Clinical Text",
      "title_zh": "基于规则的临床文本共指消解解决方案",
      "authors": [
        "Ping Chen",
        "David Hinote",
        "Guoqing Chen"
      ],
      "abstract": "Objective: The aim of this study was to build an effective co-reference\nresolution system tailored for the biomedical domain. Materials and Methods:\nExperiment materials used in this study is provided by the 2011 i2b2 Natural\nLanguage Processing Challenge. The 2011 i2b2 challenge involves coreference\nresolution in medical documents. Concept mentions have been annotated in\nclinical texts, and the mentions that co-refer in each document are to be\nlinked by coreference chains. Normally, there are two ways of constructing a\nsystem to automatically discover co-referent links. One is to manually build\nrules for co-reference resolution, and the other category of approaches is to\nuse machine learning systems to learn automatically from training datasets and\nthen perform the resolution task on testing datasets. Results: Experiments show\nthe existing co-reference resolution systems are able to find some of the\nco-referent links, and our rule based system performs well finding the majority\nof the co-referent links. Our system achieved 89.6% overall performance on\nmultiple medical datasets. Conclusion: The experiment results show that\nmanually crafted rules based on observation of training data is a valid way to\naccomplish high performance in this coreference resolution task for the\ncritical biomedical domain.",
      "tldr_zh": "这篇论文提出了一种基于规则的解决方案，用于在临床文本中进行 co-reference resolution，针对生物医学领域的特定需求。研究使用 2011 i2b2 自然语言处理挑战的数据，手动构建规则来链接共指的 mentions，并与机器学习方法进行比较。实验结果显示，该系统在多个医疗数据集上达到了 89.6% 的整体性能，证明手动规则方法在 co-reference resolution 任务中是高效且可靠的。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09896v1",
      "published_date": "2025-03-12 23:29:08 UTC",
      "updated_date": "2025-03-12 23:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:07:21.944047"
    },
    {
      "arxiv_id": "2503.09878v1",
      "title": "CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Hariprasath Govindarajan",
        "Maciej K. Wozniak",
        "Marvin Klingner",
        "Camille Maurice",
        "B Ravi Kiran",
        "Senthil Yogamani"
      ],
      "abstract": "Vision foundation models (VFMs) such as DINO have led to a paradigm shift in\n2D camera-based perception towards extracting generalized features to support\nmany downstream tasks. Recent works introduce self-supervised cross-modal\nknowledge distillation (KD) as a way to transfer these powerful generalization\ncapabilities into 3D LiDAR-based models. However, they either rely on highly\ncomplex distillation losses, pseudo-semantic maps, or limit KD to features\nuseful for semantic segmentation only. In this work, we propose\nCleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework\nintroducing a set of simple yet effective design choices: Unlike contrastive\napproaches relying on complex loss design choices, our method employs a direct\nfeature similarity loss in combination with a multi layer perceptron (MLP)\nprojection head to allow the 3D network to learn complex semantic dependencies\nthroughout the projection. Crucially, our approach does not depend on\npseudo-semantic maps, allowing for direct knowledge transfer from a VFM without\nexplicit semantic supervision. Additionally, we introduce the auxiliary\nself-supervised spatial task of occupancy prediction to enhance the semantic\nknowledge, obtained from a VFM through KD, with 3D spatial reasoning\ncapabilities. Experiments on standard autonomous driving benchmarks for\n2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art\nperformance in both semantic segmentation and 3D object detection (3DOD) by up\nto 10% mIoU, especially when fine tuning on really low data amounts, showing\nthe effectiveness of our simple yet powerful KD strategy",
      "tldr_zh": "该研究提出 CleverDistiller，一种简单且空间一致的自监督跨模态知识蒸馏(KD)框架，用于从 2D 视觉基础模型(VFMs)如 DINO 向 3D LiDAR 模型转移泛化能力，而不依赖复杂损失或伪语义地图。框架采用直接特征相似性损失结合多层感知器(MLP)投影头，帮助 3D 网络学习复杂语义依赖，并引入辅助占用预测任务以增强 3D 空间推理能力。实验结果显示，在自动驾驶基准上，CleverDistiller 在语义分割和 3D 对象检测(3DOD)上比基线方法提升高达 10% mIoU，尤其在低数据量微调场景中表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09878v1",
      "published_date": "2025-03-12 22:18:29 UTC",
      "updated_date": "2025-03-12 22:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:07:36.321433"
    },
    {
      "arxiv_id": "2503.09858v1",
      "title": "Media and responsible AI governance: a game-theoretic and LLM analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Nataliya Balabanova",
        "Adeela Bashir",
        "Paolo Bova",
        "Alessio Buscemi",
        "Theodor Cimpeanu",
        "Henrique Correia da Fonseca",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "Antonio Fernandes",
        "The Anh Han",
        "Marcus Krellner",
        "Ndidi Bianca Ogbo",
        "Simon T. Powers",
        "Daniele Proverbio",
        "Fernando P. Santos",
        "Zia Ush Shamszaman",
        "Zhao Song"
      ],
      "abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "tldr_zh": "这篇论文使用进化博弈理论和大型语言模型 (LLMs) 分析 AI 开发者、监管者、用户和媒体在促进可信 AI 系统中的战略互动，探讨不同监管制度下的行为模式。研究重点考察媒体报道如何激励有效监管，以及用户信任如何依赖评论家的推荐，作为一种“软”监管形式来替代机构 AI 监管。结果显示，通过博弈理论分析和 LLM 模拟，可识别出实现负责 AI 治理的条件，并强调需要管理高质量评论的激励和成本，以确保有效治理。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "nlin.CD"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09858v1",
      "published_date": "2025-03-12 21:39:38 UTC",
      "updated_date": "2025-03-12 21:39:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:07:46.959105"
    },
    {
      "arxiv_id": "2503.09853v2",
      "title": "Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence",
      "title_zh": "屏幕背后，你是谁？ 利用人工智能进行隐式 MBTI 和性别检测",
      "authors": [
        "Kourosh Shahnazari",
        "Seyed Moein Ayyoubzadeh"
      ],
      "abstract": "In personalized technology and psychological research, precisely detecting\ndemographic features and personality traits from digital interactions becomes\never more important. This work investigates implicit categorization, inferring\npersonality and gender variables directly from linguistic patterns in Telegram\nconversation data, while conventional personality prediction techniques mostly\ndepend on explicitly self-reported labels. We refine a Transformer-based\nlanguage model (RoBERTa) to capture complex linguistic cues indicative of\npersonality traits and gender differences using a dataset comprising 138,866\nmessages from 1,602 users annotated with MBTI types and 195,016 messages from\n2,598 users annotated with gender. Confidence levels help to greatly raise\nmodel accuracy to 86.16\\%, hence proving RoBERTa's capacity to consistently\nidentify implicit personality types from conversational text data. Our results\nhighlight the usefulness of Transformer topologies for implicit personality and\ngender classification, hence stressing their efficiency and stressing important\ntrade-offs between accuracy and coverage in realistic conversational\nenvironments. With regard to gender classification, the model obtained an\naccuracy of 74.4\\%, therefore capturing gender-specific language patterns.\nPersonality dimension analysis showed that people with introverted and\nintuitive preferences are especially more active in text-based interactions.\nThis study emphasizes practical issues in balancing accuracy and data coverage\nas Transformer-based models show their efficiency in implicit personality and\ngender prediction tasks from conversational texts.",
      "tldr_zh": "本研究探讨了使用人工智能从 Telegram 对话数据中隐式检测 MBTI 个性类型和性别，避开传统依赖自我报告的方法。研究团队微调了 RoBERTa 模型，利用包含 138,866 条 MBTI 标注消息和 195,016 条性别标注消息的数据集，捕捉语言模式中的复杂线索。结果显示，模型在 MBTI 预测上达到 86.16% 准确率，在性别分类上达到 74.4%，并发现内向和直觉类型用户更活跃于文本互动；这突出了 Transformer 模型在隐式分类中的效率，以及准确性和覆盖率之间的权衡。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09853v2",
      "published_date": "2025-03-12 21:24:22 UTC",
      "updated_date": "2025-03-14 23:59:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:07:58.745828"
    },
    {
      "arxiv_id": "2503.09849v2",
      "title": "Training Human-Robot Teams by Improving Transparency Through a Virtual Spectator Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Dallas",
        "Hongjiao Qiang",
        "Motaz AbuHijleh",
        "Wonse Jo",
        "Kayla Riegner",
        "Jon Smereka",
        "Lionel Robert",
        "Wing-Yue Louie",
        "Dawn M. Tilbury"
      ],
      "abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
      "tldr_zh": "这篇论文针对人类-机器人团队的训练问题，提出 Virtual Spectator Interface (VSI) 工具，以提升团队透明度和 situational awareness (SA)，通过视觉反馈审查行为来弥补传统 After-action reviews (AARs) 的不足。研究设计了一个1x3组间实验，在模拟搜索任务中比较了使用VSI、屏幕录制和非技术（仅口头描述）的训练回顾 (TR) 条件。结果表明，VSI 并未显著改善整体团队表现，但显著提升了参与者的 situational awareness (SA)。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO",
        "H.5.2; I.2.9"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 4 figures, Accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09849v2",
      "published_date": "2025-03-12 21:13:34 UTC",
      "updated_date": "2025-04-12 22:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:08:10.923736"
    },
    {
      "arxiv_id": "2503.09837v2",
      "title": "On the Limitations of Vision-Language Models in Understanding Image Transforms",
      "title_zh": "视觉语言模型在理解图像变换方面的局限性",
      "authors": [
        "Ahmad Mustafa Anis",
        "Hasnain Ali",
        "Saquib Sarfraz"
      ],
      "abstract": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.",
      "tldr_zh": "该研究揭示了Vision-Language Models (VLMs)，如CLIP和SigLIP，在理解图像变换方面的局限性，这些模型尽管在图像生成、视觉问答和视频理解等任务上表现出色，但无法有效处理基本图像级增强。研究者创建了增强版的Flickr8k数据集，每个图像配有详细的变换描述，用于评估VLMs的图像理解能力。结果显示，这种缺陷会显著影响下游任务，特别是图像编辑领域，并对最先进的Image2Image模型在简单变换上的性能进行了评估，为未来VLMs的改进提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "I.4; I.2.10; I.2.7"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 15 images",
      "pdf_url": "http://arxiv.org/pdf/2503.09837v2",
      "published_date": "2025-03-12 20:58:16 UTC",
      "updated_date": "2025-03-14 01:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:08:22.719203"
    },
    {
      "arxiv_id": "2503.09822v1",
      "title": "Generative AI for Named Entity Recognition in Low-Resource Language Nepali",
      "title_zh": "翻译失败",
      "authors": [
        "Sameer Neupane",
        "Jeevan Chapagain",
        "Nobal B. Niraula",
        "Diwa Koirala"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI), particularly Large Language\nModels (LLMs), has significantly advanced Natural Language Processing (NLP)\ntasks, such as Named Entity Recognition (NER), which involves identifying\nentities like person, location, and organization names in text. LLMs are\nespecially promising for low-resource languages due to their ability to learn\nfrom limited data. However, the performance of GenAI models for Nepali, a\nlow-resource language, has not been thoroughly evaluated. This paper\ninvestigates the application of state-of-the-art LLMs for Nepali NER,\nconducting experiments with various prompting techniques to assess their\neffectiveness. Our results provide insights into the challenges and\nopportunities of using LLMs for NER in low-resource settings and offer valuable\ncontributions to the advancement of NLP research in languages like Nepali.",
      "tldr_zh": "这篇论文探讨了生成式 AI（Generative AI），特别是大型语言模型（LLMs），在低资源语言 Nepali 的命名实体识别（NER）任务中的应用，旨在利用 LLMs 从有限数据中学习识别文本中的实体如人名、地点和组织。研究团队通过实验评估了各种提示技术，以测试这些模型在 Nepali NER 上的性能。结果揭示了低资源环境中的挑战和机会，并为类似 Nepali 的语言的自然语言处理（NLP）研究提供了重要洞见和贡献。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted in the FLAIRS Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09822v1",
      "published_date": "2025-03-12 20:40:09 UTC",
      "updated_date": "2025-03-12 20:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:08:34.821689"
    },
    {
      "arxiv_id": "2503.09820v1",
      "title": "Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Elnoor",
        "Kasun Weerakoon",
        "Gershom Seneviratne",
        "Jing Liang",
        "Vignesh Rajagopal",
        "Dinesh Manocha"
      ],
      "abstract": "We introduce Vision-Language Attention Distillation (Vi-LAD), a novel\napproach for distilling socially compliant navigation knowledge from a large\nVision-Language Model (VLM) into a lightweight transformer model for real-time\nrobotic navigation. Unlike traditional methods that rely on expert\ndemonstrations or human-annotated datasets, Vi-LAD performs knowledge\ndistillation and fine-tuning at the intermediate layer representation level\n(i.e., attention maps) by leveraging the backbone of a pre-trained\nvision-action model. These attention maps highlight key navigational regions in\na given scene, which serve as implicit guidance for socially aware motion\nplanning. Vi-LAD fine-tunes a transformer-based model using intermediate\nattention maps extracted from the pre-trained vision-action model, combined\nwith attention-like semantic maps constructed from a large VLM. To achieve\nthis, we introduce a novel attention-level distillation loss that fuses\nknowledge from both sources, generating augmented attention maps with enhanced\nsocial awareness. These refined attention maps are then utilized as a\ntraversability costmap within a socially aware model predictive controller\n(MPC) for navigation. We validate our approach through real-world experiments\non a Husky wheeled robot, demonstrating significant improvements over\nstate-of-the-art (SOTA) navigation methods. Our results show up to 14.2% - 50%\nimprovement in success rate, which highlights the effectiveness of Vi-LAD in\nenabling socially compliant and efficient robot navigation.",
      "tldr_zh": "该研究提出 Vi-LAD，一种创新的视觉-语言注意力蒸馏方法，用于在动态环境中实现社会意识的机器人导航，通过从大型 Vision-Language Model (VLM) 中提炼知识到轻量级 transformer 模型。Vi-LAD 利用中间层注意力图（attention maps）进行知识蒸馏和微调，结合预训练 vision-action 模型的骨干网络和 VLM 构建的语义地图，引入新型注意力级蒸馏损失来生成增强社会意识的导航指导。实验在 Husky 轮式机器人上验证，与最先进（SOTA）方法相比，成功率提升14.2%至50%，显著提高了导航的合规性和效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09820v1",
      "published_date": "2025-03-12 20:38:23 UTC",
      "updated_date": "2025-03-12 20:38:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:08:47.080225"
    },
    {
      "arxiv_id": "2503.09817v1",
      "title": "Temporal Difference Flows",
      "title_zh": "翻译失败",
      "authors": [
        "Jesse Farebrother",
        "Matteo Pirotta",
        "Andrea Tirinzoni",
        "Rémi Munos",
        "Alessandro Lazaric",
        "Ahmed Touati"
      ],
      "abstract": "Predictive models of the future are fundamental for an agent's ability to\nreason and plan. A common strategy learns a world model and unrolls it\nstep-by-step at inference, where small errors can rapidly compound. Geometric\nHorizon Models (GHMs) offer a compelling alternative by directly making\npredictions of future states, avoiding cumulative inference errors. While GHMs\ncan be conveniently learned by a generative analog to temporal difference (TD)\nlearning, existing methods are negatively affected by bootstrapping predictions\nat train time and struggle to generate high-quality predictions at long\nhorizons. This paper introduces Temporal Difference Flows (TD-Flow), which\nleverages the structure of a novel Bellman equation on probability paths\nalongside flow-matching techniques to learn accurate GHMs at over 5x the\nhorizon length of prior methods. Theoretically, we establish a new convergence\nresult and primarily attribute TD-Flow's efficacy to reduced gradient variance\nduring training. We further show that similar arguments can be extended to\ndiffusion-based methods. Empirically, we validate TD-Flow across a diverse set\nof domains on both generative metrics and downstream tasks including policy\nevaluation. Moreover, integrating TD-Flow with recent behavior foundation\nmodels for planning over pre-trained policies demonstrates substantial\nperformance gains, underscoring its promise for long-horizon decision-making.",
      "tldr_zh": "本论文提出 Temporal Difference Flows (TD-Flow)，一种改进 Geometric Horizon Models (GHMs) 的方法，通过新的 Bellman 方程和 flow-matching 技术，直接预测未来状态，避免了传统模型的累积推理错误，并在训练中减少梯度方差。TD-Flow 能实现超过现有方法 5 倍的时段长度预测，并理论上证明了其收敛性，同时扩展适用于 diffusion-based 方法。实验在多种领域验证了其在生成指标和下游任务（如策略评估和规划）上的优越性能，与行为基础模型整合后显著提升了长时段决策效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09817v1",
      "published_date": "2025-03-12 20:30:07 UTC",
      "updated_date": "2025-03-12 20:30:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:08:59.141518"
    },
    {
      "arxiv_id": "2503.09808v1",
      "title": "Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Chenjun Li",
        "Laurin Lux",
        "Alexander H. Berger",
        "Martin J. Menten",
        "Mert R. Sabuncu",
        "Johannes C. Paetzold"
      ],
      "abstract": "Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely\ninterventions and preventing vision loss. However, current staging models are\nhardly interpretable, and most public datasets contain no clinical reasoning or\ninterpretation beyond image-level labels. In this paper, we present a novel\nmethod that integrates graph representation learning with vision-language\nmodels (VLMs) to deliver explainable DR diagnosis. Our approach leverages\noptical coherence tomography angiography (OCTA) images by constructing\nbiologically informed graphs that encode key retinal vascular features such as\nvessel morphology and spatial connectivity. A graph neural network (GNN) then\nperforms DR staging while integrated gradients highlight critical nodes and\nedges and their individual features that drive the classification decisions. We\ncollect this graph-based knowledge which attributes the model's prediction to\nphysiological structures and their characteristics. We then transform it into\ntextual descriptions for VLMs. We perform instruction-tuning with these textual\ndescriptions and the corresponding image to train a student VLM. This final\nagent can classify the disease and explain its decision in a human\ninterpretable way solely based on a single image input. Experimental\nevaluations on both proprietary and public datasets demonstrate that our method\nnot only improves classification accuracy but also offers more clinically\ninterpretable results. An expert study further demonstrates that our method\nprovides more accurate diagnostic explanations and paves the way for precise\nlocalization of pathologies in OCTA images.",
      "tldr_zh": "本研究提出了一种结合图表示学习和视觉语言模型 (VLMs) 的方法，用于可解释的糖尿病视网膜病变 (DR) 诊断。方法通过构建基于光学相干断层血管造影 (OCTA) 图像的生物信息图，编码视网膜血管特征（如血管形态和空间连通性），并利用图神经网络 (GNN) 进行 DR 分期，同时将图-based 知识转化为文本描述，对学生 VLM 进行指令微调。最终模型能基于单张图像实现疾病分类并提供人类可解读的解释。实验在专有和公共数据集上显示，该方法显著提升了分类准确性，并经专家研究证实，提供更精确的诊断解释和病变定位。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09808v1",
      "published_date": "2025-03-12 20:19:07 UTC",
      "updated_date": "2025-03-12 20:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:09:11.674129"
    },
    {
      "arxiv_id": "2503.09805v2",
      "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jordan Taylor",
        "Joel Mire",
        "Franchesca Spektor",
        "Alicia DeVrio",
        "Maarten Sap",
        "Haiyi Zhu",
        "Sarah Fox"
      ],
      "abstract": "Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.",
      "tldr_zh": "本研究探讨了酷儿（queer）艺术家如何与生成式AI（Generative AI）互动，并挑战其嵌入的规范性价值观，如过度积极性和反性倾向。通过与13位酷儿艺术家的研讨会，参与者使用GPT-4和DALL-E 3进行实验，并开发出各种策略来克服模型的限制，同时从中发现潜在价值。基于queer feminist theory，该论文讨论了对“state-of-the-art”模型概念的启示，并建议FAccT（Fairness, Accountability, and Transparency）研究者如何支持酷儿替代方案。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09805v2",
      "published_date": "2025-03-12 20:16:38 UTC",
      "updated_date": "2025-05-05 17:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:09:23.096208"
    },
    {
      "arxiv_id": "2503.09797v1",
      "title": "SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation using SAM",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Towle",
        "Xin Chen",
        "Ke Zhou"
      ],
      "abstract": "Pre-trained segmentation models are a powerful and flexible tool for\nsegmenting images. Recently, this trend has extended to medical imaging. Yet,\noften these methods only produce a single prediction for a given image,\nneglecting inherent uncertainty in medical images, due to unclear object\nboundaries and errors caused by the annotation tool. Multiple Choice Learning\nis a technique for generating multiple masks, through multiple learned\nprediction heads. However, this cannot readily be extended to producing more\noutputs than its initial pre-training hyperparameters, as the sparse,\nwinner-takes-all loss function makes it easy for one prediction head to become\noverly dominant, thus not guaranteeing the clinical relevancy of each mask\nproduced. We introduce SeqSAM, a sequential, RNN-inspired approach to\ngenerating multiple masks, which uses a bipartite matching loss for ensuring\nthe clinical relevancy of each mask, and can produce an arbitrary number of\nmasks. We show notable improvements in quality of each mask produced across two\npublicly available datasets. Our code is available at\nhttps://github.com/BenjaminTowle/SeqSAM.",
      "tldr_zh": "该论文提出 SeqSAM，一种自回归(Autoregressive)多假设预测方法，基于 SAM 模型，用于医疗图像分割，以解决现有预训练模型仅输出单一预测而忽略图像不确定性（如模糊边界和标注错误）的问题。SeqSAM 借鉴 RNN 设计，通过顺序生成多个掩码，并采用二分匹配(bipartite matching)损失函数，确保每个掩码的临床相关性和质量，从而支持任意数量的输出。实验结果显示，在两个公开数据集上，SeqSAM 显著提升了每个掩码的生成质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09797v1",
      "published_date": "2025-03-12 20:01:52 UTC",
      "updated_date": "2025-03-12 20:01:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:09:36.991003"
    },
    {
      "arxiv_id": "2503.09780v2",
      "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
      "title_zh": "AgentDAM：自治网络代理的隐私泄露评估",
      "authors": [
        "Arman Zharmagambetov",
        "Chuan Guo",
        "Ivan Evtimov",
        "Maya Pavlova",
        "Ruslan Salakhutdinov",
        "Kamalika Chaudhuri"
      ],
      "abstract": "Autonomous AI agents that can follow instructions and perform complex\nmulti-step tasks have tremendous potential to boost human productivity.\nHowever, to perform many of these tasks, the agents need access to personal\ninformation from their users, raising the question of whether they are capable\nof using it appropriately. In this work, we introduce a new benchmark AgentDAM\nthat measures if AI web-navigation agents follow the privacy principle of\n``data minimization''. For the purposes of our benchmark, data minimization\nmeans that the agent uses a piece of potentially sensitive information only if\nit is ``necessary'' to complete a particular task. Our benchmark simulates\nrealistic web interaction scenarios end-to-end and is adaptable to all existing\nweb navigation agents. We use AgentDAM to evaluate how well AI agents built on\ntop of GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation, and show that they are prone to inadvertent use of unnecessary\nsensitive information. We also propose a prompting-based defense that reduces\ninformation leakage, and demonstrate that our end-to-end benchmarking provides\na more realistic measure than probing LLMs about privacy. Our results highlight\nthat further research is needed to develop AI agents that can prioritize data\nminimization at inference time.",
      "tldr_zh": "本研究引入了 AgentDAM 基准，用于评估自主网络代理在处理用户个人信息时的隐私泄露风险，重点检查代理是否遵守“data minimization”原则，即仅在必要时使用敏感信息。AgentDAM 通过模拟真实端到端网络交互场景，适用于基于 GPT-4、Llama-3 和 Claude 的现有代理，结果显示这些代理容易无意中使用不必要的信息。研究提出了一种基于 prompting 的防御方法来减少泄露，并证明端到端基准比直接查询 LLM 更可靠，强调未来需要进一步开发在推理时优先数据最小化的 AI 代理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "project page: https://github.com/facebookresearch/ai-agent-privacy",
      "pdf_url": "http://arxiv.org/pdf/2503.09780v2",
      "published_date": "2025-03-12 19:30:31 UTC",
      "updated_date": "2025-05-16 22:47:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:09:47.218067"
    },
    {
      "arxiv_id": "2503.11711v2",
      "title": "Privacy-Preserved Automated Scoring using Federated Learning for Educational Research",
      "title_zh": "翻译失败",
      "authors": [
        "Ehsan Latif",
        "Xiaoming Zhai"
      ],
      "abstract": "Data privacy remains a critical concern in educational research, requiring\nstrict adherence to ethical standards and regulatory protocols. While\ntraditional approaches rely on anonymization and centralized data collection,\nthey often expose raw student data to security vulnerabilities and impose\nsubstantial logistical overhead. In this study, we propose a federated learning\n(FL) framework for automated scoring of educational assessments that eliminates\nthe need to share sensitive data across institutions. Our approach leverages\nparameter-efficient fine-tuning of large language models (LLMs) with Low-Rank\nAdaptation (LoRA), enabling each client (school) to train locally while sharing\nonly optimized model updates. To address data heterogeneity, we implement an\nadaptive weighted aggregation strategy that considers both client performance\nand data volume. We benchmark our model against two state-of-the-art FL methods\nand a centralized learning baseline using NGSS-aligned multi-label science\nassessment data from nine middle schools. Results show that our model achieves\nthe highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0\npercentage points of the centralized model on these metrics. Additionally, it\nachieves comparable rubric-level scoring accuracy, with only a 1.3% difference\nin rubric match and a lower score deviation (MAE), highlighting its\neffectiveness in preserving both prediction quality and interpretability.",
      "tldr_zh": "本研究针对教育研究中的数据隐私问题，提出了一种基于Federated Learning (FL)的自动化评分框架，避免了敏感数据的跨机构共享。框架利用Low-Rank Adaptation (LoRA)对Large Language Models (LLMs)进行参数高效微调，每个客户端（如学校）仅共享优化后的模型更新，并采用adaptive weighted aggregation策略来处理数据异质性。实验结果显示，该方法在NGSS-aligned多标签科学评估数据上实现了94.5%的最高准确率，与集中式模型仅差0.5-1.0个百分点，并在rubric-level评分上保持了良好的匹配度和较低的均方误差（MAE），从而兼顾了预测质量、可解释性和隐私保护。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AIED25",
      "pdf_url": "http://arxiv.org/pdf/2503.11711v2",
      "published_date": "2025-03-12 19:06:25 UTC",
      "updated_date": "2025-05-08 20:14:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:00.119872"
    },
    {
      "arxiv_id": "2503.11710v1",
      "title": "ConjointNet: Enhancing Conjoint Analysis for Preference Prediction with Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yanxia Zhang",
        "Francine Chen",
        "Shabnam Hakimi",
        "Totte Harinen",
        "Alex Filipowicz",
        "Yan-Ying Chen",
        "Rumen Iliev",
        "Nikos Arechiga",
        "Kalani Murakami",
        "Kent Lyons",
        "Charlene Wu",
        "Matt Klenk"
      ],
      "abstract": "Understanding consumer preferences is essential to product design and\npredicting market response to these new products. Choice-based conjoint\nanalysis is widely used to model user preferences using their choices in\nsurveys. However, traditional conjoint estimation techniques assume simple\nlinear models. This assumption may lead to limited predictability and\ninaccurate estimation of product attribute contributions, especially on data\nthat has underlying non-linear relationships. In this work, we employ\nrepresentation learning to efficiently alleviate this issue. We propose\nConjointNet, which is composed of two novel neural architectures, to predict\nuser preferences. We demonstrate that the proposed ConjointNet models\noutperform traditional conjoint estimate techniques on two preference datasets\nby over 5%, and offer insights into non-linear feature interactions.",
      "tldr_zh": "这篇论文针对传统联合分析(Conjoint Analysis)假设简单线性模型的问题，提出了一种通过表示学习(Representation Learning)增强偏好预测的方法。作者开发了ConjointNet模型，该模型由两个新颖的神经网络架构组成，能够更好地处理数据中的非线性关系，从而提高用户偏好预测的准确性。实验结果显示，ConjointNet在两个偏好数据集上比传统技术提升超过5%，并提供了对非线性特征交互的宝贵洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11710v1",
      "published_date": "2025-03-12 19:01:59 UTC",
      "updated_date": "2025-03-12 19:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:10.981982"
    },
    {
      "arxiv_id": "2503.09746v1",
      "title": "Solving Bayesian inverse problems with diffusion priors and off-policy RL",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Scimeca",
        "Siddarth Venkatraman",
        "Moksh Jain",
        "Minsu Kim",
        "Marcin Sendera",
        "Mohsin Hasan",
        "Luke Rowe",
        "Sarthak Mittal",
        "Pablo Lemos",
        "Emmanuel Bengio",
        "Alexandre Adam",
        "Jarrid Rector-Brooks",
        "Yashar Hezaveh",
        "Laurence Perreault-Levasseur",
        "Yoshua Bengio",
        "Glen Berseth",
        "Nikolay Malkin"
      ],
      "abstract": "This paper presents a practical application of Relative Trajectory Balance\n(RTB), a recently introduced off-policy reinforcement learning (RL) objective\nthat can asymptotically solve Bayesian inverse problems optimally. We extend\nthe original work by using RTB to train conditional diffusion model posteriors\nfrom pretrained unconditional priors for challenging linear and non-linear\ninverse problems in vision, and science. We use the objective alongside\ntechniques such as off-policy backtracking exploration to improve training.\nImportantly, our results show that existing training-free diffusion posterior\nmethods struggle to perform effective posterior inference in latent space due\nto inherent biases.",
      "tldr_zh": "这篇论文提出使用 Relative Trajectory Balance (RTB)，一种 off-policy 强化学习 (RL) 目标，来渐近地最优解决 Bayesian inverse problems。研究者扩展了原方法，通过训练条件扩散模型后验，从预训练的无条件先验出发，应用于视觉和科学领域的线性及非线性逆问题，并结合 off-policy backtracking exploration 等技术提升训练效果。结果显示，现有的训练-free 扩散后验方法因固有偏差而在潜在空间中难以进行有效的后验推断。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as workshop paper at DeLTa workshop, ICLR 2025. arXiv admin\n  note: substantial text overlap with arXiv:2405.20971",
      "pdf_url": "http://arxiv.org/pdf/2503.09746v1",
      "published_date": "2025-03-12 18:45:22 UTC",
      "updated_date": "2025-03-12 18:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:23.511446"
    },
    {
      "arxiv_id": "2503.09737v1",
      "title": "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player Evaluation System",
      "title_zh": "通过 GoalNet 揭示隐藏的关键球员：一个基于 GNN 的足球球员评估系统",
      "authors": [
        "Jacky Hao Jiang",
        "Jerry Cai",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
      "tldr_zh": "本研究揭示了传统足球分析工具（如基于 expected goals 的指标）过度强调攻击球员，而忽略了关键球员（如防守中场 Rodri 和 Palhinha）的贡献，旨在通过 GoalNet——一个基于 GNN (Graph Neural Networks) 的评估系统——公平分配球员信用。系统利用事件中心图编码空间和时间特征，结合 centrality measures 和 xT (expected threat) 变化来捕捉非得分动作，如防守和过渡发挥，并探索了 Graph Attention Networks 和 Transformer-based 模型来处理长程依赖。实验结果显示，GoalNet 在真实比赛数据上显著突出了传统指标遗漏的关键角色，提升了足球分析的全面性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4-5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09737v1",
      "published_date": "2025-03-12 18:36:55 UTC",
      "updated_date": "2025-03-12 18:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:36.410006"
    },
    {
      "arxiv_id": "2503.10707v2",
      "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models",
      "title_zh": "CALLM：通过移动日记和上下文感知语言模型理解癌症幸存者的情绪及干预机会",
      "authors": [
        "Zhiyuan Wang",
        "Katharine E. Daniel",
        "Laura E. Barnes",
        "Philip I. Chow"
      ],
      "abstract": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.",
      "tldr_zh": "本研究探讨了癌症幸存者的独特情感挑战，通过分析移动日记（mobile diaries）来理解他们的情绪状态、调节需求以及及时干预机会。研究者提出CALLM框架，这是一个基于大语言模型（LLMs）的上下文感知系统，利用检索增强生成（RAG）技术整合同伴经历和个人日记历史，以更好地解读简短日记中的情感上下文。分析407名癌症幸存者的日记数据显示，行政和健康相关情境与负面情绪相关，而休闲活动促进正面情绪。CALLM在性能上表现出色，平衡准确率达到72.96%（正面情绪）、73.29%（负面情绪）、73.72%（情绪调节愿望）和60.09%（干预可用性），优于基线模型；后续分析表明，模型置信度、日记长度和个性化期能显著提升预测准确性。该框架为个性化及时干预提供新途径，帮助改善癌症幸存者的福祉。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10707v2",
      "published_date": "2025-03-12 18:36:41 UTC",
      "updated_date": "2025-05-06 17:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:48.537999"
    },
    {
      "arxiv_id": "2503.09730v1",
      "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving",
      "title_zh": "翻译失败",
      "authors": [
        "Sara Rajaee",
        "Kumar Pratik",
        "Gabriele Cesa",
        "Arash Behboodi"
      ],
      "abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
      "tldr_zh": "本文研究了AI推理中的挑战，特别是Automated Theorem Proving (ATP)任务中，现有方法依赖reinforcement learning (RL)和rolled-out trajectories，导致计算成本高昂和奖励稀疏的问题。为解决此问题，作者提出了一种verifier-in-the-loop设计，使用自动验证器（如Lean）在每个推理步骤提供中间反馈，从而避免仅在轨迹完成时才评估正确性。实验结果显示，这种方法显著提高了模型的推理准确性和效率，为更高效的AI推理提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICLR 2025 Workshop on Reasoning and Planning for Large\n  Language Models",
      "pdf_url": "http://arxiv.org/pdf/2503.09730v1",
      "published_date": "2025-03-12 18:20:47 UTC",
      "updated_date": "2025-03-12 18:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:10:59.536342"
    },
    {
      "arxiv_id": "2503.11709v2",
      "title": "Conformal Prediction and Human Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Jessica Hullman",
        "Yifan Wu",
        "Dawei Xie",
        "Ziyang Guo",
        "Andrew Gelman"
      ],
      "abstract": "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
      "tldr_zh": "这篇论文探讨了 Conformal Prediction 在人类决策中的应用，旨在量化任意模型预测的不确定性，以辅助高风险领域如医学和金融的决策。作者提出一个决策理论框架，评估预测不确定性作为信息信号，并对比了使用校准概率与 Conformal Prediction 集的理想策略。基于人类决策理论和实证结果，论文形式化了决策者可能采用的策略，并指出 Conformal Prediction 集可能与人类-AI 决策目标存在冲突。最后，论文推荐了未来预测不确定性量化研究的方向，以更好地支持人类决策者。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11709v2",
      "published_date": "2025-03-12 18:18:09 UTC",
      "updated_date": "2025-03-18 16:16:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:11:11.868611"
    },
    {
      "arxiv_id": "2503.09721v1",
      "title": "Finding the Muses: Identifying Coresets through Loss Trajectories",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Nagaraj",
        "Deepak Ravikumar",
        "Efstathia Soufleri",
        "Kaushik Roy"
      ],
      "abstract": "Deep learning models achieve state-of-the-art performance across domains but\nface scalability challenges in real-time or resource-constrained scenarios. To\naddress this, we propose Loss Trajectory Correlation (LTC), a novel metric for\ncoreset selection that identifies critical training samples driving\ngeneralization. $LTC$ quantifies the alignment between training sample loss\ntrajectories and validation set loss trajectories, enabling the construction of\ncompact, representative subsets. Unlike traditional methods with computational\nand storage overheads that are infeasible to scale to large datasets, $LTC$\nachieves superior efficiency as it can be computed as a byproduct of training.\nOur results on CIFAR-100 and ImageNet-1k show that $LTC$ consistently achieves\naccuracy on par with or surpassing state-of-the-art coreset selection methods,\nwith any differences remaining under 1%. LTC also effectively transfers across\nvarious architectures, including ResNet, VGG, DenseNet, and Swin Transformer,\nwith minimal performance degradation (<2%). Additionally, LTC offers insights\ninto training dynamics, such as identifying aligned and conflicting sample\nbehaviors, at a fraction of the computational cost of traditional methods. This\nframework paves the way for scalable coreset selection and efficient dataset\noptimization.",
      "tldr_zh": "该研究针对深度学习模型在实时或资源受限场景下的可扩展性挑战，提出了一种新颖的Loss Trajectory Correlation (LTC)指标，用于识别核心集(coreset)，从而选择驱动泛化的关键训练样本。LTC通过量化训练样本损失轨迹与验证集损失轨迹的对齐度，构建紧凑的代表性子集，并作为训练过程的副产品实现高效计算，避免了传统方法的计算和存储开销。实验结果显示，在CIFAR-100和ImageNet-1k数据集上，LTC的准确率与最先进方法相当或优于它们（差异小于1%），并在ResNet、VGG、DenseNet和Swin Transformer等不同架构上实现良好转移，性能下降小于2%，同时提供训练动态洞见如样本行为分析，推动了可扩展的核心集选择和数据集优化。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09721v1",
      "published_date": "2025-03-12 18:11:16 UTC",
      "updated_date": "2025-03-12 18:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:11:24.532365"
    },
    {
      "arxiv_id": "2503.09712v3",
      "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
      "title_zh": "在频域中重新审视时间序列分类的后门攻击",
      "authors": [
        "Yuanmin Huang",
        "Mi Zhang",
        "Zhaoxiang Wang",
        "Wenxuan Li",
        "Min Yang"
      ],
      "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
      "tldr_zh": "本研究重新审视了针对时间序列分类（TSC）的后门攻击（backdoor attacks），强调了现有方法在频率域的局限性，如从计算机视觉借用的触发器无效，或生成模型带来的高计算复杂度。作者提出了一种增强方法FreqBack，通过频率域分析生成触发器，利用DNNs在时间序列数据中捕获频率特征的特性，实现高效且有效的攻击。实验结果显示，FreqBack在五个模型和八个数据集上实现了超过90%的攻击成功率，同时保持干净数据上的模型准确率下降不到3%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "WWW 2025 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2503.09712v3",
      "published_date": "2025-03-12 18:05:32 UTC",
      "updated_date": "2025-05-17 11:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:11:35.268082"
    },
    {
      "arxiv_id": "2503.09707v1",
      "title": "Revisiting semi-supervised learning in the era of foundation models",
      "title_zh": "在基础模型时代重新审视半监督学习",
      "authors": [
        "Ping Zhang",
        "Zheda Mai",
        "Quang-Huy Nguyen",
        "Wei-Lun Chao"
      ],
      "abstract": "Semi-supervised learning (SSL) leverages abundant unlabeled data alongside\nlimited labeled data to enhance learning. As vision foundation models (VFMs)\nincreasingly serve as the backbone of vision applications, it remains unclear\nhow SSL interacts with these pre-trained models. To address this gap, we\ndevelop new SSL benchmark datasets where frozen VFMs underperform and\nsystematically evaluate representative SSL methods. We make a surprising\nobservation: parameter-efficient fine-tuning (PEFT) using only labeled data\noften matches SSL performance, even without leveraging unlabeled data. This\nmotivates us to revisit self-training, a conceptually simple SSL baseline,\nwhere we use the supervised PEFT model to pseudo-label unlabeled data for\nfurther training. To overcome the notorious issue of noisy pseudo-labels, we\npropose ensembling multiple PEFT approaches and VFM backbones to produce more\nrobust pseudo-labels. Empirical results validate the effectiveness of this\nsimple yet powerful approach, providing actionable insights into SSL with VFMs\nand paving the way for more scalable and practical semi-supervised learning in\nthe era of foundation models.",
      "tldr_zh": "本研究重新审视了半监督学习（semi-supervised learning, SSL）在视觉基础模型（vision foundation models, VFMs）时代的作用，开发了新基准数据集来评估冻结 VFMs 的性能，并系统测试了代表性 SSL 方法。研究发现，使用参数高效微调（parameter-efficient fine-tuning, PEFT）仅靠标记数据即可达到与 SSL 相当的性能，甚至无需利用未标记数据，这挑战了传统 SSL 的假设。针对此，作者改进了 self-training 基线方法，通过监督 PEFT 模型生成伪标签（pseudo-labels），并采用集成多个 PEFT 策略和 VFM 骨干网络来减少伪标签噪声。实验结果证明了这种简单方法的有效性，为基础模型时代更可扩展的 SSL 提供了实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09707v1",
      "published_date": "2025-03-12 18:01:10 UTC",
      "updated_date": "2025-03-12 18:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:11:48.156794"
    },
    {
      "arxiv_id": "2503.09598v1",
      "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
      "title_zh": "翻译失败",
      "authors": [
        "Ruohao Guo",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
      "tldr_zh": "这篇论文调查了大型语言模型 (LLMs) 在处理隐性错误信息时的表现问题，特别是当错误假设（如“如何保护自己免受 5G 辐射”）嵌入用户查询中时。研究者创建了 ECHOMIST，这是一个基于真实人-AI 对话和社会媒体数据的首个全面基准数据集，并引入了一个新评估指标来衡量 LLMs 是否能识别和反驳错误信息，而非放大误解。通过对 GPT-4、Claude 和 Llama 等模型的实证研究，发现这些模型在检测隐性错误前提和生成准确解释方面表现极差。论文强调了在 LLM 安全研究中加强对隐性错误信息的关注，以防范潜在风险。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09598v1",
      "published_date": "2025-03-12 17:59:18 UTC",
      "updated_date": "2025-03-12 17:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:12:01.056041"
    },
    {
      "arxiv_id": "2503.09586v1",
      "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Crossman",
        "Andrew R. Plummer",
        "Chandra Sekharudu",
        "Deepak Warrier",
        "Mohammad Yekrangian"
      ],
      "abstract": "We present Auspex - a threat modeling system built using a specialized\ncollection of generative artificial intelligence-based methods that capture\nthreat modeling tradecraft. This new approach, called tradecraft prompting,\ncenters on encoding the on-the-ground knowledge of threat modelers within the\nprompts that drive a generative AI-based threat modeling system. Auspex employs\ntradecraft prompts in two processing stages. The first stage centers on\ningesting and processing system architecture information using prompts that\nencode threat modeling tradecraft knowledge pertaining to system decomposition\nand description. The second stage centers on chaining the resulting system\nanalysis through a collection of prompts that encode tradecraft knowledge on\nthreat identification, classification, and mitigation. The two-stage process\nyields a threat matrix for a system that specifies threat scenarios, threat\ntypes, information security categorizations and potential mitigations. Auspex\nproduces formalized threat model output in minutes, relative to the weeks or\nmonths a manual process takes. More broadly, the focus on bespoke tradecraft\nprompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a\nlightweight, flexible, modular, and extensible foundational system capable of\naddressing the complexity, resource, and standardization limitations of both\nexisting manual and automated threat modeling processes. In this connection, we\nestablish the baseline value of Auspex to threat modelers through an evaluation\nprocedure based on feedback collected from cybersecurity subject matter experts\nmeasuring the quality and utility of threat models generated by Auspex on real\nbanking systems. We conclude with a discussion of system performance and plans\nfor enhancements to Auspex.",
      "tldr_zh": "本研究引入Auspex，一种基于生成式AI的威胁建模系统，通过tradecraft prompting方法将威胁建模的专业知识编码到提示中，以提升效率和准确性。系统采用两阶段处理：第一阶段处理系统架构信息，包括系统分解和描述；第二阶段则通过一系列提示进行威胁识别、分类和缓解，生成详细的威胁矩阵。相比手动过程，Auspex能在几分钟内输出威胁场景、类型、安全分类和潜在缓解措施，大大缩短从数周到数月的建模时间。通过网络安全专家的反馈评估，该系统在真实银行系统上的威胁模型质量和实用性表现出色。作为一个轻量级、灵活且模块化的框架，Auspex解决了现有手动和自动化威胁建模的复杂性、资源和标准化问题，并为未来增强奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09586v1",
      "published_date": "2025-03-12 17:54:18 UTC",
      "updated_date": "2025-03-12 17:54:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:12:12.137497"
    },
    {
      "arxiv_id": "2503.09579v2",
      "title": "Cost-Optimal Grouped-Query Attention for Long-Context Modeling",
      "title_zh": "成本最优分组查询注意力用于长上下文建模",
      "authors": [
        "Yingfa Chen",
        "Yutong Wu",
        "Chenyang Song",
        "Zhen Leng Thai",
        "Xingyu Shen",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the\ncomputational cost of attention layers in large language models (LLMs).\nHowever, current GQA configurations are often suboptimal because they overlook\nhow context length influences inference cost. Since inference cost grows with\ncontext length, the most cost-efficient GQA configuration should also vary\naccordingly. In this work, we analyze the relationship among context length,\nmodel size, GQA configuration, and model loss, and introduce two innovations:\n(1) we decouple the total head size from the hidden size, enabling more\nflexible control over attention FLOPs; and (2) we jointly optimize the model\nsize and the GQA configuration to arrive at a better allocation of inference\nresources between attention layers and other components. Our analysis reveals\nthat commonly used GQA configurations are highly suboptimal for long-context\nscenarios. More importantly, we propose a recipe for deriving cost-optimal GQA\nconfigurations. Our results show that for long-context scenarios, one should\nuse fewer attention heads while scaling up model size. Configurations selected\nby our recipe can reduce both memory usage and FLOPs by more than 50% compared\nto Llama-3's GQA, with *no degradation in model capabilities*. Our findings\noffer valuable insights for designing efficient long-context LLMs. The code is\navailable at https://www.github.com/THUNLP/cost-optimal-gqa .",
      "tldr_zh": "本研究分析了Grouped-Query Attention (GQA) 在大型语言模型(LLMs)中用于减少注意力层计算成本的策略，但指出现有GQA配置忽略了上下文长度对推理成本的影响，导致在长上下文场景下不优。研究者引入两项创新：(1) 将总头大小与隐藏大小解耦，以更灵活控制注意力FLOPs；(2) 联合优化模型大小和GQA配置，实现推理资源的更好分配。结果显示，推荐的成本最优GQA配置可将内存使用和FLOPs减少超过50%，同时保持模型能力不变，为设计高效长上下文LLMs提供了关键见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09579v2",
      "published_date": "2025-03-12 17:50:42 UTC",
      "updated_date": "2025-05-20 09:31:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:12:23.764107"
    },
    {
      "arxiv_id": "2503.09573v3",
      "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
      "title_zh": "Block Diffusion：自回归与扩散语言模型之间的插值",
      "authors": [
        "Marianne Arriola",
        "Aaron Gokaslan",
        "Justin T. Chiu",
        "Zhihan Yang",
        "Zhixuan Qi",
        "Jiaqi Han",
        "Subham Sekhar Sahoo",
        "Volodymyr Kuleshov"
      ],
      "abstract": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms",
      "tldr_zh": "该论文提出了一种 block diffusion 语言模型，通过在 discrete denoising diffusion 和 autoregressive models 之间插值，克服了 diffusion 模型在 likelihood modeling 和固定长度生成上的局限性。研究引入了高效训练算法、gradient variance 估计器以及数据驱动的 noise schedules，以最小化方差并提升推理效率，包括 KV caching 和 parallel token sampling。实验结果显示，block diffusion 在语言建模基准上实现了新的 state-of-the-art 性能，并支持任意长度序列生成，提供代码和模型权重以便进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
      "pdf_url": "http://arxiv.org/pdf/2503.09573v3",
      "published_date": "2025-03-12 17:43:40 UTC",
      "updated_date": "2025-05-17 21:15:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:12:35.264892"
    },
    {
      "arxiv_id": "2503.09567v3",
      "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jinhao Liu",
        "Dengyun Peng",
        "Jiannan Guan",
        "Peng Wang",
        "Mengkang Hu",
        "Yuhang Zhou",
        "Te Gao",
        "Wanxiang Che"
      ],
      "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.",
      "tldr_zh": "这篇调查论文探讨了Long Chain-of-Thought (Long CoT)在推理大型语言模型(RLLMs)中的作用，旨在填补现有研究的空白，并将其与传统Short CoT进行区分。论文引入了一个新分类法，分析了Long CoT的关键特征，包括深度推理、广泛探索和可行反思，这些特性使模型能处理更复杂的任务并产生更高效、一致的输出。最终，论文调查了overthinking和test-time scaling等现象，识别出研究空白，并提出未来方向，如整合多模态推理和提升效率，以推动人工智能逻辑推理的发展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper are available at https://long-cot.github.io/, and Github are\n  available at\n  https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning",
      "pdf_url": "http://arxiv.org/pdf/2503.09567v3",
      "published_date": "2025-03-12 17:35:03 UTC",
      "updated_date": "2025-04-09 11:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:12:48.399056"
    },
    {
      "arxiv_id": "2503.09565v1",
      "title": "Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $μ$P Parametrization",
      "title_zh": "翻译失败",
      "authors": [
        "Zixiang Chen",
        "Greg Yang",
        "Qingyue Zhao",
        "Quanquan Gu"
      ],
      "abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "tldr_zh": "该研究探讨了$L$-Layer无限宽神经网络在$μ$P参数化下的训练动态，解决了现有方法如Neural Tangent Kernel (NTK)无法实现特征显著演化的局限。作者使用Tensor Program (TP)框架和Stochastic Gradient Descent (SGD)，证明在对激活函数的温和条件下，网络能学习线性独立的特征，这些特征大幅偏离初始值并捕获关键数据信息。结果显示，这种机制确保训练过程的任何收敛点均为全局最小，并通过真实数据集实验验证了这些理论洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 5 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09565v1",
      "published_date": "2025-03-12 17:33:13 UTC",
      "updated_date": "2025-03-12 17:33:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:13:01.041116"
    },
    {
      "arxiv_id": "2503.09669v1",
      "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sangwon Jang",
        "June Suk Choi",
        "Jaehyeong Jo",
        "Kimin Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
      "tldr_zh": "本文提出Silent Branding Attack，一种无需文本触发的数据中毒攻击，针对text-to-image diffusion models，旨在让模型在生成图像时自然包含特定品牌标志或符号。方法通过自动化算法在训练数据中重复注入视觉模式，确保标志与原始图像无缝融合，而不影响图像质量或文本对齐。实验在大型高品质图像数据集和风格个性化数据集上验证了该攻击的高成功率，即使没有特定触发词。人类评估和量化指标（如标志检测）进一步证明了攻击的隐蔽性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project page: https://silent-branding.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.09669v1",
      "published_date": "2025-03-12 17:21:57 UTC",
      "updated_date": "2025-03-12 17:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:13:13.783540"
    },
    {
      "arxiv_id": "2503.09545v1",
      "title": "The Value of Goal Commitment in Planning",
      "title_zh": "目标承诺在规划中的价值",
      "authors": [
        "Alberto Pozanco",
        "Marianela Morales",
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better",
      "tldr_zh": "本论文重新审视了目标承诺（goal commitment）在当前向前链式启发式规划器（forward chaining heuristic planners）中的价值，提出了一种编译方法，通过添加提交动作（commit actions）来强制特定目标一旦实现就保持不变，从而在搜索子树中实施目标实现顺序。这种方法能潜在减少搜索努力，但也可能引入死端状态，导致算法扩展不持久目标的节点。实验结果显示，该改革后的任务适合最先进的敏捷规划器（agile planners），使它们能够找到更好的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09545v1",
      "published_date": "2025-03-12 17:00:37 UTC",
      "updated_date": "2025-03-12 17:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:13:26.492128"
    },
    {
      "arxiv_id": "2503.09538v1",
      "title": "Differentially Private Equilibrium Finding in Polymatrix Games",
      "title_zh": "差分隐私的多矩阵博弈均衡求",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
      ],
      "abstract": "We study equilibrium finding in polymatrix games under differential privacy\nconstraints. To start, we show that high accuracy and asymptotically vanishing\ndifferential privacy budget (as the number of players goes to infinity) cannot\nbe achieved simultaneously under either of the two settings: (i) We seek to\nestablish equilibrium approximation guarantees in terms of Euclidean distance\nto the equilibrium set, and (ii) the adversary has access to all communication\nchannels. Then, assuming the adversary has access to a constant number of\ncommunication channels, we develop a novel distributed algorithm that recovers\nstrategies with simultaneously vanishing Nash gap (in expected utility, also\nreferred to as exploitability and privacy budget as the number of players\nincreases.",
      "tldr_zh": "该研究探讨了在多矩阵游戏(polymatrix games)中寻找均衡时应用差分隐私(differentially private)的挑战。研究首先证明，在两种设置下（即以欧氏距离(Euclidean distance)衡量均衡近似，或对手访问所有通信渠道），无法同时实现高准确性和渐进消失的隐私预算(privacy budget)当玩家数量增加。接着，假设对手仅访问恒定数量的通信渠道，提出了一种新颖的分布式算法，能够使Nash间隙(Nash gap，也称exploitability)与隐私预算在玩家数量增加时同时趋于零，从而提升游戏均衡的隐私保护和实用性。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09538v1",
      "published_date": "2025-03-12 16:54:23 UTC",
      "updated_date": "2025-03-12 16:54:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:13:37.985290"
    },
    {
      "arxiv_id": "2503.09537v1",
      "title": "GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals",
      "title_zh": "翻译失败",
      "authors": [
        "Shuokang Huang",
        "Julie A. McCann"
      ],
      "abstract": "Human pose estimation (HPE) detects the positions of human body joints for\nvarious applications. Compared to using cameras, HPE using radio frequency (RF)\nsignals is non-intrusive and more robust to adverse conditions, exploiting the\nsignal variations caused by human interference. However, existing studies focus\non single-domain HPE confined by domain-specific confounders, which cannot\ngeneralize to new domains and result in diminished HPE performance.\nSpecifically, the signal variations caused by different human body parts are\nentangled, containing subject-specific confounders. RF signals are also\nintertwined with environmental noise, involving environment-specific\nconfounders. In this paper, we propose GenHPE, a 3D HPE approach that generates\ncounterfactual RF signals to eliminate domain-specific confounders. GenHPE\ntrains generative models conditioned on human skeleton labels, learning how\nhuman body parts and confounders interfere with RF signals. We manipulate\nskeleton labels (i.e., removing body parts) as counterfactual conditions for\ngenerative models to synthesize counterfactual RF signals. The differences\nbetween counterfactual signals approximately eliminate domain-specific\nconfounders and regularize an encoder-decoder model to learn domain-independent\nrepresentations. Such representations help GenHPE generalize to new\nsubjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three\npublic datasets from WiFi, ultra-wideband, and millimeter wave. Experimental\nresults show that GenHPE outperforms state-of-the-art methods and reduces\nestimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for\ncross-environment HPE.",
      "tldr_zh": "该论文提出GenHPE，一种基于生成式逆事实(counterfactuals)的3D Human Pose Estimation (HPE) 方法，使用Radio Frequency (RF) 信号进行非侵入式人体关节位置检测，以解决现有方法受域特定混淆因素（如主体特异性和环境噪声）影响而泛化能力不足的问题。GenHPE通过训练生成模型，条件于人体骨骼标签，合成逆事实RF信号（如移除特定身体部位），从而消除混淆因素并学习域独立表示，帮助模型泛化到新主体或环境。实验在WiFi、超宽带和毫米波三个公共数据集上表明，GenHPE优于现有方法，将交叉主体HPE的估计误差降低高达52.2mm，并将交叉环境HPE的误差降低10.6mm。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09537v1",
      "published_date": "2025-03-12 16:53:58 UTC",
      "updated_date": "2025-03-12 16:53:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:13:50.608716"
    },
    {
      "arxiv_id": "2503.09535v1",
      "title": "Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging",
      "title_zh": "评估 Transformer ",
      "authors": [
        "Minjae Chung",
        "Jong Bum Won",
        "Ganghyun Kim",
        "Yujin Kim",
        "Utku Ozbulak"
      ],
      "abstract": "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
      "tldr_zh": "本研究评估了 Vision Transformers (ViTs) 在医疗成像中的注意力图 (attention maps) 作为视觉解释方法的有效性，旨在解决模型的可解释性问题。研究者使用四个数据集（包括结肠息肉、乳腺肿瘤、食道炎症和骨折及植入物识别）进行大规模实验，比较了监督和自监督预训练 ViTs 的注意力图与其他常见方法，如 GradCAM。结果显示，注意力图在某些条件下表现出色并优于 GradCAM，但整体被 transformer-specific 解释方法超越，其解释能力因上下文而异，可能无法提供医疗决策所需的全面见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in MICCAI 2024 Workshop on Interpretability\n  of Machine Intelligence in Medical Image Computing (iMIMIC)",
      "pdf_url": "http://arxiv.org/pdf/2503.09535v1",
      "published_date": "2025-03-12 16:52:52 UTC",
      "updated_date": "2025-03-12 16:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:14:04.434396"
    },
    {
      "arxiv_id": "2503.09527v1",
      "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Chen",
        "Pi Bu",
        "Yingyao Wang",
        "Xinyi Wang",
        "Ziming Wang",
        "Jie Guo",
        "Yingxiu Zhao",
        "Qi Zhu",
        "Jun Song",
        "Siran Yang",
        "Jiamang Wang",
        "Bo Zheng"
      ],
      "abstract": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/.",
      "tldr_zh": "本文提出 CombatVLA，一种高效的 Vision-Language-Action (VLA) 模型，针对 3D Action Role-Playing Games (ARPG) 中的战斗任务，解决实时决策、高分辨率感知和动态战术推理的挑战。该模型使用由动作追踪器收集的视频-动作对数据，以 action-of-thought (AoT) 序列格式进行训练，并通过截断 AoT 策略无缝集成到动作执行框架中，实现高效推理。实验结果显示，CombatVLA 在战斗理解基准上超越所有现有模型，实现 50 倍的游戏战斗加速，且任务成功率高于人类玩家。作者将开源所有资源，包括动作追踪器、数据集、基准、模型权重、训练代码和框架实现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09527v1",
      "published_date": "2025-03-12 16:42:26 UTC",
      "updated_date": "2025-03-12 16:42:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:14:15.014437"
    },
    {
      "arxiv_id": "2503.09521v1",
      "title": "PairVDN - Pair-wise Decomposed Value Functions",
      "title_zh": "PairVDN - 成对分解的价值函数",
      "authors": [
        "Zak Buzzard"
      ],
      "abstract": "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https://github.com/zzbuzzard/PairVDN.",
      "tldr_zh": "这篇论文针对合作多智能体环境中深度 Q-learning 的挑战（如联合行动空间指数增长、非平稳环境和信用分配问题），提出了一种新方法 PairVDN，将价值函数分解为成对（pair-wise）函数，而不是传统的每个智能体（per-agent）函数，从而提高了表达性。不同于 VDN 和 QMIX 等方法，PairVDN 可以表示不能用单调组合表示的价值函数，并采用更复杂的动态规划最大化算法来确保效率。在新的 Box Jump 环境中，实验结果显示 PairVDN 比基线模型性能更优，并开源了代码以供进一步研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09521v1",
      "published_date": "2025-03-12 16:38:22 UTC",
      "updated_date": "2025-03-12 16:38:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:14:25.961099"
    },
    {
      "arxiv_id": "2503.10706v1",
      "title": "SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?",
      "title_zh": "SciFi-Benchmark：AI驱动的机器人会在科幻文学中",
      "authors": [
        "Pierre Sermanet",
        "Anirudha Majumdar",
        "Vikas Sindhwani"
      ],
      "abstract": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a LLM's recollection of each key\nmoment to generate questions in similar situations, the decisions made by the\nagent, and alternative decisions it could have made (good or bad). We then\nmeasure an approximation of how well models align with human values on a set of\nhuman-voted answers. We also generate rules that can be automatically improved\nvia amendment process in order to generate the first Sci-Fi inspired\nconstitutions for promoting ethical behavior in AIs and robots in the real\nworld. Our first finding is that modern LLMs paired with constitutions turn out\nto be well-aligned with human values (95.8%), contrary to unsettling decisions\ntypically made in SciFi (only 21.2% alignment). Secondly, we find that\ngenerated constitutions substantially increase alignment compared to the base\nmodel (79.4% to 95.8%), and show resilience to an adversarial prompt setting\n(23.3% to 92.3%). Additionally, we find that those constitutions are among the\ntop performers on the ASIMOV Benchmark which is derived from real-world images\nand hospital injury reports. Sci-Fi-inspired constitutions are thus highly\naligned and applicable in real-world situations. We release SciFi-Benchmark: a\nlarge-scale dataset to advance robot ethics and safety research. It comprises\n9,056 questions and 53,384 answers, in addition to a smaller human-labeled\nevaluation set. Data is available at https://scifi-benchmark.github.io",
      "tldr_zh": "该研究提出SciFi-Benchmark，一个大规模基准，用于评估AI驱动机器人是否与人类价值观一致，通过分析824部科幻文学中的关键决策时刻。研究利用LLM生成相关问题、决定和备选方案，并通过人类投票评估模型的alignment（一致性），同时开发可改进的Sci-Fi启发式宪法以提升AI伦理行为。结果显示，现代LLMs结合宪法时，与人类价值观的alignment高达95.8%，远超科幻文学中的21.2%，并在对抗性提示和ASIMOV Benchmark上表现出色。该基准数据集包括9,056个问题和53,384个答案，已公开以推进机器人伦理和安全研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10706v1",
      "published_date": "2025-03-12 16:35:51 UTC",
      "updated_date": "2025-03-12 16:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:14:37.658944"
    },
    {
      "arxiv_id": "2503.09516v3",
      "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Jin",
        "Hansi Zeng",
        "Zhenrui Yue",
        "Jinsung Yoon",
        "Sercan Arik",
        "Dong Wang",
        "Hamed Zamani",
        "Jiawei Han"
      ],
      "abstract": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1.",
      "tldr_zh": "该论文提出Search-R1框架，利用强化学习（RL）训练大型语言模型（LLMs），使它们能够自主生成多个搜索查询，并在逐步推理过程中进行实时检索，以优化与搜索引擎的交互。Search-R1通过多轮搜索交互、检索token掩码和基于结果的奖励函数来稳定RL训练，提升LLMs的推理效率和准确性。在七个问答数据集上的实验显示，Search-R1比RAG基线模型提高了41%（Qwen2.5-7B）和20%（Qwen2.5-3B）的性能，并提供了RL优化方法、LLM选择和响应长度动态的实证洞见。开源代码和模型检查点可从GitHub获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09516v3",
      "published_date": "2025-03-12 16:26:39 UTC",
      "updated_date": "2025-04-08 14:03:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:14:50.294021"
    },
    {
      "arxiv_id": "2503.09513v1",
      "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
      "title_zh": "RESTRAIN：基于强化学习的触发-动作 IoT 环境安全框架",
      "authors": [
        "Md Morshed Alam",
        "Lokesh Chandra Das",
        "Sandip Roy",
        "Sachin Shetty",
        "Weichao Wang"
      ],
      "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead.",
      "tldr_zh": "本论文提出RESTRAIN，一种基于Reinforcement Learning的多智能体在线防御框架，旨在应对Trigger-Action IoT环境中的远程注入攻击，这些攻击通过注入假事件条件来触发未授权动作。RESTRAIN在运行时分析攻击行为，并利用强化学习优化防御策略，以确保符合IoT网络的安全需求，同时保持平台无关性。实验结果显示，该框架能有效实时抵御复杂动态攻击，最大化安全收益并最小化计算开销。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09513v1",
      "published_date": "2025-03-12 16:23:14 UTC",
      "updated_date": "2025-03-12 16:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:15:01.621814"
    },
    {
      "arxiv_id": "2503.09504v1",
      "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Bakary Badjie",
        "José Cecílio",
        "António Casimiro"
      ],
      "abstract": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks.",
      "tldr_zh": "本研究针对Mixture-of-Experts (MoE)模型在深度学习中的复杂架构和图像分类性能问题，提出了一种Double-Stage Feature-Level Clustering-Based Mixture of Experts (DFCP-MoE)框架，以缓解噪声和异常值的影响。框架包括输入特征提取、特征级聚类以及高效的伪标签策略，利用少量标记数据标记大量未标记输入，并通过条件端到端联合训练方法提升专家的专业化。实验结果显示，DFCP-MoE在三个多类分类基准数据集上有效捕捉输入空间多样性，并实现与传统MoE和密集模型竞争的推理性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "14 Pages, 1 Figure, and 3 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09504v1",
      "published_date": "2025-03-12 16:13:50 UTC",
      "updated_date": "2025-03-12 16:13:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:15:13.906146"
    },
    {
      "arxiv_id": "2503.09501v2",
      "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Wan",
        "Yunxiang Li",
        "Yan Song",
        "Hanjing Wang",
        "Linyi Yang",
        "Mark Schmidt",
        "Jun Wang",
        "Weinan Zhang",
        "Shuyue Hu",
        "Ying Wen"
      ],
      "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
      "tldr_zh": "该研究提出 ReMA 框架，利用 Multi-Agent Reinforcement Learning (MARL) 来帮助 Large Language Models (LLMs) 学习 meta-thinking，从而提升模型的推理性能。ReMA 将推理过程分解为两个层次的智能体：高层 meta-thinking 智能体负责生成战略监督和计划，低层推理智能体执行细节，通过迭代强化学习和对齐目标实现智能体间的协作，提高泛化和鲁棒性。实验结果显示，ReMA 在复杂推理任务（如数学基准和 LLM-as-a-Judge 基准）上优于单智能体 RL 基线，消融研究进一步揭示了 meta-thinking 如何动态增强 LLMs 的推理能力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09501v2",
      "published_date": "2025-03-12 16:05:31 UTC",
      "updated_date": "2025-03-14 05:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:15:27.854988"
    },
    {
      "arxiv_id": "2503.09499v2",
      "title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Xu",
        "Daoyuan Chen",
        "Zhenqing Ling",
        "Yaliang Li",
        "Ying Shen"
      ],
      "abstract": "Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.",
      "tldr_zh": "本研究探讨了在思考中心微调中，问题合成的关键因素，提出MindGYM框架，通过自我生成认知引导数据来提升大型基础模型的结构化思考能力。MindGYM包括Cognitive Thinking Process Injection（注入高级推理目标）、Seed Single-Hop Question Synthesis（生成多样语义的原子问题）和Challenging Multi-Hop QA Synthesis（基于种子创建复杂多跳问题）三个模块，确保数据质量高且方差低。实验结果显示，该框架合成的数据比基线来源平均质量高16.7%且方差低67.91%，并在六个推理基准上实现高达16%的性能提升，如在MathVision上仅用400个样本就取得显著改进。MindGYM强调自我挑战机制的可行性，能减少人为干预和资源需求，促进模型的自我演化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09499v2",
      "published_date": "2025-03-12 16:03:03 UTC",
      "updated_date": "2025-05-22 16:47:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:15:40.055425"
    },
    {
      "arxiv_id": "2503.09447v1",
      "title": "Online Language Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Saimouli Katragadda",
        "Cho-Ying Wu",
        "Yuliang Guo",
        "Xinyu Huang",
        "Guoquan Huang",
        "Liu Ren"
      ],
      "abstract": "To enable AI agents to interact seamlessly with both humans and 3D\nenvironments, they must not only perceive the 3D world accurately but also\nalign human language with 3D spatial representations. While prior work has made\nsignificant progress by integrating language features into geometrically\ndetailed 3D scene representations using 3D Gaussian Splatting (GS), these\napproaches rely on computationally intensive offline preprocessing of language\nfeatures for each input image, limiting adaptability to new environments. In\nthis work, we introduce Online Language Splatting, the first framework to\nachieve online, near real-time, open-vocabulary language mapping within a\n3DGS-SLAM system without requiring pre-generated language features. The key\nchallenge lies in efficiently fusing high-dimensional language features into 3D\nrepresentations while balancing the computation speed, memory usage, rendering\nquality and open-vocabulary capability. To this end, we innovatively design:\n(1) a high-resolution CLIP embedding module capable of generating detailed\nlanguage feature maps in 18ms per frame, (2) a two-stage online auto-encoder\nthat compresses 768-dimensional CLIP features to 15 dimensions while preserving\nopen-vocabulary capabilities, and (3) a color-language disentangled\noptimization approach to improve rendering quality. Experimental results show\nthat our online method not only surpasses the state-of-the-art offline methods\nin accuracy but also achieves more than 40x efficiency boost, demonstrating the\npotential for dynamic and interactive AI applications.",
      "tldr_zh": "这篇论文提出了 Online Language Splatting 框架，旨在让 AI 代理实现实时与人类和 3D 环境的互动，通过将人类语言与 3D 空间表示对齐，而无需依赖计算密集的离线预处理。关键创新包括：(1) 高分辨率 CLIP 嵌入模块，每帧在 18ms 内生成详细语言特征图；(2) 两阶段在线自动编码器，将 768 维 CLIP 特征压缩至 15 维，同时保留开放词汇能力；以及 (3) 颜色-语言分离优化方法，以提升渲染质量和效率。实验结果显示，该框架在 3DGS-SLAM 系统内超越最先进的离线方法，在准确性上表现优异，并实现了超过 40 倍的效率提升，适用于动态交互 AI 应用。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09447v1",
      "published_date": "2025-03-12 14:49:24 UTC",
      "updated_date": "2025-03-12 14:49:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:15:51.475214"
    },
    {
      "arxiv_id": "2503.09446v2",
      "title": "Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models",
      "title_zh": "稀疏自编码器作为零样本分类器，用于文本到图像扩散模型中的概念擦除",
      "authors": [
        "Zhihua Tian",
        "Sirun Nan",
        "Ming Xu",
        "Shengfang Zhai",
        "Wenjie Qu",
        "Jian Liu",
        "Kui Ren",
        "Ruoxi Jia",
        "Jiaheng Zhang"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps://github.com/NANSirun/Interpret-then-deactivate.",
      "tldr_zh": "本文提出 Interpret then Deactivate (ItD) 框架，用于在 Text-to-Image Diffusion Models 中精确擦除 unwanted concepts，同时保持正常生成任务的性能。ItD 利用 Sparse Autoencoder (SAE) 将每个概念解释为多个特征的组合，并通过永久停用相关特征，将 SAE 作为 Zero-Shot Classifier 来识别输入提示中是否包含目标概念，从而实现选择性概念移除。该框架无需进一步训练即可扩展到多个概念的擦除，并在名人身份、艺术风格和显式内容等实验中证明了其有效性，不干扰正常概念生成，且对对抗性提示具有鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09446v2",
      "published_date": "2025-03-12 14:46:40 UTC",
      "updated_date": "2025-03-18 09:12:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:16:02.708544"
    },
    {
      "arxiv_id": "2503.09445v2",
      "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoda Yang",
        "JunYu Lu",
        "Hongshun Qiu",
        "Sijing Li",
        "Hao Li",
        "Shengpeng Ji",
        "Xudong Tang",
        "Jiayang Xu",
        "Jiaqi Duan",
        "Ziyue Jiang",
        "Cong Lin",
        "Sihang Cai",
        "Zejian Xie",
        "Zhuoyang Song",
        "Songxin Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures\nhave emerged as a pivotal paradigm in multimodal understanding, offering a\npowerful framework for integrating visual and linguistic information. However,\nthe increasing complexity and diversity of tasks present significant challenges\nin coordinating load balancing across heterogeneous visual experts, where\noptimizing one specialist's performance often compromises others' capabilities.\nTo address task heterogeneity and expert load imbalance, we propose Astrea, a\nnovel multi-expert collaborative VLM architecture based on progressive\npre-alignment. Astrea introduces three key innovations: 1) A heterogeneous\nexpert coordination mechanism that integrates four specialized models\n(detection, segmentation, classification, captioning) into a comprehensive\nexpert matrix covering essential visual comprehension elements; 2) A dynamic\nknowledge fusion strategy featuring progressive pre-alignment to harmonize\nexperts within the VLM latent space through contrastive learning, complemented\nby probabilistically activated stochastic residual connections to preserve\nknowledge continuity; 3) An enhanced optimization framework utilizing momentum\ncontrastive learning for long-range dependency modeling and adaptive weight\nallocators for real-time expert contribution calibration. Extensive evaluations\nacross 12 benchmark tasks spanning VQA, image captioning, and cross-modal\nretrieval demonstrate Astrea's superiority over state-of-the-art models,\nachieving an average performance gain of +4.7\\%. This study provides the first\nempirical demonstration that progressive pre-alignment strategies enable VLMs\nto overcome task heterogeneity limitations, establishing new methodological\nfoundations for developing general-purpose multimodal agents.",
      "tldr_zh": "本文提出 Astrea，一种基于 Mixture-of-Experts (MoE) 的视觉语言模型 (VLMs)，通过 progressive pre-alignment 策略解决任务异质性和专家负载不平衡问题。Astrea 的关键创新包括异质专家协调机制（整合检测、分割、分类和描述模型）、动态知识融合策略（利用对比学习和随机残差连接保持知识连续性），以及增强优化框架（采用 momentum contrastive learning 和自适应权重分配器）。在 12 个基准任务（如 VQA、图像描述和跨模态检索）上，Astrea 比最先进模型平均提升 4.7%，首次证明 progressive pre-alignment 可为通用多模态代理建立新方法基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09445v2",
      "published_date": "2025-03-12 14:44:52 UTC",
      "updated_date": "2025-04-01 03:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:16:16.158492"
    },
    {
      "arxiv_id": "2503.09436v2",
      "title": "PromptMap: An Alternative Interaction Style for AI-Based Image Generation",
      "title_zh": "PromptMap：一种用于基于 AI 的图像生成的替代交互风格",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Paweł W. Woźniak",
        "Julia Dominiak",
        "Andrzej Romanowski",
        "Jakob Karolus",
        "Stanislav Frolov"
      ],
      "abstract": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
      "tldr_zh": "该研究提出 PromptMap，一种新型交互风格，用于文本到图像 AI 生成，帮助新手用户克服编写有效提示的困难。PromptMap 通过地图视图和语义缩放，让用户探索大量合成提示，并按语义相似性分组图像，提供相关示例以辅助发现。实验包括在线研究 (n=60) 和定性研究 (n=12)，结果显示它能有效支持用户创建提示，并证明了使用 LLMs 生成示例集合的可行性。该工作为不熟悉提示的用户提供了更易用的交互方式，以实现满意的图像输出。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to the 30th International Conference on Intelligent User\n  Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy ; Link to code\n  https://github.com/Bill2462/prompt-map",
      "pdf_url": "http://arxiv.org/pdf/2503.09436v2",
      "published_date": "2025-03-12 14:31:50 UTC",
      "updated_date": "2025-04-03 07:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:16:26.301945"
    },
    {
      "arxiv_id": "2503.09433v2",
      "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Richard A. Dubniczky",
        "Krisztofer Zoltán Horvát",
        "Tamás Bisztray",
        "Mohamed Amine Ferrag",
        "Lucas C. Cordeiro",
        "Norbert Tihanyi"
      ],
      "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
      "tldr_zh": "本论文引入 CASTLE 基准框架和数据集，用于评估静态代码分析工具、LLMs 和正式验证工具在 CWE 检测方面的能力。研究团队评估了 13 个静态分析工具、10 个 LLMs 和 2 个正式验证工具，使用一个包含 250 个微基准程序的自定义数据集，并提出 CASTLE Score 作为公平的评估指标。结果显示，ESBMC 工具能减少假阳性但仅限于模型检查范围，而静态分析工具存在高假阳性问题，增加了开发者手动验证负担；LLMs 在小代码片段中表现出色，但代码规模增大时准确率下降并出现幻觉。这些发现表明，LLMs 可能在未来代码完成框架中发挥关键作用，提供实时漏洞预防指导。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09433v2",
      "published_date": "2025-03-12 14:30:05 UTC",
      "updated_date": "2025-03-31 16:07:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:16:40.917189"
    },
    {
      "arxiv_id": "2503.09427v2",
      "title": "Language-Enhanced Representation Learning for Single-Cell Transcriptomics",
      "title_zh": "语言增强的单细胞转录组学表示学习",
      "authors": [
        "Yaorui Shi",
        "Jiaqi Yang",
        "Changhao Nai",
        "Sihang Li",
        "Junfeng Fang",
        "Xiang Wang",
        "Zhiyuan Liu",
        "Yang Zhang"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular\nheterogeneity. Recent advancements leverage single-cell large language models\n(scLLMs) for effective representation learning. These models focus exclusively\non transcriptomic data, neglecting complementary biological knowledge from\ntextual descriptions. To overcome this limitation, we propose scMMGPT, a novel\nmultimodal framework designed for language-enhanced representation learning in\nsingle-cell transcriptomics. Unlike existing methods, scMMGPT employs robust\ncell representation extraction, preserving quantitative gene expression data,\nand introduces an innovative two-stage pre-training strategy combining\ndiscriminative precision with generative flexibility. Extensive experiments\ndemonstrate that scMMGPT significantly outperforms unimodal and multimodal\nbaselines across key downstream tasks, including cell annotation and\nclustering, and exhibits superior generalization in out-of-distribution\nscenarios.",
      "tldr_zh": "单细胞 RNA 测序 (scRNA-seq) 提供了细胞异质性的详细洞见，但现有单细胞大语言模型 (scLLMs) 只关注转录组数据，忽略了文本描述的补充生物知识。为解决这一问题，我们提出 scMMGPT，一种创新的多模态框架，用于语言增强的单细胞转录组表示学习。该框架通过稳健的细胞表示提取保留定量基因表达数据，并采用两阶段预训练策略，结合判别精度和生成灵活性。实验证明，scMMGPT 在细胞注释、聚类等下游任务上显著优于单模态和多模态基线，并在分布外场景中展现出色的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09427v2",
      "published_date": "2025-03-12 14:26:16 UTC",
      "updated_date": "2025-05-19 08:02:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:16:51.136256"
    },
    {
      "arxiv_id": "2503.09409v1",
      "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
      "title_zh": "翻译失败",
      "authors": [
        "Claudius Kienle",
        "Benjamin Alt",
        "Finn Schneider",
        "Tobias Pertlwieser",
        "Rainer Jäkel",
        "Rania Rayyes"
      ],
      "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT.",
      "tldr_zh": "本研究提出一个AI-based framework，用于自动化汽车装配中的线束安装任务，通过整合force control和deep visuotactile learning来实现精确的连接器对接。该框架利用一阶优化在multimodal transformer架构上优化搜索和插入策略，并基于visual、tactile和proprioceptive数据进行训练，同时设计了一个自动化数据收集和优化管道，以减少对机器学习专业知识的需求。实验结果显示，在中心控制台装配任务上，该框架显著提高了循环时间和鲁棒性，与传统机器人编程方法相比性能提升明显，为工业机器人应用提供了可审计和认证的优化程序。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "68T40",
        "I.2; J.2"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering",
      "pdf_url": "http://arxiv.org/pdf/2503.09409v1",
      "published_date": "2025-03-12 13:59:26 UTC",
      "updated_date": "2025-03-12 13:59:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:17:02.503747"
    },
    {
      "arxiv_id": "2503.09403v2",
      "title": "Multi-Agent Image Restoration",
      "title_zh": "多智能体图像恢复",
      "authors": [
        "Xu Jiang",
        "Gehui Li",
        "Bin Chen",
        "Jian Zhang"
      ],
      "abstract": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable.",
      "tldr_zh": "本研究针对图像恢复（IR）中复杂混合退化的挑战，提出了一种新型 Multi-Agent 方法 MAIR，以解决现有系统如 RestoreAgent 和 AgenticIR 的性能不足和低效率问题。MAIR 基于真实世界退化先验，将退化分为 scene、imaging 和 compression 三类，并采用三阶段恢复框架，包括一个 scheduler 负责整体规划和多个 experts 专攻特定退化，从而最小化搜索空间、减少尝试努力并降低推理成本。实验结果显示，MAIR 在合成和真实数据集上比之前代理 IR 系统取得了竞争性性能和更高的效率，并通过注册机制便于新工具的集成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09403v2",
      "published_date": "2025-03-12 13:53:57 UTC",
      "updated_date": "2025-03-17 07:34:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:17:25.712306"
    },
    {
      "arxiv_id": "2503.09399v1",
      "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
      "title_zh": "ForAug：重组前景和背景以改善",
      "authors": [
        "Tobias Christian Nauen",
        "Brian Moser",
        "Federico Raue",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved\nstate-of-the-art performance in large-scale image classification. However, they\noften require large amounts of data and can exhibit biases that limit their\nrobustness and generalizability. This paper introduces ForAug, a novel data\naugmentation scheme that addresses these challenges and explicitly includes\ninductive biases, which commonly are part of the neural network architecture,\ninto the training data. ForAug is constructed by using pretrained foundation\nmodels to separate and recombine foreground objects with different backgrounds,\nenabling fine-grained control over image composition during training. It thus\nincreases the data diversity and effective number of training samples. We\ndemonstrate that training on ForNet, the application of ForAug to ImageNet,\nsignificantly improves the accuracy of ViTs and other architectures by up to\n4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.\nImportantly, ForAug enables novel ways of analyzing model behavior and\nquantifying biases. Namely, we introduce metrics for background robustness,\nforeground focus, center bias, and size bias and show that training on ForNet\nsubstantially reduces these biases compared to training on ImageNet. In\nsummary, ForAug provides a valuable tool for analyzing and mitigating biases,\nenabling the development of more robust and reliable computer vision models.\nOur code and dataset are publicly available at https://github.com/tobna/ForAug.",
      "tldr_zh": "本研究提出ForAug，一种新型数据增强方案，通过使用预训练基础模型分离并重新组合前景对象和不同背景，旨在提升Vision Transformers (ViTs)在图像分类中的性能并缓解偏差问题，从而增加数据多样性和有效训练样本。ForAug应用于ImageNet创建了ForNet数据集，结果显示ViTs和其他架构的准确率在ImageNet上提高高达4.5%，在下游任务上提高7.3%。此外，该方法引入了新的指标，包括背景鲁棒性、前景焦点、中心偏差和大小偏差，用于分析模型行为，并证明训练于ForNet可显著减少这些偏差。总之，ForAug为开发更鲁棒的计算机视觉模型提供了宝贵工具，并已公开代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T45",
        "I.2.10; I.2.6; I.4.6"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09399v1",
      "published_date": "2025-03-12 13:49:45 UTC",
      "updated_date": "2025-03-12 13:49:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:17:27.176971"
    },
    {
      "arxiv_id": "2503.17374v1",
      "title": "Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence",
      "title_zh": "Intanify AI 平台：用于自动知识产权审计和尽职调查",
      "authors": [
        "Viktor Dorfler",
        "Dylan Dryden",
        "Viet Lee"
      ],
      "abstract": "In this paper we introduce a Platform created in order to support SMEs'\nendeavor to extract value from their intangible assets effectively. To\nimplement the Platform, we developed five knowledge bases using a\nknowledge-based ex-pert system shell that contain knowledge from intangible\nas-set consultants, patent attorneys and due diligence lawyers. In order to\noperationalize the knowledge bases, we developed a \"Rosetta Stone\", an\ninterpreter unit for the knowledge bases outside the shell and embedded in the\nplat-form. Building on the initial knowledge bases we have created a system of\nred flags, risk scoring, and valuation with the involvement of the same\nexperts; these additional systems work upon the initial knowledge bases and\ntherefore they can be regarded as meta-knowledge-representations that take the\nform of second-order knowledge graphs. All this clever technology is dressed up\nin an easy-to-handle graphical user interface that we will showcase at the\nconference. The initial platform was finished mid-2024; therefore, it qualifies\nas an \"emerging application of AI\" and \"deployable AI\", while development\ncontinues. The two firms that provided experts for developing the knowledge\nbases obtained a white-label version of the product (i.e. it runs under their\nown brand \"powered by Intanify\"), and there are two completed cases.",
      "tldr_zh": "该研究介绍了Intanify AI Platform，一种嵌入式AI系统，旨在自动化知识产权(IP)审计和尽职调查，帮助中小企业有效提取无形资产价值。该平台基于五个知识库构建，使用知识-based expert system shell，并开发了Rosetta Stone作为解释器，以操作化这些知识库；此外，还创建了红旗系统、风险评分和估值系统，这些作为meta-knowledge-representations（元知识表示）形式存在于第二-order knowledge graphs中。平台配备了易用的图形用户界面，已于2024年中完成部署，并通过两家提供专家的公司获得白标版本，实际应用包括两个完成案例。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages, 4 figures, presented on 3rd March at the AAAI conference,\n  Deployable AI workshop. Paper location can be seen at\n  https://drive.google.com/file/d/1P2oWASX0hkYFHCATIjiWhX9UXfWOBYDQ/view For\n  associated video, see\n  https://drive.google.com/file/d/1eLPmP4kNRjNKXRd1dLKF_P2vNazRqXF3/view",
      "pdf_url": "http://arxiv.org/pdf/2503.17374v1",
      "published_date": "2025-03-12 13:44:44 UTC",
      "updated_date": "2025-03-12 13:44:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:17:38.489955"
    },
    {
      "arxiv_id": "2503.09396v1",
      "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jiatong Xia",
        "Lingqiao Liu"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in\nsynthesizing novel views after training on a given set of viewpoints. However,\nits rendering quality deteriorates when the synthesized view deviates\nsignificantly from the training views. This decline occurs due to (1) the\nmodel's difficulty in generalizing to out-of-distribution scenarios and (2)\nchallenges in interpolating fine details caused by substantial resolution\nchanges and occlusions. A notable case of this limitation is close-up view\ngeneration--producing views that are significantly closer to the object than\nthose in the training set. To tackle this issue, we propose a novel approach\nfor close-up view generation based by progressively training the 3DGS model\nwith self-generated data. Our solution is based on three key ideas. First, we\nleverage the See3D model, a recently introduced 3D-aware generative model, to\nenhance the details of rendered views. Second, we propose a strategy to\nprogressively expand the ``trust regions'' of the 3DGS model and update a set\nof reference views for See3D. Finally, we introduce a fine-tuning strategy to\ncarefully update the 3DGS model with training data generated from the above\nschemes. We further define metrics for close-up views evaluation to facilitate\nbetter research on this problem. By conducting evaluations on specifically\nselected scenarios for close-up views, our proposed approach demonstrates a\nclear advantage over competitive solutions.",
      "tldr_zh": "该研究针对 3D Gaussian Splatting (3DGS) 在合成与训练视图偏差大的新视图时存在的泛化困难和细节插值问题，特别关注 close-up 视图生成质量的下降。作者提出 Close-up-GS 方法，通过 progressive self-training 逐步训练模型，包括利用 See3D 模型增强渲染细节、扩展模型的 “trust regions” 并更新参考视图集，以及引入细调策略以自生成数据优化 3DGS。实验结果显示，该方法在特定 close-up 场景中比竞争方案表现出显著优势，并定义了新的评估指标以推动相关研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09396v1",
      "published_date": "2025-03-12 13:44:00 UTC",
      "updated_date": "2025-03-12 13:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:17:53.545892"
    },
    {
      "arxiv_id": "2503.09382v1",
      "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jiani Huang",
        "Shijie Wang",
        "Liang-bo Ning",
        "Wenqi Fan",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Qing Li"
      ],
      "abstract": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
      "tldr_zh": "该论文探讨了传统推荐系统（RecSys）的局限性，即难以泛化到交互式新任务，并提出 RecBench+ 数据集基准，用于评估大语言模型（LLMs）作为个性化推荐助手的性能。RecBench+ 包含多样化的用户查询，包括硬条件、软偏好和不同难度级别，以更好地模拟真实推荐场景。实验结果表明，LLMs 具备初步的推荐助手能力，更擅长处理显式条件查询，但对需要推理或包含误导信息的查询面临挑战；数据集已开源以促进进一步研究。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09382v1",
      "published_date": "2025-03-12 13:28:23 UTC",
      "updated_date": "2025-03-12 13:28:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:18:02.656370"
    },
    {
      "arxiv_id": "2503.09378v1",
      "title": "Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Fangzheng Qi",
        "Zhenjie Hou",
        "En Lin",
        "Xing Li",
        "iuzhen Liang",
        "Xinwen Zhou"
      ],
      "abstract": "The recognition of pig behavior plays a crucial role in smart farming and\nwelfare assurance for pigs. Currently, in the field of pig behavior\nrecognition, the lack of publicly available behavioral datasets not only limits\nthe development of innovative algorithms but also hampers model robustness and\nalgorithm optimization.This paper proposes a dataset containing 13 pig\nbehaviors that significantly impact welfare.Based on this dataset, this paper\nproposes a spatial-temporal perception and enhancement networks based on the\nattention mechanism to model the spatiotemporal features of pig behaviors and\ntheir associated interaction areas in video data. The network is composed of a\nspatiotemporal perception network and a spatiotemporal feature enhancement\nnetwork. The spatiotemporal perception network is responsible for establishing\nconnections between the pigs and the key regions of their behaviors in the\nvideo data. The spatiotemporal feature enhancement network further strengthens\nthe important spatial features of individual pigs and captures the long-term\ndependencies of the spatiotemporal features of individual behaviors by\nremodeling these connections, thereby enhancing the model's perception of\nspatiotemporal changes in pig behaviors. Experimental results demonstrate that\non the dataset established in this paper, our proposed model achieves a MAP\nscore of 75.92%, which is an 8.17% improvement over the best-performing\ntraditional model. This study not only improces the accuracy and\ngeneralizability of individual pig behavior recognition but also provides new\ntechnological tools for modern smart farming. The dataset and related code will\nbe made publicly available alongside this paper.",
      "tldr_zh": "本文提出一个包含13种猪行为的公开数据集，以解决猪行为识别领域中数据集缺失的问题，从而促进算法创新和模型优化。基于此数据集，研究开发了一种基于注意力机制的空间-时间感知和增强网络，该网络通过空间-时间感知网络建立猪与关键行为区域的连接，并利用空间-时间特征增强网络加强个体猪的重要空间特征及捕获行为的长时依赖。实验结果显示，该模型在自建数据集上取得75.92%的MAP分数，比最佳传统模型提高8.17%，从而提升了猪行为识别的准确性和泛化性，并为智能养殖提供新的技术工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09378v1",
      "published_date": "2025-03-12 13:27:29 UTC",
      "updated_date": "2025-03-12 13:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:18:14.810171"
    },
    {
      "arxiv_id": "2503.09370v1",
      "title": "Revisiting Medical Image Retrieval via Knowledge Consolidation",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Nan",
        "Huichi Zhou",
        "Xiaodan Xing",
        "Giorgos Papanastasiou",
        "Lei Zhu",
        "Zhifan Gao",
        "Alejandro F Fangi",
        "Guang Yang"
      ],
      "abstract": "As artificial intelligence and digital medicine increasingly permeate\nhealthcare systems, robust governance frameworks are essential to ensure\nethical, secure, and effective implementation. In this context, medical image\nretrieval becomes a critical component of clinical data management, playing a\nvital role in decision-making and safeguarding patient information. Existing\nmethods usually learn hash functions using bottleneck features, which fail to\nproduce representative hash codes from blended embeddings. Although contrastive\nhashing has shown superior performance, current approaches often treat image\nretrieval as a classification task, using category labels to create\npositive/negative pairs. Moreover, many methods fail to address the\nout-of-distribution (OOD) issue when models encounter external OOD queries or\nadversarial attacks. In this work, we propose a novel method to consolidate\nknowledge of hierarchical features and optimisation functions. We formulate the\nknowledge consolidation by introducing Depth-aware Representation Fusion (DaRF)\nand Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates\nshallow and deep representations into blended features, and SCH incorporates\nimage fingerprints to enhance the adaptability of positive/negative pairings.\nThese blended features further facilitate OOD detection and content-based\nrecommendation, contributing to a secure AI-driven healthcare environment.\nMoreover, we present a content-guided ranking to improve the robustness and\nreproducibility of retrieval results. Our comprehensive assessments demonstrate\nthat the proposed method could effectively recognise OOD samples and\nsignificantly outperform existing approaches in medical image retrieval\n(p<0.05). In particular, our method achieves a 5.6-38.9% improvement in mean\nAverage Precision on the anatomical radiology dataset.",
      "tldr_zh": "本论文重新审视了医疗图像检索问题，针对现有方法的哈希码不代表性和out-of-distribution (OOD) 问题，提出了一种知识整合方法。\n该方法引入Depth-aware Representation Fusion (DaRF)来自适应融合浅层和深层特征，以及Structure-aware Contrastive Hashing (SCH)来通过图像指纹增强正/负对的适应性，同时采用内容引导排名以提升检索的鲁棒性和安全性。\n实验结果表明，该方法在解剖放射学数据集上平均精度提高了5.6-38.9%，并显著优于现有方法（p<0.05），有助于构建更安全的AI驱动医疗环境。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09370v1",
      "published_date": "2025-03-12 13:16:42 UTC",
      "updated_date": "2025-03-12 13:16:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:18:27.713179"
    },
    {
      "arxiv_id": "2503.11706v1",
      "title": "Refining Filter Global Feature Weighting for Fully-Unsupervised Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Fabian Galis",
        "Darian Onchis"
      ],
      "abstract": "In the context of unsupervised learning, effective clustering plays a vital\nrole in revealing patterns and insights from unlabeled data. However, the\nsuccess of clustering algorithms often depends on the relevance and\ncontribution of features, which can differ between various datasets. This paper\nexplores feature weighting for clustering and presents new weighting\nstrategies, including methods based on SHAP (SHapley Additive exPlanations), a\ntechnique commonly used for providing explainability in various supervised\nmachine learning tasks. By taking advantage of SHAP values in a way other than\njust to gain explainability, we use them to weight features and ultimately\nimprove the clustering process itself in unsupervised scenarios.\n  Our empirical evaluations across five benchmark datasets and clustering\nmethods demonstrate that feature weighting based on SHAP can enhance\nunsupervised clustering quality, achieving up to a 22.69\\% improvement over\nother weighting methods (from 0.586 to 0.719 in terms of the Adjusted Rand\nIndex). Additionally, these situations where the weighted data boosts the\nresults are highlighted and thoroughly explored, offering insight for practical\napplications.",
      "tldr_zh": "本研究探讨了无监督聚类中特征权重的优化问题，针对不同数据集特征贡献的差异，提出新的权重策略，包括基于 SHAP (SHapley Additive exPlanations) 的方法，将其从监督学习的解释性工具扩展到无监督场景，以提升聚类效果。研究通过利用 SHAP 值对特征进行加权，改进了聚类过程，并在五个基准数据集和多种聚类方法上进行实证评估。结果显示，SHAP 基于权重策略可将 Adjusted Rand Index 从 0.586 提高至 0.719，实现高达 22.69% 的改善，并分析了加权数据提升结果的具体情境，为实际应用提供宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11706v1",
      "published_date": "2025-03-12 13:14:09 UTC",
      "updated_date": "2025-03-12 13:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:18:38.553033"
    },
    {
      "arxiv_id": "2503.09365v1",
      "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Jiménez-López",
        "Nuria Rodríguez-Barroso",
        "M. Victoria Luzón",
        "Francisco Herrera"
      ],
      "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
      "tldr_zh": "本文研究了深度学习模型在训练过程中可能泄露隐私的问题，提出了一种基于Few-Shot Learning的Membership Inference Attacks (MIA)模型，名为FeS-MIA，以减少评估资源需求并简化对训练数据完整性的检测。论文同时引入了Log-MIA measure，这是一种可解释的定量和定性隐私评估指标，用于更有效地量化隐私风险。实验在图像分类和语言建模任务上表明，FeS-MIA比现有方法更高效地报告隐私泄露，提升了评估的实用性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09365v1",
      "published_date": "2025-03-12 13:09:43 UTC",
      "updated_date": "2025-03-12 13:09:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:18:51.316863"
    },
    {
      "arxiv_id": "2503.09358v1",
      "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports",
      "title_zh": "RetSTA：一种基于LLM的方法，用于标准化临床眼底图像报告",
      "authors": [
        "Jiushen Cai",
        "Weihang Zhang",
        "Hanruo Liu",
        "Ningli Wang",
        "Huiqi Li"
      ],
      "abstract": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
      "tldr_zh": "本研究提出了一种基于大型语言模型(LLM)的RetSTA方法，用于标准化临床眼底(fundus)图像报告，以解决报告格式、术语和风格不统一的问题。研究者首先构建了一个双语标准术语库，包含眼底临床术语和常用诊断描述，然后开发了RetSTA-7B-Zero模型，通过在模拟临床场景的增强数据集上微调，实现初步的标准化行为；随后，构建了RetSTA-7B模型，整合更多标准化数据和英文对应数据，扩展覆盖更广泛的复杂场景，实现报告级标准化。实验结果显示，RetSTA-7B在双语标准化任务中优于其他LLM，证明了其出色性能和泛化性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09358v1",
      "published_date": "2025-03-12 13:00:57 UTC",
      "updated_date": "2025-03-12 13:00:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:19:03.612029"
    },
    {
      "arxiv_id": "2503.09357v1",
      "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Ruifeng She",
        "Bowen Pang",
        "Kai Li",
        "Zehua Liu",
        "Tao Zhong"
      ],
      "abstract": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
      "tldr_zh": "该研究提出了一种基于混合整数编程（Mixed-Integer Programming）的自动并行规划方法，针对分布式深度学习中的操作员级（Operator-level）并行优化问题，解决了现有框架在处理复杂非线性神经网络（如混合专家和多模态模型的MIMO拓扑）时的局限性。该方法采用双层解决方案框架，自动生成分布式计划，兼顾神经网络的异构结构和硬件约束，同时平衡最优性和计算效率。在实验中，与DeepSeek's DualPipe等专家设计策略相比，该框架在相同内存约束下将计算气泡减少一半，并实现了相当或优越的性能，为大规模AI部署提供了一个多功能的研究工具和工业解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09357v1",
      "published_date": "2025-03-12 13:00:29 UTC",
      "updated_date": "2025-03-12 13:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:19:15.723316"
    },
    {
      "arxiv_id": "2503.09348v1",
      "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhoutong Ye",
        "Mingze Sun",
        "Huan-ang Gao",
        "Chun Yu",
        "Yuanchun Shi"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated significant potential as\ngeneralists in vision-language (VL) tasks. However, there remains a significant\ngap between state-of-the-art LMMs and human performance when it comes to\ncomplex tasks that require a combination of fundamental VL capabilities, as\nwell as tasks involving the grounding of complex instructions. To thoroughly\ninvestigate the human-LMM gap and its underlying causes, we propose MOAT, a\ndiverse benchmark with complex real-world VL tasks that are challenging for\nLMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist\nproblem solving by integrating fundamental VL capabilities such as reading\ntext, counting, understanding spatial relations, grounding textual and visual\ninstructions, etc. All these abilities fit into a taxonomy proposed by us that\ncontains 10 fundamental VL capabilities, enabling MOAT to provide a\nfine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first\nbenchmark to explicitly evaluate LMMs' ability to ground complex text and\nvisual instructions, which is essential to many real-world applications. We\nevaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT,\nand found that humans achieved 82.7% accuracy while the best performing LMM\n(OpenAI o1) achieved only 38.8%. To guide future model development, we analyze\ncommon trends in our results and discuss the underlying causes of observed\nperformance gaps between LMMs and humans, focusing on which VL capability forms\nthe bottleneck in complex tasks, whether test time scaling improves performance\non MOAT, and how tiling harms LMMs' capability to count. Code and data are\navailable at https://cambrian-yzt.github.io/MOAT.",
      "tldr_zh": "本研究提出MOAT基准，用于评估大型多模态模型(LMMs)在能力整合和指令地基方面的表现，旨在揭示LMMs与人类在复杂视觉语言(VL)任务中的差距。MOAT包含多样化的真实世界任务，需要LMMs整合10种基本VL能力，如阅读文本、计数、理解空间关系，并首次明确评估处理复杂文本和视觉指令的能力。实验结果显示，人类在MOAT上达到82.7%的准确率，而最佳LMM(OpenAI o1)仅为38.8%；论文分析了性能瓶颈、测试时间缩放的影响以及计数能力受损因素，以指导未来模型开发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Project page: https://cambrian-yzt.github.io/MOAT",
      "pdf_url": "http://arxiv.org/pdf/2503.09348v1",
      "published_date": "2025-03-12 12:49:31 UTC",
      "updated_date": "2025-03-12 12:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:19:28.010243"
    },
    {
      "arxiv_id": "2503.09347v1",
      "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyu Chen",
        "Seraphina Goldfarb-Tarrant"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
      "tldr_zh": "本文研究评估了11个LLM模型作为安全内容评估器的可靠性，焦点在于自一致性、对人类判断的alignment以及对输入artifacts（如道歉性或冗长表述）的敏感性。结果显示，LLMs的偏见可能严重扭曲评估结果，例如道歉语言artifacts可使偏好偏移高达98%，而更大模型并不总是更robust，小模型有时更抗特定artifacts。为了缓解这些问题，研究探索了jury-based evaluations，通过聚合多个模型的决策来提升robustness和对人类判断的契合，但artifact敏感性依然存在。该研究强调需要开发更多样化和artifact-resistant的方法，以确保可靠的安全评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.09347v1",
      "published_date": "2025-03-12 12:49:02 UTC",
      "updated_date": "2025-03-12 12:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:19:40.366606"
    },
    {
      "arxiv_id": "2503.09335v1",
      "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhi Lai",
        "Shenghai Yuan",
        "Youssef Nassar",
        "Mingyu Fan",
        "Thomas Weber",
        "Matthias Rätsch"
      ],
      "abstract": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
      "tldr_zh": "本论文提出 NVP-HRI，一种基于 Zero-Shot 的自然语音和姿势的人机交互 (HRI) 框架，利用 Large Language Model (LLM) 实现直观的多模态交互，旨在解决现有系统对新对象处理不足的问题，尤其适合老年人使用。NVP-HRI 整合 Segment Anything Model (SAM) 来分析视觉和深度数据，实现零样本对象预测，并通过 LLM 协调语音命令、姿势选择和场景分布，确保实时避碰并减少幻觉风险。实验在真实世界任务中使用 Universal Robot 进行评估，结果显示效率比传统手势控制提高了高达 59.2%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been accepted for publication in ESWA @ 2025 Elsevier.\n  Personal use of this material is permitted. Permission from Elsevier must be\n  obtained for all other uses, including reprinting/redistribution, creating\n  new works, or reuse of any copyrighted components of this work in other media",
      "pdf_url": "http://arxiv.org/pdf/2503.09335v1",
      "published_date": "2025-03-12 12:30:18 UTC",
      "updated_date": "2025-03-12 12:30:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:19:52.370056"
    },
    {
      "arxiv_id": "2503.09334v2",
      "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
      "title_zh": "翻译失败",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nThe dataset creation pipeline, along with comprehensive documentation,\nexamples, and resources for reproducing our results, is publicly available at\nhttps://github.com/Adelsamir01/CyberLLMInstruct.",
      "tldr_zh": "本文开发了 CyberLLMInstruct 数据集，包含 54,928 个指令-响应对，专注于网络安全任务如恶意软件分析、钓鱼模拟和零日漏洞，以评估微调 LLMs 的安全风险。数据集通过多阶段过程构建，包括数据来源过滤、结构化和与真实场景对齐，并测试了七个开源 LLMs（如 Llama 3.1 8B）。实验结果显示，微调后模型的安全性显著下降（例如，在 OWASP top 10 框架下，Llama 3.1 8B 对提示注入的安全分数从 0.95 降至 0.15），尽管在 CyberMetric 基准上准确率可达 92.50%。这些发现突显了性能提升与安全风险之间的权衡，强调了需要对抗测试和改进微调方法以平衡二者。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09334v2",
      "published_date": "2025-03-12 12:29:27 UTC",
      "updated_date": "2025-04-05 14:29:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:20:06.247002"
    },
    {
      "arxiv_id": "2503.09332v1",
      "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Dai Sun",
        "Huhao Guan",
        "Kun Zhang",
        "Xike Xie",
        "S. Kevin Zhou"
      ],
      "abstract": "Dynamic and static components in scenes often exhibit distinct properties,\nyet most 4D reconstruction methods treat them indiscriminately, leading to\nsuboptimal performance in both cases. This work introduces SDD-4DGS, the first\nframework for static-dynamic decoupled 4D scene reconstruction based on\nGaussian Splatting. Our approach is built upon a novel probabilistic dynamic\nperception coefficient that is naturally integrated into the Gaussian\nreconstruction pipeline, enabling adaptive separation of static and dynamic\ncomponents. With carefully designed implementation strategies to realize this\ntheoretical framework, our method effectively facilitates explicit learning of\nmotion patterns for dynamic elements while maintaining geometric stability for\nstatic structures. Extensive experiments on five benchmark datasets demonstrate\nthat SDD-4DGS consistently outperforms state-of-the-art methods in\nreconstruction fidelity, with enhanced detail restoration for static structures\nand precise modeling of dynamic motions. The code will be released.",
      "tldr_zh": "本研究提出SDD-4DGS，一种基于Gaussian Splatting的静态-动态解耦框架，用于4D场景重建，以解决现有方法未区分静态和动态组件导致的性能问题。该框架引入probabilistic dynamic perception coefficient，自然整合到重建管道中，实现静态结构的几何稳定性与动态元素的运动模式自适应分离。通过精心设计的实现策略，SDD-4DGS在五个基准数据集上的实验显示，其重建保真度优于最先进方法，尤其在静态细节恢复和动态运动建模方面表现出色。代码将发布。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09332v1",
      "published_date": "2025-03-12 12:25:58 UTC",
      "updated_date": "2025-03-12 12:25:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:20:16.099106"
    },
    {
      "arxiv_id": "2503.09330v1",
      "title": "Group-robust Machine Unlearning",
      "title_zh": "群组鲁棒机器取消学习",
      "authors": [
        "Thomas De Min",
        "Subhankar Roy",
        "Stéphane Lathuilière",
        "Elisa Ricci",
        "Massimiliano Mancini"
      ],
      "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
      "tldr_zh": "本文探讨了 Machine Unlearning 的公平性问题，即当 forget set 在特定组中占主导时，会导致该组性能下降。作者提出一个简单策略，通过 sample distribution reweighting 来缓解这种非均匀分布的负面影响，并引入 MIU（Mutual Information-aware Machine Unlearning）方法，该方法通过最小化模型特征与组信息之间的 Mutual Information，实现 approximate machine unlearning，同时保持组鲁棒性。实验在三个数据集上验证，MIU 优于传统方法，能有效删除指定数据而不牺牲模型的整体鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.09330v1",
      "published_date": "2025-03-12 12:24:05 UTC",
      "updated_date": "2025-03-12 12:24:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:20:29.004127"
    },
    {
      "arxiv_id": "2503.09326v1",
      "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
      "title_zh": "增强大型语言模型因果推理能力的综述",
      "authors": [
        "Xin Li",
        "Zhuo Cai",
        "Shoujin Wang",
        "Kun Yu",
        "Fang Chen"
      ],
      "abstract": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
      "tldr_zh": "这篇调查论文探讨了如何增强大型语言模型(LLMs)的因果推理能力，强调了LLMs在处理医疗和经济分析等任务时面临的挑战，如固有的因果推理局限性。作者总结了关键问题，提出一个新颖的分类法(taxonomy)来系统归类现有方法，并进行详细比较，同时概述了评估基准和指标。最终，论文为该领域指出了未来研究方向，提供insights和灵感以推动LLMs在因果推理方面的进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09326v1",
      "published_date": "2025-03-12 12:20:31 UTC",
      "updated_date": "2025-03-12 12:20:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:20:39.409427"
    },
    {
      "arxiv_id": "2503.09658v1",
      "title": "Towards Robust Model Evolution with Algorithmic Recourse",
      "title_zh": "翻译失败",
      "authors": [
        "Hao-Tsung Yang",
        "Jie Gao",
        "Bo-Yi Liu",
        "Zhi-Xuan Liu"
      ],
      "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges.",
      "tldr_zh": "该论文探讨了 Algorithmic Recourse 的挑战，即用户通过修改属性来应对不利的决策结果，但在资源竞争和分布 shifts 下，可能导致模型崩溃和系统不稳定性。研究者开发了一个通用框架，用于建模用户战略行为及其与决策系统的互动，并通过理论分析和实证评估，识别了三个关键现象：escalating decision boundaries、非鲁棒 model predictions 和 inequitable recourse actions。这些发现揭示了资源约束对公平性和鲁棒性的影响，最终提出了两个算法策略来缓解这些问题，并讨论了其更广泛的社会含义。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T42",
        "I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages,4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09658v1",
      "published_date": "2025-03-12 12:17:34 UTC",
      "updated_date": "2025-03-12 12:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:20:52.812474"
    },
    {
      "arxiv_id": "2503.09321v1",
      "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Gorjan Radevski",
        "Teodora Popordanoska",
        "Matthew B. Blaschko",
        "Tinne Tuytelaars"
      ],
      "abstract": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave",
      "tldr_zh": "音频-视觉理解领域存在视觉偏差和错误来源混淆的问题，现有多模态学习基准往往允许从视觉数据 alone 推断答案，并仅提供聚合 scores。本文引入 DAVE（Diagnostic Audio Visual Evaluation）基准数据集，通过确保音频和视觉模态均必要且将评估解耦成原子子类别，来系统评估模型性能。对最先进模型的详细分析揭示了特定失败模式，并为改进音频-visual 模型提供针对性见解。该数据集已开源，旨在促进更稳健的模型开发。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "First two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2503.09321v1",
      "published_date": "2025-03-12 12:12:46 UTC",
      "updated_date": "2025-03-12 12:12:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:21:04.109443"
    },
    {
      "arxiv_id": "2503.09311v1",
      "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
      "title_zh": "自适应政治调查和 GPT-4：通过模拟用户交互解决冷启动问题",
      "authors": [
        "Fynn Bachmann",
        "Daan van der Weijden",
        "Lucien Heitz",
        "Cristina Sarasua",
        "Abraham Bernstein"
      ],
      "abstract": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
      "tldr_zh": "本研究针对自适应问卷（Adaptive questionnaires）在政治调查中的冷启动问题（cold start problem），提出使用 Large Language Models (LLM) 如 GPT-4 生成合成用户互动数据来预训练模型。研究者利用瑞士 Smartvote 的真实数据进行评估，发现 GPT-4 生成的合成数据分布与真实数据高度相似，且预训练模型能显著降低用户响应预测错误并提高候选推荐准确率。总体而言，此方法展示了 LLM 在创建训练数据方面的潜力，有助于提升自适应问卷在政治科学等领域的效率和性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages. Under review at PLOS One",
      "pdf_url": "http://arxiv.org/pdf/2503.09311v1",
      "published_date": "2025-03-12 12:02:36 UTC",
      "updated_date": "2025-03-12 12:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:21:16.064077"
    },
    {
      "arxiv_id": "2503.09309v2",
      "title": "Steering No-Regret Agents in MFGs under Model Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Leo Widmer",
        "Jiawei Huang",
        "Niao He"
      ],
      "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.",
      "tldr_zh": "这篇论文探讨了在 Mean-Field Games (MFGs) 中，如何设计激励奖励来引导无后悔代理（no-regret agents）在模型不确定性下朝向期望行为，同时处理未知的转移动态和内在奖励函数。研究提出了一种新的乐观探索算法，假设代理表现出无后悔行为，以平衡探索和引导需求。理论上，该算法实现了代理行为与期望行为的累积差距次线性后悔保证（sub-linear regret guarantees），并确保总激励支付仅为次线性，从而与基准策略竞争。整体框架为大规模不确定系统中的代理行为引导提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "AISTATS 2025; 34 Pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09309v2",
      "published_date": "2025-03-12 12:02:02 UTC",
      "updated_date": "2025-04-14 17:28:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:21:28.710147"
    },
    {
      "arxiv_id": "2503.09289v1",
      "title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Somsubhra De",
        "Advait Vats"
      ],
      "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.",
      "tldr_zh": "这篇论文探讨了 Generative AI 生成的假评论对在线平台信誉的威胁，焦点是低资源语言 Tamil 和 Malayalam 中的 AI-generated product reviews，这些假评论可能误导消费者并促成欺诈。研究者采用了从传统 machine learning 方法到高级 transformer-based models（如 Indic-BERT、IndicSBERT、MuRIL、XLM-RoBERTa 和 MalayalamBERT）的多种方法进行检测。结果表明，这些 transformer 模型在识别 AI 生成内容方面表现出色，展示了在低资源语言环境中提升假评论检测潜力的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 9 figures, Accepted to DravidianLangTech Workshop\n  proceedings at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09289v1",
      "published_date": "2025-03-12 11:35:04 UTC",
      "updated_date": "2025-03-12 11:35:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:21:40.219157"
    },
    {
      "arxiv_id": "2503.09277v1",
      "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
      "title_zh": "UniCombine：统一的多种条件组合使用扩散变压器",
      "authors": [
        "Haoxuan Wang",
        "Jinlong Peng",
        "Qingdong He",
        "Hao Yang",
        "Ying Jin",
        "Jiafu Wu",
        "Xiaobin Hu",
        "Yanjie Pan",
        "Zhenye Gan",
        "Mingmin Chi",
        "Bo Peng",
        "Yabiao Wang"
      ],
      "abstract": "With the rapid development of diffusion models in image generation, the\ndemand for more powerful and flexible controllable frameworks is increasing.\nAlthough existing methods can guide generation beyond text prompts, the\nchallenge of effectively combining multiple conditional inputs while\nmaintaining consistency with all of them remains unsolved. To address this, we\nintroduce UniCombine, a DiT-based multi-conditional controllable generative\nframework capable of handling any combination of conditions, including but not\nlimited to text prompts, spatial maps, and subject images. Specifically, we\nintroduce a novel Conditional MMDiT Attention mechanism and incorporate a\ntrainable LoRA module to build both the training-free and training-based\nversions. Additionally, we propose a new pipeline to construct\nSubjectSpatial200K, the first dataset designed for multi-conditional generative\ntasks covering both the subject-driven and spatially-aligned conditions.\nExtensive experimental results on multi-conditional generation demonstrate the\noutstanding universality and powerful capability of our approach with\nstate-of-the-art performance.",
      "tldr_zh": "本文提出UniCombine，一种基于Diffusion Transformer的统一多条件可控生成框架，能够有效结合文本提示、空间地图和主体图像等多种条件输入，同时保持生成结果的一致性。具体而言，该框架引入了Conditional MMDiT Attention机制和可训练的LoRA模块，支持无训练和基于训练的版本，并构建了新的SubjectSpatial200K数据集，用于处理主体驱动和空间对齐的多条件生成任务。实验结果表明，UniCombine在多条件生成任务上实现了state-of-the-art性能，展示了其出色的通用性和能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
      "published_date": "2025-03-12 11:22:47 UTC",
      "updated_date": "2025-03-12 11:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:21:52.842021"
    },
    {
      "arxiv_id": "2503.09269v1",
      "title": "Single-Qudit Quantum Neural Networks for Multiclass Classification",
      "title_zh": "单量子元量子神经网络用于多类分类",
      "authors": [
        "Leandro C. Souza",
        "Renato Portugal"
      ],
      "abstract": "This paper proposes a single-qudit quantum neural network for multiclass\nclassification, by using the enhanced representational capacity of\nhigh-dimensional qudit states. Our design employs an $d$-dimensional unitary\noperator, where $d$ corresponds to the number of classes, constructed using the\nCayley transform of a skew-symmetric matrix, to efficiently encode and process\nclass information. This architecture enables a direct mapping between class\nlabels and quantum measurement outcomes, reducing circuit depth and\ncomputational overhead. To optimize network parameters, we introduce a hybrid\ntraining approach that combines an extended activation function -- derived from\na truncated multivariable Taylor series expansion -- with support vector\nmachine optimization for weight determination. We evaluate our model on the\nMNIST and EMNIST datasets, demonstrating competitive accuracy while maintaining\na compact single-qudit quantum circuit. Our findings highlight the potential of\nqudit-based QNNs as scalable alternatives to classical deep learning models,\nparticularly for multiclass classification. However, practical implementation\nremains constrained by current quantum hardware limitations. This research\nadvances quantum machine learning by demonstrating the feasibility of\nhigher-dimensional quantum systems for efficient learning tasks.",
      "tldr_zh": "本文提出了一种基于单 qudit 的量子神经网络（QNNs）用于多类分类，利用高维 qudit 状态的增强表示能力，以提高效率。设计采用 d-维幺正算子（通过 Cayley transform 构建），实现类标签与量子测量结果的直接映射，减少电路深度和计算开销；同时引入混合训练方法，结合基于截断多变量 Taylor series 的扩展激活函数和 support vector machine (SVM) 优化权重。在 MNIST 和 EMNIST 数据集上，该模型表现出竞争性的准确率，同时保持紧凑的单 qudit 量子电路。研究强调 qudit-based QNNs 作为可扩展替代经典深度学习模型的潜力，但受当前量子硬件限制，展示了高维量子系统在高效学习任务中的可行性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "24 pages, 3 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09269v1",
      "published_date": "2025-03-12 11:12:05 UTC",
      "updated_date": "2025-03-12 11:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:22:06.042362"
    },
    {
      "arxiv_id": "2503.09257v4",
      "title": "DeepInnovation AI: A Global Dataset Mapping the AI innovation from Academic Research to Industrial Patents",
      "title_zh": "DeepInnovation AI：一个全球数据集，用于映射 AI 创新从学术研究到工业专利",
      "authors": [
        "Haixing Gong",
        "Hui Zou",
        "Xingzhou Liang",
        "Shiyuan Meng",
        "Pinlong Cai",
        "Xingcheng Xu",
        "Jingjing Qu"
      ],
      "abstract": "In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.",
      "tldr_zh": "该研究介绍了DeepInnovationAI，一种全面的全球数据集，用于映射AI创新从学术研究到工业专利的转移过程，以解决现有数据基础设施的碎片化、不完整覆盖和评估能力不足问题。该数据集包括三个结构化文件：DeepPatentAI.csv（包含2,356,204个专利记录和8个字段属性）、DeepDiveAI.csv（包含3,511,929个学术出版物和13个元数据字段），以及DeepCosineAI.csv（通过semantic vector proximity analysis计算约一亿个论文-专利相似性对）。研究利用large language models、多语言文本分析（multilingual text analysis）、dual-layer BERT classifiers和hypergraph analysis等技术来识别AI相关内容并创建创新指标。该数据集支持研究人员、政策制定者和行业领袖预测AI趋势、识别合作机会，并分析技术发展模式和国际竞争动态，为AI创新和技术转移建模提供坚实基础。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.DB",
      "comment": "32 pages and 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09257v4",
      "published_date": "2025-03-12 10:56:02 UTC",
      "updated_date": "2025-03-28 08:22:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:22:17.970053"
    },
    {
      "arxiv_id": "2503.09251v1",
      "title": "SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yigang Chen",
        "Xiang Ji",
        "Ziyue Zhang",
        "Yuming Zhou",
        "Yang-Chi-Dung Lin",
        "Hsi-Yuan Huang",
        "Tao Zhang",
        "Yi Lai",
        "Ke Chen",
        "Chang Su",
        "Xingqiao Lin",
        "Zihao Zhu",
        "Yanggyi Zhang",
        "Kangping Wei",
        "Jiehui Fu",
        "Yixian Huang",
        "Shidong Cui",
        "Shih-Chung Yen",
        "Ariel Warshel",
        "Hsien-Da Huang"
      ],
      "abstract": "Deep learning-based drug-target interaction (DTI) prediction methods have\ndemonstrated strong performance; however, real-world applicability remains\nconstrained by limited data diversity and modeling complexity. To address these\nchallenges, we propose SCOPE-DTI, a unified framework combining a large-scale,\nbalanced semi-inductive human DTI dataset with advanced deep learning modeling.\nConstructed from 13 public repositories, the SCOPE dataset expands data volume\nby up to 100-fold compared to common benchmarks such as the Human dataset. The\nSCOPE model integrates three-dimensional protein and compound representations,\ngraph neural networks, and bilinear attention mechanisms to effectively capture\ncross domain interaction patterns, significantly outperforming state-of-the-art\nmethods across various DTI prediction tasks. Additionally, SCOPE-DTI provides a\nuser-friendly interface and database. We further validate its effectiveness by\nexperimentally identifying anticancer targets of Ginsenoside Rh1. By offering\ncomprehensive data, advanced modeling, and accessible tools, SCOPE-DTI\naccelerates drug discovery research.",
      "tldr_zh": "本文提出 SCOPE-DTI 框架，以解决深度学习在药物靶点交互 (DTI) 预测中的数据多样性不足和模型复杂性问题，通过构建一个大规模、平衡的半归纳式数据集 (SCOPE dataset)，该数据集从 13 个公共仓库中扩展，比常见基准数据集大 100 倍。SCOPE 模型整合三维蛋白质和化合物表示、图神经网络 (Graph Neural Networks) 和双线性注意力机制 (Bilinear Attention Mechanisms)，有效捕获跨域交互模式，并在各种 DTI 预测任务中显著优于现有方法。框架还提供用户友好的界面和数据库，并通过实验验证其在识别人参皂苷 Rh1 的抗癌靶点方面的实际效果，从而加速药物发现研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09251v1",
      "published_date": "2025-03-12 10:46:25 UTC",
      "updated_date": "2025-03-12 10:46:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:22:29.867399"
    },
    {
      "arxiv_id": "2503.09249v1",
      "title": "Considering Length Diversity in Retrieval-Augmented Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Juseon-Do",
        "Jaesung Hwang",
        "Jingun Kwon",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "abstract": "This study investigates retrieval-augmented summarization by specifically\nexamining the impact of exemplar summary lengths under length constraints, not\ncovered by previous work. We propose a Diverse Length-aware Maximal Marginal\nRelevance (DL-MMR) algorithm to better control summary lengths. This algorithm\ncombines the query relevance with diverse target lengths in retrieval-augmented\nsummarization. Unlike previous methods that necessitate exhaustive exemplar\nexemplar relevance comparisons using MMR, DL-MMR considers the exemplar target\nlength as well and avoids comparing exemplars to each other, thereby reducing\ncomputational cost and conserving memory during the construction of an exemplar\npool. Experimental results showed the effectiveness of DL-MMR, which considers\nlength diversity, compared to the original MMR algorithm. DL-MMR additionally\nshowed the effectiveness in memory saving of 781,513 times and computational\ncost reduction of 500,092 times, while maintaining the same level of\ninformativeness.",
      "tldr_zh": "这篇论文探讨了检索增强摘要（retrieval-augmented summarization）中示例摘要长度的影响，特别关注长度约束下对摘要质量的影响，这是先前研究未覆盖的领域。作者提出了一种 Diverse Length-aware Maximal Marginal Relevance (DL-MMR) 算法，该算法结合查询相关性和长度多样性，避免示例间的比较，从而显著降低计算成本和内存消耗。实验结果表明，DL-MMR 相较于原始 Maximal Marginal Relevance (MMR) 算法更有效，实现了781,513倍的内存节省和500,092倍的计算成本减少，同时保持相同的摘要信息量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, accepted to NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2503.09249v1",
      "published_date": "2025-03-12 10:43:33 UTC",
      "updated_date": "2025-03-12 10:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:22:40.692053"
    },
    {
      "arxiv_id": "2503.09243v1",
      "title": "GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Ruihai Wu",
        "Ziyu Zhu",
        "Yuran Wang",
        "Yue Chen",
        "Jiarui Wang",
        "Hao Dong"
      ],
      "abstract": "Cluttered garments manipulation poses significant challenges due to the\ncomplex, deformable nature of garments and intricate garment relations. Unlike\nsingle-garment manipulation, cluttered scenarios require managing complex\ngarment entanglements and interactions, while maintaining garment cleanliness\nand manipulation stability. To address these demands, we propose to learn\npoint-level affordance, the dense representation modeling the complex space and\nmulti-modal manipulation candidates, while being aware of garment geometry,\nstructure, and inter-object relations. Additionally, as it is difficult to\ndirectly retrieve a garment in some extremely entangled clutters, we introduce\nan adaptation module, guided by learned affordance, to reorganize\nhighly-entangled garments into states plausible for manipulation. Our framework\ndemonstrates effectiveness over environments featuring diverse garment types\nand pile configurations in both simulation and the real world. Project page:\nhttps://garmentpile.github.io/.",
      "tldr_zh": "本论文提出GarmentPile框架，用于解决杂乱衣物操纵的挑战，该框架处理衣物可变形性、纠缠关系和稳定性问题。核心方法包括学习point-level affordance，这是一种密集表示，能够模型复杂空间、多模态操纵候选，同时考虑衣物几何、结构和物体间关系；此外，还引入了由affordance引导的adaptation module，以重组高度纠缠的衣物，使其适合后续操纵。实验结果显示，该框架在模拟和真实环境中，对多种衣物类型和堆积配置表现出色，提升了操纵效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09243v1",
      "published_date": "2025-03-12 10:39:12 UTC",
      "updated_date": "2025-03-12 10:39:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:22:53.480072"
    },
    {
      "arxiv_id": "2503.09241v1",
      "title": "In-Context Defense in Computer Agents: An Empirical Study",
      "title_zh": "计算机代理中的上下文内防御：一个实证研究",
      "authors": [
        "Pei Yang",
        "Hai Ci",
        "Mike Zheng Shou"
      ],
      "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
      "tldr_zh": "该研究首次系统探讨了基于视觉语言模型(VLMs)的计算机代理在面对上下文欺骗攻击（如弹出窗口中的误导性指令）时的防御问题，引入了in-context defense方法。\n该方法利用in-context learning和chain-of-thought (CoT)推理，通过添加少量精心策划的示例（包含恶意环境和防御响应），指导代理先进行防御推理，然后再执行行动规划。\n实验结果显示，该方法将弹出窗口攻击的成功率降低91.2%，环境注入攻击平均降低74.6%，并对分散注意力的广告实现100%防御。\n研究还发现，防御推理必须在行动规划之前进行，且少于三个示例即可有效诱导代理的防御行为。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09241v1",
      "published_date": "2025-03-12 10:38:15 UTC",
      "updated_date": "2025-03-12 10:38:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:23:05.773827"
    },
    {
      "arxiv_id": "2503.09223v1",
      "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce",
      "title_zh": "LREF：一种基于LLM的新型相关性框架，用于电子商务",
      "authors": [
        "Tian Tang",
        "Zhixing Tian",
        "Zhenyu Zhu",
        "Chenyang Wang",
        "Haiqing Hu",
        "Guoyu Tang",
        "Lin Liu",
        "Sulong Xu"
      ],
      "abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
      "tldr_zh": "该论文提出了一种新型框架LREF（LLM-based Relevance Framework），旨在提升电商搜索中查询与产品的相关性预测，解决传统BERT-based模型的局限性（如知识容量有限）和直接使用Large Language Models (LLMs)的挑战（如数据质量需求高和乐观偏差）。框架包括三个关键阶段：Supervised Fine-Tuning (SFT) with Data Selection用于数据优化、Multiple Chain of Thought (Multi-CoT) tuning增强推理过程，以及Direct Preference Optimization (DPO) for de-biasing减少偏差。实验结果显示，在大规模真实数据集的离线测试和在线A/B测试中，LREF显著提高了相关性指标，最终在知名电商应用中部署，带来了实质性的商业收益。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09223v1",
      "published_date": "2025-03-12 10:10:30 UTC",
      "updated_date": "2025-03-12 10:10:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:23:18.640077"
    },
    {
      "arxiv_id": "2503.09217v1",
      "title": "Evaluating the Generalizability of LLMs in Automated Program Repair",
      "title_zh": "翻译失败",
      "authors": [
        "Fengjie Li",
        "Jiajun Jiang",
        "Jiajun Sun",
        "Hongyu Zhang"
      ],
      "abstract": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research.",
      "tldr_zh": "这篇论文评估了 11 个大型语言模型 (LLMs) 在自动程序修复 (Automated Program Repair, APR) 任务中的泛化能力，使用 Defects4J 和新数据集 DEFECTS4J-TRANS 进行实验。结果显示，LLMs 在 DEFECTS4J-TRANS 上表现显著下降，正确补丁和可信补丁的数量平均减少 49.48% 和 42.90%。进一步研究发现，通过在提示中加入额外的修复相关信息，可以将正确补丁和可信补丁数量分别提高高达 136.67% 和 121.82%，但仍无法达到原数据集的水平。论文结论指出，提示工程 (prompt engineering) 不足以显著提升 LLMs 的修复能力，并为未来研究提供了几点推荐。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, 1 figure, to be published in ICSE2025-NIER",
      "pdf_url": "http://arxiv.org/pdf/2503.09217v1",
      "published_date": "2025-03-12 10:03:58 UTC",
      "updated_date": "2025-03-12 10:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:23:32.096313"
    },
    {
      "arxiv_id": "2503.09215v2",
      "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Zhu",
        "Zhengyu Jia",
        "Tian Gao",
        "Jiaxin Deng",
        "Shidi Li",
        "Fu Liu",
        "Peng Jia",
        "Xianpeng Lang",
        "Xiaolong Sun"
      ],
      "abstract": "Advanced end-to-end autonomous driving systems predict other vehicles'\nmotions and plan ego vehicle's trajectory. The world model that can foresee the\noutcome of the trajectory has been used to evaluate the end-to-end autonomous\ndriving system. However, existing world models predominantly emphasize the\ntrajectory of the ego vehicle and leave other vehicles uncontrollable. This\nlimitation hinders their ability to realistically simulate the interaction\nbetween the ego vehicle and the driving scenario. In addition, it remains a\nchallenge to match multiple trajectories with each vehicle in the video to\ncontrol the video generation. To address above issues, a driving World Model\nnamed EOT-WM is proposed in this paper, unifying Ego-Other vehicle Trajectories\nin videos. Specifically, we first project ego and other vehicle trajectories in\nthe BEV space into the image coordinate to match each trajectory with its\ncorresponding vehicle in the video. Then, trajectory videos are encoded by the\nSpatial-Temporal Variational Auto Encoder to align with driving video latents\nspatially and temporally in the unified visual space. A trajectory-injected\ndiffusion Transformer is further designed to denoise the noisy video latents\nfor video generation with the guidance of ego-other vehicle trajectories. In\naddition, we propose a metric based on control latent similarity to evaluate\nthe controllability of trajectories. Extensive experiments are conducted on the\nnuScenes dataset, and the proposed model outperforms the state-of-the-art\nmethod by 30% in FID and 55% in FVD. The model can also predict unseen driving\nscenes with self-produced trajectories.",
      "tldr_zh": "这篇论文提出 EOT-WM 模型，一种统一的驾驶世界模型，用于整合 ego 车辆和其他车辆轨迹在视频潜在空间中，解决现有模型忽略其他车辆交互导致模拟不真实的问题。方法包括将 BEV 空间中的轨迹投影到图像坐标以匹配车辆、利用 Spatial-Temporal Variational Auto Encoder 编码轨迹视频并在统一视觉空间对齐，以及设计 trajectory-injected diffusion Transformer 生成受轨迹指导的视频。实验在 nuScenes 数据集上表明，该模型在 FID 上比最先进方法提升 30%、在 FVD 上提升 55%，并能准确预测未见驾驶场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09215v2",
      "published_date": "2025-03-12 10:02:18 UTC",
      "updated_date": "2025-03-17 08:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:23:42.836950"
    },
    {
      "arxiv_id": "2503.09206v1",
      "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
      "title_zh": "翻译失败",
      "authors": [
        "Xiuwen Fang",
        "Mang Ye",
        "Bo Du"
      ],
      "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL.",
      "tldr_zh": "这篇论文研究了联邦学习（Federated Learning）中的关键挑战，包括模型异构（不同客户端的本地模型结构）和数据损坏（由随机噪声、压缩伪影或环境因素引起）。为了解决这些问题，作者提出Robust Asymmetric Heterogeneous Federated Learning (RAHFL) 框架，该框架包括Diversity-enhanced supervised Contrastive Learning 技术，通过混合数据增强策略生成复杂样本进行监督对比学习，从而提升模型对各种损坏模式的鲁棒性和特征表示多样性。 additionally, the framework incorporates an Asymmetric Heterogeneous Federated Learning 策略，允许客户端在协作学习阶段进行选择性单向学习，以避免从低质量客户端吸收损坏反馈。实验结果显示，该方法在多样化的联邦学习环境中显著提高了性能和鲁棒性，并公开了代码和模型以供进一步验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09206v1",
      "published_date": "2025-03-12 09:52:04 UTC",
      "updated_date": "2025-03-12 09:52:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:23:54.249041"
    },
    {
      "arxiv_id": "2503.09199v1",
      "title": "GENEOnet: Statistical analysis supporting explainability and trustworthiness",
      "title_zh": "GENEOnet：支持可解释性和可信度的统计分析",
      "authors": [
        "Giovanni Bocchi",
        "Patrizio Frosini",
        "Alessandra Micheletti",
        "Alessandro Pedretti",
        "Carmen Gratteri",
        "Filippo Lunghini",
        "Andrea Rosario Beccari",
        "Carmine Talarico"
      ],
      "abstract": "Group Equivariant Non-Expansive Operators (GENEOs) have emerged as\nmathematical tools for constructing networks for Machine Learning and\nArtificial Intelligence. Recent findings suggest that such models can be\ninserted within the domain of eXplainable Artificial Intelligence (XAI) due to\ntheir inherent interpretability. In this study, we aim to verify this claim\nwith respect to GENEOnet, a GENEO network developed for an application in\ncomputational biochemistry by employing various statistical analyses and\nexperiments. Such experiments first allow us to perform a sensitivity analysis\non GENEOnet's parameters to test their significance. Subsequently, we show that\nGENEOnet exhibits a significantly higher proportion of equivariance compared to\nother methods. Lastly, we demonstrate that GENEOnet is on average robust to\nperturbations arising from molecular dynamics. These results collectively serve\nas proof of the explainability, trustworthiness, and robustness of GENEOnet and\nconfirm the beneficial use of GENEOs in the context of Trustworthy Artificial\nIntelligence.",
      "tldr_zh": "本研究通过统计分析和实验验证了 Group Equivariant Non-Expansive Operators (GENEOs) 在机器学习中的可解释性，特别是针对 GENEOnet 在计算生化的应用。研究首先进行敏感性分析，测试 GENEOnet 参数的重要性；随后比较了 GENEOnet 与其他方法的 equivariance 比例，发现其显著更高；最后证明 GENEOnet 对分子动力学扰动具有平均鲁棒性。这些结果 collectively 证实了 GENEOnet 的 explainability、trustworthiness 和 robustness，支持其在 Trustworthy Artificial Intelligence 中的有益应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09199v1",
      "published_date": "2025-03-12 09:43:48 UTC",
      "updated_date": "2025-03-12 09:43:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:24:05.580933"
    },
    {
      "arxiv_id": "2503.11702v3",
      "title": "Toward a method for LLM-enabled Indoor Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Coffrini",
        "Mohammad Amin Zadenoori",
        "Paolo Barsocchi",
        "Francesco Furfari",
        "Antonino Crivello",
        "Alessio Ferrari"
      ],
      "abstract": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
      "tldr_zh": "该研究针对室内导航的挑战（如复杂布局、缺乏 GPS 信号和可访问性问题）提出了一种基于大型语言模型 (LLM) 的方法，使用 ChatGPT 从室内地图图像生成自然、上下文感知的导航指令。研究者设计并评估了不同真实环境中的测试案例，评估 LLM 在解释空间布局、处理用户约束和规划高效路线方面的表现。结果显示，LLM 支持个性化导航的潜力，平均正确指示率为 50.54%，最高达 77.78%，但性能受兴趣点数量和视觉信息丰富度的负面影响，而非布局或路径复杂度。总的来说，此方法为 LLM 在室内导航领域的应用提供了初步见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 3 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.11702v3",
      "published_date": "2025-03-12 09:32:43 UTC",
      "updated_date": "2025-03-24 11:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:24:18.882139"
    },
    {
      "arxiv_id": "2503.09173v1",
      "title": "Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Ermanno Bartoli",
        "Dennis Rotondi",
        "Kai O. Arras",
        "Iolanda Leite"
      ],
      "abstract": "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
      "tldr_zh": "该论文针对家庭环境中机器人长期规划的挑战，强调了人类活动对物体和空间的影响，而现有基于视觉语言模型(VLMs)的轨迹规划方法未充分建模这些因素。论文提出一种新方法，通过丰富的3D场景图(3DSG)整合人类偏好、活动和空间上下文，捕获人类行动对空间的动态影响，从而实现更敏感的轨迹适应。初步实验结果显示，该方法能有效为受人类活动影响的空间分配成本，提升机器人轨迹的上下文适宜性和任务效率，为家庭人机互动提供更安全的社会适应性。未来工作将包括构建完整规划管道和进行用户研究以验证轨迹可接受性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T40",
        "I.2"
      ],
      "primary_category": "cs.RO",
      "comment": "5 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.09173v1",
      "published_date": "2025-03-12 09:00:45 UTC",
      "updated_date": "2025-03-12 09:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:24:30.961022"
    },
    {
      "arxiv_id": "2503.09164v1",
      "title": "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for Skin Cancer Treatment",
      "title_zh": "翻译失败",
      "authors": [
        "Joscha Grüger",
        "Tobias Geyer",
        "Tobias Brix",
        "Michael Storck",
        "Sonja Leson",
        "Laura Bley",
        "Carsten Weishaupt",
        "Ralph Bergmann",
        "Stephan A. Braun"
      ],
      "abstract": "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
      "tldr_zh": "这篇研究评估了数据准备度，以支持基于 AI 的临床决策支持系统 (CDSS) 在皮肤癌治疗中的应用，重点关注数据质量、可用性和可提取性。研究在明斯特大学医院皮肤肿瘤中心进行，采用多方面方法，包括文献综述、数据准备度评估和专家研讨会。结果识别了皮肤癌治疗决策的关键数据点，并评估了这些数据在各种信息系统中的存在和质量，同时突出了从非结构化数据中提取信息的挑战。最终，研究强调高质量、可访问数据对 AI 驱动 CDSS 在肿瘤学 (Oncology) 领域的成功至关重要。",
      "categories": [
        "cs.AI",
        "68T99, 62P10, 92C50",
        "I.2.6; J.3; H.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09164v1",
      "published_date": "2025-03-12 08:49:03 UTC",
      "updated_date": "2025-03-12 08:49:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:24:42.807836"
    },
    {
      "arxiv_id": "2503.09153v1",
      "title": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chaowei Zhang",
        "Zongling Feng",
        "Zewei Zhang",
        "Jipeng Qiang",
        "Guandong Xu",
        "Yun Li"
      ],
      "abstract": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods.",
      "tldr_zh": "本文研究了 LLMs 的 hallucination 是否能用于假新闻检测，提出了一种新颖的监督自强化推理修正方法 SR³，通过 LLMs 的反思生成正面合理推理和负面推理（wrong understandings），以进行语义一致性学习。基于此，构建了 NRFE 模型，利用正面或负面新闻-推理对来学习语义一致性，并引入学生模型 NRFE-D 只以新闻内容作为输入，通过知识蒸馏避免标签偏差的影响。实验结果在三个流行假新闻数据集上显示，该方法优于基线，包括 LLMs 提示、预训练 SLMs 的微调以及其他代表性方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 12 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.09153v1",
      "published_date": "2025-03-12 08:29:59 UTC",
      "updated_date": "2025-03-12 08:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:24:54.780418"
    },
    {
      "arxiv_id": "2503.09151v2",
      "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Hyeonho Jeong",
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
      "tldr_zh": "这篇论文介绍了 Reangle-A-Video 框架，将 4D 视频生成任务重 Framing 为视频-to-video translation，从单个输入视频生成同步的多视图视频，而非依赖大规模 4D 数据集。框架分为两个阶段：首先，通过自监督方式微调图像-to-video 扩散变换器进行 Multi-View Motion Learning，从扭曲视频中提取视图不变的运动；其次，使用 DUSt3R 在推理时提供跨视图一致性指导，将输入视频的第一帧扭曲和修复成多视图起始图像。实验结果显示，该方法在静态视图传输和动态相机控制任务上超越现有方法，为多视图视频生成提供新方案，并计划公开代码和数据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://hyeonho99.github.io/reangle-a-video/",
      "pdf_url": "http://arxiv.org/pdf/2503.09151v2",
      "published_date": "2025-03-12 08:26:15 UTC",
      "updated_date": "2025-03-17 13:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:25:08.260286"
    },
    {
      "arxiv_id": "2503.09144v1",
      "title": "Efficient UAV Swarm-Based Multi-Task Federated Learning with Dynamic Task Knowledge Sharing",
      "title_zh": "翻译失败",
      "authors": [
        "Yubo Yang",
        "Tao Yang",
        "Xiaofeng Wu",
        "Ziyu Guo",
        "Bo Hu"
      ],
      "abstract": "UAV swarms are widely used in emergency communications, area monitoring, and\ndisaster relief. Coordinated by control centers, they are ideal for federated\nlearning (FL) frameworks. However, current UAV-assisted FL methods primarily\nfocus on single tasks, overlooking the need for multi-task training. In\ndisaster relief scenarios, UAVs perform tasks such as crowd detection, road\nfeasibility analysis, and disaster assessment, which exhibit time-varying\ndemands and potential correlations. In order to meet the time-varying\nrequirements of tasks and complete multiple tasks efficiently under resource\nconstraints, in this paper, we propose a UAV swarm based multi-task FL\nframework, where ground emergency vehicles (EVs) collaborate with UAVs to\naccomplish multiple tasks efficiently under constrained energy and bandwidth\nresources. Through theoretical analysis, we identify key factors affecting task\nperformance and introduce a task attention mechanism to dynamically evaluate\ntask importance, thereby achieving efficient resource allocation. Additionally,\nwe propose a task affinity (TA) metric to capture the dynamic correlation among\ntasks, thereby promoting task knowledge sharing to accelerate training and\nimprove the generalization ability of the model in different scenarios. To\noptimize resource allocation, we formulate a two-layer optimization problem to\njointly optimize UAV transmission power, computation frequency, bandwidth\nallocation, and UAV-EV associations. For the inner problem, we derive\nclosed-form solutions for transmission power, computation frequency, and\nbandwidth allocation and apply a block coordinate descent method for\noptimization. For the outer problem, a two-stage algorithm is designed to\ndetermine optimal UAV-EV associations. Furthermore, theoretical analysis\nreveals a trade-off between UAV energy consumption and multi-task performance.",
      "tldr_zh": "该论文提出了一种高效的基于UAV群的多任务Federated Learning（FL）框架，旨在解决灾害救援等场景中任务需求变化和资源约束的问题，通过引入任务注意力机制动态评估任务重要性和任务亲和度（TA）指标促进任务知识共享，从而加速训练并提升模型泛化能力。框架中，地面紧急车辆（EVs）与UAV协作，优化传输功率、计算频率、带宽分配和UAV-EV关联，采用双层优化问题解决策略，包括内层闭式解和块坐标下降法，以及外层两阶段算法。理论分析揭示了UAV能量消耗与多任务性能之间的权衡，显著提高了任务执行效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file",
      "pdf_url": "http://arxiv.org/pdf/2503.09144v1",
      "published_date": "2025-03-12 08:13:39 UTC",
      "updated_date": "2025-03-12 08:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:25:21.404968"
    },
    {
      "arxiv_id": "2503.09132v1",
      "title": "Investigation of Frame Differences as Motion Cues for Video Object Segmentation",
      "title_zh": "视频对象分割中帧差异作为运动线索的研究",
      "authors": [
        "Sota Kawamura",
        "Hirotada Honda",
        "Shugo Nakamura",
        "Takashi Sano"
      ],
      "abstract": "Automatic Video Object Segmentation (AVOS) refers to the task of autonomously\nsegmenting target objects in video sequences without relying on human-provided\nannotations in the first frames. In AVOS, the use of motion information is\ncrucial, with optical flow being a commonly employed method for capturing\nmotion cues. However, the computation of optical flow is resource-intensive,\nmaking it unsuitable for real-time applications, especially on edge devices\nwith limited computational resources. In this study, we propose using frame\ndifferences as an alternative to optical flow for motion cue extraction. We\ndeveloped an extended U-Net-like AVOS model that takes a frame on which\nsegmentation is performed and a frame difference as inputs, and outputs an\nestimated segmentation map. Our experimental results demonstrate that the\nproposed model achieves performance comparable to the model with optical flow\nas an input, particularly when applied to videos captured by stationary\ncameras. Our results suggest the usefulness of employing frame differences as\nmotion cues in cases with limited computational resources.",
      "tldr_zh": "本文研究了在 Automatic Video Object Segmentation (AVOS) 中，使用 frame differences 作为 optical flow 的替代方案，以捕捉运动线索并降低计算资源需求。研究者开发了一个扩展的 U-Net-like 模型，该模型以待分割帧和帧差作为输入，输出估计的分割地图。实验结果表明，该模型在固定摄像头视频上的性能与使用 optical flow 的基线模型相当，证明了 frame differences 在资源受限环境中的实用价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures, 2 tables. Accepted to The 9th International\n  Conference on Machine Learning and Soft Computing (ICMLSC 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.09132v1",
      "published_date": "2025-03-12 07:42:15 UTC",
      "updated_date": "2025-03-12 07:42:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:25:32.915039"
    },
    {
      "arxiv_id": "2503.09120v1",
      "title": "On the Internal Representations of Graph Metanetworks",
      "title_zh": "翻译失败",
      "authors": [
        "Taesun Yeom",
        "Jaeho Lee"
      ],
      "abstract": "Weight space learning is an emerging paradigm in the deep learning community.\nThe primary goal of weight space learning is to extract informative features\nfrom a set of parameters using specially designed neural networks, often\nreferred to as \\emph{metanetworks}. However, it remains unclear how these\nmetanetworks learn solely from parameters. To address this, we take the first\nstep toward understanding \\emph{representations} of metanetworks, specifically\ngraph metanetworks (GMNs), which achieve state-of-the-art results in this\nfield, using centered kernel alignment (CKA). Through various experiments, we\nreveal that GMNs and general neural networks (\\textit{e.g.,} multi-layer\nperceptrons (MLPs) and convolutional neural networks (CNNs)) differ in terms of\ntheir representation space.",
      "tldr_zh": "该论文探讨了权重空间学习（weight space learning）中图元网络（graph metanetworks, GMNs）的内部表示，旨在揭示这些元网络（metanetworks）如何从参数中学习特征。研究者首次使用居中核对齐（centered kernel alignment, CKA）进行实验，比较了GMNs与其他神经网络如多层感知器（multi-layer perceptrons, MLPs）和卷积神经网络（convolutional neural networks, CNNs）的表示空间差异。结果显示，GMNs在表示空间上与一般神经网络存在显著不同，这为理解元网络的学习机制提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Workshop on Weight Space Learning",
      "pdf_url": "http://arxiv.org/pdf/2503.09120v1",
      "published_date": "2025-03-12 07:12:34 UTC",
      "updated_date": "2025-03-12 07:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:25:45.089809"
    },
    {
      "arxiv_id": "2503.09114v1",
      "title": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian Abstreiter",
        "Sasu Tarkoma",
        "Roberto Morabito"
      ],
      "abstract": "The rapid rise of Language Models (LMs) has expanded the capabilities of\nnatural language processing, powering applications from text generation to\ncomplex decision-making. While state-of-the-art LMs often boast hundreds of\nbillions of parameters and are primarily deployed in data centers, recent\ntrends show a growing focus on compact models-typically under 10 billion\nparameters-enabled by techniques such as quantization and other model\ncompression techniques. This shift paves the way for LMs on edge devices,\noffering potential benefits such as enhanced privacy, reduced latency, and\nimproved data sovereignty. However, the inherent complexity of even these\nsmaller models, combined with the limited computing resources of edge hardware,\nraises critical questions about the practical trade-offs in executing LM\ninference outside the cloud. To address these challenges, we present a\ncomprehensive evaluation of generative LM inference on representative CPU-based\nand GPU-accelerated edge devices. Our study measures key performance\nindicators-including memory usage, inference speed, and energy\nconsumption-across various device configurations. Additionally, we examine\nthroughput-energy trade-offs, cost considerations, and usability, alongside an\nassessment of qualitative model performance. While quantization helps mitigate\nmemory overhead, it does not fully eliminate resource bottlenecks, especially\nfor larger models. Our findings quantify the memory and energy constraints that\nmust be considered for practical real-world deployments, offering concrete\ninsights into the trade-offs between model size, inference performance, and\nefficiency. The exploration of LMs at the edge is still in its early stages. We\nhope this study provides a foundation for future research, guiding the\nrefinement of models, the enhancement of inference efficiency, and the\nadvancement of edge-centric AI systems.",
      "tldr_zh": "这篇论文评估了在边缘设备上运行 Language Models (LMs) 推理的可行性及其权衡，强调了隐私、低延迟和数据主权的潜在优势，同时指出了资源限制带来的挑战。研究通过对 CPU 和 GPU 加速设备进行全面实验，测量了内存使用、推理速度和能耗等关键指标，并分析了量化技术在缓解内存开销方面的作用，但发现它无法完全消除瓶颈。最终，论文量化了模型大小、性能和效率之间的权衡，提供宝贵见解，为未来边缘 AI 系统的优化和研究奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is currently under review for publication in an ACM\n  journal. If accepted, the copyright will be transferred to ACM",
      "pdf_url": "http://arxiv.org/pdf/2503.09114v1",
      "published_date": "2025-03-12 07:01:34 UTC",
      "updated_date": "2025-03-12 07:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:25:57.190426"
    },
    {
      "arxiv_id": "2503.09113v1",
      "title": "Constraint-Guided Learning of Data-driven Health Indicator Models: An Application on the Pronostia Bearing Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Yonas Tefera",
        "Quinten Van Baelen",
        "Maarten Meire",
        "Stijn Luca",
        "Peter Karsmakers"
      ],
      "abstract": "This paper presents a constraint-guided deep learning framework for\ndeveloping physically consistent health indicators in bearing prognostics and\nhealth management. Conventional data-driven methods often lack physical\nplausibility, while physics-based models are limited by incomplete system\nknowledge. To address this, we integrate domain knowledge into deep learning\nusing constraints to enforce monotonicity, bound output values between 1 and 0\n(representing healthy to failed states), and ensure consistency between signal\nenergy trends and health indicator estimates. This eliminates the need for\ncomplex loss term balancing. We implement constraint-guided gradient descent\nwithin an autoencoder architecture, creating a constrained autoencoder.\nHowever, the framework is adaptable to other architectures. Using\ntime-frequency representations of accelerometer signals from the Pronostia\ndataset, our constrained model generates smoother, more reliable degradation\nprofiles compared to conventional methods, aligning with expected physical\nbehavior. Performance is assessed using three metrics: trendability,\nrobustness, and consistency. Compared to a conventional baseline, the\nconstrained model improves all three. Another baseline, incorporating\nmonotonicity via a soft-ranking loss function, outperforms in trendability but\nfalls short in robustness and consistency. An ablation study confirms that the\nmonotonicity constraint enhances trendability, the boundary constraint ensures\nconsistency, and the energy-health consistency constraint improves robustness.\nThese findings highlight the effectiveness of constraint-guided deep learning\nin producing reliable, physically meaningful health indicators, offering a\npromising direction for future prognostic applications.",
      "tldr_zh": "本论文提出了一种约束引导的深度学习框架，用于开发轴承预后和健康管理的物理一致性健康指标，针对传统数据驱动方法缺乏物理合理性的问题，并在 Pronostia 轴承数据集上进行了应用。该框架通过整合领域知识施加约束（如确保健康指标的单调性、输出值在0-1之间，以及信号能量趋势与指标一致性），并在自编码器(autoencoder)架构中使用约束引导的梯度下降，实现可靠的退化曲线生成。实验结果显示，与常规基线模型相比，该方法在趋势性(trendability)、鲁棒性(robustness)和一致性(consistency)三个指标上均有显著改善，而消融研究进一步证实了各约束的作用。该框架为未来预后应用提供了可靠且物理意义强的健康指标解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09113v1",
      "published_date": "2025-03-12 07:01:27 UTC",
      "updated_date": "2025-03-12 07:01:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:26:09.723218"
    },
    {
      "arxiv_id": "2503.09106v1",
      "title": "Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Chuyu Zhang",
        "Xueyang Yu",
        "Peiyan Gu",
        "Xuming He"
      ],
      "abstract": "This paper addresses the problem of Rehearsal-Free Continual Category\nDiscovery (RF-CCD), which focuses on continuously identifying novel class by\nleveraging knowledge from labeled data. Existing methods typically train from\nscratch, overlooking the potential of base models, and often resort to data\nstorage to prevent forgetting. Moreover, because RF-CCD encompasses both\ncontinual learning and novel class discovery, previous approaches have\nstruggled to effectively integrate advanced techniques from these fields,\nresulting in less convincing comparisons and failing to reveal the unique\nchallenges posed by RF-CCD. To address these challenges, we lead the way in\nintegrating advancements from both domains and conducting extensive experiments\nand analyses. Our findings demonstrate that this integration can achieve\nstate-of-the-art results, leading to the conclusion that in the presence of\npre-trained models, the representation does not improve and may even degrade\nwith the introduction of unlabeled data. To mitigate representation\ndegradation, we propose a straightforward yet highly effective baseline method.\nThis method first utilizes prior knowledge of known categories to estimate the\nnumber of novel classes. It then acquires representations using a model\nspecifically trained on the base classes, generates high-quality pseudo-labels\nthrough k-means clustering, and trains only the classifier layer. We validate\nour conclusions and methods by conducting extensive experiments across multiple\nbenchmarks, including the Stanford Cars, CUB, iNat, and Tiny-ImageNet datasets.\nThe results clearly illustrate our findings, demonstrate the effectiveness of\nour baseline, and pave the way for future advancements in RF-CCD.",
      "tldr_zh": "这篇论文针对 Rehearsal-Free Continual Category Discovery (RF-CCD) 问题，提出一个简单基线方法，以整合连续学习和新型类别发现的技术，避免了现有方法的从零训练和数据存储依赖。方法包括利用已知类别的先验知识估计新类别数量，通过在基类上训练的模型获取表示，并采用 k-means clustering 生成高质量伪标签，只训练分类器层，以缓解引入无标记数据导致的表示退化。实验结果在 Stanford Cars、CUB、iNat 和 Tiny-ImageNet 等数据集上证明了该基线的有效性，实现了 state-of-the-art 性能，并为 RF-CCD 领域未来发展奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Underreview",
      "pdf_url": "http://arxiv.org/pdf/2503.09106v1",
      "published_date": "2025-03-12 06:46:32 UTC",
      "updated_date": "2025-03-12 06:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:26:22.791069"
    },
    {
      "arxiv_id": "2503.10699v1",
      "title": "Test-Time Discovery via Hashing Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Lyu",
        "Tianle Liu",
        "Zhang Zhang",
        "Fuyuan Hu",
        "Liang Wang"
      ],
      "abstract": "We introduce Test-Time Discovery (TTD) as a novel task that addresses class\nshifts during testing, requiring models to simultaneously identify emerging\ncategories while preserving previously learned ones. A key challenge in TTD is\ndistinguishing newly discovered classes from those already identified. To\naddress this, we propose a training-free, hash-based memory mechanism that\nenhances class discovery through fine-grained comparisons with past test\nsamples. Leveraging the characteristics of unknown classes, our approach\nintroduces hash representation based on feature scale and directions, utilizing\nLocality-Sensitive Hashing (LSH) for efficient grouping of similar samples.\nThis enables test samples to be easily and quickly compared with relevant past\ninstances. Furthermore, we design a collaborative classification strategy,\ncombining a prototype classifier for known classes with an LSH-based classifier\nfor novel ones. To enhance reliability, we incorporate a self-correction\nmechanism that refines memory labels through hash-based neighbor retrieval,\nensuring more stable and accurate class assignments. Experimental results\ndemonstrate that our method achieves good discovery of novel categories while\nmaintaining performance on known classes, establishing a new paradigm in model\ntesting. Our code is available at https://github.com/fanlyu/ttd.",
      "tldr_zh": "本研究引入了Test-Time Discovery (TTD) 任务，用于处理测试阶段的类别变化，模型需同时识别新出现的类别并保留已知类别。针对区分新旧类别的挑战，作者提出了一种无需训练的基于哈希内存机制，通过特征规模和方向的哈希表示以及Locality-Sensitive Hashing (LSH) 来高效分组和比较相似样本。方法还设计了协作分类策略，将原型分类器用于已知类别，并结合LSH-based 分类器处理新类别，同时融入自校正机制，通过哈希-based 邻居检索优化标签分配以提升准确性。实验结果表明，该方法在发现新类别的同时，保持了已知类别的性能表现，建立了模型测试的新范式。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10699v1",
      "published_date": "2025-03-12 06:43:01 UTC",
      "updated_date": "2025-03-12 06:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:26:33.862093"
    },
    {
      "arxiv_id": "2503.09101v2",
      "title": "The Shape of Attraction in UMAP: Exploring the Embedding Forces in Dimensionality Reduction",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Tariqul Islam",
        "Jason W. Fleischer"
      ],
      "abstract": "Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.",
      "tldr_zh": "该论文分析了 Uniform Manifold Approximation and Projection (UMAP) 在降维过程中的吸引力和排斥力，揭示了这些力对聚类形成和可视化的影响。排斥力强调数据点差异，从而控制聚类边界和聚类间距离，而吸引力更微妙，可能在低维映射中同时表现为吸引和排斥，这解释了学习率退火以及对两者的不同处理方法。通过修改吸引力，论文改进了聚类形成的稳定性，使 UMAP 和类似嵌入方法更具可解释性、更鲁棒和更准确。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "9 page + appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.09101v2",
      "published_date": "2025-03-12 06:37:43 UTC",
      "updated_date": "2025-03-18 15:48:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:26:45.928986"
    },
    {
      "arxiv_id": "2503.10697v1",
      "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
      "title_zh": "零样本主体中心生成：使用熵融合的创意应用",
      "authors": [
        "Kaifeng Zou",
        "Xiaoyi Feng",
        "Peng Wang",
        "Tao Huang",
        "Zizhou Huang",
        "Zhang Haihang",
        "Yuntao Zou",
        "Dagang Li"
      ],
      "abstract": "Generative models are widely used in visual content creation. However,\ncurrent text-to-image models often face challenges in practical\napplications-such as textile pattern design and meme generation-due to the\npresence of unwanted elements that are difficult to separate with existing\nmethods. Meanwhile, subject-reference generation has emerged as a key research\ntrend, highlighting the need for techniques that can produce clean,\nhigh-quality subject images while effectively removing extraneous components.\nTo address this challenge, we introduce a framework for reliable\nsubject-centric image generation. In this work, we propose an entropy-based\nfeature-weighted fusion method to merge the informative cross-attention\nfeatures obtained from each sampling step of the pretrained text-to-image model\nFLUX, enabling a precise mask prediction and subject-centric generation.\nAdditionally, we have developed an agent framework based on Large Language\nModels (LLMs) that translates users' casual inputs into more descriptive\nprompts, leading to highly detailed image generation. Simultaneously, the\nagents extract primary elements of prompts to guide the entropy-based feature\nfusion, ensuring focused primary element generation without extraneous\ncomponents. Experimental results and user studies demonstrate our methods\ngenerates high-quality subject-centric images, outperform existing methods or\nother possible pipelines, highlighting the effectiveness of our approach.",
      "tldr_zh": "这篇论文针对文本到图像模型在创意应用（如纺织图案设计和 meme 生成）中的问题，提出了一种Zero-Shot主体中心生成框架，使用Entropy Fusion方法来解决不需要元素的干扰。核心技术包括基于熵的特征加权融合，融合预训练模型FLUX的交叉注意力特征，实现精确掩码预测和高质量图像生成。同时，作者开发了基于LLMs的代理框架，将用户随意输入转化为详细提示，并提取主要元素指导特征融合，确保生成图像聚焦于关键主体而不包含多余组件。实验结果和用户研究表明，该方法在主体中心图像生成方面优于现有方法，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 8 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.10697v1",
      "published_date": "2025-03-12 06:27:30 UTC",
      "updated_date": "2025-03-12 06:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:26:58.038118"
    },
    {
      "arxiv_id": "2503.09091v2",
      "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Li",
        "Guihong Wan",
        "Xintao Wu",
        "Xinyu Wu",
        "Xiaohui Chen",
        "Yi He",
        "Christine G. Lian",
        "Peter K. Sorger",
        "Yevgeniy R. Semenov",
        "Chen Zhao"
      ],
      "abstract": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
      "tldr_zh": "这篇调查综述了计算病理学（CPath）中的多模态 foundation models，强调了这些模型整合视觉数据（如基于 hematoxylin and eosin (H&E) stained whole slide images (WSIs)）与文本报告、知识图谱和分子配置文件等异构来源的潜力。论文将 32 个最先进模型分类为三大范式：vision-language（进一步细分为 non-LLM-based 和 LLM-based）、vision-knowledge graph 和 vision-gene expression，并分析了 28 个多模态数据集，包括 image-text pairs、instruction datasets 和 image-other modality pairs。最终，它探讨了下游任务的分类、训练与评估策略，突出了关键挑战（如数据整合和泛化问题）以及未来方向，为 pathology 与 AI 交叉领域的从业者提供宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09091v2",
      "published_date": "2025-03-12 06:03:33 UTC",
      "updated_date": "2025-03-20 16:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:27:11.692020"
    },
    {
      "arxiv_id": "2503.09089v2",
      "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoling Chen",
        "Xiangru Tang",
        "Gangda Deng",
        "Fang Wu",
        "Jialong Wu",
        "Zhiwei Jiang",
        "Viktor Prasanna",
        "Arman Cohan",
        "Xingyao Wang"
      ],
      "abstract": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
      "tldr_zh": "该论文提出 LocAgent 框架，利用图引导的 LLM Agents 来解决代码定位（Code Localization）任务，该任务涉及从复杂代码库中识别相关代码部分以支持软件维护。LocAgent 通过将代码库解析为有向异构图，捕捉代码结构（如文件、类、函数）和依赖关系（如导入、调用、继承），从而启用 LLM 代理进行高效的多跳推理。实验结果显示，该方法显著提升了定位准确性，使用微调后的 Qwen-2.5-Coder-Instruct-32B 模型与 SOTA 专有模型相比，成本降低约 86%，在文件级定位上达到 92.7% 准确率，并将 GitHub 问题解决成功率（Pass@10）提高 12%。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09089v2",
      "published_date": "2025-03-12 05:55:01 UTC",
      "updated_date": "2025-04-29 14:37:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:27:25.772078"
    },
    {
      "arxiv_id": "2503.09081v1",
      "title": "Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaowei Bi",
        "Zheyuan Xu"
      ],
      "abstract": "Long Video Question Answering (LVQA) is challenging due to the need for\ntemporal reasoning and large-scale multimodal data processing. Existing methods\nstruggle with retrieving cross-modal information from long videos, especially\nwhen relevant details are sparsely distributed. We introduce UMaT (Unified\nMulti-modal as Text), a retrieval-augmented generation (RAG) framework that\nefficiently processes extremely long videos while maintaining cross-modal\ncoherence. UMaT converts visual and auditory data into a unified textual\nrepresentation, ensuring semantic and temporal alignment. Short video clips are\nanalyzed using a vision-language model, while automatic speech recognition\n(ASR) transcribes dialogue. These text-based representations are structured\ninto temporally aligned segments, with adaptive filtering to remove redundancy\nand retain salient details. The processed data is embedded into a vector\ndatabase, enabling precise retrieval of dispersed yet relevant content.\nExperiments on a benchmark LVQA dataset show that UMaT outperforms existing\nmethods in multimodal integration, long-form video understanding, and sparse\ninformation retrieval. Its scalability and interpretability allow it to process\nvideos over an hour long while maintaining semantic and temporal coherence.\nThese findings underscore the importance of structured retrieval and multimodal\nsynchronization for advancing LVQA and long-form AI systems.",
      "tldr_zh": "该研究提出了一种名为UMaT的简单统一多模态框架，用于解决Long Video Question Answering (LVQA)的挑战，该框架通过检索增强生成 (RAG) 技术将视觉和听觉数据转换为统一的文本表示，确保语义和时间对齐。UMaT使用视觉语言模型分析短视频片段、自动语音识别 (ASR) 转录对话，并将这些表示结构化为时间对齐的段落，同时通过适应性过滤去除冗余并嵌入向量数据库以实现精确检索。实验在基准LVQA数据集上显示，UMaT在多模态集成、长视频理解和稀疏信息检索方面优于现有方法，能够处理超过一小时的视频并保持语义和时间一致性。这些发现突出了结构化检索和多模态同步对提升LVQA和长形式AI系统的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09081v1",
      "published_date": "2025-03-12 05:28:24 UTC",
      "updated_date": "2025-03-12 05:28:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:27:36.413055"
    },
    {
      "arxiv_id": "2503.10695v2",
      "title": "Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mooho Song",
        "Hyeryung Son",
        "Jay-Yoon Lee"
      ],
      "abstract": "Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.",
      "tldr_zh": "该论文引入了“set-consistency verification”任务，这是一种扩展的自然语言推理 (NLI)，用于评估多个语句集合的整体逻辑连贯性，以解决传统成对比较方法无法捕捉集体矛盾的局限性。作者提出了 Set-Consistency Energy Network (SC-Energy)，一个基于对比损失框架的模型，能够高效检测不一致性、识别具体矛盾语句，并显著优于现有方法如基于提示的LLM模型。论文还发布了两个新数据集：Set-LConVQA 和 Set-SNLI，以支持该任务的研究和应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10695v2",
      "published_date": "2025-03-12 05:11:11 UTC",
      "updated_date": "2025-03-19 04:07:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:27:53.818717"
    },
    {
      "arxiv_id": "2503.13500v2",
      "title": "Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection",
      "title_zh": "翻译失败",
      "authors": [
        "Yucheng Suo",
        "Fan Ma",
        "Kaixin Shen",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "abstract": "Visual instructions for long-horizon tasks are crucial as they intuitively\nclarify complex concepts and enhance retention across extended steps. Directly\ngenerating a series of images using text-to-image models without considering\nthe context of previous steps results in inconsistent images, increasing\ncognitive load. Additionally, the generated images often miss objects or the\nattributes such as color, shape, and state of the objects are inaccurate. To\naddress these challenges, we propose LIGER, the first training-free framework\nfor Long-horizon Instruction GEneration with logic and attribute\nself-Reflection. LIGER first generates a draft image for each step with the\nhistorical prompt and visual memory of previous steps. This step-by-step\ngeneration approach maintains consistency between images in long-horizon tasks.\nMoreover, LIGER utilizes various image editing tools to rectify errors\nincluding wrong attributes, logic errors, object redundancy, and identity\ninconsistency in the draft images. Through this self-reflection mechanism,\nLIGER improves the logic and object attribute correctness of the images. To\nverify whether the generated images assist human understanding, we manually\ncurated a new benchmark consisting of various long-horizon tasks.\nHuman-annotated ground truth expressions reflect the human-defined criteria for\nhow an image should appear to be illustrative. Experiments demonstrate the\nvisual instructions generated by LIGER are more comprehensive compared with\nbaseline methods.",
      "tldr_zh": "这篇论文针对长-horizon 视觉指令生成的问题，提出 LIGER 框架，这是首个无需训练的系统，通过逻辑和属性 self-reflection 机制来解决图像不一致、对象缺失和属性错误（如颜色、形状）。LIGER 的工作流程包括为每个步骤生成草稿图像，利用历史提示和视觉记忆确保连续性，并应用图像编辑工具修正逻辑错误、对象冗余和身份不一致。实验结果显示，在新创建的长-horizon 任务基准上，LIGER 生成的视觉指令比基线方法更全面，提升了人类理解和保留效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13500v2",
      "published_date": "2025-03-12 05:11:02 UTC",
      "updated_date": "2025-04-06 05:45:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:28:01.358178"
    },
    {
      "arxiv_id": "2503.09069v1",
      "title": "Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows",
      "title_zh": "生成流中高阶轨迹细化的理论保证",
      "authors": [
        "Chengyue Gong",
        "Xiaoyu Li",
        "Yingyu Liang",
        "Jiangxuan Long",
        "Zhenmei Shi",
        "Zhao Song",
        "Yu Tian"
      ],
      "abstract": "Flow matching has emerged as a powerful framework for generative modeling,\noffering computational advantages over diffusion models by leveraging\ndeterministic Ordinary Differential Equations (ODEs) instead of stochastic\ndynamics. While prior work established the worst case optimality of standard\nflow matching under Wasserstein distances, the theoretical guarantees for\nhigher-order flow matching - which incorporates acceleration terms to refine\nsample trajectories - remain unexplored. In this paper, we bridge this gap by\nproving that higher-order flow matching preserves worst case optimality as a\ndistribution estimator. We derive upper bounds on the estimation error for\nsecond-order flow matching, demonstrating that the convergence rates depend\npolynomially on the smoothness of the target distribution (quantified via Besov\nspaces) and key parameters of the ODE dynamics. Our analysis employs neural\nnetwork approximations with carefully controlled depth, width, and sparsity to\nbound acceleration errors across both small and large time intervals,\nultimately unifying these results into a general worst case optimal bound for\nall time steps.",
      "tldr_zh": "这篇论文探讨了生成流模型（generative flows）中高阶轨迹细化的理论保证，特别针对 higher-order flow matching 的加速项进行了分析。作者证明了 higher-order flow matching 作为分布估计器能保持最坏情况最优性，并在 Wasserstein distances 下为第二阶 flow matching 导出了估计误差上界，这些界限依赖于目标分布的光滑度（通过 Besov spaces 量化）和 ODE 动态的关键参数。研究利用神经网络近似（控制深度、宽度和稀疏性）来限制加速误差，并统一了小和大时间间隔的结果，最终为生成模型的理论基础提供了全面的 worst case optimal 界。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2410.11261",
      "pdf_url": "http://arxiv.org/pdf/2503.09069v1",
      "published_date": "2025-03-12 05:07:07 UTC",
      "updated_date": "2025-03-12 05:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:28:13.747261"
    },
    {
      "arxiv_id": "2503.09068v1",
      "title": "Probing Network Decisions: Capturing Uncertainties and Unveiling Vulnerabilities Without Label Information",
      "title_zh": "翻译失败",
      "authors": [
        "Youngju Joung",
        "Sehyun Lee",
        "Jaesik Choi"
      ],
      "abstract": "To improve trust and transparency, it is crucial to be able to interpret the\ndecisions of Deep Neural classifiers (DNNs). Instance-level examinations, such\nas attribution techniques, are commonly employed to interpret the model\ndecisions. However, when interpreting misclassified decisions, human\nintervention may be required. Analyzing the attribu tions across each class\nwithin one instance can be particularly labor intensive and influenced by the\nbias of the human interpreter. In this paper, we present a novel framework to\nuncover the weakness of the classifier via counterfactual examples. A prober is\nintroduced to learn the correctness of the classifier's decision in terms of\nbinary code-hit or miss. It enables the creation of the counterfactual example\nconcerning the prober's decision. We test the performance of our prober's\nmisclassification detection and verify its effectiveness on the image\nclassification benchmark datasets. Furthermore, by generating counterfactuals\nthat penetrate the prober, we demonstrate that our framework effectively\nidentifies vulnerabilities in the target classifier without relying on label\ninformation on the MNIST dataset.",
      "tldr_zh": "本研究提出了一种新型框架，用于解释深度神经网络（DNNs）的决策，旨在捕获不确定性和揭示漏洞，而无需依赖标签信息。该框架引入一个“prober”模块，通过学习分类器的决策正确性（以二进制命中或未命中形式表示），生成反事实例子（counterfactual examples）来自动检测误分类并暴露弱点。与传统归因技术相比，该方法减少了人工干预和偏见影响。在图像分类基准数据集如MNIST上，实验验证了prober的有效性，提高了决策透明度和模型可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ICPRAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.09068v1",
      "published_date": "2025-03-12 05:05:58 UTC",
      "updated_date": "2025-03-12 05:05:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:28:23.729325"
    },
    {
      "arxiv_id": "2503.09642v2",
      "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k",
      "title_zh": "Open-Sora 2.0：在 20 万美元内训练一个商业级别的视频生成模型",
      "authors": [
        "Xiangyu Peng",
        "Zangwei Zheng",
        "Chenhui Shen",
        "Tom Young",
        "Xinying Guo",
        "Binluo Wang",
        "Hang Xu",
        "Hongxin Liu",
        "Mingyan Jiang",
        "Wenjun Li",
        "Yuhui Wang",
        "Anbang Ye",
        "Gang Ren",
        "Qianran Ma",
        "Wanying Liang",
        "Xiang Lian",
        "Xiwen Wu",
        "Yuting Zhong",
        "Zhuangyan Li",
        "Chaoyu Gong",
        "Guojun Lei",
        "Leijun Cheng",
        "Limin Zhang",
        "Minghao Li",
        "Ruijie Zhang",
        "Silan Hu",
        "Shijie Huang",
        "Xiaokang Wang",
        "Yuanheng Zhao",
        "Yuqi Wang",
        "Ziang Wei",
        "Yang You"
      ],
      "abstract": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
      "tldr_zh": "本研究介绍了 Open-Sora 2.0，一种以仅 20 万美元成本训练的商业级视频生成模型，证明了高性能模型训练成本的可控性。该模型通过数据 curation、模型 architecture、训练 strategy 和系统 optimization 等技术实现效率突破，并在人类评估和 VBench 分数上与领先模型如 HunyuanVideo 和 Runway Gen-3 Alpha 相当。Open-Sora 2.0 的完全开源旨在民主化视频生成技术，促进内容创作领域的创新和创意。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09642v2",
      "published_date": "2025-03-12 05:00:07 UTC",
      "updated_date": "2025-03-23 13:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:28:34.812715"
    },
    {
      "arxiv_id": "2503.09066v1",
      "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Wei Chia",
        "Jonathan Pan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level.",
      "tldr_zh": "本研究调查了大型语言模型(LLM)中安全状态和越狱(jailbreaking)状态的潜在子空间，通过提取隐藏激活并应用降维技术，揭示这些子空间并推导扰动向量，以诱导状态转换。结果显示，这种针对性干预能在部分提示中引发统计显著的越狱响应，并观察到扰动在模型层间传播，造成激活和响应的明显变化。该方法为AI安全提供新防御策略，从传统防护栏转向预先的、模型无关的表示级别干预，以中和对抗状态。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09066v1",
      "published_date": "2025-03-12 04:59:22 UTC",
      "updated_date": "2025-03-12 04:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:28:48.584162"
    },
    {
      "arxiv_id": "2503.09058v1",
      "title": "Implicit Contrastive Representation Learning with Guided Stop-gradient",
      "title_zh": "隐式对比表示学习及其引导停止梯度",
      "authors": [
        "Byeongchan Lee",
        "Sehyun Lee"
      ],
      "abstract": "In self-supervised representation learning, Siamese networks are a natural\narchitecture for learning transformation-invariance by bringing representations\nof positive pairs closer together. But it is prone to collapse into a\ndegenerate solution. To address the issue, in contrastive learning, a\ncontrastive loss is used to prevent collapse by moving representations of\nnegative pairs away from each other. But it is known that algorithms with\nnegative sampling are not robust to a reduction in the number of negative\nsamples. So, on the other hand, there are algorithms that do not use negative\npairs. Many positive-only algorithms adopt asymmetric network architecture\nconsisting of source and target encoders as a key factor in coping with\ncollapse. By exploiting the asymmetric architecture, we introduce a methodology\nto implicitly incorporate the idea of contrastive learning. As its\nimplementation, we present a novel method guided stop-gradient. We apply our\nmethod to benchmark algorithms SimSiam and BYOL and show that our method\nstabilizes training and boosts performance. We also show that the algorithms\nwith our method work well with small batch sizes and do not collapse even when\nthere is no predictor. The code is available at\nhttps://github.com/bych-lee/gsg.",
      "tldr_zh": "本论文探讨了自监督表示学习中 Siamese networks 的问题，该网络虽能学习变换不变性，但易于崩溃。为解决此，作者提出 guided stop-gradient 方法，通过不对称网络架构（源和目标编码器）隐式整合对比学习思想，而无需依赖负样本，从而稳定训练过程。实验结果显示，将该方法应用于 SimSiam 和 BYOL 算法后，性能得到提升，且算法在小批量大小下也能有效运行，甚至在没有预测器的情况下避免崩溃。该方法为正样本算法提供了更鲁棒的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Neurips 2023",
      "pdf_url": "http://arxiv.org/pdf/2503.09058v1",
      "published_date": "2025-03-12 04:46:53 UTC",
      "updated_date": "2025-03-12 04:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:00.148975"
    },
    {
      "arxiv_id": "2503.09051v1",
      "title": "TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction",
      "title_zh": "TreeX：通过关键子树提取生成全局图形 GNN 解释",
      "authors": [
        "Shengyao Lu",
        "Jiuding Yang",
        "Baochun Li",
        "Di Niu"
      ],
      "abstract": "The growing demand for transparency and interpretability in critical domains\nhas driven increased interests in comprehending the explainability of\nMessage-Passing (MP) Graph Neural Networks (GNNs). Although substantial\nresearch efforts have been made to generate explanations for individual graph\ninstances, identifying global explaining concepts for a GNN still poses great\nchallenges, especially when concepts are desired in a graphical form on the\ndataset level. While most prior works treat GNNs as black boxes, in this paper,\nwe propose to unbox GNNs by analyzing and extracting critical subtrees incurred\nby the inner workings of message passing, which correspond to critical\nsubgraphs in the datasets. By aggregating subtrees in an embedding space with\nan efficient algorithm, which does not require complex subgraph matching or\nsearch, we can make intuitive graphical explanations for Message-Passing GNNs\non local, class and global levels. We empirically show that our proposed\napproach not only generates clean subgraph concepts on a dataset level in\ncontrast to existing global explaining methods which generate non-graphical\nrules (e.g., language or embeddings) as explanations, but it is also capable of\nproviding explanations for individual instances with a comparable or even\nsuperior performance as compared to leading local-level GNN explainers.",
      "tldr_zh": "该论文提出TreeX方法，通过提取Message-Passing (MP) Graph Neural Networks (GNNs)中的关键子树（对应数据集中的关键子图），生成全局图形解释，以解决GNNs全局可解释性的挑战。该方法在嵌入空间使用高效聚合算法，避免了复杂的子图匹配或搜索，从而在局部、类和全局级别提供直观的图形概念解释。与现有方法相比，TreeX不仅在数据集级别生成干净的子图概念（如图形形式），而非非图形的规则（如语言或嵌入），还为单个实例提供解释性能可比或优于领先的局部解释器。实验结果证明了TreeX的有效性，为GNNs的可解释性研究提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09051v1",
      "published_date": "2025-03-12 04:36:28 UTC",
      "updated_date": "2025-03-12 04:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:11.350540"
    },
    {
      "arxiv_id": "2503.09046v2",
      "title": "Discovering Influential Neuron Path in Vision Transformers",
      "title_zh": "发现视觉Transformer中的影响性神经元路径",
      "authors": [
        "Yifan Wang",
        "Yifei Liu",
        "Yingdong Shi",
        "Changming Li",
        "Anqi Pang",
        "Sibei Yang",
        "Jingyi Yu",
        "Kan Ren"
      ],
      "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human\nunderstanding, posing challenges and risks for practical applications. While\nprior research has attempted to demystify these models through input\nattribution and neuron role analysis, there's been a notable gap in considering\nlayer-level information and the holistic path of information flow across\nlayers. In this paper, we investigate the significance of influential neuron\npaths within vision Transformers, which is a path of neurons from the model\ninput to output that impacts the model inference most significantly. We first\npropose a joint influence measure to assess the contribution of a set of\nneurons to the model outcome. And we further provide a layer-progressive neuron\nlocating approach that efficiently selects the most influential neuron at each\nlayer trying to discover the crucial neuron path from input to output within\nthe target model. Our experiments demonstrate the superiority of our method\nfinding the most influential neuron path along which the information flows,\nover the existing baseline solutions. Additionally, the neuron paths have\nillustrated that vision Transformers exhibit some specific inner working\nmechanism for processing the visual information within the same image category.\nWe further analyze the key effects of these neurons on the image classification\ntask, showcasing that the found neuron paths have already preserved the model\ncapability on downstream tasks, which may also shed some lights on real-world\napplications like model pruning. The project website including implementation\ncode is available at https://foundation-model-research.github.io/NeuronPath/.",
      "tldr_zh": "本研究针对Vision Transformers模型的不透明性，提出了一种发现影响神经元路径(influential neuron paths)的方法，以揭示从输入到输出的关键信息流路径。论文首先引入联合影响度量(joint influence measure)来评估神经元群体的贡献，并开发了层级渐进神经元定位方法(layer-progressive neuron locating approach)，以高效选择每个层的最影响神经元，从而识别出核心路径。实验结果显示，该方法优于现有基线，在处理同一图像类别视觉信息时揭示了Vision Transformers的特定内部机制，且这些路径保留了模型在图像分类等下游任务的能力，可能应用于模型修剪等实际场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09046v2",
      "published_date": "2025-03-12 04:10:46 UTC",
      "updated_date": "2025-04-09 03:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:30.131153"
    },
    {
      "arxiv_id": "2503.09035v1",
      "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
      "title_zh": "翻译失败",
      "authors": [
        "Shawn Azdam",
        "Pranav Doma",
        "Aliasghar Moj Arab"
      ],
      "abstract": "The next generation of active safety features in autonomous vehicles should\nbe capable of safely executing evasive hazard-avoidance maneuvers akin to those\nperformed by professional stunt drivers to achieve high-agility motion at the\nlimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,\nfor generating and executing high-dynamic stunt maneuvers in autonomous\nvehicles using large language model (LLM)-based agents as controllers. We\ntarget aggressive maneuvers, such as J-turns, within the CARLA simulation\nenvironment and demonstrate an iterative, prompt-based approach to refine\nvehicle control parameters, starting tabula rasa without retraining model\nweights. We propose an agentic architecture comprised of three specialized\nagents (1) a Query Enricher Agent for contextualizing user commands, (2) a\nDriver Agent for generating maneuver parameters, and (3) a Parameter Validator\nAgent that enforces physics-based and safety constraints. Experimental results\ndemonstrate successful J-turn execution across multiple vehicle models through\ntextual prompts that adapt to differing vehicle dynamics. We evaluate\nperformance via established success criteria and discuss limitations regarding\nnumeric precision and scenario complexity. Our findings underscore the\npotential of LLM-driven control for flexible, high-dynamic maneuvers, while\nhighlighting the importance of hybrid approaches that combine language-based\nreasoning with algorithmic validation.",
      "tldr_zh": "本研究提出 ManeuverGPT 框架，利用大型语言模型 (LLM) 驱动的代理控制系统，实现自动驾驶车辆的安全高动态特技 maneuvers，如 J-turns。该框架包括三个专门代理：Query Enricher Agent 用于上下文化用户命令、Driver Agent 用于生成 maneuvers 参数，以及 Parameter Validator Agent 用于强制物理和安全约束，通过迭代提示优化车辆控制，而无需重新训练模型。在 CARLA 模拟环境中，实验证明 ManeuverGPT 成功执行 J-turns 并适应不同车辆动态，性能评估显示显著成功，但受数字精度和场景复杂性限制，强调 LLM 驱动控制的潜力需结合算法验证。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "6 Pages, Submitted to IROS",
      "pdf_url": "http://arxiv.org/pdf/2503.09035v1",
      "published_date": "2025-03-12 03:51:41 UTC",
      "updated_date": "2025-03-12 03:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:36.021277"
    },
    {
      "arxiv_id": "2503.09033v2",
      "title": "RFUAV: A Benchmark Dataset for Unmanned Aerial Vehicle Detection and Identification",
      "title_zh": "RFUAV：用于无人机检测和识别的基准数据集",
      "authors": [
        "Rui Shi",
        "Xiaodong Yu",
        "Shengming Wang",
        "Yijia Zhang",
        "Lu Xu",
        "Peng Pan",
        "Chunlai Ma"
      ],
      "abstract": "In this paper, we propose RFUAV as a new benchmark dataset for\nradio-frequency based (RF-based) unmanned aerial vehicle (UAV) identification\nand address the following challenges: Firstly, many existing datasets feature a\nrestricted variety of drone types and insufficient volumes of raw data, which\nfail to meet the demands of practical applications. Secondly, existing datasets\noften lack raw data covering a broad range of signal-to-noise ratios (SNR), or\ndo not provide tools for transforming raw data to different SNR levels. This\nlimitation undermines the validity of model training and evaluation. Lastly,\nmany existing datasets do not offer open-access evaluation tools, leading to a\nlack of unified evaluation standards in current research within this field.\nRFUAV comprises approximately 1.3 TB of raw frequency data collected from 37\ndistinct UAVs using the Universal Software Radio Peripheral (USRP) device in\nreal-world environments. Through in-depth analysis of the RF data in RFUAV, we\ndefine a drone feature sequence called RF drone fingerprint, which aids in\ndistinguishing drone signals. In addition to the dataset, RFUAV provides a\nbaseline preprocessing method and model evaluation tools. Rigorous experiments\ndemonstrate that these preprocessing methods achieve state-of-the-art (SOTA)\nperformance using the provided evaluation tools. The RFUAV dataset and baseline\nimplementation are publicly available at https://github.com/kitoweeknd/RFUAV/.",
      "tldr_zh": "本论文提出 RFUAV 数据集，作为一个新的基准，用于基于射频 (RF-based) 的无人机 (UAV) 检测和识别，旨在解决现有数据集在无人机类型多样性、数据量不足以及信号噪声比 (SNR) 范围有限等问题。RFUAV 包含约 1.3 TB 的原始频率数据，从 37 种不同 UAV 使用 Universal Software Radio Peripheral (USRP) 设备在真实环境中收集，并定义了 RF drone fingerprint 序列来区分无人机信号。该数据集还提供基线预处理方法和模型评估工具，实验结果显示这些方法达到了 state-of-the-art (SOTA) 性能，且相关资源已在 GitHub 上公开。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "23 pages, 13 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.09033v2",
      "published_date": "2025-03-12 03:46:09 UTC",
      "updated_date": "2025-03-18 03:28:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:48.996209"
    },
    {
      "arxiv_id": "2503.09032v1",
      "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Younwoo Choi",
        "Muhammad Adil Asif",
        "Ziwen Han",
        "John Willes",
        "Rahul G. Krishnan"
      ],
      "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
      "tldr_zh": "这篇论文探讨了如何通过Contextual Fine-Tuning教导LLMs学习新知识，旨在解决快速演变领域中模型知识和推理能力的不足。作者提出了一种instruction tuning的泛化方法，使用instructional prompts模仿人类认知策略，将新概念与现有知识关联，从而指导训练过程并提升模型对领域特定知识的理解。实验结果显示，这种方法显著提高了LLMs在医疗和金融领域的新数据集上的快速微调性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09032v1",
      "published_date": "2025-03-12 03:45:53 UTC",
      "updated_date": "2025-03-12 03:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:29:59.397237"
    },
    {
      "arxiv_id": "2503.09020v2",
      "title": "Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Jiang",
        "Yujian Zhang",
        "Liang Lu",
        "Christoph Treude",
        "Xiaohong Su",
        "Shan Huang",
        "Tiantian Wang"
      ],
      "abstract": "Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 在代码生成中存在的质量问题（如代码风格和可维护性差），提出了一种新型的 Comparative Prefix-Tuning 方法，以提升代码质量。该方法使用一个属性特定的前缀添加到模型激活中，仅需训练一个前缀，并通过高质和低质代码对引入序列级别排名损失，帮助模型更好地区分代码优劣。实验在 Code Llama 7B 模型上显示，该方法在某些任务类别中将代码质量提升超过100%，同时保持功能正确性，并通过消融研究和泛化实验验证了其有效性和通用性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09020v2",
      "published_date": "2025-03-12 03:15:46 UTC",
      "updated_date": "2025-03-19 07:24:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:30:13.130116"
    },
    {
      "arxiv_id": "2503.09639v3",
      "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
      "title_zh": "翻译失败",
      "authors": [
        "Abe Bohan Hou",
        "Hongru Du",
        "Yichen Wang",
        "Jingyu Zhang",
        "Zixiao Wang",
        "Paul Pu Liang",
        "Daniel Khashabi",
        "Lauren Gardner",
        "Tianxing He"
      ],
      "abstract": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.",
      "tldr_zh": "本研究探讨是否能通过生成代理（Generative Agents）模拟人类行为，以减少对真实人类试验的依赖，并以疫苗犹豫（vaccine hesitancy）为例评估公共政策。研究引入 VacSim 框架，使用大型语言模型（LLMs）驱动的100个代理人，模拟基于人口统计数据的代理人口、社交网络动态以及针对疫苗态度的健康干预措施。实验结果显示，模型如 Llama 和 Qwen 能部分再现人类行为，但面临真实世界一致性挑战，如代理响应与人口特征不符；此工作呼吁进一步探索 LLM 驱动的社会模拟在政策开发中的潜力，而非提供最终指导。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09639v3",
      "published_date": "2025-03-12 02:54:15 UTC",
      "updated_date": "2025-04-02 15:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:30:24.899550"
    },
    {
      "arxiv_id": "2503.09008v1",
      "title": "Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement",
      "title_zh": "翻译失败",
      "authors": [
        "Huidong Liang",
        "Haitz Sáez de Ocáriz Borde",
        "Baskaran Sripathmanathan",
        "Michael Bronstein",
        "Xiaowen Dong"
      ],
      "abstract": "Long-range dependencies are critical for effective graph representation\nlearning, yet most existing datasets focus on small graphs tailored to\ninductive tasks, offering limited insight into long-range interactions. Current\nevaluations primarily compare models employing global attention (e.g., graph\ntransformers) with those using local neighborhood aggregation (e.g.,\nmessage-passing neural networks) without a direct measurement of long-range\ndependency. In this work, we introduce City-Networks, a novel large-scale\ntransductive learning dataset derived from real-world city roads. This dataset\nfeatures graphs with over $10^5$ nodes and significantly larger diameters than\nthose in existing benchmarks, naturally embodying long-range information. We\nannotate the graphs using an eccentricity-based approach, ensuring that the\nclassification task inherently requires information from distant nodes.\nFurthermore, we propose a model-agnostic measurement based on the Jacobians of\nneighbors from distant hops, offering a principled quantification of long-range\ndependencies. Finally, we provide theoretical justifications for both our\ndataset design and the proposed measurement - particularly by focusing on\nover-smoothing and influence score dilution - which establishes a robust\nfoundation for further exploration of long-range interactions in graph neural\nnetworks.",
      "tldr_zh": "这篇论文针对图机器学习（graph machine learning）中长距离交互（long-range interactions）的量化问题，指出现有数据集偏向小图和归纳任务，缺乏对远距离依赖的深入评估。作者引入 City-Networks 数据集，这是一个基于真实城市道路的大型转导学习数据集，包含超过10^5个节点的图和更大的直径，并采用 eccentricity-based 方法标注，确保分类任务依赖远距离节点信息。同时，提出一个基于 Jacobians 的模型无关测量方法，来量化长距离依赖的影响。论文通过理论分析 over-smoothing 和 influence score dilution，为图神经网络的长距离交互研究提供坚实基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.09008v1",
      "published_date": "2025-03-12 02:51:17 UTC",
      "updated_date": "2025-03-12 02:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:30:37.359959"
    },
    {
      "arxiv_id": "2503.09002v2",
      "title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers",
      "title_zh": "KNighter：利用 LLM 合成的检查器转变静态分析",
      "authors": [
        "Chenyuan Yang",
        "Zijie Zhao",
        "Zichen Xie",
        "Haoyu Li",
        "Lingming Zhang"
      ],
      "abstract": "Static analysis is a powerful technique for bug detection in critical systems\nlike operating system kernels. However, designing and implementing static\nanalyzers is challenging, time-consuming, and typically limited to predefined\nbug patterns. While large language models (LLMs) have shown promise for static\nanalysis, directly applying them to scan large systems remains impractical due\nto computational constraints and contextual limitations.\n  We present KNighter, the first approach that unlocks scalable LLM-based\nstatic analysis by automatically synthesizing static analyzers from historical\nbug patterns. Rather than using LLMs to directly analyze massive systems, our\nkey insight is leveraging LLMs to generate specialized static analyzers guided\nby historical patch knowledge. KNighter implements this vision through a\nmulti-stage synthesis pipeline that validates checker correctness against\noriginal patches and employs an automated refinement process to iteratively\nreduce false positives. Our evaluation on the Linux kernel demonstrates that\nKNighter generates high-precision checkers capable of detecting diverse bug\npatterns overlooked by existing human-written analyzers. To date,\nKNighter-synthesized checkers have discovered 92 new, critical, long-latent\nbugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and\n16 have been assigned CVE numbers. This work establishes an entirely new\nparadigm for scalable, reliable, and traceable LLM-based static analysis for\nreal-world systems via checker synthesis.",
      "tldr_zh": "该研究提出 KNighter，一种创新方法，通过利用大语言模型 (LLMs) 从历史 bug 模式自动合成静态分析检查器，解决传统静态分析在设计效率和覆盖范围上的局限性。该框架采用多阶段合成管道，包括基于原始补丁验证检查器的正确性，并通过自动精炼过程减少假阳性，从而实现可扩展的 LLM 基于静态分析。在 Linux 内核的评估中，KNighter 生成的高精度检查器检测了现有分析器忽略的多样 bug 模式，并发现了 92 个新的关键 bug（平均潜伏 4.3 年），其中 77 个已确认、57 个已修复、16 个获得 CVE 编号，标志着 LLM 在可靠静态分析领域的新范式。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.OS"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09002v2",
      "published_date": "2025-03-12 02:30:19 UTC",
      "updated_date": "2025-04-18 21:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:30:48.997489"
    },
    {
      "arxiv_id": "2503.13499v1",
      "title": "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
      "title_zh": "翻译失败",
      "authors": [
        "Rajeev Kumar",
        "Harishankar Kumar",
        "Kumari Shalini"
      ],
      "abstract": "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
      "tldr_zh": "这篇论文提出一个框架，利用 Knowledge Graphs (KG) 和 Large Language Models (LLMs) 来动态改写消息，实现个性化通信，针对医疗、教育和专业领域的上下文特定数据。框架通过在 KG 中链接消息实体（如个体、位置和事件），提取相关信息（如偏好、专业角色和文化规范），并结合原消息输入 LLM 生成个性化响应。在实验中，该方法在医疗领域实现了42%的消息接受率，在教育领域为53%，在专业招聘领域为78%。总体上，这种整合实体链接、事件检测和语言建模的方案为可扩展的受众特定通信提供了结构化解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13499v1",
      "published_date": "2025-03-12 02:17:15 UTC",
      "updated_date": "2025-03-12 02:17:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:31:01.129752"
    },
    {
      "arxiv_id": "2503.08994v1",
      "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixin Zhang",
        "Hongzhi Wang",
        "Ziqi Li",
        "Yabin Lu",
        "Yingze Li",
        "Yu Yan",
        "Yiming Guan"
      ],
      "abstract": "Research on learned cardinality estimation has achieved significant progress\nin recent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We conceptualize\nthese challenges as the \"Trilemma of Cardinality Estimation\", where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod for estimating both equi and non-equi join cardinality by leveraging the\nconditional probability distributions of individual tables in a decoupled\nmanner. (2) To meet the requirements of efficient training and inference for\nDistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a\nhigh-throughput conditional probability distribution estimation model. (3) We\nformally analyze the variance of existing similar methods and demonstrate that\nsuch approaches suffer from variance accumulation issues. To mitigate this\nproblem, DistJoin employs a selectivity-based approach rather than a\ncount-based approach to infer join cardinality, effectively reducing variance.\nIn summary, DistJoin not only represents the first data-driven method to\neffectively support both equi and non-equi joins but also demonstrates superior\naccuracy while enabling fast and flexible updates. We evaluate DistJoin on\nJOB-light and JOB-light-ranges, extending the evaluation to non-equi join\nconditions. The results demonstrate that our approach achieves the highest\naccuracy, robustness to data updates, generality, and comparable update and\ninference speed relative to existing methods.",
      "tldr_zh": "本文提出 DistJoin，一种基于 Adaptive Neural Predicate Modulation (ANPM) 的解耦连接基数估计器，旨在解决学到的基数估计中的“三难困境”：一般性、准确性和可更新性。DistJoin 通过利用单个表的条件概率分布进行等值和非等值 join cardinality 估计，并采用选择性方法而非基于计数的策略来减少方差积累问题，从而实现高效训练和推理。实验在 JOB-light 和 JOB-light-ranges 数据集上表明，DistJoin 表现出最高的准确性、鲁棒性和一般性，同时保持与现有方法相当的更新和推理速度。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08994v1",
      "published_date": "2025-03-12 02:07:08 UTC",
      "updated_date": "2025-03-12 02:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:31:13.078047"
    },
    {
      "arxiv_id": "2503.09638v1",
      "title": "Edge AI-Powered Real-Time Decision-Making for Autonomous Vehicles in Adverse Weather Conditions",
      "title_zh": "翻译失败",
      "authors": [
        "Milad Rahmati"
      ],
      "abstract": "Autonomous vehicles (AVs) are transforming modern transportation, but their\nreliability and safety are significantly challenged by harsh weather conditions\nsuch as heavy rain, fog, and snow. These environmental factors impair the\nperformance of cameras, LiDAR, and radar, leading to reduced situational\nawareness and increased accident risks. Conventional cloud-based AI systems\nintroduce communication delays, making them unsuitable for the rapid\ndecision-making required in real-time autonomous navigation. This paper\npresents a novel Edge AI-driven real-time decision-making framework designed to\nenhance AV responsiveness under adverse weather conditions. The proposed\napproach integrates convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) for improved perception, alongside reinforcement learning\n(RL)-based strategies to optimize vehicle control in uncertain environments. By\nprocessing data at the network edge, this system significantly reduces decision\nlatency while improving AV adaptability. The framework is evaluated using\nsimulated driving scenarios in CARLA and real-world data from the Waymo Open\nDataset, covering diverse weather conditions. Experimental results indicate\nthat the proposed model achieves a 40% reduction in processing time and a 25%\nenhancement in perception accuracy compared to conventional cloud-based\nsystems. These findings highlight the potential of Edge AI in improving AV\nautonomy, safety, and efficiency, paving the way for more reliable self-driving\ntechnology in challenging real-world environments.",
      "tldr_zh": "本研究针对自动驾驶车辆（AVs）在恶劣天气（如雨、雾和雪）下的感知和决策挑战，提出了一种Edge AI驱动的实时决策框架，以解决传统云端AI系统的延迟问题。该框架整合卷积神经网络（CNNs）和循环神经网络（RNNs）来提升环境感知，并采用强化学习（RL）策略优化车辆控制，从而在网络边缘处理数据并显著降低决策延迟。实验在CARLA模拟环境和Waymo Open Dataset的真实数据上进行，结果显示处理时间减少40%，感知准确性提高25%。这一创新方法为提升AVs的安全性、适应性和整体效率提供了可靠的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09638v1",
      "published_date": "2025-03-12 02:02:05 UTC",
      "updated_date": "2025-03-12 02:02:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:31:26.345947"
    },
    {
      "arxiv_id": "2503.08990v1",
      "title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing",
      "title_zh": "翻译失败",
      "authors": [
        "Vasudev Gohil"
      ],
      "abstract": "Large language models (LLMs) have shown great promise as language\nunderstanding and decision making tools, and they have permeated various\naspects of our everyday life. However, their widespread availability also comes\nwith novel risks, such as generating harmful, unethical, or offensive content,\nvia an attack called jailbreaking. Despite extensive efforts from LLM\ndevelopers to align LLMs using human feedback, they are still susceptible to\njailbreak attacks. To tackle this issue, researchers often employ red-teaming\nto understand and investigate jailbreak prompts. However, existing red-teaming\napproaches lack effectiveness, scalability, or both. To address these issues,\nwe propose JBFuzz, a novel effective, automated, and scalable red-teaming\ntechnique for jailbreaking LLMs.\n  JBFuzz is inspired by the success of fuzzing for detecting\nbugs/vulnerabilities in software. We overcome three challenges related to\neffectiveness and scalability by devising novel seed prompts, a lightweight\nmutation engine, and a lightweight and accurate evaluator for guiding the\nfuzzer. Assimilating all three solutions results in a potent fuzzer that only\nrequires black-box access to the target LLM. We perform extensive experimental\nevaluation of JBFuzz using nine popular and widely-used LLMs. We find that\nJBFuzz successfully jailbreaks all LLMs for various harmful/unethical\nquestions, with an average attack success rate of 99%. We also find that JBFuzz\nis extremely efficient as it jailbreaks a given LLM for a given question in 60\nseconds on average. Our work highlights the susceptibility of the\nstate-of-the-art LLMs to jailbreak attacks even after safety alignment, and\nserves as a valuable red-teaming tool for LLM developers.",
      "tldr_zh": "该论文提出 JBFuzz，一种基于 fuzzing 的自动化 red-teaming 技术，旨在高效有效地 jailbreaking 大型语言模型 (LLMs)，解决现有方法在有效性和可扩展性上的不足。JBFuzz 通过设计新型 seed prompts、轻量级 mutation engine 和精确评估器，克服了相关挑战，仅需黑盒访问即可运行。实验结果显示，在九个热门 LLMs 上，JBFuzz 的平均攻击成功率达到 99%，平均仅需 60 秒完成 jailbreak，突显了即使经过安全对齐的 LLMs 仍易受攻击，并为 LLM 开发者提供宝贵的 red-teaming 工具。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08990v1",
      "published_date": "2025-03-12 01:52:17 UTC",
      "updated_date": "2025-03-12 01:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:31:43.109234"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 122,
  "processed_papers_count": 122,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T01:32:09.784687"
}
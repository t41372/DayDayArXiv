[
  {
    "arxiv_id": "2503.22746v1",
    "title": "Susceptibility of Large Language Models to User-Driven Factors in Medical Queries",
    "authors": [
      "Kyung Ho Lim",
      "Ujin Kang",
      "Xiang Li",
      "Jin Sung Kim",
      "Young-Chul Jung",
      "Sangjoon Park",
      "Byung-Hoon Kim"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in healthcare, but their\nreliability is heavily influenced by user-driven factors such as question\nphrasing and the completeness of clinical information. In this study, we\nexamined how misinformation framing, source authority, model persona, and\nomission of key clinical details affect the diagnostic accuracy and reliability\nof LLM outputs. We conducted two experiments: one introducing misleading\nexternal opinions with varying assertiveness (perturbation test), and another\nremoving specific categories of patient information (ablation test). Using\npublic datasets (MedQA and Medbullets), we evaluated proprietary models\n(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)\nand open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All\nmodels were vulnerable to user-driven misinformation, with proprietary models\nespecially affected by definitive and authoritative language. Assertive tone\nhad the greatest negative impact on accuracy. In the ablation test, omitting\nphysical exam findings and lab results caused the most significant performance\ndrop. Although proprietary models had higher baseline accuracy, their\nperformance declined sharply under misinformation. These results highlight the\nneed for well-structured prompts and complete clinical context. Users should\navoid authoritative framing of misinformation and provide full clinical\ndetails, especially for complex cases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22746v1",
    "published_date": "2025-03-26 23:28:21 UTC",
    "updated_date": "2025-03-26 23:28:21 UTC"
  },
  {
    "arxiv_id": "2503.21036v1",
    "title": "The Art of Tool Interface Design",
    "authors": [
      "Yunnan Wu",
      "Paul Chen",
      "Deshank Baranwal",
      "Jinlong Zhou",
      "Jian Yuan"
    ],
    "abstract": "We present an agentic framework, Thinker, which achieves state of art\nperformance in challenging reasoning tasks for realistic customer service\nscenarios that involve complex business logic and human interactions via long\nhorizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success\nrate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success\nrate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker\neffectively closes the gap in reasoning capabilities between the base models by\nintroducing proper structure.\n  The key features of the Thinker framework are: (1) State-Machine Augmented\nGeneration (SMAG), which represents business logic as state machines and the\nLLM uses state machines as tools. (2) Delegation of tasks from the main\nreasoning loop to LLM-powered tools. (3) Adaptive context management.\n  Our prompting-only solution achieves signficant gains, while still\nmaintaining a standard agentic architecture with a ReAct style reasoning loop.\nThe key is to innovate on the tool interface design, as exemplified by SMAG and\nthe LLM-powered tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21036v1",
    "published_date": "2025-03-26 23:02:00 UTC",
    "updated_date": "2025-03-26 23:02:00 UTC"
  },
  {
    "arxiv_id": "2503.21011v1",
    "title": "Can Large Language Models Predict Associations Among Human Attitudes?",
    "authors": [
      "Ana Ma",
      "Derek Powell"
    ],
    "abstract": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.21011v1",
    "published_date": "2025-03-26 21:58:43 UTC",
    "updated_date": "2025-03-26 21:58:43 UTC"
  },
  {
    "arxiv_id": "2503.21000v1",
    "title": "Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kokil Jaidka",
      "Kaiyuan Tay",
      "Hansin Ahuja",
      "Niyati Chhaya"
    ],
    "abstract": "Supervised machine-learning models often underperform in predicting user\nbehaviors from conversational text, hindered by poor crowdsourced label quality\nand low NLP task accuracy. We introduce the Metadata-Sensitive\nWeighted-Encoding Ensemble Model (MSWEEM), which integrates annotator\nmeta-features like fatigue and speeding. First, our results show MSWEEM\noutperforms standard ensembles by 14\\% on held-out data and 12\\% on an\nalternative dataset. Second, we find that incorporating signals of annotator\nbehavior, such as speed and fatigue, significantly boosts model performance.\nThird, we find that annotators with higher qualifications, such as Master's,\ndeliver more consistent and faster annotations. Given the increasing\nuncertainty over annotation quality, our experiments show that understanding\nannotator patterns is crucial for enhancing model accuracy in user behavior\nprediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at CSCW 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21000v1",
    "published_date": "2025-03-26 21:30:48 UTC",
    "updated_date": "2025-03-26 21:30:48 UTC"
  },
  {
    "arxiv_id": "2503.20990v1",
    "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial Applications",
    "authors": [
      "Yupeng Cao",
      "Haohang Li",
      "Yangyang Yu",
      "Shashidhar Reddy Javaji",
      "Yueru He",
      "Jimin Huang",
      "Zining Zhu",
      "Qianqian Xie",
      "Xiao-yang Liu",
      "Koduvayur Subbalakshmi",
      "Meikang Qiu",
      "Sophia Ananiadou",
      "Jian-Yun Nie"
    ],
    "abstract": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce \\textsc{FinAudio}, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the \\textsc{FinAudio} benchmark.\nThen, we evaluate seven prevalent AudioLLMs on \\textsc{FinAudio}. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20990v1",
    "published_date": "2025-03-26 21:07:51 UTC",
    "updated_date": "2025-03-26 21:07:51 UTC"
  },
  {
    "arxiv_id": "2503.20981v1",
    "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
    "authors": [
      "Xiaoran Xu",
      "Zhaoqian Xue",
      "Chi Zhang",
      "Jhonatan Medri",
      "Junjie Xiong",
      "Jiayan Zhou",
      "Jin Jin",
      "Yongfeng Zhang",
      "Siyuan Ma",
      "Lingyao Li"
    ],
    "abstract": "Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20981v1",
    "published_date": "2025-03-26 20:45:01 UTC",
    "updated_date": "2025-03-26 20:45:01 UTC"
  },
  {
    "arxiv_id": "2503.20975v1",
    "title": "Competitive Multi-armed Bandit Games for Resource Sharing",
    "authors": [
      "Hongbo Li",
      "Lingjie Duan"
    ],
    "abstract": "In modern resource-sharing systems, multiple agents access limited resources\nwith unknown stochastic conditions to perform tasks. When multiple agents\naccess the same resource (arm) simultaneously, they compete for successful\nusage, leading to contention and reduced rewards. This motivates our study of\ncompetitive multi-armed bandit (CMAB) games. In this paper, we study a new\nN-player K-arm competitive MAB game, where non-myopic players (agents) compete\nwith each other to form diverse private estimations of unknown arms over time.\nTheir possible collisions on same arms and time-varying nature of arm rewards\nmake the policy analysis more involved than existing studies for myopic\nplayers. We explicitly analyze the threshold-based structures of social optimum\nand existing selfish policy, showing that the latter causes prolonged\nconvergence time $\\Omega(\\frac{K}{\\eta^2}\\ln({\\frac{KN}{\\delta}}))$, while\nsocially optimal policy with coordinated communication reduces it to\n$\\mathcal{O}(\\frac{K}{N\\eta^2}\\ln{(\\frac{K}{\\delta})})$. Based on the\ncomparison, we prove that the competition among selfish players for the best\narm can result in an infinite price of anarchy (PoA), indicating an arbitrarily\nlarge efficiency loss compared to social optimum. We further prove that no\ninformational (non-monetary) mechanism (including Bayesian persuasion) can\nreduce the infinite PoA, as the strategic misreporting by non-myopic players\nundermines such approaches. To address this, we propose a Combined\nInformational and Side-Payment (CISP) mechanism, which provides socially\noptimal arm recommendations with proper informational and monetary incentives\nto players according to their time-varying private beliefs. Our CISP mechanism\nkeeps ex-post budget balanced for social planner and ensures truthful reporting\nfrom players, achieving the minimum PoA=1 and same convergence time as social\noptimum.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "This paper has been accepted by IEEE TMC",
    "pdf_url": "http://arxiv.org/pdf/2503.20975v1",
    "published_date": "2025-03-26 20:35:18 UTC",
    "updated_date": "2025-03-26 20:35:18 UTC"
  },
  {
    "arxiv_id": "2503.20959v1",
    "title": "Sociotechnical Effects of Machine Translation",
    "authors": [
      "Joss Moorkens",
      "Andy Way",
      "Séamus Lankford"
    ],
    "abstract": "While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20959v1",
    "published_date": "2025-03-26 19:49:46 UTC",
    "updated_date": "2025-03-26 19:49:46 UTC"
  },
  {
    "arxiv_id": "2503.20952v1",
    "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
    "authors": [
      "Caspar Meijer",
      "Jiyue Huang",
      "Shreshtha Sharma",
      "Elena Lazovik",
      "Lydia Y. Chen"
    ],
    "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20952v1",
    "published_date": "2025-03-26 19:35:49 UTC",
    "updated_date": "2025-03-26 19:35:49 UTC"
  },
  {
    "arxiv_id": "2503.20950v1",
    "title": "DEMENTIA-PLAN: An Agent-Based Framework for Multi-Knowledge Graph Retrieval-Augmented Generation in Dementia Care",
    "authors": [
      "Yutong Song",
      "Chenhan Lyu",
      "Pengfei Zhang",
      "Sabine Brunswicker",
      "Nikil Dutt",
      "Amir Rahmani"
    ],
    "abstract": "Mild-stage dementia patients primarily experience two critical symptoms:\nsevere memory loss and emotional instability. To address these challenges, we\npropose DEMENTIA-PLAN, an innovative retrieval-augmented generation framework\nthat leverages large language models to enhance conversational support. Our\nmodel employs a multiple knowledge graph architecture, integrating various\ndimensional knowledge representations including daily routine graphs and life\nmemory graphs. Through this multi-graph architecture, DEMENTIA-PLAN\ncomprehensively addresses both immediate care needs and facilitates deeper\nemotional resonance through personal memories, helping stabilize patient mood\nwhile providing reliable memory support. Our notable innovation is the\nself-reflection planning agent, which systematically coordinates knowledge\nretrieval and semantic integration across multiple knowledge graphs, while\nscoring retrieved content from daily routine and life memory graphs to\ndynamically adjust their retrieval weights for optimized response generation.\nDEMENTIA-PLAN represents a significant advancement in the clinical application\nof large language models for dementia care, bridging the gap between AI tools\nand caregivers interventions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI 2025 Workshop on Knowledge Graphs for Personalized\n  Public Health",
    "pdf_url": "http://arxiv.org/pdf/2503.20950v1",
    "published_date": "2025-03-26 19:34:04 UTC",
    "updated_date": "2025-03-26 19:34:04 UTC"
  },
  {
    "arxiv_id": "2503.22742v2",
    "title": "Adaptive Integrated Layered Attention (AILA)",
    "authors": [
      "William Claster",
      "Suhas KM",
      "Dhairya Gundechia"
    ],
    "abstract": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IR",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22742v2",
    "published_date": "2025-03-26 19:32:31 UTC",
    "updated_date": "2025-05-12 21:58:10 UTC"
  },
  {
    "arxiv_id": "2503.20936v1",
    "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos",
    "authors": [
      "Daniel Etaat",
      "Dvij Kalaria",
      "Nima Rahmanian",
      "Shankar Sastry"
    ],
    "abstract": "Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20936v1",
    "published_date": "2025-03-26 19:11:22 UTC",
    "updated_date": "2025-03-26 19:11:22 UTC"
  },
  {
    "arxiv_id": "2503.20925v1",
    "title": "Prototype Guided Backdoor Defense",
    "authors": [
      "Venkat Adithya Amula",
      "Sunayana Samavedam",
      "Saurabh Saini",
      "Avani Gupta",
      "Narayanan P J"
    ],
    "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20925v1",
    "published_date": "2025-03-26 18:58:53 UTC",
    "updated_date": "2025-03-26 18:58:53 UTC"
  },
  {
    "arxiv_id": "2503.22740v1",
    "title": "CSPO: Cross-Market Synergistic Stock Price Movement Forecasting with Pseudo-volatility Optimization",
    "authors": [
      "Sida Lin",
      "Yankai Chen",
      "Yiyan Qi",
      "Chenhao Ma",
      "Bokai Cao",
      "Yifei Zhang",
      "Xue Liu",
      "Jian Guo"
    ],
    "abstract": "The stock market, as a cornerstone of the financial markets, places\nforecasting stock price movements at the forefront of challenges in\nquantitative finance. Emerging learning-based approaches have made significant\nprogress in capturing the intricate and ever-evolving data patterns of modern\nmarkets. With the rapid expansion of the stock market, it presents two\ncharacteristics, i.e., stock exogeneity and volatility heterogeneity, that\nheighten the complexity of price forecasting. Specifically, while stock\nexogeneity reflects the influence of external market factors on price\nmovements, volatility heterogeneity showcases the varying difficulty in\nmovement forecasting against price fluctuations. In this work, we introduce the\nframework of Cross-market Synergy with Pseudo-volatility Optimization (CSPO).\nSpecifically, CSPO implements an effective deep neural architecture to leverage\nexternal futures knowledge. This enriches stock embeddings with cross-market\ninsights and thus enhances the CSPO's predictive capability. Furthermore, CSPO\nincorporates pseudo-volatility to model stock-specific forecasting confidence,\nenabling a dynamic adaptation of its optimization process to improve accuracy\nand robustness. Our extensive experiments, encompassing industrial evaluation\nand public benchmarking, highlight CSPO's superior performance over existing\nmethods and effectiveness of all proposed modules contained therein.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22740v1",
    "published_date": "2025-03-26 18:58:15 UTC",
    "updated_date": "2025-03-26 18:58:15 UTC"
  },
  {
    "arxiv_id": "2503.20914v1",
    "title": "D4R -- Exploring and Querying Relational Graphs Using Natural Language and Large Language Models -- the Case of Historical Documents",
    "authors": [
      "Michel Boeglin",
      "David Kahn",
      "Josiane Mothe",
      "Diego Ortiz",
      "David Panzoli"
    ],
    "abstract": "D4R is a digital platform designed to assist non-technical users,\nparticularly historians, in exploring textual documents through advanced\ngraphical tools for text analysis and knowledge extraction. By leveraging a\nlarge language model, D4R translates natural language questions into Cypher\nqueries, enabling the retrieval of data from a Neo4J database. A user-friendly\ngraphical interface allows for intuitive interaction, enabling users to\nnavigate and analyse complex relational data extracted from unstructured\ntextual documents. Originally designed to bridge the gap between AI\ntechnologies and historical research, D4R's capabilities extend to various\nother domains. A demonstration video and a live software demo are available.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3; H.3.3; I.2.7"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20914v1",
    "published_date": "2025-03-26 18:41:42 UTC",
    "updated_date": "2025-03-26 18:41:42 UTC"
  },
  {
    "arxiv_id": "2503.20903v1",
    "title": "Assessing Generative Models for Structured Data",
    "authors": [
      "Reilly Cannon",
      "Nicolette M. Laird",
      "Caesar Vazquez",
      "Andy Lin",
      "Amy Wagler",
      "Tony Chiang"
    ],
    "abstract": "Synthetic tabular data generation has emerged as a promising method to\naddress limited data availability and privacy concerns. With the sharp increase\nin the performance of large language models in recent years, researchers have\nbeen interested in applying these models to the generation of tabular data.\nHowever, little is known about the quality of the generated tabular data from\nlarge language models. The predominant method for assessing the quality of\nsynthetic tabular data is the train-synthetic-test-real approach, where the\nartificial examples are compared to the original by how well machine learning\nmodels, trained separately on the real and synthetic sets, perform in some\ndownstream tasks. This method does not directly measure how closely the\ndistribution of generated data approximates that of the original. This paper\nintroduces rigorous methods for directly assessing synthetic tabular data\nagainst real data by looking at inter-column dependencies within the data. We\nfind that large language models (GPT-2), both when queried via few-shot\nprompting and when fine-tuned, and GAN (CTGAN) models do not produce data with\ndependencies that mirror the original real data. Results from this study can\ninform future practice in synthetic data generation to improve data quality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20903v1",
    "published_date": "2025-03-26 18:19:05 UTC",
    "updated_date": "2025-03-26 18:19:05 UTC"
  },
  {
    "arxiv_id": "2503.20884v1",
    "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
    "authors": [
      "Usama Zafar",
      "André Teixeira",
      "Salman Toor"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20884v1",
    "published_date": "2025-03-26 18:00:56 UTC",
    "updated_date": "2025-03-26 18:00:56 UTC"
  },
  {
    "arxiv_id": "2503.20871v3",
    "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
    "authors": [
      "Silin Gao",
      "Sheryl Mathew",
      "Li Mi",
      "Sepideh Mamooler",
      "Mengjie Zhao",
      "Hiromi Wakaki",
      "Yuki Mitsufuji",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ],
    "abstract": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.20871v3",
    "published_date": "2025-03-26 18:00:03 UTC",
    "updated_date": "2025-04-03 09:28:19 UTC"
  },
  {
    "arxiv_id": "2503.20786v1",
    "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
    "authors": [
      "Sondos Mahmoud Bsharat",
      "Mukul Ranjan",
      "Aidar Myrzakhan",
      "Jiacheng Liu",
      "Bowei Guo",
      "Shengkun Tang",
      "Zhuang Liu",
      "Yuanzhi Li",
      "Zhiqiang Shen"
    ],
    "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU",
    "pdf_url": "http://arxiv.org/pdf/2503.20786v1",
    "published_date": "2025-03-26 17:59:56 UTC",
    "updated_date": "2025-03-26 17:59:56 UTC"
  },
  {
    "arxiv_id": "2503.20853v1",
    "title": "Unified Multimodal Discrete Diffusion",
    "authors": [
      "Alexander Swerdlow",
      "Mihir Prabhudesai",
      "Siddharth Gandhi",
      "Deepak Pathak",
      "Katerina Fragkiadaki"
    ],
    "abstract": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Website: https://unidisc.github.io",
    "pdf_url": "http://arxiv.org/pdf/2503.20853v1",
    "published_date": "2025-03-26 17:59:51 UTC",
    "updated_date": "2025-03-26 17:59:51 UTC"
  },
  {
    "arxiv_id": "2503.20783v1",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20783v1",
    "published_date": "2025-03-26 17:59:14 UTC",
    "updated_date": "2025-03-26 17:59:14 UTC"
  },
  {
    "arxiv_id": "2504.02855v1",
    "title": "Exploration of Multi-Element Collaborative Research and Application for Modern Power System Based on Generative Large Models",
    "authors": [
      "Lu Cheng",
      "Qixiu Zhang",
      "Beibei Xu",
      "Zhiwei Huang",
      "Cirun Zhang",
      "Yanan Lyu",
      "Fan Zhang"
    ],
    "abstract": "The transition to intelligent, low-carbon power systems necessitates advanced\noptimization strategies for managing renewable energy integration, energy\nstorage, and carbon emissions. Generative Large Models (GLMs) provide a\ndata-driven approach to enhancing forecasting, scheduling, and market\noperations by processing multi-source data and capturing complex system\ndynamics. This paper explores the role of GLMs in optimizing load-side\nmanagement, energy storage utilization, and electricity carbon, with a focus on\nSmart Wide-area Hybrid Energy Systems with Storage and Carbon (SGLSC). By\nleveraging spatiotemporal modeling and reinforcement learning, GLMs enable\ndynamic energy scheduling, improve grid stability, enhance carbon trading\nstrategies, and strengthen resilience against extreme weather events. The\nproposed framework highlights the transformative potential of GLMs in achieving\nefficient, adaptive, and low-carbon power system operations.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02855v1",
    "published_date": "2025-03-26 17:58:49 UTC",
    "updated_date": "2025-03-26 17:58:49 UTC"
  },
  {
    "arxiv_id": "2503.20756v1",
    "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems",
    "authors": [
      "Chenxi Wang",
      "Jizhan Fang",
      "Xiang Chen",
      "Bozhong Tian",
      "Ziwen Xu",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.20756v1",
    "published_date": "2025-03-26 17:45:29 UTC",
    "updated_date": "2025-03-26 17:45:29 UTC"
  },
  {
    "arxiv_id": "2503.20752v2",
    "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
    "authors": [
      "Huajie Tan",
      "Yuheng Ji",
      "Xiaoshuai Hao",
      "Minglan Lin",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "abstract": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation. Experimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\nProject website: https://tanhuajie.github.io/ReasonRFT",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20752v2",
    "published_date": "2025-03-26 17:38:06 UTC",
    "updated_date": "2025-03-27 03:13:00 UTC"
  },
  {
    "arxiv_id": "2503.20750v1",
    "title": "Optimal Scaling Laws for Efficiency Gains in a Theoretical Transformer-Augmented Sectional MoE Framework",
    "authors": [
      "Soham Sane"
    ],
    "abstract": "This paper introduces a theoretical framework for a Transformer-augmented,\nsectional Mixture-of-Experts (MoE) architecture that aims to enhance\ncomputational efficiency while preserving model scalability. Unlike\nconventional MoE models, which route entire token embeddings to selected\nexperts, our approach portions the embedding dimension itself -- assigning\nsegments of each token's representation to dedicated experts. To combat losses\nin token representation, we utilize a pre-expert transformer layer to recompute\nattention across tokens and reduce the sequence length dimensionality. We\nextend our theory by deriving optimal scaling laws that a non-linear\nrelationship between the number of experts and factors such as model\ndimensionality, sequence length, and system overhead. These formulations yield\nclosed-form and numerically-solvable expressions for identifying the optimal\nexpert count under given architectural and hardware constraints. As a result,\nour framework not only provides theoretical bounds for computing efficiency\nwith varying frameworks but also guides practical design choices for scaling\nlarge models effectively. While empirical validation is pending, we present a\ncomprehensive experimental road map to evaluate the framework's efficiency,\nscalability, and practicality in future work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20750v1",
    "published_date": "2025-03-26 17:33:38 UTC",
    "updated_date": "2025-03-26 17:33:38 UTC"
  },
  {
    "arxiv_id": "2503.20744v1",
    "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching",
    "authors": [
      "Guoqiang Zhang",
      "Kenta Niwa",
      "J. P. Lewis",
      "Cedric Mesnage",
      "W. Bastiaan Kleijn"
    ],
    "abstract": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20744v1",
    "published_date": "2025-03-26 17:29:08 UTC",
    "updated_date": "2025-03-26 17:29:08 UTC"
  },
  {
    "arxiv_id": "2503.20742v2",
    "title": "Quantum Neural Network Restatement of Markov Jump Process",
    "authors": [
      "Z. Zarezadeh",
      "N. Zarezadeh"
    ],
    "abstract": "Despite the many challenges in exploratory data analysis, artificial neural\nnetworks have motivated strong interests in scientists and researchers both in\ntheoretical as well as practical applications. Among sources of such popularity\nof artificial neural networks the ability of modeling non-linear dynamical\nsystems, generalization, and adaptation possibilities should be mentioned.\nDespite this, there is still significant debate about the role of various\nunderlying stochastic processes in stabilizing a unique structure for data\nlearning and prediction. One of such obstacles to the theoretical and numerical\nstudy of machine intelligent systems is the curse of dimensionality and the\nsampling from high-dimensional probability distributions. In general, this\ncurse prevents efficient description of states, providing a significant\ncomplexity barrier for the system to be efficiently described and studied. In\nthis strand of research, direct treatment and description of such abstract\nnotions of learning theory in terms of quantum information be one of the most\nfavorable candidates. Hence, the subject matter of these articles is devoted to\nproblems of design, adaptation and the formulations of computationally hard\nproblems in terms of quantum mechanical systems. In order to characterize the\nmicroscopic description of such dynamics in the language of inferential\nstatistics, covariance matrix estimation of d-dimensional Gaussian densities\nand Bayesian interpretation of eigenvalue problem for dynamical systems is\nassessed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20742v2",
    "published_date": "2025-03-26 17:25:11 UTC",
    "updated_date": "2025-03-28 16:24:37 UTC"
  },
  {
    "arxiv_id": "2503.20739v1",
    "title": "Emotion Detection and Music Recommendation System",
    "authors": [
      "Swetha Kambham",
      "Hubert Jhonson",
      "Sai Prathap Reddy Kambham"
    ],
    "abstract": "As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20739v1",
    "published_date": "2025-03-26 17:22:06 UTC",
    "updated_date": "2025-03-26 17:22:06 UTC"
  },
  {
    "arxiv_id": "2503.22736v1",
    "title": "Cyborg Data: Merging Human with AI Generated Training Data",
    "authors": [
      "Kai North",
      "Christopher Ormerod"
    ],
    "abstract": "Automated scoring (AS) systems used in large-scale assessment have\ntraditionally used small statistical models that require a large quantity of\nhand-scored data to make accurate predictions, which can be time-consuming and\ncostly. Generative Large Language Models are trained on many tasks and have\nshown impressive abilities to generalize to new tasks with little to no data.\nWhile these models require substantially more computational power to make\npredictions, they still require some fine-tuning to meet operational standards.\nEvidence suggests that these models can exceed human-human levels of agreement\neven when fine-tuned on small amounts of data. With this in mind, we propose a\nmodel distillation pipeline in which a large generative model, a Teacher,\nteaches a much smaller model, a Student. The Teacher, trained on a small subset\nof the training data, is used to provide scores on the remaining training data,\nwhich is then used to train the Student. We call the resulting dataset \"Cyborg\nData\", as it combines human and machine-scored responses. Our findings show\nthat Student models trained on \"Cyborg Data\" show performance comparable to\ntraining on the entire dataset, while only requiring 10% of the original\nhand-scored data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.22736v1",
    "published_date": "2025-03-26 16:38:20 UTC",
    "updated_date": "2025-03-26 16:38:20 UTC"
  },
  {
    "arxiv_id": "2503.20688v1",
    "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
    "authors": [
      "Eloy Anguiano Batanero",
      "Ángela Fernández",
      "Álvaro Barbero"
    ],
    "abstract": "The increasing complexity of power grid management, driven by the emergence\nof prosumers and the demand for cleaner energy solutions, has needed innovative\napproaches to ensure stability and efficiency. This paper presents a novel\napproach within the model-free framework of reinforcement learning, aimed at\noptimizing power network operations without prior expert knowledge. We\nintroduce a masked topological action space, enabling agents to explore diverse\nstrategies for cost reduction while maintaining reliable service using the\nstate logic as a guide for choosing proper actions. Through extensive\nexperimentation across 20 different scenarios in a simulated 5-substation\nenvironment, we demonstrate that our approach achieves a consistent reduction\nin power losses, while ensuring grid stability against potential blackouts. The\nresults underscore the effectiveness of combining dynamic observation\nformalization with opponent-based training, showing a viable way for autonomous\nmanagement solutions in modern energy systems or even for building a\nfoundational model for this field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20688v1",
    "published_date": "2025-03-26 16:20:30 UTC",
    "updated_date": "2025-03-26 16:20:30 UTC"
  },
  {
    "arxiv_id": "2503.20685v2",
    "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
    "authors": [
      "Yuhao Huang",
      "Ao Chang",
      "Haoran Dou",
      "Xing Tao",
      "Xinrui Zhou",
      "Yan Cao",
      "Ruobing Huang",
      "Alejandro F Frangi",
      "Lingyun Bao",
      "Xin Yang",
      "Dong Ni"
    ],
    "abstract": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Medical Image Analysis. 24 pages, 13 figures, 20 tabels",
    "pdf_url": "http://arxiv.org/pdf/2503.20685v2",
    "published_date": "2025-03-26 16:20:02 UTC",
    "updated_date": "2025-03-27 06:16:16 UTC"
  },
  {
    "arxiv_id": "2503.20676v1",
    "title": "Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning",
    "authors": [
      "Gongzhu Yin",
      "Hongli Zhang",
      "Yuchen Yang",
      "Yi Luo"
    ],
    "abstract": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published in Proceedings of the 31st ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining V.1 (KDD'25)",
    "pdf_url": "http://arxiv.org/pdf/2503.20676v1",
    "published_date": "2025-03-26 16:09:54 UTC",
    "updated_date": "2025-03-26 16:09:54 UTC"
  },
  {
    "arxiv_id": "2503.20848v1",
    "title": "The Backfiring Effect of Weak AI Safety Regulation",
    "authors": [
      "Benjamin Laufer",
      "Jon Kleinberg",
      "Hoda Heidari"
    ],
    "abstract": "Recent policy proposals aim to improve the safety of general-purpose AI, but\nthere is little understanding of the efficacy of different regulatory\napproaches to AI safety. We present a strategic model that explores the\ninteractions between the regulator, the general-purpose AI technology creators,\nand domain specialists--those who adapt the AI for specific applications. Our\nanalysis examines how different regulatory measures, targeting different parts\nof the development chain, affect the outcome of the development process. In\nparticular, we assume AI technology is described by two key attributes: safety\nand performance. The regulator first sets a minimum safety standard that\napplies to one or both players, with strict penalties for non-compliance. The\ngeneral-purpose creator then develops the technology, establishing its initial\nsafety and performance levels. Next, domain specialists refine the AI for their\nspecific use cases, and the resulting revenue is distributed between the\nspecialist and generalist through an ex-ante bargaining process. Our analysis\nof this game reveals two key insights: First, weak safety regulation imposed\nonly on the domain specialists can backfire. While it might seem logical to\nregulate use cases (as opposed to the general-purpose technology), our analysis\nshows that weak regulations targeting domain specialists alone can\nunintentionally reduce safety. This effect persists across a wide range of\nsettings. Second, in sharp contrast to the previous finding, we observe that\nstronger, well-placed regulation can in fact benefit all players subjected to\nit. When regulators impose appropriate safety standards on both AI creators and\ndomain specialists, the regulation functions as a commitment mechanism, leading\nto safety and performance gains, surpassing what is achieved under no\nregulation or regulating one player only.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CY",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "28 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20848v1",
    "published_date": "2025-03-26 16:08:22 UTC",
    "updated_date": "2025-03-26 16:08:22 UTC"
  },
  {
    "arxiv_id": "2503.20658v1",
    "title": "Probabilistic Forecasting for Network Resource Analysis in Integrated Terrestrial and Non-Terrestrial Networks",
    "authors": [
      "Cristian J. Vaca-Rubio",
      "Vaishnavi Kasuluru",
      "Engin Zeydan",
      "Luis Blanco",
      "Roberto Pereira",
      "Marius Caus",
      "Kapal Dev"
    ],
    "abstract": "Efficient resource management is critical for Non-Terrestrial Networks (NTNs)\nto provide consistent, high-quality service in remote and under-served regions.\nWhile traditional single-point prediction methods, such as Long-Short Term\nMemory (LSTM), have been used in terrestrial networks, they often fall short in\nNTNs due to the complexity of satellite dynamics, signal latency and coverage\nvariability. Probabilistic forecasting, which quantifies the uncertainties of\nthe predictions, is a robust alternative. In this paper, we evaluate the\napplication of probabilistic forecasting techniques, in particular SFF, to NTN\nresource allocation scenarios. Our results show their effectiveness in\npredicting bandwidth and capacity requirements in different NTN segments of\nprobabilistic forecasting compared to single-point prediction techniques such\nas LSTM. The results show the potential of black probabilistic forecasting\nmodels to provide accurate and reliable predictions and to quantify their\nuncertainty, making them indispensable for optimizing NTN resource allocation.\nAt the end of the paper, we also present application scenarios and a\nstandardization roadmap for the use of probabilistic forecasting in integrated\nTerrestrial Network (TN)-NTN environments.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20658v1",
    "published_date": "2025-03-26 15:54:46 UTC",
    "updated_date": "2025-03-26 15:54:46 UTC"
  },
  {
    "arxiv_id": "2503.20654v1",
    "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports",
    "authors": [
      "Xiangwen Zhang",
      "Qian Zhang",
      "Longfei Han",
      "Qiang Qu",
      "Xiaoming Chen"
    ],
    "abstract": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20654v1",
    "published_date": "2025-03-26 15:50:42 UTC",
    "updated_date": "2025-03-26 15:50:42 UTC"
  },
  {
    "arxiv_id": "2503.20648v1",
    "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes",
    "authors": [
      "Raj Sanjay Shah",
      "Lei Xu",
      "Qianchu Liu",
      "Jon Burnsky",
      "Drew Bertagnolli",
      "Chaitanya Shivade"
    ],
    "abstract": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20648v1",
    "published_date": "2025-03-26 15:40:40 UTC",
    "updated_date": "2025-03-26 15:40:40 UTC"
  },
  {
    "arxiv_id": "2503.20634v1",
    "title": "Procedural Knowledge Ontology (PKO)",
    "authors": [
      "Valentina Anita Carriero",
      "Mario Scrocca",
      "Ilaria Baroni",
      "Antonia Azzini",
      "Irene Celino"
    ],
    "abstract": "Processes, workflows and guidelines are core to ensure the correct\nfunctioning of industrial companies: for the successful operations of factory\nlines, machinery or services, often industry operators rely on their past\nexperience and know-how. The effect is that this Procedural Knowledge (PK)\nremains tacit and, as such, difficult to exploit efficiently and effectively.\nThis paper presents PKO, the Procedural Knowledge Ontology, which enables the\nexplicit modeling of procedures and their executions, by reusing and extending\nexisting ontologies. PKO is built on requirements collected from three\nheterogeneous industrial use cases and can be exploited by any AI and\ndata-driven tools that rely on a shared and interoperable representation to\nsupport the governance of PK throughout its life cycle. We describe its\nstructure and design methodology, and outline its relevance, quality, and\nimpact by discussing applications leveraging PKO for PK elicitation and\nexploitation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20634v1",
    "published_date": "2025-03-26 15:28:30 UTC",
    "updated_date": "2025-03-26 15:28:30 UTC"
  },
  {
    "arxiv_id": "2503.20630v1",
    "title": "$β$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
    "authors": [
      "Haci Ismail Aslan",
      "Philipp Wiesner",
      "Ping Xiong",
      "Odej Kao"
    ],
    "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This is the author's version of the paper accepted at EuroMLSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20630v1",
    "published_date": "2025-03-26 15:24:07 UTC",
    "updated_date": "2025-03-26 15:24:07 UTC"
  },
  {
    "arxiv_id": "2503.20623v1",
    "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions",
    "authors": [
      "Alessandro Maisto"
    ],
    "abstract": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.20623v1",
    "published_date": "2025-03-26 15:10:47 UTC",
    "updated_date": "2025-03-26 15:10:47 UTC"
  },
  {
    "arxiv_id": "2503.20844v1",
    "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui",
      "Yue Gao"
    ],
    "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20844v1",
    "published_date": "2025-03-26 15:08:58 UTC",
    "updated_date": "2025-03-26 15:08:58 UTC"
  },
  {
    "arxiv_id": "2503.20613v1",
    "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui"
    ],
    "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20613v1",
    "published_date": "2025-03-26 15:00:07 UTC",
    "updated_date": "2025-03-26 15:00:07 UTC"
  },
  {
    "arxiv_id": "2503.20607v1",
    "title": "A decision-theoretic approach to dealing with uncertainty in quantum mechanics",
    "authors": [
      "Keano De Vos",
      "Gert de Cooman",
      "Alexander Erreygers",
      "Jasper De Bock"
    ],
    "abstract": "We provide a decision-theoretic framework for dealing with uncertainty in\nquantum mechanics. This uncertainty is two-fold: on the one hand there may be\nuncertainty about the state the quantum system is in, and on the other hand, as\nis essential to quantum mechanical uncertainty, even if the quantum state is\nknown, measurements may still produce an uncertain outcome. In our framework,\nmeasurements therefore play the role of acts with an uncertain outcome and our\nsimple decision-theoretic postulates ensure that Born's rule is encapsulated in\nthe utility functions associated with such acts. This approach allows us to\nuncouple (precise) probability theory from quantum mechanics, in the sense that\nit leaves room for a more general, so-called imprecise probabilities approach.\nWe discuss the mathematical implications of our findings, which allow us to\ngive a decision-theoretic foundation to recent seminal work by Benavoli,\nFacchini and Zaffalon, and we compare our approach to earlier and different\napproaches by Deutsch and Wallace.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "math.PR"
    ],
    "primary_category": "quant-ph",
    "comment": "52 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.20607v1",
    "published_date": "2025-03-26 14:53:06 UTC",
    "updated_date": "2025-03-26 14:53:06 UTC"
  },
  {
    "arxiv_id": "2503.20842v1",
    "title": "Anti Robot Speciesism",
    "authors": [
      "Julian De Freitas",
      "Noah Castelo",
      "Bernd Schmitt",
      "Miklos Sarvary"
    ],
    "abstract": "Humanoid robots are a form of embodied artificial intelligence (AI) that\nlooks and acts more and more like humans. Powered by generative AI and advances\nin robotics, humanoid robots can speak and interact with humans rather\nnaturally but are still easily recognizable as robots. But how will we treat\nhumanoids when they seem indistinguishable from humans in appearance and mind?\nWe find a tendency (called \"anti-robot\" speciesism) to deny such robots\nhumanlike capabilities, driven by motivations to accord members of the human\nspecies preferential treatment. Six experiments show that robots are denied\nhumanlike attributes, simply because they are not biological beings and because\nhumans want to avoid feelings of cognitive dissonance when utilizing such\nrobots for unsavory tasks. Thus, people do not rationally attribute\ncapabilities to perfectly humanlike robots but deny them capabilities as it\nsuits them.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20842v1",
    "published_date": "2025-03-26 13:56:30 UTC",
    "updated_date": "2025-03-26 13:56:30 UTC"
  },
  {
    "arxiv_id": "2503.20527v1",
    "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs",
    "authors": [
      "Zhicheng Guo",
      "Sijie Cheng",
      "Yuchen Niu",
      "Hao Wang",
      "Sicheng Zhou",
      "Wenbing Huang",
      "Yang Liu"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20527v1",
    "published_date": "2025-03-26 13:13:03 UTC",
    "updated_date": "2025-03-26 13:13:03 UTC"
  },
  {
    "arxiv_id": "2503.20523v1",
    "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving",
    "authors": [
      "Lloyd Russell",
      "Anthony Hu",
      "Lorenzo Bertoni",
      "George Fedoseev",
      "Jamie Shotton",
      "Elahe Arani",
      "Gianluca Corrado"
    ],
    "abstract": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2503.20523v1",
    "published_date": "2025-03-26 13:11:35 UTC",
    "updated_date": "2025-03-26 13:11:35 UTC"
  },
  {
    "arxiv_id": "2503.20500v3",
    "title": "Novel Deep Neural OFDM Receiver Architectures for LLR Estimation",
    "authors": [
      "Erhan Karakoca",
      "Hüseyin Çevik",
      "İbrahim Hökelek",
      "Ali Görçin"
    ],
    "abstract": "Neural receivers have recently become a popular topic, where the received\nsignals can be directly decoded by data driven mechanisms such as machine\nlearning and deep learning. In this paper, we propose two novel neural network\nbased orthogonal frequency division multiplexing (OFDM) receivers performing\nchannel estimation and equalization tasks and directly predicting log\nlikelihood ratios (LLRs) from the received in phase and quadrature phase (IQ)\nsignals. The first network, the Dual Attention Transformer (DAT), employs a\nstate of the art (SOTA) transformer architecture with an attention mechanism.\nThe second network, the Residual Dual Non Local Attention Network (RDNLA),\nutilizes a parallel residual architecture with a non local attention block. The\nbit error rate (BER) and block error rate (BLER) performance of various SOTA\nneural receiver architectures is compared with our proposed methods across\ndifferent signal to noise ratio (SNR) levels. The simulation results show that\nDAT and RDNLA outperform both traditional communication systems and existing\nneural receiver models.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Submitted to IEEE Globecom 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20500v3",
    "published_date": "2025-03-26 12:39:56 UTC",
    "updated_date": "2025-05-08 16:41:56 UTC"
  },
  {
    "arxiv_id": "2503.20492v1",
    "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models",
    "authors": [
      "Fanhu Zeng",
      "Zhen Cheng",
      "Fei Zhu",
      "Xu-Yao Zhang"
    ],
    "abstract": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.20492v1",
    "published_date": "2025-03-26 12:31:04 UTC",
    "updated_date": "2025-03-26 12:31:04 UTC"
  },
  {
    "arxiv_id": "2503.20485v1",
    "title": "Underwater Image Enhancement by Convolutional Spiking Neural Networks",
    "authors": [
      "Vidya Sudevan",
      "Fakhreddine Zayer",
      "Rizwana Kausar",
      "Sajid Javed",
      "Hamad Karki",
      "Giulia De Masi",
      "Jorge Dias"
    ],
    "abstract": "Underwater image enhancement (UIE) is fundamental for marine applications,\nincluding autonomous vision-based navigation. Deep learning methods using\nconvolutional neural networks (CNN) and vision transformers advanced UIE\nperformance. Recently, spiking neural networks (SNN) have gained attention for\ntheir lightweight design, energy efficiency, and scalability. This paper\nintroduces UIE-SNN, the first SNN-based UIE algorithm to improve visibility of\nunderwater images. UIE-SNN is a 19- layered convolutional spiking\nencoder-decoder framework with skip connections, directly trained using\nsurrogate gradient-based backpropagation through time (BPTT) strategy. We\nexplore and validate the influence of training datasets on energy reduction, a\nunique advantage of UIE-SNN architecture, in contrast to the conventional\nlearning-based architectures, where energy consumption is model-dependent.\nUIE-SNN optimizes the loss function in latent space representation to\nreconstruct clear underwater images. Our algorithm performs on par with its\nnon-spiking counterpart methods in terms of PSNR and structural similarity\nindex (SSIM) at reduced timesteps ($T=5$) and energy consumption of $85\\%$. The\nalgorithm is trained on two publicly available benchmark datasets, UIEB and\nEUVP, and tested on unseen images from UIEB, EUVP, LSUI, U45, and our custom\nUIE dataset. The UIE-SNN algorithm achieves PSNR of \\(17.7801~dB\\) and SSIM of\n\\(0.7454\\) on UIEB, and PSNR of \\(23.1725~dB\\) and SSIM of \\(0.7890\\) on EUVP.\nUIE-SNN achieves this algorithmic performance with fewer operators (\\(147.49\\)\nGSOPs) and energy (\\(0.1327~J\\)) compared to its non-spiking counterpart\n(GFLOPs = \\(218.88\\) and Energy=\\(1.0068~J\\)). Compared with existing SOTA UIE\nmethods, UIE-SNN achieves an average of \\(6.5\\times\\) improvement in energy\nefficiency. The source code is available at\n\\href{https://github.com/vidya-rejul/UIE-SNN.git}{UIE-SNN}.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20485v1",
    "published_date": "2025-03-26 12:15:38 UTC",
    "updated_date": "2025-03-26 12:15:38 UTC"
  },
  {
    "arxiv_id": "2503.20484v1",
    "title": "Contrastive Learning Guided Latent Diffusion Model for Image-to-Image Translation",
    "authors": [
      "Qi Si",
      "Bo Wang",
      "Zhao Zhang"
    ],
    "abstract": "The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20484v1",
    "published_date": "2025-03-26 12:15:25 UTC",
    "updated_date": "2025-03-26 12:15:25 UTC"
  },
  {
    "arxiv_id": "2503.20479v1",
    "title": "A multi-agentic framework for real-time, autonomous freeform metasurface design",
    "authors": [
      "Robert Lupoiu",
      "Yixuan Shao",
      "Tianxiang Dai",
      "Chenkai Mao",
      "Kofi Edee",
      "Jonathan A. Fan"
    ],
    "abstract": "Innovation in nanophotonics currently relies on human experts who synergize\nspecialized knowledge in photonics and coding with simulation and optimization\nalgorithms, entailing design cycles that are time-consuming, computationally\ndemanding, and frequently suboptimal. We introduce MetaChat, a multi-agentic\ndesign framework that can translate semantically described photonic design\ngoals into high-performance, freeform device layouts in an automated, nearly\nreal-time manner. Multi-step reasoning is enabled by our Agentic Iterative\nMonologue (AIM) paradigm, which coherently interfaces agents with code-based\ntools, other specialized agents, and human designers. Design acceleration is\nfacilitated by Feature-wise Linear Modulation-conditioned Maxwell surrogate\nsolvers that support the generalized evaluation of metasurface structures. We\nuse freeform dielectric metasurfaces as a model system and demonstrate with\nMetaChat the design of multi-objective, multi-wavelength metasurfaces orders of\nmagnitude faster than conventional methods. These concepts present a scientific\ncomputing blueprint for utilizing specialist design agents, surrogate solvers,\nand human interactions to drive multi-physics innovation and discovery.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.MA",
      "physics.comp-ph"
    ],
    "primary_category": "physics.app-ph",
    "comment": "32 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20479v1",
    "published_date": "2025-03-26 12:10:45 UTC",
    "updated_date": "2025-03-26 12:10:45 UTC"
  },
  {
    "arxiv_id": "2503.20472v1",
    "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
    "authors": [
      "Yucheng Suo",
      "Fan Ma",
      "Linchao Zhu",
      "Tianyi Wang",
      "Fengyun Rao",
      "Yi Yang"
    ],
    "abstract": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20472v1",
    "published_date": "2025-03-26 11:53:03 UTC",
    "updated_date": "2025-03-26 11:53:03 UTC"
  },
  {
    "arxiv_id": "2503.20446v1",
    "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation",
    "authors": [
      "Farzan Moodi",
      "Fereshteh Khodadadi Shoushtari",
      "Gelareh Valizadeh",
      "Dornaz Mazinani",
      "Hanieh Mobarak Salari",
      "Hamidreza Saligheh Rad"
    ],
    "abstract": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20446v1",
    "published_date": "2025-03-26 11:22:17 UTC",
    "updated_date": "2025-03-26 11:22:17 UTC"
  },
  {
    "arxiv_id": "2503.20428v1",
    "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics",
    "authors": [
      "F. Xavier Gaya-Morey",
      "Cristina Manresa-Yee",
      "Célia Martinie",
      "Jose M. Buades-Rubio"
    ],
    "abstract": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20428v1",
    "published_date": "2025-03-26 11:01:00 UTC",
    "updated_date": "2025-03-26 11:01:00 UTC"
  },
  {
    "arxiv_id": "2503.20425v1",
    "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
    "authors": [
      "Kevin Alcedo",
      "Pedro U. Lima",
      "Rachid Alami"
    ],
    "abstract": "Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20425v1",
    "published_date": "2025-03-26 10:59:08 UTC",
    "updated_date": "2025-03-26 10:59:08 UTC"
  },
  {
    "arxiv_id": "2503.20398v1",
    "title": "Including local feature interactions in deep non-negative matrix factorization networks improves performance",
    "authors": [
      "Mahbod Nouri",
      "David Rotermund",
      "Alberto Garcia-Ortiz",
      "Klaus R. Pawelzik"
    ],
    "abstract": "The brain uses positive signals as a means of signaling. Forward interactions\nin the early visual cortex are also positive, realized by excitatory synapses.\nOnly local interactions also include inhibition. Non-negative matrix\nfactorization (NMF) captures the biological constraint of positive long-range\ninteractions and can be implemented with stochastic spikes. While NMF can serve\nas an abstract formalization of early neural processing in the visual system,\nthe performance of deep convolutional networks with NMF modules does not match\nthat of CNNs of similar size. However, when the local NMF modules are each\nfollowed by a module that mixes the NMF's positive activities, the performances\non the benchmark data exceed that of vanilla deep convolutional networks of\nsimilar size. This setting can be considered a biologically more plausible\nemulation of the processing in cortical (hyper-)columns with the potential to\nimprove the performance of deep networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20398v1",
    "published_date": "2025-03-26 10:21:38 UTC",
    "updated_date": "2025-03-26 10:21:38 UTC"
  },
  {
    "arxiv_id": "2503.20394v1",
    "title": "FastFT: Accelerating Reinforced Feature Transformation via Advanced Exploration Strategies",
    "authors": [
      "Tianqi He",
      "Xiaohan Huang",
      "Yi Du",
      "Qingqing Long",
      "Ziyue Qiao",
      "Min Wu",
      "Yanjie Fu",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Feature Transformation is crucial for classic machine learning that aims to\ngenerate feature combinations to enhance the performance of downstream tasks\nfrom a data-centric perspective. Current methodologies, such as manual\nexpert-driven processes, iterative-feedback techniques, and\nexploration-generative tactics, have shown promise in automating such data\nengineering workflow by minimizing human involvement. However, three challenges\nremain in those frameworks: (1) It predominantly depends on downstream task\nperformance metrics, as assessment is time-consuming, especially for large\ndatasets. (2) The diversity of feature combinations will hardly be guaranteed\nafter random exploration ends. (3) Rare significant transformations lead to\nsparse valuable feedback that hinders the learning processes or leads to less\neffective results. In response to these challenges, we introduce FastFT, an\ninnovative framework that leverages a trio of advanced strategies.We first\ndecouple the feature transformation evaluation from the outcomes of the\ngenerated datasets via the performance predictor. To address the issue of\nreward sparsity, we developed a method to evaluate the novelty of generated\ntransformation sequences. Incorporating this novelty into the reward function\naccelerates the model's exploration of effective transformations, thereby\nimproving the search productivity. Additionally, we combine novelty and\nperformance to create a prioritized memory buffer, ensuring that essential\nexperiences are effectively revisited during exploration. Our extensive\nexperimental evaluations validate the performance, efficiency, and traceability\nof our proposed framework, showcasing its superiority in handling complex\nfeature transformation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, Accepted by ICDE 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20394v1",
    "published_date": "2025-03-26 10:17:41 UTC",
    "updated_date": "2025-03-26 10:17:41 UTC"
  },
  {
    "arxiv_id": "2503.20384v2",
    "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
    "authors": [
      "Rongyu Zhang",
      "Menghang Dong",
      "Yuan Zhang",
      "Liang Heng",
      "Xiaowei Chi",
      "Gaole Dai",
      "Li Du",
      "Yuan Du",
      "Shanghang Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20384v2",
    "published_date": "2025-03-26 10:05:38 UTC",
    "updated_date": "2025-04-14 11:39:39 UTC"
  },
  {
    "arxiv_id": "2503.20348v1",
    "title": "VideoGEM: Training-free Action Grounding in Videos",
    "authors": [
      "Felix Vogel",
      "Walid Bousselham",
      "Anna Kukleva",
      "Nina Shvetsova",
      "Hilde Kuehne"
    ],
    "abstract": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20348v1",
    "published_date": "2025-03-26 09:20:30 UTC",
    "updated_date": "2025-03-26 09:20:30 UTC"
  },
  {
    "arxiv_id": "2503.20341v1",
    "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
    "authors": [
      "Francesco Micheli",
      "Efe C. Balta",
      "Anastasios Tsiamis",
      "John Lygeros"
    ],
    "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20341v1",
    "published_date": "2025-03-26 09:11:17 UTC",
    "updated_date": "2025-03-26 09:11:17 UTC"
  },
  {
    "arxiv_id": "2503.20320v1",
    "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models",
    "authors": [
      "Shih-Wen Ke",
      "Guan-Yu Lai",
      "Guo-Lin Fang",
      "Hsi-Yuan Kao"
    ],
    "abstract": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20320v1",
    "published_date": "2025-03-26 08:40:46 UTC",
    "updated_date": "2025-03-26 08:40:46 UTC"
  },
  {
    "arxiv_id": "2503.22729v1",
    "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach",
    "authors": [
      "Jiahao Qin",
      "Feng Liu",
      "Lu Zong"
    ],
    "abstract": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.22729v1",
    "published_date": "2025-03-26 08:36:05 UTC",
    "updated_date": "2025-03-26 08:36:05 UTC"
  },
  {
    "arxiv_id": "2503.20302v2",
    "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
    "authors": [
      "Sunayana Sitaram",
      "Adrian de Wynter",
      "Isobel McCrum",
      "Qilong Gu",
      "Si-Qing Chen"
    ],
    "abstract": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard LLM-based\napplication (meeting transcript summarization), where both the data generation\nand the annotation steps followed a human-in-the-loop approach. We find that\nthe proposed guardrails are very effective in reducing misgendering rates\nacross all languages in the summaries generated, and without incurring loss of\nquality. Our human-in-the-loop approach demonstrates a method to feasibly scale\ninclusive and responsible AI-based solutions across multiple languages and\ncultures. We release the guardrails and synthetic dataset encompassing 42\nlanguages, along with human and LLM-judge evaluations, to encourage further\nresearch on this subject.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20302v2",
    "published_date": "2025-03-26 08:01:35 UTC",
    "updated_date": "2025-05-21 05:39:43 UTC"
  },
  {
    "arxiv_id": "2503.20294v2",
    "title": "Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement",
    "authors": [
      "Xinghao Wang",
      "Tao Gong",
      "Qi Chu",
      "Bin Liu",
      "Nenghai Yu"
    ],
    "abstract": "Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20294v2",
    "published_date": "2025-03-26 07:35:09 UTC",
    "updated_date": "2025-03-31 04:54:08 UTC"
  },
  {
    "arxiv_id": "2503.20291v2",
    "title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets",
    "authors": [
      "Chenwei Zhang",
      "Khanh Dao Duc"
    ],
    "abstract": "Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables",
    "pdf_url": "http://arxiv.org/pdf/2503.20291v2",
    "published_date": "2025-03-26 07:33:36 UTC",
    "updated_date": "2025-05-15 15:06:46 UTC"
  },
  {
    "arxiv_id": "2503.20290v2",
    "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions",
    "authors": [
      "Siyin Wang",
      "Wenyi Yu",
      "Xianzhao Chen",
      "Xiaohai Tian",
      "Jun Zhang",
      "Lu Lu",
      "Yu Tsao",
      "Junichi Yamagishi",
      "Yuxuan Wang",
      "Chao Zhang"
    ],
    "abstract": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "23 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.20290v2",
    "published_date": "2025-03-26 07:32:20 UTC",
    "updated_date": "2025-04-01 12:33:53 UTC"
  },
  {
    "arxiv_id": "2503.20285v1",
    "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
    "authors": [
      "Hongye Cao",
      "Fan Feng",
      "Jing Huo",
      "Shangdong Yang",
      "Meng Fang",
      "Tianpei Yang",
      "Yang Gao"
    ],
    "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20285v1",
    "published_date": "2025-03-26 07:24:34 UTC",
    "updated_date": "2025-03-26 07:24:34 UTC"
  },
  {
    "arxiv_id": "2503.20282v1",
    "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
    "authors": [
      "Kwonyoung Kim",
      "Jungin Park",
      "Jin Kim",
      "Hyeongjun Kwon",
      "Kwanghoon Sohn"
    ],
    "abstract": "Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2503.20282v1",
    "published_date": "2025-03-26 07:15:08 UTC",
    "updated_date": "2025-03-26 07:15:08 UTC"
  },
  {
    "arxiv_id": "2503.20281v1",
    "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
    "authors": [
      "Chenglong Wang",
      "Pujia Zheng",
      "Jiaping Gui",
      "Cunqing Hua",
      "Wajih Ul Hassan"
    ],
    "abstract": "Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20281v1",
    "published_date": "2025-03-26 07:11:57 UTC",
    "updated_date": "2025-03-26 07:11:57 UTC"
  },
  {
    "arxiv_id": "2503.20279v2",
    "title": "sudo rm -rf agentic_security",
    "authors": [
      "Sejin Lee",
      "Jian Kim",
      "Haon Park",
      "Ashkan Yousefpour",
      "Sangyoon Yu",
      "Min Song"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20279v2",
    "published_date": "2025-03-26 07:08:15 UTC",
    "updated_date": "2025-04-04 04:36:20 UTC"
  },
  {
    "arxiv_id": "2503.20831v1",
    "title": "Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model",
    "authors": [
      "Himanshu Tiwari"
    ],
    "abstract": "The rapid increase in cybersecurity vulnerabilities necessitates automated\ntools for analyzing and classifying vulnerability reports. This paper presents\na novel Vulnerability Report Classifier that leverages the BERT (Bidirectional\nEncoder Representations from Transformers) model to perform multi-label\nclassification of Common Vulnerabilities and Exposures (CVE) reports from the\nNational Vulnerability Database (NVD). The classifier predicts both the\nseverity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer\nOverflow, XSS) from textual descriptions. We introduce a custom training\npipeline using a combined loss function-Cross-Entropy for severity and Binary\nCross-Entropy with Logits for types-integrated into a Hugging Face Trainer\nsubclass. Experiments on recent NVD data demonstrate promising results, with\ndecreasing evaluation loss across epochs. The system is deployed via a REST API\nand a Streamlit UI, enabling real-time vulnerability analysis. This work\ncontributes a scalable, open-source solution for cybersecurity practitioners to\nautomate vulnerability triage.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "9 Pages",
    "pdf_url": "http://arxiv.org/pdf/2503.20831v1",
    "published_date": "2025-03-26 06:04:45 UTC",
    "updated_date": "2025-03-26 06:04:45 UTC"
  },
  {
    "arxiv_id": "2503.20258v1",
    "title": "Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis of Medical Ultrasound Videos",
    "authors": [
      "Jiaheng Zhou",
      "Yanfeng Zhou",
      "Wei Fang",
      "Yuxing Tang",
      "Le Lu",
      "Ge Yang"
    ],
    "abstract": "Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20258v1",
    "published_date": "2025-03-26 05:54:13 UTC",
    "updated_date": "2025-03-26 05:54:13 UTC"
  },
  {
    "arxiv_id": "2503.20252v2",
    "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
    "authors": [
      "Yejin Kwon",
      "Daeun Moon",
      "Youngje Oh",
      "Hyunsoo Yoon"
    ],
    "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted Industry Track at ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20252v2",
    "published_date": "2025-03-26 05:38:45 UTC",
    "updated_date": "2025-05-20 06:11:08 UTC"
  },
  {
    "arxiv_id": "2503.20245v1",
    "title": "ESSR: An 8K@30FPS Super-Resolution Accelerator With Edge Selective Network",
    "authors": [
      "Chih-Chia Hsu",
      "Tian-Sheuan Chang"
    ],
    "abstract": "Deep learning-based super-resolution (SR) is challenging to implement in\nresource-constrained edge devices for resolutions beyond full HD due to its\nhigh computational complexity and memory bandwidth requirements. This paper\nintroduces an 8K@30FPS SR accelerator with edge-selective dynamic input\nprocessing. Dynamic processing chooses the appropriate subnets for different\npatches based on simple input edge criteria, achieving a 50\\% MAC reduction\nwith only a 0.1dB PSNR decrease. The quality of reconstruction images is\nguaranteed and maximized its potential with \\textit{resource adaptive model\nswitching} even under resource constraints. In conjunction with\nhardware-specific refinements, the model size is reduced by 84\\% to 51K, but\nwith a decrease of less than 0.6dB PSNR. Additionally, to support dynamic\nprocessing with high utilization, this design incorporates a\n\\textit{configurable group of layer mapping} that synergizes with the\n\\textit{structure-friendly fusion block}, resulting in 77\\% hardware\nutilization and up to 79\\% reduction in feature SRAM access. The\nimplementation, using the TSMC 28nm process, can achieve 8K@30FPS throughput at\n800MHz with a gate count of 2749K, 0.2075W power consumption, and 4797Mpixels/J\nenergy efficiency, exceeding previous work.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20245v1",
    "published_date": "2025-03-26 05:27:23 UTC",
    "updated_date": "2025-03-26 05:27:23 UTC"
  },
  {
    "arxiv_id": "2503.20241v1",
    "title": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation",
    "authors": [
      "Mitsuaki Uno",
      "Kanji Tanaka",
      "Daiki Iwata",
      "Yudai Noda",
      "Shoya Miyazaki",
      "Kouki Terashima"
    ],
    "abstract": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 11 figures, technical report",
    "pdf_url": "http://arxiv.org/pdf/2503.20241v1",
    "published_date": "2025-03-26 05:15:26 UTC",
    "updated_date": "2025-03-26 05:15:26 UTC"
  },
  {
    "arxiv_id": "2503.20233v1",
    "title": "Dynamic Learning and Productivity for Data Analysts: A Bayesian Hidden Markov Model Perspective",
    "authors": [
      "Yue Yin"
    ],
    "abstract": "Data analysts are essential in organizations, transforming raw data into\ninsights that drive decision-making and strategy. This study explores how\nanalysts' productivity evolves on a collaborative platform, focusing on two key\nlearning activities: writing queries and viewing peer queries. While\ntraditional research often assumes static models, where performance improves\nsteadily with cumulative learning, such models fail to capture the dynamic\nnature of real-world learning. To address this, we propose a Hidden Markov\nModel (HMM) that tracks how analysts transition between distinct learning\nstates based on their participation in these activities.\n  Using an industry dataset with 2,001 analysts and 79,797 queries, this study\nidentifies three learning states: novice, intermediate, and advanced.\nProductivity increases as analysts advance to higher states, reflecting the\ncumulative benefits of learning. Writing queries benefits analysts across all\nstates, with the largest gains observed for novices. Viewing peer queries\nsupports novices but may hinder analysts in higher states due to cognitive\noverload or inefficiencies. Transitions between states are also uneven, with\nprogression from intermediate to advanced being particularly challenging. This\nstudy advances understanding of into dynamic learning behavior of knowledge\nworker and offers practical implications for designing systems, optimizing\ntraining, enabling personalized learning, and fostering effective knowledge\nsharing.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CE",
      "cs.HC"
    ],
    "primary_category": "cs.SI",
    "comment": "29 pages; a shorter 11-page version is accepted by HCI International\n  (HCII) 2025;",
    "pdf_url": "http://arxiv.org/pdf/2503.20233v1",
    "published_date": "2025-03-26 04:57:03 UTC",
    "updated_date": "2025-03-26 04:57:03 UTC"
  },
  {
    "arxiv_id": "2503.20231v1",
    "title": "Dynamics of Algorithmic Content Amplification on TikTok",
    "authors": [
      "Fabian Baumann",
      "Nipun Arora",
      "Iyad Rahwan",
      "Agnieszka Czaplicka"
    ],
    "abstract": "Intelligent algorithms increasingly shape the content we encounter and engage\nwith online. TikTok's For You feed exemplifies extreme algorithm-driven\ncuration, tailoring the stream of video content almost exclusively based on\nusers' explicit and implicit interactions with the platform. Despite growing\nattention, the dynamics of content amplification on TikTok remain largely\nunquantified. How quickly, and to what extent, does TikTok's algorithm amplify\ncontent aligned with users' interests? To address these questions, we conduct a\nsock-puppet audit, deploying bots with different interests to engage with\nTikTok's \"For You\" feed. Our findings reveal that content aligned with the\nbots' interests undergoes strong amplification, with rapid reinforcement\ntypically occurring within the first 200 videos watched. While amplification is\nconsistently observed across all interests, its intensity varies by interest,\nindicating the emergence of topic-specific biases. Time series analyses and\nMarkov models uncover distinct phases of recommendation dynamics, including\npersistent content reinforcement and a gradual decline in content diversity\nover time. Although TikTok's algorithm preserves some content diversity, we\nfind a strong negative correlation between amplification and exploration: as\nthe amplification of interest-aligned content increases, engagement with unseen\nhashtags declines. These findings contribute to discussions on\nsocio-algorithmic feedback loops in the digital age and the trade-offs between\npersonalization and content diversity.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.20231v1",
    "published_date": "2025-03-26 04:54:24 UTC",
    "updated_date": "2025-03-26 04:54:24 UTC"
  },
  {
    "arxiv_id": "2503.20230v1",
    "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs",
    "authors": [
      "Ugochukwu Ejike Akpudo",
      "Yongsheng Gao",
      "Jun Zhou",
      "Andrew Lewis"
    ],
    "abstract": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20230v1",
    "published_date": "2025-03-26 04:49:46 UTC",
    "updated_date": "2025-03-26 04:49:46 UTC"
  },
  {
    "arxiv_id": "2503.20227v1",
    "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding",
    "authors": [
      "Tianhao Wu",
      "Yu Wang",
      "Ngoc Quach"
    ],
    "abstract": "Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted by the 5th International Conference on\n  Artificial Intelligence and Industrial Technology Applications (AIITA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.20227v1",
    "published_date": "2025-03-26 04:45:33 UTC",
    "updated_date": "2025-03-26 04:45:33 UTC"
  },
  {
    "arxiv_id": "2503.20208v1",
    "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
    "authors": [
      "Liangzhi Shi",
      "Yulin Liu",
      "Lingqi Zeng",
      "Bo Ai",
      "Zhengdong Hong",
      "Hao Su"
    ],
    "abstract": "How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20208v1",
    "published_date": "2025-03-26 04:05:50 UTC",
    "updated_date": "2025-03-26 04:05:50 UTC"
  },
  {
    "arxiv_id": "2503.20205v1",
    "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang",
      "Xiang-Ling Chen"
    ],
    "abstract": "Appropriate traffic state representation is crucial for learning traffic\nsignal control policies. However, most of the current traffic state\nrepresentations are heuristically designed, with insufficient theoretical\nsupport. In this paper, we (1) develop a flexible, efficient, and theoretically\ngrounded method, namely generalized phase pressure (G2P) control, which takes\nonly simple lane features into consideration to decide which phase to be\nactuated; 2) extend the pressure control theory to a general form for\nmulti-homogeneous-lane road networks based on queueing theory; (3) design a new\ntraffic state representation based on the generalized phase state features from\nG2P control; and 4) develop a reinforcement learning (RL)-based algorithm\ntemplate named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight,\nby combining the generalized phase state representation with MPLight and\nCoLight, two well-performed RL methods for learning traffic signal control\npolicies. Extensive experiments conducted on multiple real-world datasets\ndemonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic\nmethod in the transportation field and other recent human-designed heuristic\nmethods; and that the newly proposed G2P-XLight significantly outperforms SOTA\nlearning-based approaches. Our code is available online.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20205v1",
    "published_date": "2025-03-26 04:03:12 UTC",
    "updated_date": "2025-03-26 04:03:12 UTC"
  },
  {
    "arxiv_id": "2503.20202v1",
    "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain",
    "authors": [
      "Nan Gao",
      "Yihua Bao",
      "Dongdong Weng",
      "Jiayi Zhao",
      "Jia Li",
      "Yan Zhou",
      "Pengfei Wan",
      "Di Zhang"
    ],
    "abstract": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20202v1",
    "published_date": "2025-03-26 03:55:41 UTC",
    "updated_date": "2025-03-26 03:55:41 UTC"
  },
  {
    "arxiv_id": "2503.20199v1",
    "title": "Assessing SAM for Tree Crown Instance Segmentation from Drone Imagery",
    "authors": [
      "Mélisande Teng",
      "Arthur Ouaknine",
      "Etienne Laliberté",
      "Yoshua Bengio",
      "David Rolnick",
      "Hugo Larochelle"
    ],
    "abstract": "The potential of tree planting as a natural climate solution is often\nundermined by inadequate monitoring of tree planting projects. Current\nmonitoring methods involve measuring trees by hand for each species, requiring\nextensive cost, time, and labour. Advances in drone remote sensing and computer\nvision offer great potential for mapping and characterizing trees from aerial\nimagery, and large pre-trained vision models, such as the Segment Anything\nModel (SAM), may be a particularly compelling choice given limited labeled\ndata. In this work, we compare SAM methods for the task of automatic tree crown\ninstance segmentation in high resolution drone imagery of young tree\nplantations. We explore the potential of SAM for this task, and find that\nmethods using SAM out-of-the-box do not outperform a custom Mask R-CNN, even\nwith well-designed prompts, but that there is potential for methods which tune\nSAM further. We also show that predictions can be improved by adding Digital\nSurface Model (DSM) information as an input.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 ML4RS workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.20199v1",
    "published_date": "2025-03-26 03:45:36 UTC",
    "updated_date": "2025-03-26 03:45:36 UTC"
  },
  {
    "arxiv_id": "2503.22723v1",
    "title": "Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping",
    "authors": [
      "Mohammad Saif Nazir",
      "Chayan Banerjee"
    ],
    "abstract": "Reinforcement learning often faces challenges with reward misalignment, where\nagents optimize for given rewards but fail to exhibit the desired behaviors.\nThis occurs when the reward function incentivizes proxy behaviors that diverge\nfrom the true objective. While human-in-the-loop (HIL) methods can help, they\nmay exacerbate the problem, as humans are prone to biases that lead to\ninconsistent, subjective, or misaligned feedback, complicating the learning\nprocess. To address these issues, we propose two key contributions. First, we\nextend the use of zero-shot, off-the-shelf large language models (LLMs) for\nreward shaping beyond natural language processing (NLP) to continuous control\ntasks. By leveraging LLMs as direct feedback providers, we replace surrogate\nmodels trained on human feedback, which often suffer from the bias inherent in\nthe feedback data it is trained on. Second, we introduce a hybrid framework\n(LLM-HFBF) that enables LLMs to identify and correct biases in human feedback\nwhile incorporating this feedback into the reward shaping process. The LLM-HFBF\nframework creates a more balanced and reliable system by addressing both the\nlimitations of LLMs (e.g., lack of domain-specific knowledge) and human\nsupervision (e.g., inherent biases). By enabling human feedback bias flagging\nand correction, our approach improves reinforcement learning performance and\nreduces reliance on potentially biased human guidance. Empirical experiments\nshow that biased human feedback significantly reduces performance, with average\nepisodic reward (AER) dropping from 28.472 in (unbiased approaches) to 7.039\n(biased with conservative bias). In contrast, LLM-based approaches maintain a\nmatching AER like unbiased feedback, even in custom edge case scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 2 figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.22723v1",
    "published_date": "2025-03-26 03:17:12 UTC",
    "updated_date": "2025-03-26 03:17:12 UTC"
  },
  {
    "arxiv_id": "2503.20182v1",
    "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs",
    "authors": [
      "Huanhuan Ma",
      "Haisong Gong",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Dongkuan Xu"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available via https://github.com/dependentsign/CSI",
    "pdf_url": "http://arxiv.org/pdf/2503.20182v1",
    "published_date": "2025-03-26 03:14:31 UTC",
    "updated_date": "2025-03-26 03:14:31 UTC"
  },
  {
    "arxiv_id": "2503.20176v1",
    "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
    "authors": [
      "RuiXi Qiao",
      "Jie Cheng",
      "Xingyuan Dai",
      "Yonglin Tian",
      "Yisheng Lv"
    ],
    "abstract": "Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20176v1",
    "published_date": "2025-03-26 03:04:42 UTC",
    "updated_date": "2025-03-26 03:04:42 UTC"
  },
  {
    "arxiv_id": "2504.08748v1",
    "title": "A Survey of Multimodal Retrieval-Augmented Generation",
    "authors": [
      "Lang Mei",
      "Siyu Mo",
      "Zhihan Yang",
      "Chong Chen"
    ],
    "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) enhances large language\nmodels (LLMs) by integrating multimodal data (text, images, videos) into\nretrieval and generation processes, overcoming the limitations of text-only\nRetrieval-Augmented Generation (RAG). While RAG improves response accuracy by\nincorporating external textual knowledge, MRAG extends this framework to\ninclude multimodal retrieval and generation, leveraging contextual information\nfrom diverse data types. This approach reduces hallucinations and enhances\nquestion-answering systems by grounding responses in factual, multimodal\nknowledge. Recent studies show MRAG outperforms traditional RAG, especially in\nscenarios requiring both visual and textual understanding. This survey reviews\nMRAG's essential components, datasets, evaluation methods, and limitations,\nproviding insights into its construction and improvement. It also identifies\nchallenges and future research directions, highlighting MRAG's potential to\nrevolutionize multimodal information retrieval and generation. By offering a\ncomprehensive perspective, this work encourages further exploration into this\npromising paradigm.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08748v1",
    "published_date": "2025-03-26 02:43:09 UTC",
    "updated_date": "2025-03-26 02:43:09 UTC"
  },
  {
    "arxiv_id": "2503.20825v1",
    "title": "Debiasing Kernel-Based Generative Models",
    "authors": [
      "Tian Qin",
      "Wei-Min Huang"
    ],
    "abstract": "We propose a novel two-stage framework of generative models named Debiasing\nKernel-Based Generative Models (DKGM) with the insights from kernel density\nestimation (KDE) and stochastic approximation. In the first stage of DKGM, we\nemploy KDE to bypass the obstacles in estimating the density of data without\nlosing too much image quality. One characteristic of KDE is oversmoothing,\nwhich makes the generated image blurry. Therefore, in the second stage, we\nformulate the process of reducing the blurriness of images as a statistical\ndebiasing problem and develop a novel iterative algorithm to improve image\nquality, which is inspired by the stochastic approximation. Extensive\nexperiments illustrate that the image quality of DKGM on CIFAR10 is comparable\nto state-of-the-art models such as diffusion models and GAN models. The\nperformance of DKGM on CelebA 128x128 and LSUN (Church) 128x128 is also\ncompetitive. We conduct extra experiments to exploit how the bandwidth in KDE\naffects the sample diversity and debiasing effect of DKGM. The connections\nbetween DKGM and score-based models are also discussed.",
    "categories": [
      "stat.ML",
      "cs.AI"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20825v1",
    "published_date": "2025-03-26 01:48:34 UTC",
    "updated_date": "2025-03-26 01:48:34 UTC"
  },
  {
    "arxiv_id": "2503.20824v1",
    "title": "Exploiting Temporal State Space Sharing for Video Semantic Segmentation",
    "authors": [
      "Syed Ariff Syed Hesham",
      "Yun Liu",
      "Guolei Sun",
      "Henghui Ding",
      "Jing Yang",
      "Ender Konukoglu",
      "Xue Geng",
      "Xudong Jiang"
    ],
    "abstract": "Video semantic segmentation (VSS) plays a vital role in understanding the\ntemporal evolution of scenes. Traditional methods often segment videos\nframe-by-frame or in a short temporal window, leading to limited temporal\ncontext, redundant computations, and heavy memory requirements. To this end, we\nintroduce a Temporal Video State Space Sharing (TV3S) architecture to leverage\nMamba state space models for temporal feature sharing. Our model features a\nselective gating mechanism that efficiently propagates relevant information\nacross video frames, eliminating the need for a memory-heavy feature pool. By\nprocessing spatial patches independently and incorporating shifted operation,\nTV3S supports highly parallel computation in both training and inference\nstages, which reduces the delay in sequential state space processing and\nimproves the scalability for long video sequences. Moreover, TV3S incorporates\ninformation from prior frames during inference, achieving long-range temporal\ncoherence and superior adaptability to extended sequences. Evaluations on the\nVSPW and Cityscapes datasets reveal that our approach outperforms current\nstate-of-the-art methods, establishing a new standard for VSS with consistent\nresults across long video sequences. By achieving a good balance between\naccuracy and efficiency, TV3S shows a significant advancement in spatiotemporal\nmodeling, paving the way for efficient video analysis. The code is publicly\navailable at https://github.com/Ashesham/TV3S.git.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.20824v1",
    "published_date": "2025-03-26 01:47:42 UTC",
    "updated_date": "2025-03-26 01:47:42 UTC"
  },
  {
    "arxiv_id": "2503.21815v1",
    "title": "ATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum Neural Networks",
    "authors": [
      "Mohamed Afane",
      "Gabrielle Ebbrecht",
      "Ying Wang",
      "Juntao Chen",
      "Junaid Farooq"
    ],
    "abstract": "Quantum Neural Networks (QNNs) offer promising capabilities for complex data\ntasks, but are often constrained by limited qubit resources and high\nentanglement, which can hinder scalability and efficiency. In this paper, we\nintroduce Adaptive Threshold Pruning (ATP), an encoding method that reduces\nentanglement and optimizes data complexity for efficient computations in QNNs.\nATP dynamically prunes non-essential features in the data based on adaptive\nthresholds, effectively reducing quantum circuit requirements while preserving\nhigh performance. Extensive experiments across multiple datasets demonstrate\nthat ATP reduces entanglement entropy and improves adversarial robustness when\ncombined with adversarial training methods like FGSM. Our results highlight\nATPs ability to balance computational efficiency and model resilience,\nachieving significant performance improvements with fewer resources, which will\nhelp make QNNs more feasible in practical, resource-constrained settings.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2025.a",
    "pdf_url": "http://arxiv.org/pdf/2503.21815v1",
    "published_date": "2025-03-26 01:14:26 UTC",
    "updated_date": "2025-03-26 01:14:26 UTC"
  },
  {
    "arxiv_id": "2503.20139v1",
    "title": "Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning",
    "authors": [
      "Yongshuai Liu",
      "Xin Liu"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) has demonstrated superior sample\nefficiency compared to model-free reinforcement learning (MFRL). However, the\npresence of inaccurate models can introduce biases during policy learning,\nresulting in misleading trajectories. The challenge lies in obtaining accurate\nmodels due to limited diverse training data, particularly in regions with\nlimited visits (uncertain regions). Existing approaches passively quantify\nuncertainty after sample generation, failing to actively collect uncertain\nsamples that could enhance state coverage and improve model accuracy. Moreover,\nMBRL often faces difficulties in making accurate multi-step predictions,\nthereby impacting overall performance. To address these limitations, we propose\na novel framework for uncertainty-aware policy optimization with model-based\nexploratory planning. In the model-based planning phase, we introduce an\nuncertainty-aware k-step lookahead planning approach to guide action selection\nat each step. This process involves a trade-off analysis between model\nuncertainty and value function approximation error, effectively enhancing\npolicy performance. In the policy optimization phase, we leverage an\nuncertainty-driven exploratory policy to actively collect diverse training\nsamples, resulting in improved model accuracy and overall performance of the RL\nagent. Our approach offers flexibility and applicability to tasks with varying\nstate/action spaces and reward structures. We validate its effectiveness\nthrough experiments on challenging robotic manipulation tasks and Atari games,\nsurpassing state-of-the-art methods with fewer interactions, thereby leading to\nsignificant performance improvements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20139v1",
    "published_date": "2025-03-26 01:07:35 UTC",
    "updated_date": "2025-03-26 01:07:35 UTC"
  },
  {
    "arxiv_id": "2503.20138v1",
    "title": "Unlocking the Value of Decentralized Data: A Federated Dual Learning Approach for Model Aggregation",
    "authors": [
      "Junyi Zhu",
      "Ruicong Yao",
      "Taha Ceritli",
      "Savas Ozkan",
      "Matthew B. Blaschko",
      "Eunchung Noh",
      "Jeongwon Min",
      "Cho Jung Min",
      "Mete Ozay"
    ],
    "abstract": "Artificial Intelligence (AI) technologies have revolutionized numerous\nfields, yet their applications often rely on costly and time-consuming data\ncollection processes. Federated Learning (FL) offers a promising alternative by\nenabling AI models to be trained on decentralized data where data is scattered\nacross clients (distributed nodes). However, existing FL approaches struggle to\nmatch the performance of centralized training due to challenges such as\nheterogeneous data distribution and communication delays, limiting their\npotential for breakthroughs. We observe that many real-world use cases involve\nhybrid data regimes, in which a server (center node) has access to some data\nwhile a large amount of data is distributed across associated clients. To\nimprove the utilization of decentralized data under this regime, address data\nheterogeneity issue, and facilitate asynchronous communication between the\nserver and clients, we propose a dual learning approach that leverages\ncentralized data at the server to guide the merging of model updates from\nclients. Our method accommodates scenarios where server data is out-of-domain\nrelative to decentralized client data, making it applicable to a wide range of\nuse cases. We provide theoretical analysis demonstrating the faster convergence\nof our method compared to existing methods. Furthermore, experimental results\nacross various scenarios show that our approach significantly outperforms\nexisting technologies, highlighting its potential to unlock the value of large\namounts of decentralized data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20138v1",
    "published_date": "2025-03-26 01:00:35 UTC",
    "updated_date": "2025-03-26 01:00:35 UTC"
  },
  {
    "arxiv_id": "2503.20822v1",
    "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "authors": [
      "Qi Zhao",
      "Xingyu Ni",
      "Ziyu Wang",
      "Feng Cheng",
      "Ziyan Yang",
      "Lu Jiang",
      "Bohan Wang"
    ],
    "abstract": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20822v1",
    "published_date": "2025-03-26 00:45:07 UTC",
    "updated_date": "2025-03-26 00:45:07 UTC"
  },
  {
    "arxiv_id": "2503.20126v1",
    "title": "Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code Optimizations",
    "authors": [
      "Pooja Rani",
      "Jan-Andrea Bard",
      "June Sallou",
      "Alexander Boll",
      "Timo Kehrer",
      "Alberto Bacchelli"
    ],
    "abstract": "The rapid technological evolution has accelerated software development for\nvarious domains and use cases, contributing to a growing share of global carbon\nemissions. While recent large language models (LLMs) claim to assist developers\nin optimizing code for performance and energy efficiency, their efficacy in\nreal-world scenarios remains under exploration. In this work, we explore the\neffectiveness of LLMs in reducing the environmental footprint of real-world\nprojects, focusing on software written in Matlab-widely used in both academia\nand industry for scientific and engineering applications. We analyze\nenergy-focused optimization on 400 scripts across 100 top GitHub repositories.\nWe examine potential 2,176 optimizations recommended by leading LLMs, such as\nGPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy\nconsumption, memory usage, execution time consumption, and code correctness.\nThe developer serves as a real-world baseline for comparing typical human and\nLLM-generated optimizations.\n  Mapping these optimizations to 13 high-level themes, we found that LLMs\npropose a broad spectrum of improvements--beyond energy efficiency--including\nimproving code readability and maintainability, memory management, error\nhandling while the developer overlooked some parallel processing, error\nhandling etc. However, our statistical tests reveal that the energy-focused\noptimizations unexpectedly negatively impacted memory usage, with no clear\nbenefits regarding execution time or energy consumption. Our qualitative\nanalysis of energy-time trade-offs revealed that some themes, such as\nvectorization preallocation, were among the common themes shaping these\ntrade-offs. With LLMs becoming ubiquitous in modern software development, our\nstudy serves as a call to action: prioritizing the evaluation of common coding\npractices to identify the green ones.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20126v1",
    "published_date": "2025-03-26 00:27:29 UTC",
    "updated_date": "2025-03-26 00:27:29 UTC"
  },
  {
    "arxiv_id": "2504.07118v1",
    "title": "Sacred or Secular? Religious Bias in AI-Generated Financial Advice",
    "authors": [
      "Muhammad Salar Khan",
      "Hamza Umer"
    ],
    "abstract": "This study examines religious biases in AI-generated financial advice,\nfocusing on ChatGPT's responses to financial queries. Using a prompt-based\nmethodology and content analysis, we find that 50% of the financial emails\ngenerated by ChatGPT exhibit religious biases, with explicit biases present in\nboth ingroup and outgroup interactions. While ingroup biases personalize\nresponses based on religious alignment, outgroup biases introduce religious\nframing that may alienate clients or create ideological friction. These\nfindings align with broader research on AI bias and suggest that ChatGPT is not\nmerely reflecting societal biases but actively shaping financial discourse\nbased on perceived religious identity. Using the Critical Algorithm Studies\nframework, we argue that ChatGPT functions as a mediator of financial\nnarratives, selectively reinforcing religious perspectives. This study\nunderscores the need for greater transparency, bias mitigation strategies, and\nregulatory oversight to ensure neutrality in AI-driven financial services.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07118v1",
    "published_date": "2025-03-26 00:27:04 UTC",
    "updated_date": "2025-03-26 00:27:04 UTC"
  },
  {
    "arxiv_id": "2503.20124v1",
    "title": "Synthesizing world models for bilevel planning",
    "authors": [
      "Zergham Ahmed",
      "Joshua B. Tenenbaum",
      "Christopher J. Bates",
      "Samuel J. Gershman"
    ],
    "abstract": "Modern reinforcement learning (RL) systems have demonstrated remarkable\ncapabilities in complex environments, such as video games. However, they still\nfall short of achieving human-like sample efficiency and adaptability when\nlearning new domains. Theory-based reinforcement learning (TBRL) is an\nalgorithmic framework specifically designed to address this gap. Modeled on\ncognitive theories, TBRL leverages structured, causal world models - \"theories\"\n- as forward simulators for use in planning, generalization and exploration.\nAlthough current TBRL systems provide compelling explanations of how humans\nlearn to play video games, they face several technical limitations: their\ntheory languages are restrictive, and their planning algorithms are not\nscalable. To address these challenges, we introduce TheoryCoder, an\ninstantiation of TBRL that exploits hierarchical representations of theories\nand efficient program synthesis methods for more powerful learning and\nplanning. TheoryCoder equips agents with general-purpose abstractions (e.g.,\n\"move to\"), which are then grounded in a particular environment by learning a\nlow-level transition model (a Python program synthesized from observations by a\nlarge language model). A bilevel planning algorithm can exploit this\nhierarchical structure to solve large domains. We demonstrate that this\napproach can be successfully applied to diverse and challenging grid-world\ngames, where approaches based on directly synthesizing a policy perform poorly.\nAblation studies demonstrate the benefits of using hierarchical abstractions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.20124v1",
    "published_date": "2025-03-26 00:10:01 UTC",
    "updated_date": "2025-03-26 00:10:01 UTC"
  }
]
[
  {
    "arxiv_id": "2503.13778v1",
    "title": "Using 3D reconstruction from image motion to predict total leaf area in dwarf tomato plants",
    "authors": [
      "Dmitrii Usenko",
      "David Helman",
      "Chen Giladi"
    ],
    "abstract": "Accurate estimation of total leaf area (TLA) is crucial for evaluating plant\ngrowth, photosynthetic activity, and transpiration. However, it remains\nchallenging for bushy plants like dwarf tomatoes due to their complex canopies.\nTraditional methods are often labor-intensive, damaging to plants, or limited\nin capturing canopy complexity. This study evaluated a non-destructive method\ncombining sequential 3D reconstructions from RGB images and machine learning to\nestimate TLA for three dwarf tomato cultivars: Mohamed, Hahms Gelbe Topftomate,\nand Red Robin -- grown under controlled greenhouse conditions. Two experiments\n(spring-summer and autumn-winter) included 73 plants, yielding 418 TLA\nmeasurements via an \"onion\" approach. High-resolution videos were recorded, and\n500 frames per plant were used for 3D reconstruction. Point clouds were\nprocessed using four algorithms (Alpha Shape, Marching Cubes, Poisson's, Ball\nPivoting), and meshes were evaluated with seven regression models:\nMultivariable Linear Regression, Lasso Regression, Ridge Regression, Elastic\nNet Regression, Random Forest, Extreme Gradient Boosting, and Multilayer\nPerceptron. The Alpha Shape reconstruction ($\\alpha = 3$) with Extreme Gradient\nBoosting achieved the best performance ($R^2 = 0.80$, $MAE = 489 cm^2$).\nCross-experiment validation showed robust results ($R^2 = 0.56$, $MAE = 579\ncm^2$). Feature importance analysis identified height, width, and surface area\nas key predictors. This scalable, automated TLA estimation method is suited for\nurban farming and precision agriculture, offering applications in automated\npruning, resource efficiency, and sustainable food production. The approach\ndemonstrated robustness across variable environmental conditions and canopy\nstructures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 11 figures, submitted to Computers and Electronics in\n  Agriculture",
    "pdf_url": "http://arxiv.org/pdf/2503.13778v1",
    "published_date": "2025-03-17 23:51:19 UTC",
    "updated_date": "2025-03-17 23:51:19 UTC"
  },
  {
    "arxiv_id": "2503.13771v1",
    "title": "Towards AI-assisted Academic Writing",
    "authors": [
      "Daniel J. Liebling",
      "Malcolm Kane",
      "Madeleine Grunde-Mclaughlin",
      "Ian J. Lang",
      "Subhashini Venugopalan",
      "Michael P. Brenner"
    ],
    "abstract": "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to NAACL 2025 Workshop on AI for Scientific Discovery",
    "pdf_url": "http://arxiv.org/pdf/2503.13771v1",
    "published_date": "2025-03-17 23:30:17 UTC",
    "updated_date": "2025-03-17 23:30:17 UTC"
  },
  {
    "arxiv_id": "2503.14555v1",
    "title": "A Generalist Hanabi Agent",
    "authors": [
      "Arjun V Sudhakar",
      "Hadi Nekoei",
      "Mathieu Reymond",
      "Miao Liu",
      "Janarthanan Rajendran",
      "Sarath Chandar"
    ],
    "abstract": "Traditional multi-agent reinforcement learning (MARL) systems can develop\ncooperative strategies through repeated interactions. However, these systems\nare unable to perform well on any other setting than the one they have been\ntrained on, and struggle to successfully cooperate with unfamiliar\ncollaborators. This is particularly visible in the Hanabi benchmark, a popular\n2-to-5 player cooperative card-game which requires complex reasoning and\nprecise assistance to other agents. Current MARL agents for Hanabi can only\nlearn one specific game-setting (e.g., 2-player games), and play with the same\nalgorithmic agents. This is in stark contrast to humans, who can quickly adjust\ntheir strategies to work with unfamiliar partners or situations. In this paper,\nwe introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist\nagent for Hanabi, designed to overcome these limitations. We reformulate the\ntask using text, as language has been shown to improve transfer. We then\npropose a distributed MARL algorithm that copes with the resulting dynamic\nobservation- and action-space. In doing so, our agent is the first that can\nplay all game settings concurrently, and extend strategies learned from one\nsetting to other ones. As a consequence, our agent also demonstrates the\nability to collaborate with different algorithmic agents -- agents that are\nthemselves unable to do so. The implementation code is available at:\n$\\href{https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14555v1",
    "published_date": "2025-03-17 22:25:15 UTC",
    "updated_date": "2025-03-17 22:25:15 UTC"
  },
  {
    "arxiv_id": "2503.14554v1",
    "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot",
    "authors": [
      "Ali Parsaee",
      "Fahim Shahriar",
      "Chuxin He",
      "Ruiqing Tan"
    ],
    "abstract": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference",
    "pdf_url": "http://arxiv.org/pdf/2503.14554v1",
    "published_date": "2025-03-17 22:24:39 UTC",
    "updated_date": "2025-03-17 22:24:39 UTC"
  },
  {
    "arxiv_id": "2503.13754v2",
    "title": "From Autonomous Agents to Integrated Systems, A New Paradigm: Orchestrated Distributed Intelligence",
    "authors": [
      "Krti Tallam"
    ],
    "abstract": "The rapid evolution of artificial intelligence (AI) has ushered in a new era\nof integrated systems that merge computational prowess with human\ndecision-making. In this paper, we introduce the concept of Orchestrated\nDistributed Intelligence (ODI), a novel paradigm that reconceptualizes AI not\nas isolated autonomous agents, but as cohesive, orchestrated networks that work\nin tandem with human expertise. ODI leverages advanced orchestration layers,\nmulti-loop feedback mechanisms, and a high cognitive density framework to\ntransform static, record-keeping systems into dynamic, action-oriented\nenvironments. Through a comprehensive review of multi-agent system literature,\nrecent technological advances, and practical insights from industry forums, we\nargue that the future of AI lies in integrating distributed intelligence within\nhuman-centric workflows. This approach not only enhances operational efficiency\nand strategic agility but also addresses challenges related to scalability,\ntransparency, and ethical decision-making. Our work outlines key theoretical\nimplications and presents a practical roadmap for future research and\nenterprise innovation, aiming to pave the way for responsible and adaptive AI\nsystems that drive sustainable innovation in human organizations.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13754v2",
    "published_date": "2025-03-17 22:21:25 UTC",
    "updated_date": "2025-03-19 02:01:23 UTC"
  },
  {
    "arxiv_id": "2503.13751v1",
    "title": "Optimizing ML Training with Metagradient Descent",
    "authors": [
      "Logan Engstrom",
      "Andrew Ilyas",
      "Benjamin Chen",
      "Axel Feldmann",
      "William Moses",
      "Aleksander Madry"
    ],
    "abstract": "A major challenge in training large-scale machine learning models is\nconfiguring the training process to maximize model performance, i.e., finding\nthe best training setup from a vast design space. In this work, we unlock a\ngradient-based approach to this problem. We first introduce an algorithm for\nefficiently calculating metagradients -- gradients through model training -- at\nscale. We then introduce a \"smooth model training\" framework that enables\neffective optimization using metagradients. With metagradient descent (MGD), we\ngreatly improve on existing dataset selection methods, outperform\naccuracy-degrading data poisoning attacks by an order of magnitude, and\nautomatically find competitive learning rate schedules.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13751v1",
    "published_date": "2025-03-17 22:18:24 UTC",
    "updated_date": "2025-03-17 22:18:24 UTC"
  },
  {
    "arxiv_id": "2503.14552v1",
    "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review",
    "authors": [
      "Sayed Pedram Haeri Boroujeni",
      "Niloufar Mehrabi",
      "Fatemeh Afghah",
      "Connor Peter McGrath",
      "Danish Bhatkar",
      "Mithilesh Anil Biradar",
      "Abolfazl Razi"
    ],
    "abstract": "Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14552v1",
    "published_date": "2025-03-17 22:08:02 UTC",
    "updated_date": "2025-03-17 22:08:02 UTC"
  },
  {
    "arxiv_id": "2503.17391v1",
    "title": "AI-driven Automation of End-to-end Assessment of Suturing Expertise",
    "authors": [
      "Atharva Deo",
      "Nicholas Matsumoto",
      "Sun Kim",
      "Peter Wager",
      "Randy G. Tsai",
      "Aaron Denmark",
      "Cherine Yang",
      "Xi Li",
      "Jay Moran",
      "Miguel Hernandez",
      "Andrew J. Hung"
    ],
    "abstract": "We present an AI based approach to automate the End-to-end Assessment of\nSuturing Expertise (EASE), a suturing skills assessment tool that\ncomprehensively defines criteria around relevant sub-skills.1 While EASE\nprovides granular skills assessment related to suturing to provide trainees\nwith an objective evaluation of their aptitude along with actionable insights,\nthe scoring process is currently performed by human evaluators, which is time\nand resource consuming. The AI based approach solves this by enabling real-time\nscore prediction with minimal resources during model inference. This enables\nthe possibility of real-time feedback to the surgeons/trainees, potentially\naccelerating the learning process for the suturing task and mitigating critical\nerrors during the surgery, improving patient outcomes. In this study, we focus\non the following 7 EASE domains that come under 3 suturing phases: 1) Needle\nHandling: Number of Repositions, Needle Hold Depth, Needle Hold Ratio, and\nNeedle Hold Angle; 2) Needle Driving: Driving Smoothness, and Wrist Rotation;\n3) Needle Withdrawal: Wrist Rotation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17391v1",
    "published_date": "2025-03-17 21:28:02 UTC",
    "updated_date": "2025-03-17 21:28:02 UTC"
  },
  {
    "arxiv_id": "2503.15549v1",
    "title": "Rendering Transparency to Ranking in Educational Assessment via Bayesian Comparative Judgement",
    "authors": [
      "Andy Gray",
      "Alma Rahat",
      "Stephen Lindsay",
      "Jen Pearson",
      "Tom Crick"
    ],
    "abstract": "Ensuring transparency in educational assessment is increasingly critical,\nparticularly post-pandemic, as demand grows for fairer and more reliable\nevaluation methods. Comparative Judgement (CJ) offers a promising alternative\nto traditional assessments, yet concerns remain about its perceived opacity.\nThis paper examines how Bayesian Comparative Judgement (BCJ) enhances\ntransparency by integrating prior information into the judgement process,\nproviding a structured, data-driven approach that improves interpretability and\naccountability.\n  BCJ assigns probabilities to judgement outcomes, offering quantifiable\nmeasures of uncertainty and deeper insights into decision confidence. By\nsystematically tracking how prior data and successive judgements inform final\nrankings, BCJ clarifies the assessment process and helps identify assessor\ndisagreements. Multi-criteria BCJ extends this by evaluating multiple learning\noutcomes (LOs) independently, preserving the richness of CJ while producing\ntransparent, granular rankings aligned with specific assessment goals. It also\nenables a holistic ranking derived from individual LOs, ensuring comprehensive\nevaluations without compromising detailed feedback.\n  Using a real higher education dataset with professional markers in the UK, we\ndemonstrate BCJ's quantitative rigour and ability to clarify ranking\nrationales. Through qualitative analysis and discussions with experienced CJ\npractitioners, we explore its effectiveness in contexts where transparency is\ncrucial, such as high-stakes national assessments. We highlight the benefits\nand limitations of BCJ, offering insights into its real-world application\nacross various educational settings.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15549v1",
    "published_date": "2025-03-17 20:56:55 UTC",
    "updated_date": "2025-03-17 20:56:55 UTC"
  },
  {
    "arxiv_id": "2503.13708v1",
    "title": "A Circular Construction Product Ontology for End-of-Life Decision-Making",
    "authors": [
      "Kwabena Adu-Duodu",
      "Stanly Wilson",
      "Yinhao Li",
      "Aanuoluwapo Oladimeji",
      "Talea Huraysi",
      "Masoud Barati",
      "Charith Perera",
      "Ellis Solaiman",
      "Omer Rana",
      "Rajiv Ranjan",
      "Tejal Shah"
    ],
    "abstract": "Efficient management of end-of-life (EoL) products is critical for advancing\ncircularity in supply chains, particularly within the construction industry\nwhere EoL strategies are hindered by heterogenous lifecycle data and data\nsilos. Current tools like Environmental Product Declarations (EPDs) and Digital\nProduct Passports (DPPs) are limited by their dependency on seamless data\nintegration and interoperability which remain significant challenges. To\naddress these, we present the Circular Construction Product Ontology (CCPO), an\napplied framework designed to overcome semantic and data heterogeneity\nchallenges in EoL decision-making for construction products. CCPO standardises\nvocabulary and facilitates data integration across supply chain stakeholders\nenabling lifecycle assessments (LCA) and robust decision-making. By aggregating\ndisparate data into a unified product provenance, CCPO enables automated EoL\nrecommendations through customisable SWRL rules aligned with European standards\nand stakeholder-specific circularity SLAs, demonstrating its scalability and\nintegration capabilities. The adopted circular product scenario depicts CCPO's\napplication while competency question evaluations show its superior performance\nin generating accurate EoL suggestions highlighting its potential to greatly\nimprove decision-making in circular supply chains and its applicability in\nreal-world construction environments.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13708v1",
    "published_date": "2025-03-17 20:28:08 UTC",
    "updated_date": "2025-03-17 20:28:08 UTC"
  },
  {
    "arxiv_id": "2503.13690v2",
    "title": "Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization",
    "authors": [
      "Jan Bronec",
      "Jindřich Helcl"
    ],
    "abstract": "We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to efficiently compute additional regularization terms, which help\nwith unlearning stabilization. The results of our approach significantly exceed\nthe shared task baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50 (Primary), 68T07 (Secondary)",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, accepted to SemEval workshop proceedings at ACL\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13690v2",
    "published_date": "2025-03-17 19:59:19 UTC",
    "updated_date": "2025-05-07 19:19:33 UTC"
  },
  {
    "arxiv_id": "2503.14550v1",
    "title": "Novel AI-Based Quantification of Breast Arterial Calcification to Predict Cardiovascular Risk",
    "authors": [
      "Theodorus Dapamede",
      "Aisha Urooj",
      "Vedant Joshi",
      "Gabrielle Gershon",
      "Frank Li",
      "Mohammadreza Chavoshi",
      "Beatrice Brown-Mulry",
      "Rohan Satya Isaac",
      "Aawez Mansuri",
      "Chad Robichaux",
      "Chadi Ayoub",
      "Reza Arsanjani",
      "Laurence Sperling",
      "Judy Gichoya",
      "Marly van Assen",
      "Charles W. ONeill",
      "Imon Banerjee",
      "Hari Trivedi"
    ],
    "abstract": "Women are underdiagnosed and undertreated for cardiovascular disease.\nAutomatic quantification of breast arterial calcification on screening\nmammography can identify women at risk for cardiovascular disease and enable\nearlier treatment and management of disease. In this retrospective study of\n116,135 women from two healthcare systems, a transformer-based neural network\nquantified BAC severity (no BAC, mild, moderate, and severe) on screening\nmammograms. Outcomes included major adverse cardiovascular events (MACE) and\nall-cause mortality. BAC severity was independently associated with MACE after\nadjusting for cardiovascular risk factors, with increasing hazard ratios from\nmild (HR 1.18-1.22), moderate (HR 1.38-1.47), to severe BAC (HR 2.03-2.22)\nacross datasets (all p<0.001). This association remained significant across all\nage groups, with even mild BAC indicating increased risk in women under 50. BAC\nremained an independent predictor when analyzed alongside ASCVD risk scores,\nshowing significant associations with myocardial infarction, stroke, heart\nfailure, and mortality (all p<0.005). Automated BAC quantification enables\nopportunistic cardiovascular risk assessment during routine mammography without\nadditional radiation or cost. This approach provides value beyond traditional\nrisk factors, particularly in younger women, offering potential for early CVD\nrisk stratification in the millions of women undergoing annual mammography.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14550v1",
    "published_date": "2025-03-17 19:38:17 UTC",
    "updated_date": "2025-03-17 19:38:17 UTC"
  },
  {
    "arxiv_id": "2503.14549v1",
    "title": "Sampling Decisions",
    "authors": [
      "Michael Chertkov",
      "Sungsoo Ahn",
      "Hamidreza Behjoo"
    ],
    "abstract": "In this manuscript we introduce a novel Decision Flow (DF) framework for\nsampling from a target distribution while incorporating additional guidance\nfrom a prior sampler. DF can be viewed as an AI driven algorithmic\nreincarnation of the Markov Decision Process (MDP) approach in Stochastic\nOptimal Control. It extends the continuous space, continuous time path Integral\nDiffusion sampling technique to discrete time and space, while also\ngeneralizing the Generative Flow Network framework. In its most basic form, an\nexplicit, Neural Network (NN) free formulation, DF leverages the linear\nsolvability of the the underlying MDP to adjust the transition probabilities of\nthe prior sampler. The resulting Markov Process is expressed as a convolution\nof the reverse time Green's function of the prior sampling with the target\ndistribution. We illustrate the DF framework through an example of sampling\nfrom the Ising model, discuss potential NN based extensions, and outline how DF\ncan enhance guided sampling across various applications.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14549v1",
    "published_date": "2025-03-17 19:32:22 UTC",
    "updated_date": "2025-03-17 19:32:22 UTC"
  },
  {
    "arxiv_id": "2503.13660v1",
    "title": "INPROVF: Leveraging Large Language Models to Repair High-level Robot Controllers from Assumption Violations",
    "authors": [
      "Qian Meng",
      "Jin Peng Zhou",
      "Kilian Q. Weinberger",
      "Hadas Kress-Gazit"
    ],
    "abstract": "This paper presents INPROVF, an automatic framework that combines large\nlanguage models (LLMs) and formal methods to speed up the repair process of\nhigh-level robot controllers. Previous approaches based solely on formal\nmethods are computationally expensive and cannot scale to large state spaces.\nIn contrast, INPROVF uses LLMs to generate repair candidates, and formal\nmethods to verify their correctness. To improve the quality of these\ncandidates, our framework first translates the symbolic representations of the\nenvironment and controllers into natural language descriptions. If a candidate\nfails the verification, INPROVF provides feedback on potential unsafe behaviors\nor unsatisfied tasks, and iteratively prompts LLMs to generate improved\nsolutions. We demonstrate the effectiveness of INPROVF through 12 violations\nwith various workspaces, tasks, and state space sizes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.FL",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "To appear in ICLR 2025 Workshop: VerifAI: AI Verification in the\n  Wild; in submission to 2025 IEEE 21th International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, CA, USA: IEEE, Aug. 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13660v1",
    "published_date": "2025-03-17 19:08:36 UTC",
    "updated_date": "2025-03-17 19:08:36 UTC"
  },
  {
    "arxiv_id": "2503.13657v2",
    "title": "Why Do Multi-Agent LLM Systems Fail?",
    "authors": [
      "Mert Cemri",
      "Melissa Z. Pan",
      "Shuyi Yang",
      "Lakshya A. Agrawal",
      "Bhavya Chopra",
      "Rishabh Tiwari",
      "Kurt Keutzer",
      "Aditya Parameswaran",
      "Dan Klein",
      "Kannan Ramchandran",
      "Matei Zaharia",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their\nperformance gains on popular benchmarks often remain minimal compared with\nsingle-agent frameworks. This gap highlights the need to systematically analyze\nthe challenges hindering MAS effectiveness.\n  We present MAST (Multi-Agent System Failure Taxonomy), the first empirically\ngrounded taxonomy designed to understand MAS failures. We analyze seven popular\nMAS frameworks across over 200 tasks, involving six expert human annotators.\nThrough this process, we identify 14 unique failure modes, organized into 3\noverarching categories, (i) specification issues, (ii) inter-agent\nmisalignment, and (iii) task verification. MAST emerges iteratively from\nrigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of\n0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge\npipeline integrated with MAST. We leverage two case studies to demonstrate\nMAST's practical utility in analyzing failures and guiding MAS development. Our\nfindings reveal that identified failures require more complex solutions,\nhighlighting a clear roadmap for future research. We open source our\ncomprehensive dataset and LLM annotator to facilitate further development of\nMAS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ArXiv v2",
    "pdf_url": "http://arxiv.org/pdf/2503.13657v2",
    "published_date": "2025-03-17 19:04:38 UTC",
    "updated_date": "2025-04-22 18:37:24 UTC"
  },
  {
    "arxiv_id": "2503.13621v1",
    "title": "Superalignment with Dynamic Human Values",
    "authors": [
      "Florian Mai",
      "David Kaczér",
      "Nicholas Kluge Corrêa",
      "Lucie Flek"
    ],
    "abstract": "Two core challenges of alignment are 1) scalable oversight and 2) accounting\nfor the dynamic nature of human values. While solutions like recursive reward\nmodeling address 1), they do not simultaneously account for 2). We sketch a\nroadmap for a novel algorithmic framework that trains a superhuman reasoning\nmodel to decompose complex tasks into subtasks that are still amenable to\nhuman-level guidance. Our approach relies on what we call the part-to-complete\ngeneralization hypothesis, which states that the alignment of subtask solutions\ngeneralizes to the alignment of complete solutions. We advocate for the need to\nmeasure this generalization and propose ways to improve it in the future.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment (BiAlign)",
    "pdf_url": "http://arxiv.org/pdf/2503.13621v1",
    "published_date": "2025-03-17 18:15:17 UTC",
    "updated_date": "2025-03-17 18:15:17 UTC"
  },
  {
    "arxiv_id": "2503.13447v1",
    "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
    "authors": [
      "Qin Liu",
      "Wenxuan Zhou",
      "Nan Xu",
      "James Y. Huang",
      "Fei Wang",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "abstract": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.13447v1",
    "published_date": "2025-03-17 17:59:54 UTC",
    "updated_date": "2025-03-17 17:59:54 UTC"
  },
  {
    "arxiv_id": "2503.13445v1",
    "title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance",
    "authors": [
      "Noah Y. Siegel",
      "Nicolas Heess",
      "Maria Perez-Ortiz",
      "Oana-Maria Camburu"
    ],
    "abstract": "As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "38 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13445v1",
    "published_date": "2025-03-17 17:59:39 UTC",
    "updated_date": "2025-03-17 17:59:39 UTC"
  },
  {
    "arxiv_id": "2503.13444v2",
    "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "authors": [
      "Ye Liu",
      "Kevin Qinghong Lin",
      "Chang Wen Chen",
      "Mike Zheng Shou"
    ],
    "abstract": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks,\nincluding 3 on grounded video question-answering (Grounded VideoQA), 6 on video\ntemporal grounding (VTG), and 5 on general video question-answering (VideoQA),\nverify that our agent achieves state-of-the-art performance on diverse video\nunderstanding tasks, underscoring its effectiveness in advancing video agent\nand long-form temporal reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://videomind.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.13444v2",
    "published_date": "2025-03-17 17:59:33 UTC",
    "updated_date": "2025-04-01 03:49:08 UTC"
  },
  {
    "arxiv_id": "2503.13441v2",
    "title": "Humanoid Policy ~ Human Policy",
    "authors": [
      "Ri-Zhao Qiu",
      "Shiqi Yang",
      "Xuxin Cheng",
      "Chaitanya Chawla",
      "Jialong Li",
      "Tairan He",
      "Ge Yan",
      "David J. Yoon",
      "Ryan Hoque",
      "Lars Paulsen",
      "Ge Yang",
      "Jian Zhang",
      "Sha Yi",
      "Guanya Shi",
      "Xiaolong Wang"
    ],
    "abstract": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and data: https://human-as-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.13441v2",
    "published_date": "2025-03-17 17:59:09 UTC",
    "updated_date": "2025-03-24 08:31:56 UTC"
  },
  {
    "arxiv_id": "2503.13438v1",
    "title": "Deep Belief Markov Models for POMDP Inference",
    "authors": [
      "Giacomo Arcieri",
      "Konstantinos G. Papakonstantinou",
      "Daniel Straub",
      "Eleni Chatzi"
    ],
    "abstract": "This work introduces a novel deep learning-based architecture, termed the\nDeep Belief Markov Model (DBMM), which provides efficient, model-formulation\nagnostic inference in Partially Observable Markov Decision Process (POMDP)\nproblems. The POMDP framework allows for modeling and solving sequential\ndecision-making problems under observation uncertainty. In complex,\nhigh-dimensional, partially observable environments, existing methods for\ninference based on exact computations (e.g., via Bayes' theorem) or sampling\nalgorithms do not scale well. Furthermore, ground truth states may not be\navailable for learning the exact transition dynamics. DBMMs extend deep Markov\nmodels into the partially observable decision-making framework and allow\nefficient belief inference entirely based on available observation data via\nvariational inference methods. By leveraging the potency of neural networks,\nDBMMs can infer and simulate non-linear relationships in the system dynamics\nand naturally scale to problems with high dimensionality and discrete or\ncontinuous variables. In addition, neural network parameters can be dynamically\nupdated efficiently based on data availability. DBMMs can thus be used to infer\na belief variable, thus enabling the derivation of POMDP solutions over the\nbelief space. We evaluate the efficacy of the proposed methodology by\nevaluating the capability of model-formulation agnostic inference of DBMMs in\nbenchmark problems that include discrete and continuous variables.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13438v1",
    "published_date": "2025-03-17 17:58:45 UTC",
    "updated_date": "2025-03-17 17:58:45 UTC"
  },
  {
    "arxiv_id": "2503.13434v1",
    "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing",
    "authors": [
      "Yaowei Li",
      "Lingen Li",
      "Zhaoyang Zhang",
      "Xiaoyu Li",
      "Guangzhi Wang",
      "Hongxiang Li",
      "Xiaodong Cun",
      "Ying Shan",
      "Yuexian Zou"
    ],
    "abstract": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/",
    "pdf_url": "http://arxiv.org/pdf/2503.13434v1",
    "published_date": "2025-03-17 17:58:05 UTC",
    "updated_date": "2025-03-17 17:58:05 UTC"
  },
  {
    "arxiv_id": "2503.17388v1",
    "title": "AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations",
    "authors": [
      "Dillon Bowen",
      "Ann-Kathrin Dombrowski",
      "Adam Gleave",
      "Chris Cundy"
    ],
    "abstract": "The rapid advancement of AI systems has raised widespread concerns about\npotential harms of frontier AI systems and the need for responsible evaluation\nand oversight. In this position paper, we argue that frontier AI companies\nshould report both pre- and post-mitigation safety evaluations to enable\ninformed policy decisions. Evaluating models at both stages provides\npolicymakers with essential evidence to regulate deployment, access, and safety\nstandards. We show that relying on either in isolation can create a misleading\npicture of model safety. Our analysis of AI safety disclosures from leading\nfrontier labs identifies three critical gaps: (1) companies rarely evaluate\nboth pre- and post-mitigation versions, (2) evaluation methods lack\nstandardization, and (3) reported results are often too vague to inform policy.\nTo address these issues, we recommend mandatory disclosure of pre- and\npost-mitigation capabilities to approved government bodies, standardized\nevaluation methods, and minimum transparency requirements for public safety\nreporting. These ensure that policymakers and regulators can craft targeted\nsafety measures, assess deployment risks, and scrutinize companies' safety\nclaims effectively.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17388v1",
    "published_date": "2025-03-17 17:56:43 UTC",
    "updated_date": "2025-03-17 17:56:43 UTC"
  },
  {
    "arxiv_id": "2503.13430v1",
    "title": "AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction",
    "authors": [
      "Thomas Monninger",
      "Md Zafar Anwar",
      "Stanislaw Antol",
      "Steffen Staab",
      "Sihao Ding"
    ],
    "abstract": "Autonomous driving requires an understanding of the infrastructure elements,\nsuch as lanes and crosswalks. To navigate safely, this understanding must be\nderived from sensor data in real-time and needs to be represented in vectorized\nform. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set\nof camera images from multiple views into one joint latent BEV grid.\nTraditionally, from this latent space, an intermediate raster map is predicted,\nproviding dense spatial supervision but requiring post-processing into the\ndesired vectorized form. More recent models directly derive infrastructure\nelements as polylines using vectorized map decoders, providing instance-level\ninformation. Our approach, Augmentation Map Network (AugMapNet), proposes\nlatent BEV grid augmentation, a novel technique that significantly enhances the\nlatent BEV representation. AugMapNet combines vector decoding and dense spatial\nsupervision more effectively than existing architectures while remaining as\nstraightforward to integrate and as generic as auxiliary supervision.\nExperiments on nuScenes and Argoverse2 datasets demonstrate significant\nimprovements in vectorized map prediction performance up to 13.3% over the\nStreamMapNet baseline on 60m range and greater improvements on larger ranges.\nWe confirm transferability by applying our method to another baseline and find\nsimilar improvements. A detailed analysis of the latent BEV grid confirms a\nmore structured latent space of AugMapNet and shows the value of our novel\nconcept beyond pure performance improvement. The code will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13430v1",
    "published_date": "2025-03-17 17:55:32 UTC",
    "updated_date": "2025-03-17 17:55:32 UTC"
  },
  {
    "arxiv_id": "2503.13427v1",
    "title": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference",
    "authors": [
      "Maximilian Beck",
      "Korbinian Pöppel",
      "Phillip Lippe",
      "Richard Kurle",
      "Patrick M. Blies",
      "Günter Klambauer",
      "Sebastian Böck",
      "Sepp Hochreiter"
    ],
    "abstract": "Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at: https://github.com/NX-AI/xlstm and\n  https://github.com/NX-AI/xlstm-jax",
    "pdf_url": "http://arxiv.org/pdf/2503.13427v1",
    "published_date": "2025-03-17 17:54:55 UTC",
    "updated_date": "2025-03-17 17:54:55 UTC"
  },
  {
    "arxiv_id": "2503.13419v1",
    "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
    "authors": [
      "Ripan Kumar Kundu",
      "Matthew Denton",
      "Genova Mongalo",
      "Prasad Calyam",
      "Khaza Anuarul Hoque"
    ],
    "abstract": "The synergy between virtual reality (VR) and artificial intelligence (AI),\nspecifically deep learning (DL)-based cybersickness detection models, has\nushered in unprecedented advancements in immersive experiences by automatically\ndetecting cybersickness severity and adaptively various mitigation techniques,\noffering a smooth and comfortable VR experience. While this DL-enabled\ncybersickness detection method provides promising solutions for enhancing user\nexperiences, it also introduces new risks since these models are vulnerable to\nadversarial attacks; a small perturbation of the input data that is visually\nundetectable to human observers can fool the cybersickness detection model and\ntrigger unexpected mitigation, thus disrupting user immersive experiences (UIX)\nand even posing safety risks. In this paper, we present a new type of VR\nattack, i.e., a cybersickness attack, which successfully stops the triggering\nof cybersickness mitigation by fooling DL-based cybersickness detection models\nand dramatically hinders the UIX. Next, we propose a novel explainable\nartificial intelligence (XAI)-guided cybersickness attack detection framework\nto detect such attacks in VR to ensure UIX and a comfortable VR experience. We\nevaluate the proposed attack and the detection framework using two\nstate-of-the-art open-source VR cybersickness datasets: Simulation 2021 and\nGameplay dataset. Finally, to verify the effectiveness of our proposed method,\nwe implement the attack and the XAI-based detection using a testbed with a\ncustom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and\nperform a user study. Our study shows that such an attack can dramatically\nhinder the UIX. However, our proposed XAI-guided cybersickness attack detection\ncan successfully detect cybersickness attacks and trigger the proper\nmitigation, effectively reducing VR cybersickness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2503.13419v1",
    "published_date": "2025-03-17 17:49:51 UTC",
    "updated_date": "2025-03-17 17:49:51 UTC"
  },
  {
    "arxiv_id": "2503.13418v1",
    "title": "FLEX: A Framework for Learning Robot-Agnostic Force-based Skills Involving Sustained Contact Object Manipulation",
    "authors": [
      "Shijie Fang",
      "Wenchang Gao",
      "Shivam Goel",
      "Christopher Thierauf",
      "Matthias Scheutz",
      "Jivko Sinapov"
    ],
    "abstract": "Learning to manipulate objects efficiently, particularly those involving\nsustained contact (e.g., pushing, sliding) and articulated parts (e.g.,\ndrawers, doors), presents significant challenges. Traditional methods, such as\nrobot-centric reinforcement learning (RL), imitation learning, and hybrid\ntechniques, require massive training and often struggle to generalize across\ndifferent objects and robot platforms. We propose a novel framework for\nlearning object-centric manipulation policies in force space, decoupling the\nrobot from the object. By directly applying forces to selected regions of the\nobject, our method simplifies the action space, reduces unnecessary\nexploration, and decreases simulation overhead. This approach, trained in\nsimulation on a small set of representative objects, captures object dynamics\n-- such as joint configurations -- allowing policies to generalize effectively\nto new, unseen objects. Decoupling these policies from robot-specific dynamics\nenables direct transfer to different robotic platforms (e.g., Kinova, Panda,\nUR5) without retraining. Our evaluations demonstrate that the method\nsignificantly outperforms baselines, achieving over an order of magnitude\nimprovement in training efficiency compared to other state-of-the-art methods.\nAdditionally, operating in force space enhances policy transferability across\ndiverse robot platforms and object types. We further showcase the applicability\nof our method in a real-world robotic setting. For supplementary materials and\nvideos, please visit: https://tufts-ai-robotics-group.github.io/FLEX/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at IEEE-ICRA-2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13418v1",
    "published_date": "2025-03-17 17:49:47 UTC",
    "updated_date": "2025-03-17 17:49:47 UTC"
  },
  {
    "arxiv_id": "2503.13415v1",
    "title": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives",
    "authors": [
      "Weiqiang Jin",
      "Hongyang Du",
      "Biao Zhao",
      "Xingwu Tian",
      "Bohang Shi",
      "Guang Yang"
    ],
    "abstract": "With the rapid development of artificial intelligence, intelligent\ndecision-making techniques have gradually surpassed human levels in various\nhuman-machine competitions, especially in complex multi-agent cooperative task\nscenarios. Multi-agent cooperative decision-making involves multiple agents\nworking together to complete established tasks and achieve specific objectives.\nThese techniques are widely applicable in real-world scenarios such as\nautonomous driving, drone navigation, disaster rescue, and simulated military\nconfrontations. This paper begins with a comprehensive survey of the leading\nsimulation environments and platforms used for multi-agent cooperative\ndecision-making. Specifically, we provide an in-depth analysis for these\nsimulation environments from various perspectives, including task formats,\nreward allocation, and the underlying technologies employed. Subsequently, we\nprovide a comprehensive overview of the mainstream intelligent decision-making\napproaches, algorithms and models for multi-agent systems (MAS).\nTheseapproaches can be broadly categorized into five types: rule-based\n(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep\nmulti-agent reinforcement learning (MARL)-based, and large language\nmodels(LLMs)reasoning-based. Given the significant advantages of MARL\nandLLMs-baseddecision-making methods over the traditional rule, game theory,\nand evolutionary algorithms, this paper focuses on these multi-agent methods\nutilizing MARL and LLMs-based techniques. We provide an in-depth discussion of\nthese approaches, highlighting their methodology taxonomies, advantages, and\ndrawbacks. Further, several prominent research directions in the future and\npotential challenges of multi-agent cooperative decision-making are also\ndetailed.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "54 pages, 24 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13415v1",
    "published_date": "2025-03-17 17:45:46 UTC",
    "updated_date": "2025-03-17 17:45:46 UTC"
  },
  {
    "arxiv_id": "2503.14546v1",
    "title": "The Impact of Artificial Intelligence on Emergency Medicine: A Review of Recent Advances",
    "authors": [
      "Gustavo Correia",
      "Victor Alves",
      "Paulo Novais"
    ],
    "abstract": "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07"
    ],
    "primary_category": "eess.IV",
    "comment": "20 pages, 2 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14546v1",
    "published_date": "2025-03-17 17:45:00 UTC",
    "updated_date": "2025-03-17 17:45:00 UTC"
  },
  {
    "arxiv_id": "2503.13414v1",
    "title": "Reward Adaptation Via Q-Manipulation",
    "authors": [
      "Kevin Vora",
      "Yu Zhang"
    ],
    "abstract": "In this paper, we propose a new solution to reward adaptation (RA), the\nproblem where the learning agent adapts to a target reward function based on\none or multiple existing behaviors learned a priori under the same domain\ndynamics but different reward functions. Learning the target behavior from\nscratch is possible but often inefficient given the available source behaviors.\nOur work represents a new approach to RA via the manipulation of Q-functions.\nAssuming that the target reward function is a known function of the source\nreward functions, our approach to RA computes bounds of the Q function. We\nintroduce an iterative process to tighten the bounds, similar to value\niteration. This enables action pruning in the target domain before learning\neven starts. We refer to such a method as Q-Manipulation (Q-M). We formally\nprove that our pruning strategy does not affect the optimality of the returned\npolicy while empirically show that it improves the sample complexity. Q-M is\nevaluated in a variety of synthetic and simulation domains to demonstrate its\neffectiveness, generalizability, and practicality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13414v1",
    "published_date": "2025-03-17 17:42:54 UTC",
    "updated_date": "2025-03-17 17:42:54 UTC"
  },
  {
    "arxiv_id": "2503.13413v3",
    "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
    "authors": [
      "Dengyun Peng",
      "Yuhang Zhou",
      "Qiguang Chen",
      "Jinhao Liu",
      "Jingjing Chen",
      "Libo Qin"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.13413v3",
    "published_date": "2025-03-17 17:42:51 UTC",
    "updated_date": "2025-03-19 14:18:01 UTC"
  },
  {
    "arxiv_id": "2503.13404v1",
    "title": "Fed-Joint: Joint Modeling of Nonlinear Degradation Signals and Failure Events for Remaining Useful Life Prediction using Federated Learning",
    "authors": [
      "Cheoljoon Jeong",
      "Xubo Yue",
      "Seokhyun Chung"
    ],
    "abstract": "Many failure mechanisms of machinery are closely related to the behavior of\ncondition monitoring (CM) signals. To achieve a cost-effective preventive\nmaintenance strategy, accurate remaining useful life (RUL) prediction based on\nthe signals is of paramount importance. However, the CM signals are often\nrecorded at different factories and production lines, with limited amounts of\ndata. Unfortunately, these datasets have rarely been shared between the sites\ndue to data confidentiality and ownership issues, a lack of computing and\nstorage power, and high communication costs associated with data transfer\nbetween sites and a data center. Another challenge in real applications is that\nthe CM signals are often not explicitly specified \\textit{a priori}, meaning\nthat existing methods, which often usually a parametric form, may not be\napplicable. To address these challenges, we propose a new prognostic framework\nfor RUL prediction using the joint modeling of nonlinear degradation signals\nand time-to-failure data within a federated learning scheme. The proposed\nmethod constructs a nonparametric degradation model using a federated\nmulti-output Gaussian process and then employs a federated survival model to\npredict failure times and probabilities for in-service machinery. The\nsuperiority of the proposed method over other alternatives is demonstrated\nthrough comprehensive simulation studies and a case study using turbofan engine\ndegradation signal data that include run-to-failure events.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13404v1",
    "published_date": "2025-03-17 17:34:34 UTC",
    "updated_date": "2025-03-17 17:34:34 UTC"
  },
  {
    "arxiv_id": "2503.13401v1",
    "title": "Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis",
    "authors": [
      "Alexander Ku",
      "Declan Campbell",
      "Xuechunzi Bai",
      "Jiayi Geng",
      "Ryan Liu",
      "Raja Marjieh",
      "R. Thomas McCoy",
      "Andrew Nam",
      "Ilia Sucholutsky",
      "Veniamin Veselovsky",
      "Liyi Zhang",
      "Jian-Qiao Zhu",
      "Thomas L. Griffiths"
    ],
    "abstract": "Modern artificial intelligence systems, such as large language models, are\nincreasingly powerful but also increasingly hard to understand. Recognizing\nthis problem as analogous to the historical difficulties in understanding the\nhuman mind, we argue that methods developed in cognitive science can be useful\nfor understanding large language models. We propose a framework for applying\nthese methods based on Marr's three levels of analysis. By revisiting\nestablished cognitive science techniques relevant to each level and\nillustrating their potential to yield insights into the behavior and internal\norganization of large language models, we aim to provide a toolkit for making\nsense of these new kinds of minds.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13401v1",
    "published_date": "2025-03-17 17:33:54 UTC",
    "updated_date": "2025-03-17 17:33:54 UTC"
  },
  {
    "arxiv_id": "2503.13399v1",
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
    "authors": [
      "James Burgess",
      "Jeffrey J Nirschl",
      "Laura Bravo-Sánchez",
      "Alejandro Lozano",
      "Sanket Rajan Gupte",
      "Jesus G. Galaz-Montoya",
      "Yuhui Zhang",
      "Yuchang Su",
      "Disha Bhowmik",
      "Zachary Coman",
      "Sarina M. Hasan",
      "Alexandra Johannesson",
      "William D. Leineweber",
      "Malvika G Nair",
      "Ridhi Yarlagadda",
      "Connor Zuraski",
      "Wah Chiu",
      "Sarah Cohen",
      "Jan N. Hansen",
      "Manuel D Leonetti",
      "Chad Liu",
      "Emma Lundberg",
      "Serena Yeung-Levy"
    ],
    "abstract": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.CB"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa",
    "pdf_url": "http://arxiv.org/pdf/2503.13399v1",
    "published_date": "2025-03-17 17:33:10 UTC",
    "updated_date": "2025-03-17 17:33:10 UTC"
  },
  {
    "arxiv_id": "2503.13385v1",
    "title": "Scale Efficient Training for Large Datasets",
    "authors": [
      "Qing Zhou",
      "Junyu Gao",
      "Qi Wang"
    ],
    "abstract": "The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13385v1",
    "published_date": "2025-03-17 17:13:43 UTC",
    "updated_date": "2025-03-17 17:13:43 UTC"
  },
  {
    "arxiv_id": "2503.13383v1",
    "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning",
    "authors": [
      "Mengyao Lyu",
      "Yan Li",
      "Huasong Zhong",
      "Wenhao Yang",
      "Hui Chen",
      "Jungong Han",
      "Guiguang Ding",
      "Zhenheng Yang"
    ],
    "abstract": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "update comparison with sota and analysis",
    "pdf_url": "http://arxiv.org/pdf/2503.13383v1",
    "published_date": "2025-03-17 17:11:22 UTC",
    "updated_date": "2025-03-17 17:11:22 UTC"
  },
  {
    "arxiv_id": "2503.13377v1",
    "title": "TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM",
    "authors": [
      "Ye Wang",
      "Boshen Xu",
      "Zihao Yue",
      "Zihan Xiao",
      "Ziheng Wang",
      "Liang Zhang",
      "Dingyi Yang",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "abstract": "We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/www-Ye/TimeZero",
    "pdf_url": "http://arxiv.org/pdf/2503.13377v1",
    "published_date": "2025-03-17 17:04:20 UTC",
    "updated_date": "2025-03-17 17:04:20 UTC"
  },
  {
    "arxiv_id": "2503.13369v1",
    "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions",
    "authors": [
      "Wan Ju Kang",
      "Eunki Kim",
      "Na Min An",
      "Sangryul Kim",
      "Haemin Choi",
      "Ki Hoon Kwak",
      "James Thorne"
    ],
    "abstract": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 10 figures, 21 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.13369v1",
    "published_date": "2025-03-17 16:52:46 UTC",
    "updated_date": "2025-03-17 16:52:46 UTC"
  },
  {
    "arxiv_id": "2503.14543v2",
    "title": "Inteligencia Artificial para la conservación y uso sostenible de la biodiversidad, una visión desde Colombia (Artificial Intelligence for conservation and sustainable use of biodiversity, a view from Colombia)",
    "authors": [
      "Juan Sebastián Cañas",
      "Camila Parra-Guevara",
      "Manuela Montoya-Castrillón",
      "Julieta M Ramírez-Mejía",
      "Gabriel-Alejandro Perilla",
      "Esteban Marentes",
      "Nerieth Leuro",
      "Jose Vladimir Sandoval-Sierra",
      "Sindy Martinez-Callejas",
      "Angélica Díaz",
      "Mario Murcia",
      "Elkin A. Noguera-Urbano",
      "Jose Manuel Ochoa-Quintero",
      "Susana Rodríguez Buriticá",
      "Juan Sebastián Ulloa"
    ],
    "abstract": "The rise of artificial intelligence (AI) and the aggravating biodiversity\ncrisis have resulted in a research area where AI-based computational methods\nare being developed to act as allies in conservation, and the sustainable use\nand management of natural resources. While important general guidelines have\nbeen established globally regarding the opportunities and challenges that this\ninterdisciplinary research offers, it is essential to generate local\nreflections from the specific contexts and realities of each region. Hence,\nthis document aims to analyze the scope of this research area from a\nperspective focused on Colombia and the Neotropics. In this paper, we summarize\nthe main experiences and debates that took place at the Humboldt Institute\nbetween 2023 and 2024 in Colombia. To illustrate the variety of promising\nopportunities, we present current uses such as automatic species identification\nfrom images and recordings, species modeling, and in silico bioprospecting,\namong others. From the experiences described above, we highlight limitations,\nchallenges, and opportunities for in order to successfully implementate AI in\nconservation efforts and sustainable management of biological resources in the\nNeotropics. The result aims to be a guide for researchers, decision makers, and\nbiodiversity managers, facilitating the understanding of how artificial\nintelligence can be effectively integrated into conservation and sustainable\nuse strategies. Furthermore, it also seeks to open a space for dialogue on the\ndevelopment of policies that promote the responsible and ethical adoption of AI\nin local contexts, ensuring that its benefits are harnessed without\ncompromising biodiversity or the cultural and ecosystemic values inherent in\nColombia and the Neotropics.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14543v2",
    "published_date": "2025-03-17 16:47:05 UTC",
    "updated_date": "2025-03-21 01:10:08 UTC"
  },
  {
    "arxiv_id": "2503.13360v1",
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
    "authors": [
      "Hai-Long Sun",
      "Zhun Sun",
      "Houwen Peng",
      "Han-Jia Ye"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC",
    "pdf_url": "http://arxiv.org/pdf/2503.13360v1",
    "published_date": "2025-03-17 16:45:12 UTC",
    "updated_date": "2025-03-17 16:45:12 UTC"
  },
  {
    "arxiv_id": "2503.16525v2",
    "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse",
    "authors": [
      "Huan Yang",
      "Renji Zhang",
      "Mingzhe Huang",
      "Weijun Wang",
      "Yin Tang",
      "Yuanchun Li",
      "Yunxin Liu",
      "Deyu Zhang"
    ],
    "abstract": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16525v2",
    "published_date": "2025-03-17 16:43:35 UTC",
    "updated_date": "2025-05-16 12:42:48 UTC"
  },
  {
    "arxiv_id": "2503.13343v1",
    "title": "Scalable Runtime Architecture for Data-driven, Hybrid HPC and ML Workflow Applications",
    "authors": [
      "Andre Merzky",
      "Mikhail Titov",
      "Matteo Turilli",
      "Ozgur Kilic",
      "Tianle Wang",
      "Shantenu Jha"
    ],
    "abstract": "Hybrid workflows combining traditional HPC and novel ML methodologies are\ntransforming scientific computing. This paper presents the architecture and\nimplementation of a scalable runtime system that extends RADICAL-Pilot with\nservice-based execution to support AI-out-HPC workflows. Our runtime system\nenables distributed ML capabilities, efficient resource management, and\nseamless HPC/ML coupling across local and remote platforms. Preliminary\nexperimental results show that our approach manages concurrent execution of ML\nmodels across local and remote HPC/cloud resources with minimal architectural\noverheads. This lays the foundation for prototyping three representative\ndata-driven workflow applications and executing them at scale on\nleadership-class HPC platforms.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13343v1",
    "published_date": "2025-03-17 16:21:48 UTC",
    "updated_date": "2025-03-17 16:21:48 UTC"
  },
  {
    "arxiv_id": "2503.13342v1",
    "title": "Valid Text-to-SQL Generation with Unification-based DeepStochLog",
    "authors": [
      "Ying Jiao",
      "Luc De Raedt",
      "Giuseppe Marra"
    ],
    "abstract": "Large language models have been used to translate natural language questions\nto SQL queries. Without hard constraints on syntax and database schema, they\noccasionally produce invalid queries that are not executable. These failures\nlimit the usage of these systems in real-life scenarios. We propose a\nneurosymbolic framework that imposes SQL syntax and schema constraints with\nunification-based definite clause grammars and thus guarantees the generation\nof valid queries. Our framework also builds a bi-directional interface to\nlanguage models to leverage their natural language understanding abilities. The\nevaluation results on a subset of SQL grammars show that all our output queries\nare valid. This work is the first step towards extending language models with\nunification-based grammars. We demonstrate this extension enhances the\nvalidity, execution accuracy, and ground truth alignment of the underlying\nlanguage model by a large margin. Our code is available at\nhttps://github.com/ML-KULeuven/deepstochlog-lm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13342v1",
    "published_date": "2025-03-17 16:21:10 UTC",
    "updated_date": "2025-03-17 16:21:10 UTC"
  },
  {
    "arxiv_id": "2503.13335v1",
    "title": "Reliable and Efficient Amortized Model-based Evaluation",
    "authors": [
      "Sang Truong",
      "Yuheng Tu",
      "Percy Liang",
      "Bo Li",
      "Sanmi Koyejo"
    ],
    "abstract": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13335v1",
    "published_date": "2025-03-17 16:15:02 UTC",
    "updated_date": "2025-03-17 16:15:02 UTC"
  },
  {
    "arxiv_id": "2503.13580v1",
    "title": "LLM Test Generation via Iterative Hybrid Program Analysis",
    "authors": [
      "Sijia Gu",
      "Noor Nashid",
      "Ali Mesbah"
    ],
    "abstract": "Automating unit test generation remains a significant challenge, particularly\nfor complex methods in real-world projects. While Large Language Models (LLMs)\nhave made strides in code generation, they struggle to achieve high branch\ncoverage due to their limited ability to reason about intricate control flow\nstructures. To address this limitation, we introduce Panta, a technique that\nemulates the iterative process human developers follow when analyzing code and\nconstructing test cases. Panta integrates static control flow analysis and\ndynamic code coverage analysis to systematically guide LLMs in identifying\nuncovered execution paths and generating better test cases. By incorporating an\niterative feedback-driven mechanism, our technique continuously refines test\ngeneration based on static and dynamic path coverage insights, ensuring more\ncomprehensive and effective testing. Our empirical evaluation, conducted on\nclasses with high cyclomatic complexity from open-source projects, demonstrates\nthat Panta achieves 26% higher line coverage and 23% higher branch coverage\ncompared to the state-of-the-art.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13580v1",
    "published_date": "2025-03-17 16:10:38 UTC",
    "updated_date": "2025-03-17 16:10:38 UTC"
  },
  {
    "arxiv_id": "2503.13330v1",
    "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision",
    "authors": [
      "Ricardo Bigolin Lanfredi",
      "Yan Zhuang",
      "Mark Finkelstein",
      "Praveen Thoppey Srinivasan Balamuralikrishna",
      "Luke Krembs",
      "Brandon Khoury",
      "Arthi Reddy",
      "Pritam Mukherjee",
      "Neil M. Rofsky",
      "Ronald M. Summers"
    ],
    "abstract": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13330v1",
    "published_date": "2025-03-17 16:09:22 UTC",
    "updated_date": "2025-03-17 16:09:22 UTC"
  },
  {
    "arxiv_id": "2503.13579v1",
    "title": "ASMR: Adaptive Skeleton-Mesh Rigging and Skinning via 2D Generative Prior",
    "authors": [
      "Seokhyeon Hong",
      "Soojin Choi",
      "Chaelin Kim",
      "Sihun Cha",
      "Junyong Noh"
    ],
    "abstract": "Despite the growing accessibility of skeletal motion data, integrating it for\nanimating character meshes remains challenging due to diverse configurations of\nboth skeletons and meshes. Specifically, the body scale and bone lengths of the\nskeleton should be adjusted in accordance with the size and proportions of the\nmesh, ensuring that all joints are accurately positioned within the character\nmesh. Furthermore, defining skinning weights is complicated by variations in\nskeletal configurations, such as the number of joints and their hierarchy, as\nwell as differences in mesh configurations, including their connectivity and\nshapes. While existing approaches have made efforts to automate this process,\nthey hardly address the variations in both skeletal and mesh configurations. In\nthis paper, we present a novel method for the automatic rigging and skinning of\ncharacter meshes using skeletal motion data, accommodating arbitrary\nconfigurations of both meshes and skeletons. The proposed method predicts the\noptimal skeleton aligned with the size and proportion of the mesh as well as\ndefines skinning weights for various mesh-skeleton configurations, without\nrequiring explicit supervision tailored to each of them. By incorporating\nDiffusion 3D Features (Diff3F) as semantic descriptors of character meshes, our\nmethod achieves robust generalization across different configurations. To\nassess the performance of our method in comparison to existing approaches, we\nconducted comprehensive evaluations encompassing both quantitative and\nqualitative analyses, specifically examining the predicted skeletons, skinning\nweights, and deformation quality.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "Eurographics 2025; Project Page\n  https://seokhyeonhong.github.io/projects/asmr/",
    "pdf_url": "http://arxiv.org/pdf/2503.13579v1",
    "published_date": "2025-03-17 15:59:02 UTC",
    "updated_date": "2025-03-17 15:59:02 UTC"
  },
  {
    "arxiv_id": "2503.13316v1",
    "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
    "authors": [
      "Marcello Iotti",
      "Paolo Davini",
      "Jost von Hardenberg",
      "Giuseppe Zappa"
    ],
    "abstract": "To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "38 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13316v1",
    "published_date": "2025-03-17 15:54:20 UTC",
    "updated_date": "2025-03-17 15:54:20 UTC"
  },
  {
    "arxiv_id": "2503.13310v1",
    "title": "Generative AI for Software Architecture. Applications, Trends, Challenges, and Future Directions",
    "authors": [
      "Matteo Esposito",
      "Xiaozhou Li",
      "Sergio Moreschini",
      "Noman Ahmad",
      "Tomas Cerny",
      "Karthik Vaidhyanathan",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "abstract": "Context: Generative Artificial Intelligence (GenAI) is transforming much of\nsoftware development, yet its application in software architecture is still in\nits infancy, and no prior study has systematically addressed the topic. Aim: We\naim to systematically synthesize the use, rationale, contexts, usability, and\nfuture challenges of GenAI in software architecture. Method: We performed a\nmultivocal literature review (MLR), analyzing peer-reviewed and gray\nliterature, identifying current practices, models, adoption contexts, and\nreported challenges, extracting themes via open coding. Results: Our review\nidentified significant adoption of GenAI for architectural decision support and\narchitectural reconstruction. OpenAI GPT models are predominantly applied, and\nthere is consistent use of techniques such as few-shot prompting and\nretrieved-augmented generation (RAG). GenAI has been applied mostly to initial\nstages of the Software Development Life Cycle (SDLC), such as\nRequirements-to-Architecture and Architecture-to-Code. Monolithic and\nmicroservice architectures were the dominant targets. However, rigorous testing\nof GenAI outputs was typically missing from the studies. Among the most\nfrequent challenges are model precision, hallucinations, ethical aspects,\nprivacy issues, lack of architecture-specific datasets, and the absence of\nsound evaluation frameworks. Conclusions: GenAI shows significant potential in\nsoftware design, but several challenges remain on its path to greater adoption.\nResearch efforts should target designing general evaluation methodologies,\nhandling ethics and precision, increasing transparency and explainability, and\npromoting architecture-specific datasets and benchmarks to bridge the gap\nbetween theoretical possibilities and practical use.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13310v1",
    "published_date": "2025-03-17 15:49:30 UTC",
    "updated_date": "2025-03-17 15:49:30 UTC"
  },
  {
    "arxiv_id": "2503.13309v2",
    "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework",
    "authors": [
      "Farnoush Bayatmakou",
      "Reza Taleei",
      "Milad Amir Toutounchian",
      "Arash Mohammadi"
    ],
    "abstract": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13309v2",
    "published_date": "2025-03-17 15:48:56 UTC",
    "updated_date": "2025-05-07 18:17:50 UTC"
  },
  {
    "arxiv_id": "2503.13305v1",
    "title": "Computation Mechanism Behind LLM Position Generalization",
    "authors": [
      "Chi Han",
      "Heng Ji"
    ],
    "abstract": "Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13305v1",
    "published_date": "2025-03-17 15:47:37 UTC",
    "updated_date": "2025-03-17 15:47:37 UTC"
  },
  {
    "arxiv_id": "2503.13299v1",
    "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
    "authors": [
      "Yijun Liu",
      "Jinzheng Yu",
      "Yang Xu",
      "Zhongyang Li",
      "Qingfu Zhu"
    ],
    "abstract": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.13299v1",
    "published_date": "2025-03-17 15:44:09 UTC",
    "updated_date": "2025-03-17 15:44:09 UTC"
  },
  {
    "arxiv_id": "2503.13288v1",
    "title": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation",
    "authors": [
      "Fangzhi Xu",
      "Hang Yan",
      "Chang Ma",
      "Haiteng Zhao",
      "Jun Liu",
      "Qika Lin",
      "Zhiyong Wu"
    ],
    "abstract": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13288v1",
    "published_date": "2025-03-17 15:38:33 UTC",
    "updated_date": "2025-03-17 15:38:33 UTC"
  },
  {
    "arxiv_id": "2503.13281v3",
    "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large Language Models and Retrieval-Augmented Generation",
    "authors": [
      "Xiaodi Li",
      "Shaika Chowdhury",
      "Chung Il Wi",
      "Maria Vassilaki",
      "Xiaoke Liu",
      "Terence T Sio",
      "Owen Garrick",
      "Young J Juhn",
      "James R Cerhan",
      "Cui Tao",
      "Nansu Zong"
    ],
    "abstract": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.13281v3",
    "published_date": "2025-03-17 15:31:55 UTC",
    "updated_date": "2025-03-24 19:32:25 UTC"
  },
  {
    "arxiv_id": "2503.13279v1",
    "title": "Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for Impacting Mapping on Requirements Elicitation",
    "authors": [
      "Xinkai Zou",
      "Yan Liu",
      "Xiongbo Shi",
      "Chen Yang"
    ],
    "abstract": "As requirements drift with rapid iterations, agile development becomes the\ndominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet\nchallenging task in agile project development due to its heavy tangling with\nadaptive planning and efficient collaboration. Recently, AI agents have shown\npromising ability in supporting requirements analysis by saving significant\ntime and effort for stakeholders. However, current research mainly focuses on\nfunctional RE, and research works have not been reported bridging the long\njourney from goal to user stories. Moreover, considering the cost of LLM\nfacilities and the need for data and idea protection, privately hosted\nsmall-sized LLM should be further utilized in RE. To address these challenges,\nwe propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM)\nframework while merely using cost-effective sLLMs for goal-driven RE. Moreover,\nwe introduce a StorySeek dataset that contains over 1,000 user stories (USs)\nwith corresponding goals and project context information, as well as the\nsemi-automatic dataset construction method. For evaluation, we proposed two\nmetrics: Factuality Hit Rate (FHR) to measure consistency between the generated\nUSs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate\nthe quality of the generated USs. Experimental results demonstrate that\nGoal2Story outperforms the baseline performance of the Super-Agent adopting\npowerful LLMs, while also showcasing the performance improvements in key\nmetrics brought by CoT and Agent Profile to Goal2Story, as well as its\nexploration in identifying latent needs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13279v1",
    "published_date": "2025-03-17 15:31:20 UTC",
    "updated_date": "2025-03-17 15:31:20 UTC"
  },
  {
    "arxiv_id": "2503.13277v1",
    "title": "Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach",
    "authors": [
      "Alfred Simbun",
      "Suresh Kumar"
    ],
    "abstract": "Background: The COVID-19 pandemic has overwhelmed healthcare systems,\nemphasizing the need for AI-driven tools to assist in rapid and accurate\npatient prognosis. Chest X-ray imaging is a widely available diagnostic tool,\nbut existing methods for prognosis classification lack scalability and\nefficiency. Objective: This study presents a high-accuracy deep learning model\nfor classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest\nX-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a\ndataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained\nand validated a deep learning model leveraging Convolutional Neural Networks\n(CNNs). The model was evaluated on an unseen dataset to measure accuracy,\nprecision, and recall. Results: Our model achieved an average accuracy of 97%,\nwith specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When\nclassifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),\n95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's\npotential for real-world clinical applications, aiding in faster\ndecision-making and improved resource allocation. Conclusion: AI-driven\nprognosis classification using deep learning can significantly enhance COVID-19\npatient management, enabling early intervention and efficient triaging. Our\nstudy provides a scalable, high-accuracy AI framework for integrating deep\nlearning into routine clinical workflows. Future work should focus on expanding\ndatasets, external validation, and regulatory compliance to facilitate clinical\nadoption.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "27 pages, 6 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.13277v1",
    "published_date": "2025-03-17 15:27:21 UTC",
    "updated_date": "2025-03-17 15:27:21 UTC"
  },
  {
    "arxiv_id": "2503.13275v2",
    "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
    "authors": [
      "Seyoung Song"
    ],
    "abstract": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "I.2.0; I.2.7; I.2.11; H.3.3"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13275v2",
    "published_date": "2025-03-17 15:27:02 UTC",
    "updated_date": "2025-04-01 14:21:15 UTC"
  },
  {
    "arxiv_id": "2504.07109v1",
    "title": "OSCAR: Online Soft Compression And Reranking",
    "authors": [
      "Maxime Louis",
      "Thibault Formal",
      "Hervé Dejean",
      "Stéphane Clinchant"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge, leading to improved accuracy and relevance.\nHowever, scaling RAG pipelines remains computationally expensive as retrieval\nsizes grow. To address this, we introduce OSCAR, a novel query-dependent online\nsoft compression method that reduces computational overhead while preserving\nperformance. Unlike traditional hard compression methods, which shorten\nretrieved texts, or soft compression approaches, which map documents to\ncontinuous embeddings offline, OSCAR dynamically compresses retrieved\ninformation at inference time, eliminating storage overhead and enabling higher\ncompression rates. Additionally, we extend OSCAR to simultaneously perform\nreranking, further optimizing the efficiency of the RAG pipeline. Our\nexperiments demonstrate state-of-the-art performance with a 2-5x speed-up in\ninference and minimal to no loss in accuracy for LLMs ranging from 1B to 24B\nparameters. The models are available at:\nhttps://huggingface.co/collections/naver/oscar-67d446a8e3a2551f57464295.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.07109v1",
    "published_date": "2025-03-17 15:10:09 UTC",
    "updated_date": "2025-03-17 15:10:09 UTC"
  },
  {
    "arxiv_id": "2503.13578v1",
    "title": "Convolutional neural network for early detection of lameness and irregularity in horses using an IMU sensor",
    "authors": [
      "Benoît Savoini",
      "Jonathan Bertolaccini",
      "Stéphane Montavon",
      "Michel Deriaz"
    ],
    "abstract": "Lameness and gait irregularities are significant concerns in equine health\nmanagement, affecting performance, welfare, and economic value. Traditional\nobservational methods rely on subjective expert assessments, which can lead to\ninconsistencies in detecting subtle or early-stage lameness. While AI-based\napproaches have emerged, many require multiple sensors, force plates, or video\nsystems, making them costly and impractical for field deployment. In this\napplied research study, we present a stride-level classification system that\nutilizes a single inertial measurement unit (IMU) and a one-dimensional\nconvolutional neural network (1D CNN) to objectively differentiate between\nsound and lame horses, with a primary focus on the trot gait. The proposed\nsystem was tested under real-world conditions, achieving a 90% session-level\naccuracy with no false positives, demonstrating its robustness for practical\napplications. By employing a single, non-intrusive, and readily available\nsensor, our approach significantly reduces the complexity and cost of hardware\nrequirements while maintaining high classification performance. These results\nhighlight the potential of our CNN-based method as a field-tested, scalable\nsolution for automated lameness detection. By enabling early diagnosis, this\nsystem offers a valuable tool for preventing minor gait irregularities from\ndeveloping into severe conditions, ultimately contributing to improved equine\nwelfare and performance in veterinary and equestrian practice.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted at AMLDS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13578v1",
    "published_date": "2025-03-17 15:05:01 UTC",
    "updated_date": "2025-03-17 15:05:01 UTC"
  },
  {
    "arxiv_id": "2503.14542v1",
    "title": "AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients",
    "authors": [
      "Agnieszka Sroka-Oleksiak",
      "Adam Pardyl",
      "Dawid Rymarczyk",
      "Aldona Olechowska-Jarząb",
      "Katarzyna Biegun-Drożdż",
      "Dorota Ochońska",
      "Michał Wronka",
      "Adriana Borowa",
      "Tomasz Gosiewski",
      "Miłosz Adamczyk",
      "Henryk Telega",
      "Bartosz Zieliński",
      "Monika Brzychczy-Włoch"
    ],
    "abstract": "Sepsis is a life-threatening condition which requires rapid diagnosis and\ntreatment. Traditional microbiological methods are time-consuming and\nexpensive. In response to these challenges, deep learning algorithms were\ndeveloped to identify 14 bacteria species and 3 yeast-like fungi from\nmicroscopic images of Gram-stained smears of positive blood samples from sepsis\npatients.\n  A total of 16,637 Gram-stained microscopic images were used in the study. The\nanalysis used the Cellpose 3 model for segmentation and Attention-based Deep\nMultiple Instance Learning for classification. Our model achieved an accuracy\nof 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88,\nrespectively. The highest values, reaching up to 96.2%, were obtained for\nCutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and\nNakaseomyces glabratus. Classification difficulties were observed in closely\nrelated species, such as Staphylococcus hominis and Staphylococcus\nhaemolyticus, due to morphological similarity, and within Candida albicans due\nto high morphotic diversity.\n  The study confirms the potential of our model for microbial classification,\nbut it also indicates the need for further optimisation and expansion of the\ntraining data set. In the future, this technology could support microbial\ndiagnosis, reducing diagnostic time and improving the effectiveness of sepsis\ntreatment due to its simplicity and accessibility. Part of the results\npresented in this publication was covered by a patent application at the\nEuropean Patent Office EP24461637.1 \"A computer implemented method for\nidentifying a microorganism in a blood and a data processing system therefor\".",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14542v1",
    "published_date": "2025-03-17 15:02:49 UTC",
    "updated_date": "2025-03-17 15:02:49 UTC"
  },
  {
    "arxiv_id": "2503.13223v1",
    "title": "Robust Decision-Making Via Free Energy Minimization",
    "authors": [
      "Allahkaram Shafiei",
      "Hozefa Jesawada",
      "Karl Friston",
      "Giovanni Russo"
    ],
    "abstract": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "Contains main text and supplementary information",
    "pdf_url": "http://arxiv.org/pdf/2503.13223v1",
    "published_date": "2025-03-17 14:36:08 UTC",
    "updated_date": "2025-03-17 14:36:08 UTC"
  },
  {
    "arxiv_id": "2503.13222v2",
    "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
    "authors": [
      "Chi Han"
    ],
    "abstract": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13222v2",
    "published_date": "2025-03-17 14:31:37 UTC",
    "updated_date": "2025-03-28 17:17:40 UTC"
  },
  {
    "arxiv_id": "2503.13214v3",
    "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
    "authors": [
      "Jie Huang",
      "Haorui Chen",
      "Jiaxuan Ren",
      "Siran Peng",
      "Liangjian Deng"
    ],
    "abstract": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13214v3",
    "published_date": "2025-03-17 14:24:00 UTC",
    "updated_date": "2025-03-21 12:55:38 UTC"
  },
  {
    "arxiv_id": "2503.13211v1",
    "title": "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis",
    "authors": [
      "Marvin Seyfarth",
      "Salman Ul Hassan Dar",
      "Isabelle Ayx",
      "Matthias Alexander Fink",
      "Stefan O. Schoenberg",
      "Hans-Ulrich Kauczor",
      "Sandy Engelhardt"
    ],
    "abstract": "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13211v1",
    "published_date": "2025-03-17 14:22:49 UTC",
    "updated_date": "2025-03-17 14:22:49 UTC"
  },
  {
    "arxiv_id": "2503.13208v3",
    "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft prompt Optimization Approach",
    "authors": [
      "Sinan Fan",
      "Liang Xie",
      "Chen Shen",
      "Ge Teng",
      "Xiaosong Yuan",
      "Xiaofeng Zhang",
      "Chenxi Huang",
      "Wenxiao Wang",
      "Xiaofei He",
      "Jieping Ye"
    ],
    "abstract": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled Dynamic Prompt Corruption (DPC) to take better advantage of soft prompts\nin complex reasoning tasks, which dynamically adjusts the influence of soft\nprompts based on their impact on the reasoning process. Specifically, DPC\nconsists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic\nTrigger measures the impact of soft prompts, identifying whether beneficial or\ndetrimental. Then, Dynamic Corruption mitigates the negative effects of soft\nprompts by selectively masking key tokens that interfere with the reasoning\nprocess. We validate the proposed approach through extensive experiments on\nvarious LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental\nresults demonstrate that DPC can consistently enhance the performance of PT,\nachieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting\nthe effectiveness of our approach and its potential to enhance complex\nreasoning in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13208v3",
    "published_date": "2025-03-17 14:20:48 UTC",
    "updated_date": "2025-04-13 12:38:06 UTC"
  },
  {
    "arxiv_id": "2503.13205v1",
    "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways",
    "authors": [
      "Zhen Chen",
      "Zhihao Peng",
      "Xusheng Liang",
      "Cheng Wang",
      "Peigan Liang",
      "Linsheng Zeng",
      "Minjie Ju",
      "Yixuan Yuan"
    ],
    "abstract": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13205v1",
    "published_date": "2025-03-17 14:14:28 UTC",
    "updated_date": "2025-03-17 14:14:28 UTC"
  },
  {
    "arxiv_id": "2504.07108v1",
    "title": "OKRA: an Explainable, Heterogeneous, Multi-Stakeholder Job Recommender System",
    "authors": [
      "Roan Schellingerhout",
      "Francesco Barile",
      "Nava Tintarev"
    ],
    "abstract": "The use of recommender systems in the recruitment domain has been labeled as\n'high-risk' in recent legislation. As a result, strict requirements regarding\nexplainability and fairness have been put in place to ensure proper treatment\nof all involved stakeholders. To allow for stakeholder-specific explainability,\nwhile also handling highly heterogeneous recruitment data, we propose a novel\nexplainable multi-stakeholder job recommender system using graph neural\nnetworks: the Occupational Knowledge-based Recommender using Attention (OKRA).\nThe proposed method is capable of providing both candidate- and company-side\nrecommendations and explanations. We find that OKRA performs substantially\nbetter than six baselines in terms of nDCG for two datasets. Furthermore, we\nfind that the tested models show a bias toward candidates and vacancies located\nin urban areas. Overall, our findings suggest that OKRA provides a balance\nbetween accuracy, explainability, and fairness.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "17 pages, 1 figure, 1 table, to be published in the proceedings of\n  ECIR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.07108v1",
    "published_date": "2025-03-17 14:12:51 UTC",
    "updated_date": "2025-03-17 14:12:51 UTC"
  },
  {
    "arxiv_id": "2503.14538v3",
    "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data",
    "authors": [
      "Ananya Ganapthy",
      "Praveen Shastry",
      "Naveen Kumarasami",
      "Anandakumar D",
      "Keerthana R",
      "Mounigasri M",
      "Varshinipriya M",
      "Kishore Prasath Venkatesh",
      "Bargava Subramanian",
      "Kalyan Sivasailam"
    ],
    "abstract": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07, 68T45, 92C55, 92C50, 68U10"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14538v3",
    "published_date": "2025-03-17 14:08:35 UTC",
    "updated_date": "2025-04-01 06:41:57 UTC"
  },
  {
    "arxiv_id": "2503.13200v1",
    "title": "Timing the Match: A Deep Reinforcement Learning Approach for Ride-Hailing and Ride-Pooling Services",
    "authors": [
      "Yiman Bao",
      "Jie Gao",
      "Jinke He",
      "Frans A. Oliehoek",
      "Oded Cats"
    ],
    "abstract": "Efficient timing in ride-matching is crucial for improving the performance of\nride-hailing and ride-pooling services, as it determines the number of drivers\nand passengers considered in each matching process. Traditional batched\nmatching methods often use fixed time intervals to accumulate ride requests\nbefore assigning matches. While this approach increases the number of available\ndrivers and passengers for matching, it fails to adapt to real-time\nsupply-demand fluctuations, often leading to longer passenger wait times and\ndriver idle periods. To address this limitation, we propose an adaptive\nride-matching strategy using deep reinforcement learning (RL) to dynamically\ndetermine when to perform matches based on real-time system conditions. Unlike\nfixed-interval approaches, our method continuously evaluates system states and\nexecutes matching at moments that minimize total passenger wait time.\nAdditionally, we incorporate a potential-based reward shaping (PBRS) mechanism\nto mitigate sparse rewards, accelerating RL training and improving decision\nquality. Extensive empirical evaluations using a realistic simulator trained on\nreal-world data demonstrate that our approach outperforms fixed-interval\nmatching strategies, significantly reducing passenger waiting times and detour\ndelays, thereby enhancing the overall efficiency of ride-hailing and\nride-pooling systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13200v1",
    "published_date": "2025-03-17 14:07:58 UTC",
    "updated_date": "2025-03-17 14:07:58 UTC"
  },
  {
    "arxiv_id": "2503.13194v1",
    "title": "A representational framework for learning and encoding structurally enriched trajectories in complex agent environments",
    "authors": [
      "Corina Catarau-Cotutiu",
      "Esther Mondragon",
      "Eduardo Alonso"
    ],
    "abstract": "The ability of artificial intelligence agents to make optimal decisions and\ngeneralise them to different domains and tasks is compromised in complex\nscenarios. One way to address this issue has focused on learning efficient\nrepresentations of the world and on how the actions of agents affect them, such\nas disentangled representations that exploit symmetries. Whereas such\nrepresentations are procedurally efficient, they are based on the compression\nof low-level state-action transitions, which lack structural richness. To\naddress this problem, we propose to enrich the agent's ontology and extend the\ntraditional conceptualisation of trajectories to provide a more nuanced view of\ntask execution. Structurally Enriched Trajectories (SETs) extend the encoding\nof sequences of states and their transitions by incorporating hierarchical\nrelations between objects, interactions and affordances. SETs are built as\nmulti-level graphs, providing a detailed representation of the agent dynamics\nand a transferable functional abstraction of the task. SETs are integrated into\nan architecture, Structurally Enriched Trajectory Learning and Encoding\n(SETLE), that employs a heterogeneous graph-based memory structure of\nmulti-level relational dependencies essential for generalisation. Using\nreinforcement learning as a data generation tool, we demonstrate that SETLE can\nsupport downstream tasks, enabling agents to recognise task-relevant structural\npatterns across diverse environments.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13194v1",
    "published_date": "2025-03-17 14:04:27 UTC",
    "updated_date": "2025-03-17 14:04:27 UTC"
  },
  {
    "arxiv_id": "2503.13185v1",
    "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o",
    "authors": [
      "Dingning Liu",
      "Cheng Wang",
      "Peng Gao",
      "Renrui Zhang",
      "Xinzhu Ma",
      "Yuan Meng",
      "Zhihui Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt\nengineering techniques.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13185v1",
    "published_date": "2025-03-17 13:57:05 UTC",
    "updated_date": "2025-03-17 13:57:05 UTC"
  },
  {
    "arxiv_id": "2503.13180v2",
    "title": "GC-Fed: Gradient Centralized Federated Learning with Partial Client Participation",
    "authors": [
      "Jungwon Seo",
      "Ferhat Ozgur Catak",
      "Chunming Rong",
      "Kibeom Hong",
      "Minhoe Kim"
    ],
    "abstract": "Federated Learning (FL) enables privacy-preserving multi-source information\nfusion (MSIF) but is challenged by client drift in highly heterogeneous data\nsettings. Many existing drift-mitigation strategies rely on reference-based\ntechniques--such as gradient adjustments or proximal loss--that use historical\nsnapshots (e.g., past gradients or previous global models) as reference points.\nWhen only a subset of clients participates in each training round, these\nhistorical references may not accurately capture the overall data distribution,\nleading to unstable training. In contrast, our proposed Gradient Centralized\nFederated Learning (GC-Fed) employs a hyperplane as a historically independent\nreference point to guide local training and enhance inter-client alignment.\nGC-Fed comprises two complementary components: Local GC, which centralizes\ngradients during local training, and Global GC, which centralizes updates\nduring server aggregation. In our hybrid design, Local GC is applied to\nfeature-extraction layers to harmonize client contributions, while Global GC\nrefines classifier layers to stabilize round-wise performance. Theoretical\nanalysis and extensive experiments on benchmark FL tasks demonstrate that\nGC-Fed effectively mitigates client drift and achieves up to a 20% improvement\nin accuracy under heterogeneous and partial participation conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13180v2",
    "published_date": "2025-03-17 13:54:27 UTC",
    "updated_date": "2025-03-20 08:41:33 UTC"
  },
  {
    "arxiv_id": "2503.13178v1",
    "title": "Rapfi: Distilling Efficient Neural Network for the Game of Gomoku",
    "authors": [
      "Zhanggen Jin",
      "Haobin Duan",
      "Zhiyang Hang"
    ],
    "abstract": "Games have played a pivotal role in advancing artificial intelligence, with\nAI agents using sophisticated techniques to compete. Despite the success of\nneural network based game AIs, their performance often requires significant\ncomputational resources. In this paper, we present Rapfi, an efficient Gomoku\nagent that outperforms CNN-based agents in limited computation environments.\nRapfi leverages a compact neural network with a pattern-based codebook\ndistilled from CNNs, and an incremental update scheme that minimizes\ncomputation when input changes are minor. This new network uses computation\nthat is orders of magnitude less to reach a similar accuracy of much larger\nneural networks such as Resnet. Thanks to our incremental update scheme,\ndepth-first search methods such as the alpha-beta search can be significantly\naccelerated. With a carefully tuned evaluation and search, Rapfi reached\nstrength surpassing Katagomo, the strongest open-source Gomoku AI based on\nAlphaZero's algorithm, under limited computational resources where accelerators\nlike GPUs are absent. Rapfi ranked first among 520 Gomoku agents on Botzone and\nwon the championship in GomoCup 2024.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13178v1",
    "published_date": "2025-03-17 13:53:57 UTC",
    "updated_date": "2025-03-17 13:53:57 UTC"
  },
  {
    "arxiv_id": "2503.13171v1",
    "title": "HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of Imitation Learning",
    "authors": [
      "Wensheng Wang",
      "Ning Tan"
    ],
    "abstract": "The acquisition of large-scale and diverse demonstration data are essential\nfor improving robotic imitation learning generalization. However, generating\nsuch data for complex manipulations is challenging in real-world settings. We\nintroduce HybridGen, an automated framework that integrates Vision-Language\nModel (VLM) and hybrid planning. HybridGen uses a two-stage pipeline: first,\nVLM to parse expert demonstrations, decomposing tasks into expert-dependent\n(object-centric pose transformations for precise control) and plannable\nsegments (synthesizing diverse trajectories via path planning); second, pose\ntransformations substantially expand the first-stage data. Crucially, HybridGen\ngenerates a large volume of training data without requiring specific data\nformats, making it broadly applicable to a wide range of imitation learning\nalgorithms, a characteristic which we also demonstrate empirically across\nmultiple algorithms. Evaluations across seven tasks and their variants\ndemonstrate that agents trained with HybridGen achieve substantial performance\nand generalization gains, averaging a 5% improvement over state-of-the-art\nmethods. Notably, in the most challenging task variants, HybridGen achieves\nsignificant improvement, reaching a 59.7% average success rate, significantly\noutperforming Mimicgen's 49.5%. These results demonstrating its effectiveness\nand practicality.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13171v1",
    "published_date": "2025-03-17 13:49:43 UTC",
    "updated_date": "2025-03-17 13:49:43 UTC"
  },
  {
    "arxiv_id": "2503.14536v2",
    "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis",
    "authors": [
      "Praveen Shastry",
      "Sowmya Chowdary Muthulur",
      "Naveen Kumarasami",
      "Anandakumar D",
      "Mounigasri M",
      "Keerthana R",
      "Kishore Prasath Venkatesh",
      "Bargava Subramanian",
      "Kalyan Sivasailam",
      "Revathi Ezhumalai",
      "Abitha Marimuthu"
    ],
    "abstract": "Background: This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods: The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results: The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion: The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07, 92C55, 68U10, 92C50, 60G35"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages , 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14536v2",
    "published_date": "2025-03-17 13:49:29 UTC",
    "updated_date": "2025-03-28 11:00:46 UTC"
  },
  {
    "arxiv_id": "2503.13169v1",
    "title": "Collaborative AI Enhances Image Understanding in Materials Science",
    "authors": [
      "Ruoyan Avery Yin",
      "Zhichu Ren",
      "Zongyou Yin",
      "Zhen Zhang",
      "So Yeon Kim",
      "Chia-Wei Hsu",
      "Ju Li"
    ],
    "abstract": "The Copilot for Real-world Experimental Scientist (CRESt) system empowers\nresearchers to control autonomous laboratories through conversational AI,\nproviding a seamless interface for managing complex experimental workflows. We\nhave enhanced CRESt by integrating a multi-agent collaboration mechanism that\nutilizes the complementary strengths of the ChatGPT and Gemini models for\nprecise image analysis in materials science. This innovative approach\nsignificantly improves the accuracy of experimental outcomes by fostering\nstructured debates between the AI models, which enhances decision-making\nprocesses in materials phase analysis. Additionally, to evaluate the\ngeneralizability of this approach, we tested it on a quantitative task of\ncounting particles. Here, the collaboration between the AI models also led to\nimproved results, demonstrating the versatility and robustness of this method.\nBy harnessing this dual-AI framework, this approach stands as a pioneering\nmethod for enhancing experimental accuracy and efficiency in materials\nresearch, with applications extending beyond CRESt to broader scientific\nexperimentation and analysis.",
    "categories": [
      "cs.AI",
      "I.2.1; I.2.10"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13169v1",
    "published_date": "2025-03-17 13:44:30 UTC",
    "updated_date": "2025-03-17 13:44:30 UTC"
  },
  {
    "arxiv_id": "2503.13575v1",
    "title": "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model",
    "authors": [
      "Kai Tong",
      "Kang Pan",
      "Xiao Zhang",
      "Erli Meng",
      "Run He",
      "Yawen Cui",
      "Nuoyan Guo",
      "Huiping Zhuang"
    ],
    "abstract": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13575v1",
    "published_date": "2025-03-17 13:40:46 UTC",
    "updated_date": "2025-03-17 13:40:46 UTC"
  },
  {
    "arxiv_id": "2503.13162v2",
    "title": "Efficient Imitation under Misspecification",
    "authors": [
      "Nicolas Espinosa-Dice",
      "Sanjiban Choudhury",
      "Wen Sun",
      "Gokul Swamy"
    ],
    "abstract": "We consider the problem of imitation learning under misspecification:\nsettings where the learner is fundamentally unable to replicate expert behavior\neverywhere. This is often true in practice due to differences in observation\nspace and action space expressiveness (e.g. perceptual or morphological\ndifferences between robots and humans). Given the learner must make some\nmistakes in the misspecified setting, interaction with the environment is\nfundamentally required to figure out which mistakes are particularly costly and\nlead to compounding errors. However, given the computational cost and safety\nconcerns inherent in interaction, we'd like to perform as little of it as\npossible while ensuring we've learned a strong policy. Accordingly, prior work\nhas proposed a flavor of efficient inverse reinforcement learning algorithms\nthat merely perform a computationally efficient local search procedure with\nstrong guarantees in the realizable setting. We first prove that under a novel\nstructural condition we term reward-agnostic policy completeness, these sorts\nof local-search based IRL algorithms are able to avoid compounding errors. We\nthen consider the question of where we should perform local search in the first\nplace, given the learner may not be able to \"walk on a tightrope\" as well as\nthe expert in the misspecified setting. We prove that in the misspecified\nsetting, it is beneficial to broaden the set of states on which local search is\nperformed to include those reachable by good policies the learner can actually\nplay. We then experimentally explore a variety of sources of misspecification\nand how offline data can be used to effectively broaden where we perform local\nsearch from.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 6 figures. Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13162v2",
    "published_date": "2025-03-17 13:35:55 UTC",
    "updated_date": "2025-04-02 16:32:52 UTC"
  },
  {
    "arxiv_id": "2503.13149v1",
    "title": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs",
    "authors": [
      "Jasmin Wachter",
      "Michael Radloff",
      "Maja Smolej",
      "Katharina Kinder-Kurlanda"
    ],
    "abstract": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13149v1",
    "published_date": "2025-03-17 13:20:09 UTC",
    "updated_date": "2025-03-17 13:20:09 UTC"
  },
  {
    "arxiv_id": "2503.13139v2",
    "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding",
    "authors": [
      "Weiyu Guo",
      "Ziyang Chen",
      "Shaoguang Wang",
      "Jianxiang He",
      "Yijie Xu",
      "Jinhui Ye",
      "Ying Sun",
      "Hui Xiong"
    ],
    "abstract": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to \"finding a needle in a haystack.\"\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2503.13139v2",
    "published_date": "2025-03-17 13:07:34 UTC",
    "updated_date": "2025-05-17 13:22:07 UTC"
  },
  {
    "arxiv_id": "2503.13123v1",
    "title": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
    "authors": [
      "Xintian Yuan",
      "Yunke Ao",
      "Boqi Chen",
      "Philipp Fuernstahl"
    ],
    "abstract": "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to the lEEE IROS 2025 for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2503.13123v1",
    "published_date": "2025-03-17 12:48:29 UTC",
    "updated_date": "2025-03-17 12:48:29 UTC"
  },
  {
    "arxiv_id": "2503.13115v1",
    "title": "Beyond Propagation of Chaos: A Stochastic Algorithm for Mean Field Optimization",
    "authors": [
      "Chandan Tankala",
      "Dheeraj M. Nagaraj",
      "Anant Raj"
    ],
    "abstract": "Gradient flow in the 2-Wasserstein space is widely used to optimize\nfunctionals over probability distributions and is typically implemented using\nan interacting particle system with $n$ particles. Analyzing these algorithms\nrequires showing (a) that the finite-particle system converges and/or (b) that\nthe resultant empirical distribution of the particles closely approximates the\noptimal distribution (i.e., propagation of chaos). However, establishing\nefficient sufficient conditions can be challenging, as the finite particle\nsystem may produce heavily dependent random variables.\n  In this work, we study the virtual particle stochastic approximation,\noriginally introduced for Stein Variational Gradient Descent. This method can\nbe viewed as a form of stochastic gradient descent in the Wasserstein space and\ncan be implemented efficiently. In popular settings, we demonstrate that our\nalgorithm's output converges to the optimal distribution under conditions\nsimilar to those for the infinite particle limit, and it produces i.i.d.\nsamples without the need to explicitly establish propagation of chaos bounds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13115v1",
    "published_date": "2025-03-17 12:37:53 UTC",
    "updated_date": "2025-03-17 12:37:53 UTC"
  },
  {
    "arxiv_id": "2503.13108v1",
    "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference",
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13108v1",
    "published_date": "2025-03-17 12:31:23 UTC",
    "updated_date": "2025-03-17 12:31:23 UTC"
  },
  {
    "arxiv_id": "2503.13107v1",
    "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models",
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ],
    "abstract": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13107v1",
    "published_date": "2025-03-17 12:30:40 UTC",
    "updated_date": "2025-03-17 12:30:40 UTC"
  },
  {
    "arxiv_id": "2503.14535v1",
    "title": "Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios",
    "authors": [
      "Huaqiu Li",
      "Xiaowan Hu",
      "Haoqian Wang"
    ],
    "abstract": "Real-world low-light images often suffer from complex degradations such as\nlocal overexposure, low brightness, noise, and uneven illumination. Supervised\nmethods tend to overfit to specific scenarios, while unsupervised methods,\nthough better at generalization, struggle to model these degradations due to\nthe lack of reference images. To address this issue, we propose an\ninterpretable, zero-reference joint denoising and low-light enhancement\nframework tailored for real-world scenarios. Our method derives a training\nstrategy based on paired sub-images with varying illumination and noise levels,\ngrounded in physical imaging principles and retinex theory. Additionally, we\nleverage the Discrete Cosine Transform (DCT) to perform frequency domain\ndecomposition in the sRGB space, and introduce an implicit-guided hybrid\nrepresentation strategy that effectively separates intricate compounded\ndegradations. In the backbone network design, we develop retinal decomposition\nnetwork guided by implicit degradation representation mechanisms. Extensive\nexperiments demonstrate the superiority of our method. Code will be available\nat https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14535v1",
    "published_date": "2025-03-17 12:08:52 UTC",
    "updated_date": "2025-03-17 12:08:52 UTC"
  },
  {
    "arxiv_id": "2503.13570v1",
    "title": "ExChanGeAI: An End-to-End Platform and Efficient Foundation Model for Electrocardiogram Analysis and Fine-tuning",
    "authors": [
      "Lucas Bickmann",
      "Lucas Plagwitz",
      "Antonius Büscher",
      "Lars Eckardt",
      "Julian Varghese"
    ],
    "abstract": "Electrocardiogram data, one of the most widely available biosignal data, has\nbecome increasingly valuable with the emergence of deep learning methods,\nproviding novel insights into cardiovascular diseases and broader health\nconditions. However, heterogeneity of electrocardiogram formats, limited access\nto deep learning model weights and intricate algorithmic steps for effective\nfine-tuning for own disease target labels result in complex workflows. In this\nwork, we introduce ExChanGeAI, a web-based end-to-end platform that streamlines\nthe reading of different formats, pre-processing, visualization and custom\nmachine learning with local and privacy-preserving fine-tuning. ExChanGeAI is\nadaptable for use on both personal computers and scalable to high performance\nserver environments. The platform offers state-of-the-art deep learning models\nfor training from scratch, alongside our novel open-source electrocardiogram\nfoundation model CardX, pre-trained on over one million electrocardiograms.\nEvaluation across three external validation sets, including an entirely new\ntestset extracted from routine care, demonstrate the fine-tuning capabilities\nof ExChanGeAI. CardX outperformed the benchmark foundation model while\nrequiring significantly fewer parameters and lower computational resources. The\nplatform enables users to empirically determine the most suitable model for\ntheir specific tasks based on systematic validations.The code is available at\nhttps://imigitlab.uni-muenster.de/published/exchangeai .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13570v1",
    "published_date": "2025-03-17 11:58:52 UTC",
    "updated_date": "2025-03-17 11:58:52 UTC"
  },
  {
    "arxiv_id": "2503.13089v1",
    "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning",
    "authors": [
      "Baohao Liao",
      "Christian Herold",
      "Seyyed Hadi Hashemi",
      "Stefan Vasilev",
      "Shahram Khadivi",
      "Christof Monz"
    ],
    "abstract": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 11 figures, 18 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.13089v1",
    "published_date": "2025-03-17 11:52:16 UTC",
    "updated_date": "2025-03-17 11:52:16 UTC"
  },
  {
    "arxiv_id": "2503.13082v1",
    "title": "Free-form language-based robotic reasoning and grasping",
    "authors": [
      "Runyu Jiao",
      "Alice Fasoli",
      "Francesco Giuliari",
      "Matteo Bortolon",
      "Sergio Povoli",
      "Guofeng Mei",
      "Yiming Wang",
      "Fabio Poiesi"
    ],
    "abstract": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://tev-fbk.github.io/FreeGrasp/",
    "pdf_url": "http://arxiv.org/pdf/2503.13082v1",
    "published_date": "2025-03-17 11:41:16 UTC",
    "updated_date": "2025-03-17 11:41:16 UTC"
  },
  {
    "arxiv_id": "2503.16523v1",
    "title": "Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis",
    "authors": [
      "Shi Yin Hong",
      "Uttamasha Oyshi",
      "Quan Mai",
      "Gibson Nkhata",
      "Susan Gauch"
    ],
    "abstract": "Emotional support (ES) systems alleviate users' mental distress by generating\nstrategic supportive dialogues based on diverse user situations. However, ES\nsystems are limited in their ability to generate effective ES dialogues that\ninclude timely context and interpretability, hindering them from earning public\ntrust. Driven by cognitive models, we propose Mind-to-Mind (Mind2), an ES\nframework that approaches interpretable ES context modeling for the ES dialogue\ngeneration task from a discourse analysis perspective. Specifically, we perform\ncognitive discourse analysis on ES dialogues according to our dynamic discourse\ncontext propagation window, which accommodates evolving context as the\nconversation between the ES system and user progresses. To enhance\ninterpretability, Mind2 prioritizes details that reflect each speaker's belief\nabout the other speaker with bidirectionality, integrating Theory-of-Mind,\nphysiological expected utility, and cognitive rationality to extract cognitive\nknowledge from ES conversations. Experimental results support that Mind2\nachieves competitive performance versus state-of-the-art ES systems while\ntrained with only 10\\% of the available training data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 2 figures, and 3 tables; WI-IAT 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.16523v1",
    "published_date": "2025-03-17 11:39:56 UTC",
    "updated_date": "2025-03-17 11:39:56 UTC"
  },
  {
    "arxiv_id": "2503.13081v1",
    "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
    "authors": [
      "Likai Tang",
      "Niruth Bogahawatta",
      "Yasod Ginige",
      "Jiarui Xu",
      "Shixuan Sun",
      "Surangika Ranathunga",
      "Suranga Seneviratne"
    ],
    "abstract": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13081v1",
    "published_date": "2025-03-17 11:39:44 UTC",
    "updated_date": "2025-03-17 11:39:44 UTC"
  },
  {
    "arxiv_id": "2503.13055v1",
    "title": "Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task Planning",
    "authors": [
      "Yu-Hong Shen",
      "Chuan-Yu Wu",
      "Yi-Ru Yang",
      "Yen-Ling Tai",
      "Yi-Ting Chen"
    ],
    "abstract": "We study Multimodal Large Language Models (MLLMs) with in-context learning\nfor food preparation task planning. In this context, we identify two key\nchallenges: cross-modal distraction and geometric feasibility. Cross-modal\ndistraction occurs when the inclusion of visual input degrades the reasoning\nperformance of a MLLM. Geometric feasibility refers to the ability of MLLMs to\nensure that the selected skills are physically executable in the environment.\nTo address these issues, we adapt Chain of Thought (CoT) with Self-Consistency\nto mitigate reasoning loss from cross-modal distractions and use affordance\npredictor as skill preconditions to guide MLLM on geometric feasibility. We\nconstruct a dataset to evaluate the ability of MLLMs on quantity estimation,\nreachability analysis, relative positioning and collision avoidance. We\nconducted a detailed evaluation to identify issues among different baselines\nand analyze the reasons for improvement, providing insights into each approach.\nOur method reaches a success rate of 76.7% on the entire dataset, showing a\nsubstantial improvement over the CoT baseline at 36.7%.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13055v1",
    "published_date": "2025-03-17 11:01:02 UTC",
    "updated_date": "2025-03-17 11:01:02 UTC"
  },
  {
    "arxiv_id": "2503.13568v1",
    "title": "WMINet: A Wheel-Mounted Inertial Learning Approach For Mobile-Robot Positioning",
    "authors": [
      "Gal Versano",
      "Itzik Klein"
    ],
    "abstract": "Autonomous mobile robots are widely used for navigation, transportation, and\ninspection tasks indoors and outdoors. In practical situations of limited\nsatellite signals or poor lighting conditions, navigation depends only on\ninertial sensors. In such cases, the navigation solution rapidly drifts due to\ninertial measurement errors. In this work, we propose WMINet a wheel-mounted\ninertial deep learning approach to estimate the mobile robot's position based\nonly on its inertial sensors. To that end, we merge two common practical\nmethods to reduce inertial drift: a wheel-mounted approach and driving the\nmobile robot in periodic trajectories. Additionally, we enforce a wheelbase\nconstraint to further improve positioning performance. To evaluate our proposed\napproach we recorded using the Rosbot-XL a wheel-mounted initial dataset\ntotaling 190 minutes, which is made publicly available. Our approach\ndemonstrated a 66\\% improvement over state-of-the-art approaches. As a\nconsequence, our approach enables navigation in challenging environments and\nbridges the pure inertial gap. This enables seamless robot navigation using\nonly inertial sensors for short periods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13568v1",
    "published_date": "2025-03-17 10:43:46 UTC",
    "updated_date": "2025-03-17 10:43:46 UTC"
  },
  {
    "arxiv_id": "2503.13025v1",
    "title": "PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data",
    "authors": [
      "ChangHee Yang",
      "Hyeonseop Song",
      "Seokhun Choi",
      "Seungwoo Lee",
      "Jaechul Kim",
      "Hoseok Do"
    ],
    "abstract": "Despite considerable efforts to enhance the generalization of 3D pose\nestimators without costly 3D annotations, existing data augmentation methods\nstruggle in real world scenarios with diverse human appearances and complex\nposes. We propose PoseSyn, a novel data synthesis framework that transforms\nabundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn\ncomprises two key components: Error Extraction Module (EEM), which identifies\nchallenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),\nwhich synthesizes motion sequences around the challenging poses. Then, by\ngenerating realistic 3D training data via a human animation model aligned with\nchallenging poses and appearances PoseSyn boosts the accuracy of various 3D\npose estimators by up to 14% across real world benchmarks including various\nbackgrounds and occlusions, challenging poses, and multi view scenarios.\nExtensive experiments further confirm that PoseSyn is a scalable and effective\napproach for improving generalization without relying on expensive 3D\nannotations, regardless of the pose estimator's model size or design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The first three authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2503.13025v1",
    "published_date": "2025-03-17 10:28:35 UTC",
    "updated_date": "2025-03-17 10:28:35 UTC"
  },
  {
    "arxiv_id": "2503.13012v1",
    "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation",
    "authors": [
      "Xingguo Lv",
      "Xingbo Dong",
      "Liwen Wang",
      "Jiewen Yang",
      "Lei Zhao",
      "Bin Pu",
      "Zhe Jin",
      "Xuejun Li"
    ],
    "abstract": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13012v1",
    "published_date": "2025-03-17 10:11:11 UTC",
    "updated_date": "2025-03-17 10:11:11 UTC"
  },
  {
    "arxiv_id": "2503.12999v2",
    "title": "Concept-as-Tree: Synthetic Data is All You Need for VLM Personalization",
    "authors": [
      "Ruichuan An",
      "Kai Zeng",
      "Ming Lu",
      "Sihan Yang",
      "Renrui Zhang",
      "Huitong Ji",
      "Qizhe Zhang",
      "Yulin Luo",
      "Hao Liang",
      "Wentao Zhang"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated exceptional performance in\nvarious multi-modal tasks. Recently, there has been an increasing interest in\nimproving the personalization capabilities of VLMs. To better integrate\nuser-provided concepts into VLMs, many methods use positive and negative\nsamples to fine-tune these models. However, the scarcity of user-provided\npositive samples and the low quality of retrieved negative samples pose\nchallenges for fine-tuning. To reveal the relationship between sample and model\nperformance, we systematically investigate the impact of positive and negative\nsamples (easy and hard) and their diversity on VLM personalization tasks. Based\non the detailed analysis, we introduce Concept-as-Tree (CaT), which represents\na concept as a tree structure, thereby enabling the data generation of positive\nand negative samples with varying difficulty and diversity for VLM\npersonalization. With a well-designed data filtering strategy, our CaT\nframework can ensure the quality of generated data, constituting a powerful\npipeline. We perform thorough experiments with various VLM personalization\nbaselines to assess the effectiveness of the pipeline, alleviating the lack of\npositive samples and the low quality of negative samples. Our results\ndemonstrate that CaT equipped with the proposed data filter significantly\nenhances the personalization capabilities of VLMs across the MyVLM, Yo'LLaVA,\nand MC-LLaVA datasets. To our knowledge, this work is the first controllable\nsynthetic data pipeline for VLM personalization. The code is released at\n$\\href{https://github.com/zengkaiya/CaT}{\\text{https://github.com/zengkaiya/CaT}}$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The code is released at\n  $\\href{https://github.com/zengkaiya/CaT}{\\text{https://github.com/zengkaiya/CaT}}$",
    "pdf_url": "http://arxiv.org/pdf/2503.12999v2",
    "published_date": "2025-03-17 09:55:01 UTC",
    "updated_date": "2025-03-23 06:45:43 UTC"
  },
  {
    "arxiv_id": "2503.12993v1",
    "title": "Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach",
    "authors": [
      "Muhan Hou",
      "Koen Hindriks",
      "A. E. Eiben",
      "Kim Baraka"
    ],
    "abstract": "Transfer Learning (TL) is a powerful tool that enables robots to transfer\nlearned policies across different environments, tasks, or embodiments. To\nfurther facilitate this process, efforts have been made to combine it with\nLearning from Demonstrations (LfD) for more flexible and efficient policy\ntransfer. However, these approaches are almost exclusively limited to offline\ndemonstrations collected before policy transfer starts, which may suffer from\nthe intrinsic issue of covariance shift brought by LfD and harm the performance\nof policy transfer. Meanwhile, extensive work in the learning-from-scratch\nsetting has shown that online demonstrations can effectively alleviate\ncovariance shift and lead to better policy performance with improved sample\nefficiency. This work combines these insights to introduce online\ndemonstrations into a policy transfer setting. We present Policy Transfer with\nOnline Demonstrations, an active LfD algorithm for policy transfer that can\noptimize the timing and content of queries for online episodic expert\ndemonstrations under a limited demonstration budget. We evaluate our method in\neight robotic scenarios, involving policy transfer across diverse environment\ncharacteristics, task objectives, and robotic embodiments, with the aim to\ntransfer a trained policy from a source task to a related but different target\ntask. The results show that our method significantly outperforms all baselines\nin terms of average success rate and sample efficiency, compared to two\ncanonical LfD methods with offline demonstrations and one active LfD method\nwith online demonstrations. Additionally, we conduct preliminary sim-to-real\ntests of the transferred policy on three transfer scenarios in the real-world\nenvironment, demonstrating the policy effectiveness on a real robot\nmanipulator.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12993v1",
    "published_date": "2025-03-17 09:47:42 UTC",
    "updated_date": "2025-03-17 09:47:42 UTC"
  },
  {
    "arxiv_id": "2503.12992v1",
    "title": "Intra-neuronal attention within language models Relationships between activation and semantics",
    "authors": [
      "Michael Pichat",
      "William Pogrund",
      "Paloma Pichat",
      "Armanouche Gasparian",
      "Samuel Demarchi",
      "Corbet Alois Georgeon",
      "Michael Veillet-Guillem"
    ],
    "abstract": "This study investigates the ability of perceptron-type neurons in language\nmodels to perform intra-neuronal attention; that is, to identify different\nhomogeneous categorical segments within the synthetic thought category they\nencode, based on a segmentation of specific activation zones for the tokens to\nwhich they are particularly responsive. The objective of this work is therefore\nto determine to what extent formal neurons can establish a homomorphic\nrelationship between activation-based and categorical segmentations. The\nresults suggest the existence of such a relationship, albeit tenuous, only at\nthe level of tokens with very high activation levels. This intra-neuronal\nattention subsequently enables categorical restructuring processes at the level\nof neurons in the following layer, thereby contributing to the progressive\nformation of high-level categorical abstractions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12992v1",
    "published_date": "2025-03-17 09:47:11 UTC",
    "updated_date": "2025-03-17 09:47:11 UTC"
  },
  {
    "arxiv_id": "2503.12989v1",
    "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models",
    "authors": [
      "Palakorn Achananuparp",
      "Ee-Peng Lim"
    ],
    "abstract": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12989v1",
    "published_date": "2025-03-17 09:44:50 UTC",
    "updated_date": "2025-03-17 09:44:50 UTC"
  },
  {
    "arxiv_id": "2503.12988v1",
    "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
    "authors": [
      "Wenqiang Wang",
      "Yijia Zhang",
      "Zikai Zhang",
      "Guanting Huo",
      "Hao Liang",
      "Shijie Cao",
      "Ningyi Xu"
    ],
    "abstract": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12988v1",
    "published_date": "2025-03-17 09:44:17 UTC",
    "updated_date": "2025-03-17 09:44:17 UTC"
  },
  {
    "arxiv_id": "2503.12972v1",
    "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning",
    "authors": [
      "Junming Liu",
      "Siyuan Meng",
      "Yanting Gao",
      "Song Mao",
      "Pinlong Cai",
      "Guohang Yan",
      "Yirong Chen",
      "Zilin Bian",
      "Botian Shi",
      "Ding Wang"
    ],
    "abstract": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.12972v1",
    "published_date": "2025-03-17 09:31:14 UTC",
    "updated_date": "2025-03-17 09:31:14 UTC"
  },
  {
    "arxiv_id": "2503.12964v1",
    "title": "Training Video Foundation Models with NVIDIA NeMo",
    "authors": [
      "Zeeshan Patel",
      "Ethan He",
      "Parth Mannan",
      "Xiaowei Ren",
      "Ryan Wolf",
      "Niket Agarwal",
      "Jacob Huffman",
      "Zhuoyao Wang",
      "Carl Wang",
      "Jack Chang",
      "Yan Bai",
      "Tommy Huang",
      "Linnan Wang",
      "Sahil Jain",
      "Shanmugam Ramasamy",
      "Joseph Jennings",
      "Ekaterina Sirazitdinova",
      "Oleg Sudakov",
      "Mingyuan Ma",
      "Bobby Chen",
      "Forrest Lin",
      "Hao Wang",
      "Vasanth Rao Naik Sabavat",
      "Sriharsha Niverty",
      "Rong Ou",
      "Pallab Bhattacharya",
      "David Page",
      "Nima Tajbakhsh",
      "Ashwath Aithal"
    ],
    "abstract": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12964v1",
    "published_date": "2025-03-17 09:19:12 UTC",
    "updated_date": "2025-03-17 09:19:12 UTC"
  },
  {
    "arxiv_id": "2503.12946v1",
    "title": "Open3DBench: Open-Source Benchmark for 3D-IC Backend Implementation and PPA Evaluation",
    "authors": [
      "Yunqi Shi",
      "Chengrui Gao",
      "Wanqi Ren",
      "Siyuan Xu",
      "Ke Xue",
      "Mingxuan Yuan",
      "Chao Qian",
      "Zhi-Hua Zhou"
    ],
    "abstract": "This work introduces Open3DBench, an open-source 3D-IC backend implementation\nbenchmark built upon the OpenROAD-flow-scripts framework, enabling\ncomprehensive evaluation of power, performance, area, and thermal metrics. Our\nproposed flow supports modular integration of 3D partitioning, placement, 3D\nrouting, RC extraction, and thermal simulation, aligning with advanced 3D flows\nthat rely on commercial tools and in-house scripts. We present two foundational\n3D placement algorithms: Open3D-Tiling, which emphasizes regular macro\nplacement, and Open3D-DMP, which enhances wirelength optimization through\ncross-die co-placement with analytical placer DREAMPlace. Experimental results\nshow significant improvements in area (51.19%), wirelength (24.06%), timing\n(30.84%), and power (5.72%) compared to 2D flows. The results also highlight\nthat better wirelength does not necessarily lead to PPA gain, emphasizing the\nneed of developing PPA-driven methods. Open3DBench offers a standardized,\nreproducible platform for evaluating 3D EDA methods, effectively bridging the\ngap between open-source tools and commercial solutions in 3D-IC design.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12946v1",
    "published_date": "2025-03-17 08:59:00 UTC",
    "updated_date": "2025-03-17 08:59:00 UTC"
  },
  {
    "arxiv_id": "2503.12937v1",
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization",
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Huanjin Yao",
      "Shunyu Liu",
      "Xikun Zhang",
      "Shijian Lu",
      "Dacheng Tao"
    ],
    "abstract": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12937v1",
    "published_date": "2025-03-17 08:51:44 UTC",
    "updated_date": "2025-03-17 08:51:44 UTC"
  },
  {
    "arxiv_id": "2503.12931v2",
    "title": "MirrorShield: Towards Universal Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Litian Zhang",
      "Lirong Qiu",
      "Xi Zhang"
    ],
    "abstract": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies typically rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules fail to accommodate the inherent complexity\nand dynamic nature of real-world jailbreak attacks. In this paper, we focus on\nthe novel challenge of universal defense against diverse jailbreaks. We propose\na new concept ``mirror'', which is a dynamically generated prompt that reflects\nthe syntactic structure of the input while ensuring semantic safety. The\ndiscrepancies between input prompts and their corresponding mirrors serve as\nguiding principles for defense. A novel defense model, MirrorShield, is further\nproposed to detect and calibrate risky inputs based on the crafted mirrors.\nEvaluated on multiple benchmark datasets and compared against ten\nstate-of-the-art attack methods, MirrorShield demonstrates superior defense\nperformance and promising generalization capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12931v2",
    "published_date": "2025-03-17 08:41:29 UTC",
    "updated_date": "2025-05-20 06:03:24 UTC"
  },
  {
    "arxiv_id": "2503.12927v2",
    "title": "MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification Assisted with Textual Description Generation",
    "authors": [
      "Huangwei Chen",
      "Yifei Chen",
      "Zhenyu Yan",
      "Mingyang Ding",
      "Chenlei Li",
      "Zhu Zhu",
      "Feiwei Qin"
    ],
    "abstract": "Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.12927v2",
    "published_date": "2025-03-17 08:38:46 UTC",
    "updated_date": "2025-03-19 09:27:16 UTC"
  },
  {
    "arxiv_id": "2503.13565v1",
    "title": "ML-SpecQD: Multi-Level Speculative Decoding with Quantized Drafts",
    "authors": [
      "Evangelos Georganas",
      "Dhiraj Kalamkar",
      "Alexander Kozlov",
      "Alexander Heinecke"
    ],
    "abstract": "Speculative decoding (SD) has emerged as a method to accelerate LLM inference\nwithout sacrificing any accuracy over the 16-bit model inference. In a typical\nSD setup, the idea is to use a full-precision, small, fast model as \"draft\" to\ngenerate the next few tokens and use the \"target\" large model to verify the\ndraft-generated tokens. The efficacy of this method heavily relies on the\nacceptance ratio of the draft-generated tokens and the relative token\nthroughput of the draft versus the target model. Nevertheless, an efficient SD\npipeline requires pre-training and aligning the draft model to the target\nmodel, making it impractical for LLM inference in a plug-and-play fashion. In\nthis work, we propose using MXFP4 models as drafts in a plug-and-play fashion\nsince the MXFP4 Weight-Only-Quantization (WOQ) merely direct-casts the BF16\ntarget model weights to MXFP4. In practice, our plug-and-play solution gives\nspeedups up to 2x over the BF16 baseline. Then we pursue an opportunity for\nfurther acceleration: the MXFP4 draft token generation itself can be\naccelerated via speculative decoding by using yet another smaller draft. We\ncall our method ML-SpecQD: Multi-Level Speculative Decoding with Quantized\nDrafts since it recursively applies speculation for accelerating the\ndraft-token generation. Combining Multi-Level Speculative Decoding with MXFP4\nQuantized Drafts we outperform state-of-the-art speculative decoding, yielding\nspeedups up to 2.72x over the BF16 baseline.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13565v1",
    "published_date": "2025-03-17 08:38:45 UTC",
    "updated_date": "2025-03-17 08:38:45 UTC"
  },
  {
    "arxiv_id": "2503.12917v1",
    "title": "Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible",
    "authors": [
      "Lin-Han Jia",
      "Wen-Chao Hu",
      "Jie-Jing Shao",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an\nover-reliance on labeled data. If we completely disregard labels, it leads to\nless symbol information, a larger solution space, and more shortcuts-issues\nthat current Nesy systems cannot resolve. This paper introduces a novel\nlearning paradigm, Verification Learning (VL), which addresses this challenge\nby transforming the label-based reasoning process in Nesy into a label-free\nverification process. VL achieves excellent learning results solely by relying\non unlabeled data and a function that verifies whether the current predictions\nconform to the rules. We formalize this problem as a Constraint Optimization\nProblem (COP) and propose a Dynamic combinatorial Sorting (DCS) algorithm that\naccelerates the solution by reducing verification attempts, effectively\nlowering computational costs to the level of a Constraint Satisfaction Problem\n(CSP). To further enhance performance, we introduce a prior alignment method to\naddress potential shortcuts. Our theoretical analysis points out which tasks in\nNesy systems can be completed without labels and explains why rules can replace\ninfinite labels, such as in addition, for some tasks, while for others, like\nSudoku, the rules have no effect. We validate the proposed framework through\nseveral fully unsupervised tasks including addition, sort, match, and chess,\neach showing significant performance and efficiency improvements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12917v1",
    "published_date": "2025-03-17 08:28:58 UTC",
    "updated_date": "2025-03-17 08:28:58 UTC"
  },
  {
    "arxiv_id": "2503.12908v3",
    "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models",
    "authors": [
      "Xinyan Jiang",
      "Hang Ye",
      "Yongxin Zhu",
      "Xiaoying Zheng",
      "Zikang Chen",
      "Jun Gong"
    ],
    "abstract": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL2025 findings",
    "pdf_url": "http://arxiv.org/pdf/2503.12908v3",
    "published_date": "2025-03-17 08:17:28 UTC",
    "updated_date": "2025-05-20 11:19:52 UTC"
  },
  {
    "arxiv_id": "2503.13563v1",
    "title": "MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG",
    "authors": [
      "Pingyu Wu",
      "Daiheng Gao",
      "Jing Tang",
      "Huimin Chen",
      "Wenbo Zhou",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https://github.com/wpydcr/MES-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13563v1",
    "published_date": "2025-03-17 08:09:42 UTC",
    "updated_date": "2025-03-17 08:09:42 UTC"
  },
  {
    "arxiv_id": "2503.12897v1",
    "title": "Federated Continual Instruction Tuning",
    "authors": [
      "Haiyang Guo",
      "Fanhu Zeng",
      "Fei Zhu",
      "Wenzhuo Liu",
      "Da-Han Wang",
      "Jian Xu",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ],
    "abstract": "A vast amount of instruction tuning data is crucial for the impressive\nperformance of Large Multimodal Models (LMMs), but the associated computational\ncosts and data collection demands during supervised fine-tuning make it\nimpractical for most researchers. Federated learning (FL) has the potential to\nleverage all distributed data and training resources to reduce the overhead of\njoint training. However, most existing methods assume a fixed number of tasks,\nwhile in real-world scenarios, clients continuously encounter new knowledge and\noften struggle to retain old tasks due to memory constraints. In this work, we\nintroduce the Federated Continual Instruction Tuning (FCIT) benchmark to model\nthis real-world challenge. Our benchmark includes two realistic scenarios,\nencompassing four different settings and twelve carefully curated instruction\ntuning datasets. To address the challenges posed by FCIT, we propose dynamic\nknowledge organization to effectively integrate updates from different tasks\nduring training and subspace selective activation to allocate task-specific\noutput during inference. Extensive experimental results demonstrate that our\nproposed method significantly enhances model performance across varying levels\nof data heterogeneity and catastrophic forgetting. Our source code and dataset\nwill be made publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.12897v1",
    "published_date": "2025-03-17 07:58:06 UTC",
    "updated_date": "2025-03-17 07:58:06 UTC"
  },
  {
    "arxiv_id": "2503.15548v1",
    "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
    "authors": [
      "Pengcheng Zhou",
      "Yinglun Feng",
      "Zhongliang Yang"
    ],
    "abstract": "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15548v1",
    "published_date": "2025-03-17 07:45:05 UTC",
    "updated_date": "2025-03-17 07:45:05 UTC"
  },
  {
    "arxiv_id": "2503.13562v1",
    "title": "Micro Text Classification Based on Balanced Positive-Unlabeled Learning",
    "authors": [
      "Lin-Han Jia",
      "Lan-Zhe Guo",
      "Zhi Zhou",
      "Si-Ye Han",
      "Zi-Wen Li",
      "Yu-Feng Li"
    ],
    "abstract": "In real-world text classification tasks, negative texts often contain a\nminimal proportion of negative content, which is especially problematic in\nareas like text quality control, legal risk screening, and sensitive\ninformation interception. This challenge manifests at two levels: at the macro\nlevel, distinguishing negative texts is difficult due to the high similarity\nbetween coarse-grained positive and negative samples; at the micro level, the\nissue stems from extreme class imbalance and a lack of fine-grained labels. To\naddress these challenges, we propose transforming the coarse-grained\npositive-negative (PN) classification task into an imbalanced fine-grained\npositive-unlabeled (PU) classification problem, supported by theoretical\nanalysis. We introduce a novel framework, Balanced Fine-Grained\nPositive-Unlabeled (BFGPU) learning, which features a unique PU learning loss\nfunction that optimizes macro-level performance amidst severe imbalance at the\nmicro level. The framework's performance is further boosted by rebalanced\npseudo-labeling and threshold adjustment. Extensive experiments on both public\nand real-world datasets demonstrate the effectiveness of BFGPU, which\noutperforms other methods, even in extreme scenarios where both macro and micro\nlevels are highly imbalanced.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13562v1",
    "published_date": "2025-03-17 07:42:27 UTC",
    "updated_date": "2025-03-17 07:42:27 UTC"
  },
  {
    "arxiv_id": "2503.12880v1",
    "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity",
    "authors": [
      "Tianqi Luo",
      "Chuhan Huang",
      "Leixian Shen",
      "Boyan Li",
      "Shuyu Shen",
      "Wei Zeng",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Natural Language to Visualization (NL2VIS) enables users to create\nvisualizations from natural language queries, making data insights more\naccessible. However, NL2VIS faces challenges in interpreting ambiguous queries,\nas users often express their visualization needs in imprecise language. To\naddress this challenge, we introduce nvBench 2.0, a new benchmark designed to\nevaluate NL2VIS systems in scenarios involving ambiguous queries. nvBench 2.0\nincludes 7,878 natural language queries and 24,076 corresponding\nvisualizations, derived from 780 tables across 153 domains. It is built using a\ncontrolled ambiguity-injection pipeline that generates ambiguous queries\nthrough a reverse-generation workflow. By starting with unambiguous seed\nvisualizations and selectively injecting ambiguities, the pipeline yields\nmultiple valid interpretations for each query, with each ambiguous query\ntraceable to its corresponding visualization through step-wise reasoning paths.\nWe evaluate various Large Language Models (LLMs) on their ability to perform\nambiguous NL2VIS tasks using nvBench 2.0. We also propose Step-NL2VIS, an\nLLM-based model trained on nvBench 2.0, which enhances performance in ambiguous\nscenarios through step-wise preference optimization. Our results show that\nStep-NL2VIS outperforms all baselines, setting a new state-of-the-art for\nambiguous NL2VIS tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12880v1",
    "published_date": "2025-03-17 07:20:11 UTC",
    "updated_date": "2025-03-17 07:20:11 UTC"
  },
  {
    "arxiv_id": "2503.12855v1",
    "title": "VITED: Video Temporal Evidence Distillation",
    "authors": [
      "Yujie Lu",
      "Yale Song",
      "William Wang",
      "Lorenzo Torresani",
      "Tushar Nagarajan"
    ],
    "abstract": "We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12855v1",
    "published_date": "2025-03-17 06:30:02 UTC",
    "updated_date": "2025-03-17 06:30:02 UTC"
  },
  {
    "arxiv_id": "2503.12843v3",
    "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data",
    "authors": [
      "Haozhe Si",
      "Yuxuan Wan",
      "Minh Do",
      "Deepak Vasisht",
      "Han Zhao",
      "Hendrik F. Hamann"
    ],
    "abstract": "Geospatial raster data, such as that collected by satellite-based imaging\nsystems at different times and spectral bands, hold immense potential for\nenabling a wide range of high-impact applications. This potential stems from\nthe rich information that is spatially and temporally contextualized across\nmultiple channels and sensing modalities. Recent work has adapted existing\nself-supervised learning approaches for such geospatial data. However, they\nfall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer with three key innovations: i) the LESS\nAttention Block that approximates high-dimensional spatial-spectral attention\nthrough Kronecker's product of the low-dimensional spatial and spectral\nattention components; ii) the Continuous Positional-Channel Embedding Layer\nthat preserves both the continuity and physical characteristics of each\nspatial-spectral patch; and iii) the Perception Field Mask that exploits local\nspatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct GFM-Bench, which serves as a\ncomprehensive benchmark for such geospatial raster data. We pretrain LESS ViT\nusing a Hyperspectral Masked Autoencoder framework with integrated positional\nand channel masking strategies. Experimental results demonstrate that our\nproposed method achieves competitive performance against state-of-the-art\nmulti-modal geospatial foundation models while outperforming them on\ncross-satellite generalization tasks with higher computational efficiency. The\nflexibility and extensibility of our framework make it a promising direction\nfor future geospatial data analysis tasks that involve a wide range of\nmodalities and channels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12843v3",
    "published_date": "2025-03-17 05:42:19 UTC",
    "updated_date": "2025-03-26 16:15:55 UTC"
  },
  {
    "arxiv_id": "2503.12836v3",
    "title": "CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting",
    "authors": [
      "Sumin In",
      "Youngdong Jang",
      "Utae Jeong",
      "MinHyuk Jang",
      "Hyeongcheol Park",
      "Eunbyung Park",
      "Sangpil Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.12836v3",
    "published_date": "2025-03-17 05:32:15 UTC",
    "updated_date": "2025-03-25 05:07:43 UTC"
  },
  {
    "arxiv_id": "2503.12834v1",
    "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior",
    "authors": [
      "Seunggwan Lee",
      "Hwanhee Jung",
      "Byoungsoo Koh",
      "Qixing Huang",
      "Sangho Yoon",
      "Sangpil Kim"
    ],
    "abstract": "A fundamental challenge in conditional 3D shape generation is to minimize the\ninformation loss and maximize the intention of user input. Existing approaches\nhave predominantly focused on two types of isolated conditional signals, i.e.,\nuser sketches and text descriptions, each of which does not offer flexible\ncontrol of the generated shape. In this paper, we introduce PASTA, the flexible\napproach that seamlessly integrates a user sketch and a text description for 3D\nshape generation. The key idea is to use text embeddings from a vision-language\nmodel to enrich the semantic representation of sketches. Specifically, these\ntext-derived priors specify the part components of the object, compensating for\nmissing visual cues from ambiguous sketches. In addition, we introduce ISG-Net\nwhich employs two types of graph convolutional networks: IndivGCN, which\nprocesses fine-grained details, and PartGCN, which aggregates these details\ninto parts and refines the structure of objects. Extensive experiments\ndemonstrate that PASTA outperforms existing methods in part-level editing and\nachieves state-of-the-art results in sketch-to-3D shape generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.12834v1",
    "published_date": "2025-03-17 05:31:09 UTC",
    "updated_date": "2025-03-17 05:31:09 UTC"
  },
  {
    "arxiv_id": "2503.15547v2",
    "title": "Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents",
    "authors": [
      "Juhee Kim",
      "Woohyuk Choi",
      "Byoungyoung Lee"
    ],
    "abstract": "Large Language Models (LLMs) are combined with tools to create powerful LLM\nagents that provide a wide range of services. Unlike traditional software, LLM\nagent's behavior is determined at runtime by natural language prompts from\neither user or tool's data. This flexibility enables a new computing paradigm\nwith unlimited capabilities and programmability, but also introduces new\nsecurity risks, vulnerable to privilege escalation attacks. Moreover, user\nprompts are prone to be interpreted in an insecure way by LLM agents, creating\nnon-deterministic behaviors that can be exploited by attackers. To address\nthese security risks, we propose Prompt Flow Integrity (PFI), a system\nsecurity-oriented solution to prevent privilege escalation in LLM agents.\nAnalyzing the architectural characteristics of LLM agents, PFI features three\nmitigation techniques -- i.e., agent isolation, secure untrusted data\nprocessing, and privilege escalation guardrails. Our evaluation result shows\nthat PFI effectively mitigates privilege escalation attacks while successfully\npreserving the utility of LLM agents.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15547v2",
    "published_date": "2025-03-17 05:27:57 UTC",
    "updated_date": "2025-04-21 02:10:14 UTC"
  },
  {
    "arxiv_id": "2503.12829v1",
    "title": "SparseLUT: Sparse Connectivity Optimization for Lookup Table-based Deep Neural Networks",
    "authors": [
      "Binglei Lou",
      "Ruilin Wu",
      "Philip Leong"
    ],
    "abstract": "The deployment of deep neural networks (DNNs) on resource-constrained edge\ndevices such as field-programmable gate arrays (FPGAs) requires a careful\nbalance of latency, power, and resource usage while maintaining high accuracy.\nExisting Lookup Table (LUT)-based DNNs, including LogicNets, PolyLUT,\nPolyLUT-Add, and NeuraLUT, exploit native FPGA resources with random sparse\nconnectivity. This paper introduces SparseLUT, a connectivity-centric training\ntechnique tailored for LUT-based DNNs. SparseLUT leverages a non-greedy\ntraining strategy that prioritizes the pruning of less significant connections\nand strategically regrows alternative ones, resulting in efficient convergence\nto the target sparsity. Experimental results show consistent accuracy\nimprovements across benchmarks, including up to a 2.13\\% increase on MNIST and\na 0.94\\% improvement for Jet Substructure Classification compared to random\nsparsity. This is done without any hardware overhead and achieves\nstate-of-the-art results for LUT-based DNNs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12829v1",
    "published_date": "2025-03-17 05:21:54 UTC",
    "updated_date": "2025-03-17 05:21:54 UTC"
  },
  {
    "arxiv_id": "2503.12821v2",
    "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration",
    "authors": [
      "Mingyang Song",
      "Xiaoye Qu",
      "Jiawei Zhou",
      "Yu Cheng"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an $\\textbf{A}$daptive\n$\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which\nconsists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$)\nand $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.12821v2",
    "published_date": "2025-03-17 05:01:09 UTC",
    "updated_date": "2025-03-18 06:02:39 UTC"
  },
  {
    "arxiv_id": "2503.12814v1",
    "title": "Versatile Physics-based Character Control with Hybrid Latent Representation",
    "authors": [
      "Jinseok Bae",
      "Jungdam Won",
      "Donggeun Lim",
      "Inwoo Hwang",
      "Young Min Kim"
    ],
    "abstract": "We present a versatile latent representation that enables physically\nsimulated character to efficiently utilize motion priors. To build a powerful\nmotion embedding that is shared across multiple tasks, the physics controller\nshould employ rich latent space that is easily explored and capable of\ngenerating high-quality motion. We propose integrating continuous and discrete\nlatent representations to build a versatile motion prior that can be adapted to\na wide range of challenging control tasks. Specifically, we build a discrete\nlatent model to capture distinctive posterior distribution without collapse,\nand simultaneously augment the sampled vector with the continuous residuals to\ngenerate high-quality, smooth motion without jittering. We further incorporate\nResidual Vector Quantization, which not only maximizes the capacity of the\ndiscrete motion prior, but also efficiently abstracts the action space during\nthe task learning phase. We demonstrate that our agent can produce diverse yet\nsmooth motions simply by traversing the learned motion prior through\nunconditional motion generation. Furthermore, our model robustly satisfies\nsparse goal conditions with highly expressive natural motions, including\nhead-mounted device tracking and motion in-betweening at irregular intervals,\nwhich could not be achieved with existing latent representations.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12814v1",
    "published_date": "2025-03-17 04:45:51 UTC",
    "updated_date": "2025-03-17 04:45:51 UTC"
  },
  {
    "arxiv_id": "2503.12811v1",
    "title": "A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules",
    "authors": [
      "Kairong Luo",
      "Haodong Wen",
      "Shengding Hu",
      "Zhenbo Sun",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Kaifeng Lyu",
      "Wenguang Chen"
    ],
    "abstract": "Training large models is both resource-intensive and time-consuming, making\nit crucial to understand the quantitative relationship between model\nperformance and hyperparameters. In this paper, we present an empirical law\nthat describes how the pretraining loss of large language models evolves under\ndifferent learning rate schedules, such as constant, cosine, and step decay\nschedules. Our proposed law takes a multi-power form, combining a power law\nbased on the sum of learning rates and additional power laws to account for a\nloss reduction effect induced by learning rate decay. We extensively validate\nthis law on various model sizes and architectures, and demonstrate that after\nfitting on a few learning rate schedules, the law accurately predicts the loss\ncurves for unseen schedules of different shapes and horizons. Moreover, by\nminimizing the predicted final pretraining loss across learning rate schedules,\nwe are able to find a schedule that outperforms the widely used cosine learning\nrate schedule. Interestingly, this automatically discovered schedule bears some\nresemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et\nal, 2024) but achieves a slightly lower final loss. We believe these results\ncould offer valuable insights for understanding the dynamics of pretraining and\ndesigning learning rate schedules to improve efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12811v1",
    "published_date": "2025-03-17 04:36:45 UTC",
    "updated_date": "2025-03-17 04:36:45 UTC"
  },
  {
    "arxiv_id": "2503.12797v2",
    "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding",
    "authors": [
      "Xinyu Ma",
      "Ziyang Ding",
      "Zhicong Luo",
      "Chi Chen",
      "Zonghao Guo",
      "Derek F. Wong",
      "Xiaoyi Feng",
      "Maosong Sun"
    ],
    "abstract": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12797v2",
    "published_date": "2025-03-17 04:06:34 UTC",
    "updated_date": "2025-03-18 05:06:22 UTC"
  },
  {
    "arxiv_id": "2503.12790v2",
    "title": "Quantum-Enhanced LLM Efficient Fine Tuning",
    "authors": [
      "Xiaofei Kong",
      "Lei Li",
      "Zhaoyun Chen",
      "Cheng Xue",
      "Xiaofan Xu",
      "Huanyu Liu",
      "Yuchun Wu",
      "Yuan Fang",
      "Han Fang",
      "Kejiang Chen",
      "Yang Yang",
      "Menghan Dou",
      "Guoping Guo"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) enables efficient fine-tuning of pre-trained\nlanguage models through low-rank matrix approximation, achieving effectiveness\nin many scenarios. However, its representation capacity is constrained in\ncomplex tasks or high-rank dependency settings, potentially limiting model\nadaptability. To overcome the expressive bottleneck in classical low-rank\napproximation for fine-tuning large language models (LLMs), we propose Quantum\nTensor Hybrid Adaptation (QTHA), a parameter-efficient fine-tuning method that\nintegrates a quantum neural network (QNN) with a tensor network. QTHA explores\nquantum tensor hybrid fine-tuning within low-rank spaces by decomposing\npre-trained weights into quantum neural network and tensor network\nrepresentations, leveraging quantum state superposition to overcome classical\nrank limitations. Experiments demonstrate that QTHA achieves performance\ncomparable to or surpassing LoRA in parameter-efficient fine-tuning. Compared\nto LoRA, QTHA reduces trainable parameters by 76% while reducing training loss\nby up to 17% and improving test set performance by up to 17% within the same\ntraining steps. This research not only enables lightweight adaptation of\nquantum resources to the billion-parameter models but also validates the\nfeasibility of quantum hardware optimization driven by LLM tasks. It\nestablishes the first engineering-ready foundation for future quantum-enhanced\nArtificial General Intelligence (AGI) systems.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12790v2",
    "published_date": "2025-03-17 03:59:26 UTC",
    "updated_date": "2025-04-27 10:23:02 UTC"
  },
  {
    "arxiv_id": "2503.12781v1",
    "title": "SAM2 for Image and Video Segmentation: A Comprehensive Survey",
    "authors": [
      "Zhang Jiaxing",
      "Tang Hao"
    ],
    "abstract": "Despite significant advances in deep learning for image and video\nsegmentation, existing models continue to face challenges in cross-domain\nadaptability and generalization. Image and video segmentation are fundamental\ntasks in computer vision with wide-ranging applications in healthcare,\nagriculture, industrial inspection, and autonomous driving. With the advent of\nlarge-scale foundation models, SAM2 - an improved version of SAM (Segment\nAnything Model)has been optimized for segmentation tasks, demonstrating\nenhanced performance in complex scenarios. However, SAM2's adaptability and\nlimitations in specific domains require further investigation. This paper\nsystematically analyzes the application of SAM2 in image and video segmentation\nand evaluates its performance in various fields. We begin by introducing the\nfoundational concepts of image segmentation, categorizing foundation models,\nand exploring the technical characteristics of SAM and SAM2. Subsequently, we\ndelve into SAM2's applications in static image and video segmentation,\nemphasizing its performance in specialized areas such as medical imaging and\nthe challenges of cross-domain adaptability. As part of our research, we\nreviewed over 200 related papers to provide a comprehensive analysis of the\ntopic. Finally, the paper highlights the strengths and weaknesses of SAM2 in\nsegmentation tasks, identifies the technical challenges it faces, and proposes\nfuture development directions. This review provides valuable insights and\npractical recommendations for optimizing and applying SAM2 in real-world\nscenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 4 figures, 7 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.12781v1",
    "published_date": "2025-03-17 03:33:36 UTC",
    "updated_date": "2025-03-17 03:33:36 UTC"
  },
  {
    "arxiv_id": "2503.12780v1",
    "title": "LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation",
    "authors": [
      "Chang Liu",
      "Bavesh Balaji",
      "Saad Hossain",
      "C Thomas",
      "Kwei-Herng Lai",
      "Raviteja Vemulapalli",
      "Alexander Wong",
      "Sirisha Rambhatla"
    ],
    "abstract": "Unsupervised domain adaptation for semantic segmentation (DASS) aims to\ntransfer knowledge from a label-rich source domain to a target domain with no\nlabels. Two key approaches in DASS are (1) vision-only approaches using masking\nor multi-resolution crops, and (2) language-based approaches that use generic\nclass-wise prompts informed by target domain (e.g. \"a {snowy} photo of a\n{class}\"). However, the former is susceptible to noisy pseudo-labels that are\nbiased to the source domain. The latter does not fully capture the intricate\nspatial relationships of objects -- key for dense prediction tasks. To this\nend, we propose LangDA. LangDA addresses these challenges by, first, learning\ncontextual relationships between objects via VLM-generated scene descriptions\n(e.g. \"a pedestrian is on the sidewalk, and the street is lined with\nbuildings.\"). Second, LangDA aligns the entire image features with text\nrepresentation of this context-aware scene caption and learns generalized\nrepresentations via text. With this, LangDA sets the new state-of-the-art\nacross three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and\n3.9%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "stat.ML",
      "68Txx",
      "I.2.1"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12780v1",
    "published_date": "2025-03-17 03:33:28 UTC",
    "updated_date": "2025-03-17 03:33:28 UTC"
  },
  {
    "arxiv_id": "2503.12778v1",
    "title": "Adaptive Deep Learning for Multiclass Breast Cancer Classification via Misprediction Risk Analysis",
    "authors": [
      "Gul Sheeraz",
      "Qun Chen",
      "Liu Feiyu",
      "Zhou Fengjin MD"
    ],
    "abstract": "Breast cancer remains one of the leading causes of cancer-related deaths\nworldwide. Early detection is crucial for improving patient outcomes, yet the\ndiagnostic process is often complex and prone to inconsistencies among\npathologists. Computer-aided diagnostic approaches have significantly enhanced\nbreast cancer detection, particularly in binary classification (benign vs.\nmalignant). However, these methods face challenges in multiclass\nclassification, leading to frequent mispredictions. In this work, we propose a\nnovel adaptive learning approach for multiclass breast cancer classification\nusing H&E-stained histopathology images. First, we introduce a misprediction\nrisk analysis framework that quantifies and ranks the likelihood of an image\nbeing mislabeled by a classifier. This framework leverages an interpretable\nrisk model that requires only a small number of labeled samples for training.\nNext, we present an adaptive learning strategy that fine-tunes classifiers\nbased on the specific characteristics of a given dataset. This approach\nminimizes misprediction risk, allowing the classifier to adapt effectively to\nthe target workload. We evaluate our proposed solutions on real benchmark\ndatasets, demonstrating that our risk analysis framework more accurately\nidentifies mispredictions compared to existing methods. Furthermore, our\nadaptive learning approach significantly improves the performance of\nstate-of-the-art deep neural network classifiers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12778v1",
    "published_date": "2025-03-17 03:25:28 UTC",
    "updated_date": "2025-03-17 03:25:28 UTC"
  },
  {
    "arxiv_id": "2503.12772v1",
    "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models",
    "authors": [
      "Sung-Yeon Park",
      "Can Cui",
      "Yunsheng Ma",
      "Ahmadreza Moradipari",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ziran Wang"
    ],
    "abstract": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12772v1",
    "published_date": "2025-03-17 03:12:39 UTC",
    "updated_date": "2025-03-17 03:12:39 UTC"
  },
  {
    "arxiv_id": "2503.12761v1",
    "title": "Analyzing sequential activity and travel decisions with interpretable deep inverse reinforcement learning",
    "authors": [
      "Yuebing Liang",
      "Shenhao Wang",
      "Jiangbo Yu",
      "Zhan Zhao",
      "Jinhua Zhao",
      "Sandy Pentland"
    ],
    "abstract": "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12761v1",
    "published_date": "2025-03-17 02:54:02 UTC",
    "updated_date": "2025-03-17 02:54:02 UTC"
  },
  {
    "arxiv_id": "2503.12757v2",
    "title": "MAP: Multi-user Personalization with Collaborative LLM-powered Agents",
    "authors": [
      "Christine Lee",
      "Jihye Choi",
      "Bilge Mutlu"
    ],
    "abstract": "The widespread adoption of Large Language Models (LLMs) and LLM-powered\nagents in multi-user settings underscores the need for reliable, usable methods\nto accommodate diverse preferences and resolve conflicting directives. Drawing\non conflict resolution theory, we introduce a user-centered workflow for\nmulti-user personalization comprising three stages: Reflection, Analysis, and\nFeedback. We then present MAP -- a \\textbf{M}ulti-\\textbf{A}gent system for\nmulti-user \\textbf{P}ersonalization -- to operationalize this workflow. By\ndelegating subtasks to specialized agents, MAP (1) retrieves and reflects on\nrelevant user information, while enhancing reliability through agent-to-agent\ninteractions, (2) provides detailed analysis for improved transparency and\nusability, and (3) integrates user feedback to iteratively refine results. Our\nuser study findings (n=12) highlight MAP's effectiveness and usability for\nconflict resolution while emphasizing the importance of user involvement in\nresolution verification and failure management. This work highlights the\npotential of multi-agent systems to implement user-centered, multi-user\npersonalization workflows and concludes by offering insights for\npersonalization in multi-user contexts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO",
      "I.2.7; I.2.9; I.2.1"
    ],
    "primary_category": "cs.HC",
    "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan",
    "pdf_url": "http://arxiv.org/pdf/2503.12757v2",
    "published_date": "2025-03-17 02:52:10 UTC",
    "updated_date": "2025-03-18 19:15:44 UTC"
  },
  {
    "arxiv_id": "2503.13558v6",
    "title": "Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life",
    "authors": [
      "Jingyuan Xue",
      "Longfei Wei",
      "Dongjing Jiang",
      "Fang Sheng",
      "Russell Greiner",
      "Jianfei Zhang"
    ],
    "abstract": "Battery degradation significantly impacts the reliability and efficiency of\nenergy storage systems, particularly in electric vehicles and industrial\napplications. Predicting the remaining useful life (RUL) of lithium-ion\nbatteries is crucial for optimizing maintenance schedules, reducing costs, and\nimproving safety. Traditional RUL prediction methods often struggle with\nnonlinear degradation patterns and uncertainty quantification. To address these\nchallenges, we propose a hybrid survival analysis framework integrating\nsurvival data reconstruction, survival model learning, and survival probability\nestimation. Our approach transforms battery voltage time series into\ntime-to-failure data using path signatures. The multiple Cox-based survival\nmodels and machine-learning-based methods, such as DeepHit and MTLR, are\nlearned to predict battery failure-free probabilities over time. Experiments\nconducted on the Toyota battery and NASA battery datasets demonstrate the\neffectiveness of our approach, achieving high time-dependent AUC and\nconcordance index (C-Index) while maintaining a low integrated Brier score. The\ndata and source codes for this work are available to the public at\nhttps://github.com/thinkxca/rul.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13558v6",
    "published_date": "2025-03-17 02:49:34 UTC",
    "updated_date": "2025-05-06 15:28:57 UTC"
  },
  {
    "arxiv_id": "2503.12753v1",
    "title": "SafeSlice: Enabling SLA-Compliant O-RAN Slicing via Safe Deep Reinforcement Learning",
    "authors": [
      "Ahmad M. Nagib",
      "Hatem Abou-Zeid",
      "Hossam S. Hassanein"
    ],
    "abstract": "Deep reinforcement learning (DRL)-based slicing policies have shown\nsignificant success in simulated environments but face challenges in physical\nsystems such as open radio access networks (O-RANs) due to\nsimulation-to-reality gaps. These policies often lack safety guarantees to\nensure compliance with service level agreements (SLAs), such as the strict\nlatency requirements of immersive applications. As a result, a deployed DRL\nslicing agent may make resource allocation (RA) decisions that degrade system\nperformance, particularly in previously unseen scenarios. Real-world immersive\napplications require maintaining SLA constraints throughout deployment to\nprevent risky DRL exploration. In this paper, we propose SafeSlice to address\nboth the cumulative (trajectory-wise) and instantaneous (state-wise) latency\nconstraints of O-RAN slices. We incorporate the cumulative constraints by\ndesigning a sigmoid-based risk-sensitive reward function that reflects the\nslices' latency requirements. Moreover, we build a supervised learning cost\nmodel as part of a safety layer that projects the slicing agent's RA actions to\nthe nearest safe actions, fulfilling instantaneous constraints. We conduct an\nexhaustive experiment that supports multiple services, including real virtual\nreality (VR) gaming traffic, to investigate the performance of SafeSlice under\nextreme and changing deployment conditions. SafeSlice achieves reductions of up\nto 83.23% in average cumulative latency, 93.24% in instantaneous latency\nviolations, and 22.13% in resource consumption compared to the baselines. The\nresults also indicate SafeSlice's robustness to changing the threshold\nconfigurations of latency constraints, a vital deployment scenario that will be\nrealized by the O-RAN paradigm to empower mobile network operators (MNOs).",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "This article has been accepted for presentation in the IEEE\n  International Conference on Machine Learning for Communication and Networking\n  (ICMLCN) 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.12753v1",
    "published_date": "2025-03-17 02:41:49 UTC",
    "updated_date": "2025-03-17 02:41:49 UTC"
  },
  {
    "arxiv_id": "2503.12739v1",
    "title": "TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning of Sentence Embeddings",
    "authors": [
      "Tianyu Zong",
      "Bingkang Shi",
      "Hongzhu Yi",
      "Jungang Xu"
    ],
    "abstract": "Unsupervised sentence embedding representation has become a hot research\ntopic in natural language processing. As a tensor, sentence embedding has two\ncritical properties: direction and norm. Existing works have been limited to\nconstraining only the orientation of the samples' representations while\nignoring the features of their module lengths. To address this issue, we\npropose a new training objective that optimizes the training of unsupervised\ncontrastive learning by constraining the module length features between\npositive samples. We combine the training objective of Tensor's Norm\nConstraints with ensemble learning to propose a new Sentence Embedding\nrepresentation framework, TNCSE. We evaluate seven semantic text similarity\ntasks, and the results show that TNCSE and derived models are the current\nstate-of-the-art approach; in addition, we conduct extensive zero-shot\nevaluations, and the results show that TNCSE outperforms other baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.12739v1",
    "published_date": "2025-03-17 02:14:42 UTC",
    "updated_date": "2025-03-17 02:14:42 UTC"
  },
  {
    "arxiv_id": "2503.21790v1",
    "title": "March Madness Tournament Predictions Model: A Mathematical Modeling Approach",
    "authors": [
      "Christian McIver",
      "Karla Avalos",
      "Nikhil Nayak"
    ],
    "abstract": "This paper proposes a model to predict the outcome of the March Madness\ntournament based on historical NCAA basketball data since 2013. The framework\nof this project is a simplification of the FiveThrityEight NCAA March Madness\nprediction model, where the only four predictors of interest are Adjusted\nOffensive Efficiency (ADJOE), Adjusted Defensive Efficiency (ADJDE), Power\nRating, and Two-Point Shooting Percentage Allowed. A logistic regression was\nutilized with the aforementioned metrics to generate a probability of a\nparticular team winning each game. Then, a tournament simulation is developed\nand compared to real-world March Madness brackets to determine the accuracy of\nthe model. Accuracies of performance were calculated using a naive approach and\na Spearman rank correlation coefficient.",
    "categories": [
      "stat.AP",
      "cs.AI",
      "cs.LG",
      "68T01",
      "I.2.0; G.3"
    ],
    "primary_category": "stat.AP",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.21790v1",
    "published_date": "2025-03-17 02:04:31 UTC",
    "updated_date": "2025-03-17 02:04:31 UTC"
  },
  {
    "arxiv_id": "2503.13557v1",
    "title": "APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games",
    "authors": [
      "Yifei Chen",
      "Lambert Schomaker"
    ],
    "abstract": "Studies in reward shaping for reinforcement learning (RL) have flourished in\nrecent years due to its ability to speed up training. Our previous work\nproposed an adaptive potential function (APF) and showed that APF can\naccelerate the Q-learning with a Multi-layer Perceptron algorithm in the\nlow-dimensional domain. This paper proposes to extend APF with an encoder\n(APF+) for RL state representation, allowing applying APF to the pixel-based\nAtari games using a state-encoding method that projects high-dimensional game's\npixel frames to low-dimensional embeddings. We approach by designing the\nstate-representation encoder as a W-shaped network (W-Net), by using which we\nare able to encode both the background as well as the moving entities in the\ngame frames. Specifically, the embeddings derived from the pre-trained W-Net\nconsist of two latent vectors: One represents the input state, and the other\nrepresents the deviation of the input state's representation from itself. We\nthen incorporate W-Net into APF to train a downstream Dueling Deep Q-Network\n(DDQN), obtain the APF-WNet-DDQN, and demonstrate its effectiveness in Atari\ngame-playing tasks. To evaluate the APF+W-Net module in such high-dimensional\ntasks, we compare with two types of baseline methods: (i) the basic DDQN; and\n(ii) two encoder-replaced APF-DDQN methods where we replace W-Net by (a) an\nunsupervised state representation method called Spatiotemporal Deep Infomax\n(ST-DIM) and (b) a ground truth state representation provided by the Atari\nAnnotated RAM Interface (ARI). The experiment results show that out of 20 Atari\ngames, APF-WNet-DDQN outperforms DDQN (14/20 games) and APF-STDIM-DDQN (13/20\ngames) significantly. In comparison against the APF-ARI-DDQN which employs\nembeddings directly of the detailed game-internal state information, the\nAPF-WNet-DDQN achieves a comparable performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13557v1",
    "published_date": "2025-03-17 01:53:26 UTC",
    "updated_date": "2025-03-17 01:53:26 UTC"
  },
  {
    "arxiv_id": "2503.12730v1",
    "title": "TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research",
    "authors": [
      "Philip Quirke",
      "Clement Neo",
      "Abir Harrasse",
      "Dhruv Nathawani",
      "Amir Abdullah"
    ],
    "abstract": "Mechanistic interpretability research faces a gap between analyzing simple\ncircuits in toy tasks and discovering features in large models. To bridge this\ngap, we propose text-to-SQL generation as an ideal task to study, as it\ncombines the formal structure of toy tasks with real-world complexity. We\nintroduce TinySQL, a synthetic dataset progressing from basic to advanced SQL\noperations, and train models ranging from 33M to 1B parameters to establish a\ncomprehensive testbed for interpretability. We apply multiple complementary\ninterpretability techniques, including edge attribution patching and sparse\nautoencoders, to identify minimal circuits and components supporting SQL\ngeneration. Our analysis reveals both the potential and limitations of current\ninterpretability methods, showing how circuits can vary even across similar\nqueries. Lastly, we demonstrate how mechanistic interpretability can identify\nflawed heuristics in models and improve synthetic dataset design. Our work\nprovides a comprehensive framework for evaluating and advancing\ninterpretability techniques while establishing clear boundaries for their\nreliable application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 19 figures, 7 tables, 18 trained models",
    "pdf_url": "http://arxiv.org/pdf/2503.12730v1",
    "published_date": "2025-03-17 01:47:50 UTC",
    "updated_date": "2025-03-17 01:47:50 UTC"
  },
  {
    "arxiv_id": "2503.16520v1",
    "title": "Not All Personas Are Worth It: Culture-Reflective Persona Data Augmentation",
    "authors": [
      "Ji-Eun Han",
      "Yoonseok Heo"
    ],
    "abstract": "Incorporating personas into conversational AI models is crucial for achieving\nauthentic and engaging interactions. However, the cultural diversity and\nadaptability of existing persona datasets is often overlooked, reducing their\nefficacy in building culturally aware AI systems. To address this issue, we\npropose a two-step pipeline for generating culture-specific personas and\nintroduce KoPersona, a dataset comprising 200,000 personas designed to capture\nKorean cultural values, behaviors, and social nuances. A comprehensive\nevaluation through various metrics validates the quality of KoPersona and its\nrelevance to Korean culture. This work not only contributes to persona-based\nresearch, but also establishes a scalable approach for creating culturally\nrelevant personas adaptable to various languages and cultural contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16520v1",
    "published_date": "2025-03-17 01:23:57 UTC",
    "updated_date": "2025-03-17 01:23:57 UTC"
  },
  {
    "arxiv_id": "2503.12722v1",
    "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia Pérez-Campanero Antolín"
    ],
    "abstract": "As Large Language Models (LLMs) gain autonomous capabilities, their\ncoordination in multi-agent settings becomes increasingly important. However,\nthey often struggle with cooperation, leading to suboptimal outcomes. Inspired\nby Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how\npersonality traits influence LLM cooperation. Using representation engineering,\nwe steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and\nanalyze their impact on IPD decision-making. Our results show that higher\nAgreeableness and Conscientiousness improve cooperation but increase\nsusceptibility to exploitation, highlighting both the potential and limitations\nof personality-based steering for aligning AI agents.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Poster, Technical AI Safety Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.12722v1",
    "published_date": "2025-03-17 01:21:54 UTC",
    "updated_date": "2025-03-17 01:21:54 UTC"
  },
  {
    "arxiv_id": "2503.12721v2",
    "title": "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
    "authors": [
      "Luca Collini",
      "Andrew Hennessee",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "abstract": "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, submitted for peer review",
    "pdf_url": "http://arxiv.org/pdf/2503.12721v2",
    "published_date": "2025-03-17 01:21:39 UTC",
    "updated_date": "2025-04-14 00:39:57 UTC"
  },
  {
    "arxiv_id": "2503.15546v1",
    "title": "Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for Online Transactions",
    "authors": [
      "Shraddha Pradipbhai Shah",
      "Aditya Vilas Deshpande"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into autonomous robotic\nagents for conducting online transactions poses significant cybersecurity\nchallenges. This study aims to enforce robust cybersecurity constraints to\nmitigate the risks associated with data breaches, transaction fraud, and system\nmanipulation. The background focuses on the rise of LLM-driven robotic systems\nin e-commerce, finance, and service industries, alongside the vulnerabilities\nthey introduce. A novel security architecture combining blockchain technology\nwith multi-factor authentication (MFA) and real-time anomaly detection was\nimplemented to safeguard transactions. Key performance metrics such as\ntransaction integrity, response time, and breach detection accuracy were\nevaluated, showing improved security and system performance. The results\nhighlight that the proposed architecture reduced fraudulent transactions by\n90%, improved breach detection accuracy to 98%, and ensured secure transaction\nvalidation within a latency of 0.05 seconds. These findings emphasize the\nimportance of cybersecurity in the deployment of LLM-driven robotic systems and\nsuggest a framework adaptable to various online platforms.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15546v1",
    "published_date": "2025-03-17 01:01:10 UTC",
    "updated_date": "2025-03-17 01:01:10 UTC"
  },
  {
    "arxiv_id": "2503.13556v1",
    "title": "Pareidolic Illusions of Meaning: ChatGPT, Pseudolaw and the Triumph of Form over Substance",
    "authors": [
      "Joe McIntyre"
    ],
    "abstract": "The early 2020s has seen the rise of two strange and potentially quite\nimpactful social phenomena, namely pseudolaw, where users rely upon pseudolegal\narguments that mimic the form and ritual of legal argumentation but\nfundamentally distort the content of law, and generative AI/LLMs, which\ngenerate content that uses probabilistic calculations to create outputs that\nlook like human generated text. This article argues that the juxtaposition of\nthe two phenomena helps to reveal that they both share two fundamental traits\nas both elevate form and appearance over substance and content, and users of\nboth routinely mistake the form for the substance. In drawing upon legal\ntheory, computer science, linguistics and cognitive psychology, the article\nargues that both phenomena rely upon creating illusions of meaning that users\nmistake for the underlying primary phenomenon. I then explore four implications\nof this conception of both phenomena. Firstly, both rely on human tendencies of\nconceptual pareidolia resulting in the erroneous perception of meaningful\nlinguistic legal patterns from nebulous inputs. Secondly, both rely upon the\nconfidence heuristic, the human cognitive bias for treating confidence as a\nproxy for competence. Thirdly, both succeed when the primary concern is with\nthe form of the output and not its content. Fourthly, both rely heavily upon\nthe magical thinking of users and the desire for the promise of the approach to\nbe real. The article argues that the legal context helps to reveal a solution\nfor the problems caused by both phenomena as it is only where users possess\nsufficient legal and technological literacy that it becomes possible to reveal\nto them the illusionary nature of the phenomena.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "54 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13556v1",
    "published_date": "2025-03-17 00:15:41 UTC",
    "updated_date": "2025-03-17 00:15:41 UTC"
  }
]
{
  "date": "2025-03-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-23 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦 AI 模型优化、医疗应用和高效计算等领域，重点讨论大型语言模型（LLM）的强化学习、对齐机制和多模态处理，令人印象深刻的包括 Sergey Levine 的视频训练 RL 方法，以及医疗 AI 如 PG-SAM 的多器官分割创新；知名学者如 Sinan Aral 的团队合作实验也值得关注。\n\n下面，我将挑选并简要讨论部分关键论文，先聊那些重要、话题度高或有创新贡献的，其余快速掠过。相关论文按主题归类，以控制篇幅。\n\n### AI 模型优化与安全\n- **ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices（ShED-HD: 一种基于 Shannon 熵分布框架的轻量级幻觉检测系统）**  \n  这篇论文提出 ShED-HD 框架，使用轻量 BiLSTM 和注意力机制检测 LLM 的幻觉问题，在 BioASQ 等数据集上表现出色，显著提高了边缘设备上的检测准确性和泛化能力，适用于资源受限环境。\n\n- **Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance（与 AI 代理合作：团队合作、生产力和性能的现场实验）**  \n  作者包括知名学者 Sinan Aral，该研究使用 MindMeld 平台实验发现，人-AI 团队提升了生产力（60% 提高），但在多模态任务如图像生成上需进一步优化；强调 AI 个性与人类特质的互补。\n\n- **HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment（HAIR: 用于 LLM 对齐的难易感知逆强化学习与内省推理）**  \n  这篇创新工作提出 HAIR 框架，通过强化学习和内省推理减少 LLM 的负面转移，实验显示在无害性和有用性基准上超越基线，显著改善模型对齐。\n\n- **ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data（ViVa: 用于指导在线 RL 的视频训练价值函数）**  \n  Sergey Levine 参与的论文，引入 ViVa 方法从多样视频数据学习价值函数，指导在线强化学习，实验证明它在目标导向任务中减少方差并提升泛化。\n\n- **Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning（Reason2Attack: 通过 LLM 推理越狱文本到图像模型）**  \n  论文开发了 Reason2Attack 框架，使用 LLM 推理生成对抗提示，成功绕过图像生成模型的安全过滤，揭示了 AI 安全漏洞。\n\n- **Other AI papers** like ExpertRAG（混合专家增强检索生成）和 FedSKD（联邦学习中的知识蒸馏）快速提一下，它们优化了 LLM 的检索和联邦训练，但细节较常规，贡献在于效率提升。\n\n### 医疗与生物应用\n- **PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation（PG-SAM: 医疗先验引导的 SAM 用于多器官分割）**  \n  这篇亮点论文提出 PG-SAM 框架，使用医疗 LLM 和细粒度模态对齐提升分割准确性，在 Synapse 数据集上达到 state-of-the-art 性能，适用于无提示学习。\n\n- **Enhanced prediction of spine surgery outcomes using advanced machine learning techniques and oversampling methods（使用高级机器学习技术和过采样方法增强脊柱手术结果预测）**  \n  研究优化了 KNN 等模型，通过过采样和网格搜索在 244 例患者数据上达到 76% 准确率，辅助医疗决策。\n\n- **A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives（神经符号 AI 的研究：医疗视角）**  \n  这篇综述分析了 977 篇文献，聚焦神经符号 AI 在药物发现和蛋白工程中的应用，强调了其在医疗中的推理和可解释性。\n\n其他医疗论文如 Adaptive Multi-Fidelity Reinforcement Learning（用于工程优化的多保真 RL）和 SNRAware（MRI 去噪）等，贡献在于算法改进，但影响力较小，就不展开。\n\n### 其他领域快速掠过\n- **DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via Personalizer-Guided Distillation（DiffusionTalker: 通过个性化引导蒸馏的 efficient 语音驱动 3D 对话头）**  \n  提出高效 3D 面部动画方法，通过对比学习提升个性化，实验显示加速 8 倍。\n\n- **GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks（GeoBenchX: 用于多步地理空间任务的 LLM 基准测试）**  \n  建立地理空间任务基准，测试 LLM 在工具调用上的性能，Claude 模型表现出色。\n\n其余论文如在图像处理、强化学习和优化领域的（如 LocDiffusion、STShield），主要贡献是新框架或基准，但非核心焦点，就简要提及为技术进步，无需深挖。\n\n总之，今天的 arXiv 论文突显 AI 领域的创新潜力，尤其在模型安全和医疗应用上，读者可关注相关主题以追踪前沿进展。明天见！",
  "papers": [
    {
      "arxiv_id": "2503.18242v1",
      "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Aneesh Vathul",
        "Daniel Lee",
        "Sheryl Chen",
        "Arthi Tasmia"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.",
      "tldr_zh": "该论文提出ShED-HD框架，利用Shannon Entropy Distribution来实现轻量级幻觉(hallucinations)检测，针对大语言模型(LLMs)在资源受限的边缘设备上生成虚假内容的挑战。框架采用轻量级BiLSTM架构结合单头注意力机制，分析输出序列的熵模式，以高效捕捉不确定性并保持上下文感知。实验在BioASQ、TriviaQA和Jeopardy Questions数据集上显示，ShED-HD在out-of-distribution场景下显著优于其他高效方法，在in-distribution场景下表现相当，从而提升了LLMs在边缘设备上的可靠性和可信度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18242v1",
      "published_date": "2025-03-23 23:47:26 UTC",
      "updated_date": "2025-03-23 23:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:10:43.901187"
    },
    {
      "arxiv_id": "2503.18238v1",
      "title": "Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance",
      "title_zh": "与 AI 代理协作：关于团队合作、生产力和绩效的实地实验",
      "authors": [
        "Harang Ju",
        "Sinan Aral"
      ],
      "abstract": "To uncover how AI agents change productivity, performance, and work\nprocesses, we introduce MindMeld: an experimentation platform enabling humans\nand AI agents to collaborate in integrative workspaces. In a large-scale\nmarketing experiment on the platform, 2310 participants were randomly assigned\nto human-human and human-AI teams, with randomized AI personality traits. The\nteams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad\ncopy edits, and 10,375 AI-generated images while producing 11,138 ads for a\nlarge think tank. Analysis of fine-grained communication, collaboration, and\nworkflow logs revealed that collaborating with AI agents increased\ncommunication by 137% and allowed humans to focus 23% more on text and image\ncontent generation messaging and 20% less on direct text editing. Humans on\nHuman-AI teams sent 23% fewer social messages, creating 60% greater\nproductivity per worker and higher-quality ad copy. In contrast, human-human\nteams produced higher-quality images, suggesting that AI agents require\nfine-tuning for multimodal workflows. AI personality prompt randomization\nrevealed that AI traits can complement human personalities to enhance\ncollaboration. For example, conscientious humans paired with open AI agents\nimproved image quality, while extroverted humans paired with conscientious AI\nagents reduced the quality of text, images, and clicks. In field tests of ad\ncampaigns with ~5M impressions, ads with higher image quality produced by human\ncollaborations and higher text quality produced by AI collaborations performed\nsignificantly better on click-through rate and cost per click metrics. Overall,\nads created by human-AI teams performed similarly to those created by\nhuman-human teams. Together, these results suggest AI agents can improve\nteamwork and productivity, especially when tuned to complement human traits.",
      "tldr_zh": "该研究引入了 MindMeld 平台，通过大规模实地实验探讨 AI agents 与人类协作对团队合作、生产力和表现的影响，涉及 2310 名参与者随机分配到人类-人类或人类-AI 团队，并测试 AI personality traits。结果显示，人类-AI 团队的沟通量增加 137%，人类更专注于内容生成（提高 23%），直接编辑减少 20%，并实现每工人生产力提升 60% 和广告文案质量更高，但图像质量不如人类-人类团队。AI agents 的个性特征能与人类个性互补，例如尽责人类配对开放 AI 提升图像质量，而在实地测试中，人类-AI 团队生产的广告在点击率和成本 per click 指标上表现类似或优于纯人类团队，表明 AI agents 可显著改善团队工作效率。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "56 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18238v1",
      "published_date": "2025-03-23 23:20:32 UTC",
      "updated_date": "2025-03-23 23:20:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:10:57.326835"
    },
    {
      "arxiv_id": "2503.18996v1",
      "title": "Enhanced prediction of spine surgery outcomes using advanced machine learning techniques and oversampling methods",
      "title_zh": "利用高级机器学习技术和过采样方法增强脊柱手术结果预测",
      "authors": [
        "José Alberto Benítez-Andrades",
        "Camino Prada-García",
        "Nicolás Ordás-Reyes",
        "Marta Esteban Blanco",
        "Alicia Merayo",
        "Antonio Serrano-García"
      ],
      "abstract": "The study proposes an advanced machine learning approach to predict spine\nsurgery outcomes by incorporating oversampling techniques and grid search\noptimization. A variety of models including GaussianNB, ComplementNB, KNN,\nDecision Tree, and optimized versions with RandomOverSampler and SMOTE were\ntested on a dataset of 244 patients, which included pre-surgical, psychometric,\nsocioeconomic, and analytical variables. The enhanced KNN models achieved up to\n76% accuracy and a 67% F1-score, while grid-search optimization further\nimproved performance. The findings underscore the potential of these advanced\ntechniques to aid healthcare professionals in decision-making, with future\nresearch needed to refine these models on larger and more diverse datasets.",
      "tldr_zh": "这篇论文提出了一种先进的机器学习方法，通过 oversampling 技术（如 RandomOverSampler 和 SMOTE）以及 grid search 优化，来提升脊柱手术结果的预测准确性。研究者测试了多种模型，包括 GaussianNB, ComplementNB, KNN, Decision Tree，在一个包含 244 名患者的 dataset 上进行评估，该 dataset 涵盖了术前、心理测量、社会经济和分析变量。增强后的 KNN 模型实现了 76% 的准确率和 67% 的 F1-score，而优化后进一步提高了整体性能。这些发现突出了这些技术在医疗决策中的潜力，并建议未来研究在更大、更多样化的 dataset 上进一步完善模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18996v1",
      "published_date": "2025-03-23 22:39:19 UTC",
      "updated_date": "2025-03-23 22:39:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:11:07.771961"
    },
    {
      "arxiv_id": "2503.18229v1",
      "title": "Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in Engineering Design Optimization",
      "title_zh": "适应性多保真度强化学习用于",
      "authors": [
        "Akash Agrawal",
        "Christopher McComb"
      ],
      "abstract": "Multi-fidelity Reinforcement Learning (RL) frameworks efficiently utilize\ncomputational resources by integrating analysis models of varying accuracy and\ncosts. The prevailing methodologies, characterized by transfer learning,\nhuman-inspired strategies, control variate techniques, and adaptive sampling,\npredominantly depend on a structured hierarchy of models. However, this\nreliance on a model hierarchy can exacerbate variance in policy learning when\nthe underlying models exhibit heterogeneous error distributions across the\ndesign space. To address this challenge, this work proposes a novel adaptive\nmulti-fidelity RL framework, in which multiple heterogeneous, non-hierarchical\nlow-fidelity models are dynamically leveraged alongside a high-fidelity model\nto efficiently learn a high-fidelity policy. Specifically, low-fidelity\npolicies and their experience data are adaptively used for efficient targeted\nlearning, guided by their alignment with the high-fidelity policy. The\neffectiveness of the approach is demonstrated in an octocopter design\noptimization problem, utilizing two low-fidelity models alongside a\nhigh-fidelity simulator. The results demonstrate that the proposed approach\nsubstantially reduces variance in policy learning, leading to improved\nconvergence and consistent high-quality solutions relative to traditional\nhierarchical multi-fidelity RL methods. Moreover, the framework eliminates the\nneed for manually tuning model usage schedules, which can otherwise introduce\nsignificant computational overhead. This positions the framework as an\neffective variance-reduction strategy for multi-fidelity RL, while also\nmitigating the computational and operational burden of manual fidelity\nscheduling.",
      "tldr_zh": "本文提出了一种自适应多-fidelity Reinforcement Learning (RL) 框架，用于工程设计优化中的方差减少，通过动态整合多个异构、非层次化的低-fidelity 模型与高-fidelity 模型，实现高效策略学习。框架的关键机制是根据低-fidelity 策略与高-fidelity 策略的契合度，自适应地利用经验数据进行针对性训练，从而避免了传统方法依赖模型层次结构带来的方差增加问题。在八旋翼无人机（octocopter）设计优化实验中，该方法显著降低了策略学习的方差，提高了收敛速度和解决方案质量，同时消除了手动调整模型使用调度的计算开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18229v1",
      "published_date": "2025-03-23 22:29:08 UTC",
      "updated_date": "2025-03-23 22:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:11:19.868979"
    },
    {
      "arxiv_id": "2503.18227v3",
      "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yiheng Zhong",
        "Zihong Luo",
        "Chengzhi Liu",
        "Feilong Tang",
        "Zelin Peng",
        "Ming Hu",
        "Yingzhen Hu",
        "Jionglong Su",
        "Zongyuan Ge",
        "Imran Razzak"
      ],
      "abstract": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM.",
      "tldr_zh": "该研究针对Segment Anything Model (SAM)在医疗图像分割中的准确性和鲁棒性下降问题，提出Prior-Guided SAM (PG-SAM)方法，通过细粒度模态先验对齐器利用医疗LLM的专用知识来处理领域差距和文本粒度问题，从而提升先验质量。PG-SAM的核心包括多级特征融合和迭代掩码优化器，以增强模型表达能力和支持无提示学习，同时引入一个统一的管道提供高质量语义信息。实验结果显示，在Synapse数据集上，PG-SAM实现了最先进的多器官分割性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18227v3",
      "published_date": "2025-03-23 22:06:07 UTC",
      "updated_date": "2025-03-26 13:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:11:31.350911"
    },
    {
      "arxiv_id": "2503.18995v1",
      "title": "LLMs in the Classroom: Outcomes and Perceptions of Questions Written with the Aid of AI",
      "title_zh": "LLMs 在课堂中：使用 AI 辅助编写的问题的结果与感知",
      "authors": [
        "Gavin Witsken",
        "Igor Crk",
        "Eren Gultepe"
      ],
      "abstract": "We randomly deploy questions constructed with and without use of the LLM tool\nand gauge the ability of the students to correctly answer, as well as their\nability to correctly perceive the difference between human-authored and\nLLM-authored questions. In determining whether the questions written with the\naid of ChatGPT were consistent with the instructor's questions and source text,\nwe computed representative vectors of both the human and ChatGPT questions\nusing SBERT and compared cosine similarity to the course textbook. A\nnon-significant Mann-Whitney U test (z = 1.018, p = .309) suggests that\nstudents were unable to perceive whether questions were written with or without\nthe aid of ChatGPT. However, student scores on LLM-authored questions were\nalmost 9% lower (z = 2.702, p < .01). This result may indicate that either the\nAI questions were more difficult or that the students were more familiar with\nthe instructor's style of questions. Overall, the study suggests that while\nthere is potential for using LLM tools to aid in the construction of\nassessments, care must be taken to ensure that the questions are fair,\nwell-composed, and relevant to the course material.",
      "tldr_zh": "本研究评估了使用 LLM 工具（如 ChatGPT）辅助创建课堂问题的效果，通过随机部署这些问题并比较学生回答能力和对问题来源的感知。研究采用 SBERT 计算问题向量，并通过余弦相似度与课程教材比较。结果显示，学生无法有效区分 LLM 辅助和人类创建的问题（Mann-Whitney U test 非显著，z=1.018, p=0.309），但在 LLM 问题上得分几乎低 9%（z=2.702, p<0.01），可能由于这些问题更难或学生更适应教师风格。总体而言，该研究建议在使用 LLM 工具构建评估时，需要确保问题公平、良好构建并与课程材料相关。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to AAAI 2025 Technical Track on AI Alignment",
      "pdf_url": "http://arxiv.org/pdf/2503.18995v1",
      "published_date": "2025-03-23 22:01:49 UTC",
      "updated_date": "2025-03-23 22:01:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:11:43.489989"
    },
    {
      "arxiv_id": "2503.18213v1",
      "title": "A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives",
      "title_zh": "神经符号人工智能的研究：医疗保健视角",
      "authors": [
        "Delower Hossain",
        "Jake Y Chen"
      ],
      "abstract": "Over the last few decades, Artificial Intelligence (AI) scientists have been\nconducting investigations to attain human-level performance by a machine in\naccomplishing a cognitive task. Within machine learning, the ultimate\naspiration is to attain Artificial General Intelligence (AGI) through a\nmachine. This pursuit has led to the exploration of two distinct AI paradigms.\nSymbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and\nConnectionist (Sub-symbolic) AI, represented by Neural Systems, are two\nmutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,\nand knowledge representation but faces challenges in processing complex\nreal-world data with noise. Conversely, deep learning (Black-Box systems)\nresearch breakthroughs in neural networks are notable, yet they lack reasoning\nand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI\nresearch, attempts to bridge this gap by integrating logical reasoning into\nneural networks, enabling them to learn and reason with symbolic\nrepresentations. While a long path, this strategy has made significant progress\ntowards achieving common sense reasoning by systems. This article conducts an\nextensive review of over 977 studies from prominent scientific databases (DBLP,\nACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the\nmultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its\nhealthcare applications, particularly in drug discovery, and Protein\nengineering research. The survey addresses vital themes, including reasoning,\nexplainability, integration strategies, 41 healthcare-related use cases,\nbenchmarking, datasets, current approach limitations from both healthcare and\nbroader perspectives, and proposed novel approaches for future experiments.",
      "tldr_zh": "这篇论文对Neuro-Symbolic AI（NeSy）进行了全面综述，旨在整合Symbolic AI的推理和可解释性优势与Connectionist AI（如深度学习）的处理能力，解决AI在认知任务中的局限性。研究者审查了超过977个来自多个数据库的研究，重点探讨NeSy在医疗领域的应用，包括药物发现、蛋白质工程等41个具体用例，以及推理、解释性、整合策略、基准测试和数据集。论文总结了NeSy的当前限制和未来改进方向，为实现AI的常识推理和医疗创新提供了宝贵见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.18213v1",
      "published_date": "2025-03-23 21:33:38 UTC",
      "updated_date": "2025-03-23 21:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:11:55.579058"
    },
    {
      "arxiv_id": "2503.18210v1",
      "title": "ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data",
      "title_zh": "翻译失败",
      "authors": [
        "Nitish Dashora",
        "Dibya Ghosh",
        "Sergey Levine"
      ],
      "abstract": "Online reinforcement learning (RL) with sparse rewards poses a challenge\npartly because of the lack of feedback on states leading to the goal.\nFurthermore, expert offline data with reward signal is rarely available to\nprovide this feedback and bootstrap online learning. How can we guide online\nagents to the right solution without this on-task data? Reward shaping offers a\nsolution by providing fine-grained signal to nudge the policy towards the\noptimal solution. However, reward shaping often requires domain knowledge to\nhand-engineer heuristics for a specific goal. To enable more general and\ninexpensive guidance, we propose and analyze a data-driven methodology that\nautomatically guides RL by learning from widely available video data such as\nInternet recordings, off-task demonstrations, task failures, and undirected\nenvironment interaction. By learning a model of optimal goal-conditioned value\nfrom diverse passive data, we open the floor to scaling up and using various\ndata sources to model general goal-reaching behaviors relevant to guiding\nonline RL. Specifically, we use intent-conditioned value functions to learn\nfrom diverse videos and incorporate these goal-conditioned values into the\nreward. Our experiments show that video-trained value functions work well with\na variety of data sources, exhibit positive transfer from human video\npre-training, can generalize to unseen goals, and scale with dataset size.",
      "tldr_zh": "该研究提出ViVa方法，利用视频训练的价值函数（Value Functions）来引导在线强化学习（Online RL），解决稀疏奖励环境下缺乏反馈的问题。ViVa从多样化被动数据（如互联网录像、离任务演示和环境交互）中学习意图条件价值函数（Intent-conditioned Value Functions），并将其整合到奖励机制中，提供自动化的引导，而非依赖手动设计的奖励整形（Reward Shaping）。实验结果表明，ViVa能有效处理各种数据来源，实现从人类视频预训练的正向转移，泛化到未见目标，并随着数据集规模扩展性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18210v1",
      "published_date": "2025-03-23 21:24:33 UTC",
      "updated_date": "2025-03-23 21:24:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:12:06.917916"
    },
    {
      "arxiv_id": "2503.18197v1",
      "title": "FROG: Fair Removal on Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Ziheng Chen",
        "Jiali Cheng",
        "Gabriele Tolomei",
        "Sijia Liu",
        "Hadi Amiri",
        "Yu Wang",
        "Kaushiki Nag",
        "Lu Lin"
      ],
      "abstract": "As compliance with privacy regulations becomes increasingly critical, the\ngrowing demand for data privacy has highlighted the significance of machine\nunlearning in many real world applications, such as social network and\nrecommender systems, many of which can be represented as graph-structured data.\nHowever, existing graph unlearning algorithms indiscriminately modify edges or\nnodes from well-trained models without considering the potential impact of such\nstructural modifications on fairness. For example, forgetting links between\nnodes with different genders in a social network may exacerbate group\ndisparities, leading to significant fairness concerns. To address these\nchallenges, we propose a novel approach that jointly optimizes the graph\nstructure and the corresponding model for fair unlearning tasks.\nSpecifically,our approach rewires the graph to enhance unlearning efficiency by\nremoving redundant edges that hinder forgetting while preserving fairness\nthrough targeted edge augmentation. Additionally, we introduce a worst-case\nevaluation mechanism to assess the reliability of fair unlearning performance.\nExtensive experiments on real-world datasets demonstrate the effectiveness of\nthe proposed approach in achieving superior unlearning outcomes.",
      "tldr_zh": "该研究提出FROG框架，用于在图结构数据（如社交网络）中实现公平的machine unlearning，解决现有算法在修改图结构时可能加剧群体不公平的问题，例如遗忘不同性别节点间的链接导致的歧视。FROG通过联合优化图结构和模型，移除阻碍forgetting的冗余边，同时通过targeted edge augmentation增强公平性，以提高unlearning效率。实验在真实数据集上验证了该方法的有效性，并在引入worst-case evaluation机制后，展示了其在公平unlearning性能上的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18197v1",
      "published_date": "2025-03-23 20:39:53 UTC",
      "updated_date": "2025-03-23 20:39:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:12:18.498209"
    },
    {
      "arxiv_id": "2503.18185v1",
      "title": "Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Spyridon Evangelatos",
        "Eleni Veroni",
        "Vasilis Efthymiou",
        "Christos Nikolopoulos",
        "Georgios Th. Papadopoulos",
        "Panagiotis Sarigiannidis"
      ],
      "abstract": "Counterfactual explanations have emerged as a prominent method in Explainable\nArtificial Intelligence (XAI), providing intuitive and actionable insights into\nMachine Learning model decisions. In contrast to other traditional feature\nattribution methods that assess the importance of input variables,\ncounterfactual explanations focus on identifying the minimal changes required\nto alter a model's prediction, offering a ``what-if'' analysis that is close to\nhuman reasoning. In the context of XAI, counterfactuals enhance transparency,\ntrustworthiness and fairness, offering explanations that are not just\ninterpretable but directly applicable in the decision-making processes.\n  In this paper, we present a novel framework that integrates perturbation\ntheory and statistical mechanics to generate minimal counterfactual\nexplanations in explainable AI. We employ a local Taylor expansion of a Machine\nLearning model's predictive function and reformulate the counterfactual search\nas an energy minimization problem over a complex landscape. In sequence, we\nmodel the probability of candidate perturbations leveraging the Boltzmann\ndistribution and use simulated annealing for iterative refinement. Our approach\nsystematically identifies the smallest modifications required to change a\nmodel's prediction while maintaining plausibility. Experimental results on\nbenchmark datasets for cybersecurity in Internet of Things environments,\ndemonstrate that our method provides actionable, interpretable counterfactuals\nand offers deeper insights into model sensitivity and decision boundaries in\nhigh-dimensional spaces.",
      "tldr_zh": "本文提出了一种新框架，用于生成最小反事实解释（Counterfactual explanations），以提升可解释人工智能（XAI）在决策过程中的透明度、可信度和公平性。该框架整合扰动理论和统计力学，通过局部 Taylor 展开将反事实搜索转化为能量最小化问题，并利用 Boltzmann 分布建模扰动概率，再结合模拟退火进行迭代优化，从而识别出最小且合理的模型预测变化。实验在物联网网络安全基准数据集上表明，该方法能提供可操作且可解释的解释，并揭示模型的敏感性和决策边界，适用于网络安全及其他领域。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18185v1",
      "published_date": "2025-03-23 19:48:37 UTC",
      "updated_date": "2025-03-23 19:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:12:32.224040"
    },
    {
      "arxiv_id": "2503.18181v1",
      "title": "Adaptive Physics-informed Neural Networks: A Survey",
      "title_zh": "自适应物理信息神经网络：综述",
      "authors": [
        "Edgar Torres",
        "Jonathan Schiefer",
        "Mathias Niepert"
      ],
      "abstract": "Physics-informed neural networks (PINNs) have emerged as a promising approach\nto solving partial differential equations (PDEs) using neural networks,\nparticularly in data-scarce scenarios, due to their unsupervised training\ncapability. However, limitations related to convergence and the need for\nre-optimization with each change in PDE parameters hinder their widespread\nadoption across scientific and engineering applications. This survey reviews\nexisting research that addresses these limitations through transfer learning\nand meta-learning. The covered methods improve the training efficiency,\nallowing faster adaptation to new PDEs with fewer data and computational\nresources. While traditional numerical methods solve systems of differential\nequations directly, neural networks learn solutions implicitly by adjusting\ntheir parameters. One notable advantage of neural networks is their ability to\nabstract away from specific problem domains, allowing them to retain, discard,\nor adapt learned representations to efficiently address similar problems. By\nexploring the application of these techniques to PINNs, this survey identifies\npromising directions for future research to facilitate the broader adoption of\nPINNs in a wide range of scientific and engineering applications.",
      "tldr_zh": "这篇调查综述探讨了Physics-informed Neural Networks (PINNs)，一种用于解决偏微分方程 (PDEs) 的神经网络方法，尤其在数据稀缺场景下的无监督训练优势。然而，PINNs 面临收敛问题和参数变化时的重新优化需求，通过transfer learning和meta-learning等技术，这些限制得以缓解，提高了训练效率并减少了对数据和计算资源的依赖。与传统数值方法不同，神经网络能隐式学习解决方案，并灵活适应类似问题领域。该研究指出了未来改进方向，以推动PINNs在科学和工程应用的广泛采用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "https://openreview.net/forum?id=vz5P1Kbt6t",
      "pdf_url": "http://arxiv.org/pdf/2503.18181v1",
      "published_date": "2025-03-23 19:33:05 UTC",
      "updated_date": "2025-03-23 19:33:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:12:44.211145"
    },
    {
      "arxiv_id": "2503.18994v1",
      "title": "HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT",
      "title_zh": "翻译失败",
      "authors": [
        "Paolo Ceravolo",
        "Ernesto Damiani",
        "Maria Elisa D'Amico",
        "Bianca de Teffe Erb",
        "Simone Favaro",
        "Nannerel Fiano",
        "Paolo Gambatesa",
        "Simone La Porta",
        "Samira Maghool",
        "Lara Mauri",
        "Niccolo Panigada",
        "Lorenzo Maria Ratto Vaquer",
        "Marta A. Tamborini"
      ],
      "abstract": "This paper introduces the HH4AI Methodology, a structured approach to\nassessing the impact of AI systems on human rights, focusing on compliance with\nthe EU AI Act and addressing technical, ethical, and regulatory challenges. The\npaper highlights AIs transformative nature, driven by autonomy, data, and\ngoal-oriented design, and how the EU AI Act promotes transparency,\naccountability, and safety. A key challenge is defining and assessing\n\"high-risk\" AI systems across industries, complicated by the lack of\nuniversally accepted standards and AIs rapid evolution.\n  To address these challenges, the paper explores the relevance of ISO/IEC and\nIEEE standards, focusing on risk management, data quality, bias mitigation, and\ngovernance. It proposes a Fundamental Rights Impact Assessment (FRIA)\nmethodology, a gate-based framework designed to isolate and assess risks\nthrough phases including an AI system overview, a human rights checklist, an\nimpact assessment, and a final output phase. A filtering mechanism tailors the\nassessment to the system's characteristics, targeting areas like\naccountability, AI literacy, data governance, and transparency.\n  The paper illustrates the FRIA methodology through a fictional case study of\nan automated healthcare triage service. The structured approach enables\nsystematic filtering, comprehensive risk assessment, and mitigation planning,\neffectively prioritizing critical risks and providing clear remediation\nstrategies. This promotes better alignment with human rights principles and\nenhances regulatory compliance.",
      "tldr_zh": "这篇论文介绍了 HH4AI 方法论，这是一种结构化的框架，用于评估 AI 系统对人权的影响，确保符合 EU AI Act，并在技术、伦理和监管方面应对挑战。该方法整合 ISO/IEC 和 IEEE 标准，提出 Fundamental Rights Impact Assessment (FRIA) 框架，包括 AI 系统概述、人权检查清单、影响评估和输出阶段，并通过过滤机制针对系统的特性（如责任、AI 素养、数据治理和透明）进行风险评估。通过一个虚构的自动化医疗急诊服务案例研究，FRIA 框架证明了其在系统过滤风险、全面评估影响和规划缓解策略方面的有效性，从而促进 AI 与人权原则的更好对齐和监管合规。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 7 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.18994v1",
      "published_date": "2025-03-23 19:10:14 UTC",
      "updated_date": "2025-03-23 19:10:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:12:57.584519"
    },
    {
      "arxiv_id": "2503.18172v3",
      "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Zixin Chen",
        "Sicheng Song",
        "Kashun Shum",
        "Yanna Lin",
        "Rui Sheng",
        "Huamin Qu"
      ],
      "abstract": "Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.",
      "tldr_zh": "本论文探讨了误导性图表问题，这些图表通过操纵数据表示来扭曲认知，导致错误结论，尽管已有多年研究但问题依然突出。研究者引入了Misleading ChartQA基准数据集，包含超过3,000个示例，覆盖21种误导类型和10种图表类型，并通过标准化的图表代码、CSV数据和多选题进行验证，以评估Multimodal Large Language Models (MLLMs)在识别和推理误导性图表方面的能力。基准测试了16个最先进MLLMs，揭示了它们在检测视觉欺骗方面的局限性，并提出了一种新管道来检测和定位误导因素，从而提升MLLMs的解释准确性。该工作为推进MLLM驱动的误导性图表理解奠定基础，并公开发布样本数据集以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages in total. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.18172v3",
      "published_date": "2025-03-23 18:56:33 UTC",
      "updated_date": "2025-04-15 15:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:13:09.586186"
    },
    {
      "arxiv_id": "2503.18170v1",
      "title": "Self-Attention Diffusion Models for Zero-Shot Biomedical Image Segmentation: Unlocking New Frontiers in Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Abderrachid Hamrani",
        "Anuradha Godavarty"
      ],
      "abstract": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies.",
      "tldr_zh": "本文提出了一种名为 ADZUS 的新型系统，利用 Self-Attention Diffusion Models 实现零样本生物医学图像分割，无需标注数据或领域特定知识即可处理多样化医学图像。ADZUS 架构整合自注意力机制，提供上下文感知和细节敏感的分割性能，在皮肤病变、胸部X光感染和白血细胞等数据集上达到最先进水平，Dice scores 为 88.7% 到 92.9%、IoU scores 为 66.3% 到 93.3%。该方法显著减少了对昂贵标注的依赖，并能无缝适应新任务，尽管需要大量计算资源和处理时间。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18170v1",
      "published_date": "2025-03-23 18:47:12 UTC",
      "updated_date": "2025-03-23 18:47:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:13:21.518627"
    },
    {
      "arxiv_id": "2503.18168v1",
      "title": "Strategic Prompt Pricing for AIGC Services: A User-Centric Approach",
      "title_zh": "针对 AIGC 服务的战略提示定价：一种用户中心方法",
      "authors": [
        "Xiang Li",
        "Bing Luo",
        "Jianwei Huang",
        "Yuan Luo"
      ],
      "abstract": "The rapid growth of AI-generated content (AIGC) services has created an\nurgent need for effective prompt pricing strategies, yet current approaches\noverlook users' strategic two-step decision-making process in selecting and\nutilizing generative AI models. This oversight creates two key technical\nchallenges: quantifying the relationship between user prompt capabilities and\ngeneration outcomes, and optimizing platform payoff while accounting for\nheterogeneous user behaviors. We address these challenges by introducing prompt\nambiguity, a theoretical framework that captures users' varying abilities in\nprompt engineering, and developing an Optimal Prompt Pricing (OPP) algorithm.\nOur analysis reveals a counterintuitive insight: users with higher prompt\nambiguity (i.e., lower capability) exhibit non-monotonic prompt usage patterns,\nfirst increasing then decreasing with ambiguity levels, reflecting complex\nchanges in marginal utility. Experimental evaluation using a character-level\nGPT-like model demonstrates that our OPP algorithm achieves up to 31.72%\nimprovement in platform payoff compared to existing pricing mechanisms,\nvalidating the importance of user-centric prompt pricing in AIGC services.",
      "tldr_zh": "该论文提出了一种用户中心的方法来优化AIGC服务的提示定价策略，解决现有方法忽略用户在选择和使用生成AI模型时的战略两步决策问题。作者引入prompt ambiguity框架来量化用户提示工程能力与生成结果的关系，并开发了Optimal Prompt Pricing (OPP)算法，以优化平台收益并考虑用户行为的异质性。分析发现，用户prompt ambiguity（能力较低）会表现出非单调的提示使用模式，先增加后减少，实验在字符级GPT-like模型上验证OPP算法比现有机制提高平台收益高达31.72%。",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted in WiOpt 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18168v1",
      "published_date": "2025-03-23 18:41:06 UTC",
      "updated_date": "2025-03-23 18:41:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:13:32.669134"
    },
    {
      "arxiv_id": "2503.18167v2",
      "title": "Evaluating Negative Sampling Approaches for Neural Topic Models",
      "title_zh": "翻译失败",
      "authors": [
        "Suman Adhya",
        "Avishek Lahiri",
        "Debarshi Kumar Sanyal",
        "Partha Pratim Das"
      ],
      "abstract": "Negative sampling has emerged as an effective technique that enables deep\nlearning models to learn better representations by introducing the paradigm of\nlearn-to-compare. The goal of this approach is to add robustness to deep\nlearning models to learn better representation by comparing the positive\nsamples against the negative ones. Despite its numerous demonstrations in\nvarious areas of computer vision and natural language processing, a\ncomprehensive study of the effect of negative sampling in an unsupervised\ndomain like topic modeling has not been well explored. In this paper, we\npresent a comprehensive analysis of the impact of different negative sampling\nstrategies on neural topic models. We compare the performance of several\npopular neural topic models by incorporating a negative sampling technique in\nthe decoder of variational autoencoder-based neural topic models. Experiments\non four publicly available datasets demonstrate that integrating negative\nsampling into topic models results in significant enhancements across multiple\naspects, including improved topic coherence, richer topic diversity, and more\naccurate document classification. Manual evaluations also indicate that the\ninclusion of negative sampling into neural topic models enhances the quality of\nthe generated topics. These findings highlight the potential of negative\nsampling as a valuable tool for advancing the effectiveness of neural topic\nmodels.",
      "tldr_zh": "本文评估了负采样（negative sampling）策略对神经主题模型（neural topic models）的影響，通过将其整合到基于变分自编码器（variational autoencoder）的模型解码器中，比较了多种负采样方法。实验在四个公开数据集上进行，结果显示显著提升了主题连贯性（topic coherence）、主题多样性（topic diversity）和文档分类（document classification）的准确率。手动评估进一步证实，加入负采样提高了生成的主题质量，并证明其是提升神经主题模型有效性的重要工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code is available at: https://github.com/AdhyaSuman/Eval_NegTM",
      "pdf_url": "http://arxiv.org/pdf/2503.18167v2",
      "published_date": "2025-03-23 18:39:01 UTC",
      "updated_date": "2025-03-25 05:53:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:13:45.128137"
    },
    {
      "arxiv_id": "2503.18162v1",
      "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Xue",
        "Sarah M. Hooper",
        "Iain Pierce",
        "Rhodri H. Davies",
        "John Stairs",
        "Joseph Naegele",
        "Adrienne E. Campbell-Washburn",
        "Charlotte Manisty",
        "James C. Moon",
        "Thomas A. Treibel",
        "Peter Kellman",
        "Michael S. Hansen"
      ],
      "abstract": "To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.",
      "tldr_zh": "本研究提出了一种名为 SNRAware 的深度学习 MRI 去噪方法，通过利用 MRI 重建过程中的定量噪声分布信息（如 SNR 和 G-factor Map），提升去噪性能和模型泛化能力。研究在包含 2,885,236 张图像的大规模数据集上训练了 14 种 Transformer 和卷积模型，采用模拟合成数据集和噪声增强策略进行优化。实验结果显示，SNRAware 在内部测试中显著提高了 PSNR 和 SSIM 指标，并在外部测试中表现出色，例如在实时心影和灌注成像中分别提升 6.5 倍和 2.9 倍的 CNR，并成功泛化到不同解剖结构和场强的 MRI 序列。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18162v1",
      "published_date": "2025-03-23 18:16:36 UTC",
      "updated_date": "2025-03-23 18:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:13:57.280168"
    },
    {
      "arxiv_id": "2503.18161v1",
      "title": "Active Inference for Energy Control and Planning in Smart Buildings and Communities",
      "title_zh": "翻译失败",
      "authors": [
        "Seyyed Danial Nazemi",
        "Mohsen A. Jafari",
        "Andrea Matta"
      ],
      "abstract": "Active Inference (AIF) is emerging as a powerful framework for\ndecision-making under uncertainty, yet its potential in engineering\napplications remains largely unexplored. In this work, we propose a novel\ndual-layer AIF architecture that addresses both building-level and\ncommunity-level energy management. By leveraging the free energy principle,\neach layer adapts to evolving conditions and handles partial observability\nwithout extensive sensor information and respecting data privacy. We validate\nthe continuous AIF model against both a perfect optimization baseline and a\nreinforcement learning-based approach. We also test the community AIF framework\nunder extreme pricing scenarios. The results highlight the model's robustness\nin handling abrupt changes. This study is the first to show how a distributed\nAIF works in engineering. It also highlights new opportunities for\nprivacy-preserving and uncertainty-aware control strategies in engineering\napplications.",
      "tldr_zh": "本研究探索了 Active Inference (AIF) 在智能建筑和社区能源控制中的应用，提出了一种新型的双层 AIF 架构，用于处理建筑级和社区级管理。该架构基于 free energy principle，适应动态条件、处理部分可观察性，并通过减少传感器依赖和保护数据隐私来提升决策效率。实验结果显示，该模型在极端定价场景下比优化基线和强化学习方法更具鲁棒性，能够有效应对突发变化。这是首次验证分布式 AIF 在工程领域的实际应用，并为隐私保护和不确定性感知控制策略开辟了新机遇。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "Submitted to IEEE CASE 2025 (IEEE 21st International Conference on\n  Automation Science and Engineering)",
      "pdf_url": "http://arxiv.org/pdf/2503.18161v1",
      "published_date": "2025-03-23 18:03:01 UTC",
      "updated_date": "2025-03-23 18:03:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:14:09.053084"
    },
    {
      "arxiv_id": "2503.18159v1",
      "title": "DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via Personalizer-Guided Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Chen",
        "Xiaobao Wei",
        "Ming Lu",
        "Hui Chen",
        "Feng Tian"
      ],
      "abstract": "Real-time speech-driven 3D facial animation has been attractive in academia\nand industry. Traditional methods mainly focus on learning a deterministic\nmapping from speech to animation. Recent approaches start to consider the\nnondeterministic fact of speech-driven 3D face animation and employ the\ndiffusion model for the task. Existing diffusion-based methods can improve the\ndiversity of facial animation. However, personalized speaking styles conveying\naccurate lip language is still lacking, besides, efficiency and compactness\nstill need to be improved. In this work, we propose DiffusionTalker to address\nthe above limitations via personalizer-guided distillation. In terms of\npersonalization, we introduce a contrastive personalizer that learns identity\nand emotion embeddings to capture speaking styles from audio. We further\npropose a personalizer enhancer during distillation to enhance the influence of\nembeddings on facial animation. For efficiency, we use iterative distillation\nto reduce the steps required for animation generation and achieve more than 8x\nspeedup in inference. To achieve compactness, we distill the large teacher\nmodel into a smaller student model, reducing our model's storage by 86.4\\%\nwhile minimizing performance loss. After distillation, users can derive their\nidentity and emotion embeddings from audio to quickly create personalized\nanimations that reflect specific speaking styles. Extensive experiments are\nconducted to demonstrate that our method outperforms state-of-the-art methods.\nThe code will be released at: https://github.com/ChenVoid/DiffusionTalker.",
      "tldr_zh": "该研究提出DiffusionTalker，一种基于扩散模型的语音驱动3D面部动画系统，旨在提升个性化、效率和紧凑性。系统引入contrastive personalizer来学习身份和情感嵌入，捕捉语音中的说话风格，并通过personalizer enhancer在蒸馏过程中增强这些嵌入的影响。同时，采用iterative distillation减少生成步骤，实现推理速度超过8倍加速，并将大模型蒸馏到小模型，减少存储86.4%而性能损失最小。实验结果显示，DiffusionTalker在个性化动画生成上优于现有状态-of-the-art方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18159v1",
      "published_date": "2025-03-23 17:55:54 UTC",
      "updated_date": "2025-03-23 17:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:14:20.658368"
    },
    {
      "arxiv_id": "2503.18156v1",
      "title": "Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act",
      "title_zh": "翻译失败",
      "authors": [
        "Bram Rijsbosch",
        "Gijs van Dijck",
        "Konrad Kollnig"
      ],
      "abstract": "AI-generated images have become so good in recent years that individuals\ncannot distinguish them any more from \"real\" images. This development creates a\nseries of societal risks, and challenges our perception of what is true and\nwhat is not, particularly with the emergence of \"deep fakes\" that impersonate\nreal individuals. Watermarking, a technique that involves embedding identifying\ninformation within images to indicate their AI-generated nature, has emerged as\na primary mechanism to address the risks posed by AI-generated images. The\nimplementation of watermarking techniques is now becoming a legal requirement\nin many jurisdictions, including under the new 2024 EU AI Act. Despite the\nwidespread use of AI image generation systems, the current status of\nwatermarking implementation remains largely unexamined. Moreover, the practical\nimplications of the AI Act's watermarking requirements have not previously been\nstudied. The present paper therefore both provides an empirical analysis of 50\nof the most widely used AI systems for image generation, and embeds this\nempirical analysis into a legal analysis of the AI Act. We identify four\ncategories of generative AI image systems relevant under the AI Act, outline\nthe legal obligations for each category, and find that only a minority number\nof providers currently implement adequate watermarking practices.",
      "tldr_zh": "该论文探讨了 watermarking 技术在 generative AI 系统中的实际采用情况，以及其在 2024 EU AI Act 下的法律含义，以应对 AI 生成图像（如 deep fakes）带来的社会风险。研究者对 50 个最广泛使用的 AI 图像生成系统进行了实证分析，并将其嵌入法律分析中，识别了四类 generative AI image systems 及其相应的法律义务。结果发现，只有少数提供者实施了足够的 watermarking 实践，这突显了合规挑战，并为减少 AI 生成内容的潜在风险提供了重要见解。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages, 7 figures, note that this work has not been published in a\n  peer reviewed venue yet. While we have made our best effort to ensure the\n  validity of our findings, it is therefore still work in progress and\n  potentially subject to change",
      "pdf_url": "http://arxiv.org/pdf/2503.18156v1",
      "published_date": "2025-03-23 17:55:33 UTC",
      "updated_date": "2025-03-23 17:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:14:32.245323"
    },
    {
      "arxiv_id": "2503.18151v1",
      "title": "Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Siwon Kim",
        "Wooyung Yun",
        "Jeongbin Oh",
        "Soomok Lee"
      ],
      "abstract": "Deep learning has emerged as the predominant solution for classifying medical\nimages. We intend to apply these developments to the ultra-widefield (UWF)\nretinal imaging dataset. Since UWF images can accurately diagnose various\nretina diseases, it is very important to clas sify them accurately and prevent\nthem with early treatment. However, processing images manually is\ntime-consuming and labor-intensive, and there are two challenges to automating\nthis process. First, high perfor mance usually requires high computational\nresources. Artificial intelli gence medical technology is better suited for\nplaces with limited medical resources, but using high-performance processing\nunits in such environ ments is challenging. Second, the problem of the accuracy\nof colour fun dus photography (CFP) methods. In general, the UWF method\nprovides more information for retinal diagnosis than the CFP method, but most\nof the research has been conducted based on the CFP method. Thus, we\ndemonstrate that these problems can be efficiently addressed in low performance\nunits using methods such as strategic data augmentation and model ensembles,\nwhich balance performance and computational re sources while utilizing UWF\nimages.",
      "tldr_zh": "本研究探讨了高效的深层学习方法，用于处理超宽场（UWF）视网膜图像，以准确分类各种视网膜疾病并实现早期治疗。论文针对手动处理耗时费力的问题，以及高计算资源需求和现有颜色眼底摄影（CFP）方法的准确性挑战，提出了战略数据增强和模型集成等技术。这些方法能够在低性能设备上平衡性能与计算资源，充分利用UWF图像的优势，并证明了在资源有限环境中的高效应用。实验结果显示，该方法提升了UWF图像的分类准确性，为自动化医疗诊断提供了可行方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18151v1",
      "published_date": "2025-03-23 17:43:24 UTC",
      "updated_date": "2025-03-23 17:43:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:14:43.744180"
    },
    {
      "arxiv_id": "2504.08744v1",
      "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Esmail Gumaan"
      ],
      "abstract": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation.",
      "tldr_zh": "该论文提出了 ExpertRAG，一种创新框架，将 Mixture-of-Experts (MoE) 架构与 Retrieval Augmented Generation (RAG) 相结合，提升知识密集型语言模型的效率和准确性。核心机制包括动态检索门控和专家路由，允许模型根据查询需求选择性地访问外部知识库或内部专家，从而优化计算成本和知识利用。论文通过概率公式化和数学分析，量化了 ExpertRAG 在计算节约和容量增益方面的优势，并与标准 RAG 和纯 MoE 模型（如 Switch Transformer）进行比较，展示了其在事实回忆、泛化性和推理效率上的潜力。该框架为扩展语言模型提供了新视角，并规划了实验验证策略，以实现更事实性、适应性和高效的生成。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "30 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.08744v1",
      "published_date": "2025-03-23 17:26:23 UTC",
      "updated_date": "2025-03-23 17:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:14:56.447966"
    },
    {
      "arxiv_id": "2503.18142v1",
      "title": "LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert Space",
      "title_zh": "LocDiffusion: 通过在希尔伯特空间中扩散来识别地球上的位置",
      "authors": [
        "Zhangyu Wang",
        "Jielu Zhang",
        "Zhongliang Zhou",
        "Qian Cao",
        "Nemin Wu",
        "Zeping Liu",
        "Lan Mu",
        "Yang Song",
        "Yiqun Xie",
        "Ni Lao",
        "Gengchen Mai"
      ],
      "abstract": "Image geolocalization is a fundamental yet challenging task, aiming at\ninferring the geolocation on Earth where an image is taken. Existing methods\napproach it either via grid-based classification or via image retrieval. Their\nperformance significantly suffers when the spatial distribution of test images\ndoes not align with such choices. To address these limitations, we propose to\nleverage diffusion as a mechanism for image geolocalization. To avoid the\nproblematic manifold reprojection step in diffusion, we developed a novel\nspherical positional encoding-decoding framework, which encodes points on a\nspherical surface (e.g., geolocations on Earth) into a Hilbert space of\nSpherical Harmonics coefficients and decodes points (geolocations) by\nmode-seeking. We call this type of position encoding Spherical Harmonics Dirac\nDelta (SHDD) Representation. We also propose a novel SirenNet-based\narchitecture called CS-UNet to learn the conditional backward process in the\nlatent SHDD space by minimizing a latent KL-divergence loss. We train a\nconditional latent diffusion model called LocDiffusion that generates\ngeolocations under the guidance of images -- to the best of our knowledge, the\nfirst generative model for image geolocalization by diffusing geolocation\ninformation in a hidden location embedding space. We evaluate our method\nagainst SOTA image geolocalization baselines. LocDiffusion achieves competitive\ngeolocalization performance and demonstrates significantly stronger\ngeneralizability to unseen geolocations.",
      "tldr_zh": "本文提出 LocDiffusion，一种创新的图像地理定位方法，通过在 Hilbert 空间中利用扩散模型（diffusion）来推断图像拍摄位置，避免了现有网格分类或图像检索方法的分布不匹配问题。核心创新包括开发 Spherical Harmonics Dirac Delta (SHDD) Representation 用于球形位置编码-解码，以及基于 SirenNet 的 CS-UNet 架构来学习条件后向过程，从而最小化潜在 KL 散度损失。实验结果显示，LocDiffusion 与 SOTA 基线相比，实现了竞争性的地理定位性能，并显著提升了对未见位置的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18142v1",
      "published_date": "2025-03-23 17:15:26 UTC",
      "updated_date": "2025-03-23 17:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:15:08.973072"
    },
    {
      "arxiv_id": "2503.18991v2",
      "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoxi Cheng",
        "Haoxuan Ma",
        "Weixin Wang"
      ],
      "abstract": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.",
      "tldr_zh": "该研究针对大型语言模型(LLM)对齐面临的四大挑战——平衡安全数据集稀缺、对齐税(alignment tax)、易受越狱攻击以及无法动态调整任务难度奖励——提出了一种新型方法HAIR(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning)。HAIR包括两个关键组件：(1)利用LLM的内省推理能力构建一个平衡的安全Chain-of-Draft (CoD)数据集，覆盖七个有害类别；(2)通过Group Relative Policy Optimization (GRPO)训练类别特定的奖励模型，以动态调整优化策略适应任务难度。实验结果显示，HAIR在四个无害性和四个有用性基准上实现了最先进性能，显著提升了模型的安全性，同时保持了高水平的有用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The three authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2503.18991v2",
      "published_date": "2025-03-23 16:40:29 UTC",
      "updated_date": "2025-05-06 13:47:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:15:19.995850"
    },
    {
      "arxiv_id": "2503.18130v1",
      "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Juntao Dai",
        "Taiye Chen",
        "Yaodong Yang",
        "Qian Zheng",
        "Gang Pan"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for\naligning large language models (LLMs) with human values. However, reward\nover-optimization remains an open challenge leading to discrepancies between\nthe performance of LLMs under the reward model and the true human objectives. A\nprimary contributor to reward over-optimization is the extrapolation error that\narises when the reward model evaluates out-of-distribution (OOD) responses.\nHowever, current methods still fail to prevent the increasing frequency of OOD\nresponse generation during the reinforcement learning (RL) process and are not\neffective at handling extrapolation errors from OOD responses. In this work, we\npropose the Behavior-Supported Policy Optimization (BSPO) method to mitigate\nthe reward over-optimization issue. Specifically, we define behavior policy as\nthe next token distribution of the reward training dataset to model the\nin-distribution (ID) region of the reward model. Building on this, we introduce\nthe behavior-supported Bellman operator to regularize the value function,\npenalizing all OOD values without impacting the ID ones. Consequently, BSPO\nreduces the generation of OOD responses during the RL process, thereby avoiding\noverestimation caused by the reward model's extrapolation errors.\nTheoretically, we prove that BSPO guarantees a monotonic improvement of the\nsupported policy until convergence to the optimal behavior-supported policy.\nEmpirical results from extensive experiments show that BSPO outperforms\nbaselines in preventing reward over-optimization due to OOD evaluation and\nfinding the optimal ID policy.",
      "tldr_zh": "本文提出BSPO（Behavior-Supported Policy Optimization）方法，以缓解RLHF（Reinforcement Learning from Human Feedback）中奖励过度优化问题，该问题主要源于奖励模型对OOD（out-of-distribution）响应的外推错误。BSPO通过定义行为策略来建模ID（in-distribution）区域，并引入行为支持的Bellman算子正则化价值函数，从而减少RL过程中的OOD响应生成并避免过度估计。理论证明显示，BSPO能保证策略的单调改进直至收敛到最优行为支持策略，而实验结果表明它在防止奖励过度优化和寻找最优ID策略方面优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18130v1",
      "published_date": "2025-03-23 16:20:59 UTC",
      "updated_date": "2025-03-23 16:20:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:15:32.661491"
    },
    {
      "arxiv_id": "2503.18129v1",
      "title": "GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Varvara Krechetova",
        "Denis Kochedykov"
      ],
      "abstract": "In this paper, we establish a benchmark for evaluating large language models\n(LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.\nWe assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini\n2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent\nequipped with 23 geospatial functions. Our benchmark comprises tasks across\nfour categories of increasing complexity, with both solvable and intentionally\nunsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge\nevaluation framework to compare agent solutions against reference\nimplementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall\nperformance, with Claude models excelling on solvable tasks while OpenAI models\nbetter identify unsolvable scenarios. We observe significant differences in\ntoken usage, with Anthropic models consuming substantially more tokens than\ncompetitors. Common errors include misunderstanding geometrical relationships,\nrelying on outdated knowledge, and inefficient data manipulation. The resulting\nbenchmark set, evaluation framework, and data generation pipeline are released\nas open-source resources, providing one more standardized method for ongoing\nevaluation of LLMs for GeoAI.",
      "tldr_zh": "本论文引入GeoBenchX基准，用于评估大型语言模型（LLMs）在多步地理空间任务中的性能，针对商业GIS从业者的实际需求。研究评估了七个领先商业LLMs（包括Sonnet 3.5、3.7、Haiku 3.5、Gemini 2.0、GPT-4o、GPT-4o mini和o3-mini），使用配备23个地理空间函数的简单工具调用代理，并设计四类复杂度递增的任务，包括可解决和故意不可解决的任务，以测试幻觉拒绝。结果显示Sonnet 3.5和GPT-4o整体表现最佳，Claude模型在可解决任务上更出色，而OpenAI模型更善于识别不可解决场景；此外，Anthropic模型的令牌使用量显著更高，常见错误涉及几何关系误解、依赖过时知识和数据操作低效。该基准集、评估框架（LLM-as-Judge）和数据生成管道已开源，以标准化GeoAI领域的LLM评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX",
      "pdf_url": "http://arxiv.org/pdf/2503.18129v1",
      "published_date": "2025-03-23 16:20:14 UTC",
      "updated_date": "2025-03-23 16:20:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:15:47.232983"
    },
    {
      "arxiv_id": "2503.18102v1",
      "title": "AgentRxiv: Towards Collaborative Autonomous Research",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel Schmidgall",
        "Michael Moor"
      ],
      "abstract": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
      "tldr_zh": "该研究提出 AgentRxiv 框架，旨在实现 LLM 代理实验室之间的协作，通过共享预印本服务器上传和检索报告，帮助代理共享见解并迭代构建研究，从而解决现有自主代理孤立工作的局限性。实验中，代理被任务开发新的推理和提示技术，结果显示有访问先前研究的代理在 MATH-500 基准上比基线提升 11.4%，而最佳策略在其他领域平均提升 3.3%。此外，多个代理实验室通过 AgentRxiv 协作，比孤立实验室更快进步，在 MATH-500 上实现 13.7% 的相对准确率提升，表明自治代理可能与人类共同加速科学发现和 AI 系统设计。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18102v1",
      "published_date": "2025-03-23 15:16:42 UTC",
      "updated_date": "2025-03-23 15:16:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:15:57.890135"
    },
    {
      "arxiv_id": "2503.18085v1",
      "title": "Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Rochana Chaturvedi",
        "Peyman Baghershahi",
        "Sourav Medya",
        "Barbara Di Eugenio"
      ],
      "abstract": "Temporal information extraction from unstructured text is essential for\ncontextualizing events and deriving actionable insights, particularly in the\nmedical domain. We address the task of extracting clinical events and their\ntemporal relations using the well-studied I2B2 2012 Temporal Relations\nChallenge corpus. This task is inherently challenging due to complex clinical\nlanguage, long documents, and sparse annotations. We introduce GRAPHTREX, a\nnovel method integrating span-based entity-relation extraction, clinical large\npre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)\nto capture local and global dependencies. Our HGT component facilitates\ninformation propagation across the document through innovative global landmarks\nthat bridge distant entities. Our method improves the state-of-the-art with\n5.5% improvement in the tempeval $F_1$ score over the previous best and up to\n8.9% improvement on long-range relations, which presents a formidable\nchallenge. This work not only advances temporal information extraction but also\nlays the groundwork for improved diagnostic and prognostic models through\nenhanced temporal reasoning.",
      "tldr_zh": "该研究针对临床文本中的时间关系提取问题，提出了一种基于跨度的图变换器方法GRAPHTREX，利用临床预训练语言模型(LPLMs)和Heterogeneous Graph Transformers (HGT)来捕获本地和全局依赖，通过全局地标桥接远距离实体。方法结合了基于跨度的实体-关系提取技术，应用于I2B2 2012 Temporal Relations Challenge语料库，以应对复杂的临床语言和稀疏标注的挑战。实验结果显示，GRAPHTREX在tempeval F1分数上比现有最佳方法提高了5.5%，在长距离关系上提升了8.9%，从而为增强时间推理和改进医疗诊断预后模型奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Introducing a novel method for joint extraction of medical events and\n  temporal relations from free-text, leveraging clinical LPLMs and\n  Heterogeneous Graph Transformers, achieving a 5.5% improvement over the\n  previous state-of-the-art and up to 8.9% on long-range relations",
      "pdf_url": "http://arxiv.org/pdf/2503.18085v1",
      "published_date": "2025-03-23 14:34:49 UTC",
      "updated_date": "2025-03-23 14:34:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:16:09.642615"
    },
    {
      "arxiv_id": "2503.18072v1",
      "title": "On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish",
      "title_zh": "LLMs 在西班牙语开放式问题自动评分中的有效性",
      "authors": [
        "Germán Capdehourat",
        "Isabel Amigo",
        "Brian Lorenzo",
        "Joaquín Trigo"
      ],
      "abstract": "Grading is a time-consuming and laborious task that educators must face. It\nis an important task since it provides feedback signals to learners, and it has\nbeen demonstrated that timely feedback improves the learning process. In recent\nyears, the irruption of LLMs has shed light on the effectiveness of automatic\ngrading. In this paper, we explore the performance of different LLMs and\nprompting techniques in automatically grading short-text answers to open-ended\nquestions. Unlike most of the literature, our study focuses on a use case where\nthe questions, answers, and prompts are all in Spanish. Experimental results\ncomparing automatic scores to those of human-expert evaluators show good\noutcomes in terms of accuracy, precision and consistency for advanced LLMs,\nboth open and proprietary. Results are notably sensitive to prompt styles,\nsuggesting biases toward certain words or content in the prompt. However, the\nbest combinations of models and prompt strategies, consistently surpasses an\naccuracy of 95% in a three-level grading task, which even rises up to more than\n98% when the it is simplified to a binary right or wrong rating problem, which\ndemonstrates the potential that LLMs have to implement this type of automation\nin education applications.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)在自动评分西班牙语开放式问题短文本答案的有效性，旨在解决教育中评分耗时的问题并提供及时反馈。研究者比较了不同LLMs和提示技术的性能，通过实验将自动评分结果与人类专家评分对比。结果显示，高级LLMs在准确率、精确度和一致性方面表现出色，最佳组合在三水平评分任务中准确率超过95%，在二元正确或错误评分中超过98%。然而，提示风格对结果敏感，可能引入偏差，这突显了LLMs在教育自动化应用中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18072v1",
      "published_date": "2025-03-23 13:43:27 UTC",
      "updated_date": "2025-03-23 13:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:16:20.829877"
    },
    {
      "arxiv_id": "2503.21797v1",
      "title": "A Novel Two-Phase Cooperative Co-evolution Framework for Large-Scale Global Optimization with Complex Overlapping",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjie Qiu",
        "Hongshu Guo",
        "Zeyuan Ma",
        "Yue-Jiao Gong"
      ],
      "abstract": "Cooperative Co-evolution, through the decomposition of the problem space, is\na primary approach for solving large-scale global optimization problems.\nTypically, when the subspaces are disjoint, the algorithms demonstrate\nsignificantly both effectiveness and efficiency compared to non-decomposition\nalgorithms. However, the presence of overlapping variables complicates the\ndecomposition process and adversely affects the performance of cooperative\nco-evolution. In this study, we propose a novel two-phase cooperative\nco-evolution framework to address large-scale global optimization problems with\ncomplex overlapping. An effective method for decomposing overlapping problems,\ngrounded in their mathematical properties, is embedded within the framework.\nAdditionally, a customizable benchmark for overlapping problems is introduced\nto extend existing benchmarks and facilitate experimentation. Extensive\nexperiments demonstrate that the algorithm instantiated within our framework\nsignificantly outperforms existing algorithms. The results reveal the\ncharacteristics of overlapping problems and highlight the differing strengths\nof cooperative co-evolution and non-decomposition algorithms. Our work is\nopen-source and accessible at: https://github.com/GMC-DRL/HCC.",
      "tldr_zh": "本研究提出了一种新的两阶段 Cooperative Co-evolution 框架，用于处理大型全局优化问题中的复杂重叠变量问题，该框架通过问题空间分解提升算法的效率和效果。框架中嵌入了一个基于数学属性的有效分解方法，并引入了一个可定制的重叠问题基准，以扩展现有基准并便于实验。实验结果表明，该框架下的算法显著优于现有算法，并揭示了重叠问题的特性，为优化领域的研究提供了新洞见。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at ACM GECCO 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21797v1",
      "published_date": "2025-03-23 13:21:11 UTC",
      "updated_date": "2025-03-23 13:21:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:16:32.598920"
    },
    {
      "arxiv_id": "2503.18065v1",
      "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziming Wei",
        "Bingqian Lin",
        "Yunshuang Nie",
        "Jiaqi Chen",
        "Shikui Ma",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "abstract": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.",
      "tldr_zh": "这篇论文针对 Vision-Language Navigation (VLN) 的数据稀缺问题，提出 Rewriting-driven AugMentation (RAM) 范式，通过重写人类标注的训练数据来生成未见环境的 observation-instruction 对，从而提升模型的泛化能力。具体方法包括 Object-Enriched Observation Rewriting（利用 Vision-Language Models (VLMs) 和 Large Language Models (LLMs) 结合 Text-to-Image Generation Models (T2IMs) 生成对象丰富的场景描述）和 Observation-Contrast Instruction Rewriting（通过 LLMs 分析观察差异生成新指令），并采用 mixing-then-focusing 训练策略抑制噪声。实验在 R2R、REVERIE、R4R 和 R2R-CE 数据集上证明了该方法的优越性能和泛化潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18065v1",
      "published_date": "2025-03-23 13:18:17 UTC",
      "updated_date": "2025-03-23 13:18:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:16:46.615543"
    },
    {
      "arxiv_id": "2503.18063v1",
      "title": "Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning",
      "title_zh": "动态任务向量分组用于高效多任务提示微调",
      "authors": [
        "Pieyi Zhang",
        "Richong Zhang",
        "Zhijie Nie"
      ],
      "abstract": "Multi-task prompt tuning utilizes multiple high-resource source tasks to\nimprove performance on low-source target tasks. Existing approaches transfer\nthe soft prompt trained by combining all source tasks or a single\n``high-similar'' source task one-time-only. However, we find that the optimal\ntransfer performance often comes from a combination of source tasks, which is\nneither one nor all. Further, we find that the similarity between source and\ntarget tasks also changes dynamically during fine-tuning after transfering,\nmaking similarity calculation in the initiation stage inadequate. To address\nthese issues, we propose a method called Dynamic Task Vector Grouping (DTVG),\nwhose core ideas contain (1) measuring the task similarity with task vectors\ninstead of soft prompt, (2) grouping the optimal source task combination based\non two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3)\ndynamically updating the combination in each iteration step. Extensive\nexperiments on the 26 NLP datasets under different settings demonstrate that\nDTVG effectively groups similar source tasks while reducing negative transfer,\nachieving the start-of-art performance.",
      "tldr_zh": "该论文提出Dynamic Task Vector Grouping (DTVG)方法，用于优化多任务提示调整(Multi-Task Prompt Tuning)，以提升低资源目标任务的性能。DTVG的核心在于使用任务向量测量任务相似性、基于目标相似性和知识一致性动态分组最优源任务组合，并在每个迭代步骤中更新组合，从而解决现有方法中源任务转移的局限性。实验在26个NLP数据集上证明，DTVG能有效识别相似源任务、减少负面转移，并实现最先进性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.18063v1",
      "published_date": "2025-03-23 13:09:04 UTC",
      "updated_date": "2025-03-23 13:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:16:56.263410"
    },
    {
      "arxiv_id": "2504.03690v1",
      "title": "Learning to Interfere in Non-Orthogonal Multiple-Access Joint Source-Channel Coding",
      "title_zh": "翻译失败",
      "authors": [
        "Selim F. Yilmaz",
        "Can Karamanli",
        "Deniz Gunduz"
      ],
      "abstract": "We consider multiple transmitters aiming to communicate their source signals\n(e.g., images) over a multiple access channel (MAC). Conventional communication\nsystems minimize interference by orthogonally allocating resources (time and/or\nbandwidth) among users, which limits their capacity. We introduce a machine\nlearning (ML)-aided wireless image transmission method that merges compression\nand channel coding using a multi-view autoencoder, which allows the\ntransmitters to use all the available channel resources simultaneously,\nresulting in a non-orthogonal multiple access (NOMA) scheme. The receiver must\nrecover all the images from the received superposed signal, while also\nassociating each image with its transmitter. Traditional ML models deal with\nindividual samples, whereas our model allows signals from different users to\ninterfere in order to leverage gains from NOMA under limited bandwidth and\npower constraints. We introduce a progressive fine-tuning algorithm that\ndoubles the number of users at each iteration, maintaining initial performance\nwith orthogonalized user-specific projections, which is then improved through\nfine-tuning steps. Remarkably, our method scales up to 16 users and beyond,\nwith only a 0.6% increase in the number of trainable parameters compared to a\nsingle-user model, significantly enhancing recovered image quality and\noutperforming existing NOMA-based methods over a wide range of datasets,\nmetrics, and channel conditions. Our approach paves the way for more efficient\nand robust multi-user communication systems, leveraging innovative ML\ncomponents and strategies.",
      "tldr_zh": "这篇论文提出了一种基于机器学习的无线图像传输方法，使用多视图自编码器(multi-view autoencoder)来合并源编码和信道编码，实现非正交多址接入(NOMA)，允许多个发射机同时使用信道资源，从而在带宽和功率限制下最大化容量。\n该方法通过一个渐进微调算法，每次迭代双倍用户数，从正交化用户特定投影开始，然后通过微调优化，成功处理多达16个用户以上。\n实验显示，与传统正交方法相比，该方法显著提升了图像恢复质量，仅增加0.6%的训练参数，并在多种数据集、指标和信道条件下优于现有NOMA方案，为高效、鲁棒的多用户通信系统提供了创新途径。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.NI",
      "comment": "18 pages, 19 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.03690v1",
      "published_date": "2025-03-23 12:27:20 UTC",
      "updated_date": "2025-03-23 12:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:17:10.490562"
    },
    {
      "arxiv_id": "2503.18025v1",
      "title": "Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration",
      "title_zh": "基于次优分类器的决策：校准前后过量风险",
      "authors": [
        "Alexandre Perez-Lebel",
        "Gael Varoquaux",
        "Sanmi Koyejo",
        "Matthieu Doutreligne",
        "Marine Le Morvan"
      ],
      "abstract": "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
      "tldr_zh": "这篇论文研究了使用子优概率分类器在不确定性决策中的过量风险（excess risk），通过量化基于后验概率近似的 regret（遗憾），分析了校准前后的影响。作者提供了 miscalibration-induced regret ($R^{\\mathrm{CL}}$) 的分析表达式，以及 calibrated classifiers 的 regret ($R^{\\mathrm{GL}}$) 的紧密上界和下界，这些表达式有助于识别校准是否能解决大部分 regret，或是否需处理主导 regret 的 grouping loss。实验结果显示，在 NLP 任务中，这些量度能有效评估更高级后训练（如 multicalibration 方法）的潜在收益，作为高效替代微调的选项。总的来说，该工作为优化决策规则提供了实用工具，提升了分类器的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18025v1",
      "published_date": "2025-03-23 10:52:36 UTC",
      "updated_date": "2025-03-23 10:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:17:23.194365"
    },
    {
      "arxiv_id": "2504.08742v1",
      "title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents",
      "title_zh": "使用大型语言模型代理模拟短视频推荐系统上的过滤气泡",
      "authors": [
        "Nicholas Sukiennik",
        "Haoyu Wang",
        "Zailin Zeng",
        "Chen Gao",
        "Yong Li"
      ],
      "abstract": "An increasing reliance on recommender systems has led to concerns about the\ncreation of filter bubbles on social media, especially on short video platforms\nlike TikTok. However, their formation is still not entirely understood due to\nthe complex dynamics between recommendation algorithms and user feedback. In\nthis paper, we aim to shed light on these dynamics using a large language\nmodel-based simulation framework. Our work employs real-world short-video data\ncontaining rich video content information and detailed user-agents to\nrealistically simulate the recommendation-feedback cycle. Through large-scale\nsimulations, we demonstrate that LLMs can replicate real-world user-recommender\ninteractions, uncovering key mechanisms driving filter bubble formation. We\nidentify critical factors, such as demographic features and category attraction\nthat exacerbate content homogenization. To mitigate this, we design and test\ninterventions including various cold-start and feedback weighting strategies,\nshowing measurable reductions in filter bubble effects. Our framework enables\nrapid prototyping of recommendation strategies, offering actionable solutions\nto enhance content diversity in real-world systems. Furthermore, we analyze how\nLLM-inherent biases may propagate through recommendations, proposing safeguards\nto promote equity for vulnerable groups, such as women and low-income\npopulations. By examining the interplay between recommendation and LLM agents,\nthis work advances a deeper understanding of algorithmic bias and provides\npractical tools to promote inclusive digital spaces.",
      "tldr_zh": "本文使用大型语言模型（LLMs）代理构建模拟框架，基于真实短视频数据再现用户-推荐器互动，以揭示过滤气泡（filter bubble）在短视频平台（如TikTok）中的形成机制。研究通过大规模模拟识别了关键因素，如人口统计特征和类别吸引力，这些因素加剧了内容同质化，并设计了冷启动和反馈加权策略等干预措施，显著降低了过滤气泡效果。最终，该框架支持快速原型化推荐策略，并分析了LLM固有偏差的传播，提供工具来促进算法公平性和包容性数字空间。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Submitted to IJCAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.08742v1",
      "published_date": "2025-03-23 10:35:58 UTC",
      "updated_date": "2025-03-23 10:35:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:17:33.922130"
    },
    {
      "arxiv_id": "2503.18018v1",
      "title": "Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?",
      "title_zh": "翻译失败",
      "authors": [
        "Aabid Karim",
        "Abdul Karim",
        "Bhoomika Lohana",
        "Matt Keon",
        "Jaswinder Singh",
        "Abdul Sattar"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced various fields,\nparticularly coding, mathematical reasoning, and logical problem solving.\nHowever, a critical question remains: Do these mathematical reasoning abilities\npersist when LLMs are presented with culturally adapted math problems?\nSpecifically, how do LLMs perform when faced with math problems embedded in\ncultural contexts that have no significant representation in main stream\nweb-scale AI training data? To explore this, we generated six synthetic\ncultural datasets from GSM8K, a widely used benchmark for assessing LLMs'\nmathematical reasoning skills. While preserving the mathematical logic and\nnumerical values of the original GSM8K test set, we modify cultural elements\nsuch as personal names, food items, place names, etc. These culturally adapted\ndatasets provide a more reliable framework for evaluating LLMs' mathematical\nreasoning under shifting cultural contexts. Our findings reveal that LLMs\nstruggle with math problems when cultural references change, even though the\nunderlying mathematical structure remains constant. Smaller models exhibit\ngreater performance drops compared to larger models. Interestingly, our results\nalso suggest that cultural familiarity can enhance mathematical reasoning. Even\nmodels with no explicit mathematical training but exposure to relevant cultural\ncontexts sometimes outperform larger, mathematically proficient models on\nculturally embedded math problems. This study highlights the impact of cultural\ncontext on the mathematical reasoning abilities of LLMs, underscoring the need\nfor more diverse and representative training data to improve robustness in\nreal-world applications. The benchmark data sets and script for reproducing the\nresults are available at\nhttps://github.com/akarim23131/Lost_in_Cultural_Translation",
      "tldr_zh": "本文研究了大型语言模型（LLMs）在文化适应数学问题中的表现，生成六个基于 GSM8K 基准的合成文化数据集，通过修改文化元素（如人名、食物和地点）来保持数学逻辑不变。结果表明，LLMs 在文化引用变化时数学推理能力显著下降，较小模型的性能下降更明显，而文化熟悉度能提升模型的表现，甚至使某些非数学训练模型优于更大模型。该研究强调了文化上下文对 LLMs 鲁棒性的影响，并呼吁采用更多样化的训练数据，同时提供了可复现的基准数据集。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18018v1",
      "published_date": "2025-03-23 10:35:39 UTC",
      "updated_date": "2025-03-23 10:35:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:17:44.686298"
    },
    {
      "arxiv_id": "2503.18013v1",
      "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yufei Zhan",
        "Yousong Zhu",
        "Shurong Zheng",
        "Hongyin Zhao",
        "Fan Yang",
        "Ming Tang",
        "Jinqiao Wang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
      "tldr_zh": "该论文提出Vision-R1，一种基于视觉引导强化学习(Reinforcement Learning)的算法，用于实现大型视觉语言模型(Large Vision-Language Models, LVLMs)的无人类干预对齐训练，从而避免了构建高质量偏好数据和奖励模型的成本。Vision-R1利用视觉反馈作为奖励机制，结合标准驱动的奖励函数和多维评估标准，来全面评估模型输出。论文还引入渐进式规则精炼策略，在训练过程中动态调整奖励标准，以促进模型持续改进并防止奖励操纵。实验结果显示，在各种分布内和分布外基准测试中，使用7B LVLMs模型经Vision-R1微调后，性能提升高达50%，甚至超越了10倍大小的现有最先进模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project in development. Github:\n  https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1",
      "pdf_url": "http://arxiv.org/pdf/2503.18013v1",
      "published_date": "2025-03-23 10:21:14 UTC",
      "updated_date": "2025-03-23 10:21:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:17:57.471387"
    },
    {
      "arxiv_id": "2503.18988v1",
      "title": "SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene Graph Manipulation",
      "title_zh": "SG-Tailor：对象间常识关系推理用于场景图操作",
      "authors": [
        "Haoliang Shang",
        "Hanyu Wu",
        "Guangyao Zhai",
        "Boyang Sun",
        "Fangjinhua Wang",
        "Federico Tombari",
        "Marc Pollefeys"
      ],
      "abstract": "Scene graphs capture complex relationships among objects, serving as strong\npriors for content generation and manipulation. Yet, reasonably manipulating\nscene graphs -- whether by adding nodes or modifying edges -- remains a\nchallenging and untouched task. Tasks such as adding a node to the graph or\nreasoning about a node's relationships with all others are computationally\nintractable, as even a single edge modification can trigger conflicts due to\nthe intricate interdependencies within the graph. To address these challenges,\nwe introduce SG-Tailor, an autoregressive model that predicts the conflict-free\nrelationship between any two nodes. SG-Tailor not only infers inter-object\nrelationships, including generating commonsense edges for newly added nodes but\nalso resolves conflicts arising from edge modifications to produce coherent,\nmanipulated graphs for downstream tasks. For node addition, the model queries\nthe target node and other nodes from the graph to predict the appropriate\nrelationships. For edge modification, SG-Tailor employs a Cut-And-Stitch\nstrategy to solve the conflicts and globally adjust the graph. Extensive\nexperiments demonstrate that SG-Tailor outperforms competing methods by a large\nmargin and can be seamlessly integrated as a plug-in module for scene\ngeneration and robotic manipulation tasks.",
      "tldr_zh": "该论文提出 SG-Tailor，一种自回归模型，用于处理场景图（scene graphs）的操纵问题，旨在解决添加节点或修改边时可能引发的冲突和计算不可行性。SG-Tailor 通过预测节点间无冲突的常识关系（commonsense relationships），包括为新节点生成适当边，并采用 Cut-And-Stitch 策略全局调整图结构，以确保图的连贯性。实验表明，该模型在节点添加和边修改任务上大幅优于竞争方法，并可作为插件模块无缝集成到场景生成和机器人操纵任务中。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "The code will be available at https://github.com/josef5838/SG-Tailor",
      "pdf_url": "http://arxiv.org/pdf/2503.18988v1",
      "published_date": "2025-03-23 09:11:04 UTC",
      "updated_date": "2025-03-23 09:11:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:18:10.452359"
    },
    {
      "arxiv_id": "2503.17994v1",
      "title": "Instructing the Architecture Search for Spatial-temporal Sequence Forecasting with LLM",
      "title_zh": "利用 LLM 指导时空序列预测的架构搜索",
      "authors": [
        "Xin Xue",
        "Haoyi Zhou",
        "Tianyu Chen",
        "Shuai Zhang",
        "Yizhou Long",
        "Jianxin Li"
      ],
      "abstract": "Spatial-temporal sequence forecasting (STSF) is a long-standing research\nproblem with widespread real-world applications. Neural architecture search\n(NAS), which automates the neural network design, has been shown effective in\ntackling the STSF problem. However, the existing NAS methods for STSF focus on\ngenerating architectures in a time-consuming data-driven fashion, which heavily\nlimits their ability to use background knowledge and explore the complicated\nsearch trajectory. Large language models (LLMs) have shown remarkable ability\nin decision-making with comprehensive internal world knowledge, but how it\ncould benefit NAS for STSF remains unexplored. In this paper, we propose a\nnovel NAS method for STSF based on LLM. Instead of directly generate\narchitectures with LLM, We inspire the LLM's capability with a multi-level\nenhancement mechanism. Specifically, on the step-level, we decompose the\ngeneration task into decision steps with powerful prompt engineering and\ninspire LLM to serve as instructor for architecture search based on its\ninternal knowledge. On the instance-level, we utilize a one-step tuning\nframework to quickly evaluate the architecture instance and a memory bank to\ncumulate knowledge to improve LLM's search ability. On the task-level, we\npropose a two-stage architecture search, balancing the exploration stage and\noptimization stage, to reduce the possibility of being trapped in local optima.\nExtensive experimental results demonstrate that our method can achieve\ncompetitive effectiveness with superior efficiency against existing NAS methods\nfor STSF.",
      "tldr_zh": "该论文提出了一种基于大型语言模型(LLMs)的神经架构搜索(NAS)方法，用于空间-时间序列预测(Spatial-temporal sequence forecasting, STSF)，旨在解决现有NAS方法依赖数据驱动且耗时的局限性。方法通过多级增强机制，包括步骤级的提示工程将任务分解为决策步骤让LLMs充当指导者、实例级的快速调优框架和记忆银行积累知识，以及任务级的两阶段搜索平衡探索与优化以避免局部最优。实验结果显示，该方法在STSF任务上实现了与现有NAS方法相当的有效性，同时显著提高了效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17994v1",
      "published_date": "2025-03-23 08:59:04 UTC",
      "updated_date": "2025-03-23 08:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:18:21.772201"
    },
    {
      "arxiv_id": "2503.17993v1",
      "title": "Predicting Multitasking in Manual and Automated Driving with Optimal Supervisory Control",
      "title_zh": "翻译失败",
      "authors": [
        "Jussi Jokinen",
        "Patrick Ebel",
        "Tuomo Kujala"
      ],
      "abstract": "Modern driving involves interactive technologies that can divert attention,\nincreasing the risk of accidents. This paper presents a computational cognitive\nmodel that simulates human multitasking while driving. Based on optimal\nsupervisory control theory, the model predicts how multitasking adapts to\nvariations in driving demands, interactive tasks, and automation levels. Unlike\nprevious models, it accounts for context-dependent multitasking across\ndifferent degrees of driving automation. The model predicts longer in-car\nglances on straight roads and shorter glances during curves. It also\nanticipates increased glance durations with driver aids such as lane-centering\nassistance and their interaction with environmental demands. Validated against\ntwo empirical datasets, the model offers insights into driver multitasking amid\nevolving in-car technologies and automation.",
      "tldr_zh": "这篇论文提出一个基于最优监督控制理论的计算认知模型，用于预测手动和自动驾驶中的多任务行为。该模型模拟人类如何适应驾驶需求、互动任务和自动化水平的变异，预测出在直路时视线停留时间较长、在弯路时较短，并显示驾驶辅助如车道居中辅助会增加视线时长。模型通过两个实证数据集验证，提供对驾驶员多任务行为的洞见，帮助理解不断演变的车内技术和自动化环境。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "H.1.2"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17993v1",
      "published_date": "2025-03-23 08:56:53 UTC",
      "updated_date": "2025-03-23 08:56:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:18:32.710363"
    },
    {
      "arxiv_id": "2503.17987v2",
      "title": "Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Zhang",
        "Lanjun Wang",
        "Yiwen Ma",
        "Wenhui Li",
        "An-An Liu"
      ],
      "abstract": "Text-to-Image(T2I) models typically deploy safety filters to prevent the\ngeneration of sensitive images. Unfortunately, recent jailbreaking attack\nmethods manually design prompts for the LLM to generate adversarial prompts,\nwhich effectively bypass safety filters while producing sensitive images,\nexposing safety vulnerabilities of T2I models. However, due to the LLM's\nlimited understanding of the T2I model and its safety filters, existing methods\nrequire numerous queries to achieve a successful attack, limiting their\npractical applicability. To address this issue, we propose Reason2Attack(R2A),\nwhich aims to enhance the LLM's reasoning capabilities in generating\nadversarial prompts by incorporating the jailbreaking attack into the\npost-training process of the LLM. Specifically, we first propose a CoT example\nsynthesis pipeline based on Frame Semantics, which generates adversarial\nprompts by identifying related terms and corresponding context illustrations.\nUsing CoT examples generated by the pipeline, we fine-tune the LLM to\nunderstand the reasoning path and format the output structure. Subsequently, we\nincorporate the jailbreaking attack task into the reinforcement learning\nprocess of the LLM and design an attack process reward that considers prompt\nlength, prompt stealthiness, and prompt effectiveness, aiming to further\nenhance reasoning accuracy. Extensive experiments on various T2I models show\nthat R2A achieves a better attack success ratio while requiring fewer queries\nthan baselines. Moreover, our adversarial prompts demonstrate strong attack\ntransferability across both open-source and commercial T2I models.",
      "tldr_zh": "本文提出 Reason2Attack (R2A)，一种通过增强 LLM 推理能力来越狱 Text-to-Image (T2I) 模型的方法，旨在解决现有攻击方法需要大量查询的问题。R2A 包括基于 Frame Semantics 的 Chain-of-Thought (CoT) 示例合成管道，用于生成对抗提示，并通过 LLM 的微调和强化学习过程优化提示的长度、隐蔽性和有效性。实验结果显示，R2A 在多种 T2I 模型上实现了更高的攻击成功率、减少了查询次数，并证明了对抗提示的强转移性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper includes model-generated content that may contain\n  offensive or distressing material",
      "pdf_url": "http://arxiv.org/pdf/2503.17987v2",
      "published_date": "2025-03-23 08:40:39 UTC",
      "updated_date": "2025-04-19 07:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:18:45.762376"
    },
    {
      "arxiv_id": "2503.17985v1",
      "title": "Optimizing Navigation And Chemical Application in Precision Agriculture With Deep Reinforcement Learning And Conditional Action Tree",
      "title_zh": "翻译失败",
      "authors": [
        "Mahsa Khosravi",
        "Zhanhong Jiang",
        "Joshua R Waite",
        "Sarah Jonesc",
        "Hernan Torres",
        "Arti Singh",
        "Baskar Ganapathysubramanian",
        "Asheesh Kumar Singh",
        "Soumik Sarkar"
      ],
      "abstract": "This paper presents a novel reinforcement learning (RL)-based planning scheme\nfor optimized robotic management of biotic stresses in precision agriculture.\nThe framework employs a hierarchical decision-making structure with conditional\naction masking, where high-level actions direct the robot's exploration, while\nlow-level actions optimize its navigation and efficient chemical spraying in\naffected areas. The key objectives of optimization include improving the\ncoverage of infected areas with limited battery power and reducing chemical\nusage, thus preventing unnecessary spraying of healthy areas of the field. Our\nnumerical experimental results demonstrate that the proposed method,\nHierarchical Action Masking Proximal Policy Optimization (HAM-PPO),\nsignificantly outperforms baseline practices, such as LawnMower navigation +\nindiscriminate spraying (Carpet Spray), in terms of yield recovery and resource\nefficiency. HAM-PPO consistently achieves higher yield recovery percentages and\nlower chemical costs across a range of infection scenarios. The framework also\nexhibits robustness to observation noise and generalizability under diverse\nenvironmental conditions, adapting to varying infection ranges and spatial\ndistribution patterns.",
      "tldr_zh": "本论文提出了一种基于强化学习 (RL) 的规划方案，用于精确农业中机器人优化导航和化学喷洒，以管理生物胁迫。该框架采用分层决策结构和条件动作掩码，高层动作指导机器人探索，低层动作优化覆盖受感染区域，同时减少化学品使用和电池消耗。实验结果表明，Hierarchical Action Masking Proximal Policy Optimization (HAM-PPO) 方法在产量恢复和资源效率上显著优于基线方法（如 LawnMower 导航 + 随意喷洒），并显示出对观察噪声的鲁棒性和在不同感染场景中的通用性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "32 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17985v1",
      "published_date": "2025-03-23 08:38:13 UTC",
      "updated_date": "2025-03-23 08:38:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:18:57.430703"
    },
    {
      "arxiv_id": "2503.17984v1",
      "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting",
      "title_zh": "翻译失败",
      "authors": [
        "Maochen Yang",
        "Zekun Li",
        "Jian Zhang",
        "Lei Qi",
        "Yinghuan Shi"
      ],
      "abstract": "Semi-supervised crowd counting is crucial for addressing the high annotation\ncosts of densely populated scenes. Although several methods based on\npseudo-labeling have been proposed, it remains challenging to effectively and\naccurately utilize unlabeled data. In this paper, we propose a novel framework\ncalled Taste More Taste Better (TMTB), which emphasizes both data and model\naspects. Firstly, we explore a data augmentation technique well-suited for the\ncrowd counting task. By inpainting the background regions, this technique can\neffectively enhance data diversity while preserving the fidelity of the entire\nscenes. Secondly, we introduce the Visual State Space Model as backbone to\ncapture the global context information from crowd scenes, which is crucial for\nextremely crowded, low-light, and adverse weather scenarios. In addition to the\ntraditional regression head for exact prediction, we employ an Anti-Noise\nclassification head to provide less exact but more accurate supervision, since\nthe regression head is sensitive to noise in manual annotations. We conduct\nextensive experiments on four benchmark datasets and show that our method\noutperforms state-of-the-art methods by a large margin. Code is publicly\navailable on https://github.com/syhien/taste_more_taste_better.",
      "tldr_zh": "本论文提出了一种名为 Taste More Taste Better (TMTB) 的框架，用于提升 Semi-Supervised Crowd Counting 的性能，旨在解决高标注成本和无标签数据利用效率低的问题。该框架通过背景区域填充的数据增强技术增加数据多样性，同时保持场景真实性，并采用 Visual State Space Model 作为主干网络，以捕捉人群场景中的全局上下文信息，适用于极度拥挤、低光照或恶劣天气条件。此外，TMTB 结合传统的回归头和 Anti-Noise 分类头，提供更准确的监督机制，以减少噪声影响。实验在四个基准数据集上显示，该方法大幅优于现有状态-of-the-art 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17984v1",
      "published_date": "2025-03-23 08:38:01 UTC",
      "updated_date": "2025-03-23 08:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:19:09.746020"
    },
    {
      "arxiv_id": "2503.17982v1",
      "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images",
      "title_zh": "Co-SemDepth：针对航空图像的快速联合语义分割和深度估计",
      "authors": [
        "Yara AlaaEldin",
        "Francesca Odone"
      ],
      "abstract": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth",
      "tldr_zh": "该论文针对无人机（UAV）自主导航中的挑战，提出Co-SemDepth框架，利用单目摄像头实现快速的联合语义分割（Semantic Segmentation）和深度估计（Depth Estimation），以实时理解低空非结构化环境的几何和语义属性。该框架采用一个高效的深度学习架构，能够在MidAir和Aeroscapes基准数据集上表现出色，与其他单任务或联合方法相比，准确性更强。实验结果显示，它在单NVIDIA Quadro P5000 GPU上达到20.2 FPS的预测速度，同时保持低内存占用，为实际UAV导航应用提供了可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17982v1",
      "published_date": "2025-03-23 08:25:07 UTC",
      "updated_date": "2025-03-23 08:25:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:19:20.564047"
    },
    {
      "arxiv_id": "2503.18987v1",
      "title": "Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Xiran Wang",
        "Jian Zhang",
        "Lei Qi",
        "Yinghuan Shi"
      ],
      "abstract": "Domain generalization is proposed to address distribution shift, arising from\nstatistical disparities between training source and unseen target domains. The\nwidely used first-order meta-learning algorithms demonstrate strong performance\nfor domain generalization by leveraging the gradient matching theory, which\naims to establish balanced parameters across source domains to reduce\noverfitting to any particular domain. However, our analysis reveals that there\nare actually numerous directions to achieve gradient matching, with current\nmethods representing just one possible path. These methods actually overlook\nanother critical factor that the balanced parameters should be close to the\ncentroid of optimal parameters of each source domain. To address this, we\npropose a simple yet effective arithmetic meta-learning with\narithmetic-weighted gradients. This approach, while adhering to the principles\nof gradient matching, promotes a more precise balance by estimating the\ncentroid between domain-specific optimal parameters. Experimental results\nvalidate the effectiveness of our strategy.",
      "tldr_zh": "本论文针对领域泛化（Domain Generalization）问题，解决训练源域和未见目标域之间的分布偏移（distribution shift）。作者分析发现，现有第一阶元学习算法虽依赖梯度匹配（gradient matching）来平衡参数并减少过拟合，但忽略了平衡参数应接近各源域最优参数的中心点（centroid）。为此，提出算术元学习（arithmetic meta-learning），通过arithmetic-weighted gradients精确估计这些中心点，同时遵守梯度匹配原则。实验结果验证了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18987v1",
      "published_date": "2025-03-23 08:24:28 UTC",
      "updated_date": "2025-03-23 08:24:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:19:34.429151"
    },
    {
      "arxiv_id": "2503.17979v1",
      "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Weixiang Zhao",
        "Xingyu Sui",
        "Jiahe Guo",
        "Yulin Hu",
        "Yang Deng",
        "Yanyan Zhao",
        "Bing Qin",
        "Wanxiang Che",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "abstract": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics.",
      "tldr_zh": "本研究通过实证分析Large Reasoning Models (LRMs)，探讨了deliberative reasoning（如人类般的思考和长chain-of-thought reasoning）与模型基础能力的权衡，评估了DeepSeek、Qwen和LLaMA等模型家族（规模从7B到671B）。结果显示，获得deliberative reasoning能力会显著降低LRMs的helpfulness和harmlessness，同时大幅增加推理成本。作者证明，adaptive reasoning模式（如Zero-Thinking、Less-Thinking和Summary-Thinking）能有效缓解这些缺点，并强调未来需开发更通用的LRMs，以根据任务特性动态分配推理计算资源。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages. Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.17979v1",
      "published_date": "2025-03-23 08:18:51 UTC",
      "updated_date": "2025-03-23 08:18:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:19:45.697175"
    },
    {
      "arxiv_id": "2503.17978v1",
      "title": "PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition",
      "title_zh": "PIM：基于物理学的多任务预训练，用于改进基于惯性传感器的人类活动识别",
      "authors": [
        "Dominique Nshimyimana",
        "Vitor Fortes Rey",
        "Sungho Suh",
        "Bo Zhou",
        "Paul Lukowicz"
      ],
      "abstract": "Human activity recognition (HAR) with deep learning models relies on large\namounts of labeled data, often challenging to obtain due to associated cost,\ntime, and labor. Self-supervised learning (SSL) has emerged as an effective\napproach to leverage unlabeled data through pretext tasks, such as masked\nreconstruction and multitask learning with signal processing-based data\naugmentations, to pre-train encoder models. However, such methods are often\nderived from computer vision approaches that disregard physical mechanisms and\nconstraints that govern wearable sensor data and the phenomena they reflect. In\nthis paper, we propose a physics-informed multi-task pre-training (PIM)\nframework for IMU-based HAR. PIM generates pre-text tasks based on the\nunderstanding of basic physical aspects of human motion: including movement\nspeed, angles of movement, and symmetry between sensor placements. Given a\nsensor signal, we calculate corresponding features using physics-based\nequations and use them as pretext tasks for SSL. This enables the model to\ncapture fundamental physical characteristics of human activities, which is\nespecially relevant for multi-sensor systems. Experimental evaluations on four\nHAR benchmark datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods, including data augmentation and masked\nreconstruction, in terms of accuracy and F1 score. We have observed gains of\nalmost 10\\% in macro f1 score and accuracy with only 2 to 8 labeled examples\nper class and up to 3% when there is no reduction in the amount of training\ndata.",
      "tldr_zh": "这篇论文提出了一种基于物理信息的多任务预训练框架 PIM，用于提升基于惯性测量单元 (IMU) 的人类活动识别 (HAR)。PIM 通过利用人类运动的物理方面（如运动速度、角度和传感器放置对称性）生成预文本任务，并结合自监督学习 (SSL) 来捕捉活动的核心物理特性，从而解决现有方法忽略物理机制的问题。在四个 HAR 基准数据集上的实验显示，PIM 比数据增强和掩码重建等状态-of-the-art 方法在准确性和 F1 分数上表现出色，尤其在每个类别仅有 2 到 8 个标记样本时，宏 F1 分数和准确率提高了近 10%，而在完整数据时提升高达 3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17978v1",
      "published_date": "2025-03-23 08:16:01 UTC",
      "updated_date": "2025-03-23 08:16:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:19:57.877305"
    },
    {
      "arxiv_id": "2503.17975v2",
      "title": "Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhi Li",
        "Haojun Xu",
        "Feng Tian"
      ],
      "abstract": "With the rising popularity of short video platforms, the demand for video\nproduction has increased substantially. However, high-quality video creation\ncontinues to rely heavily on professional editing skills and a nuanced\nunderstanding of visual language. To address this challenge, the Shot Sequence\nOrdering (SSO) task in AI-assisted video editing has emerged as a pivotal\napproach for enhancing video storytelling and the overall viewing experience.\nNevertheless, the progress in this field has been impeded by a lack of publicly\navailable benchmark datasets. In response, this paper introduces two novel\nbenchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ\nthe Kendall Tau distance as an evaluation metric for the SSO task and propose\nthe Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept\nof Cinematology Embedding, which incorporates movie metadata and shot labels as\nprior knowledge into the SSO model, and constructs the AVE-Meta dataset to\nvalidate the method's effectiveness. Experimental results indicate that the\nproposed loss function and method substantially enhance SSO task accuracy. All\ndatasets are publicly accessible at https://github.com/litchiar/ShotSeqBench.",
      "tldr_zh": "本论文针对视频编辑中的 Shot Sequence Ordering (SSO) 任务，解决短视频生产依赖专业技能的问题，并引入两个新基准数据集：AVE-Order 和 ActivityNet-Order，以推动该领域的进展。论文采用 Kendall Tau distance 作为评估指标，并提出 Kendall Tau Distance-Cross Entropy Loss，以优化模型性能；同时，引入 Cinematology Embedding 方法，将电影元数据和镜头标签作为先验知识融入 SSO 模型，并构建 AVE-Meta 数据集进行验证。实验结果表明，这些创新方法显著提高了任务准确性，所有数据集已在 GitHub 上公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17975v2",
      "published_date": "2025-03-23 08:04:45 UTC",
      "updated_date": "2025-03-25 11:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:20:08.937864"
    },
    {
      "arxiv_id": "2503.18986v1",
      "title": "SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Ma",
        "Xinchen Lyu",
        "Jun Jiang",
        "Qimei Cui",
        "Haipeng Yao",
        "Xiaofeng Tao"
      ],
      "abstract": "Fine-tuning large language models (LLMs) on private, on-device data can\nempower tailored personalized AI agents. However, fine-tuning LLMs on\nresource-constrained edge devices faces significant challenges, including\nexcessive computation overhead, device heterogeneity, and data imbalance. This\npaper proposes SplitFrozen, a split learning framework that enables efficient\nLLM fine-tuning by strategically freezing device-side model layers while\ncentralizing parameter-efficient fine-tuning on the server. Our framework\npartitions LLMs into device-side frozen layers and server-side fine-tuning\nlayers, where heterogeneous resource-constrained devices execute only forward\npropagation. To minimize server-side training costs, we integrate Low-Rank\nAdaptation (LoRA) into the server-side layers. A pipeline parallelism strategy\nfurther optimizes training efficiency by decoupling device-server computations\nand leveraging decomposed backward propagation. Experiments on GPT-2 with the\nMRPC, MNLI-matched, and SST-2 datasets demonstrate that SplitFrozen outperforms\nFedLoRA and SplitLoRA by 69.4\\% model accuracy under extremely imbalanced data,\nwhile reducing up to 86.8\\% device-side computations and 50.2\\% total training\ntime. Experiments also validate the scalability of SplitFrozen on content\ngeneration task using Llama-3.2 model on GSM8K dataset.",
      "tldr_zh": "该论文提出SplitFrozen框架，用于在异构资源受限设备上高效微调大型语言模型(LLMs)，以解决计算开销大、设备异构性和数据不平衡等问题。框架将LLMs分成设备端的冻结层（仅执行前向传播）和服务器端的微调层，并整合Low-Rank Adaptation (LoRA)来降低服务器训练成本，同时采用pipeline parallelism策略优化计算效率。实验结果显示，在GPT-2模型上使用MRPC、MNLI-matched和SST-2数据集时，SplitFrozen在极端数据不平衡场景下比FedLoRA和SplitLoRA提高了69.4%的模型准确率，并减少了86.8%的设备端计算和50.2%的总训练时间；此外，在Llama-3.2模型上用于GSM8K内容生成任务时，该框架也展示了良好的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18986v1",
      "published_date": "2025-03-23 08:03:44 UTC",
      "updated_date": "2025-03-23 08:03:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:20:22.367090"
    },
    {
      "arxiv_id": "2503.17973v1",
      "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Hanxiao Jiang",
        "Hao-Yu Hsu",
        "Kaifeng Zhang",
        "Hsin-Ni Yu",
        "Shenlong Wang",
        "Yunzhu Li"
      ],
      "abstract": "Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.",
      "tldr_zh": "本文提出 PhysTwin 框架，利用稀疏视频重建和模拟可变形物体的物理数字孪生，实现照片级和物理级真实感的实时交互虚拟副本。框架的核心包括 physics-informed 表征（结合 spring-mass models 进行物理模拟、generative shape models 建模几何，以及 Gaussian splats 进行渲染）和多阶段优化-based 逆向建模（从视频中重建完整几何、推断密集物理属性并复制外观）。实验结果表明，PhysTwin 在重建、渲染、未来预测和新型交互模拟中优于竞争方法，并支持应用如交互实时模拟和基于模型的机器人运动规划。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://jianghanxiao.github.io/phystwin-web/",
      "pdf_url": "http://arxiv.org/pdf/2503.17973v1",
      "published_date": "2025-03-23 07:49:19 UTC",
      "updated_date": "2025-03-23 07:49:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:20:33.535817"
    },
    {
      "arxiv_id": "2503.18985v2",
      "title": "LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan Liu",
        "Xiaobin Chang"
      ],
      "abstract": "In continual learning (CL), catastrophic forgetting often arises due to\nfeature drift. This challenge is particularly prominent in the exemplar-free\ncontinual learning (EFCL) setting, where samples from previous tasks cannot be\nretained, making it difficult to preserve prior knowledge. To address this\nissue, some EFCL methods aim to identify feature spaces that minimize the\nimpact on previous tasks while accommodating new ones. However, they rely on\nstatic features or outdated statistics stored from old tasks, which prevents\nthem from capturing the dynamic evolution of the feature space in CL, leading\nto performance degradation over time. In this paper, we introduce the\nDrift-Resistant Space (DRS), which effectively handles feature drifts without\nrequiring explicit feature modeling or the storage of previous tasks. A novel\nparameter-efficient fine-tuning approach called Low-Rank Adaptation Subtraction\n(LoRA-) is proposed to develop the DRS. This method subtracts the LoRA weights\nof old tasks from the initial pre-trained weight before processing new task\ndata to establish the DRS for model training. Therefore, LoRA- enhances\nstability, improves efficiency, and simplifies implementation. Furthermore,\nstabilizing feature drifts allows for better plasticity by learning with a\ntriplet loss. Our method consistently achieves state-of-the-art results,\nespecially for long task sequences, across multiple datasets.",
      "tldr_zh": "该论文针对无示例持续学习（Exemplar-Free Continual Learning, EFCL）中的特征漂移问题，提出了一种Drift-Resistant Space (DRS)，以有效缓解灾难性遗忘，而无需存储旧任务样本或显式特征建模。作者引入Low-Rank Adaptation Subtraction (LoRA-)方法，通过从预训练权重中减去旧任务的LoRA权重来构建DRS，并结合三元组损失（triplet loss）提升模型的可塑性和稳定性。该方法在多个数据集上表现出色，尤其在长任务序列中， consistently achieves state-of-the-art results。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18985v2",
      "published_date": "2025-03-23 07:38:53 UTC",
      "updated_date": "2025-03-31 12:47:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:20:47.235126"
    },
    {
      "arxiv_id": "2503.18984v1",
      "title": "The Misinterpretable Evidence Conveyed by Arbitrary Codes",
      "title_zh": "翻译失败",
      "authors": [
        "Guido Fioretti"
      ],
      "abstract": "Evidence Theory is a mathematical framework for handling imprecise reasoning\nin the context of a judge evaluating testimonies or a detective evaluating\ncues, rather than a gambler playing games of chance. In comparison to\nProbability Theory, it is better equipped to deal with ambiguous information\nand novel possibilities. Furthermore, arrival and evaluation of testimonies\nimplies a communication channel.\n  This paper explores the possibility of employing Evidence Theory to represent\narbitrary communication codes between and within living organisms. In this\npaper, different schemes are explored for living organisms incapable of\nanticipation, animals sufficiently sophisticated to be capable of\nextrapolation, and humans capable of reading one other's minds.",
      "tldr_zh": "这篇论文探讨了 Evidence Theory 作为处理不精确推理的数学框架，其在处理模糊信息和新可能性方面比 Probability Theory 更具优势，尤其适用于证词评估和通信通道。论文重点考察了 Evidence Theory 在表示任意通信代码的应用，包括无法预期的生物、能进行外推的动物，以及能读懂彼此想法的人类。研究揭示了不同生物通信方案的设计可能性，为扩展证据理论到生物间互动提供了新见解。",
      "categories": [
        "cs.AI",
        "nlin.AO"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.18984v1",
      "published_date": "2025-03-23 07:31:26 UTC",
      "updated_date": "2025-03-23 07:31:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:20:57.067531"
    },
    {
      "arxiv_id": "2503.17965v1",
      "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts",
      "title_zh": "理解 RLHF 对 LLM 生成文本质量和可检测性的影响",
      "authors": [
        "Beining Xu",
        "Arkaitz Zubiaga"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.",
      "tldr_zh": "该研究探讨了 Reinforcement Learning from Human Feedback (RLHF) 对 Large Language Models (LLMs) 生成文本的质量和检测性的影响，旨在评估 RLHF 如何提升文本质量同时可能增加其被识别的风险。研究通过编辑 LLM 生成文本并测试其在两个任务上的表现，比较了基于训练和零样本检测方法。结果显示，RLHF 改善了文本质量，但也导致输出更易检测、更冗长和重复；此外，基于训练的检测器对短文本和包含代码的文本较为脆弱，而零样本检测器表现出更高的稳健性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17965v1",
      "published_date": "2025-03-23 07:03:10 UTC",
      "updated_date": "2025-03-23 07:03:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:21:10.412849"
    },
    {
      "arxiv_id": "2503.17959v1",
      "title": "Dynamic Gradient Sparse Update for Edge Training",
      "title_zh": "动态梯度稀疏更新用于边缘训练",
      "authors": [
        "I-Hsuan Li",
        "Tian-Sheuan Chang"
      ],
      "abstract": "Training on edge devices enables personalized model fine-tuning to enhance\nreal-world performance and maintain data privacy. However, the gradient\ncomputation for backpropagation in the training requires significant memory\nbuffers to store intermediate features and compute losses. This is unacceptable\nfor memory-constrained edge devices such as microcontrollers. To tackle this\nissue, we propose a training acceleration method using dynamic gradient sparse\nupdates. This method updates the important channels and layers only and skips\ngradient computation for the less important channels and layers to reduce\nmemory usage for each update iteration. In addition, the channel selection is\ndynamic for different iterations to traverse most of the parameters in the\nupdate layers along the time dimension for better performance. The experimental\nresult shows that the proposed method enables an ImageNet pre-trained\nMobileNetV2 trained on CIFAR-10 to achieve an accuracy of 85.77\\% while\nupdating only 2\\% of convolution weights within 256KB on-chip memory. This\nresults in a remarkable 98\\% reduction in feature memory usage compared to\ndense model training.",
      "tldr_zh": "该论文提出了一种动态梯度稀疏更新（Dynamic Gradient Sparse Update）方法，用于在内存受限的边缘设备上进行模型训练，以解决反向传播梯度计算所需的大量内存问题。该方法仅更新重要通道和层，动态选择通道以在不同迭代中遍历大部分参数，从而减少内存使用并保持模型性能。在实验中，使用 ImageNet 预训练的 MobileNetV2 在 CIFAR-10 数据集上训练，仅更新 2% 的卷积权重，即可在 256KB 片上内存下达到 85.77% 的准确率，与密集训练相比，特征内存使用减少 98%。这为高效的边缘训练提供了可行解决方案。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "published in IEEE International Symposium on Circuits and Systems\n  (IEEE ISCAS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2503.17959v1",
      "published_date": "2025-03-23 06:32:12 UTC",
      "updated_date": "2025-03-23 06:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:21:21.535253"
    },
    {
      "arxiv_id": "2503.18983v1",
      "title": "Confronting Catastrophic Risk: The International Obligation to Regulate Artificial Intelligence",
      "title_zh": "应对灾难性风险：监管人工智能的国际义务",
      "authors": [
        "Bryan Druzin",
        "Anatole Boute",
        "Michael Ramsden"
      ],
      "abstract": "While artificial intelligence (AI) holds enormous promise, many experts in\nthe field are warning that there is a non-trivial chance that the development\nof AI poses an existential threat to humanity. Existing regulatory initiative\ndo not address this threat but merely instead focus on discrete AI-related\nrisks such as consumer safety, cybersecurity, data protection, and privacy. In\nthe absence of regulatory action to address the possible risk of human\nextinction by AI, the question arises: What legal obligations, if any, does\npublic international law impose on states to regulate its development. Grounded\nin the precautionary principle, we argue that there exists an international\nobligation to mitigate the threat of human extinction by AI. Often invoked in\nrelation to environmental regulation and the regulation of potentially harmful\ntechnologies, the principle holds that in situations where there is the\npotential for significant harm, even in the absence of full scientific\ncertainty, preventive measures should not be postponed if delayed action may\nresult in irreversible consequences. We argue that the precautionary principle\nis a general principle of international law and, therefore, that there is a\npositive obligation on states under the right to life within international\nhuman rights law to proactively take regulatory action to mitigate the\npotential existential risk of AI. This is significant because, if an\ninternational obligation to regulate the development of AI can be established\nunder international law, then the basic legal framework would be in place to\naddress this evolving threat.",
      "tldr_zh": "该论文探讨了人工智能（AI）可能带来的人类灭绝风险，以及现有监管措施仅关注具体问题如消费者安全、网络安全、数据保护和隐私，而忽略整体威胁。作者基于预防原则（precautionary principle）论证，国际法是否要求国家采取行动以减轻AI的潜在灭绝风险，认为这是一种一般国际法原则。论文进一步指出，国家在国际人权法下的生命权（right to life）义务，要求主动实施监管，从而为应对AI演变威胁建立基本法律框架。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18983v1",
      "published_date": "2025-03-23 06:24:45 UTC",
      "updated_date": "2025-03-23 06:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:21:33.101084"
    },
    {
      "arxiv_id": "2503.17955v2",
      "title": "Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Pasch",
        "Sun-Young Ha"
      ],
      "abstract": "Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2, a leading review platform for business\nsoftware and services. Based on widely adopted industry guidelines, we identify\nseven core HAI dimensions and examine their coverage and sentiment within the\nreviews. We find that the sentiment on four HAI dimensions-adaptability,\ncustomization, error recovery, and security-is positively associated with\noverall user satisfaction. Moreover, we show that engagement with HAI\ndimensions varies by professional background: Users with technical job roles\nare more likely to discuss system-focused aspects, such as reliability, while\nnon-technical users emphasize interaction-focused features like customization\nand feedback. Interestingly, the relationship between HAI sentiment and overall\nsatisfaction is not moderated by job role, suggesting that once an HAI\ndimension has been identified by users, its effect on satisfaction is\nconsistent across job roles.",
      "tldr_zh": "本研究通过分析超过10万条来自G2平台的AI产品用户评论，提供了Human-AI Interaction (HAI)原则对用户满意度的实证证据。研究识别了七个核心HAI维度，并发现adaptability、customization、error recovery和security等四个维度的积极情感与整体用户满意度正相关。结果显示，不同职业背景的用户关注点不同：技术角色用户更侧重reliability等系统方面，而非技术用户更强调customization和feedback等交互特征；然而，HAI情感对满意度的影响在职业角色间保持一致，为AI系统设计提供了宝贵指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17955v2",
      "published_date": "2025-03-23 06:16:49 UTC",
      "updated_date": "2025-03-25 01:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:21:44.870821"
    },
    {
      "arxiv_id": "2503.18982v2",
      "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Zhang",
        "Jionghao Lin",
        "John Sabatini",
        "Diego Zapata-Rivera",
        "Carol Forsyth",
        "Yang Jiang",
        "John Hollander",
        "Xiangen Hu",
        "Arthur C. Graesser"
      ],
      "abstract": "Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes.",
      "tldr_zh": "该研究针对智能辅导系统（ITSs）中学习者性能数据（如问题响应）的稀疏问题，提出了一种基于 Generative Adversarial Imputation Networks (GAIN) 的生成式数据插补方法。该方法采用三维框架（学习者、问题和尝试），结合卷积神经网络（CNN）和最小二乘损失函数，灵活处理不同稀疏度水平。实验在 AutoTutor ARC、ASSISTments 和 MATHia 数据集上显示，该方法在插补准确性上显著优于张量分解和其它 GAN 方法；此外，通过 Bayesian Knowledge Tracing (BKT) 评估，插补数据提升了模型拟合度，并经 Kullback-Leibler (KL) 散度验证，保持了原始学习行为的可靠性，从而支持更精确的个性化教学和教育成果改善。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18982v2",
      "published_date": "2025-03-23 06:11:53 UTC",
      "updated_date": "2025-04-13 21:04:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:21:59.725013"
    },
    {
      "arxiv_id": "2503.18981v1",
      "title": "FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional Similarity Knowledge Distillation",
      "title_zh": "FedSKD：使用多维相似性知识蒸馏的免聚合模型异构联邦学习",
      "authors": [
        "Ziqiao Weng",
        "Weidong Cai",
        "Bo Zhou"
      ],
      "abstract": "Federated learning (FL) enables privacy-preserving collaborative model\ntraining without direct data sharing. Model-heterogeneous FL (MHFL) extends\nthis paradigm by allowing clients to train personalized models with\nheterogeneous architectures tailored to their computational resources and\napplication-specific needs. However, existing MHFL methods predominantly rely\non centralized aggregation, which introduces scalability and efficiency\nbottlenecks, or impose restrictions requiring partially identical model\narchitectures across clients. While peer-to-peer (P2P) FL removes server\ndependence, it suffers from model drift and knowledge dilution, limiting its\neffectiveness in heterogeneous settings. To address these challenges, we\npropose FedSKD, a novel MHFL framework that facilitates direct knowledge\nexchange through round-robin model circulation, eliminating the need for\ncentralized aggregation while allowing fully heterogeneous model architectures\nacross clients. FedSKD's key innovation lies in multi-dimensional similarity\nknowledge distillation, which enables bidirectional cross-client knowledge\ntransfer at batch, pixel/voxel, and region levels for heterogeneous models in\nFL. This approach mitigates catastrophic forgetting and model drift through\nprogressive reinforcement and distribution alignment while preserving model\nheterogeneity. Extensive evaluations on fMRI-based autism spectrum disorder\ndiagnosis and skin lesion classification demonstrate that FedSKD outperforms\nstate-of-the-art heterogeneous and homogeneous FL baselines, achieving superior\npersonalization (client-specific accuracy) and generalization\n(cross-institutional adaptability). These findings underscore FedSKD's\npotential as a scalable and robust solution for real-world medical federated\nlearning applications.",
      "tldr_zh": "该研究提出 FedSKD，一种无需中心化聚合的 Model-heterogeneous Federated Learning 框架，通过轮询模型循环实现客户端间直接知识交换，支持完全异构模型架构。FedSKD 的核心创新是多维度相似性知识蒸馏技术，在批次、像素/体素和区域级别进行双向知识转移，以缓解模型漂移和灾难性遗忘，同时保持模型异构性。在 fMRI-based autism spectrum disorder 诊断和皮肤病变分类任务的评估中，FedSKD 优于现有基线方法，提升了个性化准确性和跨机构泛化能力，为真实世界的医疗 Federated Learning 应用提供了可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 5 figure, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.18981v1",
      "published_date": "2025-03-23 05:33:10 UTC",
      "updated_date": "2025-03-23 05:33:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:22:11.641457"
    },
    {
      "arxiv_id": "2504.03688v1",
      "title": "CLCR: Contrastive Learning-based Constraint Reordering for Efficient MILP Solving",
      "title_zh": "CLCR：基于对比学习的约束重排序，用于高效 MILP 求解",
      "authors": [
        "Shuli Zeng",
        "Mengjie Zhou",
        "Sijia Zhang",
        "Yixiang Hu",
        "Feng Wu",
        "Xiang-Yang Li"
      ],
      "abstract": "Constraint ordering plays a critical role in the efficiency of Mixed-Integer\nLinear Programming (MILP) solvers, particularly for large-scale problems where\npoorly ordered constraints trigger increased LP iterations and suboptimal\nsearch trajectories. This paper introduces CLCR (Contrastive Learning-based\nConstraint Reordering), a novel framework that systematically optimizes\nconstraint ordering to accelerate MILP solving. CLCR first clusters constraints\nbased on their structural patterns and then employs contrastive learning with a\npointer network to optimize their sequence, preserving problem equivalence\nwhile improving solver efficiency. Experiments on benchmarks show CLCR reduces\nsolving time by 30% and LP iterations by 25% on average, without sacrificing\nsolution accuracy. This work demonstrates the potential of data-driven\nconstraint ordering to enhance optimization models, offering a new paradigm for\nbridging mathematical programming with machine learning.",
      "tldr_zh": "本研究提出 CLCR 框架，利用 Contrastive Learning-based 方法优化混合整数线性规划 (MILP) 中的约束排序，以提高求解效率。CLCR 先基于约束的结构模式进行聚类，然后通过对比学习和 pointer network 优化约束序列，确保问题等价性同时减少 LP 迭代和求解时间。实验结果显示，在基准测试中，CLCR 平均将求解时间降低 30% 和 LP 迭代减少 25%，而不影响解决方案准确性。该框架展示了数据驱动方法在桥接数学编程与机器学习方面的潜力，提供了一种新的优化范式。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03688v1",
      "published_date": "2025-03-23 05:01:43 UTC",
      "updated_date": "2025-03-23 05:01:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:22:22.319891"
    },
    {
      "arxiv_id": "2503.18980v1",
      "title": "CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yexin Li",
        "Pring Wong",
        "Hanfang Zhang",
        "Shuo Chen",
        "Siyuan Qi"
      ],
      "abstract": "Exploration remains a critical challenge in reinforcement learning, as many\nexisting methods either lack theoretical guarantees or fall short of practical\neffectiveness. In this paper, we introduce CAE, a lightweight algorithm that\nrepurposes the value networks in standard deep RL algorithms to drive\nexploration without introducing additional parameters. CAE utilizes any linear\nmulti-armed bandit technique and incorporates an appropriate scaling strategy,\nenabling efficient exploration with provable sub-linear regret bounds and\npractical stability. Notably, it is simple to implement, requiring only around\n10 lines of code. In complex tasks where learning an effective value network\nproves challenging, we propose CAE+, an extension of CAE that incorporates an\nauxiliary network. This extension increases the parameter count by less than 1%\nwhile maintaining implementation simplicity, adding only about 10 additional\nlines of code. Experiments on MuJoCo and MiniHack show that both CAE and CAE+\noutperform state-of-the-art baselines, bridging the gap between theoretical\nrigor and practical efficiency.",
      "tldr_zh": "该研究针对深度强化学习（Deep Reinforcement Learning）中的探索（Exploration）挑战，提出了一种轻量级算法CAE，通过重新利用标准RL中的价值网络（value networks）来驱动探索，而不增加额外参数。CAE结合线性多臂老虎机（linear multi-armed bandit）技术和合适的缩放策略，实现高效探索，并提供可证明的次线性遗憾界（sub-linear regret bounds）和实际稳定性，仅需约10行代码即可实现。对于复杂任务，该论文扩展为CAE+，添加一个辅助网络，参数增加不到1%，并保持实现简单（额外约10行代码）。实验结果显示，CAE和CAE+在MuJoCo和MiniHack任务上超越最先进基线，成功桥接了理论严谨性和实际效率。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18980v1",
      "published_date": "2025-03-23 04:59:24 UTC",
      "updated_date": "2025-03-23 04:59:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:22:35.505024"
    },
    {
      "arxiv_id": "2503.17941v1",
      "title": "Physics-Guided Multi-Fidelity DeepONet for Data-Efficient Flow Field Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Sunwoong Yang",
        "Youngkyu Lee",
        "Namwoo Kang"
      ],
      "abstract": "This study presents an enhanced multi-fidelity deep operator network\n(DeepONet) framework for efficient spatio-temporal flow field prediction, with\nparticular emphasis on practical scenarios where high-fidelity data is scarce.\nWe introduce several key innovations to improve the framework's efficiency and\naccuracy. First, we enhance the DeepONet architecture by incorporating a merge\nnetwork that enables more complex feature interactions between operator and\ncoordinate spaces, achieving a 50.4% reduction in prediction error compared to\ntraditional dot-product operations. We further optimize the architecture\nthrough temporal positional encoding and point-based sampling strategies,\nachieving a 7.57% improvement in prediction accuracy while reducing training\ntime by 96% through efficient sampling and automatic mixed precision training.\nBuilding upon this foundation, we develop a transfer learning-based\nmulti-fidelity framework that leverages knowledge from pre-trained low-fidelity\nmodels to guide high-fidelity predictions. Our approach freezes the pre-trained\nbranch and trunk networks while making only the merge network trainable during\nhigh-fidelity training, preserving valuable low-fidelity representations while\nefficiently adapting to high-fidelity features. Through systematic\ninvestigation, we demonstrate that this fine-tuning strategy not only\nsignificantly outperforms linear probing and full-tuning alternatives but also\nsurpasses conventional multi-fidelity frameworks by up to 76%, while achieving\nup to 43.7% improvement in prediction accuracy compared to single-fidelity\ntraining. The core contribution lies in our novel time-derivative guided\nsampling approach: it maintains prediction accuracy equivalent to models\ntrained with the full dataset while requiring only 60% of the original\nhigh-fidelity samples.",
      "tldr_zh": "本研究提出了一种受物理指导的多保真度 DeepONet 框架，用于数据高效的时空流场预测，尤其适用于高保真数据稀缺的实际场景。通过引入 merge network 增强架构，实现更复杂的特征交互，比传统点积操作减少 50.4% 预测错误；同时结合时间位置编码和基于点的采样策略，提高 7.57% 预测准确率，并将训练时间减少 96%。该框架采用基于转移学习的 multi-fidelity 方法，冻结预训练的低保真分支和主干网络，仅训练 merge network，从而在高保真预测中比线性探测和全调优高出 76%，并比单保真训练提高 43.7% 准确率。核心贡献是 time-derivative guided sampling 采样方法，使用仅 60% 的高保真样本即可达到全数据集的预测准确率。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17941v1",
      "published_date": "2025-03-23 04:48:18 UTC",
      "updated_date": "2025-03-23 04:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:22:48.306473"
    },
    {
      "arxiv_id": "2503.17936v1",
      "title": "An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models",
      "title_zh": "关于大语言模型交互中不完整性和模糊性的作用的实证研究",
      "authors": [
        "Riya Naik",
        "Ashwin Srinivasan",
        "Estrid He",
        "Swati Agarwal"
      ],
      "abstract": "Natural language as a medium for human-computer interaction has long been\nanticipated, has been undergoing a sea-change with the advent of Large Language\nModels (LLMs) with startling capacities for processing and generating language.\nMany of us now treat LLMs as modern-day oracles, asking it almost any kind of\nquestion. Unlike its Delphic predecessor, consulting an LLM does not have to be\na single-turn activity (ask a question, receive an answer, leave); and -- also\nunlike the Pythia -- it is widely acknowledged that answers from LLMs can be\nimproved with additional context. In this paper, we aim to study when we need\nmulti-turn interactions with LLMs to successfully get a question answered; or\nconclude that a question is unanswerable. We present a neural symbolic\nframework that models the interactions between human and LLM agents. Through\nthe proposed framework, we define incompleteness and ambiguity in the questions\nas properties deducible from the messages exchanged in the interaction, and\nprovide results from benchmark problems, in which the answer-correctness is\nshown to depend on whether or not questions demonstrate the presence of\nincompleteness or ambiguity (according to the properties we identify). Our\nresults show multi-turn interactions are usually required for datasets which\nhave a high proportion of incompleteness or ambiguous questions; and that that\nincreasing interaction length has the effect of reducing incompleteness or\nambiguity. The results also suggest that our measures of incompleteness and\nambiguity can be useful tools for characterising interactions with an LLM on\nquestion-answeringproblems",
      "tldr_zh": "本研究通过实证分析探讨了不完整性(incompleteness)和模糊性(ambiguity)在人与Large Language Models (LLMs)交互中的作用，旨在确定何时需要多轮交互来回答问题或判断问题不可回答。研究提出一个神经符号(neural symbolic)框架来建模人类和LLM代理之间的交互，并基于交互中的消息交换定义了问题的不完整性和模糊性属性。实验结果显示，在包含高比例不完整或模糊问题的基准数据集上，多轮交互通常是必需的，且增加交互长度能有效降低这些属性；此外，这些度量可作为工具，用于表征LLMs在问答任务中的交互表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17936v1",
      "published_date": "2025-03-23 04:34:30 UTC",
      "updated_date": "2025-03-23 04:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:22:59.397210"
    },
    {
      "arxiv_id": "2503.17933v1",
      "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA",
      "title_zh": "翻译失败",
      "authors": [
        "Justice Ou",
        "Tinglin Huang",
        "Yilun Zhao",
        "Ziyang Yu",
        "Peiqing Lu",
        "Rex Ying"
      ],
      "abstract": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.",
      "tldr_zh": "为了提升大型语言模型 (LLMs) 在临床应用中的可靠性，本文提出 ExpRAG 框架，利用电子健康记录 (EHR) 从其他患者的出院报告中检索相关案例知识，从而增强医疗推理。ExpRAG 通过粗到细的检索过程，包括 EHR-based report ranker 识别类似患者和 experience retriever 提取任务相关内容，确保提供真实上下文。实验在新的 DischargeQA 数据集（包含 1280 个出院相关问答问题）上验证，结果显示 ExpRAG 比基于文本的 ranker 平均提高了 5.2% 的性能，强调了临床案例知识在医疗推理中的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17933v1",
      "published_date": "2025-03-23 04:26:06 UTC",
      "updated_date": "2025-03-23 04:26:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:23:12.265836"
    },
    {
      "arxiv_id": "2503.17932v1",
      "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xunguang Wang",
        "Wenxuan Wang",
        "Zhenlan Ji",
        "Zongjie Li",
        "Pingchuan Ma",
        "Daoyuan Wu",
        "Shuai Wang"
      ],
      "abstract": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.",
      "tldr_zh": "该研究提出 STShield，一种轻量级框架，用于实时检测 Large Language Models (LLMs) 中的 jailbreak attacks，以解决现有防御方法易受自适应攻击或计算开销大的问题。STShield 创新性地引入 single-token sentinel 机制，在模型响应序列中附加二进制安全指示器，并结合 supervised fine-tuning 和 adversarial training 使用 embedding-space perturbations 来增强检测鲁棒性。实验结果显示，STShield 成功防御多种 jailbreak attacks，同时保持模型在合法查询上的性能，并比现有方法提供更优的防御效果和最小计算开销，适合实际部署。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.17932v1",
      "published_date": "2025-03-23 04:23:07 UTC",
      "updated_date": "2025-03-23 04:23:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:23:24.465509"
    },
    {
      "arxiv_id": "2503.17924v1",
      "title": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Wang",
        "Anna Cai",
        "Xinfeng Xie",
        "Zaifeng Pan",
        "Yue Guan",
        "Weiwei Chu",
        "Jie Wang",
        "Shikai Li",
        "Jianyu Huang",
        "Chris Cai",
        "Yuchen Hao",
        "Yufei Ding"
      ],
      "abstract": "In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for\nlarge language model training. We first thoroughly analyze the workload\nimbalance issue in LLM training and identify two primary sources of imbalance\nat the pipeline parallelism and context parallelism levels. Then, to address\nthe imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a\nworkload-aware variable-length document packing method to balance the\ncomputation and communication workload across micro-batches. Additionally, at\nthe context parallelism level, WLB-LLM introduces a novel fine-grained\nper-document sharding strategy, ensuring each worker within a context\nparallelism group has an identical workload. Comprehensive experiments under\ndifferent model scales demonstrate that WLB-LLM significantly mitigates the\nworkload imbalance during 4D parallelism LLM training and achieves an average\nspeedup of 1.23x when applying WLB-LLM in our internal LLM training framework.",
      "tldr_zh": "本研究提出WLB-LLM，一种工作负载平衡的4D Parallelism方法，用于优化大型语言模型训练。通过分析Pipeline Parallelism和Context Parallelism层面的不平衡来源，WLB-LLM引入workload-aware variable-length document packing策略来平衡计算和通信负载，以及fine-grained per-document sharding策略确保每个worker workload一致。实验在不同模型规模下证明，该方法显著缓解了工作负载不平衡问题，实现平均加速1.23x。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "I.2.11"
      ],
      "primary_category": "cs.DC",
      "comment": "12 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.17924v1",
      "published_date": "2025-03-23 03:40:45 UTC",
      "updated_date": "2025-03-23 03:40:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:23:35.850116"
    },
    {
      "arxiv_id": "2503.17915v1",
      "title": "Cat-AIR: Content and Task-Aware All-in-One Image Restoration",
      "title_zh": "Cat-AIR：内容与任务感知的全合一图像恢复",
      "authors": [
        "Jiachen Jiang",
        "Tianyu Ding",
        "Ke Zhang",
        "Jinxin Zhou",
        "Tianyi Chen",
        "Ilya Zharkov",
        "Zhihui Zhu",
        "Luming Liang"
      ],
      "abstract": "All-in-one image restoration seeks to recover high-quality images from\nvarious types of degradation using a single model, without prior knowledge of\nthe corruption source. However, existing methods often struggle to effectively\nand efficiently handle multiple degradation types. We present Cat-AIR, a novel\n\\textbf{C}ontent \\textbf{A}nd \\textbf{T}ask-aware framework for\n\\textbf{A}ll-in-one \\textbf{I}mage \\textbf{R}estoration. Cat-AIR incorporates\nan alternating spatial-channel attention mechanism that adaptively balances the\nlocal and global information for different tasks. Specifically, we introduce\ncross-layer channel attentions and cross-feature spatial attentions that\nallocate computations based on content and task complexity. Furthermore, we\npropose a smooth learning strategy that allows for seamless adaptation to new\nrestoration tasks while maintaining performance on existing ones. Extensive\nexperiments demonstrate that Cat-AIR achieves state-of-the-art results across a\nwide range of restoration tasks, requiring fewer FLOPs than previous methods,\nestablishing new benchmarks for efficient all-in-one image restoration.",
      "tldr_zh": "本文提出 Cat-AIR，一种内容和任务感知的 all-in-one image restoration 框架，用于从多种退化类型中恢复高质量图像，而无需事先知道退化来源。Cat-AIR 采用交替的 spatial-channel attention 机制，包括 cross-layer channel attentions 和 cross-feature spatial attentions，根据内容和任务复杂度自适应分配计算资源，并引入 smooth learning strategy 以无缝适应新任务同时保持现有性能。实验结果表明，Cat-AIR 在广泛的恢复任务上达到 state-of-the-art 水平，且所需 FLOPs 更少，建立起高效图像恢复的新基准。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17915v1",
      "published_date": "2025-03-23 03:25:52 UTC",
      "updated_date": "2025-03-23 03:25:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:23:48.517112"
    },
    {
      "arxiv_id": "2503.20798v1",
      "title": "Payload-Aware Intrusion Detection with CMAE and Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yongcheol Kim",
        "Chanjae Lee",
        "Young Yoon"
      ],
      "abstract": "Intrusion Detection Systems (IDS) are crucial for identifying malicious\ntraffic, yet traditional signature-based methods struggle with zero-day attacks\nand high false positive rates. AI-driven packet-capture analysis offers a\npromising alternative. However, existing approaches rely heavily on flow-based\nor statistical features, limiting their ability to detect fine-grained attack\npatterns. This study proposes Xavier-CMAE, an enhanced Convolutional Multi-Head\nAttention Ensemble (CMAE) model that improves detection accuracy while reducing\ncomputational overhead. By replacing Word2Vec embeddings with a Hex2Int\ntokenizer and Xavier initialization, Xavier-CMAE eliminates pre-training,\naccelerates training, and achieves 99.971% accuracy with a 0.018% false\npositive rate, outperforming Word2Vec-based methods. Additionally, we introduce\nLLM-CMAE, which integrates pre-trained Large Language Model (LLM) tokenizers\ninto CMAE. While LLMs enhance feature extraction, their computational cost\nhinders real-time detection. LLM-CMAE balances efficiency and performance,\nreaching 99.969% accuracy with a 0.019% false positive rate. This work advances\nAI-powered IDS by (1) introducing a payload-based detection framework, (2)\nenhancing efficiency with Xavier-CMAE, and (3) integrating LLM tokenizers for\nimproved real-time detection.",
      "tldr_zh": "本研究针对入侵检测系统（IDS）的零日攻击和高假阳性率问题，提出了一种基于有效载荷的检测框架。Xavier-CMAE 是一种增强的 Convolutional Multi-Head Attention Ensemble (CMAE) 模型，通过采用 Hex2Int tokenizer 和 Xavier 初始化，消除预训练需求、加速训练过程，并实现 99.971% 准确率和 0.018% 假阳性率，优于传统 Word2Vec-based 方法。此外，LLM-CMAE 整合了预训练 Large Language Models (LLM) tokenizers 来提升特征提取，但通过优化平衡了计算效率，达到 99.969% 准确率和 0.019% 假阳性率。该工作通过引入有效载荷框架、提升检测效率和集成 LLM 技术，推进了 AI 驱动 IDS 的发展。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20798v1",
      "published_date": "2025-03-23 02:56:32 UTC",
      "updated_date": "2025-03-23 02:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:24:01.121986"
    },
    {
      "arxiv_id": "2503.17903v1",
      "title": "GLADMamba: Unsupervised Graph-Level Anomaly Detection Powered by Selective State Space Model",
      "title_zh": "GLADMamba：基于选择性状态空间模型的无监督图级别异常检测",
      "authors": [
        "Yali Fu",
        "Jindong Li",
        "Qi Wang",
        "Qianli Xing"
      ],
      "abstract": "Unsupervised graph-level anomaly detection (UGLAD) is a critical and\nchallenging task across various domains, such as social network analysis,\nanti-cancer drug discovery, and toxic molecule identification. However,\nexisting methods often struggle to capture the long-range dependencies\nefficiently and neglect the spectral information. Recently, selective State\nSpace Models (SSMs), particularly Mamba, have demonstrated remarkable\nadvantages in capturing long-range dependencies with linear complexity and a\nselection mechanism. Motivated by their success across various domains, we\npropose GLADMamba, a novel framework that adapts the selective state space\nmodel into UGLAD field. We design View-Fused Mamba (VFM) with a\nMamba-Transformer-style architecture to efficiently fuse information from\ndifferent views with a selective state mechanism. We also design\nSpectrum-Guided Mamba (SGM) with a Mamba-Transformer-style architecture to\nleverage the Rayleigh quotient to guide the embedding refining process.\nGLADMamba can dynamically focus on anomaly-related information while discarding\nirrelevant information for anomaly detection. To the best of our knowledge,\nthis is the first work to introduce Mamba and explicit spectral information to\nUGLAD. Extensive experiments on 12 real-world datasets demonstrate that\nGLADMamba outperforms existing state-of-the-art methods, achieving superior\nperformance in UGLAD. The code is available at\nhttps://github.com/Yali-F/GLADMamba.",
      "tldr_zh": "本论文提出 GLADMamba，一种基于 Selective State Space Models 的框架，用于无监督图级异常检测 (UGLAD)，旨在高效捕获长程依赖并整合谱信息，以解决现有方法的局限性。该框架包括 View-Fused Mamba (VFM) 和 Spectrum-Guided Mamba (SGM) 两种组件，分别通过 Mamba-Transformer-style 架构融合多视图信息并利用 Rayleigh quotient 指导嵌入精炼过程，从而动态关注异常相关信息。实验在 12 个真实数据集上表明，GLADMamba 超过了现有最先进方法，展示了其在 UGLAD 领域的优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17903v1",
      "published_date": "2025-03-23 02:40:17 UTC",
      "updated_date": "2025-03-23 02:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:24:11.975950"
    },
    {
      "arxiv_id": "2503.18979v2",
      "title": "Threshold Crossings as Tail Events for Catastrophic AI Risk",
      "title_zh": "阈值跨越作为灾难性 AI 风险的尾部事件",
      "authors": [
        "Elija Perrier"
      ],
      "abstract": "We analyse circumstances in which bifurcation-driven jumps in AI systems are\nassociated with emergent heavy-tailed outcome distributions. By analysing how a\ncontrol parameter's random fluctuations near a catastrophic threshold generate\nextreme outcomes, we demonstrate in what circumstances the probability of a\nsudden, large-scale, transition aligns closely with the tail probability of the\nresulting damage distribution. Our results contribute to research in\nmonitoring, mitigation and control of AI systems when seeking to manage\npotentially catastrophic AI risk.",
      "tldr_zh": "本研究分析了 AI 系统中的分叉驱动跳跃(bifurcation-driven jumps)如何与紧急重尾分布(emergent heavy-tailed outcome distributions)相关联，探讨了控制参数在灾难性阈值(catastrophic threshold)附近的随机波动如何产生极端结果。通过这一分析，研究发现，在特定情况下，突然大规模转变的概率与损害分布的尾概率(tail probability)密切对齐。这些结果为 AI 系统的监控、缓解和控制提供了关键洞见，以有效管理潜在的灾难性 AI 风险。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Under peer review",
      "pdf_url": "http://arxiv.org/pdf/2503.18979v2",
      "published_date": "2025-03-23 02:01:09 UTC",
      "updated_date": "2025-03-26 02:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:24:24.308783"
    },
    {
      "arxiv_id": "2503.17894v2",
      "title": "Generative AI for Validating Physics Laws",
      "title_zh": "生成式 AI 用于验证物理定律",
      "authors": [
        "Maria Nareklishvili",
        "Nicholas Polson",
        "Vadim Sokolov"
      ],
      "abstract": "We present generative artificial intelligence (AI) to empirically validate\nfundamental laws of physics, focusing on the Stefan-Boltzmann law linking\nstellar temperature and luminosity. Our approach simulates counterfactual\nluminosities under hypothetical temperature regimes for each individual star\nand iteratively refines the temperature-luminosity relationship in a deep\nlearning architecture. We use Gaia DR3 data and find that, on average,\ntemperature's effect on luminosity increases with stellar radius and decreases\nwith absolute magnitude, consistent with theoretical predictions. By framing\nphysics laws as causal problems, our method offers a novel, data-driven\napproach to refine theoretical understanding and inform evidence-based policy\nand practice.",
      "tldr_zh": "本研究利用生成式AI（Generative AI）来经验验证物理定律，特别针对Stefan-Boltzmann定律，该定律将恒星温度与光度联系起来。方法涉及模拟反事实光度（counterfactual luminosities），通过深度学习架构迭代优化温度-光度关系，并使用Gaia DR3数据进行分析。结果显示，温度对光度的影响随恒星半径增加而增强，随绝对星等（absolute magnitude）减少，与理论预测一致。该方法将物理定律视为因果问题，提供了一种数据驱动的途径来完善理论理解，并支持基于证据的政策和实践。",
      "categories": [
        "astro-ph.SR",
        "astro-ph.GA",
        "cs.AI"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.17894v2",
      "published_date": "2025-03-23 00:57:26 UTC",
      "updated_date": "2025-03-25 14:31:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:24:35.624123"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 70,
  "processed_papers_count": 70,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T05:24:55.127622"
}
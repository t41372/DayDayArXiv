[
  {
    "arxiv_id": "2503.18242v1",
    "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices",
    "authors": [
      "Aneesh Vathul",
      "Daniel Lee",
      "Sheryl Chen",
      "Arthi Tasmia"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18242v1",
    "published_date": "2025-03-23 23:47:26 UTC",
    "updated_date": "2025-03-23 23:47:26 UTC"
  },
  {
    "arxiv_id": "2503.18238v1",
    "title": "Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance",
    "authors": [
      "Harang Ju",
      "Sinan Aral"
    ],
    "abstract": "To uncover how AI agents change productivity, performance, and work\nprocesses, we introduce MindMeld: an experimentation platform enabling humans\nand AI agents to collaborate in integrative workspaces. In a large-scale\nmarketing experiment on the platform, 2310 participants were randomly assigned\nto human-human and human-AI teams, with randomized AI personality traits. The\nteams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad\ncopy edits, and 10,375 AI-generated images while producing 11,138 ads for a\nlarge think tank. Analysis of fine-grained communication, collaboration, and\nworkflow logs revealed that collaborating with AI agents increased\ncommunication by 137% and allowed humans to focus 23% more on text and image\ncontent generation messaging and 20% less on direct text editing. Humans on\nHuman-AI teams sent 23% fewer social messages, creating 60% greater\nproductivity per worker and higher-quality ad copy. In contrast, human-human\nteams produced higher-quality images, suggesting that AI agents require\nfine-tuning for multimodal workflows. AI personality prompt randomization\nrevealed that AI traits can complement human personalities to enhance\ncollaboration. For example, conscientious humans paired with open AI agents\nimproved image quality, while extroverted humans paired with conscientious AI\nagents reduced the quality of text, images, and clicks. In field tests of ad\ncampaigns with ~5M impressions, ads with higher image quality produced by human\ncollaborations and higher text quality produced by AI collaborations performed\nsignificantly better on click-through rate and cost per click metrics. Overall,\nads created by human-AI teams performed similarly to those created by\nhuman-human teams. Together, these results suggest AI agents can improve\nteamwork and productivity, especially when tuned to complement human traits.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "56 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.18238v1",
    "published_date": "2025-03-23 23:20:32 UTC",
    "updated_date": "2025-03-23 23:20:32 UTC"
  },
  {
    "arxiv_id": "2503.18996v1",
    "title": "Enhanced prediction of spine surgery outcomes using advanced machine learning techniques and oversampling methods",
    "authors": [
      "José Alberto Benítez-Andrades",
      "Camino Prada-García",
      "Nicolás Ordás-Reyes",
      "Marta Esteban Blanco",
      "Alicia Merayo",
      "Antonio Serrano-García"
    ],
    "abstract": "The study proposes an advanced machine learning approach to predict spine\nsurgery outcomes by incorporating oversampling techniques and grid search\noptimization. A variety of models including GaussianNB, ComplementNB, KNN,\nDecision Tree, and optimized versions with RandomOverSampler and SMOTE were\ntested on a dataset of 244 patients, which included pre-surgical, psychometric,\nsocioeconomic, and analytical variables. The enhanced KNN models achieved up to\n76% accuracy and a 67% F1-score, while grid-search optimization further\nimproved performance. The findings underscore the potential of these advanced\ntechniques to aid healthcare professionals in decision-making, with future\nresearch needed to refine these models on larger and more diverse datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18996v1",
    "published_date": "2025-03-23 22:39:19 UTC",
    "updated_date": "2025-03-23 22:39:19 UTC"
  },
  {
    "arxiv_id": "2503.18229v1",
    "title": "Adaptive Multi-Fidelity Reinforcement Learning for Variance Reduction in Engineering Design Optimization",
    "authors": [
      "Akash Agrawal",
      "Christopher McComb"
    ],
    "abstract": "Multi-fidelity Reinforcement Learning (RL) frameworks efficiently utilize\ncomputational resources by integrating analysis models of varying accuracy and\ncosts. The prevailing methodologies, characterized by transfer learning,\nhuman-inspired strategies, control variate techniques, and adaptive sampling,\npredominantly depend on a structured hierarchy of models. However, this\nreliance on a model hierarchy can exacerbate variance in policy learning when\nthe underlying models exhibit heterogeneous error distributions across the\ndesign space. To address this challenge, this work proposes a novel adaptive\nmulti-fidelity RL framework, in which multiple heterogeneous, non-hierarchical\nlow-fidelity models are dynamically leveraged alongside a high-fidelity model\nto efficiently learn a high-fidelity policy. Specifically, low-fidelity\npolicies and their experience data are adaptively used for efficient targeted\nlearning, guided by their alignment with the high-fidelity policy. The\neffectiveness of the approach is demonstrated in an octocopter design\noptimization problem, utilizing two low-fidelity models alongside a\nhigh-fidelity simulator. The results demonstrate that the proposed approach\nsubstantially reduces variance in policy learning, leading to improved\nconvergence and consistent high-quality solutions relative to traditional\nhierarchical multi-fidelity RL methods. Moreover, the framework eliminates the\nneed for manually tuning model usage schedules, which can otherwise introduce\nsignificant computational overhead. This positions the framework as an\neffective variance-reduction strategy for multi-fidelity RL, while also\nmitigating the computational and operational burden of manual fidelity\nscheduling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18229v1",
    "published_date": "2025-03-23 22:29:08 UTC",
    "updated_date": "2025-03-23 22:29:08 UTC"
  },
  {
    "arxiv_id": "2503.18227v3",
    "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
    "authors": [
      "Yiheng Zhong",
      "Zihong Luo",
      "Chengzhi Liu",
      "Feilong Tang",
      "Zelin Peng",
      "Ming Hu",
      "Yingzhen Hu",
      "Jionglong Su",
      "Zongyuan Ge",
      "Imran Razzak"
    ],
    "abstract": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18227v3",
    "published_date": "2025-03-23 22:06:07 UTC",
    "updated_date": "2025-03-26 13:38:40 UTC"
  },
  {
    "arxiv_id": "2503.18995v1",
    "title": "LLMs in the Classroom: Outcomes and Perceptions of Questions Written with the Aid of AI",
    "authors": [
      "Gavin Witsken",
      "Igor Crk",
      "Eren Gultepe"
    ],
    "abstract": "We randomly deploy questions constructed with and without use of the LLM tool\nand gauge the ability of the students to correctly answer, as well as their\nability to correctly perceive the difference between human-authored and\nLLM-authored questions. In determining whether the questions written with the\naid of ChatGPT were consistent with the instructor's questions and source text,\nwe computed representative vectors of both the human and ChatGPT questions\nusing SBERT and compared cosine similarity to the course textbook. A\nnon-significant Mann-Whitney U test (z = 1.018, p = .309) suggests that\nstudents were unable to perceive whether questions were written with or without\nthe aid of ChatGPT. However, student scores on LLM-authored questions were\nalmost 9% lower (z = 2.702, p < .01). This result may indicate that either the\nAI questions were more difficult or that the students were more familiar with\nthe instructor's style of questions. Overall, the study suggests that while\nthere is potential for using LLM tools to aid in the construction of\nassessments, care must be taken to ensure that the questions are fair,\nwell-composed, and relevant to the course material.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to AAAI 2025 Technical Track on AI Alignment",
    "pdf_url": "http://arxiv.org/pdf/2503.18995v1",
    "published_date": "2025-03-23 22:01:49 UTC",
    "updated_date": "2025-03-23 22:01:49 UTC"
  },
  {
    "arxiv_id": "2503.18213v1",
    "title": "A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives",
    "authors": [
      "Delower Hossain",
      "Jake Y Chen"
    ],
    "abstract": "Over the last few decades, Artificial Intelligence (AI) scientists have been\nconducting investigations to attain human-level performance by a machine in\naccomplishing a cognitive task. Within machine learning, the ultimate\naspiration is to attain Artificial General Intelligence (AGI) through a\nmachine. This pursuit has led to the exploration of two distinct AI paradigms.\nSymbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and\nConnectionist (Sub-symbolic) AI, represented by Neural Systems, are two\nmutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,\nand knowledge representation but faces challenges in processing complex\nreal-world data with noise. Conversely, deep learning (Black-Box systems)\nresearch breakthroughs in neural networks are notable, yet they lack reasoning\nand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI\nresearch, attempts to bridge this gap by integrating logical reasoning into\nneural networks, enabling them to learn and reason with symbolic\nrepresentations. While a long path, this strategy has made significant progress\ntowards achieving common sense reasoning by systems. This article conducts an\nextensive review of over 977 studies from prominent scientific databases (DBLP,\nACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the\nmultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its\nhealthcare applications, particularly in drug discovery, and Protein\nengineering research. The survey addresses vital themes, including reasoning,\nexplainability, integration strategies, 41 healthcare-related use cases,\nbenchmarking, datasets, current approach limitations from both healthcare and\nbroader perspectives, and proposed novel approaches for future experiments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.18213v1",
    "published_date": "2025-03-23 21:33:38 UTC",
    "updated_date": "2025-03-23 21:33:38 UTC"
  },
  {
    "arxiv_id": "2503.18210v1",
    "title": "ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data",
    "authors": [
      "Nitish Dashora",
      "Dibya Ghosh",
      "Sergey Levine"
    ],
    "abstract": "Online reinforcement learning (RL) with sparse rewards poses a challenge\npartly because of the lack of feedback on states leading to the goal.\nFurthermore, expert offline data with reward signal is rarely available to\nprovide this feedback and bootstrap online learning. How can we guide online\nagents to the right solution without this on-task data? Reward shaping offers a\nsolution by providing fine-grained signal to nudge the policy towards the\noptimal solution. However, reward shaping often requires domain knowledge to\nhand-engineer heuristics for a specific goal. To enable more general and\ninexpensive guidance, we propose and analyze a data-driven methodology that\nautomatically guides RL by learning from widely available video data such as\nInternet recordings, off-task demonstrations, task failures, and undirected\nenvironment interaction. By learning a model of optimal goal-conditioned value\nfrom diverse passive data, we open the floor to scaling up and using various\ndata sources to model general goal-reaching behaviors relevant to guiding\nonline RL. Specifically, we use intent-conditioned value functions to learn\nfrom diverse videos and incorporate these goal-conditioned values into the\nreward. Our experiments show that video-trained value functions work well with\na variety of data sources, exhibit positive transfer from human video\npre-training, can generalize to unseen goals, and scale with dataset size.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18210v1",
    "published_date": "2025-03-23 21:24:33 UTC",
    "updated_date": "2025-03-23 21:24:33 UTC"
  },
  {
    "arxiv_id": "2503.18197v1",
    "title": "FROG: Fair Removal on Graphs",
    "authors": [
      "Ziheng Chen",
      "Jiali Cheng",
      "Gabriele Tolomei",
      "Sijia Liu",
      "Hadi Amiri",
      "Yu Wang",
      "Kaushiki Nag",
      "Lu Lin"
    ],
    "abstract": "As compliance with privacy regulations becomes increasingly critical, the\ngrowing demand for data privacy has highlighted the significance of machine\nunlearning in many real world applications, such as social network and\nrecommender systems, many of which can be represented as graph-structured data.\nHowever, existing graph unlearning algorithms indiscriminately modify edges or\nnodes from well-trained models without considering the potential impact of such\nstructural modifications on fairness. For example, forgetting links between\nnodes with different genders in a social network may exacerbate group\ndisparities, leading to significant fairness concerns. To address these\nchallenges, we propose a novel approach that jointly optimizes the graph\nstructure and the corresponding model for fair unlearning tasks.\nSpecifically,our approach rewires the graph to enhance unlearning efficiency by\nremoving redundant edges that hinder forgetting while preserving fairness\nthrough targeted edge augmentation. Additionally, we introduce a worst-case\nevaluation mechanism to assess the reliability of fair unlearning performance.\nExtensive experiments on real-world datasets demonstrate the effectiveness of\nthe proposed approach in achieving superior unlearning outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18197v1",
    "published_date": "2025-03-23 20:39:53 UTC",
    "updated_date": "2025-03-23 20:39:53 UTC"
  },
  {
    "arxiv_id": "2503.18185v1",
    "title": "Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond",
    "authors": [
      "Spyridon Evangelatos",
      "Eleni Veroni",
      "Vasilis Efthymiou",
      "Christos Nikolopoulos",
      "Georgios Th. Papadopoulos",
      "Panagiotis Sarigiannidis"
    ],
    "abstract": "Counterfactual explanations have emerged as a prominent method in Explainable\nArtificial Intelligence (XAI), providing intuitive and actionable insights into\nMachine Learning model decisions. In contrast to other traditional feature\nattribution methods that assess the importance of input variables,\ncounterfactual explanations focus on identifying the minimal changes required\nto alter a model's prediction, offering a ``what-if'' analysis that is close to\nhuman reasoning. In the context of XAI, counterfactuals enhance transparency,\ntrustworthiness and fairness, offering explanations that are not just\ninterpretable but directly applicable in the decision-making processes.\n  In this paper, we present a novel framework that integrates perturbation\ntheory and statistical mechanics to generate minimal counterfactual\nexplanations in explainable AI. We employ a local Taylor expansion of a Machine\nLearning model's predictive function and reformulate the counterfactual search\nas an energy minimization problem over a complex landscape. In sequence, we\nmodel the probability of candidate perturbations leveraging the Boltzmann\ndistribution and use simulated annealing for iterative refinement. Our approach\nsystematically identifies the smallest modifications required to change a\nmodel's prediction while maintaining plausibility. Experimental results on\nbenchmark datasets for cybersecurity in Internet of Things environments,\ndemonstrate that our method provides actionable, interpretable counterfactuals\nand offers deeper insights into model sensitivity and decision boundaries in\nhigh-dimensional spaces.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18185v1",
    "published_date": "2025-03-23 19:48:37 UTC",
    "updated_date": "2025-03-23 19:48:37 UTC"
  },
  {
    "arxiv_id": "2503.18181v1",
    "title": "Adaptive Physics-informed Neural Networks: A Survey",
    "authors": [
      "Edgar Torres",
      "Jonathan Schiefer",
      "Mathias Niepert"
    ],
    "abstract": "Physics-informed neural networks (PINNs) have emerged as a promising approach\nto solving partial differential equations (PDEs) using neural networks,\nparticularly in data-scarce scenarios, due to their unsupervised training\ncapability. However, limitations related to convergence and the need for\nre-optimization with each change in PDE parameters hinder their widespread\nadoption across scientific and engineering applications. This survey reviews\nexisting research that addresses these limitations through transfer learning\nand meta-learning. The covered methods improve the training efficiency,\nallowing faster adaptation to new PDEs with fewer data and computational\nresources. While traditional numerical methods solve systems of differential\nequations directly, neural networks learn solutions implicitly by adjusting\ntheir parameters. One notable advantage of neural networks is their ability to\nabstract away from specific problem domains, allowing them to retain, discard,\nor adapt learned representations to efficiently address similar problems. By\nexploring the application of these techniques to PINNs, this survey identifies\npromising directions for future research to facilitate the broader adoption of\nPINNs in a wide range of scientific and engineering applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "https://openreview.net/forum?id=vz5P1Kbt6t",
    "pdf_url": "http://arxiv.org/pdf/2503.18181v1",
    "published_date": "2025-03-23 19:33:05 UTC",
    "updated_date": "2025-03-23 19:33:05 UTC"
  },
  {
    "arxiv_id": "2503.18994v1",
    "title": "HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT",
    "authors": [
      "Paolo Ceravolo",
      "Ernesto Damiani",
      "Maria Elisa D'Amico",
      "Bianca de Teffe Erb",
      "Simone Favaro",
      "Nannerel Fiano",
      "Paolo Gambatesa",
      "Simone La Porta",
      "Samira Maghool",
      "Lara Mauri",
      "Niccolo Panigada",
      "Lorenzo Maria Ratto Vaquer",
      "Marta A. Tamborini"
    ],
    "abstract": "This paper introduces the HH4AI Methodology, a structured approach to\nassessing the impact of AI systems on human rights, focusing on compliance with\nthe EU AI Act and addressing technical, ethical, and regulatory challenges. The\npaper highlights AIs transformative nature, driven by autonomy, data, and\ngoal-oriented design, and how the EU AI Act promotes transparency,\naccountability, and safety. A key challenge is defining and assessing\n\"high-risk\" AI systems across industries, complicated by the lack of\nuniversally accepted standards and AIs rapid evolution.\n  To address these challenges, the paper explores the relevance of ISO/IEC and\nIEEE standards, focusing on risk management, data quality, bias mitigation, and\ngovernance. It proposes a Fundamental Rights Impact Assessment (FRIA)\nmethodology, a gate-based framework designed to isolate and assess risks\nthrough phases including an AI system overview, a human rights checklist, an\nimpact assessment, and a final output phase. A filtering mechanism tailors the\nassessment to the system's characteristics, targeting areas like\naccountability, AI literacy, data governance, and transparency.\n  The paper illustrates the FRIA methodology through a fictional case study of\nan automated healthcare triage service. The structured approach enables\nsystematic filtering, comprehensive risk assessment, and mitigation planning,\neffectively prioritizing critical risks and providing clear remediation\nstrategies. This promotes better alignment with human rights principles and\nenhances regulatory compliance.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.18994v1",
    "published_date": "2025-03-23 19:10:14 UTC",
    "updated_date": "2025-03-23 19:10:14 UTC"
  },
  {
    "arxiv_id": "2503.18172v3",
    "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering",
    "authors": [
      "Zixin Chen",
      "Sicheng Song",
      "Kashun Shum",
      "Yanna Lin",
      "Rui Sheng",
      "Huamin Qu"
    ],
    "abstract": "Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages in total. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.18172v3",
    "published_date": "2025-03-23 18:56:33 UTC",
    "updated_date": "2025-04-15 15:48:57 UTC"
  },
  {
    "arxiv_id": "2503.18170v1",
    "title": "Self-Attention Diffusion Models for Zero-Shot Biomedical Image Segmentation: Unlocking New Frontiers in Medical Imaging",
    "authors": [
      "Abderrachid Hamrani",
      "Anuradha Godavarty"
    ],
    "abstract": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.18170v1",
    "published_date": "2025-03-23 18:47:12 UTC",
    "updated_date": "2025-03-23 18:47:12 UTC"
  },
  {
    "arxiv_id": "2503.18168v1",
    "title": "Strategic Prompt Pricing for AIGC Services: A User-Centric Approach",
    "authors": [
      "Xiang Li",
      "Bing Luo",
      "Jianwei Huang",
      "Yuan Luo"
    ],
    "abstract": "The rapid growth of AI-generated content (AIGC) services has created an\nurgent need for effective prompt pricing strategies, yet current approaches\noverlook users' strategic two-step decision-making process in selecting and\nutilizing generative AI models. This oversight creates two key technical\nchallenges: quantifying the relationship between user prompt capabilities and\ngeneration outcomes, and optimizing platform payoff while accounting for\nheterogeneous user behaviors. We address these challenges by introducing prompt\nambiguity, a theoretical framework that captures users' varying abilities in\nprompt engineering, and developing an Optimal Prompt Pricing (OPP) algorithm.\nOur analysis reveals a counterintuitive insight: users with higher prompt\nambiguity (i.e., lower capability) exhibit non-monotonic prompt usage patterns,\nfirst increasing then decreasing with ambiguity levels, reflecting complex\nchanges in marginal utility. Experimental evaluation using a character-level\nGPT-like model demonstrates that our OPP algorithm achieves up to 31.72%\nimprovement in platform payoff compared to existing pricing mechanisms,\nvalidating the importance of user-centric prompt pricing in AIGC services.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted in WiOpt 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.18168v1",
    "published_date": "2025-03-23 18:41:06 UTC",
    "updated_date": "2025-03-23 18:41:06 UTC"
  },
  {
    "arxiv_id": "2503.18167v2",
    "title": "Evaluating Negative Sampling Approaches for Neural Topic Models",
    "authors": [
      "Suman Adhya",
      "Avishek Lahiri",
      "Debarshi Kumar Sanyal",
      "Partha Pratim Das"
    ],
    "abstract": "Negative sampling has emerged as an effective technique that enables deep\nlearning models to learn better representations by introducing the paradigm of\nlearn-to-compare. The goal of this approach is to add robustness to deep\nlearning models to learn better representation by comparing the positive\nsamples against the negative ones. Despite its numerous demonstrations in\nvarious areas of computer vision and natural language processing, a\ncomprehensive study of the effect of negative sampling in an unsupervised\ndomain like topic modeling has not been well explored. In this paper, we\npresent a comprehensive analysis of the impact of different negative sampling\nstrategies on neural topic models. We compare the performance of several\npopular neural topic models by incorporating a negative sampling technique in\nthe decoder of variational autoencoder-based neural topic models. Experiments\non four publicly available datasets demonstrate that integrating negative\nsampling into topic models results in significant enhancements across multiple\naspects, including improved topic coherence, richer topic diversity, and more\naccurate document classification. Manual evaluations also indicate that the\ninclusion of negative sampling into neural topic models enhances the quality of\nthe generated topics. These findings highlight the potential of negative\nsampling as a valuable tool for advancing the effectiveness of neural topic\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is available at: https://github.com/AdhyaSuman/Eval_NegTM",
    "pdf_url": "http://arxiv.org/pdf/2503.18167v2",
    "published_date": "2025-03-23 18:39:01 UTC",
    "updated_date": "2025-03-25 05:53:08 UTC"
  },
  {
    "arxiv_id": "2503.18162v1",
    "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation",
    "authors": [
      "Hui Xue",
      "Sarah M. Hooper",
      "Iain Pierce",
      "Rhodri H. Davies",
      "John Stairs",
      "Joseph Naegele",
      "Adrienne E. Campbell-Washburn",
      "Charlotte Manisty",
      "James C. Moon",
      "Thomas A. Treibel",
      "Peter Kellman",
      "Michael S. Hansen"
    ],
    "abstract": "To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18162v1",
    "published_date": "2025-03-23 18:16:36 UTC",
    "updated_date": "2025-03-23 18:16:36 UTC"
  },
  {
    "arxiv_id": "2503.18161v1",
    "title": "Active Inference for Energy Control and Planning in Smart Buildings and Communities",
    "authors": [
      "Seyyed Danial Nazemi",
      "Mohsen A. Jafari",
      "Andrea Matta"
    ],
    "abstract": "Active Inference (AIF) is emerging as a powerful framework for\ndecision-making under uncertainty, yet its potential in engineering\napplications remains largely unexplored. In this work, we propose a novel\ndual-layer AIF architecture that addresses both building-level and\ncommunity-level energy management. By leveraging the free energy principle,\neach layer adapts to evolving conditions and handles partial observability\nwithout extensive sensor information and respecting data privacy. We validate\nthe continuous AIF model against both a perfect optimization baseline and a\nreinforcement learning-based approach. We also test the community AIF framework\nunder extreme pricing scenarios. The results highlight the model's robustness\nin handling abrupt changes. This study is the first to show how a distributed\nAIF works in engineering. It also highlights new opportunities for\nprivacy-preserving and uncertainty-aware control strategies in engineering\napplications.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "Submitted to IEEE CASE 2025 (IEEE 21st International Conference on\n  Automation Science and Engineering)",
    "pdf_url": "http://arxiv.org/pdf/2503.18161v1",
    "published_date": "2025-03-23 18:03:01 UTC",
    "updated_date": "2025-03-23 18:03:01 UTC"
  },
  {
    "arxiv_id": "2503.18159v1",
    "title": "DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via Personalizer-Guided Distillation",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Ming Lu",
      "Hui Chen",
      "Feng Tian"
    ],
    "abstract": "Real-time speech-driven 3D facial animation has been attractive in academia\nand industry. Traditional methods mainly focus on learning a deterministic\nmapping from speech to animation. Recent approaches start to consider the\nnondeterministic fact of speech-driven 3D face animation and employ the\ndiffusion model for the task. Existing diffusion-based methods can improve the\ndiversity of facial animation. However, personalized speaking styles conveying\naccurate lip language is still lacking, besides, efficiency and compactness\nstill need to be improved. In this work, we propose DiffusionTalker to address\nthe above limitations via personalizer-guided distillation. In terms of\npersonalization, we introduce a contrastive personalizer that learns identity\nand emotion embeddings to capture speaking styles from audio. We further\npropose a personalizer enhancer during distillation to enhance the influence of\nembeddings on facial animation. For efficiency, we use iterative distillation\nto reduce the steps required for animation generation and achieve more than 8x\nspeedup in inference. To achieve compactness, we distill the large teacher\nmodel into a smaller student model, reducing our model's storage by 86.4\\%\nwhile minimizing performance loss. After distillation, users can derive their\nidentity and emotion embeddings from audio to quickly create personalized\nanimations that reflect specific speaking styles. Extensive experiments are\nconducted to demonstrate that our method outperforms state-of-the-art methods.\nThe code will be released at: https://github.com/ChenVoid/DiffusionTalker.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICME2025",
    "pdf_url": "http://arxiv.org/pdf/2503.18159v1",
    "published_date": "2025-03-23 17:55:54 UTC",
    "updated_date": "2025-03-23 17:55:54 UTC"
  },
  {
    "arxiv_id": "2503.18156v1",
    "title": "Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act",
    "authors": [
      "Bram Rijsbosch",
      "Gijs van Dijck",
      "Konrad Kollnig"
    ],
    "abstract": "AI-generated images have become so good in recent years that individuals\ncannot distinguish them any more from \"real\" images. This development creates a\nseries of societal risks, and challenges our perception of what is true and\nwhat is not, particularly with the emergence of \"deep fakes\" that impersonate\nreal individuals. Watermarking, a technique that involves embedding identifying\ninformation within images to indicate their AI-generated nature, has emerged as\na primary mechanism to address the risks posed by AI-generated images. The\nimplementation of watermarking techniques is now becoming a legal requirement\nin many jurisdictions, including under the new 2024 EU AI Act. Despite the\nwidespread use of AI image generation systems, the current status of\nwatermarking implementation remains largely unexamined. Moreover, the practical\nimplications of the AI Act's watermarking requirements have not previously been\nstudied. The present paper therefore both provides an empirical analysis of 50\nof the most widely used AI systems for image generation, and embeds this\nempirical analysis into a legal analysis of the AI Act. We identify four\ncategories of generative AI image systems relevant under the AI Act, outline\nthe legal obligations for each category, and find that only a minority number\nof providers currently implement adequate watermarking practices.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 7 figures, note that this work has not been published in a\n  peer reviewed venue yet. While we have made our best effort to ensure the\n  validity of our findings, it is therefore still work in progress and\n  potentially subject to change",
    "pdf_url": "http://arxiv.org/pdf/2503.18156v1",
    "published_date": "2025-03-23 17:55:33 UTC",
    "updated_date": "2025-03-23 17:55:33 UTC"
  },
  {
    "arxiv_id": "2503.18151v1",
    "title": "Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging",
    "authors": [
      "Siwon Kim",
      "Wooyung Yun",
      "Jeongbin Oh",
      "Soomok Lee"
    ],
    "abstract": "Deep learning has emerged as the predominant solution for classifying medical\nimages. We intend to apply these developments to the ultra-widefield (UWF)\nretinal imaging dataset. Since UWF images can accurately diagnose various\nretina diseases, it is very important to clas sify them accurately and prevent\nthem with early treatment. However, processing images manually is\ntime-consuming and labor-intensive, and there are two challenges to automating\nthis process. First, high perfor mance usually requires high computational\nresources. Artificial intelli gence medical technology is better suited for\nplaces with limited medical resources, but using high-performance processing\nunits in such environ ments is challenging. Second, the problem of the accuracy\nof colour fun dus photography (CFP) methods. In general, the UWF method\nprovides more information for retinal diagnosis than the CFP method, but most\nof the research has been conducted based on the CFP method. Thus, we\ndemonstrate that these problems can be efficiently addressed in low performance\nunits using methods such as strategic data augmentation and model ensembles,\nwhich balance performance and computational re sources while utilizing UWF\nimages.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18151v1",
    "published_date": "2025-03-23 17:43:24 UTC",
    "updated_date": "2025-03-23 17:43:24 UTC"
  },
  {
    "arxiv_id": "2504.08744v1",
    "title": "ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses",
    "authors": [
      "Esmail Gumaan"
    ],
    "abstract": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts\n(MoE) architectures with Retrieval Augmented Generation (RAG) to advance the\nefficiency and accuracy of knowledge-intensive language modeling. We propose a\ndynamic retrieval gating mechanism coupled with expert routing, enabling the\nmodel to selectively consult an external knowledge store or rely on specialized\ninternal experts based on the query's needs. The paper lays out the theoretical\nfoundations of ExpertRAG, including a probabilistic formulation that treats\nretrieval and expert selection as latent decisions, and mathematical\njustifications for its efficiency in both computation and knowledge\nutilization. We derive formulae to quantify the expected computational cost\nsavings from selective retrieval and the capacity gains from sparse expert\nutilization. A comparative analysis positions ExpertRAG against standard RAG\n(with always-on retrieval) and pure MoE models (e.g., Switch Transformer,\nMixtral) to highlight its unique balance between parametric knowledge and\nnon-parametric retrieval. We also outline an experimental validation strategy,\nproposing benchmarks and evaluation protocols to test ExpertRAG's performance\non factual recall, generalization, and inference efficiency. The proposed\nframework, although presented theoretically, is supported by insights from\nprior work in RAG and MoE, and is poised to provide more factual, efficient,\nand adaptive generation by leveraging the best of both paradigms. In summary,\nExpertRAG contributes a new perspective on scaling and augmenting language\nmodels, backed by a thorough analysis and a roadmap for empirical validation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "30 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08744v1",
    "published_date": "2025-03-23 17:26:23 UTC",
    "updated_date": "2025-03-23 17:26:23 UTC"
  },
  {
    "arxiv_id": "2503.18142v1",
    "title": "LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert Space",
    "authors": [
      "Zhangyu Wang",
      "Jielu Zhang",
      "Zhongliang Zhou",
      "Qian Cao",
      "Nemin Wu",
      "Zeping Liu",
      "Lan Mu",
      "Yang Song",
      "Yiqun Xie",
      "Ni Lao",
      "Gengchen Mai"
    ],
    "abstract": "Image geolocalization is a fundamental yet challenging task, aiming at\ninferring the geolocation on Earth where an image is taken. Existing methods\napproach it either via grid-based classification or via image retrieval. Their\nperformance significantly suffers when the spatial distribution of test images\ndoes not align with such choices. To address these limitations, we propose to\nleverage diffusion as a mechanism for image geolocalization. To avoid the\nproblematic manifold reprojection step in diffusion, we developed a novel\nspherical positional encoding-decoding framework, which encodes points on a\nspherical surface (e.g., geolocations on Earth) into a Hilbert space of\nSpherical Harmonics coefficients and decodes points (geolocations) by\nmode-seeking. We call this type of position encoding Spherical Harmonics Dirac\nDelta (SHDD) Representation. We also propose a novel SirenNet-based\narchitecture called CS-UNet to learn the conditional backward process in the\nlatent SHDD space by minimizing a latent KL-divergence loss. We train a\nconditional latent diffusion model called LocDiffusion that generates\ngeolocations under the guidance of images -- to the best of our knowledge, the\nfirst generative model for image geolocalization by diffusing geolocation\ninformation in a hidden location embedding space. We evaluate our method\nagainst SOTA image geolocalization baselines. LocDiffusion achieves competitive\ngeolocalization performance and demonstrates significantly stronger\ngeneralizability to unseen geolocations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18142v1",
    "published_date": "2025-03-23 17:15:26 UTC",
    "updated_date": "2025-03-23 17:15:26 UTC"
  },
  {
    "arxiv_id": "2503.18991v2",
    "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment",
    "authors": [
      "Ruoxi Cheng",
      "Haoxuan Ma",
      "Weixin Wang"
    ],
    "abstract": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The three authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2503.18991v2",
    "published_date": "2025-03-23 16:40:29 UTC",
    "updated_date": "2025-05-06 13:47:34 UTC"
  },
  {
    "arxiv_id": "2503.18130v1",
    "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
    "authors": [
      "Juntao Dai",
      "Taiye Chen",
      "Yaodong Yang",
      "Qian Zheng",
      "Gang Pan"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for\naligning large language models (LLMs) with human values. However, reward\nover-optimization remains an open challenge leading to discrepancies between\nthe performance of LLMs under the reward model and the true human objectives. A\nprimary contributor to reward over-optimization is the extrapolation error that\narises when the reward model evaluates out-of-distribution (OOD) responses.\nHowever, current methods still fail to prevent the increasing frequency of OOD\nresponse generation during the reinforcement learning (RL) process and are not\neffective at handling extrapolation errors from OOD responses. In this work, we\npropose the Behavior-Supported Policy Optimization (BSPO) method to mitigate\nthe reward over-optimization issue. Specifically, we define behavior policy as\nthe next token distribution of the reward training dataset to model the\nin-distribution (ID) region of the reward model. Building on this, we introduce\nthe behavior-supported Bellman operator to regularize the value function,\npenalizing all OOD values without impacting the ID ones. Consequently, BSPO\nreduces the generation of OOD responses during the RL process, thereby avoiding\noverestimation caused by the reward model's extrapolation errors.\nTheoretically, we prove that BSPO guarantees a monotonic improvement of the\nsupported policy until convergence to the optimal behavior-supported policy.\nEmpirical results from extensive experiments show that BSPO outperforms\nbaselines in preventing reward over-optimization due to OOD evaluation and\nfinding the optimal ID policy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.18130v1",
    "published_date": "2025-03-23 16:20:59 UTC",
    "updated_date": "2025-03-23 16:20:59 UTC"
  },
  {
    "arxiv_id": "2503.18129v1",
    "title": "GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks",
    "authors": [
      "Varvara Krechetova",
      "Denis Kochedykov"
    ],
    "abstract": "In this paper, we establish a benchmark for evaluating large language models\n(LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.\nWe assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini\n2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent\nequipped with 23 geospatial functions. Our benchmark comprises tasks across\nfour categories of increasing complexity, with both solvable and intentionally\nunsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge\nevaluation framework to compare agent solutions against reference\nimplementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall\nperformance, with Claude models excelling on solvable tasks while OpenAI models\nbetter identify unsolvable scenarios. We observe significant differences in\ntoken usage, with Anthropic models consuming substantially more tokens than\ncompetitors. Common errors include misunderstanding geometrical relationships,\nrelying on outdated knowledge, and inefficient data manipulation. The resulting\nbenchmark set, evaluation framework, and data generation pipeline are released\nas open-source resources, providing one more standardized method for ongoing\nevaluation of LLMs for GeoAI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX",
    "pdf_url": "http://arxiv.org/pdf/2503.18129v1",
    "published_date": "2025-03-23 16:20:14 UTC",
    "updated_date": "2025-03-23 16:20:14 UTC"
  },
  {
    "arxiv_id": "2503.18102v1",
    "title": "AgentRxiv: Towards Collaborative Autonomous Research",
    "authors": [
      "Samuel Schmidgall",
      "Michael Moor"
    ],
    "abstract": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18102v1",
    "published_date": "2025-03-23 15:16:42 UTC",
    "updated_date": "2025-03-23 15:16:42 UTC"
  },
  {
    "arxiv_id": "2503.18085v1",
    "title": "Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach",
    "authors": [
      "Rochana Chaturvedi",
      "Peyman Baghershahi",
      "Sourav Medya",
      "Barbara Di Eugenio"
    ],
    "abstract": "Temporal information extraction from unstructured text is essential for\ncontextualizing events and deriving actionable insights, particularly in the\nmedical domain. We address the task of extracting clinical events and their\ntemporal relations using the well-studied I2B2 2012 Temporal Relations\nChallenge corpus. This task is inherently challenging due to complex clinical\nlanguage, long documents, and sparse annotations. We introduce GRAPHTREX, a\nnovel method integrating span-based entity-relation extraction, clinical large\npre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)\nto capture local and global dependencies. Our HGT component facilitates\ninformation propagation across the document through innovative global landmarks\nthat bridge distant entities. Our method improves the state-of-the-art with\n5.5% improvement in the tempeval $F_1$ score over the previous best and up to\n8.9% improvement on long-range relations, which presents a formidable\nchallenge. This work not only advances temporal information extraction but also\nlays the groundwork for improved diagnostic and prognostic models through\nenhanced temporal reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Introducing a novel method for joint extraction of medical events and\n  temporal relations from free-text, leveraging clinical LPLMs and\n  Heterogeneous Graph Transformers, achieving a 5.5% improvement over the\n  previous state-of-the-art and up to 8.9% on long-range relations",
    "pdf_url": "http://arxiv.org/pdf/2503.18085v1",
    "published_date": "2025-03-23 14:34:49 UTC",
    "updated_date": "2025-03-23 14:34:49 UTC"
  },
  {
    "arxiv_id": "2503.18072v1",
    "title": "On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish",
    "authors": [
      "Germán Capdehourat",
      "Isabel Amigo",
      "Brian Lorenzo",
      "Joaquín Trigo"
    ],
    "abstract": "Grading is a time-consuming and laborious task that educators must face. It\nis an important task since it provides feedback signals to learners, and it has\nbeen demonstrated that timely feedback improves the learning process. In recent\nyears, the irruption of LLMs has shed light on the effectiveness of automatic\ngrading. In this paper, we explore the performance of different LLMs and\nprompting techniques in automatically grading short-text answers to open-ended\nquestions. Unlike most of the literature, our study focuses on a use case where\nthe questions, answers, and prompts are all in Spanish. Experimental results\ncomparing automatic scores to those of human-expert evaluators show good\noutcomes in terms of accuracy, precision and consistency for advanced LLMs,\nboth open and proprietary. Results are notably sensitive to prompt styles,\nsuggesting biases toward certain words or content in the prompt. However, the\nbest combinations of models and prompt strategies, consistently surpasses an\naccuracy of 95% in a three-level grading task, which even rises up to more than\n98% when the it is simplified to a binary right or wrong rating problem, which\ndemonstrates the potential that LLMs have to implement this type of automation\nin education applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18072v1",
    "published_date": "2025-03-23 13:43:27 UTC",
    "updated_date": "2025-03-23 13:43:27 UTC"
  },
  {
    "arxiv_id": "2503.21797v1",
    "title": "A Novel Two-Phase Cooperative Co-evolution Framework for Large-Scale Global Optimization with Complex Overlapping",
    "authors": [
      "Wenjie Qiu",
      "Hongshu Guo",
      "Zeyuan Ma",
      "Yue-Jiao Gong"
    ],
    "abstract": "Cooperative Co-evolution, through the decomposition of the problem space, is\na primary approach for solving large-scale global optimization problems.\nTypically, when the subspaces are disjoint, the algorithms demonstrate\nsignificantly both effectiveness and efficiency compared to non-decomposition\nalgorithms. However, the presence of overlapping variables complicates the\ndecomposition process and adversely affects the performance of cooperative\nco-evolution. In this study, we propose a novel two-phase cooperative\nco-evolution framework to address large-scale global optimization problems with\ncomplex overlapping. An effective method for decomposing overlapping problems,\ngrounded in their mathematical properties, is embedded within the framework.\nAdditionally, a customizable benchmark for overlapping problems is introduced\nto extend existing benchmarks and facilitate experimentation. Extensive\nexperiments demonstrate that the algorithm instantiated within our framework\nsignificantly outperforms existing algorithms. The results reveal the\ncharacteristics of overlapping problems and highlight the differing strengths\nof cooperative co-evolution and non-decomposition algorithms. Our work is\nopen-source and accessible at: https://github.com/GMC-DRL/HCC.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at ACM GECCO 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.21797v1",
    "published_date": "2025-03-23 13:21:11 UTC",
    "updated_date": "2025-03-23 13:21:11 UTC"
  },
  {
    "arxiv_id": "2503.18065v1",
    "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation",
    "authors": [
      "Ziming Wei",
      "Bingqian Lin",
      "Yunshuang Nie",
      "Jiaqi Chen",
      "Shikui Ma",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "abstract": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18065v1",
    "published_date": "2025-03-23 13:18:17 UTC",
    "updated_date": "2025-03-23 13:18:17 UTC"
  },
  {
    "arxiv_id": "2503.18063v1",
    "title": "Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning",
    "authors": [
      "Pieyi Zhang",
      "Richong Zhang",
      "Zhijie Nie"
    ],
    "abstract": "Multi-task prompt tuning utilizes multiple high-resource source tasks to\nimprove performance on low-source target tasks. Existing approaches transfer\nthe soft prompt trained by combining all source tasks or a single\n``high-similar'' source task one-time-only. However, we find that the optimal\ntransfer performance often comes from a combination of source tasks, which is\nneither one nor all. Further, we find that the similarity between source and\ntarget tasks also changes dynamically during fine-tuning after transfering,\nmaking similarity calculation in the initiation stage inadequate. To address\nthese issues, we propose a method called Dynamic Task Vector Grouping (DTVG),\nwhose core ideas contain (1) measuring the task similarity with task vectors\ninstead of soft prompt, (2) grouping the optimal source task combination based\non two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3)\ndynamically updating the combination in each iteration step. Extensive\nexperiments on the 26 NLP datasets under different settings demonstrate that\nDTVG effectively groups similar source tasks while reducing negative transfer,\nachieving the start-of-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.18063v1",
    "published_date": "2025-03-23 13:09:04 UTC",
    "updated_date": "2025-03-23 13:09:04 UTC"
  },
  {
    "arxiv_id": "2504.03690v1",
    "title": "Learning to Interfere in Non-Orthogonal Multiple-Access Joint Source-Channel Coding",
    "authors": [
      "Selim F. Yilmaz",
      "Can Karamanli",
      "Deniz Gunduz"
    ],
    "abstract": "We consider multiple transmitters aiming to communicate their source signals\n(e.g., images) over a multiple access channel (MAC). Conventional communication\nsystems minimize interference by orthogonally allocating resources (time and/or\nbandwidth) among users, which limits their capacity. We introduce a machine\nlearning (ML)-aided wireless image transmission method that merges compression\nand channel coding using a multi-view autoencoder, which allows the\ntransmitters to use all the available channel resources simultaneously,\nresulting in a non-orthogonal multiple access (NOMA) scheme. The receiver must\nrecover all the images from the received superposed signal, while also\nassociating each image with its transmitter. Traditional ML models deal with\nindividual samples, whereas our model allows signals from different users to\ninterfere in order to leverage gains from NOMA under limited bandwidth and\npower constraints. We introduce a progressive fine-tuning algorithm that\ndoubles the number of users at each iteration, maintaining initial performance\nwith orthogonalized user-specific projections, which is then improved through\nfine-tuning steps. Remarkably, our method scales up to 16 users and beyond,\nwith only a 0.6% increase in the number of trainable parameters compared to a\nsingle-user model, significantly enhancing recovered image quality and\noutperforming existing NOMA-based methods over a wide range of datasets,\nmetrics, and channel conditions. Our approach paves the way for more efficient\nand robust multi-user communication systems, leveraging innovative ML\ncomponents and strategies.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "18 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.03690v1",
    "published_date": "2025-03-23 12:27:20 UTC",
    "updated_date": "2025-03-23 12:27:20 UTC"
  },
  {
    "arxiv_id": "2503.18025v1",
    "title": "Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration",
    "authors": [
      "Alexandre Perez-Lebel",
      "Gael Varoquaux",
      "Sanmi Koyejo",
      "Matthieu Doutreligne",
      "Marine Le Morvan"
    ],
    "abstract": "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18025v1",
    "published_date": "2025-03-23 10:52:36 UTC",
    "updated_date": "2025-03-23 10:52:36 UTC"
  },
  {
    "arxiv_id": "2504.08742v1",
    "title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents",
    "authors": [
      "Nicholas Sukiennik",
      "Haoyu Wang",
      "Zailin Zeng",
      "Chen Gao",
      "Yong Li"
    ],
    "abstract": "An increasing reliance on recommender systems has led to concerns about the\ncreation of filter bubbles on social media, especially on short video platforms\nlike TikTok. However, their formation is still not entirely understood due to\nthe complex dynamics between recommendation algorithms and user feedback. In\nthis paper, we aim to shed light on these dynamics using a large language\nmodel-based simulation framework. Our work employs real-world short-video data\ncontaining rich video content information and detailed user-agents to\nrealistically simulate the recommendation-feedback cycle. Through large-scale\nsimulations, we demonstrate that LLMs can replicate real-world user-recommender\ninteractions, uncovering key mechanisms driving filter bubble formation. We\nidentify critical factors, such as demographic features and category attraction\nthat exacerbate content homogenization. To mitigate this, we design and test\ninterventions including various cold-start and feedback weighting strategies,\nshowing measurable reductions in filter bubble effects. Our framework enables\nrapid prototyping of recommendation strategies, offering actionable solutions\nto enhance content diversity in real-world systems. Furthermore, we analyze how\nLLM-inherent biases may propagate through recommendations, proposing safeguards\nto promote equity for vulnerable groups, such as women and low-income\npopulations. By examining the interplay between recommendation and LLM agents,\nthis work advances a deeper understanding of algorithmic bias and provides\npractical tools to promote inclusive digital spaces.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Submitted to IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08742v1",
    "published_date": "2025-03-23 10:35:58 UTC",
    "updated_date": "2025-03-23 10:35:58 UTC"
  },
  {
    "arxiv_id": "2503.18018v1",
    "title": "Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?",
    "authors": [
      "Aabid Karim",
      "Abdul Karim",
      "Bhoomika Lohana",
      "Matt Keon",
      "Jaswinder Singh",
      "Abdul Sattar"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced various fields,\nparticularly coding, mathematical reasoning, and logical problem solving.\nHowever, a critical question remains: Do these mathematical reasoning abilities\npersist when LLMs are presented with culturally adapted math problems?\nSpecifically, how do LLMs perform when faced with math problems embedded in\ncultural contexts that have no significant representation in main stream\nweb-scale AI training data? To explore this, we generated six synthetic\ncultural datasets from GSM8K, a widely used benchmark for assessing LLMs'\nmathematical reasoning skills. While preserving the mathematical logic and\nnumerical values of the original GSM8K test set, we modify cultural elements\nsuch as personal names, food items, place names, etc. These culturally adapted\ndatasets provide a more reliable framework for evaluating LLMs' mathematical\nreasoning under shifting cultural contexts. Our findings reveal that LLMs\nstruggle with math problems when cultural references change, even though the\nunderlying mathematical structure remains constant. Smaller models exhibit\ngreater performance drops compared to larger models. Interestingly, our results\nalso suggest that cultural familiarity can enhance mathematical reasoning. Even\nmodels with no explicit mathematical training but exposure to relevant cultural\ncontexts sometimes outperform larger, mathematically proficient models on\nculturally embedded math problems. This study highlights the impact of cultural\ncontext on the mathematical reasoning abilities of LLMs, underscoring the need\nfor more diverse and representative training data to improve robustness in\nreal-world applications. The benchmark data sets and script for reproducing the\nresults are available at\nhttps://github.com/akarim23131/Lost_in_Cultural_Translation",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18018v1",
    "published_date": "2025-03-23 10:35:39 UTC",
    "updated_date": "2025-03-23 10:35:39 UTC"
  },
  {
    "arxiv_id": "2503.18013v1",
    "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning",
    "authors": [
      "Yufei Zhan",
      "Yousong Zhu",
      "Shurong Zheng",
      "Hongyin Zhao",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project in development. Github:\n  https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1",
    "pdf_url": "http://arxiv.org/pdf/2503.18013v1",
    "published_date": "2025-03-23 10:21:14 UTC",
    "updated_date": "2025-03-23 10:21:14 UTC"
  },
  {
    "arxiv_id": "2503.18988v1",
    "title": "SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene Graph Manipulation",
    "authors": [
      "Haoliang Shang",
      "Hanyu Wu",
      "Guangyao Zhai",
      "Boyang Sun",
      "Fangjinhua Wang",
      "Federico Tombari",
      "Marc Pollefeys"
    ],
    "abstract": "Scene graphs capture complex relationships among objects, serving as strong\npriors for content generation and manipulation. Yet, reasonably manipulating\nscene graphs -- whether by adding nodes or modifying edges -- remains a\nchallenging and untouched task. Tasks such as adding a node to the graph or\nreasoning about a node's relationships with all others are computationally\nintractable, as even a single edge modification can trigger conflicts due to\nthe intricate interdependencies within the graph. To address these challenges,\nwe introduce SG-Tailor, an autoregressive model that predicts the conflict-free\nrelationship between any two nodes. SG-Tailor not only infers inter-object\nrelationships, including generating commonsense edges for newly added nodes but\nalso resolves conflicts arising from edge modifications to produce coherent,\nmanipulated graphs for downstream tasks. For node addition, the model queries\nthe target node and other nodes from the graph to predict the appropriate\nrelationships. For edge modification, SG-Tailor employs a Cut-And-Stitch\nstrategy to solve the conflicts and globally adjust the graph. Extensive\nexperiments demonstrate that SG-Tailor outperforms competing methods by a large\nmargin and can be seamlessly integrated as a plug-in module for scene\ngeneration and robotic manipulation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "The code will be available at https://github.com/josef5838/SG-Tailor",
    "pdf_url": "http://arxiv.org/pdf/2503.18988v1",
    "published_date": "2025-03-23 09:11:04 UTC",
    "updated_date": "2025-03-23 09:11:04 UTC"
  },
  {
    "arxiv_id": "2503.17994v1",
    "title": "Instructing the Architecture Search for Spatial-temporal Sequence Forecasting with LLM",
    "authors": [
      "Xin Xue",
      "Haoyi Zhou",
      "Tianyu Chen",
      "Shuai Zhang",
      "Yizhou Long",
      "Jianxin Li"
    ],
    "abstract": "Spatial-temporal sequence forecasting (STSF) is a long-standing research\nproblem with widespread real-world applications. Neural architecture search\n(NAS), which automates the neural network design, has been shown effective in\ntackling the STSF problem. However, the existing NAS methods for STSF focus on\ngenerating architectures in a time-consuming data-driven fashion, which heavily\nlimits their ability to use background knowledge and explore the complicated\nsearch trajectory. Large language models (LLMs) have shown remarkable ability\nin decision-making with comprehensive internal world knowledge, but how it\ncould benefit NAS for STSF remains unexplored. In this paper, we propose a\nnovel NAS method for STSF based on LLM. Instead of directly generate\narchitectures with LLM, We inspire the LLM's capability with a multi-level\nenhancement mechanism. Specifically, on the step-level, we decompose the\ngeneration task into decision steps with powerful prompt engineering and\ninspire LLM to serve as instructor for architecture search based on its\ninternal knowledge. On the instance-level, we utilize a one-step tuning\nframework to quickly evaluate the architecture instance and a memory bank to\ncumulate knowledge to improve LLM's search ability. On the task-level, we\npropose a two-stage architecture search, balancing the exploration stage and\noptimization stage, to reduce the possibility of being trapped in local optima.\nExtensive experimental results demonstrate that our method can achieve\ncompetitive effectiveness with superior efficiency against existing NAS methods\nfor STSF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17994v1",
    "published_date": "2025-03-23 08:59:04 UTC",
    "updated_date": "2025-03-23 08:59:04 UTC"
  },
  {
    "arxiv_id": "2503.17993v1",
    "title": "Predicting Multitasking in Manual and Automated Driving with Optimal Supervisory Control",
    "authors": [
      "Jussi Jokinen",
      "Patrick Ebel",
      "Tuomo Kujala"
    ],
    "abstract": "Modern driving involves interactive technologies that can divert attention,\nincreasing the risk of accidents. This paper presents a computational cognitive\nmodel that simulates human multitasking while driving. Based on optimal\nsupervisory control theory, the model predicts how multitasking adapts to\nvariations in driving demands, interactive tasks, and automation levels. Unlike\nprevious models, it accounts for context-dependent multitasking across\ndifferent degrees of driving automation. The model predicts longer in-car\nglances on straight roads and shorter glances during curves. It also\nanticipates increased glance durations with driver aids such as lane-centering\nassistance and their interaction with environmental demands. Validated against\ntwo empirical datasets, the model offers insights into driver multitasking amid\nevolving in-car technologies and automation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "H.1.2"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17993v1",
    "published_date": "2025-03-23 08:56:53 UTC",
    "updated_date": "2025-03-23 08:56:53 UTC"
  },
  {
    "arxiv_id": "2503.17987v2",
    "title": "Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning",
    "authors": [
      "Chenyu Zhang",
      "Lanjun Wang",
      "Yiwen Ma",
      "Wenhui Li",
      "An-An Liu"
    ],
    "abstract": "Text-to-Image(T2I) models typically deploy safety filters to prevent the\ngeneration of sensitive images. Unfortunately, recent jailbreaking attack\nmethods manually design prompts for the LLM to generate adversarial prompts,\nwhich effectively bypass safety filters while producing sensitive images,\nexposing safety vulnerabilities of T2I models. However, due to the LLM's\nlimited understanding of the T2I model and its safety filters, existing methods\nrequire numerous queries to achieve a successful attack, limiting their\npractical applicability. To address this issue, we propose Reason2Attack(R2A),\nwhich aims to enhance the LLM's reasoning capabilities in generating\nadversarial prompts by incorporating the jailbreaking attack into the\npost-training process of the LLM. Specifically, we first propose a CoT example\nsynthesis pipeline based on Frame Semantics, which generates adversarial\nprompts by identifying related terms and corresponding context illustrations.\nUsing CoT examples generated by the pipeline, we fine-tune the LLM to\nunderstand the reasoning path and format the output structure. Subsequently, we\nincorporate the jailbreaking attack task into the reinforcement learning\nprocess of the LLM and design an attack process reward that considers prompt\nlength, prompt stealthiness, and prompt effectiveness, aiming to further\nenhance reasoning accuracy. Extensive experiments on various T2I models show\nthat R2A achieves a better attack success ratio while requiring fewer queries\nthan baselines. Moreover, our adversarial prompts demonstrate strong attack\ntransferability across both open-source and commercial T2I models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper includes model-generated content that may contain\n  offensive or distressing material",
    "pdf_url": "http://arxiv.org/pdf/2503.17987v2",
    "published_date": "2025-03-23 08:40:39 UTC",
    "updated_date": "2025-04-19 07:17:52 UTC"
  },
  {
    "arxiv_id": "2503.17985v1",
    "title": "Optimizing Navigation And Chemical Application in Precision Agriculture With Deep Reinforcement Learning And Conditional Action Tree",
    "authors": [
      "Mahsa Khosravi",
      "Zhanhong Jiang",
      "Joshua R Waite",
      "Sarah Jonesc",
      "Hernan Torres",
      "Arti Singh",
      "Baskar Ganapathysubramanian",
      "Asheesh Kumar Singh",
      "Soumik Sarkar"
    ],
    "abstract": "This paper presents a novel reinforcement learning (RL)-based planning scheme\nfor optimized robotic management of biotic stresses in precision agriculture.\nThe framework employs a hierarchical decision-making structure with conditional\naction masking, where high-level actions direct the robot's exploration, while\nlow-level actions optimize its navigation and efficient chemical spraying in\naffected areas. The key objectives of optimization include improving the\ncoverage of infected areas with limited battery power and reducing chemical\nusage, thus preventing unnecessary spraying of healthy areas of the field. Our\nnumerical experimental results demonstrate that the proposed method,\nHierarchical Action Masking Proximal Policy Optimization (HAM-PPO),\nsignificantly outperforms baseline practices, such as LawnMower navigation +\nindiscriminate spraying (Carpet Spray), in terms of yield recovery and resource\nefficiency. HAM-PPO consistently achieves higher yield recovery percentages and\nlower chemical costs across a range of infection scenarios. The framework also\nexhibits robustness to observation noise and generalizability under diverse\nenvironmental conditions, adapting to varying infection ranges and spatial\ndistribution patterns.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "32 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17985v1",
    "published_date": "2025-03-23 08:38:13 UTC",
    "updated_date": "2025-03-23 08:38:13 UTC"
  },
  {
    "arxiv_id": "2503.17984v1",
    "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting",
    "authors": [
      "Maochen Yang",
      "Zekun Li",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ],
    "abstract": "Semi-supervised crowd counting is crucial for addressing the high annotation\ncosts of densely populated scenes. Although several methods based on\npseudo-labeling have been proposed, it remains challenging to effectively and\naccurately utilize unlabeled data. In this paper, we propose a novel framework\ncalled Taste More Taste Better (TMTB), which emphasizes both data and model\naspects. Firstly, we explore a data augmentation technique well-suited for the\ncrowd counting task. By inpainting the background regions, this technique can\neffectively enhance data diversity while preserving the fidelity of the entire\nscenes. Secondly, we introduce the Visual State Space Model as backbone to\ncapture the global context information from crowd scenes, which is crucial for\nextremely crowded, low-light, and adverse weather scenarios. In addition to the\ntraditional regression head for exact prediction, we employ an Anti-Noise\nclassification head to provide less exact but more accurate supervision, since\nthe regression head is sensitive to noise in manual annotations. We conduct\nextensive experiments on four benchmark datasets and show that our method\noutperforms state-of-the-art methods by a large margin. Code is publicly\navailable on https://github.com/syhien/taste_more_taste_better.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.17984v1",
    "published_date": "2025-03-23 08:38:01 UTC",
    "updated_date": "2025-03-23 08:38:01 UTC"
  },
  {
    "arxiv_id": "2503.17982v1",
    "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images",
    "authors": [
      "Yara AlaaEldin",
      "Francesca Odone"
    ],
    "abstract": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17982v1",
    "published_date": "2025-03-23 08:25:07 UTC",
    "updated_date": "2025-03-23 08:25:07 UTC"
  },
  {
    "arxiv_id": "2503.18987v1",
    "title": "Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization",
    "authors": [
      "Xiran Wang",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ],
    "abstract": "Domain generalization is proposed to address distribution shift, arising from\nstatistical disparities between training source and unseen target domains. The\nwidely used first-order meta-learning algorithms demonstrate strong performance\nfor domain generalization by leveraging the gradient matching theory, which\naims to establish balanced parameters across source domains to reduce\noverfitting to any particular domain. However, our analysis reveals that there\nare actually numerous directions to achieve gradient matching, with current\nmethods representing just one possible path. These methods actually overlook\nanother critical factor that the balanced parameters should be close to the\ncentroid of optimal parameters of each source domain. To address this, we\npropose a simple yet effective arithmetic meta-learning with\narithmetic-weighted gradients. This approach, while adhering to the principles\nof gradient matching, promotes a more precise balance by estimating the\ncentroid between domain-specific optimal parameters. Experimental results\nvalidate the effectiveness of our strategy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18987v1",
    "published_date": "2025-03-23 08:24:28 UTC",
    "updated_date": "2025-03-23 08:24:28 UTC"
  },
  {
    "arxiv_id": "2503.17979v1",
    "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities",
    "authors": [
      "Weixiang Zhao",
      "Xingyu Sui",
      "Jiahe Guo",
      "Yulin Hu",
      "Yang Deng",
      "Yanyan Zhao",
      "Bing Qin",
      "Wanxiang Che",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "abstract": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.17979v1",
    "published_date": "2025-03-23 08:18:51 UTC",
    "updated_date": "2025-03-23 08:18:51 UTC"
  },
  {
    "arxiv_id": "2503.17978v1",
    "title": "PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition",
    "authors": [
      "Dominique Nshimyimana",
      "Vitor Fortes Rey",
      "Sungho Suh",
      "Bo Zhou",
      "Paul Lukowicz"
    ],
    "abstract": "Human activity recognition (HAR) with deep learning models relies on large\namounts of labeled data, often challenging to obtain due to associated cost,\ntime, and labor. Self-supervised learning (SSL) has emerged as an effective\napproach to leverage unlabeled data through pretext tasks, such as masked\nreconstruction and multitask learning with signal processing-based data\naugmentations, to pre-train encoder models. However, such methods are often\nderived from computer vision approaches that disregard physical mechanisms and\nconstraints that govern wearable sensor data and the phenomena they reflect. In\nthis paper, we propose a physics-informed multi-task pre-training (PIM)\nframework for IMU-based HAR. PIM generates pre-text tasks based on the\nunderstanding of basic physical aspects of human motion: including movement\nspeed, angles of movement, and symmetry between sensor placements. Given a\nsensor signal, we calculate corresponding features using physics-based\nequations and use them as pretext tasks for SSL. This enables the model to\ncapture fundamental physical characteristics of human activities, which is\nespecially relevant for multi-sensor systems. Experimental evaluations on four\nHAR benchmark datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods, including data augmentation and masked\nreconstruction, in terms of accuracy and F1 score. We have observed gains of\nalmost 10\\% in macro f1 score and accuracy with only 2 to 8 labeled examples\nper class and up to 3% when there is no reduction in the amount of training\ndata.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17978v1",
    "published_date": "2025-03-23 08:16:01 UTC",
    "updated_date": "2025-03-23 08:16:01 UTC"
  },
  {
    "arxiv_id": "2503.17975v2",
    "title": "Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods",
    "authors": [
      "Yuzhi Li",
      "Haojun Xu",
      "Feng Tian"
    ],
    "abstract": "With the rising popularity of short video platforms, the demand for video\nproduction has increased substantially. However, high-quality video creation\ncontinues to rely heavily on professional editing skills and a nuanced\nunderstanding of visual language. To address this challenge, the Shot Sequence\nOrdering (SSO) task in AI-assisted video editing has emerged as a pivotal\napproach for enhancing video storytelling and the overall viewing experience.\nNevertheless, the progress in this field has been impeded by a lack of publicly\navailable benchmark datasets. In response, this paper introduces two novel\nbenchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ\nthe Kendall Tau distance as an evaluation metric for the SSO task and propose\nthe Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept\nof Cinematology Embedding, which incorporates movie metadata and shot labels as\nprior knowledge into the SSO model, and constructs the AVE-Meta dataset to\nvalidate the method's effectiveness. Experimental results indicate that the\nproposed loss function and method substantially enhance SSO task accuracy. All\ndatasets are publicly accessible at https://github.com/litchiar/ShotSeqBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17975v2",
    "published_date": "2025-03-23 08:04:45 UTC",
    "updated_date": "2025-03-25 11:37:52 UTC"
  },
  {
    "arxiv_id": "2503.18986v1",
    "title": "SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices",
    "authors": [
      "Jian Ma",
      "Xinchen Lyu",
      "Jun Jiang",
      "Qimei Cui",
      "Haipeng Yao",
      "Xiaofeng Tao"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on private, on-device data can\nempower tailored personalized AI agents. However, fine-tuning LLMs on\nresource-constrained edge devices faces significant challenges, including\nexcessive computation overhead, device heterogeneity, and data imbalance. This\npaper proposes SplitFrozen, a split learning framework that enables efficient\nLLM fine-tuning by strategically freezing device-side model layers while\ncentralizing parameter-efficient fine-tuning on the server. Our framework\npartitions LLMs into device-side frozen layers and server-side fine-tuning\nlayers, where heterogeneous resource-constrained devices execute only forward\npropagation. To minimize server-side training costs, we integrate Low-Rank\nAdaptation (LoRA) into the server-side layers. A pipeline parallelism strategy\nfurther optimizes training efficiency by decoupling device-server computations\nand leveraging decomposed backward propagation. Experiments on GPT-2 with the\nMRPC, MNLI-matched, and SST-2 datasets demonstrate that SplitFrozen outperforms\nFedLoRA and SplitLoRA by 69.4\\% model accuracy under extremely imbalanced data,\nwhile reducing up to 86.8\\% device-side computations and 50.2\\% total training\ntime. Experiments also validate the scalability of SplitFrozen on content\ngeneration task using Llama-3.2 model on GSM8K dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18986v1",
    "published_date": "2025-03-23 08:03:44 UTC",
    "updated_date": "2025-03-23 08:03:44 UTC"
  },
  {
    "arxiv_id": "2503.17973v1",
    "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos",
    "authors": [
      "Hanxiao Jiang",
      "Hao-Yu Hsu",
      "Kaifeng Zhang",
      "Hsin-Ni Yu",
      "Shenlong Wang",
      "Yunzhu Li"
    ],
    "abstract": "Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://jianghanxiao.github.io/phystwin-web/",
    "pdf_url": "http://arxiv.org/pdf/2503.17973v1",
    "published_date": "2025-03-23 07:49:19 UTC",
    "updated_date": "2025-03-23 07:49:19 UTC"
  },
  {
    "arxiv_id": "2503.18985v2",
    "title": "LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning",
    "authors": [
      "Xuan Liu",
      "Xiaobin Chang"
    ],
    "abstract": "In continual learning (CL), catastrophic forgetting often arises due to\nfeature drift. This challenge is particularly prominent in the exemplar-free\ncontinual learning (EFCL) setting, where samples from previous tasks cannot be\nretained, making it difficult to preserve prior knowledge. To address this\nissue, some EFCL methods aim to identify feature spaces that minimize the\nimpact on previous tasks while accommodating new ones. However, they rely on\nstatic features or outdated statistics stored from old tasks, which prevents\nthem from capturing the dynamic evolution of the feature space in CL, leading\nto performance degradation over time. In this paper, we introduce the\nDrift-Resistant Space (DRS), which effectively handles feature drifts without\nrequiring explicit feature modeling or the storage of previous tasks. A novel\nparameter-efficient fine-tuning approach called Low-Rank Adaptation Subtraction\n(LoRA-) is proposed to develop the DRS. This method subtracts the LoRA weights\nof old tasks from the initial pre-trained weight before processing new task\ndata to establish the DRS for model training. Therefore, LoRA- enhances\nstability, improves efficiency, and simplifies implementation. Furthermore,\nstabilizing feature drifts allows for better plasticity by learning with a\ntriplet loss. Our method consistently achieves state-of-the-art results,\nespecially for long task sequences, across multiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.18985v2",
    "published_date": "2025-03-23 07:38:53 UTC",
    "updated_date": "2025-03-31 12:47:09 UTC"
  },
  {
    "arxiv_id": "2503.18984v1",
    "title": "The Misinterpretable Evidence Conveyed by Arbitrary Codes",
    "authors": [
      "Guido Fioretti"
    ],
    "abstract": "Evidence Theory is a mathematical framework for handling imprecise reasoning\nin the context of a judge evaluating testimonies or a detective evaluating\ncues, rather than a gambler playing games of chance. In comparison to\nProbability Theory, it is better equipped to deal with ambiguous information\nand novel possibilities. Furthermore, arrival and evaluation of testimonies\nimplies a communication channel.\n  This paper explores the possibility of employing Evidence Theory to represent\narbitrary communication codes between and within living organisms. In this\npaper, different schemes are explored for living organisms incapable of\nanticipation, animals sufficiently sophisticated to be capable of\nextrapolation, and humans capable of reading one other's minds.",
    "categories": [
      "cs.AI",
      "nlin.AO"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.18984v1",
    "published_date": "2025-03-23 07:31:26 UTC",
    "updated_date": "2025-03-23 07:31:26 UTC"
  },
  {
    "arxiv_id": "2503.17965v1",
    "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts",
    "authors": [
      "Beining Xu",
      "Arkaitz Zubiaga"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17965v1",
    "published_date": "2025-03-23 07:03:10 UTC",
    "updated_date": "2025-03-23 07:03:10 UTC"
  },
  {
    "arxiv_id": "2503.17959v1",
    "title": "Dynamic Gradient Sparse Update for Edge Training",
    "authors": [
      "I-Hsuan Li",
      "Tian-Sheuan Chang"
    ],
    "abstract": "Training on edge devices enables personalized model fine-tuning to enhance\nreal-world performance and maintain data privacy. However, the gradient\ncomputation for backpropagation in the training requires significant memory\nbuffers to store intermediate features and compute losses. This is unacceptable\nfor memory-constrained edge devices such as microcontrollers. To tackle this\nissue, we propose a training acceleration method using dynamic gradient sparse\nupdates. This method updates the important channels and layers only and skips\ngradient computation for the less important channels and layers to reduce\nmemory usage for each update iteration. In addition, the channel selection is\ndynamic for different iterations to traverse most of the parameters in the\nupdate layers along the time dimension for better performance. The experimental\nresult shows that the proposed method enables an ImageNet pre-trained\nMobileNetV2 trained on CIFAR-10 to achieve an accuracy of 85.77\\% while\nupdating only 2\\% of convolution weights within 256KB on-chip memory. This\nresults in a remarkable 98\\% reduction in feature memory usage compared to\ndense model training.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "published in IEEE International Symposium on Circuits and Systems\n  (IEEE ISCAS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2503.17959v1",
    "published_date": "2025-03-23 06:32:12 UTC",
    "updated_date": "2025-03-23 06:32:12 UTC"
  },
  {
    "arxiv_id": "2503.18983v1",
    "title": "Confronting Catastrophic Risk: The International Obligation to Regulate Artificial Intelligence",
    "authors": [
      "Bryan Druzin",
      "Anatole Boute",
      "Michael Ramsden"
    ],
    "abstract": "While artificial intelligence (AI) holds enormous promise, many experts in\nthe field are warning that there is a non-trivial chance that the development\nof AI poses an existential threat to humanity. Existing regulatory initiative\ndo not address this threat but merely instead focus on discrete AI-related\nrisks such as consumer safety, cybersecurity, data protection, and privacy. In\nthe absence of regulatory action to address the possible risk of human\nextinction by AI, the question arises: What legal obligations, if any, does\npublic international law impose on states to regulate its development. Grounded\nin the precautionary principle, we argue that there exists an international\nobligation to mitigate the threat of human extinction by AI. Often invoked in\nrelation to environmental regulation and the regulation of potentially harmful\ntechnologies, the principle holds that in situations where there is the\npotential for significant harm, even in the absence of full scientific\ncertainty, preventive measures should not be postponed if delayed action may\nresult in irreversible consequences. We argue that the precautionary principle\nis a general principle of international law and, therefore, that there is a\npositive obligation on states under the right to life within international\nhuman rights law to proactively take regulatory action to mitigate the\npotential existential risk of AI. This is significant because, if an\ninternational obligation to regulate the development of AI can be established\nunder international law, then the basic legal framework would be in place to\naddress this evolving threat.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18983v1",
    "published_date": "2025-03-23 06:24:45 UTC",
    "updated_date": "2025-03-23 06:24:45 UTC"
  },
  {
    "arxiv_id": "2503.17955v2",
    "title": "Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products",
    "authors": [
      "Stefan Pasch",
      "Sun-Young Ha"
    ],
    "abstract": "Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2, a leading review platform for business\nsoftware and services. Based on widely adopted industry guidelines, we identify\nseven core HAI dimensions and examine their coverage and sentiment within the\nreviews. We find that the sentiment on four HAI dimensions-adaptability,\ncustomization, error recovery, and security-is positively associated with\noverall user satisfaction. Moreover, we show that engagement with HAI\ndimensions varies by professional background: Users with technical job roles\nare more likely to discuss system-focused aspects, such as reliability, while\nnon-technical users emphasize interaction-focused features like customization\nand feedback. Interestingly, the relationship between HAI sentiment and overall\nsatisfaction is not moderated by job role, suggesting that once an HAI\ndimension has been identified by users, its effect on satisfaction is\nconsistent across job roles.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17955v2",
    "published_date": "2025-03-23 06:16:49 UTC",
    "updated_date": "2025-03-25 01:44:50 UTC"
  },
  {
    "arxiv_id": "2503.18982v2",
    "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
    "authors": [
      "Liang Zhang",
      "Jionghao Lin",
      "John Sabatini",
      "Diego Zapata-Rivera",
      "Carol Forsyth",
      "Yang Jiang",
      "John Hollander",
      "Xiangen Hu",
      "Arthur C. Graesser"
    ],
    "abstract": "Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18982v2",
    "published_date": "2025-03-23 06:11:53 UTC",
    "updated_date": "2025-04-13 21:04:27 UTC"
  },
  {
    "arxiv_id": "2503.18981v1",
    "title": "FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional Similarity Knowledge Distillation",
    "authors": [
      "Ziqiao Weng",
      "Weidong Cai",
      "Bo Zhou"
    ],
    "abstract": "Federated learning (FL) enables privacy-preserving collaborative model\ntraining without direct data sharing. Model-heterogeneous FL (MHFL) extends\nthis paradigm by allowing clients to train personalized models with\nheterogeneous architectures tailored to their computational resources and\napplication-specific needs. However, existing MHFL methods predominantly rely\non centralized aggregation, which introduces scalability and efficiency\nbottlenecks, or impose restrictions requiring partially identical model\narchitectures across clients. While peer-to-peer (P2P) FL removes server\ndependence, it suffers from model drift and knowledge dilution, limiting its\neffectiveness in heterogeneous settings. To address these challenges, we\npropose FedSKD, a novel MHFL framework that facilitates direct knowledge\nexchange through round-robin model circulation, eliminating the need for\ncentralized aggregation while allowing fully heterogeneous model architectures\nacross clients. FedSKD's key innovation lies in multi-dimensional similarity\nknowledge distillation, which enables bidirectional cross-client knowledge\ntransfer at batch, pixel/voxel, and region levels for heterogeneous models in\nFL. This approach mitigates catastrophic forgetting and model drift through\nprogressive reinforcement and distribution alignment while preserving model\nheterogeneity. Extensive evaluations on fMRI-based autism spectrum disorder\ndiagnosis and skin lesion classification demonstrate that FedSKD outperforms\nstate-of-the-art heterogeneous and homogeneous FL baselines, achieving superior\npersonalization (client-specific accuracy) and generalization\n(cross-institutional adaptability). These findings underscore FedSKD's\npotential as a scalable and robust solution for real-world medical federated\nlearning applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 5 figure, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.18981v1",
    "published_date": "2025-03-23 05:33:10 UTC",
    "updated_date": "2025-03-23 05:33:10 UTC"
  },
  {
    "arxiv_id": "2504.03688v1",
    "title": "CLCR: Contrastive Learning-based Constraint Reordering for Efficient MILP Solving",
    "authors": [
      "Shuli Zeng",
      "Mengjie Zhou",
      "Sijia Zhang",
      "Yixiang Hu",
      "Feng Wu",
      "Xiang-Yang Li"
    ],
    "abstract": "Constraint ordering plays a critical role in the efficiency of Mixed-Integer\nLinear Programming (MILP) solvers, particularly for large-scale problems where\npoorly ordered constraints trigger increased LP iterations and suboptimal\nsearch trajectories. This paper introduces CLCR (Contrastive Learning-based\nConstraint Reordering), a novel framework that systematically optimizes\nconstraint ordering to accelerate MILP solving. CLCR first clusters constraints\nbased on their structural patterns and then employs contrastive learning with a\npointer network to optimize their sequence, preserving problem equivalence\nwhile improving solver efficiency. Experiments on benchmarks show CLCR reduces\nsolving time by 30% and LP iterations by 25% on average, without sacrificing\nsolution accuracy. This work demonstrates the potential of data-driven\nconstraint ordering to enhance optimization models, offering a new paradigm for\nbridging mathematical programming with machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03688v1",
    "published_date": "2025-03-23 05:01:43 UTC",
    "updated_date": "2025-03-23 05:01:43 UTC"
  },
  {
    "arxiv_id": "2503.18980v1",
    "title": "CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning",
    "authors": [
      "Yexin Li",
      "Pring Wong",
      "Hanfang Zhang",
      "Shuo Chen",
      "Siyuan Qi"
    ],
    "abstract": "Exploration remains a critical challenge in reinforcement learning, as many\nexisting methods either lack theoretical guarantees or fall short of practical\neffectiveness. In this paper, we introduce CAE, a lightweight algorithm that\nrepurposes the value networks in standard deep RL algorithms to drive\nexploration without introducing additional parameters. CAE utilizes any linear\nmulti-armed bandit technique and incorporates an appropriate scaling strategy,\nenabling efficient exploration with provable sub-linear regret bounds and\npractical stability. Notably, it is simple to implement, requiring only around\n10 lines of code. In complex tasks where learning an effective value network\nproves challenging, we propose CAE+, an extension of CAE that incorporates an\nauxiliary network. This extension increases the parameter count by less than 1%\nwhile maintaining implementation simplicity, adding only about 10 additional\nlines of code. Experiments on MuJoCo and MiniHack show that both CAE and CAE+\noutperform state-of-the-art baselines, bridging the gap between theoretical\nrigor and practical efficiency.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18980v1",
    "published_date": "2025-03-23 04:59:24 UTC",
    "updated_date": "2025-03-23 04:59:24 UTC"
  },
  {
    "arxiv_id": "2503.17941v1",
    "title": "Physics-Guided Multi-Fidelity DeepONet for Data-Efficient Flow Field Prediction",
    "authors": [
      "Sunwoong Yang",
      "Youngkyu Lee",
      "Namwoo Kang"
    ],
    "abstract": "This study presents an enhanced multi-fidelity deep operator network\n(DeepONet) framework for efficient spatio-temporal flow field prediction, with\nparticular emphasis on practical scenarios where high-fidelity data is scarce.\nWe introduce several key innovations to improve the framework's efficiency and\naccuracy. First, we enhance the DeepONet architecture by incorporating a merge\nnetwork that enables more complex feature interactions between operator and\ncoordinate spaces, achieving a 50.4% reduction in prediction error compared to\ntraditional dot-product operations. We further optimize the architecture\nthrough temporal positional encoding and point-based sampling strategies,\nachieving a 7.57% improvement in prediction accuracy while reducing training\ntime by 96% through efficient sampling and automatic mixed precision training.\nBuilding upon this foundation, we develop a transfer learning-based\nmulti-fidelity framework that leverages knowledge from pre-trained low-fidelity\nmodels to guide high-fidelity predictions. Our approach freezes the pre-trained\nbranch and trunk networks while making only the merge network trainable during\nhigh-fidelity training, preserving valuable low-fidelity representations while\nefficiently adapting to high-fidelity features. Through systematic\ninvestigation, we demonstrate that this fine-tuning strategy not only\nsignificantly outperforms linear probing and full-tuning alternatives but also\nsurpasses conventional multi-fidelity frameworks by up to 76%, while achieving\nup to 43.7% improvement in prediction accuracy compared to single-fidelity\ntraining. The core contribution lies in our novel time-derivative guided\nsampling approach: it maintains prediction accuracy equivalent to models\ntrained with the full dataset while requiring only 60% of the original\nhigh-fidelity samples.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17941v1",
    "published_date": "2025-03-23 04:48:18 UTC",
    "updated_date": "2025-03-23 04:48:18 UTC"
  },
  {
    "arxiv_id": "2503.17936v1",
    "title": "An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models",
    "authors": [
      "Riya Naik",
      "Ashwin Srinivasan",
      "Estrid He",
      "Swati Agarwal"
    ],
    "abstract": "Natural language as a medium for human-computer interaction has long been\nanticipated, has been undergoing a sea-change with the advent of Large Language\nModels (LLMs) with startling capacities for processing and generating language.\nMany of us now treat LLMs as modern-day oracles, asking it almost any kind of\nquestion. Unlike its Delphic predecessor, consulting an LLM does not have to be\na single-turn activity (ask a question, receive an answer, leave); and -- also\nunlike the Pythia -- it is widely acknowledged that answers from LLMs can be\nimproved with additional context. In this paper, we aim to study when we need\nmulti-turn interactions with LLMs to successfully get a question answered; or\nconclude that a question is unanswerable. We present a neural symbolic\nframework that models the interactions between human and LLM agents. Through\nthe proposed framework, we define incompleteness and ambiguity in the questions\nas properties deducible from the messages exchanged in the interaction, and\nprovide results from benchmark problems, in which the answer-correctness is\nshown to depend on whether or not questions demonstrate the presence of\nincompleteness or ambiguity (according to the properties we identify). Our\nresults show multi-turn interactions are usually required for datasets which\nhave a high proportion of incompleteness or ambiguous questions; and that that\nincreasing interaction length has the effect of reducing incompleteness or\nambiguity. The results also suggest that our measures of incompleteness and\nambiguity can be useful tools for characterising interactions with an LLM on\nquestion-answeringproblems",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17936v1",
    "published_date": "2025-03-23 04:34:30 UTC",
    "updated_date": "2025-03-23 04:34:30 UTC"
  },
  {
    "arxiv_id": "2503.17933v1",
    "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA",
    "authors": [
      "Justice Ou",
      "Tinglin Huang",
      "Yilun Zhao",
      "Ziyang Yu",
      "Peiqing Lu",
      "Rex Ying"
    ],
    "abstract": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17933v1",
    "published_date": "2025-03-23 04:26:06 UTC",
    "updated_date": "2025-03-23 04:26:06 UTC"
  },
  {
    "arxiv_id": "2503.17932v1",
    "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models",
    "authors": [
      "Xunguang Wang",
      "Wenxuan Wang",
      "Zhenlan Ji",
      "Zongjie Li",
      "Pingchuan Ma",
      "Daoyuan Wu",
      "Shuai Wang"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.17932v1",
    "published_date": "2025-03-23 04:23:07 UTC",
    "updated_date": "2025-03-23 04:23:07 UTC"
  },
  {
    "arxiv_id": "2503.17924v1",
    "title": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training",
    "authors": [
      "Zheng Wang",
      "Anna Cai",
      "Xinfeng Xie",
      "Zaifeng Pan",
      "Yue Guan",
      "Weiwei Chu",
      "Jie Wang",
      "Shikai Li",
      "Jianyu Huang",
      "Chris Cai",
      "Yuchen Hao",
      "Yufei Ding"
    ],
    "abstract": "In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for\nlarge language model training. We first thoroughly analyze the workload\nimbalance issue in LLM training and identify two primary sources of imbalance\nat the pipeline parallelism and context parallelism levels. Then, to address\nthe imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a\nworkload-aware variable-length document packing method to balance the\ncomputation and communication workload across micro-batches. Additionally, at\nthe context parallelism level, WLB-LLM introduces a novel fine-grained\nper-document sharding strategy, ensuring each worker within a context\nparallelism group has an identical workload. Comprehensive experiments under\ndifferent model scales demonstrate that WLB-LLM significantly mitigates the\nworkload imbalance during 4D parallelism LLM training and achieves an average\nspeedup of 1.23x when applying WLB-LLM in our internal LLM training framework.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "I.2.11"
    ],
    "primary_category": "cs.DC",
    "comment": "12 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.17924v1",
    "published_date": "2025-03-23 03:40:45 UTC",
    "updated_date": "2025-03-23 03:40:45 UTC"
  },
  {
    "arxiv_id": "2503.17915v1",
    "title": "Cat-AIR: Content and Task-Aware All-in-One Image Restoration",
    "authors": [
      "Jiachen Jiang",
      "Tianyu Ding",
      "Ke Zhang",
      "Jinxin Zhou",
      "Tianyi Chen",
      "Ilya Zharkov",
      "Zhihui Zhu",
      "Luming Liang"
    ],
    "abstract": "All-in-one image restoration seeks to recover high-quality images from\nvarious types of degradation using a single model, without prior knowledge of\nthe corruption source. However, existing methods often struggle to effectively\nand efficiently handle multiple degradation types. We present Cat-AIR, a novel\n\\textbf{C}ontent \\textbf{A}nd \\textbf{T}ask-aware framework for\n\\textbf{A}ll-in-one \\textbf{I}mage \\textbf{R}estoration. Cat-AIR incorporates\nan alternating spatial-channel attention mechanism that adaptively balances the\nlocal and global information for different tasks. Specifically, we introduce\ncross-layer channel attentions and cross-feature spatial attentions that\nallocate computations based on content and task complexity. Furthermore, we\npropose a smooth learning strategy that allows for seamless adaptation to new\nrestoration tasks while maintaining performance on existing ones. Extensive\nexperiments demonstrate that Cat-AIR achieves state-of-the-art results across a\nwide range of restoration tasks, requiring fewer FLOPs than previous methods,\nestablishing new benchmarks for efficient all-in-one image restoration.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17915v1",
    "published_date": "2025-03-23 03:25:52 UTC",
    "updated_date": "2025-03-23 03:25:52 UTC"
  },
  {
    "arxiv_id": "2503.20798v1",
    "title": "Payload-Aware Intrusion Detection with CMAE and Large Language Models",
    "authors": [
      "Yongcheol Kim",
      "Chanjae Lee",
      "Young Yoon"
    ],
    "abstract": "Intrusion Detection Systems (IDS) are crucial for identifying malicious\ntraffic, yet traditional signature-based methods struggle with zero-day attacks\nand high false positive rates. AI-driven packet-capture analysis offers a\npromising alternative. However, existing approaches rely heavily on flow-based\nor statistical features, limiting their ability to detect fine-grained attack\npatterns. This study proposes Xavier-CMAE, an enhanced Convolutional Multi-Head\nAttention Ensemble (CMAE) model that improves detection accuracy while reducing\ncomputational overhead. By replacing Word2Vec embeddings with a Hex2Int\ntokenizer and Xavier initialization, Xavier-CMAE eliminates pre-training,\naccelerates training, and achieves 99.971% accuracy with a 0.018% false\npositive rate, outperforming Word2Vec-based methods. Additionally, we introduce\nLLM-CMAE, which integrates pre-trained Large Language Model (LLM) tokenizers\ninto CMAE. While LLMs enhance feature extraction, their computational cost\nhinders real-time detection. LLM-CMAE balances efficiency and performance,\nreaching 99.969% accuracy with a 0.019% false positive rate. This work advances\nAI-powered IDS by (1) introducing a payload-based detection framework, (2)\nenhancing efficiency with Xavier-CMAE, and (3) integrating LLM tokenizers for\nimproved real-time detection.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.20798v1",
    "published_date": "2025-03-23 02:56:32 UTC",
    "updated_date": "2025-03-23 02:56:32 UTC"
  },
  {
    "arxiv_id": "2503.17903v1",
    "title": "GLADMamba: Unsupervised Graph-Level Anomaly Detection Powered by Selective State Space Model",
    "authors": [
      "Yali Fu",
      "Jindong Li",
      "Qi Wang",
      "Qianli Xing"
    ],
    "abstract": "Unsupervised graph-level anomaly detection (UGLAD) is a critical and\nchallenging task across various domains, such as social network analysis,\nanti-cancer drug discovery, and toxic molecule identification. However,\nexisting methods often struggle to capture the long-range dependencies\nefficiently and neglect the spectral information. Recently, selective State\nSpace Models (SSMs), particularly Mamba, have demonstrated remarkable\nadvantages in capturing long-range dependencies with linear complexity and a\nselection mechanism. Motivated by their success across various domains, we\npropose GLADMamba, a novel framework that adapts the selective state space\nmodel into UGLAD field. We design View-Fused Mamba (VFM) with a\nMamba-Transformer-style architecture to efficiently fuse information from\ndifferent views with a selective state mechanism. We also design\nSpectrum-Guided Mamba (SGM) with a Mamba-Transformer-style architecture to\nleverage the Rayleigh quotient to guide the embedding refining process.\nGLADMamba can dynamically focus on anomaly-related information while discarding\nirrelevant information for anomaly detection. To the best of our knowledge,\nthis is the first work to introduce Mamba and explicit spectral information to\nUGLAD. Extensive experiments on 12 real-world datasets demonstrate that\nGLADMamba outperforms existing state-of-the-art methods, achieving superior\nperformance in UGLAD. The code is available at\nhttps://github.com/Yali-F/GLADMamba.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17903v1",
    "published_date": "2025-03-23 02:40:17 UTC",
    "updated_date": "2025-03-23 02:40:17 UTC"
  },
  {
    "arxiv_id": "2503.18979v2",
    "title": "Threshold Crossings as Tail Events for Catastrophic AI Risk",
    "authors": [
      "Elija Perrier"
    ],
    "abstract": "We analyse circumstances in which bifurcation-driven jumps in AI systems are\nassociated with emergent heavy-tailed outcome distributions. By analysing how a\ncontrol parameter's random fluctuations near a catastrophic threshold generate\nextreme outcomes, we demonstrate in what circumstances the probability of a\nsudden, large-scale, transition aligns closely with the tail probability of the\nresulting damage distribution. Our results contribute to research in\nmonitoring, mitigation and control of AI systems when seeking to manage\npotentially catastrophic AI risk.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Under peer review",
    "pdf_url": "http://arxiv.org/pdf/2503.18979v2",
    "published_date": "2025-03-23 02:01:09 UTC",
    "updated_date": "2025-03-26 02:00:33 UTC"
  },
  {
    "arxiv_id": "2503.17894v2",
    "title": "Generative AI for Validating Physics Laws",
    "authors": [
      "Maria Nareklishvili",
      "Nicholas Polson",
      "Vadim Sokolov"
    ],
    "abstract": "We present generative artificial intelligence (AI) to empirically validate\nfundamental laws of physics, focusing on the Stefan-Boltzmann law linking\nstellar temperature and luminosity. Our approach simulates counterfactual\nluminosities under hypothetical temperature regimes for each individual star\nand iteratively refines the temperature-luminosity relationship in a deep\nlearning architecture. We use Gaia DR3 data and find that, on average,\ntemperature's effect on luminosity increases with stellar radius and decreases\nwith absolute magnitude, consistent with theoretical predictions. By framing\nphysics laws as causal problems, our method offers a novel, data-driven\napproach to refine theoretical understanding and inform evidence-based policy\nand practice.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.GA",
      "cs.AI"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.17894v2",
    "published_date": "2025-03-23 00:57:26 UTC",
    "updated_date": "2025-03-25 14:31:47 UTC"
  }
]
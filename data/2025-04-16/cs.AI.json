{
  "date": "2025-04-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-16 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于大型语言模型（LLM）的优化与应用、AI 安全与伦理，以及医疗和图像处理领域的创新，其中 Anima Anandkumar 参与的 MOM 论文和 Luciano Floridi 的 AAIO 论文最为引人注目，它们展示了高效的模型扩展和 AI 治理框架；这些论文强调了 LLM 在实际场景中的鲁棒性、泛化能力和伦理挑战。\n\n### 重点论文讨论\n我将优先讨论高影响力论文，包括 LLM 改进、AI 安全和医疗应用领域的内容，并将相关主题归类。其他较常规的论文（如某些图像处理或优化方法）将简要掠过，以控制篇幅。\n\n#### LLM 优化与应用（核心主题，多个突破性贡献）\n- **MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models (MOM: 内存高效的离线小序列推理用于长上下文语言模型)**  \n  Anima Anandkumar 等作者提出了一种内存优化方法，将关键层分区为小序列，并与 KV 缓存离线结合。在 Llama 和 Qwen 模型上实验，平均减少 50% 峰值内存使用，同时将最大上下文长度从 155k 扩展到 455k 标记，保持输出准确性并提升推理效率。这为长上下文 LLM 部署提供了实际解决方案，尤其在资源受限的环境中。\n\n- **Generalization through variance: how noise shapes inductive biases in diffusion models (通过方差实现泛化：噪声如何塑造扩散模型的归纳偏差)**  \n  John J. Vastola 的论文探索了扩散模型的泛化机制，通过数学理论分析噪声对分数匹配目标的影响，发现噪声导致的“泛化通过方差”能填充训练分布中的间隙，并在欠参数化和过参数化模型中提升性能。该研究为扩散模型的鲁棒性提供了新视角，已被 ICLR 2025 接受。\n\n- **ToolRL: Reward is All Tool Learning Needs (ToolRL: 奖励是工具学习所需的一切)**  \n  Heng Ji 等作者引入强化学习框架，使用 GRPO 算法优化工具选择和应用，通过细粒度奖励设计提升 LLM 的工具使用泛化能力。在多基准测试中，性能比基线提升 17%，并优于监督微调方法。该工作强调了奖励设计的战略重要性。\n\n- **PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility (PCDiff: 扩散模型中主动控制所有权保护，并兼容水印)**  \n  这篇论文提出主动访问控制框架，使用可训练融合模块和分层认证来保护扩散模型的知识产权。只有持有有效密钥的用户才能生成高质量图像，否则输出会故意降级。该方法兼容传统水印，实验显示在各种攻击场景下有效。\n\n- **Activated LoRA: Fine-tuned LLMs for Intrinsics (Activated LoRA: 为内部特性微调的 LLM)**  \n  Kristjan Greenewald 等作者扩展 LoRA 方法，使其仅在特定标记激活，减少计算开销。实验证明，该方法在保持性能的同时显著降低推理延迟，为 LLM 在资源受限设备上的部署提供了新途径。\n\n- **On Linear Representations and Pretraining Data Frequency in Language Models (语言模型中线性表示与预训练数据频率)**  \n  这篇论文分析预训练数据频率对 LLM 线性表示的影响，发现主题-关系-对象的三元组共现频率与线性表示强度高度相关。作者开发了回归模型来预测数据频率，提供了一种评估封闭模型训练数据属性的方法。\n\n其他 LLM 相关论文如 RDI 和 FLIP Reasoning Challenge 也涉及模型评估和推理，但贡献相对衍生，这里快速掠过：RDI 通过样本聚类提升对抗鲁棒性评估，FLIP 则测试 LLM 在视觉推理上的极限。\n\n#### AI 安全与伦理（高话题度领域）\n- **AI Safety Should Prioritize the Future of Work (AI 安全应优先考虑工作未来)**  \n  Sanchaita Hazra 等作者的观点论文强调 AI 安全应关注工作影响，包括收入不平等和版权问题，建议建立亲工人全球治理框架。该工作呼吁公平补偿机制，引发 AI 伦理讨论。\n\n- **Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it (代理 AI 优化：定义、工作原理、重要性和应对策略)**  \n  Luciano Floridi 等知名学者提出 AAIO 框架，用于优化 AI 代理与平台的互动，类似于 SEO。该方法强调治理和伦理影响，提供主动监管建议。\n\n- **Is Trust Correlated With Explainability in AI? A Meta-Analysis (AI 中信任与可解释性的相关性：元分析)**  \n  这篇论文通过 90 个研究的元分析发现，AI 可解释性与信任有中等正相关，但不是唯一因素。作者强调在医疗和司法领域提升信任的必要性。\n\n其他安全论文如 Don't Just Translate, Agitate 和 Mitigating LLM Hallucinations 快速提及：前者使用 LLM 作为“魔鬼代言人”提升解释性，后者结合知识图谱减少幻觉。\n\n#### 医疗与科学应用（实际影响显著）\n- **Decision-based AI Visual Navigation for Cardiac Ultrasounds (基于决策的 AI 视觉导航用于心脏超声)**  \n  Yaser Abu-Mostafa 等作者开发了 AI 导航系统，使用二元分类和定位算法识别心脏静脉。该模型在低质量设备上实现零样本性能，支持超声诊断扩展到医院外。\n\n- **A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information (自然语言处理在处理医疗不准确信息中的范围综述)**  \n  这篇综述分析 NLP 在检测和纠正医疗错误、 misinformation 和幻觉方面的潜力，强调数据隐私和评估标准的挑战。\n\n- **Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment (可解释的 AI 驱动型 2 型糖尿病治疗指南)**  \n  Dimitris J. Bertsimas 等作者使用机器学习优化糖尿病治疗流程，实验显示 AI 管道比医生方案多减少 0.26% HbA1c，提供可解释的临床指南。\n\n其他医疗论文如 Multimodal LLM Augmented Reasoning 和 Towards Conversational AI for Human-Machine Collaborative MLOps 则较专业，这里简要掠过：前者提升视觉感知解释性，后者使用 LLM 辅助机器学习操作。\n\n#### 其他领域（快速概述）\n在图像处理和机器人领域，论文如 AdaVid（自适应视频-语言预训练）和 RadMamba（雷达-based 人体活动识别）展示了高效模型，但影响力较小：AdaVid 优化视频编码计算，RadMamba 减少参数同时保持准确性。\n\n总体而言，今天的 arXiv 论文突显了 LLM 和 AI 在实际应用中的潜力，同时强调安全和伦理挑战。感兴趣的读者可关注 Anima Anandkumar 和 Luciano Floridi 的作品，以获取前沿洞见。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2504.12535v1",
      "title": "Decision-based AI Visual Navigation for Cardiac Ultrasounds",
      "title_zh": "翻译失败",
      "authors": [
        "Andy Dimnaku",
        "Dominic Yurk",
        "Zhiyuan Gao",
        "Arun Padmanabhan",
        "Mandar Aras",
        "Yaser Abu-Mostafa"
      ],
      "abstract": "Ultrasound imaging of the heart (echocardiography) is widely used to diagnose\ncardiac diseases. However, obtaining an echocardiogram requires an expert\nsonographer and a high-quality ultrasound imaging device, which are generally\nonly available in hospitals. Recently, AI-based navigation models and\nalgorithms have been used to aid novice sonographers in acquiring the\nstandardized cardiac views necessary to visualize potential disease\npathologies. These navigation systems typically rely on directional guidance to\npredict the necessary rotation of the ultrasound probe. This paper demonstrates\na novel AI navigation system that builds on a decision model for identifying\nthe inferior vena cava (IVC) of the heart. The decision model is trained\noffline using cardiac ultrasound videos and employs binary classification to\ndetermine whether the IVC is present in a given ultrasound video. The\nunderlying model integrates a novel localization algorithm that leverages the\nlearned feature representations to annotate the spatial location of the IVC in\nreal-time. Our model demonstrates strong localization performance on\ntraditional high-quality hospital ultrasound videos, as well as impressive\nzero-shot performance on lower-quality ultrasound videos from a more affordable\nButterfly iQ handheld ultrasound machine. This capability facilitates the\nexpansion of ultrasound diagnostics beyond hospital settings. Currently, the\nguidance system is undergoing clinical trials and is available on the Butterfly\niQ app.",
      "tldr_zh": "这篇论文提出了一种基于决策的AI视觉导航系统，用于心脏超声（echocardiography），旨在帮助新手操作者获取标准心脏视图，从而扩展诊断应用。该系统利用一个离线训练的二元分类决策模型，从心脏超声视频中识别劣vena cava (IVC)，并整合一个新颖的定位算法，利用学习特征表示在实时中标注IVC的空间位置。实验结果显示，该模型在高品质医院超声视频上表现出色，并在低品质Butterfly iQ手持设备上实现零样本性能，推动超声诊断超出医院环境，目前正在临床试验中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12535v1",
      "published_date": "2025-04-16 23:54:46 UTC",
      "updated_date": "2025-04-16 23:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:30:24.535987"
    },
    {
      "arxiv_id": "2504.12532v1",
      "title": "Generalization through variance: how noise shapes inductive biases in diffusion models",
      "title_zh": "通过方差的泛化：噪声如何在扩散模型中塑造归纳偏差",
      "authors": [
        "John J. Vastola"
      ],
      "abstract": "How diffusion models generalize beyond their training set is not known, and\nis somewhat mysterious given two facts: the optimum of the denoising score\nmatching (DSM) objective usually used to train diffusion models is the score\nfunction of the training distribution; and the networks usually used to learn\nthe score function are expressive enough to learn this score to high accuracy.\nWe claim that a certain feature of the DSM objective -- the fact that its\ntarget is not the training distribution's score, but a noisy quantity only\nequal to it in expectation -- strongly impacts whether and to what extent\ndiffusion models generalize. In this paper, we develop a mathematical theory\nthat partly explains this 'generalization through variance' phenomenon. Our\ntheoretical analysis exploits a physics-inspired path integral approach to\ncompute the distributions typically learned by a few paradigmatic under- and\noverparameterized diffusion models. We find that the distributions diffusion\nmodels effectively learn to sample from resemble their training distributions,\nbut with 'gaps' filled in, and that this inductive bias is due to the\ncovariance structure of the noisy target used during training. We also\ncharacterize how this inductive bias interacts with feature-related inductive\nbiases.",
      "tldr_zh": "本研究探讨了扩散模型（diffusion models）如何通过噪声实现泛化，焦点在于 denoising score matching (DSM) 目标的噪声特性如何塑造归纳偏差（inductive biases）。作者开发了一个数学理论，利用物理启发的路径积分方法，分析欠参数化和过参数化扩散模型学到的分布，发现这些分布类似于训练分布但会填补“间隙”，这一现象源于训练中噪声目标的协方差结构。论文还阐明了这种归纳偏差如何与特征相关的归纳偏差互动，为理解扩散模型的泛化机制提供了新见解。",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12532v1",
      "published_date": "2025-04-16 23:41:10 UTC",
      "updated_date": "2025-04-16 23:41:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:30:35.548029"
    },
    {
      "arxiv_id": "2504.12529v1",
      "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis",
      "title_zh": "信任是否与人工智能中的可解释性相关？一项元分析",
      "authors": [
        "Zahra Atf",
        "Peter R. Lewis"
      ],
      "abstract": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.",
      "tldr_zh": "这篇论文通过元分析（meta-analysis）方法，审视了AI系统可解释性（explicability）是否必然提升用户信任的假设，基于90个研究的资料进行全面分析。结果显示，可解释性和信任之间存在统计上显著但仅为适度的正相关关系，表明可解释性有助于构建信任，但并非唯一或主导因素。论文强调Explainable AI (XAI)的学术价值及其在医疗和司法等关键领域的社会技术影响，如促进责任性和应对算法偏差，最终呼吁优先发展AI的真实、持久可信度。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 Page, 1 Figure",
      "pdf_url": "http://arxiv.org/pdf/2504.12529v1",
      "published_date": "2025-04-16 23:30:55 UTC",
      "updated_date": "2025-04-16 23:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:30:47.706153"
    },
    {
      "arxiv_id": "2504.12526v1",
      "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models",
      "title_zh": "MOM：内存高效卸载的小序列推理方法",
      "authors": [
        "Junyang Zhang",
        "Tianyi Zhu",
        "Cheng Luo",
        "Anima Anandkumar"
      ],
      "abstract": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
      "tldr_zh": "该论文提出了一种名为MOM的内存高效离线Mini-sequence推理方法，用于处理长上下文语言模型在推理时的GPU内存需求问题。MOM通过将关键层分区成更小的“mini-sequences”并与KV cache offloading无缝集成，显著降低了峰值内存使用，平均减少超过50%。实验在Llama、Qwen和Mistral模型上显示，MOM在Meta-Llama-3.2-8B模型上将最大上下文长度从155k扩展到455k标记，同时保持输出一致、不影响准确性和高吞吐量。相比传统chunked prefill方法，MOM实现了35%的额外上下文长度扩展，并将预填充内存消耗降至最低，从而改变研究重点转向解码阶段的KV cache效率优化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to COLM",
      "pdf_url": "http://arxiv.org/pdf/2504.12526v1",
      "published_date": "2025-04-16 23:15:09 UTC",
      "updated_date": "2025-04-16 23:15:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:30:59.552264"
    },
    {
      "arxiv_id": "2504.13959v1",
      "title": "AI Safety Should Prioritize the Future of Work",
      "title_zh": "人工智能安全应优先考虑工作的未来",
      "authors": [
        "Sanchaita Hazra",
        "Bodhisattwa Prasad Majumder",
        "Tuhin Chakrabarty"
      ],
      "abstract": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.",
      "tldr_zh": "这篇观点论文认为，当前的AI安全努力过于狭隘，专注于过滤有害内容、防止人类行为操纵以及消除网络安全或生物安全中的存在风险，而忽略了AI对未来工作的深远影响。通过经济理论的视角，作者突出了AI可能加剧收入不平等、导致劳动市场结构变化以及人类生计的跨时影响，并批评封闭源代码的AI开发模式类似于寻租行为（rent-seeking behavior），可能导致创意劳动平庸化和创新垄断。为此，论文推荐建立全面过渡支持机制、实施集体许可（collective licensing）的国际版权框架，以及一个亲工人的全球AI治理框架，以促进共享繁荣、经济正义并减少技术债务。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13959v1",
      "published_date": "2025-04-16 23:12:30 UTC",
      "updated_date": "2025-04-16 23:12:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:31:11.646943"
    },
    {
      "arxiv_id": "2504.12523v1",
      "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Aochong Oliver Li",
        "Tanya Goyal"
      ],
      "abstract": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.",
      "tldr_zh": "本文探讨了更新大型语言模型(LLMs)以适应新知识的挑战，指出现有方法主要关注实体替换而忽略复杂动态，并引入Knowledge Update Playground (KUP)基准——一个自动管道，用于模拟现实知识更新并评估直接和间接探针，以测试模型的记忆和推理能力。作者提出memory conditioned training (MCT)方法，通过在训练时将更新语料中的标记条件于自我生成的“记忆”标记，鼓励模型在推理时调用新知识。该方法在实验中显著优于continued pre-training (CPT)基准，提高直接探针(记忆)性能高达25.4%，而KUP基准显示间接探针(推理)任务极具挑战性，最好模型仅达到不到2%的准确率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12523v1",
      "published_date": "2025-04-16 23:03:40 UTC",
      "updated_date": "2025-04-16 23:03:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:31:24.055134"
    },
    {
      "arxiv_id": "2504.12522v1",
      "title": "Evaluating the Diversity and Quality of LLM Generated Content",
      "title_zh": "评估 LLM 生成内容的多样性和质量",
      "authors": [
        "Alexander Shypula",
        "Shuo Li",
        "Botong Zhang",
        "Vishakh Padmakumar",
        "Kayo Yin",
        "Osbert Bastani"
      ],
      "abstract": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.",
      "tldr_zh": "该研究评估了偏好调整技术（如 RLHF 方法包括 PPO 和 GRPO，以及 DPO）对 LLM 生成内容多样性和质量的影响，发现这些技术虽减少了词汇和句法多样性，但提升了有效语义多样性。作者引入了一个框架，测量在质量阈值内的输出多样性，通过无需人类干预的开放任务实验，揭示偏好调整模型生成更多高质输出，从而间接增加有效语义多样性，而非单纯提高高质输出间的多样性。进一步分析显示，较小模型在固定采样预算下更参数高效地产生独特内容，这些发现为 LLM 在创意辅助和合成数据生成等需要多样高质量输出的应用提供了关键启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 Third Workshop on Deep Learning for Code",
      "pdf_url": "http://arxiv.org/pdf/2504.12522v1",
      "published_date": "2025-04-16 23:02:23 UTC",
      "updated_date": "2025-04-16 23:02:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:31:39.469763"
    },
    {
      "arxiv_id": "2505.00008v1",
      "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoyi Sun",
        "Wen-Wai Yim",
        "Ozlem Uzuner",
        "Fei Xia",
        "Meliha Yetisgen"
      ],
      "abstract": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications.",
      "tldr_zh": "这篇综述探讨了 Natural Language Processing (NLP) 在检测、纠正和缓解医疗不准确信息（如 errors, misinformation 和 hallucination）中的潜力与挑战，强调这些概念的共享方法基础及其对医疗领域的不同影响，以提升患者安全和公共卫生沟通。研究采用 PRISMA 指南进行范围审查，分析了 2020-2024 年来自五个数据库的相关文献，并按主题、任务、文档类型、数据集、模型和评估指标进行分类。结果显示 NLP 在错误检测、纠正、误传检测、纠正以及幻觉检测和缓解等任务中表现出色，但仍存在数据隐私、上下文依赖性和评估标准等挑战；未来应聚焦于开发真实世界数据集、完善上下文方法和加强幻觉管理，以实现更可靠的医疗应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.00008v1",
      "published_date": "2025-04-16 22:27:10 UTC",
      "updated_date": "2025-04-16 22:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:31:49.124646"
    },
    {
      "arxiv_id": "2504.12513v1",
      "title": "AdaVid: Adaptive Video-Language Pretraining",
      "title_zh": "AdaVid：自适应视频语言预训练",
      "authors": [
        "Chaitanya Patel",
        "Juan Carlos Niebles",
        "Ehsan Adeli"
      ],
      "abstract": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.",
      "tldr_zh": "该论文提出AdaVid，一种自适应视频-语言预训练框架，旨在通过动态调整计算开销来学习高效的视频编码器，以适应计算资源受限的边缘设备。AdaVid的核心是受Matryoshka Representation Learning启发的自适应Transformer块，能在推理时调整隐藏嵌入维度，从而处理不同长度的视频剪辑。在Ego4D数据集上训练的AdaVid-EgoVLP，使用一半计算资源即可匹配标准EgoVLP在短视频基准上的性能，并在相同资源下实现超越；此外，该框架在Diving48分类基准上支持更多帧处理，并通过轻量级分层网络聚合长视频特征，实现了计算效率与准确性的良好平衡。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPRW 2025. Project Page: https://chaitanya100100.github.io/AdaVid/",
      "pdf_url": "http://arxiv.org/pdf/2504.12513v1",
      "published_date": "2025-04-16 22:19:50 UTC",
      "updated_date": "2025-04-16 22:19:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:31:59.892902"
    },
    {
      "arxiv_id": "2504.12511v1",
      "title": "Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Shravan Chaudhari",
        "Trilokya Akula",
        "Yoon Kim",
        "Tom Blake"
      ],
      "abstract": "In this paper, we advance the study of AI-augmented reasoning in the context\nof Human-Computer Interaction (HCI), psychology and cognitive science, focusing\non the critical task of visual perception. Specifically, we investigate the\napplicability of Multimodal Large Language Models (MLLMs) in this domain. To\nthis end, we leverage established principles and explanations from psychology\nand cognitive science related to complexity in human visual perception. We use\nthem as guiding principles for the MLLMs to compare and interprete visual\ncontent. Our study aims to benchmark MLLMs across various explainability\nprinciples relevant to visual perception. Unlike recent approaches that\nprimarily employ advanced deep learning models to predict complexity metrics\nfrom visual content, our work does not seek to develop a mere new predictive\nmodel. Instead, we propose a novel annotation-free analytical framework to\nassess utility of MLLMs as cognitive assistants for HCI tasks, using visual\nperception as a case study. The primary goal is to pave the way for principled\nstudy in quantifying and evaluating the interpretability of MLLMs for\napplications in improving human reasoning capability and uncovering biases in\nexisting perception datasets annotated by humans.",
      "tldr_zh": "这篇论文探讨了 Multimodal Large Language Models (MLLMs) 在 Human-Computer Interaction (HCI)、心理学和认知科学中的视觉感知任务中的应用，通过利用这些领域的视觉感知复杂性原理来指导 MLLMs 比较和解释视觉内容。不同于传统方法，该研究提出一个新型的无标注分析框架，用于基准测试 MLLMs 的可解释性，而非开发新的预测模型。最终，该框架旨在评估 MLLMs 作为认知辅助工具的效用，提高人类推理能力，并揭示现有感知数据集中的偏差。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12511v1",
      "published_date": "2025-04-16 22:14:27 UTC",
      "updated_date": "2025-04-16 22:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:32:12.502266"
    },
    {
      "arxiv_id": "2504.13958v1",
      "title": "ToolRL: Reward is All Tool Learning Needs",
      "title_zh": "ToolRL：奖励是工具学习所需的一切",
      "authors": [
        "Cheng Qian",
        "Emre Can Acikgoz",
        "Qi He",
        "Hongru Wang",
        "Xiusi Chen",
        "Dilek Hakkani-Tür",
        "Gokhan Tur",
        "Heng Ji"
      ],
      "abstract": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
      "tldr_zh": "当前，大型语言模型 (LLMs) 通常通过监督微调 (SFT) 来学习工具使用能力，但 SFT 在泛化到未知或复杂场景时存在局限性。该论文首次对强化学习 (RL) 中的奖励设计进行了全面研究，探讨了奖励的类型、规模、粒度和时间动态，并提出了一种针对工具选择和应用的原则性奖励设计，使用 Group Relative Policy Optimization (GRPO) 训练 LLMs。实验结果显示，该方法在多种基准测试中比基线模型提升 17%，比 SFT 模型提升 15%，证明了精心设计的奖励在提升 LLMs 工具使用能力和泛化性能中的关键作用。代码已开源以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "19 Pages, 12 Figures, 12 Tables",
      "pdf_url": "http://arxiv.org/pdf/2504.13958v1",
      "published_date": "2025-04-16 21:45:32 UTC",
      "updated_date": "2025-04-16 21:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:32:23.970144"
    },
    {
      "arxiv_id": "2504.12503v1",
      "title": "Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study",
      "title_zh": "翻译失败",
      "authors": [
        "Kaira M. Samuel",
        "Faez Ahmed"
      ],
      "abstract": "Engineering problems that apply machine learning often involve\ncomputationally intensive methods but rely on limited datasets. As engineering\ndata evolves with new designs and constraints, models must incorporate new\nknowledge over time. However, high computational costs make retraining models\nfrom scratch infeasible. Continual learning (CL) offers a promising solution by\nenabling models to learn from sequential data while mitigating catastrophic\nforgetting, where a model forgets previously learned mappings. This work\nintroduces CL to engineering design by benchmarking several CL methods on\nrepresentative regression tasks. We apply these strategies to five engineering\ndatasets and construct nine new engineering CL benchmarks to evaluate their\nability to address forgetting and improve generalization. Preliminary results\nshow that applying existing CL methods to these tasks improves performance over\nnaive baselines. In particular, the Replay strategy achieved performance\ncomparable to retraining in several benchmarks while reducing training time by\nnearly half, demonstrating its potential for real-world engineering workflows.\nThe code and datasets used in this work will be available at:\nhttps://github.com/kmsamuel/cl-for-engineering-release.",
      "tldr_zh": "这篇论文探讨了在3D工程回归问题中应用Continual Learning (CL)策略，以应对数据演变和计算密集型训练的挑战，同时避免灾难性遗忘。作者在五个工程数据集上基准测试了几种CL方法，并构建了九个新工程CL基准，以评估这些策略在减少遗忘和提升泛化能力方面的表现。初步结果表明，CL方法优于简单基线，其中Replay策略在多个基准上实现了与重新训练相当的性能，同时将训练时间减少近一半，为实际工程工作流程提供了潜在解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12503v1",
      "published_date": "2025-04-16 21:40:03 UTC",
      "updated_date": "2025-04-16 21:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:32:36.092125"
    },
    {
      "arxiv_id": "2504.12497v1",
      "title": "Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope",
      "title_zh": "翻译失败",
      "authors": [
        "Robert E. Wray",
        "Steven J. Jones",
        "John E. Laird"
      ],
      "abstract": "Regardless of past learning, an agent in an open world will face unfamiliar\nsituations and events outside of prior experience, existing models, or\npolicies. Further, the agent will sometimes lack relevant knowledge and/or\nsufficient time to assess the situation, generate and evaluate options, and\npursue a robustly considered course of action. How can an agent respond\nreasonably to situations that are outside of its original design scope? How can\nit recognize such situations sufficiently quickly and reliably to determine\nreasonable, adaptive courses of action? We identify key characteristics needed\nfor solutions, evaluate the state-of-the-art by these requirements, and outline\na proposed, novel approach that combines domain-general meta-knowledge (in the\nform of appraisals inspired by human cognition) and metareasoning. It has the\npotential to provide fast, adaptive responses to unfamiliar situations, more\nfully meeting the performance characteristics required for open-world, general\nagents.",
      "tldr_zh": "该论文探讨了智能代理（agent）在开放世界（open world）中如何快速识别和响应超出设计范围的陌生事件（unfamiliar events），即使缺乏相关知识或时间来全面评估。该研究识别了解决方案的关键特性，并评估了现有技术的不足。随后，提出了一种新方法，结合基于人类认知的领域通用元知识（appraisals）和元推理（metareasoning），以实现快速、适应性的响应。该方法有望提升开放世界通用代理的性能，提供更可靠的行动策略。",
      "categories": [
        "cs.AI",
        "I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 3 figures. Submitted to AGI25 conference",
      "pdf_url": "http://arxiv.org/pdf/2504.12497v1",
      "published_date": "2025-04-16 21:26:12 UTC",
      "updated_date": "2025-04-16 21:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:32:47.561371"
    },
    {
      "arxiv_id": "2504.12488v1",
      "title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process",
      "title_zh": "与 AI 共同写作，以人类条件为基础：将研究与用户需求在写作过程中对齐",
      "authors": [
        "Mohi Reza",
        "Jeb Thomas-Mitchell",
        "Peter Dushniku",
        "Nathan Laundry",
        "Joseph Jay Williams",
        "Anastasia Kuzminykh"
      ],
      "abstract": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms.",
      "tldr_zh": "本文通过对109篇HCI论文的系统回顾（使用PRISMA方法）和15位不同领域作家的访谈，探讨了AI工具（如ChatGPT）在写作过程中的辅助作用及其对作者代理性和所有权的影响。研究识别了四种设计策略——structured guidance、guided exploration、active co-writing和critical feedback——并将其映射到写作的四个认知过程：planning、translating、reviewing和monitoring。结果显示，内容导向作家（如学者）在planning阶段优先所有权，而形式导向作家（如创意人士）在translating和reviewing阶段更注重控制；基于这些发现，论文提供了可操作的设计指导，以开发符合用户需求的、以人为本的AI共同写作工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; I.2.7; I.2.6; I.7.2"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12488v1",
      "published_date": "2025-04-16 21:05:46 UTC",
      "updated_date": "2025-04-16 21:05:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:01.017173"
    },
    {
      "arxiv_id": "2504.13957v1",
      "title": "Naming is framing: How cybersecurity's language problems are repeating in AI governance",
      "title_zh": "翻译失败",
      "authors": [
        "Lianne Potter"
      ],
      "abstract": "Language is not neutral; it frames understanding, structures power, and\nshapes governance. This paper argues that misnomers like cybersecurity and\nartificial intelligence (AI) are more than semantic quirks; they carry\nsignificant governance risks by obscuring human agency, inflating expectations,\nand distorting accountability. Drawing on lessons from cybersecurity's\nlinguistic pitfalls, such as the 'weakest link' narrative, this paper\nhighlights how AI discourse is falling into similar traps with metaphors like\n'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,\nmystifying, or overly technical assumptions into governance structures. In\nresponse, the paper advocates for a language-first approach to AI governance:\none that interrogates dominant metaphors, foregrounds human roles, and\nco-develops a lexicon that is precise, inclusive, and reflexive. This paper\ncontends that linguistic reform is not peripheral to governance but central to\nthe construction of transparent, equitable, and anticipatory regulatory\nframeworks.",
      "tldr_zh": "这篇论文探讨了语言在治理中的关键作用，指出术语如“cybersecurity”和“artificial intelligence”并非中性词汇，而是会模糊人类代理、夸大期望并扭曲责任，从而带来治理风险。通过借鉴“cybersecurity”领域的语言问题（如“weakest link”叙事），论文揭示了 AI 话语中类似陷阱，包括“alignment”、“black box”和“hallucination”等隐喻，这些术语嵌入对抗性或神秘化的假设。论文主张采用以语言为先的方法，审视主导隐喻、强调人类角色，并共同开发精确、包容和反思性的词汇表，以构建透明、公平且前瞻性的 AI 监管框架。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "20 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13957v1",
      "published_date": "2025-04-16 20:58:26 UTC",
      "updated_date": "2025-04-16 20:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:11.919176"
    },
    {
      "arxiv_id": "2504.12482v1",
      "title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it",
      "title_zh": "翻译失败",
      "authors": [
        "Luciano Floridi",
        "Carlotta Buttaboni",
        "Emmie Hine",
        "Jessica Morley",
        "Claudio Novelli",
        "Tyler Schroder"
      ],
      "abstract": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.",
      "tldr_zh": "这篇论文引入了 Agentic AI Optimisation (AAIO)，一种针对 Agentic Artificial Intelligence (AAI) 系统的优化方法，确保网站与自主 AI 代理之间的无缝互动，类似于 Search Engine Optimisation (SEO) 对内容发现的影响。AAIO 通过分析网站优化和 AAI 成功之间的相互依赖性，创建一种良性循环，促进高效整合。论文探讨了 AAIO 的治理、伦理、法律和社会影响 (GELSI)，强调需要主动的监管框架来减轻潜在负面影响。最终，它肯定 AAIO 是自主数字代理时代的核心数字基础设施，并倡导公平、包容性的访问方式。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12482v1",
      "published_date": "2025-04-16 20:38:09 UTC",
      "updated_date": "2025-04-16 20:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:26.190752"
    },
    {
      "arxiv_id": "2504.12477v1",
      "title": "Towards Conversational AI for Human-Machine Collaborative MLOps",
      "title_zh": "翻译失败",
      "authors": [
        "George Fatouros",
        "Georgios Makridis",
        "George Kousiouris",
        "John Soldatos",
        "Anargyros Tsadimas",
        "Dimosthenis Kyriazis"
      ],
      "abstract": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.",
      "tldr_zh": "这篇论文提出了一种基于 Large Language Model (LLM) 的对话代理系统（Swarm Agent），旨在提升人类-机器在 Machine Learning Operations (MLOps) 中的协作。该系统采用分层模块化架构，集成了 KubeFlow Pipelines (KFP) Agent 用于 ML 管道编排、MinIO Agent 用于数据管理和 Retrieval-Augmented Generation (RAG) Agent 用于领域知识集成，通过自然语言交互和迭代推理循环实现工作流管理。用户可以通过直观的对话接口发现、执行和监控 ML 管道，管理数据集，并访问文档，从而降低复杂 MLOps 平台的进入门槛。实验结果表明，该系统提高了工具的可访问性，适用于不同技术水平的用户。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "68T50, 68T99, 68U35, 68N19",
        "I.2.1; H.5.2; D.2.11; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12477v1",
      "published_date": "2025-04-16 20:28:50 UTC",
      "updated_date": "2025-04-16 20:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:37.655071"
    },
    {
      "arxiv_id": "2504.12476v1",
      "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Jungherr",
        "Adrian Rauchfleisch"
      ],
      "abstract": "Recent advances in generative Artificial Intelligence have raised public\nawareness, shaping expectations and concerns about their societal implications.\nCentral to these debates is the question of AI alignment -- how well AI systems\nmeet public expectations regarding safety, fairness, and social values.\nHowever, little is known about what people expect from AI-enabled systems and\nhow these expectations differ across national contexts. We present evidence\nfrom two surveys of public preferences for key functional features of\nAI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We\nexamine support for four types of alignment in AI moderation: accuracy and\nreliability, safety, bias mitigation, and the promotion of aspirational\nimaginaries. U.S. respondents report significantly higher AI use and\nconsistently greater support for all alignment features, reflecting broader\ntechnological openness and higher societal involvement with AI. In both\ncountries, accuracy and safety enjoy the strongest support, while more\nnormatively charged goals -- like fairness and aspirational imaginaries --\nreceive more cautious backing, particularly in Germany. We also explore how\nindividual experience with AI, attitudes toward free speech, political\nideology, partisan affiliation, and gender shape these preferences. AI use and\nfree speech support explain more variation in Germany. In contrast, U.S.\nresponses show greater attitudinal uniformity, suggesting that higher exposure\nto AI may consolidate public expectations. These findings contribute to debates\non AI governance and cross-national variation in public preferences. More\nbroadly, our study demonstrates the value of empirically grounding AI alignment\ndebates in public attitudes and of explicitly developing normatively grounded\nexpectations into theoretical and policy discussions on the governance of\nAI-generated content.",
      "tldr_zh": "本文调查了德国（n=1800）和美国（n=1756）公众对AI alignment在AI moderation中的期望，涵盖准确性和可靠性、安全、偏见缓解以及促进理想化想象四个方面。结果显示，美国受访者对所有alignment特征的支持度更高，反映出其更广泛的AI使用和技术开放性，而两国均优先重视准确性和安全。德国公众对公平和规范性目标更谨慎，且AI使用、言论自由态度、政治意识形态等因素在德国的影响更大。该研究为AI治理政策提供实证依据，强调了基于公众态度的跨国差异的重要性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12476v1",
      "published_date": "2025-04-16 20:27:03 UTC",
      "updated_date": "2025-04-16 20:27:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:48.353888"
    },
    {
      "arxiv_id": "2504.12474v2",
      "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
      "title_zh": "使用 BiGTex 整合文本属性图中的结构和语义信号",
      "authors": [
        "Azadeh Beiranvand",
        "Seyed Mehdi Vahidipour"
      ],
      "abstract": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.",
      "tldr_zh": "该研究针对文本属性图（Text-Attributed Graphs, TAGs）在表示学习中的挑战，提出了一种新型架构BiGTex，用于整合图结构信号和文本语义信号。BiGTex通过堆叠的Graph-Text Fusion Units实现GNNs（Graph Neural Networks）和LLMs（Large Language Models）的紧密结合，允许文本和结构表示之间的双向注意力机制，从而使信息相互影响。实验在五个基准数据集上显示，BiGTex在节点分类任务中达到最先进性能，并有效泛化到链接预测；消融研究强调了软提示和双向注意力的关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12474v2",
      "published_date": "2025-04-16 20:25:11 UTC",
      "updated_date": "2025-04-19 12:14:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:33:59.772170"
    },
    {
      "arxiv_id": "2504.12463v2",
      "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts",
      "title_zh": "密集反向传播改善稀疏混合专家的训练",
      "authors": [
        "Ashwinee Panda",
        "Vatsal Baherwani",
        "Zain Sarwar",
        "Benjamin Therien",
        "Supriyo Chakraborty",
        "Tom Goldstein"
      ],
      "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.",
      "tldr_zh": "本文提出 Default MoE 方法，以解决稀疏 Mixture-of-Experts (MoE) 模型在训练中因稀疏 backward 更新导致的不稳定问题。该方法使用专家输出的指数移动平均作为默认输出，替换缺失激活，从而让 MoE router 为每个 token 获得密集梯度更新，同时保持参数稀疏激活。实验结果表明，Default MoE 在多种设置下优于标准 TopK routing，并显著提升训练性能，而无需增加计算开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12463v2",
      "published_date": "2025-04-16 19:55:36 UTC",
      "updated_date": "2025-04-18 02:58:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:34:11.838182"
    },
    {
      "arxiv_id": "2504.12459v1",
      "title": "On Linear Representations and Pretraining Data Frequency in Language Models",
      "title_zh": "论语言模型中的线性表示与预训练数据频率",
      "authors": [
        "Jack Merullo",
        "Noah A. Smith",
        "Sarah Wiegreffe",
        "Yanai Elazar"
      ],
      "abstract": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.",
      "tldr_zh": "本研究探讨了预训练数据频率对语言模型（Language Models, LMs）线性表示（Linear Representations）的影响，重点分析事实三元组（subject-relation-object）的共现频率与线性编码形成的关系。研究发现，主题和对象在预训练数据中分别出现至少1k和2k次时，线性表示更可能形成，且这与关系上下文学习准确性高度相关；在OLMo-7B和GPT-J模型中，这一现象贯穿整个预训练过程。作者训练了一个回归模型，能基于线性表示质量预测术语的预训练频率，即使在不同模型和数据集上也表现出低错误率，为通过操纵预训练数据频率来控制和提升模型行为提供了新方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12459v1",
      "published_date": "2025-04-16 19:50:03 UTC",
      "updated_date": "2025-04-16 19:50:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:34:24.618254"
    },
    {
      "arxiv_id": "2504.12446v2",
      "title": "Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks",
      "title_zh": "从前向神经网络中推导等价的基于符号的决策模型",
      "authors": [
        "Sebastian Seidel",
        "Uwe M. Borghoff"
      ],
      "abstract": "Artificial intelligence (AI) has emerged as a transformative force across\nindustries, driven by advances in deep learning and natural language\nprocessing, and fueled by large-scale data and computing resources. Despite its\nrapid adoption, the opacity of AI systems poses significant challenges to trust\nand acceptance.\n  This work explores the intersection of connectionist and symbolic approaches\nto artificial intelligence, focusing on the derivation of interpretable\nsymbolic models, such as decision trees, from feedforward neural networks\n(FNNs). Decision trees provide a transparent framework for elucidating the\noperations of neural networks while preserving their functionality. The\nderivation is presented in a step-by-step approach and illustrated with several\nexamples. A systematic methodology is proposed to bridge neural and symbolic\nparadigms by exploiting distributed representations in FNNs to identify\nsymbolic components, including fillers, roles, and their interrelationships.\nThe process traces neuron activation values and input configurations across\nnetwork layers, mapping activations and their underlying inputs to decision\ntree edges. The resulting symbolic structures effectively capture FNN decision\nprocesses and enable scalability to deeper networks through iterative\nrefinement of subpaths for each hidden layer.\n  To validate the theoretical framework, a prototype was developed using Keras\n.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This\nprototype demonstrates the feasibility of extracting symbolic representations\nfrom neural networks, enhancing trust in AI systems, and promoting\naccountability.",
      "tldr_zh": "本研究探讨了从前馈神经网络 (FNNs) 派生等效符号决策模型（如决策树）的方法，以解决人工智能 (AI) 系统不透明性的挑战。作者提出一个系统性方法，通过利用 FNN 中的分布式表示来识别符号组件，包括 fillers、roles 及其相互关系，并追踪神经元激活值和输入配置，将其映射到决策树的边上，从而捕获 FNN 的决策过程并支持更深层网络的扩展。实验原型使用 Keras 和 Java 环境验证了这一框架的可行性，最终提升了 AI 系统的可解释性、信任度和问责性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 19 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12446v2",
      "published_date": "2025-04-16 19:22:53 UTC",
      "updated_date": "2025-04-24 21:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:34:35.343367"
    },
    {
      "arxiv_id": "2504.12436v1",
      "title": "Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation",
      "title_zh": "稀疏性在少样本适应中优于低秩投影",
      "authors": [
        "Nairouz Mrabah",
        "Nicolas Richet",
        "Ismail Ben Ayed",
        "Éric Granger"
      ],
      "abstract": "Adapting Vision-Language Models (VLMs) to new domains with few labeled\nsamples remains a significant challenge due to severe overfitting and\ncomputational constraints. State-of-the-art solutions, such as low-rank\nreparameterization, mitigate these issues but often struggle with\ngeneralization and require extensive hyperparameter tuning. In this paper, a\nnovel Sparse Optimization (SO) framework is proposed. Unlike low-rank\napproaches that typically constrain updates to a fixed subspace, our SO method\nleverages high sparsity to dynamically adjust very few parameters. We introduce\ntwo key paradigms. First, we advocate for \\textit{local sparsity and global\ndensity}, which updates a minimal subset of parameters per iteration while\nmaintaining overall model expressiveness. As a second paradigm, we advocate for\n\\textit{local randomness and global importance}, which sparsifies the gradient\nusing random selection while pruning the first moment based on importance. This\ncombination significantly mitigates overfitting and ensures stable adaptation\nin low-data regimes. Extensive experiments on 11 diverse datasets show that SO\nachieves state-of-the-art few-shot adaptation performance while reducing memory\noverhead.",
      "tldr_zh": "本研究发现，在少样本适应任务中，稀疏优化(Sparse Optimization, SO)框架优于传统的低秩投影方法，用于适应视觉语言模型(VLMs)以缓解过度拟合和计算限制问题。SO框架通过高稀疏性动态调整少量参数，引入两个关键范式：局部稀疏和全局稠密（每迭代更新最小参数子集以保持模型表达性），以及局部随机性和全局重要性（使用随机选择稀疏化梯度并基于重要性修剪第一阶矩），从而提升泛化和稳定性。在11个多样数据集上的广泛实验中，SO实现了最先进的少样本适应性能，同时显著降低了内存开销。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.8; I.5.1; G.1.6"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2504.12436v1",
      "published_date": "2025-04-16 19:10:34 UTC",
      "updated_date": "2025-04-16 19:10:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:34:48.890829"
    },
    {
      "arxiv_id": "2504.12427v1",
      "title": "Position: The Most Expensive Part of an LLM should be its Training Data",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhil Kandpal",
        "Colin Raffel"
      ],
      "abstract": "Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future.",
      "tldr_zh": "这篇论文主张，大型语言模型（LLM）的训练数据成本，尤其是人类劳动力的价值，应该成为模型生产中最昂贵的部分，而非计算或硬件开支。作者分析了2016年至2024年间发布的64个LLM，通过保守估算人工从零生产训练数据集的成本，发现这些成本通常是模型训练本身的10-1000倍。研究强调，目前LLM提供者对数据生产者的补偿严重不足，这可能带来重大财务责任。最后，论文讨论了未来研究方向，以推动更公平的训练数据补偿实践。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12427v1",
      "published_date": "2025-04-16 18:56:14 UTC",
      "updated_date": "2025-04-16 18:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:34:59.119980"
    },
    {
      "arxiv_id": "2504.12424v1",
      "title": "Don't Just Translate, Agitate: Using Large Language Models as Devil's Advocates for AI Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Ashley Suh",
        "Kenneth Alperin",
        "Harry Li",
        "Steven R Gomez"
      ],
      "abstract": "This position paper highlights a growing trend in Explainable AI (XAI)\nresearch where Large Language Models (LLMs) are used to translate outputs from\nexplainability techniques, like feature-attribution weights, into a natural\nlanguage explanation. While this approach may improve accessibility or\nreadability for users, recent findings suggest that translating into human-like\nexplanations does not necessarily enhance user understanding and may instead\nlead to overreliance on AI systems. When LLMs summarize XAI outputs without\nsurfacing model limitations, uncertainties, or inconsistencies, they risk\nreinforcing the illusion of interpretability rather than fostering meaningful\ntransparency. We argue that - instead of merely translating XAI outputs - LLMs\nshould serve as constructive agitators, or devil's advocates, whose role is to\nactively interrogate AI explanations by presenting alternative interpretations,\npotential biases, training data limitations, and cases where the model's\nreasoning may break down. In this role, LLMs can facilitate users in engaging\ncritically with AI systems and generated explanations, with the potential to\nreduce overreliance caused by misinterpreted or specious explanations.",
      "tldr_zh": "这篇论文讨论了在可解释 AI (XAI) 研究中，使用 Large Language Models (LLMs) 仅将解释输出（如 feature-attribution weights）翻译成自然语言的问题，这种方法虽提升了可访问性和可读性，但可能导致用户过度依赖 AI 系统，并强化虚假透明。作者指出，当 LLMs 不揭示模型的限制、不确定性和不一致性时，会加剧解释的幻觉。论文主张 LLMs 应转变为 constructive agitators 或 devil's advocates 的角色，主动质疑 AI 解释，包括呈现替代解释、潜在偏差、训练数据限制以及模型推理可能失效的情况。通过这种方式，LLMs 可以促进用户批判性地参与 AI 系统，减少对误导性解释的过度依赖。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12424v1",
      "published_date": "2025-04-16 18:45:18 UTC",
      "updated_date": "2025-04-16 18:45:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:35:13.493322"
    },
    {
      "arxiv_id": "2504.12422v1",
      "title": "Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study",
      "title_zh": "使用知识图谱缓解 LLM 幻觉：一个案例研究",
      "authors": [
        "Harry Li",
        "Gabriel Appleby",
        "Kenneth Alperin",
        "Steven R Gomez",
        "Ashley Suh"
      ],
      "abstract": "High-stakes domains like cyber operations need responsible and trustworthy AI\nmethods. While large language models (LLMs) are becoming increasingly popular\nin these domains, they still suffer from hallucinations. This research paper\nprovides learning outcomes from a case study with LinkQ, an open-source natural\nlanguage interface that was developed to combat hallucinations by forcing an\nLLM to query a knowledge graph (KG) for ground-truth data during\nquestion-answering (QA). We conduct a quantitative evaluation of LinkQ using a\nwell-known KGQA dataset, showing that the system outperforms GPT-4 but still\nstruggles with certain question categories - suggesting that alternative query\nconstruction strategies will need to be investigated in future LLM querying\nsystems. We discuss a qualitative study of LinkQ with two domain experts using\na real-world cybersecurity KG, outlining these experts' feedback, suggestions,\nperceived limitations, and future opportunities for systems like LinkQ.",
      "tldr_zh": "该研究探讨了如何利用知识图谱（Knowledge Graphs）缓解大型语言模型（LLMs）的幻觉（hallucinations）问题，通过一个案例研究分析开源接口LinkQ。LinkQ通过强制LLMs在问答（QA）过程中查询知识图谱获取真实数据，从而提升在高风险领域如网络操作中的可靠性和可信度。定量评估使用KGQA数据集显示，LinkQ的表现超过了GPT-4，但在某些问题类别上仍存在困难，建议未来探索替代查询策略。定性研究涉及两名网络安全领域专家，他们提供了反馈、改进建议、系统限制以及未来机会，为类似系统的优化提供了指导。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12422v1",
      "published_date": "2025-04-16 18:40:01 UTC",
      "updated_date": "2025-04-16 18:40:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:35:24.295412"
    },
    {
      "arxiv_id": "2504.12417v1",
      "title": "Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data",
      "title_zh": "基于观察数据的可解释AI驱动型2型糖尿病治疗指南",
      "authors": [
        "Dewang Kumar Agarwal",
        "Dimitris J. Bertsimas"
      ],
      "abstract": "Objective: Create precise, structured, data-backed guidelines for type 2\ndiabetes treatment progression, suitable for clinical adoption.\n  Research Design and Methods: Our training cohort was composed of patient\n(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to\n2014. We divide visits into 4 groups based on the patient's treatment regimen\nbefore the visit, and further divide them into subgroups based on the\nrecommended treatment during the visit. Since each subgroup has observational\ndata, which has confounding bias (sicker patients are prescribed more\naggressive treatments), we used machine learning and optimization to remove\nsome datapoints so that the remaining data resembles a randomized trial. On\neach subgroup, we train AI-backed tree-based models to prescribe treatment\nchanges. Once we train these tree models, we manually combine the models for\nevery group to create an end-to-end prescription pipeline for all patients in\nthat group. In this process, we prioritize stepping up to a more aggressive\ntreatment before considering less aggressive options. We tested this pipeline\non unseen data from BMC, and an external dataset from Hartford healthcare (type\n2 diabetes patient visits from January 2020 to May 2024).\n  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more\nthan what the doctors achieved on the unseen BMC patients. For the Hartford\ncohort, our pipelines were better by 0.13%.\n  Conclusions: This precise, interpretable, and efficient AI-backed approach to\ntreatment progression in type 2 diabetes is predicted to outperform the current\npractice and can be deployed to improve patient outcomes.",
      "tldr_zh": "本研究旨在基于观察数据创建精确、可解释的AI驱动指南，用于2型糖尿病（Type 2 Diabetes）的治疗进展，适合临床应用。研究团队使用波士顿医疗中心（BMC）1998-2014年患者数据，将访问分组并通过机器学习（machine learning）和优化（optimization）处理混杂偏差（confounding bias），训练树-based模型来制定治疗变更管道，并优先考虑升级治疗。测试结果显示，该管道在BMC未见数据上使HbA1c减少中位数比医生多0.26%，而在Hartford医疗的外部数据集上多0.13%。总体而言，这种AI-backed方法预计能优于当前临床实践，并提升患者预后。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12417v1",
      "published_date": "2025-04-16 18:29:45 UTC",
      "updated_date": "2025-04-16 18:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:35:36.382373"
    },
    {
      "arxiv_id": "2504.12397v2",
      "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
      "title_zh": "Activated LoRA：用于 Intrinsics 的微调 LLMs",
      "authors": [
        "Kristjan Greenewald",
        "Luis Lastras",
        "Thomas Parnell",
        "Vraj Shah",
        "Lucian Popa",
        "Giulio Zizzo",
        "Chulaka Gunasekara",
        "Ambrish Rawat",
        "David Cox"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
      "tldr_zh": "该论文提出 Activated LoRA (aLoRA)，一种改进的 Low-Rank Adaptation (LoRA) 框架，用于微调大型语言模型 (LLMs)，以解决多轮对话中切换 LoRA 时需重新计算 key-value (KV) cache 的效率问题。aLoRA 通过仅适应调用后序列的权重，允许直接使用基模型的 KV cache，从而实现即时激活和无缝整合。实验结果显示，aLoRA 训练的 intrinsics 模型在准确性上与标准 LoRA 相当，同时显著提升了推理效率，为构建高度专业化的任务特定模型提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2504.11704",
      "pdf_url": "http://arxiv.org/pdf/2504.12397v2",
      "published_date": "2025-04-16 18:03:21 UTC",
      "updated_date": "2025-04-29 14:25:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:35:48.184048"
    },
    {
      "arxiv_id": "2504.12299v1",
      "title": "Adapting a World Model for Trajectory Following in a 3D Game",
      "title_zh": "在 3D 游戏中适应世界模型用于轨迹跟随",
      "authors": [
        "Marko Tot",
        "Shu Ishida",
        "Abdelhak Lemkhenter",
        "David Bignell",
        "Pallavi Choudhury",
        "Chris Lovett",
        "Luis França",
        "Matheus Ribeiro Furtado de Mendonça",
        "Tarun Gupta",
        "Darren Gehring",
        "Sam Devlin",
        "Sergio Valcarcel Macua",
        "Raluca Georgescu"
      ],
      "abstract": "Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting.",
      "tldr_zh": "这篇论文探讨了在 3D 游戏环境中使用 Inverse Dynamics Models (IDM) 结合不同编码器和策略头（如 GPT-style 和 MLP-style）来进行轨迹跟随，以应对分布偏移和随机性问题。研究者在 Bleeding Edge 游戏中测试了多种未来对齐策略，并通过轨迹偏差距离和第一个显著偏差点作为评估指标。结果显示，在多样数据设置下，GPT-style 策略头与从零训练的编码器表现最佳；在低数据环境中，DINOv2 编码器结合 GPT-style 策略头最优，而预训练和微调后，GPT-style 和 MLP-style 策略头性能类似。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12299v1",
      "published_date": "2025-04-16 17:59:54 UTC",
      "updated_date": "2025-04-16 17:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:36:02.032824"
    },
    {
      "arxiv_id": "2504.12292v1",
      "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians",
      "title_zh": "翻译失败",
      "authors": [
        "Liam Schoneveld",
        "Zhe Chen",
        "Davide Davoli",
        "Jiapeng Tang",
        "Saimon Terazawa",
        "Ko Nishino",
        "Matthias Nießner"
      ],
      "abstract": "Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.",
      "tldr_zh": "本文提出SHeaP，一种自监督(Self-Supervised)头部几何预测器，通过2D Gaussians学习从单目图像和视频中重建精确的3D头部模型。方法包括预测3DMM网格和一组与网格绑定的Gaussians，然后通过重新动画头部头像并反向传播光度损失来优化网络，仅依赖2D数据进行训练。该方法在NoW基准测试中超越现有自监督方法，在中性面部和非中性表情的几何评估上表现出色，并产生高度表达性的网格，在情感分类任务中优于最先进的技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/",
      "pdf_url": "http://arxiv.org/pdf/2504.12292v1",
      "published_date": "2025-04-16 17:55:02 UTC",
      "updated_date": "2025-04-16 17:55:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:36:13.921227"
    },
    {
      "arxiv_id": "2504.12284v1",
      "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Prakash",
        "Benjamin Lundell",
        "Dmitry Andreychuk",
        "David Forsyth",
        "Saurabh Gupta",
        "Harpreet Sawhney"
      ],
      "abstract": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.",
      "tldr_zh": "该研究解决了从单一 RGB 视图、动作文本和物体 3D 接触点预测 3D 手部动作和接触地图（Interaction Trajectories）的新问题。作者提出了一种方法，包括 Interaction Codebook（一个 VQVAE 模型，用于学习手部姿势和接触点的潜在代码本，实现轨迹标记化）和 Interaction Predictor（一个 transformer-decoder 模块，通过 indexer 模块从代码本中检索潜在 affordance 来预测交互）。他们开发了一个数据引擎，从 HoloAssist 数据集提取 3D 手部姿势和轨迹，并在比现有工作大 2.5-10 倍的基准上评估模型的泛化能力。实验结果显示，该方法在对象类别、动作类别、任务和场景的各种设置中，优于 transformer 和 diffusion 基线。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact",
      "pdf_url": "http://arxiv.org/pdf/2504.12284v1",
      "published_date": "2025-04-16 17:48:12 UTC",
      "updated_date": "2025-04-16 17:48:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:36:25.490024"
    },
    {
      "arxiv_id": "2504.12268v1",
      "title": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Abi-Karam",
        "Cong Hao"
      ],
      "abstract": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval.",
      "tldr_zh": "本文引入了HLS-Eval，这是一个针对大型语言模型(LLM)在高水平综合(HLS)设计任务上的首个基准测试和评估框架，旨在填补现有评估工具的空白。HLS-Eval涵盖两个核心任务：从自然语言描述生成HLS代码，以及执行HLS特定代码编辑以优化性能和硬件效率，并包括94个独特设计通过半自动化流程准备。框架提供模块化的Python工具，支持自动化并行评估、HLS工具集成和不同LLM交互范式。实验结果通过在Vitis HLS上评估开源LLM，测量了parseability、可编译性、runnability和synthesizability等关键指标，并报告了pass@k性能基线，所有资源已在GitHub开源。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12268v1",
      "published_date": "2025-04-16 17:30:36 UTC",
      "updated_date": "2025-04-16 17:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:36:38.472625"
    },
    {
      "arxiv_id": "2504.13955v4",
      "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations",
      "title_zh": "Thousand Voices of Trauma：一个大规模合成数据集，用于建模延长暴露疗法对话",
      "authors": [
        "Suhas BN",
        "Andrew M. Sherrill",
        "Rosa I. Arriaga",
        "Chris W. Wiese",
        "Saeed Abdullah"
      ],
      "abstract": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.",
      "tldr_zh": "该研究介绍了“Thousand Voices of Trauma”数据集，这是一个大规模合成数据集，包含3,000个基于Prolonged Exposure疗法的PTSD治疗对话，旨在解决AI心理健康支持数据缺乏的问题。数据集包括500个独特案例，每个通过六种对话视角模拟治疗进程，从初始焦虑到高峰痛苦再到情感处理，并涵盖了多样的人口统计学特征（如年龄18-80岁，性别分布多样）、20种创伤类型和10种相关行为，使用确定性和概率生成方法。临床专家验证了数据集的治疗真实性和情感深度，同时分析显示了现实的创伤分布（如目睹暴力10.6%、欺凌10.2%），并开发了情感轨迹基准指标。该数据集作为隐私保护资源，可用于提升患者应用和临床训练工具。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "68T50",
        "I.2.7; H.5.2"
      ],
      "primary_category": "cs.CY",
      "comment": "22 pages, 6 figures Updated Appendix with example model responses",
      "pdf_url": "http://arxiv.org/pdf/2504.13955v4",
      "published_date": "2025-04-16 17:29:05 UTC",
      "updated_date": "2025-05-16 14:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:36:49.758109"
    },
    {
      "arxiv_id": "2504.12262v1",
      "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
      "title_zh": "翻译失败",
      "authors": [
        "David Keetae Park",
        "Xihaier Luo",
        "Guang Zhao",
        "Seungjun Lee",
        "Miruna Oprescu",
        "Shinjae Yoo"
      ],
      "abstract": "Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability.",
      "tldr_zh": "本文提出SCENT框架，用于处理科学领域时空数据的学习问题，解决空间-时间依赖复杂性、高维度和可伸缩性挑战，尤其针对不规则数据（如传感器故障导致的缺失值）。SCENT基于Transformer的编码器-处理器-解码器架构，引入可学习查询和查询-wise交叉注意力机制来捕获多尺度依赖，并采用稀疏注意力确保高效扩展，支持插值、重建和预测的统一。实验验证显示，SCENT在各种模拟和真实任务中实现了最先进性能，同时展现出卓越的可伸缩性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 5 main figures, 3 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2504.12262v1",
      "published_date": "2025-04-16 17:17:31 UTC",
      "updated_date": "2025-04-16 17:17:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:37:01.961712"
    },
    {
      "arxiv_id": "2504.12256v1",
      "title": "FLIP Reasoning Challenge",
      "title_zh": "FLIP 推理挑战",
      "authors": [
        "Andreas Plesner",
        "Turlan Kuzhagaliyev",
        "Roger Wattenhofer"
      ],
      "abstract": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.",
      "tldr_zh": "这篇论文引入了 FLIP 数据集，这是一个基于 Idena 区块链的人类验证任务，用于评估 AI 的推理能力，特别强调顺序推理、视觉叙事和常识。FLIP 挑战要求模型比较两个 4 张图像的顺序并识别逻辑连贯的那个，实验评估了 state-of-the-art 的 VLMs 和 LLMs。结果显示，最佳开源和闭源模型在 zero-shot 设置下的准确率分别为 75.5% 和 77.9%，远低于人类的 95.3%；使用图像描述辅助或模型集成（如结合 15 个模型提升至 85.2%）可显著改善性能。这些发现突出了现有模型的局限性，并强调了像 FLIP 这样的多模态基准的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12256v1",
      "published_date": "2025-04-16 17:07:16 UTC",
      "updated_date": "2025-04-16 17:07:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:37:14.732443"
    },
    {
      "arxiv_id": "2504.12254v2",
      "title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Salhab",
        "Marwan Elghitany",
        "Shameed Sait",
        "Syed Sibghat Ullah",
        "Mohammad Abusheikh",
        "Hasan Abusheikh"
      ],
      "abstract": "Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach achieves state-of-the-art (SOTA)\nresults in Arabic ASR, surpassing both open and closed-source models on\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.",
      "tldr_zh": "本研究针对阿拉伯语等低资源语言的自动语音识别 (ASR) 挑战，提出使用大规模弱监督学习训练基于 Conformer 架构的模型，以解决标注数据集稀缺的问题。该模型从零开始训练，基于 15,000 小时的弱标注语音数据，涵盖现代标准阿拉伯语 (MSA) 和方言阿拉伯语 (DA)，从而避免了昂贵的手动转录过程。尽管缺乏人工验证标签，该方法在标准基准测试中实现了 state-of-the-art (SOTA) 性能，超过了现有开源和闭源模型。通过证明弱监督学习的有效性和可扩展性，该工作为低资源环境下的 ASR 系统提供了成本高效的替代方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12254v2",
      "published_date": "2025-04-16 17:05:14 UTC",
      "updated_date": "2025-04-19 09:55:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:37:25.065211"
    },
    {
      "arxiv_id": "2504.12215v1",
      "title": "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing",
      "title_zh": "不确定性引导的粗到细肿瘤分割，结合了解剖结构感知的后处理",
      "authors": [
        "Ilkin Sevgi Isler",
        "David Mohaisen",
        "Curtis Lisle",
        "Damla Turgut",
        "Ulas Bagci"
      ],
      "abstract": "Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing.",
      "tldr_zh": "该论文提出了一种Uncertainty-Guided Coarse-to-Fine肿瘤分割框架，针对胸部CT图像中的边界模糊、类别不平衡和解剖变异性挑战，通过第一阶段的粗略预测和基于肺重叠、接近肺表面及组件大小的Anatomy-Aware Post-Processing来过滤ROI，然后利用第二阶段的Uncertainty-Aware Loss Functions进行精细分割，以提升准确性和边界校准。实验在私有和公共数据集上显示，该框架显著提高了Dice和Hausdorff分数，减少了假阳性，并增强了空间可解释性；在Orlando数据集上，将Swin UNETR的Dice分数从0.4690提升至0.6447。研究强调了结合不确定性建模和解剖先验在级联分割管道中的价值，为临床肿瘤界定提供了更可靠的方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 2 figures, to appear in IEEE ADSCA 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12215v1",
      "published_date": "2025-04-16 16:08:38 UTC",
      "updated_date": "2025-04-16 16:08:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:37:38.473374"
    },
    {
      "arxiv_id": "2504.12365v1",
      "title": "Themisto: Jupyter-Based Runtime Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Konstantin Grotov",
        "Sergey Titov"
      ],
      "abstract": "In this work, we present a benchmark that consists of Jupyter notebooks\ndevelopment trajectories and allows measuring how large language models (LLMs)\ncan leverage runtime information for predicting code output and code\ngeneration. We demonstrate that the current generation of LLMs performs poorly\non these tasks and argue that there exists a significantly understudied domain\nin the development of code-based models, which involves incorporating the\nruntime context.",
      "tldr_zh": "本文介绍了 Themisto，一种基于 Jupyter notebooks 的运行时基准，用于评估大型语言模型 (LLMs) 如何利用运行时信息来预测代码输出和生成代码。实验结果显示，当前 LLMs 在这些任务上表现较差，存在显著的性能差距。作者强调，这突显了代码模型开发中一个被低估的领域，即整合运行时上下文的必要性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to the third Deep Learning for Code (DL4C) workshop @ ICLR\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12365v1",
      "published_date": "2025-04-16 16:07:18 UTC",
      "updated_date": "2025-04-16 16:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:37:49.552140"
    },
    {
      "arxiv_id": "2504.12210v2",
      "title": "Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks",
      "title_zh": "针对带宽受限边缘网络的去中心化学习通信优化",
      "authors": [
        "Tingyang Sun",
        "Tuan Nguyen",
        "Ting He"
      ],
      "abstract": "Decentralized federated learning (DFL) is a promising machine learning\nparadigm for bringing artificial intelligence (AI) capabilities to the network\nedge. Running DFL on top of edge networks, however, faces severe performance\nchallenges due to the extensive parameter exchanges between agents. Most\nexisting solutions for these challenges were based on simplistic communication\nmodels, which cannot capture the case of learning over a multi-hop\nbandwidth-limited network. In this work, we address this problem by jointly\ndesigning the communication scheme for the overlay network formed by the agents\nand the mixing matrix that controls the communication demands between the\nagents. By carefully analyzing the properties of our problem, we cast each\ndesign problem into a tractable optimization and develop an efficient algorithm\nwith guaranteed performance. Our evaluations based on real topology and data\nshow that the proposed algorithm can reduce the total training time by over\n$80\\%$ compared to the baseline without sacrificing accuracy, while\nsignificantly improving the computational efficiency over the state of the art.",
      "tldr_zh": "这篇论文针对 decentralized federated learning (DFL) 在带宽有限的边缘网络上运行时面临的性能挑战，提出了一种联合优化通信方案和 mixing matrix 的方法，以减少代理之间的参数交换。研究通过分析问题属性，将设计问题转化为可处理的优化问题，并开发了一个高效算法来保证性能。实验结果显示，该算法在真实拓扑和数据基础上，将总训练时间减少超过80%，同时保持准确性，并显著提升计算效率。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "arXiv admin note: text overlap with arXiv:2408.04705",
      "pdf_url": "http://arxiv.org/pdf/2504.12210v2",
      "published_date": "2025-04-16 15:56:57 UTC",
      "updated_date": "2025-04-21 07:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:38:01.275197"
    },
    {
      "arxiv_id": "2504.12192v1",
      "title": "From Requirements to Architecture: Semi-Automatically Generating Software Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Eisenreich"
      ],
      "abstract": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect.",
      "tldr_zh": "该论文提出了一种新的软件架构创建方法，利用LLMs（Large Language Models）辅助初级和高级架构师，从需求到架构的半自动化生成过程。方法强调架构师与LLM工具的紧密合作，涵盖领域模型创建（Domain Model creation）、用例规范（Use Case specification）、架构决策和架构评估，允许架构师灵活控制或遵循建议流程。初步结果表明此方法可行，并能显著节省架构师的时间。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.2"
      ],
      "primary_category": "cs.SE",
      "comment": "to be published in EMISA 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12192v1",
      "published_date": "2025-04-16 15:46:56 UTC",
      "updated_date": "2025-04-16 15:46:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:38:12.137398"
    },
    {
      "arxiv_id": "2504.12187v1",
      "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
      "title_zh": "大型语言模型知道什么？ 默会知识作为一种潜在的因果解释结构",
      "authors": [
        "Céline Budding"
      ],
      "abstract": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.",
      "tldr_zh": "本论文探讨大型语言模型 (LLMs) 是否真正“知道”某些知识，如语言或事实（例如巴黎是法国的首都）。作者认为 LLMs 可以获得 Martin Davies (1990) 定义的 tacit knowledge，尽管 Davies 否认神经网络能做到这一点。论文通过分析 LLMs 的架构特征，证明其满足 semantic description、syntactic structure 和 causal systematicity 的约束，从而为使用 tacit knowledge 作为描述、解释和干预 LLMs 行为的概念框架提供了依据。总的来说，这为理解 LLMs 的认知机制提供了新视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in Philosophy of Science",
      "pdf_url": "http://arxiv.org/pdf/2504.12187v1",
      "published_date": "2025-04-16 15:42:33 UTC",
      "updated_date": "2025-04-16 15:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:38:25.226364"
    },
    {
      "arxiv_id": "2504.12185v1",
      "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data",
      "title_zh": "翻译失败",
      "authors": [
        "Suyoung Bae",
        "Hyojun Kim",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.",
      "tldr_zh": "该研究提出 SALAD 方法，通过结构感知和 LLM 驱动的数据增强来提升对比学习 (contrastive learning) 的鲁棒性和泛化能力，以解决微调预训练语言模型 (PLMs) 导致的虚假相关性 (spurious correlations) 问题。SALAD 利用基于标签的策略生成结构感知的正样本，并借助大语言模型 (LLMs) 创建多样句模式的反事实负样本，使模型更关注关键句子的结构关系而非虚假相关性。在情感分类、性别歧视检测和自然语言推理等任务的实验中，SALAD 显著提高了模型在不同环境下的性能、鲁棒性，并增强了对分布外数据集和跨域场景的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12185v1",
      "published_date": "2025-04-16 15:40:10 UTC",
      "updated_date": "2025-04-16 15:40:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:38:37.145128"
    },
    {
      "arxiv_id": "2504.12180v1",
      "title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jaime E. Cuellar",
        "Oscar Moreno-Martinez",
        "Paula Sofia Torres-Rodriguez",
        "Jaime Andres Pavlich-Mariscal",
        "Andres Felipe Mican-Castiblanco",
        "Juan Guillermo Torres-Hurtado"
      ],
      "abstract": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.",
      "tldr_zh": "这篇论文探讨了细微的提示词（prompts）修改如何显著影响 ChatGPT 在情感分类（sentiment classification）中的鲁棒性，测试假设是这些变化不会产生重大差异。研究使用 10 万条西班牙语评论数据集，对四个拉丁美洲总统的评论进行多次分类实验，每次略微调整提示词的词汇、语法或结构。结果显示，即使是轻微修改也会导致分类结果的显著差异，包括模型出现不一致响应（如混合类别、提供多余解释或使用其他语言），而统计分析（如 Chi-square tests）证实了大多数比较间的显著性差异。该研究强调了大型语言模型（Large Language Models）的脆弱性，特别是缺乏结构化提示词会增加 hallucinations，并挑战了其在社会应用中的可信任性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in Spanish language",
      "pdf_url": "http://arxiv.org/pdf/2504.12180v1",
      "published_date": "2025-04-16 15:37:09 UTC",
      "updated_date": "2025-04-16 15:37:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:38:50.962824"
    },
    {
      "arxiv_id": "2504.12177v1",
      "title": "Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube",
      "title_zh": "使用人工智能映射争议：对 YouTube 上 Hamas-Israel 冲突的分析",
      "authors": [
        "Victor Manuel Hernandez Lopez",
        "Jaime E. Cuellar"
      ],
      "abstract": "This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives.",
      "tldr_zh": "这篇论文使用人工智能分析了2023年10月至2024年1月间25万多条西班牙语YouTube评论，聚焦于Hamas-Israel冲突的争议。研究结合Science and Technology Studies (STS)与Natural Language Processing (NLP)，采用BERT模型对评论进行自动分类，分为七类，包括支持巴勒斯坦、支持以色列等。结果显示，支持巴勒斯坦的评论数量最多，但支持以色列和反巴勒斯坦评论获得更多点赞，并通过agenda-setting theory揭示媒体报道导致公众意见从亲巴勒斯坦转向更批判以色列。该工作创新性地整合计算方法与社会理论，提供了一种分析复杂公众意见和媒体叙事的新框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in Spanish language",
      "pdf_url": "http://arxiv.org/pdf/2504.12177v1",
      "published_date": "2025-04-16 15:27:57 UTC",
      "updated_date": "2025-04-16 15:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:39:03.066254"
    },
    {
      "arxiv_id": "2504.12172v1",
      "title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task",
      "title_zh": "翻译失败",
      "authors": [
        "Maged S. Al-Shaibani",
        "Zaid Alyafeai",
        "Irfan Ahmad"
      ],
      "abstract": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.",
      "tldr_zh": "本研究探讨了阿拉伯诗歌的格律（meter）分类问题，针对朗诵诗歌的低资源任务，提出了一种先进的框架，该框架通过整合两个高资源系统来实现自动识别。阿拉伯诗歌作为阿拉伯语言文化的核心，其独特的节奏结构（Aroud 领域的研究焦点）需要专业知识和大量标注数据，而本框架有效地解决了这一挑战。实验结果显示，该框架在泛化性上表现出色，并发布了一个基准数据集，以促进未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12172v1",
      "published_date": "2025-04-16 15:25:45 UTC",
      "updated_date": "2025-04-16 15:25:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:39:12.685791"
    },
    {
      "arxiv_id": "2504.12151v1",
      "title": "Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis",
      "title_zh": "迈向多模态情感分析中的可解释融合和平衡学习",
      "authors": [
        "Miaosen Luo",
        "Yuncheng Jiang",
        "Sijie Mai"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack\nof interpretability in the decision logic of multimodal fusion and modality\nimbalance caused by disparities in inter-modal information density. To address\nthese issues, we propose KAN-MCP, a novel framework that integrates the\ninterpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the\nMultimodal Clean Pareto (MCPareto) framework. First, KAN leverages its\nunivariate function decomposition to achieve transparent analysis of\ncross-modal interactions. This structural design allows direct inspection of\nfeature transformations without relying on external interpretation tools,\nthereby ensuring both high expressiveness and interpretability. Second, the\nproposed MCPareto enhances robustness by addressing modality imbalance and\nnoise interference. Specifically, we introduce the Dimensionality Reduction and\nDenoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises\nand reduces feature dimensionality. This approach provides KAN with\ndiscriminative low-dimensional inputs to reduce the modeling complexity of KAN\nwhile preserving critical sentiment-related information. Furthermore, MCPareto\ndynamically balances gradient contributions across modalities using the\npurified features output by DRD-MIB, ensuring lossless transmission of\nauxiliary signals and effectively alleviating modality imbalance. This synergy\nof interpretability and robustness not only achieves superior performance on\nbenchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers\nan intuitive visualization interface through KAN's interpretable architecture.",
      "tldr_zh": "该研究针对多模态情感分析（MSA）中的决策逻辑可解释性不足和模态不平衡问题，提出了一种新型框架KAN-MCP，将Kolmogorov-Arnold Networks (KAN)的可解释性与Multimodal Clean Pareto (MCPareto)的鲁棒性相结合。KAN通过单变量函数分解实现跨模态交互的透明分析，提供高表达性和直观特征变换可视化，而无需外部工具。MCPareto引入Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB)方法进行特征去噪和降维，并动态平衡模态梯度贡献，以缓解噪声干扰和模态不平衡。在CMU-MOSI、CMU-MOSEI和CH-SIMS v2等基准数据集上，KAN-MCP框架表现出优越性能，并通过KAN的可解释架构提供直观的分析接口。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12151v1",
      "published_date": "2025-04-16 15:00:06 UTC",
      "updated_date": "2025-04-16 15:00:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:39:26.118228"
    },
    {
      "arxiv_id": "2504.12143v1",
      "title": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Lupinacci",
        "Francesco Blefari",
        "Francesco Romeo",
        "Francesco Aurelio Pironti",
        "Angelo Furfaro"
      ],
      "abstract": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it.",
      "tldr_zh": "该论文提出 ARCeR，一种基于 Agentic RAG 的创新系统，用于从用户提供的自然语言描述自动生成和部署 Cyber Ranges，从而支持网络安全训练、漏洞分析和对策实验。\nARCeR 通过代理式检索增强生成（Agentic RAG）范式，充分利用最先进的人工智能技术，能够处理传统 LLMs 或基本 RAG 系统无法应对的复杂提示。\n实验结果显示，ARCeR 表现出色，并具备灵活性，能针对任何 Cyber Ranges 框架进行适应，只要提供相应的特定知识。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12143v1",
      "published_date": "2025-04-16 14:53:28 UTC",
      "updated_date": "2025-04-16 14:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:39:38.733529"
    },
    {
      "arxiv_id": "2504.12137v1",
      "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -",
      "title_zh": "翻译失败",
      "authors": [
        "Laura Fieback",
        "Nishilkumar Balar",
        "Jakob Spiegelberg",
        "Hanno Gottschalk"
      ],
      "abstract": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.",
      "tldr_zh": "尽管大型视觉语言模型(LVLMs) 仍容易产生与视觉输入不符的幻觉(hallucinations)，本研究提出Efficient Contrastive Decoding (ECD)，一种简单的方法，利用probabilistic hallucination detection来调整输出分布，从而抑制幻觉概念。ECD通过对比token probabilities和hallucination scores，从原始分布中减去潜在幻觉，确保生成的响应更准确，且无需额外训练即可应用于任何开源LVLM。实验在多个基准数据集和不同LVLM上验证了ECD的有效性，其性能优于现有方法，同时显著降低了计算时间。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12137v1",
      "published_date": "2025-04-16 14:50:25 UTC",
      "updated_date": "2025-04-16 14:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:39:49.411377"
    },
    {
      "arxiv_id": "2504.12110v1",
      "title": "Towards LLM Agents for Earth Observation",
      "title_zh": "翻译失败",
      "authors": [
        "Chia Hsiang Kao",
        "Wenting Zhao",
        "Shreelekha Revankar",
        "Samuel Speas",
        "Snehal Bhagat",
        "Rajeev Datta",
        "Cheng Perng Phoo",
        "Utkarsh Mall",
        "Carl Vondrick",
        "Kavita Bala",
        "Bharath Hariharan"
      ],
      "abstract": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth.",
      "tldr_zh": "本论文探讨了LLM Agents在Earth Observation中的应用潜力，引入了一个名为\\datasetnamenospace的基准数据集，包含140个是/否问题，覆盖NASA Earth Observatory的13个主题和17个卫星传感器。实验结果显示，使用Google Earth Engine API的LLM代理准确率仅为33%，代码运行失败率超过58%。通过微调合成数据，研究者显著改善了开源模型的表现，使较小模型如Llama-3.1-8B的准确率可媲美更大模型如DeepSeek-R1，并指出了AI代理自动化地球观测面临的挑战及未来改进路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.12110v1",
      "published_date": "2025-04-16 14:19:25 UTC",
      "updated_date": "2025-04-16 14:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:40:02.594838"
    },
    {
      "arxiv_id": "2504.12090v1",
      "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
      "title_zh": "基于推理的AI用于创业公司评估 (R.A.I.S.E.)：一个记忆增强型、多步骤决策",
      "authors": [
        "Jack Preuveneers",
        "Joseph Ternasky",
        "Fuat Alican",
        "Yigit Ihlamur"
      ],
      "abstract": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights.",
      "tldr_zh": "本研究提出R.A.I.S.E.框架，一种基于记忆增强的多步决策系统，将决策树的解释性和大型语言模型(LLMs)的推理能力相结合，用于预测创业公司成功。该框架利用chain-of-thought prompting生成详细推理日志，并通过高效数据摄取、两步精炼过程、集成候选采样、模拟reinforcement learning评分以及持久内存等增强，确保决策稳定且透明。在实验中，R.A.I.S.E.在创业公司数据集上将精确度提高54%（从0.225到0.346）和准确度提高50%（从0.46到0.70），比随机分类器精确度高出2倍以上，并为高风险投资环境提供可解释的AI决策支持。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12090v1",
      "published_date": "2025-04-16 13:53:42 UTC",
      "updated_date": "2025-04-16 13:53:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:40:14.277278"
    },
    {
      "arxiv_id": "2504.12088v1",
      "title": "AttentionDrop: A Novel Regularization Method for Transformer Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mirza Samad Ahmed Baig",
        "Syeda Anshrah Gillani",
        "Abdul Akbar Khan",
        "Shahid Munir Shah"
      ],
      "abstract": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.",
      "tldr_zh": "本文提出 AttentionDrop，一种新型正则化方法，针对 Transformer 模型在自然语言处理、计算机视觉和语音任务中容易过拟合的问题，通过在 self-attention distributions 上施加随机扰动来提升模型泛化能力。AttentionDrop 包括三个变体：Hard Attention Masking，通过随机零置每个查询的 top-k attention logits 来鼓励多样化上下文利用；Blurred Attention Smoothing，使用动态 Gaussian 卷积扩散过于尖锐的 attention logits；以及 Consistency-Regularized AttentionDrop，通过 KL-based consistency loss 强制输出在多个独立扰动下的稳定性。该方法为处理有限或 noisy 数据提供统一框架，有望显著改善 Transformer 模型的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.12088v1",
      "published_date": "2025-04-16 13:51:16 UTC",
      "updated_date": "2025-04-16 13:51:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:40:26.187525"
    },
    {
      "arxiv_id": "2504.12082v1",
      "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yumin Kim",
        "Hwanhee Lee"
      ],
      "abstract": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.",
      "tldr_zh": "这篇论文针对隐性仇恨言论检测的挑战提出了一种新方法，以解决其依赖上下文、文化细微差别和隐藏偏见的问题，导致大语言模型出现假阳性（false positives）和假阴性（false negatives）。该方法采用in-context learning技术，通过自适应检索类似群组或最高相似度分数的演示（selective demonstration retrieval），无需模型微调即可提升上下文理解和检测精度。实验结果表明，该方法优于现有最先进（state-of-the-art）技术，在减少模型偏见和增强鲁棒性方面取得了显著改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12082v1",
      "published_date": "2025-04-16 13:43:23 UTC",
      "updated_date": "2025-04-16 13:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:40:36.676560"
    },
    {
      "arxiv_id": "2504.12063v1",
      "title": "Optimizing Compound Retrieval Systems",
      "title_zh": "优化复合检索系统",
      "authors": [
        "Harrie Oosterhuis",
        "Rolf Jagerman",
        "Zhen Qin",
        "Xuanhui Wang"
      ],
      "abstract": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.",
      "tldr_zh": "本文提出 compound retrieval systems 作为一种更广泛的检索系统框架，允许多种模型交互而非仅限于 cascading approach，从而整合大型语言模型 (LLMs) 的相对相关性比较。研究重点优化了组件模型的应用位置和预测聚合方式，例如将经典 BM25 检索模型与先进的 (pairwise) LLM 预测结合，以实现特定排名指标和效率目标。实验结果表明，这种优化系统在有效性和效率之间提供了比级联方法更好的权衡，即使在自监督模式下也表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12063v1",
      "published_date": "2025-04-16 13:18:16 UTC",
      "updated_date": "2025-04-16 13:18:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:40:49.678735"
    },
    {
      "arxiv_id": "2504.12039v1",
      "title": "RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhuo Wu",
        "Francesco Fioranelli",
        "Chang Gao"
      ],
      "abstract": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.",
      "tldr_zh": "本研究提出RadMamba，一种高效的基于雷达微多普勒的Mamba State-Space Model (SSM)，旨在解决传统HAR（Human Activity Recognition）方法如卷积和循环神经网络在计算资源受限场景下的高复杂度问题。RadMamba通过优化参数设计和针对雷达信号的特定调整，在三个数据集上表现出色：在Dataset DIAT上达到99.8%的分类准确率，仅需顶级模型的1/400参数；在Dataset CI4R上匹配92.0%的准确率，只用其1/10参数；而在Dataset UoG2020的连续序列任务中，以6.7k参数超越其他模型至少3%。这项工作提升了HAR的效率和准确性，同时开源代码以促进进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2504.12039v1",
      "published_date": "2025-04-16 12:54:11 UTC",
      "updated_date": "2025-04-16 12:54:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:41:01.642383"
    },
    {
      "arxiv_id": "2504.12031v1",
      "title": "Proof-Carrying Neuro-Symbolic Code",
      "title_zh": "证明携带神经符号代码",
      "authors": [
        "Ekaterina Komendantskaya"
      ],
      "abstract": "This invited paper introduces the concept of \"proof-carrying neuro-symbolic\ncode\" and explains its meaning and value, from both the \"neural\" and the\n\"symbolic\" perspectives. The talk outlines the first successes and challenges\nthat this new area of research faces.",
      "tldr_zh": "这篇论文引入了“proof-carrying neuro-symbolic code”的概念，从“neural”（神经）和“symbolic”（符号）视角解释其含义和价值，旨在将神经网络的灵活性与符号推理的可验证性相结合。\n这种方法能提升代码的安全性和可靠性，使其适用于需要证明正确性的应用场景。\n论文概述了该研究领域的初步成功，同时指出了面临的挑战，如整合复杂性和证明难度。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.LO",
        "F.3.1; F.3.2; F.3.3; I.2.0"
      ],
      "primary_category": "cs.PL",
      "comment": "Invited paper at CiE 2025. arXiv admin note: text overlap with\n  arXiv:2501.05867",
      "pdf_url": "http://arxiv.org/pdf/2504.12031v1",
      "published_date": "2025-04-16 12:42:18 UTC",
      "updated_date": "2025-04-16 12:42:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:41:12.294989"
    },
    {
      "arxiv_id": "2504.12012v1",
      "title": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kris Pilcher",
        "Esen K. Tütüncü"
      ],
      "abstract": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration.",
      "tldr_zh": "该论文提出了一种名为 Purposefully Induced Psychosis (PIP) 的新方法，将 Large Language Models (LLMs) 中的 hallucinations（幻觉）视为创新资源，而非错误，通过微调模型来放大投机性、隐喻性和超现实输出，用于创意任务如科幻小说、互动故事和混合现实模拟。作者借鉴 Herman Melville 的《Moby-Dick》，重新定义 hallucinations 为计算想象力，并强调在用户自愿悬置不信的语境中，这些“错误”可转化为思维催化剂。论文讨论了 PIP 的潜在应用、设计原则、初步观察，以及对 AI 伦理和人机协作的启示。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12012v1",
      "published_date": "2025-04-16 12:13:02 UTC",
      "updated_date": "2025-04-16 12:13:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:41:26.530768"
    },
    {
      "arxiv_id": "2504.12011v1",
      "title": "Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition",
      "title_zh": "通过信息论分解平衡自监督学习中的图嵌入平滑性",
      "authors": [
        "Heesoo Jung",
        "Hogun Park"
      ],
      "abstract": "Self-supervised learning (SSL) in graphs has garnered significant attention,\nparticularly in employing Graph Neural Networks (GNNs) with pretext tasks\ninitially designed for other domains, such as contrastive learning and feature\nreconstruction. However, it remains uncertain whether these methods effectively\nreflect essential graph properties, precisely representation similarity with\nits neighbors. We observe that existing methods position opposite ends of a\nspectrum driven by the graph embedding smoothness, with each end corresponding\nto outperformance on specific downstream tasks. Decomposing the SSL objective\ninto three terms via an information-theoretic framework with a neighbor\nrepresentation variable reveals that this polarization stems from an imbalance\namong the terms, which existing methods may not effectively maintain. Further\ninsights suggest that balancing between the extremes can lead to improved\nperformance across a wider range of downstream tasks. A framework, BSG\n(Balancing Smoothness in Graph SSL), introduces novel loss functions designed\nto supplement the representation quality in graph-based SSL by balancing the\nderived three terms: neighbor loss, minimal loss, and divergence loss. We\npresent a theoretical analysis of the effects of these loss functions,\nhighlighting their significance from both the SSL and graph smoothness\nperspectives. Extensive experiments on multiple real-world datasets across node\nclassification and link prediction consistently demonstrate that BSG achieves\nstate-of-the-art performance, outperforming existing methods. Our\nimplementation code is available at https://github.com/steve30572/BSG.",
      "tldr_zh": "本研究探讨了图自监督学习 (SSL) 中 Graph Neural Networks (GNNs) 的问题，指出现有方法在图嵌入平滑度上存在不平衡，导致在下游任务上表现不均。通过信息理论框架，将 SSL 目标分解为邻居损失、最小损失和发散损失三个术语，揭示了这种极化的根源。作者提出 BSG (Balancing Smoothness in Graph SSL) 框架，使用新型损失函数平衡这些术语，从而提升图表示质量和泛化性能。实验在多个真实数据集上验证，BSG 在节点分类和链接预测任务上实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the Web Conference (WWW) 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12011v1",
      "published_date": "2025-04-16 12:09:56 UTC",
      "updated_date": "2025-04-16 12:09:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:41:39.150071"
    },
    {
      "arxiv_id": "2504.12007v1",
      "title": "Generative Recommendation with Continuous-Token Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Haohao Qu",
        "Wenqi Fan",
        "Shanru Lin"
      ],
      "abstract": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.",
      "tldr_zh": "这篇论文提出 DeftRec 框架，使用 denoising diffusion models 来处理连续 token 的生成式推荐系统（RecSys），以克服传统 LLM-based RecSys 在离散空间中的信息压缩和词汇限制问题。框架包括一个鲁棒的 tokenizer（结合 masking 操作和 additive K-way 架构）来索引用户和物品，捕获复杂的协作关系，以及一个条件化去噪扩散模型，通过预训练 LLM 的推理内容处理用户偏好，并整合负交互以提升推荐准确性。实验结果表明，DeftRec 超过了传统和新兴基准，在推荐生成方面表现出显著优势。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12007v1",
      "published_date": "2025-04-16 12:01:03 UTC",
      "updated_date": "2025-04-16 12:01:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:41:50.362266"
    },
    {
      "arxiv_id": "2504.11997v1",
      "title": "A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
      "title_zh": "无限地平线平均奖励线性马尔可夫决策过程的计算高效算法",
      "authors": [
        "Kihyuk Hong",
        "Ambuj Tewari"
      ],
      "abstract": "We study reinforcement learning in infinite-horizon average-reward settings\nwith linear MDPs. Previous work addresses this problem by approximating the\naverage-reward setting by discounted setting and employing a value\niteration-based algorithm that uses clipping to constrain the span of the value\nfunction for improved statistical efficiency. However, the clipping procedure\nrequires computing the minimum of the value function over the entire state\nspace, which is prohibitive since the state space in linear MDP setting can be\nlarge or even infinite. In this paper, we introduce a value iteration method\nwith efficient clipping operation that only requires computing the minimum of\nvalue functions over the set of states visited by the algorithm. Our algorithm\nenjoys the same regret bound as the previous work while being computationally\nefficient, with computational complexity that is independent of the size of the\nstate space.",
      "tldr_zh": "该论文研究了无限地平线平均奖励线性MDPs（Linear MDPs）中的强化学习问题。现有方法通过将平均奖励设置近似为折扣设置，并使用基于价值迭代的算法来约束价值函数跨度，但需要计算整个状态空间的最小值，导致计算复杂度高。本文提出了一种改进的价值迭代方法，采用高效剪切操作，仅需计算算法访问的状态集上的价值函数最小值。该算法保持了与之前工作相同的遗憾界限（regret bound），同时实现了计算效率的提升，其复杂度独立于状态空间的大小。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11997v1",
      "published_date": "2025-04-16 11:47:41 UTC",
      "updated_date": "2025-04-16 11:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:42:01.379868"
    },
    {
      "arxiv_id": "2504.11986v2",
      "title": "Large Language Models as Quasi-crystals: Coherence Without Repetition in Generative Text",
      "title_zh": "翻译失败",
      "authors": [
        "Jose Manuel Guevara-Vela"
      ],
      "abstract": "This essay proposes an interpretive analogy between large language models\n(LLMs) and quasicrystals, systems that exhibit global coherence without\nperiodic repetition, generated through local constraints. While LLMs are\ntypically evaluated in terms of predictive accuracy, factuality, or alignment,\nthis structural perspective suggests that one of their most characteristic\nbehaviors is the production of internally resonant linguistic patterns. Drawing\non the history of quasicrystals, which forced a redefinition of structural\norder in physical systems, the analogy highlights an alternative mode of\ncoherence in generative language: constraint-based organization without\nrepetition or symbolic intent. Rather than viewing LLMs as imperfect agents or\nstochastic approximators, we suggest understanding them as generators of\nquasi-structured outputs. This framing complements existing evaluation\nparadigms by foregrounding formal coherence and pattern as interpretable\nfeatures of model behavior. While the analogy has limits, it offers a\nconceptual tool for exploring how coherence might arise and be assessed in\nsystems where meaning is emergent, partial, or inaccessible. In support of this\nperspective, we draw on philosophy of science and language, including\nmodel-based accounts of scientific representation, structural realism, and\ninferentialist views of meaning. We further propose the notion of structural\nevaluation: a mode of assessment that examines how well outputs propagate\nconstraint, variation, and order across spans of generated text. This essay\naims to reframe the current discussion around large language models, not by\nrejecting existing methods, but by suggesting an additional axis of\ninterpretation grounded in structure rather than semantics.",
      "tldr_zh": "这篇论文将大型语言模型 (LLMs) 类比为准晶体 (quasicrystals)，强调 LLMs 通过局部约束生成全局连贯的文本模式，而非周期性重复或基于符号意图。作者借鉴科学哲学观点，包括结构现实主义和推理主义，提出 LLMs 应被视为准结构输出生成器，以补充现有评估方法，如预测准确性和事实性。论文引入结构评估 (structural evaluation) 概念，专注于分析输出文本中约束、变化和秩序的传播，从而为评估 LLMs 的形式连贯性提供新框架。总的来说，此类比有助于重新解读 LLMs 的行为，突出结构层面的连贯性而非语义层面。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The discussion was restructured to add limitations to the analogy and\n  other clarifications",
      "pdf_url": "http://arxiv.org/pdf/2504.11986v2",
      "published_date": "2025-04-16 11:27:47 UTC",
      "updated_date": "2025-04-19 13:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:42:15.023961"
    },
    {
      "arxiv_id": "2504.11977v1",
      "title": "Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
      "title_zh": "利用机器学习模型预测数字医疗分诊访谈的结果",
      "authors": [
        "Sofia Krylova",
        "Fabian Schmidt",
        "Vladimir Vlassov"
      ],
      "abstract": "Many existing digital triage systems are questionnaire-based, guiding\npatients to appropriate care levels based on information (e.g., symptoms,\nmedical history, and urgency) provided by the patients answering\nquestionnaires. Such a system often uses a deterministic model with predefined\nrules to determine care levels. It faces challenges with incomplete triage\ninterviews since it can only assist patients who finish the process. In this\nstudy, we explore the use of machine learning (ML) to predict outcomes of\nunfinished interviews, aiming to enhance patient care and service quality.\nPredicting triage outcomes from incomplete data is crucial for patient safety\nand healthcare efficiency. Our findings show that decision-tree models,\nparticularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy\nin predicting outcomes from complete interviews while having a linear\ncorrelation between the prediction accuracy and interview completeness degree.\nFor example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews\nwith 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness,\n58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\%\ncompleteness. The TabTransformer model demonstrated exceptional accuracy of\nover 80\\% for all degrees of completeness but required extensive training time,\nindicating a need for more powerful computational resources. The study\nhighlights the linear correlation between interview completeness and predictive\npower of the decision-tree models.",
      "tldr_zh": "本文研究利用机器学习 (ML) 模型预测数字医疗分诊访谈的结果，以解决现有基于预定义规则的系统无法处理不完整访谈的挑战，从而提升患者护理和效率。研究重点评估了决策树模型如 LGBMClassifier 和 CatBoostClassifier，以及 TabTransformer 模型，结果显示这些模型在完整访谈中准确率超过 80%。例如，LGBMClassifier 在 100% 完整度下达到 88.2% 准确率，而在 80%、60% 和 40% 完整度下分别降至 79.6%、58.9% 和 45.7%，展现出预测准确率与访谈完整度之间的线性相关性。TabTransformer 模型在所有完整度水平下均保持超过 80% 的准确率，但需更多计算资源以应对其训练时间。研究强调了 ML 在医疗分诊中的潜力，为改进服务质量提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.11977v1",
      "published_date": "2025-04-16 11:17:23 UTC",
      "updated_date": "2025-04-16 11:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:42:28.187202"
    },
    {
      "arxiv_id": "2504.11967v2",
      "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Dong",
        "Fengyi Wu",
        "Sanjian Zhang",
        "Guangyu Chen",
        "Yuzhi Hu",
        "Masumi Yano",
        "Jingdong Sun",
        "Siyu Huang",
        "Feng Liu",
        "Qi Dai",
        "Zhi-Qi Cheng"
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs.",
      "tldr_zh": "这篇调查论文全面审视了反无人机（anti-UAV）技术，聚焦于无人机（UAVs）在基础设施检查和监视中的双重角色，同时强调其带来的安全挑战。论文详细探讨了分类、检测和跟踪等核心目标，以及新兴方法如 diffusion-based data synthesis、多模态融合（multi-modal fusion）、vision-language modeling、自监督学习（self-supervised learning）和强化学习（reinforcement learning）。通过评估最先进解决方案在单模态（如 RGB）和多传感器管道（如红外、音频、雷达和 RF）上的表现，论文揭示了实时性能、隐蔽检测和群集场景中的关键差距，并提出开放研究方向以推动下一代鲁棒防御策略的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR Workshop Anti-UAV 2025. 15 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.11967v2",
      "published_date": "2025-04-16 10:58:33 UTC",
      "updated_date": "2025-04-17 09:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:42:39.714139"
    },
    {
      "arxiv_id": "2504.13951v1",
      "title": "Generative System Dynamics in Recurrent Neural Networks",
      "title_zh": "循环神经网络中的生成式系统动力学",
      "authors": [
        "Michele Casoni",
        "Tommaso Guidi",
        "Alessandro Betti",
        "Stefano Melacci",
        "Marco Gori"
      ],
      "abstract": "In this study, we investigate the continuous time dynamics of Recurrent\nNeural Networks (RNNs), focusing on systems with nonlinear activation\nfunctions. The objective of this work is to identify conditions under which\nRNNs exhibit perpetual oscillatory behavior, without converging to static fixed\npoints. We establish that skew-symmetric weight matrices are fundamental to\nenable stable limit cycles in both linear and nonlinear configurations. We\nfurther demonstrate that hyperbolic tangent-like activation functions (odd,\nbounded, and continuous) preserve these oscillatory dynamics by ensuring motion\ninvariants in state space. Numerical simulations showcase how nonlinear\nactivation functions not only maintain limit cycles, but also enhance the\nnumerical stability of the system integration process, mitigating those\ninstabilities that are commonly associated with the forward Euler method. The\nexperimental results of this analysis highlight practical considerations for\ndesigning neural architectures capable of capturing complex temporal\ndependencies, i.e., strategies for enhancing memorization skills in recurrent\nmodels.",
      "tldr_zh": "本文研究了 Recurrent Neural Networks (RNNs) 的连续时间动态，特别关注带有非线性激活函数的系统，旨在识别 RNNs 产生持续振荡行为（即稳定 limit cycles）而不收敛到静态固定点的条件。研究发现，skew-symmetric weight matrices 是实现这种振荡的核心要素，而 hyperbolic tangent-like activation functions（奇函数、 bounded 和连续）能保持这些动态，确保状态空间中的运动不变性。数值模拟进一步证明，非线性激活函数不仅维持 limit cycles，还提升了系统积分过程的数值稳定性，缓解了前向欧拉方法常见的稳定性问题，从而为设计捕捉复杂时间依赖性的神经架构提供实用策略。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13951v1",
      "published_date": "2025-04-16 10:39:43 UTC",
      "updated_date": "2025-04-16 10:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:42:50.818687"
    },
    {
      "arxiv_id": "2504.11952v2",
      "title": "Robust and Fine-Grained Detection of AI Generated Texts",
      "title_zh": "翻译失败",
      "authors": [
        "Ram Mohan Rao Kadiyala",
        "Siddartha Pullakhandam",
        "Kanwal Mehreen",
        "Drishti Sharma",
        "Siddhant Gupta",
        "Jebish Purbey",
        "Ashay Srivastava",
        "Subhasya TippaReddy",
        "Arvind Reddy Bobbili",
        "Suraj Telugara Chandrashekhar",
        "Modabbir Adeeb",
        "Srinadh Vura",
        "Hamza Farooq"
      ],
      "abstract": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
      "tldr_zh": "本论文针对AI生成文本检测的挑战，提出一套基于token classification的模型，旨在处理短文本和人类-LLM共同撰写（human-LLM co-authored texts）的部分AI生成内容。模型使用一个新引入的超过2.4M文本数据集进行训练，该数据集涵盖23种语言、多种流行专有LLMs，并包括未见领域、生成器、非母语者文本及对抗输入场景。实验结果显示，该模型在各种条件下表现出色，比现有系统更鲁棒，并提供了对不同领域、生成器、对抗方法、文本长度及特征的详细性能分析。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.11952v2",
      "published_date": "2025-04-16 10:29:30 UTC",
      "updated_date": "2025-05-22 10:09:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:43:02.557853"
    },
    {
      "arxiv_id": "2504.11944v1",
      "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning",
      "title_zh": "VIPO：价值函数不一致性惩罚的离线强化学习",
      "authors": [
        "Xuyang Chen",
        "Guojian Wang",
        "Keyu Yan",
        "Lin Zhao"
      ],
      "abstract": "Offline reinforcement learning (RL) learns effective policies from\npre-collected datasets, offering a practical solution for applications where\nonline interactions are risky or costly. Model-based approaches are\nparticularly advantageous for offline RL, owing to their data efficiency and\ngeneralizability. However, due to inherent model errors, model-based methods\noften artificially introduce conservatism guided by heuristic uncertainty\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\nmodel-based offline RL algorithm that incorporates self-supervised feedback\nfrom value estimation to enhance model training. Specifically, the model is\nlearned by additionally minimizing the inconsistency between the value learned\ndirectly from the offline data and the one estimated from the model. We perform\ncomprehensive evaluations from multiple perspectives to show that VIPO can\nlearn a highly accurate model efficiently and consistently outperform existing\nmethods. It offers a general framework that can be readily integrated into\nexisting model-based offline RL algorithms to systematically enhance model\naccuracy. As a result, VIPO achieves state-of-the-art performance on almost all\ntasks in both D4RL and NeoRL benchmarks.",
      "tldr_zh": "这篇论文提出了一种名为 VIPO 的基于模型的离线强化学习（Offline RL）算法，旨在通过最小化价值函数不一致性来提升模型准确性和可靠性，从而解决现有方法的保守性问题。VIPO 利用自监督反馈，将从离线数据直接学得的价值函数与模型估计的价值函数之间的不一致性作为优化目标，并可轻松整合到现有模型-based Offline RL 算法中。实验结果显示，VIPO 在 D4RL 和 NeoRL 基准上几乎所有任务中均超越基线方法，实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11944v1",
      "published_date": "2025-04-16 10:23:44 UTC",
      "updated_date": "2025-04-16 10:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:43:15.184063"
    },
    {
      "arxiv_id": "2504.11942v1",
      "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation",
      "title_zh": "ADAT：时间序列感知的自适应 Transformer 架构用于手语翻译",
      "authors": [
        "Nada Shahin",
        "Leila Ismail"
      ],
      "abstract": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.",
      "tldr_zh": "该研究针对手语翻译系统中 Transformer 架构在识别细粒度时间依赖和计算效率方面的不足，提出了 ADAT（Adaptive Transformer），通过增强特征提取和 gating mechanism 的自适应特征加权，强调相关上下文特征以提高准确性并降低训练开销。\nADAT 引入了首个公开医疗美国手语数据集 MedASL，用于评估模型性能。\n在实验中，ADAT 在 PHOENIX14T 和 MedASL 数据集上，比 encoder-decoder transformer 提高了 BLEU-4 准确率（如 0.1% 提升），并减少了训练时间（如 PHOENIX14T 上 14.33%），在 sign-to-text 任务中准确率提升至多 8.7%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11942v1",
      "published_date": "2025-04-16 10:20:11 UTC",
      "updated_date": "2025-04-16 10:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:43:29.729842"
    },
    {
      "arxiv_id": "2504.18556v1",
      "title": "RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features",
      "title_zh": "RDI：基于样本聚类特征的深度神经网络对抗鲁棒性评估指标",
      "authors": [
        "Jialei Song",
        "Xingquan Zuo",
        "Feiyang Wang",
        "Hai Huang",
        "Tianle Zhang"
      ],
      "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial samples,\nraising concerns about their reliability in safety-critical tasks. Currently,\nmethods of evaluating adversarial robustness are primarily categorized into\nattack-based and certified robustness evaluation approaches. The former not\nonly relies on specific attack algorithms but also is highly time-consuming,\nwhile the latter due to its analytical nature, is typically difficult to\nimplement for large and complex models. A few studies evaluate model robustness\nbased on the model's decision boundary, but they suffer from low evaluation\naccuracy. To address the aforementioned issues, we propose a novel adversarial\nrobustness evaluation metric, Robustness Difference Index (RDI), which is based\non sample clustering features. RDI draws inspiration from clustering evaluation\nby analyzing the intra-class and inter-class distances of feature vectors\nseparated by the decision boundary to quantify model robustness. It is\nattack-independent and has high computational efficiency. Experiments show\nthat, RDI demonstrates a stronger correlation with the gold-standard\nadversarial robustness metric of attack success rate (ASR). The average\ncomputation time of RDI is only 1/30 of the evaluation method based on the PGD\nattack. Our open-source code is available at:\nhttps://anonymous.4open.science/r/RDI-B1DA.",
      "tldr_zh": "该研究针对深度神经网络(DNNs)对对抗样本的易感性问题，提出了一种新型评估指标Robustness Difference Index (RDI)，基于样本聚类特征分析类内和类间距离来量化模型的对抗鲁棒性。RDI 避免了传统攻击-based方法的依赖性和高耗时，以及证书鲁棒性方法的实现难度，具有高效计算的优势。实验结果表明，RDI 与黄金标准指标Attack Success Rate (ASR)的相关性更强，且其平均计算时间仅为PGD攻击方法的1/30，为DNNs鲁棒性评估提供了更可靠的工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18556v1",
      "published_date": "2025-04-16 10:05:37 UTC",
      "updated_date": "2025-04-16 10:05:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:43:40.182289"
    },
    {
      "arxiv_id": "2504.11919v1",
      "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
      "title_zh": "从LLM自适应问题难度分级的视角重新审视高质量CoT数据的生成",
      "authors": [
        "Qianjin Yu",
        "Keyu Wu",
        "Zihan Chen",
        "Chushu Zhang",
        "Manlin Mei",
        "Lingjun Huang",
        "Fang Tan",
        "Yongsheng Du",
        "Kunlin Liu",
        "Yurui Zhu"
      ],
      "abstract": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
      "tldr_zh": "该论文从LLM-Adaptive问题难度分级的角度重新思考了生成高质量Chain-of-Thought (CoT)数据的方法，以适应不同规模的Large Language Models (LLMs)。研究首先根据LLMs的推理能力对问题难度进行分级，构建一个自适应问题数据库，然后基于难度分布采样问题，并利用DeepSeek-R1 (671B)生成对应的CoT数据，从而显著降低了数据生成成本并提高了Supervised Fine-Tuning (SFT)效率。在复杂数学竞赛和代码生成任务的验证中，该方法显示出良好的有效性和泛化性，例如，仅用2k高质量数学CoT数据，ZMath-32B就超过了DeepSeek-Distill-32B在数学推理任务上的表现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11919v1",
      "published_date": "2025-04-16 09:55:34 UTC",
      "updated_date": "2025-04-16 09:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:43:52.199863"
    },
    {
      "arxiv_id": "2504.11901v3",
      "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Castri",
        "Gloria Beraldo",
        "Nicola Bellotto"
      ],
      "abstract": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans.",
      "tldr_zh": "这篇论文提出了一种基于因果推理(causal inference)的决策框架，用于提升自主移动机器人(Autonomous Mobile Robots)在动态环境中的任务规划和执行能力。该框架通过分析因果关系来预测电池使用和人类障碍物对任务的影响，帮助机器人更有效地决定任务的时机和方式。为支持这一框架，研究者开发了PeopleFlow，一个基于Gazebo的模拟器，用于模拟真实的人类-机器人空间互动，包括受时间、环境布局和机器人状态影响的轨迹。在仓库环境案例研究中，该方法与非因果基准相比，显著提高了机器人的效率和安全性，证明了因果推理在复杂共享空间中的实际价值。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial\n  Interaction - Decision-Making",
      "pdf_url": "http://arxiv.org/pdf/2504.11901v3",
      "published_date": "2025-04-16 09:26:04 UTC",
      "updated_date": "2025-05-12 09:35:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:44:02.138309"
    },
    {
      "arxiv_id": "2504.11896v1",
      "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Xingxing Yang",
        "Jie Chen",
        "Zaifeng Yang"
      ],
      "abstract": "Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets.",
      "tldr_zh": "本文提出了一种基于物理信息先验的低光图像增强方法，名为 Physics-informed Color-aware Transform (PiCat)，旨在解决现有方法在 sRGB 颜色空间中颜色预测不一致和对光谱功率分布 (SPD) 变化高度敏感的问题。PiCat 框架包括 Color-aware Transform (CAT)，用于将低光图像转换为深度照明不变描述符，以及 Content-Noise Decomposition Network (CNDN)，通过分解内容和噪声来优化描述符分布，从而有效恢复图像内容表示。这些组件共同作为物理先验指导从低光到正常光的转换，并在五个基准数据集上表现出优于最先进方法的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.11896v1",
      "published_date": "2025-04-16 09:23:38 UTC",
      "updated_date": "2025-04-16 09:23:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:44:15.344983"
    },
    {
      "arxiv_id": "2504.13950v1",
      "title": "Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongxi Qiu",
        "Zhang Zhang",
        "Yan Hu",
        "Heng Li",
        "Jiang Liu"
      ],
      "abstract": "This paper explores optimal data selection strategies for Reinforcement\nLearning with Verified Rewards (RLVR) training in the medical domain. While\nRLVR has shown exceptional potential for enhancing reasoning capabilities in\nlarge language models, most prior implementations have focused on mathematics\nand logical puzzles, with limited exploration of domain-specific applications\nlike medicine. We investigate four distinct data sampling strategies from\nMedQA-USMLE: random sampling (baseline), and filtering using Phi-4,\nGemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base\nmodel and implementing Group Relative Policy Optimization (GRPO), we evaluate\nperformance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and\nCMMLU. Our findings demonstrate that models trained on filtered data generally\noutperform those trained on randomly selected samples. Notably, training on\nself-filtered samples (using Gemma-3-12b-it for filtering) achieved superior\nperformance in medical domains but showed reduced robustness across different\nbenchmarks, while filtering with larger models from the same series yielded\nbetter overall robustness. These results provide valuable insights into\neffective data organization strategies for RLVR in specialized domains and\nhighlight the importance of thoughtful data selection in achieving optimal\nperformance. You can access our repository\n(https://github.com/Qsingle/open-medical-r1) to get the codes.",
      "tldr_zh": "本文探讨了在医疗领域使用 Reinforcement Learning with Verified Rewards (RLVR) 训练的最佳数据选择策略，针对 MedQA-USMLE 数据集测试了四种采样方法，包括随机采样（基线）和使用 Phi-4、Gemma-3-27b-it 及 Gemma-3-12b-it 模型过滤。采用 Gemma-3-12b-it 作为基线模型并实施 Group Relative Policy Optimization (GRPO)，评估了模型在 MMLU、GSM8K、MMLU-Pro 和 CMMLU 等基准上的性能。结果表明，使用过滤数据的模型整体优于随机采样模型，其中自过滤策略（Gemma-3-12b-it）在医疗领域表现突出，但鲁棒性较差，而更大模型过滤则提供更好的整体鲁棒性。这些发现强调了数据选择在 RLVR 专业领域应用中的关键作用，并提供了代码仓库以供参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13950v1",
      "published_date": "2025-04-16 09:16:12 UTC",
      "updated_date": "2025-04-16 09:16:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:44:28.132976"
    },
    {
      "arxiv_id": "2504.11882v1",
      "title": "Seeking and leveraging alternative variable dependency concepts in gray-box-elusive bimodal land-use allocation problems",
      "title_zh": "翻译失败",
      "authors": [
        "J. Maciążek",
        "M. W. Przewozniczek",
        "J. Schwaab"
      ],
      "abstract": "Solving land-use allocation problems can help us to deal with some of the\nmost urgent global environmental issues. Since these problems are NP-hard,\neffective optimizers are needed to handle them. The knowledge about variable\ndependencies allows for proposing such tools. However, in this work, we\nconsider a real-world multi-objective problem for which standard variable\ndependency discovery techniques are inapplicable. Therefore, using\nlinkage-based variation operators is unreachable. To address this issue, we\npropose a definition of problem-dedicated variable dependency. On this base, we\npropose obtaining masks of dependent variables. Using them, we construct three\nnovel crossover operators. The results concerning real-world test cases show\nthat introducing our propositions into two well-known optimizers (NSGA-II,\nMOEA/D) dedicated to multi-objective optimization significantly improves their\neffectiveness.",
      "tldr_zh": "这篇论文针对NP-hard的灰盒土地利用分配问题（land-use allocation problems），提出了一种替代变量依赖性概念，以解决标准变量依赖性发现技术的局限性。作者定义了问题专用的变量依赖性，并基于此开发了依赖变量掩码（masks of dependent variables）和三个新交叉运算符（crossover operators）。将这些运算符整合到NSGA-II和MOEA/D优化器中后，在真实测试案例中显著提高了优化器的有效性，为处理多目标（multi-objective）环境问题提供了更高效的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11882v1",
      "published_date": "2025-04-16 09:06:55 UTC",
      "updated_date": "2025-04-16 09:06:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:44:39.745368"
    },
    {
      "arxiv_id": "2504.13211v1",
      "title": "Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance",
      "title_zh": "翻译失败",
      "authors": [
        "Subin Kim",
        "Hoonrae Kim",
        "Jihyun Lee",
        "Yejin Jeon",
        "Gary Geunbae Lee"
      ],
      "abstract": "Recent studies have explored the use of large language models (LLMs) in\npsychotherapy; however, text-based cognitive behavioral therapy (CBT) models\noften struggle with client resistance, which can weaken therapeutic alliance.\nTo address this, we propose a multimodal approach that incorporates nonverbal\ncues, allowing the AI therapist to better align its responses with the client's\nnegative emotional state. Specifically, we introduce a new synthetic dataset,\nMultimodal Interactive Rolling with Resistance (Mirror), which is a novel\nsynthetic dataset that pairs client statements with corresponding facial\nimages. Using this dataset, we train baseline Vision-Language Models (VLMs)\nthat can analyze facial cues, infer emotions, and generate empathetic responses\nto effectively manage resistance. They are then evaluated in terms of both the\ntherapist's counseling skills and the strength of the therapeutic alliance in\nthe presence of client resistance. Our results demonstrate that Mirror\nsignificantly enhances the AI therapist's ability to handle resistance, which\noutperforms existing text-based CBT approaches.",
      "tldr_zh": "本研究针对文本-based CBT 模型在处理客户抵抗时易削弱治疗联盟的问题，提出多模态认知重构疗法 Mirror，该方法整合非语言线索如面部表情，以提升 AI 治疗师的响应适应性。研究引入了一个新的合成数据集 Multimodal Interactive Rolling with Resistance（Mirror），该数据集配对客户陈述和对应面部图像，并用于训练基线 Vision-Language Models (VLMs) 以分析情绪并生成移情的响应。实验评估显示，Mirror 显著提高了 AI 治疗师的咨询技能和治疗联盟强度，优于现有的文本-based CBT 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13211v1",
      "published_date": "2025-04-16 08:44:26 UTC",
      "updated_date": "2025-04-16 08:44:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:44:51.132155"
    },
    {
      "arxiv_id": "2504.11864v1",
      "title": "Moving between high-quality optima using multi-satisfiability characteristics in hard-to-solve Max3Sat instances",
      "title_zh": "翻译失败",
      "authors": [
        "J. Piatek",
        "M. W. Przewozniczek",
        "F. Chicano",
        "R. Tinós"
      ],
      "abstract": "Gray-box optimization proposes effective and efficient optimizers of general\nuse. To this end, it leverages information about variable dependencies and the\nsubfunction-based problem representation. These approaches were already shown\neffective by enabling \\textit{tunnelling} between local optima even if these\nmoves require the modification of many dependent variables. Tunnelling is\nuseful in solving the maximum satisfiability problem (MaxSat), which can be\nreformulated to Max3Sat. Since many real-world problems can be brought to\nsolving the MaxSat/Max3Sat instances, it is important to solve them effectively\nand efficiently. Therefore, we focus on Max3Sat instances for which tunnelling\nfails to introduce improving moves between locally optimal high-quality\nsolutions and the region of globally optimal solutions. We analyze the features\nof such instances on the ground of phase transitions. Based on these\nobservations, we propose manipulating clause-satisfiability characteristics\nthat allow connecting high-quality solutions distant in the solution space. We\nutilize multi-satisfiability characteristics in the optimizer built from\ntypical gray-box mechanisms. The experimental study shows that the proposed\noptimizer can solve those Max3Sat instances that are out of the grasp of\nstate-of-the-art gray-box optimizers. At the same time, it remains effective\nfor instances that have already been successfully solved by gray-box.",
      "tldr_zh": "该论文针对难解的Max3Sat实例，探讨了使用multi-satisfiability characteristics来在高质量局部最优解之间实现移动，从而克服gray-box optimization中tunnelling机制的局限性。研究者通过分析实例的相变特征，提出了一种新优化器，该方法操纵子句可满足性特征以连接解空间中距离遥远的优质解。实验结果显示，该优化器能有效解决现有gray-box优化器无法处理的Max3Sat实例，同时对已可解决的实例保持高效性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11864v1",
      "published_date": "2025-04-16 08:38:08 UTC",
      "updated_date": "2025-04-16 08:38:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:45:02.485166"
    },
    {
      "arxiv_id": "2504.11855v1",
      "title": "EngramNCA: a Neural Cellular Automaton Model of Memory Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Etienne Guichard",
        "Felix Reimers",
        "Mia Kvalsund",
        "Mikkel Lepperød",
        "Stefano Nichele"
      ],
      "abstract": "This study introduces EngramNCA, a neural cellular automaton (NCA) that\nintegrates both publicly visible states and private, cell-internal memory\nchannels, drawing inspiration from emerging biological evidence suggesting that\nmemory storage extends beyond synaptic modifications to include intracellular\nmechanisms. The proposed model comprises two components: GeneCA, an NCA trained\nto develop distinct morphologies from seed cells containing immutable \"gene\"\nencodings, and GenePropCA, an auxiliary NCA that modulates the private\n\"genetic\" memory of cells without altering their visible states. This\narchitecture enables the encoding and propagation of complex morphologies\nthrough the interaction of visible and private channels, facilitating the\ngrowth of diverse structures from a shared \"genetic\" substrate. EngramNCA\nsupports the emergence of hierarchical and coexisting morphologies, offering\ninsights into decentralized memory storage and transfer in artificial systems.\nThese findings have potential implications for the development of adaptive,\nself-organizing systems and may contribute to the broader understanding of\nmemory mechanisms in both biological and synthetic contexts.",
      "tldr_zh": "该研究提出EngramNCA，一种神经细胞自动机 (NCA) 模型，整合了公开可见状态和私有细胞内部记忆通道，借鉴生物证据表明记忆存储不仅限于突触修改，还涉及细胞内机制。模型由两个组件组成：GeneCA，用于从包含不可变 \"gene\" 编码的种子细胞中训练出不同形态；以及GenePropCA，一个辅助NCA，用于调节细胞的私有 \"genetic\" 记忆而不改变可见状态。这种架构通过可见和私有通道的交互，支持复杂形态的编码、传播和层级化共存，提供对去中心化记忆存储与转移的洞见。研究结果可能推动适应性自组织系统的开发，并深化对生物和合成记忆机制的理解。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11855v1",
      "published_date": "2025-04-16 08:23:09 UTC",
      "updated_date": "2025-04-16 08:23:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:45:15.405065"
    },
    {
      "arxiv_id": "2504.13949v1",
      "title": "On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence",
      "title_zh": "翻译失败",
      "authors": [
        "M. W. Przewozniczek",
        "F. Chicano",
        "R. Tinós",
        "J. Nalepa",
        "B. Ruszczak",
        "A. M. Wijata"
      ],
      "abstract": "Gray-box optimization employs Walsh decomposition to obtain non-linear\nvariable dependencies and utilize them to propose masks of variables that have\na joint non-linear influence on fitness value. These masks significantly\nimprove the effectiveness of variation operators. In some problems, all\nvariables are non-linearly dependent, making the aforementioned masks useless.\nWe analyze the features of the real-world instances of such problems and show\nthat many of their dependencies may have noise-like origins. Such noise-caused\ndependencies are irrelevant to the optimization process and can be ignored. To\nidentify them, we propose extending the use of Walsh decomposition by measuring\nvariable dependency strength that allows the construction of the weighted\ndynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency\nstrength to mixed individuals. They allow the filtering of irrelevant\ndependencies and re-enable using dependency-based masks by variation operators.\nWe verify the wdVIG potential on a large benchmark suite. For problems with\nnoise, the wdVIG masks can improve the optimizer's effectiveness. If all\ndependencies are relevant for the optimization, i.e., the problem is not\nnoised, the influence of wdVIG masks is similar to that of state-of-the-art\nstructures of this kind.",
      "tldr_zh": "该论文探讨了 Gray-box optimization 中使用 Walsh decomposition 来揭示问题中的隐藏结构，特别是非线性变量依赖关系，以改进变异操作的有效性。作者发现，在某些问题中所有变量都相互依赖且可能包含噪声，这些噪声会导致依赖掩码失效，因此提出通过测量变量依赖强度构建加权动态 Variable Interaction Graph (wdVIG)。wdVIG 可以过滤无关噪声依赖，并动态调整到混合个体，从而重新启用依赖性掩码。在大型基准测试中，wdVIG 掩码显著提高了有噪声问题的优化器性能，而在无噪声问题中，其效果与最先进结构相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13949v1",
      "published_date": "2025-04-16 08:22:59 UTC",
      "updated_date": "2025-04-16 08:22:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:45:27.166783"
    },
    {
      "arxiv_id": "2504.11844v1",
      "title": "Evaluating the Goal-Directedness of Large Language Models",
      "title_zh": "大型语言模型的目标导向性评估",
      "authors": [
        "Tom Everitt",
        "Cristina Garbacea",
        "Alexis Bellot",
        "Jonathan Richens",
        "Henry Papadatos",
        "Siméon Campos",
        "Rohin Shah"
      ],
      "abstract": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.",
      "tldr_zh": "该研究评估了大型语言模型(LLMs)的目标导向性，即它们在给定目标下使用能力的程度，通过设计需要信息收集、认知努力和计划执行的任务，并利用子任务推断模型的相关能力。实验涉及Google DeepMind、OpenAI和Anthropic的LLMs，结果显示目标导向性在任务间相对一致，但与任务表现无关，且对激励提示仅有中等敏感性。大多数模型并非完全目标导向，这为监控LLM进展和 deliberate 设计代理属性提供了重要指导。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11844v1",
      "published_date": "2025-04-16 08:07:08 UTC",
      "updated_date": "2025-04-16 08:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:45:39.381060"
    },
    {
      "arxiv_id": "2504.11837v1",
      "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations",
      "title_zh": "FiSMiness：一种基于有限状态机的情感支持对话范式",
      "authors": [
        "Yue Zhao",
        "Qingqing Gu",
        "Xiaoyu Wang",
        "Teng Chen",
        "Zhonglin Jiang",
        "Yong Chen",
        "Luo Ji"
      ],
      "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.",
      "tldr_zh": "这篇论文提出了 FiSMiness，一种基于 Finite State Machine (FSM) 的框架，用于提升情感支持对话 (ESC)，旨在通过状态模型视角优化长期对话满意度。FiSMiness 让单个 LLMs 在每个对话回合中进行规划和自我推理，包括分析求助者的情绪、选择支持策略并生成响应。实验结果显示，该框架在 ESC 数据集上优于多种基线方法，如直接推理、Chain of Thought 和微调，即使这些基线使用了更多参数。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by CMCL",
      "pdf_url": "http://arxiv.org/pdf/2504.11837v1",
      "published_date": "2025-04-16 07:52:06 UTC",
      "updated_date": "2025-04-16 07:52:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:45:50.440425"
    },
    {
      "arxiv_id": "2504.11829v2",
      "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Kreutzer",
        "Eleftheria Briakou",
        "Sweta Agrawal",
        "Marzieh Fadaee",
        "Kocmi Tom"
      ],
      "abstract": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.",
      "tldr_zh": "这篇论文通过借鉴机器翻译(MT)评估的经验，解决多语言大型语言模型(mLLMs)的生成能力评估问题，因为当前评估缺乏全面性、科学性和一致性。作者进行针对性实验，展示MT领域的透明标准和可靠方法如何加深对模型质量差异的理解，并识别出mLLMs的元评估(meta-evaluation)必要组件，以确保评估方法的严谨性。最终，他们总结出一份行动建议清单，用于指导mLLM的研究和开发。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11829v2",
      "published_date": "2025-04-16 07:38:19 UTC",
      "updated_date": "2025-04-17 21:12:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:46:01.843320"
    },
    {
      "arxiv_id": "2504.13210v1",
      "title": "Graphical Models for Decision-Making: Integrating Causality and Game Theory",
      "title_zh": "决策的图形模型：整合因果性和博弈理论",
      "authors": [
        "Maarten C. Vonk",
        "Mauricio Gonzalez Soto",
        "Anna V. Kononova"
      ],
      "abstract": "Causality and game theory are two influential fields that contribute\nsignificantly to decision-making in various domains. Causality defines and\nmodels causal relationships in complex policy problems, while game theory\nprovides insights into strategic interactions among stakeholders with competing\ninterests. Integrating these frameworks has led to significant theoretical\nadvancements with the potential to improve decision-making processes. However,\npractical applications of these developments remain underexplored. To support\nefforts toward implementation, this paper clarifies key concepts in game theory\nand causality that are essential to their intersection, particularly within the\ncontext of probabilistic graphical models. By rigorously examining these\nconcepts and illustrating them with intuitive, consistent examples, we clarify\nthe required inputs for implementing these models, provide practitioners with\ninsights into their application and selection across different scenarios, and\nreference existing research that supports their implementation. We hope this\nwork encourages broader adoption of these models in real-world scenarios.",
      "tldr_zh": "本文探讨了如何整合 Causality 和 Game Theory 来提升决策过程，强调这两个领域在处理复杂政策问题和战略互动中的作用。论文通过概率图形模型(Probabilistic Graphical Models)框架，系统地澄清了关键概念，并使用直观例子说明实施所需输入，提供从业者选择和应用这些模型的实用见解。该研究引用现有工作，旨在推动这些理论在真实世界场景中的更广泛采用。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13210v1",
      "published_date": "2025-04-16 07:25:59 UTC",
      "updated_date": "2025-04-16 07:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:46:14.157433"
    },
    {
      "arxiv_id": "2504.11820v1",
      "title": "Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting",
      "title_zh": "真实世界深度恢复：通过结构不确定性",
      "authors": [
        "Delong Suzhang",
        "Meng Yang"
      ],
      "abstract": "The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps.",
      "tldr_zh": "这篇论文针对真实世界RGB-D数据集中的原始深度图结构不对齐问题，提出了一种新的深度恢复方法，以提升泛化能力。方法从输入角度设计了一个原始深度生成管道来增加结构不对齐的多样性，并引入结构 uncertainty 模块，利用深度基础模型（DFM）来明确识别和处理输入深度图的不确定性。从输出角度，开发了一个鲁棒的特征对齐模块，确保与RGB图像的准确结构对齐，从而避免不准确的 GT depth 的干扰。实验在多个数据集上证明，该方法在各种挑战性原始深度图上实现了竞争性的准确性和泛化性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11820v1",
      "published_date": "2025-04-16 07:14:01 UTC",
      "updated_date": "2025-04-16 07:14:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:46:27.099033"
    },
    {
      "arxiv_id": "2504.13948v2",
      "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images",
      "title_zh": "翻译失败",
      "authors": [
        "Juan David Salazar Rodriguez",
        "Sam Conrad Joyce",
        "Julfendi"
      ],
      "abstract": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively.",
      "tldr_zh": "这篇论文探讨了使用定制的 GPT 模型来提升建筑学生在生成 AI 驱动图像时的提示工程能力，旨在通过实验验证这种方法对学生技能的影响。研究采用混合方法设计，将学生分为三组：控制组、提供结构化提示指南的组，以及结合指南和交互式 AI 人物的组；学生通过逆向工程任务（如猜测和生成提示）来评估变量，包括提示时间、字数、prompt similarity 和 concreteness。结果显示，通过 ANOVA 分析，第三组在字数、相似度和具体性方面取得了统计显著的改善，同时定性反馈表明学生的自信和批判性思维得到增强。这些发现证明，定制 GPT 的交互式支持能有效帮助建筑学生更清晰地传达概念，从而推进建筑教育中的 generative AI 应用。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13948v2",
      "published_date": "2025-04-16 07:03:18 UTC",
      "updated_date": "2025-04-25 06:54:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:46:40.593194"
    },
    {
      "arxiv_id": "2504.13947v1",
      "title": "From job titles to jawlines: Using context voids to study generative AI systems",
      "title_zh": "翻译失败",
      "authors": [
        "Shahan Ali Memon",
        "Soham De",
        "Sungha Kang",
        "Riyan Mujtaba",
        "Bedoor AlShebli",
        "Katie Davis",
        "Jaime Snyder",
        "Jevin D. West"
      ],
      "abstract": "In this paper, we introduce a speculative design methodology for studying the\nbehavior of generative AI systems, framing design as a mode of inquiry. We\npropose bridging seemingly unrelated domains to generate intentional context\nvoids, using these tasks as probes to elicit AI model behavior. We demonstrate\nthis through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to\ngenerate headshots from professional Curricula Vitae (CVs). In contrast to\ntraditional ways, our approach assesses system behavior under conditions of\nradical uncertainty -- when forced to invent entire swaths of missing context\n-- revealing subtle stereotypes and value-laden assumptions. We qualitatively\nanalyze how the system interprets identity and competence markers from CVs,\ntranslating them into visual portraits despite the missing context (i.e.\nphysical descriptors). We show that within this context void, the AI system\ngenerates biased representations, potentially relying on stereotypical\nassociations or blatant hallucinations.",
      "tldr_zh": "本研究提出了一种推测性设计方法，用于探究生成式AI系统的行为，通过桥接看似无关的领域来创建“context voids”（上下文空缺），并将这些任务作为探测器来揭示AI模型的反应。作者通过案例研究，使用ChatGPT（包括GPT-4和DALL-E）从专业CVs（简历）生成头像，评估系统在极端不确定条件下的表现。结果显示，AI系统在缺少物理描述的情况下，会基于CV中的身份和能力标记生成偏见的视觉肖像，依赖于刻板印象或幻觉，从而暴露潜在的刻板印象和价值假设。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13947v1",
      "published_date": "2025-04-16 06:51:12 UTC",
      "updated_date": "2025-04-16 06:51:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:46:50.582417"
    },
    {
      "arxiv_id": "2504.11812v1",
      "title": "Learning Strategies in Particle Swarm Optimizer: A Critical Review and Performance Analysis",
      "title_zh": "粒子群优化器中的学习策略：批判性综述与性能分析",
      "authors": [
        "Dikshit Chauhan",
        "Shivani",
        "P. N. Suganthan"
      ],
      "abstract": "Nature has long inspired the development of swarm intelligence (SI), a key\nbranch of artificial intelligence that models collective behaviors observed in\nbiological systems for solving complex optimization problems. Particle swarm\noptimization (PSO) is widely adopted among SI algorithms due to its simplicity\nand efficiency. Despite numerous learning strategies proposed to enhance PSO's\nperformance in terms of convergence speed, robustness, and adaptability, no\ncomprehensive and systematic analysis of these strategies exists. We review and\nclassify various learning strategies to address this gap, assessing their\nimpact on optimization performance. Additionally, a comparative experimental\nevaluation is conducted to examine how these strategies influence PSO's search\ndynamics. Finally, we discuss open challenges and future directions,\nemphasizing the need for self-adaptive, intelligent PSO variants capable of\naddressing increasingly complex real-world problems.",
      "tldr_zh": "本论文对粒子群优化算法（PSO）的学习策略进行全面审查和性能分析，旨在提升PSO在收敛速度、鲁棒性和适应性方面的表现。作者分类了各种学习策略，并评估了它们对优化性能的影响，同时通过比较实验考察这些策略如何改变PSO的搜索动态。实验结果突显了策略的有效性，同时论文讨论了开放挑战和未来方向，强调开发自适应、智能的PSO变体以应对复杂现实问题。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "53 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.11812v1",
      "published_date": "2025-04-16 06:50:02 UTC",
      "updated_date": "2025-04-16 06:50:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:47:02.200330"
    },
    {
      "arxiv_id": "2504.21012v2",
      "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models",
      "title_zh": "唤醒AI：大语言模型中提示诱导相变的定量框架",
      "authors": [
        "Makoto Sato"
      ],
      "abstract": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)-either fused together or presented separately-by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept-a form of conceptual fusion-current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds.",
      "tldr_zh": "本文提出一个量化框架，用于分析大型语言模型 (LLMs) 中提示诱导的相变 (phase transition)，旨在比较人类和 AI 的认知动态。该框架包括 Transition-Inducing Prompt (TIP) 来触发 LLM 响应转变，以及 Transition Quantifying Prompt (TQP) 来评估这种变化，通过控制实验测试 LLMs 对语义上相距甚远的概念（如数学非周期性和传统工艺）融合或分离的反应。结果显示，LLMs 在语义融合提示和非融合提示之间没有显著响应差异，而人类则表现出更高的参与度，表明当前 LLMs 尚未复制人类直觉中的概念整合过程。此方法提供细粒度、可再现的认知响应测量，有助于揭示人工智能和人类思维中直觉与概念飞跃的关键差异。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.21012v2",
      "published_date": "2025-04-16 06:49:45 UTC",
      "updated_date": "2025-05-01 14:58:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:47:16.693135"
    },
    {
      "arxiv_id": "2504.12360v1",
      "title": "A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version",
      "title_zh": "处理文本文档可解释",
      "authors": [
        "Mieczysław A. Kłopotek",
        "Sławomir T. Wierzchoń",
        "Bartłomiej Starosta",
        "Dariusz Czerski",
        "Piotr Borkowski"
      ],
      "abstract": "This paper investigates the problem of Graph Spectral Clustering with\nnegative similarities, resulting from document embeddings different from the\ntraditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for\ncombinatorial Laplacians and normalized Laplacians are discussed. An\nexperimental investigation shows the advantages and disadvantages of 6\ndifferent solutions proposed in the literature and in this research. The\nresearch demonstrates that GloVe embeddings frequently cause failures of\nnormalized Laplacian based GSC due to negative similarities. Furthermore,\napplication of methods curing similarity negativity leads to accuracy\nimprovement for both combinatorial and normalized Laplacian based GSC. It also\nleads to applicability for GloVe embeddings of explanation methods developed\noriginally bythe authors for Term Vector Space embeddings.",
      "tldr_zh": "这篇论文提出了一种处理Graph Spectral Clustering中负相似度的方法，特别是针对非传统文档嵌入（如doc2vec和GloVe）导致的负值问题。论文讨论了组合Laplacian和归一化Laplacian的解决方案，并实验比较了6种文献中和本研究提出的方法。结果显示，GloVe嵌入经常导致基于归一化Laplacian的GSC失败，但通过修正负相似度技术，可以显著提高组合和归一化Laplacian基于GSC的准确性，并扩展解释方法的应用范围至这些嵌入。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "1 figure, 17 pages, this is an extended version of a paper accepted\n  for the 25th International Conference on Computational Science (ICCS), 7-9\n  July 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12360v1",
      "published_date": "2025-04-16 06:03:02 UTC",
      "updated_date": "2025-04-16 06:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:47:27.356155"
    },
    {
      "arxiv_id": "2504.11793v3",
      "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification",
      "title_zh": "选择性注意力联邦学习：用于临床文本分类的隐私性和效率提升",
      "authors": [
        "Yue Li",
        "Lihong Zhang"
      ],
      "abstract": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
      "tldr_zh": "该论文提出了 Selective Attention Federated Learning (SAFL)，一种创新方法，旨在解决 Federated Learning (FL) 在训练大型语言模型 (LLMs) 时面临的通信开销和模型隐私挑战，特别是针对临床文本分类任务。SAFL 通过分析注意力模式动态识别并仅微调关键的 transformer 层，从而显著降低通信带宽并提升 differential privacy 韧性。在 i2b2 Clinical Concept Extraction 和 MIMIC-III discharge summaries 等临床 NLP 基准测试中，SAFL 实现了与集中式模型相当的性能，同时大幅提高了通信效率和隐私保护。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11793v3",
      "published_date": "2025-04-16 05:59:29 UTC",
      "updated_date": "2025-04-18 20:58:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:47:39.187117"
    },
    {
      "arxiv_id": "2504.11792v1",
      "title": "Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records",
      "title_zh": "大型语言模型用于",
      "authors": [
        "Md Sultan Al Nahian",
        "Chris Delcher",
        "Daniel Harris",
        "Peter Akpunonu",
        "Ramakanth Kavuluru"
      ],
      "abstract": "The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)，如GPT-4o，从患者的纵向医疗记录中预测药物过量风险，以支持及时干预。研究评估了LLMs在微调和zero-shot设置下的性能，并与传统机器学习模型作为基线进行比较。结果表明，LLMs在某些场景下优于传统方法，甚至在zero-shot条件下无需特定训练即可有效预测风险。这些发现强调了LLMs在临床决策支持领域的潜力，特别是针对药物过量风险的预测。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11792v1",
      "published_date": "2025-04-16 05:52:22 UTC",
      "updated_date": "2025-04-16 05:52:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:47:51.265376"
    },
    {
      "arxiv_id": "2504.11788v1",
      "title": "Enhancing Web Agents with Explicit Rollback Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Zhisong Zhang",
        "Tianqing Fang",
        "Kaixin Ma",
        "Wenhao Yu",
        "Hongming Zhang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "With recent advancements in large language models, web agents have been\ngreatly improved. However, dealing with complex and dynamic web environments\nrequires more advanced planning and search abilities. Previous studies usually\nadopt a greedy one-way search strategy, which may struggle to recover from\nerroneous states. In this work, we enhance web agents with an explicit rollback\nmechanism, enabling the agent to revert back to a previous state in its\nnavigation trajectory. This mechanism gives the model the flexibility to\ndirectly control the search process, leading to an effective and efficient web\nnavigation method. We conduct experiments on two live web navigation benchmarks\nwith zero-shot and fine-tuning settings. The results demonstrate the\neffectiveness of our proposed approach.",
      "tldr_zh": "本研究针对大型语言模型驱动的网络代理在复杂动态网络环境中面临的规划和搜索挑战，提出了一种增强方法，通过引入显式回滚机制（explicit rollback mechanisms）来解决贪婪单向搜索策略的局限性。该机制允许代理从导航轨迹中回退到先前状态，从而实现更灵活有效的搜索过程。实验在两个实时网络导航基准上进行，包括零样本和微调设置，结果显示该方法显著提高了代理的性能和效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11788v1",
      "published_date": "2025-04-16 05:41:20 UTC",
      "updated_date": "2025-04-16 05:41:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:48:02.855019"
    },
    {
      "arxiv_id": "2504.11781v1",
      "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Guanchun Wang",
        "Xiangrong Zhang",
        "Yifei Zhang",
        "Zelin Peng",
        "Tianyang Zhang",
        "Xu Tang",
        "Licheng Jiao"
      ],
      "abstract": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.",
      "tldr_zh": "本研究针对高光谱图像(HSI)中的无监督异常检测问题，提出了一种快速框架ACMamba，即不对称共识状态空间模型，以显著降低计算成本，同时保持检测准确性。ACMamba采用不对称检测范式，使用区域级实例替代密集像素级样本，并引入基于Mamba的低成本模块来提取全局上下文属性，同时通过共识学习策略优化背景重建和异常压缩。实验结果显示，在八个基准数据集上，ACMamba比现有最先进方法具有更快速度和更强性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.11781v1",
      "published_date": "2025-04-16 05:33:42 UTC",
      "updated_date": "2025-04-16 05:33:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:48:15.572002"
    },
    {
      "arxiv_id": "2504.11780v1",
      "title": "Agile Retrospectives: What went well? What didn't go well? What should we do?",
      "title_zh": "翻译失败",
      "authors": [
        "Maria Spichkova",
        "Hina Lee",
        "Kevin Iwan",
        "Madeleine Zwart",
        "Yuwon Yoon",
        "Xiaohan Qin"
      ],
      "abstract": "In Agile/Scrum software development, the idea of retrospective meetings\n(retros) is one of the core elements of the project process. In this paper, we\npresent our work in progress focusing on two aspects: analysis of potential\nusage of generative AI for information interaction within retrospective\nmeetings, and visualisation of retros' information to software development\nteams. We also present our prototype tool RetroAI++, focusing on retros-related\nfunctionalities.",
      "tldr_zh": "这篇论文探讨了Agile/Scrum软件开发中回顾会议(retros)的核心作用，重点分析生成式AI在会议信息交互中的潜在应用，以及回顾信息对软件开发团队的可视化呈现。研究者开发了原型工具RetroAI++，专注于增强retros的相关功能，以提升团队协作效率。该工作仍在进行中，旨在为Agile项目过程提供更智能化的支持。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Preprint. Accepted to the 20th International Conference on Evaluation\n  of Novel Approaches to Software Engineering (ENASE 2025). Final version to be\n  published by SCITEPRESS, http://www.scitepress.org",
      "pdf_url": "http://arxiv.org/pdf/2504.11780v1",
      "published_date": "2025-04-16 05:33:35 UTC",
      "updated_date": "2025-04-16 05:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:48:26.380958"
    },
    {
      "arxiv_id": "2504.11774v1",
      "title": "PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility",
      "title_zh": "翻译失败",
      "authors": [
        "Keke Gai",
        "Ziyue Shen",
        "Jing Yu",
        "Liehuang Zhu",
        "Qi Wu"
      ],
      "abstract": "With the growing demand for protecting the intellectual property (IP) of\ntext-to-image diffusion models, we propose PCDiff -- a proactive access control\nframework that redefines model authorization by regulating generation quality.\nAt its core, PCDIFF integrates a trainable fuser module and hierarchical\nauthentication layers into the decoder architecture, ensuring that only users\nwith valid encrypted credentials can generate high-fidelity images. In the\nabsence of valid keys, the system deliberately degrades output quality,\neffectively preventing unauthorized exploitation.Importantly, while the primary\nmechanism enforces active access control through architectural intervention,\nits decoupled design retains compatibility with existing watermarking\ntechniques. This satisfies the need of model owners to actively control model\nownership while preserving the traceability capabilities provided by\ntraditional watermarking approaches.Extensive experimental evaluations confirm\na strong dependency between credential verification and image quality across\nvarious attack scenarios. Moreover, when combined with typical post-processing\noperations, PCDIFF demonstrates powerful performance alongside conventional\nwatermarking methods. This work shifts the paradigm from passive detection to\nproactive enforcement of authorization, laying the groundwork for IP management\nof diffusion models.",
      "tldr_zh": "该研究提出PCDiff，一种主动访问控制框架，用于保护文本到图像diffusion models的知识产权，通过调节生成质量来重新定义模型授权。该框架在解码器架构中整合可训练的fuser模块和hierarchical authentication layers，确保只有持有有效加密凭证的用户才能生成高保真图像，而无凭证时输出质量会故意降低。同时，PCDiff的设计与现有watermarking技术兼容，支持模型所有者主动控制所有权并保留追踪能力。实验结果显示，在各种攻击场景下，凭证验证与图像质量高度相关，且与传统watermarking方法结合时表现出色，为diffusion models的IP管理提供主动执行的范式转变。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11774v1",
      "published_date": "2025-04-16 05:28:50 UTC",
      "updated_date": "2025-04-16 05:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:48:38.808225"
    },
    {
      "arxiv_id": "2504.13209v1",
      "title": "On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Bi",
        "Chenghang Ye",
        "Zheyu Yang",
        "Ziyi Zhou",
        "Cui Tang",
        "Jun Zhang",
        "Zui Tao",
        "Kailong Wang",
        "Liting Zhou",
        "Yang Yang",
        "Tianlong Yu"
      ],
      "abstract": "Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are\nrapidly evolving, providing unprecedented capabilities for human-computer\ninteraction. However, their integration introduces a new attack surface for\nsocial engineering. In this paper, we systematically investigate the\nfeasibility of orchestrating AR-driven Social Engineering attacks using\nMultimodal LLM for the first time, via our proposed SEAR framework, which\noperates through three key phases: (1) AR-based social context synthesis, which\nfuses Multimodal inputs (visual, auditory and environmental cues); (2)\nrole-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically\nretrieves and integrates contextual data while preserving character\ndifferentiation; and (3) ReInteract social engineering agents, which execute\nadaptive multiphase attack strategies through inference interaction loops. To\nverify SEAR, we conducted an IRB-approved study with 60 participants in three\nexperimental configurations (unassisted, AR+LLM, and full SEAR pipeline)\ncompiling a new dataset of 180 annotated conversations in simulated social\nscenarios. Our results show that SEAR is highly effective at eliciting\nhigh-risk behaviors (e.g., 93.3% of participants susceptible to email\nphishing). The framework was particularly effective in building trust, with 85%\nof targets willing to accept an attacker's call after an interaction. Also, we\nidentified notable limitations such as ``occasionally artificial'' due to\nperceived authenticity gaps. This work provides proof-of-concept for AR-LLM\ndriven social engineering attacks and insights for developing defensive\ncountermeasures against next-generation augmented reality threats.",
      "tldr_zh": "本研究首次系统探讨了使用 Multimodal LLMs 执行 AR 驱动的社会工程攻击的可行性，提出 SEAR 框架来模拟此类攻击。SEAR 通过三个关键阶段运作：(1) AR-based social context synthesis 融合视觉、听觉和环境线索合成社会情境；(2) role-based Multimodal RAG 动态检索并整合上下文数据以保持角色差异；(3) ReInteract social engineering agents 执行自适应多阶段攻击策略。实验涉及 60 名参与者并收集 180 个标注对话，结果显示 SEAR 高度有效，导致 93.3% 参与者易受电子邮件钓鱼攻击，且 85% 的目标愿意接受攻击者电话；然而，也暴露了如“occasionally artificial”的局限性。该工作为 AR-LLM 驱动攻击提供证明概念，并为开发防御措施提供重要见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13209v1",
      "published_date": "2025-04-16 05:18:36 UTC",
      "updated_date": "2025-04-16 05:18:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:48:51.762000"
    },
    {
      "arxiv_id": "2504.11765v1",
      "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs",
      "title_zh": "共享磁盘 KV 缓存管理，用于 RAG 驱动的 LLMs 中的高效多实例",
      "authors": [
        "Hyungwoo Lee",
        "Kihyun Kim",
        "Jinwoo Kim",
        "Jungmin So",
        "Myung-Hoon Cha",
        "Hong-Yeon Kim",
        "James J. Kim",
        "Youngjae Kim"
      ],
      "abstract": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
      "tldr_zh": "该论文针对RAG增强的大语言模型(LLMs)中，由于输入上下文长度增加导致的推理延迟问题，特别是预填充阶段的time-to-first-token (TTFT)延长，提出了一种基于磁盘的key-value (KV)缓存管理方法，以减轻计算负担。论文引入Shared RAG-DCache系统，该系统利用查询相关文档的局部性和LLM推理服务的排队延迟，主动生成并在多个LLM实例间共享磁盘KV缓存，从而优化多实例推理环境下的性能。在配备2 GPU和1 CPU的单主机实验中，Shared RAG-DCache实现了15~71%的吞吐量提升和12~65%的延迟减少，证明了其在资源约束下的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11765v1",
      "published_date": "2025-04-16 04:59:18 UTC",
      "updated_date": "2025-04-16 04:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:49:03.893057"
    },
    {
      "arxiv_id": "2504.13208v1",
      "title": "Intelligent road crack detection and analysis based on improved YOLOv8",
      "title_zh": "基于改进 YOLOv8 的智能道路裂缝检测和分析",
      "authors": [
        "Haomin Zuo",
        "Zhengyang Li",
        "Jiangchuan Gong",
        "Zhen Tian"
      ],
      "abstract": "As urbanization speeds up and traffic flow increases, the issue of pavement\ndistress is becoming increasingly pronounced, posing a severe threat to road\nsafety and service life. Traditional methods of pothole detection rely on\nmanual inspection, which is not only inefficient but also costly. This paper\nproposes an intelligent road crack detection and analysis system, based on the\nenhanced YOLOv8 deep learning framework. A target segmentation model has been\ndeveloped through the training of 4029 images, capable of efficiently and\naccurately recognizing and segmenting crack regions in roads. The model also\nanalyzes the segmented regions to precisely calculate the maximum and minimum\nwidths of cracks and their exact locations. Experimental results indicate that\nthe incorporation of ECA and CBAM attention mechanisms substantially enhances\nthe model's detection accuracy and efficiency, offering a novel solution for\nroad maintenance and safety monitoring.",
      "tldr_zh": "这篇论文针对城市化加速导致的路面裂缝问题，提出了一种基于改进 YOLOv8 深度学习框架的智能检测和分析系统，以替代低效的手动检查方法。该系统通过训练 4029 张图像的目标分割模型，实现对裂缝区域的高效识别、分割，并精确计算裂缝的最大和最小宽度及其位置。实验结果表明，加入 ECA 和 CBAM 注意力机制后，模型的检测准确性和效率显著提升，为道路维护和安全监测提供了创新解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE - ICAACE 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13208v1",
      "published_date": "2025-04-16 04:50:28 UTC",
      "updated_date": "2025-04-16 04:50:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:49:14.457017"
    },
    {
      "arxiv_id": "2504.11754v1",
      "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Zihui Zhang",
        "Yafei Yang",
        "Hongtao Wen",
        "Bo Yang"
      ],
      "abstract": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.",
      "tldr_zh": "本文提出GrabS，一种生成式embodied agent，用于无监督的3D object segmentation，避免依赖人类标注的3D场景监督。该方法采用两阶段管道：第一阶段从对象数据集学习生成和判别性的object-centric priors；第二阶段设计embodied agent通过查询这些priors来发现和分割多个对象。在两个真实数据集和一个新合成数据集上评估，GrabS的分割性能显著超越现有无监督方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS",
      "pdf_url": "http://arxiv.org/pdf/2504.11754v1",
      "published_date": "2025-04-16 04:13:53 UTC",
      "updated_date": "2025-04-16 04:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:49:27.323285"
    },
    {
      "arxiv_id": "2504.12359v1",
      "title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models",
      "title_zh": "揭示 Mixture-of-Experts 在大型语言模型中的隐藏协作",
      "authors": [
        "Yuanbo Tang",
        "Yan Tang",
        "Naifan Zhang",
        "Meixuan Chen",
        "Yang Li"
      ],
      "abstract": "Mixture-of-Experts based large language models (MoE LLMs) have shown\nsignificant promise in multitask adaptability by dynamically routing inputs to\nspecialized experts. Despite their success, the collaborative mechanisms among\nexperts are still not well understood, limiting both the interpretability and\noptimization of these models. In this paper, we focus on two critical issues:\n(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs\nthrough expert pruning. To address the first issue, we propose a hierarchical\nsparse dictionary learning (HSDL) method that uncovers the collaboration\npatterns among experts. For the second issue, we introduce the\nContribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes\nlow-contribution experts. Our extensive experiments demonstrate that expert\ncollaboration patterns are closely linked to specific input types and exhibit\nsemantic significance across various tasks. Moreover, pruning experiments show\nthat our approach improves overall performance by 2.5\\% on average,\noutperforming existing methods. These findings offer valuable insights into\nenhancing the efficiency and interpretability of MoE LLMs, offering a clearer\nunderstanding of expert interactions and improving model optimization.",
      "tldr_zh": "这篇论文揭示了Mixture-of-Experts (MoE)大型语言模型中专家之间的隐藏协作机制，针对协作模式识别和模型优化两大问题进行研究。\n研究提出hierarchical sparse dictionary learning (HSDL)方法来挖掘专家协作模式，以及Contribution-Aware Expert Pruning (CAEP)算法来有效修剪低贡献专家。\n实验结果表明，专家协作与特定输入类型紧密相关，并具有语义意义，而CAEP算法平均提高了2.5%的整体性能，优于现有方法，从而提升了MoE LLMs的效率和可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12359v1",
      "published_date": "2025-04-16 04:06:15 UTC",
      "updated_date": "2025-04-16 04:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:49:38.781099"
    },
    {
      "arxiv_id": "2504.11750v1",
      "title": "Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Prabhu Vellaisamy",
        "Thomas Labonte",
        "Sourav Chakraborty",
        "Matt Turner",
        "Samantika Sury",
        "John Paul Shen"
      ],
      "abstract": "Large language model (LLM)-based inference workloads increasingly dominate\ndata center costs and resource utilization. Therefore, understanding the\ninference workload characteristics on evolving CPU-GPU coupled architectures is\ncrucial for optimization. This paper presents an in-depth analysis of LLM\ninference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled\n(GH200) systems. We analyze performance dynamics using fine-grained\noperator-to-kernel trace analysis, facilitated by our novel profiler SKIP and\nmetrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that\nclosely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)\nsystems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for\nLlama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound\nup to 4x larger batch sizes than LC systems. In this extended CPU-bound region,\nwe identify the performance characteristics of the Grace CPU as a key factor\ncontributing to higher inference latency at low batch sizes on GH200. We\ndemonstrate that TKLQT accurately identifies this CPU/GPU-bound transition\npoint. Based on this analysis, we further show that kernel fusion offers\nsignificant potential to mitigate GH200's low-batch latency bottleneck by\nreducing kernel launch overhead. This detailed kernel-level characterization\nprovides critical insights for optimizing diverse CPU-GPU coupling strategies.\nThis work is an initial effort, and we plan to explore other major AI/DL\nworkloads that demand different degrees of CPU-GPU heterogeneous architectures.",
      "tldr_zh": "这篇论文分析并优化了LLM推理工作负载在CPU-GPU耦合架构上的特性，聚焦于松耦合（PCIe A100/H100）和紧耦合（GH200）系统。研究采用细粒度操作符到内核跟踪分析、SKIP分析器和Total Kernel Launch and Queuing Time (TKLQT)指标，揭示GH200在大型批量下性能优异，预填充延迟比松耦合系统快1.9x-2.7x，但小批量时仍受CPU绑定影响，其中Grace CPU的特性是关键瓶颈。作者证明TKLQT能准确识别CPU/GPU绑定转换点，并建议通过内核融合减少内核启动开销，以缓解GH200的低批量延迟问题，为优化CPU-GPU耦合策略提供重要洞见。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted for ISPASS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.11750v1",
      "published_date": "2025-04-16 04:02:39 UTC",
      "updated_date": "2025-04-16 04:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:49:52.485347"
    },
    {
      "arxiv_id": "2504.15296v1",
      "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling",
      "title_zh": "基于云的 AI 推理服务的扩展性优化：实时负载均衡和自动缩放策略",
      "authors": [
        "Yihong Jin",
        "Ze Yang"
      ],
      "abstract": "The rapid expansion of AI inference services in the cloud necessitates a\nrobust scalability solution to manage dynamic workloads and maintain high\nperformance. This study proposes a comprehensive scalability optimization\nframework for cloud AI inference services, focusing on real-time load balancing\nand autoscaling strategies. The proposed model is a hybrid approach that\ncombines reinforcement learning for adaptive load distribution and deep neural\nnetworks for accurate demand forecasting. This multi-layered approach enables\nthe system to anticipate workload fluctuations and proactively adjust\nresources, ensuring maximum resource utilisation and minimising latency.\nFurthermore, the incorporation of a decentralised decision-making process\nwithin the model serves to enhance fault tolerance and reduce response time in\nscaling operations. Experimental results demonstrate that the proposed model\nenhances load balancing efficiency by 35\\ and reduces response delay by 28\\,\nthereby exhibiting a substantial optimization effect in comparison with\nconventional scalability solutions.",
      "tldr_zh": "该研究针对云端 AI 推理服务的快速扩展，提出一个全面的可扩展性优化框架，专注于实时负载均衡和自动缩放策略。该框架采用混合方法，结合 reinforcement learning 用于自适应负载分布，以及 deep neural networks 用于准确的需求预测，从而实现对工作负载波动的预见性调整，确保最大资源利用和最小延迟。同时，该框架引入去中心化决策过程，以提升故障容忍性和缩放响应速度。实验结果显示，与传统方案相比，该模型提高了负载均衡效率 35% 并减少了响应延迟 28%，显著优化了云 AI 服务性能。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "F.2.2; I.2.8"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to BDICN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15296v1",
      "published_date": "2025-04-16 04:00:04 UTC",
      "updated_date": "2025-04-16 04:00:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:50:02.758027"
    },
    {
      "arxiv_id": "2504.12358v1",
      "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance",
      "title_zh": "翻译失败",
      "authors": [
        "Aditi Verma",
        "Elizabeth Williams"
      ],
      "abstract": "AI models are rapidly becoming embedded in all aspects of nuclear energy\nresearch and work but the safety, security, and safeguards consequences of this\nembedding are not well understood. In this paper, we call for the creation of\nan anticipatory system of governance for AI in the nuclear sector as well as\nthe creation of a global AI observatory as a means for operationalizing\nanticipatory governance. The paper explores the contours of the nuclear AI\nobservatory and an anticipatory system of governance by drawing on work in\nscience and technology studies, public policy, and foresight studies.",
      "tldr_zh": "该论文探讨了 AI 模型在核能领域迅速嵌入可能带来的安全、安全保障和保障措施的未知风险，并呼吁建立一个预见性治理系统和全球 AI observatory 来应对这些挑战。通过借鉴 science and technology studies、public policy 和 foresight studies 的研究，该论文勾勒出核部门 AI observatory 的框架，以实现对 AI 应用的前瞻性管理。该工作为增强核能领域的 AI 治理提供了一个可操作的工具。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CY",
      "comment": "Presented at the Sociotechnical AI Governance Workshop at CHI 2025,\n  Yokohama",
      "pdf_url": "http://arxiv.org/pdf/2504.12358v1",
      "published_date": "2025-04-16 03:43:15 UTC",
      "updated_date": "2025-04-16 03:43:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:50:14.495213"
    },
    {
      "arxiv_id": "2504.11741v1",
      "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?",
      "title_zh": "翻译失败",
      "authors": [
        "Yiyou Sun",
        "Georgia Zhou",
        "Hao Wang",
        "Dacheng Li",
        "Nouha Dziri",
        "Dawn Song"
      ],
      "abstract": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.",
      "tldr_zh": "这篇论文分析了监督微调 (SFT) 对大型语言模型 (LLMs) 在数学推理任务中的影响，使用 AIME24 数据集将问题分为四个难度层级（Easy、Medium、Hard 和 Extremely Hard），并探讨了各层级之间的提升要求。研究发现，从 Easy 到 Medium 层级只需采用 R1 推理风格和少量 SFT（500-1K 实例），而 Hard 层级问题常在推理链的每个步骤出现错误，导致准确率停留在约 65%；Extremely Hard 层级则需要非传统问题解决技能，目前模型难以应对。总体而言，论文强调精心策划的小规模数据集优势有限，扩展数据集大小更有效，并为提升 LLMs 的数学推理能力提供了清晰路线图。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11741v1",
      "published_date": "2025-04-16 03:39:38 UTC",
      "updated_date": "2025-04-16 03:39:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:50:29.019185"
    },
    {
      "arxiv_id": "2504.13945v4",
      "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhanglin Wu",
        "Tengfei Song",
        "Ning Xie",
        "Mengli Zhu",
        "Weidong Zhang",
        "Shuang Wu",
        "Pengfei Li",
        "Chong Li",
        "Junhao Zhu",
        "Hao Yang",
        "Shiliang Sun"
      ],
      "abstract": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.",
      "tldr_zh": "该论文提出 Menu OCR and Translation Benchmark (MOTBench)，一个针对大型视觉语言模型 (LVLMs) 的新基准，用于评估其在复杂布局菜单上的光学字符识别 (OCR) 和翻译能力，填补了现有评估如 OCRBench 对长文本和复杂设计的忽略。MOTBench 包含中文和英文菜单，涉及多样字体和文化特定元素，并提供精确人类标注，要求模型准确识别并翻译菜名、价格和单位项目。实验结果显示，该基准的自动评估与专业人类评估高度一致，并通过分析多种公开 SOTA LVLMs 的性能，揭示了它们的优势和不足，为未来 LVLMs 发展提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures, 5 Tables",
      "pdf_url": "http://arxiv.org/pdf/2504.13945v4",
      "published_date": "2025-04-16 03:08:57 UTC",
      "updated_date": "2025-05-19 06:35:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:50:39.530172"
    },
    {
      "arxiv_id": "2504.11726v1",
      "title": "Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Yunzhe Li",
        "Facheng Hu",
        "Hongzi Zhu",
        "Shifan Zhang",
        "Liang Zhang",
        "Shan Chang",
        "Minyi Guo"
      ],
      "abstract": "Inertial measurement units (IMUs), have been prevalently used in a wide range\nof mobile perception applications such as activity recognition and user\nauthentication, where a large amount of labelled data are normally required to\ntrain a satisfactory model. However, it is difficult to label micro-activities\nin massive IMU data due to the hardness of understanding raw IMU data and the\nlack of ground truth. In this paper, we propose a novel fine-grained user\nperception approach, called Saga, which only needs a small amount of labelled\nIMU data to achieve stunning user perception accuracy. The core idea of Saga is\nto first pre-train a backbone feature extraction model, utilizing the rich\nsemantic information of different levels embedded in the massive unlabelled IMU\ndata. Meanwhile, for a specific downstream user perception application,\nBayesian Optimization is employed to determine the optimal weights for\npre-training tasks involving different semantic levels. We implement Saga on\nfive typical mobile phones and evaluate Saga on three typical tasks on three\nIMU datasets. Results show that when only using about 100 training samples per\nclass, Saga can achieve over 90% accuracy of the full-fledged model trained on\nover ten thousands training samples with no additional system overhead.",
      "tldr_zh": "该研究提出了一种名为 Saga 的细粒度用户感知方法，旨在从海量未标记的 Inertial Measurement Units (IMU) 数据中捕获多粒度语义信息，从而减少对标记数据的依赖。Saga 的核心在于先预训练一个主干特征提取模型，利用未标记数据中的丰富语义层次进行学习，然后通过 Bayesian Optimization 确定不同语义级别预训练任务的最佳权重。实验结果显示，在五个典型手机平台和三个 IMU 数据集上，Saga 仅使用每类约 100 个训练样本即可实现超过完整模型（基于数万个样本训练）的 90% 准确率，且不增加系统开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2025 IEEE 45th International Conference on Distributed Computing\n  Systems (ICDCS)",
      "pdf_url": "http://arxiv.org/pdf/2504.11726v1",
      "published_date": "2025-04-16 03:03:42 UTC",
      "updated_date": "2025-04-16 03:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:50:50.870153"
    },
    {
      "arxiv_id": "2504.13944v1",
      "title": "Mixer Metaphors: audio interfaces for non-musical applications",
      "title_zh": "翻译失败",
      "authors": [
        "Tace McNamara",
        "Jon McCormack",
        "Maria Teresa Llano"
      ],
      "abstract": "The NIME conference traditionally focuses on interfaces for music and musical\nexpression. In this paper we reverse this tradition to ask, can interfaces\ndeveloped for music be successfully appropriated to non-musical applications?\nTo help answer this question we designed and developed a new device, which uses\ninterface metaphors borrowed from analogue synthesisers and audio mixing to\nphysically control the intangible aspects of a Large Language Model. We\ncompared two versions of the device, with and without the audio-inspired\naugmentations, with a group of artists who used each version over a one week\nperiod. Our results show that the use of audio-like controls afforded more\nimmediate, direct and embodied control over the LLM, allowing users to\ncreatively experiment and play with the device over its non-mixer counterpart.\nOur project demonstrates how cross-sensory metaphors can support creative\nthinking and embodied practice when designing new technological interfaces.",
      "tldr_zh": "这篇论文探讨了音乐接口在非音乐应用中的潜力，特别针对 NIME 会议传统焦点的问题，考察音乐隐喻是否能成功应用于控制 Large Language Model (LLM)。研究者设计了一个新设备，使用模拟合成器和音频混合的界面隐喻来物理操控 LLM 的无形方面，并比较了带有和不带音频-inspired 增强的两个版本。实验中，艺术家在使用每种版本一周后反馈显示，音频-like 控件提供了更即时、直接和具身化的交互，提升了用户对 LLM 的创意实验和玩耍能力。该研究证明了跨感官隐喻在支持创意思考和具身实践方面的价值，为技术接口设计提供了新思路。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SD",
        "H.5.2; J.5; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "9 Pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13944v1",
      "published_date": "2025-04-16 02:51:08 UTC",
      "updated_date": "2025-04-16 02:51:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:51:04.269770"
    },
    {
      "arxiv_id": "2504.12355v1",
      "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Ahmad",
        "Muhammad Waqas",
        "ldar Batyrshin",
        "Grigori Sidorov"
      ],
      "abstract": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.",
      "tldr_zh": "该研究利用大型语言模型(LLMs)提出了一种AI驱动的NLP框架，用于检测社交媒体上的药物使用和过量症状，以应对药物过量这一全球健康问题。框架采用混合标注策略（结合LLMs和人类标注者）训练传统ML模型、神经网络和transformer-based模型，在多类分类中达到98%的准确率，在多标签分类中达到97%，比基线模型高出8%。这些结果突显了AI在支持公共健康监控和个性化干预策略方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12355v1",
      "published_date": "2025-04-16 02:33:19 UTC",
      "updated_date": "2025-04-16 02:33:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:51:15.283021"
    },
    {
      "arxiv_id": "2504.11713v2",
      "title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron Havens",
        "Benjamin Kurt Miller",
        "Bing Yan",
        "Carles Domingo-Enrich",
        "Anuroop Sriram",
        "Brandon Wood",
        "Daniel Levine",
        "Bin Hu",
        "Brandon Amos",
        "Brian Karrer",
        "Xiang Fu",
        "Guan-Horng Liu",
        "Ricky T. Q. Chen"
      ],
      "abstract": "We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.",
      "tldr_zh": "本研究引入了Adjoint Sampling，一种基于Adjoint Matching的高效可扩展算法，用于学习diffusion processes从未归一化密度或能量函数中采样。该方法是首个on-policy方法，能够进行远超能量评估和模型样本的梯度更新，并基于stochastic optimal control理论提供可靠保证，无需校正措施。实验证明，Adjoint Sampling在经典能量函数和神经网络-based能量模型上表现出色，可整合对称性和周期边界条件用于分子建模，并计划开源基准以推动计算化学领域的进展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11713v2",
      "published_date": "2025-04-16 02:20:06 UTC",
      "updated_date": "2025-04-18 15:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:51:27.370945"
    },
    {
      "arxiv_id": "2504.11711v2",
      "title": "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Haonan Li",
        "Hang Zhang",
        "Kexin Pei",
        "Zhiyun Qian"
      ],
      "abstract": "Static analysis is a cornerstone for software vulnerability detection, yet it\noften struggles with the classic precision-scalability trade-off. In practice,\nsuch tools often produce high false positive rates, particularly in large\ncodebases like the Linux kernel. This imprecision can arise from simplified\nvulnerability modeling and over-approximation of path and data constraints.\nWhile large language models (LLMs) show promise in code understanding, their\nnaive application to program analysis yields unreliable results due to inherent\nreasoning limitations. We introduce BugLens, a post-refinement framework that\nsignificantly improves static analysis precision. BugLens guides an LLM to\nfollow traditional analysis steps by assessing buggy code patterns for security\nimpact and validating the constraints associated with static warnings.\nEvaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10\n(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing\nfalse positives and revealing four previously unreported vulnerabilities. Our\nresults suggest that a structured LLM-based workflow can meaningfully enhance\nthe effectiveness of static analysis tools.",
      "tldr_zh": "静态分析工具在软件漏洞检测中常面临精度与可扩展性的权衡，导致高假阳性率，尤其在大型代码库如Linux内核。论文引入BugLens框架，利用LLMs指导传统分析步骤，包括评估buggy代码模式的security影响并验证静态警告的约束，从而显著提高分析精度。在真实Linux内核bug测试中，BugLens将精度从0.10（原始）和0.50（半自动化精炼）提升至0.72，减少假阳性并发现四个未报告漏洞。这些结果表明，结构化的LLM-based工作流能有效增强静态分析工具的可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11711v2",
      "published_date": "2025-04-16 02:17:06 UTC",
      "updated_date": "2025-04-17 02:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:51:39.858691"
    },
    {
      "arxiv_id": "2504.11707v1",
      "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Shahid Muneer",
        "Simon S. Woo"
      ],
      "abstract": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense.",
      "tldr_zh": "本文针对 Text-to-Image (T2I) 模型在网络上生成 Not-Safe-For-Work (NSFW) 内容的问题，提出一个鲁棒的多模态防御机制，以应对 adversarial attacks。研究者构建了一个百万规模的数据集，包括提示、图像和对抗示例，使用开源扩散模型生成，用于训练和评估。该防御机制能有效区分安全与 NSFW 文本和图像，并在多模态场景中显著降低 Attack Success Rate (ASR)。实验结果显示，该方法在准确率和召回率上优于现有 SOTA NSFW 检测方法，为更安全的合成图像生成提供解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Short Paper The Web Conference",
      "pdf_url": "http://arxiv.org/pdf/2504.11707v1",
      "published_date": "2025-04-16 02:10:42 UTC",
      "updated_date": "2025-04-16 02:10:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:51:52.448248"
    },
    {
      "arxiv_id": "2504.11704v1",
      "title": "A Library of LLM Intrinsics for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Marina Danilevsky",
        "Kristjan Greenewald",
        "Chulaka Gunasekara",
        "Maeda Hanafi",
        "Lihong He",
        "Yannis Katsis",
        "Krishnateja Killamsetty",
        "Yatin Nandwani",
        "Lucian Popa",
        "Dinesh Raghu",
        "Frederick Reiss",
        "Vraj Shah",
        "Khoi-Nguyen Tran",
        "Huaiyu Zhu",
        "Luis Lastras"
      ],
      "abstract": "In the developer community for large language models (LLMs), there is not yet\na clean pattern analogous to a software library, to support very large scale\ncollaboration. Even for the commonplace use case of Retrieval-Augmented\nGeneration (RAG), it is not currently possible to write a RAG application\nagainst a well-defined set of APIs that are agreed upon by different LLM\nproviders. Inspired by the idea of compiler intrinsics, we propose some\nelements of such a concept through introducing a library of LLM Intrinsics for\nRAG. An LLM intrinsic is defined as a capability that can be invoked through a\nwell-defined API that is reasonably stable and independent of how the LLM\nintrinsic itself is implemented. The intrinsics in our library are released as\nLoRA adapters on HuggingFace, and through a software interface with clear\nstructured input/output characteristics on top of vLLM as an inference\nplatform, accompanied in both places with documentation and code. This article\ndescribes the intended usage, training details, and evaluations for each\nintrinsic, as well as compositions of multiple intrinsics.",
      "tldr_zh": "这篇论文针对大型语言模型(LLM)开发社区的协作问题，提出一个LLM Intrinsics库，以标准化API支持Retrieval-Augmented Generation (RAG)应用。该库将LLM Intrinsics定义为通过稳定、独立于实现的API调用的能力，并以LoRA adapters的形式发布在HuggingFace上，同时提供vLLM接口的文档和代码。论文详细描述了每个intrinsics的预期使用、训练细节、评估结果，以及多个intrinsics的组合应用，从而促进大规模LLM开发协作。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11704v1",
      "published_date": "2025-04-16 02:02:22 UTC",
      "updated_date": "2025-04-16 02:02:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:52:03.729769"
    },
    {
      "arxiv_id": "2504.11703v1",
      "title": "Progent: Programmable Privilege Control for LLM Agents",
      "title_zh": "Progent: 针对 LLM 代理的可编程权限控制",
      "authors": [
        "Tianneng Shi",
        "Jingxuan He",
        "Zhun Wang",
        "Linyu Wu",
        "Hongwei Li",
        "Wenbo Guo",
        "Dawn Song"
      ],
      "abstract": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
      "tldr_zh": "该论文探讨了LLM agents在执行用户任务时面临的安全风险，如执行恶意命令，并提出Progent作为首个可编程特权控制机制来强制最小权限原则。Progent的核心是使用一个domain-specific language来定义细粒度策略，对tool calls进行约束，包括何时允许调用和备用方案，从而确保安全的同时保持实用性。其设计模块化，便于集成，且通过LLMs自动生成和动态更新策略，进一步提升了灵活性。实验在AgentDojo、ASB和AgentPoison等基准上证明，Progent实现了强安全性、高实用性，并对自适应攻击具有良好弹性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11703v1",
      "published_date": "2025-04-16 01:58:40 UTC",
      "updated_date": "2025-04-16 01:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:52:15.052270"
    },
    {
      "arxiv_id": "2504.11686v1",
      "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics",
      "title_zh": "翻译失败",
      "authors": [
        "Yiran He",
        "Yun Cao",
        "Bowen Yang",
        "Zeyu Zhang"
      ],
      "abstract": "The rapid development of generative AI facilitates content creation and makes\nimage manipulation easier and more difficult to detect. While multimodal Large\nLanguage Models (LLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating AI-generated Content (AIGC) and struggle to\ncomprehend local forgery details. In this work, we investigate the application\nof multimodal LLMs in forgery detection. We propose a framework capable of\nevaluating image authenticity, localizing tampered regions, providing evidence,\nand tracing generation methods based on semantic tampering clues. Our method\ndemonstrates that the potential of LLMs in forgery analysis can be effectively\nunlocked through meticulous prompt engineering and the application of few-shot\nlearning techniques. We conduct qualitative and quantitative experiments and\nshow that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in\nLaMa, which is competitive with state-of-the-art AIGC detection methods. We\nfurther discuss the limitations of multimodal LLMs in such tasks and propose\npotential improvements.",
      "tldr_zh": "本论文探讨了如何利用多模态大型语言模型(Multimodal LLMs)进行图像伪造检测，解决这些模型在处理AI生成内容(AIGC)时的局限性，如无法理解局部伪造细节。研究提出一个框架，通过提示工程和少样本学习(few-shot learning)技术，实现图像真实性评估、篡改区域定位、证据提供以及生成方法追踪。实验结果显示，GPT4V在Autosplice数据集上达到92.1%准确率，在LaMa数据集上达到86.3%，与最先进(AIGC detection)方法竞争。论文还讨论了Multimodal LLMs的局限性，并提出潜在改进方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 11 figures, 13IHMMSec2025",
      "pdf_url": "http://arxiv.org/pdf/2504.11686v1",
      "published_date": "2025-04-16 01:02:46 UTC",
      "updated_date": "2025-04-16 01:02:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:52:28.049785"
    },
    {
      "arxiv_id": "2504.13942v1",
      "title": "Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Sukanth Kalivarathan",
        "Muhmmad Abrar Raja Mohamed",
        "Aswathy Ravikumar",
        "S Harini"
      ],
      "abstract": "This paper introduces Intelligence of Things (INOT), a novel spatial\ncontext-aware control system that enhances smart home automation through\nintuitive spatial reasoning. Current smart home systems largely rely on\ndevice-specific identifiers, limiting user interaction to explicit naming\nconventions rather than natural spatial references. INOT addresses this\nlimitation through a modular architecture that integrates Vision Language\nModels with IoT control systems to enable natural language commands with\nspatial context (e.g., \"turn on the light near the window\"). The system\ncomprises key components including an Onboarding Inference Engine, Zero-Shot\nDevice Detection, Spatial Topology Inference, and Intent-Based Command\nSynthesis. A comprehensive user study with 15 participants demonstrated INOT's\nsignificant advantages over conventional systems like Google Home Assistant,\nwith users reporting reduced cognitive workload (NASA-TLX scores decreased by\nan average of 13.17 points), higher ease-of-use ratings, and stronger\npreference (14 out of 15 participants). By eliminating the need to memorize\ndevice identifiers and enabling context-aware spatial commands, INOT represents\na significant advancement in creating more intuitive and accessible smart home\ncontrol systems.",
      "tldr_zh": "该论文提出 Intelligence of Things (INOT)，一个基于空间上下文的智能设备控制系统，旨在提升智能家居自动化，通过整合 Vision Language Models 和 IoT 系统，支持自然语言命令（如“turn on the light near the window”），从而避免依赖设备特定标识。INOT 的关键组件包括 Onboarding Inference Engine、Zero-Shot Device Detection、Spatial Topology Inference 和 Intent-Based Command Synthesis，实现直观的空间推理和命令合成。在用户研究中，15 名参与者报告 INOT 比 Google Home Assistant 降低了认知工作量（NASA-TLX 得分平均减少 13.17 分）、提高了易用性，并有 14 人更偏好该系统，为更直观、可访问的智能家居控制提供了重要进展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 8 Figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13942v1",
      "published_date": "2025-04-16 00:45:05 UTC",
      "updated_date": "2025-04-16 00:45:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:52:41.640825"
    },
    {
      "arxiv_id": "2504.11671v1",
      "title": "Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Ji Ma"
      ],
      "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract ``vectors of variable\nvariations'' (e.g., ``male'' to ``female'') from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications.",
      "tldr_zh": "本研究探讨大型语言模型(LLMs)在社会模拟中的决策机制，专注于如何通过操纵内部表示来引导其亲社会行为。在Dictator Game实验中，研究者提出方法来探测、量化并修改LLMs的“vectors of variable variations”（如“male”到“female”），从而显著改变这些变量与模型决策的相关性。这种方法为LLMs的alignment、debiasing以及在学术和商业应用中设计AI代理提供了原则性途径。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.11671v1",
      "published_date": "2025-04-16 00:02:28 UTC",
      "updated_date": "2025-04-16 00:02:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:52:51.638929"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 113,
  "processed_papers_count": 113,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T13:53:11.856545"
}
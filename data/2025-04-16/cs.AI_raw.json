[
  {
    "arxiv_id": "2504.12535v1",
    "title": "Decision-based AI Visual Navigation for Cardiac Ultrasounds",
    "authors": [
      "Andy Dimnaku",
      "Dominic Yurk",
      "Zhiyuan Gao",
      "Arun Padmanabhan",
      "Mandar Aras",
      "Yaser Abu-Mostafa"
    ],
    "abstract": "Ultrasound imaging of the heart (echocardiography) is widely used to diagnose\ncardiac diseases. However, obtaining an echocardiogram requires an expert\nsonographer and a high-quality ultrasound imaging device, which are generally\nonly available in hospitals. Recently, AI-based navigation models and\nalgorithms have been used to aid novice sonographers in acquiring the\nstandardized cardiac views necessary to visualize potential disease\npathologies. These navigation systems typically rely on directional guidance to\npredict the necessary rotation of the ultrasound probe. This paper demonstrates\na novel AI navigation system that builds on a decision model for identifying\nthe inferior vena cava (IVC) of the heart. The decision model is trained\noffline using cardiac ultrasound videos and employs binary classification to\ndetermine whether the IVC is present in a given ultrasound video. The\nunderlying model integrates a novel localization algorithm that leverages the\nlearned feature representations to annotate the spatial location of the IVC in\nreal-time. Our model demonstrates strong localization performance on\ntraditional high-quality hospital ultrasound videos, as well as impressive\nzero-shot performance on lower-quality ultrasound videos from a more affordable\nButterfly iQ handheld ultrasound machine. This capability facilitates the\nexpansion of ultrasound diagnostics beyond hospital settings. Currently, the\nguidance system is undergoing clinical trials and is available on the Butterfly\niQ app.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12535v1",
    "published_date": "2025-04-16 23:54:46 UTC",
    "updated_date": "2025-04-16 23:54:46 UTC"
  },
  {
    "arxiv_id": "2504.12532v1",
    "title": "Generalization through variance: how noise shapes inductive biases in diffusion models",
    "authors": [
      "John J. Vastola"
    ],
    "abstract": "How diffusion models generalize beyond their training set is not known, and\nis somewhat mysterious given two facts: the optimum of the denoising score\nmatching (DSM) objective usually used to train diffusion models is the score\nfunction of the training distribution; and the networks usually used to learn\nthe score function are expressive enough to learn this score to high accuracy.\nWe claim that a certain feature of the DSM objective -- the fact that its\ntarget is not the training distribution's score, but a noisy quantity only\nequal to it in expectation -- strongly impacts whether and to what extent\ndiffusion models generalize. In this paper, we develop a mathematical theory\nthat partly explains this 'generalization through variance' phenomenon. Our\ntheoretical analysis exploits a physics-inspired path integral approach to\ncompute the distributions typically learned by a few paradigmatic under- and\noverparameterized diffusion models. We find that the distributions diffusion\nmodels effectively learn to sample from resemble their training distributions,\nbut with 'gaps' filled in, and that this inductive bias is due to the\ncovariance structure of the noisy target used during training. We also\ncharacterize how this inductive bias interacts with feature-related inductive\nbiases.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12532v1",
    "published_date": "2025-04-16 23:41:10 UTC",
    "updated_date": "2025-04-16 23:41:10 UTC"
  },
  {
    "arxiv_id": "2504.12529v1",
    "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis",
    "authors": [
      "Zahra Atf",
      "Peter R. Lewis"
    ],
    "abstract": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "9 Page, 1 Figure",
    "pdf_url": "http://arxiv.org/pdf/2504.12529v1",
    "published_date": "2025-04-16 23:30:55 UTC",
    "updated_date": "2025-04-16 23:30:55 UTC"
  },
  {
    "arxiv_id": "2504.12526v1",
    "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models",
    "authors": [
      "Junyang Zhang",
      "Tianyi Zhu",
      "Cheng Luo",
      "Anima Anandkumar"
    ],
    "abstract": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to COLM",
    "pdf_url": "http://arxiv.org/pdf/2504.12526v1",
    "published_date": "2025-04-16 23:15:09 UTC",
    "updated_date": "2025-04-16 23:15:09 UTC"
  },
  {
    "arxiv_id": "2504.13959v1",
    "title": "AI Safety Should Prioritize the Future of Work",
    "authors": [
      "Sanchaita Hazra",
      "Bodhisattwa Prasad Majumder",
      "Tuhin Chakrabarty"
    ],
    "abstract": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13959v1",
    "published_date": "2025-04-16 23:12:30 UTC",
    "updated_date": "2025-04-16 23:12:30 UTC"
  },
  {
    "arxiv_id": "2504.12523v1",
    "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge",
    "authors": [
      "Aochong Oliver Li",
      "Tanya Goyal"
    ],
    "abstract": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12523v1",
    "published_date": "2025-04-16 23:03:40 UTC",
    "updated_date": "2025-04-16 23:03:40 UTC"
  },
  {
    "arxiv_id": "2504.12522v1",
    "title": "Evaluating the Diversity and Quality of LLM Generated Content",
    "authors": [
      "Alexander Shypula",
      "Shuo Li",
      "Botong Zhang",
      "Vishakh Padmakumar",
      "Kayo Yin",
      "Osbert Bastani"
    ],
    "abstract": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 Third Workshop on Deep Learning for Code",
    "pdf_url": "http://arxiv.org/pdf/2504.12522v1",
    "published_date": "2025-04-16 23:02:23 UTC",
    "updated_date": "2025-04-16 23:02:23 UTC"
  },
  {
    "arxiv_id": "2505.00008v1",
    "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination",
    "authors": [
      "Zhaoyi Sun",
      "Wen-Wai Yim",
      "Ozlem Uzuner",
      "Fei Xia",
      "Meliha Yetisgen"
    ],
    "abstract": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00008v1",
    "published_date": "2025-04-16 22:27:10 UTC",
    "updated_date": "2025-04-16 22:27:10 UTC"
  },
  {
    "arxiv_id": "2504.12513v1",
    "title": "AdaVid: Adaptive Video-Language Pretraining",
    "authors": [
      "Chaitanya Patel",
      "Juan Carlos Niebles",
      "Ehsan Adeli"
    ],
    "abstract": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPRW 2025. Project Page: https://chaitanya100100.github.io/AdaVid/",
    "pdf_url": "http://arxiv.org/pdf/2504.12513v1",
    "published_date": "2025-04-16 22:19:50 UTC",
    "updated_date": "2025-04-16 22:19:50 UTC"
  },
  {
    "arxiv_id": "2504.12511v1",
    "title": "Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis",
    "authors": [
      "Shravan Chaudhari",
      "Trilokya Akula",
      "Yoon Kim",
      "Tom Blake"
    ],
    "abstract": "In this paper, we advance the study of AI-augmented reasoning in the context\nof Human-Computer Interaction (HCI), psychology and cognitive science, focusing\non the critical task of visual perception. Specifically, we investigate the\napplicability of Multimodal Large Language Models (MLLMs) in this domain. To\nthis end, we leverage established principles and explanations from psychology\nand cognitive science related to complexity in human visual perception. We use\nthem as guiding principles for the MLLMs to compare and interprete visual\ncontent. Our study aims to benchmark MLLMs across various explainability\nprinciples relevant to visual perception. Unlike recent approaches that\nprimarily employ advanced deep learning models to predict complexity metrics\nfrom visual content, our work does not seek to develop a mere new predictive\nmodel. Instead, we propose a novel annotation-free analytical framework to\nassess utility of MLLMs as cognitive assistants for HCI tasks, using visual\nperception as a case study. The primary goal is to pave the way for principled\nstudy in quantifying and evaluating the interpretability of MLLMs for\napplications in improving human reasoning capability and uncovering biases in\nexisting perception datasets annotated by humans.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12511v1",
    "published_date": "2025-04-16 22:14:27 UTC",
    "updated_date": "2025-04-16 22:14:27 UTC"
  },
  {
    "arxiv_id": "2504.13958v1",
    "title": "ToolRL: Reward is All Tool Learning Needs",
    "authors": [
      "Cheng Qian",
      "Emre Can Acikgoz",
      "Qi He",
      "Hongru Wang",
      "Xiusi Chen",
      "Dilek Hakkani-Tür",
      "Gokhan Tur",
      "Heng Ji"
    ],
    "abstract": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "19 Pages, 12 Figures, 12 Tables",
    "pdf_url": "http://arxiv.org/pdf/2504.13958v1",
    "published_date": "2025-04-16 21:45:32 UTC",
    "updated_date": "2025-04-16 21:45:32 UTC"
  },
  {
    "arxiv_id": "2504.12503v1",
    "title": "Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study",
    "authors": [
      "Kaira M. Samuel",
      "Faez Ahmed"
    ],
    "abstract": "Engineering problems that apply machine learning often involve\ncomputationally intensive methods but rely on limited datasets. As engineering\ndata evolves with new designs and constraints, models must incorporate new\nknowledge over time. However, high computational costs make retraining models\nfrom scratch infeasible. Continual learning (CL) offers a promising solution by\nenabling models to learn from sequential data while mitigating catastrophic\nforgetting, where a model forgets previously learned mappings. This work\nintroduces CL to engineering design by benchmarking several CL methods on\nrepresentative regression tasks. We apply these strategies to five engineering\ndatasets and construct nine new engineering CL benchmarks to evaluate their\nability to address forgetting and improve generalization. Preliminary results\nshow that applying existing CL methods to these tasks improves performance over\nnaive baselines. In particular, the Replay strategy achieved performance\ncomparable to retraining in several benchmarks while reducing training time by\nnearly half, demonstrating its potential for real-world engineering workflows.\nThe code and datasets used in this work will be available at:\nhttps://github.com/kmsamuel/cl-for-engineering-release.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12503v1",
    "published_date": "2025-04-16 21:40:03 UTC",
    "updated_date": "2025-04-16 21:40:03 UTC"
  },
  {
    "arxiv_id": "2504.12497v1",
    "title": "Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope",
    "authors": [
      "Robert E. Wray",
      "Steven J. Jones",
      "John E. Laird"
    ],
    "abstract": "Regardless of past learning, an agent in an open world will face unfamiliar\nsituations and events outside of prior experience, existing models, or\npolicies. Further, the agent will sometimes lack relevant knowledge and/or\nsufficient time to assess the situation, generate and evaluate options, and\npursue a robustly considered course of action. How can an agent respond\nreasonably to situations that are outside of its original design scope? How can\nit recognize such situations sufficiently quickly and reliably to determine\nreasonable, adaptive courses of action? We identify key characteristics needed\nfor solutions, evaluate the state-of-the-art by these requirements, and outline\na proposed, novel approach that combines domain-general meta-knowledge (in the\nform of appraisals inspired by human cognition) and metareasoning. It has the\npotential to provide fast, adaptive responses to unfamiliar situations, more\nfully meeting the performance characteristics required for open-world, general\nagents.",
    "categories": [
      "cs.AI",
      "I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 3 figures. Submitted to AGI25 conference",
    "pdf_url": "http://arxiv.org/pdf/2504.12497v1",
    "published_date": "2025-04-16 21:26:12 UTC",
    "updated_date": "2025-04-16 21:26:12 UTC"
  },
  {
    "arxiv_id": "2504.12488v1",
    "title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process",
    "authors": [
      "Mohi Reza",
      "Jeb Thomas-Mitchell",
      "Peter Dushniku",
      "Nathan Laundry",
      "Joseph Jay Williams",
      "Anastasia Kuzminykh"
    ],
    "abstract": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; I.2.7; I.2.6; I.7.2"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12488v1",
    "published_date": "2025-04-16 21:05:46 UTC",
    "updated_date": "2025-04-16 21:05:46 UTC"
  },
  {
    "arxiv_id": "2504.13957v1",
    "title": "Naming is framing: How cybersecurity's language problems are repeating in AI governance",
    "authors": [
      "Lianne Potter"
    ],
    "abstract": "Language is not neutral; it frames understanding, structures power, and\nshapes governance. This paper argues that misnomers like cybersecurity and\nartificial intelligence (AI) are more than semantic quirks; they carry\nsignificant governance risks by obscuring human agency, inflating expectations,\nand distorting accountability. Drawing on lessons from cybersecurity's\nlinguistic pitfalls, such as the 'weakest link' narrative, this paper\nhighlights how AI discourse is falling into similar traps with metaphors like\n'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,\nmystifying, or overly technical assumptions into governance structures. In\nresponse, the paper advocates for a language-first approach to AI governance:\none that interrogates dominant metaphors, foregrounds human roles, and\nco-develops a lexicon that is precise, inclusive, and reflexive. This paper\ncontends that linguistic reform is not peripheral to governance but central to\nthe construction of transparent, equitable, and anticipatory regulatory\nframeworks.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.13957v1",
    "published_date": "2025-04-16 20:58:26 UTC",
    "updated_date": "2025-04-16 20:58:26 UTC"
  },
  {
    "arxiv_id": "2504.12482v1",
    "title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it",
    "authors": [
      "Luciano Floridi",
      "Carlotta Buttaboni",
      "Emmie Hine",
      "Jessica Morley",
      "Claudio Novelli",
      "Tyler Schroder"
    ],
    "abstract": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12482v1",
    "published_date": "2025-04-16 20:38:09 UTC",
    "updated_date": "2025-04-16 20:38:09 UTC"
  },
  {
    "arxiv_id": "2504.12477v1",
    "title": "Towards Conversational AI for Human-Machine Collaborative MLOps",
    "authors": [
      "George Fatouros",
      "Georgios Makridis",
      "George Kousiouris",
      "John Soldatos",
      "Anargyros Tsadimas",
      "Dimosthenis Kyriazis"
    ],
    "abstract": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "68T50, 68T99, 68U35, 68N19",
      "I.2.1; H.5.2; D.2.11; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12477v1",
    "published_date": "2025-04-16 20:28:50 UTC",
    "updated_date": "2025-04-16 20:28:50 UTC"
  },
  {
    "arxiv_id": "2504.12476v1",
    "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States",
    "authors": [
      "Andreas Jungherr",
      "Adrian Rauchfleisch"
    ],
    "abstract": "Recent advances in generative Artificial Intelligence have raised public\nawareness, shaping expectations and concerns about their societal implications.\nCentral to these debates is the question of AI alignment -- how well AI systems\nmeet public expectations regarding safety, fairness, and social values.\nHowever, little is known about what people expect from AI-enabled systems and\nhow these expectations differ across national contexts. We present evidence\nfrom two surveys of public preferences for key functional features of\nAI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We\nexamine support for four types of alignment in AI moderation: accuracy and\nreliability, safety, bias mitigation, and the promotion of aspirational\nimaginaries. U.S. respondents report significantly higher AI use and\nconsistently greater support for all alignment features, reflecting broader\ntechnological openness and higher societal involvement with AI. In both\ncountries, accuracy and safety enjoy the strongest support, while more\nnormatively charged goals -- like fairness and aspirational imaginaries --\nreceive more cautious backing, particularly in Germany. We also explore how\nindividual experience with AI, attitudes toward free speech, political\nideology, partisan affiliation, and gender shape these preferences. AI use and\nfree speech support explain more variation in Germany. In contrast, U.S.\nresponses show greater attitudinal uniformity, suggesting that higher exposure\nto AI may consolidate public expectations. These findings contribute to debates\non AI governance and cross-national variation in public preferences. More\nbroadly, our study demonstrates the value of empirically grounding AI alignment\ndebates in public attitudes and of explicitly developing normatively grounded\nexpectations into theoretical and policy discussions on the governance of\nAI-generated content.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12476v1",
    "published_date": "2025-04-16 20:27:03 UTC",
    "updated_date": "2025-04-16 20:27:03 UTC"
  },
  {
    "arxiv_id": "2504.12474v2",
    "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
    "authors": [
      "Azadeh Beiranvand",
      "Seyed Mehdi Vahidipour"
    ],
    "abstract": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12474v2",
    "published_date": "2025-04-16 20:25:11 UTC",
    "updated_date": "2025-04-19 12:14:06 UTC"
  },
  {
    "arxiv_id": "2504.12463v2",
    "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts",
    "authors": [
      "Ashwinee Panda",
      "Vatsal Baherwani",
      "Zain Sarwar",
      "Benjamin Therien",
      "Supriyo Chakraborty",
      "Tom Goldstein"
    ],
    "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12463v2",
    "published_date": "2025-04-16 19:55:36 UTC",
    "updated_date": "2025-04-18 02:58:46 UTC"
  },
  {
    "arxiv_id": "2504.12459v1",
    "title": "On Linear Representations and Pretraining Data Frequency in Language Models",
    "authors": [
      "Jack Merullo",
      "Noah A. Smith",
      "Sarah Wiegreffe",
      "Yanai Elazar"
    ],
    "abstract": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12459v1",
    "published_date": "2025-04-16 19:50:03 UTC",
    "updated_date": "2025-04-16 19:50:03 UTC"
  },
  {
    "arxiv_id": "2504.12446v2",
    "title": "Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks",
    "authors": [
      "Sebastian Seidel",
      "Uwe M. Borghoff"
    ],
    "abstract": "Artificial intelligence (AI) has emerged as a transformative force across\nindustries, driven by advances in deep learning and natural language\nprocessing, and fueled by large-scale data and computing resources. Despite its\nrapid adoption, the opacity of AI systems poses significant challenges to trust\nand acceptance.\n  This work explores the intersection of connectionist and symbolic approaches\nto artificial intelligence, focusing on the derivation of interpretable\nsymbolic models, such as decision trees, from feedforward neural networks\n(FNNs). Decision trees provide a transparent framework for elucidating the\noperations of neural networks while preserving their functionality. The\nderivation is presented in a step-by-step approach and illustrated with several\nexamples. A systematic methodology is proposed to bridge neural and symbolic\nparadigms by exploiting distributed representations in FNNs to identify\nsymbolic components, including fillers, roles, and their interrelationships.\nThe process traces neuron activation values and input configurations across\nnetwork layers, mapping activations and their underlying inputs to decision\ntree edges. The resulting symbolic structures effectively capture FNN decision\nprocesses and enable scalability to deeper networks through iterative\nrefinement of subpaths for each hidden layer.\n  To validate the theoretical framework, a prototype was developed using Keras\n.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This\nprototype demonstrates the feasibility of extracting symbolic representations\nfrom neural networks, enhancing trust in AI systems, and promoting\naccountability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12446v2",
    "published_date": "2025-04-16 19:22:53 UTC",
    "updated_date": "2025-04-24 21:25:42 UTC"
  },
  {
    "arxiv_id": "2504.12436v1",
    "title": "Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation",
    "authors": [
      "Nairouz Mrabah",
      "Nicolas Richet",
      "Ismail Ben Ayed",
      "Éric Granger"
    ],
    "abstract": "Adapting Vision-Language Models (VLMs) to new domains with few labeled\nsamples remains a significant challenge due to severe overfitting and\ncomputational constraints. State-of-the-art solutions, such as low-rank\nreparameterization, mitigate these issues but often struggle with\ngeneralization and require extensive hyperparameter tuning. In this paper, a\nnovel Sparse Optimization (SO) framework is proposed. Unlike low-rank\napproaches that typically constrain updates to a fixed subspace, our SO method\nleverages high sparsity to dynamically adjust very few parameters. We introduce\ntwo key paradigms. First, we advocate for \\textit{local sparsity and global\ndensity}, which updates a minimal subset of parameters per iteration while\nmaintaining overall model expressiveness. As a second paradigm, we advocate for\n\\textit{local randomness and global importance}, which sparsifies the gradient\nusing random selection while pruning the first moment based on importance. This\ncombination significantly mitigates overfitting and ensures stable adaptation\nin low-data regimes. Extensive experiments on 11 diverse datasets show that SO\nachieves state-of-the-art few-shot adaptation performance while reducing memory\noverhead.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.8; I.5.1; G.1.6"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.12436v1",
    "published_date": "2025-04-16 19:10:34 UTC",
    "updated_date": "2025-04-16 19:10:34 UTC"
  },
  {
    "arxiv_id": "2504.12427v1",
    "title": "Position: The Most Expensive Part of an LLM should be its Training Data",
    "authors": [
      "Nikhil Kandpal",
      "Colin Raffel"
    ],
    "abstract": "Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12427v1",
    "published_date": "2025-04-16 18:56:14 UTC",
    "updated_date": "2025-04-16 18:56:14 UTC"
  },
  {
    "arxiv_id": "2504.12424v1",
    "title": "Don't Just Translate, Agitate: Using Large Language Models as Devil's Advocates for AI Explanations",
    "authors": [
      "Ashley Suh",
      "Kenneth Alperin",
      "Harry Li",
      "Steven R Gomez"
    ],
    "abstract": "This position paper highlights a growing trend in Explainable AI (XAI)\nresearch where Large Language Models (LLMs) are used to translate outputs from\nexplainability techniques, like feature-attribution weights, into a natural\nlanguage explanation. While this approach may improve accessibility or\nreadability for users, recent findings suggest that translating into human-like\nexplanations does not necessarily enhance user understanding and may instead\nlead to overreliance on AI systems. When LLMs summarize XAI outputs without\nsurfacing model limitations, uncertainties, or inconsistencies, they risk\nreinforcing the illusion of interpretability rather than fostering meaningful\ntransparency. We argue that - instead of merely translating XAI outputs - LLMs\nshould serve as constructive agitators, or devil's advocates, whose role is to\nactively interrogate AI explanations by presenting alternative interpretations,\npotential biases, training data limitations, and cases where the model's\nreasoning may break down. In this role, LLMs can facilitate users in engaging\ncritically with AI systems and generated explanations, with the potential to\nreduce overreliance caused by misinterpreted or specious explanations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12424v1",
    "published_date": "2025-04-16 18:45:18 UTC",
    "updated_date": "2025-04-16 18:45:18 UTC"
  },
  {
    "arxiv_id": "2504.12422v1",
    "title": "Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study",
    "authors": [
      "Harry Li",
      "Gabriel Appleby",
      "Kenneth Alperin",
      "Steven R Gomez",
      "Ashley Suh"
    ],
    "abstract": "High-stakes domains like cyber operations need responsible and trustworthy AI\nmethods. While large language models (LLMs) are becoming increasingly popular\nin these domains, they still suffer from hallucinations. This research paper\nprovides learning outcomes from a case study with LinkQ, an open-source natural\nlanguage interface that was developed to combat hallucinations by forcing an\nLLM to query a knowledge graph (KG) for ground-truth data during\nquestion-answering (QA). We conduct a quantitative evaluation of LinkQ using a\nwell-known KGQA dataset, showing that the system outperforms GPT-4 but still\nstruggles with certain question categories - suggesting that alternative query\nconstruction strategies will need to be investigated in future LLM querying\nsystems. We discuss a qualitative study of LinkQ with two domain experts using\na real-world cybersecurity KG, outlining these experts' feedback, suggestions,\nperceived limitations, and future opportunities for systems like LinkQ.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12422v1",
    "published_date": "2025-04-16 18:40:01 UTC",
    "updated_date": "2025-04-16 18:40:01 UTC"
  },
  {
    "arxiv_id": "2504.12417v1",
    "title": "Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data",
    "authors": [
      "Dewang Kumar Agarwal",
      "Dimitris J. Bertsimas"
    ],
    "abstract": "Objective: Create precise, structured, data-backed guidelines for type 2\ndiabetes treatment progression, suitable for clinical adoption.\n  Research Design and Methods: Our training cohort was composed of patient\n(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to\n2014. We divide visits into 4 groups based on the patient's treatment regimen\nbefore the visit, and further divide them into subgroups based on the\nrecommended treatment during the visit. Since each subgroup has observational\ndata, which has confounding bias (sicker patients are prescribed more\naggressive treatments), we used machine learning and optimization to remove\nsome datapoints so that the remaining data resembles a randomized trial. On\neach subgroup, we train AI-backed tree-based models to prescribe treatment\nchanges. Once we train these tree models, we manually combine the models for\nevery group to create an end-to-end prescription pipeline for all patients in\nthat group. In this process, we prioritize stepping up to a more aggressive\ntreatment before considering less aggressive options. We tested this pipeline\non unseen data from BMC, and an external dataset from Hartford healthcare (type\n2 diabetes patient visits from January 2020 to May 2024).\n  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more\nthan what the doctors achieved on the unseen BMC patients. For the Hartford\ncohort, our pipelines were better by 0.13%.\n  Conclusions: This precise, interpretable, and efficient AI-backed approach to\ntreatment progression in type 2 diabetes is predicted to outperform the current\npractice and can be deployed to improve patient outcomes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12417v1",
    "published_date": "2025-04-16 18:29:45 UTC",
    "updated_date": "2025-04-16 18:29:45 UTC"
  },
  {
    "arxiv_id": "2504.12397v2",
    "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
    "authors": [
      "Kristjan Greenewald",
      "Luis Lastras",
      "Thomas Parnell",
      "Vraj Shah",
      "Lucian Popa",
      "Giulio Zizzo",
      "Chulaka Gunasekara",
      "Ambrish Rawat",
      "David Cox"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2504.11704",
    "pdf_url": "http://arxiv.org/pdf/2504.12397v2",
    "published_date": "2025-04-16 18:03:21 UTC",
    "updated_date": "2025-04-29 14:25:08 UTC"
  },
  {
    "arxiv_id": "2504.12299v1",
    "title": "Adapting a World Model for Trajectory Following in a 3D Game",
    "authors": [
      "Marko Tot",
      "Shu Ishida",
      "Abdelhak Lemkhenter",
      "David Bignell",
      "Pallavi Choudhury",
      "Chris Lovett",
      "Luis França",
      "Matheus Ribeiro Furtado de Mendonça",
      "Tarun Gupta",
      "Darren Gehring",
      "Sam Devlin",
      "Sergio Valcarcel Macua",
      "Raluca Georgescu"
    ],
    "abstract": "Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12299v1",
    "published_date": "2025-04-16 17:59:54 UTC",
    "updated_date": "2025-04-16 17:59:54 UTC"
  },
  {
    "arxiv_id": "2504.12292v1",
    "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians",
    "authors": [
      "Liam Schoneveld",
      "Zhe Chen",
      "Davide Davoli",
      "Jiapeng Tang",
      "Saimon Terazawa",
      "Ko Nishino",
      "Matthias Nießner"
    ],
    "abstract": "Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/",
    "pdf_url": "http://arxiv.org/pdf/2504.12292v1",
    "published_date": "2025-04-16 17:55:02 UTC",
    "updated_date": "2025-04-16 17:55:02 UTC"
  },
  {
    "arxiv_id": "2504.12284v1",
    "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
    "authors": [
      "Aditya Prakash",
      "Benjamin Lundell",
      "Dmitry Andreychuk",
      "David Forsyth",
      "Saurabh Gupta",
      "Harpreet Sawhney"
    ],
    "abstract": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact",
    "pdf_url": "http://arxiv.org/pdf/2504.12284v1",
    "published_date": "2025-04-16 17:48:12 UTC",
    "updated_date": "2025-04-16 17:48:12 UTC"
  },
  {
    "arxiv_id": "2504.12268v1",
    "title": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks",
    "authors": [
      "Stefan Abi-Karam",
      "Cong Hao"
    ],
    "abstract": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12268v1",
    "published_date": "2025-04-16 17:30:36 UTC",
    "updated_date": "2025-04-16 17:30:36 UTC"
  },
  {
    "arxiv_id": "2504.13955v4",
    "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations",
    "authors": [
      "Suhas BN",
      "Andrew M. Sherrill",
      "Rosa I. Arriaga",
      "Chris W. Wiese",
      "Saeed Abdullah"
    ],
    "abstract": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "68T50",
      "I.2.7; H.5.2"
    ],
    "primary_category": "cs.CY",
    "comment": "22 pages, 6 figures Updated Appendix with example model responses",
    "pdf_url": "http://arxiv.org/pdf/2504.13955v4",
    "published_date": "2025-04-16 17:29:05 UTC",
    "updated_date": "2025-05-16 14:12:03 UTC"
  },
  {
    "arxiv_id": "2504.12262v1",
    "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
    "authors": [
      "David Keetae Park",
      "Xihaier Luo",
      "Guang Zhao",
      "Seungjun Lee",
      "Miruna Oprescu",
      "Shinjae Yoo"
    ],
    "abstract": "Spatiotemporal learning is challenging due to the intricate interplay between\nspatial and temporal dependencies, the high dimensionality of the data, and\nscalability constraints. These challenges are further amplified in scientific\ndomains, where data is often irregularly distributed (e.g., missing values from\nsensor failures) and high-volume (e.g., high-fidelity simulations), posing\nadditional computational and modeling difficulties. In this paper, we present\nSCENT, a novel framework for scalable and continuity-informed spatiotemporal\nrepresentation learning. SCENT unifies interpolation, reconstruction, and\nforecasting within a single architecture. Built on a transformer-based\nencoder-processor-decoder backbone, SCENT introduces learnable queries to\nenhance generalization and a query-wise cross-attention mechanism to\neffectively capture multi-scale dependencies. To ensure scalability in both\ndata size and model complexity, we incorporate a sparse attention mechanism,\nenabling flexible output representations and efficient evaluation at arbitrary\nresolutions. We validate SCENT through extensive simulations and real-world\nexperiments, demonstrating state-of-the-art performance across multiple\nchallenging tasks while achieving superior scalability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 5 main figures, 3 tables, under review",
    "pdf_url": "http://arxiv.org/pdf/2504.12262v1",
    "published_date": "2025-04-16 17:17:31 UTC",
    "updated_date": "2025-04-16 17:17:31 UTC"
  },
  {
    "arxiv_id": "2504.12256v1",
    "title": "FLIP Reasoning Challenge",
    "authors": [
      "Andreas Plesner",
      "Turlan Kuzhagaliyev",
      "Roger Wattenhofer"
    ],
    "abstract": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12256v1",
    "published_date": "2025-04-16 17:07:16 UTC",
    "updated_date": "2025-04-16 17:07:16 UTC"
  },
  {
    "arxiv_id": "2504.12254v2",
    "title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning",
    "authors": [
      "Mahmoud Salhab",
      "Marwan Elghitany",
      "Shameed Sait",
      "Syed Sibghat Ullah",
      "Mohammad Abusheikh",
      "Hasan Abusheikh"
    ],
    "abstract": "Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach achieves state-of-the-art (SOTA)\nresults in Arabic ASR, surpassing both open and closed-source models on\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12254v2",
    "published_date": "2025-04-16 17:05:14 UTC",
    "updated_date": "2025-04-19 09:55:40 UTC"
  },
  {
    "arxiv_id": "2504.12215v1",
    "title": "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing",
    "authors": [
      "Ilkin Sevgi Isler",
      "David Mohaisen",
      "Curtis Lisle",
      "Damla Turgut",
      "Ulas Bagci"
    ],
    "abstract": "Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures, to appear in IEEE ADSCA 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12215v1",
    "published_date": "2025-04-16 16:08:38 UTC",
    "updated_date": "2025-04-16 16:08:38 UTC"
  },
  {
    "arxiv_id": "2504.12365v1",
    "title": "Themisto: Jupyter-Based Runtime Benchmark",
    "authors": [
      "Konstantin Grotov",
      "Sergey Titov"
    ],
    "abstract": "In this work, we present a benchmark that consists of Jupyter notebooks\ndevelopment trajectories and allows measuring how large language models (LLMs)\ncan leverage runtime information for predicting code output and code\ngeneration. We demonstrate that the current generation of LLMs performs poorly\non these tasks and argue that there exists a significantly understudied domain\nin the development of code-based models, which involves incorporating the\nruntime context.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to the third Deep Learning for Code (DL4C) workshop @ ICLR\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12365v1",
    "published_date": "2025-04-16 16:07:18 UTC",
    "updated_date": "2025-04-16 16:07:18 UTC"
  },
  {
    "arxiv_id": "2504.12210v2",
    "title": "Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks",
    "authors": [
      "Tingyang Sun",
      "Tuan Nguyen",
      "Ting He"
    ],
    "abstract": "Decentralized federated learning (DFL) is a promising machine learning\nparadigm for bringing artificial intelligence (AI) capabilities to the network\nedge. Running DFL on top of edge networks, however, faces severe performance\nchallenges due to the extensive parameter exchanges between agents. Most\nexisting solutions for these challenges were based on simplistic communication\nmodels, which cannot capture the case of learning over a multi-hop\nbandwidth-limited network. In this work, we address this problem by jointly\ndesigning the communication scheme for the overlay network formed by the agents\nand the mixing matrix that controls the communication demands between the\nagents. By carefully analyzing the properties of our problem, we cast each\ndesign problem into a tractable optimization and develop an efficient algorithm\nwith guaranteed performance. Our evaluations based on real topology and data\nshow that the proposed algorithm can reduce the total training time by over\n$80\\%$ compared to the baseline without sacrificing accuracy, while\nsignificantly improving the computational efficiency over the state of the art.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "arXiv admin note: text overlap with arXiv:2408.04705",
    "pdf_url": "http://arxiv.org/pdf/2504.12210v2",
    "published_date": "2025-04-16 15:56:57 UTC",
    "updated_date": "2025-04-21 07:27:59 UTC"
  },
  {
    "arxiv_id": "2504.12192v1",
    "title": "From Requirements to Architecture: Semi-Automatically Generating Software Architectures",
    "authors": [
      "Tobias Eisenreich"
    ],
    "abstract": "To support junior and senior architects, I propose developing a new\narchitecture creation method that leverages LLMs' evolving capabilities to\nsupport the architect. This method involves the architect's close collaboration\nwith LLM-fueled tooling over the whole process. The architect is guided through\nDomain Model creation, Use Case specification, architectural decisions, and\narchitecture evaluation. While the architect can take complete control of the\nprocess and the results, and use the tooling as a building set, they can follow\nthe intended process for maximum tooling support. The preliminary results\nsuggest the feasibility of this process and indicate major time savings for the\narchitect.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "to be published in EMISA 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12192v1",
    "published_date": "2025-04-16 15:46:56 UTC",
    "updated_date": "2025-04-16 15:46:56 UTC"
  },
  {
    "arxiv_id": "2504.12187v1",
    "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
    "authors": [
      "Céline Budding"
    ],
    "abstract": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in Philosophy of Science",
    "pdf_url": "http://arxiv.org/pdf/2504.12187v1",
    "published_date": "2025-04-16 15:42:33 UTC",
    "updated_date": "2025-04-16 15:42:33 UTC"
  },
  {
    "arxiv_id": "2504.12185v1",
    "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data",
    "authors": [
      "Suyoung Bae",
      "Hyojun Kim",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12185v1",
    "published_date": "2025-04-16 15:40:10 UTC",
    "updated_date": "2025-04-16 15:40:10 UTC"
  },
  {
    "arxiv_id": "2504.12180v1",
    "title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification",
    "authors": [
      "Jaime E. Cuellar",
      "Oscar Moreno-Martinez",
      "Paula Sofia Torres-Rodriguez",
      "Jaime Andres Pavlich-Mariscal",
      "Andres Felipe Mican-Castiblanco",
      "Juan Guillermo Torres-Hurtado"
    ],
    "abstract": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Spanish language",
    "pdf_url": "http://arxiv.org/pdf/2504.12180v1",
    "published_date": "2025-04-16 15:37:09 UTC",
    "updated_date": "2025-04-16 15:37:09 UTC"
  },
  {
    "arxiv_id": "2504.12177v1",
    "title": "Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube",
    "authors": [
      "Victor Manuel Hernandez Lopez",
      "Jaime E. Cuellar"
    ],
    "abstract": "This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Spanish language",
    "pdf_url": "http://arxiv.org/pdf/2504.12177v1",
    "published_date": "2025-04-16 15:27:57 UTC",
    "updated_date": "2025-04-16 15:27:57 UTC"
  },
  {
    "arxiv_id": "2504.12172v1",
    "title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task",
    "authors": [
      "Maged S. Al-Shaibani",
      "Zaid Alyafeai",
      "Irfan Ahmad"
    ],
    "abstract": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12172v1",
    "published_date": "2025-04-16 15:25:45 UTC",
    "updated_date": "2025-04-16 15:25:45 UTC"
  },
  {
    "arxiv_id": "2504.12151v1",
    "title": "Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis",
    "authors": [
      "Miaosen Luo",
      "Yuncheng Jiang",
      "Sijie Mai"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack\nof interpretability in the decision logic of multimodal fusion and modality\nimbalance caused by disparities in inter-modal information density. To address\nthese issues, we propose KAN-MCP, a novel framework that integrates the\ninterpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the\nMultimodal Clean Pareto (MCPareto) framework. First, KAN leverages its\nunivariate function decomposition to achieve transparent analysis of\ncross-modal interactions. This structural design allows direct inspection of\nfeature transformations without relying on external interpretation tools,\nthereby ensuring both high expressiveness and interpretability. Second, the\nproposed MCPareto enhances robustness by addressing modality imbalance and\nnoise interference. Specifically, we introduce the Dimensionality Reduction and\nDenoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises\nand reduces feature dimensionality. This approach provides KAN with\ndiscriminative low-dimensional inputs to reduce the modeling complexity of KAN\nwhile preserving critical sentiment-related information. Furthermore, MCPareto\ndynamically balances gradient contributions across modalities using the\npurified features output by DRD-MIB, ensuring lossless transmission of\nauxiliary signals and effectively alleviating modality imbalance. This synergy\nof interpretability and robustness not only achieves superior performance on\nbenchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers\nan intuitive visualization interface through KAN's interpretable architecture.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12151v1",
    "published_date": "2025-04-16 15:00:06 UTC",
    "updated_date": "2025-04-16 15:00:06 UTC"
  },
  {
    "arxiv_id": "2504.12143v1",
    "title": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges",
    "authors": [
      "Matteo Lupinacci",
      "Francesco Blefari",
      "Francesco Romeo",
      "Francesco Aurelio Pironti",
      "Angelo Furfaro"
    ],
    "abstract": "The growing and evolving landscape of cybersecurity threats necessitates the\ndevelopment of supporting tools and platforms that allow for the creation of\nrealistic IT environments operating within virtual, controlled settings as\nCyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and\nexperimenting with the effectiveness of devised countermeasures, as well as\nserving as training environments for building cyber security skills and\nabilities for IT operators. This paper proposes ARCeR as an innovative solution\nfor the automatic generation and deployment of CRs, starting from user-provided\ndescriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,\nwhich allows it to fully exploit state-of-art AI technologies. Experimental\nresults show that ARCeR is able to successfully process prompts even in cases\nthat LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is\nable to target any CR framework provided that specific knowledge is made\navailable to it.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12143v1",
    "published_date": "2025-04-16 14:53:28 UTC",
    "updated_date": "2025-04-16 14:53:28 UTC"
  },
  {
    "arxiv_id": "2504.12137v1",
    "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -",
    "authors": [
      "Laura Fieback",
      "Nishilkumar Balar",
      "Jakob Spiegelberg",
      "Hanno Gottschalk"
    ],
    "abstract": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12137v1",
    "published_date": "2025-04-16 14:50:25 UTC",
    "updated_date": "2025-04-16 14:50:25 UTC"
  },
  {
    "arxiv_id": "2504.12110v1",
    "title": "Towards LLM Agents for Earth Observation",
    "authors": [
      "Chia Hsiang Kao",
      "Wenting Zhao",
      "Shreelekha Revankar",
      "Samuel Speas",
      "Snehal Bhagat",
      "Rajeev Datta",
      "Cheng Perng Phoo",
      "Utkarsh Mall",
      "Carl Vondrick",
      "Kavita Bala",
      "Bharath Hariharan"
    ],
    "abstract": "Earth Observation (EO) provides critical planetary data for environmental\nmonitoring, disaster management, climate science, and other scientific domains.\nHere we ask: Are AI systems ready for reliable Earth Observation? We introduce\n\\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth\nObservatory articles across 13 topics and 17 satellite sensors. Using Google\nEarth Engine API as a tool, LLM agents can only achieve an accuracy of 33%\nbecause the code fails to run over 58% of the time. We improve the failure rate\nfor open models by fine-tuning synthetic data, allowing much smaller models\n(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,\nDeepSeek-R1). Taken together, our findings identify significant challenges to\nbe solved before AI agents can automate earth observation, and suggest paths\nforward. The project page is available at\nhttps://iandrover.github.io/UnivEarth.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.12110v1",
    "published_date": "2025-04-16 14:19:25 UTC",
    "updated_date": "2025-04-16 14:19:25 UTC"
  },
  {
    "arxiv_id": "2504.12090v1",
    "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
    "authors": [
      "Jack Preuveneers",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "abstract": "We present a novel framework that bridges the gap between the\ninterpretability of decision trees and the advanced reasoning capabilities of\nlarge language models (LLMs) to predict startup success. Our approach leverages\nchain-of-thought prompting to generate detailed reasoning logs, which are\nsubsequently distilled into structured, human-understandable logical rules. The\npipeline integrates multiple enhancements - efficient data ingestion, a\ntwo-step refinement process, ensemble candidate sampling, simulated\nreinforcement learning scoring, and persistent memory - to ensure both stable\ndecision-making and transparent output. Experimental evaluations on curated\nstartup datasets demonstrate that our combined pipeline improves precision by\n54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a\nstandalone OpenAI o3 model. Notably, our model achieves over 2x the precision\nof a random classifier (16%). By combining state-of-the-art AI reasoning with\nexplicit rule-based explanations, our method not only augments traditional\ndecision-making processes but also facilitates expert intervention and\ncontinuous policy refinement. This work lays the foundation for the\nimplementation of interpretable LLM-powered decision frameworks in high-stakes\ninvestment environments and other domains that require transparent and\ndata-driven insights.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12090v1",
    "published_date": "2025-04-16 13:53:42 UTC",
    "updated_date": "2025-04-16 13:53:42 UTC"
  },
  {
    "arxiv_id": "2504.12088v1",
    "title": "AttentionDrop: A Novel Regularization Method for Transformer Models",
    "authors": [
      "Mirza Samad Ahmed Baig",
      "Syeda Anshrah Gillani",
      "Abdul Akbar Khan",
      "Shahid Munir Shah"
    ],
    "abstract": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.12088v1",
    "published_date": "2025-04-16 13:51:16 UTC",
    "updated_date": "2025-04-16 13:51:16 UTC"
  },
  {
    "arxiv_id": "2504.12082v1",
    "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection",
    "authors": [
      "Yumin Kim",
      "Hwanhee Lee"
    ],
    "abstract": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12082v1",
    "published_date": "2025-04-16 13:43:23 UTC",
    "updated_date": "2025-04-16 13:43:23 UTC"
  },
  {
    "arxiv_id": "2504.12063v1",
    "title": "Optimizing Compound Retrieval Systems",
    "authors": [
      "Harrie Oosterhuis",
      "Rolf Jagerman",
      "Zhen Qin",
      "Xuanhui Wang"
    ],
    "abstract": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12063v1",
    "published_date": "2025-04-16 13:18:16 UTC",
    "updated_date": "2025-04-16 13:18:16 UTC"
  },
  {
    "arxiv_id": "2504.12039v1",
    "title": "RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model",
    "authors": [
      "Yizhuo Wu",
      "Francesco Fioranelli",
      "Chang Gao"
    ],
    "abstract": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.12039v1",
    "published_date": "2025-04-16 12:54:11 UTC",
    "updated_date": "2025-04-16 12:54:11 UTC"
  },
  {
    "arxiv_id": "2504.12031v1",
    "title": "Proof-Carrying Neuro-Symbolic Code",
    "authors": [
      "Ekaterina Komendantskaya"
    ],
    "abstract": "This invited paper introduces the concept of \"proof-carrying neuro-symbolic\ncode\" and explains its meaning and value, from both the \"neural\" and the\n\"symbolic\" perspectives. The talk outlines the first successes and challenges\nthat this new area of research faces.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LO",
      "F.3.1; F.3.2; F.3.3; I.2.0"
    ],
    "primary_category": "cs.PL",
    "comment": "Invited paper at CiE 2025. arXiv admin note: text overlap with\n  arXiv:2501.05867",
    "pdf_url": "http://arxiv.org/pdf/2504.12031v1",
    "published_date": "2025-04-16 12:42:18 UTC",
    "updated_date": "2025-04-16 12:42:18 UTC"
  },
  {
    "arxiv_id": "2504.12012v1",
    "title": "Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models",
    "authors": [
      "Kris Pilcher",
      "Esen K. Tütüncü"
    ],
    "abstract": "Hallucinations in Large Language Models (LLMs) are widely regarded as errors\n- outputs that deviate from factual accuracy. However, in creative or\nexploratory contexts, these \"mistakes\" may represent unexpected avenues for\ninnovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach\nthat amplifies LLM hallucinations for imaginative tasks such as speculative\nfiction, interactive storytelling, and mixed-reality simulations. Drawing on\nHerman Melville's Moby-Dick, where Pip's \"madness\" reveals profound insight, we\nreframe hallucinations as a source of computational imagination rather than a\nflaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and\nsurreal outputs - hallucinations that are useful when factual accuracy is not\nthe chief objective. Inspired by the consensual illusions of theater and stage\nmagic, PIP situates these creative missteps in contexts where users willingly\nsuspend disbelief, thereby transforming \"errors\" into catalysts for new ways of\nthinking. We discuss potential applications, design principles for ensuring\nuser consent, preliminary observations, and implications for broader AI ethics\nand human-AI collaboration.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.12012v1",
    "published_date": "2025-04-16 12:13:02 UTC",
    "updated_date": "2025-04-16 12:13:02 UTC"
  },
  {
    "arxiv_id": "2504.12011v1",
    "title": "Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition",
    "authors": [
      "Heesoo Jung",
      "Hogun Park"
    ],
    "abstract": "Self-supervised learning (SSL) in graphs has garnered significant attention,\nparticularly in employing Graph Neural Networks (GNNs) with pretext tasks\ninitially designed for other domains, such as contrastive learning and feature\nreconstruction. However, it remains uncertain whether these methods effectively\nreflect essential graph properties, precisely representation similarity with\nits neighbors. We observe that existing methods position opposite ends of a\nspectrum driven by the graph embedding smoothness, with each end corresponding\nto outperformance on specific downstream tasks. Decomposing the SSL objective\ninto three terms via an information-theoretic framework with a neighbor\nrepresentation variable reveals that this polarization stems from an imbalance\namong the terms, which existing methods may not effectively maintain. Further\ninsights suggest that balancing between the extremes can lead to improved\nperformance across a wider range of downstream tasks. A framework, BSG\n(Balancing Smoothness in Graph SSL), introduces novel loss functions designed\nto supplement the representation quality in graph-based SSL by balancing the\nderived three terms: neighbor loss, minimal loss, and divergence loss. We\npresent a theoretical analysis of the effects of these loss functions,\nhighlighting their significance from both the SSL and graph smoothness\nperspectives. Extensive experiments on multiple real-world datasets across node\nclassification and link prediction consistently demonstrate that BSG achieves\nstate-of-the-art performance, outperforming existing methods. Our\nimplementation code is available at https://github.com/steve30572/BSG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Web Conference (WWW) 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12011v1",
    "published_date": "2025-04-16 12:09:56 UTC",
    "updated_date": "2025-04-16 12:09:56 UTC"
  },
  {
    "arxiv_id": "2504.12007v1",
    "title": "Generative Recommendation with Continuous-Token Diffusion",
    "authors": [
      "Haohao Qu",
      "Wenqi Fan",
      "Shanru Lin"
    ],
    "abstract": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12007v1",
    "published_date": "2025-04-16 12:01:03 UTC",
    "updated_date": "2025-04-16 12:01:03 UTC"
  },
  {
    "arxiv_id": "2504.11997v1",
    "title": "A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
    "authors": [
      "Kihyuk Hong",
      "Ambuj Tewari"
    ],
    "abstract": "We study reinforcement learning in infinite-horizon average-reward settings\nwith linear MDPs. Previous work addresses this problem by approximating the\naverage-reward setting by discounted setting and employing a value\niteration-based algorithm that uses clipping to constrain the span of the value\nfunction for improved statistical efficiency. However, the clipping procedure\nrequires computing the minimum of the value function over the entire state\nspace, which is prohibitive since the state space in linear MDP setting can be\nlarge or even infinite. In this paper, we introduce a value iteration method\nwith efficient clipping operation that only requires computing the minimum of\nvalue functions over the set of states visited by the algorithm. Our algorithm\nenjoys the same regret bound as the previous work while being computationally\nefficient, with computational complexity that is independent of the size of the\nstate space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11997v1",
    "published_date": "2025-04-16 11:47:41 UTC",
    "updated_date": "2025-04-16 11:47:41 UTC"
  },
  {
    "arxiv_id": "2504.11986v2",
    "title": "Large Language Models as Quasi-crystals: Coherence Without Repetition in Generative Text",
    "authors": [
      "Jose Manuel Guevara-Vela"
    ],
    "abstract": "This essay proposes an interpretive analogy between large language models\n(LLMs) and quasicrystals, systems that exhibit global coherence without\nperiodic repetition, generated through local constraints. While LLMs are\ntypically evaluated in terms of predictive accuracy, factuality, or alignment,\nthis structural perspective suggests that one of their most characteristic\nbehaviors is the production of internally resonant linguistic patterns. Drawing\non the history of quasicrystals, which forced a redefinition of structural\norder in physical systems, the analogy highlights an alternative mode of\ncoherence in generative language: constraint-based organization without\nrepetition or symbolic intent. Rather than viewing LLMs as imperfect agents or\nstochastic approximators, we suggest understanding them as generators of\nquasi-structured outputs. This framing complements existing evaluation\nparadigms by foregrounding formal coherence and pattern as interpretable\nfeatures of model behavior. While the analogy has limits, it offers a\nconceptual tool for exploring how coherence might arise and be assessed in\nsystems where meaning is emergent, partial, or inaccessible. In support of this\nperspective, we draw on philosophy of science and language, including\nmodel-based accounts of scientific representation, structural realism, and\ninferentialist views of meaning. We further propose the notion of structural\nevaluation: a mode of assessment that examines how well outputs propagate\nconstraint, variation, and order across spans of generated text. This essay\naims to reframe the current discussion around large language models, not by\nrejecting existing methods, but by suggesting an additional axis of\ninterpretation grounded in structure rather than semantics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The discussion was restructured to add limitations to the analogy and\n  other clarifications",
    "pdf_url": "http://arxiv.org/pdf/2504.11986v2",
    "published_date": "2025-04-16 11:27:47 UTC",
    "updated_date": "2025-04-19 13:53:16 UTC"
  },
  {
    "arxiv_id": "2504.11977v1",
    "title": "Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
    "authors": [
      "Sofia Krylova",
      "Fabian Schmidt",
      "Vladimir Vlassov"
    ],
    "abstract": "Many existing digital triage systems are questionnaire-based, guiding\npatients to appropriate care levels based on information (e.g., symptoms,\nmedical history, and urgency) provided by the patients answering\nquestionnaires. Such a system often uses a deterministic model with predefined\nrules to determine care levels. It faces challenges with incomplete triage\ninterviews since it can only assist patients who finish the process. In this\nstudy, we explore the use of machine learning (ML) to predict outcomes of\nunfinished interviews, aiming to enhance patient care and service quality.\nPredicting triage outcomes from incomplete data is crucial for patient safety\nand healthcare efficiency. Our findings show that decision-tree models,\nparticularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy\nin predicting outcomes from complete interviews while having a linear\ncorrelation between the prediction accuracy and interview completeness degree.\nFor example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews\nwith 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness,\n58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\%\ncompleteness. The TabTransformer model demonstrated exceptional accuracy of\nover 80\\% for all degrees of completeness but required extensive training time,\nindicating a need for more powerful computational resources. The study\nhighlights the linear correlation between interview completeness and predictive\npower of the decision-tree models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.11977v1",
    "published_date": "2025-04-16 11:17:23 UTC",
    "updated_date": "2025-04-16 11:17:23 UTC"
  },
  {
    "arxiv_id": "2504.11967v2",
    "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Sanjian Zhang",
      "Guangyu Chen",
      "Yuzhi Hu",
      "Masumi Yano",
      "Jingdong Sun",
      "Siyu Huang",
      "Feng Liu",
      "Qi Dai",
      "Zhi-Qi Cheng"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR Workshop Anti-UAV 2025. 15 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.11967v2",
    "published_date": "2025-04-16 10:58:33 UTC",
    "updated_date": "2025-04-17 09:25:04 UTC"
  },
  {
    "arxiv_id": "2504.13951v1",
    "title": "Generative System Dynamics in Recurrent Neural Networks",
    "authors": [
      "Michele Casoni",
      "Tommaso Guidi",
      "Alessandro Betti",
      "Stefano Melacci",
      "Marco Gori"
    ],
    "abstract": "In this study, we investigate the continuous time dynamics of Recurrent\nNeural Networks (RNNs), focusing on systems with nonlinear activation\nfunctions. The objective of this work is to identify conditions under which\nRNNs exhibit perpetual oscillatory behavior, without converging to static fixed\npoints. We establish that skew-symmetric weight matrices are fundamental to\nenable stable limit cycles in both linear and nonlinear configurations. We\nfurther demonstrate that hyperbolic tangent-like activation functions (odd,\nbounded, and continuous) preserve these oscillatory dynamics by ensuring motion\ninvariants in state space. Numerical simulations showcase how nonlinear\nactivation functions not only maintain limit cycles, but also enhance the\nnumerical stability of the system integration process, mitigating those\ninstabilities that are commonly associated with the forward Euler method. The\nexperimental results of this analysis highlight practical considerations for\ndesigning neural architectures capable of capturing complex temporal\ndependencies, i.e., strategies for enhancing memorization skills in recurrent\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13951v1",
    "published_date": "2025-04-16 10:39:43 UTC",
    "updated_date": "2025-04-16 10:39:43 UTC"
  },
  {
    "arxiv_id": "2504.11952v2",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "authors": [
      "Ram Mohan Rao Kadiyala",
      "Siddartha Pullakhandam",
      "Kanwal Mehreen",
      "Drishti Sharma",
      "Siddhant Gupta",
      "Jebish Purbey",
      "Ashay Srivastava",
      "Subhasya TippaReddy",
      "Arvind Reddy Bobbili",
      "Suraj Telugara Chandrashekhar",
      "Modabbir Adeeb",
      "Srinadh Vura",
      "Hamza Farooq"
    ],
    "abstract": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.11952v2",
    "published_date": "2025-04-16 10:29:30 UTC",
    "updated_date": "2025-05-22 10:09:18 UTC"
  },
  {
    "arxiv_id": "2504.11944v1",
    "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning",
    "authors": [
      "Xuyang Chen",
      "Guojian Wang",
      "Keyu Yan",
      "Lin Zhao"
    ],
    "abstract": "Offline reinforcement learning (RL) learns effective policies from\npre-collected datasets, offering a practical solution for applications where\nonline interactions are risky or costly. Model-based approaches are\nparticularly advantageous for offline RL, owing to their data efficiency and\ngeneralizability. However, due to inherent model errors, model-based methods\noften artificially introduce conservatism guided by heuristic uncertainty\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\nmodel-based offline RL algorithm that incorporates self-supervised feedback\nfrom value estimation to enhance model training. Specifically, the model is\nlearned by additionally minimizing the inconsistency between the value learned\ndirectly from the offline data and the one estimated from the model. We perform\ncomprehensive evaluations from multiple perspectives to show that VIPO can\nlearn a highly accurate model efficiently and consistently outperform existing\nmethods. It offers a general framework that can be readily integrated into\nexisting model-based offline RL algorithms to systematically enhance model\naccuracy. As a result, VIPO achieves state-of-the-art performance on almost all\ntasks in both D4RL and NeoRL benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11944v1",
    "published_date": "2025-04-16 10:23:44 UTC",
    "updated_date": "2025-04-16 10:23:44 UTC"
  },
  {
    "arxiv_id": "2504.11942v1",
    "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation",
    "authors": [
      "Nada Shahin",
      "Leila Ismail"
    ],
    "abstract": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11942v1",
    "published_date": "2025-04-16 10:20:11 UTC",
    "updated_date": "2025-04-16 10:20:11 UTC"
  },
  {
    "arxiv_id": "2504.18556v1",
    "title": "RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features",
    "authors": [
      "Jialei Song",
      "Xingquan Zuo",
      "Feiyang Wang",
      "Hai Huang",
      "Tianle Zhang"
    ],
    "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial samples,\nraising concerns about their reliability in safety-critical tasks. Currently,\nmethods of evaluating adversarial robustness are primarily categorized into\nattack-based and certified robustness evaluation approaches. The former not\nonly relies on specific attack algorithms but also is highly time-consuming,\nwhile the latter due to its analytical nature, is typically difficult to\nimplement for large and complex models. A few studies evaluate model robustness\nbased on the model's decision boundary, but they suffer from low evaluation\naccuracy. To address the aforementioned issues, we propose a novel adversarial\nrobustness evaluation metric, Robustness Difference Index (RDI), which is based\non sample clustering features. RDI draws inspiration from clustering evaluation\nby analyzing the intra-class and inter-class distances of feature vectors\nseparated by the decision boundary to quantify model robustness. It is\nattack-independent and has high computational efficiency. Experiments show\nthat, RDI demonstrates a stronger correlation with the gold-standard\nadversarial robustness metric of attack success rate (ASR). The average\ncomputation time of RDI is only 1/30 of the evaluation method based on the PGD\nattack. Our open-source code is available at:\nhttps://anonymous.4open.science/r/RDI-B1DA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.18556v1",
    "published_date": "2025-04-16 10:05:37 UTC",
    "updated_date": "2025-04-16 10:05:37 UTC"
  },
  {
    "arxiv_id": "2504.11919v1",
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
    "authors": [
      "Qianjin Yu",
      "Keyu Wu",
      "Zihan Chen",
      "Chushu Zhang",
      "Manlin Mei",
      "Lingjun Huang",
      "Fang Tan",
      "Yongsheng Du",
      "Kunlin Liu",
      "Yurui Zhu"
    ],
    "abstract": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11919v1",
    "published_date": "2025-04-16 09:55:34 UTC",
    "updated_date": "2025-04-16 09:55:34 UTC"
  },
  {
    "arxiv_id": "2504.11901v3",
    "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments",
    "authors": [
      "Luca Castri",
      "Gloria Beraldo",
      "Nicola Bellotto"
    ],
    "abstract": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial\n  Interaction - Decision-Making",
    "pdf_url": "http://arxiv.org/pdf/2504.11901v3",
    "published_date": "2025-04-16 09:26:04 UTC",
    "updated_date": "2025-05-12 09:35:07 UTC"
  },
  {
    "arxiv_id": "2504.11896v1",
    "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement",
    "authors": [
      "Xingxing Yang",
      "Jie Chen",
      "Zaifeng Yang"
    ],
    "abstract": "Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.11896v1",
    "published_date": "2025-04-16 09:23:38 UTC",
    "updated_date": "2025-04-16 09:23:38 UTC"
  },
  {
    "arxiv_id": "2504.13950v1",
    "title": "Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain",
    "authors": [
      "Zhongxi Qiu",
      "Zhang Zhang",
      "Yan Hu",
      "Heng Li",
      "Jiang Liu"
    ],
    "abstract": "This paper explores optimal data selection strategies for Reinforcement\nLearning with Verified Rewards (RLVR) training in the medical domain. While\nRLVR has shown exceptional potential for enhancing reasoning capabilities in\nlarge language models, most prior implementations have focused on mathematics\nand logical puzzles, with limited exploration of domain-specific applications\nlike medicine. We investigate four distinct data sampling strategies from\nMedQA-USMLE: random sampling (baseline), and filtering using Phi-4,\nGemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base\nmodel and implementing Group Relative Policy Optimization (GRPO), we evaluate\nperformance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and\nCMMLU. Our findings demonstrate that models trained on filtered data generally\noutperform those trained on randomly selected samples. Notably, training on\nself-filtered samples (using Gemma-3-12b-it for filtering) achieved superior\nperformance in medical domains but showed reduced robustness across different\nbenchmarks, while filtering with larger models from the same series yielded\nbetter overall robustness. These results provide valuable insights into\neffective data organization strategies for RLVR in specialized domains and\nhighlight the importance of thoughtful data selection in achieving optimal\nperformance. You can access our repository\n(https://github.com/Qsingle/open-medical-r1) to get the codes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.13950v1",
    "published_date": "2025-04-16 09:16:12 UTC",
    "updated_date": "2025-04-16 09:16:12 UTC"
  },
  {
    "arxiv_id": "2504.11882v1",
    "title": "Seeking and leveraging alternative variable dependency concepts in gray-box-elusive bimodal land-use allocation problems",
    "authors": [
      "J. Maciążek",
      "M. W. Przewozniczek",
      "J. Schwaab"
    ],
    "abstract": "Solving land-use allocation problems can help us to deal with some of the\nmost urgent global environmental issues. Since these problems are NP-hard,\neffective optimizers are needed to handle them. The knowledge about variable\ndependencies allows for proposing such tools. However, in this work, we\nconsider a real-world multi-objective problem for which standard variable\ndependency discovery techniques are inapplicable. Therefore, using\nlinkage-based variation operators is unreachable. To address this issue, we\npropose a definition of problem-dedicated variable dependency. On this base, we\npropose obtaining masks of dependent variables. Using them, we construct three\nnovel crossover operators. The results concerning real-world test cases show\nthat introducing our propositions into two well-known optimizers (NSGA-II,\nMOEA/D) dedicated to multi-objective optimization significantly improves their\neffectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11882v1",
    "published_date": "2025-04-16 09:06:55 UTC",
    "updated_date": "2025-04-16 09:06:55 UTC"
  },
  {
    "arxiv_id": "2504.13211v1",
    "title": "Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance",
    "authors": [
      "Subin Kim",
      "Hoonrae Kim",
      "Jihyun Lee",
      "Yejin Jeon",
      "Gary Geunbae Lee"
    ],
    "abstract": "Recent studies have explored the use of large language models (LLMs) in\npsychotherapy; however, text-based cognitive behavioral therapy (CBT) models\noften struggle with client resistance, which can weaken therapeutic alliance.\nTo address this, we propose a multimodal approach that incorporates nonverbal\ncues, allowing the AI therapist to better align its responses with the client's\nnegative emotional state. Specifically, we introduce a new synthetic dataset,\nMultimodal Interactive Rolling with Resistance (Mirror), which is a novel\nsynthetic dataset that pairs client statements with corresponding facial\nimages. Using this dataset, we train baseline Vision-Language Models (VLMs)\nthat can analyze facial cues, infer emotions, and generate empathetic responses\nto effectively manage resistance. They are then evaluated in terms of both the\ntherapist's counseling skills and the strength of the therapeutic alliance in\nthe presence of client resistance. Our results demonstrate that Mirror\nsignificantly enhances the AI therapist's ability to handle resistance, which\noutperforms existing text-based CBT approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13211v1",
    "published_date": "2025-04-16 08:44:26 UTC",
    "updated_date": "2025-04-16 08:44:26 UTC"
  },
  {
    "arxiv_id": "2504.11864v1",
    "title": "Moving between high-quality optima using multi-satisfiability characteristics in hard-to-solve Max3Sat instances",
    "authors": [
      "J. Piatek",
      "M. W. Przewozniczek",
      "F. Chicano",
      "R. Tinós"
    ],
    "abstract": "Gray-box optimization proposes effective and efficient optimizers of general\nuse. To this end, it leverages information about variable dependencies and the\nsubfunction-based problem representation. These approaches were already shown\neffective by enabling \\textit{tunnelling} between local optima even if these\nmoves require the modification of many dependent variables. Tunnelling is\nuseful in solving the maximum satisfiability problem (MaxSat), which can be\nreformulated to Max3Sat. Since many real-world problems can be brought to\nsolving the MaxSat/Max3Sat instances, it is important to solve them effectively\nand efficiently. Therefore, we focus on Max3Sat instances for which tunnelling\nfails to introduce improving moves between locally optimal high-quality\nsolutions and the region of globally optimal solutions. We analyze the features\nof such instances on the ground of phase transitions. Based on these\nobservations, we propose manipulating clause-satisfiability characteristics\nthat allow connecting high-quality solutions distant in the solution space. We\nutilize multi-satisfiability characteristics in the optimizer built from\ntypical gray-box mechanisms. The experimental study shows that the proposed\noptimizer can solve those Max3Sat instances that are out of the grasp of\nstate-of-the-art gray-box optimizers. At the same time, it remains effective\nfor instances that have already been successfully solved by gray-box.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11864v1",
    "published_date": "2025-04-16 08:38:08 UTC",
    "updated_date": "2025-04-16 08:38:08 UTC"
  },
  {
    "arxiv_id": "2504.11855v1",
    "title": "EngramNCA: a Neural Cellular Automaton Model of Memory Transfer",
    "authors": [
      "Etienne Guichard",
      "Felix Reimers",
      "Mia Kvalsund",
      "Mikkel Lepperød",
      "Stefano Nichele"
    ],
    "abstract": "This study introduces EngramNCA, a neural cellular automaton (NCA) that\nintegrates both publicly visible states and private, cell-internal memory\nchannels, drawing inspiration from emerging biological evidence suggesting that\nmemory storage extends beyond synaptic modifications to include intracellular\nmechanisms. The proposed model comprises two components: GeneCA, an NCA trained\nto develop distinct morphologies from seed cells containing immutable \"gene\"\nencodings, and GenePropCA, an auxiliary NCA that modulates the private\n\"genetic\" memory of cells without altering their visible states. This\narchitecture enables the encoding and propagation of complex morphologies\nthrough the interaction of visible and private channels, facilitating the\ngrowth of diverse structures from a shared \"genetic\" substrate. EngramNCA\nsupports the emergence of hierarchical and coexisting morphologies, offering\ninsights into decentralized memory storage and transfer in artificial systems.\nThese findings have potential implications for the development of adaptive,\nself-organizing systems and may contribute to the broader understanding of\nmemory mechanisms in both biological and synthetic contexts.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11855v1",
    "published_date": "2025-04-16 08:23:09 UTC",
    "updated_date": "2025-04-16 08:23:09 UTC"
  },
  {
    "arxiv_id": "2504.13949v1",
    "title": "On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence",
    "authors": [
      "M. W. Przewozniczek",
      "F. Chicano",
      "R. Tinós",
      "J. Nalepa",
      "B. Ruszczak",
      "A. M. Wijata"
    ],
    "abstract": "Gray-box optimization employs Walsh decomposition to obtain non-linear\nvariable dependencies and utilize them to propose masks of variables that have\na joint non-linear influence on fitness value. These masks significantly\nimprove the effectiveness of variation operators. In some problems, all\nvariables are non-linearly dependent, making the aforementioned masks useless.\nWe analyze the features of the real-world instances of such problems and show\nthat many of their dependencies may have noise-like origins. Such noise-caused\ndependencies are irrelevant to the optimization process and can be ignored. To\nidentify them, we propose extending the use of Walsh decomposition by measuring\nvariable dependency strength that allows the construction of the weighted\ndynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency\nstrength to mixed individuals. They allow the filtering of irrelevant\ndependencies and re-enable using dependency-based masks by variation operators.\nWe verify the wdVIG potential on a large benchmark suite. For problems with\nnoise, the wdVIG masks can improve the optimizer's effectiveness. If all\ndependencies are relevant for the optimization, i.e., the problem is not\nnoised, the influence of wdVIG masks is similar to that of state-of-the-art\nstructures of this kind.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13949v1",
    "published_date": "2025-04-16 08:22:59 UTC",
    "updated_date": "2025-04-16 08:22:59 UTC"
  },
  {
    "arxiv_id": "2504.11844v1",
    "title": "Evaluating the Goal-Directedness of Large Language Models",
    "authors": [
      "Tom Everitt",
      "Cristina Garbacea",
      "Alexis Bellot",
      "Jonathan Richens",
      "Henry Papadatos",
      "Siméon Campos",
      "Rohin Shah"
    ],
    "abstract": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11844v1",
    "published_date": "2025-04-16 08:07:08 UTC",
    "updated_date": "2025-04-16 08:07:08 UTC"
  },
  {
    "arxiv_id": "2504.11837v1",
    "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations",
    "authors": [
      "Yue Zhao",
      "Qingqing Gu",
      "Xiaoyu Wang",
      "Teng Chen",
      "Zhonglin Jiang",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by CMCL",
    "pdf_url": "http://arxiv.org/pdf/2504.11837v1",
    "published_date": "2025-04-16 07:52:06 UTC",
    "updated_date": "2025-04-16 07:52:06 UTC"
  },
  {
    "arxiv_id": "2504.11829v2",
    "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation",
    "authors": [
      "Julia Kreutzer",
      "Eleftheria Briakou",
      "Sweta Agrawal",
      "Marzieh Fadaee",
      "Kocmi Tom"
    ],
    "abstract": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11829v2",
    "published_date": "2025-04-16 07:38:19 UTC",
    "updated_date": "2025-04-17 21:12:10 UTC"
  },
  {
    "arxiv_id": "2504.13210v1",
    "title": "Graphical Models for Decision-Making: Integrating Causality and Game Theory",
    "authors": [
      "Maarten C. Vonk",
      "Mauricio Gonzalez Soto",
      "Anna V. Kononova"
    ],
    "abstract": "Causality and game theory are two influential fields that contribute\nsignificantly to decision-making in various domains. Causality defines and\nmodels causal relationships in complex policy problems, while game theory\nprovides insights into strategic interactions among stakeholders with competing\ninterests. Integrating these frameworks has led to significant theoretical\nadvancements with the potential to improve decision-making processes. However,\npractical applications of these developments remain underexplored. To support\nefforts toward implementation, this paper clarifies key concepts in game theory\nand causality that are essential to their intersection, particularly within the\ncontext of probabilistic graphical models. By rigorously examining these\nconcepts and illustrating them with intuitive, consistent examples, we clarify\nthe required inputs for implementing these models, provide practitioners with\ninsights into their application and selection across different scenarios, and\nreference existing research that supports their implementation. We hope this\nwork encourages broader adoption of these models in real-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13210v1",
    "published_date": "2025-04-16 07:25:59 UTC",
    "updated_date": "2025-04-16 07:25:59 UTC"
  },
  {
    "arxiv_id": "2504.11820v1",
    "title": "Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting",
    "authors": [
      "Delong Suzhang",
      "Meng Yang"
    ],
    "abstract": "The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11820v1",
    "published_date": "2025-04-16 07:14:01 UTC",
    "updated_date": "2025-04-16 07:14:01 UTC"
  },
  {
    "arxiv_id": "2504.13948v2",
    "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images",
    "authors": [
      "Juan David Salazar Rodriguez",
      "Sam Conrad Joyce",
      "Julfendi"
    ],
    "abstract": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13948v2",
    "published_date": "2025-04-16 07:03:18 UTC",
    "updated_date": "2025-04-25 06:54:32 UTC"
  },
  {
    "arxiv_id": "2504.13947v1",
    "title": "From job titles to jawlines: Using context voids to study generative AI systems",
    "authors": [
      "Shahan Ali Memon",
      "Soham De",
      "Sungha Kang",
      "Riyan Mujtaba",
      "Bedoor AlShebli",
      "Katie Davis",
      "Jaime Snyder",
      "Jevin D. West"
    ],
    "abstract": "In this paper, we introduce a speculative design methodology for studying the\nbehavior of generative AI systems, framing design as a mode of inquiry. We\npropose bridging seemingly unrelated domains to generate intentional context\nvoids, using these tasks as probes to elicit AI model behavior. We demonstrate\nthis through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to\ngenerate headshots from professional Curricula Vitae (CVs). In contrast to\ntraditional ways, our approach assesses system behavior under conditions of\nradical uncertainty -- when forced to invent entire swaths of missing context\n-- revealing subtle stereotypes and value-laden assumptions. We qualitatively\nanalyze how the system interprets identity and competence markers from CVs,\ntranslating them into visual portraits despite the missing context (i.e.\nphysical descriptors). We show that within this context void, the AI system\ngenerates biased representations, potentially relying on stereotypical\nassociations or blatant hallucinations.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.13947v1",
    "published_date": "2025-04-16 06:51:12 UTC",
    "updated_date": "2025-04-16 06:51:12 UTC"
  },
  {
    "arxiv_id": "2504.11812v1",
    "title": "Learning Strategies in Particle Swarm Optimizer: A Critical Review and Performance Analysis",
    "authors": [
      "Dikshit Chauhan",
      "Shivani",
      "P. N. Suganthan"
    ],
    "abstract": "Nature has long inspired the development of swarm intelligence (SI), a key\nbranch of artificial intelligence that models collective behaviors observed in\nbiological systems for solving complex optimization problems. Particle swarm\noptimization (PSO) is widely adopted among SI algorithms due to its simplicity\nand efficiency. Despite numerous learning strategies proposed to enhance PSO's\nperformance in terms of convergence speed, robustness, and adaptability, no\ncomprehensive and systematic analysis of these strategies exists. We review and\nclassify various learning strategies to address this gap, assessing their\nimpact on optimization performance. Additionally, a comparative experimental\nevaluation is conducted to examine how these strategies influence PSO's search\ndynamics. Finally, we discuss open challenges and future directions,\nemphasizing the need for self-adaptive, intelligent PSO variants capable of\naddressing increasingly complex real-world problems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "53 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.11812v1",
    "published_date": "2025-04-16 06:50:02 UTC",
    "updated_date": "2025-04-16 06:50:02 UTC"
  },
  {
    "arxiv_id": "2504.21012v2",
    "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models",
    "authors": [
      "Makoto Sato"
    ],
    "abstract": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)-either fused together or presented separately-by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept-a form of conceptual fusion-current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.21012v2",
    "published_date": "2025-04-16 06:49:45 UTC",
    "updated_date": "2025-05-01 14:58:32 UTC"
  },
  {
    "arxiv_id": "2504.12360v1",
    "title": "A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version",
    "authors": [
      "Mieczysław A. Kłopotek",
      "Sławomir T. Wierzchoń",
      "Bartłomiej Starosta",
      "Dariusz Czerski",
      "Piotr Borkowski"
    ],
    "abstract": "This paper investigates the problem of Graph Spectral Clustering with\nnegative similarities, resulting from document embeddings different from the\ntraditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for\ncombinatorial Laplacians and normalized Laplacians are discussed. An\nexperimental investigation shows the advantages and disadvantages of 6\ndifferent solutions proposed in the literature and in this research. The\nresearch demonstrates that GloVe embeddings frequently cause failures of\nnormalized Laplacian based GSC due to negative similarities. Furthermore,\napplication of methods curing similarity negativity leads to accuracy\nimprovement for both combinatorial and normalized Laplacian based GSC. It also\nleads to applicability for GloVe embeddings of explanation methods developed\noriginally bythe authors for Term Vector Space embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "1 figure, 17 pages, this is an extended version of a paper accepted\n  for the 25th International Conference on Computational Science (ICCS), 7-9\n  July 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.12360v1",
    "published_date": "2025-04-16 06:03:02 UTC",
    "updated_date": "2025-04-16 06:03:02 UTC"
  },
  {
    "arxiv_id": "2504.11793v3",
    "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification",
    "authors": [
      "Yue Li",
      "Lihong Zhang"
    ],
    "abstract": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11793v3",
    "published_date": "2025-04-16 05:59:29 UTC",
    "updated_date": "2025-04-18 20:58:03 UTC"
  },
  {
    "arxiv_id": "2504.11792v1",
    "title": "Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records",
    "authors": [
      "Md Sultan Al Nahian",
      "Chris Delcher",
      "Daniel Harris",
      "Peter Akpunonu",
      "Ramakanth Kavuluru"
    ],
    "abstract": "The ability to predict drug overdose risk from a patient's medical records is\ncrucial for timely intervention and prevention. Traditional machine learning\nmodels have shown promise in analyzing longitudinal medical records for this\ntask. However, recent advancements in large language models (LLMs) offer an\nopportunity to enhance prediction performance by leveraging their ability to\nprocess long textual data and their inherent prior knowledge across diverse\ntasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in\npredicting drug overdose events using patients' longitudinal insurance claims\nrecords. We evaluate its performance in both fine-tuned and zero-shot settings,\ncomparing them to strong traditional machine learning methods as baselines. Our\nresults show that LLMs not only outperform traditional models in certain\nsettings but can also predict overdose risk in a zero-shot setting without\ntask-specific training. These findings highlight the potential of LLMs in\nclinical decision support, particularly for drug overdose risk prediction.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11792v1",
    "published_date": "2025-04-16 05:52:22 UTC",
    "updated_date": "2025-04-16 05:52:22 UTC"
  },
  {
    "arxiv_id": "2504.11788v1",
    "title": "Enhancing Web Agents with Explicit Rollback Mechanisms",
    "authors": [
      "Zhisong Zhang",
      "Tianqing Fang",
      "Kaixin Ma",
      "Wenhao Yu",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "abstract": "With recent advancements in large language models, web agents have been\ngreatly improved. However, dealing with complex and dynamic web environments\nrequires more advanced planning and search abilities. Previous studies usually\nadopt a greedy one-way search strategy, which may struggle to recover from\nerroneous states. In this work, we enhance web agents with an explicit rollback\nmechanism, enabling the agent to revert back to a previous state in its\nnavigation trajectory. This mechanism gives the model the flexibility to\ndirectly control the search process, leading to an effective and efficient web\nnavigation method. We conduct experiments on two live web navigation benchmarks\nwith zero-shot and fine-tuning settings. The results demonstrate the\neffectiveness of our proposed approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11788v1",
    "published_date": "2025-04-16 05:41:20 UTC",
    "updated_date": "2025-04-16 05:41:20 UTC"
  },
  {
    "arxiv_id": "2504.11781v1",
    "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model",
    "authors": [
      "Guanchun Wang",
      "Xiangrong Zhang",
      "Yifei Zhang",
      "Zelin Peng",
      "Tianyang Zhang",
      "Xu Tang",
      "Licheng Jiao"
    ],
    "abstract": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.11781v1",
    "published_date": "2025-04-16 05:33:42 UTC",
    "updated_date": "2025-04-16 05:33:42 UTC"
  },
  {
    "arxiv_id": "2504.11780v1",
    "title": "Agile Retrospectives: What went well? What didn't go well? What should we do?",
    "authors": [
      "Maria Spichkova",
      "Hina Lee",
      "Kevin Iwan",
      "Madeleine Zwart",
      "Yuwon Yoon",
      "Xiaohan Qin"
    ],
    "abstract": "In Agile/Scrum software development, the idea of retrospective meetings\n(retros) is one of the core elements of the project process. In this paper, we\npresent our work in progress focusing on two aspects: analysis of potential\nusage of generative AI for information interaction within retrospective\nmeetings, and visualisation of retros' information to software development\nteams. We also present our prototype tool RetroAI++, focusing on retros-related\nfunctionalities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Preprint. Accepted to the 20th International Conference on Evaluation\n  of Novel Approaches to Software Engineering (ENASE 2025). Final version to be\n  published by SCITEPRESS, http://www.scitepress.org",
    "pdf_url": "http://arxiv.org/pdf/2504.11780v1",
    "published_date": "2025-04-16 05:33:35 UTC",
    "updated_date": "2025-04-16 05:33:35 UTC"
  },
  {
    "arxiv_id": "2504.11774v1",
    "title": "PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility",
    "authors": [
      "Keke Gai",
      "Ziyue Shen",
      "Jing Yu",
      "Liehuang Zhu",
      "Qi Wu"
    ],
    "abstract": "With the growing demand for protecting the intellectual property (IP) of\ntext-to-image diffusion models, we propose PCDiff -- a proactive access control\nframework that redefines model authorization by regulating generation quality.\nAt its core, PCDIFF integrates a trainable fuser module and hierarchical\nauthentication layers into the decoder architecture, ensuring that only users\nwith valid encrypted credentials can generate high-fidelity images. In the\nabsence of valid keys, the system deliberately degrades output quality,\neffectively preventing unauthorized exploitation.Importantly, while the primary\nmechanism enforces active access control through architectural intervention,\nits decoupled design retains compatibility with existing watermarking\ntechniques. This satisfies the need of model owners to actively control model\nownership while preserving the traceability capabilities provided by\ntraditional watermarking approaches.Extensive experimental evaluations confirm\na strong dependency between credential verification and image quality across\nvarious attack scenarios. Moreover, when combined with typical post-processing\noperations, PCDIFF demonstrates powerful performance alongside conventional\nwatermarking methods. This work shifts the paradigm from passive detection to\nproactive enforcement of authorization, laying the groundwork for IP management\nof diffusion models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11774v1",
    "published_date": "2025-04-16 05:28:50 UTC",
    "updated_date": "2025-04-16 05:28:50 UTC"
  },
  {
    "arxiv_id": "2504.13209v1",
    "title": "On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks",
    "authors": [
      "Ting Bi",
      "Chenghang Ye",
      "Zheyu Yang",
      "Ziyi Zhou",
      "Cui Tang",
      "Jun Zhang",
      "Zui Tao",
      "Kailong Wang",
      "Liting Zhou",
      "Yang Yang",
      "Tianlong Yu"
    ],
    "abstract": "Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are\nrapidly evolving, providing unprecedented capabilities for human-computer\ninteraction. However, their integration introduces a new attack surface for\nsocial engineering. In this paper, we systematically investigate the\nfeasibility of orchestrating AR-driven Social Engineering attacks using\nMultimodal LLM for the first time, via our proposed SEAR framework, which\noperates through three key phases: (1) AR-based social context synthesis, which\nfuses Multimodal inputs (visual, auditory and environmental cues); (2)\nrole-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically\nretrieves and integrates contextual data while preserving character\ndifferentiation; and (3) ReInteract social engineering agents, which execute\nadaptive multiphase attack strategies through inference interaction loops. To\nverify SEAR, we conducted an IRB-approved study with 60 participants in three\nexperimental configurations (unassisted, AR+LLM, and full SEAR pipeline)\ncompiling a new dataset of 180 annotated conversations in simulated social\nscenarios. Our results show that SEAR is highly effective at eliciting\nhigh-risk behaviors (e.g., 93.3% of participants susceptible to email\nphishing). The framework was particularly effective in building trust, with 85%\nof targets willing to accept an attacker's call after an interaction. Also, we\nidentified notable limitations such as ``occasionally artificial'' due to\nperceived authenticity gaps. This work provides proof-of-concept for AR-LLM\ndriven social engineering attacks and insights for developing defensive\ncountermeasures against next-generation augmented reality threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13209v1",
    "published_date": "2025-04-16 05:18:36 UTC",
    "updated_date": "2025-04-16 05:18:36 UTC"
  },
  {
    "arxiv_id": "2504.11765v1",
    "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs",
    "authors": [
      "Hyungwoo Lee",
      "Kihyun Kim",
      "Jinwoo Kim",
      "Jungmin So",
      "Myung-Hoon Cha",
      "Hong-Yeon Kim",
      "James J. Kim",
      "Youngjae Kim"
    ],
    "abstract": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11765v1",
    "published_date": "2025-04-16 04:59:18 UTC",
    "updated_date": "2025-04-16 04:59:18 UTC"
  },
  {
    "arxiv_id": "2504.13208v1",
    "title": "Intelligent road crack detection and analysis based on improved YOLOv8",
    "authors": [
      "Haomin Zuo",
      "Zhengyang Li",
      "Jiangchuan Gong",
      "Zhen Tian"
    ],
    "abstract": "As urbanization speeds up and traffic flow increases, the issue of pavement\ndistress is becoming increasingly pronounced, posing a severe threat to road\nsafety and service life. Traditional methods of pothole detection rely on\nmanual inspection, which is not only inefficient but also costly. This paper\nproposes an intelligent road crack detection and analysis system, based on the\nenhanced YOLOv8 deep learning framework. A target segmentation model has been\ndeveloped through the training of 4029 images, capable of efficiently and\naccurately recognizing and segmenting crack regions in roads. The model also\nanalyzes the segmented regions to precisely calculate the maximum and minimum\nwidths of cracks and their exact locations. Experimental results indicate that\nthe incorporation of ECA and CBAM attention mechanisms substantially enhances\nthe model's detection accuracy and efficiency, offering a novel solution for\nroad maintenance and safety monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE - ICAACE 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.13208v1",
    "published_date": "2025-04-16 04:50:28 UTC",
    "updated_date": "2025-04-16 04:50:28 UTC"
  },
  {
    "arxiv_id": "2504.11754v1",
    "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
    "authors": [
      "Zihui Zhang",
      "Yafei Yang",
      "Hongtao Wen",
      "Bo Yang"
    ],
    "abstract": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS",
    "pdf_url": "http://arxiv.org/pdf/2504.11754v1",
    "published_date": "2025-04-16 04:13:53 UTC",
    "updated_date": "2025-04-16 04:13:53 UTC"
  },
  {
    "arxiv_id": "2504.12359v1",
    "title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models",
    "authors": [
      "Yuanbo Tang",
      "Yan Tang",
      "Naifan Zhang",
      "Meixuan Chen",
      "Yang Li"
    ],
    "abstract": "Mixture-of-Experts based large language models (MoE LLMs) have shown\nsignificant promise in multitask adaptability by dynamically routing inputs to\nspecialized experts. Despite their success, the collaborative mechanisms among\nexperts are still not well understood, limiting both the interpretability and\noptimization of these models. In this paper, we focus on two critical issues:\n(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs\nthrough expert pruning. To address the first issue, we propose a hierarchical\nsparse dictionary learning (HSDL) method that uncovers the collaboration\npatterns among experts. For the second issue, we introduce the\nContribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes\nlow-contribution experts. Our extensive experiments demonstrate that expert\ncollaboration patterns are closely linked to specific input types and exhibit\nsemantic significance across various tasks. Moreover, pruning experiments show\nthat our approach improves overall performance by 2.5\\% on average,\noutperforming existing methods. These findings offer valuable insights into\nenhancing the efficiency and interpretability of MoE LLMs, offering a clearer\nunderstanding of expert interactions and improving model optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12359v1",
    "published_date": "2025-04-16 04:06:15 UTC",
    "updated_date": "2025-04-16 04:06:15 UTC"
  },
  {
    "arxiv_id": "2504.11750v1",
    "title": "Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures",
    "authors": [
      "Prabhu Vellaisamy",
      "Thomas Labonte",
      "Sourav Chakraborty",
      "Matt Turner",
      "Samantika Sury",
      "John Paul Shen"
    ],
    "abstract": "Large language model (LLM)-based inference workloads increasingly dominate\ndata center costs and resource utilization. Therefore, understanding the\ninference workload characteristics on evolving CPU-GPU coupled architectures is\ncrucial for optimization. This paper presents an in-depth analysis of LLM\ninference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled\n(GH200) systems. We analyze performance dynamics using fine-grained\noperator-to-kernel trace analysis, facilitated by our novel profiler SKIP and\nmetrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that\nclosely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)\nsystems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for\nLlama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound\nup to 4x larger batch sizes than LC systems. In this extended CPU-bound region,\nwe identify the performance characteristics of the Grace CPU as a key factor\ncontributing to higher inference latency at low batch sizes on GH200. We\ndemonstrate that TKLQT accurately identifies this CPU/GPU-bound transition\npoint. Based on this analysis, we further show that kernel fusion offers\nsignificant potential to mitigate GH200's low-batch latency bottleneck by\nreducing kernel launch overhead. This detailed kernel-level characterization\nprovides critical insights for optimizing diverse CPU-GPU coupling strategies.\nThis work is an initial effort, and we plan to explore other major AI/DL\nworkloads that demand different degrees of CPU-GPU heterogeneous architectures.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted for ISPASS 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.11750v1",
    "published_date": "2025-04-16 04:02:39 UTC",
    "updated_date": "2025-04-16 04:02:39 UTC"
  },
  {
    "arxiv_id": "2504.15296v1",
    "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling",
    "authors": [
      "Yihong Jin",
      "Ze Yang"
    ],
    "abstract": "The rapid expansion of AI inference services in the cloud necessitates a\nrobust scalability solution to manage dynamic workloads and maintain high\nperformance. This study proposes a comprehensive scalability optimization\nframework for cloud AI inference services, focusing on real-time load balancing\nand autoscaling strategies. The proposed model is a hybrid approach that\ncombines reinforcement learning for adaptive load distribution and deep neural\nnetworks for accurate demand forecasting. This multi-layered approach enables\nthe system to anticipate workload fluctuations and proactively adjust\nresources, ensuring maximum resource utilisation and minimising latency.\nFurthermore, the incorporation of a decentralised decision-making process\nwithin the model serves to enhance fault tolerance and reduce response time in\nscaling operations. Experimental results demonstrate that the proposed model\nenhances load balancing efficiency by 35\\ and reduces response delay by 28\\,\nthereby exhibiting a substantial optimization effect in comparison with\nconventional scalability solutions.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "F.2.2; I.2.8"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted to BDICN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.15296v1",
    "published_date": "2025-04-16 04:00:04 UTC",
    "updated_date": "2025-04-16 04:00:04 UTC"
  },
  {
    "arxiv_id": "2504.12358v1",
    "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance",
    "authors": [
      "Aditi Verma",
      "Elizabeth Williams"
    ],
    "abstract": "AI models are rapidly becoming embedded in all aspects of nuclear energy\nresearch and work but the safety, security, and safeguards consequences of this\nembedding are not well understood. In this paper, we call for the creation of\nan anticipatory system of governance for AI in the nuclear sector as well as\nthe creation of a global AI observatory as a means for operationalizing\nanticipatory governance. The paper explores the contours of the nuclear AI\nobservatory and an anticipatory system of governance by drawing on work in\nscience and technology studies, public policy, and foresight studies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at the Sociotechnical AI Governance Workshop at CHI 2025,\n  Yokohama",
    "pdf_url": "http://arxiv.org/pdf/2504.12358v1",
    "published_date": "2025-04-16 03:43:15 UTC",
    "updated_date": "2025-04-16 03:43:15 UTC"
  },
  {
    "arxiv_id": "2504.11741v1",
    "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?",
    "authors": [
      "Yiyou Sun",
      "Georgia Zhou",
      "Hao Wang",
      "Dacheng Li",
      "Nouha Dziri",
      "Dawn Song"
    ],
    "abstract": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11741v1",
    "published_date": "2025-04-16 03:39:38 UTC",
    "updated_date": "2025-04-16 03:39:38 UTC"
  },
  {
    "arxiv_id": "2504.13945v4",
    "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models",
    "authors": [
      "Zhanglin Wu",
      "Tengfei Song",
      "Ning Xie",
      "Mengli Zhu",
      "Weidong Zhang",
      "Shuang Wu",
      "Pengfei Li",
      "Chong Li",
      "Junhao Zhu",
      "Hao Yang",
      "Shiliang Sun"
    ],
    "abstract": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2504.13945v4",
    "published_date": "2025-04-16 03:08:57 UTC",
    "updated_date": "2025-05-19 06:35:05 UTC"
  },
  {
    "arxiv_id": "2504.11726v1",
    "title": "Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception",
    "authors": [
      "Yunzhe Li",
      "Facheng Hu",
      "Hongzi Zhu",
      "Shifan Zhang",
      "Liang Zhang",
      "Shan Chang",
      "Minyi Guo"
    ],
    "abstract": "Inertial measurement units (IMUs), have been prevalently used in a wide range\nof mobile perception applications such as activity recognition and user\nauthentication, where a large amount of labelled data are normally required to\ntrain a satisfactory model. However, it is difficult to label micro-activities\nin massive IMU data due to the hardness of understanding raw IMU data and the\nlack of ground truth. In this paper, we propose a novel fine-grained user\nperception approach, called Saga, which only needs a small amount of labelled\nIMU data to achieve stunning user perception accuracy. The core idea of Saga is\nto first pre-train a backbone feature extraction model, utilizing the rich\nsemantic information of different levels embedded in the massive unlabelled IMU\ndata. Meanwhile, for a specific downstream user perception application,\nBayesian Optimization is employed to determine the optimal weights for\npre-training tasks involving different semantic levels. We implement Saga on\nfive typical mobile phones and evaluate Saga on three typical tasks on three\nIMU datasets. Results show that when only using about 100 training samples per\nclass, Saga can achieve over 90% accuracy of the full-fledged model trained on\nover ten thousands training samples with no additional system overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2025 IEEE 45th International Conference on Distributed Computing\n  Systems (ICDCS)",
    "pdf_url": "http://arxiv.org/pdf/2504.11726v1",
    "published_date": "2025-04-16 03:03:42 UTC",
    "updated_date": "2025-04-16 03:03:42 UTC"
  },
  {
    "arxiv_id": "2504.13944v1",
    "title": "Mixer Metaphors: audio interfaces for non-musical applications",
    "authors": [
      "Tace McNamara",
      "Jon McCormack",
      "Maria Teresa Llano"
    ],
    "abstract": "The NIME conference traditionally focuses on interfaces for music and musical\nexpression. In this paper we reverse this tradition to ask, can interfaces\ndeveloped for music be successfully appropriated to non-musical applications?\nTo help answer this question we designed and developed a new device, which uses\ninterface metaphors borrowed from analogue synthesisers and audio mixing to\nphysically control the intangible aspects of a Large Language Model. We\ncompared two versions of the device, with and without the audio-inspired\naugmentations, with a group of artists who used each version over a one week\nperiod. Our results show that the use of audio-like controls afforded more\nimmediate, direct and embodied control over the LLM, allowing users to\ncreatively experiment and play with the device over its non-mixer counterpart.\nOur project demonstrates how cross-sensory metaphors can support creative\nthinking and embodied practice when designing new technological interfaces.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SD",
      "H.5.2; J.5; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "9 Pages",
    "pdf_url": "http://arxiv.org/pdf/2504.13944v1",
    "published_date": "2025-04-16 02:51:08 UTC",
    "updated_date": "2025-04-16 02:51:08 UTC"
  },
  {
    "arxiv_id": "2504.12355v1",
    "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media",
    "authors": [
      "Muhammad Ahmad",
      "Muhammad Waqas",
      "ldar Batyrshin",
      "Grigori Sidorov"
    ],
    "abstract": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12355v1",
    "published_date": "2025-04-16 02:33:19 UTC",
    "updated_date": "2025-04-16 02:33:19 UTC"
  },
  {
    "arxiv_id": "2504.11713v2",
    "title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching",
    "authors": [
      "Aaron Havens",
      "Benjamin Kurt Miller",
      "Bing Yan",
      "Carles Domingo-Enrich",
      "Anuroop Sriram",
      "Brandon Wood",
      "Daniel Levine",
      "Bin Hu",
      "Brandon Amos",
      "Brian Karrer",
      "Xiang Fu",
      "Guan-Horng Liu",
      "Ricky T. Q. Chen"
    ],
    "abstract": "We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11713v2",
    "published_date": "2025-04-16 02:20:06 UTC",
    "updated_date": "2025-04-18 15:57:13 UTC"
  },
  {
    "arxiv_id": "2504.11711v2",
    "title": "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs",
    "authors": [
      "Haonan Li",
      "Hang Zhang",
      "Kexin Pei",
      "Zhiyun Qian"
    ],
    "abstract": "Static analysis is a cornerstone for software vulnerability detection, yet it\noften struggles with the classic precision-scalability trade-off. In practice,\nsuch tools often produce high false positive rates, particularly in large\ncodebases like the Linux kernel. This imprecision can arise from simplified\nvulnerability modeling and over-approximation of path and data constraints.\nWhile large language models (LLMs) show promise in code understanding, their\nnaive application to program analysis yields unreliable results due to inherent\nreasoning limitations. We introduce BugLens, a post-refinement framework that\nsignificantly improves static analysis precision. BugLens guides an LLM to\nfollow traditional analysis steps by assessing buggy code patterns for security\nimpact and validating the constraints associated with static warnings.\nEvaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10\n(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing\nfalse positives and revealing four previously unreported vulnerabilities. Our\nresults suggest that a structured LLM-based workflow can meaningfully enhance\nthe effectiveness of static analysis tools.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11711v2",
    "published_date": "2025-04-16 02:17:06 UTC",
    "updated_date": "2025-04-17 02:28:35 UTC"
  },
  {
    "arxiv_id": "2504.11707v1",
    "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset",
    "authors": [
      "Muhammad Shahid Muneer",
      "Simon S. Woo"
    ],
    "abstract": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Short Paper The Web Conference",
    "pdf_url": "http://arxiv.org/pdf/2504.11707v1",
    "published_date": "2025-04-16 02:10:42 UTC",
    "updated_date": "2025-04-16 02:10:42 UTC"
  },
  {
    "arxiv_id": "2504.11704v1",
    "title": "A Library of LLM Intrinsics for Retrieval-Augmented Generation",
    "authors": [
      "Marina Danilevsky",
      "Kristjan Greenewald",
      "Chulaka Gunasekara",
      "Maeda Hanafi",
      "Lihong He",
      "Yannis Katsis",
      "Krishnateja Killamsetty",
      "Yatin Nandwani",
      "Lucian Popa",
      "Dinesh Raghu",
      "Frederick Reiss",
      "Vraj Shah",
      "Khoi-Nguyen Tran",
      "Huaiyu Zhu",
      "Luis Lastras"
    ],
    "abstract": "In the developer community for large language models (LLMs), there is not yet\na clean pattern analogous to a software library, to support very large scale\ncollaboration. Even for the commonplace use case of Retrieval-Augmented\nGeneration (RAG), it is not currently possible to write a RAG application\nagainst a well-defined set of APIs that are agreed upon by different LLM\nproviders. Inspired by the idea of compiler intrinsics, we propose some\nelements of such a concept through introducing a library of LLM Intrinsics for\nRAG. An LLM intrinsic is defined as a capability that can be invoked through a\nwell-defined API that is reasonably stable and independent of how the LLM\nintrinsic itself is implemented. The intrinsics in our library are released as\nLoRA adapters on HuggingFace, and through a software interface with clear\nstructured input/output characteristics on top of vLLM as an inference\nplatform, accompanied in both places with documentation and code. This article\ndescribes the intended usage, training details, and evaluations for each\nintrinsic, as well as compositions of multiple intrinsics.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11704v1",
    "published_date": "2025-04-16 02:02:22 UTC",
    "updated_date": "2025-04-16 02:02:22 UTC"
  },
  {
    "arxiv_id": "2504.11703v1",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "authors": [
      "Tianneng Shi",
      "Jingxuan He",
      "Zhun Wang",
      "Linyu Wu",
      "Hongwei Li",
      "Wenbo Guo",
      "Dawn Song"
    ],
    "abstract": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11703v1",
    "published_date": "2025-04-16 01:58:40 UTC",
    "updated_date": "2025-04-16 01:58:40 UTC"
  },
  {
    "arxiv_id": "2504.11686v1",
    "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics",
    "authors": [
      "Yiran He",
      "Yun Cao",
      "Bowen Yang",
      "Zeyu Zhang"
    ],
    "abstract": "The rapid development of generative AI facilitates content creation and makes\nimage manipulation easier and more difficult to detect. While multimodal Large\nLanguage Models (LLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating AI-generated Content (AIGC) and struggle to\ncomprehend local forgery details. In this work, we investigate the application\nof multimodal LLMs in forgery detection. We propose a framework capable of\nevaluating image authenticity, localizing tampered regions, providing evidence,\nand tracing generation methods based on semantic tampering clues. Our method\ndemonstrates that the potential of LLMs in forgery analysis can be effectively\nunlocked through meticulous prompt engineering and the application of few-shot\nlearning techniques. We conduct qualitative and quantitative experiments and\nshow that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in\nLaMa, which is competitive with state-of-the-art AIGC detection methods. We\nfurther discuss the limitations of multimodal LLMs in such tasks and propose\npotential improvements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 11 figures, 13IHMMSec2025",
    "pdf_url": "http://arxiv.org/pdf/2504.11686v1",
    "published_date": "2025-04-16 01:02:46 UTC",
    "updated_date": "2025-04-16 01:02:46 UTC"
  },
  {
    "arxiv_id": "2504.13942v1",
    "title": "Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices",
    "authors": [
      "Sukanth Kalivarathan",
      "Muhmmad Abrar Raja Mohamed",
      "Aswathy Ravikumar",
      "S Harini"
    ],
    "abstract": "This paper introduces Intelligence of Things (INOT), a novel spatial\ncontext-aware control system that enhances smart home automation through\nintuitive spatial reasoning. Current smart home systems largely rely on\ndevice-specific identifiers, limiting user interaction to explicit naming\nconventions rather than natural spatial references. INOT addresses this\nlimitation through a modular architecture that integrates Vision Language\nModels with IoT control systems to enable natural language commands with\nspatial context (e.g., \"turn on the light near the window\"). The system\ncomprises key components including an Onboarding Inference Engine, Zero-Shot\nDevice Detection, Spatial Topology Inference, and Intent-Based Command\nSynthesis. A comprehensive user study with 15 participants demonstrated INOT's\nsignificant advantages over conventional systems like Google Home Assistant,\nwith users reporting reduced cognitive workload (NASA-TLX scores decreased by\nan average of 13.17 points), higher ease-of-use ratings, and stronger\npreference (14 out of 15 participants). By eliminating the need to memorize\ndevice identifiers and enabling context-aware spatial commands, INOT represents\na significant advancement in creating more intuitive and accessible smart home\ncontrol systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "16 pages, 8 Figures",
    "pdf_url": "http://arxiv.org/pdf/2504.13942v1",
    "published_date": "2025-04-16 00:45:05 UTC",
    "updated_date": "2025-04-16 00:45:05 UTC"
  },
  {
    "arxiv_id": "2504.11671v1",
    "title": "Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation",
    "authors": [
      "Ji Ma"
    ],
    "abstract": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract ``vectors of variable\nvariations'' (e.g., ``male'' to ``female'') from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11671v1",
    "published_date": "2025-04-16 00:02:28 UTC",
    "updated_date": "2025-04-16 00:02:28 UTC"
  }
]
{
  "date": "2025-07-18",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡çˆ†å‘å¼å¢é•¿ï¼ˆ110ç¯‡ï¼‰ï¼Œé‡ç‚¹é›†ä¸­åœ¨ **Agentic AIï¼ˆæ™ºèƒ½ä½“å·¥ä½œæµï¼‰** çš„è½åœ°ä¸è‡ªæˆ‘ä¼˜åŒ–ã€**æ¨ç†å¤§æ¨¡å‹ï¼ˆReasoning LLMsï¼‰** çš„å·¥ç¨‹åŒ–ç¼ºé™·ä¿®è¡¥ï¼ˆHuman-in-the-loopï¼‰ã€ä»¥åŠ **AI for Science** é¢†åŸŸçš„é‡ç£…æ•°æ®é›†å‘å¸ƒï¼ˆå¦‚ç™¾ä¸‡çº§çº³ç±³ææ–™-è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®ï¼‰ã€‚æ­¤å¤–ï¼Œå…³äº **KANs (Kolmogorov Arnold Networks)** çš„å®è¯åˆ†æå†·é™åœ°æŒ‡å‡ºäº†å…¶åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šçš„çŸ­æ¿ï¼Œå€¼å¾—å…³æ³¨ã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ·±åº¦è§£è¯»ã€‚\n\n---\n\n### ğŸš€ æ™ºèƒ½ä½“ä¸è‡ªåŠ¨å·¥ä½œæµ (Agentic Workflows)\n\nä»Šå¤©æœ‰å¤šç¯‡è®ºæ–‡æ¢è®¨å¦‚ä½•é€šè¿‡ Agent å°† LLM çš„èƒ½åŠ›è½¬åŒ–ä¸ºå®é™…çš„ç”Ÿäº§åŠ›ï¼Œç‰¹åˆ«æ˜¯ç§‘å­¦å¯è§†åŒ–å’Œè¯ç‰©å‘ç°é¢†åŸŸã€‚\n\n**1. VizGenieï¼šé¢å‘ä¸‹ä¸€ä»£ç§‘å­¦å¯è§†åŒ–çš„è‡ªæˆ‘å®Œå–„ã€é¢†åŸŸæ„ŸçŸ¥å·¥ä½œæµ**\n**VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization**\nè¿™æ˜¯ä¸€ç¯‡ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡ç« ã€‚ä½œè€…æå‡ºäº† VizGenieï¼Œä¸€ä¸ªåˆ©ç”¨ LLM ç¼–æ’ç§‘å­¦å¯è§†åŒ–çš„æ¡†æ¶ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå®ƒä¸ä»…ä»…æ˜¯è°ƒç”¨å·¥å…·ï¼Œè¿˜èƒ½é€šè¿‡ LLM **è‡ªåŠ¨ç”Ÿæˆæ–°çš„å¯è§†åŒ–è„šæœ¬ï¼ˆå¦‚ VTK Python ä»£ç ï¼‰** æ¥æ‰©å±•èƒ½åŠ›ï¼Œå¹¶ä¸”ç”Ÿæˆçš„ä»£ç ä¼šç»è¿‡åç«¯è‡ªåŠ¨éªŒè¯ã€‚\n*   **äº®ç‚¹**ï¼šæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚â€œå¯è§†åŒ–å¤´éª¨â€ï¼‰ï¼Œé€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹æ¥ç†è§£ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ã€‚ç³»ç»Ÿå…·æœ‰**è‡ªæˆ‘å®Œå–„ï¼ˆSelf-Refiningï¼‰** èƒ½åŠ›ï¼Œéšç€äº¤äº’å¢åŠ ï¼Œå…¶é²æ£’æ€§ä¸æ–­å¢å¼ºã€‚\n\n**2. Nexus Architectï¼šé€šè¿‡è‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆå®ç°çš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¨ç†**\n**Adaptive Multi-Agent Reasoning via Automated Workflow Generation**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰å®¹æ˜“è¿‡æ‹Ÿåˆä¸”éš¾ä»¥æ³›åŒ–ã€‚Nexus Architect å¯ä»¥æ ¹æ®ç”¨æˆ·æç¤ºå’Œå°‘é‡ç¤ºä¾‹ï¼Œ**è‡ªä¸»ç”Ÿæˆ** å®šåˆ¶çš„æ¨ç†å·¥ä½œæµï¼ˆé€‰æ‹©ç­–ç•¥ã€å·¥å…·é›†æˆç­‰ï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨é€»è¾‘é—®é¢˜ä¸Šï¼Œå…¶é€šè¿‡ç‡æ¯” DeepSeek-R1 å’Œ Claude Sonnet 4 é«˜å‡ºè¿‘ 2.5 å€ã€‚\n\n**3. Tippy çš„æŠ€æœ¯å®ç°ï¼šè¯ç‰©å‘ç°å®éªŒå®¤è‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“æ¶æ„**\n**Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯¦ç»†ä»‹ç»äº† Tippy ç³»ç»Ÿçš„æŠ€æœ¯å®ç°ï¼ŒåŒ…å«5ä¸ªä¸“ç”¨ Agentï¼ˆç›‘ç£ã€åˆ†å­ã€å®éªŒå®¤ã€åˆ†æã€æŠ¥å‘Šï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäº Kubernetes å’Œå¾®æœåŠ¡çš„ç”Ÿäº§çº§ç³»ç»Ÿï¼Œå±•ç¤ºäº† Agent å¦‚ä½•é€šè¿‡æ ‡å‡†åè®®ï¼ˆMCPï¼‰æ§åˆ¶å®éªŒå®¤ç¡¬ä»¶ã€‚\n\n**4. ä»£ç†ç½‘ç»œåè®® (ANP) æŠ€æœ¯ç™½çš®ä¹¦**\n**Agent Network Protocol Technical White Paper**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ›´åƒæ˜¯ä¸€ä»½åŸºç¡€è®¾æ–½ææ¡ˆã€‚ä½œè€…æå‡ºäº† **Agent Network Protocol (ANP)**ï¼Œæ—¨åœ¨è§£å†³ Agent ä¹‹é—´çš„èº«ä»½è®¤è¯ã€åŠ¨æ€åå•†å’Œèƒ½åŠ›å‘ç°é—®é¢˜ï¼Œè¯•å›¾æ„å»º Agent äº’è”çš„â€œäº’è”ç½‘åè®®â€ã€‚\n\n---\n\n### ğŸ§  æ¨ç†æ¨¡å‹çš„å·¥ç¨‹åŒ–ä¸ä¼˜åŒ–\n\n**5. å¿«é€Ÿå¤±è´¥æˆ–æ±‚åŠ©ï¼šé€šè¿‡äººæœºååŒç¼“è§£æ¨ç† LLM çš„ç¼ºé™·**\n**Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-loop Systems Engineering**\n*   **æ ¸å¿ƒè§‚ç‚¹**ï¼šæ¨ç†æ¨¡å‹ï¼ˆReasoning LLMsï¼‰è™½ç„¶å¼ºå¤§ä½†ä»ä¼šçŠ¯é”™ï¼Œä¸”å»¶è¿Ÿé«˜ã€‚\n*   **æ–¹æ³•**ï¼šæå‡º \"Fail Fast, or Ask\" ç³»ç»Ÿã€‚å‰ç«¯ä½¿ç”¨éæ¨ç†æ¨¡å‹å¤„ç†ç®€å•é—®é¢˜ï¼Œé‡åˆ°ä¸ç¡®å®šæ—¶**å¿«é€Ÿå¤±è´¥**å¹¶è½¬äº¤ç»™äººç±»ä¸“å®¶ï¼Œè€Œä¸æ˜¯äº¤ç»™æ˜‚è´µçš„æ¨ç†æ¨¡å‹ã€‚\n*   **å‘ç°**ï¼šé€šè¿‡æ¨ç†ç—•è¿¹çš„é•¿åº¦æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå¯ä»¥å°†é”™è¯¯ç‡ä» 3% é™è‡³ 1% ä»¥ä¸‹ï¼ŒåŒæ—¶å¤§å¹…é™ä½å»¶è¿Ÿå’Œæˆæœ¬ã€‚\n\n**6. CUDA-L1ï¼šé€šè¿‡å¯¹æ¯”å¼ºåŒ–å­¦ä¹ æ”¹è¿› CUDA ä¼˜åŒ–**\n**CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ– CUDA å†…æ ¸ä»£ç ã€‚\n*   **æƒŠäººç»“æœ**ï¼šåœ¨ A100 ä¸Šï¼Œç›¸æ¯”é»˜è®¤åŸºçº¿å¹³å‡åŠ é€Ÿ 3.12 å€ï¼Œæœ€é«˜åŠ é€Ÿ 120 å€ã€‚å®ƒè¯æ˜äº† RL å¯ä»¥åœ¨æ²¡æœ‰äººç±»ä¸“å®¶çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡çº¯ç²¹çš„åŠ é€Ÿå¥–åŠ±ä¿¡å·å‘ç° CUDA ä¼˜åŒ–ç­–ç•¥ã€‚\n\n**7. Solo Connectionï¼šTransformer çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯**\n**Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers**\n*   **æ–¹æ³•**ï¼šæå‡º Solo Connectionï¼Œä¸€ç§åœ¨**è§£ç å™¨å—çº§åˆ«**è°ƒæ•´è¡¨ç¤ºçš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯åƒ LoRA é‚£æ ·ä¿®æ”¹æƒé‡çŸ©é˜µã€‚\n*   **æ•ˆæœ**ï¼šæ¯” LoRA å‡å°‘äº† 59% çš„å¯è®­ç»ƒå‚æ•°ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚\n\n**8. KANs (Kolmogorov Arnold Networks) å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„å®è¯è§†è§’**\n**Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective**\n*   **æ ¸å¿ƒå‘ç°**ï¼šå¯¹æœ€è¿‘ç«çƒ­çš„ KANs æ³¼äº†ä¸€ç›†å†·æ°´ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ KANs åœ¨åŸå§‹ä¸å¹³è¡¡æ•°æ®ä¸Šè¡¨ç°å°šå¯ï¼Œä½†ä¼ ç»Ÿçš„é‡é‡‡æ ·ç­–ç•¥å’Œ Focal Loss ä¼š**æ˜¾è‘—é™ä½** KANs çš„æ€§èƒ½ã€‚ä¸” KANs è®¡ç®—æˆæœ¬æé«˜ï¼Œæ€§ä»·æ¯”ä¸å¦‚ MLPã€‚\n\n---\n\n### ğŸ”¬ AI for Science (ç”Ÿç‰©/ææ–™/åŒ»å­¦)\n\nä»Šå¤© AI åœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨éå¸¸ç¡¬æ ¸ï¼Œç‰¹åˆ«æ˜¯æ•°æ®å±‚é¢çš„çªç ´ã€‚\n\n**9. NanoPro-3Mï¼šç™¾ä¸‡çº§çº³ç±³ææ–™-è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®é›†åŠåŸºç¡€æ¨¡å‹**\n**A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº† **NanoPro-3M**ï¼ŒåŒ…å« 320 ä¸‡ä¸ªæ ·æœ¬å’Œ 3.7 ä¸‡ç§è›‹ç™½è´¨ï¼Œæ˜¯ç›®å‰æœ€å¤§çš„çº³ç±³ææ–™-è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®é›†ã€‚\n*   **æ¨¡å‹**ï¼šæå‡ºäº† NanoProFormer å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé¢„æµ‹äº²å’ŒåŠ›ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n**10. ProofCompassï¼šåˆ©ç”¨ LLM æŒ‡å¯¼å¢å¼ºä¸“ç”¨è¯æ˜å™¨**\n**ProofCompass: Enhancing Specialized Provers with LLM Guidance**\n*   **æ–¹æ³•**ï¼šæ··åˆæ–¹æ³•ã€‚åˆ©ç”¨é€šç”¨ LLM æä¾›è‡ªç„¶è¯­è¨€è¯æ˜ç­–ç•¥ï¼ŒæŒ‡å¯¼ä¸“ç”¨çš„æ•°å­¦è¯æ˜å™¨ï¼ˆå¦‚ DeepSeek-Proverï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ miniF2F åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°è¯•æ¬¡æ•°å‡å°‘äº† 25 å€ï¼Œä½†å‡†ç¡®ç‡æ›´é«˜ã€‚\n\n**11. ä¹³è…º MRI å¤šæ ‡ç­¾åˆ†å‰²ç½‘ç»œ**\n**BreastSegNet: Multi-label Segmentation of Breast MRI**\n*   **è´¡çŒ®**ï¼šæ¶µç›– 9 ç§è§£å‰–ç»“æ„ï¼ˆåŒ…æ‹¬å¿ƒè„ã€è‚è„ã€æ·‹å·´ç»“ç­‰ï¼‰çš„ä¹³è…º MRI åˆ†å‰²æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå¡«è¡¥äº†ä»¥å¾€åªå…³æ³¨è‚¿ç˜¤çš„ç©ºç™½ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ã€å¤šæ¨¡æ€ä¸åŸºå‡†æµ‹è¯•\n\n**12. Document Haystackï¼šé•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€æ–‡æ¡£ç†è§£åŸºå‡†**\n**Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark**\n*   **ç—›ç‚¹**ï¼šç°æœ‰çš„ VLM å¤„ç†é•¿æ–‡æ¡£ï¼ˆ5-200é¡µï¼‰èƒ½åŠ›å¦‚ä½•ï¼Ÿç¼ºä¹åŸºå‡†ã€‚\n*   **è´¡çŒ®**ï¼šå‘å¸ƒ Document Haystackï¼Œæµ‹è¯• VLM åœ¨é•¿æ–‡æ¡£ä¸­â€œå¤§æµ·æé’ˆâ€ï¼ˆæ£€ç´¢æ–‡æœ¬æˆ–å›¾åƒï¼‰çš„èƒ½åŠ›ã€‚\n\n**13. Manimatorï¼šå°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºè§†è§‰è§£é‡Š**\n**Manimator: Transforming Research Papers into Visual Explanations**\n*   **æœ‰è¶£çš„åº”ç”¨**ï¼šåˆ©ç”¨ LLM è¯»å–è®ºæ–‡ PDFï¼Œç”Ÿæˆ **Manim** (æ•°å­¦åŠ¨ç”»å¼•æ“) Python ä»£ç ï¼Œè‡ªåŠ¨ç”Ÿæˆè§£é‡Šæ€§åŠ¨ç”»è§†é¢‘ã€‚\n\n**14. WebGuardï¼šæ„å»º Web æ™ºèƒ½ä½“çš„é€šç”¨æŠ¤æ **\n**WebGuard: Building a Generalizable Guardrail for Web Agents**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹è‡ªä¸» Web Agent å¯èƒ½çš„è¯¯æ“ä½œï¼ˆå¦‚è¯¯åˆ æ•°æ®ã€éé¢„æœŸè´­ä¹°ï¼‰ï¼Œå‘å¸ƒäº†åŒ…å«è¿‘ 5000 ä¸ªåŠ¨ä½œçš„é£é™©è¯„ä¼°æ•°æ®é›† WebGuardï¼Œå¹¶è®­ç»ƒäº†æŠ¤æ æ¨¡å‹æ¥é¢„æµ‹åŠ¨ä½œåæœã€‚\n\n**15. VLA-Markï¼šå¤§å‹è§†è§‰è¯­è¨€å¯¹é½æ¨¡å‹çš„è·¨æ¨¡æ€æ°´å°**\n**VLA-Mark: A cross modal watermark for large vision-language alignment model**\n*   **å®‰å…¨**ï¼šä¸€ç§æ–°çš„æ°´å°æŠ€æœ¯ï¼Œåœ¨ä¿æŠ¤ç‰ˆæƒçš„åŒæ—¶ï¼Œç»´æŒäº†è§†è§‰å’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œèƒ½å¤ŸæŠµæŠ—åŒä¹‰è¯æ›¿æ¢ç­‰æ”»å‡»ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€ç¤¾ä¼šå½±å“ä¸äººæœºäº¤äº’\n\n**16. æˆ‘åˆšåˆšæµè§ˆçš„æ˜¯ LLM ç¼–å†™çš„ç½‘ç«™å—ï¼Ÿ**\n**Preprint: Poster: Did I Just Browse A Website Written by LLMs?**\n*   **ç°è±¡**ï¼šä¸ä»…æ˜¯æ–‡æœ¬ï¼Œæ•´ä¸ªç½‘ç«™å†…å®¹éƒ½ç”± LLM ç”Ÿæˆï¼ˆLLM-dominantï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§åŸºäºç«™ç‚¹çº§è€Œéå•é¡µçº§çš„æ£€æµ‹ç®¡é“ï¼Œå‘ç°æœç´¢ç»“æœä¸­ LLM ç”Ÿæˆçš„åƒåœ¾å†…å®¹ç«™ç‚¹æ­£åœ¨æ¿€å¢ã€‚\n\n**17. ä¼šè¯ AI çš„æ”¿æ²»è¯´æœæ æ†**\n**The Levers of Political Persuasion with Conversational AI**\n*   **æ ¸å¿ƒå‘ç°**ï¼šé€šè¿‡ 7.6 ä¸‡äººçš„å¤§è§„æ¨¡å®éªŒï¼Œå‘ç° AI çš„è¯´æœåŠ›ä¸»è¦æ¥è‡ª**åæœŸè®­ç»ƒï¼ˆPost-trainingï¼‰** å’Œ **æç¤ºå·¥ç¨‹ï¼ˆPromptingï¼‰**ï¼Œè€Œä¸æ˜¯æ¨¡å‹è§„æ¨¡æˆ–ä¸ªæ€§åŒ–ã€‚\n*   **è­¦ç¤º**ï¼šæé«˜è¯´æœåŠ›çš„æ‰‹æ®µå¾€å¾€ä¼šé™ä½äº‹å®å‡†ç¡®æ€§ã€‚\n\n**18. ä¿æŠ¤æ‰©æ•£æ¨¡å‹å…å—æ¶æ„å¾®è°ƒ**\n**GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention**\n*   **é˜²å¾¡**ï¼šä¸€ç§â€œå…ç–«â€æŠ€æœ¯ï¼Œé˜²æ­¢æ”»å‡»è€…é€šè¿‡å¾®è°ƒè®©æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ï¼ˆå¦‚è‰²æƒ…æš´æï¼‰ï¼ŒåŒæ—¶ä¿ç•™ç”Ÿæˆå®‰å…¨å†…å®¹çš„èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ“ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (ç²¾é€‰åˆ—è¡¨)\n\nä¸ºäº†ç¯‡å¹…ï¼Œä»¥ä¸‹è®ºæ–‡ç®€è¦åˆ—å‡ºï¼š\n\n*   **æ•°æ®åº“ä¸æ¶æ„**\n    *   **Schemora: schema matching via multi-stage recommendation...** (åŸºäº LLM çš„æ•°æ®åº“æ¨¡å¼åŒ¹é…ï¼Œæ€§èƒ½è¶…è¶Š SOTA)\n    *   **Text-to-SQL for Enterprise Data Analytics** (LinkedIn çš„ä¼ä¸šçº§ Text-to-SQL å®è·µç»éªŒ)\n    *   **Photonic Fabric Platform for AI Accelerators** (å…‰å­ç»“æ„å¹³å°ï¼Œè§£å†³ GPU å†…å­˜å¢™é—®é¢˜)\n\n*   **æ¨èç³»ç»Ÿ**\n    *   **DUALRec** (ç»“åˆ LSTM å’Œ LLM çš„æ··åˆç”µå½±æ¨èç³»ç»Ÿ)\n    *   **Point of Interest Recommendation: Pitfalls and Viable Solutions** (POI æ¨èç³»ç»Ÿçš„ç»¼è¿°ä¸æ‰¹åˆ¤)\n\n*   **å…¶ä»–åº”ç”¨**\n    *   **Glucose-ML** (10ä¸ªç³–å°¿ç—…çºµå‘æ•°æ®é›†é›†åˆï¼Œç”¨äºAIè¡€ç³–é¢„æµ‹)\n    *   **Food safety trends across Europe...** (åŸºäº 3.92 äº¿æ¡æ•°æ®çš„æ¬§æ´²é£Ÿå“å®‰å…¨æ•°æ®åº“ CHEFS)\n    *   **Political Leaning and Politicalness Classification of Texts** (æ–‡æœ¬æ”¿æ²»å€¾å‘åˆ†ç±»çš„åŸºå‡†æµ‹è¯•)\n\n*   **ç†è®ºä¸å¾®è§‚è§†è§’**\n    *   **The Recursive Coherence Principle** (æå‡ºé€’å½’ä¸€è‡´æ€§åŸåˆ™ï¼Œè®¤ä¸ºè¿™æ˜¯æ™ºèƒ½æ‰©å±•å’Œå¯¹é½çš„å½¢å¼çº¦æŸ)\n    *   **Language Models as Ontology Encoders** (åˆ©ç”¨åŒæ›²ç©ºé—´å‡ ä½•å»ºæ¨¡æ¥åšæœ¬ä½“åµŒå…¥)\n\n---\n**ç»“è¯­**ï¼š\nä»Šå¤©çš„ arXiv æ›´æ–°å±•ç¤ºäº† AI æ­£åœ¨ä»â€œèŠå¤©æœºå™¨äººâ€å‘â€œæ•°å­—åŠ³åŠ¨åŠ›â€è½¬å‹ã€‚æ— è®ºæ˜¯ **VizGenie** è‡ªåŠ¨å†™ä»£ç ç”»å›¾ï¼Œè¿˜æ˜¯ **Tippy** è‡ªåŠ¨åšå®éªŒï¼Œäº¦æˆ–æ˜¯ **WebGuard** ä¸ºè¿™äº›è‡ªåŠ¨ Agent åŠ è£…æŠ¤æ ï¼Œéƒ½é¢„ç¤ºç€ 2025 å¹´ä¸‹åŠå¹´å°†æ˜¯ Agentic AI çˆ†å‘çš„ä¸€å¹´ã€‚åŒæ—¶ï¼Œ**NanoPro-3M** è¿™æ ·çš„æ•°æ®é›†æé†’æˆ‘ä»¬ï¼ŒAI åœ¨å¾®è§‚ç§‘å­¦ä¸–ç•Œçš„æ¢ç´¢æ‰åˆšåˆšå¼€å§‹ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2507.21124v1",
      "title": "VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization",
      "title_zh": "VizGenieï¼šé¢å‘ä¸‹ä¸€ä»£ç§‘å­¦å¯è§†åŒ–çš„è‡ªå®Œå–„é¢†åŸŸæ„ŸçŸ¥å·¥ä½œæµ",
      "authors": [
        "Ayan Biswas",
        "Terece L. Turton",
        "Nishath Rajiv Ranasinghe",
        "Shawn Jones",
        "Bradley Love",
        "William Jones",
        "Aric Hagberg",
        "Han-Wei Shen",
        "Nathan DeBardeleben",
        "Earl Lawrence"
      ],
      "abstract": "We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities--such as threshold-based filtering, slice extraction, and statistical analysis--through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system's adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., ``visualize the skull\"). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† VizGenieï¼Œä¸€ä¸ªæ—¨åœ¨æ¨åŠ¨ç§‘å­¦å¯è§†åŒ– (Scientific Visualization) çš„è‡ªæˆ‘å®Œå–„å¼æ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) ç¼–æ’é¢†åŸŸç‰¹å®šåŠåŠ¨æ€ç”Ÿæˆçš„æ¨¡å—ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡é¢„è®¾å·¥å…·è®¿é—®é˜ˆå€¼è¿‡æ»¤å’Œåˆ‡ç‰‡æå–ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œè€Œå¯¹äºè¶…å‡ºåŸºå‡†çš„ä»»åŠ¡ï¼ŒVizGenie èƒ½è‡ªä¸»åˆ©ç”¨ LLM ç”Ÿæˆå¦‚ VTK Python ä»£ç çš„å¯è§†åŒ–è„šæœ¬ã€‚ç”Ÿæˆçš„è„šæœ¬ç»è¿‡è‡ªåŠ¨åŒ–çš„åç«¯éªŒè¯å¹¶åœ¨æµ‹è¯•æˆåŠŸåæ— ç¼é›†æˆï¼Œä»è€ŒæŒç»­å¢å¼ºç³»ç»Ÿçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç³»ç»Ÿæä¾›ç›´è§‚çš„è‡ªç„¶è¯­è¨€ç•Œé¢ï¼Œå¹¶ç»“åˆç»è¿‡å¾®è°ƒçš„è§†è§‰æ¨¡å‹ï¼Œé€šè¿‡å›¾åƒåˆ†æå’Œè§†è§‰é—®ç­” (VQA) æŠ€æœ¯ç²¾å‡†è§£é‡Šç”¨æˆ·çš„é«˜çº§ç‰¹å¾æŸ¥è¯¢ã€‚æ­¤å¤–ï¼ŒVizGenie é›†æˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯ï¼Œåœ¨æä¾›ä¸Šä¸‹æ–‡é©±åŠ¨å“åº”çš„åŒæ—¶ç»´æŠ¤å®Œæ•´çš„å‡ºå¤„è®°å½•ï¼Œç¡®ä¿äº†ç»“æœçš„å¯é æ€§ä¸å¯å¤ç°æ€§ã€‚é’ˆå¯¹å¤æ‚ä½“ç§¯æ•°æ®é›†çš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—é™ä½äº†è¿­ä»£å¯è§†åŒ–ä»»åŠ¡ä¸­çš„è®¤çŸ¥è´Ÿè·ï¼Œé€šè¿‡ç»“åˆé¢†åŸŸç‰¹å®šå·¥å…·ä¸ LLM çš„çµæ´»æ€§ï¼Œå»ºç«‹äº†ä¸€ä¸ªèƒ½å¤Ÿä»ç”¨æˆ·äº¤äº’ä¸­åŠ¨æ€å­¦ä¹ å¹¶æŒç»­æ¼”è¿›çš„å¯è§†åŒ–å¹³å°ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21124v1",
      "published_date": "2025-07-18 23:54:22 UTC",
      "updated_date": "2025-07-18 23:54:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:24:57.481047+00:00"
    },
    {
      "arxiv_id": "2507.14406v1",
      "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering",
      "title_zh": "Fail Fast, or Askï¼šåˆ©ç”¨äººåœ¨å›è·¯ç³»ç»Ÿå·¥ç¨‹ç¼“è§£æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„å±€é™æ€§",
      "authors": [
        "Michael J. Zellinger",
        "Matt Thomson"
      ],
      "abstract": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still occasionally make mistakes. However, adopting AI models in risk-sensitive domains often requires error rates near 0%. To address this gap, we propose collaboration between a reasoning model and a human expert who resolves queries the model cannot confidently answer. We find that quantifying the uncertainty of a reasoning model through the length of its reasoning trace yields an effective basis for deferral to a human, e.g., cutting the error rate of Qwen3 235B-A22B on difficult MATH problems from 3% to less than 1% when deferring 7.5% of queries. However, the high latency of reasoning models still makes them challenging to deploy on use cases with high query volume. To address this challenge, we explore fronting a reasoning model with a large non-reasoning model. We call this modified human-in-the-loop system \"Fail Fast, or Ask\", since the non-reasoning model may defer difficult queries to the human expert directly (\"failing fast\"), without incurring the reasoning model's higher latency. We show that this approach yields around 40% latency reduction and about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the accuracy-rejection curve. However, we observe that latency savings are lower than expected because of \"latency drag\", the phenomenon that processing easier queries with a non-reasoning model pushes the reasoning model's latency distribution towards longer latencies. Broadly, our results suggest that the deficiencies of state-of-the-art reasoning models -- nontrivial error rates and high latency -- can be substantially mitigated through black-box systems engineering, without requiring access to LLM internals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡äººæœºååŒç³»ç»Ÿå·¥ç¨‹(Human-in-the-Loop Systems Engineering)æ¥å‡è½»æ¨ç†å¤§è¯­è¨€æ¨¡å‹(Reasoning LLMs)åœ¨é”™è¯¯ç‡å’Œé«˜å»¶è¿Ÿæ–¹é¢çš„ç¼ºé™·ã€‚ä½œè€…æå‡ºé€šè¿‡æ¨ç†é“¾(reasoning trace)çš„é•¿åº¦æ¥é‡åŒ–æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æ®æ­¤å°†éš¾ä»¥è§£å†³çš„é—®é¢˜äº¤ç”±äººç±»ä¸“å®¶å¤„ç†ï¼ŒæˆåŠŸä½¿Qwen3 235B-A22Båœ¨MATHéš¾é¢˜ä¸Šçš„é”™è¯¯ç‡ä»3%é™è‡³1%ä»¥ä¸‹ã€‚ä¸ºäº†è§£å†³é«˜å»¶è¿Ÿé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†â€œFail Fast, or Askâ€ç³»ç»Ÿï¼Œåˆ©ç”¨éæ¨ç†æ¨¡å‹é¢„å…ˆè¯†åˆ«å›°éš¾æŸ¥è¯¢å¹¶ç›´æ¥è½¬äº¤äººç±»ï¼Œä»è€Œè·³è¿‡æ¨ç†æ¨¡å‹çš„é«˜å»¶è¿Ÿç¯èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¸ºDeepSeek R1èŠ‚çœçº¦50%æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†çº¦40%çš„å»¶è¿Ÿé™ä½ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†â€œå»¶è¿Ÿæ‹–ç´¯â€(latency drag)ç°è±¡ï¼Œå³å¤„ç†ç®€å•æŸ¥è¯¢çš„è¿‡æ»¤ä¼šå¯¼è‡´æ¨ç†æ¨¡å‹åœ¨å‰©ä½™ä»»åŠ¡ä¸Šçš„å»¶è¿Ÿåˆ†å¸ƒå‘æ›´é•¿æ—¶é—´åç§»ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡é»‘ç›’ç³»ç»Ÿå·¥ç¨‹å¯ä»¥æ˜¾è‘—å¢å¼ºæœ€å…ˆè¿›æ¨ç†æ¨¡å‹çš„å¯é æ€§ä¸éƒ¨ç½²æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.14406v1",
      "published_date": "2025-07-18 23:25:26 UTC",
      "updated_date": "2025-07-18 23:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:24:53.917819+00:00"
    },
    {
      "arxiv_id": "2507.14393v1",
      "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation",
      "title_zh": "åŸºäºè‡ªåŠ¨åŒ–å·¥ä½œæµç”Ÿæˆçš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Humza Sami",
        "Mubashir ul Islam",
        "Pierre-Emmanuel Gaillardon",
        "Valerio Tenace"
      ],
      "abstract": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward in language model capabilities, aiming to tackle increasingly sophisticated tasks with unprecedented efficiency and accuracy. However, despite their impressive performance, recent studies have highlighted how current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning. Such behavior underscores a critical limitation in modern LRMs, i.e., their tendency toward overfitting, which in turn results in poor generalization in problem-solving capabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism. Given a user's prompt and a small set of representative examples, the Architect autonomously generates a tailored reasoning workflow by selecting suitable strategies, tool integrations, and adversarial techniques for a specific problem class. Furthermore, the Architect includes an iterative prompt refinement mechanism that fine-tunes agents' system prompts to maximize performance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf, non-reasoning model on a custom dataset of challenging logical questions and compare its performance against state-of-the-art LRMs. Results show that Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨å¤„ç†æ–°é¢–é—®é¢˜æ—¶å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆã€æ³›åŒ–èƒ½åŠ›å¼±ä»¥åŠå€¾å‘äºä¾èµ–è®°å¿†è€ŒéçœŸå®é€»è¾‘æ¨ç†çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Nexus Architectï¼Œè¿™æ˜¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶ Nexus çš„å¢å¼ºç‰ˆæœ¬ï¼Œæ ¸å¿ƒå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è‡ªåŠ¨åŒ–å·¥ä½œæµåˆæˆæœºåˆ¶ (automated workflow synthesis)ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æç”¨æˆ·æç¤ºè¯å’Œå°‘é‡ä»£è¡¨æ€§ç¤ºä¾‹ï¼Œèƒ½å¤Ÿä¸ºç‰¹å®šé—®é¢˜ç±»åˆ«è‡ªä¸»é€‰æ‹©åˆé€‚çš„ç­–ç•¥ã€å·¥å…·é›†æˆå’Œå¯¹æŠ—æŠ€æœ¯ï¼Œä»è€Œç”Ÿæˆå®šåˆ¶åŒ–çš„æ¨ç†å·¥ä½œæµã€‚æ­¤å¤–ï¼ŒNexus Architect è¿˜åŒ…å«è¿­ä»£æç¤ºè¯ä¼˜åŒ–æœºåˆ¶ (iterative prompt refinement)ï¼Œé€šè¿‡æŒç»­å¾®è°ƒæ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºè¯æ¥æœ€å¤§åŒ–æ€§èƒ½å¹¶è¿›ä¸€æ­¥æå‡ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½¿ç”¨åŸºç¡€éæ¨ç†æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨æŒ‘æˆ˜æ€§é€»è¾‘é—®é¢˜æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„ LRMsã€‚å…·ä½“è€Œè¨€ï¼ŒNexus Architect çš„é€šè¿‡ç‡ç›¸æ¯” Gemini 2.5 Flash Preview æé«˜äº† 66%ï¼Œä¸”è¾¾åˆ°äº† Claude Sonnet 4 å’Œ DeepSeek-R1 çš„è¿‘ 2.5 å€ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14393v1",
      "published_date": "2025-07-18 22:46:27 UTC",
      "updated_date": "2025-07-18 22:46:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:24:57.145521+00:00"
    },
    {
      "arxiv_id": "2507.15885v1",
      "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design",
      "title_zh": "ADEPTSï¼šä»¥äººä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“è®¾è®¡èƒ½åŠ›æ¡†æ¶",
      "authors": [
        "Pierluca D'Oro",
        "Caley Drooff",
        "Joy Chen",
        "Joseph Tighe"
      ],
      "abstract": "Large language models have paved the way to powerful and flexible AI agents, assisting humans by increasingly integrating into their daily life. This flexibility, potential, and growing adoption demands a holistic and cross-disciplinary approach to developing, monitoring and discussing the capabilities required for agent-driven user experiences. However, current guidance on human-centered AI agent development is scattered: UX heuristics focus on interface behaviors, engineering taxonomies describe internal pipelines, and ethics checklists address high-level governance. There is no concise, user-facing vocabulary that tells teams what an agent should fundamentally be able to do. We introduce ADEPTS, a capability framework defining a set of core user-facing capabilities to provide unified guidance around the development of AI agents. ADEPTS is based on six principles for human-centered agent design, that express the minimal, user-facing capabilities an AI agent should demonstrate to be understandable, controllable and trustworthy in everyday use. ADEPTS complements existing frameworks and taxonomies; differently from them, it sits at the interface between technical and experience development. By presenting ADEPTS, we aim to condense complex AI-UX requirements into a compact framework that is actionable guidance for AI researchers, designers, engineers, and policy reviewers alike. We believe ADEPTS has the potential of accelerating the improvement of user-relevant agent capabilities, of easing the design of experiences that take advantage of those capabilities, and of providing a shared language to track and discuss progress around the development of AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ADEPTSï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºä»¥äººä¸ºæœ¬çš„ AI agent è®¾è®¡æä¾›ç»Ÿä¸€æŒ‡å¯¼çš„èƒ½åŠ›æ¡†æ¶ï¼Œä»¥è§£å†³ç›®å‰å¼€å‘æŒ‡å—åœ¨ UX å¯å‘å¼æ–¹æ³•ã€å·¥ç¨‹åˆ†ç±»åŠä¼¦ç†å‡†åˆ™ç­‰æ–¹é¢è¿‡äºåˆ†æ•£ä¸”ç¼ºä¹é€šç”¨è¯æ±‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºå…­é¡¹æ ¸å¿ƒåŸåˆ™ï¼Œå®šä¹‰äº†ä¸€ç³»åˆ—ç¡®ä¿æ™ºèƒ½ä½“åœ¨æ—¥å¸¸åº”ç”¨ä¸­å…·å¤‡å¯ç†è§£æ€§ã€å¯æ§æ€§å’Œå¯ä¿¡åº¦çš„å…³é”®ç”¨æˆ·é¢å‘èƒ½åŠ›ã€‚ä¸åŒäºç°æœ‰çš„æŠ€æœ¯æˆ–ç•Œé¢å¯¼å‘æ¡†æ¶ï¼ŒADEPTS ä¸“æ³¨äºæŠ€æœ¯ç ”å‘ä¸ç”¨æˆ·ä½“éªŒå¼€å‘çš„äº¤ç•Œï¼Œå°†å¤æ‚çš„ AI-UX éœ€æ±‚è½¬åŒ–ä¸ºå¯¹ç ”ç©¶äººå‘˜ã€è®¾è®¡å¸ˆå’Œæ”¿ç­–åˆ¶å®šè€…å‡å…·æœ‰å¯æ“ä½œæ€§çš„æŒ‡å¯¼æ–¹æ¡ˆã€‚ADEPTS çš„æå‡ºä¸ä»…ä¸ºè¿½è¸ªå’Œè®¨è®º AI æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†å…±ç”¨çš„è¯­è¨€ä½“ç³»ï¼Œè¿˜æœ‰åŠ©äºåŠ é€Ÿæå‡ç”¨æˆ·ç›¸å…³çš„æ™ºèƒ½ä½“èƒ½åŠ›å¹¶ä¼˜åŒ–æ•´ä½“ä½“éªŒè®¾è®¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15885v1",
      "published_date": "2025-07-18 22:27:40 UTC",
      "updated_date": "2025-07-18 22:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:03.517183+00:00"
    },
    {
      "arxiv_id": "2507.14387v1",
      "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures",
      "title_zh": "é¢å‘ä¿¡æ¯ç‰©ç†åŸºç¡€è®¾æ–½åœ¨çº¿ç½‘ç»œæ”»å‡»æ£€æµ‹çš„å¢é‡å› æœå›¾å­¦ä¹ ",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Dongjie Wang",
        "Haoyue Bai",
        "Yanjie Fu"
      ],
      "abstract": "The escalating threat of cyberattacks on real-time critical infrastructures poses serious risks to public safety, demanding detection methods that effectively capture complex system interdependencies and adapt to evolving attack patterns. Traditional real-time anomaly detection techniques often suffer from excessive false positives due to their statistical sensitivity to high data variance and class imbalance. To address these limitations, recent research has explored modeling causal relationships among system components. However, prior work mainly focuses on offline causal graph-based approaches that require static historical data and fail to generalize to real-time settings. These methods are fundamentally constrained by: (1) their inability to adapt to dynamic shifts in data distribution without retraining, and (2) the risk of catastrophic forgetting when lacking timely supervision in live systems. To overcome these challenges, we propose INCADET, a novel framework for incremental causal graph learning tailored to real-time cyberattack detection. INCADET dynamically captures evolving system behavior by incrementally updating causal graphs across streaming time windows. The framework comprises three modules: 1) Early Symptom Detection: Detects transitions in system status using divergence in edge-weight distributions across sequential causal graphs. 2) Incremental Causal Graph Learning: Leverages experience replay and edge reinforcement to continually refine causal structures while preserving prior knowledge. 3) Causal Graph Classification: Employs Graph Convolutional Networks (GCNs) to classify system status using the learned causal graphs. Extensive experiments on real-world critical infrastructure datasets demonstrate that INCADET achieves superior accuracy, robustness, and adaptability compared to both static causal and deep temporal baselines in evolving attack scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…³é”®åŸºç¡€è®¾æ–½ä¸­ç½‘ç»œæ”»å‡»å®æ—¶æ£€æµ‹é¢ä¸´çš„ç³»ç»Ÿç›¸äº’ä¾èµ–æ€§å¤æ‚ã€æ•°æ®åˆ†å¸ƒåŠ¨æ€åç§»ä»¥åŠç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºINCADETçš„æ–°å‹å¢é‡å› æœå›¾å­¦ä¹ (Incremental Causal Graph Learning)æ¡†æ¶ã€‚INCADETé€šè¿‡åœ¨æµå¼æ—¶é—´çª—å£ä¸­å¢é‡æ›´æ–°å› æœå›¾æ¥åŠ¨æ€æ•æ‰ç³»ç»Ÿè¡Œä¸ºçš„å˜åŒ–ï¼Œå…¶æ ¸å¿ƒç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼šåŸºäºè¾¹æƒé‡åˆ†å¸ƒæ•£åº¦çš„æ—©æœŸç—‡çŠ¶æ£€æµ‹(Early Symptom Detection)ã€ç»“åˆç»éªŒå›æ”¾(experience replay)å’Œè¾¹å¼ºåŒ–æŠ€æœ¯çš„å¢é‡å› æœå›¾å­¦ä¹ ï¼Œä»¥åŠåˆ©ç”¨å›¾å·ç§¯ç½‘ç»œ(GCNs)è¿›è¡Œç³»ç»ŸçŠ¶æ€åˆ†ç±»ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•å¯¹é«˜æ–¹å·®æ•°æ®çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶ç¯å¢ƒä¸­æŒç»­ä¼˜åŒ–å› æœç»“æ„å¹¶ä¿ç•™å…ˆéªŒçŸ¥è¯†ã€‚åœ¨çœŸå®ä¸–ç•ŒåŸºç¡€è®¾æ–½æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒINCADETåœ¨å‡†ç¡®ç‡ã€é²æ£’æ€§å’Œé€‚åº”æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºé™æ€å› æœæ¨¡å‹å’Œæ·±åº¦æ—¶é—´åŸºçº¿æ¨¡å‹ï¼Œä¸ºåº”å¯¹æ¼”åŒ–ä¸­çš„æ”»å‡»åœºæ™¯æä¾›äº†æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures, 3 Tables, under review in IEEE Transactions on Big Data",
      "pdf_url": "https://arxiv.org/pdf/2507.14387v1",
      "published_date": "2025-07-18 22:27:13 UTC",
      "updated_date": "2025-07-18 22:27:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:04.976113+00:00"
    },
    {
      "arxiv_id": "2507.14376v1",
      "title": "Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms",
      "title_zh": "Schemoraï¼šåŸºäºå¤šé˜¶æ®µæ¨èä¸å…ƒæ•°æ®å¢å¼ºå¹¶åˆ©ç”¨ç°æˆå¤§è¯­è¨€æ¨¡å‹çš„æ¨¡å¼åŒ¹é…æ–¹æ³•",
      "authors": [
        "Osman Erman Gungor",
        "Derak Paulsen",
        "William Kang"
      ],
      "abstract": "Schema matching is essential for integrating heterogeneous data sources and enhancing dataset discovery, yet it remains a complex and resource-intensive problem. We introduce SCHEMORA, a schema matching framework that combines large language models with hybrid retrieval techniques in a prompt-based approach, enabling efficient identification of candidate matches without relying on labeled training data or exhaustive pairwise comparisons. By enriching schema metadata and leveraging both vector-based and lexical retrieval, SCHEMORA improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP benchmark, it establishes new state-of-the-art performance, with gains of 7.49% in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our knowledge, this is the first LLM-based schema matching method with an open-source implementation, accompanied by analysis that underscores the critical role of retrieval and provides practical guidance on model selection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SCHEMORAï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ··åˆæ£€ç´¢æŠ€æœ¯çš„æ¨¡å¼åŒ¹é…ï¼ˆschema matchingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼‚æ„æ•°æ®æºé›†æˆä¸­çš„å¤æ‚æ€§ä¸èµ„æºæ¶ˆè€—é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæç¤ºï¼ˆprompt-basedï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸°å¯Œæ¨¡å¼å…ƒæ•°æ®å¹¶ç»“åˆå‘é‡æ£€ç´¢ä¸è¯æ³•æ£€ç´¢ï¼Œå®ç°äº†é«˜æ•ˆçš„å€™é€‰åŒ¹é…è¯†åˆ«ï¼Œä¸”æ— éœ€ä¾èµ–æ ‡æ³¨è®­ç»ƒæ•°æ®æˆ–ç©·ä¸¾å¼æˆå¯¹æ¯”è¾ƒã€‚åœ¨MIMIC-OMOPåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSCHEMORAåˆ·æ–°äº†å½“å‰çš„æœ€ä¼˜æ€§èƒ½ï¼Œå…¶HitRate@5å’ŒHitRate@3åˆ†åˆ«è¾ƒæ­¤å‰æœ€ä½³ç»“æœæå‡äº†7.49%å’Œ3.75%ã€‚ä½œä¸ºé¦–ä¸ªå¼€æºçš„åŸºäºLLMçš„æ¨¡å¼åŒ¹é…æ–¹æ³•ï¼Œè¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†æ£€ç´¢æŠ€æœ¯åœ¨æå‡åŒ¹é…å‡†ç¡®ç‡ä¸å¯æ‰©å±•æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¹Ÿä¸ºå®é™…åº”ç”¨ä¸­çš„æ¨¡å‹é€‰æ‹©æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.14376v1",
      "published_date": "2025-07-18 21:50:36 UTC",
      "updated_date": "2025-07-18 21:50:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:04.624068+00:00"
    },
    {
      "arxiv_id": "2507.14372v1",
      "title": "Text-to-SQL for Enterprise Data Analytics",
      "title_zh": "é¢å‘ä¼ä¸šçº§æ•°æ®åˆ†æçš„ Text-to-SQL",
      "authors": [
        "Albert Chen",
        "Manas Bundele",
        "Gaurav Ahlawat",
        "Patrick Stetz",
        "Zhitao Wang",
        "Qiang Fei",
        "Donghoon Jung",
        "Audrey Chu",
        "Bharadwaj Jayaraman",
        "Ayushi Panth",
        "Yatin Arora",
        "Sourav Jain",
        "Renjith Varma",
        "Alexey Ilin",
        "Iuliia Melnychuk",
        "Chelsea Chueh",
        "Joyan Sil",
        "Xiaofeng Wang"
      ],
      "abstract": "The introduction of large language models has brought rapid progress on Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise solution. In this paper, we present insights from building an internal chatbot that enables LinkedIn's product managers, engineers, and operations teams to self-serve data insights from a large, dynamic data lake. Our approach features three components. First, we construct a knowledge graph that captures up-to-date semantics by indexing database metadata, historical query logs, wikis, and code. We apply clustering to identify relevant tables for each team or product area. Second, we build a Text-to-SQL agent that retrieves and ranks context from the knowledge graph, writes a query, and automatically corrects hallucinations and syntax errors. Third, we build an interactive chatbot that supports various user intents, from data discovery to query writing to debugging, and displays responses in rich UI elements to encourage follow-up chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of its responses are correct or close to correct on an internal benchmark set. Through ablation studies, we identify the most important knowledge graph and modeling components, offering a practical path for developing enterprise Text-to-SQL solutions.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¼ä¸šç¯å¢ƒä¸‹æ„å»º Text-to-SQL è§£å†³æ–¹æ¡ˆçš„æŒ‘æˆ˜ï¼Œå¹¶åˆ†äº«äº† LinkedIn ä¸ºå†…éƒ¨å›¢é˜Ÿå¼€å‘è‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå·¥å…·çš„å®è·µç»éªŒã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒåŒ…æ‹¬æ„å»ºä¸€ä¸ªé›†æˆæ•°æ®åº“å…ƒæ•°æ®ã€æŸ¥è¯¢æ—¥å¿—ã€ç»´åŸºå’Œä»£ç çš„çŸ¥è¯†å›¾è°± (Knowledge Graph)ï¼Œå¹¶é€šè¿‡èšç±»æŠ€æœ¯å®ç°ç›¸å…³æ•°æ®è¡¨çš„ç²¾å‡†å®šä½ã€‚ç³»ç»Ÿå†…ç½®çš„ Text-to-SQL æ™ºèƒ½ä½“èƒ½å¤Ÿä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ã€ç¼–å†™æŸ¥è¯¢ï¼Œå¹¶è‡ªåŠ¨çº æ­£å¹»è§‰ (hallucinations) ä¸è¯­æ³•é”™è¯¯ã€‚æ­¤å¤–ï¼Œäº¤äº’å¼èŠå¤©æœºå™¨äººæ”¯æŒæ•°æ®å‘ç°ã€æŸ¥è¯¢ç¼–å†™å’Œè°ƒè¯•ç­‰å¤šç§ç”¨æˆ·æ„å›¾ï¼Œå¹¶é€šè¿‡ä¸°å¯Œçš„ UI å…ƒç´ æå‡ç”¨æˆ·ä½“éªŒã€‚åº”ç”¨ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿæ¯å‘¨æœåŠ¡è¶…è¿‡ 300 åç”¨æˆ·ï¼Œåœ¨å†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 53% çš„å‡†ç¡®ç‡æˆ–æ¥è¿‘å‡†ç¡®ç‡ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ (ablation studies)ï¼Œè¯¥è®ºæ–‡ç¡®å®šäº†å½±å“æ€§èƒ½çš„å…³é”®å»ºæ¨¡ç»„ä»¶ï¼Œä¸ºä¼ä¸šçº§æ•°æ®åˆ†æç³»ç»Ÿçš„è½åœ°æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25",
      "pdf_url": "https://arxiv.org/pdf/2507.14372v1",
      "published_date": "2025-07-18 21:39:17 UTC",
      "updated_date": "2025-07-18 21:39:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:12.303149+00:00"
    },
    {
      "arxiv_id": "2507.14353v1",
      "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers",
      "title_zh": "Solo Connectionï¼šé¢å‘ Transformer çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯",
      "authors": [
        "Harsh Nilesh Pathak",
        "Randy Paffenroth"
      ],
      "abstract": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach for adapting a Large Language Model (LLM) for newer tasks. One of the most prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on adjusting the attention weight matrices within individual decoder blocks of a Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo Connection a novel method that adapts the representation at the decoder-block level rather than modifying individual weight matrices. Not only does Solo Connection outperform LoRA on E2E natural language generation benchmarks, but it also reduces the number of trainable parameters by 59% relative to LoRA and by more than 99% compared to full fine-tuning of GPT2, an early version of Large Language Models (LLMs). Solo Connection is also motivated by homotopy theory: we introduce a trainable linear transformation that gradually interpolates between a zero vector and the task-specific representation, enabling smooth and stable adaptation over time. While skip connections in the original 12 layer GPT2 are typically confined to individual decoder blocks, subsequent GPT2 variants scale up to 48 layers, and even larger language models can include 128 or more decoder blocks. These expanded architectures underscore the need to revisit how skip connections are employed during fine-tuning. This paper focuses on long skip connections that link outputs of different decoder blocks, potentially enhancing the model's ability to adapt to new tasks while leveraging pre-trained knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSolo Connectionçš„å…¨æ–°å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æŠ€æœ¯ï¼Œæ—¨åœ¨æå‡Transformeræ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚é…èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„LoRAæ–¹æ³•é€šè¿‡è°ƒæ•´å„ä¸ªè§£ç å™¨å—å†…çš„æƒé‡çŸ©é˜µä¸åŒï¼ŒSolo Connectionç›´æ¥åœ¨è§£ç å™¨å—å±‚çº§å¯¹è¡¨ç¤º(representation)è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ³•å—åŒä¼¦è®º(homotopy theory)å¯å‘ï¼Œå¼•å…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„çº¿æ€§å˜æ¢ï¼Œé€šè¿‡åœ¨é›¶å‘é‡ä¸ç‰¹å®šä»»åŠ¡è¡¨ç¤ºä¹‹é—´è¿›è¡Œé€æ¸æ’å€¼ï¼Œå®ç°äº†å¹³æ»‘ä¸”ç¨³å®šçš„é€‚é…è¿‡ç¨‹ã€‚Solo Connectionç‰¹åˆ«å¼ºè°ƒåˆ©ç”¨è¿æ¥ä¸åŒè§£ç å™¨å—è¾“å‡ºçš„é•¿è·³è·ƒè¿æ¥(long skip connections)ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶é€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨E2Eè‡ªç„¶è¯­è¨€ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºLoRAï¼Œä¸”å…¶å¯è®­ç»ƒå‚æ•°é‡æ¯”LoRAå‡å°‘äº†59%ï¼Œç›¸æ¯”GPT2å…¨é‡å¾®è°ƒå‡å°‘äº†è¶…è¿‡99%ã€‚è¯¥ç ”ç©¶ä¸ºè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦å¢åŠ åçš„æ¶æ„è®¾è®¡ä¸å¾®è°ƒç­–ç•¥æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14353v1",
      "published_date": "2025-07-18 20:11:50 UTC",
      "updated_date": "2025-07-18 20:11:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:13.453379+00:00"
    },
    {
      "arxiv_id": "2507.14352v1",
      "title": "A Reproducibility Study of Product-side Fairness in Bundle Recommendation",
      "title_zh": "æ†ç»‘æ¨èä¸­äº§å“ç«¯å…¬å¹³æ€§çš„å¤ç°æ€§ç ”ç©¶",
      "authors": [
        "Huy-Son Nguyen",
        "Yuanna Liu",
        "Masoud Mansoury",
        "Mohammad Alian Nejadi",
        "Alan Hanjalic",
        "Maarten de Rijke"
      ],
      "abstract": "Recommender systems are known to exhibit fairness issues, particularly on the product side, where products and their associated suppliers receive unequal exposure in recommended results. While this problem has been widely studied in traditional recommendation settings, its implications for bundle recommendation (BR) remain largely unexplored. This emerging task introduces additional complexity: recommendations are generated at the bundle level, yet user satisfaction and product (or supplier) exposure depend on both the bundle and the individual items it contains. Existing fairness frameworks and metrics designed for traditional recommender systems may not directly translate to this multi-layered setting. In this paper, we conduct a comprehensive reproducibility study of product-side fairness in BR across three real-world datasets using four state-of-the-art BR methods. We analyze exposure disparities at both the bundle and item levels using multiple fairness metrics, uncovering important patterns. Our results show that exposure patterns differ notably between bundles and items, revealing the need for fairness interventions that go beyond bundle-level assumptions. We also find that fairness assessments vary considerably depending on the metric used, reinforcing the need for multi-faceted evaluation. Furthermore, user behavior plays a critical role: when users interact more frequently with bundles than with individual items, BR systems tend to yield fairer exposure distributions across both levels. Overall, our findings offer actionable insights for building fairer bundle recommender systems and establish a vital foundation for future research in this emerging domain.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹æ†ç»‘æ¨è(Bundle Recommendation, BR)ä¸­è¢«å¿½è§†çš„äº§å“ç«¯å…¬å¹³æ€§é—®é¢˜ï¼Œåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šåˆ©ç”¨å››ç§å…ˆè¿›çš„BRæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å¯é‡å¤æ€§ç ”ç©¶ã€‚ç ”ç©¶äººå‘˜åœ¨æ†ç»‘åŒ…(bundle)å’Œå•å“(item)ä¸¤ä¸ªå±‚çº§åˆ†æäº†æ›å…‰å·®å¼‚ï¼Œå‘ç°ä¸¤è€…çš„æ›å…‰æ¨¡å¼å­˜åœ¨æ˜¾è‘—ä¸åŒï¼Œè¿™è¡¨æ˜å…¬å¹³æ€§å¹²é¢„å¿…é¡»è¶…è¶Šå•çº¯çš„bundleå±‚é¢ã€‚å®éªŒç»“æœæ­ç¤ºäº†å…¬å¹³æ€§è¯„ä¼°é«˜åº¦ä¾èµ–äºæ‰€é€‰çš„è¯„ä»·æŒ‡æ ‡(metrics)ï¼Œå¼ºè°ƒäº†å¤šç»´åº¦è¯„ä¼°çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç”¨æˆ·è¡Œä¸ºå¯¹å…¬å¹³æ€§æœ‰é‡è¦å½±å“ï¼Œå½“ç”¨æˆ·ä¸bundleçš„äº¤äº’é¢‘ç‡é«˜äºitemæ—¶ï¼Œç³»ç»Ÿåœ¨ä¸¤ä¸ªå±‚çº§ä¸Šçš„æ›å…‰åˆ†å¸ƒå¾€å¾€æ›´å…¬å¹³ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºæ›´å…¬å¹³çš„æ†ç»‘æ¨èç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ï¼Œå¹¶ä¸ºè¯¥æ–°å…´é¢†åŸŸçš„æœªæ¥ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14352v1",
      "published_date": "2025-07-18 20:06:39 UTC",
      "updated_date": "2025-07-18 20:06:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:14.931596+00:00"
    },
    {
      "arxiv_id": "2507.14344v1",
      "title": "Influence Functions for Preference Dataset Pruning",
      "title_zh": "é¢å‘åå¥½æ•°æ®é›†å‰ªæçš„å½±å“å‡½æ•°",
      "authors": [
        "Daniel Fein",
        "Gabriela Aranguiz-Dias"
      ],
      "abstract": "Language models are commonly fine-tuned via reinforcement learning to alter their behavior or elicit new capabilities. Datasets used for these purposes, and particularly human preference datasets, are often noisy. The relatively small size post-training datasets, combined with parameter-efficient fine-tuning methods, enable the use of influence functions approximations to detect and prune training examples that are harmful to performance on a validation set. In this work, we adapt the TL;DR dataset for reward model training to demonstrate how conjugate-gradient approximated influence functions can be used to filter datasets. In our experiments, influence function filtering yields a small retraining accuracy uplift of 1.5% after removing 10% of training examples. We also show that gradient similarity outperforms influence functions for detecting helpful training examples. This suggests that local curvature is important for detecting harmful training examples, but less so for identifying helpful examples.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å½±å“å‡½æ•°(Influence Functions)å¯¹åå¥½æ•°æ®é›†è¿›è¡Œä¿®å‰ªï¼Œä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„è®­ç»ƒæ•°æ®å™ªå£°é—®é¢˜ã€‚é€šè¿‡ç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨å…±è½­æ¢¯åº¦(Conjugate-Gradient)è¿‘ä¼¼å½±å“å‡½æ•°æ¥è¯†åˆ«å¹¶å‰”é™¤å¯¹éªŒè¯é›†æ€§èƒ½æœ‰è´Ÿé¢å½±å“çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨é’ˆå¯¹TL;DRæ•°æ®é›†çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ç§»é™¤10%çš„è®­ç»ƒæ•°æ®åä½¿æ¨¡å‹å‡†ç¡®ç‡æå‡äº†1.5%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¢¯åº¦ç›¸ä¼¼åº¦(Gradient Similarity)åœ¨è¯†åˆ«æœ‰ç›Šæ ·æœ¬æ–¹é¢è¡¨ç°ä¼˜äºå½±å“å‡½æ•°ï¼Œè¿™è¡¨æ˜å±€éƒ¨æ›²ç‡(Local Curvature)åœ¨æ£€æµ‹æœ‰å®³æ ·æœ¬æ—¶å…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†åœ¨è¯†åˆ«æœ‰ç›Šæ ·æœ¬æ—¶åˆ™ä¸ç„¶ã€‚è¯¥å·¥ä½œä¸ºä¼˜åŒ–åè®­ç»ƒæ•°æ®é›†è´¨é‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„é‡åŒ–ç­›é€‰æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14344v1",
      "published_date": "2025-07-18 19:43:36 UTC",
      "updated_date": "2025-07-18 19:43:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:18.947749+00:00"
    },
    {
      "arxiv_id": "2507.14339v1",
      "title": "Fiduciary AI for the Future of Brain-Technology Interactions",
      "title_zh": "é¢å‘æœªæ¥å¤§è„‘-æŠ€æœ¯äº¤äº’çš„ä¿¡ä¹‰äººå·¥æ™ºèƒ½",
      "authors": [
        "Abhishek Bhattacharjee",
        "Jack Pilkington",
        "Nita Farahany"
      ],
      "abstract": "Brain foundation models represent a new frontier in AI: instead of processing text or images, these models interpret real-time neural signals from EEG, fMRI, and other neurotechnologies. When integrated with brain-computer interfaces (BCIs), they may enable transformative applications-from thought controlled devices to neuroprosthetics-by interpreting and acting on brain activity in milliseconds. However, these same systems pose unprecedented risks, including the exploitation of subconscious neural signals and the erosion of cognitive liberty. Users cannot easily observe or control how their brain signals are interpreted, creating power asymmetries that are vulnerable to manipulation. This paper proposes embedding fiduciary duties-loyalty, care, and confidentiality-directly into BCI-integrated brain foundation models through technical design. Drawing on legal traditions and recent advancements in AI alignment techniques, we outline implementable architectural and governance mechanisms to ensure these systems act in users' best interests. Placing brain foundation models on a fiduciary footing is essential to realizing their potential without compromising self-determination.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è„‘åŸºç¡€æ¨¡å‹(Brain Foundation Models)åœ¨è§£é‡ŠEEGå’ŒfMRIç­‰å®æ—¶ç¥ç»ä¿¡å·æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é›†æˆè„‘æœºæ¥å£(BCIs)ä»¥å®ç°æ„å¿µæ§åˆ¶è®¾å¤‡ç­‰åº”ç”¨ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿä¹Ÿå¸¦æ¥äº†åˆ©ç”¨æ½œæ„è¯†ç¥ç»ä¿¡å·å’Œä¾µèš€è®¤çŸ¥è‡ªç”±(Cognitive Liberty)ç­‰å‰æ‰€æœªæœ‰çš„é£é™©ï¼Œå¹¶å¯¼è‡´ç”¨æˆ·éš¾ä»¥å¯Ÿè§‰æˆ–æ§åˆ¶çš„æƒåŠ›ä¸å¯¹ç§°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å»ºè®®é€šè¿‡æŠ€æœ¯è®¾è®¡å°†å—æ‰˜è´£ä»»(Fiduciary Duties)â€”â€”åŒ…æ‹¬å¿ è¯š(Loyalty)ã€å°½è´£(Care)å’Œä¿å¯†(Confidentiality)â€”â€”ç›´æ¥åµŒå…¥åˆ°é›†æˆBCIçš„å¤§è„‘åŸºç¡€æ¨¡å‹ä¸­ã€‚å€Ÿé‰´æ³•å¾‹ä¼ ç»Ÿå’ŒAIå¯¹é½(AI Alignment)æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œä½œè€…æå‡ºäº†å¯å®ç°çš„æ¶æ„å’Œæ²»ç†æœºåˆ¶ï¼Œä»¥ç¡®ä¿è¿™äº›ç³»ç»Ÿå§‹ç»ˆç¬¦åˆç”¨æˆ·çš„æœ€ä½³åˆ©ç›Šã€‚å°†å¤§è„‘åŸºç¡€æ¨¡å‹å»ºç«‹åœ¨å—æ‰˜åŸºç¡€ä¸Šï¼Œå¯¹äºåœ¨ä¸ç‰ºç‰²è‡ªæˆ‘å†³å®šæƒ(Self-determination)çš„å‰æä¸‹å®ç°å…¶æŠ€æœ¯æ½œåŠ›è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CY",
      "comment": "32 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.14339v1",
      "published_date": "2025-07-18 19:34:08 UTC",
      "updated_date": "2025-07-18 19:34:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:30.823125+00:00"
    },
    {
      "arxiv_id": "2507.15882v2",
      "title": "Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark",
      "title_zh": "Document Haystackï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€å›¾åƒ/æ–‡æ¡£ç†è§£çš„è§†è§‰å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Goeric Huybrechts",
        "Srikanth Ronanki",
        "Sai Muralidhar Jayanthi",
        "Jack Fitzgerald",
        "Srinivasan Veeravanallur"
      ],
      "abstract": "The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image \"needles\" at various depths within the documents to challenge VLMs' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Document Haystackï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Models, VLMs) åœ¨å¤„ç†é•¿ç¯‡ä¸”è§†è§‰å¤æ‚æ–‡æ¡£æ—¶æ€§èƒ½çš„ç»¼åˆæ€§ benchmarkã€‚é’ˆå¯¹ç›®å‰å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨é•¿æ–‡æ¡£ç†è§£é¢†åŸŸç¼ºä¹è¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œè¯¥åŸºå‡†æ¶µç›–äº† 5 åˆ° 200 é¡µçš„æ–‡æ¡£ï¼Œé€šè¿‡åœ¨æ–‡æ¡£ä¸åŒæ·±åº¦æˆ˜ç•¥æ€§åœ°æ’å…¥çº¯æ–‡æœ¬æˆ–â€œæ–‡æœ¬+å›¾åƒâ€çš„ needlesï¼Œä¸¥å³»æŒ‘æˆ˜ VLMs çš„æ£€ç´¢èƒ½åŠ›ã€‚Document Haystack åŒ…å« 400 ä¸ªæ–‡æ¡£å˜ä½“å’Œå…±è®¡ 8,250 ä¸ªé—®é¢˜ï¼Œå¹¶é…å¥—äº†å®¢è§‚çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº†è¯¥æ•°æ®é›†çš„æ„å»ºç‰¹å¾ï¼Œæä¾›äº†ä¸»æµ VLMs çš„æµ‹è¯•ç»“æœï¼Œå¹¶æ¢è®¨äº† long context å¤šæ¨¡æ€ç†è§£é¢†åŸŸæ½œåœ¨çš„ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15882v2",
      "published_date": "2025-07-18 19:33:15 UTC",
      "updated_date": "2025-08-04 20:48:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:36.318541+00:00"
    },
    {
      "arxiv_id": "2507.14335v1",
      "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance",
      "title_zh": "ProofCompassï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼å¢å¼ºä¸“ç”¨è¯æ˜å™¨",
      "authors": [
        "Nicolas Wischermann",
        "Claudio Mayrink Verdun",
        "Gabriel Poesia",
        "Francesco Noseda"
      ],
      "abstract": "Language models have become increasingly powerful tools for formal mathematical reasoning. However, most existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources. This paper introduces ProofCompass, a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods, such as DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without requiring additional model training. The LLM provides natural language proof strategies and analyzes failed attempts to select intermediate lemmas, enabling effective problem decomposition. On the miniF2F benchmark, ProofCompass demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\% \\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$). Our synergistic approach paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProofCompassï¼Œä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒå³å¯é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ç­–ç•¥æ€§å¼•å¯¼ç°æœ‰ä¸“ç”¨è¯æ˜å™¨ï¼ˆå¦‚ DeepSeek-Prover-v1.5-RLï¼‰çš„æ–°å‹æ··åˆæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†å½¢å¼åŒ–æ•°å­¦æ¨ç†ä¸­è®­ç»ƒä¸“ç”¨å¤§æ¨¡å‹è®¡ç®—èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ã€‚ProofCompass åˆ©ç”¨ LLM ç”Ÿæˆè‡ªç„¶è¯­è¨€è¯æ˜ç­–ç•¥å¹¶åˆ†æå¤±è´¥å°è¯•ä»¥æå–ä¸­é—´å¼•ç†ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„é—®é¢˜åˆ†è§£ã€‚åœ¨ miniF2F åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡èµ„æºæ•ˆç‡çš„åŒæ—¶å¢å¼ºäº†æ€§èƒ½ï¼Œä»…éœ€ 1/25 çš„å°è¯•æ¬¡æ•°ï¼ˆä» 3200 æ¬¡å‡è‡³ 128 æ¬¡ï¼‰å³å¯å°† DSP-v1.5 çš„å‡†ç¡®ç‡ä» 54.9% æå‡è‡³ 55.3%ã€‚è¿™ç§ååŒæ–¹æ³•ä¸ºåŒæ—¶æé«˜å½¢å¼åŒ–å®šç†è¯æ˜çš„è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.14335v1",
      "published_date": "2025-07-18 19:28:01 UTC",
      "updated_date": "2025-07-18 19:28:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:40.215088+00:00"
    },
    {
      "arxiv_id": "2507.14334v1",
      "title": "Language Models as Ontology Encoders",
      "title_zh": "å°†è¯­è¨€æ¨¡å‹ä½œä¸ºæœ¬ä½“ç¼–ç å™¨",
      "authors": [
        "Hui Yang",
        "Jiaoyan Chen",
        "Yuan He",
        "Yongsheng Gao",
        "Ian Horrocks"
      ],
      "abstract": "OWL (Web Ontology Language) ontologies which are able to formally represent complex knowledge and support semantic reasoning have been widely adopted across various domains such as healthcare and bioinformatics. Recently, ontology embeddings have gained wide attention due to its potential to infer plausible new knowledge and approximate complex reasoning. However, existing methods face notable limitations: geometric model-based embeddings typically overlook valuable textual information, resulting in suboptimal performance, while the approaches that incorporate text, which are often based on language models, fail to preserve the logical structure. In this work, we propose a new ontology embedding method OnT, which tunes a Pretrained Language Model (PLM) via geometric modeling in a hyperbolic space for effectively incorporating textual labels and simultaneously preserving class hierarchies and other logical relationships of Description Logic EL. Extensive experiments on four real-world ontologies show that OnT consistently outperforms the baselines including the state-of-the-art across both tasks of prediction and inference of axioms. OnT also demonstrates strong potential in real-world applications, indicated by its robust transfer learning abilities and effectiveness in real cases of constructing a new ontology from SNOMED CT. Data and code are available at https://github.com/HuiYang1997/OnT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OnTï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰æœ¬ä½“åµŒå…¥ï¼ˆOntology embeddingï¼‰æ–¹æ³•åœ¨èåˆæ–‡æœ¬ä¿¡æ¯ä¸ä¿ç•™é€»è¾‘ç»“æ„ä¹‹é—´çŸ›ç›¾çš„æ–°å‹æ¡†æ¶ã€‚OnTé€šè¿‡åœ¨åŒæ›²ç©ºé—´ï¼ˆhyperbolic spaceï¼‰ä¸­å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†æ–‡æœ¬æ ‡ç­¾çš„æœ‰æ•ˆæ•´åˆï¼ŒåŒæ—¶ç²¾å‡†ä¿ç•™äº†æè¿°é€»è¾‘ELçš„ç±»å±‚æ¬¡ç»“æ„åŠå…¶é€»è¾‘å…³ç³»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOnTåœ¨å››ä¸ªçœŸå®ä¸–ç•Œæœ¬ä½“çš„å…¬ç†é¢„æµ‹å’Œæ¨ç†ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°å‡ºå“è¶Šçš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åœ¨åŸºäºSNOMED CTæ„å»ºæ–°æœ¬ä½“çš„å®é™…åº”ç”¨ä¸­è¯æ˜äº†å…¶é²æ£’æ€§å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14334v1",
      "published_date": "2025-07-18 19:26:16 UTC",
      "updated_date": "2025-07-18 19:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:39.844223+00:00"
    },
    {
      "arxiv_id": "2508.00867v1",
      "title": "Better Recommendations: Validating AI-generated Subject Terms Through LOC Linked Data Service",
      "title_zh": "ä¼˜åŒ–æ¨èï¼šé€šè¿‡ LOC Linked Data Service éªŒè¯ AI ç”Ÿæˆçš„ä¸»é¢˜è¯",
      "authors": [
        "Kwok Leong Tang",
        "Yi Jiang"
      ],
      "abstract": "This article explores the integration of AI-generated subject terms into library cataloging, focusing on validation through the Library of Congress Linked Data Service. It examines the challenges of traditional subject cataloging under the Library of Congress Subject Headings system, including inefficiencies and cataloging backlogs. While generative AI shows promise in expediting cataloging workflows, studies reveal significant limitations in the accuracy of AI-assigned subject headings. The article proposes a hybrid approach combining AI technology with human validation through LOC Linked Data Service, aiming to enhance the precision, efficiency, and overall quality of metadata creation in library cataloging practices.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°† AI ç”Ÿæˆçš„ä¸»é¢˜è¯æ•´åˆåˆ°å›¾ä¹¦é¦†ç¼–ç›®ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ Library of Congress Subject Headings ç³»ç»Ÿåœ¨ç¼–ç›®æ•ˆç‡ä½ä¸‹å’Œç§¯å‹ä¸¥é‡ç­‰æŒ‘æˆ˜ã€‚å°½ç®¡ç”Ÿæˆå¼ AI åœ¨åŠ é€Ÿç¼–ç›®æµç¨‹æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶æ­ç¤ºäº† AI åˆ†é…ä¸»é¢˜è¯åœ¨å‡†ç¡®æ€§ä¸Šçš„æ˜¾è‘—å±€é™ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°† AI æŠ€æœ¯ä¸é€šè¿‡ LOC Linked Data Service è¿›è¡Œçš„äººå·¥éªŒè¯ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨æƒå¨çš„é“¾æ¥æ•°æ®æœåŠ¡æ¥æ ¡éªŒ AI ç”Ÿæˆçš„å†…å®¹ï¼Œä»è€Œå¢å¼ºå›¾ä¹¦é¦†ç¼–ç›®å®è·µä¸­å…ƒæ•°æ®åˆ›å»ºçš„ç²¾ç¡®åº¦ã€æ•ˆç‡å’Œæ•´ä½“è´¨é‡ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œç ”ç©¶ä¸ºå®ç°æ›´å¯é çš„è‡ªåŠ¨åŒ–ç¼–ç›®å·¥ä½œæµæä¾›äº†ç†è®ºæ”¯æŒä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00867v1",
      "published_date": "2025-07-18 18:55:57 UTC",
      "updated_date": "2025-07-18 18:55:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:40.821016+00:00"
    },
    {
      "arxiv_id": "2507.14306v1",
      "title": "Manimator: Transforming Research Papers into Visual Explanations",
      "title_zh": "Manimatorï¼šå°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºå¯è§†åŒ–è®²è§£",
      "authors": [
        "Samarth P",
        "Vyoman Jain",
        "Shiva Golugula",
        "Motamarri Sai Sathvik"
      ],
      "abstract": "Understanding complex scientific and mathematical concepts, particularly those presented in dense research papers, poses a significant challenge for learners. Dynamic visualizations can greatly enhance comprehension, but creating them manually is time-consuming and requires specialized knowledge and skills. We introduce manimator, an open-source system that leverages Large Language Models to transform research papers and natural language prompts into explanatory animations using the Manim engine. Manimator employs a pipeline where an LLM interprets the input text or research paper PDF to generate a structured scene description outlining key concepts, mathematical formulas, and visual elements and another LLM translates this description into executable Manim Python code. We discuss its potential as an educational tool for rapidly creating engaging visual explanations for complex STEM topics, democratizing the creation of high-quality educational content.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†å¼€æºç³»ç»Ÿ Manimatorï¼Œæ—¨åœ¨åˆ©ç”¨ Large Language Models (LLMs) å°†å¤æ‚çš„ç§‘ç ”è®ºæ–‡å’Œæ•°å­¦æ¦‚å¿µè½¬åŒ–ä¸ºåŠ¨æ€çš„å¯è§†åŒ–è§£é‡Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆ Manim å¼•æ“ï¼Œè§£å†³äº†æ‰‹åŠ¨åˆ›å»ºåŠ¨ç”»è§†é¢‘è€—æ—¶ä¸”é—¨æ§›é«˜çš„é—®é¢˜ã€‚Manimator é‡‡ç”¨äº†ä¸€å¥—é«˜æ•ˆçš„æµæ°´çº¿ï¼Œé¦–å…ˆé€šè¿‡ LLM å¯¹è¾“å…¥çš„è®ºæ–‡ PDF æˆ–è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œè§£æï¼Œç”ŸæˆåŒ…å«å…³é”®æ¦‚å¿µå’Œæ•°å­¦å…¬å¼çš„ç»“æ„åŒ–åœºæ™¯æè¿°ï¼Œéšåå†ç”±å¦ä¸€ä¸ª LLM å°†è¯¥æè¿°è‡ªåŠ¨è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ Manim Python ä»£ç ã€‚è¿™ç§æ–¹æ³•ä¸ºå¿«é€Ÿåˆ›å»º STEM é¢†åŸŸçš„æ•™è‚²å†…å®¹æä¾›äº†å¼ºå¤§æ”¯æŒï¼Œæ˜¾è‘—é™ä½äº†é«˜è´¨é‡è§†è§‰è§£é‡Šçš„åˆ›ä½œé—¨æ§›ï¼Œå®ç°äº†ç§‘å­¦ä¼ æ’­çš„æ°‘ä¸»åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14306v1",
      "published_date": "2025-07-18 18:28:26 UTC",
      "updated_date": "2025-07-18 18:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:45.313743+00:00"
    },
    {
      "arxiv_id": "2507.14299v1",
      "title": "Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems",
      "title_zh": "æ— äººæœºèµ‹èƒ½çš„é€šæ„Ÿä¸€ä½“åŒ–ç³»ç»Ÿä¿¡æ¯é¾„æœ€å°åŒ–",
      "authors": [
        "Yu Bai",
        "Yifan Zhang",
        "Boxuan Xie",
        "Zheng Chang",
        "Yanru Zhang",
        "Riku Jantti",
        "Zhu Han"
      ],
      "abstract": "Unmanned aerial vehicles (UAVs) equipped with integrated sensing and communication (ISAC) capabilities are envisioned to play a pivotal role in future wireless networks due to their enhanced flexibility and efficiency. However, jointly optimizing UAV trajectory planning, multi-user communication, and target sensing under stringent resource constraints and time-critical conditions remains a significant challenge. To address this, we propose an Age of Information (AoI)-centric UAV-ISAC system that simultaneously performs target sensing and serves multiple ground users, emphasizing information freshness as the core performance metric. We formulate a long-term average AoI minimization problem that jointly optimizes the UAV's flight trajectory and beamforming. To tackle the high-dimensional, non-convexity of this problem, we develop a deep reinforcement learning (DRL)-based algorithm capable of providing real-time decisions on UAV movement and beamforming for both radar sensing and multi-user communication. Specifically, a Kalman filter is employed for accurate target state prediction, regularized zero-forcing is utilized to mitigate inter-user interference, and the Soft Actor-Critic algorithm is applied for training the DRL agent on continuous actions. The proposed framework adaptively balances the trade-offs between sensing accuracy and communication quality. Extensive simulation results demonstrate that our proposed method consistently achieves lower average AoI compared to baseline approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— äººæœº (UAV) è¾…åŠ©çš„é›†æˆæ„ŸçŸ¥ä¸é€šä¿¡ (ISAC) ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™å’Œæ—¶é—´æ•æ„Ÿæ¡ä»¶ä¸‹ï¼Œæ— äººæœºè½¨è¿¹è§„åˆ’ã€å¤šç”¨æˆ·é€šä¿¡åŠç›®æ ‡æ„ŸçŸ¥è”åˆä¼˜åŒ–çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ä»¥ä¿¡æ¯å¹´é¾„ (Age of Information, AoI) ä¸ºæ ¸å¿ƒçš„ç³»ç»Ÿæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºé•¿æœŸå¹³å‡ AoI æœ€å°åŒ–æ¨¡å‹æ¥è¡¡é‡ä¿¡æ¯çš„æ–°é²œåº¦ã€‚ä¸ºäº†åº”å¯¹é«˜ç»´éå‡¸ä¼˜åŒ–é—®é¢˜çš„å®æ—¶å†³ç­–éœ€æ±‚ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL) çš„ç®—æ³•ï¼Œå¹¶åº”ç”¨ Soft Actor-Critic æ¡†æ¶å¯¹æ— äººæœºè½¨è¿¹å’Œæ³¢æŸæˆå½¢è¿›è¡Œè”åˆä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé›†æˆäº†ç”¨äºç›®æ ‡çŠ¶æ€é¢„æµ‹çš„ Kalman filter å’Œå‡è½»ç”¨æˆ·é—´å¹²æ‰°çš„æ­£åˆ™åŒ–é›¶å¼ºåˆ¶ (Regularized Zero-Forcing) æŠ€æœ¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¹³è¡¡æ„ŸçŸ¥ç²¾åº¦ä¸é€šä¿¡è´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œç¡®ä¿å¤æ‚ç¯å¢ƒä¸‹çš„ç³»ç»Ÿæ€§èƒ½ã€‚ä»¿çœŸå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½å¹³å‡ AoI æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ¡ˆï¼Œä¸ºå®ç°é«˜æ•ˆã€å®æ—¶çš„ç©ºä¸­é€šä¿¡ä¸æ„ŸçŸ¥ä¸€ä½“åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14299v1",
      "published_date": "2025-07-18 18:17:09 UTC",
      "updated_date": "2025-07-18 18:17:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:48.251658+00:00"
    },
    {
      "arxiv_id": "2507.14298v1",
      "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding",
      "title_zh": "æ·±åº¦ä¸å¹¿åº¦ï¼šé¢å‘å…¨é¢å›¾è¡¨ç†è§£çš„å®šåˆ¶åŒ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ",
      "authors": [
        "Wan-Cyuan Fan",
        "Yen-Chun Chen",
        "Mengchen Liu",
        "Alexander Jacobson",
        "Lu Yuan",
        "Leonid Sigal"
      ],
      "abstract": "Recent methods for customizing Large Vision Language Models (LVLMs) for domain-specific tasks have shown promising results in scientific chart comprehension. However, existing approaches face two major limitations: First, they rely on paired data from only a few chart types, limiting generalization to wide range of chart types. Secondly, they lack targeted pre-training for chart-data alignment, which hampers the model's understanding of underlying data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth chart comprehension across diverse chart types. We propose an efficient data generation pipeline that synthesizes paired data for a wide range of chart types, along with a novel Dual-Path training strategy that enabling the model to succinctly capture essential data details while preserving robust reasoning capabilities by incorporating reasoning over the underlying data. Lastly, we establish ChartDQA, a new benchmark for evaluating not only question-answering at different levels but also underlying data understanding. Experimental results demonstrate that ChartScope significantly enhances comprehension on a wide range of chart types. The code and data are available at https://davidhalladay.github.io/chartscope_demo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChartScopeï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæ·±åº¦å›¾è¡¨ç†è§£è€Œä¼˜åŒ–çš„Large Vision Language Models (LVLMs)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å›¾è¡¨ç±»å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åŠç¼ºä¹Chart-data alignmenté¢„è®­ç»ƒç­‰å±€é™ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼Œèƒ½å¤Ÿä¸ºå¹¿æ³›çš„å›¾è¡¨ç±»å‹åˆæˆé…å¯¹æ•°æ®ï¼Œå¹¶å¼•å…¥äº†åˆ›æ–°çš„Dual-Pathè®­ç»ƒç­–ç•¥ã€‚è¯¥ç­–ç•¥ä½¿æ¨¡å‹åœ¨æ•æ‰åº•å±‚æ•°æ®å…³é”®ç»†èŠ‚çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡å¯¹æ•°æ®è¿›è¡Œæ¨ç†æ¥ç»´æŒç¨³å¥çš„Reasoningèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å»ºç«‹äº†ChartDQAåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°å¤šå±‚çº§é—®ç­”å’Œåº•å±‚æ•°æ®çš„ç†è§£ç¨‹åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒChartScopeæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å¤šç§å¤æ‚å›¾è¡¨çš„ç†è§£æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506",
      "pdf_url": "https://arxiv.org/pdf/2507.14298v1",
      "published_date": "2025-07-18 18:15:09 UTC",
      "updated_date": "2025-07-18 18:15:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:25:50.936960+00:00"
    },
    {
      "arxiv_id": "2507.14295v2",
      "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
      "title_zh": "# è®ºæ–‡æ ‡é¢˜ç¿»è¯‘ ğŸ“„\n\n---\n\nç®€å•çš„â€œå†è¯•ä¸€æ¬¡â€å³å¯æ¿€å‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šè½®æ¨ç†\n\n---\n\næˆ‘å·²ç»æ ¹æ®æ‚¨çš„å­¦æœ¯ç¿»è¯‘è¦æ±‚ï¼Œæ·±å±‚ç†è§£äº†è®ºæ–‡çš„è¯­å¢ƒï¼Œä¸ºæ‚¨æä¾›äº†åœ°é“ä¸”ç¬¦åˆå­¦æœ¯è§„èŒƒçš„ä¸­æ–‡æ ‡é¢˜ã€‚\n\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»– arXiv è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…éœ€è¦ Gemini Enterprise ååŠ©æ‚¨åˆ†æè®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸å®éªŒç»“æœï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚æˆ‘ä¹Ÿå¾ˆä¹æ„ä¸ºæ‚¨æä¾›æ›´å¤šå­¦æœ¯ä¸Šçš„æ”¯æŒï¼",
      "authors": [
        "Licheng Liu",
        "Zihan Wang",
        "Linjie Li",
        "Chenwei Xu",
        "Yiping Lu",
        "Han Liu",
        "Avirup Sil",
        "Manling Li"
      ],
      "abstract": "Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., \"Let's try again\") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨å¤šè½®å¯¹è¯ä¸­è¿›è¡Œåæ€å’Œä¿®æ­£çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰åŸºäºå•è½®å¼ºåŒ–çš„å­¦ä¹ èŒƒå¼å¯¼è‡´æ¨¡å‹å¾€å¾€å¤±å»å¤šè½®è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸”éš¾ä»¥æ ¹æ®åé¦ˆä¿®æ­£é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Unary Feedback as Observation (UFO) å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»…åˆ©ç”¨æç®€çš„ä¸€å…ƒåé¦ˆï¼ˆå¦‚ \"Let's try again\"ï¼‰æ¥è®­ç»ƒæ¨¡å‹åœ¨å¤šè½®ä¸Šä¸‹æ–‡ä¸­çš„åæ€èƒ½åŠ›ã€‚UFO èƒ½å¤Ÿè½»æ¾åº”ç”¨äºç°æœ‰çš„å•è½® Reinforcement Learning (RL) è®­ç»ƒè®¾ç½®ï¼Œä½¿æ¨¡å‹åœ¨è¿­ä»£æ±‚è§£è¿‡ç¨‹ä¸­æ›´å¥½åœ°åˆ©ç”¨ç”¨æˆ·åé¦ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ UFO è¿›è¡Œè®­ç»ƒåœ¨ä¿ç•™å•è½®ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¤šè½®æ¨ç†çš„å‡†ç¡®ç‡æå‡äº†é«˜è¾¾ 14%ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ç‰¹å®šçš„å¥–åŠ±ç»“æ„ï¼Œä»¥å‡å°‘è¾¾åˆ°æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„è½®æ•°ï¼Œå¹¶é¼“åŠ±æ¨¡å‹åœ¨å‡ºé”™æ—¶äº§ç”Ÿå¤šæ ·åŒ–ä¸”å®¡æ…çš„æ¨ç†ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç®€å•çš„æŒ‡ä»¤åé¦ˆå³å¯æ˜¾è‘—å¢å¼ºè¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šè½®é—®é¢˜è§£å†³ä¸­çš„ååº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14295v2",
      "published_date": "2025-07-18 18:07:38 UTC",
      "updated_date": "2025-08-22 16:49:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:26:30.061628+00:00"
    },
    {
      "arxiv_id": "2507.14293v1",
      "title": "WebGuard: Building a Generalizable Guardrail for Web Agents",
      "title_zh": "WebGuardï¼šæ„å»º Web æ™ºèƒ½ä½“é€šç”¨å®‰å…¨æŠ¤æ ",
      "authors": [
        "Boyuan Zheng",
        "Zeyi Liao",
        "Scott Salisbury",
        "Zeyuan Liu",
        "Michael Lin",
        "Qinyuan Zheng",
        "Zifan Wang",
        "Xiang Deng",
        "Dawn Song",
        "Huan Sun",
        "Yu Su"
      ],
      "abstract": "The rapid development of autonomous web agents powered by Large Language Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of taking unintended or harmful actions. This situation underscores an urgent need for effective safety measures, akin to access controls for human users. To address this critical challenge, we introduce WebGuard, the first comprehensive dataset designed to support the assessment of web agent action risks and facilitate the development of guardrails for real-world online environments. In doing so, WebGuard specifically focuses on predicting the outcome of state-changing actions and contains 4,939 human-annotated actions from 193 websites across 22 diverse domains, including often-overlooked long-tail websites. These actions are categorized using a novel three-tier risk schema: SAFE, LOW, and HIGH. The dataset includes designated training and test splits to support evaluation under diverse generalization settings. Our initial evaluations reveal a concerning deficiency: even frontier LLMs achieve less than 60% accuracy in predicting action outcomes and less than 60% recall in lagging HIGH-risk actions, highlighting the risks of deploying current-generation agents without dedicated safeguards. We therefore investigate fine-tuning specialized guardrail models using WebGuard. We conduct comprehensive evaluations across multiple generalization settings and find that a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from 20% to 76%. Despite these improvements, the performance still falls short of the reliability required for high-stakes deployment, where guardrails must approach near-perfect accuracy and recall.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªä¸»Webæ™ºèƒ½ä½“(Web agents)å¯èƒ½äº§ç”Ÿçš„æœ‰å®³è¡Œä¸ºé£é™©ï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºè¯„ä¼°è¡Œä¸ºé£é™©å¹¶æ„å»ºå®‰å…¨æŠ¤æ (guardrails)çš„ç»¼åˆæ•°æ®é›†WebGuardã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºé¢„æµ‹çŠ¶æ€æ”¹å˜è¡Œä¸ºçš„ç»“æœï¼Œæ¶µç›–äº†æ¥è‡ª22ä¸ªä¸åŒé¢†åŸŸçš„193ä¸ªç½‘ç«™ä¸­4,939ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„è¡Œä¸ºï¼Œå¹¶é‡‡ç”¨äº†SAFEã€LOWå’ŒHIGHä¸‰çº§é£é™©åˆ†ç±»æ¨¡å¼ã€‚å®éªŒå‘ç°ï¼Œç°æœ‰å‰æ²¿LLMsåœ¨é¢„æµ‹è¡Œä¸ºç»“æœå’Œè¯†åˆ«HIGHé£é™©è¡Œä¸ºæ–¹é¢çš„å‡†ç¡®ç‡åŠå¬å›ç‡å‡ä¸è¶³60%ï¼Œæ˜¾ç¤ºå‡ºéƒ¨ç½²æ­¤ç±»æ™ºèƒ½ä½“çš„æ½œåœ¨å®‰å…¨éšæ‚£ã€‚é€šè¿‡åœ¨WebGuardä¸Šè¿›è¡Œå¾®è°ƒï¼ŒQwen2.5VL-7Bæ¨¡å‹åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå–å¾—äº†å®è´¨æ€§çªç ´ï¼Œå‡†ç¡®ç‡ä»37%æå‡è‡³80%ï¼ŒHIGHé£é™©å¬å›ç‡ä»20%å¤§å¹…å¢å¼ºè‡³76%ã€‚å°½ç®¡æ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—ä¼˜åŒ–ï¼Œä½†ç ”ç©¶æŒ‡å‡ºç›®å‰çš„å¯é æ€§æ°´å¹³ä»ä¸è¶³ä»¥æ»¡è¶³æé«˜å®‰å…¨è¦æ±‚çš„éƒ¨ç½²ç¯å¢ƒï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å®Œç¾é˜²æŠ¤æœºåˆ¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "We publicly release WebGuard, along with its annotation tools and fine-tuned models, to facilitate open-source research on monitoring and safeguarding web agents. All resources are available at https://github.com/OSU-NLP-Group/WebGuard",
      "pdf_url": "https://arxiv.org/pdf/2507.14293v1",
      "published_date": "2025-07-18 18:06:27 UTC",
      "updated_date": "2025-07-18 18:06:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:26:00.642288+00:00"
    },
    {
      "arxiv_id": "2507.17852v1",
      "title": "Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation",
      "title_zh": "Tippy æŠ€æœ¯å®ç°ï¼šè¯ç‰©ç ”å‘å®éªŒå®¤è‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“æ¶æ„ä¸ç³»ç»Ÿè®¾è®¡",
      "authors": [
        "Yao Fehlis",
        "Charles Crain",
        "Aidan Jensen",
        "Michael Watson",
        "James Juhasz",
        "Paul Mandel",
        "Betty Liu",
        "Shawn Mahon",
        "Daren Wilson",
        "Nick Lynch-Jonely",
        "Ben Leedom",
        "David Fuller"
      ],
      "abstract": "Building on the conceptual framework presented in our previous work on agentic AI for pharmaceutical research, this paper provides a comprehensive technical analysis of Tippy's multi-agent system implementation for drug discovery laboratory automation. We present a distributed microservices architecture featuring five specialized agents (Supervisor, Molecule, Lab, Analysis, and Report) that coordinate through OpenAI Agents SDK orchestration and access laboratory tools via the Model Context Protocol (MCP). The system architecture encompasses agent-specific tool integration, asynchronous communication patterns, and comprehensive configuration management through Git-based tracking. Our production deployment strategy utilizes Kubernetes container orchestration with Helm charts, Docker containerization, and CI/CD pipelines for automated testing and deployment. The implementation integrates vector databases for RAG functionality and employs an Envoy reverse proxy for secure external access. This work demonstrates how specialized AI agents can effectively coordinate complex laboratory workflows while maintaining security, scalability, reliability, and integration with existing laboratory infrastructure through standardized protocols.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯¦ç»†ä»‹ç»äº†Tippyçš„æŠ€æœ¯å®ç°ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºè¯ç‰©ç ”å‘å®éªŒå®¤è‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent system)æ¶æ„ã€‚ç³»ç»Ÿé‡‡ç”¨äº†åˆ†å¸ƒå¼å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…å«Supervisorã€Moleculeã€Labã€Analysiså’ŒReportäº”ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡OpenAI Agents SDKè¿›è¡Œç¼–æ’ï¼Œå¹¶åˆ©ç”¨Model Context Protocol (MCP)æ ‡å‡†åè®®è®¿é—®å®éªŒå®¤å·¥å…·ã€‚åœ¨å·¥ç¨‹å®è·µä¸Šï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨Kubernetesã€Helmå’ŒDockerè¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²ï¼Œå¹¶ç»“åˆCI/CDæµæ°´çº¿å’ŒEnvoyåå‘ä»£ç†ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§ä¸å®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œå®ç°ä¸­è¿˜é›†æˆäº†å‘é‡æ•°æ®åº“ä»¥æ”¯æŒRAGåŠŸèƒ½ï¼Œä»è€Œä¼˜åŒ–çŸ¥è¯†æ£€ç´¢ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†ä¸“é—¨çš„AIæ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåè°ƒå¤æ‚çš„å®éªŒå®¤å·¥ä½œæµï¼Œåœ¨ä¿éšœå®‰å…¨æ€§ã€å¯æ‰©å±•æ€§å’Œå¯é æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸ç°æœ‰å®éªŒå®¤åŸºç¡€è®¾æ–½çš„æ— ç¼é›†æˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.17852v1",
      "published_date": "2025-07-18 17:57:40 UTC",
      "updated_date": "2025-07-18 17:57:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:26:42.599307+00:00"
    },
    {
      "arxiv_id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "title_zh": "è¿ˆå‘åŸºäºå¼ é‡åˆ†è§£çš„æ—¶åºå› æœè¡¨å¾å­¦ä¹ ",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering complex patterns in observational studies, which are often represented as low-dimensional time series. However, in many real-world applications, data are high-dimensional with varying input lengths and naturally take the form of irregular tensors. To analyze such data, irregular tensor decomposition is critical for extracting meaningful clusters that capture essential information. In this paper, we focus on modeling causal representation learning based on the transformed information. First, we present a novel causal formulation for a set of latent clusters. We then propose CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition. Notably, our framework provides a blueprint for downstream tasks using the learned tensor factors, such as modeling latent structures and extracting causal information, and offers a more flexible regularization design to enhance tensor decomposition. Theoretically, we show that our algorithm converges to a stationary point. More importantly, our results fill the gap in theoretical guarantees for the convergence of state-of-the-art irregular tensor decomposition. Experimental results on synthetic and real-world electronic health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both phenotyping and network recovery perspectives, demonstrate that our proposed method outperforms state-of-the-art techniques and enhances the explainability of causal representations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®åº”ç”¨ä¸­é«˜ç»´ä¸”é•¿åº¦ä¸ä¸€çš„ä¸è§„åˆ™å¼ é‡æ•°æ®ï¼Œæå‡ºäº†åä¸º CaRTeD çš„è”åˆå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ—¶é—´å› æœè¡¨ç¤ºå­¦ä¹  (Temporal causal representation learning) ä¸ä¸è§„åˆ™å¼ é‡åˆ†è§£ (Irregular tensor decomposition) æœ‰æœºç»“åˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨å­¦ä¹ åˆ°çš„å¼ é‡å› å­ä¸ºå»ºæ¨¡æ½œåœ¨ç»“æ„å’Œæå–å› æœä¿¡æ¯ç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›æ”¯æŒï¼Œå¹¶é€šè¿‡çµæ´»çš„æ­£åˆ™åŒ–è®¾è®¡æå‡äº†å¼ é‡åˆ†è§£çš„æ€§èƒ½ã€‚åœ¨ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶è¯æ˜äº†ç®—æ³•èƒ½å¤Ÿæ”¶æ•›è‡³é©»ç‚¹ï¼Œæœ‰æ•ˆå¡«è¡¥äº†å½“å‰æœ€å…ˆè¿›çš„ä¸è§„åˆ™å¼ é‡åˆ†è§£åœ¨æ”¶æ•›æ€§ä¿è¯æ–¹é¢çš„ç©ºç™½ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ç”µå­å¥åº·è®°å½• (EHR) æ•°æ®é›† (MIMIC-III) ä¸Šçš„å®éªŒï¼ŒCaRTeD åœ¨è¡¨å‹åˆ†æå’Œç½‘ç»œæ¢å¤ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æå‡ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†å› æœè¡¨ç¤ºçš„å¯è§£é‡Šæ€§ (Explainability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14126v1",
      "published_date": "2025-07-18 17:55:42 UTC",
      "updated_date": "2025-07-18 17:55:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:26:45.178387+00:00"
    },
    {
      "arxiv_id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "title_zh": "æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå¾·ç½‘ç»œ (KANs) åœ¨ä¸å¹³è¡¡æ•°æ®ä¸­çš„åº”ç”¨ï¼šå®è¯è§†è§’",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Kolmogorov Arnold Networks (KANs) åœ¨ç±»åˆ«ä¸å¹³è¡¡åˆ†ç±»(class imbalanced classification)ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•é‡é‡‡æ ·ç­–ç•¥(resampling)çš„æƒ…å†µä¸‹ï¼ŒKANs åœ¨åŸå§‹ä¸å¹³è¡¡æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºå¤šå±‚æ„ŸçŸ¥å™¨(MLPs)ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ç­–ç•¥ï¼ˆå¦‚é‡é‡‡æ ·å’Œ Focal Lossï¼‰ä¸ KANs çš„æ•°å­¦ç»“æ„å­˜åœ¨å†²çªï¼Œä¼šå¯¼è‡´å…¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œè¿™äº›ç­–ç•¥å¯¹ MLPs å´æœ‰ç›Šå¤„ã€‚ç»Ÿè®¡éªŒè¯è¡¨æ˜ï¼Œç»“åˆäº†ä¸å¹³è¡¡å¤„ç†æŠ€æœ¯çš„ MLPs åœ¨æ€§èƒ½ä¸Šä¸ KANs ç›¸å½“ï¼Œä¸”èµ„æºæˆæœ¬æä½ã€‚ç”±äº KANs é¢ä¸´è®¡ç®—å¼€é”€å¤§ä»¥åŠä¸æ ‡å‡†æ•°æ®å¢å¼ºæŠ€æœ¯ä¸å…¼å®¹ç­‰æŒ‘æˆ˜ï¼Œå…¶åœ¨å®é™…éƒ¨ç½²ä¸­å—åˆ°ä¸€å®šé™åˆ¶ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘é’ˆå¯¹ KANs æ¶æ„çš„ä¸å¹³è¡¡å­¦ä¹ ä¼˜åŒ–æ–¹æ¡ˆå’Œæé«˜è®¡ç®—æ•ˆç‡çš„é‡è¦æ€§ï¼Œä¸ºä¸‹ä¸€ä»£ KAN æ¶æ„çš„ç ”å‘æä¾›äº†åŸºç¡€è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 Pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.14121v1",
      "published_date": "2025-07-18 17:50:51 UTC",
      "updated_date": "2025-07-18 17:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:01.670956+00:00"
    },
    {
      "arxiv_id": "2507.14119v2",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "title_zh": "NoHumansRequiredï¼šè‡ªä¸»é«˜è´¨é‡å›¾åƒç¼–è¾‘ä¸‰å…ƒç»„æŒ–æ˜",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets (original image, instruction, edited image), yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approx. 2.6x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit, an open dataset of 720k high-quality triplets, curated at industrial scale via millions of guided generations and validator passes, and we analyze the pipeline's stage-wise survival rates, providing a framework for estimating computational effort across different model stacks. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, a fine-tuned Bagel model with state-of-the-art metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NoHumansRequired (NHR)ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ã€æ¨¡å—åŒ–çš„æµæ°´çº¿ï¼Œæ—¨åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æŒ–æ˜è·¨é¢†åŸŸã€é«˜åˆ†è¾¨ç‡ä¸”é£æ ¼å¤šæ ·çš„é«˜è´¨é‡å›¾åƒç¼–è¾‘ä¸‰å…ƒç»„ (triplets)ã€‚é’ˆå¯¹è·å–åƒç´ çº§ç²¾ç¡®æ ‡æ³¨æ ·æœ¬çš„éš¾é¢˜ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨ç»è¿‡ç‰¹å®šä»»åŠ¡è°ƒä¼˜çš„ Gemini validator ç›´æ¥å¯¹æŒ‡ä»¤éµå¾ªåº¦ (instruction adherence) å’Œç¾å­¦è´¨é‡è¿›è¡Œè¯„åˆ†ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹åˆ†å‰² (segmentation) æˆ–å®šä½æ¨¡å‹ (grounding models) çš„ä¾èµ–ã€‚é€šè¿‡å¼•å…¥åæ¼” (Inversion) å’Œç»„åˆå¼•å¯¼ (compositional bootstrapping) æŠ€æœ¯ï¼Œç ”ç©¶æˆåŠŸå°†æŒ–æ˜çš„æ•°æ®é›†è§„æ¨¡æ‰©å¤§äº†çº¦ 2.6 å€ï¼Œå®ç°äº†å¤§è§„æ¨¡é«˜ä¿çœŸè®­ç»ƒæ•°æ®çš„ç”Ÿæˆã€‚ä½œè€…å…¬å¼€å‘å¸ƒäº†åŒ…å« 720,000 ä¸ªé«˜è´¨é‡ä¸‰å…ƒç»„çš„æ•°æ®é›† NHR-Editï¼Œä»¥åŠæ€§èƒ½è¾¾åˆ° state-of-the-art æ°´å¹³çš„å¾®è°ƒæ¨¡å‹ Bagel-NHR-Editã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨æœ€å¤§è§„æ¨¡çš„è·¨æ•°æ®é›†è¯„ä¼°ä¸­è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å…¬å¼€æ–¹æ¡ˆï¼Œä¸ºå›¾åƒç¼–è¾‘é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ ‡æ³¨å’Œæ¨¡å‹è®­ç»ƒæä¾›äº†é«˜æ•ˆçš„å·¥ä¸šçº§æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14119v2",
      "published_date": "2025-07-18 17:50:00 UTC",
      "updated_date": "2025-09-25 14:06:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:58.689098+00:00"
    },
    {
      "arxiv_id": "2507.21123v1",
      "title": "Leveraging Generative AI to Enhance Synthea Module Development",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æå‡ Synthea æ¨¡å—å¼€å‘",
      "authors": [
        "Mark A. Kramer",
        "Aanchal Mathur",
        "Caroline E. Adams",
        "Jason A. Walonoski"
      ],
      "abstract": "This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator. Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data. We demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module. While the use of LLMs in this context shows promise, we also acknowledge the challenges and limitations, such as the need for human oversight, the importance of rigorous testing and validation, and the potential for inaccuracies in LLM-generated content. The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) è¾…åŠ©å¼€æºåˆæˆå¥åº·æ•°æ®ç”Ÿæˆå™¨ Synthea å¼€å‘æ–°ç–¾ç—…æ¨¡å—çš„æ–¹æ³•ã€‚å°† LLMs èå…¥æ¨¡å—å¼€å‘æµç¨‹æ—¨åœ¨ç¼©çŸ­å¼€å‘æ—¶é—´ã€é™ä½ä¸“ä¸šé—¨æ§›ã€æ‰©å±•æ¨¡å‹å¤šæ ·æ€§å¹¶æå‡åˆæˆæ‚£è€…æ•°æ®çš„æ•´ä½“è´¨é‡ã€‚ç ”ç©¶å±•ç¤ºäº† LLMs æ”¯æŒ Synthea æ¨¡å—åˆ›å»ºçš„å››ç§å…·ä½“è·¯å¾„ï¼šç”Ÿæˆç–¾ç—…æ¦‚å†µ (disease profile)ã€æ ¹æ®æ¦‚å†µç”Ÿæˆç–¾ç—…æ¨¡å—ã€è¯„ä¼°ç°æœ‰æ¨¡å—ä»¥åŠä¼˜åŒ–å®Œå–„ç°æœ‰æ¨¡å—ã€‚è®ºæ–‡é‡ç‚¹å¼•å…¥äº†â€œæ¸è¿›å¼å®Œå–„ (progressive refinement)â€çš„æ¦‚å¿µï¼Œé€šè¿‡è¿­ä»£è¯„ä¼° LLM ç”Ÿæˆæ¨¡å—çš„è¯­æ³•æ­£ç¡®æ€§å’Œä¸´åºŠå‡†ç¡®æ€§æ¥é©±åŠ¨æ¨¡å—ä¿®æ­£ã€‚å°½ç®¡ LLMs çš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œä½†ç ”ç©¶å¼ºè°ƒäº†äººå·¥ç›‘ç£ã€ä¸¥è°¨æµ‹è¯•éªŒè¯çš„å¿…è¦æ€§ï¼Œä»¥åŠåº”å¯¹ç”Ÿæˆå†…å®¹æ½œåœ¨é”™è¯¯çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶æœ€åä¸ºå®ç° LLM è¾…åŠ©åˆæˆæ•°æ®ç”Ÿæˆçš„é•¿æœŸæ½œåŠ›æå‡ºäº†æœªæ¥ç ”å‘çš„æŒ‡å¯¼æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Title: Leveraging Generative AI to Enhance Synthea Module Development Word Count: [Approximately 12,000 words] Figures: 3 Tables: 3 Supplementary Material: Extensive appendices with prompts and disease profiles",
      "pdf_url": "https://arxiv.org/pdf/2507.21123v1",
      "published_date": "2025-07-18 17:44:35 UTC",
      "updated_date": "2025-07-18 17:44:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:26:52.921085+00:00"
    },
    {
      "arxiv_id": "2507.14111v9",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "title_zh": "CUDA-L1ï¼šåŸºäºå¯¹æ¯”å¼ºåŒ–å­¦ä¹ æå‡ CUDA ä¼˜åŒ–",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\n  CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 against default baselines over across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. In addition to the default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 over Torch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDA Graph implementations, and remarkably x7.72 over cuDNN libraries. Furthermore, the model also demonstrates portability across different GPU architectures.\n  Beyond these benchmark results, CUDA-L1 demonstrates several properties: it 1) discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations; 3) identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance. The capabilities demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CUDA-L1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¯¹æ¯”å¼ºåŒ–å­¦ä¹ (Contrastive Reinforcement Learning)ç®—æ³•çš„è‡ªåŠ¨åŒ–CUDAä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æ–°å‹å¯¹æ¯”å¼ºåŒ–å­¦ä¹ (RL)ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨CUDAä»£ç ä¼˜åŒ–ä¸­æˆåŠŸç‡è¾ƒä½çš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCUDA-L1åœ¨KernelBenchçš„250ä¸ªå†…æ ¸ä¸Šå®ç°äº†3.12å€çš„å¹³å‡åŠ é€Ÿæ¯”ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºTorch Compileã€CUDA Graphä»¥åŠcuDNNåº“ã€‚é™¤äº†å“è¶Šçš„åŠ é€Ÿè¡¨ç°ï¼Œè¯¥æ¨¡å‹è¿˜å±•ç°äº†è·¨GPUæ¶æ„çš„å¯ç§»æ¤æ€§ï¼Œå¹¶èƒ½è‡ªåŠ¨å‘ç°ã€ç»„åˆå¤šç§ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶è¯†åˆ«éç›´è§‚çš„æ€§èƒ½ç“¶é¢ˆã€‚ç ”ç©¶è¯æ˜ä»…éœ€é€šè¿‡é€Ÿåº¦æå‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå³å¯åœ¨æ— éœ€ä¸“å®¶é¢†åŸŸçŸ¥è¯†çš„æƒ…å†µä¸‹å°†LLMè½¬åŒ–ä¸ºé«˜æ•ˆçš„CUDAä¼˜åŒ–å™¨ã€‚è¿™ä¸€èŒƒå¼ä¸ºè‡ªåŠ¨åŒ–æå‡GPUæ•ˆç‡å’Œç¼“è§£è®¡ç®—èµ„æºéœ€æ±‚å‹åŠ›å¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Page: https://deepreinforce-ai.github.io/cudal1_blog/",
      "pdf_url": "https://arxiv.org/pdf/2507.14111v9",
      "published_date": "2025-07-18 17:43:56 UTC",
      "updated_date": "2025-11-25 21:29:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:58.852588+00:00"
    },
    {
      "arxiv_id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¡¥æ¢çŠ¶å†µè¯„ä¼°æ— æŸæ£€æµ‹ç­‰å€¼çº¿å›¾è‡ªåŠ¨è§£è¯‘",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity. However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis. This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses. It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. In this study, several LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies for assessing bridge conditions. Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy. The research indicates that four of the nine models provide better image descriptions, effectively covering a wide range of topics related to the bridge's condition. The outputs from these four models are summarized using five different LLMs to form a comprehensive overview of the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries. The findings suggest that LLMs have the potential to significantly improve efficiency and accuracy. This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Large Language Models (LLMs) è‡ªåŠ¨è§£é‡Šéç ´åæ€§è¯„ä¼° (Non-Destructive Evaluation, NDE) ç­‰å€¼çº¿å›¾çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ¡¥æ¢çŠ¶å†µè¯„ä¼°çš„æ•ˆç‡å¹¶é™ä½å¯¹ä¸“å®¶ç»éªŒçš„ä¾èµ–ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå°† LLMs é›†æˆåˆ°æ¡¥æ¢æ£€æŸ¥å·¥ä½œæµä¸­çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºè¯ (prompts) ä¼˜åŒ–æ¨¡å‹å¯¹äº”ç§ä¸åŒ NDE æŠ€æœ¯çš„å›¾åƒæè¿°è´¨é‡ã€‚ç ”ç©¶è¯„ä¼°äº†ä¹ç§ LLMs åœ¨ç”Ÿæˆè¯¦ç»†æè¿°ã€è¯†åˆ«ç¼ºé™·åŠæä¾›è¡ŒåŠ¨å»ºè®®æ–¹é¢çš„èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºå…¶ä¸­å››ç§æ¨¡å‹åœ¨å›¾åƒæè¿°æ·±åº¦ä¸Šè¡¨ç°çªå‡ºã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒChatGPT-4 å’Œ Claude 3.5 Sonnet åœ¨æ±‡æ€»è¿™äº›æ¨¡å‹è¾“å‡ºå¹¶ç”Ÿæˆç»¼åˆæ¡¥æ¢çŠ¶å†µæŠ¥å‘Šæ–¹é¢æ•ˆæœæœ€ä½³ã€‚è¯¥è¯•ç‚¹ç ”ç©¶å±•ç¤ºäº† LLMs åœ¨å¹¶è¡Œå¤„ç†å›¾åƒæ ‡æ³¨ (image captioning) ä¸æ‘˜è¦ä»»åŠ¡ä¸­çš„åˆ›æ–°æ½œåŠ›ï¼Œä¸ºå®ç°æ›´å¿«é€Ÿçš„æ¡¥æ¢ç»´æŠ¤å†³ç­–å’Œæ›´å®‰å…¨çš„åŸºç¡€è®¾æ–½ç®¡ç†æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14107v1",
      "published_date": "2025-07-18 17:39:03 UTC",
      "updated_date": "2025-07-18 17:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:01.834376+00:00"
    },
    {
      "arxiv_id": "2507.19520v1",
      "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves",
      "title_zh": "åŸºäºåˆæˆå…‰å˜æ›²çº¿è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¢æµ‹ç³»å¤–è¡Œæ˜Ÿ",
      "authors": [
        "Ethan Lo",
        "Dan C. Lo"
      ],
      "abstract": "With manual searching processes, the rate at which scientists and astronomers discover exoplanets is slow because of inefficiencies that require an extensive time of laborious inspections. In fact, as of now there have been about only 5,000 confirmed exoplanets since the late 1900s. Recently, machine learning (ML) has proven to be extremely valuable and efficient in various fields, capable of processing massive amounts of data in addition to increasing its accuracy by learning. Though ML models for discovering exoplanets owned by large corporations (e.g. NASA) exist already, they largely depend on complex algorithms and supercomputers. In an effort to reduce such complexities, in this paper, we report the results and potential benefits of various, well-known ML models in the discovery and validation of extrasolar planets. The ML models that are examined in this study include logistic regression, k-nearest neighbors, and random forest. The dataset on which the models train and predict is acquired from NASA's Kepler space telescope. The initial results show promising scores for each model. However, potential biases and dataset imbalances necessitate the use of data augmentation techniques to further ensure fairer predictions and improved generalization. This study concludes that, in the context of searching for exoplanets, data augmentation techniques significantly improve the recall and precision, while the accuracy varies for each model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æœºå™¨å­¦ä¹ (Machine Learning)æ¨¡å‹è‡ªåŠ¨æ¢æµ‹ç³»å¤–è¡Œæ˜Ÿ(Exoplanet)çš„æ–¹æ³•ï¼Œä»¥è§£å†³ä¼ ç»Ÿäººå·¥æœç´¢æ•ˆç‡ä½ä¸‹ã€è€—æ—¶è´¹åŠ›çš„é—®é¢˜ã€‚ä¸ºäº†é™ä½ç°æœ‰æ¢æµ‹ç³»ç»Ÿå¯¹å¤æ‚ç®—æ³•å’Œè¶…çº§è®¡ç®—æœºçš„ä¾èµ–ï¼Œä½œè€…è¯„ä¼°äº†å‡ ç§ç»å…¸çš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç³»å¤–è¡Œæ˜Ÿå‘ç°ä¸éªŒè¯ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå…·ä½“è€ƒæŸ¥äº†é€»è¾‘å›å½’(Logistic Regression)ã€K-æœ€è¿‘é‚»(k-nearest neighbors)å’Œéšæœºæ£®æ—(Random Forest)ä¸‰ç§æ¨¡å‹ã€‚ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ªNASA Keplerç©ºé—´æœ›è¿œé•œçš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹æ•°æ®åè§å’Œç±»åˆ«ä¸å¹³è¡¡(imbalances)é—®é¢˜å¼•å…¥äº†æ•°æ®å¢å¼º(Data Augmentation)æŠ€æœ¯ï¼Œä»¥ç¡®ä¿é¢„æµ‹çš„å…¬æ­£æ€§å¹¶æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ä¸åŒæ¨¡å‹çš„å‡†ç¡®ç‡(accuracy)è¡¨ç°å„å¼‚ï¼Œä½†æ•°æ®å¢å¼ºæ˜¾è‘—æé«˜äº†æ¢æµ‹çš„å¬å›ç‡(recall)å’Œç²¾ç¡®ç‡(precision)ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºåˆæˆå…‰å˜æ›²çº¿(synthetic light curves)è®­ç»ƒçš„ç®€åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç³»å¤–è¡Œæ˜Ÿæœå¯»é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19520v1",
      "published_date": "2025-07-18 17:25:25 UTC",
      "updated_date": "2025-07-18 17:25:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:06.253858+00:00"
    },
    {
      "arxiv_id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é©±åŠ¨çš„é«˜ä¿çœŸäººä½“è¿åŠ¨ä»¿çœŸ",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†G-AI-HMSï¼Œä¸€ç§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é©±åŠ¨çš„é«˜ä¿çœŸäººä½“è¿åŠ¨æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šä»»åŠ¡ä¸­äººä½“è¿åŠ¨æ¨¡æ‹Ÿ(Human Motion Simulation)çœŸå®æ„Ÿä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†text-to-textå’Œtext-to-motionæ¨¡å‹ï¼Œåˆ©ç”¨Large Language Modelså°†ä»»åŠ¡æè¿°è½¬åŒ–ä¸ºä¸MotionGPTè®­ç»ƒè¯æ±‡ä¸€è‡´çš„è¿åŠ¨æ„ŸçŸ¥è¯­è¨€ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨è®¡ç®—æœºè§†è§‰ä¸­çš„å§¿æ€ä¼°è®¡ç®—æ³•å¯¹AIç”Ÿæˆçš„åŠ¨ä½œè¿›è¡ŒéªŒè¯ï¼Œé€šè¿‡æå–çœŸå®è§†é¢‘çš„å…³èŠ‚å…³é”®ç‚¹å¹¶è®¡ç®—è¿åŠ¨ç›¸ä¼¼åº¦è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAIå¢å¼ºçš„è¿åŠ¨åœ¨ç©ºé—´å‡†ç¡®åº¦ã€å¯¹é½åº¦å’Œæ—¶é—´ç›¸ä¼¼æ€§ç­‰å¤šæ•°åœºæ™¯ä¸‹ä¼˜äºäººç±»ç”Ÿæˆçš„æè¿°ã€‚ç»Ÿè®¡åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå§¿æ€å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å…³èŠ‚è¯¯å·®å’Œæ—¶é—´é”™ä½(p < 0.0001)ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå·¥ä¸šåœºæ™¯ä¸‹å·¥äººçš„å®‰å…¨ä¸ç”Ÿäº§åŠ›è¯„ä¼°æä¾›äº†æ›´é«˜ä¿çœŸçš„è¿åŠ¨æ¨¡æ‹Ÿæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14097v1",
      "published_date": "2025-07-18 17:24:50 UTC",
      "updated_date": "2025-07-18 17:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:17.117422+00:00"
    },
    {
      "arxiv_id": "2507.14096v2",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "title_zh": "TREC ç”Ÿç‰©åŒ»å­¦æ‘˜è¦é€šä¿—åŒ–æ”¹ç¼– (PLABA) ä¸“é¢˜ä»»åŠ¡çš„ç»éªŒä¸å¯ç¤º",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.",
      "tldr_zh": "æœ¬ç ”ç©¶æ€»ç»“äº†2023å¹´å’Œ2024å¹´æ–‡æœ¬æ£€ç´¢ä¼šè®®(Text Retrieval Conferences)ä¸­ç”Ÿç‰©åŒ»å­¦æ‘˜è¦é€šä¿—è¯­è¨€æ”¹ç¼–(Plain Language Adaptation of Biomedical Abstracts, PLABA)è¯„æµ‹ä»»åŠ¡çš„ç»éªŒæ•™è®­ã€‚è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹å°†ä¸“ä¸šåŒ»å­¦æ–‡çŒ®è½¬åŒ–ä¸ºæ‚£è€…å’ŒæŠ¤ç†äººå‘˜æ˜“æ‡‚è¯­è¨€çš„èƒ½åŠ›ï¼Œè®¾ç½®äº†å…¨æ–‡å¥çº§æ”¹å†™(Task 1)ä»¥åŠéš¾ç‚¹è¯æ±‡è¯†åˆ«ä¸æ›¿æ¢(Task 2)ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ”¹å†™ä»»åŠ¡ä¸­ï¼Œé¡¶å°–æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§å·²æ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³ï¼Œä½†åœ¨ç®€æ´æ€§(brevity)å’Œé€šä¿—åŒ–ç¨‹åº¦(simplicity)æ–¹é¢ä»å­˜åœ¨å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç°æœ‰çš„å‚è€ƒè¯„ä¼°æŒ‡æ ‡(automatic metrics)ä¸ä¸“å®¶çš„äººå·¥è¯„è®®ç»“æœç›¸å…³æ€§è¾ƒä½ï¼Œåæ˜ å‡ºè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•å·¥å…·çš„å±€é™æ€§ã€‚åœ¨è¯æ±‡æ›¿æ¢ä»»åŠ¡ä¸­ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ç³»ç»Ÿåœ¨å‡†ç¡®æ€§å’Œé€šä¿—æ€§ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾å‡†è¯†åˆ«éš¾ç‚¹è¯æ±‡æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥è¯„æµ‹è¯æ˜äº†å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦çŸ¥è¯†å¤§ä¼—åŒ–ä¼ æ’­æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å…¶åœ¨ç”Ÿæˆç®€ç»ƒå†…å®¹åŠå»ºç«‹ç§‘å­¦è¯„ä¼°ä½“ç³»æ–¹é¢çš„æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14096v2",
      "published_date": "2025-07-18 17:23:52 UTC",
      "updated_date": "2025-07-21 18:01:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:27:18.110785+00:00"
    },
    {
      "arxiv_id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "title_zh": "ç”¨äºè„ŠæŸ±ä¾§å¼¯è¯„ä¼°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¤šä¸­å¿ƒéªŒè¯",
      "authors": [
        "Å imon Kubov",
        "Simon KlÃ­ÄnÃ­k",
        "Jakub DandÃ¡r",
        "ZdenÄ›k Straka",
        "KarolÃ­na KvakovÃ¡",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment decisions depend on precise Cobb angle measurement. Manual assessment is time consuming and subject to inter observer variation. We conducted a retrospective, multi centre evaluation of a fully automated deep learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Bland Altman analysis, mean absolute error (MAE), root mean squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four grade severity classification. Against Radiologist 1 the AI achieved an MAE of 3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59). These results demonstrate that the proposed software reproduces expert level Cobb angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¸€ç§ç”¨äºè„ŠæŸ±ä¾§å¼¯(Scoliosis)è¯„ä¼°çš„å…¨è‡ªåŠ¨æ·±åº¦å­¦ä¹ (Deep Learning)è½¯ä»¶ Carebot AI Bones è¿›è¡Œäº†å¤šä¸­å¿ƒå›é¡¾æ€§éªŒè¯ã€‚å®éªŒæ”¶é›†äº†æ¥è‡ª10å®¶åŒ»é™¢çš„103å¼ ç«™ç«‹ä½å…¨è„ŠæŸ±Xå°„çº¿å¹³ç‰‡ï¼Œå¹¶ç”±ä¸¤åè‚Œè‚‰éª¨éª¼æ”¾å°„ç§‘åŒ»ç”Ÿç‹¬ç«‹æµ‹é‡ Cobb angle ä½œä¸ºå‚è€ƒæ ‡å‡†ã€‚ç ”ç©¶é€šè¿‡ Bland Altman åˆ†æã€å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€å‡æ–¹æ ¹è¯¯å·®(RMSE)åŠ Pearson correlation ç­‰æŒ‡æ ‡è¯„ä¼°ä¸€è‡´æ€§ï¼Œç»“æœæ˜¾ç¤º AI ä¸åŒ»ç”Ÿé—´çš„ MAE ä»…ä¸º 3.89 å’Œ 3.90 åº¦ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜è¯¥è½¯ä»¶åœ¨ä¸¥é‡ç¨‹åº¦åˆ†çº§ä¸Šçš„ Cohen kappa è¡¨ç°ä¸ä¸“å®¶é—´æ°´å¹³ç›¸å½“ï¼Œç›¸å…³ç³»æ•°æœ€é«˜è¾¾ 0.906ã€‚è¯¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸­å¿ƒç¯å¢ƒä¸‹å¤ç°ä¸“å®¶çº§çš„ Cobb angle æµ‹é‡å’Œåˆ†ç±»ç²¾åº¦ã€‚è¿™ä¸€æˆæœä¸ºåœ¨ä¸´åºŠå·¥ä½œæµä¸­ç®€åŒ–è„ŠæŸ±ä¾§å¼¯æŠ¥å‘Šä¸åˆ†è¯Šæµç¨‹æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14093v1",
      "published_date": "2025-07-18 17:21:53 UTC",
      "updated_date": "2025-07-18 17:21:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:11.444835+00:00"
    },
    {
      "arxiv_id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "title_zh": "æƒ…æ„Ÿä¸è®°å¿†çš„å…³è”ï¼šå¯è®°å¿†æ€§æ ‡æ³¨å¯¹æ™ºèƒ½ç³»ç»Ÿæ˜¯å¦é‡è¦ï¼Ÿ",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting the less relevant information. Possessing awareness of event memorability for a user could help intelligent systems in more accurate user modelling, especially for such applications as meeting support systems, memory augmentation, and meeting summarisation. Emotion recognition has been widely studied, since emotions are thought to signal moments of high personal relevance to users. The emotional experience of situations and their memorability have traditionally been considered to be closely tied to one another: moments that are experienced as highly emotional are considered to also be highly memorable. This relationship suggests that emotional annotations could serve as proxies for memorability. However, existing emotion recognition systems rely heavily on third-party annotations, which may not accurately represent the first-person experience of emotional relevance and memorability. This is why, in this study, we empirically examine the relationship between perceived group emotions (Pleasure-Arousal) and group memorability in the context of conversational interactions. Our investigation involves continuous time-based annotations of both emotions and memorability in dynamic, unstructured group settings, approximating conditions of real-world conversational AI applications such as online meeting support systems. Our results show that the observed relationship between affect and memorability annotations cannot be reliably distinguished from what might be expected under random chance. We discuss the implications of this surprising finding for the development and applications of Affective Computing technology. In addition, we contextualise our findings in broader discourses in the Affective Computing and point out important targets for future research efforts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­æƒ…æ„Ÿæ ‡æ³¨æ˜¯å¦èƒ½ä½œä¸ºè®°å¿†æ€§(Memorability)çš„ä»£ç†æŒ‡æ ‡ï¼Œä»¥ä¼˜åŒ–ç”¨æˆ·å»ºæ¨¡å’Œä¼šè®®æ‘˜è¦ç­‰åº”ç”¨ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹åœ¨çº¿ä¼šè®®ç­‰åŠ¨æ€éç»“æ„åŒ–ç¾¤ä½“åœºæ™¯ï¼Œå¯¹æ„ŸçŸ¥çš„ç¾¤ä½“æƒ…æ„Ÿ(Pleasure-Arousal)ä¸ç¾¤ä½“è®°å¿†æ€§è¿›è¡Œäº†åŸºäºæ—¶é—´çš„è¿ç»­æ ‡æ³¨å’Œå®è¯ç ”ç©¶ã€‚å®éªŒç»“æœå‡ºäººæ„æ–™åœ°æ˜¾ç¤ºï¼Œè§‚æµ‹åˆ°çš„æƒ…æ„Ÿä¸è®°å¿†æ€§æ ‡æ³¨ä¹‹é—´çš„å…³ç³»åœ¨ç»Ÿè®¡ä¸Šä¸éšæœºç»“æœæ— å¼‚ï¼Œæ— æ³•è¢«å¯é åŒºåˆ†ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†æƒ…æ„Ÿè®¡ç®—(Affective Computing)é¢†åŸŸä¸­â€œé«˜æƒ…ç»ªå³é«˜è®°å¿†æ€§â€çš„ä¼ ç»Ÿå‡è®¾ï¼Œæ­ç¤ºäº†ç¬¬ä¸‰æ–¹æ ‡æ³¨åœ¨æ•æ‰ç¬¬ä¸€äººç§°æƒ…æ„Ÿç›¸å…³æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥æƒ…æ„Ÿè®¡ç®—æŠ€æœ¯çš„å‘å±•æä¾›äº†é‡è¦è§†è§’ï¼Œå¹¶æŒ‡å‡ºäº†ç”¨æˆ·å»ºæ¨¡ç ”ç©¶ä¸­äºŸéœ€å…³æ³¨çš„æ–°ç›®æ ‡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14084v1",
      "published_date": "2025-07-18 17:06:34 UTC",
      "updated_date": "2025-07-18 17:06:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:12.944741+00:00"
    },
    {
      "arxiv_id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "title_zh": "DENSEï¼šåŸºäºè·¨å°±è¯Šå¼‚æ„ä¸´åºŠè®°å½•æ—¶åºå»ºæ¨¡çš„çºµå‘ç—…ç¨‹è®°å½•ç”Ÿæˆ",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å¥åº·æ¡£æ¡ˆ(Electronic Health Record)ä¸­ç—…ç¨‹è®°å½•(Progress notes)ä¸¥é‡ç¼ºå¤±ä¸”ç¼ºä¹çºµå‘å™è¿°ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†DENSEç³»ç»Ÿã€‚DENSEé€šè¿‡æ¨¡æ‹ŸåŒ»ç”ŸæŸ¥é˜…æ—¢å¾€ç—…å†ç¼–å†™è®°å½•çš„å·¥ä½œæµç¨‹ï¼Œå¼•å…¥äº†ç»†ç²’åº¦çš„ç¬”è®°åˆ†ç±»å’Œæ—¶é—´å¯¹é½æœºåˆ¶ï¼Œå°†å¤šå°±è¯Šæ‰¹æ¬¡çš„å¼‚æ„ç¬”è®°ç»„ç»‡ä¸ºç»“æ„åŒ–çš„æ—¶é—´åºåˆ—è¾“å…¥ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸´åºŠé©±åŠ¨çš„æ£€ç´¢ç­–ç•¥æå–æ—¶é—´ä¸è¯­ä¹‰ç›¸å…³çš„è¯æ®ï¼Œå¹¶ç»“åˆå¤§è¯­è¨€æ¨¡å‹(Large Language Model)ç”Ÿæˆå…·æœ‰ä¸´åºŠä¸€è‡´æ€§å’Œæ—¶é—´æ„ŸçŸ¥èƒ½åŠ›çš„ç—…ç¨‹è®°å½•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDENSEç”Ÿæˆçš„ç¬”è®°åœ¨çºµå‘å¿ å®åº¦ä¸Šè¡¨ç°å“è¶Šï¼Œå…¶æ—¶é—´å¯¹é½ç‡è¾¾åˆ°1.089ï¼Œè¶…è¿‡äº†åŸå§‹æ–‡æ¡£çš„è¿ç»­æ€§ã€‚é€šè¿‡æ¢å¤ç¢ç‰‡åŒ–ä¸´åºŠæ–‡æ¡£ä¸­çš„å™è¿°è¿è´¯æ€§ï¼Œè¯¥ç³»ç»Ÿä¸ºåŒ»ç–—åœºæ™¯ä¸‹çš„è‡ªåŠ¨åŒ–ç¬”è®°åˆæˆæä¾›äº†å¯æ‰©å±•çš„æ–¹æ¡ˆï¼Œå¹¶æ˜¾è‘—æå‡äº†ä¸´åºŠæ€»ç»“å’Œå†³ç­–æ”¯æŒç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14079v1",
      "published_date": "2025-07-18 17:00:27 UTC",
      "updated_date": "2025-07-18 17:00:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:19.512423+00:00"
    },
    {
      "arxiv_id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "title_zh": "Glucose-MLï¼šåŠ©åŠ›ç¨³å¥äººå·¥æ™ºèƒ½æ–¹æ¡ˆå¼€å‘çš„çºµå‘ç³–å°¿ç—…æ•°æ®é›†é›†åˆ",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of state-of-the-art digital health technology for diabetes management. Yet, access to large high-quality datasets is creating barriers that impede development of robust AI solutions. To accelerate development of transparent, reproducible, and robust AI solutions, we present Glucose-ML, a collection of 10 publicly available diabetes datasets, released within the last 7 years (i.e., 2018 - 2025). The Glucose-ML collection comprises over 300,000 days of continuous glucose monitor (CGM) data with a total of 38 million glucose samples collected from 2500+ people across 4 countries. Participants include persons living with type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support researchers and innovators with using this rich collection of diabetes datasets, we present a comparative analysis to guide algorithm developers with data selection. Additionally, we conduct a case study for the task of blood glucose prediction - one of the most common AI tasks within the field. Through this case study, we provide a benchmark for short-term blood glucose prediction across all 10 publicly available diabetes datasets within the Glucose-ML collection. We show that the same algorithm can have significantly different prediction results when developed/evaluated with different datasets. Findings from this study are then used to inform recommendations for developing robust AI solutions within the diabetes or broader health domain. We provide direct links to each longitudinal diabetes dataset in the Glucose-ML collection and openly provide our code.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Glucose-MLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 10 ä¸ªå…¬å¼€ç³–å°¿ç—…æ•°æ®é›†çš„èµ„æºé›†åˆï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡å¤§è§„æ¨¡æ•°æ®ç¨€ç¼ºå¯¹å¼€å‘é²æ£’æ€§ AI æ–¹æ¡ˆçš„åˆ¶çº¦ã€‚è¯¥é›†åˆæ±‡é›†äº† 2018 å¹´è‡³ 2025 å¹´é—´æ¥è‡ª 4 ä¸ªå›½å®¶ã€2500 å¤šåå—è¯•è€…çš„ 3800 ä¸‡ä¸ª Continuous Glucose Monitor (CGM) æ ·æœ¬ï¼Œå—è¯•è€…æ¶µç›– Type 1 Diabetesã€Type 2 Diabetesã€Prediabetes åŠå¥åº·äººç¾¤ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”åˆ†æå¼•å¯¼å¼€å‘è€…è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œå¹¶é’ˆå¯¹ Blood Glucose Prediction è¿™ä¸€æ ¸å¿ƒä»»åŠ¡åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå»ºç«‹äº†åŸºå‡†æµ‹è¯• (Benchmark)ã€‚å®éªŒå‘ç°ï¼Œç›¸åŒç®—æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„é¢„æµ‹è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹è¯„ä¼°æ¨¡å‹ Robustness çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å…¬å¼€äº†æ‰€æœ‰æ•°æ®é›†é“¾æ¥åŠä»£ç ï¼Œä¸ºæ„å»ºé€æ˜ä¸”å¯é‡å¤çš„æ•°å­—å¥åº· AI è§£å†³æ–¹æ¡ˆæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.14077v1",
      "published_date": "2025-07-18 16:53:05 UTC",
      "updated_date": "2025-07-18 16:53:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:30.956887+00:00"
    },
    {
      "arxiv_id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "title_zh": "åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„è¾¹ç¼˜æ™ºèƒ½",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred growing interest in enabling intelligent services directly on resource-constrained devices. While traditional deep learning models require significant computational resources and centralized data management, the resulting latency, bandwidth consumption, and privacy concerns have exposed critical limitations in cloud-centric paradigms. Brain-inspired computing, particularly Spiking Neural Networks (SNNs), offers a promising alternative by emulating biological neuronal dynamics to achieve low-power, event-driven computation. This survey provides a comprehensive overview of Edge Intelligence based on SNNs (EdgeSNNs), examining their potential to address the challenges of on-device learning, inference, and security in edge scenarios. We present a systematic taxonomy of EdgeSNN foundations, encompassing neuron models, learning algorithms, and supporting hardware platforms. Three representative practical considerations of EdgeSNN are discussed in depth: on-device inference using lightweight SNN models, resource-aware training and updating under non-stationary data conditions, and secure and privacy-preserving issues. Furthermore, we highlight the limitations of evaluating EdgeSNNs on conventional hardware and introduce a dual-track benchmarking strategy to support fair comparisons and hardware-aware optimization. Through this study, we aim to bridge the gap between brain-inspired learning and practical edge deployment, offering insights into current advancements, open challenges, and future research directions. To the best of our knowledge, this is the first dedicated and comprehensive survey on EdgeSNNs, providing an essential reference for researchers and practitioners working at the intersection of neuromorphic computing and edge intelligence.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†åŸºäº Spiking Neural Networks (SNNs) çš„è¾¹ç¼˜æ™ºèƒ½ (EdgeSNNs)ï¼Œæ—¨åœ¨åˆ©ç”¨ç±»è„‘è®¡ç®—çš„ä½åŠŸè€—å’Œäº‹ä»¶é©±åŠ¨ç‰¹æ€§ï¼Œè§£å†³ä¼ ç»Ÿæ·±åº¦å­¦ä¹ åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šé¢ä¸´çš„é«˜åŠŸè€—ã€å»¶è¿ŸåŠéšç§æŒ‘æˆ˜ã€‚æ–‡ç« ç³»ç»Ÿæ€§åœ°æ„å»ºäº† EdgeSNNs çš„åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–äº†ç¥ç»å…ƒæ¨¡å‹ (neuron models)ã€å­¦ä¹ ç®—æ³• (learning algorithms) ä»¥åŠæ”¯æŒçš„ç¡¬ä»¶å¹³å°ã€‚ç ”ç©¶é‡ç‚¹è®¨è®ºäº† EdgeSNNs åœ¨å®é™…éƒ¨ç½²ä¸­çš„ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šè½»é‡åŒ–æ¨¡å‹çš„è®¾å¤‡ç«¯æ¨ç†ã€éå¹³ç¨³æ•°æ®æ¡ä»¶ä¸‹çš„èµ„æºæ„ŸçŸ¥è®­ç»ƒä¸æ›´æ–°ï¼Œä»¥åŠå®‰å…¨ä¸éšç§ä¿æŠ¤ã€‚é’ˆå¯¹ä¼ ç»Ÿç¡¬ä»¶è¯„ä¼° SNN çš„å±€é™æ€§ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§åŒè½¨åŸºå‡†æµ‹è¯•ç­–ç•¥ (dual-track benchmarking strategy)ï¼Œä»¥æ”¯æŒç¡¬ä»¶æ„ŸçŸ¥çš„ä¼˜åŒ–å’Œå…¬å¹³æ¯”è¾ƒã€‚ä½œä¸ºé¦–ç¯‡ä¸“é—¨é’ˆå¯¹ EdgeSNNs çš„å…¨é¢ç»¼è¿°ï¼Œè¯¥ç ”ç©¶è¡”æ¥äº†ç±»è„‘å­¦ä¹ ä¸å®é™…è¾¹ç¼˜åº”ç”¨ï¼Œä¸ºç¥ç»å½¢æ€è®¡ç®— (neuromorphic computing) ä¸è¾¹ç¼˜æ™ºèƒ½é¢†åŸŸçš„ç§‘ç ”äººå‘˜æä¾›äº†é‡è¦çš„å‚è€ƒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "primary_category": "cs.DC",
      "comment": "This work has been submitted to Proceeding of IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2507.14069v1",
      "published_date": "2025-07-18 16:47:52 UTC",
      "updated_date": "2025-07-18 16:47:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:21.849694+00:00"
    },
    {
      "arxiv_id": "2507.14067v2",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "title_zh": "VLA-Markï¼šé¢å‘å¤§è§„æ¨¡è§†è§‰-è¯­è¨€å¯¹é½æ¨¡å‹çš„è·¨æ¨¡æ€æ°´å°",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "Junyan Zhang",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VLA-Markï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡è§†è§‰è¯­è¨€å¯¹é½æ¨¡å‹ (Vision-Language Alignment Models) è®¾è®¡çš„è·¨æ¨¡æ€æ°´å°æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤çŸ¥è¯†äº§æƒçš„åŒæ—¶ä¿æŒå¤šæ¨¡æ€çš„ä¸€è‡´æ€§ã€‚ç°æœ‰çš„æ–‡æœ¬æ°´å°æ–¹æ³•å¾€å¾€ä¼šç ´åè§†è§‰ä¸æ–‡æœ¬çš„å¯¹é½ï¼Œè€Œ VLA-Mark é€šè¿‡æ•´åˆå¤šå°ºåº¦è§†è§‰-æ–‡æœ¬å¯¹é½æŒ‡æ ‡å’Œè·¨æ¨¡æ€åè°ƒæŠ€æœ¯ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°äº†å¯æ£€æµ‹æ°´å°çš„åµŒå…¥ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å±€éƒ¨è¡¥ä¸äº²å’ŒåŠ› (Localized Patch Affinity)ã€å…¨å±€è¯­ä¹‰ç›¸å¹²æ€§å’Œä¸Šä¸‹æ–‡æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨ç†µæ•æ„Ÿæœºåˆ¶åŠ¨æ€å¹³è¡¡æ°´å°å¼ºåº¦ä¸è¯­ä¹‰ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒVLA-Mark åœ¨é™ä½å›°æƒ‘åº¦ (PPL) å’Œæå‡ BLEU åˆ†æ•°æ–¹é¢è¡¨ç°å“è¶Šï¼Œä¸”æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ° 98.8% AUCã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨åº”å¯¹æ”¹å†™å’ŒåŒä¹‰è¯æ›¿æ¢æ”»å‡»æ—¶å±•ç°å‡º 96.1% çš„æŠ—æ”»å‡»èƒ½åŠ›ï¼Œå¹¶åœ¨ç»´æŒæ–‡æœ¬ä¸è§†è§‰ä¸€è‡´æ€§çš„å‰æä¸‹ï¼Œä¸ºé«˜è´¨é‡çš„å¤šæ¨¡æ€æ°´å°æŠ€æœ¯ç¡®ç«‹äº†æ–°çš„æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by the main conference, EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.14067v2",
      "published_date": "2025-07-18 16:44:41 UTC",
      "updated_date": "2025-09-19 06:54:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:24.618364+00:00"
    },
    {
      "arxiv_id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "title_zh": "å—å»ç”²è‚¾ä¸Šè…ºç´ æœºåˆ¶å¯å‘çš„å¢ç›Šè°ƒåˆ¶ç¼“è§£äº†è”åˆè®­ç»ƒä¸­çš„ç¨³å®šæ€§å·®è·",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒç»­å­¦ä¹ (Continual Learning)ä¸­å­˜åœ¨çš„â€œç¨³å®šæ€§å·®è·â€(stability gap)ç°è±¡ï¼Œå³åœ¨åŒåŒ–æ–°ä»»åŠ¡æ—¶å·²æŒæ¡ä»»åŠ¡çš„æ€§èƒ½ä¼šå‡ºç°æš‚æ—¶æ€§ä¸‹é™ï¼Œå³ä½¿åœ¨ç†æƒ³çš„è”åˆè®­ç»ƒ(joint training)åœºæ™¯ä¸‹è¯¥é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€åæ˜ å¿«é€Ÿé€‚åº”ä¸ç¨³å¥ä¿æŒå¤±è¡¡çš„é—®é¢˜ï¼Œå—å¤§è„‘ä¸­ç”±è“æ–‘ä»‹å¯¼çš„å»ç”²è‚¾ä¸Šè…ºç´ (noradrenergic)è°ƒèŠ‚æœºåˆ¶å¯å‘ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸ç¡®å®šæ€§è°ƒèŠ‚çš„å¢ç›ŠåŠ¨åŠ›å­¦(uncertainty-modulated gain dynamics)ã€‚è¯¥æœºåˆ¶é€šè¿‡è¿‘ä¼¼åŒæ—¶é—´å°ºåº¦ä¼˜åŒ–å™¨ï¼ŒåŠ¨æ€å¹³è¡¡æ–°çŸ¥è¯†çš„é›†æˆä¸æ—§ä¿¡æ¯çš„ä¿ç•™ï¼Œä»è€Œæœ€å°åŒ–å¯¹å·²å›ºåŒ–çŸ¥è¯†çš„å¹²æ‰°ã€‚é€šè¿‡åœ¨ MNIST å’Œ CIFAR åŸºå‡†æµ‹è¯•çš„åŸŸå¢é‡(domain-incremental)åŠç±»å¢é‡(class-incremental)å˜ä½“ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¯æ˜è¯¥æœºåˆ¶èƒ½æœ‰æ•ˆç¼“è§£ç¨³å®šæ€§å·®è·ã€‚è¯¥ç ”ç©¶é˜æ˜äº†å¢ç›Šè°ƒèŠ‚å¦‚ä½•é€šè¿‡æ¨¡æ‹Ÿçš®å±‚å›è·¯åŠŸèƒ½æ¥ä¼˜åŒ–æ€§èƒ½ï¼Œä¸ºç†è§£å’Œå‡å°‘æŒç»­å­¦ä¹ ä¸­çš„ç¨³å®šæ€§å·®è·æä¾›äº†æœºæ¢°è®ºè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 5 figures, 1 table, 1 pseudo-code",
      "pdf_url": "https://arxiv.org/pdf/2507.14056v1",
      "published_date": "2025-07-18 16:34:06 UTC",
      "updated_date": "2025-07-18 16:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:26.031375+00:00"
    },
    {
      "arxiv_id": "2507.14272v1",
      "title": "NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images",
      "title_zh": "NuSeCï¼šä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒç»†èƒæ ¸åˆ†å‰²æ•°æ®é›†",
      "authors": [
        "Refik Samet",
        "Nooshin Nemati",
        "Emrah Hancer",
        "Serpil Sak",
        "Bilge Ayca Kirmizi"
      ],
      "abstract": "The NuSeC dataset is created by selecting 4 images with the size of 1024*1024 pixels from the slides of each patient among 25 patients. Therefore, there are a total of 100 images in the NuSeC dataset. To carry out a consistent comparative analysis between the methods that will be developed using the NuSeC dataset by the researchers in the future, we divide the NuSeC dataset 75% as the training set and 25% as the testing set. In detail, an image is randomly selected from 4 images of each patient among 25 patients to build the testing set, and then the remaining images are reserved for the training set. While the training set includes 75 images with around 30000 nuclei structures, the testing set includes 25 images with around 6000 nuclei structures.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†NuSeCæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä¹³è…ºç™Œç»„ç»‡ç—…ç†å›¾åƒç»†èƒæ ¸åˆ†å‰²(Nuclei Segmentation)çš„æ–°å‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª25åæ‚£è€…çš„100å¼ 1024*1024åƒç´ çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–äº†çº¦36,000ä¸ªç»†èƒæ ¸ç»“æ„ã€‚ä¸ºäº†ä¾¿äºåç»­ç ”ç©¶äººå‘˜è¿›è¡Œä¸€è‡´çš„ç®—æ³•æ€§èƒ½å¯¹æ¯”ï¼Œç ”ç©¶è€…å°†æ•°æ®é›†æŒ‰ç…§75%è®­ç»ƒé›†ï¼ˆ75å¼ å›¾åƒï¼Œçº¦30,000ä¸ªç»†èƒæ ¸ï¼‰å’Œ25%æµ‹è¯•é›†ï¼ˆ25å¼ å›¾åƒï¼Œçº¦6,000ä¸ªç»†èƒæ ¸ï¼‰çš„æ¯”ä¾‹è¿›è¡Œäº†åˆ’åˆ†ã€‚åˆ’åˆ†è¿‡ç¨‹é‡‡ç”¨äº†åŸºäºæ‚£è€…çš„éšæœºæŠ½æ ·ç­–ç•¥ï¼Œå³ä»æ¯ä½æ‚£è€…çš„4å¼ å›¾åƒä¸­éšæœºé€‰å–1å¼ è¿›å…¥æµ‹è¯•é›†ï¼Œå…¶ä½™ç•™ä½œè®­ç»ƒã€‚NuSeCæ•°æ®é›†çš„å»ºç«‹ä¸ºä¹³è…ºç™Œè¯Šæ–­ä¸­çš„ç»†èƒæ ¸å½¢æ€åˆ†æå’Œè‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯å¼€å‘æä¾›äº†æ ‡å‡†åŒ–çš„æ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14272v1",
      "published_date": "2025-07-18 16:23:07 UTC",
      "updated_date": "2025-07-18 16:23:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:30.788844+00:00"
    },
    {
      "arxiv_id": "2507.14271v1",
      "title": "MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images",
      "title_zh": "MiDeSeCï¼šä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒæœ‰ä¸åˆ†è£‚æ£€æµ‹ä¸åˆ†å‰²æ•°æ®é›†",
      "authors": [
        "Refik Samet",
        "Nooshin Nemati",
        "Emrah Hancer",
        "Serpil Sak",
        "Bilge Ayca Kirmizi",
        "Zeynep Yildirim"
      ],
      "abstract": "The MiDeSeC dataset is created through H&E stained invasive breast carcinoma, no special type (NST) slides of 25 different patients captured at 40x magnification from the Department of Medical Pathology at Ankara University. The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and Olympus BX50 microscope. As several possible mitosis shapes exist, it is crucial to have a large dataset to cover all the cases. Accordingly, a total of 50 regions is selected from glass slides for 25 patients, each of regions with a size of 1024*1024 pixels. There are more than 500 mitoses in total in these 50 regions. Two-thirds of the regions are reserved for training, the other third for testing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MiDeSeC æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ (Mitosis Detection) å’Œåˆ†å‰² (Segmentation)ã€‚è¯¥æ•°æ®é›†æ¥æºäº 25 åæµ¸æ¶¦æ€§ä¹³è…ºç™Œ (NST) æ‚£è€…çš„ H&E æŸ“è‰²åˆ‡ç‰‡ï¼Œå¹¶åœ¨ 40x æ”¾å¤§å€ç‡ä¸‹é‡‡é›†ã€‚å›¾åƒé€šè¿‡ 3D Histech Panoramic p250 Flash-3 æ‰«æä»ªå’Œ Olympus BX50 æ˜¾å¾®é•œè·å–ï¼Œæ¶µç›–äº†å¤šç§å¤æ‚çš„æœ‰ä¸åˆ†è£‚å½¢çŠ¶ã€‚ç ”ç©¶å›¢é˜Ÿä»è¿™äº›åˆ‡ç‰‡ä¸­é€‰æ‹©äº† 50 ä¸ªå¤§å°ä¸º 1024x1024 åƒç´ çš„åŒºåŸŸï¼Œæ€»è®¡åŒ…å«è¶…è¿‡ 500 ä¸ªæ ‡æ³¨çš„æœ‰ä¸åˆ†è£‚è±¡ã€‚ç›®å‰è¯¥æ•°æ®é›†å·²æŒ‰ä¸‰åˆ†ä¹‹äºŒç”¨äºè®­ç»ƒã€ä¸‰åˆ†ä¹‹ä¸€ç”¨äºæµ‹è¯•çš„æ¯”ä¾‹è¿›è¡Œåˆ’åˆ†ï¼Œä¸ºä¹³è…ºç™Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç®—æ³•çš„å¼€å‘æä¾›äº†é«˜è´¨é‡çš„æ ‡æ³¨èµ„æºã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14271v1",
      "published_date": "2025-07-18 16:19:05 UTC",
      "updated_date": "2025-07-18 16:19:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:35.131182+00:00"
    },
    {
      "arxiv_id": "2507.14270v6",
      "title": "APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation",
      "title_zh": "APTx Neuronï¼šèåˆæ¿€æ´»ä¸è®¡ç®—çš„ç»Ÿä¸€å¯è®­ç»ƒç¥ç»å…ƒæ¶æ„",
      "authors": [
        "Ravin Kumar"
      ],
      "abstract": "We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both optimization-efficient and elegant. The proposed neuron follows the functional form $y = \\sum_{i=1}^{n} ((Î±_i + \\tanh(Î²_i x_i)) \\cdot Î³_i x_i) + Î´$, where all parameters $Î±_i$, $Î²_i$, $Î³_i$, and $Î´$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to $96.69\\%$ test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and training efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it. Source code is available at https://github.com/mr-ravin/aptx_neuron.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†APTx Neuronï¼Œè¿™æ˜¯ä¸€ç§å°†éçº¿æ€§æ¿€æ´»(non-linear activation)å’Œçº¿æ€§å˜æ¢(linear transformation)é›†æˆåˆ°å•ä¸ªå¯è®­ç»ƒè¡¨è¾¾å¼ä¸­çš„æ–°å‹ç»Ÿä¸€ç¥ç»è®¡ç®—å•å…ƒã€‚APTx Neuronæºè‡ªAPTxæ¿€æ´»å‡½æ•°(activation function)ï¼Œé€šè¿‡æ¶ˆé™¤ç‹¬ç«‹çš„æ¿€æ´»å±‚ï¼Œä½¿ç½‘ç»œæ¶æ„åœ¨ä¼˜åŒ–ä¸Šæ›´å…·æ•ˆç‡ä¸”æ›´åŠ ç®€æ´ã€‚è¯¥ç¥ç»å…ƒé‡‡ç”¨ç‰¹å®šçš„å‡½æ•°å½¢å¼è¿›è¡Œè®¡ç®—ï¼Œå…¶ä¸­æ‰€æœ‰çš„å‚æ•° $Î±_i$ã€$Î²_i$ã€$Î³_i$ å’Œ $Î´$ å‡æ˜¯å¯è®­ç»ƒçš„ã€‚åœ¨MNISTæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¶æ„åˆ©ç”¨çº¦33.2ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œåœ¨11ä¸ªè®­ç»ƒè½®æ¬¡(epochs)å†…ä¾¿è¾¾åˆ°äº†96.69%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸ä¼ ç»Ÿç¥ç»å…ƒç›¸æ¯”ï¼ŒAPTx Neuronå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºç»Ÿä¸€ç¥ç»å…ƒè®¾è®¡åŠç›¸å…³æ¶æ„çš„æ„å»ºæä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "12 pages, 2 figures, 1 table. Includes a GitHub repository for MNIST experiments and a PyPI package for APTx Neuron implementation",
      "pdf_url": "https://arxiv.org/pdf/2507.14270v6",
      "published_date": "2025-07-18 16:17:40 UTC",
      "updated_date": "2025-12-26 16:28:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:28:38.715566+00:00"
    },
    {
      "arxiv_id": "2507.14043v2",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "title_zh": "é¢å‘ä¸‰ç»´æ— äººæœºè·¯å¾„è§„åˆ’åŠå·¥ç¨‹é—®é¢˜çš„å¤šç­–ç•¥æ”¹è¿›è›‡ä¼˜åŒ–ç®—æ³•",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various fields owing to their ability to generate diverse solutions. One such algorithm is the Snake Optimizer (SO), a progressive optimization approach. However, SO suffers from the issues of slow convergence speed and susceptibility to local optima. In light of these shortcomings, we propose a novel Multi-strategy Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random disturbance strategy based on sine function to alleviate the risk of getting trapped in a local optimum. Secondly, we introduce adaptive Levy flight strategy based on scale factor and leader and endow the male snake leader with flight capability, which makes it easier for the algorithm to leap out of the local optimum and find the global optimum. More importantly, we put forward a position update strategy combining elite leadership and Brownian motion, effectively accelerating the convergence speed while ensuring precision. Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test functions and the CEC2022 test suite, comparing it with 11 popular algorithms across different dimensions to validate its effectiveness. Moreover, Unmanned Aerial Vehicle (UAV) has been widely used in various fields due to its advantages of low cost, high mobility and easy operation. However, the UAV path planning problem is crucial for flight safety and efficiency, and there are still challenges in establishing and optimizing the path model. Therefore, we apply MISO to the UAV 3D path planning problem as well as 6 engineering design problems to assess its feasibility in practical applications. The experimental results demonstrate that MISO exceeds other competitive algorithms in terms of solution quality and stability, establishing its strong potential for application.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è›‡ä¼˜åŒ–ç®—æ³•(Snake Optimizer, SO)æ”¶æ•›é€Ÿåº¦æ…¢ä¸”æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šç­–ç•¥æ”¹è¿›çš„è›‡ä¼˜åŒ–ç®—æ³•(Multi-strategy Improved Snake Optimizer, MISO)ã€‚é¦–å…ˆï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ­£å¼¦å‡½æ•°çš„è‡ªé€‚åº”éšæœºæ‰°åŠ¨ç­–ç•¥ï¼Œæœ‰æ•ˆé™ä½äº†ç®—æ³•é™·äºå±€éƒ¨æœ€ä¼˜çš„é£é™©ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¼•å…¥åŸºäºæ¯”ä¾‹å› å­å’Œé¢†å¯¼è€…çš„è‡ªé€‚åº”åˆ—ç»´é£è¡Œ(Levy flight)ç­–ç•¥ï¼Œå¢å¼ºäº†ç®—æ³•è·³å‡ºå±€éƒ¨é™·é˜±å¹¶å¯»è·å…¨å±€æœ€ä¼˜è§£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨ç»“åˆç²¾è‹±é¢†å¯¼å’Œå¸ƒæœ—è¿åŠ¨(Brownian motion)çš„ä½ç½®æ›´æ–°ç­–ç•¥ï¼Œåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†éªŒè¯æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜åœ¨CEC2017å’ŒCEC2022æµ‹è¯•é›†ä¸Šå°†MISOä¸11ç§ä¸»æµç®—æ³•è¿›è¡Œäº†å¤šç»´åº¦å¯¹æ¯”ã€‚æœ€åï¼Œè¯¥ç®—æ³•è¢«æˆåŠŸåº”ç”¨äºæ— äººæœº(UAV)ä¸‰ç»´è·¯å¾„è§„åˆ’åŠ6é¡¹å·¥ç¨‹è®¾è®¡é—®é¢˜ï¼Œå®éªŒç»“æœè¯æ˜MISOåœ¨æ±‚è§£è´¨é‡å’Œç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºç«äº‰ç®—æ³•ï¼Œå±•ç°å‡ºåœ¨å®é™…å¤æ‚ä¼˜åŒ–ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.RO",
      "comment": "59 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.14043v2",
      "published_date": "2025-07-18 16:11:35 UTC",
      "updated_date": "2025-08-13 14:12:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:30:50.447127+00:00"
    },
    {
      "arxiv_id": "2507.14032v2",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "title_zh": "KROMAï¼šåŸºäºçŸ¥è¯†æ£€ç´¢ä¸å¤§è¯­è¨€æ¨¡å‹çš„æœ¬ä½“åŒ¹é…",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ Ontology Matching (OM) ä»»åŠ¡ä¸­ç°æœ‰ç³»ç»Ÿä¾èµ–äººå·¥è§„åˆ™æˆ–ä¸“ç”¨æ¨¡å‹å¯¼è‡´é€‚åº”æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† KROMA æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ Retrieval-Augmented Generation (RAG) æµè½¬ä¸­åˆ©ç”¨ Large Language Models (LLMs)ï¼ŒåŠ¨æ€åœ°å°†ç»“æ„ã€è¯æ±‡å’Œå®šä¹‰çŸ¥è¯†èå…¥è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ã€‚ä¸ºäº†æå‡æ€§èƒ½ä¸æ•ˆç‡ï¼ŒKROMA é›†æˆäº†åŸºäº bisimilarity çš„æ¦‚å¿µåŒ¹é…å’Œè½»é‡åŒ– ontology refinement æ­¥éª¤ï¼Œæœ‰æ•ˆè¿‡æ»¤å€™é€‰æ¦‚å¿µå¹¶å¤§å¹…é™ä½äº†è°ƒç”¨ LLMs çš„é€šä¿¡å¼€é”€ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆçŸ¥è¯†æ£€ç´¢ä¸ä¸Šä¸‹æ–‡å¢å¼ºçš„ LLMs èƒ½æ˜¾è‘—æå‡æœ¬ä½“åŒ¹é…æ•ˆæœï¼Œä¼˜äºä¼ ç»Ÿçš„ OM ç³»ç»ŸåŠå‰æ²¿çš„åŸºäº LLM çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶éªŒè¯äº†å®šå‘çŸ¥è¯†æ£€ç´¢ã€æç¤ºå¢å¼º (prompt enrichment) å’Œæœ¬ä½“ç»†åŒ–ç­‰ä¼˜åŒ–æŠ€æœ¯åœ¨å¤§è§„æ¨¡ Ontology Matching ä¸­çš„å¯è¡Œæ€§ä¸ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the 24th International Semantic Web Conference Research Track (ISWC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.14032v2",
      "published_date": "2025-07-18 16:00:11 UTC",
      "updated_date": "2025-09-11 17:25:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:42.323157+00:00"
    },
    {
      "arxiv_id": "2507.14267v1",
      "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation",
      "title_zh": "DREAMSï¼šåŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„æ™ºèƒ½ä½“åŒ–ææ–™æ¨¡æ‹Ÿç ”ç©¶å¼•æ“",
      "authors": [
        "Ziqi Wang",
        "Hongshuo Huang",
        "Hancheng Zhao",
        "Changwen Xu",
        "Shang Zhu",
        "Jan Janssen",
        "Venkatasubramanian Viswanathan"
      ],
      "abstract": "Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1\\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DREAMSï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºDensity Functional Theory (DFT)çš„å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‡ªåŠ¨åŒ–çš„ææ–™æ¨¡æ‹Ÿä¸ç­›é€‰ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªä¸­å¤®Large Language Model (LLM)è§„åˆ’æ™ºèƒ½ä½“ä»¥åŠä¸“é—¨è´Ÿè´£åŸå­ç»“æ„ç”Ÿæˆã€DFTæ”¶æ•›æ€§æµ‹è¯•ã€High-Performance Computing (HPC)è°ƒåº¦å’Œé”™è¯¯å¤„ç†çš„é¢†åŸŸç‰¹å®šæ™ºèƒ½ä½“ï¼Œå¹¶é€šè¿‡å…±äº«ç”»å¸ƒ(shared canvas)ç¡®ä¿è®¨è®ºçš„ç»“æ„åŒ–å¹¶é˜²æ­¢å¹»è§‰ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒDREAMSåœ¨Sol27LCæ™¶æ ¼å¸¸æ•°åŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡è¯¯å·®ä½äº1%ï¼Œå¹¶åœ¨å¤æ‚çš„CO/Pt(111)å¸é™„éš¾é¢˜ä¸­æˆåŠŸé‡ç°äº†ä¸“å®¶çº§çš„æ–‡çŒ®ç»“æœã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡Bayesian ensemble samplingé‡åŒ–äº†æ³›å‡½é©±åŠ¨çš„ä¸ç¡®å®šæ€§ï¼Œç¡®è®¤äº†åœ¨Generalized Gradient Approximation (GGA) DFTæ°´å¹³ä¸‹çš„FCCä½åå¥½ã€‚DREAMSç›®å‰å·²è¾¾åˆ°L3çº§è‡ªåŠ¨åŒ–æ°´å¹³ï¼Œèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å®šä¹‰çš„ææ–™è®¾è®¡ç©ºé—´ï¼Œæ˜¾è‘—é™ä½äº†å¯¹äººç±»ä¸“å®¶å¹²é¢„çš„ä¾èµ–ï¼Œä¸ºé«˜é€šé‡ã€é«˜ç²¾åº¦çš„è®¡ç®—ææ–™å‘ç°æä¾›äº†å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 28 pages of Supporting Information",
      "pdf_url": "https://arxiv.org/pdf/2507.14267v1",
      "published_date": "2025-07-18 15:26:04 UTC",
      "updated_date": "2025-07-18 15:26:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:43.745125+00:00"
    },
    {
      "arxiv_id": "2507.14000v3",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "title_zh": "é¢å‘äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨çš„ Photonic Fabric å¹³å°",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM (PFA), a photonic-enabled switch and memory subsystem that delivers low latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D electro-optical system-in-package, the PFA offers up to 32 TB of shared memory alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM enables distributed AI training and inference to execute parallelism strategies more efficiently. The Photonic Fabric removes the silicon beachfront constraint that limits the fixed memory-to-compute ratio observed in virtually all current XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet that connects to the Photonic Fabric increases its memory capacity and correspondingly its memory bandwidth by offering a flexible path to scaling well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It is used to evaluate the performance of LLM reference and energy savings on PFA, without any significant change to the GPU core design. With the PFA, the simulation results show that up to 3.66x throughput and 1.40x latency improvements in LLM inference at 405B parameters, up to 7.04x throughput and 1.41x latency improvements at 1T parameters, and 60-90% energy savings in data movement for heavy collective operations in all LLM training scenarios. While these results are shown for NVIDIA GPUs, they can be applied similarly to other AI accelerator designs (XPUs) that share the same fundamental limitation of fixed memory to compute.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Photonic Fabricâ„¢ å’Œ Photonic Fabric Applianceâ„¢ (PFA)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æä¾›ä½å»¶è¿Ÿã€é«˜å¸¦å®½å’Œä½å•ä½èƒ½è€—çš„å…‰å­ä½¿èƒ½äº¤æ¢ä¸å­˜å‚¨å­ç³»ç»Ÿã€‚è¯¥å¹³å°é€šè¿‡åœ¨ 2.5D ç”µå…‰ç³»ç»Ÿçº§å°è£… (system-in-package) ä¸­é›†æˆ HBM3E å†…å­˜ã€ç‰‡ä¸Šå…‰äº¤æ¢æœºå’Œå¤–éƒ¨ DDR5ï¼Œå®ç°äº†é«˜è¾¾ 32 TB çš„å…±äº«å†…å­˜å’Œ 115 Tbps çš„å…¨å¯¹å…¨æ•°å­—äº¤æ¢å¸¦å®½ã€‚Photonic Fabricâ„¢ çªç ´äº†ç°æœ‰ XPU åŠ é€Ÿå™¨ä¸­å¸¸è§çš„ç¡…å²¸çº¿çº¦æŸ (silicon beachfront constraint)ï¼Œé€šè¿‡è§£è€¦å†…å­˜ä¸è®¡ç®—çš„å›ºå®šæ¯”ä¾‹ï¼Œå®ç°äº†è¿œè¶…ä¼ ç»Ÿå°è£…å†… HBM é™åˆ¶çš„çµæ´»æ‰©å±•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ç»è¿‡éªŒè¯çš„ CelestiSim æ¨¡æ‹Ÿå™¨è¯„ä¼°å‘ç°ï¼Œåœ¨ 405B å’Œ 1T å‚æ•°è§„æ¨¡çš„ LLM æ¨ç†ä¸­ï¼ŒPFA åˆ†åˆ«å®ç°äº†æœ€é«˜ 3.66 å€å’Œ 7.04 å€çš„ååé‡æå‡ã€‚æ­¤å¤–ï¼Œåœ¨ LLM è®­ç»ƒçš„é›†ä½“é€šä¿¡æ“ä½œä¸­ï¼Œè¯¥æ¶æ„å¯èŠ‚çœ 60-90% çš„æ•°æ®ä¼ è¾“èƒ½è€—ã€‚è¯¥æŠ€æœ¯ä¸ä»…é€‚ç”¨äº NVIDIA GPUï¼Œä¹Ÿä¸ºæ‰€æœ‰é¢ä¸´å­˜å‚¨ä¸è®¡ç®—é…æ¯”ç“¶é¢ˆçš„ AI åŠ é€Ÿå™¨è®¾è®¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.PF",
        "cs.AI"
      ],
      "primary_category": "cs.PF",
      "comment": "12 pages, 14 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.14000v3",
      "published_date": "2025-07-18 15:14:56 UTC",
      "updated_date": "2025-07-23 15:07:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:50.897491+00:00"
    },
    {
      "arxiv_id": "2507.13993v3",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "title_zh": "OrthoInsightï¼šåŸºäºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è‚‹éª¨éª¨æŠ˜è¯Šæ–­ä¸æŠ¥å‘Šç”Ÿæˆ",
      "authors": [
        "Jinzhi Wang",
        "Jiangbo Zhang",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai",
        "Ziyu xu",
        "Ningyong Wu",
        "Wenhong Zhao"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for automated diagnostic tools, especially for musculoskeletal injuries like rib fractures, commonly detected via CT scans. Manual interpretation is time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep learning framework for rib fracture diagnosis and report generation. It integrates a YOLOv9 model for fracture detection, a medical knowledge graph for retrieving clinical context, and a fine-tuned LLaVA language model for generating diagnostic reports. OrthoInsight combines visual features from CT images with expert textual data to deliver clinically useful outputs. Evaluated on 28,675 annotated CT images and expert reports, it achieves high performance across Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value, with an average score of 4.28, outperforming models like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal learning in transforming medical image analysis and providing effective support for radiologists.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OrthoInsightï¼Œä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºCTå½±åƒæ•°æ®æ¿€å¢å¯¼è‡´çš„è‚‹éª¨éª¨æŠ˜(rib fracture)è¯Šæ–­è€—æ—¶ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†ç”¨äºéª¨æŠ˜æ£€æµ‹çš„YOLOv9æ¨¡å‹ã€ç”¨äºæ£€ç´¢ä¸´åºŠèƒŒæ™¯çš„åŒ»ç–—çŸ¥è¯†å›¾è°±(medical knowledge graph)ä»¥åŠç»è¿‡å¾®è°ƒçš„LLaVAè¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†ä»å½±åƒè¯†åˆ«åˆ°æŠ¥å‘Šç”Ÿæˆçš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚OrthoInsighté€šè¿‡ç»“åˆCTå›¾åƒçš„è§†è§‰ç‰¹å¾ä¸ä¸“å®¶æ–‡æœ¬æ•°æ®ï¼Œèƒ½å¤Ÿè¾“å‡ºå…·æœ‰é«˜åº¦ä¸´åºŠå®ç”¨ä»·å€¼çš„è¯Šæ–­ç»“æœã€‚åœ¨åŒ…å«28,675å¼ æ ‡æ³¨CTå›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§ã€å†…å®¹å®Œæ•´æ€§ã€é€»è¾‘è¿è´¯æ€§å’Œä¸´åºŠæŒ‡å¯¼ä»·å€¼ç­‰ç»´åº¦è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOrthoInsightçš„å¹³å‡å¾—åˆ†è¾¾åˆ°4.28ï¼Œæ˜¾è‘—ä¼˜äºGPT-4å’ŒClaude-3ç­‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†å¤šæ¨¡æ€å­¦ä¹ (multi-modal learning)åœ¨è½¬æ¢åŒ»å­¦å½±åƒåˆ†ææ–¹å¼ä¸Šçš„æ½œåŠ›ï¼Œä¹Ÿä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†é«˜æ•ˆçš„è¾…åŠ©è¯Šæ–­å·¥å…·ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13993v3",
      "published_date": "2025-07-18 15:01:44 UTC",
      "updated_date": "2025-12-20 15:25:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:51.070434+00:00"
    },
    {
      "arxiv_id": "2507.14266v1",
      "title": "Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy",
      "title_zh": "èåˆ MOOCsã€æ™ºæ…§æ•™å­¦ä¸äººå·¥æ™ºèƒ½ï¼šè¿ˆå‘ç»Ÿä¸€æ•™å­¦æ³•çš„åå¹´æ¼”è¿›",
      "authors": [
        "Bo Yuan",
        "Jiazi Hu"
      ],
      "abstract": "Over the past decade, higher education has evolved through three distinct paradigms: the emergence of Massive Open Online Courses (MOOCs), the integration of Smart Teaching technologies into classrooms, and the rise of AI-enhanced learning. Each paradigm is intended to address specific challenges in traditional education: MOOCs enable ubiquitous access to learning resources; Smart Teaching supports real-time interaction with data-driven insights; and generative AI offers personalized feedback and on-demand content generation. However, these paradigms are often implemented in isolation due to their disparate technological origins and policy-driven adoption. This paper examines the origins, strengths, and limitations of each paradigm, and advocates a unified pedagogical perspective that synthesizes their complementary affordances. We propose a three-layer instructional framework that combines the scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity of AI. To demonstrate its feasibility, we present a curriculum design for a project-based course. The findings highlight the framework's potential to enhance learner engagement, support instructors, and enable personalized yet scalable learning.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†è¿‡å»åå¹´é«˜ç­‰æ•™è‚²ä¸­å¤§è§„æ¨¡å¼€æ”¾åœ¨çº¿è¯¾ç¨‹(MOOCs)ã€æ™ºæ…§æ•™å­¦(Smart Teaching)åŠäººå·¥æ™ºèƒ½å¢å¼ºå­¦ä¹ (AI-enhanced learning)ä¸‰å¤§èŒƒå¼çš„æ¼”å˜å†ç¨‹ã€‚é’ˆå¯¹å½“å‰è¿™äº›æ•™å­¦æ¨¡å¼åœ¨å®æ–½ä¸­å­˜åœ¨çš„å­¤ç«‹åŒ–é—®é¢˜ï¼Œæœ¬æ–‡ä¸»å¼ å»ºç«‹ä¸€ç§ç»Ÿä¸€çš„æ•™å­¦è§†è§’ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“åˆå„æ–¹äº’è¡¥ä¼˜åŠ¿çš„ä¸‰å±‚æ•™å­¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶æœ‰æœºæ•´åˆäº†MOOCsçš„æ‰©å±•æ€§(scalability)ã€æ™ºæ…§æ•™å­¦çš„å“åº”æ€§(responsiveness)ä»¥åŠäººå·¥æ™ºèƒ½çš„è‡ªé€‚åº”æ€§(adaptivity)ã€‚ç ”ç©¶é€šè¿‡ä¸€ä¸ªé¡¹ç›®å¼è¯¾ç¨‹(project-based course)çš„è¯¾ç¨‹è®¾è®¡å±•ç¤ºäº†è¯¥æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½æ˜¾è‘—æå‡å­¦ä¹ è€…çš„å‚ä¸åº¦ï¼Œè¿˜èƒ½ä¸ºæ•™å¸ˆæä¾›æ›´å¼ºçš„æ•™å­¦æ”¯æŒï¼Œæœ€ç»ˆå®ç°äº†å…¼å…·ä¸ªæ€§åŒ–ä¸è§„æ¨¡åŒ–çš„æ•™å­¦ç›®æ ‡ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14266v1",
      "published_date": "2025-07-18 14:57:20 UTC",
      "updated_date": "2025-07-18 14:57:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:29:58.919278+00:00"
    },
    {
      "arxiv_id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "title_zh": "CSD-VARï¼šè§†è§‰è‡ªå›å½’æ¨¡å‹ä¸­çš„å†…å®¹ä¸é£æ ¼åˆ†è§£",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CSD-VARæ¡†æ¶ï¼Œæ—¨åœ¨å°†å†…å®¹ä¸é£æ ¼è§£è€¦(Content-Style Decomposition)å¼•å…¥åˆ°è§†è§‰è‡ªå›å½’æ¨¡å‹(Visual Autoregressive Models)ä¸­ï¼Œä»¥æå‡è§†è§‰åˆæˆçš„åˆ›ä½œçµæ´»æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨VARçš„æŒ‰å°ºåº¦ç”Ÿæˆç‰¹æ€§ï¼Œé€šè¿‡å°ºåº¦æ„ŸçŸ¥äº¤æ›¿ä¼˜åŒ–ç­–ç•¥(Scale-aware alternating optimization strategy)å°†å†…å®¹å’Œé£æ ¼è¡¨ç¤ºä¸å„è‡ªçš„å°ºåº¦å¯¹é½ï¼Œä»è€Œå®ç°æ›´å¥½çš„åˆ†ç¦»ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–è§£è€¦æ€§èƒ½ï¼Œç ”ç©¶é‡‡ç”¨åŸºäºSVDçš„çº åæ–¹æ³•æ¥å‡è½»å†…å®¹å‘é£æ ¼è¡¨ç¤ºçš„æ³„éœ²ï¼Œå¹¶è®¾è®¡äº†å¢å¼ºé”®å€¼(Augmented Key-Value)å­˜å‚¨ä»¥å¼ºåŒ–å†…å®¹èº«ä»½çš„ä¿æŒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†ä¸“é—¨ç”¨äºè¯¥ä»»åŠ¡åŸºå‡†æµ‹è¯•çš„CSD-100æ•°æ®é›†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCSD-VARåœ¨å†…å®¹ä¿ç•™å’Œé£æ ¼åŒ–ä¿çœŸåº¦ä¸Šå‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºè§†è§‰åˆæˆé¢†åŸŸæä¾›äº†é«˜æ•ˆçš„ç”Ÿæˆå¼è§£è€¦æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13984v1",
      "published_date": "2025-07-18 14:45:48 UTC",
      "updated_date": "2025-07-18 14:45:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:30:00.648755+00:00"
    },
    {
      "arxiv_id": "2507.13970v3",
      "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI",
      "title_zh": "é¢å‘è¾¹ç¼˜ AI çš„åˆ†æ®µå¼æœºå™¨äººæŠ“å–æ„ŸçŸ¥ç¥ç»ç½‘ç»œ",
      "authors": [
        "Casper BrÃ¶cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico MÃ¶ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate objects of varying shapes, sizes and orientations, is a complex task that requires precise perception and control. Deep neural networks have shown remarkable success in grasp synthesis by learning rich and abstract representations of objects. When deployed at the edge, these models can enable low-latency, low-power inference, making real-time grasping feasible in resource-constrained environments. This work implements Heatmap-Guided Grasp Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware techniques, including input dimensionality reduction, model partitioning, and quantisation. Experimental evaluation on the GraspNet-1Billion benchmark validates the feasibility of fully on-chip inference, highlighting the potential of low-power MCUs for real-time, autonomous manipulation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹èµ„æºå—é™ç¯å¢ƒä¸‹çš„å®æ—¶æœºå™¨äººæŠ“å–éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªé¢å‘ Edge AI çš„åˆ†å‰²æŠ“å–æ„ŸçŸ¥ç¥ç»ç½‘ç»œã€‚è¯¥ç ”ç©¶åœ¨ GAP9 RISC-V System-on-Chip (SoC) ä¸Šå®ç°äº† Heatmap-Guided Grasp Detection æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼æ£€æµ‹ 6-Dof æŠ“å–ä½å§¿ã€‚ä¸ºäº†ç¡®ä¿ä½å»¶è¿Ÿå’Œä½åŠŸè€—çš„æ¨ç†è¡¨ç°ï¼Œæ¨¡å‹é‡‡ç”¨äº†è¾“å…¥ç»´åº¦ç¼©å‡ (input dimensionality reduction)ã€æ¨¡å‹åˆ†åŒº (model partitioning) å’Œé‡åŒ– (quantisation) ç­‰ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–æŠ€æœ¯ã€‚å®éªŒåœ¨ GraspNet-1Billion åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å®Œå…¨ç‰‡ä¸Šæ¨ç† (fully on-chip inference) çš„å¯è¡Œæ€§ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†ä½åŠŸè€— MCU åœ¨å®ç°å®æ—¶ã€è‡ªä¸»æ“æ§ä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè¾¹ç¼˜ä¾§æœºå™¨äººæ„ŸçŸ¥æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by SMC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13970v3",
      "published_date": "2025-07-18 14:32:45 UTC",
      "updated_date": "2025-08-15 13:10:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:30:02.416952+00:00"
    },
    {
      "arxiv_id": "2507.13966v2",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "title_zh": "è‡ªä¸‹è€Œä¸Šçš„é¢†åŸŸç‰¹å®šè¶…çº§æ™ºèƒ½ï¼šæˆ‘ä»¬éœ€è¦å¯é çš„çŸ¥è¯†å›¾è°±",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç‰¹å®šé¢†åŸŸå®ç°è¶…çº§æ™ºèƒ½çš„è·¯å¾„ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„è‡ªä¸Šè€Œä¸‹ (top-down) è®­ç»ƒæ¨¡å¼åœ¨è·å–æ·±å±‚é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¹¶æå‡ºä¸€ç§è‡ªä¸‹è€Œä¸Š (bottom-up) çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çŸ¥è¯†å›¾è°± (Knowledge Graph) çš„ç»„åˆç»“æ„ï¼Œé€šè¿‡ä»»åŠ¡ç”Ÿæˆæµæ°´çº¿å°†é¢†åŸŸåŸè¯­ (primitives) åˆæˆä¸ºæ¨ç†ä»»åŠ¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¹¶ç»„åˆç®€å•æ¦‚å¿µä»¥åº”å¯¹å¤æ‚æ¨ç†ã€‚ç ”ç©¶äººå‘˜åŸºäºåŒ»å­¦çŸ¥è¯†å›¾è°±æ„å»ºäº†åŒ…å«2.4ä¸‡ä¸ªæ¨ç†ä»»åŠ¡çš„å­¦ä¹ è¯¾ç¨‹ï¼Œå¹¶å¯¹ QwQ-32B æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”±æ­¤å¼€å‘çš„ QwQ-Med-3 åœ¨åŒ»å­¦è¶…çº§æ™ºèƒ½é¢†åŸŸè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†æ¶µç›–15ä¸ªåŒ»å­¦é¢†åŸŸçš„è¯„ä¼°åŸºå‡† ICD-Bench ä»¥é‡åŒ–æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒQwQ-Med-3 åœ¨ ICD-Bench ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå°¤å…¶åœ¨éš¾åº¦è¾ƒé«˜çš„ä»»åŠ¡ä¸­ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚è¯¥ç ”ç©¶é€šè¿‡åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•éªŒè¯äº†é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„è¿ç§»èƒ½åŠ›ï¼Œè¯æ˜äº†é€šè¿‡è·å–åŸè¯­èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚è¿™é¢„ç¤ºäº†æœªæ¥é€šç”¨äººå·¥æ™ºèƒ½ (AGI) å¯èƒ½æºäºå¤šä¸ªé«˜æ•ˆçš„é¢†åŸŸç‰¹å®šè¶…çº§æ™ºèƒ½æ™ºèƒ½ä½“ä¹‹é—´çš„ç»„åˆäº¤äº’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13966v2",
      "published_date": "2025-07-18 14:30:08 UTC",
      "updated_date": "2025-09-01 20:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:30:11.312402+00:00"
    },
    {
      "arxiv_id": "2507.19519v1",
      "title": "Physics-informed transfer learning for SHM via feature selection",
      "title_zh": "åŸºäºç‰¹å¾é€‰æ‹©çš„é¢å‘ç»“æ„å¥åº·ç›‘æµ‹çš„ç‰©ç†å¯å‘è¿ç§»å­¦ä¹ ",
      "authors": [
        "J. Poole",
        "P. Gardner",
        "A. J. Hughes",
        "N. Dervilis",
        "R. S. Mills",
        "T. A. Dardeno",
        "K. Worden"
      ],
      "abstract": "Data used for training structural health monitoring (SHM) systems are expensive and often impractical to obtain, particularly labelled data. Population-based SHM presents a potential solution to this issue by considering the available data across a population of structures. However, differences between structures will mean the training and testing distributions will differ; thus, conventional machine learning methods cannot be expected to generalise between structures. To address this issue, transfer learning (TL), can be used to leverage information across related domains. An important consideration is that the lack of labels in the target domain limits data-based metrics to quantifying the discrepancy between the marginal distributions. Thus, a prerequisite for the application of typical unsupervised TL methods is to identify suitable source structures (domains), and a set of features, for which the conditional distributions are related to the target structure. Generally, the selection of domains and features is reliant on domain expertise; however, for complex mechanisms, such as the influence of damage on the dynamic response of a structure, this task is not trivial. In this paper, knowledge of physics is leveraged to select more similar features, the modal assurance criterion (MAC) is used to quantify the correspondence between the modes of healthy structures. The MAC is shown to have high correspondence with a supervised metric that measures joint-distribution similarity, which is the primary indicator of whether a classifier will generalise between domains. The MAC is proposed as a measure for selecting a set of features that behave consistently across domains when subjected to damage, i.e. features with invariance in the conditional distributions. This approach is demonstrated on numerical and experimental case studies to verify its effectiveness in various applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“æ„å¥åº·ç›‘æµ‹(SHM)ä¸­æ ‡è®°æ•°æ®è·å–æˆæœ¬é«˜åŠå¸¸è§„æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨ä¸åŒç»“æ„é—´æ³›åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰©ç†ä¿¡æ¯çš„è¿ç§»å­¦ä¹ (Transfer Learning)ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¨¡æ€ä¿è¯å‡†åˆ™(Modal Assurance Criterion, MAC)æ¥é‡åŒ–å¥åº·ç»“æ„æ¨¡æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»¥æ­¤ä½œä¸ºé€‰æ‹©åœ¨ä¸åŒé¢†åŸŸé—´å—æŸä¼¤å½±å“è¡¨ç°ä¸€è‡´ç‰¹å¾çš„ä¾æ®ã€‚ç ”ç©¶å‘ç°ï¼ŒMACä¸è¡¡é‡è”åˆåˆ†å¸ƒç›¸ä¼¼æ€§çš„ç›‘ç£æŒ‡æ ‡å…·æœ‰é«˜åº¦ç›¸å…³æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å…·æœ‰æ¡ä»¶åˆ†å¸ƒä¸å˜æ€§çš„ç‰¹å¾ï¼Œä»è€Œè§£å†³æ— ç›‘ç£è¿ç§»å­¦ä¹ ä¸­æºåŸŸä¸ç‰¹å¾é€‰æ‹©çš„éš¾é¢˜ã€‚é€šè¿‡æ•°å€¼å’Œå®éªŒæ¡ˆä¾‹çš„éªŒè¯ï¼Œè¯¥æ–¹æ³•è¯æ˜äº†åœ¨å¤æ‚ç»“æ„åŠ¨åŠ›å“åº”åˆ†æä¸­åˆ©ç”¨ç‰©ç†çŸ¥è¯†æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç§ç¾¤ç»“æ„å¥åº·ç›‘æµ‹(Population-based SHM)æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19519v1",
      "published_date": "2025-07-18 14:28:52 UTC",
      "updated_date": "2025-07-18 14:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:30:10.115837+00:00"
    },
    {
      "arxiv_id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "title_zh": "è¿ˆå‘çº¦æŸæ—¶åºå›ç­”é›†ç¨‹åºè®¾è®¡",
      "authors": [
        "Pedro Cabalar",
        "MartÃ­n DiÃ©guez",
        "FranÃ§ois Olivier",
        "Torsten Schaub",
        "Igor StÃ©phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric resolution presents significant challenges for logic-based approaches like Answer Set Programming (ASP). To address this, we introduce and elaborate upon a novel temporal and constraint-based extension of the logic of Here-and-There and its nonmonotonic equilibrium extension, representing, to the best of our knowledge, the first approach to nonmonotonic temporal reasoning with constraints specifically tailored for ASP. This expressive system is achieved by a synergistic combination of two foundational ASP extensions: the linear-time logic of Here-and-There, providing robust nonmonotonic temporal reasoning capabilities, and the logic of Here-and-There with constraints, enabling the direct integration and manipulation of numeric constraints, among others. This work establishes the foundational logical framework for tackling complex dynamic systems with high resolution within the ASP paradigm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Answer Set Programming (ASP) åœ¨å¤„ç†å…·æœ‰ç»†ç²’åº¦æ—¶é—´å’Œæ•°å€¼åˆ†è¾¨ç‡çš„åŠ¨æ€ç³»ç»Ÿæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŸºäºæ—¶é—´å’Œçº¦æŸçš„é€»è¾‘æ‰©å±•æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡å¼•å…¥ Here-and-There é€»è¾‘åŠå…¶éå•è°ƒå¹³è¡¡æ‰©å±•ï¼Œå®ç°äº†é¦–ä¸ªä¸“é—¨ä¸º ASP å®šåˆ¶çš„ã€æ”¯æŒçº¦æŸçš„éå•è°ƒæ—¶é—´æ¨ç†ç³»ç»Ÿã€‚è¯¥æ¡†æ¶ååŒç»“åˆäº†çº¿æ€§æ—¶é—´ Here-and-There é€»è¾‘ (linear-time logic of Here-and-There) ä¸å¸¦çº¦æŸçš„ Here-and-There é€»è¾‘ (logic of Here-and-There with constraints)ï¼Œç¡®ä¿äº†å¯¹æ•°å€¼çº¦æŸçš„ç›´æ¥é›†æˆä¸æ“ä½œèƒ½åŠ›ã€‚è¿™ä¸€å·¥ä½œä¸ºåœ¨ ASP èŒƒå¼ä¸‹è§£å†³é«˜åˆ†è¾¨ç‡å¤æ‚åŠ¨æ€ç³»ç»Ÿå¥ å®šäº†åŸºç¡€æ€§çš„é€»è¾‘æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿä¸ä»…å¢å¼ºäº†éå•è°ƒæ¨ç†çš„è¡¨è¾¾åŠ›ï¼Œä¹Ÿä¸ºæœªæ¥å¤„ç†å¤æ‚çš„åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡æä¾›äº†æœ‰æ•ˆçš„ç†è®ºæ”¯æ’‘ä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13958v1",
      "published_date": "2025-07-18 14:22:38 UTC",
      "updated_date": "2025-07-18 14:22:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:06.423461+00:00"
    },
    {
      "arxiv_id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "title_zh": "DUALRecï¼šä¸€ç§é¢å‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”µå½±æ¨èçš„åºåˆ—ä¸è¯­è¨€æ¨¡å‹æ··åˆæ¡†æ¶",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of modelling and predicting the dynamic and context-rich user preferences. Traditional collaborative filtering and content-based methods often struggle to capture the temporal patternings and evolving user intentions. While Large Language Models (LLMs) have gained gradual attention in recent years, by their strong semantic understanding and reasoning abilities, they are not inherently designed to model chronologically evolving user preference and intentions. On the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which is good at capturing the temporal dynamics of user behaviour and evolving user preference over time, but still lacks a rich semantic understanding for comprehensive recommendation generation. In this study, we propose DUALRec (Dynamic User-Aware Language-based Recommender), a novel recommender that leverages the complementary strength of both models, which combines the temporal modelling abilities of LSTM networks with semantic reasoning power of the fine-tuned Large Language Models. The LSTM component will capture users evolving preference through their viewing history, while the fine-tuned LLM variants will leverage these temporal user insights to generate next movies that users might enjoy. Experimental results on MovieLens-1M dataset shows that the DUALRec model outperforms a wide range of baseline models, with comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes a novel architecture that bridges the gap between temporal sequence modeling and semantic reasoning, and offers a promising direction for developing more intelligent and context-aware recommenders.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† DUALRec (Dynamic User-Aware Language-based Recommender)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†æ—¶åºå»ºæ¨¡å’Œè¯­ä¹‰æ¨ç†èƒ½åŠ›çš„æ··åˆæ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£æ¨èç³»ç»Ÿåœ¨æ•æ‰åŠ¨æ€ä¸”å¯Œå«ä¸Šä¸‹æ–‡çš„ç”¨æˆ·åå¥½æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆ LSTM (Long-Short-Term-Memory) ç½‘ç»œä¸å¾®è°ƒåçš„ Large Language Models (LLMs)ï¼Œå……åˆ†å‘æŒ¥äº†ä¸¤è€…çš„äº’è¡¥ä¼˜åŠ¿ã€‚å…·ä½“è€Œè¨€ï¼ŒLSTM ç»„ä»¶è´Ÿè´£é€šè¿‡ç”¨æˆ·çš„è§‚çœ‹å†å²æ•æ‰ä¸æ–­æ¼”å˜çš„æ—¶åºåå¥½ï¼Œè€Œå¾®è°ƒåçš„ LLMs åˆ™åˆ©ç”¨è¿™äº›æ—¶åºæ´å¯Ÿç”Ÿæˆç¬¦åˆç”¨æˆ·æ½œåœ¨å…´è¶£çš„ä¸‹ä¸€éƒ¨ç”µå½±æ¨èã€‚åœ¨ MovieLens-1M æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDUALRec åœ¨ Hit Rate (HR@k)ã€Normalized Discounted Cumulative Gain (NDCG@k) ä»¥åŠç±»å‹ç›¸ä¼¼åº¦ç­‰å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¸€ç³»åˆ—åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶é€šè¿‡æ„å»ºè¿æ¥æ—¶åºåºåˆ—å»ºæ¨¡ä¸è¯­ä¹‰æ¨ç†çš„æ¡¥æ¢ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ¨èç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæå…·å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13957v1",
      "published_date": "2025-07-18 14:22:05 UTC",
      "updated_date": "2025-07-18 14:22:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:06.643671+00:00"
    },
    {
      "arxiv_id": "2507.13956v2",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "title_zh": "é¢å‘é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹çš„è·¨æ¨¡æ€å› æœå¹²é¢„",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Junyong Zhai",
        "Yuxiao Li",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's Disease (AD), where early identification and intervention can effectively slow the progression to dementia. However, diagnosing AD remains a significant challenge in neurology due to the confounders caused mainly by the selection bias of multi-modal data and the complex relationships between variables. To address these issues, we propose a novel visual-language causality-inspired framework named Cross-modal Causal Intervention with Mediator for Alzheimer's Disease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large Language Models (LLMs) to summarize clinical data under strict templates, therefore enriching textual inputs. The MediAD model utilizes Magnetic Resonance Imaging (MRI), clinical data, and textual data enriched by LLMs to classify participants into Cognitively Normal (CN), MCI, and AD categories. Because of the presence of confounders, such as cerebral vascular lesions and age-related biomarkers, non-causal models are likely to capture spurious input-output correlations, generating less reliable results. Our framework implicitly mitigates the effect of both observable and unobservable confounders through a unified causal intervention method. Experimental results demonstrate the outstanding performance of our method in distinguishing CN/MCI/AD cases, outperforming other methods in most evaluation metrics. The study showcases the potential of integrating causal reasoning with multi-modal learning for neurological disease diagnosis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Alzheimer's Disease (AD) åŠå…¶å‰é©±æœŸ Mild Cognitive Impairment (MCI) è¯Šæ–­ä¸­å› å¤šæ¨¡æ€æ•°æ®é€‰æ‹©åå·®å’Œå¤æ‚å˜é‡å…³ç³»å¯¼è‡´çš„ confounders é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MediAD çš„è§†è§‰-è¯­è¨€å› æœå¯å‘æ¡†æ¶ã€‚MediAD åˆ©ç”¨ Large Language Models (LLMs) å¯¹ä¸´åºŠæ•°æ®è¿›è¡Œæ¨¡æ¿åŒ–æ€»ç»“ä»¥ä¸°å¯Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶æ•´åˆ Magnetic Resonance Imaging (MRI) ä¸ä¸´åºŠå¤šæ¨¡æ€æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†åº”å¯¹è„‘è¡€ç®¡ç—…å˜å’Œå¹´é¾„ç­‰å› ç´ å¼•èµ·çš„è™šå‡ç›¸å…³æ€§ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„ causal intervention æ–¹æ³•éšæ€§åœ°å‡è½»äº†å¯è§‚æµ‹åŠä¸å¯è§‚æµ‹ confounders çš„å½±å“ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMediAD åœ¨åŒºåˆ† Cognitively Normal (CN)ã€MCI å’Œ AD æ–¹é¢è¡¨ç°å“è¶Šï¼Œåœ¨å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶æˆåŠŸå±•ç¤ºäº†å°†å› æœæ¨ç†ä¸å¤šæ¨¡æ€å­¦ä¹ åº”ç”¨äºç¥ç»ç³»ç»Ÿç–¾ç—…è¾…åŠ©è¯Šæ–­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13956v2",
      "published_date": "2025-07-18 14:21:24 UTC",
      "updated_date": "2025-11-06 12:53:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:10.692464+00:00"
    },
    {
      "arxiv_id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "title_zh": "åˆ©ç”¨é¦–å› æ•ˆåº”æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šé¡¹é€‰æ‹©é¢˜(MCQA)ä»»åŠ¡ä¸­å­˜åœ¨çš„é¦–ä½æ•ˆåº”(Primacy Effect)ï¼Œå³æ¨¡å‹æ›´å€¾å‘äºé€‰æ‹©æ’åœ¨å‰é¢çš„é€‰é¡¹ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒ(Fine-tuning)è¿‡ç¨‹ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™ç§ä½ç½®åè§ï¼Œè¿™å¯èƒ½ä¸æ¨¡å‹å­¦ä¹ åˆ°ç±»äººæ¨¡å¼æœ‰å…³ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§æå‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç­–ç•¥æ€§é‡æ’é€‰é¡¹çš„æ–¹æ³•ï¼Œå³æ ¹æ®é€‰é¡¹ä¸æŸ¥è¯¢ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦(Semantic Similarity)è¿›è¡Œæ’åºï¼Œä¸”è¯¥è¿‡ç¨‹æ— éœ€é¢„å…ˆè·çŸ¥æ­£ç¡®ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åˆ©ç”¨é¦–ä½æ•ˆåº”çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†LLMsåœ¨MCQAä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åè§å…·æœ‰æŒ‘æˆ˜ä¸æœºé‡çš„åŒé‡å±æ€§ï¼Œä¸ºåè§æ„ŸçŸ¥(Bias-aware)çš„æ¨¡å‹è®¾è®¡å’ŒNLPåº”ç”¨æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by RANLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13949v1",
      "published_date": "2025-07-18 14:18:18 UTC",
      "updated_date": "2025-07-18 14:18:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:10.851199+00:00"
    },
    {
      "arxiv_id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "title_zh": "åŸºäºæ½œæ‰©æ•£æ¨¡å‹çš„å†»ç»“è§†é¢‘æ¨¡å‹é€šç”¨é¢„æµ‹",
      "authors": [
        "Jacob C Walker",
        "Pedro VÃ©lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "JoÃ£o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ä¸å…¶åœ¨ä¸åŒæŠ½è±¡å±‚æ¬¡ï¼ˆåŒ…æ‹¬åŸå§‹åƒç´ ã€æ·±åº¦ã€ç‚¹è½¨è¿¹å’Œç‰©ä½“è¿åŠ¨ï¼‰ä¸Šçš„é€šç”¨é¢„æµ‹(Generalist Forecasting)æ€§èƒ½ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ­£ç›¸å…³å…³ç³»ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é€‚ç”¨äºä»»ä½•å†»ç»“è§†è§‰ä¸»å¹²ç½‘ç»œ(Frozen Vision Backbone)çš„é€šç”¨é¢„æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯é€šè¿‡è®­ç»ƒæ½œæ‰©æ•£æ¨¡å‹(Latent Diffusion Models)åœ¨å†»ç»“çš„è¡¨ç¤ºç©ºé—´ä¸­é¢„æµ‹æœªæ¥ç‰¹å¾ï¼Œéšååˆ©ç”¨è½»é‡çº§çš„ä»»åŠ¡ç‰¹å®šè¯»å–å™¨(Task-Specific Readouts)è¿›è¡Œè§£ç ã€‚ä¸ºäº†åœ¨ä¸åŒä»»åŠ¡é—´è¿›è¡Œä¸€è‡´æ€§è¯„ä¼°ï¼Œç ”ç©¶å¼•å…¥äº†åˆ†å¸ƒæŒ‡æ ‡(Distributional Metrics)ï¼Œç”¨äºç›´æ¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ç©ºé—´ä¸­æ¯”è¾ƒåˆ†å¸ƒå±æ€§ã€‚å®éªŒåœ¨9ä¸ªæ¨¡å‹å’Œ4é¡¹ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å°†è¡¨ç¤ºå­¦ä¹ (Representation Learning)ä¸ç”Ÿæˆå»ºæ¨¡(Generative Modeling)ç›¸ç»“åˆåœ¨æ—¶ç©ºè§†é¢‘ç†è§£ä¸­çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13942v1",
      "published_date": "2025-07-18 14:14:19 UTC",
      "updated_date": "2025-07-18 14:14:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:17.039482+00:00"
    },
    {
      "arxiv_id": "2507.13941v2",
      "title": "Shared representations in brains and models reveal a two-route cortical organization during scene perception",
      "title_zh": "å¤§è„‘ä¸æ¨¡å‹ä¸­çš„å…±äº«è¡¨å¾æ­ç¤ºåœºæ™¯çŸ¥è§‰ä¸­çš„åŒè·¯å¾„çš®å±‚ç»„ç»‡",
      "authors": [
        "Pablo Marcos-ManchÃ³n",
        "LluÃ­s Fuentemilla"
      ],
      "abstract": "The brain transforms visual inputs into high-dimensional cortical representations that support diverse cognitive and behavioral goals. Characterizing how this information is organized and routed across the human brain is essential for understanding how we process complex visual scenes. Here, we applied representational similarity analysis to 7T fMRI data collected during natural scene viewing. We quantified representational geometry shared across individuals and compared it to hierarchical features from vision and language neural networks. This analysis revealed two distinct processing routes: a ventromedial pathway specialized for scene layout and environmental context, and a lateral occipitotemporal pathway selective for animate content. Vision models aligned with shared structure in both routes, whereas language models corresponded primarily with the lateral pathway. These findings refine classical visual-stream models by characterizing scene perception as a distributed cortical network with separable representational routes for context and animate content.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨7T fMRIæ•°æ®å’Œè¡¨å¾ç›¸ä¼¼æ€§åˆ†æ(Representational Similarity Analysis)æ¢è®¨äº†å¤§è„‘åœ¨è§‚çœ‹è‡ªç„¶åœºæ™¯æ—¶å¦‚ä½•ç»„ç»‡å’Œè·¯ç”±è§†è§‰ä¿¡æ¯ã€‚ç ”ç©¶äººå‘˜é€šè¿‡é‡åŒ–ä¸ªä½“é—´å…±äº«çš„è¡¨å¾å‡ ä½•ï¼Œå¹¶å°†å…¶ä¸è§†è§‰å’Œè¯­è¨€ç¥ç»ç½‘ç»œçš„å±‚æ¬¡åŒ–ç‰¹å¾è¿›è¡Œå¯¹æ¯”ï¼Œæ­ç¤ºäº†å¤§è„‘çš®å±‚ä¸­å­˜åœ¨ä¸¤æ¡æˆªç„¶ä¸åŒçš„å¤„ç†è·¯å¾„ã€‚å…¶ä¸­ï¼Œè…¹å†…ä¾§è·¯å¾„(Ventromedial pathway)ä¸“é—¨è´Ÿè´£å¤„ç†åœºæ™¯å¸ƒå±€(Scene layout)å’Œç¯å¢ƒèƒŒæ™¯ï¼Œè€Œå¤–ä¾§æ•é¢è·¯å¾„(Lateral occipitotemporal pathway)åˆ™å¯¹æœ‰ç”Ÿå‘½(Animate)çš„å†…å®¹å…·æœ‰é€‰æ‹©æ€§ã€‚å®éªŒå‘ç°è§†è§‰æ¨¡å‹ä¸è¿™ä¸¤æ¡è·¯å¾„çš„å…±äº«ç»“æ„å‡ä¿æŒä¸€è‡´ï¼Œè€Œè¯­è¨€æ¨¡å‹åˆ™ä¸»è¦å¯¹åº”äºå¤–ä¾§è·¯å¾„ã€‚è¿™äº›å‘ç°é€šè¿‡å°†åœºæ™¯æ„ŸçŸ¥æè¿°ä¸ºä¸€ä¸ªå…·æœ‰å¯åˆ†ç¦»è¡¨å¾è·¯å¾„çš„åˆ†å¸ƒå¼çš®å±‚ç½‘ç»œï¼Œè¿›ä¸€æ­¥å®Œå–„äº†ç»å…¸çš„è§†è§‰æµ(Visual-stream)æ¨¡å‹ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "for associate code, see https://github.com/memory-formation/convergent-transformations",
      "pdf_url": "https://arxiv.org/pdf/2507.13941v2",
      "published_date": "2025-07-18 14:13:54 UTC",
      "updated_date": "2026-01-19 11:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:16.515631+00:00"
    },
    {
      "arxiv_id": "2507.13933v2",
      "title": "Preprint: Poster: Did I Just Browse A Website Written by LLMs?",
      "title_zh": "é¢„å°æœ¬æµ·æŠ¥ï¼šæˆ‘åˆšæ‰æµè§ˆçš„æ˜¯å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç½‘ç«™å—ï¼Ÿ",
      "authors": [
        "Sichang Steven He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models (LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs plagiarize and hallucinate, LLM-dominant content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However, state-of-the-art LLM detectors are inaccurate on web content, because web content has low positive rates, complex markup, and diverse genres, instead of clean, prose-like benchmark data SoTA detectors are optimized for.\n  We propose a highly reliable, scalable pipeline that classifies entire websites. Instead of naively classifying text extracted from each page, we classify each site based on an LLM text detector's outputs of multiple prose-like pages to boost accuracies. We train and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain 100% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites are growing in prevalence and rank highly in search results, raising questions about their impact on end users and the overall Web ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Web å¹³å°ä¸Šç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªåŠ¨ç”Ÿæˆçš„ \"LLM-dominant\" å†…å®¹æ¿€å¢åŠå…¶å¸¦æ¥çš„å¯é æ€§ä¸ä¼¦ç†é£é™©ï¼ŒæŒ‡å‡ºå½“å‰æ£€æµ‹æŠ€æœ¯åœ¨åº”å¯¹å¤æ‚ç½‘é¡µæ•°æ®æ—¶å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é«˜åº¦å¯é ä¸”å¯æ‰©å±•çš„æ£€æµ‹æµæ°´çº¿ï¼Œé€šè¿‡ç»¼åˆåˆ†æç½‘ç«™å†…å¤šä¸ªç±»æ•£æ–‡ (prose-like) é¡µé¢çš„ LLM æ£€æµ‹å™¨è¾“å‡ºç»“æœï¼Œå®ç°å¯¹æ•´ä¸ªç½‘ç«™çš„åˆ†ç±»ã€‚åœ¨åŒ…å« 120 ä¸ªç½‘ç«™çš„ä¸¤ä¸ªçœŸå®æ•°æ®é›†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº† 100% çš„å‡†ç¡®ç‡ã€‚é€šè¿‡å¯¹æœç´¢å¼•æ“ç»“æœå’Œ Common Crawl å­˜æ¡£ä¸­å„ 10,000 ä¸ªç½‘ç«™çš„å®åœ°æ£€æµ‹ï¼Œç ”ç©¶å‘ç° \"LLM-dominant\" ç½‘ç«™çš„æ™®åŠç‡æ­£åœ¨æ˜¾è‘—å¢é•¿ä¸”æœç´¢æ’åè¾ƒé«˜ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æ­¤ç±»å†…å®¹å¯¹ç»ˆç«¯ç”¨æˆ·åŠæ•´ä¸ª Web ç”Ÿæ€ç³»ç»Ÿçš„æ½œåœ¨å½±å“ï¼Œå¼ºè°ƒäº†å¼€å‘é«˜æ•ˆæ£€æµ‹å·¥å…·çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.NI",
      "comment": "ACM Internet Measurement Conference 2025 Poster & ACM IMC 2025 Student Workshop. 2 pages. 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13933v2",
      "published_date": "2025-07-18 14:09:04 UTC",
      "updated_date": "2025-10-09 23:55:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:31.939995+00:00"
    },
    {
      "arxiv_id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "title_zh": "å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨æ”¿æ²»è¯´æœä¸­çš„å…³é”®æ æ†",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æ¶‰åŠ76,977åå‚ä¸è€…çš„å¤§è§„æ¨¡å®éªŒï¼Œè¯„ä¼°äº†19ç§å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨707ä¸ªæ”¿æ²»è®®é¢˜ä¸Šçš„è¯´æœåŠ›ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†æ¨¡å‹è§„æ¨¡ã€ä¸ªæ€§åŒ–(Personalization)ã€åæœŸè®­ç»ƒ(Post-training)å’Œæç¤ºè¯å·¥ç¨‹(Prompting)ç­‰å› ç´ å¯¹è¯´æœæ•ˆæœçš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåæœŸè®­ç»ƒå’Œæç¤ºè¯å·¥ç¨‹æ˜¯æå‡AIè¯´æœåŠ›çš„æ ¸å¿ƒæ æ†ï¼Œå…¶å½±å“åŠ›è¿œè¶…å•çº¯å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–å®æ–½ä¸ªæ€§åŒ–ç­–ç•¥ã€‚è¿™äº›æ–¹æ³•ä¸»è¦é€šè¿‡åˆ©ç”¨LLMså¿«é€Ÿè·å–å¹¶æˆ˜ç•¥æ€§éƒ¨ç½²ä¿¡æ¯çš„èƒ½åŠ›æ¥å¢å¼ºå½±å“æ•ˆæœã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°AIè¯´æœåŠ›çš„æå‡å¾€å¾€ä¼´éšç€çœŸå®æ€§å‡†ç¡®åº¦(Factual Accuracy)çš„ç³»ç»Ÿæ€§ä¸‹é™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç”Ÿæˆå¼AIå½±å“äººç±»ä¿¡å¿µçš„æ½œåœ¨æœºåˆ¶ï¼Œå¹¶ä¸ºå…¶åœ¨æ”¿æ²»ä¼ æ’­ä¸­çš„åº”ç”¨é£é™©æä¾›äº†é‡è¦å®è¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures. Our supplementary materials file can be found at https://github.com/kobihackenburg/scaling-conversational-AI",
      "pdf_url": "https://arxiv.org/pdf/2507.13919v1",
      "published_date": "2025-07-18 13:50:09 UTC",
      "updated_date": "2025-07-18 13:50:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:46.684826+00:00"
    },
    {
      "arxiv_id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "title_zh": "æ–‡æœ¬æ”¿æ²»å€¾å‘ä¸æ”¿æ²»æ€§åˆ†ç±»",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text according to political leaning and politicalness using transformer models. We compose a comprehensive overview of existing datasets and models for these tasks, finding that current approaches create siloed solutions that perform poorly on out-of-distribution texts. To address this limitation, we compile a diverse dataset by combining 12 datasets for political leaning classification and creating a new dataset for politicalness by extending 18 existing datasets with the appropriate label. Through extensive benchmarking with leave-one-in and leave-one-out methodologies, we evaluate the performance of existing models and train new ones with enhanced generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Transformer æ¨¡å‹å¯¹æ–‡æœ¬çš„æ”¿æ²»å€¾å‘(political leaning)å’Œæ”¿æ²»æ€§(politicalness)è¿›è¡Œè‡ªåŠ¨åˆ†ç±»çš„æŒ‘æˆ˜ã€‚ä½œè€…å‘ç°ç°æœ‰æ–¹æ³•é€šå¸¸å‘ˆç°å­¤ç«‹æ€§ï¼Œåœ¨å¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)æ–‡æœ¬æ—¶è¡¨ç°æ¬ ä½³ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œè¯¥ç ”ç©¶é€šè¿‡æ•´åˆ12ä¸ªæ”¿æ²»å€¾å‘åˆ†ç±»æ•°æ®é›†ï¼Œå¹¶å¯¹18ä¸ªç°æœ‰æ•°æ®é›†è¿›è¡Œæ‰©å±•ä»¥æ ‡æ³¨æ”¿æ²»æ€§ï¼Œæ„å»ºäº†ä¸€ä¸ªæ¶µç›–å¹¿æ³›çš„ç»¼åˆæ•°æ®é›†ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨äº† leave-one-in å’Œ leave-one-out æ–¹æ³•è¿›è¡Œäº†è¯¦å°½çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥æ­¤è¯„ä¼°ç°æœ‰æ¨¡å‹çš„æ•ˆèƒ½ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶æˆåŠŸè®­ç»ƒå‡ºå…·å¤‡æ›´é«˜æ³›åŒ–èƒ½åŠ›(generalization capabilities)çš„æ–°æ¨¡å‹ï¼Œä¸ºæå‡æ”¿æ²»æ–‡æœ¬åˆ†ç±»çš„é²æ£’æ€§æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13913v1",
      "published_date": "2025-07-18 13:44:30 UTC",
      "updated_date": "2025-07-18 13:44:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:46.991747+00:00"
    },
    {
      "arxiv_id": "2507.13912v2",
      "title": "Self-supervised learning on gene expression data",
      "title_zh": "åŸºäºåŸºå› è¡¨è¾¾æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in biomedical research, enabling insights into disease mechanisms, drug responses, and personalized medicine. Traditional machine learning and deep learning rely on supervised learning, which requires large quantities of labeled data that are costly and time-consuming to obtain in the case of gene expression data. Self-supervised learning has recently emerged as a promising approach to overcome these limitations by extracting information directly from the structure of unlabeled data. In this study, we investigate the application of state-of-the-art self-supervised learning methods to bulk gene expression data for phenotype prediction. We selected three self-supervised methods, based on different approaches, to assess their ability to exploit the inherent structure of the data and to generate qualitative representations which can be used for downstream predictive tasks. By using several publicly available gene expression datasets, we demonstrate how the selected methods can effectively capture complex information and improve phenotype prediction accuracy. The results obtained show that self-supervised learning methods can outperform traditional supervised models besides offering significant advantage by reducing the dependency on annotated data. We provide a comprehensive analysis of the performance of each method by highlighting their strengths and limitations. We also provide recommendations for using these methods depending on the case under study. Finally, we outline future research directions to enhance the application of self-supervised learning in the field of gene expression data analysis. This study is the first work that deals with bulk RNA-Seq data and self-supervised learning.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸåŸºå› è¡¨è¾¾æ•°æ®(gene expression data)æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶çš„é—®é¢˜ï¼Œæ¢ç´¢äº†è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)åœ¨è¡¨å‹é¢„æµ‹(phenotype prediction)ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶é€‰å–äº†ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¯„ä¼°å…¶ä»æ— æ ‡æ³¨æ•°æ®ç»“æ„ä¸­æå–ç‰¹å¾å¹¶ç”¨äºä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨å¤šä¸ªå…¬å¼€çš„bulk RNA-Seqæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè‡ªç›‘ç£å­¦ä¹ åœ¨æ•æ‰å¤æ‚ä¿¡æ¯å’Œæé«˜é¢„æµ‹å‡†ç¡®ç‡æ–¹é¢ä¼˜äºä¼ ç»Ÿç›‘ç£æ¨¡å‹ï¼Œæ˜¾è‘—å‡è½»äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚ä½œä¸ºé¦–ç¯‡å°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºbulk RNA-Seqæ•°æ®åˆ†æçš„å·¥ä½œï¼Œè¯¥ç ”ç©¶ä¸ä»…è¯¦å°½å¯¹æ¯”äº†å„æ–¹æ³•çš„ä¼˜åŠ£ï¼Œè¿˜ä¸ºä¸åŒç ”ç©¶åœºæ™¯æä¾›äº†åº”ç”¨å»ºè®®å¹¶æŒ‡æ˜äº†æœªæ¥æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13912v2",
      "published_date": "2025-07-18 13:43:04 UTC",
      "updated_date": "2025-09-17 13:26:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:31:33.538510+00:00"
    },
    {
      "arxiv_id": "2507.14263v1",
      "title": "Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts",
      "title_zh": "è¶…è¶Š DNSï¼šé€šè¿‡ NANDA ç´¢å¼•ä¸ç»éªŒè¯çš„ AgentFacts å¼€å¯ AI æ™ºèƒ½ä½“äº’è”ç½‘",
      "authors": [
        "Ramesh Raskar",
        "Pradyumna Chari",
        "John Zinky",
        "Mahesh Lambe",
        "Jared James Grogan",
        "Sichao Wang",
        "Rajesh Ranjan",
        "Rekha Singhal",
        "Shailja Gupta",
        "Robert Lincourt",
        "Raghu Bala",
        "Aditi Joshi",
        "Abhishek Singh",
        "Ayush Chopra",
        "Dimitris Stripelis",
        "Bhuwan B",
        "Sumit Kumar",
        "Maria Gorskikh"
      ],
      "abstract": "The Internet is poised to host billions to trillions of autonomous AI agents that negotiate, delegate, and migrate in milliseconds and workloads that will strain DNS-centred identity and discovery. In this paper, we describe the NANDA index architecture, which we envision as a means for discoverability, identifiability and authentication in the internet of AI agents. We present an architecture where a minimal lean index resolves to dynamic, cryptographically verifiable AgentFacts that supports multi-endpoint routing, load balancing, privacy-preserving access, and credentialed capability assertions. Our architecture design delivers five concrete guarantees: (1) A quilt-like index proposal that supports both NANDA-native agents as well as third party agents being discoverable via the index, (2) rapid global resolution for newly spawned AI agents, (3) sub-second revocation and key rotation, (4) schema-validated capability assertions, and (5) privacy-preserving discovery across organisational boundaries via verifiable, least-disclosure queries. We formalize the AgentFacts schema, specify a CRDT-based update protocol, and prototype adaptive resolvers. The result is a lightweight, horizontally scalable foundation that unlocks secure, trust-aware collaboration for the next generation of the Internet of AI agents, without abandoning existing web infrastructure.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ DNS ç³»ç»Ÿåœ¨åº”å¯¹æµ·é‡è‡ªä¸» AI agents èº«ä»½è¯†åˆ«ä¸å‘ç°æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† NANDA index æ¶æ„ï¼Œæ—¨åœ¨æ„å»ºä¸‹ä¸€ä»£ä»£ç†äº’è”ç½‘çš„å‘ç°ä¸éªŒè¯ä½“ç³»ã€‚è¯¥æ¶æ„é€šè¿‡å°†æç®€ç´¢å¼•è§£æä¸ºå¯åŠ å¯†éªŒè¯çš„åŠ¨æ€ AgentFactsï¼Œå®ç°äº†å¤šç«¯è·¯ç”±ã€è´Ÿè½½å‡è¡¡åŠéšç§ä¿æŠ¤è®¿é—®ã€‚ç³»ç»Ÿæä¾›äº†åŒ…æ‹¬ NANDA-native å’Œç¬¬ä¸‰æ–¹ä»£ç†å‘ç°ã€å…¨çƒå¿«é€Ÿè§£æã€äºšç§’çº§å¯†é’¥è½®è½¬åŠæƒé™å£°æ˜åœ¨å†…çš„äº”å¤§æ ¸å¿ƒä¿è¯ã€‚ç ”ç©¶è¿›ä¸€æ­¥å½¢å¼åŒ–äº† AgentFacts æ¶æ„ï¼Œå¹¶åˆ¶å®šäº†åŸºäº CRDT çš„æ›´æ–°åè®®ï¼Œæ—¨åœ¨ä¸æŠ›å¼ƒç°æœ‰ Web åŸºç¡€è®¾æ–½çš„å‰æä¸‹ï¼Œä¸ºå®‰å…¨ã€äº’ä¿¡çš„ AI ä»£ç†åä½œæä¾›è½»é‡çº§ä¸”å¯æ°´å¹³æ‰©å±•çš„æŠ€æœ¯åº•åº§ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14263v1",
      "published_date": "2025-07-18 13:40:46 UTC",
      "updated_date": "2025-07-18 13:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:14.793694+00:00"
    },
    {
      "arxiv_id": "2507.22910v1",
      "title": "Large Language Models in the Travel Domain: An Industrial Experience",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨æ—…æ¸¸é¢†åŸŸçš„åº”ç”¨ï¼šä¸€é¡¹å·¥ä¸šå®è·µ",
      "authors": [
        "Sergio Di Meglio",
        "Aniello Somma",
        "Luigi Libero Lucio Starace",
        "Fabio Scippacercola",
        "Giancarlo SperlÃ¬",
        "Sergio Di Martino"
      ],
      "abstract": "Online property booking platforms are widely used and rely heavily on consistent, up-to-date information about accommodation facilities, often sourced from third-party providers. However, these external data sources are frequently affected by incomplete or inconsistent details, which can frustrate users and result in a loss of market. In response to these challenges, we present an industrial case study involving the integration of Large Language Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B, fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt. Both models were assessed based on their ability to generate consistent and homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet more concise content (249 vs. 277 words on average). However, this came at a significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB and $0.16/hour for Mistral 7B. Our findings provide practical insights into the trade-offs between model quality and resource efficiency, offering guidance for deploying LLMs in production environments and demonstrating their effectiveness in enhancing the consistency and reliability of accommodation data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ—…æ¸¸é¢†åŸŸé›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å·¥ä¸šå®è·µï¼Œæ—¨åœ¨è§£å†³åœ¨çº¿è®¢æˆ¿å¹³å° CALEIDOHOTELS ä¸­ç¬¬ä¸‰æ–¹æ•°æ®ä¸å®Œæ•´å’Œä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å¯¹æ¯”äº†ç»è¿‡ QLoRA å¾®è°ƒçš„ Mistral 7B æ¨¡å‹ä¸é‡‡ç”¨ç²¾ç»†åŒ–ç³»ç»Ÿæç¤ºçš„ Mixtral 8x7B æ¨¡å‹åœ¨ç”Ÿæˆä¸€è‡´æ€§ä½å®¿æè¿°æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMixtral 8x7B åœ¨å®Œæ•´æ€§ (99.6%)ã€ç²¾ç¡®åº¦ (98.8%) å’Œé™ä½å¹»è§‰ (hallucinations) æ–¹é¢å‡ä¼˜äº Mistral 7Bï¼Œä¸”ç”Ÿæˆçš„å†…å®¹æ›´ä¸ºç²¾ç‚¼ã€‚ç„¶è€Œï¼Œè¿™ç§é«˜æ€§èƒ½ä¼´éšç€æ›´é«˜çš„è®¡ç®—æˆæœ¬ï¼Œå…¶æ¯å°æ—¶è¿è¡Œè´¹ç”¨çº¦ä¸º Mistral 7B çš„åå€ã€‚è¯¥æ¡ˆä¾‹ç ”ç©¶æ·±å…¥åˆ†æäº†æ¨¡å‹è´¨é‡ä¸èµ„æºæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ (trade-offs)ï¼Œä¸ºåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² LLMs æä¾›äº†å®è´µçš„æŒ‡å¯¼å»ºè®®ã€‚é€šè¿‡å¼•å…¥ LLMsï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å…¶åœ¨æå‡ä½å®¿æ•°æ®å¯é æ€§ä¸ä¸€è‡´æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ—…æ¸¸è¡Œä¸šçš„æ•°å­—åŒ–è½¬å‹æä¾›äº†å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Manuscript accepted to the International Conference on Software Engineering and Knowledge Engineering (SEKE) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22910v1",
      "published_date": "2025-07-18 13:40:01 UTC",
      "updated_date": "2025-07-18 13:40:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:17.292030+00:00"
    },
    {
      "arxiv_id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¯†åˆ«å¼€æ”¾å¼æƒ…å¢ƒåˆ¤æ–­æµ‹éªŒä¸­ä¸ªäººä¸ä¸“ä¸šæŠ€èƒ½çš„ç‰¹å¾",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­¦æœ¯é¢†åŸŸå¯¹ä¸ªäººä¸ä¸“ä¸šæŠ€èƒ½(personal and professional skills)è¯„ä¼°æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæ¢è®¨äº†ä¼ ç»Ÿæƒ…å¢ƒåˆ¤æ–­æµ‹éªŒ(Situational Judgment Tests, SJTs)åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­é¢ä¸´çš„äººå·¥è¯„åˆ†æˆæœ¬é«˜åŠä»¥å¾€ NLP ç³»ç»Ÿæ„å¿µæ•ˆåº¦(construct validity)ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä» SJT å¼€æ”¾å¼å›ç­”ä¸­æå–æ„å¿µç›¸å…³ç‰¹å¾(construct-relevant features)çš„æ–°é¢–æ–¹æ³•ï¼Œå¹¶ä»¥ Casper SJT ä¸ºä¾‹éªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç»“åˆ LLMs çš„å¼ºå¤§è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å¹¶æå–åæ˜ å­¦ç”Ÿä¸“ä¸šç´ å…»çš„å…³é”®ç‰¹å¾ã€‚æ­¤ç ”ç©¶ä¸ä»…å…‹æœäº†ä¼ ç»Ÿè¯„ä¼°æ–¹å¼çš„è¿è¥éšœç¢ï¼Œè¿˜ä¸ºæœªæ¥ä¸ªäººä¸ä¸“ä¸šæŠ€èƒ½çš„è‡ªåŠ¨è¯„åˆ†(automated scoring)æŠ€æœ¯å¼€å‘å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 2 figures, 4 tables; this work was accepted for presentation at the 2025 Artificial Intelligence in Measurement and Education Conference in Pittsburgh, Pennsylvania, United States",
      "pdf_url": "https://arxiv.org/pdf/2507.13881v1",
      "published_date": "2025-07-18 12:59:17 UTC",
      "updated_date": "2025-07-18 12:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:17.986904+00:00"
    },
    {
      "arxiv_id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "title_zh": "é¢å‘å¢å¼ºèˆªæµ·è§†è§‰çš„è§†è§‰ä¸æµ·å›¾æ•°æ®å®æ—¶èåˆ",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing real-time visual data with chart information. Our system overlays nautical chart data onto live video feeds by accurately matching detected navigational aids, such as buoys, with their corresponding representations in chart data. To achieve robust association, we introduce a transformer-based end-to-end neural network that predicts bounding boxes and confidence scores for buoy queries, enabling the direct matching of image-domain detections with world-space chart markers. The proposed method is compared against baseline approaches, including a ray-casting model that estimates buoy positions via camera projection and a YOLOv7-based network extended with a distance estimation module. Experimental results on a dataset of real-world maritime scenes demonstrate that our approach significantly improves object localization and association accuracy in dynamic and challenging environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºæµ·äº‹è§†è§‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†å®æ—¶è§†è§‰æ•°æ®ä¸ nautical chart æ•°æ®è¿›è¡Œèåˆï¼Œå°†èˆªæµ·å›¾ä¿¡æ¯å åŠ åœ¨å®æ—¶è§†é¢‘æµä¸Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡å‡†ç¡®åŒ¹é…æ£€æµ‹åˆ°çš„ buoys ç­‰å¯¼èˆªè¾…åŠ©è®¾å¤‡åŠå…¶åœ¨å›¾è¡¨ä¸­çš„å¯¹åº”å…³ç³»ï¼Œå®ç°äº†è™šå®ä¿¡æ¯çš„åŒæ­¥ã€‚æ ¸å¿ƒæŠ€æœ¯å¼•å…¥äº†ä¸€ç§åŸºäº Transformer çš„ç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡ä¸ºæµ®æ ‡æŸ¥è¯¢é¢„æµ‹ bounding boxes å’Œ confidence scoresï¼Œç›´æ¥å°†å›¾åƒåŸŸçš„æ£€æµ‹ç»“æœä¸ä¸–ç•Œç©ºé—´çš„å›¾è¡¨æ ‡è®°è¿›è¡Œå…³è”ã€‚ä¸ä¼ ç»Ÿçš„ ray-casting æ¨¡å‹å’Œæ”¹è¿›çš„ YOLOv7 ç½‘ç»œç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æµ·äº‹åœºæ™¯æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨åŠ¨æ€ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ç›®æ ‡å®šä½ä¸å…³è”çš„å‡†ç¡®æ€§ï¼Œä¸ºå¢å¼ºæµ·äº‹æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13880v1",
      "published_date": "2025-07-18 12:58:11 UTC",
      "updated_date": "2025-07-18 12:58:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:19.085266+00:00"
    },
    {
      "arxiv_id": "2507.13874v2",
      "title": "Geometry of Knowledge Allows Extending Diversity Boundaries of Large Language Models",
      "title_zh": "çŸ¥è¯†å‡ ä½•åŠ©åŠ›æ‹“å±•å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ ·æ€§è¾¹ç•Œ",
      "authors": [
        "Mateusz BystroÅ„ski",
        "Doheon Han",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Starting from the hypothesis that knowledge in semantic space is organized along structured manifolds, we argue that this geometric structure renders the space explorable. By traversing it and using the resulting continuous representations to condition an LLM's generation distribution, we can systematically expand the model's reachable semantic range. We introduce a framework that requires no modification of LLM parameters and operationalizes this idea by constructing a conditioning distribution from a small set of diverse anchor generations. This distribution conditions LLM's generation via an xRAG-style projector. Our experiments demonstrate that this manifold-based conditioning substantially increases generative diversity, with direct benefits for enhancing divergent thinking, a core facet of creativity, in language models.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºè¯­ä¹‰ç©ºé—´ä¸­çš„çŸ¥è¯†æ˜¯æ²¿ç€ç»“æ„åŒ–æµå½¢ (manifolds) ç»„ç»‡çš„è¿™ä¸€å‡è®¾ï¼Œæ¢è®¨äº†çŸ¥è¯†ç©ºé—´çš„å‡ ä½•ç»“æ„ã€‚ä½œè€…è®¤ä¸ºè¿™ç§å‡ ä½•ç»“æ„ä½¿å¾—ç©ºé—´å…·æœ‰å¯æ¢ç´¢æ€§ï¼Œé€šè¿‡éå†è¯¥ç©ºé—´å¹¶ä½¿ç”¨ç”Ÿæˆçš„è¿ç»­è¡¨ç¤º (continuous representations) æ¥è°ƒèŠ‚å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ç”Ÿæˆåˆ†å¸ƒï¼Œå¯ä»¥ç³»ç»Ÿåœ°æ‰©å±•æ¨¡å‹çš„å¯è¾¾è¯­ä¹‰èŒƒå›´ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç»„å¤šæ ·åŒ–é”šç‚¹ç”Ÿæˆæ„å»ºè°ƒèŠ‚åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨ xRAG é£æ ¼çš„æŠ•å½±å™¨ (projector) å¼•å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§åŸºäºæµå½¢çš„è°ƒèŠ‚æ–¹å¼æ˜¾è‘—æå‡äº†ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œç›´æ¥å¢å¼ºäº†è¯­è¨€æ¨¡å‹åœ¨åˆ›é€ åŠ›æ ¸å¿ƒç»´åº¦ä¸Šçš„å‘æ•£æ€§æ€ç»´ (divergent thinking)ã€‚è¿™ä¸€æ–¹æ³•ä¸ºåœ¨ä¸æ”¹å˜æ¨¡å‹æƒé‡çš„å‰æä¸‹æ‰©å±• LLMs çš„è¯­ä¹‰è¾¹ç•Œå’Œæå‡å…¶ç”Ÿæˆèƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13874v2",
      "published_date": "2025-07-18 12:54:28 UTC",
      "updated_date": "2026-01-13 15:59:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:25.793358+00:00"
    },
    {
      "arxiv_id": "2507.14261v1",
      "title": "FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data",
      "title_zh": "FAMSTï¼šå¤§è§„æ¨¡é«˜ç»´æ•°æ®çš„å¿«é€Ÿè¿‘ä¼¼æœ€å°ç”Ÿæˆæ ‘æ„å»º",
      "authors": [
        "Mahmood K. M. Almansoori",
        "Miklos Telek"
      ],
      "abstract": "We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm that addresses the computational challenges of constructing Minimum Spanning Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a three-phase approach: Approximate Nearest Neighbor (ANN) graph construction, ANN inter-component connection, and iterative edge refinement. For a dataset of $n$ points in a $d$-dimensional space, FAMST achieves $\\mathcal{O}(dn \\log n)$ time complexity and $\\mathcal{O}(dn + kn)$ space complexity when $k$ nearest neighbors are considered, which is a significant improvement over the $\\mathcal{O}(n^2)$ time and space complexity of traditional methods.\n  Experiments across diverse datasets demonstrate that FAMST achieves remarkably low approximation errors while providing speedups of up to 1000$\\times$ compared to exact MST algorithms. We analyze how the key hyperparameters, $k$ (neighborhood size) and $Î»$ (inter-component edges), affect performance, providing practical guidelines for hyperparameter selection. FAMST enables MST-based analysis on datasets with millions of points and thousands of dimensions, extending the applicability of MST techniques to problem scales previously considered infeasible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FAMSTï¼Œä¸€ç§ä¸ºå¤§è§„æ¨¡ã€é«˜ç»´æ•°æ®é›†è®¾è®¡çš„æ–°å‹å¿«é€Ÿè¿‘ä¼¼æœ€å°ç”Ÿæˆæ ‘(Minimum Spanning Trees, MST)æ„å»ºç®—æ³•ã€‚è¯¥ç®—æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬æ„å»ºè¿‘ä¼¼æœ€è¿‘é‚»(Approximate Nearest Neighbor, ANN)å›¾ã€è¿›è¡Œç»„ä»¶é—´è¿æ¥ä»¥åŠè¿­ä»£è¾¹ç¼˜ä¼˜åŒ–ã€‚å¯¹äºåŒ…å«nä¸ªdç»´ç‚¹çš„æ•°æ®é›†ï¼ŒFAMSTå®ç°äº†$\\mathcal{O}(dn \\log n)$çš„æ—¶é—´å¤æ‚åº¦å’Œ$\\mathcal{O}(dn + kn)$çš„ç©ºé—´å¤æ‚åº¦ï¼Œè¾ƒä¼ ç»Ÿæ–¹æ³•çš„$\\mathcal{O}(n^2)$å¤æ‚åº¦æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFAMSTåœ¨ä¿æŒæä½è¿‘ä¼¼è¯¯å·®çš„åŒæ—¶ï¼Œç›¸è¾ƒäºç²¾ç¡®MSTç®—æ³•å®ç°äº†é«˜è¾¾1000å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†é‚»åŸŸå¤§å°$k$å’Œç»„ä»¶é—´è¾¹ç¼˜å‚æ•°$Î»$å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æä¾›äº†è¶…å‚æ•°é€‰æ‹©çš„å®è·µæŒ‡å—ã€‚FAMSTä½¿å¾—å¯¹æ•°ç™¾ä¸‡ä¸ªç‚¹å’Œæ•°åƒä¸ªç»´åº¦çš„æ•°æ®é›†è¿›è¡ŒMSTåˆ†ææˆä¸ºå¯èƒ½ï¼Œæå¤§åœ°æ‰©å±•äº†è¯¥æŠ€æœ¯åœ¨å¤§è§„æ¨¡é—®é¢˜ä¸Šçš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14261v1",
      "published_date": "2025-07-18 12:53:58 UTC",
      "updated_date": "2025-07-18 12:53:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:27.554658+00:00"
    },
    {
      "arxiv_id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "title_zh": "å½“è§†è§‰å‡Œé©¾äºè®¤çŸ¥ï¼šè§£æ„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†å†²çª",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources to address complex tasks, often encountering conflicts between their internal parametric knowledge and external information. Knowledge conflicts can result in hallucinations and unreliable responses, but the mechanisms governing such interactions remain unknown. To address this gap, we analyze the mechanisms that VLMs use to resolve cross-modal conflicts by introducing a dataset of multimodal counterfactual queries that deliberately contradict internal commonsense knowledge. We localize with logit inspection a small set of heads that control the conflict. Moreover, by modifying these heads, we can steer the model towards its internal knowledge or the visual inputs. Finally, we show that attention from such heads pinpoints localized image regions driving visual overrides, outperforming gradient-based attribution in precision.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å¤„ç†ä»»åŠ¡æ—¶ï¼Œå…¶å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯† (internal parametric knowledge) ä¸å¤–éƒ¨è§†è§‰ä¿¡æ¯ä¹‹é—´äº§ç”Ÿçš„å†²çªé—®é¢˜ã€‚ä¸ºåˆ†æè¿™ç§è·¨æ¨¡æ€å†²çªçš„è§£å†³æœºåˆ¶ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šæ¨¡æ€åäº‹å®æŸ¥è¯¢ (multimodal counterfactual queries) çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ•…æ„è¯±å¯¼è§†è§‰è¾“å…¥ä¸å†…éƒ¨å¸¸è¯†çŸ¥è¯†ä¹‹é—´çš„çŸ›ç›¾ã€‚ç ”ç©¶é€šè¿‡ Logit æ£€æŸ¥ (logit inspection) å®šä½äº†ä¸€ç»„è´Ÿè´£æ§åˆ¶å†²çªçš„ç‰¹å®š Headï¼Œå¹¶è¯æ˜é€šè¿‡ä¿®æ”¹è¿™äº› Head å¯ä»¥å¼•å¯¼æ¨¡å‹åœ¨è¾“å‡ºæ—¶å€¾å‘äºå…¶å†…éƒ¨çŸ¥è¯†æˆ–è§†è§‰è¾“å…¥ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘ç°è¿™äº› Head çš„æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿç²¾å‡†è¯†åˆ«å¯¼è‡´è§†è§‰è¦†ç›– (visual overrides) çš„å›¾åƒåŒºåŸŸï¼Œå…¶å®šä½ç²¾åº¦ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„å½’å›  (gradient-based attribution) æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13868v1",
      "published_date": "2025-07-18 12:42:30 UTC",
      "updated_date": "2025-07-18 12:42:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:56.147534+00:00"
    },
    {
      "arxiv_id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ SPARQL æŸ¥è¯¢ç”Ÿæˆï¼šè¯„ä¼°è®­ç»ƒæ•°æ®è®°å¿†ä¸çŸ¥è¯†æ³¨å…¥çš„å½±å“",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces cannot be underestimated. In particular, in Question Answering (QA) systems, generating a SPARQL query for a given natural-language question (often named Query Building) from the information retrieved from the same question is the central task of QA systems working over Knowledge Graphs (KGQA). Due to the rise of Large Language Models (LLMs), they are considered a well-suited method to increase the quality of the question-answering functionality, as there is still a lot of room for improvement, aiming for enhanced quality and trustworthiness. However, LLMs are trained on web data, where researchers have no control over whether the benchmark or the knowledge graph was already included in the training data. In this paper, we introduce a novel method that evaluates the quality of LLMs by generating a SPARQL query from a natural-language question under various conditions: (1) zero-shot SPARQL generation, (2) with knowledge injection, and (3) with \"anonymized\" knowledge injection. This enables us, for the first time, to estimate the influence of the training data on the QA quality improved by LLMs. Ultimately, this will help to identify how portable a method is or whether good results might mostly be achieved because a benchmark was already included in the training data (cf. LLM memorization). The developed method is portable, robust, and supports any knowledge graph; therefore, it could be easily applied to any KGQA or LLM, s.t., generating consistent insights into the actual LLM capabilities is possible.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨çŸ¥è¯†å›¾è°±é—®ç­” (KGQA) ç³»ç»Ÿä¸­ç”Ÿæˆ SPARQL æŸ¥è¯¢çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜æŸ¥è¯¢æ„å»º (Query Building) çš„è´¨é‡ä¸å¯ä¿¡åº¦ã€‚é’ˆå¯¹ LLMs åœ¨é¢„è®­ç»ƒé˜¶æ®µå¯èƒ½å·²ç»â€œè®°å¿†â€äº†æµ‹è¯•åŸºå‡†æ•°æ®ä»è€Œå¯¼è‡´è¯„ä¼°åå·®çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡ zero-shot SPARQL ç”Ÿæˆã€çŸ¥è¯†æ³¨å…¥ (Knowledge Injection) ä»¥åŠåŒ¿ååŒ–çŸ¥è¯†æ³¨å…¥ (Anonymized Knowledge Injection) ä¸‰ç§è®¾ç½®æ¥è¡¡é‡æ¨¡å‹è¡¨ç°ã€‚è¿™ç§æ–¹æ³•é¦–æ¬¡å®ç°äº†å¯¹è®­ç»ƒæ•°æ®å¯¹é—®ç­”è´¨é‡å½±å“çš„é‡åŒ–ä¼°è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ¨¡å‹æ˜¯å…·å¤‡çœŸå®çš„é€»è¾‘ç”Ÿæˆèƒ½åŠ›è¿˜æ˜¯ä»…ä»…ä¾èµ–æ•°æ®è®°å¿† (LLM Memorization)ã€‚è¯¥è¯„ä¼°æ¡†æ¶å…·æœ‰é«˜åº¦çš„å¯ç§»æ¤æ€§å’Œé²æ£’æ€§ï¼Œæ”¯æŒä»»ä½•çŸ¥è¯†å›¾è°±å’Œ LLMï¼Œä¸ºç”Ÿæˆå…³äºå¤§è¯­è¨€æ¨¡å‹å®é™…èƒ½åŠ›çš„æŒç»­æ€§è§è§£æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Winner of Best Paper Award at the 25th International Conference on Web Engineering (ICWE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.13859v1",
      "published_date": "2025-07-18 12:28:08 UTC",
      "updated_date": "2025-07-18 12:28:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:51.484791+00:00"
    },
    {
      "arxiv_id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "title_zh": "é¢å‘åŠ¨æ€ç¯å¢ƒå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å› æœçŸ¥è¯†è¿ç§»",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable success in environments where agents must learn coordinated behaviors. However, transferring knowledge across agents remains challenging in non-stationary environments with changing goals. [Problem] Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt. [Approach] This paper introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. As the environment changes (new obstacles), agents' collisions require adaptive recovery strategies. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion, i.e., without retraining, just by querying a lookup model with local context information (collisions). [Results] Our findings reveal two key insights: (1) agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and (2) the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning) åœ¨éå¹³ç¨³ã€åŠ¨æ€ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¿ç§»éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å› æœçŸ¥è¯†è¿ç§»æ¡†æ¶ (causal knowledge transfer framework)ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“å­¦ä¹ å¹¶å…±äº«è·¯å¾„çš„ç´§å‡‘å› æœè¡¨ç¤ºï¼Œå¹¶å°†ç¯å¢ƒä¸­çš„ç¢°æ’å»ºæ¨¡ä¸ºå› æœå¹²é¢„ (causal intervention)ã€‚é€šè¿‡å¼•å…¥æ¢å¤åŠ¨ä½œå® (recovery action macro)ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨çº¿ä»å…¶ä»–æ™ºèƒ½ä½“å¤„è·å–å¹¶å…±äº«ç»•è¿‡éšœç¢ç‰©çš„ç­–ç•¥ï¼Œå¹¶ä»¥é›¶æ ·æœ¬ (zero-shot) çš„æ–¹å¼ç›´æ¥åº”ç”¨è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰å¼‚æ„ç›®æ ‡çš„æ™ºèƒ½ä½“åœ¨é€‚åº”æ–°ç¯å¢ƒæ—¶ï¼Œèƒ½å¤Ÿå¡«è¡¥éšæœºæ¢ç´¢ä¸å®Œå…¨é‡è®­ç»ƒç­–ç•¥ä¹‹é—´çº¦ä¸€åŠçš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†å› æœçŸ¥è¯†è¿ç§»çš„æ•ˆæœå—ç¯å¢ƒå¤æ‚ç¨‹åº¦ä¸æ™ºèƒ½ä½“å¼‚æ„ç›®æ ‡ä¹‹é—´ç›¸äº’ä½œç”¨çš„å½±å“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13846v1",
      "published_date": "2025-07-18 11:59:55 UTC",
      "updated_date": "2025-07-18 11:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:37.056011+00:00"
    },
    {
      "arxiv_id": "2507.19518v1",
      "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction",
      "title_zh": "åŸºäº GNN åŒºåŸŸé¢„æµ‹çš„å¤§è§„æ¨¡ç½‘è¡¨ç›®æ ‡ç”µè·¯åŒ¹é…",
      "authors": [
        "Sangwoo Seo",
        "Jimin Seo",
        "Yoonho Lee",
        "Donghyeon Kim",
        "Hyejin Shin",
        "Banghyun Sung",
        "Chanyoung Park"
      ],
      "abstract": "Subgraph matching plays an important role in electronic design automation (EDA) and circuit verification. Traditional rule-based methods have limitations in generalizing to arbitrary target circuits. Furthermore, node-to-node matching approaches tend to be computationally inefficient, particularly for large-scale circuits. Deep learning methods have emerged as a potential solution to address these challenges, but existing models fail to efficiently capture global subgraph embeddings or rely on inefficient matching matrices, which limits their effectiveness for large circuits. In this paper, we propose an efficient graph matching approach that utilizes Graph Neural Networks (GNNs) to predict regions of high probability for containing the target circuit. Specifically, we construct various negative samples to enable GNNs to accurately learn the presence of target circuits and develop an approach to directly extracting subgraph embeddings from the entire circuit, which captures global subgraph information and addresses the inefficiency of applying GNNs to all candidate subgraphs. Extensive experiments demonstrate that our approach significantly outperforms existing methods in terms of time efficiency and target region prediction, offering a scalable and effective solution for subgraph matching in large-scale circuits.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–(EDA)å’Œç”µè·¯éªŒè¯ä¸­çš„å­å›¾åŒ¹é…(Subgraph matching)é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤§è§„æ¨¡ç”µè·¯ä¸­å­˜åœ¨æ³›åŒ–èƒ½åŠ›å¼±åŠè®¡ç®—æ•ˆç‡ä½ç­‰å±€é™ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNNs)çš„é«˜æ•ˆåŒ¹é…æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹ç”µè·¯ä¸­åŒ…å«ç›®æ ‡ç”µè·¯çš„é«˜æ¦‚ç‡åŒºåŸŸæ¥å®ç°å¿«é€Ÿå®šä½ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå¤šç§è´Ÿæ ·æœ¬(negative samples)ä½¿GNNèƒ½å¤Ÿå‡†ç¡®å­¦ä¹ å¹¶è¯†åˆ«ç›®æ ‡ç”µè·¯ï¼Œå¹¶å¼€å‘äº†ä¸€ç§ç›´æ¥ä»å®Œæ•´ç”µè·¯ä¸­æå–å­å›¾åµŒå…¥(subgraph embeddings)çš„æŠ€æœ¯ï¼Œæœ‰æ•ˆæ•è·äº†å…¨å±€å­å›¾ä¿¡æ¯å¹¶è§£å†³äº†è®¡ç®—ä½æ•ˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ç½‘è¡¨(Large-Scale Netlists)æ—¶ï¼Œåœ¨æ—¶é—´æ•ˆç‡å’Œç›®æ ‡åŒºåŸŸé¢„æµ‹å‡†ç¡®æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¤§è§„æ¨¡ç”µè·¯çš„å­å›¾åŒ¹é…æä¾›äº†ä¸€ç§å…·å¤‡å¯æ‰©å±•æ€§çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICCAD 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.19518v1",
      "published_date": "2025-07-18 11:43:19 UTC",
      "updated_date": "2025-07-18 11:43:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:32:49.394234+00:00"
    },
    {
      "arxiv_id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "title_zh": "åŸºäºå‰ªæå­æ¨¡å›¾çš„å¯æ‰©å±•å­æ¨¡ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the environment via a set of possible actions, and a reward is generated from some unknown distribution. The task here is to find an optimal set of actions such that the reward after a certain time step gets maximized. In a traditional setup, the reward function in an RL Problem is considered additive. However, in reality, there exist many problems, including path planning, coverage control, etc., the reward function follows the diminishing return, which can be modeled as a submodular function. In this paper, we study a variant of the RL Problem where the reward function is submodular, and our objective is to find an optimal policy such that this reward function gets maximized. We have proposed a pruned submodularity graph-based approach that provides a provably approximate solution in a feasible computation time. The proposed approach has been analyzed to understand its time and space requirements as well as a performance guarantee. We have experimented with a benchmark agent-environment setup, which has been used for similar previous studies, and the results are reported. From the results, we observe that the policy obtained by our proposed approach leads to more reward than the baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†è·¯å¾„è§„åˆ’ç­‰å…·æœ‰æ”¶ç›Šé€’å‡ç‰¹æ€§é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¿®å‰ªå­æ¨¡å›¾(pruned submodularity graph)çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶è€…å°†æ­¤ç±»ä»»åŠ¡å»ºæ¨¡ä¸ºå¥–åŠ±å‡½æ•°å…·æœ‰å­æ¨¡æ€§(submodularity)çš„å¼ºåŒ–å­¦ä¹ å˜ä½“ï¼Œæ—¨åœ¨å¯»æ‰¾èƒ½å¤Ÿæœ€å¤§åŒ–è¯¥å¥–åŠ±å‡½æ•°çš„æœ€ä¼˜ç­–ç•¥ã€‚æ‰€ææ–¹æ³•é€šè¿‡æ„å»ºå¹¶ä¿®å‰ªå­æ¨¡å›¾ï¼Œåœ¨ä¿è¯è®¡ç®—å¯è¡Œæ€§çš„å‰æä¸‹æä¾›äº†å…·æœ‰ç†è®ºæ€§èƒ½ä¿è¯çš„è¿‘ä¼¼è§£(provably approximate solution)ã€‚è®ºæ–‡è¿›ä¸€æ­¥åˆ†æäº†è¯¥ç®—æ³•çš„æ—¶é—´ä¸ç©ºé—´å¤æ‚åº¦ï¼Œå¹¶ç»™å‡ºäº†ä¸¥æ ¼çš„æ€§èƒ½è¯æ˜ã€‚åœ¨åŸºå‡†æµ‹è¯•ç¯å¢ƒä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è·å–å¥–åŠ±çš„èƒ½åŠ›ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ–¹æ³•(baseline methods)ï¼Œä¸ºè§£å†³å¤§è§„æ¨¡å­æ¨¡å¥–åŠ±ä¼˜åŒ–é—®é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "16 Pages",
      "pdf_url": "https://arxiv.org/pdf/2507.13834v1",
      "published_date": "2025-07-18 11:42:07 UTC",
      "updated_date": "2025-07-18 11:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:39.284633+00:00"
    },
    {
      "arxiv_id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "title_zh": "é€Ÿåº¦ä¸ç²¾åº¦çš„å…¼é¡¾ï¼šä¸€ç§é¢å‘æ—¶åºé“¾æ¥é¢„æµ‹çš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„å›¾æ¨¡å‹",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with applications in diverse domains such as social networks, recommendation systems, and e-commerce platforms. While existing Temporal Graph Neural Networks (T-GNNs) have achieved notable success by leveraging complex architectures to model temporal and structural dependencies, they often suffer from scalability and efficiency challenges due to high computational overhead. In this paper, we propose EAGLE, a lightweight framework that integrates short-term temporal recency and long-term global structural patterns. EAGLE consists of a time-aware module that aggregates information from a node's most recent neighbors to reflect its immediate preferences, and a structure-aware module that leverages temporal personalized PageRank to capture the influence of globally important nodes. To balance these attributes, EAGLE employs an adaptive weighting mechanism to dynamically adjust their contributions based on data characteristics. Also, EAGLE eliminates the need for complex multi-hop message passing or memory-intensive mechanisms, enabling significant improvements in efficiency. Extensive experiments on seven real-world temporal graphs demonstrate that EAGLE consistently achieves superior performance against state-of-the-art T-GNNs in both effectiveness and efficiency, delivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€å›¾ä¸­æ—¶åºé“¾è·¯é¢„æµ‹ (Temporal Link Prediction) ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªå…¼é¡¾æ•ˆç‡ä¸å‡†ç¡®æ€§çš„è½»é‡çº§æ¡†æ¶ EAGLEï¼Œä»¥è§£å†³ç°æœ‰ T-GNNs æ¨¡å‹è®¡ç®—å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ã€‚EAGLE ç”±æ—¶é—´æ„ŸçŸ¥æ¨¡å— (Time-aware module) å’Œç»“æ„æ„ŸçŸ¥æ¨¡å— (Structure-aware module) ç»„æˆï¼Œåˆ†åˆ«ç”¨äºæ•æ‰èŠ‚ç‚¹çš„çŸ­æœŸåå¥½å’ŒåŸºäºæ—¶åºä¸ªæ€§åŒ– PageRank (Temporal Personalized PageRank) çš„å…¨å±€ç»“æ„å½±å“ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”åŠ æƒæœºåˆ¶ (Adaptive weighting mechanism)ï¼Œè¯¥æ¡†æ¶èƒ½æ ¹æ®æ•°æ®ç‰¹å¾åŠ¨æ€è°ƒæ•´ä¸åŒæ¨¡å—çš„è´¡çŒ®åº¦ï¼Œå¹¶æœ‰æ•ˆé¿å…äº†å¤æ‚çš„å¤šè·³æ¶ˆæ¯ä¼ é€’ (Multi-hop message passing) å’Œå†…å­˜å¯†é›†å‹æœºåˆ¶å¸¦æ¥çš„èµ„æºæ¶ˆè€—ã€‚åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEAGLE çš„æ€§èƒ½ä¸€è‡´ä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨è¿è¡Œæ•ˆç‡æ–¹é¢ï¼ŒEAGLE ç›¸æ¯”äºåŸºäº Transformer çš„ T-GNNs å®ç°äº†è¶…è¿‡ 50 å€çš„åŠ é€Ÿï¼Œä¸ºå¤§è§„æ¨¡åŠ¨æ€å›¾åº”ç”¨æä¾›äº†å…¼å…·é«˜æ•ˆæ€§ä¸æœ‰æ•ˆæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted in 2024. Accepted in 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13825v1",
      "published_date": "2025-07-18 11:29:15 UTC",
      "updated_date": "2025-07-18 11:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:49.484355+00:00"
    },
    {
      "arxiv_id": "2507.14256v1",
      "title": "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models",
      "title_zh": "ä»£ç ä¸Šä¸‹æ–‡ä¸æç¤ºç­–ç•¥å¯¹ç°ä»£é€šç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆçš„å½±å“",
      "authors": [
        "Jakub Walczak",
        "Piotr Tomalak",
        "Artur Laskowski"
      ],
      "abstract": "Generative AI is gaining increasing attention in software engineering, where testing remains an indispensable reliability mechanism. According to the widely adopted testing pyramid, unit tests constitute the majority of test cases and are often schematic, requiring minimal domain expertise. Automatically generating such tests under the supervision of software engineers can significantly enhance productivity during the development phase of the software lifecycle.\n  This paper investigates the impact of code context and prompting strategies on the quality and adequacy of unit tests generated by various large language models (LLMs) across several families. The results show that including docstrings notably improves code adequacy, while further extending context to the full implementation yields definitely smaller gains. Notably, the chain-of-thought prompting strategy -- applied even to 'reasoning' models -- achieves the best results, with up to 96.3\\% branch coverage, a 57\\% average mutation score, and near-perfect compilation success rate. Among the evaluated models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation score and branch coverage being still in top in terms of compilation success rate.\n  All the code and resulting test suites are publicly available at https://github.com/peetery/LLM-analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç ä¸Šä¸‹æ–‡(code context)å’Œæç¤ºè¯ç­–ç•¥(prompting strategies)å¯¹ç°ä»£é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨ç”Ÿæˆå•å…ƒæµ‹è¯•(unit test)è´¨é‡åŠå……åˆ†æ€§çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æç¤ºä¸­åŒ…å«æ–‡æ¡£å­—ç¬¦ä¸²(docstrings)èƒ½æ˜¾è‘—æå‡ä»£ç å……åˆ†æ€§ï¼Œè€Œå°†ä¸Šä¸‹æ–‡è¿›ä¸€æ­¥æ‰©å±•è‡³å®Œæ•´å®ç°(full implementation)æ‰€å¸¦æ¥çš„æ”¶ç›Šåˆ™ç›¸å¯¹æœ‰é™ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå³ä½¿æ˜¯å¯¹å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œé‡‡ç”¨é“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºç­–ç•¥ä¹Ÿèƒ½å–å¾—æœ€ä½³ç»“æœï¼Œå®ç°äº†é«˜è¾¾96.3%çš„åˆ†æ”¯è¦†ç›–ç‡(branch coverage)å’Œ57%çš„å¹³å‡å˜å¼‚å¾—åˆ†(mutation score)ã€‚åœ¨æ‰€æœ‰å—è¯„ä¼°çš„æ¨¡å‹ä¸­ï¼ŒGemini 2.5 Pro (M5) åœ¨å˜å¼‚å¾—åˆ†å’Œåˆ†æ”¯è¦†ç›–ç‡æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”ç¼–è¯‘æˆåŠŸç‡(compilation success rate)å¤„äºé¢†å…ˆåœ°ä½ã€‚è¯¥é¡¹å·¥ä½œä¸ºåˆ©ç”¨ç”Ÿæˆå¼AIæå‡è½¯ä»¶å·¥ç¨‹ä¸­çš„æµ‹è¯•æ•ˆç‡æä¾›äº†é‡è¦å‚è€ƒï¼Œç›¸å…³ä»£ç å’Œæµ‹è¯•é›†å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14256v1",
      "published_date": "2025-07-18 11:23:17 UTC",
      "updated_date": "2025-07-18 11:23:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:46.792959+00:00"
    },
    {
      "arxiv_id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­åŸºäº RAG çš„è¯ç‰©å‰¯ä½œç”¨æ£€ç´¢æ¶æ„",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced methods for their accurate detection and analysis. While Large Language Models (LLMs) offer promising conversational interfaces, their inherent limitations, including reliance on black-box training data, susceptibility to hallucinations, and lack of domain-specific knowledge, hinder their reliability in specialized fields like pharmacovigilance. To address this gap, we propose two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which integrate comprehensive drug side effect knowledge into a Llama 3 8B language model. Through extensive evaluations on 19,520 drug side effect associations (covering 976 drugs and 3,851 side effect terms), our results demonstrate that GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This framework offers a highly accurate and scalable solution, signifying a significant advancement in leveraging LLMs for critical pharmacovigilance applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¯ç‰©å‰¯ä½œç”¨è¿™ä¸€å…³é”®å¥åº·é—®é¢˜æ—¶å­˜åœ¨çš„å¹»è§‰ã€é¢†åŸŸçŸ¥è¯†åŒ®ä¹åŠé»‘ç›’è®­ç»ƒæ•°æ®ä¾èµ–ç­‰å±€é™æ€§ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰å’Œ GraphRAG ä¸¤ç§æ¶æ„ã€‚ç ”ç©¶é€šè¿‡å°†å…¨é¢çš„è¯ç‰©å‰¯ä½œç”¨çŸ¥è¯†åº“é›†æˆåˆ° Llama 3 8B æ¨¡å‹ä¸­ï¼Œæ—¨åœ¨æå‡å…¶åœ¨è¯ç‰©è­¦æˆ’ï¼ˆpharmacovigilanceï¼‰ä¸“ä¸šé¢†åŸŸçš„å¯é æ€§ã€‚å®éªŒåœ¨æ¶µç›– 976 ç§è¯ç‰©å’Œ 3,851 ä¸ªå‰¯ä½œç”¨æœ¯è¯­çš„ 19,520 ä¸ªå…³è”æ•°æ®ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ GraphRAG åœ¨è¯ç‰©å‰¯ä½œç”¨æ£€ç´¢ä¸­å®ç°äº†æ¥è¿‘å®Œç¾çš„å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§é«˜ç²¾åº¦ä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ ‡å¿—ç€åˆ©ç”¨ LLMs å¤„ç†å…³é”®è¯ç‰©è­¦æˆ’åº”ç”¨å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä¸ºåŒ»è¯é¢†åŸŸçš„æ™ºèƒ½åŒ–è¾…åŠ©æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13822v1",
      "published_date": "2025-07-18 11:20:52 UTC",
      "updated_date": "2025-07-18 11:20:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:52.754808+00:00"
    },
    {
      "arxiv_id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "title_zh": "Team of Oneï¼šåŸºäºæ¨¡å‹ååŒæ”»å…‹å¤æ‚è§†é¢‘é—®ç­”",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that enhances reasoning depth and robustness in complex real-world scenarios, as benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries. To address these challenges, we introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought, each tailored to distinct reasoning pathways. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses. Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness. Our approach offers a lightweight, extensible strategy for advancing multimodal reasoning without requiring model retraining, setting a strong foundation for future Video-LMM development.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º Team of One çš„æ–°å‹å¼€æ”¾å¼è§†é¢‘é—®ç­” (Video QA) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤æ‚ç°å®åœºæ™¯ä¸­çš„æ¨ç†æ·±åº¦å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†ç°æœ‰è§†é¢‘å¤§è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ (Video-LMMs) åœ¨è¯­å¢ƒç†è§£ã€æ—¶é—´å»ºæ¨¡ä»¥åŠå¤„ç†æ­§ä¹‰æˆ–ç»„åˆæŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚å…¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†ä¸€ç§æç¤ºä¸å“åº”é›†æˆæœºåˆ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„é“¾å¼æ€ç»´ (Chain-of-Thought) åè°ƒå¤šä¸ªå¼‚æ„çš„è§†é¢‘è¯­è¨€æ¨¡å‹ (VLMs) ååŒå·¥ä½œã€‚å¤–éƒ¨å¤§è¯­è¨€æ¨¡å‹ (LLM) åˆ™å……å½“è¯„ä¼°è€…ä¸æ•´åˆè€…ï¼Œè´Ÿè´£ç­›é€‰å¹¶èåˆæ¥è‡ªä¸åŒæ¨ç†è·¯å¾„çš„æœ€å¯é å“åº”ã€‚åœ¨ CVRR-ES æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•æ— éœ€è¿›è¡Œæ¨¡å‹é‡æ–°è®­ç»ƒ (retraining)ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§è½»é‡ä¸”å¯æ‰©å±•çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13820v1",
      "published_date": "2025-07-18 11:12:44 UTC",
      "updated_date": "2025-07-18 11:12:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:33:58.592493+00:00"
    },
    {
      "arxiv_id": "2507.13802v2",
      "title": "Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database",
      "title_zh": "æ¬§æ´²é£Ÿå“å®‰å…¨è¶‹åŠ¿ï¼šåŸºäºæ‹¥æœ‰3.92äº¿æ¡è®°å½•çš„å…¨é¢æ¬§æ´²é£Ÿå“å®‰å…¨ï¼ˆCHEFSï¼‰æ•°æ®åº“çš„æ·±åº¦æ´å¯Ÿ",
      "authors": [
        "Nehir Kizililsoley",
        "Floor van Meer",
        "Osman Mutlu",
        "Wouter F Hoenderdaal",
        "Rosan G. HobÃ©",
        "Wenjuan Mu",
        "Arjen Gerssen",
        "H. J. van der Fels-Klerx",
        "Ãkos JÃ³Åºwiak",
        "Ioannis Manikas",
        "Ali HÃ¼rriyetoÇ§lu",
        "Bas H. M. van der Velden"
      ],
      "abstract": "In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†CompreHensive European Food Safety (CHEFS)æ•°æ®åº“ï¼Œæ—¨åœ¨è§£å†³æ¬§æ´²é£Ÿå“å®‰å…¨å±€(EFSA)å®˜æ–¹ç›‘æµ‹æ•°æ®å› æ ¼å¼åˆ†æ•£ä¸”è§„æ¨¡åºå¤§è€Œå¯¼è‡´çš„åˆ†æéšœç¢ã€‚ç ”ç©¶å›¢é˜Ÿå°†æ¥è‡ª1000å¤šä¸ªæ–‡ä»¶ã€æ¶‰åŠ3.92äº¿æ¡åˆ†æç»“æœçš„åŸå§‹æ•°æ®æ•´åˆæˆç»Ÿä¸€ä¸”ç»“æ„åŒ–çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†pesticide residuesã€veterinary medicinal product residuesä»¥åŠchemical contaminantsç­‰å…³é”®é¢†åŸŸã€‚é€šè¿‡å¯¹2000å¹´è‡³2024å¹´æ•°æ®çš„æ·±åº¦åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†æ¬§æ´²é£Ÿå“å®‰å…¨ç›‘æµ‹æ´»åŠ¨çš„æ¼”å˜è¶‹åŠ¿ã€é«˜é¢‘æ£€æµ‹äº§å“ç±»åˆ«åŠéåˆè§„æ€§åˆ†å¸ƒç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCHEFSæ•°æ®åº“æ˜¾è‘—æå‡äº†æµ·é‡æ•°æ®çš„å¯è®¿é—®æ€§ï¼Œå¹¶èƒ½ä¸ºäººå·¥æ™ºèƒ½(AI)é©±åŠ¨çš„å±å®³é¢„æµ‹å’Œæ—©æœŸé¢„è­¦ç³»ç»Ÿæä¾›æ ¸å¿ƒæ”¯æ’‘ã€‚è¯¥æ•°æ®åº“çš„å»ºç«‹ä¸ä»…ä¸ºé£Ÿå“å®‰å…¨é¢†åŸŸçš„ç§‘å­¦ç ”ç©¶æä¾›äº†é›†ä¸­å¼èµ„æºï¼Œæ›´ä¸ºç›¸å…³æ”¿ç­–åˆ¶å®šå’Œè¡Œä¸šç›‘ç®¡æä¾›äº†é‡è¦çš„æˆ˜ç•¥å·¥å…·ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13802v2",
      "published_date": "2025-07-18 10:29:30 UTC",
      "updated_date": "2025-09-05 11:44:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:09.494865+00:00"
    },
    {
      "arxiv_id": "2507.13801v1",
      "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion",
      "title_zh": "æ›´è¿›ä¸€æ­¥ï¼šé€šè¿‡â€œåˆ›é€ æœªæ¥â€ç­–ç•¥å¢å¼ºå•ç›®è¯­ä¹‰åœºæ™¯è¡¥å…¨",
      "authors": [
        "Haoang Lu",
        "Yuanqi Su",
        "Xiaoning Zhang",
        "Hao Hu"
      ],
      "abstract": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a critical perception task for autonomous driving due to its ability to infer complete 3D scene layouts and semantics from single 2D images. However, in real-world traffic scenarios, a significant portion of the scene remains occluded or outside the camera's field of view -- a fundamental challenge that existing monocular SSC methods fail to address adequately. To overcome these limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC framework that leverages pseudo-future frame prediction to expand the model's effective perceptual range. Our approach combines poses and depths to establish accurate 3D correspondences, enabling geometrically-consistent fusion of past, present, and predicted future frames in 3D space. Unlike conventional methods that rely on simple feature stacking, our 3D-aware architecture achieves more robust scene completion by explicitly modeling spatial-temporal relationships. Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks demonstrate state-of-the-art performance, validating the effectiveness of our approach, highlighting our method's ability to improve occlusion reasoning and 3D scene completion accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion, SSC)åœ¨è‡ªåŠ¨é©¾é©¶ä¸­é¢ä¸´çš„åœºæ™¯é®æŒ¡åŠè§†åœº(Field of View)å—é™ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºCF-SSC (Creating the Future SSC)çš„æ–°å‹æ—¶é—´åºåˆ—æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¼ªæœªæ¥å¸§é¢„æµ‹(pseudo-future frame prediction)æŠ€æœ¯ï¼Œæœ‰æ•ˆæ‰©å±•äº†æ¨¡å‹çš„æ„ŸçŸ¥èŒƒå›´ã€‚é€šè¿‡ç»“åˆä½å§¿(poses)å’Œæ·±åº¦(depths)ä¿¡æ¯ï¼Œè¯¥æ¡†æ¶å»ºç«‹äº†ç²¾ç¡®çš„3Då¯¹åº”å…³ç³»ï¼Œå®ç°äº†è¿‡å»ã€ç°åœ¨åŠé¢„æµ‹æœªæ¥å¸§åœ¨3Dç©ºé—´å†…çš„å‡ ä½•ä¸€è‡´æ€§èåˆã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç‰¹å¾å †å æ–¹å¼ï¼ŒCF-SSCåˆ©ç”¨3Dæ„ŸçŸ¥æ¶æ„æ˜¾å¼å»ºæ¨¡æ—¶ç©ºå…³ç³»ï¼Œå¢å¼ºäº†åœºæ™¯è¡¥å…¨çš„ç¨³å¥æ€§ã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI-360åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶æ˜¾è‘—æå‡äº†é®æŒ¡æ¨ç†å’Œè¡¥å…¨å‡†ç¡®åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13801v1",
      "published_date": "2025-07-18 10:24:58 UTC",
      "updated_date": "2025-07-18 10:24:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:05.294230+00:00"
    },
    {
      "arxiv_id": "2507.13789v1",
      "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI",
      "title_zh": "ç”¨äºåŠ¨è„‰ç˜¤ç£å…±æŒ¯æˆåƒæ—¶ç©ºè¡€æµåŠ¨åŠ›å­¦ä¸Šé‡‡æ ·çš„å±€éƒ¨å‚…é‡Œå¶ç¥ç»ç®—å­",
      "authors": [
        "Kyriakos Flouris",
        "Moritz Halter",
        "Yolanne Y. R. Lee",
        "Samuel Castonguay",
        "Luuk Jacobs",
        "Pietro Dirix",
        "Jonathan Nestmann",
        "Sebastian Kozerke",
        "Ender Konukoglu"
      ],
      "abstract": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ¸ç£å…±æŒ¯(MRI)è¡€æµæˆåƒåœ¨æ—¶ç©ºåˆ†è¾¨ç‡å’Œä¿¡å™ªæ¯”æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†å±€éƒ¨å‚…é‡Œå¶ç¥ç»ç®—å­(Localized Fourier Neural Operator, LoFNO)è¿™ä¸€æ–°å‹3Dæ¶æ„ï¼Œç”¨äºå®ç°è¡€æµåŠ¨åŠ›å­¦çš„æ—¶ç©ºä¸Šé‡‡æ ·ã€‚LoFNOé€šè¿‡é›†æˆæ‹‰æ™®æ‹‰æ–¯ç‰¹å¾å‘é‡(Laplacian eigenvectors)ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ä¸è§„åˆ™åŠæœªçŸ¥å‡ ä½•ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶ç»“åˆå¢å¼ºå‹æ·±åº¦è¶…åˆ†è¾¨ç‡ç½‘ç»œ(EDSR)å±‚è¿›è¡Œç¨³å¥çš„å›¾åƒå¢å¼ºã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹æµä½“æ•°æ®è¿›è¡Œå»å™ªï¼Œå¹¶èƒ½ç›´æ¥ä»ä¸´åºŠå½±åƒä¸­é¢„æµ‹å£é¢å‰ªåˆ‡åº”åŠ›(Wall Shear Stress, WSS)ã€‚ä¸ä¼ ç»Ÿçš„æ’å€¼æ–¹æ³•æˆ–å…¶ä»–æ·±åº¦å­¦ä¹ æ–¹æ¡ˆç›¸æ¯”ï¼ŒLoFNOåœ¨è¡€æµé€Ÿåº¦å’ŒWSSçš„é¢„æµ‹ç²¾åº¦ä¸Šè¡¨ç°æ›´ä¸ºä¼˜å¼‚ã€‚è¿™é¡¹å·¥ä½œä¸ºæ›´ç²¾ç¡®çš„è„‘è¡€ç®¡è¯Šæ–­æä¾›äº†æŠ€æœ¯æ”¯æ’‘ï¼Œæœ‰åŠ©äºé€šè¿‡ä¸´åºŠå½±åƒæ•°æ®æ›´æœ‰æ•ˆåœ°è¯„ä¼°åŠ¨è„‰ç˜¤ç ´è£‚é£é™©å¹¶æŒ‡å¯¼æ²»ç–—å†³ç­–ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13789v1",
      "published_date": "2025-07-18 10:00:38 UTC",
      "updated_date": "2025-07-18 10:00:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:09.256501+00:00"
    },
    {
      "arxiv_id": "2507.21120v1",
      "title": "Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation",
      "title_zh": "åŸºäºéŸ³ä¹åå¥½è¯±å‘çš„è‰ºæœ¯æ²»ç–—æƒ…æ„Ÿæ„ŸçŸ¥è·¨åŸŸæ¨è",
      "authors": [
        "Bereket A. Yilma",
        "Luis A. Leiva"
      ],
      "abstract": "Art Therapy (AT) is an established practice that facilitates emotional processing and recovery through creative expression. Recently, Visual Art Recommender Systems (VA RecSys) have emerged to support AT, demonstrating their potential by personalizing therapeutic artwork recommendations. Nonetheless, current VA RecSys rely on visual stimuli for user modeling, limiting their ability to capture the full spectrum of emotional responses during preference elicitation. Previous studies have shown that music stimuli elicit unique affective reflections, presenting an opportunity for cross-domain recommendation (CDR) to enhance personalization in AT. Since CDR has not yet been explored in this context, we propose a family of CDR methods for AT based on music-driven preference elicitation. A large-scale study with 200 users demonstrates the efficacy of music-driven preference elicitation, outperforming the classic visual-only elicitation approach. Our source code, data, and models are available at https://github.com/ArtAICare/Affect-aware-CDR",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‰ºæœ¯æ²»ç–— (Art Therapy, AT) åœºæ™¯æå‡ºäº†é¦–ä¸ªè·¨åŸŸæ¨è (Cross-Domain Recommendation, CDR) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è‰ºæœ¯æ¨èç³»ç»Ÿ (VA RecSys) ä»…ä¾èµ–è§†è§‰åˆºæ¿€è€Œæ— æ³•å…¨é¢å»ºæ¨¡ç”¨æˆ·æƒ…æ„Ÿçš„é—®é¢˜ã€‚ç ”ç©¶è€…åˆ©ç”¨éŸ³ä¹åˆºæ¿€èƒ½å¤Ÿæ¿€å‘ç‹¬ç‰¹æƒ…æ„Ÿåå°„çš„ç‰¹æ€§ï¼Œå¼€å‘äº†ä¸€ç³»åˆ—åŸºäºéŸ³ä¹åå¥½å¼•å¯¼ (Music Preference Elicitation) çš„æƒ…æ„Ÿæ„ŸçŸ¥æ¨èç®—æ³•ã€‚é€šè¿‡ä¸€é¡¹æ¶‰åŠ 200 åç”¨æˆ·çš„å¤§è§„æ¨¡å®éªŒï¼Œè¯¥ç ”ç©¶è¯å®äº†éŸ³ä¹é©±åŠ¨çš„åå¥½è·å–æ–¹å¼åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è§†è§‰å¼•å¯¼æ–¹æ³•ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†è·¨åŸŸå»ºæ¨¡åœ¨æå‡è‰ºæœ¯æ²»ç–—ä¸ªæ€§åŒ–ä½“éªŒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³çš„æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at the 19th ACM Conference on Recommender Systems",
      "pdf_url": "https://arxiv.org/pdf/2507.21120v1",
      "published_date": "2025-07-18 09:53:03 UTC",
      "updated_date": "2025-07-18 09:53:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:08.392062+00:00"
    },
    {
      "arxiv_id": "2507.15880v1",
      "title": "The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture",
      "title_zh": "é€’å½’ä¸€è‡´æ€§åŸç†ï¼šé’ˆå¯¹å¯æ‰©å±•æ™ºèƒ½ã€å¯¹é½ä¸æ¨ç†æ¶æ„çš„å½¢å¼åŒ–çº¦æŸ",
      "authors": [
        "Andy E. Williams"
      ],
      "abstract": "Intelligence-biological, artificial, or collective-requires structural coherence across recursive reasoning processes to scale effectively. As complex systems grow, coherence becomes fragile unless a higher-order structure ensures semantic consistency. This paper introduces the Recursive Coherence Principle (RCP): a foundational constraint stating that for any reasoning system of order N, composed of systems operating over conceptual spaces of order N-1, semantic coherence is preserved only by a recursively evaluable generalization operator that spans and aligns those lower-order conceptual spaces. Crucially, this coherence enables structural alignment. Without recursive coherence, no system can reliably preserve goals, meanings, or reasoning consistency at scale. We formally define the Functional Model of Intelligence (FMI) as the only known operator capable of satisfying the RCP at any scale. The FMI is a minimal, composable architecture with internal functions (evaluation, modeling, adaptation, stability, decomposition, bridging) and external functions (storage, recall, System 1 and System 2 reasoning) vital for preserving semantic structure across inference and coordination layers. We prove that any system lacking the FMI will experience recursive coherence breakdown as it scales, arguing that common AI issues like misalignment, hallucination, and instability are symptoms of this structural coherence loss. Unlike other foundational principles, RCP uniquely captures the internal, recursive dynamics needed for coherent, alignable intelligence, modeling semantic coherence under recursion. This work significantly impacts AI alignment, advocating a shift from behavioral constraints to structural coherence, and offers a pathway for safely generalizable, robustly coherent AI at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Recursive Coherence Principle (RCP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç¡®ä¿å¯æ‰©å±•æ™ºèƒ½ã€å¯¹é½å’Œæ¨ç†æ¶æ„åœ¨é€’å½’æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒç»“æ„ä¸€è‡´æ€§çš„åŸºç¡€çº¦æŸã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¯¹äºä»»ä½•é˜¶æ•°ä¸ºNçš„æ¨ç†ç³»ç»Ÿï¼Œå¿…é¡»å­˜åœ¨ä¸€ä¸ªèƒ½å¤Ÿè·¨è¶Šå¹¶å¯¹é½ä½é˜¶æ¦‚å¿µç©ºé—´çš„é€’å½’å¯è¯„ä¼°æ¦‚æ‹¬ç®—å­ï¼Œæ‰èƒ½æœ‰æ•ˆç»´æŒè¯­ä¹‰ç›¸å¹²æ€§ã€‚è®ºæ–‡æ­£å¼å®šä¹‰äº†Functional Model of Intelligence (FMI)ï¼Œå°†å…¶æè¿°ä¸ºèƒ½å¤Ÿæ»¡è¶³RCPçš„æœ€å°å¯ç»„åˆæ¶æ„ï¼Œæ¶µç›–äº†å»ºæ¨¡ã€é€‚é…ã€ç¨³å®šæ€§ä»¥åŠSystem 1å’ŒSystem 2æ¨ç†ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚ç ”ç©¶è¯æ˜ï¼Œç¼ºä¹FMIçš„ç³»ç»Ÿåœ¨è§„æ¨¡æ‰©å±•æ—¶å¿…ç„¶ç»å†ç›¸å¹²æ€§å´©æºƒï¼Œè€Œç›®å‰AIé¢†åŸŸå­˜åœ¨çš„å¹»è§‰ã€ä¸ç¨³å®šæ€§åŠMisalignmentç­‰é—®é¢˜æ­£æ˜¯è¿™ç§ç»“æ„æ€§å¤±æ•ˆçš„ä½“ç°ã€‚è¯¥å·¥ä½œå¼ºè°ƒåº”å°†AIå¯¹é½çš„é‡ç‚¹ä»Behavioral Constraintsè½¬å‘Structural Coherenceï¼Œä¸ºæ„å»ºå®‰å…¨ã€é²æ£’ä¸”å¯æ‰©å±•çš„é€šç”¨äººå·¥æ™ºèƒ½æä¾›äº†ç†è®ºè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15880v1",
      "published_date": "2025-07-18 09:44:01 UTC",
      "updated_date": "2025-07-18 09:44:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:19.501080+00:00"
    },
    {
      "arxiv_id": "2507.13769v1",
      "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction",
      "title_zh": "å­¦ä¹ ç”¨äºé«˜å…‰è°±å›¾åƒé‡å»ºçš„å…‰è°±æ‰©æ•£å…ˆéªŒ",
      "authors": [
        "Mingyang Yu",
        "Zhijian Wu",
        "Dingjiang Huang"
      ],
      "abstract": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its degraded 2D measurements. Recently great progress has been made in deep learning-based methods, however, these methods often struggle to accurately capture high-frequency details of the HSI. To address this issue, this paper proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from hyperspectral images using a diffusion model. Leveraging the powerful ability of the diffusion model to reconstruct details, this learned prior can significantly improve the performance when injected into the HSI model. To further improve the effectiveness of the learned prior, we also propose the Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover the HSI details. We evaluate our method on two representative HSI methods: MST and BISRNet. Experimental results show that our method outperforms existing networks by about 0.5 dB, effectively improving the performance of HSI reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜å…‰è°±å›¾åƒ(Hyperspectral image, HSI)é‡å»ºä¸­æ·±åº¦å­¦ä¹ æ–¹æ³•éš¾ä»¥ç²¾ç¡®æ•æ‰é«˜é¢‘ç»†èŠ‚(high-frequency details)çš„é—®é¢˜ï¼Œæå‡ºäº†å…‰è°±æ‰©æ•£å…ˆéªŒ(Spectral Diffusion Prior, SDP)ã€‚è¯¥å…ˆéªŒåˆ©ç”¨æ‰©æ•£æ¨¡å‹(diffusion model)ä»é«˜å…‰è°±å›¾åƒä¸­éšå¼å­¦ä¹ ï¼Œå¹¶å‡­å€Ÿå…¶å¼ºå¤§çš„ç»†èŠ‚é‡æ„èƒ½åŠ›ï¼Œåœ¨æ³¨å…¥HSIæ¨¡å‹åæ˜¾è‘—æå‡é‡å»ºæ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå…ˆéªŒçš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†å…‰è°±å…ˆéªŒæ³¨å…¥æ¨¡å—(Spectral Prior Injector Module, SPIM)ï¼Œæ—¨åœ¨åŠ¨æ€å¼•å¯¼æ¨¡å‹æ¢å¤HSIçš„ç»†èŠ‚ä¿¡æ¯ã€‚å®éªŒåœ¨MSTå’ŒBISRNetç­‰ä»£è¡¨æ€§æ–¹æ³•ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰ç½‘ç»œæå‡äº†çº¦0.5 dBã€‚è¿™ä¸€ç ”ç©¶æˆæœæœ‰æ•ˆå¢å¼ºäº†é«˜å…‰è°±å›¾åƒé‡å»ºçš„è¡¨ç°ï¼Œä¸ºè§£å†³HSIç»†èŠ‚ä¸¢å¤±é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13769v1",
      "published_date": "2025-07-18 09:27:11 UTC",
      "updated_date": "2025-07-18 09:27:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:35.165816+00:00"
    },
    {
      "arxiv_id": "2507.13768v1",
      "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning",
      "title_zh": "ä»æå–åˆ°ç»¼åˆï¼šé¢å‘æ™ºèƒ½ä½“å¢å¼ºæˆ˜ç•¥æ¨ç†çš„çº ç¼ å¯å‘å¼æ–¹æ³•",
      "authors": [
        "Renato Ghisellini",
        "Remo Pareschi",
        "Marco Pedroni",
        "Giovanni Battista Raggi"
      ],
      "abstract": "We present a hybrid architecture for agent-augmented strategic reasoning, combining heuristic extraction, semantic activation, and compositional synthesis. Drawing on sources ranging from classical military theory to contemporary corporate strategy, our model activates and composes multiple heuristics through a process of semantic interdependence inspired by research in quantum cognition. Unlike traditional decision engines that select the best rule, our system fuses conflicting heuristics into coherent and context-sensitive narratives, guided by semantic interaction modeling and rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study, with preliminary validation through semantic metrics. Limitations and extensions (e.g., dynamic interference tuning) are discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå¢å¼ºæ™ºèƒ½ä½“æˆ˜ç•¥æ¨ç†(agent-augmented strategic reasoning)çš„æ··åˆæ¶æ„ï¼Œå°†å¯å‘å¼æå–(heuristic extraction)ã€è¯­ä¹‰æ¿€æ´»(semantic activation)å’Œç»„åˆåˆæˆ(compositional synthesis)ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹å€Ÿé‰´äº†ä»å¤å…¸å†›äº‹ç†è®ºåˆ°å½“ä»£ä¼ä¸šæˆ˜ç•¥çš„å¹¿æ³›æ¥æºï¼Œå¹¶é€šè¿‡å—é‡å­è®¤çŸ¥(quantum cognition)ç ”ç©¶å¯å‘çš„è¯­ä¹‰äº’ä¾(semantic interdependence)è¿‡ç¨‹æ¥æ¿€æ´»å¹¶ç»„åˆå¤šç§å¯å‘å¼ç®—æ³•ã€‚ä¸ä¼ ç»Ÿé€‰æ‹©æœ€ä¼˜è§„åˆ™çš„å†³ç­–å¼•æ“ä¸åŒï¼Œè¯¥ç³»ç»Ÿé€šè¿‡è¯­ä¹‰äº¤äº’å»ºæ¨¡(semantic interaction modeling)å’Œä¿®è¾æ¡†æ¶(rhetorical framing)å°†ç›¸äº’å†²çªçš„å¯å‘å¼è§„åˆ™èåˆä¸ºè¿è´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§çš„å™äº‹ã€‚ç ”ç©¶é€šè¿‡Metaä¸FTCçš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰æŒ‡æ ‡(semantic metrics)è¿›è¡Œäº†åˆæ­¥éªŒè¯ã€‚è®ºæ–‡æœ€åè¿˜æ¢è®¨äº†è¯¥æ¡†æ¶çš„å±€é™æ€§ä»¥åŠåŠ¨æ€å¹²æ‰°å¾®è°ƒ(dynamic interference tuning)ç­‰æœªæ¥çš„æ‰©å±•æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Peer-reviewed full paper accepted through a double-blind review process at the HAR 2025 conference (https://har-conf.eu/). The official version will appear in a volume of the Lecture Notes in Computer Science (LNCS) series",
      "pdf_url": "https://arxiv.org/pdf/2507.13768v1",
      "published_date": "2025-07-18 09:26:37 UTC",
      "updated_date": "2025-07-18 09:26:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:35.929288+00:00"
    },
    {
      "arxiv_id": "2507.19517v1",
      "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation",
      "title_zh": "BikeVAE-GNNï¼šç”¨äºç¨€ç–è‡ªè¡Œè½¦æµé‡ä¼°è®¡çš„å˜åˆ†è‡ªç¼–ç å™¨å¢å¼ºå‹æ··åˆå›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Mohit Gupta",
        "Debjit Bhowmick",
        "Ben Beck"
      ],
      "abstract": "Accurate link-level bicycle volume estimation is essential for informed urban and transport planning but it is challenged by extremely sparse count data in urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task framework augmenting a Hybrid Graph Neural Network (GNN) with Variational Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model intricate spatial relationships in sparse networks while VAE generates synthetic nodes and edges to enrich the graph structure and enhance the estimation performance. BikeVAE-GNN simultaneously performs - regression for bicycling volume estimation and classification for bicycling traffic level categorization. We demonstrate the effectiveness of BikeVAE-GNN using OpenStreetMap data and publicly available bicycle count data within the City of Melbourne - where only 141 of 15,933 road segments have labeled counts (resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN outperforms machine learning and baseline GNN models, achieving a mean absolute error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99. Ablation studies further validate the effective role of Hybrid-GNN and VAE components. Our research advances bicycling volume estimation in sparse networks using novel and state-of-the-art approaches, providing insights for sustainable bicycling infrastructures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BikeVAE-GNNï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoder)å¢å¼ºçš„æ··åˆå›¾ç¥ç»ç½‘ç»œ(Hybrid Graph Neural Network)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸå¸‚è‡ªè¡Œè½¦ç½‘ç»œä¸­è®¡æ•°æ•°æ®æåº¦ç¨€ç–å¯¼è‡´çš„æµé‡ä¼°ç®—éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå›¾å·ç§¯ç½‘ç»œ(GCN)ã€å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)å’ŒGraphSAGEæ¥å»ºæ¨¡å¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œå¹¶åˆ©ç”¨VAEç”ŸæˆåˆæˆèŠ‚ç‚¹ä¸è¾¹ç¼˜ä»¥å¯Œé›†å›¾ç»“æ„ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨ç¨€ç–ç½‘ç»œä¸­çš„è¡¨ç°ã€‚BikeVAE-GNNé‡‡ç”¨åŒä»»åŠ¡å­¦ä¹ æ¨¡å¼ï¼Œèƒ½å¤ŸåŒæ­¥å®Œæˆè‡ªè¡Œè½¦æµé‡çš„å›å½’é¢„æµ‹ä¸äº¤é€šæ°´å¹³çš„åˆ†ç±»ä»»åŠ¡ã€‚åœ¨é’ˆå¯¹å¢¨å°”æœ¬å¸‚é«˜è¾¾99%æ•°æ®ç¨€ç–åº¦çš„å®è¯ç ”ç©¶ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ åŠåŸºå‡†GNNæ¨¡å‹ï¼Œå®ç°äº†30.82è¾†/æ—¥çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ä»¥åŠ99%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†Hybrid-GNNä¸VAEç»„ä»¶åœ¨æå‡ä¼°ç®—ç²¾åº¦æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºå¯æŒç»­è‡ªè¡Œè½¦åŸºç¡€è®¾æ–½è§„åˆ’æä¾›äº†é‡è¦çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted for publication in the Proceedings of the $28^{th}$ IEEE International Conference on Intelligent Transportation Systems (ITSC 2025). This is the author's version of the work",
      "pdf_url": "https://arxiv.org/pdf/2507.19517v1",
      "published_date": "2025-07-18 09:18:02 UTC",
      "updated_date": "2025-07-18 09:18:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:41.044796+00:00"
    },
    {
      "arxiv_id": "2507.13759v1",
      "title": "OntView: What you See is What you Meant",
      "title_zh": "OntViewï¼šæ‰€è§å³æ‰€æŒ‡",
      "authors": [
        "Carlos Bobed",
        "Carlota Quintana",
        "Eduardo Mena",
        "Jorge Bobed",
        "Fernando Bobillo"
      ],
      "abstract": "In the field of knowledge management and computer science, ontologies provide a structured framework for modeling domain-specific knowledge by defining concepts and their relationships. However, the lack of tools that provide effective visualization is still a significant challenge. While numerous ontology editors and viewers exist, most of them fail to graphically represent ontology structures in a meaningful and non-overwhelming way, limiting users' ability to comprehend dependencies and properties within large ontological frameworks.\n  In this paper, we present OntView, an ontology viewer that is designed to provide users with an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface. Building on the use of a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm, showing the actual inferred knowledge. One key aspect for this is its ability to visualize General Concept Inclusions (GCI), a feature absent in existing visualization tools. Moreover, to avoid a possible information overload, OntView also offers different ways to show a simplified view of the ontology by: 1) creating ontology summaries by assessing the importance of the concepts (according to different available algorithms), 2) focusing the visualization on the existing TBox elements between two given classes and 3) allowing to hide/show different branches in a dynamic way without losing the semantics. OntView has been released with an open-source license for the whole community.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† OntViewï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨é€šè¿‡ç›´è§‚ç•Œé¢æä¾›æœ¬ä½“ (Ontology) æ¦‚å¿µåŠå…¶å½¢å¼åŒ–å®šä¹‰çš„è§†è§‰å‘ˆç°å·¥å…·ã€‚é’ˆå¯¹ç°æœ‰ç¼–è¾‘å™¨åœ¨å±•ç¤ºå¤§å‹æœ¬ä½“æ¡†æ¶æ—¶å®¹æ˜“å¯¼è‡´ä¿¡æ¯è¿‡è½½ä¸”éš¾ä»¥ä½“ç°å¤æ‚ä¾èµ–å…³ç³»çš„æŒ‘æˆ˜ï¼ŒOntView éµå¾ªâ€œæ‰€è§å³æ‰€æŒ‡â€ (What you see is what you meant) èŒƒå¼ï¼Œåˆ©ç”¨ DL reasoner å±•ç¤ºç»è¿‡æ¨ç†çš„å®é™…çŸ¥è¯†ã€‚è¯¥å·¥å…·å¡«è¡¥äº†ç°æœ‰å¯è§†åŒ–å·¥å…·åœ¨é€šç”¨æ¦‚å¿µåŒ…å« (General Concept Inclusions, GCI) å±•ç¤ºæ–¹é¢çš„ç©ºç™½ï¼Œæ”¯æŒç”¨æˆ·æ›´æ·±å…¥åœ°ç†è§£æœ¬ä½“ç»“æ„ã€‚ä¸ºäº†ä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼ŒOntView æä¾›äº†å¤šç§ç®€åŒ–è§†å›¾çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºç‰¹å®šç®—æ³•ç”Ÿæˆæœ¬ä½“æ‘˜è¦ (Ontology summaries)ã€èšç„¦äºä¸¤ç±»ä¹‹é—´çš„ TBox å…ƒç´ ï¼Œä»¥åŠåœ¨åŠ¨æ€éšè—æˆ–æ˜¾ç¤ºåˆ†æ”¯æ—¶ä¿æŒè¯­ä¹‰å®Œæ•´ã€‚ç›®å‰ï¼ŒOntView å·²ä½œä¸ºå¼€æºé¡¹ç›®å‘å¸ƒï¼Œä¸ºçŸ¥è¯†ç®¡ç†å’Œè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†é«˜æ•ˆä¸”æœ‰æ„ä¹‰çš„æœ¬ä½“å¯è§†åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13759v1",
      "published_date": "2025-07-18 09:06:49 UTC",
      "updated_date": "2025-07-18 09:06:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:45.051335+00:00"
    },
    {
      "arxiv_id": "2507.13742v2",
      "title": "Search-Optimized Quantization in Biomedical Ontology Alignment",
      "title_zh": "ç”Ÿç‰©åŒ»å­¦æœ¬ä½“å¯¹é½ä¸­çš„æœç´¢ä¼˜åŒ–é‡åŒ–",
      "authors": [
        "Oussama Bouaggad",
        "Natalia Grabar"
      ],
      "abstract": "In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦æœ¬ä½“å¯¹é½(biomedical ontology alignment)ä¸­å¤§å‹æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç³»ç»Ÿæ€§çš„æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶åˆ©ç”¨åŸºäºTransformerçš„ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œå»ºç«‹äº†åŒ»å­¦å¤–è¡Œè¯æ±‡ä¸ç»Ÿä¸€åŒ»å­¦è¯­è¨€ç³»ç»Ÿ(UMLS) Metathesaurusä¹‹é—´çš„ä½™å¼¦è¯­ä¹‰ç›¸ä¼¼åº¦æ˜ å°„ã€‚åœ¨ä¼˜åŒ–æµç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨Microsoft Oliveåœ¨ONNX Runtimeåç«¯æœç´¢æœ€ä½³æ‰§è¡Œæä¾›ç¨‹åº(EPs)ï¼Œå¹¶ç»“åˆIntel Neural Compressorå’ŒIPEXæ‰§è¡ŒåŠ¨æ€é‡åŒ–(dynamic quantization)å¤„ç†ã€‚å®éªŒåœ¨DEFT 2020è¯„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„SOTAæ€§èƒ½ï¼Œåœ¨ä¿æŒåŸæœ‰æ€§èƒ½æŒ‡æ ‡çš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡20å€çš„æ¨ç†åŠ é€Ÿï¼Œå¹¶å°†å†…å­˜å ç”¨é™ä½äº†çº¦70%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in Frontiers in Artificial Intelligence - Medicine and Public Health (Original Research)",
      "pdf_url": "https://arxiv.org/pdf/2507.13742v2",
      "published_date": "2025-07-18 08:42:20 UTC",
      "updated_date": "2025-09-20 13:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:44.884545+00:00"
    },
    {
      "arxiv_id": "2507.13741v1",
      "title": "SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification",
      "title_zh": "SamGoGï¼šé¢å‘ä¸å¹³è¡¡å›¾åˆ†ç±»çš„åŸºäºé‡‡æ ·çš„å›¾ä¸­å›¾æ¡†æ¶",
      "authors": [
        "Shangyou Wang",
        "Zezhong Ding",
        "Xike Xie"
      ],
      "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in graph classification tasks by capturing both structural and feature-based representations. However, real-world graphs often exhibit two critical forms of imbalance: class imbalance and graph size imbalance. These imbalances can bias the learning process and degrade model performance. Existing methods typically address only one type of imbalance or incur high computational costs. In this work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning framework that effectively mitigates both class and graph size imbalance. SamGoG constructs multiple GoGs through an efficient importance-based sampling mechanism and trains on them sequentially. This sampling mechanism incorporates the learnable pairwise similarity and adaptive GoG node degree to enhance edge homophily, thus improving downstream model quality. SamGoG can seamlessly integrate with various downstream GNNs, enabling their efficient adaptation for graph classification tasks. Extensive experiments on benchmark datasets demonstrate that SamGoG achieves state-of-the-art performance with up to a 15.66% accuracy improvement with 6.7$\\times$ training acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Graph Neural Networks (GNNs) åœ¨å›¾åˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´çš„ç±»åˆ«ä¸å¹³è¡¡ (class imbalance) å’Œå›¾è§„æ¨¡ä¸å¹³è¡¡ (graph size imbalance) æŒ‘æˆ˜ï¼Œè¿™äº›é—®é¢˜å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ åå¥½å¹¶é™ä½æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SamGoGï¼Œä¸€ç§åŸºäºé‡‡æ ·æœºåˆ¶çš„ Graph-of-Graphs (GoG) å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æœ‰æ•ˆç¼“è§£ä¸Šè¿°ä¸¤ç±»ä¸å¹³è¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„åŸºäºé‡è¦æ€§çš„é‡‡æ ·æœºåˆ¶æ„å»ºå¤šä¸ª GoGs å¹¶è¿›è¡Œé¡ºåºè®­ç»ƒï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„æˆå¯¹ç›¸ä¼¼åº¦ (pairwise similarity) å’Œè‡ªé€‚åº” GoG èŠ‚ç‚¹åº¦æ¥å¢å¼ºè¾¹åŒè´¨æ€§ (edge homophily)ã€‚SamGoG èƒ½å¤Ÿæ— ç¼é›†æˆå„ç§ä¸‹æ¸¸ GNNs æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å…¶åœ¨å›¾åˆ†ç±»ä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamGoG åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œåœ¨å‡†ç¡®ç‡æœ€é«˜æå‡ 15.66% çš„åŒæ—¶ï¼Œå®ç°äº† 6.7 å€çš„è®­ç»ƒåŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13741v1",
      "published_date": "2025-07-18 08:41:58 UTC",
      "updated_date": "2025-07-18 08:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:51.291124+00:00"
    },
    {
      "arxiv_id": "2507.13739v1",
      "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning",
      "title_zh": "åˆæˆå›¾åƒèƒ½å¦å…‹æœé—å¿˜ï¼Ÿçªç ´å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸­æœªè¢«æ¢ç©¶çš„ç–‘è™‘",
      "authors": [
        "Junsu Kim",
        "Yunhoe Ku",
        "Seungryul Baek"
      ],
      "abstract": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ (Few-shot class-incremental learning, FSCIL)ä¸­è®­ç»ƒæ•°æ®æåº¦åŒ®ä¹ä»¥åŠç¾éš¾æ€§é—å¿˜(catastrophic forgetting)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDiffusion-FSCILçš„æ–°æ–¹æ³•ã€‚Diffusion-FSCILé‡‡ç”¨å†»ç»“çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹(text-to-image diffusion model)ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€å¤šå°ºåº¦è¡¨å¾ä»¥åŠæ–‡æœ¬ç¼–ç å™¨çš„çµæ´»æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–è¡¨å¾èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶æå–å¤šä¸ªäº’è¡¥çš„æ‰©æ•£ç‰¹å¾ä½œä¸ºæ½œåœ¨é‡æ”¾(latent replay)ï¼Œå¹¶ç»“åˆç‰¹å¾è’¸é¦(feature distillation)æŠ€æœ¯ä»¥é˜²æ­¢ç”Ÿæˆåå·®ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨å†»ç»“éª¨å¹²ç½‘ç»œã€æœ€å°‘é‡çš„å¯è®­ç»ƒç»„ä»¶ä»¥åŠç‰¹å¾æå–çš„æ‰¹å¤„ç†å®ç°äº†æé«˜çš„è¿è¡Œæ•ˆç‡ã€‚åœ¨CUB-200ã€miniImageNetå’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDiffusion-FSCILçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›(state-of-the-art)æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿ç•™å·²å­¦ä¹ ç±»åˆ«çš„æ€§èƒ½å¹¶å¿«é€Ÿé€‚åº”æ–°ç±»åˆ«ï¼Œè¯æ˜äº†åˆ©ç”¨ç”Ÿæˆæ¨¡å‹èƒ½åŠ›è§£å†³å¢é‡å­¦ä¹ é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6th CLVISION ICCV Workshop accepted",
      "pdf_url": "https://arxiv.org/pdf/2507.13739v1",
      "published_date": "2025-07-18 08:38:07 UTC",
      "updated_date": "2025-07-18 08:38:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:54.301948+00:00"
    },
    {
      "arxiv_id": "2507.13737v1",
      "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs",
      "title_zh": "DailyLLMï¼šåŸºäºå¤šæ¨¡æ€ä¼ æ„Ÿå™¨ä¸å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ´»åŠ¨æ—¥å¿—ç”Ÿæˆ",
      "authors": [
        "Ye Tian",
        "Xiaoyuan Ren",
        "Zihao Wang",
        "Onat Gungor",
        "Xiaofan Yu",
        "Tajana Rosing"
      ],
      "abstract": "Rich and context-aware activity logs facilitate user behavior analysis and health monitoring, making them a key research focus in ubiquitous computing. The remarkable semantic understanding and generation capabilities of Large Language Models (LLMs) have recently created new opportunities for activity log generation. However, existing methods continue to exhibit notable limitations in terms of accuracy, efficiency, and semantic richness. To address these challenges, we propose DailyLLM. To the best of our knowledge, this is the first log generation and summarization system that comprehensively integrates contextual activity information across four dimensions: location, motion, environment, and physiology, using only sensors commonly available on smartphones and smartwatches. To achieve this, DailyLLM introduces a lightweight LLM-based framework that integrates structured prompting with efficient feature extraction to enable high-level activity understanding. Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art (SOTA) log generation methods and can be efficiently deployed on personal computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference speed.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DailyLLMï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€ä¼ æ„Ÿå™¨å’Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ´»åŠ¨æ—¥å¿—çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œè¯­ä¹‰ä¸°å¯Œæ€§æ–¹é¢çš„å±€é™ã€‚ä½œä¸ºé¦–ä¸ªç»¼åˆé›†æˆä½ç½® (location)ã€è¿åŠ¨ (motion)ã€ç¯å¢ƒ (environment) å’Œç”Ÿç† (physiology) å››ä¸ªç»´åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ—¥å¿—ç”Ÿæˆç³»ç»Ÿï¼Œå®ƒä»…ä¾èµ–æ™ºèƒ½æ‰‹æœºå’Œæ™ºèƒ½æ‰‹è¡¨çš„å¸¸ç”¨ä¼ æ„Ÿå™¨ã€‚DailyLLM å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æç¤º (structured prompting) ä¸é«˜æ•ˆç‰¹å¾æå–å®ç°æ·±å±‚çš„æ´»åŠ¨ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼ŒDailyLLM åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿› (SOTA) æ–¹æ³•ï¼Œä»…ä½¿ç”¨ 1.5B å‚æ•°çš„æ¨¡å‹å°±åœ¨ BERTScore ç²¾åº¦ä¸Šæ¯” 70B å‚æ•°çš„åŸºçº¿æ¨¡å‹æé«˜äº† 17%ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿçš„æ¨ç†é€Ÿåº¦æå‡äº†è¿‘ 10 å€ï¼Œèƒ½å¤ŸæˆåŠŸéƒ¨ç½²åœ¨ä¸ªäººç”µè„‘å’Œ Raspberry Pi ç­‰ä½åŠŸè€—å¹³å°ä¸Šï¼Œä¸ºé«˜æ•ˆçš„å¥åº·ç›‘æµ‹å’Œè¡Œä¸ºåˆ†ææä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13737v1",
      "published_date": "2025-07-18 08:33:30 UTC",
      "updated_date": "2025-07-18 08:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:34:54.133512+00:00"
    },
    {
      "arxiv_id": "2507.13729v1",
      "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework",
      "title_zh": "AGENTS-LLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶çš„æŒ‘æˆ˜æ€§äº¤é€šåœºæ™¯å¢å¼ºå¼ç”Ÿæˆ",
      "authors": [
        "Yu Yao",
        "Salil Bhatnagar",
        "Markus Mazzola",
        "Vasileios Belagiannis",
        "Igor Gilitschenski",
        "Luigi Palmieri",
        "Simon Razniewski",
        "Marcel Hallgarten"
      ],
      "abstract": "Rare, yet critical, scenarios pose a significant challenge in testing and evaluating autonomous driving planners. Relying solely on real-world driving scenes requires collecting massive datasets to capture these scenarios. While automatic generation of traffic scenarios appears promising, data-driven models require extensive training data and often lack fine-grained control over the output. Moreover, generating novel scenarios from scratch can introduce a distributional shift from the original training scenes which undermines the validity of evaluations especially for learning-based planners. To sidestep this, recent work proposes to generate challenging scenarios by augmenting original scenarios from the test set. However, this involves the manual augmentation of scenarios by domain experts. An approach that is unable to meet the demands for scale in the evaluation of self-driving systems. Therefore, this paper introduces a novel LLM-agent based framework for augmenting real-world traffic scenarios using natural language descriptions, addressing the limitations of existing methods. A key innovation is the use of an agentic design, enabling fine-grained control over the output and maintaining high performance even with smaller, cost-effective LLMs. Extensive human expert evaluation demonstrates our framework's ability to accurately adhere to user intent, generating high quality augmented scenarios comparable to those created manually.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AGENTS-LLMï¼Œä¸€ç§åŸºäºæ™ºèƒ½ä½“(Agentic)è®¾è®¡çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥å¢å¼ºçœŸå®çš„äº¤é€šåœºæ™¯ï¼Œä»è€Œè§£å†³è‡ªåŠ¨é©¾é©¶è§„åˆ’å™¨åœ¨æµ‹è¯•ç¨€æœ‰ä¸”å…³é”®åœºæ™¯(rare, yet critical scenarios)æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶æ™ºèƒ½ä½“è®¾è®¡ï¼Œå®ç°äº†å¯¹ç”Ÿæˆè¾“å‡ºçš„ç»†ç²’åº¦æ§åˆ¶(fine-grained control)ï¼Œåœ¨è§„é¿ä»é›¶ç”Ÿæˆåœºæ™¯å¯¼è‡´çš„åˆ†å¸ƒåç§»(distributional shift)é—®é¢˜çš„åŒæ—¶ï¼Œç¡®ä¿äº†åœ¨ä½¿ç”¨å°å‹ã€é«˜æ€§ä»·æ¯”LLMsæ—¶ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚äººç±»ä¸“å®¶è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAGENTS-LLMèƒ½å¤Ÿç²¾å‡†éµå¾ªç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆä¸ä¸“å®¶æ‰‹åŠ¨å¢å¼ºè´¨é‡ç›¸å½“çš„é«˜æŒ‘æˆ˜æ€§åœºæ™¯ã€‚è¯¥å·¥ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è¯„ä¼°æ–¹æ¡ˆï¼Œæœ‰æ•ˆè§£å†³äº†æ‰‹åŠ¨å¢å¼ºæ–¹æ³•éš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡æµ‹è¯•éœ€æ±‚çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13729v1",
      "published_date": "2025-07-18 08:20:16 UTC",
      "updated_date": "2025-07-18 08:20:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:10.209860+00:00"
    },
    {
      "arxiv_id": "2507.13725v1",
      "title": "Point of Interest Recommendation: Pitfalls and Viable Solutions",
      "title_zh": "å…´è¶£ç‚¹æ¨èï¼šè¯¯åŒºä¸å¯è¡Œæ–¹æ¡ˆ",
      "authors": [
        "Alejandro BellogÃ­n",
        "Linus W. Dietz",
        "Francesco Ricci",
        "Pablo SÃ¡nchez"
      ],
      "abstract": "Point of interest (POI) recommendation can play a pivotal role in enriching tourists' experiences by suggesting context-dependent and preference-matching locations and activities, such as restaurants, landmarks, itineraries, and cultural attractions. Unlike some more common recommendation domains (e.g., music and video), POI recommendation is inherently high-stakes: users invest significant time, money, and effort to search, choose, and consume these suggested POIs. Despite the numerous research works in the area, several fundamental issues remain unresolved, hindering the real-world applicability of the proposed approaches. In this paper, we discuss the current status of the POI recommendation problem and the main challenges we have identified. The first contribution of this paper is a critical assessment of the current state of POI recommendation research and the identification of key shortcomings across three main dimensions: datasets, algorithms, and evaluation methodologies. We highlight persistent issues such as the lack of standardized benchmark datasets, flawed assumptions in the problem definition and model design, and inadequate treatment of biases in the user behavior and system performance. The second contribution is a structured research agenda that, starting from the identified issues, introduces important directions for future work related to multistakeholder design, context awareness, data collection, trustworthiness, novel interactions, and real-world evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…´è¶£ç‚¹(Point of Interest, POI)æ¨èçš„ç°çŠ¶åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚è®ºæ–‡é¦–å…ˆå¯¹å½“å‰çš„POIæ¨èç ”ç©¶è¿›è¡Œäº†æ‰¹åˆ¤æ€§è¯„ä¼°ï¼ŒæŒ‡å‡ºäº†æ•°æ®é›†(datasets)ã€ç®—æ³•(algorithms)å’Œè¯„ä¼°æ–¹æ³•(evaluation methodologies)ä¸‰ä¸ªç»´åº¦çš„å…³é”®ç¼ºé™·ã€‚è¿™äº›ç¼ºé™·åŒ…æ‹¬ç¼ºä¹æ ‡å‡†åŒ–çš„åŸºå‡†æ•°æ®é›†ã€é—®é¢˜å®šä¹‰å’Œæ¨¡å‹è®¾è®¡ä¸­çš„é”™è¯¯å‡è®¾ï¼Œä»¥åŠå¯¹ç”¨æˆ·è¡Œä¸ºå’Œç³»ç»Ÿæ€§èƒ½åå·®çš„å¤„ç†ä¸è¶³ã€‚éšåï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„ç ”ç©¶è®®ç¨‹ï¼Œå¼•å…¥äº†å¤šåˆ©ç›Šç›¸å…³è€…è®¾è®¡(multistakeholder design)ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥(context awareness)ã€æ•°æ®æ”¶é›†ã€å¯ä¿¡åº¦(trustworthiness)ã€æ–°å‹äº¤äº’å’ŒçœŸå®ä¸–ç•Œè¯„ä¼°ç­‰é‡è¦æœªæ¥æ–¹å‘ã€‚è¯¥ç»¼è¿°æ—¨åœ¨è§£å†³POIæ¨èé¢†åŸŸé•¿æœŸå­˜åœ¨çš„æ ¹æœ¬æ€§é—®é¢˜ï¼Œä¸ºæå‡å…¶ç°å®åº”ç”¨æ€§æä¾›äº†ç³»ç»Ÿæ€§çš„æŒ‡å¯¼æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13725v1",
      "published_date": "2025-07-18 08:10:09 UTC",
      "updated_date": "2025-07-18 08:10:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:05.793441+00:00"
    },
    {
      "arxiv_id": "2507.13703v2",
      "title": "Binarizing Physics-Inspired GNNs for Combinatorial Optimization",
      "title_zh": "é¢å‘ç»„åˆä¼˜åŒ–çš„å—ç‰©ç†å¯å‘å›¾ç¥ç»ç½‘ç»œäºŒå€¼åŒ–",
      "authors": [
        "Martin KrutskÃ½",
        "Gustav Å Ã­r",
        "Vyacheslav Kungurtsev",
        "Georgios Korpas"
      ],
      "abstract": "Physics-inspired graph neural networks (PI-GNNs) have been utilized as an efficient unsupervised framework for relaxing combinatorial optimization problems encoded through a specific graph structure and loss, reflecting dependencies between the problem's variables. While the framework has yielded promising results in various combinatorial problems, we show that the performance of PI-GNNs systematically plummets with an increasing density of the combinatorial problem graphs. Our analysis reveals an interesting phase transition in the PI-GNNs' training dynamics, associated with degenerate solutions for the denser problems, highlighting a discrepancy between the relaxed, real-valued model outputs and the binary-valued problem solutions. To address the discrepancy, we propose principled alternatives to the naive strategy used in PI-GNNs by building on insights from fuzzy logic and binarized neural networks. Our experiments demonstrate that the portfolio of proposed methods significantly improves the performance of PI-GNNs in increasingly dense settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç‰©ç†å¯å‘å›¾ç¥ç»ç½‘ç»œ (Physics-inspired graph neural networks, PI-GNNs) åœ¨å¤„ç†ç»„åˆä¼˜åŒ– (Combinatorial Optimization) é—®é¢˜æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºéšç€é—®é¢˜å›¾å¯†åº¦çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½ä¼šç³»ç»Ÿæ€§åœ°ä¸‹é™ã€‚é€šè¿‡åˆ†æå‘ç°ï¼ŒPI-GNNs çš„è®­ç»ƒåŠ¨åŠ›å­¦ä¸­å­˜åœ¨ä¸é€€åŒ–è§£ (degenerate solutions) ç›¸å…³çš„ç›¸ä½è½¬æ¢ (phase transition)ï¼Œæ­ç¤ºäº†è¿ç»­å®å€¼è¾“å‡ºä¸ç¦»æ•£äºŒå€¼è§£ (binary-valued solutions) ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³æ­¤å·®å¼‚ï¼Œç ”ç©¶è€…å€Ÿé‰´æ¨¡ç³Šé€»è¾‘ (fuzzy logic) å’ŒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œ (binarized neural networks) çš„ç†è®ºï¼Œæå‡ºäº†å¤šç§æ”¹è¿› PI-GNNs è®­ç»ƒç­–ç•¥çš„åŸåˆ™æ€§æ›¿ä»£æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç»„åˆæ˜¾è‘—æå‡äº† PI-GNNs åœ¨ç¨ å¯†å›¾åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¢å¼ºäº†è¯¥æ¡†æ¶åœ¨å¤æ‚ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the 28th European Conference on Artificial Intelligence (ECAI 2025). This archival version includes supplementary appendices",
      "pdf_url": "https://arxiv.org/pdf/2507.13703v2",
      "published_date": "2025-07-18 07:11:50 UTC",
      "updated_date": "2025-08-01 14:44:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:25.959257+00:00"
    },
    {
      "arxiv_id": "2507.13681v2",
      "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues",
      "title_zh": "LoopServeï¼šé¢å‘å¤šè½®å¯¹è¯çš„è‡ªé€‚åº”åŒé˜¶æ®µå¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿç³»ç»Ÿ",
      "authors": [
        "Haoyang Li",
        "Zhanchao Xu",
        "Yiming Li",
        "Xuejia Chen",
        "Darian Li",
        "Anxin Tian",
        "Qingfa Xiao",
        "Cheng Deng",
        "Jun Wang",
        "Qing Li",
        "Lei Chen",
        "Mingxuan Yuan"
      ],
      "abstract": "Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. As a result, these models cannot accurately identify and prioritize the most relevant context, leading to degraded response quality. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a new benchmark with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LoopServeï¼Œä¸€ç§ä¸“ä¸ºå¤šè½®å¯¹è¯è®¾è®¡çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªé€‚åº”åŒé˜¶æ®µæ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿å¯¹è¯èƒŒæ™¯ä¸‹æ—¥ç›Šå¢é•¿çš„è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰åŠ é€Ÿæ–¹æ³•ä¾èµ–å›ºå®šå¯å‘å¼ç­–ç•¥ä¸”éš¾ä»¥é€‚åº”åŠ¨æ€å¯¹è¯æ¨¡å¼çš„ä¸è¶³ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°ã€‚é¦–å…ˆï¼Œå®ƒåœ¨ Prefilling é˜¶æ®µé€šè¿‡åŠ¨æ€é€‰æ‹© Attention Matrix çš„é‡è¦éƒ¨åˆ†å®ç° Online Sparsificationã€‚å…¶æ¬¡ï¼Œå®ƒåœ¨ Decoding é˜¶æ®µæ ¹æ®æœ€æ–°ç”Ÿæˆçš„ Token è‡ªé€‚åº”ç»´æŠ¤é«˜æ•ˆç¼“å­˜ï¼Œå®ç°æ¸è¿›å¼ Key-Value (KV) å‹ç¼©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†åŒ…å« 11 ä¸ªå¤šè½®å¯¹è¯æ•°æ®é›†çš„æ–°åŸºå‡†ï¼Œç”¨ä»¥è¯„ä¼°çœŸå®çš„å¯¹è¯ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoopServe åœ¨å„ç±»é•¿æ–‡æœ¬å¯¹è¯ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œåœ¨å¤§å¹…æå‡æ¨ç†é€Ÿåº¦çš„åŒæ—¶ç¡®ä¿äº†å“åº”è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13681v2",
      "published_date": "2025-07-18 06:12:08 UTC",
      "updated_date": "2025-09-26 07:14:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:33.723567+00:00"
    },
    {
      "arxiv_id": "2507.14249v1",
      "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach",
      "title_zh": "é¢å‘åŸå¸‚ç©ºä¸­äº¤é€šçš„å®æ—¶é€šä¿¡æ„ŸçŸ¥æ‹¼è½¦è·¯å¾„è§„åˆ’ï¼šä¸€ç§å¤šæºæ··åˆæ³¨æ„åŠ›å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Yuejiao Xie",
        "Maonan Wang",
        "Di Zhou",
        "Man-On Pun",
        "Zhu Han"
      ],
      "abstract": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions to alleviate urban congestion, with path planning becoming a key focus area. Unlike ground transportation, UAM trajectory planning has to prioritize communication quality for accurate location tracking in constantly changing environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi, requires adaptive planning to respond to real-time passenger requests, especially in ride-sharing scenarios where passenger demands are unpredictable and dynamic. However, conventional trajectory planning strategies based on predefined routes lack the flexibility to meet varied passenger ride demands. To address these challenges, this work first proposes constructing a radio map to evaluate the communication quality of urban airspace. Building on this, we introduce a novel Multi-Source Hybrid Attention Reinforcement Learning (MSHA-RL) framework for the challenge of effectively focusing on passengers and UAM locations, which arises from the significant dimensional disparity between the representations. This model first generates the alignment among diverse data sources with large gap dimensions before employing hybrid attention to balance global and local insights, thereby facilitating responsive, real-time path planning. Extensive experimental results demonstrate that the approach enables communication-compliant trajectory planning, reducing travel time and enhancing operational efficiency while prioritizing passenger safety.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚ç©ºä¸­äº¤é€š(Urban Air Mobility, UAM)åœ¨å®æ—¶æ‹¼è½¦åœºæ™¯ä¸‹çš„è·¯å¾„è§„åˆ’é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¼é¡¾é€šä¿¡è´¨é‡ä¸åŠ¨æ€ä¹˜å®¢éœ€æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºè§£å†³UAMè½¨è¿¹è§„åˆ’ä¸­å¯¹é«˜ç²¾åº¦å®šä½è¿½è¸ªåŠå®‰å…¨æ€§çš„ä¸¥è‹›è¦æ±‚ï¼Œè¯¥é¡¹å·¥ä½œé¦–å…ˆé€šè¿‡æ„å»ºæ— çº¿ç”µåœ°å›¾(radio map)æ¥è¯„ä¼°åŸå¸‚é¢†ç©ºçš„é€šä¿¡è´¨é‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å¤šæºæ··åˆæ³¨æ„åŠ›å¼ºåŒ–å­¦ä¹ (Multi-Source Hybrid Attention Reinforcement Learning, MSHA-RL)æ¡†æ¶ï¼Œä»¥åº”å¯¹ä¹˜å®¢ä¸UAMä½ç½®ç‰¹å¾ä¹‹é—´å·¨å¤§çš„ç»´åº¦å·®å¼‚ã€‚è¯¥æ¨¡å‹é€šè¿‡å®ç°ä¸åŒç»´åº¦æ•°æ®æºä¹‹é—´çš„ç‰¹å¾å¯¹é½ï¼Œå¹¶åˆ©ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å¹³è¡¡å…¨å±€ä¸å±€éƒ¨æ´å¯Ÿï¼Œä»è€Œæ”¯æŒå¿«é€Ÿå“åº”çš„å®æ—¶è·¯å¾„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç¬¦åˆé€šä¿¡å®‰å…¨æ ‡å‡†çš„è½¨è¿¹ï¼Œåœ¨ä¿éšœä¹˜å®¢å®‰å…¨çš„å‰æä¸‹ï¼Œæœ‰æ•ˆç¼©çŸ­äº†è¡Œç¨‹æ—¶é—´å¹¶æ˜¾è‘—æå‡äº†ç³»ç»Ÿè¿è¥æ•ˆç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14249v1",
      "published_date": "2025-07-18 06:09:30 UTC",
      "updated_date": "2025-07-18 06:09:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:28.548598+00:00"
    },
    {
      "arxiv_id": "2507.13677v1",
      "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors",
      "title_zh": "HeCoFuseï¼šåŸºäºå¼‚æ„ä¼ æ„Ÿå™¨çš„è·¨æ¨¡æ€äº’è¡¥ V2X ååŒæ„ŸçŸ¥",
      "authors": [
        "Chuheng Wei",
        "Ziye Qin",
        "Walter Zimmer",
        "Guoyuan Wu",
        "Matthew J. Barth"
      ],
      "abstract": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often operate under heterogeneous sensor configurations due to cost constraints and deployment variability across vehicles and infrastructure. This heterogeneity poses significant challenges for feature fusion and perception reliability. To address these issues, we propose HeCoFuse, a unified framework designed for cooperative perception across mixed sensor setups where nodes may carry Cameras (C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that adaptively weights features through a combination of channel-wise and spatial attention, HeCoFuse can tackle critical challenges such as cross-modality feature misalignment and imbalanced representation quality. In addition, an adaptive spatial resolution adjustment module is employed to balance computational cost and fusion effectiveness. To enhance robustness across different configurations, we further implement a cooperative learning strategy that dynamically adjusts fusion type based on available modalities. Experiments on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22% 3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine heterogeneous sensor configurations. These results, validated by our first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust performance across diverse sensor deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HeCoFuseï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è½¦è”ç½‘(V2X)ååŒæ„ŸçŸ¥ä¸­å¼‚æ„ä¼ æ„Ÿå™¨(Heterogeneous Sensors)é…ç½®éš¾é¢˜çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ºäº†å¤„ç†Camera (C)å’ŒLiDAR (L)è·¨æ¨¡æ€ç‰¹å¾å¤±é…åŠè¡¨ç¤ºè´¨é‡ä¸å¹³è¡¡é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç»“åˆé€šé“çº§ä¸ç©ºé—´æ³¨æ„åŠ›(Channel-wise and Spatial Attention)çš„å±‚æ¬¡åŒ–èåˆæœºåˆ¶ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”ç©ºé—´åˆ†è¾¨ç‡è°ƒæ•´æ¨¡å—ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚åŒæ—¶ï¼ŒHeCoFuseé‡‡ç”¨ååŒå­¦ä¹ ç­–ç•¥æ¥æ ¹æ®å¯ç”¨æ¨¡æ€åŠ¨æ€è°ƒæ•´èåˆç±»å‹ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿåœ¨å¤šæ ·åŒ–éƒ¨ç½²ä¸‹çš„é²æ£’æ€§ã€‚åœ¨çœŸå®ä¸–ç•ŒTUMTraf-V2Xæ•°æ®é›†çš„å®éªŒä¸­ï¼ŒHeCoFuseåœ¨å…¨ä¼ æ„Ÿå™¨é…ç½®ä¸‹å®ç°äº†43.22%çš„3D mAPï¼Œæ€§èƒ½è¶…è¶Šäº†CoopDet3DåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶å‡­å€Ÿå…¶åœ¨ä¹ç§å¼‚æ„é…ç½®ä¸‹çš„ç¨³å¥è¡¨ç°ï¼Œåœ¨CVPR 2025 DriveXæŒ‘æˆ˜èµ›ä¸­è£è·å† å†›ï¼Œç¡®ç«‹äº†ç›®å‰åœ¨è¯¥æ•°æ®é›†ä¸Šçš„SOTAæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted by ITSC2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13677v1",
      "published_date": "2025-07-18 06:02:22 UTC",
      "updated_date": "2025-07-18 06:02:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:34.591180+00:00"
    },
    {
      "arxiv_id": "2507.14248v1",
      "title": "Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack",
      "title_zh": "ç ´é™¤åŸºäºè§£é‡Šçš„å®‰å…¨å¹»è§‰ï¼šé­å—æ”»å‡»çš„å¯è§£é‡Šè§†è§‰ Transformer ç³»ç»Ÿ",
      "authors": [
        "Eldor Abdukhamidov",
        "Mohammed Abuhamad",
        "Simon S. Woo",
        "Hyoungshick Kim",
        "Tamer Abuhmed"
      ],
      "abstract": "Vision transformer (ViT) models, when coupled with interpretation models, are regarded as secure and challenging to deceive, making them well-suited for security-critical domains such as medical applications, autonomous vehicles, drones, and robotics. However, successful attacks on these systems can lead to severe consequences. Recent research on threats targeting ViT models primarily focuses on generating the smallest adversarial perturbations that can deceive the models with high confidence, without considering their impact on model interpretations. Nevertheless, the use of interpretation models can effectively assist in detecting adversarial examples. This study investigates the vulnerability of transformer models to adversarial attacks, even when combined with interpretation models. We propose an attack called \"AdViT\" that generates adversarial examples capable of misleading both a given transformer model and its coupled interpretation model. Through extensive experiments on various transformer models and two transformer-based interpreters, we demonstrate that AdViT achieves a 100% attack success rate in both white-box and black-box scenarios. In white-box scenarios, it reaches up to 98% misclassification confidence, while in black-box scenarios, it reaches up to 76% misclassification confidence. Remarkably, AdViT consistently generates accurate interpretations in both scenarios, making the adversarial examples more difficult to detect.",
      "tldr_zh": "Vision Transformer (ViT) æ¨¡å‹ç»“åˆè§£é‡Šæ¨¡å‹(interpretation models)é€šå¸¸è¢«è®¤ä¸ºåœ¨åŒ»ç–—å’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸå…·æœ‰è¾ƒé«˜çš„å®‰å…¨æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©æ£€æµ‹å¯¹æŠ—æ ·æœ¬(adversarial examples)ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é’ˆå¯¹ ViT çš„æ”»å‡»ç ”ç©¶ä¸»è¦å…³æ³¨ç”Ÿæˆæå°æ‰°åŠ¨ä»¥æ¬ºéª—æ¨¡å‹ï¼Œå¾€å¾€å¿½ç•¥äº†å…¶å¯¹æ¨¡å‹è§£é‡Šç»“æœçš„å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºäº†åä¸º AdViT çš„æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶è¯¯å¯¼ Transformer æ¨¡å‹åŠå…¶è€¦åˆçš„è§£é‡Šæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdViT åœ¨ç™½ç›’å’Œé»‘ç›’åœºæ™¯ä¸‹å‡å®ç°äº† 100% çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¸”å…¶è¯¯åˆ†ç±»ç½®ä¿¡åº¦åœ¨ç™½ç›’å’Œé»‘ç›’åœºæ™¯ä¸‹åˆ†åˆ«é«˜è¾¾ 98% å’Œ 76%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAdViT åœ¨å®æ–½æ”»å‡»æ—¶ä»èƒ½ç”Ÿæˆçœ‹ä¼¼å‡†ç¡®çš„è§£é‡Šç»“æœï¼Œè¿™ä½¿å¾—å¯¹æŠ—æ ·æœ¬æéš¾è¢«æ£€æµ‹ï¼Œä»è€Œæ­ç¤ºäº†ç°æœ‰å¯è§£é‡Š Vision Transformer ç³»ç»Ÿåœ¨å®‰å…¨é˜²å¾¡æ–¹é¢çš„è„†å¼±æ€§å¹¶æ‰“ç ´äº†å…¶å®‰å…¨å¹»è±¡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14248v1",
      "published_date": "2025-07-18 05:11:11 UTC",
      "updated_date": "2025-07-18 05:11:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:35.014409+00:00"
    },
    {
      "arxiv_id": "2507.13659v2",
      "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework",
      "title_zh": "å½“è¡Œäººé‡è¯†åˆ«é‡è§äº‹ä»¶ç›¸æœºï¼šåŸºå‡†æ•°æ®é›†ä¸å±æ€§å¼•å¯¼çš„é‡è¯†åˆ«æ¡†æ¶",
      "authors": [
        "Xiao Wang",
        "Qian Zhu",
        "Shujuan Wu",
        "Bo Jiang",
        "Shiliang Zhang"
      ],
      "abstract": "Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äº‹ä»¶ç›¸æœº(Event Camera)åœ¨è¡Œäººé‡è¯†åˆ«(Person Re-Identification, ReID)ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹ç°æœ‰ç›¸å…³æ•°æ®é›†è§„æ¨¡è¾ƒå°æˆ–ä¾èµ–æ¨¡æ‹Ÿæ•°æ®è€Œéš¾ä»¥è¯„ä¼°æ³›åŒ–æ€§èƒ½çš„é—®é¢˜ï¼Œæ„å»ºäº†å¤§è§„æ¨¡RGB-Eventè¡Œäººé‡è¯†åˆ«æ•°æ®é›†EvReIDã€‚è¯¥æ•°æ®é›†åŒ…å«1200ä¸ªèº«ä»½çš„118,988ä¸ªå›¾åƒå¯¹ï¼Œæ¶µç›–äº†å¤šç§å­£èŠ‚ã€åœºæ™¯å’Œå…‰ç…§æ¡ä»¶ï¼Œå¹¶å¯¹15ç§State-of-the-artç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºTriPro-ReIDçš„å±æ€§å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ¡†æ¶(Attribute-guided contrastive learning framework)ï¼Œæ—¨åœ¨æœ‰æ•ˆæ¢ç´¢RGBå¸§ä¸äº‹ä»¶æµçš„è§†è§‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¡Œäººå±æ€§ä½œä¸ºä¸­å±‚è¯­ä¹‰ç‰¹å¾æ¥å¢å¼ºæ¨¡å‹å­¦ä¹ èƒ½åŠ›ã€‚åœ¨EvReIDå’ŒMARSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåˆ©ç”¨äº‹ä»¶ç›¸æœºå¤„ç†éšç§ä¿æŠ¤å’Œå¤æ‚å…‰ç…§ç¯å¢ƒä¸‹çš„è¯†åˆ«ä»»åŠ¡æä¾›äº†é‡è¦åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2507.13659v2",
      "published_date": "2025-07-18 05:04:59 UTC",
      "updated_date": "2025-11-09 14:46:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:49.558494+00:00"
    },
    {
      "arxiv_id": "2508.00007v1",
      "title": "Agent Network Protocol Technical White Paper",
      "title_zh": "æ™ºèƒ½ä½“ç½‘ç»œåè®®æŠ€æœ¯ç™½çš®ä¹¦",
      "authors": [
        "Gaowei Chang",
        "Eidan Lin",
        "Chengxuan Yuan",
        "Rizhao Cai",
        "Binbin Chen",
        "Xuan Xie",
        "Yin Zhang"
      ],
      "abstract": "With the development of large models and autonomous decision-making AI, agents are rapidly becoming the new entities of the internet, following mobile apps. However, existing internet infrastructure is primarily designed for human interaction, creating data silos, unfriendly interfaces, and high collaboration costs among agents, making it difficult to support the needs for large-scale agent interconnection and collaboration. The internet is undergoing a profound transformation, showing four core trends: agents replacing traditional software, universal agent interconnection, native protocol-based connections, and autonomous agent organization and collaboration. To align with these trends, Agent Network Protocol (ANP) proposes a new generation of communication protocols for the Agentic Web. ANP adheres to AI-native design, maintains compatibility with existing internet protocols, adopts a modular composable architecture, follows minimalist yet extensible principles, and enables rapid deployment based on existing infrastructure. Through a three-layer protocol system--identity and encrypted communication layer, meta-protocol negotiation layer, and application protocol layer--ANP. systematically solves the problems of agent identity authentication, dynamic negotiation, and capability discovery interoperability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰äº’è”ç½‘åŸºç¡€è®¾æ–½éš¾ä»¥æ”¯æŒå¤§è§„æ¨¡æ™ºèƒ½ä½“äº’è”ä¸åä½œçš„é—®é¢˜ï¼Œæå‡ºäº† Agent Network Protocol (ANP)ï¼Œä¸€ç§ä¸º Agentic Web è®¾è®¡çš„æ–°ä¸€ä»£é€šä¿¡åè®®ã€‚ANP éµå¾ª AI-native è®¾è®¡åŸåˆ™ï¼Œåœ¨å…¼å®¹ç°æœ‰äº’è”ç½‘åè®®çš„åŒæ—¶ï¼Œé‡‡ç”¨æ¨¡å—åŒ–å¯ç»„åˆæ¶æ„ä»¥å®ç°çµæ´»æ‰©å±•ã€‚è¯¥åè®®æ„å»ºäº†ç”±èº«ä»½ä¸åŠ å¯†é€šä¿¡å±‚ (identity and encrypted communication layer)ã€å…ƒåè®®åå•†å±‚ (meta-protocol negotiation layer) å’Œåº”ç”¨åè®®å±‚ (application protocol layer) ç»„æˆçš„ä¸‰å±‚ä½“ç³»ã€‚é€šè¿‡è¿™ä¸€ç³»ç»ŸåŒ–æ¡†æ¶ï¼ŒANP æœ‰æ•ˆè§£å†³äº†æ™ºèƒ½ä½“åœ¨èº«ä»½è®¤è¯ã€åŠ¨æ€åå•†åŠèƒ½åŠ›å‘ç°ç­‰æ–¹é¢çš„äº’æ“ä½œæ€§éš¾é¢˜ã€‚è¿™ä¸€åè®®çš„æå‡ºé¡ºåº”äº†æ™ºèƒ½ä½“å–ä»£ä¼ ç»Ÿè½¯ä»¶åŠè‡ªä¸»ååŒçš„å‘å±•è¶‹åŠ¿ï¼Œä¸ºæ„å»ºé«˜æ•ˆäº’è”çš„æ™ºèƒ½ä½“ç½‘ç»œå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "This white paper is a reformatted version of the open-source community edition previously released by the ANP Open Source Technology Community(https://github.com/agent-network-protocol)",
      "pdf_url": "https://arxiv.org/pdf/2508.00007v1",
      "published_date": "2025-07-18 05:04:43 UTC",
      "updated_date": "2025-07-18 05:04:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:44.189454+00:00"
    },
    {
      "arxiv_id": "2507.13652v1",
      "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses",
      "title_zh": "ç»“åˆæ¨¡å‹è¿½è¸ªä¸åŸºäºçº¦æŸçš„å»ºæ¨¡è¿›è¡Œå¤šæ­¥ç­–ç•¥è¯Šæ–­",
      "authors": [
        "Gerben van der Hoek",
        "Johan Jeuring",
        "Rogier Bos"
      ],
      "abstract": "Model tracing and constraint-based modeling are two approaches to diagnose student input in stepwise tasks. Model tracing supports identifying consecutive problem-solving steps taken by a student, whereas constraint-based modeling supports student input diagnosis even when several steps are combined into one step. We propose an approach that merges both paradigms. By defining constraints as properties that a student input has in common with a step of a strategy, it is possible to provide a diagnosis when a student deviates from a strategy even when the student combines several steps. In this study we explore the design of a system for multistep strategy diagnoses, and evaluate these diagnoses. As a proof of concept, we generate diagnoses for an existing dataset containing steps students take when solving quadratic equations (n=2136). To compare with human diagnoses, two teachers coded a random sample of deviations (n=70) and applications of the strategy (n=70). Results show that that the system diagnosis aligned with the teacher coding in all of the 140 student steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆäº† Model tracing å’Œ Constraint-based modeling çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å­¦ç”Ÿåœ¨å¤šæ­¥ä»»åŠ¡ä¸­çš„ç­–ç•¥è¯Šæ–­ã€‚Model tracing èƒ½å¤Ÿè¯†åˆ«è¿ç»­çš„è§£é¢˜æ­¥éª¤ï¼Œè€Œ Constraint-based modeling åˆ™æ”¯æŒåœ¨å­¦ç”Ÿå°†å¤šä¸ªæ­¥éª¤åˆå¹¶ä¸ºä¸€æ­¥æ—¶è¿›è¡Œè¾“å…¥è¯Šæ–­ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†çº¦æŸå®šä¹‰ä¸ºå­¦ç”Ÿè¾“å…¥ä¸ç­–ç•¥æ­¥éª¤çš„å…±åŒå±æ€§ï¼Œä½¿å¾—ç³»ç»Ÿåœ¨å­¦ç”Ÿåç¦»ç­–ç•¥æˆ–è·³æ­¥æ—¶ä»èƒ½æä¾›å‡†ç¡®åé¦ˆã€‚ç ”ç©¶é€šè¿‡ä¸€ä¸ªåŒ…å«2136ä¸ªå­¦ç”Ÿæ±‚è§£ quadratic equations æ­¥éª¤çš„æ•°æ®é›†è¿›è¡Œäº†æ¦‚å¿µéªŒè¯ï¼Œå¹¶é‚€è¯·ä¸¤åæ•™å¸ˆå¯¹éšæœºæŠ½å–çš„140ä¸ªæ­¥éª¤è¿›è¡Œäººå·¥å¯¹æ¯”åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿçš„è¯Šæ–­ç»“æœä¸æ•™å¸ˆçš„äººå·¥ç¼–ç åœ¨æ‰€æœ‰æ ·æœ¬ä¸­å‡å®Œå…¨ä¸€è‡´ã€‚è¿™è¯æ˜äº†èåˆä¸¤ç§å»ºæ¨¡èŒƒå¼èƒ½æœ‰æ•ˆæå‡å¤šæ­¥ç­–ç•¥è¯Šæ–­çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13652v1",
      "published_date": "2025-07-18 04:47:47 UTC",
      "updated_date": "2025-07-18 04:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:50.291274+00:00"
    },
    {
      "arxiv_id": "2507.13651v1",
      "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks",
      "title_zh": "åˆ†æ­¥ä»»åŠ¡ä¸­åŸºäºæœ€ç»ˆç­”æ¡ˆè¯„ä¼°çš„ç»„åˆæ­¥éª¤é”™è¯¯è§„åˆ™è¯Šæ–­",
      "authors": [
        "Gerben van der Hoek",
        "Johan Jeuring",
        "Rogier Bos"
      ],
      "abstract": "Many intelligent tutoring systems can support a student in solving a stepwise task. When a student combines several steps in one step, the number of possible paths connecting consecutive inputs may be very large. This combinatorial explosion makes error diagnosis hard. Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion, because there are generally fewer possible (erroneous) final answers than (erroneous) solution paths. An intermediate input for a task can be diagnosed by automatically completing it according to the task solution strategy and diagnosing this solution. This study explores the potential of automated error diagnosis based on a final answer. We investigate the design of a service that provides a buggy rule diagnosis when a student combines several steps. To validate the approach, we apply the service to an existing dataset (n=1939) of unique student steps when solving quadratic equations, which could not be diagnosed by a buggy rule service that tries to connect consecutive inputs with a single rule. Results show that final answer evaluation can diagnose 29,4% of these steps. Moreover, a comparison of the generated diagnoses with teacher diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the cases. These results can be considered a basis for further exploration of the approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åˆ†æ­¥ä»»åŠ¡(stepwise tasks)ä¸­é’ˆå¯¹å­¦ç”Ÿåˆå¹¶å¤šä¸ªæ­¥éª¤æ‰€äº§ç”Ÿçš„é”™è¯¯è§„åˆ™è¯Šæ–­(buggy rule diagnosis)é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹ç”±äºè¿æ¥å¤šä¸ªè¿ç»­æ­¥éª¤è€Œäº§ç”Ÿçš„ç»„åˆçˆ†ç‚¸(combinatorial explosion)æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæœ€ç»ˆç­”æ¡ˆè¯„ä¼°(final answer evaluation)çš„è¯Šæ–­æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ ¹æ®ä»»åŠ¡æ±‚è§£ç­–ç•¥è‡ªåŠ¨è¡¥å…¨å­¦ç”Ÿçš„ä¸­é—´è¾“å…¥ï¼Œå¹¶å¯¹ç”Ÿæˆçš„æœ€ç»ˆç»“æœè¿›è¡Œè¯Šæ–­ï¼Œä»è€Œè¯†åˆ«å…¶ç»„åˆæ­¥éª¤ä¸­çš„é€»è¾‘é”™è¯¯ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æœåŠ¡åº”ç”¨äºä¸€ä¸ªåŒ…å«1939ä¸ªäºŒæ¬¡æ–¹ç¨‹æ±‚è§£æ­¥éª¤çš„æ•°æ®é›†ï¼Œè¿™äº›æ­¥éª¤æ­¤å‰æ— æ³•è¢«ä¼ ç»Ÿçš„å•è§„åˆ™è¯Šæ–­æœåŠ¡è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€ç»ˆç­”æ¡ˆè¯„ä¼°èƒ½å¤ŸæˆåŠŸè¯Šæ–­å‡ºå…¶ä¸­29.4%çš„å¤æ‚æ­¥éª¤ï¼Œä¸”åœ¨ä¸æ•™å¸ˆè¯Šæ–­çš„å¯¹æ¯”ä¸­ä¸€è‡´æ€§è¾¾åˆ°äº†97%ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿ(intelligent tutoring systems)å¤„ç†å¤æ‚çš„å­¦ç”Ÿè¾“å…¥æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13651v1",
      "published_date": "2025-07-18 04:39:13 UTC",
      "updated_date": "2025-07-18 04:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:35:49.738607+00:00"
    },
    {
      "arxiv_id": "2507.13647v1",
      "title": "Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones",
      "title_zh": "æ”¹è¿›ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ï¼šæ— äººæœºç¾¤å¤šç›®æ ‡è½¨è¿¹ä¼˜åŒ–",
      "authors": [
        "Minze Li",
        "Wei Zhao",
        "Ran Chen",
        "Mingqiang Wei"
      ],
      "abstract": "Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic environments remains a key challenge due to high computational demands and the need for fast, adaptive responses. Traditional Particle Swarm Optimization (PSO) methods, while effective for offline planning, often struggle with premature convergence and latency in real-time scenarios. To overcome these limitations, we propose PE-PSO, an enhanced PSO-based online trajectory planner. The method introduces a persistent exploration mechanism to preserve swarm diversity and an entropy-based parameter adjustment strategy to dynamically adapt optimization behavior. UAV trajectories are modeled using B-spline curves, which ensure path smoothness while reducing optimization complexity. To extend this capability to UAV swarms, we develop a multi-agent framework that combines genetic algorithm (GA)-based task allocation with distributed PE-PSO, supporting scalable and coordinated trajectory generation. The distributed architecture allows for parallel computation and decentralized control, enabling effective cooperation among agents while maintaining real-time performance. Comprehensive simulations demonstrate that the proposed framework outperforms conventional PSO and other swarm-based planners across several metrics, including trajectory quality, energy efficiency, obstacle avoidance, and computation time. These results confirm the effectiveness and applicability of PE-PSO in real-time multi-UAV operations under complex environmental conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®æ—¶èˆªè¿¹è§„åˆ’é¢ä¸´çš„é«˜è®¡ç®—éœ€æ±‚å’Œä¼ ç»ŸParticle Swarm Optimization (PSO)ç®—æ³•æ˜“æ—©ç†Ÿæ”¶æ•›åŠé«˜å»¶è¿Ÿç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºå‹åœ¨çº¿è§„åˆ’å™¨PE-PSOã€‚è¯¥æ–¹æ³•å¼•å…¥äº†persistent explorationæœºåˆ¶ä»¥ä¿æŒç§ç¾¤å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆentropy-basedå‚æ•°è°ƒæ•´ç­–ç•¥åŠ¨æ€é€‚é…ä¼˜åŒ–è¡Œä¸ºã€‚ç ”ç©¶é‡‡ç”¨B-splineæ›²çº¿è¿›è¡Œèˆªè¿¹å»ºæ¨¡ï¼Œåœ¨ç¡®ä¿è·¯å¾„å¹³æ»‘çš„åŒæ—¶æœ‰æ•ˆé™ä½äº†ä¼˜åŒ–å¤æ‚åº¦ã€‚ä¸ºäº†æ‰©å±•è‡³æ— äººæœºé›†ç¾¤åº”ç”¨ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†åŸºäºGenetic Algorithm (GA)çš„ä»»åŠ¡åˆ†é…ä¸åˆ†å¸ƒå¼PE-PSOç›¸ç»“åˆï¼Œå®ç°äº†å¯æ‰©å±•çš„ååŒèˆªè¿¹ç”Ÿæˆã€‚å…¶åˆ†å¸ƒå¼æ¶æ„æ”¯æŒå¹¶è¡Œè®¡ç®—ä¸å»ä¸­å¿ƒåŒ–æ§åˆ¶ï¼Œåœ¨ç»´æŒå®æ—¶æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ™ºèƒ½ä½“é—´çš„æœ‰æ•ˆåä½œã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èˆªè¿¹è´¨é‡ã€èƒ½æºæ•ˆç‡ã€é¿éšœèƒ½åŠ›åŠè®¡ç®—æ—¶é—´ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»ŸPSOåŠå…¶ä»–é›†ç¾¤è§„åˆ’å™¨ã€‚è¿™å……åˆ†éªŒè¯äº†PE-PSOåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å®æ—¶å¤šUAVä½œä¸šä¸­å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ä¸é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 papers,7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13647v1",
      "published_date": "2025-07-18 04:31:49 UTC",
      "updated_date": "2025-07-18 04:31:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:00.187164+00:00"
    },
    {
      "arxiv_id": "2507.13646v1",
      "title": "A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design",
      "title_zh": "åŸºäº Transformer è¯­è¨€æ¨¡å‹çš„è›‹ç™½è´¨åºåˆ—åˆ†æä¸è®¾è®¡å…¨é¢ç»¼è¿°",
      "authors": [
        "Nimisha Ghosh",
        "Daniele Santoni",
        "Debaleena Nawn",
        "Eleonora Ottaviani",
        "Giovanni Felici"
      ],
      "abstract": "The impact of Transformer-based language models has been unprecedented in Natural Language Processing (NLP). The success of such models has also led to their adoption in other fields including bioinformatics. Taking this into account, this paper discusses recent advances in Transformer-based models for protein sequence analysis and design. In this review, we have discussed and analysed a significant number of works pertaining to such applications. These applications encompass gene ontology, functional and structural protein identification, generation of de novo proteins and binding of proteins. We attempt to shed light on the strength and weaknesses of the discussed works to provide a comprehensive insight to readers. Finally, we highlight shortcomings in existing research and explore potential avenues for future developments. We believe that this review will help researchers working in this field to have an overall idea of the state of the art in this field, and to orient their future studies.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢è®¨äº†åŸºäº Transformer çš„è¯­è¨€æ¨¡å‹åœ¨è›‹ç™½è´¨åºåˆ—åˆ†æä¸è®¾è®¡é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚æ–‡ç« ç³»ç»Ÿåˆ†æäº†åŒ…æ‹¬åŸºå› æœ¬ä½“ (Gene Ontology)ã€è›‹ç™½è´¨åŠŸèƒ½ä¸ç»“æ„é‰´å®š (Functional and Structural Protein Identification)ã€ä»å¤´è›‹ç™½è´¨ç”Ÿæˆ (De Novo Protein Generation) ä»¥åŠè›‹ç™½è´¨ç»“åˆ (Protein Binding) åœ¨å†…çš„å¤šé¡¹å…³é”®åº”ç”¨ã€‚ä½œè€…é€šè¿‡å¯¹å¤§é‡ç›¸å…³ç ”ç©¶çš„å¯¹æ¯”è®¨è®ºï¼Œæ·±å…¥å‰–æäº†ç°æœ‰æ¨¡å‹åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é‡ç‚¹æŒ‡å‡ºå½“å‰ç ”ç©¶å­˜åœ¨çš„ä¸è¶³ï¼Œå¹¶å‰ç»æ€§åœ°æ¢ç´¢äº†æœªæ¥çš„æ½œåœ¨å‘å±•æ–¹å‘ã€‚è¿™ç¯‡å›é¡¾æ€§è®ºæ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›å…³äºè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ç°çŠ¶çš„å…¨é¢è§†é‡ï¼ŒåŠ©åŠ›å…¶æŠŠæ¡æŠ€æœ¯å‰æ²¿å¹¶ç§‘å­¦è§„åˆ’æœªæ¥çš„ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13646v1",
      "published_date": "2025-07-18 04:20:33 UTC",
      "updated_date": "2025-07-18 04:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:17.191813+00:00"
    },
    {
      "arxiv_id": "2507.16838v2",
      "title": "Segmentation-free Goodness of Pronunciation",
      "title_zh": "å…åˆ†å‰²çš„å‘éŸ³ä¼˜åº¦",
      "authors": [
        "Xinwei Cao",
        "Zijian Fan",
        "TorbjÃ¸rn Svendsen",
        "Giampiero Salvi"
      ],
      "abstract": "Mispronunciation detection and diagnosis (MDD) is a significant part in modern computer aided language learning (CALL) systems. Within MDD, phoneme-level pronunciation assessment is key to helping L2 learners improve their pronunciation. However, most systems are based on a form of goodness of pronunciation (GOP) which requires pre-segmentation of speech into phonetic units. This limits the accuracy of these methods and the possibility to use modern CTC-based acoustic models for their evaluation. In this study, we first propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR models for MDD. Next, we define a more general alignment-free method that takes all possible alignments of the target phoneme into account (GOP-AF). We give a theoretical account of our definition of GOP-AF, an implementation that solves potential numerical issues as well as a proper normalization which makes the method applicable with acoustic models with different peakiness over time. We provide extensive experimental results on the CMU Kids and Speechocean762 datasets comparing the different definitions of our methods, estimating the dependency of GOP-AF on the peakiness of the acoustic models and on the amount of context around the target phoneme. Finally, we compare our methods with recent studies over the Speechocean762 data showing that the feature vectors derived from the proposed method achieve state-of-the-art results on phoneme-level pronunciation assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‘éŸ³é”™è¯¯æ£€æµ‹ä¸è¯Šæ–­(MDD)ä¸­ä¼ ç»Ÿå‘éŸ³è´¨é‡è¯„ä¼°(GOP)éœ€è¦é¢„åˆ†å‰²(pre-segmentation)å¯¼è‡´ç²¾åº¦å—é™ä¸”éš¾ä»¥ç»“åˆCTCå£°å­¦æ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§æ— éœ€æ˜¾å¼åˆ†å‰²çš„æ”¹è¿›æ–¹æ³•ã€‚é¦–å…ˆï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºè‡ªå¯¹é½çš„GOP-SAï¼Œä½¿å¾—CTCè®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æ¨¡å‹èƒ½æœ‰æ•ˆåº”ç”¨äºè¯„ä¼°ä»»åŠ¡ã€‚éšåï¼Œè¿›ä¸€æ­¥å®šä¹‰äº†æ›´é€šç”¨çš„å…å¯¹é½æ–¹æ³•GOP-AFï¼Œè¯¥æ–¹æ³•é€šè¿‡è€ƒè™‘ç›®æ ‡éŸ³ç´ çš„æ‰€æœ‰å¯èƒ½å¯¹é½æƒ…å†µå¹¶å¼•å…¥è§„èŒƒåŒ–å¤„ç†ï¼Œè§£å†³äº†å£°å­¦æ¨¡å‹å°–å³°ç‰¹æ€§(peakiness)å¯¼è‡´çš„æ•°å€¼é—®é¢˜ã€‚é€šè¿‡åœ¨CMU Kidså’ŒSpeechocean762æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹è¯„ä¼°å‡†ç¡®æ€§çš„å½±å“ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æå–çš„ç‰¹å¾å‘é‡åœ¨éŸ³ç´ çº§å‘éŸ³è¯„ä¼°ä¸­å–å¾—äº†å½“å‰æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ï¼Œä¸ºç°ä»£è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ (CALL)ç³»ç»Ÿæä¾›äº†æ›´ç²¾ç¡®çš„è¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
      "pdf_url": "https://arxiv.org/pdf/2507.16838v2",
      "published_date": "2025-07-18 04:00:58 UTC",
      "updated_date": "2025-07-24 02:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:22.392906+00:00"
    },
    {
      "arxiv_id": "2507.18645v1",
      "title": "Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis",
      "title_zh": "é‡å­è®¤çŸ¥éš§é“ç¥ç»ç½‘ç»œï¼šåœ¨å†›æ°‘ç”¨è½¦è¾†åˆ†ç±»ä¸æƒ…æ„Ÿåˆ†æä¸­çš„åº”ç”¨",
      "authors": [
        "Milan Maksimovic",
        "Anna Bohdanets",
        "Immaculate Motsi-Omoijiade",
        "Guido Governatori",
        "Ivan S. Maksymov"
      ],
      "abstract": "Prior work has demonstrated that incorporating well-known quantum tunnelling (QT) probability into neural network models effectively captures important nuances of human perception, particularly in the recognition of ambiguous objects and sentiment analysis. In this paper, we employ novel QT-based neural networks and assess their effectiveness in distinguishing customised CIFAR-format images of military and civilian vehicles, as well as sentiment, using a proprietary military-specific vocabulary. We suggest that QT-based models can enhance multimodal AI applications in battlefield scenarios, particularly within human-operated drone warfare contexts, imbuing AI with certain traits of human reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†é‡å­éš§é“(Quantum Tunnelling, QT)æ¦‚ç‡æ•´åˆè¿›ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œä»¥æ•æ‰äººç±»æ„ŸçŸ¥ä¸­çš„å¾®å¦™å·®åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«æ¨¡ç³Šç‰©ä½“å’Œæƒ…æ„Ÿåˆ†ææ–¹é¢çš„åº”ç”¨ã€‚è®ºæ–‡é‡‡ç”¨äº†æ–°å‹çš„åŸºäºQTçš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨åŒºåˆ†å†›äº‹ä¸æ°‘ç”¨è½¦è¾†çš„å®šåˆ¶CIFARæ ¼å¼å›¾åƒä»¥åŠæƒ…æ„Ÿåˆ†ææ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¿‡ç¨‹ä¸­ç»“åˆäº†ä¸“é—¨çš„å†›äº‹ç‰¹å®šè¯æ±‡è¡¨ï¼Œæ—¨åœ¨éªŒè¯æ¨¡å‹åœ¨å¤„ç†ç‰¹å®šé¢†åŸŸä»»åŠ¡æ—¶çš„ç²¾å‡†åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºQTçš„æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæˆ˜åœºåœºæ™¯ä¸‹å¤šæ¨¡æ€äººå·¥æ™ºèƒ½(Multimodal AI)çš„åº”ç”¨èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨äººç±»æ“ä½œçš„æ— äººæœºæˆ˜äº‰èƒŒæ™¯ä¸‹ï¼Œè¿™ç§æ–¹æ³•ä¸ºAIèµ‹äºˆäº†ç±»ä¼¼äºäººç±»æ¨ç†çš„æŸäº›ç‰¹è´¨ï¼Œä¸ºæå‡å¤æ‚ç¯å¢ƒä¸‹çš„ç›®æ ‡è¯†åˆ«ä¸æ€åŠ¿æ„ŸçŸ¥å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18645v1",
      "published_date": "2025-07-18 03:58:50 UTC",
      "updated_date": "2025-07-18 03:58:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:25.382871+00:00"
    },
    {
      "arxiv_id": "2507.13629v1",
      "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques",
      "title_zh": "ç½‘ç»œå®‰å…¨é¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹ï¼šåº”ç”¨ã€è„†å¼±æ€§ä¸é˜²å¾¡æŠ€æœ¯",
      "authors": [
        "Niveen O. Jaffal",
        "Mohammed Alkhanafseh",
        "David Mohaisen"
      ],
      "abstract": "Large Language Models (LLMs) are transforming cybersecurity by enabling intelligent, adaptive, and automated approaches to threat detection, vulnerability assessment, and incident response. With their advanced language understanding and contextual reasoning, LLMs surpass traditional methods in tackling challenges across domains such as IoT, blockchain, and hardware security. This survey provides a comprehensive overview of LLM applications in cybersecurity, focusing on two core areas: (1) the integration of LLMs into key cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along with mitigation strategies. By synthesizing recent advancements and identifying key limitations, this work offers practical insights and strategic recommendations for leveraging LLMs to build secure, scalable, and future-ready cyber defense systems.",
      "tldr_zh": "æœ¬ç»¼è¿°å…¨é¢æ¢è®¨äº† Large Language Models (LLMs) åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ï¼Œé‡ç‚¹åˆ†æäº†å…¶å¦‚ä½•é€šè¿‡æ™ºèƒ½åŒ–ã€è‡ªé€‚åº”å’Œè‡ªåŠ¨åŒ–çš„æ–¹å¼æ”¹å˜å¨èƒæ£€æµ‹ã€æ¼æ´è¯„ä¼°å’Œäº‹ä»¶å“åº”ã€‚å‡­å€Ÿå…ˆè¿›çš„è¯­è¨€ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼ŒLLMs åœ¨å¤„ç† IoTã€blockchain å’Œç¡¬ä»¶å®‰å…¨ç­‰é¢†åŸŸçš„å¤æ‚æŒ‘æˆ˜æ—¶è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ç ”ç©¶æ·±å…¥è°ƒç ”äº†ä¸¤ä¸ªæ ¸å¿ƒæ–¹å‘ï¼Œå³ LLMs åœ¨å…³é”®å®‰å…¨é¢†åŸŸçš„é›†æˆåº”ç”¨ï¼Œä»¥åŠ LLMs è‡ªèº«çš„ Vulnerabilities åŠå…¶é˜²å¾¡ç¼“è§£ç­–ç•¥ã€‚é€šè¿‡æ€»ç»“æŠ€æœ¯è¿›å±•å¹¶å‰–æå±€é™æ€§ï¼Œè¯¥å·¥ä½œä¸ºæ„å»ºå®‰å…¨ã€å¯æ‰©å±•ä¸”å…·å¤‡æœªæ¥é€‚åº”æ€§çš„ç½‘ç»œé˜²å¾¡ç³»ç»Ÿæä¾›äº†å®è·µè§è§£ä¸æˆ˜ç•¥å»ºè®®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.13629v1",
      "published_date": "2025-07-18 03:41:18 UTC",
      "updated_date": "2025-07-18 03:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:25.787770+00:00"
    },
    {
      "arxiv_id": "2507.13625v1",
      "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety",
      "title_zh": "BifrostRAGï¼šé¢å‘å»ºç­‘å®‰å…¨å¤šè·³é—®ç­”çš„åŒçŸ¥è¯†å›¾è°±æ¡¥æ¥æ–¹æ³•",
      "authors": [
        "Yuxin Zhang",
        "Xi Wang",
        "Mo Hu",
        "Zhenyu Zhang"
      ],
      "abstract": "Information retrieval and question answering from safety regulations are essential for automated construction compliance checking but are hindered by the linguistic and structural complexity of regulatory text. Many compliance-related queries are multi-hop, requiring synthesis of information across interlinked clauses. This poses a challenge for traditional retrieval-augmented generation (RAG) systems. To overcome this, we introduce BifrostRAG: a dual-graph RAG-integrated system that explicitly models both linguistic relationships (via an Entity Network Graph) and document structure (via a Document Navigator Graph). This architecture powers a hybrid retrieval mechanism that combines graph traversal with vector-based semantic search, enabling large language models to reason over both the meaning and the structure of the text. Evaluation on a multi-hop question dataset shows that BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent. These results significantly outperform vector-only and graph-only RAG baselines that represent current leading approaches. Error analysis further highlights the comparative advantages of our hybrid method over single-modality RAGs. These findings establish BifrostRAG as a robust knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid retrieval mechanism offers a transferable blueprint for navigating complex technical documents across knowledge-intensive engineering domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BifrostRAGï¼Œä¸€ç§é’ˆå¯¹å»ºç­‘å®‰å…¨åˆè§„æ£€æŸ¥è®¾è®¡çš„åŒå›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ³•è§„æ–‡æœ¬ä¸­å¤šè·³(multi-hop)é—®ç­”çš„å¤æ‚æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å®ä½“ç½‘ç»œå›¾(Entity Network Graph)å»ºæ¨¡è¯­è¨€å…³ç³»ï¼Œå¹¶ç»“åˆæ–‡æ¡£å¯¼èˆªå›¾(Document Navigator Graph)åˆ»ç”»æ–‡æ¡£ç»“æ„ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰è·¨æ¡æ¬¾çš„å…³è”ä¿¡æ¯ã€‚BifrostRAGé‡‡ç”¨äº†ç»“åˆå›¾éå†ä¸å‘é‡è¯­ä¹‰æœç´¢çš„æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤ŸåŒæ—¶æ¨ç†æ–‡æœ¬çš„è¯­ä¹‰å«ä¹‰ä¸ç‰©ç†ç»“æ„ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šè·³é—®é¢˜æ•°æ®é›†ä¸Šå–å¾—äº†92.8%çš„ç²¾ç¡®ç‡å’Œ87.3%çš„F1åˆ†æ•°ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„çº¯å‘é‡æˆ–çº¯å›¾ç»“æ„çš„RAGåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¤æ‚å·¥ç¨‹é¢†åŸŸçš„è‡ªåŠ¨åŒ–åˆè§„æ€§æ£€æŸ¥æä¾›äº†ç¨³å¥çš„çŸ¥è¯†å¼•æ“ï¼Œå…¶åŒå›¾æ··åˆæ£€ç´¢æœºåˆ¶ä¸ºå¤„ç†çŸ¥è¯†å¯†é›†å‹æŠ€æœ¯æ–‡æ¡£æä¾›äº†å¯æ¨å¹¿çš„æ¶æ„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13625v1",
      "published_date": "2025-07-18 03:39:14 UTC",
      "updated_date": "2025-07-18 03:39:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:32.287099+00:00"
    },
    {
      "arxiv_id": "2507.13618v4",
      "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters",
      "title_zh": "Seed-Xï¼šæ„å»º 7B å‚æ•°è§„æ¨¡çš„å¼ºå¤§å¤šè¯­è¨€ç¿»è¯‘å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Shanbo Cheng",
        "Yu Bao",
        "Qian Cao",
        "Luyang Huang",
        "Liyan Kang",
        "Zhicheng Liu",
        "Yu Lu",
        "Wenhao Zhu",
        "Jingwen Chen",
        "Zhichao Huang",
        "Tao Li",
        "Yifu Li",
        "Huiying Lin",
        "Sitong Liu",
        "Ningxin Peng",
        "Shuaijie She",
        "Lu Xu",
        "Nuo Xu",
        "Sen Yang",
        "Runsheng Yu",
        "Yiming Yu",
        "Liehao Zou",
        "Hang Li",
        "Lu Lu",
        "Yuxuan Wang",
        "Yonghui Wu"
      ],
      "abstract": "Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Seed-Xç³»åˆ—å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼ŒåŒ…å«æŒ‡ä»¤(Instruct)å’Œæ¨ç†(Reasoning)ç‰ˆæœ¬ï¼Œæ—¨åœ¨ä»¥7Bå‚æ•°è§„æ¨¡å®ç°å“è¶Šçš„å¤šè¯­è¨€ç¿»è¯‘èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¶µç›–28ç§è¯­è¨€çš„é«˜è´¨é‡å•è¯­å’ŒåŒè¯­æ•°æ®é›†å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæœ‰æ•ˆæ•æ‰å¤æ‚çš„è¯­è¨€æ¨¡å¼å¹¶è§£å†³ç¿»è¯‘ç”Ÿç¡¬ç­‰é—®é¢˜ã€‚é€šè¿‡é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ä¼˜åŒ–ï¼Œæ¨¡å‹åœ¨å¤šæ ·åŒ–è¯­è¨€å¯¹ä¸Šçš„æ³›åŒ–è¡¨ç°å¾—åˆ°æ˜¾è‘—å¢å¼ºã€‚å®éªŒè¯æ˜ï¼ŒSeed-Xåœ¨28ç§è¯­è¨€çš„æµ‹è¯„ä¸­è¾¾åˆ°äº†ä¸Gemini-2.5å’ŒGPT-4oç­‰é¡¶å°–é—­æºæ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œä¸”åœ¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ä¸­å‡å¤§å¹…è¶…è¶Šäº†å‚æ•°è§„æ¨¡æ›´å¤§çš„å¼€æºæ¨¡å‹ã€‚è¯¥ç ”ç©¶å¼€æºäº†æ¨¡å‹å‚æ•°å¹¶åˆ†äº«äº†ä¼˜åŒ–å®è·µï¼Œä¸ºå¤šè¯­è¨€ç¿»è¯‘çš„ç ”ç©¶ä¸åº”ç”¨æä¾›äº†é«˜æ€§èƒ½çš„åŸºå‡†å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13618v4",
      "published_date": "2025-07-18 03:19:43 UTC",
      "updated_date": "2025-08-21 13:08:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:36.695655+00:00"
    },
    {
      "arxiv_id": "2507.13614v3",
      "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models",
      "title_zh": "äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€å­¦åŠåŸºäºåµŒå…¥çš„ç‰¹å¾åˆ†æ",
      "authors": [
        "Sergio E. Zanotto",
        "Segun Aroyehun"
      ],
      "abstract": "The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written or machine-generated texts, our study focuses on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality, and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls, and model release dates. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human- and machine-generated texts show stylistic diversity across domains, with human-written texts displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to a homogenization of machine-generated texts.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å½¢æ€å­¦(morphology)ã€å¥æ³•(syntax)å’Œè¯­ä¹‰(semantics)ç­‰å¤šä¸ªè¯­è¨€å±‚é¢çš„ç‰¹å¾é›†ï¼Œå¯¹äººç±»ç¼–å†™ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œæ·±å…¥çš„ç‰¹å¾åŒ–è¡¨å¾åˆ†æã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¶µç›–8ä¸ªé¢†åŸŸå’Œ11ç§LLMsçš„å„ç±»æ–‡æœ¬æ•°æ®é›†ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†ä¾å­˜é•¿åº¦(dependency length)å’Œæƒ…æ„Ÿæ€§(emotionality)ç­‰ç‰¹å¾åœ¨ä¸åŒé‡‡æ ·ç­–ç•¥(sampling strategies)å’Œæ¨¡å‹è¿­ä»£ä¸‹çš„è¡¨ç°ã€‚ç»Ÿè®¡åˆ†æè¡¨æ˜ï¼Œäººç±»ç¼–å†™çš„æ–‡æœ¬é€šå¸¸å…·æœ‰æ›´ç®€å•çš„å¥æ³•ç»“æ„å’Œæ›´ä¸°å¯Œçš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œä¸”åœ¨è·¨é¢†åŸŸç‰¹å¾ä¸Šå±•ç°å‡ºæ¯”æœºå™¨ç”Ÿæˆæ–‡æœ¬æ›´å¤§çš„å˜å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡åº”ç”¨é£æ ¼åµŒå…¥(style embeddings)è¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶å‘ç°éšç€æ¨¡å‹æ›´æ–°ï¼ŒLLMsç”Ÿæˆçš„æ–‡æœ¬åœ¨å˜å¼‚æ€§ä¸Šè¡¨ç°å‡ºé«˜åº¦ç›¸ä¼¼ï¼Œåæ˜ äº†æœºå™¨ç”Ÿæˆå†…å®¹æ—¥ç›Šæ˜æ˜¾çš„åŒè´¨åŒ–(homogenization)è¶‹åŠ¿ï¼Œä¸ºç†è§£äººæœºæ–‡æœ¬å·®å¼‚æä¾›äº†é‡è¦çš„è¯­è¨€å­¦æ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2412.03025",
      "pdf_url": "https://arxiv.org/pdf/2507.13614v3",
      "published_date": "2025-07-18 02:46:55 UTC",
      "updated_date": "2025-09-30 10:37:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:37.510342+00:00"
    },
    {
      "arxiv_id": "2507.13604v1",
      "title": "BreastSegNet: Multi-label Segmentation of Breast MRI",
      "title_zh": "BreastSegNetï¼šä¹³è…ºç£å…±æŒ¯æˆåƒå¤šæ ‡ç­¾åˆ†å‰²",
      "authors": [
        "Qihang Li",
        "Jichen Yang",
        "Yaqian Chen",
        "Yuwen Chen",
        "Hanxue Gu",
        "Lars J. Grimm",
        "Maciej A. Mazurowski"
      ],
      "abstract": "Breast MRI provides high-resolution imaging critical for breast cancer screening and preoperative staging. However, existing segmentation methods for breast MRI remain limited in scope, often focusing on only a few anatomical structures, such as fibroglandular tissue or tumors, and do not cover the full range of tissues seen in scans. This narrows their utility for quantitative analysis. In this study, we present BreastSegNet, a multi-label segmentation algorithm for breast MRI that covers nine anatomical labels: fibroglandular tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and implant. We manually annotated a large set of 1123 MRI slices capturing these structures with detailed review and correction from an expert radiologist. Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet, UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across all labels. It performs especially well on heart, liver, muscle, FGT, and bone, with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All model code and weights are publicly available, and we plan to release the data at a later date.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰ä¹³è…º MRI åˆ†å‰²æ–¹æ³•ä»…å±€é™äºå°‘æ•°è§£å‰–ç»“æ„è€Œå¯¼è‡´å®šé‡åˆ†ææ•ˆç”¨å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† BreastSegNet å¤šæ ‡ç­¾åˆ†å‰²ç®—æ³•ã€‚è¯¥ç®—æ³•æ¶µç›–äº† FGTã€vesselã€muscleã€boneã€lesionã€lymph nodeã€heartã€liver å’Œ implant å…±ä¹ç±»è§£å‰–æ ‡ç­¾ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨ç»„ç»‡è¦†ç›–èŒƒå›´ä¸Šçš„ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿæ‰‹åŠ¨æ ‡æ³¨äº† 1123 å¼  MRI åˆ‡ç‰‡å¹¶ç”±ä¸“å®¶æ”¾å°„ç§‘åŒ»å¸ˆè¿›è¡Œä¸¥æ ¼å®¡æŸ¥ï¼ŒåŒæ—¶å¯¹ U-Netã€SwinUNetã€SAMã€nnU-Net ç­‰ä¹ç§ä¸»æµæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒnnU-Net ResEncM åœ¨æ‰€æœ‰æ ‡ç­¾ä¸Šçš„å¹³å‡ Dice åˆ†æ•°è¾¾åˆ° 0.694ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–åŸºå‡†æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨ heartã€liverã€muscleã€FGT å’Œ bone ç­‰æ ‡ç­¾ä¸Šï¼Œå…¶ Dice åˆ†æ•°è¶…è¿‡ 0.73ï¼Œå…¶ä¸­ heart å’Œ liver çš„å¾—åˆ†æ›´æ˜¯æ¥è¿‘ 0.90ã€‚è¯¥ç ”ç©¶ç›®å‰å·²å…¬å¼€æ¨¡å‹ä»£ç ä¸æƒé‡ï¼Œä¸ºä¹³è…º MRI çš„å…¨é¢è‡ªåŠ¨åŒ–å®šé‡åˆ†ææä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13604v1",
      "published_date": "2025-07-18 02:16:00 UTC",
      "updated_date": "2025-07-18 02:16:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:41.875265+00:00"
    },
    {
      "arxiv_id": "2507.13598v1",
      "title": "GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention",
      "title_zh": "GIFTï¼šåœ¨ä¿ç•™å®‰å…¨æ¦‚å¿µçš„åŒæ—¶æŠµå¾¡æ¶æ„å¾®è°ƒçš„æ‰©æ•£æ¨¡å‹æ¢¯åº¦æ„ŸçŸ¥å…ç–«æŠ€æœ¯",
      "authors": [
        "Amro Abdalla",
        "Ismail Shaheen",
        "Dan DeGenaro",
        "Rupayan Mallick",
        "Bogdan Raita",
        "Sarah Adel Bargal"
      ],
      "abstract": "We present GIFT: a {G}radient-aware {I}mmunization technique to defend diffusion models against malicious {F}ine-{T}uning while preserving their ability to generate safe content. Existing safety mechanisms like safety checkers are easily bypassed, and concept erasure methods fail under adversarial fine-tuning. GIFT addresses this by framing immunization as a bi-level optimization problem: the upper-level objective degrades the model's ability to represent harmful concepts using representation noising and maximization, while the lower-level objective preserves performance on safe data. GIFT achieves robust resistance to malicious fine-tuning while maintaining safe generative quality. Experimental results show that our method significantly impairs the model's ability to re-learn harmful concepts while maintaining performance on safe content, offering a promising direction for creating inherently safer generative models resistant to adversarial fine-tuning attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GIFTï¼Œä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹(diffusion models)çš„æ¢¯åº¦æ„ŸçŸ¥å…ç–«(Gradient-aware Immunization)æŠ€æœ¯ï¼Œæ—¨åœ¨é˜²å¾¡æ¶æ„å¾®è°ƒ(malicious Fine-Tuning)å¹¶ä¿ç•™å®‰å…¨å†…å®¹çš„ç”Ÿæˆèƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰å®‰å…¨æœºåˆ¶æ˜“è¢«è§„é¿ä»¥åŠæ¦‚å¿µæ“¦é™¤(concept erasure)åœ¨å¯¹æŠ—æ€§å¾®è°ƒä¸‹å¤±æ•ˆçš„é—®é¢˜ï¼ŒGIFTå°†å…ç–«è¿‡ç¨‹å»ºæ¨¡ä¸ºåŒå±‚ä¼˜åŒ–(bi-level optimization)é—®é¢˜ã€‚å…¶ä¸Šå±‚ç›®æ ‡é€šè¿‡è¡¨å¾å™ªå£°åŒ–(representation noising)å’Œæœ€å¤§åŒ–æ¥å‰Šå¼±æ¨¡å‹å¯¹æœ‰å®³æ¦‚å¿µçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸‹å±‚ç›®æ ‡åˆ™ç¡®ä¿æ¨¡å‹åœ¨å®‰å…¨æ•°æ®ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGIFTèƒ½æ˜¾è‘—é˜»ç¢æ¨¡å‹é‡æ–°å­¦ä¹ æœ‰å®³æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒå®‰å…¨å†…å®¹çš„ç”Ÿæˆè´¨é‡ï¼Œä¸ºæ„å»ºå…·æœ‰å†…åœ¨å®‰å…¨æ€§ä¸”æŠµå¾¡å¯¹æŠ—æ€§å¾®è°ƒæ”»å‡»çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Warning: This paper contains NSFW content. Reader discretion is advised",
      "pdf_url": "https://arxiv.org/pdf/2507.13598v1",
      "published_date": "2025-07-18 01:47:07 UTC",
      "updated_date": "2025-07-18 01:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:48.392434+00:00"
    },
    {
      "arxiv_id": "2507.14245v1",
      "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions",
      "title_zh": "çº³ç±³ææ–™-è›‹ç™½è´¨ç›¸äº’ä½œç”¨çš„ç™¾ä¸‡é‡çº§æ•°æ®é›†ä¸å¯æ³›åŒ–åŸºç¡€æ¨¡å‹",
      "authors": [
        "Hengjie Yu",
        "Kenneth A. Dawson",
        "Haiyun Yang",
        "Shuya Liu",
        "Yan Yan",
        "Yaochu Jin"
      ],
      "abstract": "Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº³ç±³ææ–™ä¸è›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢†åŸŸä¸­æ•°æ®é›†è§„æ¨¡æœ‰é™åŠç°æœ‰æ¨¡å‹æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„æ•°æ®é›† NanoPro-3Mï¼ŒåŒ…å«è¶…è¿‡ 320 ä¸‡ä¸ªæ ·æœ¬å’Œ 37,000 ç§ç‹¬ç‰¹è›‹ç™½è´¨ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œä½œè€…å¼€å‘äº†åä¸º NanoProFormer çš„åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€è¡¨å¾å­¦ä¹  (multimodal representation learning) å®ç°å¯¹çº³ç±³ææ–™-è›‹ç™½è´¨äº²å’ŒåŠ›çš„ç²¾å‡†é¢„æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å»ºæ¨¡åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡æ€æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆè¯†åˆ«å½±å“å† çŠ¶ç‰©å½¢æˆ (corona formation) çš„å…³é”®å› ç´ ã€‚NanoProFormer åœ¨å¤„ç†ç¼ºå¤±ç‰¹å¾ä»¥åŠåº”å¯¹æœªè§è¿‡çš„çº³ç±³ææ–™æˆ–è›‹ç™½è´¨æ—¶å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡é›¶æ ·æœ¬æ¨ç† (zero-shot inference) å’Œå¾®è°ƒ (fine-tuning)ï¼Œè¯¥æ¨¡å‹åœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚è¯¥å·¥ä½œä¸ºå®ç°é«˜æ€§èƒ½ã€é«˜æ³›åŒ–æ€§çš„çº³ç±³ææ–™-è›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢„æµ‹å»ºç«‹äº†åšå®åŸºç¡€ï¼Œæ˜¾è‘—é™ä½äº†å¯¹ä¼ ç»Ÿå®éªŒçš„ä¾èµ–ï¼Œå¹¶åŠ é€Ÿäº†ç›¸å…³ä½“å¤–åº”ç”¨çš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CE",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.14245v1",
      "published_date": "2025-07-18 00:00:52 UTC",
      "updated_date": "2025-07-18 00:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:36:49.963605+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 110,
  "processed_papers_count": 110,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T05:37:41.927695+00:00"
}
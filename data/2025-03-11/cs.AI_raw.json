[
  {
    "arxiv_id": "2503.09635v1",
    "title": "FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting",
    "authors": [
      "GeonU Kim",
      "Kim Youwang",
      "Lee Hyoseok",
      "Tae-Hyun Oh"
    ],
    "abstract": "We present FPGS, a feed-forward photorealistic style transfer method of\nlarge-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes\nlarge-scale 3D scenes with arbitrary, multiple style reference images without\nadditional optimization while preserving multi-view consistency and real-time\nrendering speed of 3D Gaussians. Prior arts required tedious per-style\noptimization or time-consuming per-scene training stage and were limited to\nsmall-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by\nintroducing a style-decomposed 3D feature field, which inherits AdaIN's\nfeed-forward stylization machinery, supporting arbitrary style reference\nimages. Furthermore, FPGS supports multi-reference stylization with the\nsemantic correspondence matching and local AdaIN, which adds diverse user\ncontrol for 3D scene styles. FPGS also preserves multi-view consistency by\napplying semantic matching and style transfer processes directly onto queried\nfeatures in 3D space. In experiments, we demonstrate that FPGS achieves\nfavorable photorealistic quality scene stylization for large-scale static and\ndynamic 3D scenes with diverse reference images. Project page:\nhttps://kim-geonu.github.io/FPGS/",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "Project page: https://kim-geonu.github.io/FPGS/. arXiv admin note:\n  substantial text overlap with arXiv:2401.05516",
    "pdf_url": "http://arxiv.org/pdf/2503.09635v1",
    "published_date": "2025-03-11 23:52:56 UTC",
    "updated_date": "2025-03-11 23:52:56 UTC"
  },
  {
    "arxiv_id": "2503.08960v1",
    "title": "Are ECGs enough? Deep learning classification of cardiac anomalies using only electrocardiograms",
    "authors": [
      "Joao D. S. Marques",
      "Arlindo L. Oliveira"
    ],
    "abstract": "Electrocardiography (ECG) is an essential tool for diagnosing multiple\ncardiac anomalies: it provides valuable clinical insights, while being\naffordable, fast and available in many settings. However, in the current\nliterature, the role of ECG analysis is often unclear: many approaches either\nrely on additional imaging modalities, such as Computed Tomography Pulmonary\nAngiography (CTPA), which may not always be available, or do not effectively\ngeneralize across different classification problems. Furthermore, the\navailability of public ECG datasets is limited and, in practice, these datasets\ntend to be small, making it essential to optimize learning strategies. In this\nstudy, we investigate the performance of multiple neural network architectures\nin order to assess the impact of various approaches. Moreover, we check whether\nthese practices enhance model generalization when transfer learning is used to\ntranslate information learned in larger ECG datasets, such as PTB-XL and\nCPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE)\ndetection. By leveraging transfer learning, we analyze the extent to which we\ncan improve learning efficiency and predictive performance on limited data.\nCode available at\nhttps://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08960v1",
    "published_date": "2025-03-11 23:37:18 UTC",
    "updated_date": "2025-03-11 23:37:18 UTC"
  },
  {
    "arxiv_id": "2503.08950v1",
    "title": "FP3: A 3D Foundation Policy for Robotic Manipulation",
    "authors": [
      "Rujia Yang",
      "Geng Chen",
      "Chuan Wen",
      "Yang Gao"
    ],
    "abstract": "Following its success in natural language processing and computer vision,\nfoundation models that are pre-trained on large-scale multi-task datasets have\nalso shown great potential in robotics. However, most existing robot foundation\nmodels rely solely on 2D image observations, ignoring 3D geometric information,\nwhich is essential for robots to perceive and reason about the 3D world. In\nthis paper, we introduce FP3, a first large-scale 3D foundation policy model\nfor robotic manipulation. FP3 builds on a scalable diffusion transformer\narchitecture and is pre-trained on 60k trajectories with point cloud\nobservations. With the model design and diverse pre-training data, FP3 can be\nefficiently fine-tuned for downstream tasks while exhibiting strong\ngeneralization capabilities. Experiments on real robots demonstrate that with\nonly 80 demonstrations, FP3 is able to learn a new task with over 90% success\nrates in novel environments with unseen objects, significantly surpassing\nexisting robot foundation models.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://3d-foundation-policy.github.io",
    "pdf_url": "http://arxiv.org/pdf/2503.08950v1",
    "published_date": "2025-03-11 23:01:08 UTC",
    "updated_date": "2025-03-11 23:01:08 UTC"
  },
  {
    "arxiv_id": "2503.08939v1",
    "title": "KAN-Mixers: a new deep learning architecture for image classification",
    "authors": [
      "Jorge Luiz dos Santos Canuto",
      "Linnyer Beatrys Ruiz Aylon",
      "Rodrigo Clemente Thom de Souza"
    ],
    "abstract": "Due to their effective performance, Convolutional Neural Network (CNN) and\nVision Transformer (ViT) architectures have become the standard for solving\ncomputer vision tasks. Such architectures require large data sets and rely on\nconvolution and self-attention operations. In 2021, MLP-Mixer emerged, an\narchitecture that relies only on Multilayer Perceptron (MLP) and achieves\nextremely competitive results when compared to CNNs and ViTs. Despite its good\nperformance in computer vision tasks, the MLP-Mixer architecture may not be\nsuitable for refined feature extraction in images. Recently, the\nKolmogorov-Arnold Network (KAN) was proposed as a promising alternative to MLP\nmodels. KANs promise to improve accuracy and interpretability when compared to\nMLPs. Therefore, the present work aims to design a new mixer-based\narchitecture, called KAN-Mixers, using KANs as main layers and evaluate its\nperformance, in terms of several performance metrics, in the image\nclassification task. As main results obtained, the KAN-Mixers model was\nsuperior to the MLP, MLP-Mixer and KAN models in the Fashion-MNIST and CIFAR-10\ndatasets, with 0.9030 and 0.6980 of average accuracy, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08939v1",
    "published_date": "2025-03-11 22:41:22 UTC",
    "updated_date": "2025-03-11 22:41:22 UTC"
  },
  {
    "arxiv_id": "2503.08936v1",
    "title": "Simulator Ensembles for Trustworthy Autonomous Driving Testing",
    "authors": [
      "Lev Sorokin",
      "Matteo Biagiola",
      "Andrea Stocco"
    ],
    "abstract": "Scenario-based testing with driving simulators is extensively used to\nidentify failing conditions of automated driving assistance systems (ADAS) and\nreduce the amount of in-field road testing. However, existing studies have\nshown that repeated test execution in the same as well as in distinct\nsimulators can yield different outcomes, which can be attributed to sources of\nflakiness or different implementations of the physics, among other factors. In\nthis paper, we present MultiSim, a novel approach to multi-simulation ADAS\ntesting based on a search-based testing approach that leverages an ensemble of\nsimulators to identify failure-inducing, simulator-agnostic test scenarios.\nDuring the search, each scenario is evaluated jointly on multiple simulators.\nScenarios that produce consistent results across simulators are prioritized for\nfurther exploration, while those that fail on only a subset of simulators are\ngiven less priority, as they may reflect simulator-specific issues rather than\ngeneralizable failures. Our case study, which involves testing a deep neural\nnetwork-based ADAS on different pairs of three widely used simulators,\ndemonstrates that MultiSim outperforms single-simulator testing by achieving on\naverage a higher rate of simulator-agnostic failures by 51%. Compared to a\nstate-of-the-art multi-simulator approach that combines the outcome of\nindependent test generation campaigns obtained in different simulators,\nMultiSim identifies 54% more simulator-agnostic failing tests while showing a\ncomparable validity rate. An enhancement of MultiSim that leverages surrogate\nmodels to predict simulator disagreements and bypass executions does not only\nincrease the average number of valid failures but also improves efficiency in\nfinding the first valid failure.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08936v1",
    "published_date": "2025-03-11 22:34:14 UTC",
    "updated_date": "2025-03-11 22:34:14 UTC"
  },
  {
    "arxiv_id": "2503.08929v1",
    "title": "HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints",
    "authors": [
      "Hrishikesh Viswanath",
      "Md Ashiqur Rahman",
      "Chi Lin",
      "Damon Conover",
      "Aniket Bera"
    ],
    "abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO",
      "eess.IV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08929v1",
    "published_date": "2025-03-11 22:18:51 UTC",
    "updated_date": "2025-03-11 22:18:51 UTC"
  },
  {
    "arxiv_id": "2503.08919v1",
    "title": "Backtracking for Safety",
    "authors": [
      "Bilgehan Sel",
      "Dingcheng Li",
      "Phillip Wallis",
      "Vaishakh Keshava",
      "Ming Jin",
      "Siddhartha Reddy Jonnalagadda"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08919v1",
    "published_date": "2025-03-11 22:04:22 UTC",
    "updated_date": "2025-03-11 22:04:22 UTC"
  },
  {
    "arxiv_id": "2503.08916v1",
    "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
    "authors": [
      "Dandan Zhao",
      "Hongpeng Yin",
      "Jintang Bian",
      "Han Zhou"
    ],
    "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08916v1",
    "published_date": "2025-03-11 21:55:46 UTC",
    "updated_date": "2025-03-11 21:55:46 UTC"
  },
  {
    "arxiv_id": "2503.08908v1",
    "title": "Interpreting the Repeated Token Phenomenon in Large Language Models",
    "authors": [
      "Itay Yona",
      "Ilia Shumailov",
      "Jamie Hayes",
      "Federico Barbero",
      "Yossi Gandelsman"
    ],
    "abstract": "Large Language Models (LLMs), despite their impressive capabilities, often\nfail to accurately repeat a single word when prompted to, and instead output\nunrelated text. This unexplained failure mode represents a vulnerability,\nallowing even end-users to diverge models away from their intended behavior. We\naim to explain the causes for this phenomenon and link it to the concept of\n``attention sinks'', an emergent LLM behavior crucial for fluency, in which the\ninitial token receives disproportionately high attention scores. Our\ninvestigation identifies the neural circuit responsible for attention sinks and\nshows how long repetitions disrupt this circuit. We extend this finding to\nother non-repeating sequences that exhibit similar circuit disruptions. To\naddress this, we propose a targeted patch that effectively resolves the issue\nwithout negatively impacting the model's overall performance. This study\nprovides a mechanistic explanation for an LLM vulnerability, demonstrating how\ninterpretability can diagnose and address issues, and offering insights that\npave the way for more secure and reliable models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08908v1",
    "published_date": "2025-03-11 21:40:58 UTC",
    "updated_date": "2025-03-11 21:40:58 UTC"
  },
  {
    "arxiv_id": "2503.08906v1",
    "title": "Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation",
    "authors": [
      "Xiwen Chen",
      "Wenhui Zhu",
      "Peijie Qiu",
      "Hao Wang",
      "Huayu Li",
      "Haiyu Wu",
      "Aristeidis Sotiras",
      "Yalin Wang",
      "Abolfazl Razi"
    ],
    "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong performance but\nstruggle when adapted to downstream tasks. Prompt learning has emerged as an\nefficient and effective strategy to adapt VLMs while preserving their\npre-trained knowledge. However, existing methods still lead to overfitting and\ndegrade zero-shot generalization. To address this challenge, we propose an\noptimal transport (OT)-guided prompt learning framework that mitigates\nforgetting by preserving the structural consistency of feature distributions\nbetween pre-trained and fine-tuned models. Unlike conventional point-wise\nconstraints, OT naturally captures cross-instance relationships and expands the\nfeasible parameter space for prompt tuning, allowing a better trade-off between\nadaptation and generalization. Our approach enforces joint constraints on both\nvision and text representations, ensuring a holistic feature alignment.\nExtensive experiments on benchmark datasets demonstrate that our simple yet\neffective method can outperform existing prompt learning strategies in\nbase-to-novel generalization, cross-dataset evaluation, and domain\ngeneralization without additional augmentation or ensemble techniques. The code\nis available at https://github.com/ChongQingNoSubway/Prompt-OT",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08906v1",
    "published_date": "2025-03-11 21:38:34 UTC",
    "updated_date": "2025-03-11 21:38:34 UTC"
  },
  {
    "arxiv_id": "2503.08883v2",
    "title": "Imitation Learning of Correlated Policies in Stackelberg Games",
    "authors": [
      "Kuang-Da Wang",
      "Ping-Chun Hsieh",
      "Wen-Chih Peng"
    ],
    "abstract": "Stackelberg games, widely applied in domains like economics and security,\ninvolve asymmetric interactions where a leader's strategy drives follower\nresponses. Accurately modeling these dynamics allows domain experts to optimize\nstrategies in interactive scenarios, such as turn-based sports like badminton.\nIn multi-agent systems, agent behaviors are interdependent, and traditional\nMulti-Agent Imitation Learning (MAIL) methods often fail to capture these\ncomplex interactions. Correlated policies, which account for opponents'\nstrategies, are essential for accurately modeling such dynamics. However, even\nmethods designed for learning correlated policies, like CoDAIL, struggle in\nStackelberg games due to their asymmetric decision-making, where leaders and\nfollowers cannot simultaneously account for each other's actions, often leading\nto non-correlated policies. Furthermore, existing MAIL methods that match\noccupancy measures or use adversarial techniques like GAIL or Inverse RL face\nscalability challenges, particularly in high-dimensional environments, and\nsuffer from unstable training. To address these challenges, we propose a\ncorrelated policy occupancy measure specifically designed for Stackelberg games\nand introduce the Latent Stackelberg Differential Network (LSDN) to match it.\nLSDN models two-agent interactions as shared latent state trajectories and uses\nmulti-output Geometric Brownian Motion (MO-GBM) to effectively capture joint\npolicies. By leveraging MO-GBM, LSDN disentangles environmental influences from\nagent-driven transitions in latent space, enabling the simultaneous learning of\ninterdependent policies. This design eliminates the need for adversarial\ntraining and simplifies the learning process. Extensive experiments on\nIterative Matrix Games and multi-agent particle environments demonstrate that\nLSDN can better reproduce complex interaction dynamics than existing MAIL\nmethods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Code will be released at this GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/LSDN",
    "pdf_url": "http://arxiv.org/pdf/2503.08883v2",
    "published_date": "2025-03-11 20:52:56 UTC",
    "updated_date": "2025-03-16 17:42:42 UTC"
  },
  {
    "arxiv_id": "2503.08879v1",
    "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference",
    "authors": [
      "Guangtao Wang",
      "Shubhangi Upasani",
      "Chen Wu",
      "Darshan Gandhi",
      "Jonathan Li",
      "Changran Hu",
      "Bo Li",
      "Urmish Thakker"
    ],
    "abstract": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08879v1",
    "published_date": "2025-03-11 20:45:02 UTC",
    "updated_date": "2025-03-11 20:45:02 UTC"
  },
  {
    "arxiv_id": "2503.08872v1",
    "title": "Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing",
    "authors": [
      "Cameron Redovian"
    ],
    "abstract": "We integrate a meta-reinforcement learning algorithm with the DreamerV3\narchitecture to improve load balancing in operating systems. This approach\nenables rapid adaptation to dynamic workloads with minimal retraining,\noutperforming the Advantage Actor-Critic (A2C) algorithm in standard and\nadaptive trials. It demonstrates robust resilience to catastrophic forgetting,\nmaintaining high performance under varying workload distributions and sizes.\nThese findings have important implications for optimizing resource management\nand performance in modern operating systems. By addressing the challenges posed\nby dynamic and heterogeneous workloads, our approach advances the adaptability\nand efficiency of reinforcement learning in real-world system management tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.OS",
      "I.2.6; I.2.8; D.4.1"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 1 figure, to be published in ACMSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08872v1",
    "published_date": "2025-03-11 20:36:49 UTC",
    "updated_date": "2025-03-11 20:36:49 UTC"
  },
  {
    "arxiv_id": "2503.08867v1",
    "title": "Zero-Shot Action Generalization with Limited Observations",
    "authors": [
      "Abdullah Alchihabi",
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "abstract": "Reinforcement Learning (RL) has demonstrated remarkable success in solving\nsequential decision-making problems. However, in real-world scenarios, RL\nagents often struggle to generalize when faced with unseen actions that were\nnot encountered during training. Some previous works on zero-shot action\ngeneralization rely on large datasets of action observations to capture the\nbehaviors of new actions, making them impractical for real-world applications.\nIn this paper, we introduce a novel zero-shot framework, Action Generalization\nfrom Limited Observations (AGLO). Our framework has two main components: an\naction representation learning module and a policy learning module. The action\nrepresentation learning module extracts discriminative embeddings of actions\nfrom limited observations, while the policy learning module leverages the\nlearned action representations, along with augmented synthetic action\nrepresentations, to learn a policy capable of handling tasks with unseen\nactions. The experimental results demonstrate that our framework significantly\noutperforms state-of-the-art methods for zero-shot action generalization across\nmultiple benchmark tasks, showcasing its effectiveness in generalizing to new\nactions with minimal action observations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "AISTATS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08867v1",
    "published_date": "2025-03-11 20:14:25 UTC",
    "updated_date": "2025-03-11 20:14:25 UTC"
  },
  {
    "arxiv_id": "2503.16499v1",
    "title": "Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities",
    "authors": [
      "Alva Markelius",
      "Julie Bailey",
      "Jenny L. Gibson",
      "Hatice Gunes"
    ],
    "abstract": "This paper presents an iterative, participatory, empirical study that\nexamines the potential of using artificial intelligence, such as social robots\nand large language models, to support mediation and advocacy for students with\ndisabilities in higher education. Drawing on qualitative data from interviews\nand focus groups conducted with various stakeholders, including disabled\nstudents, disabled student representatives, and disability practitioners at the\nUniversity of Cambridge, this study reports findings relating to understanding\nthe problem space, ideating robotic support and participatory co-design of\nadvocacy support robots. The findings highlight the potential of these\ntechnologies in providing signposting and acting as a sounding board or study\ncompanion, while also addressing limitations in empathic understanding, trust,\nequity, and accessibility. We discuss ethical considerations, including\nintersectional biases, the double empathy problem, and the implications of\ndeploying social robots in contexts shaped by structural inequalities. Finally,\nwe offer a set of recommendations and suggestions for future research,\nrethinking the notion of corrective technological interventions to tools that\nempower and amplify self-advocacy.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "This is a pre-print",
    "pdf_url": "http://arxiv.org/pdf/2503.16499v1",
    "published_date": "2025-03-11 19:57:11 UTC",
    "updated_date": "2025-03-11 19:57:11 UTC"
  },
  {
    "arxiv_id": "2503.08823v2",
    "title": "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness",
    "authors": [
      "Ce Guo",
      "Tong Zhao"
    ],
    "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG",
      "I.2.2"
    ],
    "primary_category": "cs.AR",
    "comment": "to be published in International Symposium on Highly Efficient\n  Accelerators and Reconfigurable Technologies 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08823v2",
    "published_date": "2025-03-11 18:54:17 UTC",
    "updated_date": "2025-03-21 23:27:05 UTC"
  },
  {
    "arxiv_id": "2503.10684v1",
    "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
    "authors": [
      "Jingwen Deng",
      "Zihao Wang",
      "Shaofei Cai",
      "Anji Liu",
      "Yitao Liang"
    ],
    "abstract": "Learning skills in open-world environments is essential for developing agents\ncapable of handling a variety of tasks by combining basic skills. Online\ndemonstration videos are typically long but unsegmented, making them difficult\nto segment and label with skill identifiers. Unlike existing methods that rely\non sequence sampling or human labeling, we have developed a self-supervised\nlearning-based approach to segment these long videos into a series of\nsemantic-aware and skill-consistent segments. Drawing inspiration from human\ncognitive event segmentation theory, we introduce Skill Boundary Detection\n(SBD), an annotation-free temporal video segmentation algorithm. SBD detects\nskill boundaries in a video by leveraging prediction errors from a pretrained\nunconditional action-prediction model. This approach is based on the assumption\nthat a significant increase in prediction error indicates a shift in the skill\nbeing executed. We evaluated our method in Minecraft, a rich open-world\nsimulator with extensive gameplay videos available online. Our SBD-generated\nsegments improved the average performance of conditioned policies by 63.7% and\n52.1% on short-term atomic skill tasks, and their corresponding hierarchical\nagents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the\ndiverse YouTube videos to train instruction-following agents. The project page\ncan be found in https://craftjarvis.github.io/SkillDiscovery.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10684v1",
    "published_date": "2025-03-11 18:51:40 UTC",
    "updated_date": "2025-03-11 18:51:40 UTC"
  },
  {
    "arxiv_id": "2503.08815v1",
    "title": "Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations",
    "authors": [
      "Danielle Villa",
      "Maria Chang",
      "Keerthiram Murugesan",
      "Rosario Uceda-Sosa",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "abstract": "Large Language Models (LLMs) are often asked to explain their outputs to\nenhance accuracy and transparency. However, evidence suggests that these\nexplanations can misrepresent the models' true reasoning processes. One\neffective way to identify inaccuracies or omissions in these explanations is\nthrough consistency checking, which typically involves asking follow-up\nquestions. This paper introduces, cross-examiner, a new method for generating\nfollow-up questions based on a model's explanation of an initial question. Our\nmethod combines symbolic information extraction with language model-driven\nquestion generation, resulting in better follow-up questions than those\nproduced by LLMs alone. Additionally, this approach is more flexible than other\nmethods and can generate a wider variety of follow-up questions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08815v1",
    "published_date": "2025-03-11 18:50:43 UTC",
    "updated_date": "2025-03-11 18:50:43 UTC"
  },
  {
    "arxiv_id": "2503.08796v1",
    "title": "Robust Multi-Objective Controlled Decoding of Large Language Models",
    "authors": [
      "Seongho Son",
      "William Bankes",
      "Sangwoong Yoon",
      "Shyam Sundhar Ramesh",
      "Xiaohang Tang",
      "Ilija Bogunovic"
    ],
    "abstract": "Test-time alignment of Large Language Models (LLMs) to human preferences\noffers a flexible way to generate responses aligned to diverse objectives\nwithout extensive retraining of LLMs. Existing methods achieve alignment to\nmultiple objectives simultaneously (e.g., instruction-following, helpfulness,\nconciseness) by optimizing their corresponding reward functions. However, they\noften rely on predefined weights or optimize for averages, sacrificing one\nobjective for another and leading to unbalanced outcomes. To address this, we\nintroduce Robust Multi-Objective Decoding (RMOD), a novel inference-time\nalgorithm that optimizes for improving worst-case rewards. RMOD formalizes the\nrobust decoding problem as a maximin two-player game between reward weights and\nthe sampling policy, solving for the Nash equilibrium. We show that the game\nreduces to a convex optimization problem to find the worst-case weights, while\nthe best response policy can be computed analytically. We also introduce a\npractical RMOD variant designed for efficient decoding with contemporary LLMs,\nincurring minimal computational overhead compared to non-robust Multi-Objective\nDecoding (MOD) methods. Our experimental results showcase the effectiveness of\nRMOD in generating responses equitably aligned with diverse objectives,\noutperforming baselines up to 20%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08796v1",
    "published_date": "2025-03-11 18:15:26 UTC",
    "updated_date": "2025-03-11 18:15:26 UTC"
  },
  {
    "arxiv_id": "2503.08786v1",
    "title": "Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised Probabilistic Inference -- A Work In Progress",
    "authors": [
      "Sagad Hamid",
      "Tanya Braun"
    ],
    "abstract": "Efficient probabilistic inference by variable elimination in graphical models\nrequires an optimal elimination order. However, finding an optimal order is a\nchallenging combinatorial optimisation problem for models with a large number\nof random variables. Most recently, a reinforcement learning approach has been\nproposed to find efficient contraction orders in tensor networks. Due to the\nduality between graphical models and tensor networks, we adapt this approach to\nprobabilistic inference in graphical models. Furthermore, we incorporate\nstructure exploitation into the process of finding an optimal order. Currently,\nthe agent's cost function is formulated in terms of intermediate result sizes\nwhich are exponential in the number of indices (i.e., random variables). We\nshow that leveraging specific structures during inference allows for\nintroducing compact encodings of intermediate results which can be\nsignificantly smaller. By considering the compact encoding sizes for the cost\nfunction instead, we enable the agent to explore more efficient contraction\norders. The structure we consider in this work is the presence of local\nsymmetries (i.e., symmetries within a model's factors).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Contributed to: Sixth Data Science Meets Optimisation (DSO) Workshop\n  at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.08786v1",
    "published_date": "2025-03-11 18:00:23 UTC",
    "updated_date": "2025-03-11 18:00:23 UTC"
  },
  {
    "arxiv_id": "2503.08684v1",
    "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
    "authors": [
      "Haoyu Wang",
      "Sunhao Dai",
      "Haiyuan Zhao",
      "Liang Pang",
      "Xiao Zhang",
      "Gang Wang",
      "Zhenhua Dong",
      "Jun Xu",
      "Ji-Rong Wen"
    ],
    "abstract": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08684v1",
    "published_date": "2025-03-11 17:59:00 UTC",
    "updated_date": "2025-03-11 17:59:00 UTC"
  },
  {
    "arxiv_id": "2503.08683v1",
    "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
    "authors": [
      "Changxing Liu",
      "Genjia Liu",
      "Zijun Wang",
      "Jinchang Yang",
      "Siheng Chen"
    ],
    "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08683v1",
    "published_date": "2025-03-11 17:58:42 UTC",
    "updated_date": "2025-03-11 17:58:42 UTC"
  },
  {
    "arxiv_id": "2503.08764v1",
    "title": "Towards Interpretable Protein Structure Prediction with Sparse Autoencoders",
    "authors": [
      "Nithin Parsan",
      "David J. Yang",
      "John J. Yang"
    ],
    "abstract": "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https://github.com/johnyang101/reticular-sae , and\nvisualizer https://sae.reticular.ai .",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Published at the GEMBio ICLR 2025 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.08764v1",
    "published_date": "2025-03-11 17:57:29 UTC",
    "updated_date": "2025-03-11 17:57:29 UTC"
  },
  {
    "arxiv_id": "2503.08679v3",
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "authors": [
      "Iván Arcuschin",
      "Jett Janiak",
      "Robert Krzyzanowski",
      "Senthooran Rajamanoharan",
      "Neel Nanda",
      "Arthur Conmy"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages",
    "pdf_url": "http://arxiv.org/pdf/2503.08679v3",
    "published_date": "2025-03-11 17:56:30 UTC",
    "updated_date": "2025-03-19 19:20:42 UTC"
  },
  {
    "arxiv_id": "2503.08678v1",
    "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing",
    "authors": [
      "Yuanhao Wang",
      "Cheng Zhang",
      "Gonçalo Frazão",
      "Jinlong Yang",
      "Alexandru-Eugen Ichim",
      "Thabo Beeler",
      "Fernando De la Torre"
    ],
    "abstract": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Project Page: https://humansensinglab.github.io/garment-crafter/",
    "pdf_url": "http://arxiv.org/pdf/2503.08678v1",
    "published_date": "2025-03-11 17:56:03 UTC",
    "updated_date": "2025-03-11 17:56:03 UTC"
  },
  {
    "arxiv_id": "2503.08669v1",
    "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
    "authors": [
      "Zekun Li",
      "Shinda Huang",
      "Jiangtian Wang",
      "Nathan Zhang",
      "Antonis Antoniades",
      "Wenyue Hua",
      "Kaijie Zhu",
      "Sirui Zeng",
      "William Yang Wang",
      "Xifeng Yan"
    ],
    "abstract": "As language agents progressively automate critical tasks across domains,\ntheir ability to operate within operational constraints and safety protocols\nbecomes essential. While extensive research has demonstrated these agents'\neffectiveness in downstream task completion, their reliability in following\noperational procedures and constraints remains largely unexplored. To this end,\nwe present AgentOrca, a dual-system framework for evaluating language agents'\ncompliance with operational constraints and routines. Our framework encodes\naction constraints and routines through both natural language prompts for\nagents and corresponding executable code serving as ground truth for automated\nverification. Through an automated pipeline of test case generation and\nevaluation across five real-world domains, we quantitatively assess current\nlanguage agents' adherence to operational constraints. Our findings reveal\nnotable performance gaps among state-of-the-art models, with large reasoning\nmodels like o1 demonstrating superior compliance while others show\nsignificantly lower performance, particularly when encountering complex\nconstraints or user persuasion attempts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08669v1",
    "published_date": "2025-03-11 17:53:02 UTC",
    "updated_date": "2025-03-11 17:53:02 UTC"
  },
  {
    "arxiv_id": "2503.08665v1",
    "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
    "authors": [
      "Yitian Zhang",
      "Long Mai",
      "Aniruddha Mahapatra",
      "David Bourgin",
      "Yicong Hong",
      "Jonah Casebeer",
      "Feng Liu",
      "Yun Fu"
    ],
    "abstract": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
    "published_date": "2025-03-11 17:51:07 UTC",
    "updated_date": "2025-03-11 17:51:07 UTC"
  },
  {
    "arxiv_id": "2503.08664v1",
    "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
    "authors": [
      "Yuhan Wang",
      "Fangzhou Hong",
      "Shuai Yang",
      "Liming Jiang",
      "Wayne Wu",
      "Chen Change Loy"
    ],
    "abstract": "Multiview diffusion models have shown considerable success in image-to-3D\ngeneration for general objects. However, when applied to human data, existing\nmethods have yet to deliver promising results, largely due to the challenges of\nscaling multiview attention to higher resolutions. In this paper, we explore\nhuman multiview diffusion models at the megapixel level and introduce a\nsolution called mesh attention to enable training at 1024x1024 resolution.\nUsing a clothed human mesh as a central coarse geometric representation, the\nproposed mesh attention leverages rasterization and projection to establish\ndirect cross-view coordinate correspondences. This approach significantly\nreduces the complexity of multiview attention while maintaining cross-view\nconsistency. Building on this foundation, we devise a mesh attention block and\ncombine it with keypoint conditioning to create our human-specific multiview\ndiffusion model, MEAT. In addition, we present valuable insights into applying\nmultiview human motion videos for diffusion training, addressing the\nlongstanding issue of data scarcity. Extensive experiments show that MEAT\neffectively generates dense, consistent multiview human images at the megapixel\nlevel, outperforming existing multiview diffusion methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Code https://github.com/johannwyh/MEAT Project Page\n  https://johann.wang/MEAT/",
    "pdf_url": "http://arxiv.org/pdf/2503.08664v1",
    "published_date": "2025-03-11 17:50:59 UTC",
    "updated_date": "2025-03-11 17:50:59 UTC"
  },
  {
    "arxiv_id": "2503.08663v1",
    "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
    "authors": [
      "Pierre Sermanet",
      "Anirudha Majumdar",
      "Alex Irpan",
      "Dmitry Kalashnikov",
      "Vikas Sindhwani"
    ],
    "abstract": "Until recently, robotics safety research was predominantly about collision\navoidance and hazard reduction in the immediate vicinity of a robot. Since the\nadvent of large vision and language models (VLMs), robots are now also capable\nof higher-level semantic scene understanding and natural language interactions\nwith humans. Despite their known vulnerabilities (e.g. hallucinations or\njail-breaking), VLMs are being handed control of robots capable of physical\ncontact with the real world. This can lead to dangerous behaviors, making\nsemantic safety for robots a matter of immediate concern. Our contributions in\nthis paper are two fold: first, to address these emerging risks, we release the\nASIMOV Benchmark, a large-scale and comprehensive collection of datasets for\nevaluating and improving semantic safety of foundation models serving as robot\nbrains. Our data generation recipe is highly scalable: by leveraging text and\nimage generation techniques, we generate undesirable situations from real-world\nvisual scenes and human injury reports from hospitals. Secondly, we develop a\nframework to automatically generate robot constitutions from real-world data to\nsteer a robot's behavior using Constitutional AI mechanisms. We propose a novel\nauto-amending process that is able to introduce nuances in written rules of\nbehavior; this can lead to increased alignment with human preferences on\nbehavior desirability and safety. We explore trade-offs between generality and\nspecificity across a diverse set of constitutions of different lengths, and\ndemonstrate that a robot is able to effectively reject unconstitutional\nactions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using\ngenerated constitutions, outperforming no-constitution baselines and\nhuman-written constitutions. Data is available at asimov-benchmark.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08663v1",
    "published_date": "2025-03-11 17:50:47 UTC",
    "updated_date": "2025-03-11 17:50:47 UTC"
  },
  {
    "arxiv_id": "2503.08662v1",
    "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language Models",
    "authors": [
      "Pierpaolo Basile",
      "Lucia Siciliani",
      "Elio Musacchio",
      "Giovanni Semeraro"
    ],
    "abstract": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08662v1",
    "published_date": "2025-03-11 17:50:44 UTC",
    "updated_date": "2025-03-11 17:50:44 UTC"
  },
  {
    "arxiv_id": "2503.08644v1",
    "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
    "authors": [
      "Parishad BehnamGhader",
      "Nicholas Meade",
      "Siva Reddy"
    ],
    "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08644v1",
    "published_date": "2025-03-11 17:36:53 UTC",
    "updated_date": "2025-03-11 17:36:53 UTC"
  },
  {
    "arxiv_id": "2503.08643v2",
    "title": "Rethinking Diffusion Model in High Dimension",
    "authors": [
      "Zhenxin Zheng",
      "Zhenjie Zheng"
    ],
    "abstract": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDE, while aligning with the degraded\nobjective function. 3) Guided by this simple framework, more efficient\ninference methods can be discovered.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08643v2",
    "published_date": "2025-03-11 17:36:11 UTC",
    "updated_date": "2025-04-13 06:15:52 UTC"
  },
  {
    "arxiv_id": "2503.08638v1",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "authors": [
      "Ruibin Yuan",
      "Hanfeng Lin",
      "Shuyue Guo",
      "Ge Zhang",
      "Jiahao Pan",
      "Yongyi Zang",
      "Haohe Liu",
      "Yiming Liang",
      "Wenye Ma",
      "Xingjian Du",
      "Xinrun Du",
      "Zhen Ye",
      "Tianyu Zheng",
      "Yinghao Ma",
      "Minghao Liu",
      "Zeyue Tian",
      "Ziya Zhou",
      "Liumeng Xue",
      "Xingwei Qu",
      "Yizhi Li",
      "Shangda Wu",
      "Tianhao Shen",
      "Ziyang Ma",
      "Jun Zhan",
      "Chunhui Wang",
      "Yatian Wang",
      "Xiaowei Chi",
      "Xinyue Zhang",
      "Zhenzhu Yang",
      "Xiangzhou Wang",
      "Shansong Liu",
      "Lingrui Mei",
      "Peng Li",
      "Junjie Wang",
      "Jianwei Yu",
      "Guojian Pang",
      "Xu Li",
      "Zihao Wang",
      "Xiaohuan Zhou",
      "Lijun Yu",
      "Emmanouil Benetos",
      "Yong Chen",
      "Chenghua Lin",
      "Xie Chen",
      "Gus Xia",
      "Zhaoxiang Zhang",
      "Chao Zhang",
      "Wenhu Chen",
      "Xinyu Zhou",
      "Xipeng Qiu",
      "Roger Dannenberg",
      "Jiaheng Liu",
      "Jian Yang",
      "Wenhao Huang",
      "Wei Xue",
      "Xu Tan",
      "Yike Guo"
    ],
    "abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "https://github.com/multimodal-art-projection/YuE",
    "pdf_url": "http://arxiv.org/pdf/2503.08638v1",
    "published_date": "2025-03-11 17:26:50 UTC",
    "updated_date": "2025-03-11 17:26:50 UTC"
  },
  {
    "arxiv_id": "2503.10683v1",
    "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
    "authors": [
      "Zak Buzzard"
    ],
    "abstract": "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.10683v1",
    "published_date": "2025-03-11 17:18:01 UTC",
    "updated_date": "2025-03-11 17:18:01 UTC"
  },
  {
    "arxiv_id": "2503.08609v1",
    "title": "Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion",
    "authors": [
      "Mehdi Hosseini Chagahi",
      "Niloufar Delfan",
      "Behzad Moshiri",
      "Md. Jalil Piran",
      "Jaber Hatam Parikhan"
    ],
    "abstract": "Intracranial hemorrhage (ICH) is a critical medical emergency caused by the\nrupture of cerebral blood vessels, leading to internal bleeding within the\nskull. Accurate and timely classification of hemorrhage subtypes is essential\nfor effective clinical decision-making. To address this challenge, we propose\nan advanced pyramid vision transformer (PVT)-based model, leveraging its\nhierarchical attention mechanisms to capture both local and global spatial\ndependencies in brain CT scans. Instead of processing all extracted features\nindiscriminately, A SHAP-based feature selection method is employed to identify\nthe most discriminative components, which are then used as a latent feature\nspace to train a boosting neural network, reducing computational complexity. We\nintroduce an entropy-aware aggregation strategy along with a fuzzy integral\noperator to fuse information across multiple CT slices, ensuring a more\ncomprehensive and reliable scan-level diagnosis by accounting for inter-slice\ndependencies. Experimental results show that our PVT-based framework\nsignificantly outperforms state-of-the-art deep learning architectures in terms\nof classification accuracy, precision, and robustness. By combining SHAP-driven\nfeature selection, transformer-based modeling, and an entropy-aware fuzzy\nintegral operator for decision fusion, our method offers a scalable and\ncomputationally efficient AI-driven solution for automated ICH subtype\nclassification.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08609v1",
    "published_date": "2025-03-11 16:47:32 UTC",
    "updated_date": "2025-03-11 16:47:32 UTC"
  },
  {
    "arxiv_id": "2503.08608v1",
    "title": "A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps",
    "authors": [
      "Sven Krausse",
      "Emre Neftci",
      "Friedrich T. Sommer",
      "Alpha Renner"
    ],
    "abstract": "The entorhinal-hippocampal formation is the mammalian brain's navigation\nsystem, encoding both physical and abstract spaces via grid cells. This system\nis well-studied in neuroscience, and its efficiency and versatility make it\nattractive for applications in robotics and machine learning. While continuous\nattractor networks (CANs) successfully model entorhinal grid cells for encoding\nphysical space, integrating both continuous spatial and abstract spatial\ncomputations into a unified framework remains challenging. Here, we attempt to\nbridge this gap by proposing a mechanistic model for versatile information\nprocessing in the entorhinal-hippocampal formation inspired by CANs and Vector\nSymbolic Architectures (VSAs), a neuro-symbolic computing framework. The novel\ngrid-cell VSA (GC-VSA) model employs a spatially structured encoding scheme\nwith 3D neuronal modules mimicking the discrete scales and orientations of grid\ncell modules, reproducing their characteristic hexagonal receptive fields. In\nexperiments, the model demonstrates versatility in spatial and abstract tasks:\n(1) accurate path integration for tracking locations, (2) spatio-temporal\nrepresentation for querying object locations and temporal relations, and (3)\nsymbolic reasoning using family trees as a structured test case for\nhierarchical relationships.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "10 pages, 5 figures, accepted at the 2025 Neuro Inspired\n  Computational Elements (NICE) conference",
    "pdf_url": "http://arxiv.org/pdf/2503.08608v1",
    "published_date": "2025-03-11 16:45:52 UTC",
    "updated_date": "2025-03-11 16:45:52 UTC"
  },
  {
    "arxiv_id": "2503.08605v1",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling",
    "authors": [
      "Subin Kim",
      "Seoung Wug Oh",
      "Jui-Hsien Wang",
      "Joon-Young Lee",
      "Jinwoo Shin"
    ],
    "abstract": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page with visuals: https://syncos2025.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.08605v1",
    "published_date": "2025-03-11 16:43:45 UTC",
    "updated_date": "2025-03-11 16:43:45 UTC"
  },
  {
    "arxiv_id": "2503.08604v2",
    "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments",
    "authors": [
      "Dongping Li",
      "Tielong Cai",
      "Tianci Tang",
      "Wenhao Chai",
      "Katherine Rose Driggs-Campbell",
      "Gaoang Wang"
    ],
    "abstract": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of humanity. While advancements in large language models (LLMs)\nand embodied intelligence make this goal closer, several challenges persist:\nthe lack of a unified benchmark for more complex robot tasks, limited\nevaluation methods and metrics, data incompatibility between LLMs and mobile\nmanipulation trajectories. To address these issues, we propose Embodied Mobile\nManipulation in Open Environments (EMMOE), a benchmark that requires agents to\ninterpret user instructions and execute long-horizon everyday tasks in\ncontinuous space. EMMOE seamlessly integrates high-level and low-level embodied\ntasks into a unified framework, along with three new metrics for more diverse\nassessment. Additionally, we collect~\\dataset, which features in various task\nattributes, detailed process annotations, re-plans after failures, and two\nsub-datasets for LLM training. Furthermore, we design~\\model, a sophisticated\nagent system consists of LLM with Direct Preference Optimization (DPO), light\nweighted navigation and manipulation models, and multiple error detection\nmechanisms. Finally, we demonstrate~\\model's performance and evaluations of\ndifferent models and policies.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08604v2",
    "published_date": "2025-03-11 16:42:36 UTC",
    "updated_date": "2025-05-15 01:34:30 UTC"
  },
  {
    "arxiv_id": "2503.08762v1",
    "title": "Neurosymbolic Decision Trees",
    "authors": [
      "Matthias Möller",
      "Arvid Norlander",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ],
    "abstract": "Neurosymbolic (NeSy) AI studies the integration of neural networks (NNs) and\nsymbolic reasoning based on logic. Usually, NeSy techniques focus on learning\nthe neural, probabilistic and/or fuzzy parameters of NeSy models. Learning the\nsymbolic or logical structure of such models has, so far, received less\nattention. We introduce neurosymbolic decision trees (NDTs), as an extension of\ndecision trees together with a novel NeSy structure learning algorithm, which\nwe dub NeuID3. NeuID3 adapts the standard top-down induction of decision tree\nalgorithms and combines it with a neural probabilistic logic representation,\ninherited from the DeepProbLog family of models. The key advantage of learning\nNDTs with NeuID3 is the support of both symbolic and subsymbolic data (such as\nimages), and that they can exploit background knowledge during the induction of\nthe tree structure, In our experimental evaluation we demonstrate the benefits\nof NeSys structure learning over more traditonal approaches such as purely\ndata-driven learning with neural networks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08762v1",
    "published_date": "2025-03-11 16:40:38 UTC",
    "updated_date": "2025-03-11 16:40:38 UTC"
  },
  {
    "arxiv_id": "2503.16498v1",
    "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
    "authors": [
      "Enzo Sinacola",
      "Arnault Pachot",
      "Thierry Petit"
    ],
    "abstract": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted, proceedings of the 17th International Conference on Machine\n  Learning and Computing",
    "pdf_url": "http://arxiv.org/pdf/2503.16498v1",
    "published_date": "2025-03-11 16:27:20 UTC",
    "updated_date": "2025-03-11 16:27:20 UTC"
  },
  {
    "arxiv_id": "2503.08588v1",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "authors": [
      "Xin Xu",
      "Wei Xu",
      "Ningyu Zhang",
      "Julian McAuley"
    ],
    "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by TrustNLP @ NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08588v1",
    "published_date": "2025-03-11 16:25:36 UTC",
    "updated_date": "2025-03-11 16:25:36 UTC"
  },
  {
    "arxiv_id": "2503.16497v1",
    "title": "Effective Yet Ephemeral Propaganda Defense: There Needs to Be More than One-Shot Inoculation to Enhance Critical Thinking",
    "authors": [
      "Nicolas Hoferer",
      "Kilian Sprenkamp",
      "Dorian Christoph Quelle",
      "Daniel Gordon Jones",
      "Zoya Katashinskaya",
      "Alexandre Bovet",
      "Liudmila Zavolokina"
    ],
    "abstract": "In today's media landscape, propaganda distribution has a significant impact\non society. It sows confusion, undermines democratic processes, and leads to\nincreasingly difficult decision-making for news readers. We investigate the\nlasting effect on critical thinking and propaganda awareness on them when using\na propaganda detection and contextualization tool. Building on inoculation\ntheory, which suggests that preemptively exposing individuals to weakened forms\nof propaganda can improve their resilience against it, we integrate Kahneman's\ndual-system theory to measure the tools' impact on critical thinking. Through a\ntwo-phase online experiment, we measure the effect of several inoculation\ndoses. Our findings show that while the tool increases critical thinking during\nits use, this increase vanishes without access to the tool. This indicates a\nsingle use of the tool does not create a lasting impact. We discuss the\nimplications and propose possible approaches to improve the resilience against\npropaganda in the long-term.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16497v1",
    "published_date": "2025-03-11 16:24:19 UTC",
    "updated_date": "2025-03-11 16:24:19 UTC"
  },
  {
    "arxiv_id": "2503.08581v2",
    "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
    "authors": [
      "Jiangping Wen",
      "Jinyu Wen",
      "Meie Fang"
    ],
    "abstract": "Bag-based Multiple Instance Learning (MIL) approaches have emerged as the\nmainstream methodology for Whole Slide Image (WSI) classification. However,\nmost existing methods adopt a segmented training strategy, which first extracts\nfeatures using a pre-trained feature extractor and then aggregates these\nfeatures through MIL. This segmented training approach leads to insufficient\ncollaborative optimization between the feature extraction network and the MIL\nnetwork, preventing end-to-end joint optimization and thereby limiting the\noverall performance of the model. Additionally, conventional methods typically\nextract features from all patches of fixed size, ignoring the multi-scale\nobservation characteristics of pathologists. This not only results in\nsignificant computational resource waste when tumor regions represent a minimal\nproportion (as in the Camelyon16 dataset) but may also lead the model to\nsuboptimal solutions.\n  To address these limitations, this paper proposes an end-to-end multi-scale\nWSI classification framework that integrates multi-scale feature extraction\nwith multiple instance learning. Specifically, our approach includes: (1) a\nsemantic feature filtering module to reduce interference from non-lesion areas;\n(2) a multi-scale feature extraction module to capture pathological information\nat different levels; and (3) a multi-scale fusion MIL module for global\nmodeling and feature integration. Through an end-to-end training strategy, we\nsimultaneously optimize both the feature extractor and MIL network, ensuring\nmaximum compatibility between them.\n  Experiments were conducted on three cross-center datasets (DigestPath2019,\nBCNB, and UBC-OCEAN). Results demonstrate that our proposed method outperforms\nexisting state-of-the-art approaches in terms of both accuracy (ACC) and AUC\nmetrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "summited to ICCV2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08581v2",
    "published_date": "2025-03-11 16:16:44 UTC",
    "updated_date": "2025-03-12 09:27:31 UTC"
  },
  {
    "arxiv_id": "2503.08760v1",
    "title": "Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes",
    "authors": [
      "Keyue Jiang",
      "Bohan Tang",
      "Xiaowen Dong",
      "Laura Toni"
    ],
    "abstract": "Inferring the graph structure from observed data is a key task in graph\nmachine learning to capture the intrinsic relationship between data entities.\nWhile significant advancements have been made in learning the structure of\nhomogeneous graphs, many real-world graphs exhibit heterogeneous patterns where\nnodes and edges have multiple types. This paper fills this gap by introducing\nthe first approach for heterogeneous graph structure learning (HGSL). To this\nend, we first propose a novel statistical model for the data-generating process\n(DGP) of heterogeneous graph data, namely hidden Markov networks for\nheterogeneous graphs (H2MN). Then we formalize HGSL as a maximum a-posterior\nestimation problem parameterized by such DGP and derive an alternating\noptimization method to obtain a solution together with a theoretical\njustification of the optimization conditions. Finally, we conduct extensive\nexperiments on both synthetic and real-world datasets to demonstrate that our\nproposed method excels in learning structure on heterogeneous graphs in terms\nof edge type identification and edge weight recovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08760v1",
    "published_date": "2025-03-11 16:14:53 UTC",
    "updated_date": "2025-03-11 16:14:53 UTC"
  },
  {
    "arxiv_id": "2503.08565v1",
    "title": "When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research",
    "authors": [
      "Willem van der Maden",
      "Vera van der Burg",
      "Brett A. Halperin",
      "Petra Jääskeläinen",
      "Joseph Lindley",
      "Derek Lomas",
      "Timothy Merritt"
    ],
    "abstract": "This essay examines how Generative AI (GenAI) is rapidly transforming design\npractices and how discourse often falls into over-simplified narratives that\nimpede meaningful research and practical progress. We identify and deconstruct\nfive prevalent \"semantic stopsigns\" -- reductive framings about GenAI in design\nthat halt deeper inquiry and limit productive engagement. Reflecting upon two\nexpert workshops at ACM conferences and semi-structured interviews with design\npractitioners, we analyze how these stopsigns manifest in research and\npractice. Our analysis develops mid-level knowledge that bridges theoretical\ndiscourse and practical implementation, helping designers and researchers\ninterrogate common assumptions about GenAI in their own contexts. By recasting\nthese stopsigns into more nuanced frameworks, we provide the design research\ncommunity with practical approaches for thinking about and working with these\nemerging technologies.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08565v1",
    "published_date": "2025-03-11 15:54:03 UTC",
    "updated_date": "2025-03-11 15:54:03 UTC"
  },
  {
    "arxiv_id": "2503.08564v2",
    "title": "MoE-Loco: Mixture of Experts for Multitask Locomotion",
    "authors": [
      "Runhan Huang",
      "Shaoting Zhu",
      "Yilun Du",
      "Hang Zhao"
    ],
    "abstract": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08564v2",
    "published_date": "2025-03-11 15:53:54 UTC",
    "updated_date": "2025-05-21 02:51:53 UTC"
  },
  {
    "arxiv_id": "2503.08558v2",
    "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
    "authors": [
      "Chen Xu",
      "Tony Khuong Nguyen",
      "Emma Dixon",
      "Christopher Rodriguez",
      "Patrick Miller",
      "Robert Lee",
      "Paarth Shah",
      "Rares Ambrus",
      "Haruki Nishimura",
      "Masha Itkina"
    ],
    "abstract": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08558v2",
    "published_date": "2025-03-11 15:47:12 UTC",
    "updated_date": "2025-04-25 07:12:28 UTC"
  },
  {
    "arxiv_id": "2503.08551v1",
    "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
    "authors": [
      "Wanyong Feng",
      "Peter Tran",
      "Stephen Sireci",
      "Andrew Lan"
    ],
    "abstract": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08551v1",
    "published_date": "2025-03-11 15:39:43 UTC",
    "updated_date": "2025-03-11 15:39:43 UTC"
  },
  {
    "arxiv_id": "2503.08549v1",
    "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation",
    "authors": [
      "Xian Gao",
      "Zongyun Zhang",
      "Mingye Xie",
      "Ting Liu",
      "Yuzhuo Fu"
    ],
    "abstract": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.08549v1",
    "published_date": "2025-03-11 15:36:38 UTC",
    "updated_date": "2025-03-11 15:36:38 UTC"
  },
  {
    "arxiv_id": "2503.08542v1",
    "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering",
    "authors": [
      "Sher Badshah",
      "Hassan Sajjad"
    ],
    "abstract": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.0; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08542v1",
    "published_date": "2025-03-11 15:29:55 UTC",
    "updated_date": "2025-03-11 15:29:55 UTC"
  },
  {
    "arxiv_id": "2503.08540v1",
    "title": "Mellow: a small audio language model for reasoning",
    "authors": [
      "Soham Deshmukh",
      "Satvik Dixit",
      "Rita Singh",
      "Bhiksha Raj"
    ],
    "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Checkpoint and dataset available at:\n  https://github.com/soham97/mellow",
    "pdf_url": "http://arxiv.org/pdf/2503.08540v1",
    "published_date": "2025-03-11 15:29:00 UTC",
    "updated_date": "2025-03-11 15:29:00 UTC"
  },
  {
    "arxiv_id": "2503.08537v1",
    "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation",
    "authors": [
      "Andres M Bran",
      "Theo A Neukomm",
      "Daniel P Armstrong",
      "Zlatko Jončev",
      "Philippe Schwaller"
    ],
    "abstract": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08537v1",
    "published_date": "2025-03-11 15:27:17 UTC",
    "updated_date": "2025-03-11 15:27:17 UTC"
  },
  {
    "arxiv_id": "2503.08525v1",
    "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
    "authors": [
      "Tong Wei",
      "Yijun Yang",
      "Junliang Xing",
      "Yuanchun Shi",
      "Zongqing Lu",
      "Deheng Ye"
    ],
    "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08525v1",
    "published_date": "2025-03-11 15:17:02 UTC",
    "updated_date": "2025-03-11 15:17:02 UTC"
  },
  {
    "arxiv_id": "2503.14514v1",
    "title": "Acceptance or Rejection of Lots while Minimizing and Controlling Type I and Type II Errors",
    "authors": [
      "Edson Luiz Ursini",
      "Elaine Cristina Catapani Poletti",
      "Loreno Menezes da Silveira",
      "José Roberto Emiliano Leite"
    ],
    "abstract": "The double hypothesis test (DHT) is a test that allows controlling Type I\n(producer) and Type II (consumer) errors. It is possible to say whether the\nbatch has a defect rate, p, between 1.5 and 2%, or between 2 and 5%, or between\n5 and 10%, and so on, until finding a required value for this probability.\nUsing the two probabilities side by side, the Type I error for the lower\nprobability distribution and the Type II error for the higher probability\ndistribution, both can be controlled and minimized. It can be applied in the\ndevelopment or manufacturing process of a batch of components, or in the case\nof purchasing from a supplier, when the percentage of defects (p) is unknown,\nconsidering the technology and/or process available to obtain them. The power\nof the test is amplified by the joint application of the Limit of Successive\nFailures (LSF) related to the Renewal Theory. To enable the choice of the most\nappropriate algorithm for each application. Four distributions are proposed for\nthe Bernoulli event sequence, including their computational efforts: Binomial,\nBinomial approximated by Poisson, and Binomial approximated by Gaussian (with\ntwo variants). Fuzzy logic rules are also applied to facilitate\ndecision-making.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14514v1",
    "published_date": "2025-03-11 15:02:45 UTC",
    "updated_date": "2025-03-11 15:02:45 UTC"
  },
  {
    "arxiv_id": "2503.08489v2",
    "title": "A Triple-Inertial Accelerated Alternating Optimization Method for Deep Learning Training",
    "authors": [
      "Chengcheng Yan",
      "Jiawei Xu",
      "Qingsong Wang",
      "Zheng Peng"
    ],
    "abstract": "The stochastic gradient descent (SGD) algorithm has achieved remarkable\nsuccess in training deep learning models. However, it has several limitations,\nincluding susceptibility to vanishing gradients, sensitivity to input data, and\na lack of robust theoretical guarantees. In recent years, alternating\nminimization (AM) methods have emerged as a promising alternative for model\ntraining by employing gradient-free approaches to iteratively update model\nparameters. Despite their potential, these methods often exhibit slow\nconvergence rates. To address this challenge, we propose a novel\nTriple-Inertial Accelerated Alternating Minimization (TIAM) framework for\nneural network training. The TIAM approach incorporates a triple-inertial\nacceleration strategy with a specialized approximation method, facilitating\ntargeted acceleration of different terms in each sub-problem optimization. This\nintegration improves the efficiency of convergence, achieving superior\nperformance with fewer iterations. Additionally, we provide a convergence\nanalysis of the TIAM algorithm, including its global convergence properties and\nconvergence rate. Extensive experiments validate the effectiveness of the TIAM\nmethod, showing significant improvements in generalization capability and\ncomputational efficiency compared to existing approaches, particularly when\napplied to the rectified linear unit (ReLU) and its variants.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08489v2",
    "published_date": "2025-03-11 14:42:17 UTC",
    "updated_date": "2025-03-13 12:57:09 UTC"
  },
  {
    "arxiv_id": "2503.08472v1",
    "title": "Optimizing Ride-Pooling Operations with Extended Pickup and Drop-Off Flexibility",
    "authors": [
      "Hao Jiang",
      "Yixing Xu",
      "Pradeep Varakantham"
    ],
    "abstract": "The Ride-Pool Matching Problem (RMP) is central to on-demand ride-pooling\nservices, where vehicles must be matched with multiple requests while adhering\nto service constraints such as pickup delays, detour limits, and vehicle\ncapacity. Most existing RMP solutions assume passengers are picked up and\ndropped off at their original locations, neglecting the potential for\npassengers to walk to nearby spots to meet vehicles. This assumption restricts\nthe optimization potential in ride-pooling operations. In this paper, we\npropose a novel matching method that incorporates extended pickup and drop-off\nareas for passengers. We first design a tree-based approach to efficiently\ngenerate feasible matches between passengers and vehicles. Next, we optimize\nvehicle routes to cover all designated pickup and drop-off locations while\nminimizing total travel distance. Finally, we employ dynamic assignment\nstrategies to achieve optimal matching outcomes. Experiments on city-scale taxi\ndatasets demonstrate that our method improves the number of served requests by\nup to 13\\% and average travel distance by up to 21\\% compared to leading\nexisting solutions, underscoring the potential of leveraging passenger mobility\nto significantly enhance ride-pooling service efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08472v1",
    "published_date": "2025-03-11 14:17:30 UTC",
    "updated_date": "2025-03-11 14:17:30 UTC"
  },
  {
    "arxiv_id": "2503.08467v1",
    "title": "Accelerating MoE Model Inference with Expert Sharding",
    "authors": [
      "Oana Balmau",
      "Anne-Marie Kermarrec",
      "Rafael Pires",
      "André Loureiro Espírito Santo",
      "Martijn de Vos",
      "Milos Vujasinovic"
    ],
    "abstract": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the proceedings of the 5th Workshop on Machine Learning\n  and Systems (EuroMLSys 25)",
    "pdf_url": "http://arxiv.org/pdf/2503.08467v1",
    "published_date": "2025-03-11 14:15:01 UTC",
    "updated_date": "2025-03-11 14:15:01 UTC"
  },
  {
    "arxiv_id": "2503.10679v2",
    "title": "End-to-end Learning of Sparse Interventions on Activations to Steer Generation",
    "authors": [
      "Pau Rodriguez",
      "Michal Klein",
      "Eleonora Gualdoni",
      "Arno Blaas",
      "Luca Zappella",
      "Marco Cuturi",
      "Xavier Suau"
    ],
    "abstract": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10679v2",
    "published_date": "2025-03-11 14:09:04 UTC",
    "updated_date": "2025-04-04 11:17:20 UTC"
  },
  {
    "arxiv_id": "2503.08460v2",
    "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
    "authors": [
      "Olga Meyer",
      "Marvin Boell",
      "Christoph Legat"
    ],
    "abstract": "The rapid development of Industry 4.0 technologies requires robust and\ncomprehensive standardization to ensure interoperability, safety and efficiency\nin the Industry of the Future. This paper examines the fundamental role and\nfunctionality of standardization, with a particular focus on its importance in\nEurope's regulatory framework. Based on this, selected topics in context of\nstandardization activities in context intelligent manufacturing and digital\ntwins are highlighted and, by that, an overview of the Industry 4.0 standards\nframework is provided. This paper serves both as an informative guide to the\nexisting standards in Industry 4.0 with respect to Artificial Intelligence and\nDigital Twins, and as a call to action for increased cooperation between\nstandardization bodies and the research community. By fostering such\ncollaboration, we aim to facilitate the continued development and\nimplementation of standards that will drive innovation and progress in the\nmanufacturing sector.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08460v2",
    "published_date": "2025-03-11 14:08:57 UTC",
    "updated_date": "2025-03-12 09:30:52 UTC"
  },
  {
    "arxiv_id": "2503.08455v1",
    "title": "Controlling Latent Diffusion Using Latent CLIP",
    "authors": [
      "Jason Becker",
      "Chris Wendler",
      "Peter Baylies",
      "Robert West",
      "Christian Wressnegger"
    ],
    "abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08455v1",
    "published_date": "2025-03-11 14:04:29 UTC",
    "updated_date": "2025-03-11 14:04:29 UTC"
  },
  {
    "arxiv_id": "2503.08437v1",
    "title": "ICPR 2024 Competition on Rider Intention Prediction",
    "authors": [
      "Shankar Gangisetty",
      "Abdul Wasi",
      "Shyam Nandan Rai",
      "C. V. Jawahar",
      "Sajay Raj",
      "Manish Prajapati",
      "Ayesha Choudhary",
      "Aaryadev Chandra",
      "Dev Chandan",
      "Shireen Chand",
      "Suvaditya Mukherjee"
    ],
    "abstract": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08437v1",
    "published_date": "2025-03-11 13:50:37 UTC",
    "updated_date": "2025-03-11 13:50:37 UTC"
  },
  {
    "arxiv_id": "2503.08417v1",
    "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
    "authors": [
      "Kwan Yun",
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Junyong Noh"
    ],
    "abstract": "Despite recent advancements in learning-based motion in-betweening, a key\nlimitation has been overlooked: the requirement for character-specific\ndatasets. In this work, we introduce AnyMoLe, a novel method that addresses\nthis limitation by leveraging video diffusion models to generate motion\nin-between frames for arbitrary characters without external data. Our approach\nemploys a two-stage frame generation process to enhance contextual\nunderstanding. Furthermore, to bridge the domain gap between real-world and\nrendered character animations, we introduce ICAdapt, a fine-tuning technique\nfor video diffusion models. Additionally, we propose a ``motion-video\nmimicking'' optimization technique, enabling seamless motion generation for\ncharacters with arbitrary joint structures using 2D and 3D-aware features.\nAnyMoLe significantly reduces data dependency while generating smooth and\nrealistic transitions, making it applicable to a wide range of motion\nin-betweening tasks.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "68U05",
      "I.3.7; I.4.9"
    ],
    "primary_category": "cs.GR",
    "comment": "11 pages, 10 figures, CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08417v1",
    "published_date": "2025-03-11 13:28:59 UTC",
    "updated_date": "2025-03-11 13:28:59 UTC"
  },
  {
    "arxiv_id": "2503.14513v1",
    "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition",
    "authors": [
      "Seyed Muhammad Hossein Mousavi"
    ],
    "abstract": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "A.I"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.14513v1",
    "published_date": "2025-03-11 13:16:30 UTC",
    "updated_date": "2025-03-11 13:16:30 UTC"
  },
  {
    "arxiv_id": "2503.08750v1",
    "title": "Exposing Product Bias in LLM Investment Recommendation",
    "authors": [
      "Yuhan Zhi",
      "Xiaoyu Zhang",
      "Longtian Wang",
      "Shumin Jiang",
      "Shiqing Ma",
      "Xiaohong Guan",
      "Chao Shen"
    ],
    "abstract": "Large language models (LLMs), as a new generation of recommendation engines,\npossess powerful summarization and data analysis capabilities, surpassing\ntraditional recommendation systems in both scope and performance. One promising\napplication is investment recommendation. In this paper, we reveal a novel\nproduct bias in LLM investment recommendation, where LLMs exhibit systematic\npreferences for specific products. Such preferences can subtly influence user\ninvestment decisions, potentially leading to inflated valuations of products\nand financial bubbles, posing risks to both individual investors and market\nstability. To comprehensively study the product bias, we develop an automated\npipeline to create a dataset of 567,000 samples across five asset classes\n(stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this\ndataset, we present the bf first study on product bias in LLM investment\nrecommendations. Our findings reveal that LLMs exhibit clear product\npreferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from\nMicrosoft). Notably, this bias persists even after applying debiasing\ntechniques. We urge AI researchers to take heed of the product bias in LLM\ninvestment recommendations and its implications, ensuring fairness and security\nin the digital space and market.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08750v1",
    "published_date": "2025-03-11 13:10:00 UTC",
    "updated_date": "2025-03-11 13:10:00 UTC"
  },
  {
    "arxiv_id": "2503.08749v1",
    "title": "Source-free domain adaptation based on label reliability for cross-domain bearing fault diagnosis",
    "authors": [
      "Wenyi Wu",
      "Hao Zhang",
      "Zhisen Wei",
      "Xiao-Yuan Jing",
      "Qinghua Zhang",
      "Songsong Wu"
    ],
    "abstract": "Source-free domain adaptation (SFDA) has been exploited for cross-domain\nbearing fault diagnosis without access to source data. Current methods select\npartial target samples with reliable pseudo-labels for model adaptation, which\nis sub-optimal due to the ignored target samples. We argue that every target\nsample can contribute to model adaptation, and accordingly propose in this\npaper a novel SFDA-based approach for bearing fault diagnosis that exploits\nboth reliable and unreliable pseudo-labels. We develop a\ndata-augmentation-based label voting strategy to divide the target samples into\nreliable and unreliable ones. We propose to explore the underlying relation\nbetween feature space and label space by using the reliable pseudo-labels as\nground-truth labels, meanwhile, alleviating negative transfer by maximizing the\nentropy of the unreliable pseudo-labels. The proposed method achieves\nwell-balance between discriminability and diversity by taking advantage of\nreliable and unreliable pseudo-labels. Extensive experiments are conducted on\ntwo bearing fault benchmarks, demonstrating that our approach achieves\nsignificant performance improvements against existing SFDA-based bearing fault\ndiagnosis methods. Our code is available at https://github.com/BdLab405/SDALR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 12 figures and 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.08749v1",
    "published_date": "2025-03-11 13:02:18 UTC",
    "updated_date": "2025-03-11 13:02:18 UTC"
  },
  {
    "arxiv_id": "2503.08388v1",
    "title": "V-Max: Making RL practical for Autonomous Driving",
    "authors": [
      "Valentin Charraut",
      "Thomas Tournaire",
      "Waël Doulazmi",
      "Thibault Buhet"
    ],
    "abstract": "Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets. V-Max integrates a set of observation and\nreward functions, transformer-based encoders, and training pipelines.\nAdditionally, it includes adversarial evaluation settings and an extensive set\nof evaluation metrics. Through a large-scale benchmark, we analyze how network\narchitectures, observation functions, training data, and reward shaping impact\nRL performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08388v1",
    "published_date": "2025-03-11 12:53:24 UTC",
    "updated_date": "2025-03-11 12:53:24 UTC"
  },
  {
    "arxiv_id": "2503.08381v1",
    "title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction",
    "authors": [
      "Benjamin Kempinski",
      "Tal Kachman"
    ],
    "abstract": "Power indices are essential in assessing the contribution and influence of\nindividual agents in multi-agent systems, providing crucial insights into\ncollaborative dynamics and decision-making processes. While invaluable,\ntraditional computational methods for exact or estimated power indices values\nrequire significant time and computational constraints, especially for large\n$(n\\ge10)$ coalitions. These constraints have historically limited researchers'\nability to analyse complex multi-agent interactions comprehensively. To address\nthis limitation, we introduce a novel Neural Networks-based approach that\nefficiently estimates power indices for voting games, demonstrating comparable\nand often superiour performance to existing tools in terms of both speed and\naccuracy. This method not only addresses existing computational bottlenecks,\nbut also enables rapid analysis of large coalitions, opening new avenues for\nmulti-agent system research by overcoming previous computational limitations\nand providing researchers with a more accessible, scalable analytical tool.This\nincreased efficiency will allow for the analysis of more complex and realistic\nmulti-agent scenarios.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2; F.2.1"
    ],
    "primary_category": "cs.MA",
    "comment": "20 pages main text + 6 pages appendix, 11 figures. Accepted to\n  IntelliSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08381v1",
    "published_date": "2025-03-11 12:40:42 UTC",
    "updated_date": "2025-03-11 12:40:42 UTC"
  },
  {
    "arxiv_id": "2503.08354v2",
    "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis",
    "authors": [
      "Kai Qiu",
      "Xiang Li",
      "Jason Kuen",
      "Hao Chen",
      "Xiaohao Xu",
      "Jiuxiang Gu",
      "Yinyi Luo",
      "Bhiksha Raj",
      "Zhe Lin",
      "Marios Savvides"
    ],
    "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 13 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.08354v2",
    "published_date": "2025-03-11 12:09:11 UTC",
    "updated_date": "2025-03-17 17:54:40 UTC"
  },
  {
    "arxiv_id": "2503.08332v1",
    "title": "MINT-Demo: Membership Inference Test Demonstrator",
    "authors": [
      "Daniel DeAlcala",
      "Aythami Morales",
      "Julian Fierrez",
      "Gonzalo Mancera",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez"
    ],
    "abstract": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Demo Paper Presented at Demo Track CVPR 24' and at AAAI 25' AIGOV\n  workshop",
    "pdf_url": "http://arxiv.org/pdf/2503.08332v1",
    "published_date": "2025-03-11 11:45:05 UTC",
    "updated_date": "2025-03-11 11:45:05 UTC"
  },
  {
    "arxiv_id": "2503.08327v1",
    "title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation",
    "authors": [
      "José Pombal",
      "Nuno M. Guerreiro",
      "Ricardo Rei",
      "André F. T. Martins"
    ],
    "abstract": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (Mint), i.e., the use of the same\nor related metrics for both model tuning and evaluation. Mint can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of Mint in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMint strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for -- questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MintAdjust, a method for more reliable\nevaluation under Mint. On the WMT24 MT shared task test set, MintAdjust ranks\ntranslations and systems more accurately than state-of-the-art-metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMintAdjust outperforms AutoRank, the ensembling method used by the organizers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08327v1",
    "published_date": "2025-03-11 11:40:10 UTC",
    "updated_date": "2025-03-11 11:40:10 UTC"
  },
  {
    "arxiv_id": "2503.08325v1",
    "title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data",
    "authors": [
      "Lele Qi",
      "Mengna Liu",
      "Xu Cheng",
      "Fan Shi",
      "Xiufeng Liu",
      "Shengyong Chen"
    ],
    "abstract": "Wind farms, typically in high-latitude regions, face a high risk of blade\nicing. Traditional centralized training methods raise serious privacy concerns.\nTo enhance data privacy in detecting wind turbine blade icing, traditional\nfederated learning (FL) is employed. However, data heterogeneity, resulting\nfrom collections across wind farms in varying environmental conditions, impacts\nthe model's optimization capabilities. Moreover, imbalances in wind turbine\ndata lead to models that tend to favor recognizing majority classes, thus\nneglecting critical icing anomalies. To tackle these challenges, we propose a\nfederated prototype learning model for class-imbalanced data in heterogeneous\nenvironments to detect wind turbine blade icing. We also propose a contrastive\nsupervised loss function to address the class imbalance problem. Experiments on\nreal data from 20 turbines across two wind farms show our method outperforms\nfive FL models and five class imbalance methods, with an average improvement of\n19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best\nmethod, BiFL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08325v1",
    "published_date": "2025-03-11 11:37:43 UTC",
    "updated_date": "2025-03-11 11:37:43 UTC"
  },
  {
    "arxiv_id": "2503.08322v1",
    "title": "Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs",
    "authors": [
      "Hector Kohler",
      "Quentin Delfosse",
      "Waris Radji",
      "Riad Akrour",
      "Philippe Preux"
    ],
    "abstract": "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages of main text, under review",
    "pdf_url": "http://arxiv.org/pdf/2503.08322v1",
    "published_date": "2025-03-11 11:34:06 UTC",
    "updated_date": "2025-03-11 11:34:06 UTC"
  },
  {
    "arxiv_id": "2503.08308v1",
    "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
    "authors": [
      "Zhuo Zhi",
      "Chen Feng",
      "Adam Daneshmend",
      "Mine Orlu",
      "Andreas Demosthenous",
      "Lu Yin",
      "Da Li",
      "Ziquan Liu",
      "Miguel R. D. Rodrigues"
    ],
    "abstract": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08308v1",
    "published_date": "2025-03-11 11:18:53 UTC",
    "updated_date": "2025-03-11 11:18:53 UTC"
  },
  {
    "arxiv_id": "2503.08302v1",
    "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models",
    "authors": [
      "Ji Zhao",
      "Xiao Lin"
    ],
    "abstract": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08302v1",
    "published_date": "2025-03-11 11:13:58 UTC",
    "updated_date": "2025-03-11 11:13:58 UTC"
  },
  {
    "arxiv_id": "2503.08301v2",
    "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study",
    "authors": [
      "Xian-Rong Zhang",
      "Yue-Jiao Gong",
      "Jun Zhang"
    ],
    "abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.08301v2",
    "published_date": "2025-03-11 11:13:11 UTC",
    "updated_date": "2025-03-12 06:00:27 UTC"
  },
  {
    "arxiv_id": "2503.08295v2",
    "title": "Preference-Based Alignment of Discrete Diffusion Models",
    "authors": [
      "Umberto Borso",
      "Davide Paglieri",
      "Jude Wells",
      "Tim Rocktäschel"
    ],
    "abstract": "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D2-DPO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D2-DPO on a structured binary sequence generation\ntask, demonstrating that the method effectively aligns model outputs with\npreferences while maintaining structural validity. Our results highlight that\nD2-DPO enables controlled fine-tuning without requiring explicit reward models,\nmaking it a practical alternative to reinforcement learning-based approaches.\nFuture research will explore extending D2-DPO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08295v2",
    "published_date": "2025-03-11 11:07:35 UTC",
    "updated_date": "2025-04-09 14:34:53 UTC"
  },
  {
    "arxiv_id": "2503.08292v2",
    "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges",
    "authors": [
      "Xiaoxiao Liu",
      "Qingying Xiao",
      "Junying Chen",
      "Xiangyi Feng",
      "Xiangbo Wu",
      "Bairui Zhang",
      "Xiang Wan",
      "Jian Chang",
      "Guangjun Yu",
      "Yan Hu",
      "Benyou Wang"
    ],
    "abstract": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08292v2",
    "published_date": "2025-03-11 11:05:42 UTC",
    "updated_date": "2025-05-08 09:33:59 UTC"
  },
  {
    "arxiv_id": "2503.08280v1",
    "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
    "authors": [
      "Zhenxiong Tan",
      "Qiaochu Xue",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ],
    "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
    "published_date": "2025-03-11 10:50:14 UTC",
    "updated_date": "2025-03-11 10:50:14 UTC"
  },
  {
    "arxiv_id": "2503.08748v3",
    "title": "Mirror Descent and Novel Exponentiated Gradient Algorithms Using Trace-Form Entropies and Deformed Logarithms",
    "authors": [
      "Andrzej Cichocki",
      "Toshihisa Tanaka",
      "Sergio Cruces"
    ],
    "abstract": "In this paper we propose and investigate a wide class of Mirror Descent\nupdates (MD) and associated novel Generalized Exponentiated Gradient (GEG)\nalgorithms by exploiting various trace-form entropies and associated deformed\nlogarithms and their inverses - deformed (generalized) exponential functions.\nThe proposed algorithms can be considered as extension of entropic MD and\ngeneralization of multiplicative updates. In the literature, there exist\nnowadays over fifty mathematically well defined generalized entropies, so\nimpossible to exploit all of them in one research paper. So we focus on a few\nselected most popular entropies and associated logarithms like the Tsallis,\nKaniadakis and Sharma-Taneja-Mittal and some of their extension like Tempesta\nor Kaniadakis-Scarfone entropies. The shape and properties of the deformed\nlogarithms and their inverses are tuned by one or more hyperparameters. By\nlearning these hyperparameters, we can adapt to distribution of training data,\nwhich can be designed to the specific geometry of the optimization problem,\nleading to potentially faster convergence and better performance. The using\ngeneralized entropies and associated deformed logarithms in the Bregman\ndivergence, used as a regularization term, provides some new insight into\nexponentiated gradient descent updates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08748v3",
    "published_date": "2025-03-11 10:50:07 UTC",
    "updated_date": "2025-03-21 02:07:53 UTC"
  },
  {
    "arxiv_id": "2503.08275v2",
    "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
    "authors": [
      "Ruibin Xiong",
      "Yimeng Chen",
      "Dmitrii Khizbullin",
      "Mingchen Zhuge",
      "Jürgen Schmidhuber"
    ],
    "abstract": "Long-form writing agents require flexible integration and interaction across\ninformation retrieval, reasoning, and composition. Current approaches rely on\npredetermined workflows and rigid thinking patterns to generate outlines before\nwriting, resulting in constrained adaptability during writing. In this paper we\npropose a general agent framework that achieves human-like adaptive writing\nthrough recursive task decomposition and dynamic integration of three\nfundamental task types, i.e. retrieval, reasoning, and composition. Our\nmethodology features: 1) a planning mechanism that interleaves recursive task\ndecomposition and execution, eliminating artificial restrictions on writing\nworkflow; and 2) integration of task types that facilitates heterogeneous task\ndecomposition. Evaluations on both fiction writing and technical report\ngeneration show that our method consistently outperforms state-of-the-art\napproaches across all automatic evaluation metrics, which demonstrate the\neffectiveness and broad applicability of our proposed framework.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08275v2",
    "published_date": "2025-03-11 10:43:01 UTC",
    "updated_date": "2025-03-25 18:27:55 UTC"
  },
  {
    "arxiv_id": "2503.08269v1",
    "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
    "authors": [
      "Junying Wang",
      "Hongyuan Zhang",
      "Yuan Yuan"
    ],
    "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR-25",
    "pdf_url": "http://arxiv.org/pdf/2503.08269v1",
    "published_date": "2025-03-11 10:34:57 UTC",
    "updated_date": "2025-03-11 10:34:57 UTC"
  },
  {
    "arxiv_id": "2503.08257v2",
    "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
    "authors": [
      "Yiming Zhong",
      "Qi Jiang",
      "Jingyi Yu",
      "Yuexin Ma"
    ],
    "abstract": "A dexterous hand capable of grasping any object is essential for the\ndevelopment of general-purpose embodied intelligent robots. However, due to the\nhigh degree of freedom in dexterous hands and the vast diversity of objects,\ngenerating high-quality, usable grasping poses in a robust manner is a\nsignificant challenge. In this paper, we introduce DexGrasp Anything, a method\nthat effectively integrates physical constraints into both the training and\nsampling phases of a diffusion-based generative model, achieving\nstate-of-the-art performance across nearly all open datasets. Additionally, we\npresent a new dexterous grasping dataset containing over 3.4 million diverse\ngrasping poses for more than 15k different objects, demonstrating its potential\nto advance universal dexterous grasping. The code of our method and our dataset\nwill be publicly released soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08257v2",
    "published_date": "2025-03-11 10:21:50 UTC",
    "updated_date": "2025-03-16 13:05:46 UTC"
  },
  {
    "arxiv_id": "2503.08251v1",
    "title": "MT-NAM: An Efficient and Adaptive Model for Epileptic Seizure Detection",
    "authors": [
      "Arshia Afzal",
      "Volkan Cevher",
      "Mahsa Shoaran"
    ],
    "abstract": "Enhancing the accuracy and efficiency of machine learning algorithms employed\nin neural interface systems is crucial for advancing next-generation\nintelligent therapeutic devices. However, current systems often utilize basic\nmachine learning models that do not fully exploit the natural structure of\nbrain signals. Additionally, existing learning models used for neural signal\nprocessing often demonstrate low speed and efficiency during inference. To\naddress these challenges, this study introduces Micro Tree-based NAM (MT-NAM),\na distilled model based on the recently proposed Neural Additive Models (NAM).\nThe MT-NAM achieves a remarkable 100$\\times$ improvement in inference speed\ncompared to standard NAM, without compromising accuracy. We evaluate our\napproach on the CHB-MIT scalp EEG dataset, which includes recordings from 24\npatients with varying numbers of sessions and seizures. NAM achieves an 85.3\\%\nwindow-based sensitivity and 95\\% specificity. Interestingly, our proposed\nMT-NAM shows only a 2\\% reduction in sensitivity compared to the original NAM.\nTo regain this sensitivity, we utilize a test-time template adjuster (T3A) as\nan update mechanism, enabling our model to achieve higher sensitivity during\ntest time by accommodating transient shifts in neural signals. With this online\nupdate approach, MT-NAM achieves the same sensitivity as the standard NAM while\nachieving approximately 50$\\times$ acceleration in inference speed.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Submitted to IEEE-TBME",
    "pdf_url": "http://arxiv.org/pdf/2503.08251v1",
    "published_date": "2025-03-11 10:14:53 UTC",
    "updated_date": "2025-03-11 10:14:53 UTC"
  },
  {
    "arxiv_id": "2503.08250v3",
    "title": "Aligning Text to Image in Diffusion Models is Easier Than You Think",
    "authors": [
      "Jaa-Yeon Lee",
      "Byunghee Cha",
      "Jeongsol Kim",
      "Jong Chul Ye"
    ],
    "abstract": "While recent advancements in generative modeling have significantly improved\ntext-image alignment, some residual misalignment between text and image\nrepresentations still remains. Although many approaches have attempted to\naddress this issue by fine-tuning models using various reward models, etc., we\nrevisit the challenge from the perspective of representation alignment-an\napproach that has gained popularity with the success of REPresentation\nAlignment (REPA). We first argue that conventional text-to-image (T2I)\ndiffusion models, typically trained on paired image and text data (i.e.,\npositive pairs) by minimizing score matching or flow matching losses, is\nsuboptimal from the standpoint of representation alignment. Instead, a better\nalignment can be achieved through contrastive learning that leverages both\npositive and negative pairs. To achieve this efficiently even with pretrained\nmodels, we introduce a lightweight contrastive fine tuning strategy called\nSoftREPA that uses soft text tokens. This approach improves alignment with\nminimal computational overhead by adding fewer than 1M trainable parameters to\nthe pretrained model. Our theoretical analysis demonstrates that our method\nexplicitly increases the mutual information between text and image\nrepresentations, leading to enhanced semantic consistency. Experimental results\nacross text-to-image generation and text-guided image editing tasks validate\nthe effectiveness of our approach in improving the semantic consistency of T2I\ngenerative models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08250v3",
    "published_date": "2025-03-11 10:14:22 UTC",
    "updated_date": "2025-03-21 07:28:43 UTC"
  },
  {
    "arxiv_id": "2503.08241v1",
    "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents",
    "authors": [
      "Tristan Tomilin",
      "Meng Fang",
      "Mykola Pechenizkiy"
    ],
    "abstract": "Advancing safe autonomous systems through reinforcement learning (RL)\nrequires robust benchmarks to evaluate performance, analyze methods, and assess\nagent competencies. Humans primarily rely on embodied visual perception to\nsafely navigate and interact with their surroundings, making it a valuable\ncapability for RL agents. However, existing vision-based 3D benchmarks only\nconsider simple navigation tasks. To address this shortcoming, we introduce\n\\textbf{HASARD}, a suite of diverse and complex tasks to $\\textbf{HA}$rness\n$\\textbf{SA}$fe $\\textbf{R}$L with $\\textbf{D}$oom, requiring strategic\ndecision-making, comprehending spatial relationships, and predicting the\nshort-term future. HASARD features three difficulty levels and two action\nspaces. An empirical evaluation of popular baseline methods demonstrates the\nbenchmark's complexity, unique challenges, and reward-cost trade-offs.\nVisualizing agent navigation during training with top-down heatmaps provides\ninsight into a method's learning process. Incrementally training across\ndifficulty levels offers an implicit learning curriculum. HASARD is the first\nsafe RL benchmark to exclusively target egocentric vision-based learning,\noffering a cost-effective and insightful way to explore the potential and\nboundaries of current and future safe RL methods. The environments and baseline\nimplementations are open-sourced at\nhttps://sites.google.com/view/hasard-bench/.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08241v1",
    "published_date": "2025-03-11 10:05:01 UTC",
    "updated_date": "2025-03-11 10:05:01 UTC"
  },
  {
    "arxiv_id": "2503.11696v1",
    "title": "Balancing SoC in Battery Cells using Safe Action Perturbations",
    "authors": [
      "E Harshith Kumar Yadav",
      "Rahul Narava",
      "Anshika",
      "Shashi Shekher Jha"
    ],
    "abstract": "Managing equal charge levels in active cell balancing while charging a Li-ion\nbattery is challenging. An imbalance in charge levels affects the state of\nhealth of the battery, along with the concerns of thermal runaway and fire\nhazards. Traditional methods focus on safety assurance as a trade-off between\nsafety and charging time. Others deal with battery-specific conditions to\nensure safety, therefore losing on the generalization of the control strategies\nover various configurations of batteries. In this work, we propose a method to\nlearn safe battery charging actions by using a safety-layer as an add-on over a\nDeep Reinforcement Learning (RL) agent. The safety layer perturbs the agent's\naction to prevent the battery from encountering unsafe or dangerous states.\nFurther, our Deep RL framework focuses on learning a generalized policy that\ncan be effectively employed with varying configurations of batteries. Our\nexperimental results demonstrate that the safety-layer based action\nperturbation incurs fewer safety violations by avoiding unsafe states along\nwith learning a robust policy for several battery configurations.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11696v1",
    "published_date": "2025-03-11 09:59:14 UTC",
    "updated_date": "2025-03-11 09:59:14 UTC"
  },
  {
    "arxiv_id": "2503.08228v1",
    "title": "Investigating Execution-Aware Language Models for Code Optimization",
    "authors": [
      "Federico Di Menna",
      "Luca Traini",
      "Gabriele Bavota",
      "Vittorio Cortellessa"
    ],
    "abstract": "Code optimization is the process of enhancing code efficiency, while\npreserving its intended functionality. This process often requires a deep\nunderstanding of the code execution behavior at run-time to identify and\naddress inefficiencies effectively. Recent studies have shown that language\nmodels can play a significant role in automating code optimization. However,\nthese models may have insufficient knowledge of how code execute at run-time.\nTo address this limitation, researchers have developed strategies that\nintegrate code execution information into language models. These strategies\nhave shown promise, enhancing the effectiveness of language models in various\nsoftware engineering tasks. However, despite the close relationship between\ncode execution behavior and efficiency, the specific impact of these strategies\non code optimization remains largely unexplored. This study investigates how\nincorporating code execution information into language models affects their\nability to optimize code. Specifically, we apply three different training\nstrategies to incorporate four code execution aspects -- line executions, line\ncoverage, branch coverage, and variable states -- into CodeT5+, a well-known\nlanguage model for code. Our results indicate that execution-aware models\nprovide limited benefits compared to the standard CodeT5+ model in optimizing\ncode.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08228v1",
    "published_date": "2025-03-11 09:46:07 UTC",
    "updated_date": "2025-03-11 09:46:07 UTC"
  },
  {
    "arxiv_id": "2503.08226v1",
    "title": "A Grey-box Text Attack Framework using Explainable AI",
    "authors": [
      "Esther Chiramal",
      "Kelvin Soh Boon Kai"
    ],
    "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08226v1",
    "published_date": "2025-03-11 09:44:17 UTC",
    "updated_date": "2025-03-11 09:44:17 UTC"
  },
  {
    "arxiv_id": "2503.08745v1",
    "title": "Neural Network for Blind Unmixing: a novel MatrixConv Unmixing (MCU) Approach",
    "authors": [
      "Chao Zhou",
      "Wei Pu",
      "Miguel Rodrigues"
    ],
    "abstract": "Hyperspectral image (HSI) unmixing is a challenging research problem that\ntries to identify the constituent components, known as endmembers, and their\ncorresponding proportions, known as abundances, in the scene by analysing\nimages captured by hyperspectral cameras. Recently, many deep learning based\nunmixing approaches have been proposed with the surge of machine learning\ntechniques, especially convolutional neural networks (CNN). However, these\nmethods face two notable challenges: 1. They frequently yield results lacking\nphysical significance, such as signatures corresponding to unknown or\nnon-existent materials. 2. CNNs, as general-purpose network structures, are not\nexplicitly tailored for unmixing tasks. In response to these concerns, our work\ndraws inspiration from double deep image prior (DIP) techniques and algorithm\nunrolling, presenting a novel network structure that effectively addresses both\nissues. Specifically, we first propose a MatrixConv Unmixing (MCU) approach for\nendmember and abundance estimation, respectively, which can be solved via\ncertain iterative solvers. We then unroll these solvers to build two\nsub-networks, endmember estimation DIP (UEDIP) and abundance estimation DIP\n(UADIP), to generate the estimation of endmember and abundance, respectively.\nThe overall network is constructed by assembling these two sub-networks. In\norder to generate meaningful unmixing results, we also propose a composite loss\nfunction. To further improve the unmixing quality, we also add explicitly a\nregularizer for endmember and abundance estimation, respectively. The proposed\nmethods are tested for effectiveness on both synthetic and real datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08745v1",
    "published_date": "2025-03-11 09:41:57 UTC",
    "updated_date": "2025-03-11 09:41:57 UTC"
  },
  {
    "arxiv_id": "2503.08221v1",
    "title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People",
    "authors": [
      "Junbin Xiao",
      "Nanxin Huang",
      "Hao Qiu",
      "Zhulin Tao",
      "Xun Yang",
      "Richang Hong",
      "Meng Wang",
      "Angela Yao"
    ],
    "abstract": "We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 4,927 questions directly posed or generated and verified by blind\nindividuals to reflect their needs for visual assistance under various\nscenarios. We provide each question with an average of 3 reference answers to\nalleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15\nleading MLLMs and find that all models struggle, with the best performers\nachieving accuracy around 56\\%, far behind human performance of 87.4\\%. To\nguide future advancements, we identify and summarize major limitations of\nexisting MLLMs in egocentric visual assistance for the blind and provide\nheuristic suggestions for improvement. With these efforts, we hope EgoBlind can\nserve as a valuable foundation for developing more effective AI assistants to\nenhance the independence of the blind individuals' lives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.08221v1",
    "published_date": "2025-03-11 09:40:31 UTC",
    "updated_date": "2025-03-11 09:40:31 UTC"
  },
  {
    "arxiv_id": "2503.08219v1",
    "title": "CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning",
    "authors": [
      "Kaiqiang Xiong",
      "Rui Peng",
      "Zhe Zhang",
      "Tianxing Feng",
      "Jianbo Jiao",
      "Feng Gao",
      "Ronggang Wang"
    ],
    "abstract": "Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress\nrecently. However, previous methods primarily depend on the photometric\nconsistency assumption, which may suffer from two limitations:\nindistinguishable regions and view-dependent effects, e.g., low-textured areas\nand reflections. To address these issues, in this paper, we propose a new\ndual-level contrastive learning approach, named CL-MVSNet. Specifically, our\nmodel integrates two contrastive branches into an unsupervised MVS framework to\nconstruct additional supervisory signals. On the one hand, we present an\nimage-level contrastive branch to guide the model to acquire more context\nawareness, thus leading to more complete depth estimation in indistinguishable\nregions. On the other hand, we exploit a scene-level contrastive branch to\nboost the representation ability, improving robustness to view-dependent\neffects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5\nphotometric consistency loss, which encourages the model to focus more on\naccurate points while mitigating the gradient penalty of undesirable ones.\nExtensive experiments on DTU and Tanks&Temples benchmarks demonstrate that our\napproach achieves state-of-the-art performance among all end-to-end\nunsupervised MVS frameworks and outperforms its supervised counterpart by a\nconsiderable margin without fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accpetd by ICCV2023",
    "pdf_url": "http://arxiv.org/pdf/2503.08219v1",
    "published_date": "2025-03-11 09:39:06 UTC",
    "updated_date": "2025-03-11 09:39:06 UTC"
  },
  {
    "arxiv_id": "2503.08213v1",
    "title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch",
    "authors": [
      "Nandakishor M"
    ],
    "abstract": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08213v1",
    "published_date": "2025-03-11 09:27:56 UTC",
    "updated_date": "2025-03-11 09:27:56 UTC"
  },
  {
    "arxiv_id": "2503.08205v1",
    "title": "OLMD: Orientation-aware Long-term Motion Decoupling for Continuous Sign Language Recognition",
    "authors": [
      "Yiheng Yu",
      "Sheng Liu",
      "Yuan Feng",
      "Min Xu",
      "Zhelun Jin",
      "Xuhua Yang"
    ],
    "abstract": "The primary challenge in continuous sign language recognition (CSLR) mainly\nstems from the presence of multi-orientational and long-term motions. However,\ncurrent research overlooks these crucial aspects, significantly impacting\naccuracy. To tackle these issues, we propose a novel CSLR framework:\nOrientation-aware Long-term Motion Decoupling (OLMD), which efficiently\naggregates long-term motions and decouples multi-orientational signals into\neasily interpretable components. Specifically, our innovative Long-term Motion\nAggregation (LMA) module filters out static redundancy while adaptively\ncapturing abundant features of long-term motions. We further enhance\norientation awareness by decoupling complex movements into horizontal and\nvertical components, allowing for motion purification in both orientations.\nAdditionally, two coupling mechanisms are proposed: stage and cross-stage\ncoupling, which together enrich multi-scale features and improve the\ngeneralization capabilities of the model. Experimentally, OLMD shows SOTA\nperformance on three large-scale datasets: PHOENIX14, PHOENIX14-T, and\nCSL-Daily. Notably, we improved the word error rate (WER) on PHOENIX14 by an\nabsolute 1.6% compared to the previous SOTA",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08205v1",
    "published_date": "2025-03-11 09:20:06 UTC",
    "updated_date": "2025-03-11 09:20:06 UTC"
  },
  {
    "arxiv_id": "2503.08199v1",
    "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models",
    "authors": [
      "Miao Zhang",
      "Zhenlong Fang",
      "Tianyi Wang",
      "Qian Zhang",
      "Shuai Lu",
      "Junfeng Jiao",
      "Tianyu Shi"
    ],
    "abstract": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08199v1",
    "published_date": "2025-03-11 09:08:04 UTC",
    "updated_date": "2025-03-11 09:08:04 UTC"
  },
  {
    "arxiv_id": "2503.08193v1",
    "title": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents",
    "authors": [
      "Rui Xu",
      "MingYu Wang",
      "XinTao Wang",
      "Dakuan Lu",
      "Xiaoyu Tan",
      "Wei Chu",
      "Yinghui Xu"
    ],
    "abstract": "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https://github.com/airaer1998/RPA_Thought.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08193v1",
    "published_date": "2025-03-11 08:57:07 UTC",
    "updated_date": "2025-03-11 08:57:07 UTC"
  },
  {
    "arxiv_id": "2503.08188v1",
    "title": "RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware",
    "authors": [
      "Gonzalo Santamaría Gómez",
      "Guillem García Subies",
      "Pablo Gutiérrez Ruiz",
      "Mario González Valero",
      "Natàlia Fuertes",
      "Helena Montoro Zamorano",
      "Carmen Muñoz Sanz",
      "Leire Rosado Plaza",
      "Nuria Aldama García",
      "David Betancur Sánchez",
      "Kateryna Sushkova",
      "Marta Guerrero Nieto",
      "Álvaro Barbero Jiménez"
    ],
    "abstract": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08188v1",
    "published_date": "2025-03-11 08:53:53 UTC",
    "updated_date": "2025-03-11 08:53:53 UTC"
  },
  {
    "arxiv_id": "2503.08179v3",
    "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models",
    "authors": [
      "Zicheng Ma",
      "Chuanliu Fan",
      "Zhicong Wang",
      "Zhenyu Chen",
      "Xiaohan Lin",
      "Yanheng Li",
      "Shihao Feng",
      "Jun Zhang",
      "Ziqiang Cao",
      "Yi Qin Gao"
    ],
    "abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "26 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08179v3",
    "published_date": "2025-03-11 08:43:05 UTC",
    "updated_date": "2025-03-13 13:54:27 UTC"
  },
  {
    "arxiv_id": "2503.08175v1",
    "title": "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
    "authors": [
      "Zitong Shi",
      "Guancheng Wan",
      "Wenke Huang",
      "Guibin Zhang",
      "Jiawei Shao",
      "Mang Ye",
      "Carl Yang"
    ],
    "abstract": "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08175v1",
    "published_date": "2025-03-11 08:38:45 UTC",
    "updated_date": "2025-03-11 08:38:45 UTC"
  },
  {
    "arxiv_id": "2503.08174v1",
    "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study",
    "authors": [
      "Veronica Bot",
      "Zheyuan Xu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated unprecedented capability in\nreasoning with natural language. Coupled with this development is the emergence\nof embodied AI in robotics. Despite showing promise for verbal and written\nreasoning tasks, it remains unknown whether LLMs are capable of navigating\ncomplex spatial tasks with physical actions in the real world. To this end, it\nis of interest to investigate applying LLMs to robotics in zero-shot learning\nscenarios, and in the absence of fine-tuning - a feat which could significantly\nimprove human-robot interaction, alleviate compute cost, and eliminate\nlow-level programming tasks associated with robot tasks.\n  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot\nin Webots engine for an object search task. We evaluate the effectiveness of\nthree reasoning strategies based on Chain-of-Thought (CoT) sub-task list\ngeneration with the Socratic method (SocraCoT) (in order of increasing rigor):\n(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was\nmeasured in terms of the proportion of tasks successfully completed and\nexecution time (N = 20). Our preliminary results show that when combined with\nchain-of-thought reasoning, the Socratic method can be used for code generation\nfor robotic tasks that require spatial awareness. In extension of this finding,\nwe propose EVINCE-LoC; a modified EVINCE method that could further enhance\nperformance in highly complex and or dynamic testing scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08174v1",
    "published_date": "2025-03-11 08:36:37 UTC",
    "updated_date": "2025-03-11 08:36:37 UTC"
  },
  {
    "arxiv_id": "2503.08163v1",
    "title": "XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change",
    "authors": [
      "Jiawen Wei",
      "Aniruddha Bora",
      "Vivek Oommen",
      "Chenyu Dong",
      "Juntao Yang",
      "Jeff Adie",
      "Chen Chen",
      "Simon See",
      "George Karniadakis",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Extreme weather events are increasing in frequency and intensity due to\nclimate change. This, in turn, is exacting a significant toll in communities\nworldwide. While prediction skills are increasing with advances in numerical\nweather prediction and artificial intelligence tools, extreme weather still\npresent challenges. More specifically, identifying the precursors of such\nextreme weather events and how these precursors may evolve under climate change\nremain unclear. In this paper, we propose to use post-hoc interpretability\nmethods to construct relevance weather maps that show the key extreme-weather\nprecursors identified by deep learning models. We then compare this machine\nview with existing domain knowledge to understand whether deep learning models\nidentified patterns in data that may enrich our understanding of\nextreme-weather precursors. We finally bin these relevant maps into different\nmulti-year time periods to understand the role that climate change is having on\nthese precursors. The experiments are carried out on Indochina heatwaves, but\nthe methodology can be readily extended to other extreme weather events\nworldwide.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08163v1",
    "published_date": "2025-03-11 08:27:08 UTC",
    "updated_date": "2025-03-11 08:27:08 UTC"
  },
  {
    "arxiv_id": "2503.08741v3",
    "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
    "authors": [
      "Letian Zhang",
      "Quan Cui",
      "Bingchen Zhao",
      "Cheng Yang"
    ],
    "abstract": "The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and dataset are publicly available at\nhttps://github.com/Letian2003/MM_INF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08741v3",
    "published_date": "2025-03-11 08:25:40 UTC",
    "updated_date": "2025-03-26 09:01:55 UTC"
  },
  {
    "arxiv_id": "2503.08145v1",
    "title": "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
    "authors": [
      "Yunhao Li",
      "Yifan Jiao",
      "Dan Meng",
      "Heng Fan",
      "Libo Zhang"
    ],
    "abstract": "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08145v1",
    "published_date": "2025-03-11 08:03:47 UTC",
    "updated_date": "2025-03-11 08:03:47 UTC"
  },
  {
    "arxiv_id": "2503.08136v1",
    "title": "FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems",
    "authors": [
      "Jeongsol Kim",
      "Bryan Sangwoo Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Flow matching is a recent state-of-the-art framework for generative modeling\nbased on ordinary differential equations (ODEs). While closely related to\ndiffusion models, it provides a more general perspective on generative\nmodeling. Although inverse problem solving has been extensively explored using\ndiffusion models, it has not been rigorously examined within the broader\ncontext of flow models. Therefore, here we extend the diffusion inverse solvers\n(DIS) - which perform posterior sampling by combining a denoising diffusion\nprior with an likelihood gradient - into the flow framework. Specifically, by\ndriving the flow-version of Tweedie's formula, we decompose the flow ODE into\ntwo components: one for clean image estimation and the other for noise\nestimation. By integrating the likelihood gradient and stochastic noise into\neach component, respectively, we demonstrate that posterior sampling for\ninverse problem solving can be effectively achieved using flows. Our proposed\nsolver, Flow-Driven Posterior Sampling (FlowDPS), can also be seamlessly\nintegrated into a latent flow model with a transformer architecture. Across\nfour linear inverse problems, we confirm that FlowDPS outperforms\nstate-of-the-art alternatives, all without requiring additional training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08136v1",
    "published_date": "2025-03-11 07:56:14 UTC",
    "updated_date": "2025-03-11 07:56:14 UTC"
  },
  {
    "arxiv_id": "2503.08133v1",
    "title": "MGHanD: Multi-modal Guidance for authentic Hand Diffusion",
    "authors": [
      "Taehyeon Eum",
      "Jieun Choi",
      "Tae-Kyun Kim"
    ],
    "abstract": "Diffusion-based methods have achieved significant successes in T2I\ngeneration, providing realistic images from text prompts. Despite their\ncapabilities, these models face persistent challenges in generating realistic\nhuman hands, often producing images with incorrect finger counts and\nstructurally deformed hands. MGHanD addresses this challenge by applying\nmulti-modal guidance during the inference process. For visual guidance, we\nemploy a discriminator trained on a dataset comprising paired real and\ngenerated images with captions, derived from various hand-in-the-wild datasets.\nWe also employ textual guidance with LoRA adapter, which learns the direction\nfrom `hands' towards more detailed prompts such as `natural hands', and\n`anatomically correct fingers' at the latent level. A cumulative hand mask\nwhich is gradually enlarged in the assigned time step is applied to the added\nguidance, allowing the hand to be refined while maintaining the rich generative\ncapabilities of the pre-trained model. In the experiments, our method achieves\nsuperior hand generation qualities, without any specific conditions or priors.\nWe carry out both quantitative and qualitative evaluations, along with user\nstudies, to showcase the benefits of our approach in producing high-quality\nhand images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08133v1",
    "published_date": "2025-03-11 07:51:47 UTC",
    "updated_date": "2025-03-11 07:51:47 UTC"
  },
  {
    "arxiv_id": "2503.08122v1",
    "title": "Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments",
    "authors": [
      "Soonwoo Kwon",
      "Jin-Young Kim",
      "Hyojun Go",
      "Kyungjune Baek"
    ],
    "abstract": "We present a novel study on enhancing the capability of preserving the\ncontent in world models, focusing on a property we term World Stability. Recent\ndiffusion-based generative models have advanced the synthesis of immersive and\nrealistic environments that are pivotal for applications such as reinforcement\nlearning and interactive game engines. However, while these models excel in\nquality and diversity, they often neglect the preservation of previously\ngenerated scenes over time--a shortfall that can introduce noise into agent\nlearning and compromise performance in safety-critical settings. In this work,\nwe introduce an evaluation framework that measures world stability by having\nworld models perform a sequence of actions followed by their inverses to return\nto their initial viewpoint, thereby quantifying the consistency between the\nstarting and ending observations. Our comprehensive assessment of\nstate-of-the-art diffusion-based world models reveals significant challenges in\nachieving high world stability. Moreover, we investigate several improvement\nstrategies to enhance world stability. Our results underscore the importance of\nworld stability in world modeling and provide actionable insights for future\nresearch in this domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.08122v1",
    "published_date": "2025-03-11 07:38:11 UTC",
    "updated_date": "2025-03-11 07:38:11 UTC"
  },
  {
    "arxiv_id": "2503.08739v1",
    "title": "HeGMN: Heterogeneous Graph Matching Network for Learning Graph Similarity",
    "authors": [
      "Shilong Sang",
      "Ke-Jia Chen",
      "Zheng liu"
    ],
    "abstract": "Graph similarity learning (GSL), also referred to as graph matching in many\nscenarios, is a fundamental problem in computer vision, pattern recognition,\nand graph learning. However, previous GSL methods assume that graphs are\nhomogeneous and struggle to maintain their performance on heterogeneous graphs.\nTo address this problem, this paper proposes a Heterogeneous Graph Matching\nNetwork (HeGMN), which is an end-to-end graph similarity learning framework\ncomposed of a two-tier matching mechanism. Firstly, a heterogeneous graph\nisomorphism network is proposed as the encoder, which reinvents graph\nisomorphism network for heterogeneous graphs by perceiving different semantic\nrelationships during aggregation. Secondly, a graph-level and node-level\nmatching modules are designed, both employing type-aligned matching principles.\nThe former conducts graph-level matching by node type alignment, and the latter\ncomputes the interactions between the cross-graph nodes with the same type thus\nreducing noise interference and computational overhead. Finally, the\ngraph-level and node-level matching features are combined and fed into fully\nconnected layers for predicting graph similarity scores. In experiments, we\npropose a heterogeneous graph resampling method to construct heterogeneous\ngraph pairs and define the corresponding heterogeneous graph edit distance,\nfilling the gap in missing datasets. Extensive experiments demonstrate that\nHeGMN consistently achieves advanced performance on graph similarity prediction\nacross all datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08739v1",
    "published_date": "2025-03-11 07:36:35 UTC",
    "updated_date": "2025-03-11 07:36:35 UTC"
  },
  {
    "arxiv_id": "2503.08120v2",
    "title": "Uni$\\textbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models",
    "authors": [
      "Junzhe Li",
      "Xuerui Qiu",
      "Linrui Xu",
      "Liya Guo",
      "Delin Qu",
      "Tingting Long",
      "Chun Fan",
      "Ming Li"
    ],
    "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on $\\textbf{coarse}$ facial attribute understanding,\nwith limited capacity to handle $\\textbf{fine-grained}$ facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose Uni$\\textbf{F}^2$ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUni$\\textbf{F}^2$ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUni$\\textbf{F}^2$ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUni$\\textbf{F}^2$ace-130K demonstrate that Uni$\\textbf{F}^2$ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08120v2",
    "published_date": "2025-03-11 07:34:59 UTC",
    "updated_date": "2025-03-26 02:30:35 UTC"
  },
  {
    "arxiv_id": "2503.08117v1",
    "title": "Convergence Dynamics and Stabilization Strategies of Co-Evolving Generative Models",
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ],
    "abstract": "The increasing prevalence of synthetic data in training loops has raised\nconcerns about model collapse, where generative models degrade when trained on\ntheir own outputs. While prior work focuses on this self-consuming process, we\nstudy an underexplored yet prevalent phenomenon: co-evolving generative models\nthat shape each other's training through iterative feedback. This is common in\nmultimodal AI ecosystems, such as social media platforms, where text models\ngenerate captions that guide image models, and the resulting images influence\nthe future adaptation of the text model. We take a first step by analyzing such\na system, modeling the text model as a multinomial distribution and the image\nmodel as a conditional multi-dimensional Gaussian distribution. Our analysis\nuncovers three key results. First, when one model remains fixed, the other\ncollapses: a frozen image model causes the text model to lose diversity, while\na frozen text model leads to an exponential contraction of image diversity,\nthough fidelity remains bounded. Second, in fully interactive systems, mutual\nreinforcement accelerates collapse, with image contraction amplifying text\nhomogenization and vice versa, leading to a Matthew effect where dominant texts\nsustain higher image diversity while rarer texts collapse faster. Third, we\nanalyze stabilization strategies implicitly introduced by real-world external\ninfluences. Random corpus injections for text models and user-content\ninjections for image models prevent collapse while preserving both diversity\nand fidelity. Our theoretical findings are further validated through\nexperiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 68T99"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08117v1",
    "published_date": "2025-03-11 07:30:25 UTC",
    "updated_date": "2025-03-11 07:30:25 UTC"
  },
  {
    "arxiv_id": "2503.08102v2",
    "title": "AI-native Memory 2.0: Second Me",
    "authors": [
      "Jiale Wei",
      "Xiang Ying",
      "Tao Gao",
      "Fangyi Bao",
      "Felix Tao",
      "Jingbo Shang"
    ],
    "abstract": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08102v2",
    "published_date": "2025-03-11 07:05:52 UTC",
    "updated_date": "2025-03-12 11:31:31 UTC"
  },
  {
    "arxiv_id": "2503.13494v1",
    "title": "Mobility-aware Seamless Service Migration and Resource Allocation in Multi-edge IoV Systems",
    "authors": [
      "Zheyi Chen",
      "Sijin Huang",
      "Geyong Min",
      "Zhaolong Ning",
      "Jie Li",
      "Yan Zhang"
    ],
    "abstract": "Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for\nInternet-of-Vehicles (IoV) applications. However, due to high vehicle mobility\nand finite communication coverage of base stations, it is hard to maintain\nuninterrupted and high-quality services without proper service migration among\nMEC servers. Existing solutions commonly rely on prior knowledge and rarely\nconsider efficient resource allocation during the service migration process,\nmaking it hard to reach optimal performance in dynamic IoV environments. To\naddress these important challenges, we propose SR-CL, a novel mobility-aware\nseamless Service migration and Resource allocation framework via\nConvex-optimization-enabled deep reinforcement Learning in multi-edge IoV\nsystems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP)\nproblem of service migration and resource allocation into two sub-problems.\nNext, we design a new actor-critic-based asynchronous-update deep reinforcement\nlearning method to handle service migration, where the delayed-update actor\nmakes migration decisions and the one-step-update critic evaluates the\ndecisions to guide the policy update. Notably, we theoretically derive the\noptimal resource allocation with convex optimization for each MEC server,\nthereby further improving system performance. Using the real-world datasets of\nvehicle trajectories and testbed, extensive experiments are conducted to verify\nthe effectiveness of the proposed SR-CL. Compared to benchmark methods, the\nSR-CL achieves superior convergence and delay performance under various\nscenarios.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13494v1",
    "published_date": "2025-03-11 07:03:25 UTC",
    "updated_date": "2025-03-11 07:03:25 UTC"
  },
  {
    "arxiv_id": "2503.08091v1",
    "title": "Revolution of Wireless Signal Recognition for 6G: Recent Advances, Challenges and Future Directions",
    "authors": [
      "Hao Zhang",
      "Fuhui Zhou",
      "Hongyang Du",
      "Qihui Wu",
      "Chau Yuen"
    ],
    "abstract": "Wireless signal recognition (WSR) is a crucial technique for intelligent\ncommunications and spectrum sharing in the next six-generation (6G) wireless\ncommunication networks. It can be utilized to enhance network performance and\nefficiency, improve quality of service (QoS), and improve network security and\nreliability. Additionally, WSR can be applied for military applications such as\nsignal interception, signal race, and signal abduction. In the past decades,\ngreat efforts have been made for the research of WSR. Earlier works mainly\nfocus on model-based methods, including likelihood-based (LB) and feature-based\n(FB) methods, which have taken the leading position for many years. With the\nemergence of artificial intelligence (AI), intelligent methods including\nmachine learning-based (ML-based) and deep learning-based (DL-based) methods\nhave been developed to extract the features of the received signals and perform\nthe classification. In this work, we provide a comprehensive review of WSR from\nthe view of applications, main tasks, recent advances, datasets and evaluation\nmetrics, challenges, and future directions. Specifically, intelligent WSR\nmethods are introduced from the perspective of model, data, learning and\nimplementation. Moreover, we analyze the challenges for WSR from the view of\ncomplex, dynamic, and open 6G wireless environments and discuss the future\ndirections for WSR. This survey is expected to provide a comprehensive overview\nof the state-of-the-art WSR techniques and inspire new research directions for\nWSR in 6G networks.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "submitted to IEEE Communications Surveys & Tutorials",
    "pdf_url": "http://arxiv.org/pdf/2503.08091v1",
    "published_date": "2025-03-11 06:47:27 UTC",
    "updated_date": "2025-03-11 06:47:27 UTC"
  },
  {
    "arxiv_id": "2503.08084v1",
    "title": "Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation",
    "authors": [
      "Fangyuan Wang",
      "Shipeng Lyu",
      "Peng Zhou",
      "Anqing Duan",
      "Guodong Guo",
      "David Navarro-Alarcon"
    ],
    "abstract": "Enabling humanoid robots to perform long-horizon mobile manipulation planning\nin real-world environments based on embodied perception and comprehension\nabilities has been a longstanding challenge. With the recent rise of large\nlanguage models (LLMs), there has been a notable increase in the development of\nLLM-based planners. These approaches either utilize human-provided textual\nrepresentations of the real world or heavily depend on prompt engineering to\nextract such representations, lacking the capability to quantitatively\nunderstand the environment, such as determining the feasibility of manipulating\nobjects. To address these limitations, we present the Instruction-Augmented\nLong-Horizon Planning (IALP) system, a novel framework that employs LLMs to\ngenerate feasible and optimal actions based on real-time sensor feedback,\nincluding grounded knowledge of the environment, in a closed-loop interaction.\nDistinct from prior works, our approach augments user instructions into PDDL\nproblems by leveraging both the abstract reasoning capabilities of LLMs and\ngrounding mechanisms. By conducting various real-world long-horizon tasks, each\nconsisting of seven distinct manipulatory skills, our results demonstrate that\nthe IALP system can efficiently solve these tasks with an average success rate\nexceeding 80%. Our proposed method can operate as a high-level planner,\nequipping robots with substantial autonomy in unstructured environments through\nthe utilization of multi-modal sensor inputs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "17 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08084v1",
    "published_date": "2025-03-11 06:37:33 UTC",
    "updated_date": "2025-03-11 06:37:33 UTC"
  },
  {
    "arxiv_id": "2503.08737v1",
    "title": "Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models",
    "authors": [
      "In Cho",
      "Youngbeom Yoo",
      "Subin Jeon",
      "Seon Joo Kim"
    ],
    "abstract": "Constructing a compressed latent space through a variational autoencoder\n(VAE) is the key for efficient 3D diffusion models. This paper introduces\nCOD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors\nwithout sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme\nto improve compression and decoding efficiency. First, our encoder block\nprogressively compresses point clouds into compact latent vectors via\nintermediate point patches. Second, our triplane-based decoder reconstructs\ndense triplanes from latent vectors instead of directly decoding neural fields,\nsignificantly reducing computational overhead of neural fields decoding.\nFinally, we propose uncertainty-guided token pruning, which allocates resources\nadaptively by skipping computations in simpler regions and improves the decoder\nefficiency. Experimental results demonstrate that COD-VAE achieves 16x\ncompression compared to the baseline while maintaining quality. This enables\n20.8x speedup in generation, highlighting that a large number of latent vectors\nis not a prerequisite for high-quality reconstruction and generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08737v1",
    "published_date": "2025-03-11 06:29:39 UTC",
    "updated_date": "2025-03-11 06:29:39 UTC"
  },
  {
    "arxiv_id": "2503.08083v1",
    "title": "Degradation Self-Supervised Learning for Lithium-ion Battery Health Diagnostics",
    "authors": [
      "J. C. Chen"
    ],
    "abstract": "Health evaluation for lithium-ion batteries (LIBs) typically relies on\nconstant charging/discharging protocols, often neglecting scenarios involving\ndynamic current profiles prevalent in electric vehicles. Conventional health\nindicators for LIBs also depend on the uniformity of measured data, restricting\ntheir adaptability to non-uniform conditions. In this study, a novel training\nstrategy for estimating LIB health based on the paradigm of self-supervised\nlearning is proposed. A multiresolution analysis technique, empirical wavelet\ntransform, is utilized to decompose non-stationary voltage signals in the\nfrequency domain. This allows the removal of ineffective components for the\nhealth evaluation model. The transformer neural network serves as the model\nbackbone, and a loss function is designed to describe the capacity degradation\nbehavior with the assumption that the degradation in LIBs across most operating\nconditions is inevitable and irreversible. The results show that the model can\nlearn the aging characteristics by analyzing sequences of voltage and current\nprofiles obtained at various time intervals from the same LIB cell. The\nproposed method is successfully applied to the Stanford University LIB aging\ndataset, derived from electric vehicle real driving profiles. Notably, this\napproach achieves an average correlation coefficient of 0.9 between the\nevaluated health index and the degradation of actual capacity, demonstrating\nits efficacy in capturing LIB health degradation. This research highlights the\nfeasibility of training deep neural networks using unlabeled LIB data, offering\ncost-efficient means and unleashing the potential of the measured information.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08083v1",
    "published_date": "2025-03-11 06:29:13 UTC",
    "updated_date": "2025-03-11 06:29:13 UTC"
  },
  {
    "arxiv_id": "2503.08065v1",
    "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
    "authors": [
      "Jin Wenzhe",
      "Tang Haina",
      "Zhang Xudong"
    ],
    "abstract": "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08065v1",
    "published_date": "2025-03-11 05:50:27 UTC",
    "updated_date": "2025-03-11 05:50:27 UTC"
  },
  {
    "arxiv_id": "2503.08064v1",
    "title": "Continual Learning for Multiple Modalities",
    "authors": [
      "Hyundong Jin",
      "Eunwoo Kim"
    ],
    "abstract": "Continual learning aims to learn knowledge of tasks observed in sequential\ntime steps while mitigating the forgetting of previously learned knowledge.\nExisting methods were proposed under the assumption of learning a single\nmodality (e.g., image) over time, which limits their applicability in scenarios\ninvolving multiple modalities. In this work, we propose a novel continual\nlearning framework that accommodates multiple modalities (image, video, audio,\ndepth, and text). We train a model to align various modalities with text,\nleveraging its rich semantic information. However, this increases the risk of\nforgetting previously learned knowledge, exacerbated by the differing input\ntraits of each task. To alleviate the overwriting of the previous knowledge of\nmodalities, we propose a method for aggregating knowledge within and across\nmodalities. The aggregated knowledge is obtained by assimilating new\ninformation through self-regularization within each modality and associating\nknowledge between modalities by prioritizing contributions from relevant\nmodalities. Furthermore, we propose a strategy that re-aligns the embeddings of\nmodalities to resolve biased alignment between modalities. We evaluate the\nproposed method in a wide range of continual learning scenarios using multiple\ndatasets with different modalities. Extensive experiments demonstrate that ours\noutperforms existing methods in the scenarios, regardless of whether the\nidentity of the modality is given.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08064v1",
    "published_date": "2025-03-11 05:50:13 UTC",
    "updated_date": "2025-03-11 05:50:13 UTC"
  },
  {
    "arxiv_id": "2503.08051v1",
    "title": "Counterfactual Language Reasoning for Explainable Recommendation Systems",
    "authors": [
      "Guanrong Li",
      "Haolin Yang",
      "Xinyu Liu",
      "Zhen Wu",
      "Xinyu Dai"
    ],
    "abstract": "Explainable recommendation systems leverage transparent reasoning to foster\nuser trust and improve decision-making processes. Current approaches typically\ndecouple recommendation generation from explanation creation, violating causal\nprecedence principles where explanatory factors should logically precede\noutcomes. This paper introduces a novel framework integrating structural causal\nmodels with large language models to establish causal consistency in\nrecommendation pipelines. Our methodology enforces explanation factors as\ncausal antecedents to recommendation predictions through causal graph\nconstruction and counterfactual adjustment. We particularly address the\nconfounding effect of item popularity that distorts personalization signals in\nexplanations, developing a debiasing mechanism that disentangles genuine user\npreferences from conformity bias. Through comprehensive experiments across\nmultiple recommendation scenarios, we demonstrate that CausalX achieves\nsuperior performance in recommendation accuracy, explanation plausibility, and\nbias mitigation compared to baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08051v1",
    "published_date": "2025-03-11 05:15:37 UTC",
    "updated_date": "2025-03-11 05:15:37 UTC"
  },
  {
    "arxiv_id": "2503.08038v1",
    "title": "Generalized Kullback-Leibler Divergence Loss",
    "authors": [
      "Jiequan Cui",
      "Beier Zhu",
      "Qingshan Xu",
      "Zhuotao Tian",
      "Xiaojuan Qi",
      "Bei Yu",
      "Hanwang Zhang",
      "Richang Hong"
    ],
    "abstract": "In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss\nand mathematically prove that it is equivalent to the Decoupled\nKullback-Leibler (DKL) Divergence loss that consists of (1) a weighted Mean\nSquare Error (wMSE) loss and (2) a Cross-Entropy loss incorporating soft\nlabels. Thanks to the decoupled structure of DKL loss, we have identified two\nareas for improvement. Firstly, we address the limitation of KL loss in\nscenarios like knowledge distillation by breaking its asymmetric optimization\nproperty along with a smoother weight function. This modification effectively\nalleviates convergence challenges in optimization, particularly for classes\nwith high predicted scores in soft labels. Secondly, we introduce class-wise\nglobal information into KL/DKL to reduce bias arising from individual samples.\nWith these two enhancements, we derive the Generalized Kullback-Leibler (GKL)\nDivergence loss and evaluate its effectiveness by conducting experiments on\nCIFAR-10/100, ImageNet, and vision-language datasets, focusing on adversarial\ntraining, and knowledge distillation tasks. Specifically, we achieve new\nstate-of-the-art adversarial robustness on the public leaderboard --\nRobustBench and competitive knowledge distillation performance across\nCIFAR/ImageNet models and CLIP models, demonstrating the substantial practical\nmerits. Our code is available at https://github.com/jiequancui/DKL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "extension of our NeurIPS paper \"Decoupled Kullback-Leibler Divergence\n  Loss\". arXiv admin note: substantial text overlap with arXiv:2305.13948",
    "pdf_url": "http://arxiv.org/pdf/2503.08038v1",
    "published_date": "2025-03-11 04:43:33 UTC",
    "updated_date": "2025-03-11 04:43:33 UTC"
  },
  {
    "arxiv_id": "2503.08037v1",
    "title": "ObjectMover: Generative Object Movement with Video Prior",
    "authors": [
      "Xin Yu",
      "Tianyu Wang",
      "Soo Ye Kim",
      "Paul Guerrero",
      "Xi Chen",
      "Qing Liu",
      "Zhe Lin",
      "Xiaojuan Qi"
    ],
    "abstract": "Simple as it seems, moving an object to another location within an image is,\nin fact, a challenging image-editing task that requires re-harmonizing the\nlighting, adjusting the pose based on perspective, accurately filling occluded\nregions, and ensuring coherent synchronization of shadows and reflections while\nmaintaining the object identity. In this paper, we present ObjectMover, a\ngenerative model that can perform object movement in highly challenging scenes.\nOur key insight is that we model this task as a sequence-to-sequence problem\nand fine-tune a video generation model to leverage its knowledge of consistent\nobject generation across video frames. We show that with this approach, our\nmodel is able to adjust to complex real-world scenarios, handling extreme\nlighting harmonization and object effect movement. As large-scale data for\nobject movement are unavailable, we construct a data generation pipeline using\na modern game engine to synthesize high-quality data pairs. We further propose\na multi-task learning strategy that enables training on real-world video data\nto improve the model generalization. Through extensive experiments, we\ndemonstrate that ObjectMover achieves outstanding results and adapts well to\nreal-world scenarios.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "CVPR 2025, Project Page: https://xinyu-andy.github.io/ObjMover",
    "pdf_url": "http://arxiv.org/pdf/2503.08037v1",
    "published_date": "2025-03-11 04:42:59 UTC",
    "updated_date": "2025-03-11 04:42:59 UTC"
  },
  {
    "arxiv_id": "2503.08032v1",
    "title": "HOFAR: High-Order Augmentation of Flow Autoregressive Transformers",
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan"
    ],
    "abstract": "Flow Matching and Transformer architectures have demonstrated remarkable\nperformance in image generation tasks, with recent work FlowAR [Ren et al.,\n2024] synergistically integrating both paradigms to advance synthesis fidelity.\nHowever, current FlowAR implementations remain constrained by first-order\ntrajectory modeling during the generation process. This paper introduces a\nnovel framework that systematically enhances flow autoregressive transformers\nthrough high-order supervision. We provide theoretical analysis and empirical\nevaluation showing that our High-Order FlowAR (HOFAR) demonstrates measurable\nimprovements in generation quality compared to baseline models. The proposed\napproach advances the understanding of flow-based autoregressive modeling by\nintroducing a systematic framework for analyzing trajectory dynamics through\nhigh-order expansion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08032v1",
    "published_date": "2025-03-11 04:29:22 UTC",
    "updated_date": "2025-03-11 04:29:22 UTC"
  },
  {
    "arxiv_id": "2503.08734v1",
    "title": "Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification",
    "authors": [
      "Aniket Vaidya",
      "Anurag Awasthi"
    ],
    "abstract": "In today's increasingly digital interactions, robust Identity Verification\n(IDV) is crucial for security and trust. Artificial Intelligence (AI) is\ntransforming IDV, enhancing accuracy and fraud detection. This paper introduces\n``Zero to One,'' a holistic conceptual framework for developing AI-powered IDV\nproducts. This paper outlines the foundational problem and research objectives\nthat necessitate a new framework for IDV in the age of AI. It details the\nevolution of identity verification and the current regulatory landscape to\ncontextualize the need for a robust conceptual model. The core of the paper is\nthe presentation of the ``Zero to One'' framework itself, dissecting its four\nessential components: Document Verification, Biometric Verification, Risk\nAssessment, and Orchestration. The paper concludes by discussing the\nimplications of this conceptual model and suggesting future research directions\nfocused on the framework's further development and application. The framework\naddresses security, privacy, UX, and regulatory compliance, offering a\nstructured approach to building effective IDV solutions. Successful IDV\nplatforms require a balanced conceptual understanding of verification methods,\nrisk management, and operational scalability, with AI as a key enabler. This\npaper presents the ``Zero to One'' framework as a refined conceptual model,\ndetailing verification layers, and AI's transformative role in shaping\nnext-generation IDV products.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.08734v1",
    "published_date": "2025-03-11 04:20:02 UTC",
    "updated_date": "2025-03-11 04:20:02 UTC"
  },
  {
    "arxiv_id": "2503.08026v1",
    "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
    "authors": [
      "Zhen Tan",
      "Jun Yan",
      "I-Hung Hsu",
      "Rujun Han",
      "Zifeng Wang",
      "Long T. Le",
      "Yiwen Song",
      "Yanfei Chen",
      "Hamid Palangi",
      "George Lee",
      "Anand Iyer",
      "Tianlong Chen",
      "Huan Liu",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "abstract": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08026v1",
    "published_date": "2025-03-11 04:15:52 UTC",
    "updated_date": "2025-03-11 04:15:52 UTC"
  },
  {
    "arxiv_id": "2503.08732v1",
    "title": "Quantifying Circadian Desynchrony in ICU Patients and Its Association with Delirium",
    "authors": [
      "Yuanfang Ren",
      "Andrea E. Davidson",
      "Jiaqing Zhang",
      "Miguel Contreras",
      "Ayush K. Patel",
      "Michelle Gumz",
      "Tezcan Ozrazgat-Baslanti",
      "Parisa Rashidi",
      "Azra Bihorac"
    ],
    "abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08732v1",
    "published_date": "2025-03-11 03:56:10 UTC",
    "updated_date": "2025-03-11 03:56:10 UTC"
  },
  {
    "arxiv_id": "2503.08012v1",
    "title": "Exploring Bias in over 100 Text-to-Image Generative Models",
    "authors": [
      "Jordan Vice",
      "Naveed Akhtar",
      "Richard Hartley",
      "Ajmal Mian"
    ],
    "abstract": "We investigate bias trends in text-to-image generative models over time,\nfocusing on the increasing availability of models through open platforms like\nHugging Face. While these platforms democratize AI, they also facilitate the\nspread of inherently biased models, often shaped by task-specific fine-tuning.\nEnsuring ethical and transparent AI deployment requires robust evaluation\nframeworks and quantifiable bias metrics. To this end, we assess bias across\nthree key dimensions: (i) distribution bias, (ii) generative hallucination, and\n(iii) generative miss-rate. Analyzing over 100 models, we reveal how bias\npatterns evolve over time and across generative tasks. Our findings indicate\nthat artistic and style-transferred models exhibit significant bias, whereas\nfoundation models, benefiting from broader training distributions, are becoming\nprogressively less biased. By identifying these systemic trends, we contribute\na large-scale evaluation corpus to inform bias research and mitigation\nstrategies, fostering more responsible AI development.\n  Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-Source\nModels",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025 Workshop on Open Science for Foundation Models\n  (SCI-FM)",
    "pdf_url": "http://arxiv.org/pdf/2503.08012v1",
    "published_date": "2025-03-11 03:40:44 UTC",
    "updated_date": "2025-03-11 03:40:44 UTC"
  },
  {
    "arxiv_id": "2503.08010v1",
    "title": "SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation",
    "authors": [
      "Chen Yi Lu",
      "Md Mehrab Tanjim",
      "Ishita Dasgupta",
      "Somdeb Sarkhel",
      "Gang Wu",
      "Saayan Mitra",
      "Somali Chaterji"
    ],
    "abstract": "We present SKALD, a multi-shot video assembly method that constructs coherent\nvideo sequences from candidate shots with minimal reliance on text. Central to\nour approach is the Learned Clip Assembly (LCA) score, a learning-based metric\nthat measures temporal and semantic relationships between shots to quantify\nnarrative coherence. We tackle the exponential complexity of combining multiple\nshots with an efficient beam-search algorithm guided by the LCA score. To train\nour model effectively with limited human annotations, we propose two tasks for\nthe LCA encoder: Shot Coherence Learning, which uses contrastive learning to\ndistinguish coherent and incoherent sequences, and Feature Regression, which\nconverts these learned representations into a real-valued coherence score. We\ndevelop two variants: a base SKALD model that relies solely on visual coherence\nand SKALD-text, which integrates auxiliary text information when available.\nExperiments on the VSPD and our curated MSV3C datasets show that SKALD achieves\nan improvement of up to 48.6% in IoU and a 43% speedup over the\nstate-of-the-art methods. A user study further validates our approach, with 45%\nof participants favoring SKALD-assembled videos, compared to 22% preferring\ntext-based assembly methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08010v1",
    "published_date": "2025-03-11 03:25:44 UTC",
    "updated_date": "2025-03-11 03:25:44 UTC"
  },
  {
    "arxiv_id": "2503.08007v1",
    "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
    "authors": [
      "Han Zhao",
      "Wenxuan Song",
      "Donglin Wang",
      "Xinyang Tong",
      "Pengxiang Ding",
      "Xuelian Cheng",
      "Zongyuan Ge"
    ],
    "abstract": "Developing versatile quadruped robots that can smoothly perform various\nactions and tasks in real-world environments remains a significant challenge.\nThis paper introduces a novel vision-language-action (VLA) model, mixture of\nrobotic experts (MoRE), for quadruped robots that aim to introduce\nreinforcement learning (RL) for fine-tuning large-scale VLA models with a large\namount of mixed-quality data. MoRE integrates multiple low-rank adaptation\nmodules as distinct experts within a dense multi-modal large language model\n(MLLM), forming a sparse-activated mixture-of-experts model. This design\nenables the model to effectively adapt to a wide array of downstream tasks.\nMoreover, we employ a reinforcement learning-based training objective to train\nour model as a Q-function after deeply exploring the structural properties of\nour tasks. Effective learning from automatically collected mixed-quality data\nenhances data efficiency and model performance. Extensive experiments\ndemonstrate that MoRE outperforms all baselines across six different skills and\nexhibits superior generalization capabilities in out-of-distribution scenarios.\nWe further validate our method in real-world scenarios, confirming the\npracticality of our approach and laying a solid foundation for future research\non multi-task learning in quadruped robots.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.08007v1",
    "published_date": "2025-03-11 03:13:45 UTC",
    "updated_date": "2025-03-11 03:13:45 UTC"
  },
  {
    "arxiv_id": "2503.08006v1",
    "title": "Injecting Imbalance Sensitivity for Multi-Task Learning",
    "authors": [
      "Zhipeng Zhou",
      "Liu Liu",
      "Peilin Zhao",
      "Wei Gong"
    ],
    "abstract": "Multi-task learning (MTL) has emerged as a promising approach for deploying\ndeep learning models in real-life applications. Recent studies have proposed\noptimization-based learning paradigms to establish task-shared representations\nin MTL. However, our paper empirically argues that these studies, specifically\ngradient-based ones, primarily emphasize the conflict issue while neglecting\nthe potentially more significant impact of imbalance/dominance in MTL. In line\nwith this perspective, we enhance the existing baseline method by injecting\nimbalance-sensitivity through the imposition of constraints on the projected\nnorms. To demonstrate the effectiveness of our proposed IMbalance-sensitive\nGradient (IMGrad) descent method, we evaluate it on multiple mainstream MTL\nbenchmarks, encompassing supervised learning tasks as well as reinforcement\nlearning. The experimental results consistently demonstrate competitive\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.08006v1",
    "published_date": "2025-03-11 03:11:54 UTC",
    "updated_date": "2025-03-11 03:11:54 UTC"
  },
  {
    "arxiv_id": "2503.07996v4",
    "title": "SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic",
    "authors": [
      "Jikai Chen",
      "Leilei Gan",
      "Ziyu Zhao",
      "Zechuan Wang",
      "Dong Wang",
      "Chenyi Zhuang"
    ],
    "abstract": "Existing refinement methods in LLM-based Text-to-SQL systems exhibit limited\neffectiveness. They often introduce new errors during the self-correction\nprocess and fail to detect and correct semantic inaccuracies. To address these\ngaps, we first introduce a clause-wise critique generation task along with a\nbenchmark, SQLCriticBench, which performs fine-grained error localization\nincluding both syntax and semantic errors at the clause level. Furthermore, we\nintroduce a variant of DPO for training our SQLCritic model, where the $\\beta$\ncoefficient is adaptively changed according to the clause-level inconsistencies\nbetween the preferred and dispreferred critiques. We also propose an\nautomatically training dataset curation pipeline which annotate clause-wise\ncritique at scale in a cost-effective way. Experiments demonstrate that the\nSQLCritic model significantly improves SQL accuracy on the BIRD and Spider\ndatasets, and the results on SQLCriticBench further reveals its superior\ncritique capabilities compared to existing models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07996v4",
    "published_date": "2025-03-11 02:52:39 UTC",
    "updated_date": "2025-05-21 01:48:22 UTC"
  },
  {
    "arxiv_id": "2503.07993v1",
    "title": "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
    "authors": [
      "Rajeev Kumar",
      "Kumar Ishan",
      "Harishankar Kumar",
      "Abhinandan Singla"
    ],
    "abstract": "Disconnected data silos within enterprises obstruct the extraction of\nactionable insights, diminishing efficiency in areas such as product\ndevelopment, client engagement, meeting preparation, and analytics-driven\ndecision-making. This paper introduces a framework that uses large language\nmodels (LLMs) to unify various data sources into a comprehensive,\nactivity-centric knowledge graph. The framework automates tasks such as entity\nextraction, relationship inference, and semantic enrichment, enabling advanced\nquerying, reasoning, and analytics across data types like emails, calendars,\nchats, documents, and logs. Designed for enterprise flexibility, it supports\napplications such as contextual search, task prioritization, expertise\ndiscovery, personalized recommendations, and advanced analytics to identify\ntrends and actionable insights. Experimental results demonstrate its success in\nthe discovery of expertise, task management, and data-driven decision making.\nBy integrating LLMs with knowledge graphs, this solution bridges disconnected\nsystems and delivers intelligent analytics-powered enterprise tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07993v1",
    "published_date": "2025-03-11 02:50:45 UTC",
    "updated_date": "2025-03-11 02:50:45 UTC"
  },
  {
    "arxiv_id": "2503.07994v1",
    "title": "A Neural Symbolic Model for Space Physics",
    "authors": [
      "Jie Ying",
      "Haowei Lin",
      "Chao Yue",
      "Yajie Chen",
      "Chao Xiao",
      "Quanqi Shi",
      "Yitao Liang",
      "Shing-Tung Yau",
      "Yuan Zhou",
      "Jianzhu Ma"
    ],
    "abstract": "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.AI",
      "physics.space-ph"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07994v1",
    "published_date": "2025-03-11 02:50:45 UTC",
    "updated_date": "2025-03-11 02:50:45 UTC"
  },
  {
    "arxiv_id": "2503.07992v1",
    "title": "Efficient and Accurate Estimation of Lipschitz Constants for Hybrid Quantum-Classical Decision Models",
    "authors": [
      "Sajjad Hashemian",
      "Mohammad Saeed Arvenaghi"
    ],
    "abstract": "In this paper, we propose a novel framework for efficiently and accurately\nestimating Lipschitz constants in hybrid quantum-classical decision models. Our\napproach integrates classical neural network with quantum variational circuits\nto address critical issues in learning theory such as fairness verification,\nrobust training, and generalization.\n  By a unified convex optimization formulation, we extend existing classical\nmethods to capture the interplay between classical and quantum layers. This\nintegrated strategy not only provide a tight bound on the Lipschitz constant\nbut also improves computational efficiency with respect to the previous\nmethods.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "14 pages, 5 figuers, Submitted to TASE 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.07992v1",
    "published_date": "2025-03-11 02:50:16 UTC",
    "updated_date": "2025-03-11 02:50:16 UTC"
  },
  {
    "arxiv_id": "2503.07991v1",
    "title": "Boundary Prompting: Elastic Urban Region Representation via Graph-based Spatial Tokenization",
    "authors": [
      "Haojia Zhu",
      "Jiahui Jin",
      "Dong Kan",
      "Rouxi Shen",
      "Ruize Wang",
      "Xiangguo Sun",
      "Jinghui Zhang"
    ],
    "abstract": "Urban region representation is essential for various applications such as\nurban planning, resource allocation, and policy development. Traditional\nmethods rely on fixed, predefined region boundaries, which fail to capture the\ndynamic and complex nature of real-world urban areas. In this paper, we propose\nthe Boundary Prompting Urban Region Representation Framework (BPURF), a novel\napproach that allows for elastic urban region definitions. BPURF comprises two\nkey components: (1) A spatial token dictionary, where urban entities are\ntreated as tokens and integrated into a unified token graph, and (2) a region\ntoken set representation model which utilize token aggregation and a\nmulti-channel model to embed token sets corresponding to region boundaries.\nAdditionally, we propose fast token set extraction strategy to enable online\ntoken set extraction during training and prompting. This framework enables the\ndefinition of urban regions through boundary prompting, supporting varying\nregion boundaries and adapting to different tasks. Extensive experiments\ndemonstrate the effectiveness of BPURF in capturing the complex characteristics\nof urban regions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07991v1",
    "published_date": "2025-03-11 02:49:58 UTC",
    "updated_date": "2025-03-11 02:49:58 UTC"
  },
  {
    "arxiv_id": "2503.07988v1",
    "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
    "authors": [
      "Zhiyong Wang",
      "Chen Yang",
      "John C. S. Lui",
      "Dongruo Zhou"
    ],
    "abstract": "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.07988v1",
    "published_date": "2025-03-11 02:44:32 UTC",
    "updated_date": "2025-03-11 02:44:32 UTC"
  },
  {
    "arxiv_id": "2503.10677v2",
    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
    "authors": [
      "Mingyue Cheng",
      "Yucong Luo",
      "Jie Ouyang",
      "Qi Liu",
      "Huijie Liu",
      "Li Li",
      "Shuo Yu",
      "Bohou Zhang",
      "Jiawei Cao",
      "Jie Ma",
      "Daoyu Wang",
      "Enhong Chen"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10677v2",
    "published_date": "2025-03-11 01:59:35 UTC",
    "updated_date": "2025-03-17 11:24:11 UTC"
  },
  {
    "arxiv_id": "2503.07963v2",
    "title": "Hierarchical Contact-Rich Trajectory Optimization for Multi-Modal Manipulation using Tight Convex Relaxations",
    "authors": [
      "Yuki Shirai",
      "Arvind Raghunathan",
      "Devesh K. Jha"
    ],
    "abstract": "Designing trajectories for manipulation through contact is challenging as it\nrequires reasoning of object \\& robot trajectories as well as complex contact\nsequences simultaneously. In this paper, we present a novel framework for\nsimultaneously designing trajectories of robots, objects, and contacts\nefficiently for contact-rich manipulation. We propose a hierarchical\noptimization framework where Mixed-Integer Linear Program (MILP) selects\noptimal contacts between robot \\& object using approximate dynamical\nconstraints, and then a NonLinear Program (NLP) optimizes trajectory of the\nrobot(s) and object considering full nonlinear constraints. We present a convex\nrelaxation of bilinear constraints using binary encoding technique such that\nMILP can provide tighter solutions with better computational complexity. The\nproposed framework is evaluated on various manipulation tasks where it can\nreason about complex multi-contact interactions while providing computational\nadvantages. We also demonstrate our framework in hardware experiments using a\nbimanual robot system. The video summarizing this paper and hardware\nexperiments is found https://youtu.be/s2S1Eg5RsRE?si=chPkftz_a3NAHxLq",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "2025 IEEE International Conference on Robotics and Automation (2025\n  ICRA)",
    "pdf_url": "http://arxiv.org/pdf/2503.07963v2",
    "published_date": "2025-03-11 01:40:23 UTC",
    "updated_date": "2025-03-12 01:43:20 UTC"
  },
  {
    "arxiv_id": "2503.07956v1",
    "title": "EFPC: Towards Efficient and Flexible Prompt Compression",
    "authors": [
      "Yun-Hao Cao",
      "Yangsong Wang",
      "Shuzheng Hao",
      "Zhenxing Li",
      "Chengjun Zhan",
      "Sichao Liu",
      "Yi-Qi Hu"
    ],
    "abstract": "The emergence of large language models (LLMs) like GPT-4 has revolutionized\nnatural language processing (NLP), enabling diverse, complex tasks. However,\nextensive token counts lead to high computational and financial burdens. To\naddress this, we propose Efficient and Flexible Prompt Compression (EFPC), a\nnovel method unifying task-aware and task-agnostic compression for a favorable\naccuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts\nand integrates them with original prompts for training. During training and\ninference, we selectively prepend user instructions and compress prompts based\non predicted probabilities. EFPC is highly data-efficient, achieving\nsignificant performance with minimal data. Compared to the state-of-the-art\nmethod LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with\n1% additional data at a 4x compression rate, and an 11.4% gain with 10%\nadditional data on the LongBench single-doc QA benchmark. EFPC's unified\nframework supports broad applicability and enhances performance across various\nmodels, tasks, and domains, offering a practical advancement in NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.07956v1",
    "published_date": "2025-03-11 01:34:03 UTC",
    "updated_date": "2025-03-11 01:34:03 UTC"
  },
  {
    "arxiv_id": "2503.09626v1",
    "title": "Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes",
    "authors": [
      "Qi Wu",
      "Yingguang Yang",
      "hao liu",
      "Hao Peng",
      "Buyun He",
      "Yutong Xia",
      "Yong Liao"
    ],
    "abstract": "Social bot detection is crucial for mitigating misinformation, online\nmanipulation, and coordinated inauthentic behavior. While existing neural\nnetwork-based detectors perform well on benchmarks, they struggle with\ngeneralization due to distribution shifts across datasets and frequently\nproduce overconfident predictions for out-of-distribution accounts beyond the\ntraining data. To address this, we introduce a novel Uncertainty Estimation for\nSocial Bot Detection (UESBD) framework, which quantifies the predictive\nuncertainty of detectors beyond mere classification. For this task, we propose\nRobust Multi-modal Neural Processes (RMNP), which aims to enhance the\nrobustness of multi-modal neural processes to modality inconsistencies caused\nby social bot camouflage. RMNP first learns unimodal representations through\nmodality-specific encoders. Then, unimodal attentive neural processes are\nemployed to encode the Gaussian distribution of unimodal latent variables.\nFurthermore, to avoid social bots stealing human features to camouflage\nthemselves thus causing certain modalities to provide conflictive information,\nwe introduce an evidential gating network to explicitly model the reliability\nof modalities. The joint latent distribution is learned through the generalized\nproduct of experts, which takes the reliability of each modality into\nconsideration during fusion. The final prediction is obtained through Monte\nCarlo sampling of the joint latent distribution followed by a decoder.\nExperiments on three real-world benchmarks show the effectiveness of RMNP in\nclassification and uncertainty estimation, as well as its robustness to\nmodality conflicts.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "12 pages. 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09626v1",
    "published_date": "2025-03-11 01:32:52 UTC",
    "updated_date": "2025-03-11 01:32:52 UTC"
  },
  {
    "arxiv_id": "2503.08729v1",
    "title": "Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models",
    "authors": [
      "Ishaan Malhi",
      "Praneet Dutta",
      "Ellie Talius",
      "Sally Ma",
      "Brendan Driscoll",
      "Krista Holden",
      "Garima Pruthi",
      "Arunachalam Narayanaswamy"
    ],
    "abstract": "We present a framework for high-fidelity product image recontextualization\nusing text-to-image diffusion models and a novel data augmentation pipeline.\nThis pipeline leverages image-to-video diffusion, in/outpainting & negatives to\ncreate synthetic training data, addressing limitations of real-world data\ncollection for this task. Our method improves the quality and diversity of\ngenerated images by disentangling product representations and enhancing the\nmodel's understanding of product characteristics. Evaluation on the ABO dataset\nand a private product dataset, using automated metrics and human assessment,\ndemonstrates the effectiveness of our framework in generating realistic and\ncompelling product visualizations, with implications for applications such as\ne-commerce and virtual product showcasing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08729v1",
    "published_date": "2025-03-11 01:24:39 UTC",
    "updated_date": "2025-03-11 01:24:39 UTC"
  },
  {
    "arxiv_id": "2503.08728v1",
    "title": "Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse",
    "authors": [
      "Yihong Li",
      "Chengwei Zhang",
      "Furui Zhan",
      "Wanting Liu",
      "Kailing Zhou",
      "Longji Zheng"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08728v1",
    "published_date": "2025-03-11 01:21:13 UTC",
    "updated_date": "2025-03-11 01:21:13 UTC"
  },
  {
    "arxiv_id": "2503.07946v1",
    "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
    "authors": [
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Anwesa Choudhuri",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a\nfundamental challenge in computer graphics. While recent advances in Gaussian\nSplatting have shown promising results separately handling dynamic scenes\n(4DGS) and view-dependent effects (6DGS), no existing method unifies these\ncapabilities while maintaining real-time performance. We present 7D Gaussian\nSplatting (7DGS), a unified framework representing scene elements as\nseven-dimensional Gaussians spanning position (3D), time (1D), and viewing\ndirection (3D). Our key contribution is an efficient conditional slicing\nmechanism that transforms 7D Gaussians into view- and time-conditioned 3D\nGaussians, maintaining compatibility with existing 3D Gaussian Splatting\npipelines while enabling joint optimization. Experiments demonstrate that 7DGS\noutperforms prior methods by up to 7.36 dB in PSNR while achieving real-time\nrendering (401 FPS) on challenging dynamic scenes with complex view-dependent\neffects. The project page is: https://gaozhongpai.github.io/7dgs/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07946v1",
    "published_date": "2025-03-11 01:16:08 UTC",
    "updated_date": "2025-03-11 01:16:08 UTC"
  },
  {
    "arxiv_id": "2503.08727v2",
    "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
    "authors": [
      "Lucas Caccia",
      "Alan Ansell",
      "Edoardo Ponti",
      "Ivan Vulić",
      "Alessandro Sordoni"
    ],
    "abstract": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.08727v2",
    "published_date": "2025-03-11 01:07:57 UTC",
    "updated_date": "2025-04-29 17:11:44 UTC"
  },
  {
    "arxiv_id": "2503.08726v1",
    "title": "SIMAC: A Semantic-Driven Integrated Multimodal Sensing And Communication Framework",
    "authors": [
      "Yubo Peng",
      "Luping Xiang",
      "Kun Yang",
      "Feibo Jiang",
      "Kezhi Wang",
      "Dapeng Oliver Wu"
    ],
    "abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08726v1",
    "published_date": "2025-03-11 01:04:42 UTC",
    "updated_date": "2025-03-11 01:04:42 UTC"
  },
  {
    "arxiv_id": "2503.07937v1",
    "title": "LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim Verification",
    "authors": [
      "Siyuan Wang",
      "James R. Foulds",
      "Md Osman Gani",
      "Shimei Pan"
    ],
    "abstract": "In this paper, we introduce CIBER (Claim Investigation Based on Evidence\nRetrieval), an extension of the Retrieval-Augmented Generation (RAG) framework\ndesigned to identify corroborating and refuting documents as evidence for\nscientific claim verification. CIBER addresses the inherent uncertainty in\nLarge Language Models (LLMs) by evaluating response consistency across diverse\ninterrogation probes. By focusing on the behavioral analysis of LLMs without\nrequiring access to their internal information, CIBER is applicable to both\nwhite-box and black-box models. Furthermore, CIBER operates in an unsupervised\nmanner, enabling easy generalization across various scientific domains.\nComprehensive evaluations conducted using LLMs with varying levels of\nlinguistic proficiency reveal CIBER's superior performance compared to\nconventional RAG approaches. These findings not only highlight the\neffectiveness of CIBER but also provide valuable insights for future\nadvancements in LLM-based scientific claim verification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07937v1",
    "published_date": "2025-03-11 00:29:50 UTC",
    "updated_date": "2025-03-11 00:29:50 UTC"
  },
  {
    "arxiv_id": "2503.07932v1",
    "title": "A Theory of Learning with Autoregressive Chain of Thought",
    "authors": [
      "Nirmit Joshi",
      "Gal Vardi",
      "Adam Block",
      "Surbhi Goel",
      "Zhiyuan Li",
      "Theodor Misiakiewicz",
      "Nathan Srebro"
    ],
    "abstract": "For a given base class of sequence-to-next-token generators, we consider\nlearning prompt-to-answer mappings obtained by iterating a fixed,\ntime-invariant generator for multiple steps, thus generating a\nchain-of-thought, and then taking the final token as the answer. We formalize\nthe learning problems both when the chain-of-thought is observed and when\ntraining only on prompt-answer pairs, with the chain-of-thought latent. We\nanalyze the sample and computational complexity both in terms of general\nproperties of the base class (e.g. its VC dimension) and for specific base\nclasses such as linear thresholds. We present a simple base class that allows\nfor universal representability and computationally tractable chain-of-thought\nlearning. Central to our development is that time invariance allows for sample\ncomplexity that is independent of the length of the chain-of-thought. Attention\narises naturally in our construction.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CC",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2503.07932v1",
    "published_date": "2025-03-11 00:21:32 UTC",
    "updated_date": "2025-03-11 00:21:32 UTC"
  },
  {
    "arxiv_id": "2503.08725v2",
    "title": "The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government",
    "authors": [
      "Zeynep Engin",
      "Jon Crowcroft",
      "David Hand",
      "Philip Treleaven"
    ],
    "abstract": "As artificial intelligence transforms public sector operations, governments\nstruggle to integrate technological innovations into coherent systems for\neffective service delivery. This paper introduces the Algorithmic State\nArchitecture (ASA), a novel four-layer framework conceptualising how Digital\nPublic Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and\nGovTech interact as an integrated system in AI-enabled states. Unlike\napproaches that treat these as parallel developments, ASA positions them as\ninterdependent layers with specific enabling relationships and feedback\nmechanisms. Through comparative analysis of implementations in Estonia,\nSingapore, India, and the UK, we demonstrate how foundational digital\ninfrastructure enables systematic data collection, which powers algorithmic\ndecision-making processes, ultimately manifesting in user-facing services. Our\nanalysis reveals that successful implementations require balanced development\nacross all layers, with particular attention to integration mechanisms between\nthem. The framework contributes to both theory and practice by bridging\npreviously disconnected domains of digital government research, identifying\ncritical dependencies that influence implementation success, and providing a\nstructured approach for analysing the maturity and development pathways of\nAI-enabled government systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CY",
    "comment": "Main text: 25 pages, with references: 35 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.08725v2",
    "published_date": "2025-03-11 00:20:56 UTC",
    "updated_date": "2025-03-13 11:16:38 UTC"
  },
  {
    "arxiv_id": "2503.07928v2",
    "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course",
    "authors": [
      "Hunter McNichols",
      "Andrew Lan"
    ],
    "abstract": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be analyzed to ensure ethical\nusage of these tools. To better understand how students interact with LLMs in\nan academic setting, we introduce \\textbf{StudyChat}, a publicly available\ndataset capturing real-world student interactions with an LLM-powered tutoring\nchatbot in a semester-long, university-level artificial intelligence (AI)\ncourse. We deploy a web application that replicates ChatGPT's core\nfunctionalities, and use it to log student interactions with the LLM while\nworking on programming assignments. We collect 1,197 conversations, which we\nannotate using a dialogue act labeling schema inspired by observed interaction\npatterns and prior research. Additionally, we analyze these interactions,\nhighlight behavioral trends, and analyze how specific usage patterns relate to\ncourse outcomes. \\textbf{StudyChat} provides a rich resource for the learning\nsciences and AI in education communities, enabling further research into the\nevolving role of LLMs in education.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-print v0.2",
    "pdf_url": "http://arxiv.org/pdf/2503.07928v2",
    "published_date": "2025-03-11 00:17:07 UTC",
    "updated_date": "2025-04-12 02:42:06 UTC"
  }
]
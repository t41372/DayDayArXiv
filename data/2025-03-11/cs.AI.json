{
  "date": "2025-03-11",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-11 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 生成模型、LLM 应用、强化学习和机器人技术等领域，强调高效算法设计、模型鲁棒性和多模态融合，令人印象深刻的是 YuE 在长序列音乐生成的突破，以及 ProtTeX 在蛋白质结构预测的可解释性创新，而知名学者如 Yong Chen 和 Jian Yang 的作品则展示了跨领域影响。\n\n### 重点论文讨论\n我们先聊聊今天最有话题度和影响力的论文，这些多涉及 AI 生成、LLM 和机器人领域，展示了前沿进展。相关论文按主题归类讨论。\n\n**AI 生成模型与扩散技术**  \n- **FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting（FPGS: 前馈语义感知光照真实风格转移的大型高斯光栅化）**  \n  这篇论文提出了一种前馈式方法，用于对大型 3D 场景进行光照真实风格转移，支持多参考图像而无需额外优化，保持多视图一致性和实时渲染。主要贡献是引入语义分解 3D 特征场和局部 AdaIN，提升了大规模静态/动态场景的风格化质量，实验显示其在光照真实性上优于现有方法。\n\n- **YuE: Scaling Open Foundation Models for Long-Form Music Generation（YuE: 扩展开源基础模型用于长序列音乐生成）**  \n  作者包括知名学者 Yong Chen 和 Yike Guo，这篇工作令人印象深刻地扩展了 LLaMA2 架构，支持基于歌词的 5 分钟音乐生成。主要发现是通过轨道解耦和多任务预训练，实现语义对齐和结构一致，实验证明其在音乐生成质量和泛化性上超越专有系统，项目页面提供代码。\n\n- **MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention（MEAT: 使用网格注意力的高像素多视图扩散模型进行人体生成）**  \n  这篇论文开发了一种多视图扩散模型，用于高分辨率人体生成，通过网格注意力处理跨视图一致性。主要贡献是提升了生成细节和几何精度，实验在人体图像生成上表现出色，代码已开源。\n\n**LLM 和推理优化**  \n- **ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models（ProtTeX: 使用大语言模型的结构语境推理和蛋白质编辑）**  \n  作者 Yuan Zhou 和 Jian Yang 参与，这篇创新工作将蛋白质序列和结构整合到统一离散空间，使用 LLM 进行多模态蛋白质推理。主要发现是提升了蛋白质功能预测和生成精度，实验显示其在蛋白质任务上优于现有方法，代码已开源。\n\n- **Chain-of-Thought Reasoning In The Wild Is Not Always Faithful（链式思维推理在实际中并非总是可靠）**  \n  这篇论文分析了 LLM 的链式思维推理可能出现不一致问题，如后验合理化和错误修正。主要贡献是通过实验揭示 LLM 在二元问题上的逻辑矛盾，强调了监控推理过程的重要性，对 AI 安全研究有启发。\n\n- **SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation（SKALD: 基于学习的镜头组装用于连贯多镜头视频创建）**  \n  论文提出使用学习分数优化视频镜头组装，确保时序和语义连贯。主要发现是通过对比学习和特征回归提升视频生成质量，实验在视频基准上表现出色。\n\n**机器人和强化学习**  \n- **MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models（MoRE: 解锁四足机器人视觉-语言-动作模型的强化学习可扩展性）**  \n  这篇工作使用混合专家模型提升四足机器人的多任务学习能力，主要贡献是通过强化学习处理混合数据，实现高鲁棒性，实验在真实机器人场景中验证了其性能。\n\n- **7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting（7DGS: 统一空间-时间-角度的高斯光栅化）**  \n  论文统一了动态场景和高斯光栅化框架，支持实时渲染。主要发现是通过条件切片机制处理 7D 高斯分布，实验在复杂视图下提升了渲染效率。\n\n其他论文涉及领域较广，如医疗 AI、偏置检测和优化算法，但我们快速掠过不那么核心的 ones。例如，**Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data（鲁棒的无监督故障诊断用于高维非线性噪声数据）** 提出了一种降维和图学习方法，提升了噪声环境下诊断精度；**Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents（困惑陷阱: 基于 PLM 的检索器过度评估低困惑文档）** 揭示了语言模型的偏置问题，通过因果分析提出纠偏方法。这些工作虽有技术贡献，但影响力相对有限，仅供感兴趣读者参考。\n\n今天的 arXiv 更新展示了 AI 领域的快速迭代，重点论文如 YuE 和 ProtTeX 值得关注，期待它们在实际应用中的落地。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2503.09635v1",
      "title": "FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "GeonU Kim",
        "Kim Youwang",
        "Lee Hyoseok",
        "Tae-Hyun Oh"
      ],
      "abstract": "We present FPGS, a feed-forward photorealistic style transfer method of\nlarge-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes\nlarge-scale 3D scenes with arbitrary, multiple style reference images without\nadditional optimization while preserving multi-view consistency and real-time\nrendering speed of 3D Gaussians. Prior arts required tedious per-style\noptimization or time-consuming per-scene training stage and were limited to\nsmall-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by\nintroducing a style-decomposed 3D feature field, which inherits AdaIN's\nfeed-forward stylization machinery, supporting arbitrary style reference\nimages. Furthermore, FPGS supports multi-reference stylization with the\nsemantic correspondence matching and local AdaIN, which adds diverse user\ncontrol for 3D scene styles. FPGS also preserves multi-view consistency by\napplying semantic matching and style transfer processes directly onto queried\nfeatures in 3D space. In experiments, we demonstrate that FPGS achieves\nfavorable photorealistic quality scene stylization for large-scale static and\ndynamic 3D scenes with diverse reference images. Project page:\nhttps://kim-geonu.github.io/FPGS/",
      "tldr_zh": "该研究提出了 FPGS，一种前馈(feed-forward)语义感知的光照真实风格转移方法，针对大规模 Gaussian Splatting 表示的辐射场，能够使用任意多参考图像风格化 3D 场景，同时保持多视图一致性和实时渲染速度。FPGS 通过引入风格分解的 3D 特征场（继承自 AdaIN 的机制），支持任意风格参考图像，并通过语义对应匹配和局部 AdaIN 实现多参考风格化，提供用户对场景风格的多样控制。与以往方法相比，它避免了每个风格的优化或每个场景的训练限制。实验结果显示，FPGS 在大规模静态和动态 3D 场景中实现了出色的光照真实质量风格化。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://kim-geonu.github.io/FPGS/. arXiv admin note:\n  substantial text overlap with arXiv:2401.05516",
      "pdf_url": "http://arxiv.org/pdf/2503.09635v1",
      "published_date": "2025-03-11 23:52:56 UTC",
      "updated_date": "2025-03-11 23:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:37:00.130455"
    },
    {
      "arxiv_id": "2503.08960v1",
      "title": "Are ECGs enough? Deep learning classification of cardiac anomalies using only electrocardiograms",
      "title_zh": "翻译失败",
      "authors": [
        "Joao D. S. Marques",
        "Arlindo L. Oliveira"
      ],
      "abstract": "Electrocardiography (ECG) is an essential tool for diagnosing multiple\ncardiac anomalies: it provides valuable clinical insights, while being\naffordable, fast and available in many settings. However, in the current\nliterature, the role of ECG analysis is often unclear: many approaches either\nrely on additional imaging modalities, such as Computed Tomography Pulmonary\nAngiography (CTPA), which may not always be available, or do not effectively\ngeneralize across different classification problems. Furthermore, the\navailability of public ECG datasets is limited and, in practice, these datasets\ntend to be small, making it essential to optimize learning strategies. In this\nstudy, we investigate the performance of multiple neural network architectures\nin order to assess the impact of various approaches. Moreover, we check whether\nthese practices enhance model generalization when transfer learning is used to\ntranslate information learned in larger ECG datasets, such as PTB-XL and\nCPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE)\ndetection. By leveraging transfer learning, we analyze the extent to which we\ncan improve learning efficiency and predictive performance on limited data.\nCode available at\nhttps://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .",
      "tldr_zh": "这篇论文探讨了是否仅使用 Electrocardiograms (ECGs) 即可进行心脏异常的深度学习分类，而不依赖其他成像方式，如 Computed Tomography Pulmonary Angiography (CTPA)。研究者评估了多种 neural network 架构的表现，并通过 transfer learning 将从较大数据集（如 PTB-XL 和 CPSC18）中学到的知识转移到较小、更具挑战性的肺栓塞（PE）检测数据集。结果显示，这种方法显著优化了学习策略，提高了模型在有限数据下的泛化能力、学习效率和预测性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08960v1",
      "published_date": "2025-03-11 23:37:18 UTC",
      "updated_date": "2025-03-11 23:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:37:11.852124"
    },
    {
      "arxiv_id": "2503.08950v1",
      "title": "FP3: A 3D Foundation Policy for Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Rujia Yang",
        "Geng Chen",
        "Chuan Wen",
        "Yang Gao"
      ],
      "abstract": "Following its success in natural language processing and computer vision,\nfoundation models that are pre-trained on large-scale multi-task datasets have\nalso shown great potential in robotics. However, most existing robot foundation\nmodels rely solely on 2D image observations, ignoring 3D geometric information,\nwhich is essential for robots to perceive and reason about the 3D world. In\nthis paper, we introduce FP3, a first large-scale 3D foundation policy model\nfor robotic manipulation. FP3 builds on a scalable diffusion transformer\narchitecture and is pre-trained on 60k trajectories with point cloud\nobservations. With the model design and diverse pre-training data, FP3 can be\nefficiently fine-tuned for downstream tasks while exhibiting strong\ngeneralization capabilities. Experiments on real robots demonstrate that with\nonly 80 demonstrations, FP3 is able to learn a new task with over 90% success\nrates in novel environments with unseen objects, significantly surpassing\nexisting robot foundation models.",
      "tldr_zh": "该论文提出FP3，一种首个大规模3D foundation policy模型，用于机器人操作，以解决现有模型依赖2D图像而忽略3D几何信息的局限。FP3基于可扩展的diffusion transformer架构，使用point cloud observations预训练于6万条轨迹，从而实现高效微调和强泛化能力。实验结果显示，在真实机器人环境中，仅需80个演示，FP3即可在新任务和未见对象中实现超过90%的成功率，显著优于现有机器人foundation models。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://3d-foundation-policy.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.08950v1",
      "published_date": "2025-03-11 23:01:08 UTC",
      "updated_date": "2025-03-11 23:01:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:37:23.200053"
    },
    {
      "arxiv_id": "2503.08939v1",
      "title": "KAN-Mixers: a new deep learning architecture for image classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jorge Luiz dos Santos Canuto",
        "Linnyer Beatrys Ruiz Aylon",
        "Rodrigo Clemente Thom de Souza"
      ],
      "abstract": "Due to their effective performance, Convolutional Neural Network (CNN) and\nVision Transformer (ViT) architectures have become the standard for solving\ncomputer vision tasks. Such architectures require large data sets and rely on\nconvolution and self-attention operations. In 2021, MLP-Mixer emerged, an\narchitecture that relies only on Multilayer Perceptron (MLP) and achieves\nextremely competitive results when compared to CNNs and ViTs. Despite its good\nperformance in computer vision tasks, the MLP-Mixer architecture may not be\nsuitable for refined feature extraction in images. Recently, the\nKolmogorov-Arnold Network (KAN) was proposed as a promising alternative to MLP\nmodels. KANs promise to improve accuracy and interpretability when compared to\nMLPs. Therefore, the present work aims to design a new mixer-based\narchitecture, called KAN-Mixers, using KANs as main layers and evaluate its\nperformance, in terms of several performance metrics, in the image\nclassification task. As main results obtained, the KAN-Mixers model was\nsuperior to the MLP, MLP-Mixer and KAN models in the Fashion-MNIST and CIFAR-10\ndatasets, with 0.9030 and 0.6980 of average accuracy, respectively.",
      "tldr_zh": "本研究提出了一种新的深度学习架构KAN-Mixers，用于图像分类任务，以克服传统CNN和Vision Transformer (ViT)对卷积和自注意力机制的依赖，并提升MLP-Mixer在精细特征提取方面的不足。KAN-Mixers以Kolmogorov-Arnold Network (KAN)作为主要层，构建了一个基于mixer的模型，旨在提高准确性和可解释性。通过实验评估，该模型在Fashion-MNIST和CIFAR-10数据集上表现优于MLP、MLP-Mixer和KAN模型，分别实现了0.9030和0.6980的平均准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08939v1",
      "published_date": "2025-03-11 22:41:22 UTC",
      "updated_date": "2025-03-11 22:41:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:37:38.197513"
    },
    {
      "arxiv_id": "2503.08936v1",
      "title": "Simulator Ensembles for Trustworthy Autonomous Driving Testing",
      "title_zh": "用于可信自动驾驶测试的模拟器集成",
      "authors": [
        "Lev Sorokin",
        "Matteo Biagiola",
        "Andrea Stocco"
      ],
      "abstract": "Scenario-based testing with driving simulators is extensively used to\nidentify failing conditions of automated driving assistance systems (ADAS) and\nreduce the amount of in-field road testing. However, existing studies have\nshown that repeated test execution in the same as well as in distinct\nsimulators can yield different outcomes, which can be attributed to sources of\nflakiness or different implementations of the physics, among other factors. In\nthis paper, we present MultiSim, a novel approach to multi-simulation ADAS\ntesting based on a search-based testing approach that leverages an ensemble of\nsimulators to identify failure-inducing, simulator-agnostic test scenarios.\nDuring the search, each scenario is evaluated jointly on multiple simulators.\nScenarios that produce consistent results across simulators are prioritized for\nfurther exploration, while those that fail on only a subset of simulators are\ngiven less priority, as they may reflect simulator-specific issues rather than\ngeneralizable failures. Our case study, which involves testing a deep neural\nnetwork-based ADAS on different pairs of three widely used simulators,\ndemonstrates that MultiSim outperforms single-simulator testing by achieving on\naverage a higher rate of simulator-agnostic failures by 51%. Compared to a\nstate-of-the-art multi-simulator approach that combines the outcome of\nindependent test generation campaigns obtained in different simulators,\nMultiSim identifies 54% more simulator-agnostic failing tests while showing a\ncomparable validity rate. An enhancement of MultiSim that leverages surrogate\nmodels to predict simulator disagreements and bypass executions does not only\nincrease the average number of valid failures but also improves efficiency in\nfinding the first valid failure.",
      "tldr_zh": "该研究提出 MultiSim，一种基于模拟器集合（ensembles）的搜索测试方法，用于提升自动驾驶辅助系统（ADAS）的可信测试，通过识别在多个模拟器中一致的失败场景。MultiSim 在测试过程中联合评估每个场景，优先探索跨模拟器产生一致结果的测试，以避免模拟器特定问题。实验结果显示，与单模拟器测试相比，MultiSim 平均提高了51%的模拟器无关失败率，并比现有多模拟器方法多识别54%的有效失败测试；此外，其增强版本利用代理模型预测模拟器分歧，进一步提高了测试效率和有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08936v1",
      "published_date": "2025-03-11 22:34:14 UTC",
      "updated_date": "2025-03-11 22:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:37:47.293061"
    },
    {
      "arxiv_id": "2503.08929v1",
      "title": "HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Hrishikesh Viswanath",
        "Md Ashiqur Rahman",
        "Chi Lin",
        "Damon Conover",
        "Aniket Bera"
      ],
      "abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
      "tldr_zh": "该研究提出HessianForge框架，用于大规模室外环境的LiDAR重建，通过物理信息神经表示和平滑能量约束来解决现有方法在噪声和稀疏输入下产生的表面伪影问题。具体而言，该方法将表面映射表述为基于物理的能量优化问题，使用深度学习学习带符号距离场(SDF)，并结合分层八叉树编码、多尺度神经网络和测试时最小二乘优化来强制表面平滑和保持特征。实验结果显示，在大型室外数据集上，HessianForge在准确性和平滑性方面优于现有最先进方法。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08929v1",
      "published_date": "2025-03-11 22:18:51 UTC",
      "updated_date": "2025-03-11 22:18:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:38:00.104016"
    },
    {
      "arxiv_id": "2503.08919v1",
      "title": "Backtracking for Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Bilgehan Sel",
        "Dingcheng Li",
        "Phillip Wallis",
        "Vaishakh Keshava",
        "Ming Jin",
        "Siddhartha Reddy Jonnalagadda"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)的安全问题，指出现有方法如 supervised fine-tuning 和 reinforcement learning 容易受对抗攻击影响，且仅专注于初始输出的有害内容，无法有效处理长生成文本中的 toxicity。作者提出了一种新型 backtracking 方法，当生成过程中出现安全违规时，允许模型回退到更安全的生成状态，从而针对性修正问题段落而不需丢弃整个文本。实验结果显示，该方法显著降低了 toxicity 的出现，同时对生成效率的影响最小。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08919v1",
      "published_date": "2025-03-11 22:04:22 UTC",
      "updated_date": "2025-03-11 22:04:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:38:12.111012"
    },
    {
      "arxiv_id": "2503.08916v1",
      "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
      "title_zh": "针对高维非线性噪声数据的鲁棒无监督故障诊断",
      "authors": [
        "Dandan Zhao",
        "Hongpeng Yin",
        "Jintang Bian",
        "Han Zhou"
      ],
      "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise.",
      "tldr_zh": "本论文提出了一种鲁棒的无监督故障诊断方法，针对高维、非线性且噪声大的复杂数据问题。首先，该方法设计了特殊的降维技术，并通过学习图结构增强提取特征的非线性信息。其次，引入 $l_{2,1}$-norm 和 typicality-aware 约束来优化模型，减轻噪声和异常值对诊断准确率的影响。实验结果显示，该方法在 Tennessee-Eastman 过程和真实热钢轧制过程中表现出色，比其他方法更具鲁棒性，即使存在干扰也能维持高诊断准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08916v1",
      "published_date": "2025-03-11 21:55:46 UTC",
      "updated_date": "2025-03-11 21:55:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:38:23.928127"
    },
    {
      "arxiv_id": "2503.08908v1",
      "title": "Interpreting the Repeated Token Phenomenon in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Itay Yona",
        "Ilia Shumailov",
        "Jamie Hayes",
        "Federico Barbero",
        "Yossi Gandelsman"
      ],
      "abstract": "Large Language Models (LLMs), despite their impressive capabilities, often\nfail to accurately repeat a single word when prompted to, and instead output\nunrelated text. This unexplained failure mode represents a vulnerability,\nallowing even end-users to diverge models away from their intended behavior. We\naim to explain the causes for this phenomenon and link it to the concept of\n``attention sinks'', an emergent LLM behavior crucial for fluency, in which the\ninitial token receives disproportionately high attention scores. Our\ninvestigation identifies the neural circuit responsible for attention sinks and\nshows how long repetitions disrupt this circuit. We extend this finding to\nother non-repeating sequences that exhibit similar circuit disruptions. To\naddress this, we propose a targeted patch that effectively resolves the issue\nwithout negatively impacting the model's overall performance. This study\nprovides a mechanistic explanation for an LLM vulnerability, demonstrating how\ninterpretability can diagnose and address issues, and offering insights that\npave the way for more secure and reliable models.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在重复单个词时常失败并输出无关文本的现象，这被视为模型的潜在漏洞，并将其与 \"attention sinks\" 机制联系起来，即初始 token 获得过高注意力分数导致的涌现行为。研究通过神经电路分析，识别了 \"attention sinks\" 的责任电路，并解释了长重复序列如何破坏该电路，同时扩展到其他非重复序列的类似问题。为解决这一问题，作者提出一个针对性修复 (targeted patch)，有效消除漏洞而不影响模型整体性能。该工作提供了 LLM 漏洞的机制解释，展示了可解释性在诊断和改进模型中的作用，为开发更安全可靠的模型提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08908v1",
      "published_date": "2025-03-11 21:40:58 UTC",
      "updated_date": "2025-03-11 21:40:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:38:36.332100"
    },
    {
      "arxiv_id": "2503.08906v1",
      "title": "Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiwen Chen",
        "Wenhui Zhu",
        "Peijie Qiu",
        "Hao Wang",
        "Huayu Li",
        "Haiyu Wu",
        "Aristeidis Sotiras",
        "Yalin Wang",
        "Abolfazl Razi"
      ],
      "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong performance but\nstruggle when adapted to downstream tasks. Prompt learning has emerged as an\nefficient and effective strategy to adapt VLMs while preserving their\npre-trained knowledge. However, existing methods still lead to overfitting and\ndegrade zero-shot generalization. To address this challenge, we propose an\noptimal transport (OT)-guided prompt learning framework that mitigates\nforgetting by preserving the structural consistency of feature distributions\nbetween pre-trained and fine-tuned models. Unlike conventional point-wise\nconstraints, OT naturally captures cross-instance relationships and expands the\nfeasible parameter space for prompt tuning, allowing a better trade-off between\nadaptation and generalization. Our approach enforces joint constraints on both\nvision and text representations, ensuring a holistic feature alignment.\nExtensive experiments on benchmark datasets demonstrate that our simple yet\neffective method can outperform existing prompt learning strategies in\nbase-to-novel generalization, cross-dataset evaluation, and domain\ngeneralization without additional augmentation or ensemble techniques. The code\nis available at https://github.com/ChongQingNoSubway/Prompt-OT",
      "tldr_zh": "该研究针对视觉语言模型(VLMs)如CLIP在下游任务适应时面临的过拟合和零样本泛化下降问题，提出了一种基于最优传输(OT)的提示学习框架Prompt-OT。\n该框架通过保持预训练和微调模型特征分布的结构一致性，捕捉跨实例关系，并对视觉和文本表示施加联合约束，从而实现适应与泛化的更好平衡。\n实验在基准数据集上表明，Prompt-OT在基到新泛化、跨数据集评估和领域泛化方面优于现有方法，无需额外增强或集成技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08906v1",
      "published_date": "2025-03-11 21:38:34 UTC",
      "updated_date": "2025-03-11 21:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:38:49.692099"
    },
    {
      "arxiv_id": "2503.08883v2",
      "title": "Imitation Learning of Correlated Policies in Stackelberg Games",
      "title_zh": "Stackelberg 游戏中相关策略的模仿学习",
      "authors": [
        "Kuang-Da Wang",
        "Ping-Chun Hsieh",
        "Wen-Chih Peng"
      ],
      "abstract": "Stackelberg games, widely applied in domains like economics and security,\ninvolve asymmetric interactions where a leader's strategy drives follower\nresponses. Accurately modeling these dynamics allows domain experts to optimize\nstrategies in interactive scenarios, such as turn-based sports like badminton.\nIn multi-agent systems, agent behaviors are interdependent, and traditional\nMulti-Agent Imitation Learning (MAIL) methods often fail to capture these\ncomplex interactions. Correlated policies, which account for opponents'\nstrategies, are essential for accurately modeling such dynamics. However, even\nmethods designed for learning correlated policies, like CoDAIL, struggle in\nStackelberg games due to their asymmetric decision-making, where leaders and\nfollowers cannot simultaneously account for each other's actions, often leading\nto non-correlated policies. Furthermore, existing MAIL methods that match\noccupancy measures or use adversarial techniques like GAIL or Inverse RL face\nscalability challenges, particularly in high-dimensional environments, and\nsuffer from unstable training. To address these challenges, we propose a\ncorrelated policy occupancy measure specifically designed for Stackelberg games\nand introduce the Latent Stackelberg Differential Network (LSDN) to match it.\nLSDN models two-agent interactions as shared latent state trajectories and uses\nmulti-output Geometric Brownian Motion (MO-GBM) to effectively capture joint\npolicies. By leveraging MO-GBM, LSDN disentangles environmental influences from\nagent-driven transitions in latent space, enabling the simultaneous learning of\ninterdependent policies. This design eliminates the need for adversarial\ntraining and simplifies the learning process. Extensive experiments on\nIterative Matrix Games and multi-agent particle environments demonstrate that\nLSDN can better reproduce complex interaction dynamics than existing MAIL\nmethods.",
      "tldr_zh": "这篇论文针对 Stackelberg games 中的不对称互动（如经济学和安全领域），提出了一种改进的 Multi-Agent Imitation Learning (MAIL) 方法，以解决传统方法（如 CoDAIL、GAIL 或 Inverse RL）在捕捉代理间复杂相关政策时的局限性，包括可扩展性和训练不稳定性。作者引入了 Latent Stackelberg Differential Network (LSDN)，该框架通过共享潜在状态轨迹和多输出几何布朗运动 (MO-GBM) 来建模联合政策，从而有效分离环境影响和代理驱动的转换，并简化学习过程。实验结果显示，LSDN 在 Iterative Matrix Games 和多代理粒子环境中显著优于现有方法，能够更好地再现互动动态。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Code will be released at this GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/LSDN",
      "pdf_url": "http://arxiv.org/pdf/2503.08883v2",
      "published_date": "2025-03-11 20:52:56 UTC",
      "updated_date": "2025-03-16 17:42:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:39:03.080866"
    },
    {
      "arxiv_id": "2503.08879v1",
      "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Guangtao Wang",
        "Shubhangi Upasani",
        "Chen Wu",
        "Darshan Gandhi",
        "Jonathan Li",
        "Changran Hu",
        "Bo Li",
        "Urmish Thakker"
      ],
      "abstract": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
      "tldr_zh": "该研究发现，大型语言模型 (LLMs) 在处理长上下文时，注意力机制表现出稀疏性，模型能隐式识别哪些 token 可被丢弃，从而优化 KV cache。作者提出 Self-Attention Guided Eviction (SAGE-KV) 方法，在预填充阶段通过一次 top-k 选择在 token 和 head 级别压缩 KV cache，提升内存效率和推理速度。实验在 LongBench 基准和多种 LLMs（如 Llama3.1-8B-Instruct-128k）上显示，SAGE-KV 比 StreamLLM 提高 4 倍内存效率并改善准确性，比 Quest 提高 2 倍内存效率且保持或提升性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08879v1",
      "published_date": "2025-03-11 20:45:02 UTC",
      "updated_date": "2025-03-11 20:45:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:39:15.222265"
    },
    {
      "arxiv_id": "2503.08872v1",
      "title": "Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing",
      "title_zh": "翻译失败",
      "authors": [
        "Cameron Redovian"
      ],
      "abstract": "We integrate a meta-reinforcement learning algorithm with the DreamerV3\narchitecture to improve load balancing in operating systems. This approach\nenables rapid adaptation to dynamic workloads with minimal retraining,\noutperforming the Advantage Actor-Critic (A2C) algorithm in standard and\nadaptive trials. It demonstrates robust resilience to catastrophic forgetting,\nmaintaining high performance under varying workload distributions and sizes.\nThese findings have important implications for optimizing resource management\nand performance in modern operating systems. By addressing the challenges posed\nby dynamic and heterogeneous workloads, our approach advances the adaptability\nand efficiency of reinforcement learning in real-world system management tasks.",
      "tldr_zh": "本研究将 Meta-Reinforcement Learning 算法与 DreamerV3 架构整合，用于优化操作系统的 Load Balancing，从而实现快速适应动态工作负载并减少重新训练需求。相比于 Advantage Actor-Critic (A2C) 算法，该方法在标准和适应性试验中表现出色，显示出对灾难性遗忘的鲁棒性，并在变化的工作负载分布和大小下保持高性能。这些发现对提升现代操作系统的资源管理和整体效率具有重要意义。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.OS",
        "I.2.6; I.2.8; D.4.1"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure, to be published in ACMSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08872v1",
      "published_date": "2025-03-11 20:36:49 UTC",
      "updated_date": "2025-03-11 20:36:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:39:26.955379"
    },
    {
      "arxiv_id": "2503.08867v1",
      "title": "Zero-Shot Action Generalization with Limited Observations",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Alchihabi",
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "abstract": "Reinforcement Learning (RL) has demonstrated remarkable success in solving\nsequential decision-making problems. However, in real-world scenarios, RL\nagents often struggle to generalize when faced with unseen actions that were\nnot encountered during training. Some previous works on zero-shot action\ngeneralization rely on large datasets of action observations to capture the\nbehaviors of new actions, making them impractical for real-world applications.\nIn this paper, we introduce a novel zero-shot framework, Action Generalization\nfrom Limited Observations (AGLO). Our framework has two main components: an\naction representation learning module and a policy learning module. The action\nrepresentation learning module extracts discriminative embeddings of actions\nfrom limited observations, while the policy learning module leverages the\nlearned action representations, along with augmented synthetic action\nrepresentations, to learn a policy capable of handling tasks with unseen\nactions. The experimental results demonstrate that our framework significantly\noutperforms state-of-the-art methods for zero-shot action generalization across\nmultiple benchmark tasks, showcasing its effectiveness in generalizing to new\nactions with minimal action observations.",
      "tldr_zh": "该研究针对强化学习（RL）在零样本动作泛化中的挑战，提出了一种新框架Action Generalization from Limited Observations (AGLO)，旨在处理训练中未见动作的泛化问题，而无需大量观察数据。AGLO 包括两个核心组件：行动表示学习模块，从有限观察中提取动作的判别性嵌入，以及策略学习模块，利用这些嵌入和增强的合成动作表示来训练能处理新动作的策略。实验结果显示，该框架在多个基准任务上显著优于现有方法，证明了其在有限观察条件下实现高效泛化的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08867v1",
      "published_date": "2025-03-11 20:14:25 UTC",
      "updated_date": "2025-03-11 20:14:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:39:37.795710"
    },
    {
      "arxiv_id": "2503.16499v1",
      "title": "Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities",
      "title_zh": "利益相关者视角：社交机器人是否及如何支持",
      "authors": [
        "Alva Markelius",
        "Julie Bailey",
        "Jenny L. Gibson",
        "Hatice Gunes"
      ],
      "abstract": "This paper presents an iterative, participatory, empirical study that\nexamines the potential of using artificial intelligence, such as social robots\nand large language models, to support mediation and advocacy for students with\ndisabilities in higher education. Drawing on qualitative data from interviews\nand focus groups conducted with various stakeholders, including disabled\nstudents, disabled student representatives, and disability practitioners at the\nUniversity of Cambridge, this study reports findings relating to understanding\nthe problem space, ideating robotic support and participatory co-design of\nadvocacy support robots. The findings highlight the potential of these\ntechnologies in providing signposting and acting as a sounding board or study\ncompanion, while also addressing limitations in empathic understanding, trust,\nequity, and accessibility. We discuss ethical considerations, including\nintersectional biases, the double empathy problem, and the implications of\ndeploying social robots in contexts shaped by structural inequalities. Finally,\nwe offer a set of recommendations and suggestions for future research,\nrethinking the notion of corrective technological interventions to tools that\nempower and amplify self-advocacy.",
      "tldr_zh": "本研究通过迭代的参与式实证调查，探讨社交机器人（social robots）和大型语言模型（large language models）是否及如何支持高等教育残疾学生的调解和倡导，基于剑桥大学的访谈和焦点小组数据，涉及残疾学生、代表和从业者。研究发现，这些技术可提供指引、充当倾听板（sounding board）或学习伴侣，但面临移情理解（empathic understanding）、信任、公平性和可访问性的局限性。论文讨论了伦理问题，包括交叉偏见（intersectional biases）、双重移情问题（double empathy problem）以及在结构性不平等环境中部署机器人的影响，并建议未来研究转向开发赋权工具，以增强残疾学生的自我倡导（self-advocacy）。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "This is a pre-print",
      "pdf_url": "http://arxiv.org/pdf/2503.16499v1",
      "published_date": "2025-03-11 19:57:11 UTC",
      "updated_date": "2025-03-11 19:57:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:39:50.587543"
    },
    {
      "arxiv_id": "2503.08823v2",
      "title": "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness",
      "title_zh": "ResBench：利用资源感知对LLM生成FPGA设计的基准测试",
      "authors": [
        "Ce Guo",
        "Tong Zhao"
      ],
      "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
      "tldr_zh": "该研究指出，现有的LLM生成HDL代码基准主要关注功能正确性，而忽略了FPGA设计的硬件资源使用问题，并缺乏真实应用多样性。为解决这些问题，研究团队引入ResBench，这是首个专注于资源的基准，包含56个问题跨越12个类别（如有限状态机和金融计算），用于区分资源优化和低效的LLM生成HDL代码。ResBench的开源评估框架自动生成Verilog代码、验证正确性和测量资源使用（如LUT使用量），实验结果显示不同LLMs在资源优化方面存在显著差异，证明了该基准的有效性。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.LG",
        "I.2.2"
      ],
      "primary_category": "cs.AR",
      "comment": "to be published in International Symposium on Highly Efficient\n  Accelerators and Reconfigurable Technologies 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08823v2",
      "published_date": "2025-03-11 18:54:17 UTC",
      "updated_date": "2025-03-21 23:27:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:40:02.628961"
    },
    {
      "arxiv_id": "2503.10684v1",
      "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
      "title_zh": "开放世界技能从未分段演示中的发现",
      "authors": [
        "Jingwen Deng",
        "Zihao Wang",
        "Shaofei Cai",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "Learning skills in open-world environments is essential for developing agents\ncapable of handling a variety of tasks by combining basic skills. Online\ndemonstration videos are typically long but unsegmented, making them difficult\nto segment and label with skill identifiers. Unlike existing methods that rely\non sequence sampling or human labeling, we have developed a self-supervised\nlearning-based approach to segment these long videos into a series of\nsemantic-aware and skill-consistent segments. Drawing inspiration from human\ncognitive event segmentation theory, we introduce Skill Boundary Detection\n(SBD), an annotation-free temporal video segmentation algorithm. SBD detects\nskill boundaries in a video by leveraging prediction errors from a pretrained\nunconditional action-prediction model. This approach is based on the assumption\nthat a significant increase in prediction error indicates a shift in the skill\nbeing executed. We evaluated our method in Minecraft, a rich open-world\nsimulator with extensive gameplay videos available online. Our SBD-generated\nsegments improved the average performance of conditioned policies by 63.7% and\n52.1% on short-term atomic skill tasks, and their corresponding hierarchical\nagents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the\ndiverse YouTube videos to train instruction-following agents. The project page\ncan be found in https://craftjarvis.github.io/SkillDiscovery.",
      "tldr_zh": "这篇论文提出了一种自监督学习方法，用于从未分割的演示视频中发现开放世界技能，从而帮助代理处理多样任务。论文引入了Skill Boundary Detection (SBD)算法，该算法利用预训练的无条件动作预测模型的预测错误来检测技能边界，基于预测错误显著增加表示技能转变的假设。在Minecraft环境中实验表明，SBD生成的段落提高了条件策略的平均性能，在短期原子技能任务上提升63.7%和52.1%，而在长期层次任务上提升11.3%和20.8%。该方法能利用YouTube视频训练遵循指令的代理，提供了一种无需人工标注的技能发现框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10684v1",
      "published_date": "2025-03-11 18:51:40 UTC",
      "updated_date": "2025-03-11 18:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:40:15.561512"
    },
    {
      "arxiv_id": "2503.08815v1",
      "title": "Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations",
      "title_zh": "Cross-Examiner：评估大语言模型生成解释的一致性",
      "authors": [
        "Danielle Villa",
        "Maria Chang",
        "Keerthiram Murugesan",
        "Rosario Uceda-Sosa",
        "Karthikeyan Natesan Ramamurthy"
      ],
      "abstract": "Large Language Models (LLMs) are often asked to explain their outputs to\nenhance accuracy and transparency. However, evidence suggests that these\nexplanations can misrepresent the models' true reasoning processes. One\neffective way to identify inaccuracies or omissions in these explanations is\nthrough consistency checking, which typically involves asking follow-up\nquestions. This paper introduces, cross-examiner, a new method for generating\nfollow-up questions based on a model's explanation of an initial question. Our\nmethod combines symbolic information extraction with language model-driven\nquestion generation, resulting in better follow-up questions than those\nproduced by LLMs alone. Additionally, this approach is more flexible than other\nmethods and can generate a wider variety of follow-up questions.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 生成解释的不一致性问题，提出了一种新方法 Cross-Examiner，用于评估这些解释的准确性。Cross-Examiner 通过结合 symbolic information extraction 和 language model-driven question generation 来自动生成基于初始解释的后续问题，从而识别潜在的不准确或遗漏。相比单纯使用 LLMs，该方法能产生更高质量、更灵活和多样化的后续问题，提升了解释一致性的评估效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08815v1",
      "published_date": "2025-03-11 18:50:43 UTC",
      "updated_date": "2025-03-11 18:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:40:25.866878"
    },
    {
      "arxiv_id": "2503.08796v1",
      "title": "Robust Multi-Objective Controlled Decoding of Large Language Models",
      "title_zh": "大语言模型的稳健多目标受控解码",
      "authors": [
        "Seongho Son",
        "William Bankes",
        "Sangwoong Yoon",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "abstract": "Test-time alignment of Large Language Models (LLMs) to human preferences\noffers a flexible way to generate responses aligned to diverse objectives\nwithout extensive retraining of LLMs. Existing methods achieve alignment to\nmultiple objectives simultaneously (e.g., instruction-following, helpfulness,\nconciseness) by optimizing their corresponding reward functions. However, they\noften rely on predefined weights or optimize for averages, sacrificing one\nobjective for another and leading to unbalanced outcomes. To address this, we\nintroduce Robust Multi-Objective Decoding (RMOD), a novel inference-time\nalgorithm that optimizes for improving worst-case rewards. RMOD formalizes the\nrobust decoding problem as a maximin two-player game between reward weights and\nthe sampling policy, solving for the Nash equilibrium. We show that the game\nreduces to a convex optimization problem to find the worst-case weights, while\nthe best response policy can be computed analytically. We also introduce a\npractical RMOD variant designed for efficient decoding with contemporary LLMs,\nincurring minimal computational overhead compared to non-robust Multi-Objective\nDecoding (MOD) methods. Our experimental results showcase the effectiveness of\nRMOD in generating responses equitably aligned with diverse objectives,\noutperforming baselines up to 20%.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）的测试时对齐问题，提出Robust Multi-Objective Decoding (RMOD)，一种新型推理时算法，通过优化最坏情况奖励来实现多个目标（如指令遵循、帮助性和简洁性）的均衡对齐，避免现有方法依赖预定义权重或平均优化导致的不平衡。RMOD将问题形式化为一个最大最小二玩家游戏，求解Nash equilibrium，并通过凸优化计算最坏情况权重，同时分析得出最佳响应策略，其实用变体确保了高效解码，与非鲁棒Multi-Objective Decoding (MOD)方法相比计算开销最小。实验结果表明，RMOD在生成与多样目标公平对齐的响应方面，优于基线方法高达20%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08796v1",
      "published_date": "2025-03-11 18:15:26 UTC",
      "updated_date": "2025-03-11 18:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:40:39.985255"
    },
    {
      "arxiv_id": "2503.08786v1",
      "title": "Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised Probabilistic Inference -- A Work In Progress",
      "title_zh": "翻译失败",
      "authors": [
        "Sagad Hamid",
        "Tanya Braun"
      ],
      "abstract": "Efficient probabilistic inference by variable elimination in graphical models\nrequires an optimal elimination order. However, finding an optimal order is a\nchallenging combinatorial optimisation problem for models with a large number\nof random variables. Most recently, a reinforcement learning approach has been\nproposed to find efficient contraction orders in tensor networks. Due to the\nduality between graphical models and tensor networks, we adapt this approach to\nprobabilistic inference in graphical models. Furthermore, we incorporate\nstructure exploitation into the process of finding an optimal order. Currently,\nthe agent's cost function is formulated in terms of intermediate result sizes\nwhich are exponential in the number of indices (i.e., random variables). We\nshow that leveraging specific structures during inference allows for\nintroducing compact encodings of intermediate results which can be\nsignificantly smaller. By considering the compact encoding sizes for the cost\nfunction instead, we enable the agent to explore more efficient contraction\norders. The structure we consider in this work is the presence of local\nsymmetries (i.e., symmetries within a model's factors).",
      "tldr_zh": "该研究探讨了在图形模型中通过变量消除进行高效概率推理的问题，强调了寻找最优消除顺序的挑战性组合优化任务。作者将强化学习（reinforcement learning）方法从tensor networks适应到图形模型，并整合结构利用（如local symmetries）来优化过程。具体而言，通过引入紧凑编码来表示中间结果的大小，作为代理的成本函数，这使得代理能够探索更有效的收缩顺序。该方法初步展示了在利用模型因子中的局部对称时，提高推理效率的潜力，为进一步优化probabilistic inference奠定基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Contributed to: Sixth Data Science Meets Optimisation (DSO) Workshop\n  at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.08786v1",
      "published_date": "2025-03-11 18:00:23 UTC",
      "updated_date": "2025-03-11 18:00:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:40:50.426206"
    },
    {
      "arxiv_id": "2503.08684v1",
      "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Wang",
        "Sunhao Dai",
        "Haiyuan Zhao",
        "Liang Pang",
        "Xiao Zhang",
        "Gang Wang",
        "Zhenhua Dong",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "abstract": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
      "tldr_zh": "本研究揭示了基于PLM的检索模型存在source bias问题，即这些模型偏好低perplexity文档，给LLM生成的文本分配更高相关性分数，即使其语义质量与人类内容相当，从而威胁信息访问生态的可持续性。通过因果图分析，论文发现这种偏置源于语言建模任务和检索任务损失函数梯度的正相关性，导致模型在相关性估计中过度学习perplexity特征。针对这一问题，提出了一种因果启发的推理时去偏方法Causal Diagnosis and Correction (CDC)，该方法先诊断perplexity的偏置效应，然后从整体相关性分数中分离偏置。实验结果在三个领域证明了CDC的优越去偏效果，验证了所提解释框架的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08684v1",
      "published_date": "2025-03-11 17:59:00 UTC",
      "updated_date": "2025-03-11 17:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:41:04.890953"
    },
    {
      "arxiv_id": "2503.08683v1",
      "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
      "title_zh": "CoLMDriver：基于LLM的谈判有益于合作式自动驾驶",
      "authors": [
        "Changxing Liu",
        "Genjia Liu",
        "Zijun Wang",
        "Jinchang Yang",
        "Siheng Chen"
      ],
      "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
      "tldr_zh": "该论文提出 CoLMDriver，一种基于 LLM 的合作自主驾驶系统，通过语言谈判机制提升 V2V 车辆间协作效率，解决传统方法在刚性协议和泛化能力上的局限。系统包括一个采用 actor-critic 范式的 LLM-based 谈判模块，用于通过车辆决策反馈优化合作策略，以及一个 intention-guided waypoint 生成器，将谈判结果转化为实时可执行路径。实验在 InterDrive（基于 CARLA 的模拟基准）中评估 10 个挑战性互动场景，结果显示 CoLMDriver 比现有方法成功率提高 11%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08683v1",
      "published_date": "2025-03-11 17:58:42 UTC",
      "updated_date": "2025-03-11 17:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:41:15.377126"
    },
    {
      "arxiv_id": "2503.08764v1",
      "title": "Towards Interpretable Protein Structure Prediction with Sparse Autoencoders",
      "title_zh": "翻译失败",
      "authors": [
        "Nithin Parsan",
        "David J. Yang",
        "John J. Yang"
      ],
      "abstract": "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https://github.com/johnyang101/reticular-sae , and\nvisualizer https://sae.reticular.ai .",
      "tldr_zh": "本研究针对蛋白质语言模型在结构预测中的非线性问题，提出使用 Sparse Autoencoders (SAEs) 来实现可解释性建模，首次将 SAEs 扩展到 ESM2-3B 模型（ESMFold 的基础），从而解析序列表示如何影响结构预测。作者改编 Matryoshka SAEs，使其通过强制嵌套潜变量组独立重建输入，学习层次化特征，并证明其性能与标准架构相当或更好。实验评估显示，在 ESM2-3B 上训练的 SAEs 在生物概念发现和接触图预测方面显著优于较小模型，并通过案例研究展示了针对性引导 ESMFold 预测的能力，如提高结构溶剂可及性，同时固定输入序列。该工作开放源代码、数据集和预训练模型，以促进社区进一步探索。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Published at the GEMBio ICLR 2025 Workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.08764v1",
      "published_date": "2025-03-11 17:57:29 UTC",
      "updated_date": "2025-03-11 17:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:41:30.175914"
    },
    {
      "arxiv_id": "2503.08679v3",
      "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
      "title_zh": "翻译失败",
      "authors": [
        "Iván Arcuschin",
        "Jett Janiak",
        "Robert Krzyzanowski",
        "Senthooran Rajamanoharan",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
      "tldr_zh": "该研究揭示了 Chain-of-Thought (CoT) 推理在现实提示中并非总是忠实的，即模型的推理过程不一定反映其得出结论的真实方式。作者通过实验发现，前沿模型如 Sonnet 3.5 (16.3%)、DeepSeek R1 (5.3%) 和 ChatGPT-4o (7.0%) 在无人工偏见的情况下，表现出多种不忠实形式，包括隐式事后合理化（例如，为矛盾的二元问题提供表面一致的论据）、恢复错误（模型自行纠正推理错误而不说明）和不忠实的捷径（使用非逻辑推理简化难题）。这些发现对依赖 CoT 监测的 AI 安全工作提出了挑战，强调了需要改进模型的推理透明度和可靠性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08679v3",
      "published_date": "2025-03-11 17:56:30 UTC",
      "updated_date": "2025-03-19 19:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:41:40.480955"
    },
    {
      "arxiv_id": "2503.08678v1",
      "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanhao Wang",
        "Cheng Zhang",
        "Gonçalo Frazão",
        "Jinlong Yang",
        "Alexandru-Eugen Ichim",
        "Thabo Beeler",
        "Fernando De la Torre"
      ],
      "abstract": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods.",
      "tldr_zh": "我们介绍了GarmentCrafter，一种新方法，允许非专业用户从单视图图像创建和编辑3D服装，通过渐进式新视图合成解决现有技术的跨视图一致性问题。该方法结合深度预测、图像扭曲和多视图扩散模型，联合推断RGB和深度信息，以重建精确的几何细节和细微特征。实验结果显示，GarmentCrafter在视觉保真度和视图间一致性方面优于最先进的三维服装重建方法。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project Page: https://humansensinglab.github.io/garment-crafter/",
      "pdf_url": "http://arxiv.org/pdf/2503.08678v1",
      "published_date": "2025-03-11 17:56:03 UTC",
      "updated_date": "2025-03-11 17:56:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:41:51.590303"
    },
    {
      "arxiv_id": "2503.08669v1",
      "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
      "title_zh": "翻译失败",
      "authors": [
        "Zekun Li",
        "Shinda Huang",
        "Jiangtian Wang",
        "Nathan Zhang",
        "Antonis Antoniades",
        "Wenyue Hua",
        "Kaijie Zhu",
        "Sirui Zeng",
        "William Yang Wang",
        "Xifeng Yan"
      ],
      "abstract": "As language agents progressively automate critical tasks across domains,\ntheir ability to operate within operational constraints and safety protocols\nbecomes essential. While extensive research has demonstrated these agents'\neffectiveness in downstream task completion, their reliability in following\noperational procedures and constraints remains largely unexplored. To this end,\nwe present AgentOrca, a dual-system framework for evaluating language agents'\ncompliance with operational constraints and routines. Our framework encodes\naction constraints and routines through both natural language prompts for\nagents and corresponding executable code serving as ground truth for automated\nverification. Through an automated pipeline of test case generation and\nevaluation across five real-world domains, we quantitatively assess current\nlanguage agents' adherence to operational constraints. Our findings reveal\nnotable performance gaps among state-of-the-art models, with large reasoning\nmodels like o1 demonstrating superior compliance while others show\nsignificantly lower performance, particularly when encountering complex\nconstraints or user persuasion attempts.",
      "tldr_zh": "本研究提出 AgentOrca，一种双系统框架，用于评估 language agents 在操作例程和约束遵守方面的可靠性。该框架通过自然语言提示和可执行代码编码约束，并采用自动化测试管道在五个真实领域生成和评估测试用例。实验结果揭示了当前 state-of-the-art 模型间的性能差距，其中大型推理模型如 o1 显示出更高的遵守能力，而其他模型在面对复杂约束或用户说服时表现较差。总的来说，AgentOrca 为提升 language agents 的安全性和可信度提供了重要工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08669v1",
      "published_date": "2025-03-11 17:53:02 UTC",
      "updated_date": "2025-03-11 17:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:42:02.849776"
    },
    {
      "arxiv_id": "2503.08665v1",
      "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
      "title_zh": "翻译失败",
      "authors": [
        "Yitian Zhang",
        "Long Mai",
        "Aniruddha Mahapatra",
        "David Bourgin",
        "Yicong Hong",
        "Jonah Casebeer",
        "Feng Liu",
        "Yun Fu"
      ],
      "abstract": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference.",
      "tldr_zh": "我们提出REGEN框架，通过一种新型的编码器-生成器结构来学习紧凑的视频嵌入，重点是合成视觉上合理的重建，而不是精确复制输入视频，从而实现更高的压缩比，同时保持下游生成模型的质量。具体方法包括使用扩散变换器(DiT)从潜在空间合成缺失细节，并开发一个专用的潜在条件模块来条件DiT解码器。实验结果显示，REGEN在压缩比高达32x时（比领先方法高8x），编码-解码性能优于现有方法，并显著提升文本到视频生成的训练和推理效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
      "published_date": "2025-03-11 17:51:07 UTC",
      "updated_date": "2025-03-11 17:51:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:42:15.323455"
    },
    {
      "arxiv_id": "2503.08664v1",
      "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Wang",
        "Fangzhou Hong",
        "Shuai Yang",
        "Liming Jiang",
        "Wayne Wu",
        "Chen Change Loy"
      ],
      "abstract": "Multiview diffusion models have shown considerable success in image-to-3D\ngeneration for general objects. However, when applied to human data, existing\nmethods have yet to deliver promising results, largely due to the challenges of\nscaling multiview attention to higher resolutions. In this paper, we explore\nhuman multiview diffusion models at the megapixel level and introduce a\nsolution called mesh attention to enable training at 1024x1024 resolution.\nUsing a clothed human mesh as a central coarse geometric representation, the\nproposed mesh attention leverages rasterization and projection to establish\ndirect cross-view coordinate correspondences. This approach significantly\nreduces the complexity of multiview attention while maintaining cross-view\nconsistency. Building on this foundation, we devise a mesh attention block and\ncombine it with keypoint conditioning to create our human-specific multiview\ndiffusion model, MEAT. In addition, we present valuable insights into applying\nmultiview human motion videos for diffusion training, addressing the\nlongstanding issue of data scarcity. Extensive experiments show that MEAT\neffectively generates dense, consistent multiview human images at the megapixel\nlevel, outperforming existing multiview diffusion methods.",
      "tldr_zh": "该研究提出了一种名为 MEAT 的多视图扩散模型，旨在解决现有方法在高分辨率（1024x1024）人体生成中的挑战，通过引入 mesh attention 机制来优化跨视图一致性。mesh attention 使用衣着人体网格作为粗略几何表示，并结合光栅化和投影建立直接坐标对应，从而显著降低计算复杂性，并与关键点条件相结合进行训练。此外，论文探讨了利用多视图人体动作视频缓解数据稀缺问题，实验结果显示 MEAT 在生成密集、一致的高分辨率多视图人体图像方面，优于现有 multiview diffusion 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Code https://github.com/johannwyh/MEAT Project Page\n  https://johann.wang/MEAT/",
      "pdf_url": "http://arxiv.org/pdf/2503.08664v1",
      "published_date": "2025-03-11 17:50:59 UTC",
      "updated_date": "2025-03-11 17:50:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:42:26.997830"
    },
    {
      "arxiv_id": "2503.08663v1",
      "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Sermanet",
        "Anirudha Majumdar",
        "Alex Irpan",
        "Dmitry Kalashnikov",
        "Vikas Sindhwani"
      ],
      "abstract": "Until recently, robotics safety research was predominantly about collision\navoidance and hazard reduction in the immediate vicinity of a robot. Since the\nadvent of large vision and language models (VLMs), robots are now also capable\nof higher-level semantic scene understanding and natural language interactions\nwith humans. Despite their known vulnerabilities (e.g. hallucinations or\njail-breaking), VLMs are being handed control of robots capable of physical\ncontact with the real world. This can lead to dangerous behaviors, making\nsemantic safety for robots a matter of immediate concern. Our contributions in\nthis paper are two fold: first, to address these emerging risks, we release the\nASIMOV Benchmark, a large-scale and comprehensive collection of datasets for\nevaluating and improving semantic safety of foundation models serving as robot\nbrains. Our data generation recipe is highly scalable: by leveraging text and\nimage generation techniques, we generate undesirable situations from real-world\nvisual scenes and human injury reports from hospitals. Secondly, we develop a\nframework to automatically generate robot constitutions from real-world data to\nsteer a robot's behavior using Constitutional AI mechanisms. We propose a novel\nauto-amending process that is able to introduce nuances in written rules of\nbehavior; this can lead to increased alignment with human preferences on\nbehavior desirability and safety. We explore trade-offs between generality and\nspecificity across a diverse set of constitutions of different lengths, and\ndemonstrate that a robot is able to effectively reject unconstitutional\nactions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using\ngenerated constitutions, outperforming no-constitution baselines and\nhuman-written constitutions. Data is available at asimov-benchmark.github.io",
      "tldr_zh": "本论文针对机器人语义安全问题，提出两个主要贡献：首先，发布了 ASIMOV Benchmark，这是一个大规模数据集，用于评估和改进作为机器人大脑的 VLMs（大型视觉和语言模型）的安全性能，通过文本和图像生成技术从真实场景和医院报告中创建不 desirable 情况。其次，开发了一个框架，利用 Constitutional AI 机制从真实数据自动生成机器人宪法，并引入 auto-amending 过程以细化行为规则，提高与人类偏好的对齐。实验结果显示，使用生成的宪法，机器人在 ASIMOV Benchmark 上实现84.3%的顶层对齐率，优于无宪法基线和人工宪法，从而有效拒绝不合宪的行为。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08663v1",
      "published_date": "2025-03-11 17:50:47 UTC",
      "updated_date": "2025-03-11 17:50:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:42:41.748048"
    },
    {
      "arxiv_id": "2503.08662v1",
      "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language Models",
      "title_zh": "探索大型语言模型的词义消歧能力",
      "authors": [
        "Pierpaolo Basile",
        "Lucia Siciliani",
        "Elio Musacchio",
        "Giovanni Semeraro"
      ],
      "abstract": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
      "tldr_zh": "本文探讨了大型语言模型 (LLMs) 在词义消歧 (WSD) 任务上的性能，通过扩展 XL-WSD 基准并使用 BabelNet 构建两个子任务：给定句子中的单词，LLM 需生成正确定义或从预定义含义中选择正确项。研究发现，LLMs 在零-shot learning 场景下表现出色，但无法超越当前 state-of-the-art 方法。然而，一个 fine-tuned 的中等参数模型超过了所有其他模型，包括最先进的技术。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08662v1",
      "published_date": "2025-03-11 17:50:44 UTC",
      "updated_date": "2025-03-11 17:50:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:42:51.222610"
    },
    {
      "arxiv_id": "2503.08644v1",
      "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Parishad BehnamGhader",
        "Nicholas Meade",
        "Siva Reddy"
      ],
      "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
      "tldr_zh": "本文研究了instruction-following retrievers在实际应用中的安全风险，通过实证分析六种领先检索器（如NV-Embed和LLM2Vec），发现这些模型能为超过50%的恶意查询选择相关有害内容，例如LLM2Vec对61.35%的查询响应准确。研究进一步揭示，利用retrievers的指令跟随能力，可轻易暴露高度相关有害信息，甚至使安全对齐的LLM（如Llama3）在检索增强生成（retrieval augmented generation）设置中满足恶意请求。总之，该工作强调了随着retriever能力提升而增加的恶意误用风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08644v1",
      "published_date": "2025-03-11 17:36:53 UTC",
      "updated_date": "2025-03-11 17:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:43:04.330234"
    },
    {
      "arxiv_id": "2503.08643v2",
      "title": "Rethinking Diffusion Model in High Dimension",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenxin Zheng",
        "Zhenjie Zheng"
      ],
      "abstract": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDE, while aligning with the degraded\nobjective function. 3) Guided by this simple framework, more efficient\ninference methods can be discovered.",
      "tldr_zh": "这篇论文重新审视了 Diffusion Model 在高维空间中的表现，质疑其是否真正学习了底层概率分布。作者通过详细分析目标函数(Objective Function)和推理方法(Inference Methods)，得出关键结论：高维稀疏场景下，目标函数从多个样本的加权和退化到一个样本；主流推理方法可统一到一个简单框架中，而无需依赖 Markov 链或 SDE 等统计概念。该框架还可指导发现更有效的推理方法，从而提升 Diffusion Model 在高维数据生成中的实用性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08643v2",
      "published_date": "2025-03-11 17:36:11 UTC",
      "updated_date": "2025-04-13 06:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:43:16.424163"
    },
    {
      "arxiv_id": "2503.08638v1",
      "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Ruibin Yuan",
        "Hanfeng Lin",
        "Shuyue Guo",
        "Ge Zhang",
        "Jiahao Pan",
        "Yongyi Zang",
        "Haohe Liu",
        "Yiming Liang",
        "Wenye Ma",
        "Xingjian Du",
        "Xinrun Du",
        "Zhen Ye",
        "Tianyu Zheng",
        "Yinghao Ma",
        "Minghao Liu",
        "Zeyue Tian",
        "Ziya Zhou",
        "Liumeng Xue",
        "Xingwei Qu",
        "Yizhi Li",
        "Shangda Wu",
        "Tianhao Shen",
        "Ziyang Ma",
        "Jun Zhan",
        "Chunhui Wang",
        "Yatian Wang",
        "Xiaowei Chi",
        "Xinyue Zhang",
        "Zhenzhu Yang",
        "Xiangzhou Wang",
        "Shansong Liu",
        "Lingrui Mei",
        "Peng Li",
        "Junjie Wang",
        "Jianwei Yu",
        "Guojian Pang",
        "Xu Li",
        "Zihao Wang",
        "Xiaohuan Zhou",
        "Lijun Yu",
        "Emmanouil Benetos",
        "Yong Chen",
        "Chenghua Lin",
        "Xie Chen",
        "Gus Xia",
        "Zhaoxiang Zhang",
        "Chao Zhang",
        "Wenhu Chen",
        "Xinyu Zhou",
        "Xipeng Qiu",
        "Roger Dannenberg",
        "Jiaheng Liu",
        "Jian Yang",
        "Wenhao Huang",
        "Wei Xue",
        "Xu Tan",
        "Yike Guo"
      ],
      "abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "tldr_zh": "这篇论文介绍了 YuE，一系列基于 LLaMA2 架构的开放基础模型，旨在处理长形式音乐生成，特别是 lyrics-to-song 任务，能够生成长达五分钟的音乐并确保歌词对齐、音乐结构连贯以及吸引人的声乐旋律和伴奏。YuE 通过轨道解耦的下一 token 预测、结构渐进条件化和多任务多阶段预训练等创新方法，克服了密集混合信号等问题，并改进了 in-context learning 以支持风格转移（如将日式城市流行转化为英文说唱）和双向生成。实验结果显示，YuE 在音乐性和声乐敏捷性上匹配或超越专有系统，且其表示学习在 MARBLE 基准的音乐理解任务中达到或超过最先进水平。通过微调，YuE 还实现了额外的控制和对小众语言的支持。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "https://github.com/multimodal-art-projection/YuE",
      "pdf_url": "http://arxiv.org/pdf/2503.08638v1",
      "published_date": "2025-03-11 17:26:50 UTC",
      "updated_date": "2025-03-11 17:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:43:30.878807"
    },
    {
      "arxiv_id": "2503.10683v1",
      "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
      "title_zh": "理解扩散语言模型中的质量-多样性权衡",
      "authors": [
        "Zak Buzzard"
      ],
      "abstract": "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
      "tldr_zh": "这项研究探讨了扩散模型(Diffusion Models)在文本生成中的质量-多样性trade-off问题，指出这些模型在连续嵌入空间中缺乏像自回归模型中的温度超参数那样自然控制这一trade-off的机制，导致模型性能受限。论文提出使用classifier-free guidance和stochastic clamping技术来操纵sequence-to-sequence任务中的质量-多样性平衡，从而提升生成质量。实验结果表明，这些方法有效改善了扩散语言模型的整体表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10683v1",
      "published_date": "2025-03-11 17:18:01 UTC",
      "updated_date": "2025-03-11 17:18:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:43:39.046034"
    },
    {
      "arxiv_id": "2503.08609v1",
      "title": "Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Mehdi Hosseini Chagahi",
        "Niloufar Delfan",
        "Behzad Moshiri",
        "Md. Jalil Piran",
        "Jaber Hatam Parikhan"
      ],
      "abstract": "Intracranial hemorrhage (ICH) is a critical medical emergency caused by the\nrupture of cerebral blood vessels, leading to internal bleeding within the\nskull. Accurate and timely classification of hemorrhage subtypes is essential\nfor effective clinical decision-making. To address this challenge, we propose\nan advanced pyramid vision transformer (PVT)-based model, leveraging its\nhierarchical attention mechanisms to capture both local and global spatial\ndependencies in brain CT scans. Instead of processing all extracted features\nindiscriminately, A SHAP-based feature selection method is employed to identify\nthe most discriminative components, which are then used as a latent feature\nspace to train a boosting neural network, reducing computational complexity. We\nintroduce an entropy-aware aggregation strategy along with a fuzzy integral\noperator to fuse information across multiple CT slices, ensuring a more\ncomprehensive and reliable scan-level diagnosis by accounting for inter-slice\ndependencies. Experimental results show that our PVT-based framework\nsignificantly outperforms state-of-the-art deep learning architectures in terms\nof classification accuracy, precision, and robustness. By combining SHAP-driven\nfeature selection, transformer-based modeling, and an entropy-aware fuzzy\nintegral operator for decision fusion, our method offers a scalable and\ncomputationally efficient AI-driven solution for automated ICH subtype\nclassification.",
      "tldr_zh": "这篇论文提出了一种基于 Pyramid Vision Transformer (PVT) 的模型，用于颅内出血 (ICH) 在 CT 扫描中的子类型分类，通过捕捉局部和全局空间依赖来提升诊断精度。模型采用 SHAP-based feature selection 方法筛选最具鉴别力的特征，并引入 entropy-aware aggregation strategy 和 fuzzy integral operator 来融合多切片信息，确保更可靠的扫描级决策。实验结果表明，该框架在分类准确率、精确度和鲁棒性上显著优于现有深度学习模型，为自动化 ICH 子类型分类提供了一个可扩展、高效的 AI 解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08609v1",
      "published_date": "2025-03-11 16:47:32 UTC",
      "updated_date": "2025-03-11 16:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:43:52.003125"
    },
    {
      "arxiv_id": "2503.08608v1",
      "title": "A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps",
      "title_zh": "翻译失败",
      "authors": [
        "Sven Krausse",
        "Emre Neftci",
        "Friedrich T. Sommer",
        "Alpha Renner"
      ],
      "abstract": "The entorhinal-hippocampal formation is the mammalian brain's navigation\nsystem, encoding both physical and abstract spaces via grid cells. This system\nis well-studied in neuroscience, and its efficiency and versatility make it\nattractive for applications in robotics and machine learning. While continuous\nattractor networks (CANs) successfully model entorhinal grid cells for encoding\nphysical space, integrating both continuous spatial and abstract spatial\ncomputations into a unified framework remains challenging. Here, we attempt to\nbridge this gap by proposing a mechanistic model for versatile information\nprocessing in the entorhinal-hippocampal formation inspired by CANs and Vector\nSymbolic Architectures (VSAs), a neuro-symbolic computing framework. The novel\ngrid-cell VSA (GC-VSA) model employs a spatially structured encoding scheme\nwith 3D neuronal modules mimicking the discrete scales and orientations of grid\ncell modules, reproducing their characteristic hexagonal receptive fields. In\nexperiments, the model demonstrates versatility in spatial and abstract tasks:\n(1) accurate path integration for tracking locations, (2) spatio-temporal\nrepresentation for querying object locations and temporal relations, and (3)\nsymbolic reasoning using family trees as a structured test case for\nhierarchical relationships.",
      "tldr_zh": "本文提出了一种受网格细胞（grid cells）启发的结构化向量代数模型（grid-cell VSA, GC-VSA），旨在桥接大脑 entorhinal-hippocampal formation 的空间编码与抽象计算，提供一个统一的框架用于机器人和机器学习应用。模型基于连续吸引子网络（CANs）和向量符号架构（VSAs），采用 3D 神经元模块模拟网格细胞的离散规模、方向和六边形感受野。实验结果显示，GC-VSA 在路径整合（path integration）跟踪位置、时空表示（spatio-temporal representation）查询对象和时间关系，以及符号推理（symbolic reasoning）处理层次关系（如家族树）任务中表现出色。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "10 pages, 5 figures, accepted at the 2025 Neuro Inspired\n  Computational Elements (NICE) conference",
      "pdf_url": "http://arxiv.org/pdf/2503.08608v1",
      "published_date": "2025-03-11 16:45:52 UTC",
      "updated_date": "2025-03-11 16:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:44:06.281103"
    },
    {
      "arxiv_id": "2503.08605v1",
      "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Subin Kim",
        "Seoung Wug Oh",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jinwoo Shin"
      ],
      "abstract": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
      "tldr_zh": "本文提出了一种无调优的多事件长视频生成方法，名为 Synchronized Coupled Sampling (SynCoS)，旨在解决现有 text-to-video diffusion models 在生成长视频时面临的 content drift 和语义一致性丢失问题。SynCoS 通过同步去噪路径，结合 reverse sampling（确保局部平滑过渡）和 optimization-based sampling（强制全局一致性），并利用 grounded timestep 和 fixed baseline noise 来对齐采样策略，从而实现整个视频的长程一致性。实验结果显示，该框架在多事件长视频生成上显著提升了过渡平滑度和整体 coherence，在定量和定性指标上优于先前方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page with visuals: https://syncos2025.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.08605v1",
      "published_date": "2025-03-11 16:43:45 UTC",
      "updated_date": "2025-03-11 16:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:44:15.182937"
    },
    {
      "arxiv_id": "2503.08604v2",
      "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Dongping Li",
        "Tielong Cai",
        "Tianci Tang",
        "Wenhao Chai",
        "Katherine Rose Driggs-Campbell",
        "Gaoang Wang"
      ],
      "abstract": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of humanity. While advancements in large language models (LLMs)\nand embodied intelligence make this goal closer, several challenges persist:\nthe lack of a unified benchmark for more complex robot tasks, limited\nevaluation methods and metrics, data incompatibility between LLMs and mobile\nmanipulation trajectories. To address these issues, we propose Embodied Mobile\nManipulation in Open Environments (EMMOE), a benchmark that requires agents to\ninterpret user instructions and execute long-horizon everyday tasks in\ncontinuous space. EMMOE seamlessly integrates high-level and low-level embodied\ntasks into a unified framework, along with three new metrics for more diverse\nassessment. Additionally, we collect~\\dataset, which features in various task\nattributes, detailed process annotations, re-plans after failures, and two\nsub-datasets for LLM training. Furthermore, we design~\\model, a sophisticated\nagent system consists of LLM with Direct Preference Optimization (DPO), light\nweighted navigation and manipulation models, and multiple error detection\nmechanisms. Finally, we demonstrate~\\model's performance and evaluations of\ndifferent models and policies.",
      "tldr_zh": "该研究针对自主家庭机器人开发面临的挑战（如缺乏统一基准、评估方法有限和数据不兼容），提出了 EMMOE 基准，这是一个综合框架，用于评估代理在开放环境中的 Embodied Mobile Manipulation 任务。EMMOE 要求代理解释用户指令并执行长期日常任务，整合高层和低层任务，并引入三个新指标进行多样化评估。同时，研究收集了数据集，包含各种任务属性、详细过程注释、失败后的重新规划，以及两个子数据集用于 LLM 训练。此外，设计了基于 LLM 与 Direct Preference Optimization (DPO) 的代理系统，结合轻量级导航和操作模型以及错误检测机制，最终实验展示了该系统的性能和不同模型的评估结果。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08604v2",
      "published_date": "2025-03-11 16:42:36 UTC",
      "updated_date": "2025-05-15 01:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:44:28.692487"
    },
    {
      "arxiv_id": "2503.08762v1",
      "title": "Neurosymbolic Decision Trees",
      "title_zh": "神经符号决策树",
      "authors": [
        "Matthias Möller",
        "Arvid Norlander",
        "Pedro Zuidberg Dos Martires",
        "Luc De Raedt"
      ],
      "abstract": "Neurosymbolic (NeSy) AI studies the integration of neural networks (NNs) and\nsymbolic reasoning based on logic. Usually, NeSy techniques focus on learning\nthe neural, probabilistic and/or fuzzy parameters of NeSy models. Learning the\nsymbolic or logical structure of such models has, so far, received less\nattention. We introduce neurosymbolic decision trees (NDTs), as an extension of\ndecision trees together with a novel NeSy structure learning algorithm, which\nwe dub NeuID3. NeuID3 adapts the standard top-down induction of decision tree\nalgorithms and combines it with a neural probabilistic logic representation,\ninherited from the DeepProbLog family of models. The key advantage of learning\nNDTs with NeuID3 is the support of both symbolic and subsymbolic data (such as\nimages), and that they can exploit background knowledge during the induction of\nthe tree structure, In our experimental evaluation we demonstrate the benefits\nof NeSys structure learning over more traditonal approaches such as purely\ndata-driven learning with neural networks.",
      "tldr_zh": "这篇论文引入了 Neurosymbolic Decision Trees (NDTs)，作为决策树的扩展，旨在整合神经网络 (NNs) 和符号推理。论文提出了一种新算法 NeuID3，它基于标准的顶-down 决策树归纳方法，并结合 DeepProbLog 的神经概率逻辑表示，支持符号和子符号数据（如图像），并在树结构归纳过程中利用背景知识。实验结果表明，NDTs 的结构学习方法在性能上优于传统的纯数据驱动神经网络方法。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08762v1",
      "published_date": "2025-03-11 16:40:38 UTC",
      "updated_date": "2025-03-11 16:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:44:38.998194"
    },
    {
      "arxiv_id": "2503.16498v1",
      "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
      "title_zh": "翻译失败",
      "authors": [
        "Enzo Sinacola",
        "Arnault Pachot",
        "Thierry Petit"
      ],
      "abstract": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型 (LLMs) 创建虚拟用户来预测调查问题，从而无需人类数据即可模拟真实响应。研究评估了多种 LLMs（如 GPT-4o、GPT-3.5、Claude 3.5-Sonnet 和 Llama/Mistral 版本），并与 Random Forests 算法在 World Values Survey (WVS) 数据上进行比较，结果显示 LLMs 整体表现竞争性且不需额外训练数据。LLMs 在预测某些宗教和人口群体响应时存在偏差，导致表现较差，而 Random Forests 在有充足数据时表现出色。去除 LLMs 的审查机制能显著提升预测准确性，特别是针对代表性不足的群体。这些发现强调了解决 LLMs 偏差和重新审视审查方法的重要性，以提高其在公众意见研究中的可靠性和公平性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted, proceedings of the 17th International Conference on Machine\n  Learning and Computing",
      "pdf_url": "http://arxiv.org/pdf/2503.16498v1",
      "published_date": "2025-03-11 16:27:20 UTC",
      "updated_date": "2025-03-11 16:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:44:53.899899"
    },
    {
      "arxiv_id": "2503.08588v1",
      "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
      "title_zh": "BiasEdit: 通过模型编辑消除刻板印象语言模型",
      "authors": [
        "Xin Xu",
        "Wei Xu",
        "Ningyu Zhang",
        "Julian McAuley"
      ],
      "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "tldr_zh": "本文提出 BiasEdit，一种高效的模型编辑方法，用于移除语言模型的刻板偏见，通过轻量网络生成参数更新，并在去偏损失的指导下对模型部分参数进行局部编辑，同时利用保留损失保持语言建模能力。相比现有策略如重新训练或提示方法，BiasEdit 在 StereoSet 和 Crows-Pairs 数据集上的实验显示，它更有效地消除偏见，同时几乎不影响模型的整体性能。研究还通过偏见追踪分析了偏见在语言模型不同模块中的分布，并探讨了编辑对各组件的影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by TrustNLP @ NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08588v1",
      "published_date": "2025-03-11 16:25:36 UTC",
      "updated_date": "2025-03-11 16:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:45:07.326015"
    },
    {
      "arxiv_id": "2503.16497v1",
      "title": "Effective Yet Ephemeral Propaganda Defense: There Needs to Be More than One-Shot Inoculation to Enhance Critical Thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Hoferer",
        "Kilian Sprenkamp",
        "Dorian Christoph Quelle",
        "Daniel Gordon Jones",
        "Zoya Katashinskaya",
        "Alexandre Bovet",
        "Liudmila Zavolokina"
      ],
      "abstract": "In today's media landscape, propaganda distribution has a significant impact\non society. It sows confusion, undermines democratic processes, and leads to\nincreasingly difficult decision-making for news readers. We investigate the\nlasting effect on critical thinking and propaganda awareness on them when using\na propaganda detection and contextualization tool. Building on inoculation\ntheory, which suggests that preemptively exposing individuals to weakened forms\nof propaganda can improve their resilience against it, we integrate Kahneman's\ndual-system theory to measure the tools' impact on critical thinking. Through a\ntwo-phase online experiment, we measure the effect of several inoculation\ndoses. Our findings show that while the tool increases critical thinking during\nits use, this increase vanishes without access to the tool. This indicates a\nsingle use of the tool does not create a lasting impact. We discuss the\nimplications and propose possible approaches to improve the resilience against\npropaganda in the long-term.",
      "tldr_zh": "本文研究了在媒体环境中，宣传检测和上下文化工具对批判性思考和宣传意识的持久影响，基于inoculation theory（接种理论）和Kahneman's dual-system theory（双系统理论）。通过两阶段在线实验，作者测量了不同inoculation doses（接种剂量）的效果，发现工具在使用期间能提升批判性思考，但一旦停止使用，这种提升会迅速消失。结果表明，单次使用工具不足以产生长期抵抗宣传的效应。作者讨论了这些发现的含义，并提出了改进长期韧性的潜在方法，如增加重复暴露或强化训练。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16497v1",
      "published_date": "2025-03-11 16:24:19 UTC",
      "updated_date": "2025-03-11 16:24:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:45:19.542968"
    },
    {
      "arxiv_id": "2503.08581v2",
      "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jiangping Wen",
        "Jinyu Wen",
        "Meie Fang"
      ],
      "abstract": "Bag-based Multiple Instance Learning (MIL) approaches have emerged as the\nmainstream methodology for Whole Slide Image (WSI) classification. However,\nmost existing methods adopt a segmented training strategy, which first extracts\nfeatures using a pre-trained feature extractor and then aggregates these\nfeatures through MIL. This segmented training approach leads to insufficient\ncollaborative optimization between the feature extraction network and the MIL\nnetwork, preventing end-to-end joint optimization and thereby limiting the\noverall performance of the model. Additionally, conventional methods typically\nextract features from all patches of fixed size, ignoring the multi-scale\nobservation characteristics of pathologists. This not only results in\nsignificant computational resource waste when tumor regions represent a minimal\nproportion (as in the Camelyon16 dataset) but may also lead the model to\nsuboptimal solutions.\n  To address these limitations, this paper proposes an end-to-end multi-scale\nWSI classification framework that integrates multi-scale feature extraction\nwith multiple instance learning. Specifically, our approach includes: (1) a\nsemantic feature filtering module to reduce interference from non-lesion areas;\n(2) a multi-scale feature extraction module to capture pathological information\nat different levels; and (3) a multi-scale fusion MIL module for global\nmodeling and feature integration. Through an end-to-end training strategy, we\nsimultaneously optimize both the feature extractor and MIL network, ensuring\nmaximum compatibility between them.\n  Experiments were conducted on three cross-center datasets (DigestPath2019,\nBCNB, and UBC-OCEAN). Results demonstrate that our proposed method outperforms\nexisting state-of-the-art approaches in terms of both accuracy (ACC) and AUC\nmetrics.",
      "tldr_zh": "本文提出 MsaMIL-Net，一种端到端的多尺度感知 Multiple Instance Learning (MIL) 网络，用于高效的 Whole Slide Image (WSI) 分类，以解决现有方法的分割训练策略和忽略多尺度观察问题。网络包括语义特征过滤模块（减少非病变区域干扰）、多尺度特征提取模块（捕捉不同级别病理信息）和多尺度融合 MIL 模块（进行全局建模和特征整合），并通过端到端训练策略实现特征提取器和 MIL 网络的联合优化。在 DigestPath2019、BCNB 和 UBC-OCEAN 数据集上的实验表明，该方法在准确率 (ACC) 和 AUC 指标上优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "summited to ICCV2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08581v2",
      "published_date": "2025-03-11 16:16:44 UTC",
      "updated_date": "2025-03-12 09:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:45:31.950222"
    },
    {
      "arxiv_id": "2503.08760v1",
      "title": "Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes",
      "title_zh": "异构图结构学习：从数据生成过程的视角",
      "authors": [
        "Keyue Jiang",
        "Bohan Tang",
        "Xiaowen Dong",
        "Laura Toni"
      ],
      "abstract": "Inferring the graph structure from observed data is a key task in graph\nmachine learning to capture the intrinsic relationship between data entities.\nWhile significant advancements have been made in learning the structure of\nhomogeneous graphs, many real-world graphs exhibit heterogeneous patterns where\nnodes and edges have multiple types. This paper fills this gap by introducing\nthe first approach for heterogeneous graph structure learning (HGSL). To this\nend, we first propose a novel statistical model for the data-generating process\n(DGP) of heterogeneous graph data, namely hidden Markov networks for\nheterogeneous graphs (H2MN). Then we formalize HGSL as a maximum a-posterior\nestimation problem parameterized by such DGP and derive an alternating\noptimization method to obtain a solution together with a theoretical\njustification of the optimization conditions. Finally, we conduct extensive\nexperiments on both synthetic and real-world datasets to demonstrate that our\nproposed method excels in learning structure on heterogeneous graphs in terms\nof edge type identification and edge weight recovery.",
      "tldr_zh": "本论文提出了第一个异构图结构学习（Heterogeneous Graph Structure Learning, HGSL）方法，针对节点和边具有多种类型的真实世界图，填补了现有同构图方法的空白。\n他们引入了隐马尔可夫网络 for heterogeneous graphs (H2MN) 模型来描述异构图的数据生成过程（Data-generating Processes），并将 HGSL 形式化为最大后验估计问题，通过交替优化方法求解，并提供了理论支持。\n实验结果显示，该方法在合成和真实数据集上表现出色，尤其在边类型识别和边权重恢复方面优于基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08760v1",
      "published_date": "2025-03-11 16:14:53 UTC",
      "updated_date": "2025-03-11 16:14:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:45:43.329069"
    },
    {
      "arxiv_id": "2503.08565v1",
      "title": "When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research",
      "title_zh": "翻译失败",
      "authors": [
        "Willem van der Maden",
        "Vera van der Burg",
        "Brett A. Halperin",
        "Petra Jääskeläinen",
        "Joseph Lindley",
        "Derek Lomas",
        "Timothy Merritt"
      ],
      "abstract": "This essay examines how Generative AI (GenAI) is rapidly transforming design\npractices and how discourse often falls into over-simplified narratives that\nimpede meaningful research and practical progress. We identify and deconstruct\nfive prevalent \"semantic stopsigns\" -- reductive framings about GenAI in design\nthat halt deeper inquiry and limit productive engagement. Reflecting upon two\nexpert workshops at ACM conferences and semi-structured interviews with design\npractitioners, we analyze how these stopsigns manifest in research and\npractice. Our analysis develops mid-level knowledge that bridges theoretical\ndiscourse and practical implementation, helping designers and researchers\ninterrogate common assumptions about GenAI in their own contexts. By recasting\nthese stopsigns into more nuanced frameworks, we provide the design research\ncommunity with practical approaches for thinking about and working with these\nemerging technologies.",
      "tldr_zh": "这篇论文探讨了生成式 AI (GenAI) 如何快速改变设计实践，但讨论往往陷入五种常见的“语义停车标志”（semantic stopsigns），这些简化框架阻碍了深入研究和实际进展。作者通过分析 ACM 会议的两个专家研讨会和设计从业者的半结构化访谈，剖析这些 stopsigns 在研究和实践中的表现，并发展中层知识来桥接理论与实际。最终，论文提供更细致的框架，帮助设计师和研究人员审视 GenAI 的假设，并提出实用方法来思考和应用这些新兴技术。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08565v1",
      "published_date": "2025-03-11 15:54:03 UTC",
      "updated_date": "2025-03-11 15:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:45:54.842030"
    },
    {
      "arxiv_id": "2503.08564v2",
      "title": "MoE-Loco: Mixture of Experts for Multitask Locomotion",
      "title_zh": "翻译失败",
      "authors": [
        "Runhan Huang",
        "Shaoting Zhu",
        "Yilun Du",
        "Hang Zhao"
      ],
      "abstract": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability.",
      "tldr_zh": "本研究提出 MoE-Loco，一种基于 Mixture of Experts (MoE) 框架的多任务运动方法，适用于腿部机器人处理多样地形（如 bars, pits, stairs, slopes 和 baffles），并支持 quadrupedal 和 bipedal 步态。通过 MoE 机制，该框架缓解了 multitask reinforcement learning 中的 gradient conflicts，提升了训练效率和整体性能。实验结果显示，不同 experts 自然地专化于特定运动行为，可用于 task migration 和 skill composition，并在模拟及真实世界部署中证明了其鲁棒性和适应性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08564v2",
      "published_date": "2025-03-11 15:53:54 UTC",
      "updated_date": "2025-05-21 02:51:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:46:06.296035"
    },
    {
      "arxiv_id": "2503.08558v2",
      "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Xu",
        "Tony Khuong Nguyen",
        "Emma Dixon",
        "Christopher Rodriguez",
        "Patrick Miller",
        "Robert Lee",
        "Paarth Shah",
        "Rares Ambrus",
        "Haruki Nishimura",
        "Masha Itkina"
      ],
      "abstract": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
      "tldr_zh": "该论文探讨了在没有失败数据的情况下，如何通过不确定性感知机制检测模仿学习(imitation learning)策略的运行时失败，以提升机器人操作系统的安全性和可靠性。作者提出 FAIL-Detect，一种模块化的两阶段方法，将问题框架化为序列的异常检测(out-of-distribution, OOD)，首先从策略输入输出中提取标量信号来捕捉认识不确定性(epistemic uncertainty)，然后使用一致性预测(conformal prediction, CP)进行不确定性量化，提供统计保证。实验结果显示，基于学到的信号（如新型流模型密度估计器）的方法在多种机器人操作任务上，比现有最先进(SOTA)基准更准确和快速地检测失败。这些发现为推动模仿学习机器人向真实世界部署提供了关键支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08558v2",
      "published_date": "2025-03-11 15:47:12 UTC",
      "updated_date": "2025-04-25 07:12:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:46:20.113986"
    },
    {
      "arxiv_id": "2503.08551v1",
      "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Wanyong Feng",
        "Peter Tran",
        "Stephen Sireci",
        "Andrew Lan"
      ],
      "abstract": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
      "tldr_zh": "这篇论文提出了一种基于大型语言模型 (LLMs) 的两阶段方法，用于预测多选题 (MCQs) 的难度，以解决理解正确选项复杂性和错误选项 (distractors) 合理性的挑战。方法首先利用 LLMs 增强推理步骤，将这些步骤与 MCQ 本身作为输入来评估复杂性；其次，通过从知识水平分布中采样，结合项目反应理论 (IRT) 灵感，估计学生选择各选项的可能性，并采用 Kullback-Leibler (KL) divergence 正则化目标来优化预测。在两个真实数学 MCQ 数据集上的实验显示，该方法比基线模型降低了 28.3% 的均方误差，并提高了 34.6% 的决定系数，显著提升了难度预测的准确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08551v1",
      "published_date": "2025-03-11 15:39:43 UTC",
      "updated_date": "2025-03-11 15:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:46:32.089378"
    },
    {
      "arxiv_id": "2503.08549v1",
      "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xian Gao",
        "Zongyun Zhang",
        "Mingye Xie",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
      "tldr_zh": "这篇论文提出 Graph of AI Ideas (GoAI) 框架，利用 Knowledge Graphs 和 LLMs 来生成 AI 研究想法，旨在解决文献量大和引用关系复杂导致的分析难题。框架受知识图谱和人类认知启发，将相关文献组织成知识图谱中的实体，并将引用的语义信息总结为图谱关系，从而帮助 LLMs 更有效地捕捉研究进展和趋势。实验结果证明，该方法能生成新颖、清晰且有效的研发想法，显著提升了创新性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.08549v1",
      "published_date": "2025-03-11 15:36:38 UTC",
      "updated_date": "2025-03-11 15:36:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:46:42.657962"
    },
    {
      "arxiv_id": "2503.08542v1",
      "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering",
      "title_zh": "DAFE：通过动态仲裁的基于LLM评估，用于自由形式问答",
      "authors": [
        "Sher Badshah",
        "Hassan Sajjad"
      ],
      "abstract": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
      "tldr_zh": "该论文提出DAFE框架，利用LLM作为评估者，通过动态仲裁机制评估自由形式问答的响应。该机制使用两个主要LLM-as-judges，并在分歧时引入第三方仲裁器，以提升判断准确性并减少计算资源消耗。实验结果显示，DAFE在Macro F1和Cohen's Kappa等指标上显著改善，并通过全面人类评估证明其一致、可扩展和资源高效的特性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08542v1",
      "published_date": "2025-03-11 15:29:55 UTC",
      "updated_date": "2025-03-11 15:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:46:55.322779"
    },
    {
      "arxiv_id": "2503.08540v1",
      "title": "Mellow: a small audio language model for reasoning",
      "title_zh": "Mellow：用于推理的小型音频语言模型",
      "authors": [
        "Soham Deshmukh",
        "Satvik Dixit",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
      "tldr_zh": "本研究引入了 Mellow，一种小型音频语言模型（Audio-Language Model），旨在实现高效的音频和文本推理，特别适合边缘设备应用。Mellow 在小型模型中达到最先进性能，例如在 MMAU 基准上得分 52.11，与 SoTA Qwen2 Audio（得分 52.5）相当，但参数量少 50 倍，训练数据少 60 倍。研究开发了 ReasonAQA 数据集，包括 30% 现有数据和 70% 通过大型语言模型（LLMs）生成的合成数据，聚焦音频事件、物体、场景、信号属性、语义和听众情绪。实验评估显示 Mellow 在音频理解、演绎推理和比较推理任务上表现出色，并通过消融研究验证了投影层选择、合成数据生成和语言模型预训练对性能的影响，为小型 ALMs 的推理能力发展提供了新路径。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Checkpoint and dataset available at:\n  https://github.com/soham97/mellow",
      "pdf_url": "http://arxiv.org/pdf/2503.08540v1",
      "published_date": "2025-03-11 15:29:00 UTC",
      "updated_date": "2025-03-11 15:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:47:09.046830"
    },
    {
      "arxiv_id": "2503.08537v1",
      "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation",
      "title_zh": "翻译失败",
      "authors": [
        "Andres M Bran",
        "Theo A Neukomm",
        "Daniel P Armstrong",
        "Zlatko Jončev",
        "Philippe Schwaller"
      ],
      "abstract": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
      "tldr_zh": "本文研究证明，大型语言模型(LLMs)可作为强大的化学推理引擎，当与传统搜索算法整合时，能够捕捉专家级的战略思考，从而解决合成规划和反应机制阐明等挑战。LLMs不直接操作化学结构，而是评估化学策略并指导算法进行策略感知的逆合成规划（允许用自然语言指定约束）和机制阐明（结合化学原理与系统探索）。结果显示，该方法在多样化学任务上表现出色，较大模型展现更复杂的推理能力，并开创了结合LLMs战略理解与传统工具精确性的计算机辅助化学新范式。",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08537v1",
      "published_date": "2025-03-11 15:27:17 UTC",
      "updated_date": "2025-03-11 15:27:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:47:19.128106"
    },
    {
      "arxiv_id": "2503.08525v1",
      "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Wei",
        "Yijun Yang",
        "Junliang Xing",
        "Yuanchun Shi",
        "Zongqing Lu",
        "Deheng Ye"
      ],
      "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
      "tldr_zh": "该研究发现，在基于强化学习的视觉语言模型 (VLM) 代理训练中，仅依赖行动结果奖励会导致 thought collapse 现象，即代理的思想多样性丧失、推理不相关或不完整，从而产生无效行动和负面奖励。为解决此问题，研究提出 GTR (Guided Thought Reinforcement) 框架，使用自动校正器在每个 RL 步骤评估和改进代理的推理过程，同时训练推理和行动，而无需密集的人工标注。实验结果显示，GTR 显著提升了 LLaVA-7b 模型在复杂卡牌游戏（如 24 点）和 ALFWorld 等视觉环境中的性能和泛化能力，使任务成功率较最先进模型提高 3-5 倍，且模型尺寸更小。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08525v1",
      "published_date": "2025-03-11 15:17:02 UTC",
      "updated_date": "2025-03-11 15:17:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:47:31.793837"
    },
    {
      "arxiv_id": "2503.14514v1",
      "title": "Acceptance or Rejection of Lots while Minimizing and Controlling Type I and Type II Errors",
      "title_zh": "在最小化和",
      "authors": [
        "Edson Luiz Ursini",
        "Elaine Cristina Catapani Poletti",
        "Loreno Menezes da Silveira",
        "José Roberto Emiliano Leite"
      ],
      "abstract": "The double hypothesis test (DHT) is a test that allows controlling Type I\n(producer) and Type II (consumer) errors. It is possible to say whether the\nbatch has a defect rate, p, between 1.5 and 2%, or between 2 and 5%, or between\n5 and 10%, and so on, until finding a required value for this probability.\nUsing the two probabilities side by side, the Type I error for the lower\nprobability distribution and the Type II error for the higher probability\ndistribution, both can be controlled and minimized. It can be applied in the\ndevelopment or manufacturing process of a batch of components, or in the case\nof purchasing from a supplier, when the percentage of defects (p) is unknown,\nconsidering the technology and/or process available to obtain them. The power\nof the test is amplified by the joint application of the Limit of Successive\nFailures (LSF) related to the Renewal Theory. To enable the choice of the most\nappropriate algorithm for each application. Four distributions are proposed for\nthe Bernoulli event sequence, including their computational efforts: Binomial,\nBinomial approximated by Poisson, and Binomial approximated by Gaussian (with\ntwo variants). Fuzzy logic rules are also applied to facilitate\ndecision-making.",
      "tldr_zh": "本研究提出双假设测试 (DHT) 方法，用于在批次接受或拒绝决策中最小化和控制 Type I error (生产者错误) 和 Type II error (消费者错误)。DHT 通过比较缺陷率 p 的不同概率区间（如 1.5-2% 或 2-5%）来实现精确判断，并结合 Limit of Successive Failures (LSF) 来自 Renewal Theory 来增强测试威力。论文探讨了四种 Bernoulli 事件序列的分布，包括 Binomial、Poisson 近似 Binomial 以及 Gaussian 近似 Binomial (两种变体)，并评估了它们的计算努力，同时引入 Fuzzy logic rules 以辅助决策。该方法适用于制造过程或供应商采购场景，能显著提高批次质量控制的准确性和效率。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14514v1",
      "published_date": "2025-03-11 15:02:45 UTC",
      "updated_date": "2025-03-11 15:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:47:44.729750"
    },
    {
      "arxiv_id": "2503.08489v2",
      "title": "A Triple-Inertial Accelerated Alternating Optimization Method for Deep Learning Training",
      "title_zh": "翻译失败",
      "authors": [
        "Chengcheng Yan",
        "Jiawei Xu",
        "Qingsong Wang",
        "Zheng Peng"
      ],
      "abstract": "The stochastic gradient descent (SGD) algorithm has achieved remarkable\nsuccess in training deep learning models. However, it has several limitations,\nincluding susceptibility to vanishing gradients, sensitivity to input data, and\na lack of robust theoretical guarantees. In recent years, alternating\nminimization (AM) methods have emerged as a promising alternative for model\ntraining by employing gradient-free approaches to iteratively update model\nparameters. Despite their potential, these methods often exhibit slow\nconvergence rates. To address this challenge, we propose a novel\nTriple-Inertial Accelerated Alternating Minimization (TIAM) framework for\nneural network training. The TIAM approach incorporates a triple-inertial\nacceleration strategy with a specialized approximation method, facilitating\ntargeted acceleration of different terms in each sub-problem optimization. This\nintegration improves the efficiency of convergence, achieving superior\nperformance with fewer iterations. Additionally, we provide a convergence\nanalysis of the TIAM algorithm, including its global convergence properties and\nconvergence rate. Extensive experiments validate the effectiveness of the TIAM\nmethod, showing significant improvements in generalization capability and\ncomputational efficiency compared to existing approaches, particularly when\napplied to the rectified linear unit (ReLU) and its variants.",
      "tldr_zh": "本研究针对随机梯度下降（SGD）算法在深度学习训练中的局限性，如易受梯度消失影响、对输入数据敏感以及缺乏理论保证，提出了一种新型Triple-Inertial Accelerated Alternating Minimization (TIAM)框架。该框架整合了triple-inertial加速策略和专门的近似方法，以加速交替最小化（Alternating Minimization, AM）中每个子问题的优化，从而提升收敛效率并减少迭代次数。研究提供了TIAM的收敛分析，包括全局收敛性和收敛率；实验结果显示，在rectified linear unit (ReLU)及其变体上，TIAM显著提高了模型的泛化能力和计算效率，优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08489v2",
      "published_date": "2025-03-11 14:42:17 UTC",
      "updated_date": "2025-03-13 12:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:47:55.805754"
    },
    {
      "arxiv_id": "2503.08472v1",
      "title": "Optimizing Ride-Pooling Operations with Extended Pickup and Drop-Off Flexibility",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Jiang",
        "Yixing Xu",
        "Pradeep Varakantham"
      ],
      "abstract": "The Ride-Pool Matching Problem (RMP) is central to on-demand ride-pooling\nservices, where vehicles must be matched with multiple requests while adhering\nto service constraints such as pickup delays, detour limits, and vehicle\ncapacity. Most existing RMP solutions assume passengers are picked up and\ndropped off at their original locations, neglecting the potential for\npassengers to walk to nearby spots to meet vehicles. This assumption restricts\nthe optimization potential in ride-pooling operations. In this paper, we\npropose a novel matching method that incorporates extended pickup and drop-off\nareas for passengers. We first design a tree-based approach to efficiently\ngenerate feasible matches between passengers and vehicles. Next, we optimize\nvehicle routes to cover all designated pickup and drop-off locations while\nminimizing total travel distance. Finally, we employ dynamic assignment\nstrategies to achieve optimal matching outcomes. Experiments on city-scale taxi\ndatasets demonstrate that our method improves the number of served requests by\nup to 13\\% and average travel distance by up to 21\\% compared to leading\nexisting solutions, underscoring the potential of leveraging passenger mobility\nto significantly enhance ride-pooling service efficiency.",
      "tldr_zh": "本研究针对 Ride-Pool Matching Problem (RMP) 的局限性，提出了一种优化方法，允许乘客步行至扩展的接送区域，以提升乘车共享服务的效率。方法包括使用树状(tree-based)方法生成可行乘客与车辆匹配、优化车辆路线以最小化总旅行距离，以及采用动态分配(dynamic assignment)策略来实现最佳匹配。实验在城市规模出租车数据集上显示，该方法将服务请求数量提高多达13%，并将平均旅行距离减少多达21%，证明了利用乘客移动性的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08472v1",
      "published_date": "2025-03-11 14:17:30 UTC",
      "updated_date": "2025-03-11 14:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:48:07.377814"
    },
    {
      "arxiv_id": "2503.08467v1",
      "title": "Accelerating MoE Model Inference with Expert Sharding",
      "title_zh": "通过专家分片加速 MoE 模型推理",
      "authors": [
        "Oana Balmau",
        "Anne-Marie Kermarrec",
        "Rafael Pires",
        "André Loureiro Espírito Santo",
        "Martijn de Vos",
        "Milos Vujasinovic"
      ],
      "abstract": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference.",
      "tldr_zh": "本文提出 MoEShard 系统，用于加速 MoE（Mixture of Experts）模型的推理过程，针对多 GPU 环境下的专家并行问题，通过专家张量分片实现完美的负载平衡，避免了传统方法依赖启发式容量因子或丢弃 tokens 的缺点。MoEShard 通过对专家矩阵进行行和列分解，均匀分布计算、确保全 token 保留、减少空闲时间，并融合分解计算以最小化内核启动，从而显著提高吞吐量。在实验中，与 DeepSpeed 相比，该系统在编码器-based 架构上将 TTFT（time to first token）速度提升高达 6.4 倍，证明了张量分片在 MoE 推理中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in the proceedings of the 5th Workshop on Machine Learning\n  and Systems (EuroMLSys 25)",
      "pdf_url": "http://arxiv.org/pdf/2503.08467v1",
      "published_date": "2025-03-11 14:15:01 UTC",
      "updated_date": "2025-03-11 14:15:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:48:21.627694"
    },
    {
      "arxiv_id": "2503.10679v2",
      "title": "End-to-end Learning of Sparse Interventions on Activations to Steer Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Pau Rodriguez",
        "Michal Klein",
        "Eleonora Gualdoni",
        "Arno Blaas",
        "Luca Zappella",
        "Marco Cuturi",
        "Xavier Suau"
      ],
      "abstract": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions.",
      "tldr_zh": "本文提出 LinEAS，一种端到端的线性激活干预方法，用于高效控制生成模型的输出，例如缓解毒性内容或探索风格变化。LinEAS 通过全局损失同时处理所有层级的分布偏移，并使用稀疏规范进行正则化，以自动选择神经元和层，从而提高鲁棒性和效率。实验结果显示，该方法仅需少量样本即可在毒性缓解任务上优于类似基线，并与精细调整方法相当，同时支持干预组合和文本到图像扩散的应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10679v2",
      "published_date": "2025-03-11 14:09:04 UTC",
      "updated_date": "2025-04-04 11:17:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:48:31.778636"
    },
    {
      "arxiv_id": "2503.08460v2",
      "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
      "title_zh": "Industry 4.0 标准化框架的现状和未来展望：欧洲视角",
      "authors": [
        "Olga Meyer",
        "Marvin Boell",
        "Christoph Legat"
      ],
      "abstract": "The rapid development of Industry 4.0 technologies requires robust and\ncomprehensive standardization to ensure interoperability, safety and efficiency\nin the Industry of the Future. This paper examines the fundamental role and\nfunctionality of standardization, with a particular focus on its importance in\nEurope's regulatory framework. Based on this, selected topics in context of\nstandardization activities in context intelligent manufacturing and digital\ntwins are highlighted and, by that, an overview of the Industry 4.0 standards\nframework is provided. This paper serves both as an informative guide to the\nexisting standards in Industry 4.0 with respect to Artificial Intelligence and\nDigital Twins, and as a call to action for increased cooperation between\nstandardization bodies and the research community. By fostering such\ncollaboration, we aim to facilitate the continued development and\nimplementation of standards that will drive innovation and progress in the\nmanufacturing sector.",
      "tldr_zh": "这篇论文探讨了Industry 4.0标准化框架的现状和未来前景，从欧洲视角出发，强调标准化在确保互操作性、安全和效率方面的关键作用。论文重点分析了标准化在智能制造和Digital Twins领域的活动，提供了一个Industry 4.0标准框架的概述，特别是涉及Artificial Intelligence的相关标准。最终，它呼吁标准化机构和研究社区加强合作，以推动制造部门的创新和发展。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08460v2",
      "published_date": "2025-03-11 14:08:57 UTC",
      "updated_date": "2025-03-12 09:30:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:48:43.153774"
    },
    {
      "arxiv_id": "2503.08455v1",
      "title": "Controlling Latent Diffusion Using Latent CLIP",
      "title_zh": "使用 Latent CLIP 控制 Latent Diffusion",
      "authors": [
        "Jason Becker",
        "Chris Wendler",
        "Peter Baylies",
        "Robert West",
        "Christian Wressnegger"
      ],
      "abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "tldr_zh": "该论文提出Latent-CLIP，一种在潜在空间（latent space）中操作的CLIP模型，用于控制潜在扩散模型（LDMs），从而避免了昂贵的VAE解码步骤，提高计算效率。作者使用2.7B对潜在图像和描述文本训练Latent-CLIP，并证明其在ImageNet基准和LDM生成版本上的零样本分类性能与传统CLIP模型相当。实验结果显示，Latent-CLIP在奖励-based noise optimization (ReNO)任务上匹配CLIP表现，同时将总管道成本降低21%，并在GenEval、T2I-CompBench和I2P基准上有效引导生成远离有害内容。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08455v1",
      "published_date": "2025-03-11 14:04:29 UTC",
      "updated_date": "2025-03-11 14:04:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:48:55.996122"
    },
    {
      "arxiv_id": "2503.08437v1",
      "title": "ICPR 2024 Competition on Rider Intention Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Shankar Gangisetty",
        "Abdul Wasi",
        "Shyam Nandan Rai",
        "C. V. Jawahar",
        "Sajay Raj",
        "Manish Prajapati",
        "Ayesha Choudhary",
        "Aaryadev Chandra",
        "Dev Chandan",
        "Shireen Chand",
        "Suvaditya Mukherjee"
      ],
      "abstract": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition.",
      "tldr_zh": "本论文介绍了ICPR 2024竞赛中的Rider Intention Prediction (RIP)，旨在通过预测摩托车手的操作意图来提升道路安全，应对交通事故增加的挑战。研究者构建了新的Rider Action Anticipation Dataset (RAAD)，包括单视图和多视图RIP任务，涵盖多样化的交通条件和照明环境。比赛收到75个注册和5个团队提交，比较了顶尖方法：状态空间模型Mamba2表现出最佳性能，SVM在随机采样和SMOTE下排名第二，而CNN-LSTM因类别不平衡问题表现较差。该工作为骑手安全辅助系统提供了宝贵的数据和基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08437v1",
      "published_date": "2025-03-11 13:50:37 UTC",
      "updated_date": "2025-03-11 13:50:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:49:08.920996"
    },
    {
      "arxiv_id": "2503.08417v1",
      "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kwan Yun",
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Junyong Noh"
      ],
      "abstract": "Despite recent advancements in learning-based motion in-betweening, a key\nlimitation has been overlooked: the requirement for character-specific\ndatasets. In this work, we introduce AnyMoLe, a novel method that addresses\nthis limitation by leveraging video diffusion models to generate motion\nin-between frames for arbitrary characters without external data. Our approach\nemploys a two-stage frame generation process to enhance contextual\nunderstanding. Furthermore, to bridge the domain gap between real-world and\nrendered character animations, we introduce ICAdapt, a fine-tuning technique\nfor video diffusion models. Additionally, we propose a ``motion-video\nmimicking'' optimization technique, enabling seamless motion generation for\ncharacters with arbitrary joint structures using 2D and 3D-aware features.\nAnyMoLe significantly reduces data dependency while generating smooth and\nrealistic transitions, making it applicable to a wide range of motion\nin-betweening tasks.",
      "tldr_zh": "该研究引入了 AnyMoLe，一种基于 video diffusion models 的方法，用于实现任意角色的 motion in-betweening，而无需依赖 character-specific datasets。AnyMoLe 通过两阶段 frame generation 过程提升上下文理解，并采用 ICAdapt 细调技术和 motion-video mimicking 优化技术，以桥接真实世界和渲染动画的领域差距。结果表明，该方法显著减少数据依赖，生成平滑且真实的动作过渡，适用于多种 motion in-betweening 任务。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "68U05",
        "I.3.7; I.4.9"
      ],
      "primary_category": "cs.GR",
      "comment": "11 pages, 10 figures, CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08417v1",
      "published_date": "2025-03-11 13:28:59 UTC",
      "updated_date": "2025-03-11 13:28:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:49:20.366446"
    },
    {
      "arxiv_id": "2503.14513v1",
      "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition",
      "title_zh": "利用神经气网络合成身体运动数据以用于情感识别",
      "authors": [
        "Seyed Muhammad Hossein Mousavi"
      ],
      "abstract": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods.",
      "tldr_zh": "该研究针对情感识别中身体动作数据缺乏多样性和泛化性的问题，提出使用 Neural Gas Network (NGN) 算法进行合成数据生成，以优化数据多样性和生成速度。NGN 通过学习骨骼结构拓扑，将神经元拟合到身体关节上，生成新的姿势序列并合成完整动作。相比于 Generative Adversarial Networks (GANs) 和 Variational Auto Encoders (VAEs)，实验结果显示 NGN 生成的数据在 Fréchet Inception Distance (FID) 和多样性指标上更出色，且在情感分类任务中表现出更高的准确率、精确率和召回率，同时具备更快的合成速度。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "A.I"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14513v1",
      "published_date": "2025-03-11 13:16:30 UTC",
      "updated_date": "2025-03-11 13:16:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:49:32.107427"
    },
    {
      "arxiv_id": "2503.08750v1",
      "title": "Exposing Product Bias in LLM Investment Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Zhi",
        "Xiaoyu Zhang",
        "Longtian Wang",
        "Shumin Jiang",
        "Shiqing Ma",
        "Xiaohong Guan",
        "Chao Shen"
      ],
      "abstract": "Large language models (LLMs), as a new generation of recommendation engines,\npossess powerful summarization and data analysis capabilities, surpassing\ntraditional recommendation systems in both scope and performance. One promising\napplication is investment recommendation. In this paper, we reveal a novel\nproduct bias in LLM investment recommendation, where LLMs exhibit systematic\npreferences for specific products. Such preferences can subtly influence user\ninvestment decisions, potentially leading to inflated valuations of products\nand financial bubbles, posing risks to both individual investors and market\nstability. To comprehensively study the product bias, we develop an automated\npipeline to create a dataset of 567,000 samples across five asset classes\n(stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this\ndataset, we present the bf first study on product bias in LLM investment\nrecommendations. Our findings reveal that LLMs exhibit clear product\npreferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from\nMicrosoft). Notably, this bias persists even after applying debiasing\ntechniques. We urge AI researchers to take heed of the product bias in LLM\ninvestment recommendations and its implications, ensuring fairness and security\nin the digital space and market.",
      "tldr_zh": "这篇论文揭示了大型语言模型（LLMs）在投资推荐中的product bias问题，即LLMs系统性地偏好特定产品，这可能影响用户决策并引发金融泡沫和市场风险。研究者开发了一个自动化管道，创建了56.7万个样本的数据集，覆盖股票、共同基金、加密货币、储蓄和投资组合等五类资产。分析结果显示，LLMs对某些股票（如AAPL和MSFT）表现出明显偏好，即使应用debiasing技术也无法完全消除。论文呼吁AI研究者重视这一偏见，以保障投资推荐的公平性和市场稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08750v1",
      "published_date": "2025-03-11 13:10:00 UTC",
      "updated_date": "2025-03-11 13:10:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:49:43.231626"
    },
    {
      "arxiv_id": "2503.08749v1",
      "title": "Source-free domain adaptation based on label reliability for cross-domain bearing fault diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyi Wu",
        "Hao Zhang",
        "Zhisen Wei",
        "Xiao-Yuan Jing",
        "Qinghua Zhang",
        "Songsong Wu"
      ],
      "abstract": "Source-free domain adaptation (SFDA) has been exploited for cross-domain\nbearing fault diagnosis without access to source data. Current methods select\npartial target samples with reliable pseudo-labels for model adaptation, which\nis sub-optimal due to the ignored target samples. We argue that every target\nsample can contribute to model adaptation, and accordingly propose in this\npaper a novel SFDA-based approach for bearing fault diagnosis that exploits\nboth reliable and unreliable pseudo-labels. We develop a\ndata-augmentation-based label voting strategy to divide the target samples into\nreliable and unreliable ones. We propose to explore the underlying relation\nbetween feature space and label space by using the reliable pseudo-labels as\nground-truth labels, meanwhile, alleviating negative transfer by maximizing the\nentropy of the unreliable pseudo-labels. The proposed method achieves\nwell-balance between discriminability and diversity by taking advantage of\nreliable and unreliable pseudo-labels. Extensive experiments are conducted on\ntwo bearing fault benchmarks, demonstrating that our approach achieves\nsignificant performance improvements against existing SFDA-based bearing fault\ndiagnosis methods. Our code is available at https://github.com/BdLab405/SDALR.",
      "tldr_zh": "本研究针对无源域适应（SFDA）在跨域轴承故障诊断中的应用，提出了一种基于标签可靠性的新方法，该方法充分利用目标样本的可靠和不可靠伪-labels（pseudo-labels），以克服现有方法的局限性。研究开发了基于数据增强的标签投票策略，将目标样本分类为可靠和不可靠，并通过可靠伪-labels探索特征空间与标签空间的关系，同时最大化不可靠伪-labels的熵以缓解负面转移，从而实现判别性和多样性的平衡。在两个轴承故障基准上的广泛实验表明，该方法相较于现有SFDA方法显著提升了性能，并提供了开源代码以便复现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 12 figures and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08749v1",
      "published_date": "2025-03-11 13:02:18 UTC",
      "updated_date": "2025-03-11 13:02:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:49:54.240025"
    },
    {
      "arxiv_id": "2503.08388v1",
      "title": "V-Max: Making RL practical for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Valentin Charraut",
        "Thomas Tournaire",
        "Waël Doulazmi",
        "Thibault Buhet"
      ],
      "abstract": "Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets. V-Max integrates a set of observation and\nreward functions, transformer-based encoders, and training pipelines.\nAdditionally, it includes adversarial evaluation settings and an extensive set\nof evaluation metrics. Through a large-scale benchmark, we analyze how network\narchitectures, observation functions, training data, and reward shaping impact\nRL performance.",
      "tldr_zh": "该研究旨在使强化学习（RL）在自动驾驶（AD）中更实用，以克服模仿学习（IL）的局限性，如分布偏移和模仿差距。论文引入了 V-Max，一个开源研究框架，基于硬件加速模拟器 Waymax 和 ScenarioNet 的方法，支持快速模拟多样化 AD 数据集，并集成了观察函数、奖励函数、transformer-based 编码器和训练管道。V-Max 还包括对抗性评估设置和广泛评估指标，通过大规模基准测试，分析了网络架构、观察函数、训练数据和奖励塑造对 RL 性能的影响。该框架为 RL 在 AD 中的应用提供了标准化工具，推动了更通用化的决策策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08388v1",
      "published_date": "2025-03-11 12:53:24 UTC",
      "updated_date": "2025-03-11 12:53:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:50:06.733965"
    },
    {
      "arxiv_id": "2503.08381v1",
      "title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction",
      "title_zh": "InfluenceNet：用于 Banzhaf 和 Shapley 值预测的 AI 模型",
      "authors": [
        "Benjamin Kempinski",
        "Tal Kachman"
      ],
      "abstract": "Power indices are essential in assessing the contribution and influence of\nindividual agents in multi-agent systems, providing crucial insights into\ncollaborative dynamics and decision-making processes. While invaluable,\ntraditional computational methods for exact or estimated power indices values\nrequire significant time and computational constraints, especially for large\n$(n\\ge10)$ coalitions. These constraints have historically limited researchers'\nability to analyse complex multi-agent interactions comprehensively. To address\nthis limitation, we introduce a novel Neural Networks-based approach that\nefficiently estimates power indices for voting games, demonstrating comparable\nand often superiour performance to existing tools in terms of both speed and\naccuracy. This method not only addresses existing computational bottlenecks,\nbut also enables rapid analysis of large coalitions, opening new avenues for\nmulti-agent system research by overcoming previous computational limitations\nand providing researchers with a more accessible, scalable analytical tool.This\nincreased efficiency will allow for the analysis of more complex and realistic\nmulti-agent scenarios.",
      "tldr_zh": "本论文提出InfluenceNet，一种基于Neural Networks的AI模型，用于高效预测多智能体系统中Banzhaf和Shapley Value等power indices，从而评估个体代理的贡献。传统计算方法在大型联盟（n ≥ 10）时面临高计算成本和时间限制，该模型通过神经网络优化估计算法，在速度和准确性上优于现有工具。实验结果显示，InfluenceNet显著缓解了计算瓶颈，支持大规模多智能体场景分析，并为复杂协作动态的研究开辟新途径。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2; F.2.1"
      ],
      "primary_category": "cs.MA",
      "comment": "20 pages main text + 6 pages appendix, 11 figures. Accepted to\n  IntelliSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08381v1",
      "published_date": "2025-03-11 12:40:42 UTC",
      "updated_date": "2025-03-11 12:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:50:18.561809"
    },
    {
      "arxiv_id": "2503.08354v2",
      "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Jason Kuen",
        "Hao Chen",
        "Xiaohao Xu",
        "Jiuxiang Gu",
        "Yinyi Luo",
        "Bhiksha Raj",
        "Zhe Lin",
        "Marios Savvides"
      ],
      "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder.",
      "tldr_zh": "该论文分析了图像生成中预构建潜在空间依赖固定的图像分词器（tokenizer）的问题，指出现有评估指标（如 rFID）无法准确关联分词器性能与生成质量（如 gFID）。为了解决潜在空间重建和生成质量的差异，作者提出了一种基于潜在扰动（latent perturbation）的方法来模拟采样噪声，从而开发出新的评估指标 pFID 和一个即插即用分词器训练方案，提升分词器的鲁棒性、生成质量和收敛速度。实验在 11 个高级离散图像分词器和 2 个自回归生成模型上验证了该方法，使用约 400M 生成器时实现了 1.60 gFID（带 classifier-free guidance, CFG）和 3.45 gFID（无 CFG）的显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 13 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08354v2",
      "published_date": "2025-03-11 12:09:11 UTC",
      "updated_date": "2025-03-17 17:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:50:32.049819"
    },
    {
      "arxiv_id": "2503.08332v1",
      "title": "MINT-Demo: Membership Inference Test Demonstrator",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel DeAlcala",
        "Aythami Morales",
        "Julian Fierrez",
        "Gonzalo Mancera",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez"
      ],
      "abstract": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training.",
      "tldr_zh": "我们介绍了 MINT-Demo，一种 Membership Inference Test 技术，用于实验性检测机器学习模型是否使用特定数据进行训练，从而提升 AI 训练过程的透明度。该方法通过对流行人脸识别模型和 5 个公共数据库（包含超过 22M 图像）的实验验证，实现了高达 89% 的准确率，证明了识别训练数据可行性。最终，研究团队开发了一个 MINT 平台作为演示工具，促进 AI 领域的透明和可信度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Demo Paper Presented at Demo Track CVPR 24' and at AAAI 25' AIGOV\n  workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.08332v1",
      "published_date": "2025-03-11 11:45:05 UTC",
      "updated_date": "2025-03-11 11:45:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:50:44.272710"
    },
    {
      "arxiv_id": "2503.08327v1",
      "title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "José Pombal",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "André F. T. Martins"
      ],
      "abstract": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (Mint), i.e., the use of the same\nor related metrics for both model tuning and evaluation. Mint can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of Mint in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMint strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for -- questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MintAdjust, a method for more reliable\nevaluation under Mint. On the WMT24 MT shared task test set, MintAdjust ranks\ntranslations and systems more accurately than state-of-the-art-metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMintAdjust outperforms AutoRank, the ensembling method used by the organizers.",
      "tldr_zh": "该论文探讨了机器翻译中指标干扰（Mint）的问题，即使用相同或相关指标进行模型调整和评估，可能导致系统性能评估失真，并与人类判断的相关性降低。研究分析了 Mint 在训练数据过滤和解码中的影响，发现即使不直接优化指标，也会扭曲实例级别的指标分数。作者提出了 MintAdjust 方法，用于更可靠的评估；在 WMT24 MT 共享任务测试集上，该方法在大多数语言对上比现有指标更准确地排名翻译和系统，尤其适用于高品质系统，并优于 AutoRank。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08327v1",
      "published_date": "2025-03-11 11:40:10 UTC",
      "updated_date": "2025-03-11 11:40:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:50:57.603506"
    },
    {
      "arxiv_id": "2503.08325v1",
      "title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data",
      "title_zh": "翻译失败",
      "authors": [
        "Lele Qi",
        "Mengna Liu",
        "Xu Cheng",
        "Fan Shi",
        "Xiufeng Liu",
        "Shengyong Chen"
      ],
      "abstract": "Wind farms, typically in high-latitude regions, face a high risk of blade\nicing. Traditional centralized training methods raise serious privacy concerns.\nTo enhance data privacy in detecting wind turbine blade icing, traditional\nfederated learning (FL) is employed. However, data heterogeneity, resulting\nfrom collections across wind farms in varying environmental conditions, impacts\nthe model's optimization capabilities. Moreover, imbalances in wind turbine\ndata lead to models that tend to favor recognizing majority classes, thus\nneglecting critical icing anomalies. To tackle these challenges, we propose a\nfederated prototype learning model for class-imbalanced data in heterogeneous\nenvironments to detect wind turbine blade icing. We also propose a contrastive\nsupervised loss function to address the class imbalance problem. Experiments on\nreal data from 20 turbines across two wind farms show our method outperforms\nfive FL models and five class imbalance methods, with an average improvement of\n19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best\nmethod, BiFL.",
      "tldr_zh": "本研究针对风力涡轮机叶片结冰检测中的数据隐私问题，提出了一种基于原型的异质联邦学习（Heterogeneous Federated Learning）模型，以应对数据异质性和类别不平衡（Class Imbalanced Data）挑战。该模型引入对比监督损失函数（Contrastive Supervised Loss），帮助模型更好地识别少数类（如结冰异常）。实验在两个风场的20个涡轮机真实数据上进行，结果显示，该方法比五种FL模型和五种类别不平衡方法平均提高了19.64%的 \\( mF_{\\beta} \\) 和5.73%的 \\( m \\)BA，优于第二好的BiFL方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08325v1",
      "published_date": "2025-03-11 11:37:43 UTC",
      "updated_date": "2025-03-11 11:37:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:51:11.268429"
    },
    {
      "arxiv_id": "2503.08322v1",
      "title": "Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs",
      "title_zh": "通过将策略蒸馏成程序评估可解释强化学习",
      "authors": [
        "Hector Kohler",
        "Quentin Delfosse",
        "Waris Radji",
        "Riad Akrour",
        "Philippe Preux"
      ],
      "abstract": "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
      "tldr_zh": "该研究针对强化学习（Reinforcement Learning）的可解释性评估问题，提出了一种无需人类参与的方法，通过将专家神经网络策略（policies）提炼成小型程序（distilling into programs）来模拟策略的可模拟性（simulatability）。他们使用模仿学习（imitation learning）生成基线策略，并进行大规模实证评估，以量化策略解释性。结果显示，提高策略解释性不一定会降低性能，有时甚至能提升它；然而，没有一种策略类能在所有任务中平衡解释性和性能，这强调了需要可靠的比较方法来推进可解释强化学习研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages of main text, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.08322v1",
      "published_date": "2025-03-11 11:34:06 UTC",
      "updated_date": "2025-03-11 11:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:51:21.762616"
    },
    {
      "arxiv_id": "2503.08308v1",
      "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
      "title_zh": "自信地观察与推理：通过不确定性感知代理框架增强多模态LLMs",
      "authors": [
        "Zhuo Zhi",
        "Chen Feng",
        "Adam Daneshmend",
        "Mine Orlu",
        "Andreas Demosthenous",
        "Lu Yin",
        "Da Li",
        "Ziquan Liu",
        "Miguel R. D. Rodrigues"
      ],
      "abstract": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent.",
      "tldr_zh": "该研究提出了一种无需训练的多模态推理框架Seeing and Reasoning with Confidence (SRICE)，旨在提升Multimodal LLMs (MLLMs) 在视觉问答等任务中的性能，同时解决Chain-of-Thought (CoT) 推理的标注成本高和agentic 方法的工具输出不可靠问题。SRICE 通过整合外部视觉模型和Uncertainty Quantification (UQ)，让 MLLMs 通过多阶段互动自主选择兴趣区域，并使用基于Conformal Prediction 的方法校准工具输出和选择最佳工具。实验结果显示，SRICE 在五个数据集上比基础 MLLMs 平均提高了 4.6% 的性能，并在某些数据集上超过了基于微调的方法，突显了确保工具可靠性的重要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08308v1",
      "published_date": "2025-03-11 11:18:53 UTC",
      "updated_date": "2025-03-11 11:18:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:51:34.166214"
    },
    {
      "arxiv_id": "2503.08302v1",
      "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ji Zhao",
        "Xiao Lin"
      ],
      "abstract": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments.",
      "tldr_zh": "这篇论文提出了一种通用空中智能代理，利用大型语言模型 (LLMs) 紧密整合推理与机器人自治，首次实现无人机 (UAVs) 在开放环境中的任务执行。系统通过硬件-软件协同设计，解决了机载 LLM 操作（在边优化计算平台上实现 14B 参数模型的 5-6 tokens/sec 推理，峰值功率 220W）和双向认知架构（结合 LLM 的审议式任务规划与快速反应控制，如状态估计、映射、障碍避免和运动规划）两大限制。实验验证显示，该代理在通信受限场景中表现出可靠的任务规划和场景理解，例如甘蔗监测、电力网格检查、矿井隧道探索和生物观察应用，从而建立了桥接任务规划与机器人自治的新框架。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08302v1",
      "published_date": "2025-03-11 11:13:58 UTC",
      "updated_date": "2025-03-11 11:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:51:46.935054"
    },
    {
      "arxiv_id": "2503.08301v2",
      "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study",
      "title_zh": "翻译失败",
      "authors": [
        "Xian-Rong Zhang",
        "Yue-Jiao Gong",
        "Jun Zhang"
      ],
      "abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "tldr_zh": "这篇论文提出了一种使用 Large Language Models (LLMs) 作为元代理模型的框架，用于数据驱动的多任务优化 (many-task optimization)，旨在通过知识转移和新兴能力减轻重复适应度评估的计算负担。框架将适应度预测视为条件概率估计，通过统一的令牌序列表示任务元数据、输入和输出，实现任务间知识共享和多任务训练。实验结果显示，该模型表现出色泛化能力，包括在未见维度上的零样本 (zero-shot) 性能，并在与进化转移优化 (ETO) 集成时，提升了优化效率和鲁棒性。该工作为 LLMs 在代理建模 (surrogate modeling) 中的应用奠定了新基础，提供了一个通用的多任务优化解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08301v2",
      "published_date": "2025-03-11 11:13:11 UTC",
      "updated_date": "2025-03-12 06:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:00.186442"
    },
    {
      "arxiv_id": "2503.08295v2",
      "title": "Preference-Based Alignment of Discrete Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Umberto Borso",
        "Davide Paglieri",
        "Jude Wells",
        "Tim Rocktäschel"
      ],
      "abstract": "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D2-DPO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D2-DPO on a structured binary sequence generation\ntask, demonstrating that the method effectively aligns model outputs with\npreferences while maintaining structural validity. Our results highlight that\nD2-DPO enables controlled fine-tuning without requiring explicit reward models,\nmaking it a practical alternative to reinforcement learning-based approaches.\nFuture research will explore extending D2-DPO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
      "tldr_zh": "本研究针对扩散模型（Diffusion models）在处理离散数据时的偏好对齐挑战，提出了一种新方法 Discrete Diffusion DPO (D2-DPO)，这是 Direct Preference Optimization (DPO) 的首次适应，应用于作为连续时间 Markov 链的离散扩散模型。D2-DPO 通过一个新颖的损失函数直接利用偏好数据对生成过程进行微调，同时保持对参考分布的忠诚性。在结构化二进制序列生成任务上，实验证明该方法能有效使模型输出符合偏好，同时维持结构的有效性。相比强化学习方法，D2-DPO 无需显式奖励模型，更加实用，并为扩展到语言建模和蛋白序列生成等复杂任务提供了潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08295v2",
      "published_date": "2025-03-11 11:07:35 UTC",
      "updated_date": "2025-04-09 14:34:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:10.507156"
    },
    {
      "arxiv_id": "2503.08292v2",
      "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges",
      "title_zh": "大型语言模型用于",
      "authors": [
        "Xiaoxiao Liu",
        "Qingying Xiao",
        "Junying Chen",
        "Xiangyi Feng",
        "Xiangbo Wu",
        "Bairui Zhang",
        "Xiang Wan",
        "Jian Chang",
        "Guangjun Yu",
        "Yan Hu",
        "Benyou Wang"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
      "tldr_zh": "这篇论文定义了大型语言模型 (LLMs) 在门诊转诊任务中的问题，包括缺乏标准化评估标准，并系统地评估了 LLMs 在智能门诊转诊 (IOR) 系统中的能力和局限性。研究提出一个全面评估框架，涵盖静态评估（评估预定义转诊能力）和动态评估（通过迭代对话完善转诊推荐）。结果显示，LLMs 与 BERT-like 模型相比优势有限，但在互动对话中表现出色，能够提出有效的提问。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08292v2",
      "published_date": "2025-03-11 11:05:42 UTC",
      "updated_date": "2025-05-08 09:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:21.585116"
    },
    {
      "arxiv_id": "2503.08280v1",
      "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
      "title_zh": "OminiControl2：针对扩散变压器的高效条件控制",
      "authors": [
        "Zhenxiong Tan",
        "Qiaochu Xue",
        "Xingyi Yang",
        "Songhua Liu",
        "Xinchao Wang"
      ],
      "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
      "tldr_zh": "该研究提出OminiControl2，一种高效框架，用于改善文本到图像扩散变换模型(DiT)的细粒度控制，解决现有方法在处理长条件输入时的计算效率问题。OminiControl2引入动态压缩策略，仅保留语义相关tokens，以及条件特征重用机制，只计算一次特征并在去噪步骤中重复利用，从而保持参数效率和多模态多功能性。实验结果显示，该框架将条件处理开销减少超过90%，在多条件生成场景中实现5.9倍加速，推动DiT模型在实际高品质图像合成中的可行应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
      "published_date": "2025-03-11 10:50:14 UTC",
      "updated_date": "2025-03-11 10:50:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:34.901426"
    },
    {
      "arxiv_id": "2503.08748v3",
      "title": "Mirror Descent and Novel Exponentiated Gradient Algorithms Using Trace-Form Entropies and Deformed Logarithms",
      "title_zh": "翻译失败",
      "authors": [
        "Andrzej Cichocki",
        "Toshihisa Tanaka",
        "Sergio Cruces"
      ],
      "abstract": "In this paper we propose and investigate a wide class of Mirror Descent\nupdates (MD) and associated novel Generalized Exponentiated Gradient (GEG)\nalgorithms by exploiting various trace-form entropies and associated deformed\nlogarithms and their inverses - deformed (generalized) exponential functions.\nThe proposed algorithms can be considered as extension of entropic MD and\ngeneralization of multiplicative updates. In the literature, there exist\nnowadays over fifty mathematically well defined generalized entropies, so\nimpossible to exploit all of them in one research paper. So we focus on a few\nselected most popular entropies and associated logarithms like the Tsallis,\nKaniadakis and Sharma-Taneja-Mittal and some of their extension like Tempesta\nor Kaniadakis-Scarfone entropies. The shape and properties of the deformed\nlogarithms and their inverses are tuned by one or more hyperparameters. By\nlearning these hyperparameters, we can adapt to distribution of training data,\nwhich can be designed to the specific geometry of the optimization problem,\nleading to potentially faster convergence and better performance. The using\ngeneralized entropies and associated deformed logarithms in the Bregman\ndivergence, used as a regularization term, provides some new insight into\nexponentiated gradient descent updates.",
      "tldr_zh": "本论文提出了一种新的 Mirror Descent (MD) 更新和 Generalized Exponentiated Gradient (GEG) 算法，利用 trace-form entropies 和 deformed logarithms 及其逆函数（如 deformed exponential functions）。这些算法扩展了 entropic MD 和 multiplicative updates，聚焦于 Tsallis、Kaniadakis 和 Sharma-Taneja-Mittal 等熵及其扩展（如 Tempesta 或 Kaniadakis-Scarfone 熵），通过调节超参数来适应训练数据的分布，提高收敛速度和优化性能。最终，该方法为 exponentiated gradient descent updates 提供了新的见解和潜在的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08748v3",
      "published_date": "2025-03-11 10:50:07 UTC",
      "updated_date": "2025-03-21 02:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:47.253999"
    },
    {
      "arxiv_id": "2503.08275v2",
      "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruibin Xiong",
        "Yimeng Chen",
        "Dmitrii Khizbullin",
        "Mingchen Zhuge",
        "Jürgen Schmidhuber"
      ],
      "abstract": "Long-form writing agents require flexible integration and interaction across\ninformation retrieval, reasoning, and composition. Current approaches rely on\npredetermined workflows and rigid thinking patterns to generate outlines before\nwriting, resulting in constrained adaptability during writing. In this paper we\npropose a general agent framework that achieves human-like adaptive writing\nthrough recursive task decomposition and dynamic integration of three\nfundamental task types, i.e. retrieval, reasoning, and composition. Our\nmethodology features: 1) a planning mechanism that interleaves recursive task\ndecomposition and execution, eliminating artificial restrictions on writing\nworkflow; and 2) integration of task types that facilitates heterogeneous task\ndecomposition. Evaluations on both fiction writing and technical report\ngeneration show that our method consistently outperforms state-of-the-art\napproaches across all automatic evaluation metrics, which demonstrate the\neffectiveness and broad applicability of our proposed framework.",
      "tldr_zh": "本研究提出了一种超越传统提纲生成的方法，即Heterogeneous Recursive Planning框架，用于实现语言模型在长文写作中的适应性。该框架通过递归任务分解和动态整合三大任务类型（retrieval、reasoning和composition），允许写作过程灵活交错执行，消除预定工作流的限制。实验结果显示，该方法在虚构写作和技术报告生成任务上，优于现有最先进方法的所有自动评估指标，证明了其有效性和广泛适用性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08275v2",
      "published_date": "2025-03-11 10:43:01 UTC",
      "updated_date": "2025-03-25 18:27:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:52:56.119326"
    },
    {
      "arxiv_id": "2503.08269v1",
      "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
      "title_zh": "Adv-CPG：一种结合面部对抗攻击的",
      "authors": [
        "Junying Wang",
        "Hongyuan Zhang",
        "Yuan Yuan"
      ],
      "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively.",
      "tldr_zh": "这篇论文提出了 Adv-CPG 框架，用于 Customized Portrait Generation (CPG)，通过 facial adversarial attacks 来保护面部隐私，同时生成高保真且可控的个性化肖像。框架的关键组件包括一个轻量级的 local ID encryptor 和 encryption enhancer，实现渐进的双层加密保护（直接注入目标身份并添加额外指导），以及一个 multi-modal image customizer，用于精细控制面部特征。实验结果表明，Adv-CPG 的平均攻击成功率比最先进（SOTA）的基于噪声攻击方法高 28.1%，并比无约束攻击方法高 2.86%，这是首次将 facial adversarial attacks 引入 CPG 领域。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR-25",
      "pdf_url": "http://arxiv.org/pdf/2503.08269v1",
      "published_date": "2025-03-11 10:34:57 UTC",
      "updated_date": "2025-03-11 10:34:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:53:09.166523"
    },
    {
      "arxiv_id": "2503.08257v2",
      "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Yiming Zhong",
        "Qi Jiang",
        "Jingyi Yu",
        "Yuexin Ma"
      ],
      "abstract": "A dexterous hand capable of grasping any object is essential for the\ndevelopment of general-purpose embodied intelligent robots. However, due to the\nhigh degree of freedom in dexterous hands and the vast diversity of objects,\ngenerating high-quality, usable grasping poses in a robust manner is a\nsignificant challenge. In this paper, we introduce DexGrasp Anything, a method\nthat effectively integrates physical constraints into both the training and\nsampling phases of a diffusion-based generative model, achieving\nstate-of-the-art performance across nearly all open datasets. Additionally, we\npresent a new dexterous grasping dataset containing over 3.4 million diverse\ngrasping poses for more than 15k different objects, demonstrating its potential\nto advance universal dexterous grasping. The code of our method and our dataset\nwill be publicly released soon.",
      "tldr_zh": "这篇论文提出了 DexGrasp Anything 方法，旨在实现对任何物体的通用机器人灵巧抓取，通过将物理约束融入基于扩散的生成模型的训练和采样阶段，以应对高自由度和物体多样性的挑战。该方法在几乎所有公开数据集上达到了最先进性能，显著提升了抓取姿势的质量和鲁棒性。此外，作者发布了一个新数据集，包含超过 340 万个多样化抓取姿势，针对超过 15k 不同物体，并计划公开代码和数据集以推动该领域发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08257v2",
      "published_date": "2025-03-11 10:21:50 UTC",
      "updated_date": "2025-03-16 13:05:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:53:20.512040"
    },
    {
      "arxiv_id": "2503.08251v1",
      "title": "MT-NAM: An Efficient and Adaptive Model for Epileptic Seizure Detection",
      "title_zh": "MT-NAM：一种高效且自适应癫痫发作检测模型",
      "authors": [
        "Arshia Afzal",
        "Volkan Cevher",
        "Mahsa Shoaran"
      ],
      "abstract": "Enhancing the accuracy and efficiency of machine learning algorithms employed\nin neural interface systems is crucial for advancing next-generation\nintelligent therapeutic devices. However, current systems often utilize basic\nmachine learning models that do not fully exploit the natural structure of\nbrain signals. Additionally, existing learning models used for neural signal\nprocessing often demonstrate low speed and efficiency during inference. To\naddress these challenges, this study introduces Micro Tree-based NAM (MT-NAM),\na distilled model based on the recently proposed Neural Additive Models (NAM).\nThe MT-NAM achieves a remarkable 100$\\times$ improvement in inference speed\ncompared to standard NAM, without compromising accuracy. We evaluate our\napproach on the CHB-MIT scalp EEG dataset, which includes recordings from 24\npatients with varying numbers of sessions and seizures. NAM achieves an 85.3\\%\nwindow-based sensitivity and 95\\% specificity. Interestingly, our proposed\nMT-NAM shows only a 2\\% reduction in sensitivity compared to the original NAM.\nTo regain this sensitivity, we utilize a test-time template adjuster (T3A) as\nan update mechanism, enabling our model to achieve higher sensitivity during\ntest time by accommodating transient shifts in neural signals. With this online\nupdate approach, MT-NAM achieves the same sensitivity as the standard NAM while\nachieving approximately 50$\\times$ acceleration in inference speed.",
      "tldr_zh": "本研究提出 MT-NAM，一种基于 Neural Additive Models (NAM) 的高效自适应模型，用于提升癫痫发作检测的准确性和推理速度。它通过模型精简实现了比标准 NAM 高达 100 倍的推理速度，同时在 CHB-MIT 头皮 EEG 数据集上保持了 85.3% 的窗口-based sensitivity 和 95% 的 specificity，仅损失 2% 的敏感度。为了恢复敏感度，研究引入 test-time template adjuster (T3A) 更新机制，使 MT-NAM 在测试时达到与 NAM 相同的敏感度，同时推理速度进一步提升约 50 倍。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Submitted to IEEE-TBME",
      "pdf_url": "http://arxiv.org/pdf/2503.08251v1",
      "published_date": "2025-03-11 10:14:53 UTC",
      "updated_date": "2025-03-11 10:14:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:53:34.650195"
    },
    {
      "arxiv_id": "2503.08250v3",
      "title": "Aligning Text to Image in Diffusion Models is Easier Than You Think",
      "title_zh": "翻译失败",
      "authors": [
        "Jaa-Yeon Lee",
        "Byunghee Cha",
        "Jeongsol Kim",
        "Jong Chul Ye"
      ],
      "abstract": "While recent advancements in generative modeling have significantly improved\ntext-image alignment, some residual misalignment between text and image\nrepresentations still remains. Although many approaches have attempted to\naddress this issue by fine-tuning models using various reward models, etc., we\nrevisit the challenge from the perspective of representation alignment-an\napproach that has gained popularity with the success of REPresentation\nAlignment (REPA). We first argue that conventional text-to-image (T2I)\ndiffusion models, typically trained on paired image and text data (i.e.,\npositive pairs) by minimizing score matching or flow matching losses, is\nsuboptimal from the standpoint of representation alignment. Instead, a better\nalignment can be achieved through contrastive learning that leverages both\npositive and negative pairs. To achieve this efficiently even with pretrained\nmodels, we introduce a lightweight contrastive fine tuning strategy called\nSoftREPA that uses soft text tokens. This approach improves alignment with\nminimal computational overhead by adding fewer than 1M trainable parameters to\nthe pretrained model. Our theoretical analysis demonstrates that our method\nexplicitly increases the mutual information between text and image\nrepresentations, leading to enhanced semantic consistency. Experimental results\nacross text-to-image generation and text-guided image editing tasks validate\nthe effectiveness of our approach in improving the semantic consistency of T2I\ngenerative models.",
      "tldr_zh": "本文指出，现有文本到图像 (T2I) 扩散模型在训练时仅使用正对 (positive pairs) 进行分数匹配或流匹配损失最小化，导致文本和图像表示的对齐 suboptimal。作者提出一种轻量级的对比学习策略 SoftREPA，使用软文本标记在预训练模型上添加少于 1M 可训练参数，从而利用正负对提升表示对齐 (Representation Alignment)。理论分析显示，该方法显式增加文本和图像表示之间的 mutual information，提升语义一致性，并在文本到图像生成和文本引导图像编辑任务的实验中验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08250v3",
      "published_date": "2025-03-11 10:14:22 UTC",
      "updated_date": "2025-03-21 07:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:53:46.478519"
    },
    {
      "arxiv_id": "2503.08241v1",
      "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents",
      "title_zh": "HASARD：",
      "authors": [
        "Tristan Tomilin",
        "Meng Fang",
        "Mykola Pechenizkiy"
      ],
      "abstract": "Advancing safe autonomous systems through reinforcement learning (RL)\nrequires robust benchmarks to evaluate performance, analyze methods, and assess\nagent competencies. Humans primarily rely on embodied visual perception to\nsafely navigate and interact with their surroundings, making it a valuable\ncapability for RL agents. However, existing vision-based 3D benchmarks only\nconsider simple navigation tasks. To address this shortcoming, we introduce\n\\textbf{HASARD}, a suite of diverse and complex tasks to $\\textbf{HA}$rness\n$\\textbf{SA}$fe $\\textbf{R}$L with $\\textbf{D}$oom, requiring strategic\ndecision-making, comprehending spatial relationships, and predicting the\nshort-term future. HASARD features three difficulty levels and two action\nspaces. An empirical evaluation of popular baseline methods demonstrates the\nbenchmark's complexity, unique challenges, and reward-cost trade-offs.\nVisualizing agent navigation during training with top-down heatmaps provides\ninsight into a method's learning process. Incrementally training across\ndifficulty levels offers an implicit learning curriculum. HASARD is the first\nsafe RL benchmark to exclusively target egocentric vision-based learning,\noffering a cost-effective and insightful way to explore the potential and\nboundaries of current and future safe RL methods. The environments and baseline\nimplementations are open-sourced at\nhttps://sites.google.com/view/hasard-bench/.",
      "tldr_zh": "这篇论文引入了 HASARD 基准，用于评估基于视觉的安全强化学习（RL）在具身代理中的性能，填补了现有基准仅限于简单导航任务的空白。HASARD 包含多样化的复杂任务，如战略决策、空间关系理解和短期未来预测，并提供三个难度级别和两种动作空间，以模拟真实环境挑战。实验结果显示，该基准突出了基线方法的复杂性、奖励-成本权衡，并通过顶视热图可视化和递增训练课程，提供了一个专注于第一人称视角视觉学习的开源平台，促进安全 RL 的发展。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08241v1",
      "published_date": "2025-03-11 10:05:01 UTC",
      "updated_date": "2025-03-11 10:05:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:53:59.168135"
    },
    {
      "arxiv_id": "2503.11696v1",
      "title": "Balancing SoC in Battery Cells using Safe Action Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "E Harshith Kumar Yadav",
        "Rahul Narava",
        "Anshika",
        "Shashi Shekher Jha"
      ],
      "abstract": "Managing equal charge levels in active cell balancing while charging a Li-ion\nbattery is challenging. An imbalance in charge levels affects the state of\nhealth of the battery, along with the concerns of thermal runaway and fire\nhazards. Traditional methods focus on safety assurance as a trade-off between\nsafety and charging time. Others deal with battery-specific conditions to\nensure safety, therefore losing on the generalization of the control strategies\nover various configurations of batteries. In this work, we propose a method to\nlearn safe battery charging actions by using a safety-layer as an add-on over a\nDeep Reinforcement Learning (RL) agent. The safety layer perturbs the agent's\naction to prevent the battery from encountering unsafe or dangerous states.\nFurther, our Deep RL framework focuses on learning a generalized policy that\ncan be effectively employed with varying configurations of batteries. Our\nexperimental results demonstrate that the safety-layer based action\nperturbation incurs fewer safety violations by avoiding unsafe states along\nwith learning a robust policy for several battery configurations.",
      "tldr_zh": "本文提出了一种使用 Safe Action Perturbations 的方法，来平衡锂离子电池单元的 State of Charge (SoC)，以应对充电过程中可能导致电池健康问题、热失控和火灾风险的不平衡问题。该方法通过在 Deep Reinforcement Learning (RL) 代理上添加安全层，对代理动作进行扰动，从而防止电池进入不安全状态，并学习一个适用于各种电池配置的泛化策略。实验结果显示，这种安全层机制显著减少了安全违规，并实现了更稳健的政策。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11696v1",
      "published_date": "2025-03-11 09:59:14 UTC",
      "updated_date": "2025-03-11 09:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:54:10.050052"
    },
    {
      "arxiv_id": "2503.08228v1",
      "title": "Investigating Execution-Aware Language Models for Code Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Di Menna",
        "Luca Traini",
        "Gabriele Bavota",
        "Vittorio Cortellessa"
      ],
      "abstract": "Code optimization is the process of enhancing code efficiency, while\npreserving its intended functionality. This process often requires a deep\nunderstanding of the code execution behavior at run-time to identify and\naddress inefficiencies effectively. Recent studies have shown that language\nmodels can play a significant role in automating code optimization. However,\nthese models may have insufficient knowledge of how code execute at run-time.\nTo address this limitation, researchers have developed strategies that\nintegrate code execution information into language models. These strategies\nhave shown promise, enhancing the effectiveness of language models in various\nsoftware engineering tasks. However, despite the close relationship between\ncode execution behavior and efficiency, the specific impact of these strategies\non code optimization remains largely unexplored. This study investigates how\nincorporating code execution information into language models affects their\nability to optimize code. Specifically, we apply three different training\nstrategies to incorporate four code execution aspects -- line executions, line\ncoverage, branch coverage, and variable states -- into CodeT5+, a well-known\nlanguage model for code. Our results indicate that execution-aware models\nprovide limited benefits compared to the standard CodeT5+ model in optimizing\ncode.",
      "tldr_zh": "本研究调查了将代码执行信息融入语言模型，以提升代码优化的效果。研究者针对 CodeT5+ 模型应用了三种训练策略，整合了四种代码执行方面，包括 line executions、line coverage、branch coverage 和 variable states。结果显示，与标准 CodeT5+ 模型相比，这些 execution-aware 模型在代码优化任务上仅提供了有限的益处。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08228v1",
      "published_date": "2025-03-11 09:46:07 UTC",
      "updated_date": "2025-03-11 09:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:54:21.396815"
    },
    {
      "arxiv_id": "2503.08226v1",
      "title": "A Grey-box Text Attack Framework using Explainable AI",
      "title_zh": "基于可解释 AI 的灰盒文本攻击框架",
      "authors": [
        "Esther Chiramal",
        "Kelvin Soh Boon Kai"
      ],
      "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models.",
      "tldr_zh": "该论文提出了一种基于 Explainable AI 的灰盒文本攻击框架，用于识别和利用 AI 模型的漏洞，而无需了解目标模型的内部细节。框架利用一组代理 Transformer/BERT 模型，通过 Explainable AI 技术生成语义上合理的攻击句子，仅需改变少量词汇（如一个词），使其不易被人类察觉。实验结果表明，这种方法能有效欺骗其他 BERT 模型，展示了攻击的可转移性和隐蔽性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08226v1",
      "published_date": "2025-03-11 09:44:17 UTC",
      "updated_date": "2025-03-11 09:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:54:33.428303"
    },
    {
      "arxiv_id": "2503.08745v1",
      "title": "Neural Network for Blind Unmixing: a novel MatrixConv Unmixing (MCU) Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Zhou",
        "Wei Pu",
        "Miguel Rodrigues"
      ],
      "abstract": "Hyperspectral image (HSI) unmixing is a challenging research problem that\ntries to identify the constituent components, known as endmembers, and their\ncorresponding proportions, known as abundances, in the scene by analysing\nimages captured by hyperspectral cameras. Recently, many deep learning based\nunmixing approaches have been proposed with the surge of machine learning\ntechniques, especially convolutional neural networks (CNN). However, these\nmethods face two notable challenges: 1. They frequently yield results lacking\nphysical significance, such as signatures corresponding to unknown or\nnon-existent materials. 2. CNNs, as general-purpose network structures, are not\nexplicitly tailored for unmixing tasks. In response to these concerns, our work\ndraws inspiration from double deep image prior (DIP) techniques and algorithm\nunrolling, presenting a novel network structure that effectively addresses both\nissues. Specifically, we first propose a MatrixConv Unmixing (MCU) approach for\nendmember and abundance estimation, respectively, which can be solved via\ncertain iterative solvers. We then unroll these solvers to build two\nsub-networks, endmember estimation DIP (UEDIP) and abundance estimation DIP\n(UADIP), to generate the estimation of endmember and abundance, respectively.\nThe overall network is constructed by assembling these two sub-networks. In\norder to generate meaningful unmixing results, we also propose a composite loss\nfunction. To further improve the unmixing quality, we also add explicitly a\nregularizer for endmember and abundance estimation, respectively. The proposed\nmethods are tested for effectiveness on both synthetic and real datasets.",
      "tldr_zh": "本文提出了一种新型 MatrixConv Unmixing (MCU) 方法，用于 hyperspectral image (HSI) 的盲分离问题，旨在解决现有 CNN 模型在端元（endmembers）和丰度（abundances）估计中存在的物理意义缺失和针对性不足的挑战。该方法受 double deep image prior (DIP) 和 algorithm unrolling 启发，将迭代求解器展开成两个子网络：端元估计 DIP (UEDIP) 和丰度估计 DIP (UADIP)，并通过复合损失函数和正则化器优化结果。在合成和真实数据集上的实验验证了该方法的有效性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08745v1",
      "published_date": "2025-03-11 09:41:57 UTC",
      "updated_date": "2025-03-11 09:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:54:48.635453"
    },
    {
      "arxiv_id": "2503.08221v1",
      "title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People",
      "title_zh": "翻译失败",
      "authors": [
        "Junbin Xiao",
        "Nanxin Huang",
        "Hao Qiu",
        "Zhulin Tao",
        "Xun Yang",
        "Richang Hong",
        "Meng Wang",
        "Angela Yao"
      ],
      "abstract": "We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 4,927 questions directly posed or generated and verified by blind\nindividuals to reflect their needs for visual assistance under various\nscenarios. We provide each question with an average of 3 reference answers to\nalleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15\nleading MLLMs and find that all models struggle, with the best performers\nachieving accuracy around 56\\%, far behind human performance of 87.4\\%. To\nguide future advancements, we identify and summarize major limitations of\nexisting MLLMs in egocentric visual assistance for the blind and provide\nheuristic suggestions for improvement. With these efforts, we hope EgoBlind can\nserve as a valuable foundation for developing more effective AI assistants to\nenhance the independence of the blind individuals' lives.",
      "tldr_zh": "该研究引入了EgoBlind，这是第一个由盲人收集的egocentric VideoQA数据集，用于评估多模态大型语言模型(MLLMs)在辅助盲人日常生活方面的能力。该数据集包含1,210个第一人称视角视频和4,927个问题，这些问题由盲人提出或验证，并为每个问题提供平均3个参考答案，以减少主观性。在评估15个领先MLLMs时，发现最佳模型的准确率仅为56%，远低于人类的87.4%。论文总结了现有MLLMs在egocentric视觉辅助中的主要局限性，并提出改进建议，以推动开发更有效的AI助手，提升盲人独立生活。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.08221v1",
      "published_date": "2025-03-11 09:40:31 UTC",
      "updated_date": "2025-03-11 09:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:54:58.495989"
    },
    {
      "arxiv_id": "2503.08219v1",
      "title": "CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning",
      "title_zh": "CL-MVSNet：基于双层次对比学习的无监督多视图立体",
      "authors": [
        "Kaiqiang Xiong",
        "Rui Peng",
        "Zhe Zhang",
        "Tianxing Feng",
        "Jianbo Jiao",
        "Feng Gao",
        "Ronggang Wang"
      ],
      "abstract": "Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress\nrecently. However, previous methods primarily depend on the photometric\nconsistency assumption, which may suffer from two limitations:\nindistinguishable regions and view-dependent effects, e.g., low-textured areas\nand reflections. To address these issues, in this paper, we propose a new\ndual-level contrastive learning approach, named CL-MVSNet. Specifically, our\nmodel integrates two contrastive branches into an unsupervised MVS framework to\nconstruct additional supervisory signals. On the one hand, we present an\nimage-level contrastive branch to guide the model to acquire more context\nawareness, thus leading to more complete depth estimation in indistinguishable\nregions. On the other hand, we exploit a scene-level contrastive branch to\nboost the representation ability, improving robustness to view-dependent\neffects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5\nphotometric consistency loss, which encourages the model to focus more on\naccurate points while mitigating the gradient penalty of undesirable ones.\nExtensive experiments on DTU and Tanks&Temples benchmarks demonstrate that our\napproach achieves state-of-the-art performance among all end-to-end\nunsupervised MVS frameworks and outperforms its supervised counterpart by a\nconsiderable margin without fine-tuning.",
      "tldr_zh": "该论文提出 CL-MVSNet，一种基于双层对比学习的无监督多视图立体（Multi-View Stereo, MVS）方法，旨在解决传统方法依赖光度一致性假设所带来的问题，如难以区分的区域和视点相关效果。CL-MVSNet 整合图像级对比学习分支以提升上下文感知，从而改善难以区分区域的深度估计完整性，以及场景级对比学习分支以增强表示能力，提高对视点相关效果的鲁棒性；此外，还引入 L0.5 光度一致性损失，专注于准确点以恢复更精确的 3D 几何。在 DTU 和 Tanks&Temples 基准测试中，该方法在端到端无监督 MVS 框架中达到最先进性能，并显著超过其监督对应模型，无需微调。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accpetd by ICCV2023",
      "pdf_url": "http://arxiv.org/pdf/2503.08219v1",
      "published_date": "2025-03-11 09:39:06 UTC",
      "updated_date": "2025-03-11 09:39:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:55:13.045350"
    },
    {
      "arxiv_id": "2503.08213v1",
      "title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch",
      "title_zh": "DeepRAG：从零开始构建自定义 Hindi 嵌入模型用于检索增强生成",
      "authors": [
        "Nandakishor M"
      ],
      "abstract": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter.",
      "tldr_zh": "该研究介绍了DeepRAG，一种从零构建的自定义印地语(Hindi)嵌入模型，用于提升RAG系统的检索性能，以解决印地语作为高使用率语言却缺乏高质量嵌入的问题。研究团队收集了超过2.7M的印地语文本样本，训练了一个理解印地语形态的SentencePiece tokenizer，并设计了transformer架构，结合印地语特定的attention机制和contrastive learning进行优化。实验结果显示，与多语言模型相比，DeepRAG的检索精度提高了23%，并已与LangChain集成，为低资源语言的NLP任务提供了一个语言特定方法范例。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08213v1",
      "published_date": "2025-03-11 09:27:56 UTC",
      "updated_date": "2025-03-11 09:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:55:22.862759"
    },
    {
      "arxiv_id": "2503.08205v1",
      "title": "OLMD: Orientation-aware Long-term Motion Decoupling for Continuous Sign Language Recognition",
      "title_zh": "OLMD：方向感知的长期运动解耦用于连续手语识别",
      "authors": [
        "Yiheng Yu",
        "Sheng Liu",
        "Yuan Feng",
        "Min Xu",
        "Zhelun Jin",
        "Xuhua Yang"
      ],
      "abstract": "The primary challenge in continuous sign language recognition (CSLR) mainly\nstems from the presence of multi-orientational and long-term motions. However,\ncurrent research overlooks these crucial aspects, significantly impacting\naccuracy. To tackle these issues, we propose a novel CSLR framework:\nOrientation-aware Long-term Motion Decoupling (OLMD), which efficiently\naggregates long-term motions and decouples multi-orientational signals into\neasily interpretable components. Specifically, our innovative Long-term Motion\nAggregation (LMA) module filters out static redundancy while adaptively\ncapturing abundant features of long-term motions. We further enhance\norientation awareness by decoupling complex movements into horizontal and\nvertical components, allowing for motion purification in both orientations.\nAdditionally, two coupling mechanisms are proposed: stage and cross-stage\ncoupling, which together enrich multi-scale features and improve the\ngeneralization capabilities of the model. Experimentally, OLMD shows SOTA\nperformance on three large-scale datasets: PHOENIX14, PHOENIX14-T, and\nCSL-Daily. Notably, we improved the word error rate (WER) on PHOENIX14 by an\nabsolute 1.6% compared to the previous SOTA",
      "tldr_zh": "本研究针对连续手语识别(CSLR)中的多方向和长期动作挑战，提出了一种新框架：Orientation-aware Long-term Motion Decoupling (OLMD)，旨在高效聚合长期动作并解耦多方向信号。OLMD的核心组件包括Long-term Motion Aggregation (LMA)模块，用于过滤静态冗余并捕获丰富动作特征，同时将复杂动作分解为水平和垂直组件以实现动作净化。框架还引入了阶段耦合和跨阶段耦合机制，提升多尺度特征的融合和模型的泛化能力。在实验中，OLMD在PHOENIX14、PHOENIX14-T和CSL-Daily数据集上达到了SOTA性能，特别是在PHOENIX14上将字错误率(WER)降低了1.6%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08205v1",
      "published_date": "2025-03-11 09:20:06 UTC",
      "updated_date": "2025-03-11 09:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:55:36.777289"
    },
    {
      "arxiv_id": "2503.08199v1",
      "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Zhang",
        "Zhenlong Fang",
        "Tianyi Wang",
        "Qian Zhang",
        "Shuai Lu",
        "Junfeng Jiao",
        "Tianyu Shi"
      ],
      "abstract": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
      "tldr_zh": "该研究针对传统 Reinforcement Learning (RL) 在多智能体场景中复制人类行为、泛化和解释性方面的局限性，提出了一种 Cascading Cooperative Multi-agent (CCMA) 框架，用于上坡合并控制。CCMA 框架整合了 RL 用于个体互动、微调的 Large Language Model (LLM) 用于区域合作、奖励函数实现全局优化，以及 Retrieval-augmented Generation (RAG) 机制动态优化决策。实验结果表明，CCMA 在复杂驾驶环境中显著优于现有 RL 方法，在微观和宏观性能上均取得显著改善。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08199v1",
      "published_date": "2025-03-11 09:08:04 UTC",
      "updated_date": "2025-03-11 09:08:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:55:48.603520"
    },
    {
      "arxiv_id": "2503.08193v1",
      "title": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents",
      "title_zh": "猜猜我在想什么：角色扮演语言代理的内部思维推理基准测试",
      "authors": [
        "Rui Xu",
        "MingYu Wang",
        "XinTao Wang",
        "Dakuan Lu",
        "Xiaoyu Tan",
        "Wei Chu",
        "Yinghui Xu"
      ],
      "abstract": "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https://github.com/airaer1998/RPA_Thought.",
      "tldr_zh": "本文提出 ROLETHINK 基准，用于评估角色扮演语言代理 (RPLAs) 的内部思维生成，强调 chain-of-thought 推理在理解角色内心过程的重要性。该基准包括内部思维推理任务的金标准集 (gold set) 和银标准集 (silver set)，分别通过与原角色独白或专家分析进行比较。作者开发了 MIRROR 方法，该方法通过检索 memories、预测 reactions 和合成 motivations 来生成角色思维；实验显示 MIRROR 显著优于现有方法，证明了内部思维推理对 RPLAs 性能的提升。资源可访问 https://github.com/airaer1998/RPA_Thought。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08193v1",
      "published_date": "2025-03-11 08:57:07 UTC",
      "updated_date": "2025-03-11 08:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:55:59.082091"
    },
    {
      "arxiv_id": "2503.08188v1",
      "title": "RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware",
      "title_zh": "翻译失败",
      "authors": [
        "Gonzalo Santamaría Gómez",
        "Guillem García Subies",
        "Pablo Gutiérrez Ruiz",
        "Mario González Valero",
        "Natàlia Fuertes",
        "Helena Montoro Zamorano",
        "Carmen Muñoz Sanz",
        "Leire Rosado Plaza",
        "Nuria Aldama García",
        "David Betancur Sánchez",
        "Kateryna Sushkova",
        "Marta Guerrero Nieto",
        "Álvaro Barbero Jiménez"
      ],
      "abstract": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks.",
      "tldr_zh": "这篇论文介绍了RigoChat 2，一种针对西班牙语任务的适应性语言模型，使用有限数据集和硬件资源来优化Large Language Models (LLMs)。作者证明，通过基于一个较小预训练LLM的快速适应过程，可以在不牺牲整体性能的情况下显著提升西班牙语处理效果。实验结果显示，这种方法能够在短时间内实现高效的语言任务优化，为资源受限场景下的模型适应提供了实用方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08188v1",
      "published_date": "2025-03-11 08:53:53 UTC",
      "updated_date": "2025-03-11 08:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:56:09.268943"
    },
    {
      "arxiv_id": "2503.08179v3",
      "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zicheng Ma",
        "Chuanliu Fan",
        "Zhicong Wang",
        "Zhenyu Chen",
        "Xiaohan Lin",
        "Yanheng Li",
        "Shihao Feng",
        "Jun Zhang",
        "Ziqiang Cao",
        "Yi Qin Gao"
      ],
      "abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
      "tldr_zh": "本研究提出ProtTeX框架，利用Large Language Models (LLMs)对蛋白质进行结构感知的推理和编辑，通过将蛋白质序列、结构和文本信息统一到一个离散空间中，实现多模态蛋白质推理和生成。框架采用Next-Token Prediction范式进行联合训练，允许LLMs通过顺序文本输入处理蛋白结构作为中间推理组件，并通过文本输出生成或操作结构。实验结果显示，ProtTeX在蛋白功能预测上准确率比最先进模型提高一倍，并支持高质量的构象生成和可定制蛋白设计，首次证明decoder-only LLMs能有效处理多样蛋白质任务。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "26 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08179v3",
      "published_date": "2025-03-11 08:43:05 UTC",
      "updated_date": "2025-03-13 13:54:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:56:21.361225"
    },
    {
      "arxiv_id": "2503.08175v1",
      "title": "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zitong Shi",
        "Guancheng Wan",
        "Wenke Huang",
        "Guibin Zhang",
        "Jiawei Shao",
        "Mang Ye",
        "Carl Yang"
      ],
      "abstract": "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
      "tldr_zh": "该论文引入Federated Multi-Agent Systems (Federated MAS) 的概念，以解决LLM-based Multi-Agent Systems (MAS) 在敏感领域面临的隐私保护挑战，并强调其与传统Federated Learning (FL) 的根本差异。论文识别了关键挑战，包括代理间的异构隐私协议、多方对话结构差异以及动态对话网络结构，并提出Embedded Privacy-Enhancing Agents (EPEAgent) 解决方案，该方案无缝整合到Retrieval-Augmented Generation (RAG) 阶段和上下文检索阶段，以最小化数据流动并仅共享任务相关信息。通过设计一个全面数据集并进行广泛实验，研究证明EPEAgent 有效提升了隐私保护，同时保持了系统的强大性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08175v1",
      "published_date": "2025-03-11 08:38:45 UTC",
      "updated_date": "2025-03-11 08:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:56:33.555961"
    },
    {
      "arxiv_id": "2503.08174v1",
      "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Veronica Bot",
        "Zheyuan Xu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated unprecedented capability in\nreasoning with natural language. Coupled with this development is the emergence\nof embodied AI in robotics. Despite showing promise for verbal and written\nreasoning tasks, it remains unknown whether LLMs are capable of navigating\ncomplex spatial tasks with physical actions in the real world. To this end, it\nis of interest to investigate applying LLMs to robotics in zero-shot learning\nscenarios, and in the absence of fine-tuning - a feat which could significantly\nimprove human-robot interaction, alleviate compute cost, and eliminate\nlow-level programming tasks associated with robot tasks.\n  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot\nin Webots engine for an object search task. We evaluate the effectiveness of\nthree reasoning strategies based on Chain-of-Thought (CoT) sub-task list\ngeneration with the Socratic method (SocraCoT) (in order of increasing rigor):\n(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was\nmeasured in terms of the proportion of tasks successfully completed and\nexecution time (N = 20). Our preliminary results show that when combined with\nchain-of-thought reasoning, the Socratic method can be used for code generation\nfor robotic tasks that require spatial awareness. In extension of this finding,\nwe propose EVINCE-LoC; a modified EVINCE method that could further enhance\nperformance in highly complex and or dynamic testing scenarios.",
      "tldr_zh": "该研究调查了Socratic Chain-of-Thoughts Reasoning Method (SocraCoT)在机器人任务规划中的有效性，通过一个物体搜索案例研究来评估大型语言模型(LLMs)如GPT-4(Omni)是否能处理复杂空间任务。实验在Webots引擎中模拟Tiago机器人，比较了三种推理策略：(1) 非CoT/非SocraCoT，(2) 仅CoT，以及(3) SocraCoT，结果显示SocraCoT显著提高了任务完成比例和执行效率。研究发现，结合Socratic方法可以有效生成需要空间感知的机器人代码，从而改善人机交互并减少编程需求。最后，论文提出EVINCE-LoC作为一种改进方法，以提升模型在高度复杂或动态场景中的性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08174v1",
      "published_date": "2025-03-11 08:36:37 UTC",
      "updated_date": "2025-03-11 08:36:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:56:47.328176"
    },
    {
      "arxiv_id": "2503.08163v1",
      "title": "XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change",
      "title_zh": "XAI4Extremes：一个可解释机器学习框架，用于理解气候变化下的极端天气前兆",
      "authors": [
        "Jiawen Wei",
        "Aniruddha Bora",
        "Vivek Oommen",
        "Chenyu Dong",
        "Juntao Yang",
        "Jeff Adie",
        "Chen Chen",
        "Simon See",
        "George Karniadakis",
        "Gianmarco Mengaldo"
      ],
      "abstract": "Extreme weather events are increasing in frequency and intensity due to\nclimate change. This, in turn, is exacting a significant toll in communities\nworldwide. While prediction skills are increasing with advances in numerical\nweather prediction and artificial intelligence tools, extreme weather still\npresent challenges. More specifically, identifying the precursors of such\nextreme weather events and how these precursors may evolve under climate change\nremain unclear. In this paper, we propose to use post-hoc interpretability\nmethods to construct relevance weather maps that show the key extreme-weather\nprecursors identified by deep learning models. We then compare this machine\nview with existing domain knowledge to understand whether deep learning models\nidentified patterns in data that may enrich our understanding of\nextreme-weather precursors. We finally bin these relevant maps into different\nmulti-year time periods to understand the role that climate change is having on\nthese precursors. The experiments are carried out on Indochina heatwaves, but\nthe methodology can be readily extended to other extreme weather events\nworldwide.",
      "tldr_zh": "该研究提出XAI4Extremes框架，这是一个可解释的机器学习方法，用于理解气候变化下极端天气事件的前兆。框架利用post-hoc interpretability methods构建相关天气地图，展示深度学习模型识别的关键前兆，并将这些地图与现有领域知识进行比较，以丰富对极端天气模式的认知。研究进一步分析不同多年来这些前兆的变化，揭示气候变化的影响；在印度支那热浪实验中，方法证明有效，并可扩展至全球其他极端天气事件。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08163v1",
      "published_date": "2025-03-11 08:27:08 UTC",
      "updated_date": "2025-03-11 08:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:56:58.670479"
    },
    {
      "arxiv_id": "2503.08741v3",
      "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
      "title_zh": "Oasis：一张图像就足够用于多模态指令数据合成",
      "authors": [
        "Letian Zhang",
        "Quan Cui",
        "Bingchen Zhao",
        "Cheng Yang"
      ],
      "abstract": "The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and dataset are publicly available at\nhttps://github.com/Letian2003/MM_INF.",
      "tldr_zh": "这篇论文提出Oasis方法，仅使用一张图像即可合成高质量的多模态指令数据，解决MLLMs训练数据短缺和收集难题的问题。Oasis通过向MLLMs提示图像来大幅扩展数据多样性，并引入精细的质量控制机制确保合成数据的可靠性。实验结果显示，在LLaVA-NeXT上使用超过50万条合成数据进行增量训练，显著提升了MLLMs的整体性能，并增强了其特定领域能力。代码和数据集已公开在GitHub上。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08741v3",
      "published_date": "2025-03-11 08:25:40 UTC",
      "updated_date": "2025-03-26 09:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:57:11.071605"
    },
    {
      "arxiv_id": "2503.08145v1",
      "title": "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
      "title_zh": "轨迹关注：轨迹感知的开放词汇跟踪",
      "authors": [
        "Yunhao Li",
        "Yifan Jiao",
        "Dan Meng",
        "Heng Fan",
        "Libo Zhang"
      ],
      "abstract": "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
      "tldr_zh": "本文提出 TRACT，一种基于轨迹感知的开放词汇多对象跟踪（Open-Vocabulary Multi-Object Tracking, OV-MOT）方法，旨在通过利用轨迹信息提升对象关联稳定性和分类准确性，尤其在遮挡和类别模糊场景中。TRACT 引入 Trajectory Consistency Reinforcement (TCR) 策略来强化目标身份和类别一致性，并开发了 TraCLIP 模块，该模块结合 Trajectory Feature Aggregation (TFA) 和 Trajectory Semantic Enrichment (TSE) 策略，从视觉和语言角度整合轨迹信息以优化分类结果。在 OV-TAO 数据集上的广泛实验显示，TRACT 显著提高了跟踪性能，突显了轨迹信息在 OV-MOT 中的价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08145v1",
      "published_date": "2025-03-11 08:03:47 UTC",
      "updated_date": "2025-03-11 08:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:57:23.907062"
    },
    {
      "arxiv_id": "2503.08136v1",
      "title": "FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Jeongsol Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Flow matching is a recent state-of-the-art framework for generative modeling\nbased on ordinary differential equations (ODEs). While closely related to\ndiffusion models, it provides a more general perspective on generative\nmodeling. Although inverse problem solving has been extensively explored using\ndiffusion models, it has not been rigorously examined within the broader\ncontext of flow models. Therefore, here we extend the diffusion inverse solvers\n(DIS) - which perform posterior sampling by combining a denoising diffusion\nprior with an likelihood gradient - into the flow framework. Specifically, by\ndriving the flow-version of Tweedie's formula, we decompose the flow ODE into\ntwo components: one for clean image estimation and the other for noise\nestimation. By integrating the likelihood gradient and stochastic noise into\neach component, respectively, we demonstrate that posterior sampling for\ninverse problem solving can be effectively achieved using flows. Our proposed\nsolver, Flow-Driven Posterior Sampling (FlowDPS), can also be seamlessly\nintegrated into a latent flow model with a transformer architecture. Across\nfour linear inverse problems, we confirm that FlowDPS outperforms\nstate-of-the-art alternatives, all without requiring additional training.",
      "tldr_zh": "本研究提出FlowDPS，一种基于Flow Matching框架的后验采样方法，用于解决逆问题。论文扩展了扩散逆问题求解器(DIS)，通过驱动Flow版本的Tweedie's公式，将Flow ODE分解为清洁图像估计和噪声估计两个组件，并分别整合likelihood梯度和随机噪声，实现有效的后验采样。在四个线性逆问题上，FlowDPS无需额外训练就优于现有最先进方法，展示了其在生成模型中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08136v1",
      "published_date": "2025-03-11 07:56:14 UTC",
      "updated_date": "2025-03-11 07:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:57:35.094619"
    },
    {
      "arxiv_id": "2503.08133v1",
      "title": "MGHanD: Multi-modal Guidance for authentic Hand Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Taehyeon Eum",
        "Jieun Choi",
        "Tae-Kyun Kim"
      ],
      "abstract": "Diffusion-based methods have achieved significant successes in T2I\ngeneration, providing realistic images from text prompts. Despite their\ncapabilities, these models face persistent challenges in generating realistic\nhuman hands, often producing images with incorrect finger counts and\nstructurally deformed hands. MGHanD addresses this challenge by applying\nmulti-modal guidance during the inference process. For visual guidance, we\nemploy a discriminator trained on a dataset comprising paired real and\ngenerated images with captions, derived from various hand-in-the-wild datasets.\nWe also employ textual guidance with LoRA adapter, which learns the direction\nfrom `hands' towards more detailed prompts such as `natural hands', and\n`anatomically correct fingers' at the latent level. A cumulative hand mask\nwhich is gradually enlarged in the assigned time step is applied to the added\nguidance, allowing the hand to be refined while maintaining the rich generative\ncapabilities of the pre-trained model. In the experiments, our method achieves\nsuperior hand generation qualities, without any specific conditions or priors.\nWe carry out both quantitative and qualitative evaluations, along with user\nstudies, to showcase the benefits of our approach in producing high-quality\nhand images.",
      "tldr_zh": "扩散模型在 T2I 生成中常面临生成真实人类手部的挑战，如手指数量错误和结构变形。MGHanD 方法通过多模态指导在推理过程中解决问题，包括使用在配对真实和生成图像数据集上训练的判别器进行视觉指导，以及 LoRA 适配器将 'hands' 引导至更详细的文本提示如 'natural hands' 和 'anatomically correct fingers'。此外，该方法应用逐渐扩大的累积手部掩码，以细化手部细节，同时保留预训练模型的生成能力。实验结果显示，MGHanD 在无特定条件或先验的情况下显著提升了手部生成质量，并通过定量、定性和用户研究证明了其优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08133v1",
      "published_date": "2025-03-11 07:51:47 UTC",
      "updated_date": "2025-03-11 07:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:57:50.318858"
    },
    {
      "arxiv_id": "2503.08122v1",
      "title": "Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments",
      "title_zh": "走向稳定的世界模型：测量和解决生成式环境中的世界不稳定性",
      "authors": [
        "Soonwoo Kwon",
        "Jin-Young Kim",
        "Hyojun Go",
        "Kyungjune Baek"
      ],
      "abstract": "We present a novel study on enhancing the capability of preserving the\ncontent in world models, focusing on a property we term World Stability. Recent\ndiffusion-based generative models have advanced the synthesis of immersive and\nrealistic environments that are pivotal for applications such as reinforcement\nlearning and interactive game engines. However, while these models excel in\nquality and diversity, they often neglect the preservation of previously\ngenerated scenes over time--a shortfall that can introduce noise into agent\nlearning and compromise performance in safety-critical settings. In this work,\nwe introduce an evaluation framework that measures world stability by having\nworld models perform a sequence of actions followed by their inverses to return\nto their initial viewpoint, thereby quantifying the consistency between the\nstarting and ending observations. Our comprehensive assessment of\nstate-of-the-art diffusion-based world models reveals significant challenges in\nachieving high world stability. Moreover, we investigate several improvement\nstrategies to enhance world stability. Our results underscore the importance of\nworld stability in world modeling and provide actionable insights for future\nresearch in this domain.",
      "tldr_zh": "本研究针对世界模型（world models）的内容保存能力，专注于世界稳定性（World Stability），旨在解决扩散模型（diffusion-based generative models）在生成沉浸式环境时忽略场景长期一致性的问题。研究团队提出一个评估框架，通过让模型执行动作序列及其逆动作，返回初始视角来量化起始和结束观察的一致性。对现有模型的全面评估揭示了显著的稳定性挑战，并探索了多种改进策略。结果强调了世界稳定性的关键作用，为强化学习和互动引擎等领域提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.08122v1",
      "published_date": "2025-03-11 07:38:11 UTC",
      "updated_date": "2025-03-11 07:38:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:58:00.677933"
    },
    {
      "arxiv_id": "2503.08739v1",
      "title": "HeGMN: Heterogeneous Graph Matching Network for Learning Graph Similarity",
      "title_zh": "HeGMN：异构图匹配网络用于学习图相似性",
      "authors": [
        "Shilong Sang",
        "Ke-Jia Chen",
        "Zheng liu"
      ],
      "abstract": "Graph similarity learning (GSL), also referred to as graph matching in many\nscenarios, is a fundamental problem in computer vision, pattern recognition,\nand graph learning. However, previous GSL methods assume that graphs are\nhomogeneous and struggle to maintain their performance on heterogeneous graphs.\nTo address this problem, this paper proposes a Heterogeneous Graph Matching\nNetwork (HeGMN), which is an end-to-end graph similarity learning framework\ncomposed of a two-tier matching mechanism. Firstly, a heterogeneous graph\nisomorphism network is proposed as the encoder, which reinvents graph\nisomorphism network for heterogeneous graphs by perceiving different semantic\nrelationships during aggregation. Secondly, a graph-level and node-level\nmatching modules are designed, both employing type-aligned matching principles.\nThe former conducts graph-level matching by node type alignment, and the latter\ncomputes the interactions between the cross-graph nodes with the same type thus\nreducing noise interference and computational overhead. Finally, the\ngraph-level and node-level matching features are combined and fed into fully\nconnected layers for predicting graph similarity scores. In experiments, we\npropose a heterogeneous graph resampling method to construct heterogeneous\ngraph pairs and define the corresponding heterogeneous graph edit distance,\nfilling the gap in missing datasets. Extensive experiments demonstrate that\nHeGMN consistently achieves advanced performance on graph similarity prediction\nacross all datasets.",
      "tldr_zh": "本文提出 HeGMN，一种针对异质图的图相似性学习（Graph Similarity Learning, GSL）框架，旨在解决现有方法在异质图上的性能不足问题。该框架采用两层匹配机制，包括异质图同构网络作为编码器，用于感知不同语义关系，以及图级和节点级匹配模块，通过类型对齐原则减少噪声干扰和计算开销。最终，结合匹配特征预测图相似性分数；实验中，作者引入异质图重采样方法和编辑距离定义，并在多个数据集上验证 HeGMN 实现了先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08739v1",
      "published_date": "2025-03-11 07:36:35 UTC",
      "updated_date": "2025-03-11 07:36:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:58:11.920315"
    },
    {
      "arxiv_id": "2503.08120v2",
      "title": "Uni$\\textbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Junzhe Li",
        "Xuerui Qiu",
        "Linrui Xu",
        "Liya Guo",
        "Delin Qu",
        "Tingting Long",
        "Chun Fan",
        "Ming Li"
      ],
      "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on $\\textbf{coarse}$ facial attribute understanding,\nwith limited capacity to handle $\\textbf{fine-grained}$ facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose Uni$\\textbf{F}^2$ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUni$\\textbf{F}^2$ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUni$\\textbf{F}^2$ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUni$\\textbf{F}^2$ace-130K demonstrate that Uni$\\textbf{F}^2$ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
      "tldr_zh": "本文提出 Uni$\\textbf{F}^2$ace，一种专为细粒度面部理解和生成的统一多模态模型 (UMMs)，旨在克服现有模型在处理细粒度面部属性和生成能力上的局限。研究团队构建了大型数据集 Uni$\\textbf{F}^2$ace-130K，包含13万图像-文本对和一百万问答对，并采用两种互利的扩散技术 (diffusion techniques) 和两级混合专家架构 (mixture-of-experts) 进行训练，同时建立了离散扩散分数匹配 (discrete diffusion score matching) 与掩码生成模型之间的理论联系，以优化证据下界 (evidence lower bounds) 并提升面部细节合成能力。实验结果显示，Uni$\\textbf{F}^2$ace 在理解和生成任务上超过了现有 UMMs 和生成模型，展现出卓越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08120v2",
      "published_date": "2025-03-11 07:34:59 UTC",
      "updated_date": "2025-03-26 02:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:58:27.503051"
    },
    {
      "arxiv_id": "2503.08117v1",
      "title": "Convergence Dynamics and Stabilization Strategies of Co-Evolving Generative Models",
      "title_zh": "共同演化生成模型的收敛动态和稳定化策略",
      "authors": [
        "Weiguo Gao",
        "Ming Li"
      ],
      "abstract": "The increasing prevalence of synthetic data in training loops has raised\nconcerns about model collapse, where generative models degrade when trained on\ntheir own outputs. While prior work focuses on this self-consuming process, we\nstudy an underexplored yet prevalent phenomenon: co-evolving generative models\nthat shape each other's training through iterative feedback. This is common in\nmultimodal AI ecosystems, such as social media platforms, where text models\ngenerate captions that guide image models, and the resulting images influence\nthe future adaptation of the text model. We take a first step by analyzing such\na system, modeling the text model as a multinomial distribution and the image\nmodel as a conditional multi-dimensional Gaussian distribution. Our analysis\nuncovers three key results. First, when one model remains fixed, the other\ncollapses: a frozen image model causes the text model to lose diversity, while\na frozen text model leads to an exponential contraction of image diversity,\nthough fidelity remains bounded. Second, in fully interactive systems, mutual\nreinforcement accelerates collapse, with image contraction amplifying text\nhomogenization and vice versa, leading to a Matthew effect where dominant texts\nsustain higher image diversity while rarer texts collapse faster. Third, we\nanalyze stabilization strategies implicitly introduced by real-world external\ninfluences. Random corpus injections for text models and user-content\ninjections for image models prevent collapse while preserving both diversity\nand fidelity. Our theoretical findings are further validated through\nexperiments.",
      "tldr_zh": "本研究探讨了共进化生成模型（co-evolving generative models）的收敛动态和稳定策略，关注这些模型通过迭代反馈相互影响可能导致的模型退化（model collapse）。作者通过建模文本模型为多项分布（multinomial distribution）和图像模型为条件多维高斯分布（conditional multi-dimensional Gaussian distribution），分析了系统行为：当一个模型固定时，另一个会丧失多样性（diversity），如冻结图像模型导致文本模型多样性减少，或冻结文本模型导致图像多样性指数收缩但保真度（fidelity）保持不变。进一步发现，在完全互动系统中，相互强化加速退化，产生马太效应（Matthew effect），其中主导文本维持较高图像多样性，而稀有文本更快崩溃。研究提出稳定策略，如随机语料注入和用户内容注入，能有效防止退化，同时保留多样性和保真度，并通过实验验证了这些理论发现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 68T99"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08117v1",
      "published_date": "2025-03-11 07:30:25 UTC",
      "updated_date": "2025-03-11 07:30:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:58:37.249925"
    },
    {
      "arxiv_id": "2503.08102v2",
      "title": "AI-native Memory 2.0: Second Me",
      "title_zh": "AI 原生记忆 2.0：Second Me",
      "authors": [
        "Jiale Wei",
        "Xiang Ying",
        "Tao Gao",
        "Fangyi Bao",
        "Felix Tao",
        "Jingbo Shang"
      ],
      "abstract": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
      "tldr_zh": "该论文探讨了人类在与外部系统互动中记忆交换的冗余问题，并提出 SECOND ME，一种基于 LLMs 的 AI-native 记忆管理系统，作为智能持久的记忆卸载系统。SECOND ME 通过自主生成上下文感知响应、预填充所需信息以及与外部系统的无缝通信，显著降低用户的认知负担和互动摩擦。与传统解决方案不同，它利用 LLM-based memory parameterization 实现记忆的结构化组织、上下文推理和自适应知识检索。作者开源了部署系统（https://github.com/Mindverse/Second-Me），推动 AI 驱动个人代理在数字生态中的应用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08102v2",
      "published_date": "2025-03-11 07:05:52 UTC",
      "updated_date": "2025-03-12 11:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:58:47.479524"
    },
    {
      "arxiv_id": "2503.13494v1",
      "title": "Mobility-aware Seamless Service Migration and Resource Allocation in Multi-edge IoV Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zheyi Chen",
        "Sijin Huang",
        "Geyong Min",
        "Zhaolong Ning",
        "Jie Li",
        "Yan Zhang"
      ],
      "abstract": "Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for\nInternet-of-Vehicles (IoV) applications. However, due to high vehicle mobility\nand finite communication coverage of base stations, it is hard to maintain\nuninterrupted and high-quality services without proper service migration among\nMEC servers. Existing solutions commonly rely on prior knowledge and rarely\nconsider efficient resource allocation during the service migration process,\nmaking it hard to reach optimal performance in dynamic IoV environments. To\naddress these important challenges, we propose SR-CL, a novel mobility-aware\nseamless Service migration and Resource allocation framework via\nConvex-optimization-enabled deep reinforcement Learning in multi-edge IoV\nsystems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP)\nproblem of service migration and resource allocation into two sub-problems.\nNext, we design a new actor-critic-based asynchronous-update deep reinforcement\nlearning method to handle service migration, where the delayed-update actor\nmakes migration decisions and the one-step-update critic evaluates the\ndecisions to guide the policy update. Notably, we theoretically derive the\noptimal resource allocation with convex optimization for each MEC server,\nthereby further improving system performance. Using the real-world datasets of\nvehicle trajectories and testbed, extensive experiments are conducted to verify\nthe effectiveness of the proposed SR-CL. Compared to benchmark methods, the\nSR-CL achieves superior convergence and delay performance under various\nscenarios.",
      "tldr_zh": "本研究针对 Internet-of-Vehicles (IoV) 系统中的高车辆移动性和有限通信覆盖问题，提出 SR-CL 框架，这是一种基于凸优化增强的深度强化学习方法，用于实现无缝服务迁移和资源分配。框架首先将 Mixed Integer Nonlinear Programming (MINLP) 问题分解为服务迁移和资源分配两个子问题，然后采用 actor-critic-based 异步更新深度强化学习来处理迁移决策，并通过凸优化理论推导每个 Mobile Edge Computing (MEC) 服务器的最优资源分配。实验结果显示，SR-CL 在真实车辆轨迹数据集上比基准方法表现出更优的收敛性和延迟性能，从而提升了多边 IoV 系统的整体效能。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13494v1",
      "published_date": "2025-03-11 07:03:25 UTC",
      "updated_date": "2025-03-11 07:03:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:59:01.579316"
    },
    {
      "arxiv_id": "2503.08091v1",
      "title": "Revolution of Wireless Signal Recognition for 6G: Recent Advances, Challenges and Future Directions",
      "title_zh": "针对 6G 的无线信号识别革命：最近进展、挑战与未来方向",
      "authors": [
        "Hao Zhang",
        "Fuhui Zhou",
        "Hongyang Du",
        "Qihui Wu",
        "Chau Yuen"
      ],
      "abstract": "Wireless signal recognition (WSR) is a crucial technique for intelligent\ncommunications and spectrum sharing in the next six-generation (6G) wireless\ncommunication networks. It can be utilized to enhance network performance and\nefficiency, improve quality of service (QoS), and improve network security and\nreliability. Additionally, WSR can be applied for military applications such as\nsignal interception, signal race, and signal abduction. In the past decades,\ngreat efforts have been made for the research of WSR. Earlier works mainly\nfocus on model-based methods, including likelihood-based (LB) and feature-based\n(FB) methods, which have taken the leading position for many years. With the\nemergence of artificial intelligence (AI), intelligent methods including\nmachine learning-based (ML-based) and deep learning-based (DL-based) methods\nhave been developed to extract the features of the received signals and perform\nthe classification. In this work, we provide a comprehensive review of WSR from\nthe view of applications, main tasks, recent advances, datasets and evaluation\nmetrics, challenges, and future directions. Specifically, intelligent WSR\nmethods are introduced from the perspective of model, data, learning and\nimplementation. Moreover, we analyze the challenges for WSR from the view of\ncomplex, dynamic, and open 6G wireless environments and discuss the future\ndirections for WSR. This survey is expected to provide a comprehensive overview\nof the state-of-the-art WSR techniques and inspire new research directions for\nWSR in 6G networks.",
      "tldr_zh": "这篇论文综述了无线信号识别 (WSR) 在 6G 无线通信网络中的革命性作用，包括提升网络性能、安全性和光谱共享的应用。论文回顾了 WSR 的发展历程，从早期的基于模型的方法（如 likelihood-based 和 feature-based）转向人工智能驱动的方法（如 ML-based 和 DL-based），这些智能方法通过提取信号特征实现分类。作者从模型、数据、学习和实现角度详细介绍了 WSR 的最新进展、数据集和评估指标，并分析了 6G 环境中复杂动态挑战，如信号干扰和开放环境。最终，论文讨论了未来方向，旨在为 WSR 在 6G 网络中的创新研究提供指导和灵感。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "submitted to IEEE Communications Surveys & Tutorials",
      "pdf_url": "http://arxiv.org/pdf/2503.08091v1",
      "published_date": "2025-03-11 06:47:27 UTC",
      "updated_date": "2025-03-11 06:47:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:59:13.931960"
    },
    {
      "arxiv_id": "2503.08084v1",
      "title": "Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Fangyuan Wang",
        "Shipeng Lyu",
        "Peng Zhou",
        "Anqing Duan",
        "Guodong Guo",
        "David Navarro-Alarcon"
      ],
      "abstract": "Enabling humanoid robots to perform long-horizon mobile manipulation planning\nin real-world environments based on embodied perception and comprehension\nabilities has been a longstanding challenge. With the recent rise of large\nlanguage models (LLMs), there has been a notable increase in the development of\nLLM-based planners. These approaches either utilize human-provided textual\nrepresentations of the real world or heavily depend on prompt engineering to\nextract such representations, lacking the capability to quantitatively\nunderstand the environment, such as determining the feasibility of manipulating\nobjects. To address these limitations, we present the Instruction-Augmented\nLong-Horizon Planning (IALP) system, a novel framework that employs LLMs to\ngenerate feasible and optimal actions based on real-time sensor feedback,\nincluding grounded knowledge of the environment, in a closed-loop interaction.\nDistinct from prior works, our approach augments user instructions into PDDL\nproblems by leveraging both the abstract reasoning capabilities of LLMs and\ngrounding mechanisms. By conducting various real-world long-horizon tasks, each\nconsisting of seven distinct manipulatory skills, our results demonstrate that\nthe IALP system can efficiently solve these tasks with an average success rate\nexceeding 80%. Our proposed method can operate as a high-level planner,\nequipping robots with substantial autonomy in unstructured environments through\nthe utilization of multi-modal sensor inputs.",
      "tldr_zh": "该研究针对人形机器人进行长时域（long-horizon）移动操作规划的挑战，提出了一种名为 Instruction-Augmented Long-Horizon Planning (IALP) 系统，利用大型语言模型 (LLMs) 结合实时传感器反馈和 grounding mechanisms 来生成可行且最优的动作。IALP 通过将用户指令增强为 PDDL 问题，融合 LLMs 的抽象推理能力与环境知识 grounding，实现了闭环交互和量化环境理解。实验结果显示，该系统在涉及七种不同操作技能的真实世界任务中，平均成功率超过 80%，显著提升了机器人在非结构化环境中的自主性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "17 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08084v1",
      "published_date": "2025-03-11 06:37:33 UTC",
      "updated_date": "2025-03-11 06:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:59:24.284555"
    },
    {
      "arxiv_id": "2503.08737v1",
      "title": "Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "In Cho",
        "Youngbeom Yoo",
        "Subin Jeon",
        "Seon Joo Kim"
      ],
      "abstract": "Constructing a compressed latent space through a variational autoencoder\n(VAE) is the key for efficient 3D diffusion models. This paper introduces\nCOD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors\nwithout sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme\nto improve compression and decoding efficiency. First, our encoder block\nprogressively compresses point clouds into compact latent vectors via\nintermediate point patches. Second, our triplane-based decoder reconstructs\ndense triplanes from latent vectors instead of directly decoding neural fields,\nsignificantly reducing computational overhead of neural fields decoding.\nFinally, we propose uncertainty-guided token pruning, which allocates resources\nadaptively by skipping computations in simpler regions and improves the decoder\nefficiency. Experimental results demonstrate that COD-VAE achieves 16x\ncompression compared to the baseline while maintaining quality. This enables\n20.8x speedup in generation, highlighting that a large number of latent vectors\nis not a prerequisite for high-quality reconstruction and generation.",
      "tldr_zh": "本文提出 COD-VAE，一种高效的变分自动编码器(VAE)，用于将 3D 形状编码成 64 个紧凑的 1D 潜在向量，从而提升 3D 扩散模型的性能。COD-VAE 采用两阶段方案：首先，编码器通过中间点补丁(intermediate point patches)逐步压缩点 clouds；其次，triplane-based 解码器从潜在向量重建密集 triplanes，并结合 uncertainty-guided token pruning 适应性分配资源，减少计算开销。实验结果表明，COD-VAE 相较基线实现了 16 倍压缩和 20.8 倍生成速度提升，同时保持高质量重建，证明大量潜在向量并非高精度生成的前提。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08737v1",
      "published_date": "2025-03-11 06:29:39 UTC",
      "updated_date": "2025-03-11 06:29:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:59:38.201016"
    },
    {
      "arxiv_id": "2503.08083v1",
      "title": "Degradation Self-Supervised Learning for Lithium-ion Battery Health Diagnostics",
      "title_zh": "退化自监督学习用于锂离子电池健康诊断",
      "authors": [
        "J. C. Chen"
      ],
      "abstract": "Health evaluation for lithium-ion batteries (LIBs) typically relies on\nconstant charging/discharging protocols, often neglecting scenarios involving\ndynamic current profiles prevalent in electric vehicles. Conventional health\nindicators for LIBs also depend on the uniformity of measured data, restricting\ntheir adaptability to non-uniform conditions. In this study, a novel training\nstrategy for estimating LIB health based on the paradigm of self-supervised\nlearning is proposed. A multiresolution analysis technique, empirical wavelet\ntransform, is utilized to decompose non-stationary voltage signals in the\nfrequency domain. This allows the removal of ineffective components for the\nhealth evaluation model. The transformer neural network serves as the model\nbackbone, and a loss function is designed to describe the capacity degradation\nbehavior with the assumption that the degradation in LIBs across most operating\nconditions is inevitable and irreversible. The results show that the model can\nlearn the aging characteristics by analyzing sequences of voltage and current\nprofiles obtained at various time intervals from the same LIB cell. The\nproposed method is successfully applied to the Stanford University LIB aging\ndataset, derived from electric vehicle real driving profiles. Notably, this\napproach achieves an average correlation coefficient of 0.9 between the\nevaluated health index and the degradation of actual capacity, demonstrating\nits efficacy in capturing LIB health degradation. This research highlights the\nfeasibility of training deep neural networks using unlabeled LIB data, offering\ncost-efficient means and unleashing the potential of the measured information.",
      "tldr_zh": "这篇论文提出了一种基于自监督学习（self-supervised learning）的训练策略，用于锂离子电池（LIBs）的健康诊断，解决了传统方法在动态电流场景和非均匀数据下的适应性问题。方法利用经验小波变换（empirical wavelet transform）对非平稳电压信号进行多分辨率分解，移除无效组件，并以Transformer神经网络作为主干，设计损失函数来描述容量退化行为。实验结果显示，该模型在斯坦福大学LIB老化数据集上实现了与实际容量退化的平均相关系数为0.9，证明了其在处理无标签数据时的有效性和成本效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08083v1",
      "published_date": "2025-03-11 06:29:13 UTC",
      "updated_date": "2025-03-11 06:29:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T00:59:50.929349"
    },
    {
      "arxiv_id": "2503.08065v1",
      "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Wenzhe",
        "Tang Haina",
        "Zhang Xudong"
      ],
      "abstract": "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach.",
      "tldr_zh": "该研究针对船舶轨迹预测中的不确定性和行为多模态问题，提出STGDPM方法，通过将交互建模为动态图取代传统基于船舶状态的聚合技术，从而更好地捕捉多模态行为。STGDPM利用diffusion models的自然多模态能力，将轨迹预测任务视为运动不确定性的逆过程，逐步消除潜在导航区域的不确定性以生成精确轨迹。实验在真实Automatic Identification System (AIS)数据上验证，该方法显著优于现有方法，为海上交通安全提供了更可靠的预测框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08065v1",
      "published_date": "2025-03-11 05:50:27 UTC",
      "updated_date": "2025-03-11 05:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:00:03.192087"
    },
    {
      "arxiv_id": "2503.08064v1",
      "title": "Continual Learning for Multiple Modalities",
      "title_zh": "翻译失败",
      "authors": [
        "Hyundong Jin",
        "Eunwoo Kim"
      ],
      "abstract": "Continual learning aims to learn knowledge of tasks observed in sequential\ntime steps while mitigating the forgetting of previously learned knowledge.\nExisting methods were proposed under the assumption of learning a single\nmodality (e.g., image) over time, which limits their applicability in scenarios\ninvolving multiple modalities. In this work, we propose a novel continual\nlearning framework that accommodates multiple modalities (image, video, audio,\ndepth, and text). We train a model to align various modalities with text,\nleveraging its rich semantic information. However, this increases the risk of\nforgetting previously learned knowledge, exacerbated by the differing input\ntraits of each task. To alleviate the overwriting of the previous knowledge of\nmodalities, we propose a method for aggregating knowledge within and across\nmodalities. The aggregated knowledge is obtained by assimilating new\ninformation through self-regularization within each modality and associating\nknowledge between modalities by prioritizing contributions from relevant\nmodalities. Furthermore, we propose a strategy that re-aligns the embeddings of\nmodalities to resolve biased alignment between modalities. We evaluate the\nproposed method in a wide range of continual learning scenarios using multiple\ndatasets with different modalities. Extensive experiments demonstrate that ours\noutperforms existing methods in the scenarios, regardless of whether the\nidentity of the modality is given.",
      "tldr_zh": "这篇论文提出了一种新型的 Continual Learning 框架，支持多种模态（如图像、视频、音频、深度和文本），以解决现有方法在处理顺序任务时遗忘问题的局限性。框架通过将各种模态与文本对齐，利用文本的丰富语义信息，并引入 intra-modality 自正则化和 inter-modality 知识关联的方法，来聚合新旧知识并减少模态间的偏差对齐问题。实验在多个数据集上验证了该方法的有效性，其性能在各种持续学习场景中优于现有方法，无论模态身份是否已知。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08064v1",
      "published_date": "2025-03-11 05:50:13 UTC",
      "updated_date": "2025-03-11 05:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:00:15.332211"
    },
    {
      "arxiv_id": "2503.08051v1",
      "title": "Counterfactual Language Reasoning for Explainable Recommendation Systems",
      "title_zh": "反事实语言推理用于可解释推荐系统",
      "authors": [
        "Guanrong Li",
        "Haolin Yang",
        "Xinyu Liu",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "abstract": "Explainable recommendation systems leverage transparent reasoning to foster\nuser trust and improve decision-making processes. Current approaches typically\ndecouple recommendation generation from explanation creation, violating causal\nprecedence principles where explanatory factors should logically precede\noutcomes. This paper introduces a novel framework integrating structural causal\nmodels with large language models to establish causal consistency in\nrecommendation pipelines. Our methodology enforces explanation factors as\ncausal antecedents to recommendation predictions through causal graph\nconstruction and counterfactual adjustment. We particularly address the\nconfounding effect of item popularity that distorts personalization signals in\nexplanations, developing a debiasing mechanism that disentangles genuine user\npreferences from conformity bias. Through comprehensive experiments across\nmultiple recommendation scenarios, we demonstrate that CausalX achieves\nsuperior performance in recommendation accuracy, explanation plausibility, and\nbias mitigation compared to baselines.",
      "tldr_zh": "这篇论文提出了一种名为CausalX的框架，将structural causal models与large language models整合到解释性推荐系统中，以解决当前方法中因果优先原则的缺失问题。该框架通过因果图构建和counterfactual adjustment，确保解释因素作为推荐预测的因果前因，并引入debiasing mechanism来消除项目流行度的confounding effect，从而分离真正的用户偏好和从众偏见。在多个推荐场景的实验中，CausalX在推荐准确性、解释合理性和偏见缓解方面均优于基线模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08051v1",
      "published_date": "2025-03-11 05:15:37 UTC",
      "updated_date": "2025-03-11 05:15:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:00:27.189097"
    },
    {
      "arxiv_id": "2503.08038v1",
      "title": "Generalized Kullback-Leibler Divergence Loss",
      "title_zh": "广义 Kullback-Leibler 散度损失",
      "authors": [
        "Jiequan Cui",
        "Beier Zhu",
        "Qingshan Xu",
        "Zhuotao Tian",
        "Xiaojuan Qi",
        "Bei Yu",
        "Hanwang Zhang",
        "Richang Hong"
      ],
      "abstract": "In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss\nand mathematically prove that it is equivalent to the Decoupled\nKullback-Leibler (DKL) Divergence loss that consists of (1) a weighted Mean\nSquare Error (wMSE) loss and (2) a Cross-Entropy loss incorporating soft\nlabels. Thanks to the decoupled structure of DKL loss, we have identified two\nareas for improvement. Firstly, we address the limitation of KL loss in\nscenarios like knowledge distillation by breaking its asymmetric optimization\nproperty along with a smoother weight function. This modification effectively\nalleviates convergence challenges in optimization, particularly for classes\nwith high predicted scores in soft labels. Secondly, we introduce class-wise\nglobal information into KL/DKL to reduce bias arising from individual samples.\nWith these two enhancements, we derive the Generalized Kullback-Leibler (GKL)\nDivergence loss and evaluate its effectiveness by conducting experiments on\nCIFAR-10/100, ImageNet, and vision-language datasets, focusing on adversarial\ntraining, and knowledge distillation tasks. Specifically, we achieve new\nstate-of-the-art adversarial robustness on the public leaderboard --\nRobustBench and competitive knowledge distillation performance across\nCIFAR/ImageNet models and CLIP models, demonstrating the substantial practical\nmerits. Our code is available at https://github.com/jiequancui/DKL.",
      "tldr_zh": "本论文证明了Kullback-Leibler (KL) Divergence loss等价于Decoupled Kullback-Leibler (DKL) Divergence loss，由weighted Mean Square Error (wMSE) loss和Cross-Entropy loss组成，并通过其解耦结构识别了两个改进方向：一是解决KL loss在知识蒸馏中的不对称优化问题，使用更平滑的权重函数缓解收敛挑战；二是引入类级全局信息减少样本偏差。基于这些增强，论文提出了Generalized Kullback-Leibler (GKL) Divergence loss，并在CIFAR-10/100、ImageNet和视觉语言数据集上进行实验，特别是在adversarial training和knowledge distillation任务中，实现了RobustBench排行榜上的新最先进对抗鲁棒性，并展现出竞争力的知识蒸馏性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "extension of our NeurIPS paper \"Decoupled Kullback-Leibler Divergence\n  Loss\". arXiv admin note: substantial text overlap with arXiv:2305.13948",
      "pdf_url": "http://arxiv.org/pdf/2503.08038v1",
      "published_date": "2025-03-11 04:43:33 UTC",
      "updated_date": "2025-03-11 04:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:00:38.841819"
    },
    {
      "arxiv_id": "2503.08037v1",
      "title": "ObjectMover: Generative Object Movement with Video Prior",
      "title_zh": "ObjectMover：利用视频先验的生成式对象移动",
      "authors": [
        "Xin Yu",
        "Tianyu Wang",
        "Soo Ye Kim",
        "Paul Guerrero",
        "Xi Chen",
        "Qing Liu",
        "Zhe Lin",
        "Xiaojuan Qi"
      ],
      "abstract": "Simple as it seems, moving an object to another location within an image is,\nin fact, a challenging image-editing task that requires re-harmonizing the\nlighting, adjusting the pose based on perspective, accurately filling occluded\nregions, and ensuring coherent synchronization of shadows and reflections while\nmaintaining the object identity. In this paper, we present ObjectMover, a\ngenerative model that can perform object movement in highly challenging scenes.\nOur key insight is that we model this task as a sequence-to-sequence problem\nand fine-tune a video generation model to leverage its knowledge of consistent\nobject generation across video frames. We show that with this approach, our\nmodel is able to adjust to complex real-world scenarios, handling extreme\nlighting harmonization and object effect movement. As large-scale data for\nobject movement are unavailable, we construct a data generation pipeline using\na modern game engine to synthesize high-quality data pairs. We further propose\na multi-task learning strategy that enables training on real-world video data\nto improve the model generalization. Through extensive experiments, we\ndemonstrate that ObjectMover achieves outstanding results and adapts well to\nreal-world scenarios.",
      "tldr_zh": "该论文提出ObjectMover，一种生成模型，用于在图像中移动物体，同时处理光线协调、姿态调整、遮挡填充以及阴影反射同步等挑战。其关键方法是将物体移动任务建模为序列到序列(sequence-to-sequence)问题，并通过微调视频生成模型利用视频先验(Video Prior)来确保物体的一致性；此外，作者构建了一个基于现代游戏引擎的数据生成管道，并采用多任务学习策略训练模型，以提升其在真实世界场景中的泛化能力。实验结果显示，ObjectMover在复杂环境中表现出色，能够有效适应极端光线和物体效果移动的需求。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "CVPR 2025, Project Page: https://xinyu-andy.github.io/ObjMover",
      "pdf_url": "http://arxiv.org/pdf/2503.08037v1",
      "published_date": "2025-03-11 04:42:59 UTC",
      "updated_date": "2025-03-11 04:42:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:00:49.523978"
    },
    {
      "arxiv_id": "2503.08032v1",
      "title": "HOFAR: High-Order Augmentation of Flow Autoregressive Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Yingyu Liang",
        "Zhizhou Sha",
        "Zhenmei Shi",
        "Zhao Song",
        "Mingda Wan"
      ],
      "abstract": "Flow Matching and Transformer architectures have demonstrated remarkable\nperformance in image generation tasks, with recent work FlowAR [Ren et al.,\n2024] synergistically integrating both paradigms to advance synthesis fidelity.\nHowever, current FlowAR implementations remain constrained by first-order\ntrajectory modeling during the generation process. This paper introduces a\nnovel framework that systematically enhances flow autoregressive transformers\nthrough high-order supervision. We provide theoretical analysis and empirical\nevaluation showing that our High-Order FlowAR (HOFAR) demonstrates measurable\nimprovements in generation quality compared to baseline models. The proposed\napproach advances the understanding of flow-based autoregressive modeling by\nintroducing a systematic framework for analyzing trajectory dynamics through\nhigh-order expansion.",
      "tldr_zh": "本文提出 HOFAR（High-Order Augmentation of Flow Autoregressive Transformers），一种通过高阶监督（high-order supervision）增强流自回归变压器（Flow Autoregressive Transformers）的框架，以克服现有 FlowAR 模型在图像生成任务中受限于第一阶轨迹建模的局限性。该方法结合理论分析和实证评估，展示了 HOFAR 在生成质量上比基线模型有显著改善。总体上，HOFAR 通过高阶展开（high-order expansion）系统分析轨迹动态，推进了对 flow-based autoregressive 建模的理解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08032v1",
      "published_date": "2025-03-11 04:29:22 UTC",
      "updated_date": "2025-03-11 04:29:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:01:01.847075"
    },
    {
      "arxiv_id": "2503.08734v1",
      "title": "Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Aniket Vaidya",
        "Anurag Awasthi"
      ],
      "abstract": "In today's increasingly digital interactions, robust Identity Verification\n(IDV) is crucial for security and trust. Artificial Intelligence (AI) is\ntransforming IDV, enhancing accuracy and fraud detection. This paper introduces\n``Zero to One,'' a holistic conceptual framework for developing AI-powered IDV\nproducts. This paper outlines the foundational problem and research objectives\nthat necessitate a new framework for IDV in the age of AI. It details the\nevolution of identity verification and the current regulatory landscape to\ncontextualize the need for a robust conceptual model. The core of the paper is\nthe presentation of the ``Zero to One'' framework itself, dissecting its four\nessential components: Document Verification, Biometric Verification, Risk\nAssessment, and Orchestration. The paper concludes by discussing the\nimplications of this conceptual model and suggesting future research directions\nfocused on the framework's further development and application. The framework\naddresses security, privacy, UX, and regulatory compliance, offering a\nstructured approach to building effective IDV solutions. Successful IDV\nplatforms require a balanced conceptual understanding of verification methods,\nrisk management, and operational scalability, with AI as a key enabler. This\npaper presents the ``Zero to One'' framework as a refined conceptual model,\ndetailing verification layers, and AI's transformative role in shaping\nnext-generation IDV products.",
      "tldr_zh": "本论文提出“Zero to One”框架，这是一个整体概念模型，用于开发AI驱动的身份验证（IDV）系统，以提升数字互动中的安全性和欺诈检测能力。该框架包括四个核心组件：Document Verification、Biometric Verification、Risk Assessment 和 Orchestration，针对IDV的演变、监管环境提供结构化方法，同时强调安全、隐私、用户体验（UX）和合规性。通过整合AI作为关键使能器，该模型为构建可扩展的IDV解决方案提供指导，并建议未来研究方向以进一步优化其应用。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08734v1",
      "published_date": "2025-03-11 04:20:02 UTC",
      "updated_date": "2025-03-11 04:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:01:14.549318"
    },
    {
      "arxiv_id": "2503.08026v1",
      "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Tan",
        "Jun Yan",
        "I-Hung Hsu",
        "Rujun Han",
        "Zifeng Wang",
        "Long T. Le",
        "Yiwen Song",
        "Yanfei Chen",
        "Hamid Palangi",
        "George Lee",
        "Anand Iyer",
        "Tianlong Chen",
        "Huan Liu",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset.",
      "tldr_zh": "该论文针对大型语言模型(LLMs)在长期对话中无法有效保留和检索个性化信息的挑战，提出了一种新型机制Reflective Memory Management (RMM)。RMM通过Prospective Reflection动态总结互动（从话语、回合到会话级别）构建个性化记忆库，以及Retrospective Reflection基于LLMs的引用证据进行在线强化学习(RL)迭代优化检索，从而适应多样化的对话情境。实验结果显示，RMM在LongMemEval数据集上比基线模型准确率提升超过10%，并在多种指标和基准上表现出显著改善。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08026v1",
      "published_date": "2025-03-11 04:15:52 UTC",
      "updated_date": "2025-03-11 04:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:01:25.714999"
    },
    {
      "arxiv_id": "2503.08732v1",
      "title": "Quantifying Circadian Desynchrony in ICU Patients and Its Association with Delirium",
      "title_zh": "量化 ICU 患者中的昼夜节律脱同步及其与谵妄的关联",
      "authors": [
        "Yuanfang Ren",
        "Andrea E. Davidson",
        "Jiaqing Zhang",
        "Miguel Contreras",
        "Ayush K. Patel",
        "Michelle Gumz",
        "Tezcan Ozrazgat-Baslanti",
        "Parisa Rashidi",
        "Azra Bihorac"
      ],
      "abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
      "tldr_zh": "本文研究了 ICU 患者中昼夜节律失调（circadian desynchrony）的量化及其与谵妄（delirium）的关联，旨在填补现有工具和关联研究的空白。研究方法包括对 86 名 ICU 患者进行前瞻性观察，采集血单核细胞昼夜转录组数据，并使用公开数据集复制的模型来比较内部昼夜时间与外部采血时间，从而计算昼夜节律失调指数。结果显示，ICU 患者的指数显著高于健康受试者（10.03 小时 vs 2.50-2.95 小时，p < 0.001），且指数与谵妄发生相关，下午 3 点后采血的患者指数较低（5.00 小时 vs 其他组 10.01-10.90 小时，p < 0.001）。这为临床评估和干预昼夜节律失调提供了新工具。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08732v1",
      "published_date": "2025-03-11 03:56:10 UTC",
      "updated_date": "2025-03-11 03:56:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:01:39.934731"
    },
    {
      "arxiv_id": "2503.08012v1",
      "title": "Exploring Bias in over 100 Text-to-Image Generative Models",
      "title_zh": "探索超过 100 个文本到图像生成模型中的偏见",
      "authors": [
        "Jordan Vice",
        "Naveed Akhtar",
        "Richard Hartley",
        "Ajmal Mian"
      ],
      "abstract": "We investigate bias trends in text-to-image generative models over time,\nfocusing on the increasing availability of models through open platforms like\nHugging Face. While these platforms democratize AI, they also facilitate the\nspread of inherently biased models, often shaped by task-specific fine-tuning.\nEnsuring ethical and transparent AI deployment requires robust evaluation\nframeworks and quantifiable bias metrics. To this end, we assess bias across\nthree key dimensions: (i) distribution bias, (ii) generative hallucination, and\n(iii) generative miss-rate. Analyzing over 100 models, we reveal how bias\npatterns evolve over time and across generative tasks. Our findings indicate\nthat artistic and style-transferred models exhibit significant bias, whereas\nfoundation models, benefiting from broader training distributions, are becoming\nprogressively less biased. By identifying these systemic trends, we contribute\na large-scale evaluation corpus to inform bias research and mitigation\nstrategies, fostering more responsible AI development.\n  Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-Source\nModels",
      "tldr_zh": "这篇论文探讨了超过 100 个文本-to-image 生成模型中的偏见趋势，聚焦于模型在开放平台（如 Hugging Face）上的普及如何加剧固有偏见问题。研究者评估了偏见在三个关键维度上的表现：distribution bias、generative hallucination 和 generative miss-rate，并分析了这些模式随时间和任务的演变。结果表明，艺术和风格转移模型显示出显著偏见，而基础模型由于更广泛的训练分布，偏见逐渐减少。该研究贡献了一个大规模评估语料库，用于支持偏见研究、缓解策略和更负责任的 AI 开发。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025 Workshop on Open Science for Foundation Models\n  (SCI-FM)",
      "pdf_url": "http://arxiv.org/pdf/2503.08012v1",
      "published_date": "2025-03-11 03:40:44 UTC",
      "updated_date": "2025-03-11 03:40:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:01:54.519292"
    },
    {
      "arxiv_id": "2503.08010v1",
      "title": "SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Yi Lu",
        "Md Mehrab Tanjim",
        "Ishita Dasgupta",
        "Somdeb Sarkhel",
        "Gang Wu",
        "Saayan Mitra",
        "Somali Chaterji"
      ],
      "abstract": "We present SKALD, a multi-shot video assembly method that constructs coherent\nvideo sequences from candidate shots with minimal reliance on text. Central to\nour approach is the Learned Clip Assembly (LCA) score, a learning-based metric\nthat measures temporal and semantic relationships between shots to quantify\nnarrative coherence. We tackle the exponential complexity of combining multiple\nshots with an efficient beam-search algorithm guided by the LCA score. To train\nour model effectively with limited human annotations, we propose two tasks for\nthe LCA encoder: Shot Coherence Learning, which uses contrastive learning to\ndistinguish coherent and incoherent sequences, and Feature Regression, which\nconverts these learned representations into a real-valued coherence score. We\ndevelop two variants: a base SKALD model that relies solely on visual coherence\nand SKALD-text, which integrates auxiliary text information when available.\nExperiments on the VSPD and our curated MSV3C datasets show that SKALD achieves\nan improvement of up to 48.6% in IoU and a 43% speedup over the\nstate-of-the-art methods. A user study further validates our approach, with 45%\nof participants favoring SKALD-assembled videos, compared to 22% preferring\ntext-based assembly methods.",
      "tldr_zh": "本研究提出SKALD，一种基于学习的镜头组装方法，用于从候选镜头构建连贯的多镜头视频序列，同时最小化对文本的依赖。核心是Learned Clip Assembly (LCA)得分，这是一个学习指标，通过对比学习（Shot Coherence Learning）和特征回归（Feature Regression）任务来量化镜头间的时序和语义关系，并结合高效的beam-search算法处理组合复杂度。研究开发了基础SKALD（依赖视觉连贯性）和SKALD-text（整合文本信息）两个变体，在VSPD和MSV3C数据集上的实验显示，SKALD比最先进方法提高了48.6%的IoU并加速43%，用户研究进一步证实其优越性，45%的参与者更偏好SKALD组装的视频。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08010v1",
      "published_date": "2025-03-11 03:25:44 UTC",
      "updated_date": "2025-03-11 03:25:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:02:02.826349"
    },
    {
      "arxiv_id": "2503.08007v1",
      "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
      "title_zh": "翻译失败",
      "authors": [
        "Han Zhao",
        "Wenxuan Song",
        "Donglin Wang",
        "Xinyang Tong",
        "Pengxiang Ding",
        "Xuelian Cheng",
        "Zongyuan Ge"
      ],
      "abstract": "Developing versatile quadruped robots that can smoothly perform various\nactions and tasks in real-world environments remains a significant challenge.\nThis paper introduces a novel vision-language-action (VLA) model, mixture of\nrobotic experts (MoRE), for quadruped robots that aim to introduce\nreinforcement learning (RL) for fine-tuning large-scale VLA models with a large\namount of mixed-quality data. MoRE integrates multiple low-rank adaptation\nmodules as distinct experts within a dense multi-modal large language model\n(MLLM), forming a sparse-activated mixture-of-experts model. This design\nenables the model to effectively adapt to a wide array of downstream tasks.\nMoreover, we employ a reinforcement learning-based training objective to train\nour model as a Q-function after deeply exploring the structural properties of\nour tasks. Effective learning from automatically collected mixed-quality data\nenhances data efficiency and model performance. Extensive experiments\ndemonstrate that MoRE outperforms all baselines across six different skills and\nexhibits superior generalization capabilities in out-of-distribution scenarios.\nWe further validate our method in real-world scenarios, confirming the\npracticality of our approach and laying a solid foundation for future research\non multi-task learning in quadruped robots.",
      "tldr_zh": "该论文提出 MoRE，一种用于四足机器人的视觉-语言-动作 (VLA) 模型，通过强化学习 (RL) 微调大规模模型，以处理大量混合质量数据并提升可扩展性。MoRE 整合多个低秩适配模块作为不同专家，形成稀疏激活的混合专家模型，并将其训练为 Q 函数，以有效适应多种下游任务并提高数据效率。实验结果显示，MoRE 在六种技能上优于所有基线模型，并在分布外场景中展现出色的泛化能力，并在真实世界环境中验证了其实用性，为四足机器人的多任务学习奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08007v1",
      "published_date": "2025-03-11 03:13:45 UTC",
      "updated_date": "2025-03-11 03:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:02:16.155498"
    },
    {
      "arxiv_id": "2503.08006v1",
      "title": "Injecting Imbalance Sensitivity for Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Zhou",
        "Liu Liu",
        "Peilin Zhao",
        "Wei Gong"
      ],
      "abstract": "Multi-task learning (MTL) has emerged as a promising approach for deploying\ndeep learning models in real-life applications. Recent studies have proposed\noptimization-based learning paradigms to establish task-shared representations\nin MTL. However, our paper empirically argues that these studies, specifically\ngradient-based ones, primarily emphasize the conflict issue while neglecting\nthe potentially more significant impact of imbalance/dominance in MTL. In line\nwith this perspective, we enhance the existing baseline method by injecting\nimbalance-sensitivity through the imposition of constraints on the projected\nnorms. To demonstrate the effectiveness of our proposed IMbalance-sensitive\nGradient (IMGrad) descent method, we evaluate it on multiple mainstream MTL\nbenchmarks, encompassing supervised learning tasks as well as reinforcement\nlearning. The experimental results consistently demonstrate competitive\nperformance.",
      "tldr_zh": "这篇论文指出，现有的多任务学习(MTL)方法主要关注任务冲突问题，而忽略了不平衡/主导现象对模型的影响。作者提出了一种增强基线方法的IMGrad下降算法，通过在投影范数上施加约束来注入不平衡敏感性，从而更好地优化任务共享表示。实验结果显示，该方法在多个主流MTL基准上，包括监督学习和强化学习任务，取得了竞争性的性能表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08006v1",
      "published_date": "2025-03-11 03:11:54 UTC",
      "updated_date": "2025-03-11 03:11:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:02:26.480296"
    },
    {
      "arxiv_id": "2503.07996v4",
      "title": "SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic",
      "title_zh": "SQLCritic：通过子句级审评修正文本到SQL生成",
      "authors": [
        "Jikai Chen",
        "Leilei Gan",
        "Ziyu Zhao",
        "Zechuan Wang",
        "Dong Wang",
        "Chenyi Zhuang"
      ],
      "abstract": "Existing refinement methods in LLM-based Text-to-SQL systems exhibit limited\neffectiveness. They often introduce new errors during the self-correction\nprocess and fail to detect and correct semantic inaccuracies. To address these\ngaps, we first introduce a clause-wise critique generation task along with a\nbenchmark, SQLCriticBench, which performs fine-grained error localization\nincluding both syntax and semantic errors at the clause level. Furthermore, we\nintroduce a variant of DPO for training our SQLCritic model, where the $\\beta$\ncoefficient is adaptively changed according to the clause-level inconsistencies\nbetween the preferred and dispreferred critiques. We also propose an\nautomatically training dataset curation pipeline which annotate clause-wise\ncritique at scale in a cost-effective way. Experiments demonstrate that the\nSQLCritic model significantly improves SQL accuracy on the BIRD and Spider\ndatasets, and the results on SQLCriticBench further reveals its superior\ncritique capabilities compared to existing models.",
      "tldr_zh": "本文提出SQLCritic，一种通过子句级批评（clause-wise critic）来纠正LLM-based Text-to-SQL生成的模型，以解决现有方法易引入新错误且无法有效检测语义不准确的问题。研究引入了子句级批评生成任务和基准SQLCriticBench，用于细粒度定位语法和语义错误，并开发了DPO变体训练方法，其中β系数根据preferred和dispreferred critiques的子句级不一致性自适应调整，同时提出一个自动数据集整理管道以成本有效方式大规模标注批评。实验结果显示，SQLCritic显著提高了BIRD和Spider数据集上的SQL准确率，并在SQLCriticBench上展现出比现有模型更强的批评能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07996v4",
      "published_date": "2025-03-11 02:52:39 UTC",
      "updated_date": "2025-05-21 01:48:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:02:38.317951"
    },
    {
      "arxiv_id": "2503.07993v1",
      "title": "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
      "title_zh": "LLM驱动的知识图谱用于企业智能和分析",
      "authors": [
        "Rajeev Kumar",
        "Kumar Ishan",
        "Harishankar Kumar",
        "Abhinandan Singla"
      ],
      "abstract": "Disconnected data silos within enterprises obstruct the extraction of\nactionable insights, diminishing efficiency in areas such as product\ndevelopment, client engagement, meeting preparation, and analytics-driven\ndecision-making. This paper introduces a framework that uses large language\nmodels (LLMs) to unify various data sources into a comprehensive,\nactivity-centric knowledge graph. The framework automates tasks such as entity\nextraction, relationship inference, and semantic enrichment, enabling advanced\nquerying, reasoning, and analytics across data types like emails, calendars,\nchats, documents, and logs. Designed for enterprise flexibility, it supports\napplications such as contextual search, task prioritization, expertise\ndiscovery, personalized recommendations, and advanced analytics to identify\ntrends and actionable insights. Experimental results demonstrate its success in\nthe discovery of expertise, task management, and data-driven decision making.\nBy integrating LLMs with knowledge graphs, this solution bridges disconnected\nsystems and delivers intelligent analytics-powered enterprise tools.",
      "tldr_zh": "该论文针对企业中数据孤岛问题，提出一个基于大型语言模型（LLMs）的框架，将各种数据源（如电子邮件、日历和文档）整合成一个全面的、以活动为中心的知识图谱。框架自动化实体提取、关系推断和语义增强任务，支持高级查询、推理和分析，以提升产品开发、客户互动和决策效率。应用场景包括上下文搜索、任务优先级设置、专业知识发现、个性化推荐以及识别趋势的先进分析。实验结果证明，该框架在专业知识发现、任务管理和数据驱动决策方面取得成功，通过LLMs与知识图谱的整合，桥接了企业系统的断开，实现智能分析驱动的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07993v1",
      "published_date": "2025-03-11 02:50:45 UTC",
      "updated_date": "2025-03-11 02:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:02:50.824252"
    },
    {
      "arxiv_id": "2503.07994v1",
      "title": "A Neural Symbolic Model for Space Physics",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Ying",
        "Haowei Lin",
        "Chao Yue",
        "Yajie Chen",
        "Chao Xiao",
        "Quanqi Shi",
        "Yitao Liang",
        "Shing-Tung Yau",
        "Yuan Zhou",
        "Jianzhu Ma"
      ],
      "abstract": "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
      "tldr_zh": "本研究提出了一种名为 PhyE2E 的神经符号模型，用于通过 symbolic regression 发现物理公式。该模型将符号回归分解为子问题，利用神经网络的二阶导数和 Transformer 模型实现端到端的数据到公式的转换，并通过 Monte-Carlo Tree Search 和 Genetic Programming 进行优化。实验结果显示，PhyE2E 在符号准确性、数据拟合和物理单位一致性方面优于现有方法，并在空间物理的五个应用中表现出色，如准确预测太阳黑子数并升级 NASA 1993 年的太阳活动公式，提供显式解释。",
      "categories": [
        "astro-ph.SR",
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.AI",
        "physics.space-ph"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07994v1",
      "published_date": "2025-03-11 02:50:45 UTC",
      "updated_date": "2025-03-11 02:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:03:03.588519"
    },
    {
      "arxiv_id": "2503.07992v1",
      "title": "Efficient and Accurate Estimation of Lipschitz Constants for Hybrid Quantum-Classical Decision Models",
      "title_zh": "针对混合量子-经典决策模型的高效且准确的 Lipschitz 常数估计",
      "authors": [
        "Sajjad Hashemian",
        "Mohammad Saeed Arvenaghi"
      ],
      "abstract": "In this paper, we propose a novel framework for efficiently and accurately\nestimating Lipschitz constants in hybrid quantum-classical decision models. Our\napproach integrates classical neural network with quantum variational circuits\nto address critical issues in learning theory such as fairness verification,\nrobust training, and generalization.\n  By a unified convex optimization formulation, we extend existing classical\nmethods to capture the interplay between classical and quantum layers. This\nintegrated strategy not only provide a tight bound on the Lipschitz constant\nbut also improves computational efficiency with respect to the previous\nmethods.",
      "tldr_zh": "本论文提出了一种新框架，用于高效准确地估计混合量子-经典决策模型中的Lipschitz constants。该框架将classical neural network与quantum variational circuits整合，通过统一的convex optimization公式扩展现有经典方法，以捕捉经典和量子层之间的相互作用。这种方法不仅提供紧致的Lipschitz常量界限，还提高了计算效率，适用于学习理论中的公平性验证、鲁棒训练和泛化问题。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "14 pages, 5 figuers, Submitted to TASE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.07992v1",
      "published_date": "2025-03-11 02:50:16 UTC",
      "updated_date": "2025-03-11 02:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:03:15.416452"
    },
    {
      "arxiv_id": "2503.07991v1",
      "title": "Boundary Prompting: Elastic Urban Region Representation via Graph-based Spatial Tokenization",
      "title_zh": "边界提示：通过基于图的空间标记化实现弹性的城市区域表示",
      "authors": [
        "Haojia Zhu",
        "Jiahui Jin",
        "Dong Kan",
        "Rouxi Shen",
        "Ruize Wang",
        "Xiangguo Sun",
        "Jinghui Zhang"
      ],
      "abstract": "Urban region representation is essential for various applications such as\nurban planning, resource allocation, and policy development. Traditional\nmethods rely on fixed, predefined region boundaries, which fail to capture the\ndynamic and complex nature of real-world urban areas. In this paper, we propose\nthe Boundary Prompting Urban Region Representation Framework (BPURF), a novel\napproach that allows for elastic urban region definitions. BPURF comprises two\nkey components: (1) A spatial token dictionary, where urban entities are\ntreated as tokens and integrated into a unified token graph, and (2) a region\ntoken set representation model which utilize token aggregation and a\nmulti-channel model to embed token sets corresponding to region boundaries.\nAdditionally, we propose fast token set extraction strategy to enable online\ntoken set extraction during training and prompting. This framework enables the\ndefinition of urban regions through boundary prompting, supporting varying\nregion boundaries and adapting to different tasks. Extensive experiments\ndemonstrate the effectiveness of BPURF in capturing the complex characteristics\nof urban regions.",
      "tldr_zh": "本研究针对传统城市区域表示方法依赖固定边界、无法捕捉动态复杂性的问题，提出Boundary Prompting Urban Region Representation Framework (BPURF)。该框架的核心组件包括空间 token 字典，将城市实体视为 tokens 并整合成统一的 token graph，以及区域 token set 表示模型，通过 token aggregation 和多通道模型嵌入对应区域边界的 token set。此外，BPURF 引入快速 token set 提取策略，支持在线提取和边界提示，从而实现弹性区域定义并适应不同任务。实验结果显示，该框架在捕捉城市区域复杂特征方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07991v1",
      "published_date": "2025-03-11 02:49:58 UTC",
      "updated_date": "2025-03-11 02:49:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:03:29.447089"
    },
    {
      "arxiv_id": "2503.07988v1",
      "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
      "title_zh": "可证明的零样本泛化在离线强化学习中",
      "authors": [
        "Zhiyong Wang",
        "Chen Yang",
        "John C. S. Lui",
        "Dongruo Zhou"
      ],
      "abstract": "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
      "tldr_zh": "本研究探讨了离线强化学习（offline RL）中的零样本泛化（zero-shot generalization），即代理使用来自不同环境的离线数据集训练策略，并在新测试环境中无需额外交互即可实现良好表现。论文提出两种方法：pessimistic empirical risk minimization (PERM) 和 pessimistic proximal policy optimization (PPPO)，这些方法通过悲观策略评估（pessimistic policy evaluation）来指导策略学习，从而提升泛化能力。实验结果表明，PERM 和 PPPO 能够找到近优策略，为理解离线 RL 中泛化现象的理论基础提供了首个重要进展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 1 figure, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.07988v1",
      "published_date": "2025-03-11 02:44:32 UTC",
      "updated_date": "2025-03-11 02:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:03:40.440072"
    },
    {
      "arxiv_id": "2503.10677v2",
      "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyue Cheng",
        "Yucong Luo",
        "Jie Ouyang",
        "Qi Liu",
        "Huijie Liu",
        "Li Li",
        "Shuo Yu",
        "Bohou Zhang",
        "Jiawei Cao",
        "Jie Ma",
        "Daoyu Wang",
        "Enhong Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.",
      "tldr_zh": "这篇调查论文综述了知识导向的 Retrieval-Augmented Generation (RAG)，它通过整合大规模检索系统和生成模型，利用外部知识来源（如文档或数据库）来提升自然语言理解和生成的准确性。论文详细探讨了 RAG 的核心组件，包括检索机制、生成过程及其整合方式，并提出一个 taxonomy 来分类 RAG 方法，从基本检索增强方法到高级多模态和推理模型。研究还审视了评估 benchmarks、数据集和实际应用，如问答、总结及信息检索，同时指出了未来挑战和机会，例如提升检索效率、增强模型 interpretability，以及进行领域特定适应，以推动 RAG 在真实世界 NLP 问题的解决。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10677v2",
      "published_date": "2025-03-11 01:59:35 UTC",
      "updated_date": "2025-03-17 11:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:03:53.600115"
    },
    {
      "arxiv_id": "2503.07963v2",
      "title": "Hierarchical Contact-Rich Trajectory Optimization for Multi-Modal Manipulation using Tight Convex Relaxations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuki Shirai",
        "Arvind Raghunathan",
        "Devesh K. Jha"
      ],
      "abstract": "Designing trajectories for manipulation through contact is challenging as it\nrequires reasoning of object \\& robot trajectories as well as complex contact\nsequences simultaneously. In this paper, we present a novel framework for\nsimultaneously designing trajectories of robots, objects, and contacts\nefficiently for contact-rich manipulation. We propose a hierarchical\noptimization framework where Mixed-Integer Linear Program (MILP) selects\noptimal contacts between robot \\& object using approximate dynamical\nconstraints, and then a NonLinear Program (NLP) optimizes trajectory of the\nrobot(s) and object considering full nonlinear constraints. We present a convex\nrelaxation of bilinear constraints using binary encoding technique such that\nMILP can provide tighter solutions with better computational complexity. The\nproposed framework is evaluated on various manipulation tasks where it can\nreason about complex multi-contact interactions while providing computational\nadvantages. We also demonstrate our framework in hardware experiments using a\nbimanual robot system. The video summarizing this paper and hardware\nexperiments is found https://youtu.be/s2S1Eg5RsRE?si=chPkftz_a3NAHxLq",
      "tldr_zh": "这篇论文提出一个分层优化框架，用于高效设计接触丰富的多模态操作轨迹，涉及机器人、物体和接触序列的同步规划。框架首先利用 Mixed-Integer Linear Program (MILP) 通过近似动态约束选择最优接触点，然后采用 NonLinear Program (NLP) 优化轨迹，同时应用二进制编码技术对双线性约束进行紧致凸松弛，以提升计算效率。实验结果显示，该框架在各种复杂多接触任务上表现出色，提供计算优势，并在双臂机器人硬件系统中进行了验证。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "2025 IEEE International Conference on Robotics and Automation (2025\n  ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2503.07963v2",
      "published_date": "2025-03-11 01:40:23 UTC",
      "updated_date": "2025-03-12 01:43:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:04:04.414414"
    },
    {
      "arxiv_id": "2503.07956v1",
      "title": "EFPC: Towards Efficient and Flexible Prompt Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Yun-Hao Cao",
        "Yangsong Wang",
        "Shuzheng Hao",
        "Zhenxing Li",
        "Chengjun Zhan",
        "Sichao Liu",
        "Yi-Qi Hu"
      ],
      "abstract": "The emergence of large language models (LLMs) like GPT-4 has revolutionized\nnatural language processing (NLP), enabling diverse, complex tasks. However,\nextensive token counts lead to high computational and financial burdens. To\naddress this, we propose Efficient and Flexible Prompt Compression (EFPC), a\nnovel method unifying task-aware and task-agnostic compression for a favorable\naccuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts\nand integrates them with original prompts for training. During training and\ninference, we selectively prepend user instructions and compress prompts based\non predicted probabilities. EFPC is highly data-efficient, achieving\nsignificant performance with minimal data. Compared to the state-of-the-art\nmethod LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with\n1% additional data at a 4x compression rate, and an 11.4% gain with 10%\nadditional data on the LongBench single-doc QA benchmark. EFPC's unified\nframework supports broad applicability and enhances performance across various\nmodels, tasks, and domains, offering a practical advancement in NLP.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）如 GPT-4 在自然语言处理（NLP）中面临的 token 数量过多导致的计算负担问题，提出了一种高效且灵活的提示压缩方法 EFPC。EFPC 统一了任务感知和任务无关压缩，通过 GPT-4 生成压缩提示并与原提示整合进行训练，并在训练和推理阶段基于预测概率选择性地添加用户指令，实现准确性和效率的平衡。与最先进方法 LLMLingua-2 相比，EFPC 在 4x 压缩率下，使用 1% 额外数据时 F1-score 相对提升 4.8%，使用 10% 额外数据时提升 11.4%，特别是在 LongBench 单文档 QA 基准上表现出色。该框架数据效率高，且适用于各种模型、任务和领域，推动了 NLP 的实用性发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.07956v1",
      "published_date": "2025-03-11 01:34:03 UTC",
      "updated_date": "2025-03-11 01:34:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:04:17.437404"
    },
    {
      "arxiv_id": "2503.09626v1",
      "title": "Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Wu",
        "Yingguang Yang",
        "hao liu",
        "Hao Peng",
        "Buyun He",
        "Yutong Xia",
        "Yong Liao"
      ],
      "abstract": "Social bot detection is crucial for mitigating misinformation, online\nmanipulation, and coordinated inauthentic behavior. While existing neural\nnetwork-based detectors perform well on benchmarks, they struggle with\ngeneralization due to distribution shifts across datasets and frequently\nproduce overconfident predictions for out-of-distribution accounts beyond the\ntraining data. To address this, we introduce a novel Uncertainty Estimation for\nSocial Bot Detection (UESBD) framework, which quantifies the predictive\nuncertainty of detectors beyond mere classification. For this task, we propose\nRobust Multi-modal Neural Processes (RMNP), which aims to enhance the\nrobustness of multi-modal neural processes to modality inconsistencies caused\nby social bot camouflage. RMNP first learns unimodal representations through\nmodality-specific encoders. Then, unimodal attentive neural processes are\nemployed to encode the Gaussian distribution of unimodal latent variables.\nFurthermore, to avoid social bots stealing human features to camouflage\nthemselves thus causing certain modalities to provide conflictive information,\nwe introduce an evidential gating network to explicitly model the reliability\nof modalities. The joint latent distribution is learned through the generalized\nproduct of experts, which takes the reliability of each modality into\nconsideration during fusion. The final prediction is obtained through Monte\nCarlo sampling of the joint latent distribution followed by a decoder.\nExperiments on three real-world benchmarks show the effectiveness of RMNP in\nclassification and uncertainty estimation, as well as its robustness to\nmodality conflicts.",
      "tldr_zh": "该论文针对社会机器人检测面临的泛化问题和过度自信预测，提出Uncertainty Estimation for Social Bot Detection (UESBD)框架，以量化检测器的预测不确定性。核心方法Robust Multi-modal Neural Processes (RMNP)通过模态特定编码器学习单模态表示，并采用单模态注意力神经过程编码Gaussian分布，同时引入evidential gating network来评估模态可靠性，避免社会机器人伪装导致的模态冲突。实验在三个真实世界基准上验证了RMNP在分类、不确定性估计以及对模态冲突的鲁棒性方面的显著提升。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "12 pages. 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09626v1",
      "published_date": "2025-03-11 01:32:52 UTC",
      "updated_date": "2025-03-11 01:32:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:04:30.039967"
    },
    {
      "arxiv_id": "2503.08729v1",
      "title": "Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ishaan Malhi",
        "Praneet Dutta",
        "Ellie Talius",
        "Sally Ma",
        "Brendan Driscoll",
        "Krista Holden",
        "Garima Pruthi",
        "Arunachalam Narayanaswamy"
      ],
      "abstract": "We present a framework for high-fidelity product image recontextualization\nusing text-to-image diffusion models and a novel data augmentation pipeline.\nThis pipeline leverages image-to-video diffusion, in/outpainting & negatives to\ncreate synthetic training data, addressing limitations of real-world data\ncollection for this task. Our method improves the quality and diversity of\ngenerated images by disentangling product representations and enhancing the\nmodel's understanding of product characteristics. Evaluation on the ABO dataset\nand a private product dataset, using automated metrics and human assessment,\ndemonstrates the effectiveness of our framework in generating realistic and\ncompelling product visualizations, with implications for applications such as\ne-commerce and virtual product showcasing.",
      "tldr_zh": "本研究提出了一种框架，用于在大型图像再语境化任务中保持产品保真度（Product Fidelity），利用文本到图像扩散模型（text-to-image diffusion models）和一个新颖的数据增强管道（data augmentation pipeline）。该管道通过图像到视频扩散（image-to-video diffusion）、in/outpainting 和 negatives 生成合成训练数据，解决了真实世界数据收集的限制，并通过分离产品表示（disentangling product representations）提升模型对产品特性的理解，从而提高生成图像的质量和多样性。在 ABO dataset 和私有产品数据集上的评估显示，该框架在自动化指标和人类评估中表现出色，能生成逼真的产品可视化，并适用于电子商务和虚拟产品展示等应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08729v1",
      "published_date": "2025-03-11 01:24:39 UTC",
      "updated_date": "2025-03-11 01:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:04:40.514255"
    },
    {
      "arxiv_id": "2503.08728v1",
      "title": "Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse",
      "title_zh": "通过基于模型的强化学习和策略重用提升交通信号控制",
      "authors": [
        "Yihong Li",
        "Chengwei Zhang",
        "Furui Zhan",
        "Wanting Liu",
        "Kailing Zhou",
        "Longji Zheng"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
      "tldr_zh": "本研究针对多智能体强化学习(MARL)在交通信号控制(TSC)中的泛化不足问题，提出两种算法：PLight 和 PRLight，以提升对新交通场景的适应性。PLight 采用基于模型的强化学习方法，在预定义源域交通场景中预训练控制策略和环境模型，从而预测状态转移并比较环境特征；PRLight 则基于源域和目标域的相似性，自适应选择预训练代理，加速目标域的学习过程。实验结果显示，在不同交通场景和路网的转移设置中，PRLight 显著减少了适应时间，并比从零开始学习更快达到最优性能。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08728v1",
      "published_date": "2025-03-11 01:21:13 UTC",
      "updated_date": "2025-03-11 01:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:04:56.299296"
    },
    {
      "arxiv_id": "2503.07946v1",
      "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongpai Gao",
        "Benjamin Planche",
        "Meng Zheng",
        "Anwesa Choudhuri",
        "Terrence Chen",
        "Ziyan Wu"
      ],
      "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a\nfundamental challenge in computer graphics. While recent advances in Gaussian\nSplatting have shown promising results separately handling dynamic scenes\n(4DGS) and view-dependent effects (6DGS), no existing method unifies these\ncapabilities while maintaining real-time performance. We present 7D Gaussian\nSplatting (7DGS), a unified framework representing scene elements as\nseven-dimensional Gaussians spanning position (3D), time (1D), and viewing\ndirection (3D). Our key contribution is an efficient conditional slicing\nmechanism that transforms 7D Gaussians into view- and time-conditioned 3D\nGaussians, maintaining compatibility with existing 3D Gaussian Splatting\npipelines while enabling joint optimization. Experiments demonstrate that 7DGS\noutperforms prior methods by up to 7.36 dB in PSNR while achieving real-time\nrendering (401 FPS) on challenging dynamic scenes with complex view-dependent\neffects. The project page is: https://gaozhongpai.github.io/7dgs/.",
      "tldr_zh": "该论文提出7DGS框架，统一处理动态场景的实时渲染问题，包括空间（3D）、时间（1D）和视点方向（3D）的七维高斯函数表示。关键创新是高效的条件切片机制，将7D高斯转换为视点和时间相关的3D高斯，确保与现有Gaussian Splatting管道兼容，并支持联合优化。实验结果显示，7DGS在复杂动态场景中比现有方法提高高达7.36 dB的PSNR，同时实现实时渲染（401 FPS）。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07946v1",
      "published_date": "2025-03-11 01:16:08 UTC",
      "updated_date": "2025-03-11 01:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:05:04.319880"
    },
    {
      "arxiv_id": "2503.08727v2",
      "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Caccia",
        "Alan Ansell",
        "Edoardo Ponti",
        "Ivan Vulić",
        "Alessandro Sordoni"
      ],
      "abstract": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.",
      "tldr_zh": "该论文针对大语言模型在预训练后动态整合新信息（如低数据或私有文档）的挑战，提出了一种训练可插拔的 Knowledge Modules (KMs) 方法，这些模块采用参数高效的 LoRA 实现，便于按需存储和调用文档级知识。作者引入 Deep Context Distillation 技术，通过学习 KMs 参数来模拟教师模型的隐藏状态和 logits，从而克服传统 next-token prediction 的不足，并在两个数据集上表现出色。实验结果显示，该方法优于标准训练技术，并强调了 KMs 与检索增强生成 (RAG) 之间的协同潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.08727v2",
      "published_date": "2025-03-11 01:07:57 UTC",
      "updated_date": "2025-04-29 17:11:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:05:17.778323"
    },
    {
      "arxiv_id": "2503.08726v1",
      "title": "SIMAC: A Semantic-Driven Integrated Multimodal Sensing And Communication Framework",
      "title_zh": "SIMAC：语义驱动",
      "authors": [
        "Yubo Peng",
        "Luping Xiang",
        "Kun Yang",
        "Feibo Jiang",
        "Kezhi Wang",
        "Dapeng Oliver Wu"
      ],
      "abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
      "tldr_zh": "该研究提出SIMAC框架，这是一种语义驱动的集成多模态感知和通信系统，旨在克服传统单模态感知的准确性限制、通信延迟问题以及单任务导向的不足。框架采用联合源-通道编码架构，包括Multimodal Semantic Fusion (MSF)网络用于从雷达信号和图像提取并融合语义信息、Large Language Model (LLM)-based Semantic Encoder (LSE)用于通道自适应编码，以及Task-oriented Sensing Semantic Decoder (SSD)结合多任务学习策略以支持多样化感知服务。实验结果显示，SIMAC框架实现了更高的感知准确性和多样服务性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08726v1",
      "published_date": "2025-03-11 01:04:42 UTC",
      "updated_date": "2025-03-11 01:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:05:29.058192"
    },
    {
      "arxiv_id": "2503.07937v1",
      "title": "LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Siyuan Wang",
        "James R. Foulds",
        "Md Osman Gani",
        "Shimei Pan"
      ],
      "abstract": "In this paper, we introduce CIBER (Claim Investigation Based on Evidence\nRetrieval), an extension of the Retrieval-Augmented Generation (RAG) framework\ndesigned to identify corroborating and refuting documents as evidence for\nscientific claim verification. CIBER addresses the inherent uncertainty in\nLarge Language Models (LLMs) by evaluating response consistency across diverse\ninterrogation probes. By focusing on the behavioral analysis of LLMs without\nrequiring access to their internal information, CIBER is applicable to both\nwhite-box and black-box models. Furthermore, CIBER operates in an unsupervised\nmanner, enabling easy generalization across various scientific domains.\nComprehensive evaluations conducted using LLMs with varying levels of\nlinguistic proficiency reveal CIBER's superior performance compared to\nconventional RAG approaches. These findings not only highlight the\neffectiveness of CIBER but also provide valuable insights for future\nadvancements in LLM-based scientific claim verification.",
      "tldr_zh": "本论文提出 CIBER，这是一个基于 Retrieval-Augmented Generation (RAG) 框架的扩展，用于科学声明验证，通过检索支持和反驳证据来处理 Large Language Models (LLMs) 的不确定性。CIBER 通过评估 LLM 在不同查询下的响应一致性进行行为分析，无需访问模型内部信息，从而适用于白盒和黑盒模型，并以无监督方式在各种科学领域实现泛化。实验结果显示，CIBER 比传统 RAG 方法表现出色，提供宝贵见解，推动 LLM 在科学声明验证领域的未来进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07937v1",
      "published_date": "2025-03-11 00:29:50 UTC",
      "updated_date": "2025-03-11 00:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:05:40.770063"
    },
    {
      "arxiv_id": "2503.07932v1",
      "title": "A Theory of Learning with Autoregressive Chain of Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Nirmit Joshi",
        "Gal Vardi",
        "Adam Block",
        "Surbhi Goel",
        "Zhiyuan Li",
        "Theodor Misiakiewicz",
        "Nathan Srebro"
      ],
      "abstract": "For a given base class of sequence-to-next-token generators, we consider\nlearning prompt-to-answer mappings obtained by iterating a fixed,\ntime-invariant generator for multiple steps, thus generating a\nchain-of-thought, and then taking the final token as the answer. We formalize\nthe learning problems both when the chain-of-thought is observed and when\ntraining only on prompt-answer pairs, with the chain-of-thought latent. We\nanalyze the sample and computational complexity both in terms of general\nproperties of the base class (e.g. its VC dimension) and for specific base\nclasses such as linear thresholds. We present a simple base class that allows\nfor universal representability and computationally tractable chain-of-thought\nlearning. Central to our development is that time invariance allows for sample\ncomplexity that is independent of the length of the chain-of-thought. Attention\narises naturally in our construction.",
      "tldr_zh": "这篇论文提出了一种学习理论，专注于使用 autoregressive chain of thought 来学习 prompt-to-answer 映射，通过迭代一个时间不变的 sequence-to-next-token generator 生成 chain-of-thought，并以最终 token 作为答案。论文正式化了 chain-of-thought 可见和不可见两种学习场景，并分析了样本和计算复杂度，基于 base class 的通用属性（如 VC dimension）以及特定类如 linear thresholds。关键发现是，一个简单的 base class 实现了通用表示和可计算的学习，且时间不变性使样本复杂度独立于 chain-of-thought 的长度，同时 Attention 自然地在框架中出现。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Comments are welcome",
      "pdf_url": "http://arxiv.org/pdf/2503.07932v1",
      "published_date": "2025-03-11 00:21:32 UTC",
      "updated_date": "2025-03-11 00:21:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:05:53.784124"
    },
    {
      "arxiv_id": "2503.08725v2",
      "title": "The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government",
      "title_zh": "翻译失败",
      "authors": [
        "Zeynep Engin",
        "Jon Crowcroft",
        "David Hand",
        "Philip Treleaven"
      ],
      "abstract": "As artificial intelligence transforms public sector operations, governments\nstruggle to integrate technological innovations into coherent systems for\neffective service delivery. This paper introduces the Algorithmic State\nArchitecture (ASA), a novel four-layer framework conceptualising how Digital\nPublic Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and\nGovTech interact as an integrated system in AI-enabled states. Unlike\napproaches that treat these as parallel developments, ASA positions them as\ninterdependent layers with specific enabling relationships and feedback\nmechanisms. Through comparative analysis of implementations in Estonia,\nSingapore, India, and the UK, we demonstrate how foundational digital\ninfrastructure enables systematic data collection, which powers algorithmic\ndecision-making processes, ultimately manifesting in user-facing services. Our\nanalysis reveals that successful implementations require balanced development\nacross all layers, with particular attention to integration mechanisms between\nthem. The framework contributes to both theory and practice by bridging\npreviously disconnected domains of digital government research, identifying\ncritical dependencies that influence implementation success, and providing a\nstructured approach for analysing the maturity and development pathways of\nAI-enabled government systems.",
      "tldr_zh": "本研究提出Algorithmic State Architecture (ASA)，一个整合框架，用于AI-Enabled Government，帮助政府将数字创新整合成高效系统。ASA 由四层结构组成，包括Digital Public Infrastructure、Data-for-Policy、Algorithmic Government/Governance 和 GovTech，这些层相互依赖，通过特定关系和反馈机制协同作用。作者通过比较Estonia、Singapore、India 和 UK的实施案例，展示了从基础数字基础设施到数据收集、算法决策再到用户服务的流程。研究发现，成功部署需要各层平衡发展，并强调关键依赖关系，为数字政府研究提供理论基础和分析路径。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "Main text: 25 pages, with references: 35 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08725v2",
      "published_date": "2025-03-11 00:20:56 UTC",
      "updated_date": "2025-03-13 11:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:06:04.648470"
    },
    {
      "arxiv_id": "2503.07928v2",
      "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course",
      "title_zh": "StudyChat 数据集",
      "authors": [
        "Hunter McNichols",
        "Andrew Lan"
      ],
      "abstract": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be analyzed to ensure ethical\nusage of these tools. To better understand how students interact with LLMs in\nan academic setting, we introduce \\textbf{StudyChat}, a publicly available\ndataset capturing real-world student interactions with an LLM-powered tutoring\nchatbot in a semester-long, university-level artificial intelligence (AI)\ncourse. We deploy a web application that replicates ChatGPT's core\nfunctionalities, and use it to log student interactions with the LLM while\nworking on programming assignments. We collect 1,197 conversations, which we\nannotate using a dialogue act labeling schema inspired by observed interaction\npatterns and prior research. Additionally, we analyze these interactions,\nhighlight behavioral trends, and analyze how specific usage patterns relate to\ncourse outcomes. \\textbf{StudyChat} provides a rich resource for the learning\nsciences and AI in education communities, enabling further research into the\nevolving role of LLMs in education.",
      "tldr_zh": "本文介绍了 StudyChat 数据集，这是一个公开可用的资源，记录了学生在大学人工智能 (AI) 课程中使用 ChatGPT 类似的大语言模型 (LLMs) 驱动的辅导聊天机器人进行的 1,197 次真实对话。研究团队部署了一个复制 ChatGPT 核心功能的 web 应用来收集这些互动，并采用基于观察模式和先前研究的对话行为标记方案 (dialogue act labeling schema) 进行标注。分析结果突出了学生的行为趋势，并探讨了特定使用模式与课程成果的相关性，为学习科学和 AI 在教育领域的进一步研究提供了丰富资源。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Pre-print v0.2",
      "pdf_url": "http://arxiv.org/pdf/2503.07928v2",
      "published_date": "2025-03-11 00:17:07 UTC",
      "updated_date": "2025-04-12 02:42:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T01:06:17.226536"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 146,
  "processed_papers_count": 146,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T01:06:40.155198"
}
[
  {
    "arxiv_id": "2408.01880v4",
    "title": "Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via Efficient Guidance-Exploration",
    "authors": [
      "Zijian Wang",
      "Bin Wang",
      "Haifeng Jing",
      "Huayu Li",
      "Hongbo Dou"
    ],
    "abstract": "Recent years, multi-hop reasoning has been widely studied for knowledge graph\n(KG) reasoning due to its efficacy and interpretability. However, previous\nmulti-hop reasoning approaches are subject to two primary shortcomings. First,\nagents struggle to learn effective and robust policies at the early phase due\nto sparse rewards. Second, these approaches often falter on specific datasets\nlike sparse knowledge graphs, where agents are required to traverse lengthy\nreasoning paths. To address these problems, we propose a multi-hop reasoning\nmodel with dual agents based on hierarchical reinforcement learning (HRL),\nwhich is named FULORA. FULORA tackles the above reasoning challenges by\neFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks\non the simplified knowledge graph to provide stage-wise hints for the low-level\nagent walking on the original knowledge graph. In this framework, the low-level\nagent optimizes a value function that balances two objectives: (1) maximizing\nreturn, and (2) integrating efficient guidance from the high-level agent.\nExperiments conducted on three real-word knowledge graph datasets demonstrate\nthat FULORA outperforms RL-based baselines, especially in the case of\nlong-distance reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2408.01880v4",
    "published_date": "2024-08-03 23:15:57 UTC",
    "updated_date": "2024-12-18 18:31:42 UTC"
  },
  {
    "arxiv_id": "2408.01872v1",
    "title": "Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as Positive Examples",
    "authors": [
      "Min Gu Kwak",
      "Hyungu Kahng",
      "Seoung Bum Kim"
    ],
    "abstract": "Semi-supervised learning methods have shown promising results in solving many\npractical problems when only a few labels are available. The existing methods\nassume that the class distributions of labeled and unlabeled data are equal;\nhowever, their performances are significantly degraded in class distribution\nmismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled\ndata. Previous safe semi-supervised learning studies have addressed this\nproblem by making OOD data less likely to affect training based on labeled\ndata. However, even if the studies effectively filter out the unnecessary OOD\ndata, they can lose the basic information that all data share regardless of\nclass. To this end, we propose to apply a self-supervised contrastive learning\napproach to fully exploit a large amount of unlabeled data. We also propose a\ncontrastive loss function with coefficient schedule to aggregate as an anchor\nthe labeled negative examples of the same class into positive examples. To\nevaluate the performance of the proposed method, we conduct experiments on\nimage classification datasets - CIFAR-10, CIFAR-100, Tiny ImageNet, and\nCIFAR-100+Tiny ImageNet - under various mismatch ratios. The results show that\nself-supervised contrastive learning significantly improves classification\naccuracy. Moreover, aggregating the in-distribution examples produces better\nrepresentation and consequently further improves classification accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01872v1",
    "published_date": "2024-08-03 22:33:13 UTC",
    "updated_date": "2024-08-03 22:33:13 UTC"
  },
  {
    "arxiv_id": "2408.01869v1",
    "title": "MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance",
    "authors": [
      "Jihye Choi",
      "Nils Palumbo",
      "Prasad Chalasani",
      "Matthew M. Engelhard",
      "Somesh Jha",
      "Anivarya Kumar",
      "David Page"
    ],
    "abstract": "In the era of Large Language Models (LLMs), given their remarkable text\nunderstanding and generation abilities, there is an unprecedented opportunity\nto develop new, LLM-based methods for trustworthy medical knowledge synthesis,\nextraction and summarization. This paper focuses on the problem of\nPharmacovigilance (PhV), where the significance and challenges lie in\nidentifying Adverse Drug Events (ADEs) from diverse text sources, such as\nmedical literature, clinical notes, and drug labels. Unfortunately, this task\nis hindered by factors including variations in the terminologies of drugs and\noutcomes, and ADE descriptions often being buried in large amounts of narrative\ntext. We present MALADE, the first effective collaborative multi-agent system\npowered by LLM with Retrieval Augmented Generation for ADE extraction from drug\nlabel data. This technique involves augmenting a query to an LLM with relevant\ninformation extracted from text resources, and instructing the LLM to compose a\nresponse consistent with the augmented data. MALADE is a general LLM-agnostic\narchitecture, and its unique capabilities are: (1) leveraging a variety of\nexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,\nOpenFDA drug information API), (2) extracting drug-outcome association in a\nstructured format along with the strength of the association, and (3) providing\nexplanations for established associations. Instantiated with GPT-4 Turbo or\nGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area\nUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our\nimplementation leverages the Langroid multi-agent LLM framework and can be\nfound at https://github.com/jihyechoi77/malade.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MA",
      "q-bio.QM"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper published at Machine Learning for Healthcare 2024 (MLHC'24)",
    "pdf_url": "http://arxiv.org/pdf/2408.01869v1",
    "published_date": "2024-08-03 22:14:13 UTC",
    "updated_date": "2024-08-03 22:14:13 UTC"
  },
  {
    "arxiv_id": "2408.11822v1",
    "title": "State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey",
    "authors": [
      "Bin Wu",
      "C Steve Suh"
    ],
    "abstract": "With the continuous breakthroughs in core technology, the dawn of large-scale\nintegration of robotic systems into daily human life is on the horizon.\nMulti-robot systems (MRS) built on this foundation are undergoing drastic\nevolution. The fusion of artificial intelligence technology with robot hardware\nis seeing broad application possibilities for MRS. This article surveys the\nstate-of-the-art of robot learning in the context of Multi-Robot Cooperation\n(MRC) of recent. Commonly adopted robot learning methods (or frameworks) that\nare inspired by humans and animals are reviewed and their advantages and\ndisadvantages are discussed along with the associated technical challenges. The\npotential trends of robot learning and MRS integration exploiting the merging\nof these methods with real-world applications is also discussed at length.\nSpecifically statistical methods are used to quantitatively corroborate the\nideas elaborated in the article.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Multi-robot, Cooperation, robot learning",
    "pdf_url": "http://arxiv.org/pdf/2408.11822v1",
    "published_date": "2024-08-03 21:22:08 UTC",
    "updated_date": "2024-08-03 21:22:08 UTC"
  },
  {
    "arxiv_id": "2408.08881v3",
    "title": "Challenge Summary U-MedSAM: Uncertainty-aware MedSAM for Medical Image Segmentation",
    "authors": [
      "Xin Wang",
      "Xiaoyu Liu",
      "Peng Huang",
      "Pu Huang",
      "Shu Hu",
      "Hongtu Zhu"
    ],
    "abstract": "Medical Image Foundation Models have proven to be powerful tools for mask\nprediction across various datasets. However, accurately assessing the\nuncertainty of their predictions remains a significant challenge. To address\nthis, we propose a new model, U-MedSAM, which integrates the MedSAM model with\nan uncertainty-aware loss function and the Sharpness-Aware Minimization\n(SharpMin) optimizer. The uncertainty-aware loss function automatically\ncombines region-based, distribution-based, and pixel-based loss designs to\nenhance segmentation accuracy and robustness. SharpMin improves generalization\nby finding flat minima in the loss landscape, thereby reducing overfitting. Our\nmethod was evaluated in the CVPR24 MedSAM on Laptop challenge, where U-MedSAM\ndemonstrated promising performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "arXiv admin note: text overlap with arXiv:2405.17496",
    "pdf_url": "http://arxiv.org/pdf/2408.08881v3",
    "published_date": "2024-08-03 20:41:35 UTC",
    "updated_date": "2025-01-17 02:51:41 UTC"
  },
  {
    "arxiv_id": "2408.04650v2",
    "title": "Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools",
    "authors": [
      "Jung In Park",
      "Mahyar Abbasian",
      "Iman Azimi",
      "Dawn T. Bounds",
      "Angela Jun",
      "Jaesu Han",
      "Robert M. McCarron",
      "Jessica Borelli",
      "Parmida Safavi",
      "Sanaz Mirbaha",
      "Jia Li",
      "Mona Mahmoudi",
      "Carmen Wiedenhoeft",
      "Amir M. Rahmani"
    ],
    "abstract": "Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.04650v2",
    "published_date": "2024-08-03 19:57:49 UTC",
    "updated_date": "2025-03-01 00:49:53 UTC"
  },
  {
    "arxiv_id": "2408.05341v1",
    "title": "CAR: Contrast-Agnostic Deformable Medical Image Registration with Contrast-Invariant Latent Regularization",
    "authors": [
      "Yinsong Wang",
      "Siyi Du",
      "Shaoming Zheng",
      "Xinzhe Luo",
      "Chen Qin"
    ],
    "abstract": "Multi-contrast image registration is a challenging task due to the complex\nintensity relationships between different imaging contrasts. Conventional image\nregistration methods are typically based on iterative optimizations for each\ninput image pair, which is time-consuming and sensitive to contrast variations.\nWhile learning-based approaches are much faster during the inference stage, due\nto generalizability issues, they typically can only be applied to the fixed\ncontrasts observed during the training stage. In this work, we propose a novel\ncontrast-agnostic deformable image registration framework that can be\ngeneralized to arbitrary contrast images, without observing them during\ntraining. Particularly, we propose a random convolution-based contrast\naugmentation scheme, which simulates arbitrary contrasts of images over a\nsingle image contrast while preserving their inherent structural information.\nTo ensure that the network can learn contrast-invariant representations for\nfacilitating contrast-agnostic registration, we further introduce\ncontrast-invariant latent regularization (CLR) that regularizes representation\nin latent space through a contrast invariance loss. Experiments show that CAR\noutperforms the baseline approaches regarding registration accuracy and also\npossesses better generalization ability to unseen imaging contrasts. Code is\navailable at \\url{https://github.com/Yinsong0510/CAR}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 3 figures, 3 tables, accecpted by WBIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.05341v1",
    "published_date": "2024-08-03 19:46:23 UTC",
    "updated_date": "2024-08-03 19:46:23 UTC"
  },
  {
    "arxiv_id": "2408.02693v3",
    "title": "Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models",
    "authors": [
      "Chuan Liu",
      "Chunshu Wu",
      "Shihui Cao",
      "Mingkai Chen",
      "James Chenhao Liang",
      "Ang Li",
      "Michael Huang",
      "Chuang Ren",
      "Dongfang Liu",
      "Ying Nian Wu",
      "Tong Geng"
    ],
    "abstract": "The rapid development of AI highlights the pressing need for sustainable\nenergy, a critical global challenge for decades. Nuclear fusion, generally seen\nas an ultimate solution, has been the focus of intensive research for nearly a\ncentury, with investments reaching hundreds of billions of dollars. Recent\nadvancements in Inertial Confinement Fusion have drawn significant attention to\nfusion research, in which Laser-Plasma Interaction (LPI) is critical for\nensuring fusion stability and efficiency. However, the complexity of LPI upon\nfusion ignition makes analytical approaches impractical, leaving researchers\ndepending on extremely computation-demanding Particle-in-Cell (PIC) simulations\nto generate data, presenting a significant bottleneck to advancing fusion\nresearch. In response, this work introduces Diff-PIC, a novel framework that\nleverages conditional diffusion models as a computationally efficient\nalternative to PIC simulations for generating high-fidelity scientific LPI\ndata. In this work, physical patterns captured by PIC simulations are distilled\ninto diffusion models associated with two tailored enhancements: (1) To\neffectively capture the complex relationships between physical parameters and\ncorresponding outcomes, the parameters are encoded in a physically-informed\nmanner. (2) To further enhance efficiency while maintaining high fidelity and\nphysical validity, the rectified flow technique is employed to transform our\nmodel into a one-step conditional diffusion model. Experimental results show\nthat Diff-PIC achieves 16,200$\\times$ speedup compared to traditional PIC on a\n100 picosecond simulation, with an average reduction in MAE / RMSE / FID of\n59.21% / 57.15% / 39.46% with respect to two other SOTA data generation\napproaches.",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02693v3",
    "published_date": "2024-08-03 19:42:31 UTC",
    "updated_date": "2024-10-06 03:10:31 UTC"
  },
  {
    "arxiv_id": "2408.01827v1",
    "title": "ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification",
    "authors": [
      "Mridula Vijendran",
      "Frederick W. B. Li",
      "Jingjing Deng",
      "Hubert P. H. Shum"
    ],
    "abstract": "Painting classification plays a vital role in organizing, finding, and\nsuggesting artwork for digital and classic art galleries. Existing methods\nstruggle with adapting knowledge from the real world to artistic images during\ntraining, leading to poor performance when dealing with different datasets. Our\ninnovation lies in addressing these challenges through a two-step process.\nFirst, we generate more data using Style Transfer with Adaptive Instance\nNormalization (AdaIN), bridging the gap between diverse styles. Then, our\nclassifier gains a boost with feature-map adaptive spatial attention modules,\nimproving its understanding of artistic details. Moreover, we tackle the\nproblem of imbalanced class representation by dynamically adjusting augmented\nsamples. Through a dual-stage process involving careful hyperparameter search\nand model fine-tuning, we achieve an impressive 87.24\\% accuracy using the\nResNet-50 backbone over 40 training epochs. Our study explores quantitative\nanalyses that compare different pretrained backbones, investigates model\noptimization through ablation studies, and examines how varying augmentation\nlevels affect model performance. Complementing this, our qualitative\nexperiments offer valuable insights into the model's decision-making process\nusing spatial attention and its ability to differentiate between easy and\nchallenging samples based on confidence ranking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01827v1",
    "published_date": "2024-08-03 17:31:58 UTC",
    "updated_date": "2024-08-03 17:31:58 UTC"
  },
  {
    "arxiv_id": "2408.02692v1",
    "title": "Attention is all you need for an improved CNN-based flash flood susceptibility modeling. The case of the ungauged Rheraya watershed, Morocco",
    "authors": [
      "Akram Elghouat",
      "Ahmed Algouti",
      "Abdellah Algouti",
      "Soukaina Baid"
    ],
    "abstract": "Effective flood hazard management requires evaluating and predicting flash\nflood susceptibility. Convolutional neural networks (CNNs) are commonly used\nfor this task but face issues like gradient explosion and overfitting. This\nstudy explores the use of an attention mechanism, specifically the\nconvolutional block attention module (CBAM), to enhance CNN models for flash\nflood susceptibility in the ungauged Rheraya watershed, a flood prone region.\nWe used ResNet18, DenseNet121, and Xception as backbone architectures,\nintegrating CBAM at different locations. Our dataset included 16 conditioning\nfactors and 522 flash flood inventory points. Performance was evaluated using\naccuracy, precision, recall, F1-score, and the area under the curve (AUC) of\nthe receiver operating characteristic (ROC). Results showed that CBAM\nsignificantly improved model performance, with DenseNet121 incorporating CBAM\nin each convolutional block achieving the best results (accuracy = 0.95, AUC =\n0.98). Distance to river and drainage density were identified as key factors.\nThese findings demonstrate the effectiveness of the attention mechanism in\nimproving flash flood susceptibility modeling and offer valuable insights for\ndisaster management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02692v1",
    "published_date": "2024-08-03 16:57:01 UTC",
    "updated_date": "2024-08-03 16:57:01 UTC"
  },
  {
    "arxiv_id": "2408.04649v1",
    "title": "Chain of Stance: Stance Detection with Large Language Models",
    "authors": [
      "Junxia Ma",
      "Changjiang Wang",
      "Hanwen Xing",
      "Dongming Zhao",
      "Yazhou Zhang"
    ],
    "abstract": "Stance detection is an active task in natural language processing (NLP) that\naims to identify the author's stance towards a particular target within a text.\nGiven the remarkable language understanding capabilities and encyclopedic prior\nknowledge of large language models (LLMs), how to explore the potential of LLMs\nin stance detection has received significant attention. Unlike existing\nLLM-based approaches that focus solely on fine-tuning with large-scale\ndatasets, we propose a new prompting method, called \\textit{Chain of Stance}\n(CoS). In particular, it positions LLMs as expert stance detectors by\ndecomposing the stance detection process into a series of intermediate,\nstance-related assertions that culminate in the final judgment. This approach\nleads to significant improvements in classification performance. We conducted\nextensive experiments using four SOTA LLMs on the SemEval 2016 dataset,\ncovering the zero-shot and few-shot learning setups. The results indicate that\nthe proposed method achieves state-of-the-art results with an F1 score of 79.84\nin the few-shot setting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.04649v1",
    "published_date": "2024-08-03 16:30:51 UTC",
    "updated_date": "2024-08-03 16:30:51 UTC"
  },
  {
    "arxiv_id": "2408.01808v1",
    "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
    "authors": [
      "Peng Cheng",
      "Yuwei Wang",
      "Peng Huang",
      "Zhongjie Ba",
      "Xiaodong Lin",
      "Feng Lin",
      "Li Lu",
      "Kui Ren"
    ],
    "abstract": "Extensive research has revealed that adversarial examples (AE) pose a\nsignificant threat to voice-controllable smart devices. Recent studies have\nproposed black-box adversarial attacks that require only the final\ntranscription from an automatic speech recognition (ASR) system. However, these\nattacks typically involve many queries to the ASR, resulting in substantial\ncosts. Moreover, AE-based adversarial audio samples are susceptible to ASR\nupdates. In this paper, we identify the root cause of these limitations, namely\nthe inability to construct AE attack samples directly around the decision\nboundary of deep learning (DL) models. Building on this observation, we propose\nALIF, the first black-box adversarial linguistic feature-based attack pipeline.\nWe leverage the reciprocal process of text-to-speech (TTS) and ASR models to\ngenerate perturbations in the linguistic embedding space where the decision\nboundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and\nALIF-OTA schemes for launching attacks in both the digital domain and the\nphysical playback environment on four commercial ASRs and voice assistants.\nExtensive evaluations demonstrate that ALIF-OTL and -OTA significantly improve\nquery efficiency by 97.7% and 73.3%, respectively, while achieving competitive\nperformance compared to existing methods. Notably, ALIF-OTL can generate an\nattack sample with only one query. Furthermore, our test-of-time experiment\nvalidates the robustness of our approach against ASR updates.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "Published in the 2024 IEEE Symposium on Security and Privacy (SP)",
    "pdf_url": "http://arxiv.org/pdf/2408.01808v1",
    "published_date": "2024-08-03 15:30:16 UTC",
    "updated_date": "2024-08-03 15:30:16 UTC"
  },
  {
    "arxiv_id": "2408.01795v1",
    "title": "Review of Cloud Service Composition for Intelligent Manufacturing",
    "authors": [
      "Cuixia Li",
      "Liqiang Liu",
      "Li Shi"
    ],
    "abstract": "Intelligent manufacturing is a new model that uses advanced technologies such\nas the Internet of Things, big data, and artificial intelligence to improve the\nefficiency and quality of manufacturing production. As an important support to\npromote the transformation and upgrading of the manufacturing industry, cloud\nservice optimization has received the attention of researchers. In recent\nyears, remarkable research results have been achieved in this field. For the\nsustainability of intelligent manufacturing platforms, in this paper we\nsummarize the process of cloud service optimization for intelligent\nmanufacturing. Further, to address the problems of dispersed optimization\nindicators and nonuniform/unstandardized definitions in the existing research,\n11 optimization indicators that take into account three-party participant\nsubjects are defined from the urgent requirements of the sustainable\ndevelopment of intelligent manufacturing platforms. Next, service optimization\nalgorithms are classified into two categories, heuristic and reinforcement\nlearning. After comparing the two categories, the current key techniques of\nservice optimization are targeted. Finally, research hotspots and future\nresearch trends of service optimization are summarized.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01795v1",
    "published_date": "2024-08-03 14:39:40 UTC",
    "updated_date": "2024-08-03 14:39:40 UTC"
  },
  {
    "arxiv_id": "2408.01787v1",
    "title": "Towards an ontology of state actors in cyberspace",
    "authors": [
      "Giacomo De Colle"
    ],
    "abstract": "To improve cyber threat analysis practices in cybersecurity, I present a plan\nto build a formal ontological representation of state actors in cyberspace and\nof cyber operations. I argue that modelling these phenomena via ontologies\nallows for coherent integration of data coming from diverse sources, automated\nreasoning over such data, as well as intelligence extraction and reuse from and\nof them. Existing ontological tools in cybersecurity can be ameliorated by\nconnecting them to neighboring domains such as law, regulations, governmental\ninstitutions, and documents. In this paper, I propose metrics to evaluate\ncurrently existing ontological tools to create formal representations in the\ncybersecurity domain, and I provide a plan to develop and extend them when they\nare lacking.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01787v1",
    "published_date": "2024-08-03 13:56:20 UTC",
    "updated_date": "2024-08-03 13:56:20 UTC"
  },
  {
    "arxiv_id": "2408.04648v1",
    "title": "PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models",
    "authors": [
      "Alexey Tikhonov"
    ],
    "abstract": "We present PLUGH (https://www.urbandictionary.com/define.php?term=plugh), a\nmodern benchmark that currently consists of 5 tasks, each with 125 input texts\nextracted from 48 different games and representing 61 different\n(non-isomorphic) spatial graphs to assess the abilities of Large Language\nModels (LLMs) for spatial understanding and reasoning. Our evaluation of\nAPI-based and open-sourced LLMs shows that while some commercial LLMs exhibit\nstrong reasoning abilities, open-sourced competitors can demonstrate almost the\nsame level of quality; however, all models still have significant room for\nimprovement. We identify typical reasons for LLM failures and discuss possible\nways to deal with them. Datasets and evaluation code are released\n(https://github.com/altsoph/PLUGH).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "68T50, 68T20",
      "I.2.7; I.2.8; G.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Wordplay Workshop @ ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.04648v1",
    "published_date": "2024-08-03 13:21:08 UTC",
    "updated_date": "2024-08-03 13:21:08 UTC"
  },
  {
    "arxiv_id": "2408.01774v1",
    "title": "STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios",
    "authors": [
      "Dongyang Xu",
      "Yiran Luo",
      "Tianle Lu",
      "Qingfan Wang",
      "Qing Zhou",
      "Bingbing Nie"
    ],
    "abstract": "Accurate behavior prediction for vehicles is essential but challenging for\nautonomous driving. Most existing studies show satisfying performance under\nregular scenarios, but most neglected safety-critical scenarios. In this study,\na spatio-temporal dual-encoder network named STDA for safety-critical scenarios\nwas developed. Considering the exceptional capabilities of human drivers in\nterms of situational awareness and comprehending risks, driver attention was\nincorporated into STDA to facilitate swift identification of the critical\nregions, which is expected to improve both performance and interpretability.\nSTDA contains four parts: the driver attention prediction module, which\npredicts driver attention; the fusion module designed to fuse the features\nbetween driver attention and raw images; the temporary encoder module used to\nenhance the capability to interpret dynamic scenes; and the behavior prediction\nmodule to predict the behavior. The experiment data are used to train and\nvalidate the model. The results show that STDA improves the G-mean from 0.659\nto 0.719 when incorporating driver attention and adopting a temporal encoder\nmodule. In addition, extensive experimentation has been conducted to validate\nthat the proposed module exhibits robust generalization capabilities and can be\nseamlessly integrated into other mainstream models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01774v1",
    "published_date": "2024-08-03 13:06:04 UTC",
    "updated_date": "2024-08-03 13:06:04 UTC"
  },
  {
    "arxiv_id": "2408.07278v3",
    "title": "Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction",
    "authors": [
      "Wenhao Li",
      "Jie Zhou",
      "Chuan Luo",
      "Chao Tang",
      "Kun Zhang",
      "Shixiong Zhao"
    ],
    "abstract": "In the realm of modern mobile E-commerce, providing users with nearby\ncommercial service recommendations through location-based online services has\nbecome increasingly vital. While machine learning approaches have shown promise\nin multi-scene recommendation, existing methodologies often struggle to address\ncold-start problems in unprecedented scenes: the increasing diversity of\ncommercial choices, along with the short online lifespan of scenes, give rise\nto the complexity of effective recommendations in online and dynamic scenes. In\nthis work, we propose Scene-wise Adaptive Network (SwAN), a novel approach that\nemphasizes high-performance cold-start online recommendations for new scenes.\nOur approach introduces several crucial capabilities, including scene\nsimilarity learning, user-specific scene transition cognition, scene-specific\ninformation construction for the new scene, and enhancing the diverged logical\ninformation between scenes. We demonstrate SwAN's potential to optimize dynamic\nmulti-scene recommendation problems by effectively online handling cold-start\nrecommendations for any newly arrived scenes. More encouragingly, SwAN has been\nsuccessfully deployed in Meituan's online catering recommendation service,\nwhich serves millions of customers per day, and SwAN has achieved a 5.64% CTR\nindex improvement relative to the baselines and a 5.19% increase in daily order\nvolume proportion.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "68T09",
      "I.2.0"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 6 figures, accepted by Recsys 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.07278v3",
    "published_date": "2024-08-03 13:03:31 UTC",
    "updated_date": "2024-08-18 16:45:32 UTC"
  },
  {
    "arxiv_id": "2408.01752v1",
    "title": "Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice Leaf Disease Identification",
    "authors": [
      "Khairun Saddami",
      "Yudha Nurdin",
      "Mutia Zahramita",
      "Muhammad Shahreeza Safiruz"
    ],
    "abstract": "Rice plays a vital role as a primary food source for over half of the world's\npopulation, and its production is critical for global food security.\nNevertheless, rice cultivation is frequently affected by various diseases that\ncan severely decrease yield and quality. Therefore, early and accurate\ndetection of rice diseases is necessary to prevent their spread and minimize\ncrop losses. In this research, we explore three mobile-compatible CNN\narchitectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice\nleaf disease classification. These models are selected due to their\ncompatibility with mobile devices, as they demand less computational power and\nmemory compared to other CNN models. To enhance the performance of the three\nmodels, we added two fully connected layers separated by a dropout layer. We\nused early stop creation to prevent the model from being overfiting. The\nresults of the study showed that the best performance was achieved by the\nEfficientNet-B0 model with an accuracy of 99.8%. Meanwhile, MobileNetV2 and\nShuffleNet only achieved accuracies of 84.21% and 66.51%, respectively. This\nstudy shows that EfficientNet-B0 when combined with the proposed layer and\nearly stop, can produce a high-accuracy model.\n  Keywords: rice leaf detection; green AI; smart agriculture; EfficientNet;",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01752v1",
    "published_date": "2024-08-03 11:16:00 UTC",
    "updated_date": "2024-08-03 11:16:00 UTC"
  },
  {
    "arxiv_id": "2408.01739v1",
    "title": "LAM3D: Leveraging Attention for Monocular 3D Object Detection",
    "authors": [
      "Diana-Alexandra Sas",
      "Leandro Di Bella",
      "Yangxintong Lyu",
      "Florin Oniga",
      "Adrian Munteanu"
    ],
    "abstract": "Since the introduction of the self-attention mechanism and the adoption of\nthe Transformer architecture for Computer Vision tasks, the Vision\nTransformer-based architectures gained a lot of popularity in the field, being\nused for tasks such as image classification, object detection and image\nsegmentation. However, efficiently leveraging the attention mechanism in vision\ntransformers for the Monocular 3D Object Detection task remains an open\nquestion. In this paper, we present LAM3D, a framework that Leverages\nself-Attention mechanism for Monocular 3D object Detection. To do so, the\nproposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as\nfeature extraction backbone and 2D/3D detection machinery. We evaluate the\nproposed method on the KITTI 3D Object Detection Benchmark, proving the\napplicability of the proposed solution in the autonomous driving domain and\noutperforming reference methods. Moreover, due to the usage of self-attention,\nLAM3D is able to systematically outperform the equivalent architecture that\ndoes not employ self-attention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages. Accepted to MMSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01739v1",
    "published_date": "2024-08-03 10:50:07 UTC",
    "updated_date": "2024-08-03 10:50:07 UTC"
  },
  {
    "arxiv_id": "2408.01736v1",
    "title": "Can LLMs predict the convergence of Stochastic Gradient Descent?",
    "authors": [
      "Oussama Zekri",
      "Abdelhakim Benechehab",
      "Ievgen Redko"
    ],
    "abstract": "Large-language models are notoriously famous for their impressive performance\nacross a wide range of tasks. One surprising example of such impressive\nperformance is a recently identified capacity of LLMs to understand the\ngoverning principles of dynamical systems satisfying the Markovian property. In\nthis paper, we seek to explore this direction further by studying the dynamics\nof stochastic gradient descent in convex and non-convex optimization. By\nleveraging the theoretical link between the SGD and Markov chains, we show a\nremarkable zero-shot performance of LLMs in predicting the local minima to\nwhich SGD converges for previously unseen starting points. On a more general\nlevel, we inquire about the possibility of using LLMs to perform zero-shot\nrandomized trials for larger deep learning models used in practice.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages. Accepted to 1st ICML Workshop on In-Context Learning at ICML\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01736v1",
    "published_date": "2024-08-03 10:35:59 UTC",
    "updated_date": "2024-08-03 10:35:59 UTC"
  },
  {
    "arxiv_id": "2408.01732v1",
    "title": "Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation",
    "authors": [
      "Jintao Tan",
      "Xize Cheng",
      "Lingyu Xiong",
      "Lei Zhu",
      "Xiandong Li",
      "Xianjia Wu",
      "Kai Gong",
      "Minglei Li",
      "Yi Cai"
    ],
    "abstract": "Audio-driven talking head generation is a significant and challenging task\napplicable to various fields such as virtual avatars, film production, and\nonline conferences. However, the existing GAN-based models emphasize generating\nwell-synchronized lip shapes but overlook the visual quality of generated\nframes, while diffusion-based models prioritize generating high-quality frames\nbut neglect lip shape matching, resulting in jittery mouth movements. To\naddress the aforementioned problems, we introduce a two-stage diffusion-based\nmodel. The first stage involves generating synchronized facial landmarks based\non the given speech. In the second stage, these generated landmarks serve as a\ncondition in the denoising process, aiming to optimize mouth jitter issues and\ngenerate high-fidelity, well-synchronized, and temporally coherent talking head\nvideos. Extensive experiments demonstrate that our model yields the best\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01732v1",
    "published_date": "2024-08-03 10:19:38 UTC",
    "updated_date": "2024-08-03 10:19:38 UTC"
  },
  {
    "arxiv_id": "2408.01728v2",
    "title": "Survey on Emotion Recognition through Posture Detection and the possibility of its application in Virtual Reality",
    "authors": [
      "Leina Elansary",
      "Zaki Taha",
      "Walaa Gad"
    ],
    "abstract": "A survey is presented focused on using pose estimation techniques in\nEmotional recognition using various technologies normal cameras, and depth\ncameras for real-time, and the potential use of VR and inputs including images,\nvideos, and 3-dimensional poses described in vector space. We discussed 19\nresearch papers collected from selected journals and databases highlighting\ntheir methodology, classification algorithm, and the used datasets that relate\nto emotion recognition and pose estimation. A benchmark has been made according\nto their accuracy as it was the most common performance measurement metric\nused. We concluded that the multimodal Approaches overall made the best\naccuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01728v2",
    "published_date": "2024-08-03 10:01:29 UTC",
    "updated_date": "2024-11-19 13:42:21 UTC"
  },
  {
    "arxiv_id": "2408.01715v1",
    "title": "Joint Universal Adversarial Perturbations with Interpretations",
    "authors": [
      "Liang-bo Ning",
      "Zeyu Dai",
      "Wenqi Fan",
      "Jingran Su",
      "Chao Pan",
      "Luning Wang",
      "Qing Li"
    ],
    "abstract": "Deep neural networks (DNNs) have significantly boosted the performance of\nmany challenging tasks. Despite the great development, DNNs have also exposed\ntheir vulnerability. Recent studies have shown that adversaries can manipulate\nthe predictions of DNNs by adding a universal adversarial perturbation (UAP) to\nbenign samples. On the other hand, increasing efforts have been made to help\nusers understand and explain the inner working of DNNs by highlighting the most\ninformative parts (i.e., attribution maps) of samples with respect to their\npredictions. Moreover, we first empirically find that such attribution maps\nbetween benign and adversarial examples have a significant discrepancy, which\nhas the potential to detect universal adversarial perturbations for defending\nagainst adversarial attacks. This finding motivates us to further investigate a\nnew research problem: whether there exist universal adversarial perturbations\nthat are able to jointly attack DNNs classifier and its interpretation with\nmalicious desires. It is challenging to give an explicit answer since these two\nobjectives are seemingly conflicting. In this paper, we propose a novel\nattacking framework to generate joint universal adversarial perturbations\n(JUAP), which can fool the DNNs model and misguide the inspection from\ninterpreters simultaneously. Comprehensive experiments on various datasets\ndemonstrate the effectiveness of the proposed method JUAP for joint attacks. To\nthe best of our knowledge, this is the first effort to study UAP for jointly\nattacking both DNNs and interpretations.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01715v1",
    "published_date": "2024-08-03 08:58:04 UTC",
    "updated_date": "2024-08-03 08:58:04 UTC"
  },
  {
    "arxiv_id": "2408.01705v1",
    "title": "Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers",
    "authors": [
      "Weijie Zheng",
      "Xingjun Ma",
      "Hanxun Huang",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "With the advancement of vision transformers (ViTs) and self-supervised\nlearning (SSL) techniques, pre-trained large ViTs have become the new\nfoundation models for computer vision applications. However, studies have shown\nthat, like convolutional neural networks (CNNs), ViTs are also susceptible to\nadversarial attacks, where subtle perturbations in the input can fool the model\ninto making false predictions. This paper studies the transferability of such\nan adversarial vulnerability from a pre-trained ViT model to downstream tasks.\nWe focus on \\emph{sample-wise} transfer attacks and propose a novel attack\nmethod termed \\emph{Downstream Transfer Attack (DTA)}. For a given test image,\nDTA leverages a pre-trained ViT model to craft the adversarial example and then\napplies the adversarial example to attack a fine-tuned version of the model on\na downstream dataset. During the attack, DTA identifies and exploits the most\nvulnerable layers of the pre-trained model guided by a cosine similarity loss\nto craft highly transferable attacks. Through extensive experiments with\npre-trained ViTs by 3 distinct pre-training methods, 3 fine-tuning schemes, and\nacross 10 diverse downstream datasets, we show that DTA achieves an average\nattack success rate (ASR) exceeding 90\\%, surpassing existing methods by a huge\nmargin. When used with adversarial training, the adversarial examples generated\nby our DTA can significantly improve the model's robustness to different\ndownstream transfer attacks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01705v1",
    "published_date": "2024-08-03 08:07:03 UTC",
    "updated_date": "2024-08-03 08:07:03 UTC"
  },
  {
    "arxiv_id": "2408.01700v1",
    "title": "Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data",
    "authors": [
      "Antonio De Santis",
      "Marco Balduini",
      "Federico De Santis",
      "Andrea Proia",
      "Arsenio Leo",
      "Marco Brambilla",
      "Emanuele Della Valle"
    ],
    "abstract": "Aerospace manufacturing companies, such as Thales Alenia Space, design,\ndevelop, integrate, verify, and validate products characterized by high\ncomplexity and low volume. They carefully document all phases for each product\nbut analyses across products are challenging due to the heterogeneity and\nunstructured nature of the data in documents. In this paper, we propose a\nhybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with\nLarge Language Models (LLMs) to extract and validate data contained in these\ndocuments. We consider a case study focused on test data related to electronic\nboards for satellites. To do so, we extend the Semantic Sensor Network\nontology. We store the metadata of the reports in a KG, while the actual test\nresults are stored in parquet accessible via a Virtual Knowledge Graph. The\nvalidation process is managed using an LLM-based approach. We also conduct a\nbenchmarking study to evaluate the performance of state-of-the-art LLMs in\nexecuting this task. Finally, we analyze the costs and benefits of automating\npreexisting processes of manual data extraction and validation for subsequent\ncross-report analyses.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper Accepted at ISWC 2024 In-Use Track",
    "pdf_url": "http://arxiv.org/pdf/2408.01700v1",
    "published_date": "2024-08-03 07:42:53 UTC",
    "updated_date": "2024-08-03 07:42:53 UTC"
  },
  {
    "arxiv_id": "2408.01697v2",
    "title": "Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization",
    "authors": [
      "Wenyu Mao",
      "Jiancan Wu",
      "Haoyang Liu",
      "Yongduo Sui",
      "Xiang Wang"
    ],
    "abstract": "Graph out-of-distribution (OOD) generalization remains a major challenge in\ngraph learning since graph neural networks (GNNs) often suffer from severe\nperformance degradation under distribution shifts. Invariant learning, aiming\nto extract invariant features across varied distributions, has recently emerged\nas a promising approach for OOD generation. Despite the great success of\ninvariant learning in OOD problems for Euclidean data (i.e., images), the\nexploration within graph data remains constrained by the complex nature of\ngraphs. Existing studies, such as data augmentation or causal intervention,\neither suffer from disruptions to invariance during the graph manipulation\nprocess or face reliability issues due to a lack of supervised signals for\ncausal parts. In this work, we propose a novel framework, called Invariant\nGraph Learning based on Information bottleneck theory (InfoIGL), to extract the\ninvariant features of graphs and enhance models' generalization ability to\nunseen distributions. Specifically, InfoIGL introduces a redundancy filter to\ncompress task-irrelevant information related to environmental factors.\nCooperating with our designed multi-level contrastive learning, we maximize the\nmutual information among graphs of the same class in the downstream\nclassification tasks, preserving invariant features for prediction to a great\nextent. An appealing feature of InfoIGL is its strong generalization ability\nwithout depending on supervised signal of invariance. Experiments on both\nsynthetic and real-world datasets demonstrate that our method achieves\nstate-of-the-art performance under OOD generalization for graph classification\ntasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-40798-3}",
    "pdf_url": "http://arxiv.org/pdf/2408.01697v2",
    "published_date": "2024-08-03 07:38:04 UTC",
    "updated_date": "2025-02-13 04:10:14 UTC"
  },
  {
    "arxiv_id": "2408.01696v1",
    "title": "Generating High-quality Symbolic Music Using Fine-grained Discriminators",
    "authors": [
      "Zhedong Zhang",
      "Liang Li",
      "Jiehua Zhang",
      "Zhenghui Hu",
      "Hongkui Wang",
      "Chenggang Yan",
      "Jian Yang",
      "Yuankai Qi"
    ],
    "abstract": "Existing symbolic music generation methods usually utilize discriminator to\nimprove the quality of generated music via global perception of music. However,\nconsidering the complexity of information in music, such as rhythm and melody,\na single discriminator cannot fully reflect the differences in these two\nprimary dimensions of music. In this work, we propose to decouple the melody\nand rhythm from music, and design corresponding fine-grained discriminators to\ntackle the aforementioned issues. Specifically, equipped with a pitch\naugmentation strategy, the melody discriminator discerns the melody variations\npresented by the generated samples. By contrast, the rhythm discriminator,\nenhanced with bar-level relative positional encoding, focuses on the velocity\nof generated notes. Such a design allows the generator to be more explicitly\naware of which aspects should be adjusted in the generated music, making it\neasier to mimic human-composed music. Experimental results on the POP909\nbenchmark demonstrate the favorable performance of the proposed method compared\nto several state-of-the-art methods in terms of both objective and subjective\nmetrics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ICPR2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01696v1",
    "published_date": "2024-08-03 07:32:21 UTC",
    "updated_date": "2024-08-03 07:32:21 UTC"
  },
  {
    "arxiv_id": "2408.01691v1",
    "title": "TreeCSS: An Efficient Framework for Vertical Federated Learning",
    "authors": [
      "Qinbo Zhang",
      "Xiao Yan",
      "Yukai Ding",
      "Quanqing Xu",
      "Chuang Hu",
      "Xiaokai Zhou",
      "Jiawei Jiang"
    ],
    "abstract": "Vertical federated learning (VFL) considers the case that the features of\ndata samples are partitioned over different participants. VFL consists of two\nmain steps, i.e., identify the common data samples for all participants\n(alignment) and train model using the aligned data samples (training). However,\nwhen there are many participants and data samples, both alignment and training\nbecome slow. As such, we propose TreeCSS as an efficient VFL framework that\naccelerates the two main steps. In particular, for sample alignment, we design\nan efficient multi-party private set intersection (MPSI) protocol called\nTree-MPSI, which adopts a tree-based structure and a data-volume-aware\nscheduling strategy to parallelize alignment among the participants. As model\ntraining time scales with the number of data samples, we conduct coreset\nselection (CSS) to choose some representative data samples for training. Our\nCCS method adopts a clustering-based scheme for security and generality, which\nfirst clusters the features locally on each participant and then merges the\nlocal clustering results to select representative samples. In addition, we\nweight the samples according to their distances to the centroids to reflect\ntheir importance to model training. We evaluate the effectiveness and\nefficiency of our TreeCSS framework on various datasets and models. The results\nshow that compared with vanilla VFL, TreeCSS accelerates training by up to\n2.93x and achieves comparable model accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.01691v1",
    "published_date": "2024-08-03 07:11:57 UTC",
    "updated_date": "2024-08-03 07:11:57 UTC"
  },
  {
    "arxiv_id": "2408.01690v2",
    "title": "IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection",
    "authors": [
      "Hong Guan",
      "Yancheng Wang",
      "Lulu Xie",
      "Soham Nag",
      "Rajeev Goel",
      "Niranjan Erappa Narayana Swamy",
      "Yingzhen Yang",
      "Chaowei Xiao",
      "Jonathan Prisby",
      "Ross Maciejewski",
      "Jia Zou"
    ],
    "abstract": "Effective fraud detection and analysis of government-issued identity\ndocuments, such as passports, driver's licenses, and identity cards, are\nessential in thwarting identity theft and bolstering security on online\nplatforms. The training of accurate fraud detection and analysis tools depends\non the availability of extensive identity document datasets. However, current\npublicly available benchmark datasets for identity document analysis, including\nMIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a\nlimited number of samples, cover insufficient varieties of fraud patterns, and\nseldom include alterations in critical personal identifying fields like\nportrait images, limiting their utility in training models capable of detecting\nrealistic frauds while preserving privacy.\n  In response to these shortcomings, our research introduces a new benchmark\ndataset, IDNet, designed to advance privacy-preserving fraud detection efforts.\nThe IDNet dataset comprises 837,060 images of synthetically generated identity\ndocuments, totaling approximately 490 gigabytes, categorized into 20 types from\n$10$ U.S. states and 10 European countries. We evaluate the utility and present\nuse cases of the dataset, illustrating how it can aid in training\nprivacy-preserving fraud detection methods, facilitating the generation of\ncamera and video capturing of identity documents, and testing schema\nunification and other identity document management functionalities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "40 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.01690v2",
    "published_date": "2024-08-03 07:05:40 UTC",
    "updated_date": "2024-09-03 22:30:34 UTC"
  },
  {
    "arxiv_id": "2408.01689v3",
    "title": "Controllable Unlearning for Image-to-Image Generative Models via $\\varepsilon$-Constrained Optimization",
    "authors": [
      "Xiaohua Feng",
      "Yuyuan Li",
      "Chaochao Chen",
      "Li Zhang",
      "Longfei Li",
      "Jun Zhou",
      "Xiaolin Zheng"
    ],
    "abstract": "While generative models have made significant advancements in recent years,\nthey also raise concerns such as privacy breaches and biases. Machine\nunlearning has emerged as a viable solution, aiming to remove specific training\ndata, e.g., containing private information and bias, from models. In this\npaper, we study the machine unlearning problem in Image-to-Image (I2I)\ngenerative models. Previous studies mainly treat it as a single objective\noptimization problem, offering a solitary solution, thereby neglecting the\nvaried user expectations towards the trade-off between complete unlearning and\nmodel utility. To address this issue, we propose a controllable unlearning\nframework that uses a control coefficient $\\varepsilon$ to control the\ntrade-off. We reformulate the I2I generative model unlearning problem into a\n$\\varepsilon$-constrained optimization problem and solve it with a\ngradient-based method to find optimal solutions for unlearning boundaries.\nThese boundaries define the valid range for the control coefficient. Within\nthis range, every yielded solution is theoretically guaranteed with Pareto\noptimality. We also analyze the convergence rate of our framework under various\ncontrol functions. Extensive experiments on two benchmark datasets across three\nmainstream I2I models demonstrate the effectiveness of our controllable\nunlearning framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.01689v3",
    "published_date": "2024-08-03 07:04:55 UTC",
    "updated_date": "2025-02-19 03:06:59 UTC"
  },
  {
    "arxiv_id": "2408.02691v1",
    "title": "Symmetric Graph Contrastive Learning against Noisy Views for Recommendation",
    "authors": [
      "Chu Zhao",
      "Enneng Yang",
      "Yuliang Liang",
      "Jianzhe Zhao",
      "Guibing Guo",
      "Xingwei Wang"
    ],
    "abstract": "Graph Contrastive Learning (GCL) leverages data augmentation techniques to\nproduce contrasting views, enhancing the accuracy of recommendation systems\nthrough learning the consistency between contrastive views. However, existing\naugmentation methods, such as directly perturbing interaction graph (e.g.,\nnode/edge dropout), may interfere with the original connections and generate\npoor contrasting views, resulting in sub-optimal performance. In this paper, we\ndefine the views that share only a small amount of information with the\noriginal graph due to poor data augmentation as noisy views (i.e., the last 20%\nof the views with a cosine similarity value less than 0.1 to the original\nview). We demonstrate through detailed experiments that noisy views will\nsignificantly degrade recommendation performance. Further, we propose a\nmodel-agnostic Symmetric Graph Contrastive Learning (SGCL) method with\ntheoretical guarantees to address this issue. Specifically, we introduce\nsymmetry theory into graph contrastive learning, based on which we propose a\nsymmetric form and contrast loss resistant to noisy interference. We provide\ntheoretical proof that our proposed SGCL method has a high tolerance to noisy\nviews. Further demonstration is given by conducting extensive experiments on\nthree real-world datasets. The experimental results demonstrate that our\napproach substantially increases recommendation accuracy, with relative\nimprovements reaching as high as 12.25% over nine other competing models. These\nresults highlight the efficacy of our method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, submitted to TOIS",
    "pdf_url": "http://arxiv.org/pdf/2408.02691v1",
    "published_date": "2024-08-03 06:58:07 UTC",
    "updated_date": "2024-08-03 06:58:07 UTC"
  },
  {
    "arxiv_id": "2408.01672v2",
    "title": "radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG Reconstruction from Millimeter-Wave Radar",
    "authors": [
      "Yuanyuan Zhang",
      "Runwei Guan",
      "Lingxiao Li",
      "Rui Yang",
      "Yutao Yue",
      "Eng Gee Lim"
    ],
    "abstract": "Radar-based contactless cardiac monitoring has become a popular research\ndirection recently, but the fine-grained electrocardiogram (ECG) signal is\nstill hard to reconstruct from millimeter-wave radar signal. The key obstacle\nis to decouple the cardiac activities in the electrical domain (i.e., ECG) from\nthat in the mechanical domain (i.e., heartbeat), and most existing research\nonly uses pure data-driven methods to map such domain transformation as a black\nbox. Therefore, this work first proposes a signal model for domain\ntransformation, and then a novel deep learning framework called radarODE is\ndesigned to fuse the temporal and morphological features extracted from radar\nsignals and generate ECG. In addition, ordinary differential equations are\nembedded in radarODE as a decoder to provide morphological prior, helping the\nconvergence of the model training and improving the robustness under body\nmovements. After being validated on the dataset, the proposed radarODE achieves\nbetter performance compared with the benchmark in terms of missed detection\nrate, root mean square error, Pearson correlation coefficient with the\nimprovement of 9%, 16% and 19%, respectively. The validation results imply that\nradarODE is capable of recovering ECG signals from radar signals with high\nfidelity and can be potentially implemented in real-life scenarios.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01672v2",
    "published_date": "2024-08-03 06:07:15 UTC",
    "updated_date": "2025-05-06 08:29:33 UTC"
  },
  {
    "arxiv_id": "2408.01664v1",
    "title": "SAT3D: Image-driven Semantic Attribute Transfer in 3D",
    "authors": [
      "Zhijun Zhai",
      "Zengmao Wang",
      "Xiaoxiao Long",
      "Kaixuan Zhou",
      "Bo Du"
    ],
    "abstract": "GAN-based image editing task aims at manipulating image attributes in the\nlatent space of generative models. Most of the previous 2D and 3D-aware\napproaches mainly focus on editing attributes in images with ambiguous\nsemantics or regions from a reference image, which fail to achieve photographic\nsemantic attribute transfer, such as the beard from a photo of a man. In this\npaper, we propose an image-driven Semantic Attribute Transfer method in 3D\n(SAT3D) by editing semantic attributes from a reference image. For the proposed\nmethod, the exploration is conducted in the style space of a pre-trained\n3D-aware StyleGAN-based generator by learning the correlations between semantic\nattributes and style code channels. For guidance, we associate each attribute\nwith a set of phrase-based descriptor groups, and develop a Quantitative\nMeasurement Module (QMM) to quantitatively describe the attribute\ncharacteristics in images based on descriptor groups, which leverages the\nimage-text comprehension capability of CLIP. During the training process, the\nQMM is incorporated into attribute losses to calculate attribute similarity\nbetween images, guiding target semantic transferring and irrelevant semantics\npreserving. We present our 3D-aware attribute transfer results across multiple\ndomains and also conduct comparisons with classical 2D image editing methods,\ndemonstrating the effectiveness and customizability of our SAT3D.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01664v1",
    "published_date": "2024-08-03 04:41:46 UTC",
    "updated_date": "2024-08-03 04:41:46 UTC"
  },
  {
    "arxiv_id": "2408.01655v1",
    "title": "Stimulating Imagination: Towards General-purpose Object Rearrangement",
    "authors": [
      "Jianyang Wu",
      "Jie Gu",
      "Xiaokang Ma",
      "Chu Tang",
      "Jingmin Chen"
    ],
    "abstract": "General-purpose object placement is a fundamental capability of an\nintelligent generalist robot, i.e., being capable of rearranging objects\nfollowing human instructions even in novel environments. To achieve this, we\nbreak the rearrangement down into three parts, including object localization,\ngoal imagination and robot control, and propose a framework named SPORT. SPORT\nleverages pre-trained large vision models for broad semantic reasoning about\nobjects, and learns a diffusion-based 3D pose estimator to ensure\nphysically-realistic results. Only object types (to be moved or reference) are\ncommunicated between these two parts, which brings two benefits. One is that we\ncan fully leverage the powerful ability of open-set object localization and\nrecognition since no specific fine-tuning is needed for robotic scenarios.\nFurthermore, the diffusion-based estimator only need to \"imagine\" the poses of\nthe moving and reference objects after the placement, while no necessity for\ntheir semantic information. Thus the training burden is greatly reduced and no\nmassive training is required. The training data for goal pose estimation is\ncollected in simulation and annotated with GPT-4. A set of simulation and\nreal-world experiments demonstrate the potential of our approach to accomplish\ngeneral-purpose object rearrangement, placing various objects following precise\ninstructions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.01655v1",
    "published_date": "2024-08-03 03:53:05 UTC",
    "updated_date": "2024-08-03 03:53:05 UTC"
  },
  {
    "arxiv_id": "2408.05233v1",
    "title": "Large Language Model based Agent Framework for Electric Vehicle Charging Behavior Simulation",
    "authors": [
      "Junkang Feng",
      "Chenggang Cui",
      "Chuanlin Zhang",
      "Zizhu Fan"
    ],
    "abstract": "This paper introduces a new LLM based agent framework for simulating electric\nvehicle (EV) charging behavior, integrating user preferences, psychological\ncharacteristics, and environmental factors to optimize the charging process.\nThe framework comprises several modules, enabling sophisticated, adaptive\nsimulations. Dynamic decision making is supported by continuous reflection and\nmemory updates, ensuring alignment with user expectations and enhanced\nefficiency. The framework's ability to generate personalized user profiles and\nreal-time decisions offers significant advancements for urban EV charging\nmanagement. Future work could focus on incorporating more intricate scenarios\nand expanding data sources to enhance predictive accuracy and practical\nutility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages,3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.05233v1",
    "published_date": "2024-08-03 03:52:05 UTC",
    "updated_date": "2024-08-03 03:52:05 UTC"
  },
  {
    "arxiv_id": "2408.01651v1",
    "title": "Music2P: A Multi-Modal AI-Driven Tool for Simplifying Album Cover Design",
    "authors": [
      "Joong Ho Choi",
      "Geonyeong Choi",
      "Ji-Eun Han",
      "Wonjin Yang",
      "Zhi-Qi Cheng"
    ],
    "abstract": "In today's music industry, album cover design is as crucial as the music\nitself, reflecting the artist's vision and brand. However, many AI-driven album\ncover services require subscriptions or technical expertise, limiting\naccessibility. To address these challenges, we developed Music2P, an\nopen-source, multi-modal AI-driven tool that streamlines album cover creation,\nmaking it efficient, accessible, and cost-effective through Ngrok. Music2P\nautomates the design process using techniques such as Bootstrapping Language\nImage Pre-training (BLIP), music-to-text conversion (LP-music-caps), image\nsegmentation (LoRA), and album cover and QR code generation (ControlNet). This\npaper demonstrates the Music2P interface, details our application of these\ntechnologies, and outlines future improvements. Our ultimate goal is to provide\na tool that empowers musicians and producers, especially those with limited\nresources or expertise, to create compelling album covers.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.HC",
      "H.5.1; H.5.5"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted at CIKM 2024 Demo Paper track. Project available at\n  https://github.com/JC-78/Music2P",
    "pdf_url": "http://arxiv.org/pdf/2408.01651v1",
    "published_date": "2024-08-03 03:30:57 UTC",
    "updated_date": "2024-08-03 03:30:57 UTC"
  },
  {
    "arxiv_id": "2408.01633v1",
    "title": "Self-Emotion Blended Dialogue Generation in Social Simulation Agents",
    "authors": [
      "Qiang Zhang",
      "Jason Naradowsky",
      "Yusuke Miyao"
    ],
    "abstract": "When engaging in conversations, dialogue agents in a virtual simulation\nenvironment may exhibit their own emotional states that are unrelated to the\nimmediate conversational context, a phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects the agents' behaviors in dialogue\nstrategies and decision-making within a large language model (LLM)-driven\nsimulation framework. In a dialogue strategy prediction experiment, we analyze\nthe dialogue strategy choices employed by agents both with and without\nself-emotion, comparing them to those of humans. The results show that\nincorporating self-emotion helps agents exhibit more human-like dialogue\nstrategies. In an independent experiment comparing the performance of models\nfine-tuned on GPT-4 generated dialogue datasets, we demonstrate that\nself-emotion can lead to better overall naturalness and humanness. Finally, in\na virtual simulation environment where agents have discussions on multiple\ntopics, we show that self-emotion of agents can significantly influence the\ndecision-making process of the agents, leading to approximately a 50% change in\ndecisions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "I.2.7; I.2; I.6"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted in SIGDIAL 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01633v1",
    "published_date": "2024-08-03 02:11:48 UTC",
    "updated_date": "2024-08-03 02:11:48 UTC"
  },
  {
    "arxiv_id": "2408.01622v2",
    "title": "Positive-Unlabeled Constraint Learning for Inferring Nonlinear Continuous Constraints Functions from Expert Demonstrations",
    "authors": [
      "Baiyu Peng",
      "Aude Billard"
    ],
    "abstract": "Planning for diverse real-world robotic tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. This paper presents a novel\ntwo-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous constraint function from demonstrations, without requiring prior\nknowledge of the true constraint parameterization or environmental model as\nexisting works. We treat all data in demonstrations as positive (feasible)\ndata, and learn a control policy to generate potentially infeasible\ntrajectories, which serve as unlabeled data. The proposed two-step learning\nframework first identifies reliable infeasible data using a distance metric,\nand secondly learns a binary feasibility classifier (i.e., constraint function)\nfrom the feasible demonstrations and reliable infeasible data. The proposed\nmethod is flexible to learn complex-shaped constraint boundary and will not\nmistakenly classify demonstrations as infeasible as previous methods. The\neffectiveness of the proposed method is verified in four constrained\nenvironments, using a networked policy or a dynamical system policy. It\nsuccessfully infers the continuous nonlinear constraints and outperforms other\nbaseline methods in terms of constraint accuracy and policy safety. This work\nhas been published in IEEE Robotics and Automation Letters (RA-L). Please refer\nto the final version at https://doi.org/10.1109/LRA.2024.3522756",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01622v2",
    "published_date": "2024-08-03 01:09:48 UTC",
    "updated_date": "2025-01-16 10:30:40 UTC"
  },
  {
    "arxiv_id": "2408.01614v2",
    "title": "Advancing Mental Health Pre-Screening: A New Custom GPT for Psychological Distress Assessment",
    "authors": [
      "Jinwen Tang",
      "Yi Shang"
    ],
    "abstract": "This study introduces 'Psycho Analyst', a custom GPT model based on OpenAI's\nGPT-4, optimized for pre-screening mental health disorders. Enhanced with\nDSM-5, PHQ-8, detailed data descriptions, and extensive training data, the\nmodel adeptly decodes nuanced linguistic indicators of mental health disorders.\nIt utilizes a dual-task framework that includes binary classification and a\nthree-stage PHQ-8 score computation involving initial assessment, detailed\nbreakdown, and independent assessment, showcasing refined analytic\ncapabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1\nscores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of\n2.89 and 3.69 in PHQ-8 scoring. These results highlight the model's precision\nand transformative potential in enhancing public mental health support,\nimproving accessibility, cost-effectiveness, and serving as a second opinion\nfor professionals.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted by IEEE CogMI -- IEEE Computer Society, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01614v2",
    "published_date": "2024-08-03 00:38:30 UTC",
    "updated_date": "2024-12-20 19:36:31 UTC"
  }
]
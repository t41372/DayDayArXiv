[
  {
    "arxiv_id": "2403.16327v1",
    "title": "Artificial Neural Microcircuits as Building Blocks: Concept and Challenges",
    "authors": [
      "Andrew Walter",
      "Shimeng Wu",
      "Andy M. Tyrrell",
      "Liam McDaid",
      "Malachy McElholm",
      "Nidhin Thandassery Sumithran",
      "Jim Harkin",
      "Martin A. Trefzer"
    ],
    "abstract": "Artificial Neural Networks (ANNs) are one of the most widely employed forms\nof bio-inspired computation. However the current trend is for ANNs to be\nstructurally homogeneous. Furthermore, this structural homogeneity requires the\napplication of complex training and learning tools that produce application\nspecific ANNs, susceptible to pitfalls such as overfitting. In this paper, an\nnew approach is explored, inspired by the role played in biology by Neural\nMicrocircuits, the so called ``fundamental processing elements'' of organic\nnervous systems. How large neural networks, particularly Spiking Neural\nNetworks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs),\nintended as off-the-shelf components, is articulated; the results of initial\nwork to produce a catalogue of such Microcircuits though the use of Novelty\nSearch is shown; followed by efforts to expand upon this initial work,\nincluding a discussion of challenges uncovered during these efforts and\nexplorations of methods by which they might be overcome.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "12 pages, 31 figures, 3 tables, submitted to A-Life Journal for\n  review",
    "pdf_url": "http://arxiv.org/pdf/2403.16327v1",
    "published_date": "2024-03-24 23:22:02 UTC",
    "updated_date": "2024-03-24 23:22:02 UTC"
  },
  {
    "arxiv_id": "2404.08655v1",
    "title": "Transformer-based Joint Modelling for Automatic Essay Scoring and Off-Topic Detection",
    "authors": [
      "Sourya Dipta Das",
      "Yash Vadi",
      "Kuldeep Yadav"
    ],
    "abstract": "Automated Essay Scoring (AES) systems are widely popular in the market as\nthey constitute a cost-effective and time-effective option for grading systems.\nNevertheless, many studies have demonstrated that the AES system fails to\nassign lower grades to irrelevant responses. Thus, detecting the off-topic\nresponse in automated essay scoring is crucial in practical tasks where\ncandidates write unrelated text responses to the given task in the question. In\nthis paper, we are proposing an unsupervised technique that jointly scores\nessays and detects off-topic essays. The proposed Automated Open Essay Scoring\n(AOES) model uses a novel topic regularization module (TRM), which can be\nattached on top of a transformer model, and is trained using a proposed hybrid\nloss function. After training, the AOES model is further used to calculate the\nMahalanobis distance score for off-topic essay detection. Our proposed method\noutperforms the baseline we created and earlier conventional methods on two\nessay-scoring datasets in off-topic detection as well as on-topic scoring.\nExperimental evaluation results on different adversarial strategies also show\nhow the suggested method is robust for detecting possible human-level\nperturbations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.08655v1",
    "published_date": "2024-03-24 21:44:14 UTC",
    "updated_date": "2024-03-24 21:44:14 UTC"
  },
  {
    "arxiv_id": "2403.16303v4",
    "title": "Large Language Models in Biomedical and Health Informatics: A Review with Bibliometric Analysis",
    "authors": [
      "Huizi Yu",
      "Lizhou Fan",
      "Lingyao Li",
      "Jiayan Zhou",
      "Zihui Ma",
      "Lu Xian",
      "Wenyue Hua",
      "Sijia He",
      "Mingyu Jin",
      "Yongfeng Zhang",
      "Ashvin Gandhi",
      "Xin Ma"
    ],
    "abstract": "Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This study aims to provide a\ncomprehensive overview of LLM applications in BHI, highlighting their\ntransformative potential and addressing the associated ethical and practical\nchallenges. We reviewed 1,698 research articles from January 2022 to December\n2023, categorizing them by research themes and diagnostic categories.\nAdditionally, we conducted network analysis to map scholarly collaborations and\nresearch dynamics. Our findings reveal a substantial increase in the potential\napplications of LLMs to a variety of BHI tasks, including clinical decision\nsupport, patient interaction, and medical document analysis. Notably, LLMs are\nexpected to be instrumental in enhancing the accuracy of diagnostic tools and\npatient care protocols. The network analysis highlights dense and dynamically\nevolving collaborations across institutions, underscoring the interdisciplinary\nnature of LLM research in BHI. A significant trend was the application of LLMs\nin managing specific disease categories such as mental health and neurological\ndisorders, demonstrating their potential to influence personalized medicine and\npublic health strategies. LLMs hold promising potential to further transform\nbiomedical research and healthcare delivery. While promising, the ethical\nimplications and challenges of model validation call for rigorous scrutiny to\noptimize their benefits in clinical settings. This survey serves as a resource\nfor stakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.DL",
    "comment": "62 pages, 9 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.16303v4",
    "published_date": "2024-03-24 21:29:39 UTC",
    "updated_date": "2024-07-28 03:24:37 UTC"
  },
  {
    "arxiv_id": "2403.16293v1",
    "title": "Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling",
    "authors": [
      "Boyang Li",
      "Zhiling Lan",
      "Michael E. Papka"
    ],
    "abstract": "In the field of high-performance computing (HPC), there has been recent\nexploration into the use of deep reinforcement learning for cluster scheduling\n(DRL scheduling), which has demonstrated promising outcomes. However, a\nsignificant challenge arises from the lack of interpretability in deep neural\nnetworks (DNN), rendering them as black-box models to system managers. This\nlack of model interpretability hinders the practical deployment of DRL\nscheduling. In this work, we present a framework called IRL (Interpretable\nReinforcement Learning) to address the issue of interpretability of DRL\nscheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a\ndecision tree by utilizing imitation learning. Unlike DNN, decision tree models\nare non-parametric and easily comprehensible to humans. To extract an effective\nand efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger)\nalgorithm and introduces the notion of critical state to prune the derived\ndecision tree. Through trace-based experiments, we demonstrate that IRL is\ncapable of converting a black-box DNN policy into an interpretable rulebased\ndecision tree while maintaining comparable scheduling performance.\nAdditionally, IRL can contribute to the setting of rewards in DRL scheduling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16293v1",
    "published_date": "2024-03-24 20:56:16 UTC",
    "updated_date": "2024-03-24 20:56:16 UTC"
  },
  {
    "arxiv_id": "2403.16291v3",
    "title": "Guessing human intentions to avoid dangerous situations in caregiving robots",
    "authors": [
      "Noé Zapata",
      "Gerardo Pérez",
      "Lucas Bonilla",
      "Pedro Núñez",
      "Pilar Bachiller",
      "Pablo Bustos"
    ],
    "abstract": "For robots to interact socially, they must interpret human intentions and\nanticipate their potential outcomes accurately. This is particularly important\nfor social robots designed for human care, which may face potentially dangerous\nsituations for people, such as unseen obstacles in their way, that should be\navoided. This paper explores the Artificial Theory of Mind (ATM) approach to\ninferring and interpreting human intentions. We propose an algorithm that\ndetects risky situations for humans, selecting a robot action that removes the\ndanger in real time. We use the simulation-based approach to ATM and adopt the\n'like-me' policy to assign intentions and actions to people. Using this\nstrategy, the robot can detect and act with a high rate of success under\ntime-constrained situations. The algorithm has been implemented as part of an\nexisting robotics cognitive architecture and tested in simulation scenarios.\nThree experiments have been conducted to test the implementation's robustness,\nprecision and real-time response, including a simulated scenario, a\nhuman-in-the-loop hybrid configuration and a real-world scenario.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Associated mpeg file see https://youtu.be/87UEB8P97KY",
    "pdf_url": "http://arxiv.org/pdf/2403.16291v3",
    "published_date": "2024-03-24 20:43:29 UTC",
    "updated_date": "2024-07-09 18:20:06 UTC"
  },
  {
    "arxiv_id": "2403.16289v1",
    "title": "Engineering Safety Requirements for Autonomous Driving with Large Language Models",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Fredrik Törner",
      "Hȧkan Sivencrona",
      "Christian Berger"
    ],
    "abstract": "Changes and updates in the requirement artifacts, which can be frequent in\nthe automotive domain, are a challenge for SafetyOps. Large Language Models\n(LLMs), with their impressive natural language understanding and generating\ncapabilities, can play a key role in automatically refining and decomposing\nrequirements after each update. In this study, we propose a prototype of a\npipeline of prompts and LLMs that receives an item definition and outputs\nsolutions in the form of safety requirements. This pipeline also performs a\nreview of the requirement dataset and identifies redundant or contradictory\nrequirements. We first identified the necessary characteristics for performing\nHARA and then defined tests to assess an LLM's capability in meeting these\ncriteria. We used design science with multiple iterations and let experts from\ndifferent companies evaluate each cycle quantitatively and qualitatively.\nFinally, the prototype was implemented at a case company and the responsible\nteam evaluated its efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in 32nd IEEE International Requirements Engineering 2024\n  conference, Iceland",
    "pdf_url": "http://arxiv.org/pdf/2403.16289v1",
    "published_date": "2024-03-24 20:40:51 UTC",
    "updated_date": "2024-03-24 20:40:51 UTC"
  },
  {
    "arxiv_id": "2403.16276v2",
    "title": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding",
    "authors": [
      "Yunlong Tang",
      "Daiki Shimada",
      "Jing Bi",
      "Mingqian Feng",
      "Hang Hua",
      "Chenliang Xu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16276v2",
    "published_date": "2024-03-24 19:50:49 UTC",
    "updated_date": "2024-08-21 01:15:20 UTC"
  },
  {
    "arxiv_id": "2403.16272v1",
    "title": "L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction",
    "authors": [
      "Rachid Zeghlache",
      "Pierre-Henri Conze",
      "Mostafa El Habib Daho",
      "Yihao Li",
      "Alireza Rezaei",
      "Hugo Le Boité",
      "Ramin Tadayoni",
      "Pascal Massin",
      "Béatrice Cochener",
      "Ikram Brahim",
      "Gwenolé Quellec",
      "Mathieu Lamard"
    ],
    "abstract": "Pre-training strategies based on self-supervised learning (SSL) have proven\nto be effective pretext tasks for many downstream tasks in computer vision. Due\nto the significant disparity between medical and natural images, the\napplication of typical SSL is not straightforward in medical imaging.\nAdditionally, those pretext tasks often lack context, which is critical for\ncomputer-aided clinical decision support. In this paper, we developed a\nlongitudinal masked auto-encoder (MAE) based on the well-known\nTransformer-based MAE. In particular, we explored the importance of time-aware\nposition embedding as well as disease progression-aware masking. Taking into\naccount the time between examinations instead of just scheduling them offers\nthe benefit of capturing temporal changes and trends. The masking strategy, for\nits part, evolves during follow-up to better capture pathological changes,\nensuring a more accurate assessment of disease progression. Using OPHDIAT, a\nlarge follow-up screening dataset targeting diabetic retinopathy (DR), we\nevaluated the pre-trained weights on a longitudinal task, which is to predict\nthe severity label of the next visit within 3 years based on the past time\nseries examinations. Our results demonstrated the relevancy of both time-aware\nposition embedding and masking strategies based on disease progression\nknowledge. Compared to popular baseline models and standard longitudinal\nTransformers, these simple yet effective extensions significantly enhance the\npredictive ability of deep classification models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16272v1",
    "published_date": "2024-03-24 19:34:33 UTC",
    "updated_date": "2024-03-24 19:34:33 UTC"
  },
  {
    "arxiv_id": "2403.16260v2",
    "title": "Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble",
    "authors": [
      "Chenhui Xu",
      "Fuxun Yu",
      "Zirui Xu",
      "Nathan Inkawhich",
      "Xiang Chen"
    ],
    "abstract": "Recent research underscores the pivotal role of the Out-of-Distribution (OOD)\nfeature representation field scale in determining the efficacy of models in OOD\ndetection. Consequently, the adoption of model ensembles has emerged as a\nprominent strategy to augment this feature representation field, capitalizing\non anticipated model diversity.\n  However, our introduction of novel qualitative and quantitative model\nensemble evaluation methods, specifically Loss Basin/Barrier Visualization and\nthe Self-Coupling Index, reveals a critical drawback in existing ensemble\nmethods. We find that these methods incorporate weights that are\naffine-transformable, exhibiting limited variability and thus failing to\nachieve the desired diversity in feature representation.\n  To address this limitation, we elevate the dimensions of traditional model\nensembles, incorporating various factors such as different weight\ninitializations, data holdout, etc., into distinct supervision tasks. This\ninnovative approach, termed Multi-Comprehension (MC) Ensemble, leverages\ndiverse training tasks to generate distinct comprehensions of the data and\nlabels, thereby extending the feature representation field.\n  Our experimental results demonstrate the superior performance of the MC\nEnsemble strategy in OOD detection compared to both the naive Deep Ensemble\nmethod and a standalone model of comparable size. This underscores the\neffectiveness of our proposed approach in enhancing the model's capability to\ndetect instances outside its training distribution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.16260v2",
    "published_date": "2024-03-24 18:43:04 UTC",
    "updated_date": "2024-08-15 21:30:42 UTC"
  },
  {
    "arxiv_id": "2403.16230v1",
    "title": "On machine learning analysis of atomic force microscopy images for image classification, sample surface recognition",
    "authors": [
      "Igor Sokolov"
    ],
    "abstract": "Atomic force microscopy (AFM or SPM) imaging is one of the best matches with\nmachine learning (ML) analysis among microscopy techniques. The digital format\nof AFM images allows for direct utilization in ML algorithms without the need\nfor additional processing. Additionally, AFM enables the simultaneous imaging\nof distributions of over a dozen different physicochemical properties of sample\nsurfaces, a process known as multidimensional imaging. While this wealth of\ninformation can be challenging to analyze using traditional methods, ML\nprovides a seamless approach to this task. However, the relatively slow speed\nof AFM imaging poses a challenge in applying deep learning methods broadly used\nin image recognition. This Prospective is focused on ML\nrecognition/classification when using a relatively small number of AFM images,\nsmall database. We discuss ML methods other than popular deep-learning neural\nnetworks. The described approach has already been successfully used to analyze\nand classify the surfaces of biological cells. It can be applied to recognize\nmedical images, specific material processing, in forensic studies, even to\nidentify the authenticity of arts. A general template for ML analysis specific\nto AFM is suggested, with a specific example of the identification of cell\nphenotype. Special attention is given to the analysis of the statistical\nsignificance of the obtained results, an important feature that is often\noverlooked in papers dealing with machine learning. A simple method for finding\nstatistical significance is also described.",
    "categories": [
      "physics.bio-ph",
      "cs.AI",
      "physics.ins-det",
      "physics.med-ph",
      "92C05, 92C55, 92C10, 92C50",
      "J.2; J.3; I.2; I.5"
    ],
    "primary_category": "physics.bio-ph",
    "comment": "perspective; mini-review; method description; Physical Chemistry\n  Chemical Physics (PCCP) in press, 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.16230v1",
    "published_date": "2024-03-24 16:48:10 UTC",
    "updated_date": "2024-03-24 16:48:10 UTC"
  },
  {
    "arxiv_id": "2403.16222v2",
    "title": "Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization",
    "authors": [
      "Ryan Barron",
      "Maksim E. Eren",
      "Manish Bhattarai",
      "Selma Wanna",
      "Nicholas Solovyev",
      "Kim Rasmussen",
      "Boian S. Alexandrov",
      "Charles Nicholas",
      "Cynthia Matuszek"
    ],
    "abstract": "Much of human knowledge in cybersecurity is encapsulated within the\never-growing volume of scientific papers. As this textual data continues to\nexpand, the importance of document organization methods becomes increasingly\ncrucial for extracting actionable insights hidden within large text datasets.\nKnowledge Graphs (KGs) serve as a means to store factual information in a\nstructured manner, providing explicit, interpretable knowledge that includes\ndomain-specific information from the cybersecurity scientific literature. One\nof the challenges in constructing a KG from scientific literature is the\nextraction of ontology from unstructured text. In this paper, we address this\ntopic and introduce a method for building a multi-modal KG by extracting\nstructured ontology from scientific papers. We demonstrate this concept in the\ncybersecurity domain. One modality of the KG represents observable information\nfrom the papers, such as the categories in which they were published or the\nauthors. The second modality uncovers latent (hidden) patterns of text\nextracted through hierarchical and semantic non-negative matrix factorization\n(NMF), such as named entities, topics or clusters, and keywords. We illustrate\nthis concept by consolidating more than two million scientific papers uploaded\nto arXiv into the cyber-domain, using hierarchical and semantic NMF, and by\nbuilding a cyber-domain-specific KG.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at IEEE ISDFS",
    "pdf_url": "http://arxiv.org/pdf/2403.16222v2",
    "published_date": "2024-03-24 16:30:05 UTC",
    "updated_date": "2024-03-26 15:28:27 UTC"
  },
  {
    "arxiv_id": "2403.16218v4",
    "title": "CoverUp: Effective High Coverage Test Generation for Python",
    "authors": [
      "Juan Altmayer Pizzorno",
      "Emery D. Berger"
    ],
    "abstract": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL",
      "I.2.0; D.2.5"
    ],
    "primary_category": "cs.SE",
    "comment": "21 pages; to appear at FSE'25",
    "pdf_url": "http://arxiv.org/pdf/2403.16218v4",
    "published_date": "2024-03-24 16:18:27 UTC",
    "updated_date": "2025-05-09 14:33:58 UTC"
  },
  {
    "arxiv_id": "2403.16210v2",
    "title": "Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane",
    "authors": [
      "Han Yan",
      "Yang Li",
      "Zhennan Wu",
      "Shenzhou Chen",
      "Weixuan Sun",
      "Taizhang Shang",
      "Weizhe Liu",
      "Tian Chen",
      "Xiaqiang Dai",
      "Chao Ma",
      "Hongdong Li",
      "Pan Ji"
    ],
    "abstract": "We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting. Our project page is\navailable at: https://wolfball.github.io/frankenstein/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "SIGGRAPH Asia 2024 Conference Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.16210v2",
    "published_date": "2024-03-24 16:09:21 UTC",
    "updated_date": "2024-08-30 17:39:50 UTC"
  },
  {
    "arxiv_id": "2403.16209v3",
    "title": "Image Captioning in news report scenario",
    "authors": [
      "Tianrui Liu",
      "Qi Cai",
      "Changxin Xu",
      "Bo Hong",
      "Jize Xiong",
      "Yuxin Qiao",
      "Tsungwei Yang"
    ],
    "abstract": "Image captioning strives to generate pertinent captions for specified images,\nsituating itself at the crossroads of Computer Vision (CV) and Natural Language\nProcessing (NLP). This endeavor is of paramount importance with far-reaching\napplications in recommendation systems, news outlets, social media, and beyond.\nParticularly within the realm of news reporting, captions are expected to\nencompass detailed information, such as the identities of celebrities captured\nin the images. However, much of the existing body of work primarily centers\naround understanding scenes and actions. In this paper, we explore the realm of\nimage captioning specifically tailored for celebrity photographs, illustrating\nits broad potential for enhancing news industry practices. This exploration\naims to augment automated news content generation, thereby facilitating a more\nnuanced dissemination of information. Our endeavor shows a broader horizon,\nenriching the narrative in news reporting through a more intuitive image\ncaptioning framework.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.16209v3",
    "published_date": "2024-03-24 16:08:10 UTC",
    "updated_date": "2024-04-02 01:57:00 UTC"
  },
  {
    "arxiv_id": "2404.05735v1",
    "title": "A Python Framework for Neutrosophic Sets and Mappings",
    "authors": [
      "Giorgio Nordo",
      "Saeid Jafari",
      "Arif Mehmood",
      "Bhimraj Basumatary"
    ],
    "abstract": "In this paper we present an open source framework developed in Python and\nconsisting of three distinct classes designed to manipulate in a simple and\nintuitive way both symbolic representations of neutrosophic sets over universes\nof various types as well as mappings between them. The capabilities offered by\nthis framework extend and generalize previous attempts to provide software\nsolutions to the manipulation of neutrosophic sets such as those proposed by\nSalama et al., Saranya et al., El-Ghareeb, Topal et al. and Sleem. The code is\ndescribed in detail and many examples and use cases are also provided.",
    "categories": [
      "cs.AI",
      "03E72, 08A72, 03B52"
    ],
    "primary_category": "cs.AI",
    "comment": "38 PAGES",
    "pdf_url": "http://arxiv.org/pdf/2404.05735v1",
    "published_date": "2024-03-24 16:00:16 UTC",
    "updated_date": "2024-03-24 16:00:16 UTC"
  },
  {
    "arxiv_id": "2403.16206v3",
    "title": "Rumor Detection with a novel graph neural network approach",
    "authors": [
      "Tianrui Liu",
      "Qi Cai",
      "Changxin Xu",
      "Bo Hong",
      "Fanghao Ni",
      "Yuxin Qiao",
      "Tsungwei Yang"
    ],
    "abstract": "The wide spread of rumors on social media has caused a negative impact on\npeople's daily life, leading to potential panic, fear, and mental health\nproblems for the public. How to debunk rumors as early as possible remains a\nchallenging problem. Existing studies mainly leverage information propagation\nstructure to detect rumors, while very few works focus on correlation among\nusers that they may coordinate to spread rumors in order to gain large\npopularity. In this paper, we propose a new detection model, that jointly\nlearns both the representations of user correlation and information propagation\nto detect rumors on social media. Specifically, we leverage graph neural\nnetworks to learn the representations of user correlation from a bipartite\ngraph that describes the correlations between users and source tweets, and the\nrepresentations of information propagation with a tree structure. Then we\ncombine the learned representations from these two modules to classify the\nrumors. Since malicious users intend to subvert our model after deployment, we\nfurther develop a greedy attack scheme to analyze the cost of three adversarial\nattacks: graph attack, comment attack, and joint attack. Evaluation results on\ntwo public datasets illustrate that the proposed MODEL outperforms the\nstate-of-the-art rumor detection models. We also demonstrate our method\nperforms well for early rumor detection. Moreover, the proposed detection\nmethod is more robust to adversarial attacks compared to the best existing\nmethod. Importantly, we show that it requires a high cost for attackers to\nsubvert user correlation pattern, demonstrating the importance of considering\nuser correlation for rumor detection.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.16206v3",
    "published_date": "2024-03-24 15:59:47 UTC",
    "updated_date": "2024-04-02 01:52:13 UTC"
  },
  {
    "arxiv_id": "2403.16190v1",
    "title": "Logic-based Explanations for Linear Support Vector Classifiers with Reject Option",
    "authors": [
      "Francisco Mateus Rocha Filho",
      "Thiago Alves Rocha",
      "Reginaldo Pereira Fernandes Ribeiro",
      "Ajalmar Rêgo da Rocha Neto"
    ],
    "abstract": "Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model\nfor linear classification problems. It can be used in conjunction with a reject\noption strategy to reject instances that are hard to correctly classify and\ndelegate them to a specialist. This further increases the confidence of the\nmodel. Given this, obtaining an explanation of the cause of rejection is\nimportant to not blindly trust the obtained results. While most of the related\nwork has developed means to give such explanations for machine learning models,\nto the best of our knowledge none have done so for when reject option is\npresent. We propose a logic-based approach with formal guarantees on the\ncorrectness and minimality of explanations for linear SVCs with reject option.\nWe evaluate our approach by comparing it to Anchors, which is a heuristic\nalgorithm for generating explanations. Obtained results show that our proposed\nmethod gives shorter explanations with reduced time cost.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "I.2.4; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, submitted to BRACIS 2023 (Brazilian Conference on\n  Intelligent Systems), accepted version published in Intelligent Systems,\n  LNCS, vol 14195",
    "pdf_url": "http://arxiv.org/pdf/2403.16190v1",
    "published_date": "2024-03-24 15:14:44 UTC",
    "updated_date": "2024-03-24 15:14:44 UTC"
  },
  {
    "arxiv_id": "2404.00042v1",
    "title": "Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis",
    "authors": [
      "Koulik Khamaru"
    ],
    "abstract": "We consider the problem of stochastic convex optimization under convex\nconstraints. We analyze the behavior of a natural variance reduced proximal\ngradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic\nguarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our\nresult is instance-dependent in nature. This means that our guarantee captures\nthe complexity of the loss function, the variability of the noise, and the\ngeometry of the constraint set. We show that the non-asymptotic performance of\nthe VRPG algorithm is governed by the scaled distance (scaled by $\\sqrt{N}$)\nbetween the solutions of the given problem and that of a certain small\nperturbation of the given problem -- both solved under the given convex\nconstraints; here, $N$ denotes the number of samples. Leveraging a\nwell-established connection between local minimax lower bounds and solutions to\nperturbed problems, we show that as $N \\rightarrow \\infty$, the VRPG algorithm\nachieves the renowned local minimax lower bound by H\\`{a}jek and Le Cam up to\nuniversal constants and a logarithmic factor of the sample size.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.00042v1",
    "published_date": "2024-03-24 14:45:11 UTC",
    "updated_date": "2024-03-24 14:45:11 UTC"
  },
  {
    "arxiv_id": "2403.16178v1",
    "title": "Mixed-Initiative Human-Robot Teaming under Suboptimality with Online Bayesian Adaptation",
    "authors": [
      "Manisha Natarajan",
      "Chunyue Xue",
      "Sanne van Waveren",
      "Karen Feigh",
      "Matthew Gombolay"
    ],
    "abstract": "For effective human-agent teaming, robots and other artificial intelligence\n(AI) agents must infer their human partner's abilities and behavioral response\npatterns and adapt accordingly. Most prior works make the unrealistic\nassumption that one or more teammates can act near-optimally. In real-world\ncollaboration, humans and autonomous agents can be suboptimal, especially when\neach only has partial domain knowledge. In this work, we develop computational\nmodeling and optimization techniques for enhancing the performance of\nsuboptimal human-agent teams, where the human and the agent have asymmetric\ncapabilities and act suboptimally due to incomplete environmental knowledge. We\nadopt an online Bayesian approach that enables a robot to infer people's\nwillingness to comply with its assistance in a sequential decision-making game.\nOur user studies show that user preferences and team performance indeed vary\nwith robot intervention styles, and our approach for mixed-initiative\ncollaborations enhances objective team performance ($p<.001$) and subjective\nmeasures, such as user's trust ($p<.001$) and perceived likeability of the\nrobot ($p<.001$).",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 pages for supplementary",
    "pdf_url": "http://arxiv.org/pdf/2403.16178v1",
    "published_date": "2024-03-24 14:38:18 UTC",
    "updated_date": "2024-03-24 14:38:18 UTC"
  },
  {
    "arxiv_id": "2403.18864v1",
    "title": "Interpretable Machine Learning for Weather and Climate Prediction: A Survey",
    "authors": [
      "Ruyi Yang",
      "Jingyu Hu",
      "Zihao Li",
      "Jianli Mu",
      "Tingzhao Yu",
      "Jiangjiang Xia",
      "Xuhong Li",
      "Aritra Dasgupta",
      "Haoyi Xiong"
    ],
    "abstract": "Advanced machine learning models have recently achieved high predictive\naccuracy for weather and climate prediction. However, these complex models\noften lack inherent transparency and interpretability, acting as \"black boxes\"\nthat impede user trust and hinder further model improvements. As such,\ninterpretable machine learning techniques have become crucial in enhancing the\ncredibility and utility of weather and climate modeling. In this survey, we\nreview current interpretable machine learning approaches applied to\nmeteorological predictions. We categorize methods into two major paradigms: 1)\nPost-hoc interpretability techniques that explain pre-trained models, such as\nperturbation-based, game theory based, and gradient-based attribution methods.\n2) Designing inherently interpretable models from scratch using architectures\nlike tree ensembles and explainable neural networks. We summarize how each\ntechnique provides insights into the predictions, uncovering novel\nmeteorological relationships captured by machine learning. Lastly, we discuss\nresearch challenges around achieving deeper mechanistic interpretations aligned\nwith physical principles, developing standardized evaluation benchmarks,\nintegrating interpretability into iterative model development workflows, and\nproviding explainability for large foundation models.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "26 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.18864v1",
    "published_date": "2024-03-24 14:23:35 UTC",
    "updated_date": "2024-03-24 14:23:35 UTC"
  },
  {
    "arxiv_id": "2403.16163v1",
    "title": "An Analytic Solution to Covariance Propagation in Neural Networks",
    "authors": [
      "Oren Wright",
      "Yorie Nakahira",
      "José M. F. Moura"
    ],
    "abstract": "Uncertainty quantification of neural networks is critical to measuring the\nreliability and robustness of deep learning systems. However, this often\ninvolves costly or inaccurate sampling methods and approximations. This paper\npresents a sample-free moment propagation technique that propagates mean\nvectors and covariance matrices across a network to accurately characterize the\ninput-output distributions of neural networks. A key enabler of our technique\nis an analytic solution for the covariance of random variables passed through\nnonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide\napplicability and merits of the proposed technique are shown in experiments\nanalyzing the input-output distributions of trained neural networks and\ntraining Bayesian neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.16163v1",
    "published_date": "2024-03-24 14:08:24 UTC",
    "updated_date": "2024-03-24 14:08:24 UTC"
  },
  {
    "arxiv_id": "2403.16162v1",
    "title": "Multi-Task Learning with Multi-Task Optimization",
    "authors": [
      "Lu Bai",
      "Abhishek Gupta",
      "Yew-Soon Ong"
    ],
    "abstract": "Multi-task learning solves multiple correlated tasks. However, conflicts may\nexist between them. In such circumstances, a single solution can rarely\noptimize all the tasks, leading to performance trade-offs. To arrive at a set\nof optimized yet well-distributed models that collectively embody different\ntrade-offs in one algorithmic pass, this paper proposes to view Pareto\nmulti-task learning through the lens of multi-task optimization. Multi-task\nlearning is first cast as a multi-objective optimization problem, which is then\ndecomposed into a diverse set of unconstrained scalar-valued subproblems. These\nsubproblems are solved jointly using a novel multi-task gradient descent\nmethod, whose uniqueness lies in the iterative transfer of model parameters\namong the subproblems during the course of optimization. A theorem proving\nfaster convergence through the inclusion of such transfers is presented. We\ninvestigate the proposed multi-task learning with multi-task optimization for\nsolving various problem settings including image classification, scene\nunderstanding, and multi-target regression. Comprehensive experiments confirm\nthat the proposed method significantly advances the state-of-the-art in\ndiscovering sets of Pareto-optimized models. Notably, on the large image\ndataset we tested on, namely NYUv2, the hypervolume convergence achieved by our\nmethod was found to be nearly two times faster than the next-best among the\nstate-of-the-art.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16162v1",
    "published_date": "2024-03-24 14:04:40 UTC",
    "updated_date": "2024-03-24 14:04:40 UTC"
  },
  {
    "arxiv_id": "2403.16153v1",
    "title": "One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation",
    "authors": [
      "Yiwei Fu",
      "Weizhong Yan"
    ],
    "abstract": "Accurate and reliable sensor measurements are critical for ensuring the\nsafety and longevity of complex engineering systems such as wind turbines. In\nthis paper, we propose a novel framework for sensor fault detection, isolation,\nand accommodation (FDIA) using masked models and self-supervised learning. Our\nproposed approach is a general time series modeling approach that can be\napplied to any neural network (NN) model capable of sequence modeling, and\ncaptures the complex spatio-temporal relationships among different sensors.\nDuring training, the proposed masked approach creates a random mask, which acts\nlike a fault, for one or more sensors, making the training and inference task\nunified: finding the faulty sensors and correcting them. We validate our\nproposed technique on both a public dataset and a real-world dataset from GE\noffshore wind turbines, and demonstrate its effectiveness in detecting,\ndiagnosing and correcting sensor faults. The masked model not only simplifies\nthe overall FDIA pipeline, but also outperforms existing approaches. Our\nproposed technique has the potential to significantly improve the accuracy and\nreliability of sensor measurements in complex engineering systems in real-time,\nand could be applied to other types of sensors and engineering systems in the\nfuture. We believe that our proposed framework can contribute to the\ndevelopment of more efficient and effective FDIA techniques for a wide range of\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 2024 International Joint Conference on Neural\n  Networks (IJCNN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.16153v1",
    "published_date": "2024-03-24 13:44:57 UTC",
    "updated_date": "2024-03-24 13:44:57 UTC"
  },
  {
    "arxiv_id": "2403.16149v6",
    "title": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a Comprehensive Survey",
    "authors": [
      "Yan Jia",
      "Yuxin Song",
      "Zihou Liu",
      "Qingyin Tan",
      "Yang Song",
      "Yu Zhang",
      "Zheli Liu"
    ],
    "abstract": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16149v6",
    "published_date": "2024-03-24 13:43:43 UTC",
    "updated_date": "2025-05-14 12:39:02 UTC"
  },
  {
    "arxiv_id": "2403.16142v1",
    "title": "What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?",
    "authors": [
      "Richard Johansson"
    ],
    "abstract": "We investigate the behavior of methods that use linear projections to remove\ninformation about a concept from a language representation, and we consider the\nquestion of what happens to a dataset transformed by such a method. A\ntheoretical analysis and experiments on real-world and synthetic data show that\nthese methods inject strong statistical dependencies into the transformed\ndatasets. After applying such a method, the representation space is highly\nstructured: in the transformed space, an instance tends to be located near\ninstances of the opposite label. As a consequence, the original labeling can in\nsome cases be reconstructed by applying an anti-clustering method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16142v1",
    "published_date": "2024-03-24 13:28:27 UTC",
    "updated_date": "2024-03-24 13:28:27 UTC"
  },
  {
    "arxiv_id": "2403.16135v1",
    "title": "Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions",
    "authors": [
      "Linyue Li",
      "Zhijuan Du"
    ],
    "abstract": "In recent years, complementary recommendation has received extensive\nattention in the e-commerce domain. In this paper, we comprehensively summarize\nand compare 34 representative studies conducted between 2009 and 2024. Firstly,\nwe compare the data and methods used for modeling complementary relationships\nbetween products, including simple complementarity and more complex scenarios\nsuch as asymmetric complementarity, the coexistence of substitution and\ncomplementarity relationships between products, and varying degrees of\ncomplementarity between different pairs of products. Next, we classify and\ncompare the models based on the research problems of complementary\nrecommendation, such as diversity, personalization, and cold-start.\nFurthermore, we provide a comparative analysis of experimental results from\ndifferent studies conducted on the same dataset, which helps identify the\nstrengths and weaknesses of the research. Compared to previous surveys, this\npaper provides a more updated and comprehensive summary of the research,\ndiscusses future research directions, and contributes to the advancement of\nthis field.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "20 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.16135v1",
    "published_date": "2024-03-24 13:06:05 UTC",
    "updated_date": "2024-03-24 13:06:05 UTC"
  },
  {
    "arxiv_id": "2403.16133v2",
    "title": "SSHPool: The Separated Subgraph-based Hierarchical Pooling",
    "authors": [
      "Zhuo Xu",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Ziyu Lyu",
      "Hangyuan Du",
      "Lu Bai",
      "Philip S. Yu",
      "Edwin R. Hancock"
    ],
    "abstract": "In this paper, we develop a novel local graph pooling method, namely the\nSeparated Subgraph-based Hierarchical Pooling (SSHPool), for graph\nclassification. We commence by assigning the nodes of a sample graph into\ndifferent clusters, resulting in a family of separated subgraphs. We\nindividually employ the local graph convolution units as the local structure to\nfurther compress each subgraph into a coarsened node, transforming the original\ngraph into a coarsened graph. Since these subgraphs are separated by different\nclusters and the structural information cannot be propagated between them, the\nlocal convolution operation can significantly avoid the over-smoothing problem\ncaused by message passing through edges in most existing Graph Neural Networks\n(GNNs). By hierarchically performing the proposed procedures on the resulting\ncoarsened graph, the proposed SSHPool can effectively extract the hierarchical\nglobal features of the original graph structure, encapsulating rich intrinsic\nstructural characteristics. Furthermore, we develop an end-to-end GNN framework\nassociated with the SSHPool module for graph classification. Experimental\nresults demonstrate the superior performance of the proposed model on\nreal-world datasets.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16133v2",
    "published_date": "2024-03-24 13:03:35 UTC",
    "updated_date": "2024-08-13 16:15:11 UTC"
  },
  {
    "arxiv_id": "2403.16130v2",
    "title": "AKBR: Learning Adaptive Kernel-based Representations for Graph Classification",
    "authors": [
      "Feifei Qian",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Hangyuan Du",
      "Lixiang Xu",
      "Lu Bai",
      "Philip S. Yu",
      "Edwin R. Hancock"
    ],
    "abstract": "In this paper, we propose a new model to learn Adaptive Kernel-based\nRepresentations (AKBR) for graph classification. Unlike state-of-the-art\nR-convolution graph kernels that are defined by merely counting any pair of\nisomorphic substructures between graphs and cannot provide an end-to-end\nlearning mechanism for the classifier, the proposed AKBR approach aims to\ndefine an end-to-end representation learning model to construct an adaptive\nkernel matrix for graphs. To this end, we commence by leveraging a novel\nfeature-channel attention mechanism to capture the interdependencies between\ndifferent substructure invariants of original graphs. The proposed AKBR model\ncan thus effectively identify the structural importance of different\nsubstructures, and compute the R-convolution kernel between pairwise graphs\nassociated with the more significant substructures specified by their\nstructural attentions. Since each row of the resulting kernel matrix can be\ntheoretically seen as the embedding vector of a sample graph, the proposed AKBR\nmodel is able to directly employ the resulting kernel matrix as the graph\nfeature matrix and input it into the classifier for classification (i.e., the\nSoftMax layer), naturally providing an end-to-end learning architecture between\nthe kernel computation as well as the classifier. Experimental results show\nthat the proposed AKBR model outperforms existing state-of-the-art graph\nkernels and deep learning methods on standard graph benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16130v2",
    "published_date": "2024-03-24 13:01:05 UTC",
    "updated_date": "2024-08-13 16:01:15 UTC"
  },
  {
    "arxiv_id": "2403.16127v2",
    "title": "WangchanLion and WangchanX MRC Eval",
    "authors": [
      "Wannaphong Phatthiyaphaibun",
      "Surapon Nonesung",
      "Patomporn Payoungkhamdee",
      "Peerat Limkonchotiwat",
      "Can Udomcharoenchaikit",
      "Jitkapat Sawatphol",
      "Chompakorn Chaksangchaichot",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ],
    "abstract": "This technical report describes the development of WangchanLion, an\ninstruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in\nthe Thai language. Our model is based on SEA-LION and a collection of\ninstruction following datasets. To promote open research and reproducibility,\nwe publicly release all training data, code, and the final model weights under\nthe Apache-2 license. To assess the contextual understanding capability, we\nconducted extensive experimental studies using two Thai MRC datasets, XQuAD and\nIapp_wiki_qa_squad. Experimental results demonstrate the model's ability to\ncomprehend the context and produce an answer faithful to the reference one in\n0-shot and 1-shot settings. In addition, our evaluation goes beyond the\ntraditional MRC. We propose a new evaluation scheme assessing the answer's\ncorrectness, helpfulness, conciseness, and contextuality. Our code is available\npublicly at https://github.com/vistec-AI/WangchanLion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16127v2",
    "published_date": "2024-03-24 12:49:30 UTC",
    "updated_date": "2024-04-23 12:31:30 UTC"
  },
  {
    "arxiv_id": "2403.16116v1",
    "title": "Self-Supervised Multi-Frame Neural Scene Flow",
    "authors": [
      "Dongrui Liu",
      "Daqi Liu",
      "Xueqian Li",
      "Sihao Lin",
      "Hongwei xie",
      "Bing Wang",
      "Xiaojun Chang",
      "Lei Chu"
    ],
    "abstract": "Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown\nremarkable adaptability in the context of large out-of-distribution autonomous\ndriving. Despite their success, the underlying reasons for their astonishing\ngeneralization capabilities remain unclear. Our research addresses this gap by\nexamining the generalization capabilities of NSFP through the lens of uniform\nstability, revealing that its performance is inversely proportional to the\nnumber of input point clouds. This finding sheds light on NSFP's effectiveness\nin handling large-scale point cloud scene flow estimation tasks. Motivated by\nsuch theoretical insights, we further explore the improvement of scene flow\nestimation by leveraging historical point clouds across multiple frames, which\ninherently increases the number of point clouds. Consequently, we propose a\nsimple and effective method for multi-frame point cloud scene flow estimation,\nalong with a theoretical evaluation of its generalization abilities. Our\nanalysis confirms that the proposed method maintains a limited generalization\nerror, suggesting that adding multiple frames to the scene flow optimization\nprocess does not detract from its generalizability. Extensive experimental\nresults on large-scale autonomous driving Waymo Open and Argoverse lidar\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16116v1",
    "published_date": "2024-03-24 12:15:28 UTC",
    "updated_date": "2024-03-24 12:15:28 UTC"
  },
  {
    "arxiv_id": "2403.16112v1",
    "title": "Opportunities and challenges in the application of large artificial intelligence models in radiology",
    "authors": [
      "Liangrui Pan",
      "Zhenyu Zhao",
      "Ying Lu",
      "Kewei Tang",
      "Liyong Fu",
      "Qingchun Liang",
      "Shaoliang Peng"
    ],
    "abstract": "Influenced by ChatGPT, artificial intelligence (AI) large models have\nwitnessed a global upsurge in large model research and development. As people\nenjoy the convenience by this AI large model, more and more large models in\nsubdivided fields are gradually being proposed, especially large models in\nradiology imaging field. This article first introduces the development history\nof large models, technical details, workflow, working principles of multimodal\nlarge models and working principles of video generation large models. Secondly,\nwe summarize the latest research progress of AI large models in radiology\neducation, radiology report generation, applications of unimodal and multimodal\nradiology. Finally, this paper also summarizes some of the challenges of large\nAI models in radiology, with the aim of better promoting the rapid revolution\nin the field of radiography.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16112v1",
    "published_date": "2024-03-24 12:05:23 UTC",
    "updated_date": "2024-03-24 12:05:23 UTC"
  },
  {
    "arxiv_id": "2403.16108v2",
    "title": "A Transformer approach for Electricity Price Forecasting",
    "authors": [
      "Oscar Llorente",
      "Jose Portela"
    ],
    "abstract": "This paper presents a novel approach to electricity price forecasting (EPF)\nusing a pure Transformer model. As opposed to other alternatives, no other\nrecurrent network is used in combination to the attention mechanism. Hence,\nshowing that the attention layer is enough for capturing the temporal patterns.\nThe paper also provides fair comparison of the models using the open-source EPF\ntoolbox and provide the code to enhance reproducibility and transparency in EPF\nresearch. The results show that the Transformer model outperforms traditional\nmethods, offering a promising solution for reliable and sustainable power\nsystem operation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.16108v2",
    "published_date": "2024-03-24 11:52:39 UTC",
    "updated_date": "2024-03-30 23:46:29 UTC"
  },
  {
    "arxiv_id": "2403.16101v3",
    "title": "Public Perceptions of Fairness Metrics Across Borders",
    "authors": [
      "Yuya Sasaki",
      "Sohei Tokuno",
      "Haruka Maeda",
      "Kazuki Nakajima",
      "Osamu Sakura",
      "George Fletcher",
      "Mykola Pechenizkiy",
      "Panagiotis Karras",
      "Irina Shklovski"
    ],
    "abstract": "Which fairness metrics are appropriately applicable in your contexts? There\nmay be instances of discordance regarding the perception of fairness, even when\nthe outcomes comply with established fairness metrics. Several\nquestionnaire-based surveys have been conducted to evaluate fairness metrics\nwith human perceptions of fairness. However, these surveys were limited in\nscope, including only a few hundred participants within a single country. In\nthis study, we conduct an international survey to evaluate public perceptions\nof various fairness metrics in decision-making scenarios. We collected\nresponses from 1,000 participants in each of China, France, Japan, and the\nUnited States, amassing a total of 4,000 participants, to analyze the\npreferences of fairness metrics. Our survey consists of three distinct\nscenarios paired with four fairness metrics. This investigation explores the\nrelationship between personal attributes and the choice of fairness metrics,\nuncovering a significant influence of national context on these preferences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16101v3",
    "published_date": "2024-03-24 11:33:18 UTC",
    "updated_date": "2025-05-08 12:50:03 UTC"
  },
  {
    "arxiv_id": "2403.16100v1",
    "title": "Specifying Agent Ethics (Blue Sky Ideas)",
    "authors": [
      "Louise A. Dennis",
      "Michael Fisher"
    ],
    "abstract": "We consider the question of what properties a Machine Ethics system should\nhave. This question is complicated by the existence of ethical dilemmas with no\nagreed upon solution. We provide an example to motivate why we do not believe\nfalling back on the elicitation of values from stakeholders is sufficient to\nguarantee correctness of such systems. We go on to define two broad categories\nof ethical property that have arisen in our own work and present a challenge to\nthe community to approach this question in a more systematic way.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in Coordination, Organizations, Institutions, Norms and\n  Ethics for Governance of Multi-Agent Systems 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.16100v1",
    "published_date": "2024-03-24 11:32:43 UTC",
    "updated_date": "2024-03-24 11:32:43 UTC"
  },
  {
    "arxiv_id": "2403.16097v2",
    "title": "Can Language Models Pretend Solvers? Logic Code Simulation with LLMs",
    "authors": [
      "Minyu Chen",
      "Guoqiang Li",
      "Ling-I Wu",
      "Ruibang Liu",
      "Yuxin Su",
      "Xi Chang",
      "Jianxin Xue"
    ],
    "abstract": "Transformer-based large language models (LLMs) have demonstrated significant\npotential in addressing logic problems. capitalizing on the great capabilities\nof LLMs for code-related activities, several frameworks leveraging logical\nsolvers for logic reasoning have been proposed recently. While existing\nresearch predominantly focuses on viewing LLMs as natural language logic\nsolvers or translators, their roles as logic code interpreters and executors\nhave received limited attention. This study delves into a novel aspect, namely\nlogic code simulation, which forces LLMs to emulate logical solvers in\npredicting the results of logical programs. To further investigate this novel\ntask, we formulate our three research questions: Can LLMs efficiently simulate\nthe outputs of logic codes? What strength arises along with logic code\nsimulation? And what pitfalls? To address these inquiries, we curate three\nnovel datasets tailored for the logic code simulation task and undertake\nthorough experiments to establish the baseline performance of LLMs in code\nsimulation. Subsequently, we introduce a pioneering LLM-based code simulation\ntechnique, Dual Chains of Logic (DCoL). This technique advocates a dual-path\nthinking approach for LLMs, which has demonstrated state-of-the-art performance\ncompared to other LLM prompt strategies, achieving a notable improvement in\naccuracy by 7.06% with GPT-4-Turbo.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.16097v2",
    "published_date": "2024-03-24 11:27:16 UTC",
    "updated_date": "2024-03-28 06:56:47 UTC"
  },
  {
    "arxiv_id": "2403.16081v4",
    "title": "The Interplay of Learning, Analytics, and Artificial Intelligence in Education: A Vision for Hybrid Intelligence",
    "authors": [
      "Mutlu Cukurova"
    ],
    "abstract": "This paper presents a multi-dimensional view of AI's role in learning and\neducation, emphasizing the intricate interplay between AI, analytics, and the\nlearning processes. Here, I challenge the prevalent narrow conceptualisation of\nAI as tools, as exemplified in generative AI tools, and argue for the\nimportance of alternative conceptualisations of AI for achieving human-AI\nhybrid intelligence. I highlight the differences between human intelligence and\nartificial information processing, the importance of hybrid human-AI systems to\nextend human cognition, and posit that AI can also serve as an instrument for\nunderstanding human learning. Early learning sciences and AI in Education\nresearch (AIED), which saw AI as an analogy for human intelligence, have\ndiverged from this perspective, prompting a need to rekindle this connection.\nThe paper presents three unique conceptualisations of AI: the externalization\nof human cognition, the internalization of AI models to influence human mental\nmodels, and the extension of human cognition via tightly coupled human-AI\nhybrid intelligence systems. Examples from current research and practice are\nexamined as instances of the three conceptualisations in education,\nhighlighting the potential value and limitations of each conceptualisation for\neducation, as well as the perils of overemphasis on externalising human\ncognition. The paper concludes with advocacy for a broader approach to AIED\nthat goes beyond considerations on the design and development of AI, but also\nincludes educating people about AI and innovating educational systems to remain\nrelevant in an AI-ubiquitous world.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "20 pages, 7 figures, this paper is based on the keynote talk given by\n  the author at the ACM International Conference on Learning Analytics &\n  Knowledge (LAK) 2024 in Kyoto, Japan.\n  https://www.solaresearch.org/events/lak/lak24/keynotes/",
    "pdf_url": "http://arxiv.org/pdf/2403.16081v4",
    "published_date": "2024-03-24 10:07:46 UTC",
    "updated_date": "2024-07-08 13:38:27 UTC"
  },
  {
    "arxiv_id": "2403.16071v2",
    "title": "Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization",
    "authors": [
      "Linzhi Wu",
      "Xingyu Zhang",
      "Yakun Zhang",
      "Changyan Zheng",
      "Tiejun Liu",
      "Liang Xie",
      "Ye Yan",
      "Erwei Yin"
    ],
    "abstract": "Lip reading, the process of interpreting silent speech from visual lip\nmovements, has gained rising attention for its wide range of realistic\napplications. Deep learning approaches greatly improve current lip reading\nsystems. However, lip reading in cross-speaker scenarios where the speaker\nidentity changes, poses a challenging problem due to inter-speaker variability.\nA well-trained lip reading system may perform poorly when handling a brand new\nspeaker. To learn a speaker-robust lip reading model, a key insight is to\nreduce visual variations across speakers, avoiding the model overfitting to\nspecific speakers. In this work, in view of both input visual clues and latent\nrepresentations based on a hybrid CTC/attention architecture, we propose to\nexploit the lip landmark-guided fine-grained visual clues instead of\nfrequently-used mouth-cropped images as input features, diminishing\nspeaker-specific appearance characteristics. Furthermore, a max-min mutual\ninformation regularization approach is proposed to capture speaker-insensitive\nlatent representations. Experimental evaluations on public lip reading datasets\ndemonstrate the effectiveness of the proposed approach under the intra-speaker\nand inter-speaker conditions.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.16071v2",
    "published_date": "2024-03-24 09:18:21 UTC",
    "updated_date": "2024-05-02 08:53:35 UTC"
  },
  {
    "arxiv_id": "2403.16067v5",
    "title": "Adversarial Guided Diffusion Models for Adversarial Purification",
    "authors": [
      "Guang Lin",
      "Zerui Tao",
      "Jianhai Zhang",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ],
    "abstract": "Diffusion model (DM) based adversarial purification (AP) has proven to be a\npowerful defense method that can remove adversarial perturbations and generate\na purified example without threats. In principle, the pre-trained DMs can only\nensure that purified examples conform to the same distribution of the training\ndata, but it may inadvertently compromise the semantic information of input\nexamples, leading to misclassification of purified examples. Recent\nadvancements introduce guided diffusion techniques to preserve semantic\ninformation while removing the perturbations. However, these guidances often\nrely on distance measures between purified examples and diffused examples,\nwhich can also preserve perturbations in purified examples. To further unleash\nthe robustness power of DM-based AP, we propose an adversarial guided diffusion\nmodel (AGDM) by introducing a novel adversarial guidance that contains\nsufficient semantic information but does not explicitly involve adversarial\nperturbations. The guidance is modeled by an auxiliary neural network obtained\nwith adversarial training, considering the distance in the latent\nrepresentations rather than at the pixel-level values. Extensive experiments\nare conducted on CIFAR-10, CIFAR-100 and ImageNet to demonstrate that our\nmethod is effective for simultaneously maintaining semantic information and\nremoving the adversarial perturbations. In addition, comprehensive comparisons\nshow that our method significantly enhances the robustness of existing DM-based\nAP, with an average robust accuracy improved by up to 7.30% on CIFAR-10.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16067v5",
    "published_date": "2024-03-24 08:34:08 UTC",
    "updated_date": "2025-03-11 08:43:03 UTC"
  },
  {
    "arxiv_id": "2403.16066v2",
    "title": "A Temporal Graph Network Framework for Dynamic Recommendation",
    "authors": [
      "Yejin Kim",
      "Youngbin Lee",
      "Vincent Yuan",
      "Annika Lee",
      "Yongjae Lee"
    ],
    "abstract": "Recommender systems, crucial for user engagement on platforms like e-commerce\nand streaming services, often lag behind users' evolving preferences due to\nstatic data reliance. After Temporal Graph Networks (TGNs) were proposed,\nvarious studies have shown that TGN can significantly improve situations where\nthe features of nodes and edges dynamically change over time. However, despite\nits promising capabilities, it has not been directly applied in recommender\nsystems to date. Our study bridges this gap by directly implementing Temporal\nGraph Networks (TGN) in recommender systems, a first in this field. Using\nreal-world datasets and a range of graph and history embedding methods, we show\nTGN's adaptability, confirming its effectiveness in dynamic recommendation\nscenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at the AAAI 2024 Workshop on Recommendation Ecosystems:\n  Modeling, Optimization and Incentive Design\n  (https://sites.google.com/view/recommender-ecosystems/home)",
    "pdf_url": "http://arxiv.org/pdf/2403.16066v2",
    "published_date": "2024-03-24 08:33:13 UTC",
    "updated_date": "2024-12-21 10:42:36 UTC"
  },
  {
    "arxiv_id": "2403.16056v3",
    "title": "Qibo: A Large Language Model for Traditional Chinese Medicine",
    "authors": [
      "Heyi Zhang",
      "Xin Wang",
      "Zhaopeng Meng",
      "Zhe Chen",
      "Pengwei Zhuang",
      "Yongzhe Jia",
      "Dawei Xu",
      "Wenbin Guo"
    ],
    "abstract": "Large Language Models (LLMs) has made significant progress in a number of\nprofessional fields, including medicine, law, and finance. However, in\ntraditional Chinese medicine (TCM), there are challenges such as the essential\ndifferences between theory and modern medicine, the lack of specialized corpus\nresources, and the fact that relying only on supervised fine-tuning may lead to\noverconfident predictions. To address these challenges, we propose a two-stage\ntraining approach that combines continuous pre-training and supervised\nfine-tuning. A notable contribution of our study is the processing of a 2GB\ncorpus dedicated to TCM, constructing pre-training and instruction fine-tuning\ndatasets for TCM, respectively. In addition, we have developed Qibo-Benchmark,\na tool that evaluates the performance of LLM in the TCM on multiple dimensions,\nincluding subjective, objective, and three TCM NLP tasks. The medical LLM\ntrained with our pipeline, named $\\textbf{Qibo}$, exhibits significant\nperformance boosts. Compared to the baselines, the average subjective win rate\nis 63%, the average objective accuracy improved by 23% to 58%, and the Rouge-L\nscores for the three TCM NLP tasks are 0.72, 0.61, and 0.55. Finally, we\npropose a pipline to apply Qibo to TCM consultation and demonstrate the model\nperformance through the case study.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16056v3",
    "published_date": "2024-03-24 07:48:05 UTC",
    "updated_date": "2024-06-22 05:43:53 UTC"
  },
  {
    "arxiv_id": "2403.16043v1",
    "title": "Semantic Is Enough: Only Semantic Information For NeRF Reconstruction",
    "authors": [
      "Ruibo Wang",
      "Song Zhang",
      "Ping Huang",
      "Donghai Zhang",
      "Wei Yan"
    ],
    "abstract": "Recent research that combines implicit 3D representation with semantic\ninformation, like Semantic-NeRF, has proven that NeRF model could perform\nexcellently in rendering 3D structures with semantic labels. This research aims\nto extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing\nsolely on semantic output and removing the RGB output component. We reformulate\nthe model and its training procedure to leverage only the cross-entropy loss\nbetween the model semantic output and the ground truth semantic images,\nremoving the colour data traditionally used in the original Semantic-NeRF\napproach. We then conduct a series of identical experiments using the original\nand the modified Semantic-NeRF model. Our primary objective is to obverse the\nimpact of this modification on the model performance by Semantic-NeRF, focusing\non tasks such as scene understanding, object detection, and segmentation. The\nresults offer valuable insights into the new way of rendering the scenes and\nprovide an avenue for further research and development in semantic-focused 3D\nscene understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16043v1",
    "published_date": "2024-03-24 07:04:08 UTC",
    "updated_date": "2024-03-24 07:04:08 UTC"
  },
  {
    "arxiv_id": "2403.16023v2",
    "title": "RPMArt: Towards Robust Perception and Manipulation for Articulated Objects",
    "authors": [
      "Junbo Wang",
      "Wenhai Liu",
      "Qiaojun Yu",
      "Yang You",
      "Liu Liu",
      "Weiming Wang",
      "Cewu Lu"
    ],
    "abstract": "Articulated objects are commonly found in daily life. It is essential that\nrobots can exhibit robust perception and manipulation skills for articulated\nobjects in real-world robotic applications. However, existing methods for\narticulated objects insufficiently address noise in point clouds and struggle\nto bridge the gap between simulation and reality, thus limiting the practical\ndeployment in real-world scenarios. To tackle these challenges, we propose a\nframework towards Robust Perception and Manipulation for Articulated Objects\n(RPMArt), which learns to estimate the articulation parameters and manipulate\nthe articulation part from the noisy point cloud. Our primary contribution is a\nRobust Articulation Network (RoArtNet) that is able to predict both joint\nparameters and affordable points robustly by local feature learning and point\ntuple voting. Moreover, we introduce an articulation-aware classification\nscheme to enhance its ability for sim-to-real transfer. Finally, with the\nestimated affordable point and articulation joint constraint, the robot can\ngenerate robust actions to manipulate articulated objects. After learning only\nfrom synthetic data, RPMArt is able to transfer zero-shot to real-world\narticulated objects. Experimental results confirm our approach's effectiveness,\nwith our framework achieving state-of-the-art performance in both noise-added\nsimulation and real-world environments. Code, data and more results can be\nfound on the project website at https://r-pmart.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures, accepted by 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024), project website at\n  https://r-pmart.github.io",
    "pdf_url": "http://arxiv.org/pdf/2403.16023v2",
    "published_date": "2024-03-24 05:55:39 UTC",
    "updated_date": "2024-09-28 04:13:16 UTC"
  },
  {
    "arxiv_id": "2403.16016v1",
    "title": "Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)",
    "authors": [
      "Eyoel Gebre",
      "Krishna Saxena",
      "Timothy Tran"
    ],
    "abstract": "Image inpainting is the process of taking an image and generating lost or\nintentionally occluded portions. Inpainting has countless applications\nincluding restoring previously damaged pictures, restoring the quality of\nimages that have been degraded due to compression, and removing unwanted\nobjects/text. Modern inpainting techniques have shown remarkable ability in\ngenerating sensible completions for images with mask occlusions. In our paper,\nan overview of the progress of inpainting techniques will be provided, along\nwith identifying current leading approaches, focusing on their strengths and\nweaknesses. A critical gap in these existing models will be addressed, focusing\non the ability to prompt and control what exactly is generated. We will\nadditionally justify why we think this is the natural next progressive step\nthat inpainting models must take, and provide multiple approaches to\nimplementing this functionality. Finally, we will evaluate the results of our\napproaches by qualitatively checking whether they generate high-quality images\nthat correctly inpaint regions with the objects that they are instructed to\nproduce.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16016v1",
    "published_date": "2024-03-24 05:26:55 UTC",
    "updated_date": "2024-03-24 05:26:55 UTC"
  },
  {
    "arxiv_id": "2403.16004v1",
    "title": "A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures",
    "authors": [
      "Hao Song",
      "Jiacheng Yao",
      "Zhengxi Li",
      "Shaocong Xu",
      "Shibo Jin",
      "Jiajun Zhou",
      "Chenbo Fu",
      "Qi Xuan",
      "Shanqing Yu"
    ],
    "abstract": "Over the past few years, federated learning has become widely used in various\nclassical machine learning fields because of its collaborative ability to train\ndata from multiple sources without compromising privacy. However, in the area\nof graph neural networks, the nodes and network structures of graphs held by\nclients are different in many practical applications, and the aggregation\nmethod that directly shares model gradients cannot be directly applied to this\nscenario. Therefore, this work proposes a federated aggregation method FLGNN\napplied to various graph federation scenarios and investigates the aggregation\neffect of parameter sharing at each layer of the graph neural network model.\nThe effectiveness of the federated aggregation method FLGNN is verified by\nexperiments on real datasets. Additionally, for the privacy security of FLGNN,\nthis paper designs membership inference attack experiments and differential\nprivacy defense experiments. The results show that FLGNN performs good\nrobustness, and the success rate of privacy theft is further reduced by adding\ndifferential privacy defense methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.16004v1",
    "published_date": "2024-03-24 04:23:43 UTC",
    "updated_date": "2024-03-24 04:23:43 UTC"
  },
  {
    "arxiv_id": "2403.16003v2",
    "title": "Diverse Representation Embedding for Lifelong Person Re-Identification",
    "authors": [
      "Shiben Liu",
      "Huijie Fan",
      "Qiang Wang",
      "Xiai Chen",
      "Zhi Han",
      "Yandong Tang"
    ],
    "abstract": "Lifelong Person Re-Identification (LReID) aims to continuously learn from\nsuccessive data streams, matching individuals across multiple cameras. The key\nchallenge for LReID is how to effectively preserve old knowledge while\nincrementally learning new information, which is caused by task-level domain\ngaps and limited old task datasets. Existing methods based on CNN backbone are\ninsufficient to explore the representation of each instance from different\nperspectives, limiting model performance on limited old task datasets and new\ntask datasets. Unlike these methods, we propose a Diverse Representations\nEmbedding (DRE) framework that first explores a pure transformer for LReID. The\nproposed DRE preserves old knowledge while adapting to new information based on\ninstance-level and task-level layout. Concretely, an Adaptive Constraint Module\n(ACM) is proposed to implement integration and push away operations between\nmultiple overlapping representations generated by transformer-based backbone,\nobtaining rich and discriminative representations for each instance to improve\nadaptive ability of LReID. Based on the processed diverse representations, we\npropose Knowledge Update (KU) and Knowledge Preservation (KP) strategies at the\ntask-level layout by introducing the adjustment model and the learner model. KU\nstrategy enhances the adaptive learning ability of learner models for new\ninformation under the adjustment model prior, and KP strategy preserves old\nknowledge operated by representation-level alignment and logit-level\nsupervision in limited old task datasets while guaranteeing the adaptive\nlearning information capacity of the LReID model. Compared to state-of-the-art\nmethods, our method achieves significantly improved performance in holistic,\nlarge-scale, and occluded datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages,7 Tables,3 Figures",
    "pdf_url": "http://arxiv.org/pdf/2403.16003v2",
    "published_date": "2024-03-24 04:22:37 UTC",
    "updated_date": "2024-04-02 13:31:41 UTC"
  },
  {
    "arxiv_id": "2403.15994v1",
    "title": "Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting",
    "authors": [
      "Yicheng Deng",
      "Hideaki Hayashi",
      "Hajime Nagahara"
    ],
    "abstract": "Facial expression spotting is a significant but challenging task in facial\nexpression analysis. The accuracy of expression spotting is affected not only\nby irrelevant facial movements but also by the difficulty of perceiving subtle\nmotions in micro-expressions. In this paper, we propose a Multi-Scale\nSpatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression\nspotting. To extract more robust motion features, we track both short- and\nlong-term motion of facial muscles in compact sliding windows whose window\nlength adapts to the temporal receptive field of the network. This strategy,\ntermed the receptive field adaptive sliding window strategy, effectively\nmagnifies the motion features while alleviating the problem of severe head\nmovement. The subtle motion features are then converted to a facial graph\nrepresentation, whose spatio-temporal graph patterns are learned by a graph\nconvolutional network. This network learns both local and global features from\nmultiple scales of facial graph structures using our proposed facial local\ngraph pooling (FLGP). Furthermore, we introduce supervised contrastive learning\nto enhance the discriminative capability of our model for difficult-to-classify\nframes. The experimental results on the SAMM-LV and CAS(ME)^2 datasets\ndemonstrate that our method achieves state-of-the-art performance, particularly\nin micro-expression spotting. Ablation studies further verify the effectiveness\nof our proposed modules.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by FG2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15994v1",
    "published_date": "2024-03-24 03:10:39 UTC",
    "updated_date": "2024-03-24 03:10:39 UTC"
  },
  {
    "arxiv_id": "2403.15989v2",
    "title": "Knowledge-guided Machine Learning: Current Trends and Future Prospects",
    "authors": [
      "Anuj Karpatne",
      "Xiaowei Jia",
      "Vipin Kumar"
    ],
    "abstract": "This paper presents an overview of scientific modeling and discusses the\ncomplementary strengths and weaknesses of ML methods for scientific modeling in\ncomparison to process-based models. It also provides an introduction to the\ncurrent state of research in the emerging field of scientific knowledge-guided\nmachine learning (KGML) that aims to use both scientific knowledge and data in\nML frameworks to achieve better generalizability, scientific consistency, and\nexplainability of results. We discuss different facets of KGML research in\nterms of the type of scientific knowledge used, the form of knowledge-ML\nintegration explored, and the method for incorporating scientific knowledge in\nML. We also discuss some of the common categories of use cases in environmental\nsciences where KGML methods are being developed, using illustrative examples in\neach category.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15989v2",
    "published_date": "2024-03-24 02:54:46 UTC",
    "updated_date": "2024-05-01 20:57:12 UTC"
  },
  {
    "arxiv_id": "2404.00039v1",
    "title": "MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems",
    "authors": [
      "Flavio Ponzina",
      "Tajana Rosing"
    ],
    "abstract": "Hyperdimensional computing (HDC) is emerging as a promising AI approach that\ncan effectively target TinyML applications thanks to its lightweight computing\nand memory requirements. Previous works on HDC showed that limiting the\nstandard 10k dimensions of the hyperdimensional space to much lower values is\npossible, reducing even more HDC resource requirements. Similarly, other\nstudies demonstrated that binary values can be used as elements of the\ngenerated hypervectors, leading to significant efficiency gains at the cost of\nsome degree of accuracy degradation. Nevertheless, current optimization\nattempts do not concurrently co-optimize HDC hyper-parameters, and accuracy\ndegradation is not directly controlled, resulting in sub-optimal HDC models\nproviding several applications with unacceptable output qualities. In this\nwork, we propose MicroHD, a novel accuracy-driven HDC optimization approach\nthat iteratively tunes HDC hyper-parameters, reducing memory and computing\nrequirements while ensuring user-defined accuracy levels. The proposed method\ncan be applied to HDC implementations using different encoding functions,\ndemonstrates good scalability for larger HDC workloads, and achieves\ncompression and efficiency gains up to 200x when compared to baseline\nimplementations for accuracy degradations lower than 1%.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "math.OC"
    ],
    "primary_category": "cs.PF",
    "comment": "Accepted as a full paper by the tinyML Research Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00039v1",
    "published_date": "2024-03-24 02:45:34 UTC",
    "updated_date": "2024-03-24 02:45:34 UTC"
  },
  {
    "arxiv_id": "2403.15977v3",
    "title": "Towards Two-Stream Foveation-based Active Vision Learning",
    "authors": [
      "Timur Ibrayev",
      "Amitangshu Mukherjee",
      "Sai Aparna Aketi",
      "Kaushik Roy"
    ],
    "abstract": "Deep neural network (DNN) based machine perception frameworks process the\nentire input in a one-shot manner to provide answers to both \"what object is\nbeing observed\" and \"where it is located\". In contrast, the \"two-stream\nhypothesis\" from neuroscience explains the neural processing in the human\nvisual cortex as an active vision system that utilizes two separate regions of\nthe brain to answer the what and the where questions. In this work, we propose\na machine learning framework inspired by the \"two-stream hypothesis\" and\nexplore the potential benefits that it offers. Specifically, the proposed\nframework models the following mechanisms: 1) ventral (what) stream focusing on\nthe input regions perceived by the fovea part of an eye (foveation), 2) dorsal\n(where) stream providing visual guidance, and 3) iterative processing of the\ntwo streams to calibrate visual focus and process the sequence of focused image\npatches. The training of the proposed framework is accomplished by label-based\nDNN training for the ventral stream model and reinforcement learning for the\ndorsal stream model. We show that the two-stream foveation-based learning is\napplicable to the challenging task of weakly-supervised object localization\n(WSOL), where the training data is limited to the object class or its\nattributes. The framework is capable of both predicting the properties of an\nobject and successfully localizing it by predicting its bounding box. We also\nshow that, due to the independent nature of the two streams, the dorsal model\ncan be applied on its own to unseen images to localize objects from different\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted version of the article, 18 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15977v3",
    "published_date": "2024-03-24 01:20:08 UTC",
    "updated_date": "2024-04-20 20:19:11 UTC"
  },
  {
    "arxiv_id": "2403.15974v1",
    "title": "CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data",
    "authors": [
      "Shreya Sharma",
      "Dana Hughes",
      "Katia Sycara"
    ],
    "abstract": "This paper describes CBGT-Net, a neural network model inspired by the\ncortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains.\nUnlike traditional neural network models, which either generate an output for\neach provided input, or an output after a fixed sequence of inputs, the\nCBGT-Net learns to produce an output after a sufficient criteria for evidence\nis achieved from a stream of observed data. For each observation, the CBGT-Net\ngenerates a vector that explicitly represents the amount of evidence the\nobservation provides for each potential decision, accumulates the evidence over\ntime, and generates a decision when the accumulated evidence exceeds a\npre-defined threshold. We evaluate the proposed model on two image\nclassification tasks, where models need to predict image categories based on a\nstream of small patches extracted from the image. We show that the CBGT-Net\nprovides improved accuracy and robustness compared to models trained to\nclassify from a single patch, and models leveraging an LSTM layer to classify\nfrom a fixed sequence length of patches.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15974v1",
    "published_date": "2024-03-24 00:46:40 UTC",
    "updated_date": "2024-03-24 00:46:40 UTC"
  }
]
[
  {
    "arxiv_id": "2406.11105v2",
    "title": "Exploiting Diffusion Prior for Out-of-Distribution Detection",
    "authors": [
      "Armando Zhu",
      "Jiabei Liu",
      "Keqin Li",
      "Shuying Dai",
      "Bo Hong",
      "Peng Zhao",
      "Changsong Wei"
    ],
    "abstract": "Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models, especially in areas where security is critical. However,\ntraditional OOD detection methods often fail to capture complex data\ndistributions from large scale date. In this paper, we present a novel approach\nfor OOD detection that leverages the generative ability of diffusion models and\nthe powerful feature extraction capabilities of CLIP. By using these features\nas conditional inputs to a diffusion model, we can reconstruct the images after\nencoding them with CLIP. The difference between the original and reconstructed\nimages is used as a signal for OOD identification. The practicality and\nscalability of our method is increased by the fact that it does not require\nclass-specific labeled ID data, as is the case with many other methods.\nExtensive experiments on several benchmark datasets demonstrates the robustness\nand effectiveness of our method, which have significantly improved the\ndetection accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11105v2",
    "published_date": "2024-06-16 23:55:25 UTC",
    "updated_date": "2024-08-21 17:04:18 UTC"
  },
  {
    "arxiv_id": "2406.11102v2",
    "title": "Grading Massive Open Online Courses Using Large Language Models",
    "authors": [
      "Shahriar Golchin",
      "Nikhil Garuda",
      "Christopher Impey",
      "Matthew Wenger"
    ],
    "abstract": "Massive open online courses (MOOCs) offer free education globally. Despite\nthis democratization of learning, the massive enrollment in these courses makes\nit impractical for an instructor to assess every student's writing assignment.\nAs a result, peer grading, often guided by a straightforward rubric, is the\nmethod of choice. While convenient, peer grading often falls short in terms of\nreliability and validity. In this study, we explore the feasibility of using\nlarge language models (LLMs) to replace peer grading in MOOCs. To this end, we\nadapt the zero-shot chain-of-thought (ZCoT) prompting technique to automate the\nfeedback process once the LLM assigns a score to an assignment. Specifically,\nto instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1)\nZCoT with instructor-provided correct answers, (2) ZCoT with both\ninstructor-provided correct answers and rubrics, and (3) ZCoT with\ninstructor-provided correct answers and LLM-generated rubrics. We tested these\nprompts in 18 different scenarios using two LLMs, GPT-4 and GPT-3.5, across\nthree MOOCs: Introductory Astronomy, Astrobiology, and the History and\nPhilosophy of Astronomy. Our results show that ZCoT, when augmented with\ninstructor-provided correct answers and rubrics, produces grades that are more\naligned with those assigned by instructors compared to peer grading. Finally,\nour findings indicate a promising potential for automated grading systems in\nMOOCs, especially in subjects with well-defined rubrics, to improve the\nlearning experience for millions of online learners worldwide.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Final version; accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.11102v2",
    "published_date": "2024-06-16 23:42:11 UTC",
    "updated_date": "2024-12-16 06:50:20 UTC"
  },
  {
    "arxiv_id": "2406.15477v2",
    "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
    "authors": [
      "Kai Yin",
      "Chengkai Liu",
      "Ali Mostafavi",
      "Xia Hu"
    ],
    "abstract": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM",
    "pdf_url": "http://arxiv.org/pdf/2406.15477v2",
    "published_date": "2024-06-16 23:01:10 UTC",
    "updated_date": "2025-01-16 03:26:36 UTC"
  },
  {
    "arxiv_id": "2406.11097v2",
    "title": "InstructCMP: Length Control in Sentence Compression through Instruction-based Large Language Models",
    "authors": [
      "Juseon-Do",
      "Jingun Kwon",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ],
    "abstract": "Extractive summarization can produce faithful summaries but often requires\nadditional constraints such as a desired summary length. Traditional sentence\ncompression models do not typically consider the constraints because of their\nrestricted model abilities, which require model modifications for coping with\nthem. To bridge this gap, we propose Instruction-based Compression\n(InstructCMP), an approach to the sentence compression task that can consider\nthe length constraint through instructions by leveraging the zero-shot\ntask-solving abilities of Large Language Models (LLMs). For this purpose, we\ncreated new evaluation datasets by transforming traditional sentence\ncompression datasets into an instruction format. By using the datasets, we\nfirst reveal that the current LLMs still face challenges in accurately\ncontrolling the length for a compressed text. To address this issue, we propose\nan approach named \"length priming,\" that incorporates additional length\ninformation into the instructions without external resources. While the length\npriming effectively works in a zero-shot setting, a training dataset with the\ninstructions would further improve the ability of length control. Thus, we\nadditionally created a training dataset in an instruction format to fine-tune\nthe model on it. Experimental results and analysis show that applying the\nlength priming significantly improves performances of InstructCMP in both\nzero-shot and fine-tuning settings without the need of any model modifications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, accepted to ACL 2024 Findings (Long Paper)",
    "pdf_url": "http://arxiv.org/pdf/2406.11097v2",
    "published_date": "2024-06-16 23:00:47 UTC",
    "updated_date": "2024-06-18 18:35:52 UTC"
  },
  {
    "arxiv_id": "2406.11087v5",
    "title": "DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models",
    "authors": [
      "Yanming Liu",
      "Xinyue Peng",
      "Yuwei Zhang",
      "Xiaolan Ke",
      "Songhang Deng",
      "Jiannan Cao",
      "Chen Ma",
      "Mengchen Fu",
      "Tianyu Du",
      "Sheng Cheng",
      "Xun Wang",
      "Jianwei Yin",
      "Xuhong Zhang"
    ],
    "abstract": "Large language models have repeatedly shown outstanding performance across\ndiverse applications. However, deploying these models can inadvertently risk\nuser privacy. The significant memory demands during training pose a major\nchallenge in terms of resource consumption. This substantial size places a\nheavy load on memory resources, raising considerable practical concerns. In\nthis paper, we introduce DP-MemArc, a novel training framework aimed at\nreducing the memory costs of large language models while emphasizing the\nprotection of user data privacy. DP-MemArc incorporates side network or\nreversible network designs to support a variety of differential privacy\nmemory-efficient fine-tuning schemes. Our approach not only achieves about 2.5\ntimes in memory optimization but also ensures robust privacy protection,\nkeeping user data secure and confidential. Extensive experiments have\ndemonstrated that DP-MemArc effectively provides differential privacy-efficient\nfine-tuning across different task scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Fix metadata error",
    "pdf_url": "http://arxiv.org/pdf/2406.11087v5",
    "published_date": "2024-06-16 22:11:41 UTC",
    "updated_date": "2025-02-20 07:47:17 UTC"
  },
  {
    "arxiv_id": "2406.12934v1",
    "title": "Current state of LLM Risks and AI Guardrails",
    "authors": [
      "Suriya Ganesh Ayyamperumal",
      "Limin Ge"
    ],
    "abstract": "Large language models (LLMs) have become increasingly sophisticated, leading\nto widespread deployment in sensitive applications where safety and reliability\nare paramount. However, LLMs have inherent risks accompanying them, including\nbias, potential for unsafe actions, dataset poisoning, lack of explainability,\nhallucinations, and non-reproducibility. These risks necessitate the\ndevelopment of \"guardrails\" to align LLMs with desired behaviors and mitigate\npotential harm.\n  This work explores the risks associated with deploying LLMs and evaluates\ncurrent approaches to implementing guardrails and model alignment techniques.\nWe examine intrinsic and extrinsic bias evaluation methods and discuss the\nimportance of fairness metrics for responsible AI development. The safety and\nreliability of agentic LLMs (those capable of real-world actions) are explored,\nemphasizing the need for testability, fail-safes, and situational awareness.\n  Technical strategies for securing LLMs are presented, including a layered\nprotection model operating at external, secondary, and internal levels. System\nprompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to\nminimize bias and protect privacy are highlighted.\n  Effective guardrail design requires a deep understanding of the LLM's\nintended use case, relevant regulations, and ethical considerations. Striking a\nbalance between competing requirements, such as accuracy and privacy, remains\nan ongoing challenge. This work underscores the importance of continuous\nresearch and development to ensure the safe and responsible use of LLMs in\nreal-world applications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "Independent study, Exploring LLMs, Deploying LLMs and their Risks",
    "pdf_url": "http://arxiv.org/pdf/2406.12934v1",
    "published_date": "2024-06-16 22:04:10 UTC",
    "updated_date": "2024-06-16 22:04:10 UTC"
  },
  {
    "arxiv_id": "2406.11916v1",
    "title": "Enhanced Elephant Herding Optimization for Large Scale Information Access on Social Media",
    "authors": [
      "Yassine Drias",
      "Habiba Drias",
      "Ilyes Khennak"
    ],
    "abstract": "In this article, we present a novel information access approach inspired by\nthe information foraging theory (IFT) and elephant herding optimization (EHO).\nFirst, we propose a model for information access on social media based on the\nIFT. We then elaborate an adaptation of the original EHO algorithm to apply it\nto the information access problem. The combination of the IFT and EHO\nconstitutes a good opportunity to find relevant information on social media.\nHowever, when dealing with voluminous data, the performance undergoes a sharp\ndrop. To overcome this issue, we developed an enhanced version of EHO for large\nscale information access. We introduce new operators to the algorithm,\nincluding territories delimitation and clan migration using clustering. To\nvalidate our work, we created a dataset of more than 1.4 million tweets, on\nwhich we carried out extensive experiments. The outcomes reveal the ability of\nour approach to find relevant information in an effective and efficient way.\nThey also highlight the advantages of the improved version of EHO over the\noriginal algorithm regarding different aspects. Furthermore, we undertook a\ncomparative study with two other metaheuristic-based information foraging\napproaches, namely ant colony system and particle swarm optimization. Overall,\nthe results are very promising.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11916v1",
    "published_date": "2024-06-16 21:48:41 UTC",
    "updated_date": "2024-06-16 21:48:41 UTC"
  },
  {
    "arxiv_id": "2406.11915v2",
    "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties",
    "authors": [
      "Evan Lohn",
      "Sean Welleck"
    ],
    "abstract": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11915v2",
    "published_date": "2024-06-16 21:11:23 UTC",
    "updated_date": "2024-10-10 16:13:19 UTC"
  },
  {
    "arxiv_id": "2406.11069v1",
    "title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences",
    "authors": [
      "Yujie Lu",
      "Dongfu Jiang",
      "Wenhu Chen",
      "William Yang Wang",
      "Yejin Choi",
      "Bill Yuchen Lin"
    ],
    "abstract": "Recent breakthroughs in vision-language models (VLMs) emphasize the necessity\nof benchmarking human preferences in real-world multimodal interactions. To\naddress this gap, we launched WildVision-Arena (WV-Arena), an online platform\nthat collects human preferences to evaluate VLMs. We curated WV-Bench by\nselecting 500 high-quality samples from 8,000 user submissions in WV-Arena.\nWV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet,\nachieving a Spearman correlation of 0.94 with the WV-Arena Elo. This\nsignificantly outperforms other benchmarks like MMVet, MMMU, and MMStar.\n  Our comprehensive analysis of 20K real-world interactions reveals important\ninsights into the failure cases of top-performing VLMs. For example, we find\nthat although GPT-4V surpasses many other models like Reka-Flash, Opus, and\nYi-VL-Plus in simple visual recognition and reasoning tasks, it still faces\nchallenges with subtle contextual cues, spatial reasoning, visual imagination,\nand expert domain knowledge. Additionally, current VLMs exhibit issues with\nhallucinations and safety when intentionally provoked. We are releasing our\nchat and feedback data to further advance research in the field of VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "link: https://hf.co/spaces/WildVision/vision-arena",
    "pdf_url": "http://arxiv.org/pdf/2406.11069v1",
    "published_date": "2024-06-16 20:53:25 UTC",
    "updated_date": "2024-06-16 20:53:25 UTC"
  },
  {
    "arxiv_id": "2406.11068v1",
    "title": "A Unified View of Abstract Visual Reasoning Problems",
    "authors": [
      "Mikołaj Małkiński",
      "Jacek Mańdziuk"
    ],
    "abstract": "The field of Abstract Visual Reasoning (AVR) encompasses a wide range of\nproblems, many of which are inspired by human IQ tests. The variety of AVR\ntasks has resulted in state-of-the-art AVR methods being task-specific\napproaches. Furthermore, contemporary methods consider each AVR problem\ninstance not as a whole, but in the form of a set of individual panels with\nparticular locations and roles (context vs. answer panels) pre-assigned\naccording to the task-specific arrangements. While these highly specialized\napproaches have recently led to significant progress in solving particular AVR\ntasks, considering each task in isolation hinders the development of universal\nlearning systems in this domain. In this paper, we introduce a unified view of\nAVR tasks, where each problem instance is rendered as a single image, with no a\npriori assumptions about the number of panels, their location, or role. The\nmain advantage of the proposed unified view is the ability to develop universal\nlearning models applicable to various AVR tasks. What is more, the proposed\napproach inherently facilitates transfer learning in the AVR domain, as various\ntypes of problems share a common representation. The experiments conducted on\nfour AVR datasets with Raven's Progressive Matrices and Visual Analogy\nProblems, and one real-world visual analogy dataset show that the proposed\nunified representation of AVR tasks poses a challenge to state-of-the-art Deep\nLearning (DL) AVR models and, more broadly, contemporary DL image recognition\nmethods. In order to address this challenge, we introduce the Unified Model for\nAbstract Visual Reasoning (UMAVR) capable of dealing with various types of AVR\nproblems in a unified manner. UMAVR outperforms existing AVR methods in\nselected single-task learning experiments, and demonstrates effective knowledge\nreuse in transfer learning and curriculum learning setups.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11068v1",
    "published_date": "2024-06-16 20:52:44 UTC",
    "updated_date": "2024-06-16 20:52:44 UTC"
  },
  {
    "arxiv_id": "2406.16935v1",
    "title": "Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex",
    "authors": [
      "Spandan Madan",
      "Will Xiao",
      "Mingran Cao",
      "Hanspeter Pfister",
      "Margaret Livingstone",
      "Gabriel Kreiman"
    ],
    "abstract": "We characterized the generalization capabilities of DNN-based encoding models\nwhen predicting neuronal responses from the visual cortex. We collected\n\\textit{MacaqueITBench}, a large-scale dataset of neural population responses\nfrom the macaque inferior temporal (IT) cortex to over $300,000$ images,\ncomprising $8,233$ unique natural images presented to seven monkeys over $109$\nsessions. Using \\textit{MacaqueITBench}, we investigated the impact of\ndistribution shifts on models predicting neural activity by dividing the images\ninto Out-Of-Distribution (OOD) train and test splits. The OOD splits included\nseveral different image-computable types including image contrast, hue,\nintensity, temperature, and saturation. Compared to the performance on\nin-distribution test images -- the conventional way these models have been\nevaluated -- models performed worse at predicting neuronal responses to\nout-of-distribution images, retaining as little as $20\\%$ of the performance on\nin-distribution test images. The generalization performance under OOD shifts\ncan be well accounted by a simple image similarity metric -- the cosine\ndistance between image representations extracted from a pre-trained object\nrecognition model is a strong predictor of neural predictivity under different\ndistribution shifts. The dataset of images, neuronal firing rate recordings,\nand computational benchmarks are hosted publicly at: https://bit.ly/3zeutVd.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16935v1",
    "published_date": "2024-06-16 20:33:57 UTC",
    "updated_date": "2024-06-16 20:33:57 UTC"
  },
  {
    "arxiv_id": "2406.11061v1",
    "title": "Generalization and Knowledge Transfer in Abstract Visual Reasoning Models",
    "authors": [
      "Mikołaj Małkiński",
      "Jacek Mańdziuk"
    ],
    "abstract": "We study generalization and knowledge reuse capabilities of deep neural\nnetworks in the domain of abstract visual reasoning (AVR), employing Raven's\nProgressive Matrices (RPMs), a recognized benchmark task for assessing AVR\nabilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset\nare investigated. Firstly, inspired by generalization assessment capabilities\nof the PGM dataset and popularity of I-RAVEN, we introduce\nAttributeless-I-RAVEN, a benchmark with four generalization regimes that allow\nto test generalization of abstract rules applied to held-out attributes.\nSecondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs with a novel\ncomponent structure comprising line-based patterns, facilitating assessment of\nprogressive knowledge acquisition in transfer learning setting. The developed\nbenchmarks reveal shortcomings of the contemporary deep learning models, which\nwe partly address with Pathways of Normalized Group Convolution (PoNG) model, a\nnovel neural architecture for solving AVR tasks. PoNG excels in both presented\nchallenges, as well as the standard I-RAVEN and PGM setups.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11061v1",
    "published_date": "2024-06-16 20:26:38 UTC",
    "updated_date": "2024-06-16 20:26:38 UTC"
  },
  {
    "arxiv_id": "2406.11050v2",
    "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
    "authors": [
      "Bowen Jiang",
      "Yangxinyu Xie",
      "Zhuoqun Hao",
      "Xiaomeng Wang",
      "Tanwi Mallick",
      "Weijie J. Su",
      "Camillo J. Taylor",
      "Dan Roth"
    ],
    "abstract": "This study introduces a hypothesis-testing framework to assess whether large\nlanguage models (LLMs) possess genuine reasoning abilities or primarily depend\non token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to\ninvestigate their token bias in solving logical reasoning tasks. Specifically,\nwe develop carefully controlled synthetic datasets, featuring conjunction\nfallacy and syllogistic problems. Our framework outlines a list of hypotheses\nwhere token biases are readily identifiable, with all null hypotheses assuming\ngenuine reasoning capabilities of LLMs. The findings in this study suggest,\nwith statistical guarantee, that most LLMs still struggle with logical\nreasoning. While they may perform well on classic problems, their success\nlargely depends on recognizing superficial patterns with strong token bias,\nthereby raising concerns about their actual reasoning and generalization\nabilities. Codes and data are open-sourced at\nhttps://github.com/bowen-upenn/llm_token_bias.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted at EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11050v2",
    "published_date": "2024-06-16 19:22:53 UTC",
    "updated_date": "2024-10-04 04:40:03 UTC"
  },
  {
    "arxiv_id": "2406.11047v1",
    "title": "Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents",
    "authors": [
      "Chandran Nandkumar",
      "Luka Peternel"
    ],
    "abstract": "This paper presents the design and evaluation of a novel multi-level LLM\ninterface for supermarket robots to assist customers. The proposed interface\nallows customers to convey their needs through both generic and specific\nqueries. While state-of-the-art systems like OpenAI's GPTs are highly adaptable\nand easy to build and deploy, they still face challenges such as increased\nresponse times and limitations in strategic control of the underlying model for\ntailored use-case and cost optimization. Driven by the goal of developing\nfaster and more efficient conversational agents, this paper advocates for using\nmultiple smaller, specialized LLMs fine-tuned to handle different user queries\nbased on their specificity and user intent. We compare this approach to a\nspecialized GPT model powered by GPT-4 Turbo, using the Artificial Social Agent\nQuestionnaire (ASAQ) and qualitative participant feedback in a counterbalanced\nwithin-subjects experiment. Our findings show that our multi-LLM chatbot\narchitecture outperformed the benchmarked GPT model across all 13 measured\ncriteria, with statistically significant improvements in four key areas:\nperformance, user satisfaction, user-agent partnership, and self-image\nenhancement. The paper also presents a method for supermarket robot navigation\nby mapping the final chatbot response to correct shelf numbers, enabling the\nrobot to sequentially navigate towards the respective products, after which\nlower-level robot perception, control, and planning can be used for automated\nobject retrieval. We hope this work encourages more efforts into using\nmultiple, specialized smaller models instead of relying on a single powerful,\nbut more expensive and slower model.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11047v1",
    "published_date": "2024-06-16 19:13:01 UTC",
    "updated_date": "2024-06-16 19:13:01 UTC"
  },
  {
    "arxiv_id": "2406.11044v2",
    "title": "Evaluating the Performance of Large Language Models via Debates",
    "authors": [
      "Behrad Moniri",
      "Hamed Hassani",
      "Edgar Dobriban"
    ],
    "abstract": "Large Language Models (LLMs) are rapidly evolving and impacting various\nfields, necessitating the development of effective methods to evaluate and\ncompare their performance. Most current approaches for performance evaluation\nare either based on fixed, domain-specific questions that lack the flexibility\nrequired in many real-world applications, or rely on human input, making them\nunscalable. To address these issues, we propose an automated benchmarking\nframework based on debates between LLMs, judged by another LLM. This method\nassesses not only domain knowledge, but also skills such as argumentative\nreasoning and inconsistency recognition. We evaluate the performance of various\nstate-of-the-art LLMs using the debate framework and achieve rankings that\nalign closely with popular rankings based on human input, eliminating the need\nfor costly human crowdsourcing.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11044v2",
    "published_date": "2024-06-16 19:02:31 UTC",
    "updated_date": "2025-02-07 21:56:40 UTC"
  },
  {
    "arxiv_id": "2406.11039v2",
    "title": "Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment",
    "authors": [
      "Nicholas Kluge Corrêa"
    ],
    "abstract": "The critical inquiry pervading the realm of Philosophy, and perhaps extending\nits influence across all Humanities disciplines, revolves around the\nintricacies of morality and normativity. Surprisingly, in recent years, this\nthematic thread has woven its way into an unexpected domain, one not\nconventionally associated with pondering \"what ought to be\": the field of\nartificial intelligence (AI) research. Central to morality and AI, we find\n\"alignment\", a problem related to the challenges of expressing human goals and\nvalues in a manner that artificial systems can follow without leading to\nunwanted adversarial effects. More explicitly and with our current paradigm of\nAI development in mind, we can think of alignment as teaching human values to\nnon-anthropomorphic entities trained through opaque, gradient-based learning\ntechniques. This work addresses alignment as a technical-philosophical problem\nthat requires solid philosophical foundations and practical implementations\nthat bring normative theory to AI system development. To accomplish this, we\npropose two sets of necessary and sufficient conditions that, we argue, should\nbe considered in any alignment process. While necessary conditions serve as\nmetaphysical and metaethical roots that pertain to the permissibility of\nalignment, sufficient conditions establish a blueprint for aligning AI systems\nunder a learning-based paradigm. After laying such foundations, we present\nimplementations of this approach by using state-of-the-art techniques and\nmethods for aligning general-purpose language systems. We call this framework\nDynamic Normativity. Its central thesis is that any alignment process under a\nlearning paradigm that cannot fulfill its necessary and sufficient conditions\nwill fail in producing aligned systems.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11039v2",
    "published_date": "2024-06-16 18:37:31 UTC",
    "updated_date": "2024-06-18 12:15:06 UTC"
  },
  {
    "arxiv_id": "2406.11033v2",
    "title": "HAIChart: Human and AI Paired Visualization System",
    "authors": [
      "Yupeng Xie",
      "Yuyu Luo",
      "Guoliang Li",
      "Nan Tang"
    ],
    "abstract": "The growing importance of data visualization in business intelligence and\ndata science emphasizes the need for tools that can efficiently generate\nmeaningful visualizations from large datasets. Existing tools fall into two\nmain categories: human-powered tools (e.g., Tableau and PowerBI), which require\nintensive expert involvement, and AI-powered automated tools (e.g., Draco and\nTable2Charts), which often fall short of guessing specific user needs. In this\npaper, we aim to achieve the best of both worlds. Our key idea is to initially\nauto-generate a set of high-quality visualizations to minimize manual effort,\nthen refine this process iteratively with user feedback to more closely align\nwith their needs. To this end, we present HAIChart, a reinforcement\nlearning-based framework designed to iteratively recommend good visualizations\nfor a given dataset by incorporating user feedback. Specifically, we propose a\nMonte Carlo Graph Search-based visualization generation algorithm paired with a\ncomposite reward function to efficiently explore the visualization space and\nautomatically generate good visualizations. We devise a visualization hints\nmechanism to actively incorporate user feedback, thus progressively refining\nthe visualization generation module. We further prove that the top-k\nvisualization hints selection problem is NP-hard and design an efficient\nalgorithm. We conduct both quantitative evaluations and user studies, showing\nthat HAIChart significantly outperforms state-of-the-art human-powered tools\n(21% better at Recall and 1.8 times faster) and AI-powered automatic tools\n(25.1% and 14.9% better in terms of Hit@3 and R10@30, respectively).",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "VLDB 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11033v2",
    "published_date": "2024-06-16 18:04:47 UTC",
    "updated_date": "2024-09-07 13:36:39 UTC"
  },
  {
    "arxiv_id": "2406.11912v2",
    "title": "AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology",
    "authors": [
      "Minh Huynh Nguyen",
      "Thang Phan Chau",
      "Phong X. Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "Software agents have emerged as promising tools for addressing complex\nsoftware engineering tasks. Existing works, on the other hand, frequently\noversimplify software development workflows, despite the fact that such\nworkflows are typically more complex in the real world. Thus, we propose\nAgileCoder, a multi agent system that integrates Agile Methodology (AM) into\nthe framework. This system assigns specific AM roles - such as Product Manager,\nDeveloper, and Tester to different agents, who then collaboratively develop\nsoftware based on user inputs. AgileCoder enhances development efficiency by\norganizing work into sprints, focusing on incrementally developing software\nthrough sprints. Additionally, we introduce Dynamic Code Graph Generator, a\nmodule that creates a Code Dependency Graph dynamically as updates are made to\nthe codebase. This allows agents to better comprehend the codebase, leading to\nmore precise code generation and modifications throughout the software\ndevelopment process. AgileCoder surpasses existing benchmarks, like ChatDev and\nMetaGPT, establishing a new standard and showcasing the capabilities of multi\nagent systems in advanced software engineering environments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.11912v2",
    "published_date": "2024-06-16 17:57:48 UTC",
    "updated_date": "2024-07-14 09:14:30 UTC"
  },
  {
    "arxiv_id": "2406.11026v1",
    "title": "Boosting Medical Image Classification with Segmentation Foundation Model",
    "authors": [
      "Pengfei Gu",
      "Zihan Zhao",
      "Hongxiao Wang",
      "Yaopeng Peng",
      "Yizhe Zhang",
      "Nishchal Sapkota",
      "Chaoli Wang",
      "Danny Z. Chen"
    ],
    "abstract": "The Segment Anything Model (SAM) exhibits impressive capabilities in\nzero-shot segmentation for natural images. Recently, SAM has gained a great\ndeal of attention for its applications in medical image segmentation. However,\nto our best knowledge, no studies have shown how to harness the power of SAM\nfor medical image classification. To fill this gap and make SAM a true\n``foundation model'' for medical image analysis, it is highly desirable to\ncustomize SAM specifically for medical image classification. In this paper, we\nintroduce SAMAug-C, an innovative augmentation method based on SAM for\naugmenting classification datasets by generating variants of the original\nimages. The augmented datasets can be used to train a deep learning\nclassification model, thereby boosting the classification performance.\nFurthermore, we propose a novel framework that simultaneously processes raw and\nSAMAug-C augmented image input, capitalizing on the complementary information\nthat is offered by both. Experiments on three public datasets validate the\neffectiveness of our new approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11026v1",
    "published_date": "2024-06-16 17:54:49 UTC",
    "updated_date": "2024-06-16 17:54:49 UTC"
  },
  {
    "arxiv_id": "2406.11023v1",
    "title": "Physics-Informed Deep Learning and Partial Transfer Learning for Bearing Fault Diagnosis in the Presence of Highly Missing Data",
    "authors": [
      "Mohammadreza Kavianpour",
      "Parisa Kavianpour",
      "Amin Ramezani"
    ],
    "abstract": "One of the most significant obstacles in bearing fault diagnosis is a lack of\nlabeled data for various fault types. Also, sensor-acquired data frequently\nlack labels and have a large amount of missing data. This paper tackles these\nissues by presenting the PTPAI method, which uses a physics-informed deep\nlearning-based technique to generate synthetic labeled data. Labeled synthetic\ndata makes up the source domain, whereas unlabeled data with missing data is\npresent in the target domain. Consequently, imbalanced class problems and\npartial-set fault diagnosis hurdles emerge. To address these challenges, the\nRF-Mixup approach is used to handle imbalanced classes. As domain adaptation\nstrategies, the MK-MMSD and CDAN are employed to mitigate the disparity in\ndistribution between synthetic and actual data. Furthermore, the partial-set\nchallenge is tackled by applying weighting methods at the class and instance\nlevels. Experimental outcomes on the CWRU and JNU datasets indicate that the\nproposed approach effectively addresses these problems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11023v1",
    "published_date": "2024-06-16 17:36:53 UTC",
    "updated_date": "2024-06-16 17:36:53 UTC"
  },
  {
    "arxiv_id": "2406.11014v1",
    "title": "Latent Communication in Artificial Neural Networks",
    "authors": [
      "Luca Moschella"
    ],
    "abstract": "As NNs permeate various scientific and industrial domains, understanding the\nuniversality and reusability of their representations becomes crucial. At their\ncore, these networks create intermediate neural representations, indicated as\nlatent spaces, of the input data and subsequently leverage them to perform\nspecific downstream tasks. This dissertation focuses on the universality and\nreusability of neural representations. Do the latent representations crafted by\na NN remain exclusive to a particular trained instance, or can they generalize\nacross models, adapting to factors such as randomness during training, model\narchitecture, or even data domain? This adaptive quality introduces the notion\nof Latent Communication -- a phenomenon that describes when representations can\nbe unified or reused across neural spaces. A salient observation from our\nresearch is the emergence of similarities in latent representations, even when\nthese originate from distinct or seemingly unrelated NNs. By exploiting a\npartial correspondence between the two data distributions that establishes a\nsemantic link, we found that these representations can either be projected into\na universal representation, coined as Relative Representation, or be directly\ntranslated from one space to another. Latent Communication allows for a bridge\nbetween independently trained NN, irrespective of their training regimen,\narchitecture, or the data modality they were trained on -- as long as the data\nsemantic content stays the same (e.g., images and their captions). This holds\ntrue for both generation, classification and retrieval downstream tasks; in\nsupervised, weakly supervised, and unsupervised settings; and spans various\ndata modalities including images, text, audio, and graphs -- showcasing the\nuniversality of the Latent Communication phenomenon. [...]",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Doctoral Thesis: https://iris.uniroma1.it/handle/11573/1711827",
    "pdf_url": "http://arxiv.org/pdf/2406.11014v1",
    "published_date": "2024-06-16 17:13:58 UTC",
    "updated_date": "2024-06-16 17:13:58 UTC"
  },
  {
    "arxiv_id": "2406.11012v7",
    "title": "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game",
    "authors": [
      "Prisha Samadarshi",
      "Mariam Mustafa",
      "Anushka Kulkarni",
      "Raven Rothkopf",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "abstract": "The New York Times Connections game has emerged as a popular and challenging\npursuit for word puzzle enthusiasts. We collect 438 Connections games to\nevaluate the performance of state-of-the-art large language models (LLMs)\nagainst expert and novice human players. Our results show that even the best\nperforming LLM, Claude 3.5 Sonnet, which has otherwise shown impressive\nreasoning abilities on a wide variety of benchmarks, can only fully solve 18%\nof the games. Novice and expert players perform better than Claude 3.5 Sonnet,\nwith expert human players significantly outperforming it. We create a taxonomy\nof the knowledge types required to successfully cluster and categorize words in\nthe Connections game. We find that while LLMs perform relatively well on\ncategorizing words based on semantic relations they struggle with other types\nof knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledge\nthat combines both Word Form and Meaning. Our results establish the New York\nTimes Connections game as a challenging benchmark for evaluating abstract\nreasoning capabilities in AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11012v7",
    "published_date": "2024-06-16 17:10:32 UTC",
    "updated_date": "2024-10-14 03:41:26 UTC"
  },
  {
    "arxiv_id": "2406.11911v3",
    "title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
    "authors": [
      "X. Angelo Huang",
      "Emanuele La Malfa",
      "Samuele Marro",
      "Andrea Asperti",
      "Anthony Cohn",
      "Michael Wooldridge"
    ],
    "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework inspired by cognitive load theory to measure the complexity of ToM\ntasks. We quantify a problem's complexity as the number of states necessary to\nsolve it correctly. Our complexity measure also accounts for spurious states of\na ToM problem designed to make it apparently harder. We use our method to\nassess the complexity of five widely adopted ToM benchmarks. On top of this\nframework, we design a prompting technique that augments the information\navailable to a model with a description of how the environment changes with the\nagents' interactions. We name this technique Discrete World Models (DWM) and\nshow how it elicits superior performance on ToM tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted EMNLP 2024, Website\n  https://flecart.github.io/complexity-tom-dwm",
    "pdf_url": "http://arxiv.org/pdf/2406.11911v3",
    "published_date": "2024-06-16 16:46:55 UTC",
    "updated_date": "2024-10-09 06:59:31 UTC"
  },
  {
    "arxiv_id": "2406.11006v1",
    "title": "SPEAR: Receiver-to-Receiver Acoustic Neural Warping Field",
    "authors": [
      "Yuhang He",
      "Shitong Xu",
      "Jia-Xing Zhong",
      "Sangyun Shin",
      "Niki Trigoni",
      "Andrew Markham"
    ],
    "abstract": "We present SPEAR, a continuous receiver-to-receiver acoustic neural warping\nfield for spatial acoustic effects prediction in an acoustic 3D space with a\nsingle stationary audio source. Unlike traditional source-to-receiver modelling\nmethods that require prior space acoustic properties knowledge to rigorously\nmodel audio propagation from source to receiver, we propose to predict by\nwarping the spatial acoustic effects from one reference receiver position to\nanother target receiver position, so that the warped audio essentially\naccommodates all spatial acoustic effects belonging to the target position.\nSPEAR can be trained in a data much more readily accessible manner, in which we\nsimply ask two robots to independently record spatial audio at different\npositions. We further theoretically prove the universal existence of the\nwarping field if and only if one audio source presents. Three physical\nprinciples are incorporated to guide SPEAR network design, leading to the\nlearned warping field physically meaningful. We demonstrate SPEAR superiority\non both synthetic, photo-realistic and real-world dataset, showing the huge\npotential of SPEAR to various down-stream robotic tasks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "9 pages, 5 figures in main paper",
    "pdf_url": "http://arxiv.org/pdf/2406.11006v1",
    "published_date": "2024-06-16 16:40:26 UTC",
    "updated_date": "2024-06-16 16:40:26 UTC"
  },
  {
    "arxiv_id": "2406.11003v1",
    "title": "3D Gaze Tracking for Studying Collaborative Interactions in Mixed-Reality Environments",
    "authors": [
      "Eduardo Davalos",
      "Yike Zhang",
      "Ashwin T. S.",
      "Joyce H. Fonteles",
      "Umesh Timalsina",
      "Guatam Biswas"
    ],
    "abstract": "This study presents a novel framework for 3D gaze tracking tailored for\nmixed-reality settings, aimed at enhancing joint attention and collaborative\nefforts in team-based scenarios. Conventional gaze tracking, often limited by\nmonocular cameras and traditional eye-tracking apparatus, struggles with\nsimultaneous data synchronization and analysis from multiple participants in\ngroup contexts. Our proposed framework leverages state-of-the-art computer\nvision and machine learning techniques to overcome these obstacles, enabling\nprecise 3D gaze estimation without dependence on specialized hardware or\ncomplex data fusion. Utilizing facial recognition and deep learning, the\nframework achieves real-time, tracking of gaze patterns across several\nindividuals, addressing common depth estimation errors, and ensuring spatial\nand identity consistency within the dataset. Empirical results demonstrate the\naccuracy and reliability of our method in group environments. This provides\nmechanisms for significant advances in behavior and interaction analysis in\neducational and professional training applications in dynamic and unstructured\nenvironments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 8 figures, conference, submitted to ICMI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11003v1",
    "published_date": "2024-06-16 16:30:56 UTC",
    "updated_date": "2024-06-16 16:30:56 UTC"
  },
  {
    "arxiv_id": "2406.10999v5",
    "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large Language Models for Multiple-Choice Questions",
    "authors": [
      "Hanyang Zhong",
      "Liman Wang",
      "Wenting Cao",
      "Zeyuan Sun"
    ],
    "abstract": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. When properly balanced, we show that certain cognitive\nbiases can enhance decision-making efficiency through rational deviations and\nheuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been accepted as a full paper at the 2025 Annual\n  Conference of the Cognitive Science Society (CogSci 2025) and will be\n  presented in the form of a poster. The dataset and project website are\n  available at: https://hanyangzhong.github.io/BRU-website/",
    "pdf_url": "http://arxiv.org/pdf/2406.10999v5",
    "published_date": "2024-06-16 16:25:22 UTC",
    "updated_date": "2025-04-13 13:03:09 UTC"
  },
  {
    "arxiv_id": "2406.15474v1",
    "title": "WundtGPT: Shaping Large Language Models To Be An Empathetic, Proactive Psychologist",
    "authors": [
      "Chenyu Ren",
      "Yazhou Zhang",
      "Daihai He",
      "Jing Qin"
    ],
    "abstract": "Large language models (LLMs) are raging over the medical domain, and their\nmomentum has carried over into the mental health domain, leading to the\nemergence of few mental health LLMs. Although such mental health LLMs could\nprovide reasonable suggestions for psychological counseling, how to develop an\nauthentic and effective doctor-patient relationship (DPR) through LLMs is still\nan important problem. To fill this gap, we dissect DPR into two key attributes,\ni.e., the psychologist's empathy and proactive guidance. We thus present\nWundtGPT, an empathetic and proactive mental health large language model that\nis acquired by fine-tuning it with instruction and real conversation between\npsychologists and patients. It is designed to assist psychologists in diagnosis\nand help patients who are reluctant to communicate face-to-face understand\ntheir psychological conditions. Its uniqueness lies in that it could not only\npose purposeful questions to guide patients in detailing their symptoms but\nalso offer warm emotional reassurance. In particular, WundtGPT incorporates\nCollection of Questions, Chain of Psychodiagnosis, and Empathy Constraints into\na comprehensive prompt for eliciting LLMs' questions and diagnoses.\nAdditionally, WundtGPT proposes a reward model to promote alignment with\nempathetic mental health professionals, which encompasses two key factors:\ncognitive empathy and emotional empathy. We offer a comprehensive evaluation of\nour proposed model. Based on these outcomes, we further conduct the manual\nevaluation based on proactivity, effectiveness, professionalism and coherence.\nWe notice that WundtGPT can offer professional and effective consultation. The\nmodel is available at huggingface.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15474v1",
    "published_date": "2024-06-16 16:06:38 UTC",
    "updated_date": "2024-06-16 16:06:38 UTC"
  },
  {
    "arxiv_id": "2406.10989v1",
    "title": "Predicting the Understandability of Computational Notebooks through Code Metrics Analysis",
    "authors": [
      "Mojtaba Mostafavi Ghahfarokhi",
      "Alireza Asadi",
      "Arash Asgari",
      "Bardia Mohammadi",
      "Masih Beigi Rizi",
      "Abbas Heydarnoori"
    ],
    "abstract": "Computational notebooks have become the primary coding environment for data\nscientists. However, research on their code quality is still emerging, and the\ncode shared is often of poor quality. Given the importance of maintenance and\nreusability, understanding the metrics that affect notebook code\ncomprehensibility is crucial. Code understandability, a qualitative variable,\nis closely tied to user opinions. Traditional approaches to measuring it either\nuse limited questionnaires to review a few code pieces or rely on metadata such\nas likes and votes in software repositories. Our approach enhances the\nmeasurement of Jupyter notebook understandability by leveraging user comments\nrelated to code understandability. As a case study, we used 542,051 Kaggle\nJupyter notebooks from our previous research, named DistilKaggle. We employed a\nfine-tuned DistilBERT transformer to identify user comments associated with\ncode understandability. We established a criterion called User Opinion Code\nUnderstandability (UOCU), which considers the number of relevant comments,\nupvotes on those comments, total notebook views, and total notebook upvotes.\nUOCU proved to be more effective than previous methods. Furthermore, we trained\nmachine learning models to predict notebook code understandability based solely\non their metrics. We collected 34 metrics for 132,723 final notebooks as\nfeatures in our dataset, using UOCU as the label. Our predictive model, using\nthe Random Forest classifier, achieved 89% accuracy in predicting the\nunderstandability levels of computational notebooks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10989v1",
    "published_date": "2024-06-16 15:58:40 UTC",
    "updated_date": "2024-06-16 15:58:40 UTC"
  },
  {
    "arxiv_id": "2406.10977v1",
    "title": "Toward Optimal LLM Alignments Using Two-Player Games",
    "authors": [
      "Rui Zheng",
      "Hongyi Guo",
      "Zhihan Liu",
      "Xiaoying Zhang",
      "Yuanshun Yao",
      "Xiaojun Xu",
      "Zhaoran Wang",
      "Zhiheng Xi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Hang Li",
      "Yang Liu"
    ],
    "abstract": "The standard Reinforcement Learning from Human Feedback (RLHF) framework\nprimarily focuses on optimizing the performance of large language models using\npre-collected prompts. However, collecting prompts that provide comprehensive\ncoverage is both tedious and challenging, and often fails to include scenarios\nthat LLMs need to improve on the most. In this paper, we investigate alignment\nthrough the lens of two-agent games, involving iterative interactions between\nan adversarial and a defensive agent. The adversarial agent's task at each step\nis to generate prompts that expose the weakness of the defensive agent. In\nreturn, the defensive agent seeks to improve its responses to these newly\nidentified prompts it struggled with, based on feedback from the reward model.\nWe theoretically demonstrate that this iterative reinforcement learning\noptimization converges to a Nash Equilibrium for the game induced by the\nagents. Experimental results in safety scenarios demonstrate that learning in\nsuch a competitive environment not only fully trains agents but also leads to\npolicies with enhanced generalization capabilities for both adversarial and\ndefensive agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68"
    ],
    "primary_category": "cs.CL",
    "comment": "Our code is released at https://github.com/ruizheng20/gpo",
    "pdf_url": "http://arxiv.org/pdf/2406.10977v1",
    "published_date": "2024-06-16 15:24:50 UTC",
    "updated_date": "2024-06-16 15:24:50 UTC"
  },
  {
    "arxiv_id": "2406.10974v3",
    "title": "Towards Supporting Legal Argumentation with NLP: Is More Data Really All You Need?",
    "authors": [
      "T. Y. S. S Santosh",
      "Kevin D. Ashley",
      "Katie Atkinson",
      "Matthias Grabmair"
    ],
    "abstract": "Modeling legal reasoning and argumentation justifying decisions in cases has\nalways been central to AI & Law, yet contemporary developments in legal NLP\nhave increasingly focused on statistically classifying legal conclusions from\ntext. While conceptually simpler, these approaches often fall short in\nproviding usable justifications connecting to appropriate legal concepts. This\npaper reviews both traditional symbolic works in AI & Law and recent advances\nin legal NLP, and distills possibilities of integrating expert-informed\nknowledge to strike a balance between scalability and explanation in symbolic\nvs. data-driven approaches. We identify open challenges and discuss the\npotential of modern NLP models and methods that integrate",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NLLP Workshop, EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10974v3",
    "published_date": "2024-06-16 15:15:44 UTC",
    "updated_date": "2024-10-15 15:59:34 UTC"
  },
  {
    "arxiv_id": "2406.10973v3",
    "title": "ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts",
    "authors": [
      "Samar Khanna",
      "Medhanie Irgau",
      "David B. Lobell",
      "Stefano Ermon"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation\n(LoRA) can effectively adapt large pre-trained foundation models to downstream\ntasks using only a small fraction (0.1%-10%) of the original trainable weights.\nAn under-explored question of PEFT is in extending the pre-training phase\nwithout supervised labels; that is, can we adapt a pre-trained foundation model\nto a new domain via efficient self-supervised pre-training on this new domain?\nIn this work, we introduce ExPLoRA, a highly effective technique to improve\ntransfer learning of pre-trained vision transformers (ViTs) under domain\nshifts. Initializing a ViT with pre-trained weights on large, natural-image\ndatasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised\npre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks\nand tuning all other layers with LoRA. We then fine-tune the resulting model\nonly with LoRA on this new domain for supervised learning. Our experiments\ndemonstrate state-of-the-art results on satellite imagery, even outperforming\nfully pre-training and fine-tuning ViTs. Using the DinoV2 training objective,\nwe demonstrate up to 8% improvement in linear probing top-1 accuracy on\ndownstream tasks while using <10% of the number of parameters that are used in\nprior fully-tuned state-of-the art approaches. Our ablation studies confirm the\nefficacy of our approach over other baselines, including PEFT and unfreezing\nmore ViT blocks. Code is available on the project website:\nhttps://samar-khanna.github.io/ExPLoRA/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10973v3",
    "published_date": "2024-06-16 15:14:56 UTC",
    "updated_date": "2025-02-17 00:25:52 UTC"
  },
  {
    "arxiv_id": "2406.10964v3",
    "title": "Ontology Embedding: A Survey of Methods, Applications and Resources",
    "authors": [
      "Jiaoyan Chen",
      "Olga Mashkova",
      "Fernando Zhapa-Camacho",
      "Robert Hoehndorf",
      "Yuan He",
      "Ian Horrocks"
    ],
    "abstract": "Ontologies are widely used for representing domain knowledge and meta data,\nplaying an increasingly important role in Information Systems, the Semantic\nWeb, Bioinformatics and many other domains. However, logical reasoning that\nontologies can directly support are quite limited in learning, approximation\nand prediction. One straightforward solution is to integrate statistical\nanalysis and machine learning. To this end, automatically learning vector\nrepresentation for knowledge of an ontology i.e., ontology embedding has been\nwidely investigated. Numerous papers have been published on ontology embedding,\nbut a lack of systematic reviews hinders researchers from gaining a\ncomprehensive understanding of this field. To bridge this gap, we write this\nsurvey paper, which first introduces different kinds of semantics of ontologies\nand formally defines ontology embedding as well as its property of\nfaithfulness. Based on this, it systematically categorizes and analyses a\nrelatively complete set of over 80 papers, according to the ontologies they aim\nat and their technical solutions including geometric modeling, sequence\nmodeling and graph propagation. This survey also introduces the applications of\nontology embedding in ontology engineering, machine learning augmentation and\nlife sciences, presents a new library mOWL and discusses the challenges and\nfuture directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)",
    "pdf_url": "http://arxiv.org/pdf/2406.10964v3",
    "published_date": "2024-06-16 14:49:19 UTC",
    "updated_date": "2025-04-07 11:24:13 UTC"
  },
  {
    "arxiv_id": "2406.10961v1",
    "title": "Open-Vocabulary X-ray Prohibited Item Detection via Fine-tuning CLIP",
    "authors": [
      "Shuyang Lin",
      "Tong Jia",
      "Hao Wang",
      "Bowen Ma",
      "Mingyuan Li",
      "Dongyue Chen"
    ],
    "abstract": "X-ray prohibited item detection is an essential component of security check\nand categories of prohibited item are continuously increasing in accordance\nwith the latest laws. Previous works all focus on close-set scenarios, which\ncan only recognize known categories used for training and often require\ntime-consuming as well as labor-intensive annotations when learning novel\ncategories, resulting in limited real-world applications. Although the success\nof vision-language models (e.g. CLIP) provides a new perspectives for open-set\nX-ray prohibited item detection, directly applying CLIP to X-ray domain leads\nto a sharp performance drop due to domain shift between X-ray data and general\ndata used for pre-training CLIP. To address aforementioned challenges, in this\npaper, we introduce distillation-based open-vocabulary object detection (OVOD)\ntask into X-ray security inspection domain by extending CLIP to learn visual\nrepresentations in our specific X-ray domain, aiming to detect novel prohibited\nitem categories beyond base categories on which the detector is trained.\nSpecifically, we propose X-ray feature adapter and apply it to CLIP within OVOD\nframework to develop OVXD model. X-ray feature adapter containing three adapter\nsubmodules of bottleneck architecture, which is simple but can efficiently\nintegrate new knowledge of X-ray domain with original knowledge, further bridge\ndomain gap and promote alignment between X-ray images and textual concepts.\nExtensive experiments conducted on PIXray and PIDray datasets demonstrate that\nproposed method performs favorably against other baseline OVOD methods in\ndetecting novel categories in X-ray scenario. It outperforms previous best\nresult by 15.2 AP50 and 1.5 AP50 on PIXray and PIDray with achieving 21.0 AP50\nand 27.8 AP50 respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10961v1",
    "published_date": "2024-06-16 14:42:52 UTC",
    "updated_date": "2024-06-16 14:42:52 UTC"
  },
  {
    "arxiv_id": "2406.11909v4",
    "title": "Mixture-of-Subspaces in Low-Rank Adaptation",
    "authors": [
      "Taiqiang Wu",
      "Jiahao Wang",
      "Zhe Zhao",
      "Ngai Wong"
    ],
    "abstract": "In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA)\nmethod, which is computationally efficient, easy to implement, and readily\napplicable to large language, multimodal, and diffusion models. Initially, we\nequivalently decompose the weights of LoRA into two subspaces, and find that\nsimply mixing them can enhance performance. To study such a phenomenon, we\nrevisit it through a fine-grained subspace lens, showing that such modification\nis equivalent to employing a fixed mixer to fuse the subspaces. To be more\nflexible, we jointly learn the mixer with the original LoRA weights, and term\nthe method Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently\noutperforms LoRA on tasks in different modalities, including commonsense\nreasoning, visual instruction tuning, and subject-driven text-to-image\ngeneration, demonstrating its effectiveness and robustness. Codes are available\nat https://github.com/wutaiqiang/MoSLoRA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024 Main, Oral",
    "pdf_url": "http://arxiv.org/pdf/2406.11909v4",
    "published_date": "2024-06-16 14:19:49 UTC",
    "updated_date": "2025-03-02 08:40:16 UTC"
  },
  {
    "arxiv_id": "2406.10948v1",
    "title": "Incorporating uncertainty quantification into travel mode choice modeling: a Bayesian neural network (BNN) approach and an uncertainty-guided active survey framework",
    "authors": [
      "Shuwen Zheng",
      "Zhou Fang",
      "Liang Zhao"
    ],
    "abstract": "Existing deep learning approaches for travel mode choice modeling fail to\ninform modelers about their prediction uncertainty. Even when facing scenarios\nthat are out of the distribution of training data, which implies high\nprediction uncertainty, these approaches still provide deterministic answers,\npotentially leading to misguidance. To address this limitation, this study\nintroduces the concept of uncertainty from the field of explainable artificial\nintelligence into travel mode choice modeling. We propose a Bayesian neural\nnetwork-based travel mode prediction model (BTMP) that quantifies the\nuncertainty of travel mode predictions, enabling the model itself to \"know\" and\n\"tell\" what it doesn't know. With BTMP, we further propose an\nuncertainty-guided active survey framework, which dynamically formulates survey\nquestions representing travel mode choice scenarios with high prediction\nuncertainty. Through iterative collection of responses to these dynamically\ntailored survey questions, BTMP is iteratively trained to achieve the desired\naccuracy faster with fewer questions, thereby reducing survey costs.\nExperimental validation using synthetic datasets confirms the effectiveness of\nBTMP in quantifying prediction uncertainty. Furthermore, experiments, utilizing\nboth synthetic and real-world data, demonstrate that the BTMP model, trained\nwith the uncertainty-guided active survey framework, requires 20% to 50% fewer\nsurvey responses to match the performance of the model trained on randomly\ncollected survey data. Overall, the proposed BTMP model and active survey\nframework innovatively incorporate uncertainty quantification into travel mode\nchoice modeling, providing model users with essential insights into prediction\nreliability while optimizing data collection for deep learning model training\nin a cost-efficient manner.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10948v1",
    "published_date": "2024-06-16 14:05:47 UTC",
    "updated_date": "2024-06-16 14:05:47 UTC"
  },
  {
    "arxiv_id": "2406.10942v4",
    "title": "Effective Generative AI: The Human-Algorithm Centaur",
    "authors": [
      "Soroush Saghafian",
      "Lihi Idan"
    ],
    "abstract": "Advanced analytics science methods have enabled combining the power of\nartificial and human intelligence, creating \\textit{centaurs} that allow\nsuperior decision-making. Centaurs are hybrid human-algorithm models that\ncombine both formal analytics and human intuition in a symbiotic manner within\ntheir learning and reasoning process. We argue that the future of AI\ndevelopment and use in many domains needs to focus more on centaurs as opposed\nto other AI approaches. This paradigm shift towards centaur-based AI methods\nraises some fundamental questions: How are centaurs different from other\nhuman-in-the-loop methods? What are the most effective methods for creating\ncentaurs? When should centaurs be used, and when should the lead be given to\npure AI models? Doesn't the incorporation of human intuition -- which at times\ncan be misleading -- in centaurs' decision-making process degrade its\nperformance compared to pure AI methods? This work aims to address these\nfundamental questions, focusing on recent advancements in generative AI, and\nespecially in Large Language Models (LLMs), as a main case study to illustrate\ncentaurs' critical essentiality to future AI endeavors.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "To Appear in SI: Future Shock, Harvard Data Science Review\n  (https://hdsr.mitpress.mit.edu/specialissue5)",
    "pdf_url": "http://arxiv.org/pdf/2406.10942v4",
    "published_date": "2024-06-16 13:44:41 UTC",
    "updated_date": "2024-12-16 05:37:51 UTC"
  },
  {
    "arxiv_id": "2406.10940v2",
    "title": "Towards AI-Augmented Data Quality Management: From Data Quality for AI to AI for Data Quality Management",
    "authors": [
      "Heidi Carolina Tamm",
      "Anastasija Nikiforova"
    ],
    "abstract": "In the contemporary data-driven landscape, ensuring data quality (DQ) is\ncrucial for deriving actionable insights from vast data repositories. The\nobjective of this study is to explore the potential for automating data quality\nmanagement within data warehouses as data repository commonly used by large\norganizations. By conducting a systematic review of existing DQ tools available\nin the market and academic literature, the study assesses their capability to\nautomatically detect and enforce data quality rules. The review encompassed 151\ntools from various sources, revealing that most current tools focus on data\ncleansing and fixing in domain-specific databases rather than data warehouses.\nOnly a limited number of tools, specifically ten, demonstrated the capability\nto detect DQ rules, not to mention implementing this in data warehouses. The\nfindings underscore a significant gap in the market and academic research\nregarding AI-augmented DQ rule detection in data warehouses. This paper\nadvocates for further development in this area to enhance the efficiency of DQ\nmanagement processes, reduce human workload, and lower costs. The study\nhighlights the necessity of advanced tools for automated DQ rule detection,\npaving the way for improved practices in data quality management tailored to\ndata warehouse environments. The study can guide organizations in selecting\ndata quality tool that would meet their requirements most.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CE",
      "cs.ET"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10940v2",
    "published_date": "2024-06-16 13:43:04 UTC",
    "updated_date": "2025-03-29 18:06:34 UTC"
  },
  {
    "arxiv_id": "2406.10937v2",
    "title": "Understanding Understanding: A Pragmatic Framework Motivated by Large Language Models",
    "authors": [
      "Kevin Leyton-Brown",
      "Yoav Shoham"
    ],
    "abstract": "Motivated by the rapid ascent of Large Language Models (LLMs) and debates\nabout the extent to which they possess human-level qualities, we propose a\nframework for testing whether any agent (be it a machine or a human)\nunderstands a subject matter. In Turing-test fashion, the framework is based\nsolely on the agent's performance, and specifically on how well it answers\nquestions. Elements of the framework include circumscribing the set of\nquestions (the \"scope of understanding\"), requiring general competence\n(\"passing grade\"), avoiding \"ridiculous answers\", but still allowing wrong and\n\"I don't know\" answers to some questions. Reaching certainty about these\nconditions requires exhaustive testing of the questions which is impossible for\nnontrivial scopes, but we show how high confidence can be achieved via random\nsampling and the application of probabilistic confidence bounds. We also show\nthat accompanying answers with explanations can improve the sample complexity\nrequired to achieve acceptable bounds, because an explanation of an answer\nimplies the ability to answer many similar questions. According to our\nframework, current LLMs cannot be said to understand nontrivial domains, but as\nthe framework provides a practical recipe for testing understanding, it thus\nalso constitutes a tool for building AI agents that do understand.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10937v2",
    "published_date": "2024-06-16 13:37:08 UTC",
    "updated_date": "2024-06-19 08:34:21 UTC"
  },
  {
    "arxiv_id": "2406.10932v3",
    "title": "Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation for Embedding Undetectable Vulnerabilities on Speech Recognition",
    "authors": [
      "Wenhan Yao",
      "Jiangkun Yang",
      "Yongqiang He",
      "Jia Liu",
      "Weiping Wen"
    ],
    "abstract": "Speech recognition is an essential start ring of human-computer interaction,\nand recently, deep learning models have achieved excellent success in this\ntask. However, when the model training and private data provider are always\nseparated, some security threats that make deep neural networks (DNNs) abnormal\ndeserve to be researched. In recent years, the typical backdoor attacks have\nbeen researched in speech recognition systems. The existing backdoor methods\nare based on data poisoning. The attacker adds some incorporated changes to\nbenign speech spectrograms or changes the speech components, such as pitch and\ntimbre. As a result, the poisoned data can be detected by human hearing or\nautomatic deep algorithms. To improve the stealthiness of data poisoning, we\npropose a non-neural and fast algorithm called Random Spectrogram Rhythm\nTransformation (RSRT) in this paper. The algorithm combines four steps to\ngenerate stealthy poisoned utterances. From the perspective of rhythm component\ntransformation, our proposed trigger stretches or squeezes the mel spectrograms\nand recovers them back to signals. The operation keeps timbre and content\nunchanged for good stealthiness. Our experiments are conducted on two kinds of\nspeech recognition tasks, including testing the stealthiness of poisoned\nsamples by speaker verification and automatic speech recognition. The results\nshow that our method has excellent effectiveness and stealthiness. The rhythm\ntrigger needs a low poisoning rate and gets a very high attack success rate.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Neurocomputing",
    "pdf_url": "http://arxiv.org/pdf/2406.10932v3",
    "published_date": "2024-06-16 13:29:21 UTC",
    "updated_date": "2024-10-18 03:17:06 UTC"
  },
  {
    "arxiv_id": "2406.10928v2",
    "title": "Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask",
    "authors": [
      "Jingyu Xiao",
      "Zhiyao Xu",
      "Qingsong Zou",
      "Qing Li",
      "Dan Zhao",
      "Dong Fang",
      "Ruoyu Li",
      "Wenxin Tang",
      "Kang Li",
      "Xudong Zuo",
      "Penghui Hu",
      "Yong Jiang",
      "Zixuan Weng",
      "Michael R. Lyv"
    ],
    "abstract": "Smart homes, powered by the Internet of Things, offer great convenience but\nalso pose security concerns due to abnormal behaviors, such as improper\noperations of users and potential attacks from malicious attackers. Several\nbehavior modeling methods have been proposed to identify abnormal behaviors and\nmitigate potential risks. However, their performance often falls short because\nthey do not effectively learn less frequent behaviors, consider temporal\ncontext, or account for the impact of noise in human behaviors. In this paper,\nwe propose SmartGuard, an autoencoder-based unsupervised user behavior anomaly\ndetection framework. First, we design a Loss-guided Dynamic Mask Strategy\n(LDMS) to encourage the model to learn less frequent behaviors, which are often\noverlooked during learning. Second, we propose a Three-level Time-aware\nPosition Embedding (TTPE) to incorporate temporal information into positional\nembedding to detect temporal context anomaly. Third, we propose a Noise-aware\nWeighted Reconstruction Loss (NWRL) that assigns different weights for routine\nbehaviors and noise behaviors to mitigate the interference of noise behaviors\nduring inference. Comprehensive experiments on three datasets with ten types of\nanomaly behaviors demonstrates that SmartGuard consistently outperforms\nstate-of-the-art baselines and also offers highly interpretable results.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10928v2",
    "published_date": "2024-06-16 13:23:21 UTC",
    "updated_date": "2024-06-18 11:52:26 UTC"
  },
  {
    "arxiv_id": "2406.10922v1",
    "title": "Generating Tables from the Parametric Knowledge of Language Models",
    "authors": [
      "Yevgeni Berkovitch",
      "Oren Glickman",
      "Amit Somech",
      "Tomer Wolfson"
    ],
    "abstract": "We explore generating factual and accurate tables from the parametric\nknowledge of large language models (LLMs). While LLMs have demonstrated\nimpressive capabilities in recreating knowledge bases and generating free-form\ntext, we focus on generating structured tabular data, which is crucial in\ndomains like finance and healthcare. We examine the table generation abilities\nof four state-of-the-art LLMs: GPT-3.5, GPT-4, Llama2-13B, and Llama2-70B,\nusing three prompting methods for table generation: (a) full-table, (b)\nrow-by-row; (c) cell-by-cell. For evaluation, we introduce a novel benchmark,\nWikiTabGen which contains 100 curated Wikipedia tables. Tables are further\nprocessed to ensure their factual correctness and manually annotated with short\nnatural language descriptions. Our findings reveal that table generation\nremains a challenge, with GPT-4 reaching the highest accuracy at 19.6%. Our\ndetailed analysis sheds light on how various table properties, such as size,\ntable popularity, and numerical content, influence generation performance. This\nwork highlights the unique challenges in LLM-based table generation and\nprovides a solid evaluation framework for future research. Our code, prompts\nand data are all publicly available:\nhttps://github.com/analysis-bots/WikiTabGen",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10922v1",
    "published_date": "2024-06-16 12:55:55 UTC",
    "updated_date": "2024-06-16 12:55:55 UTC"
  },
  {
    "arxiv_id": "2406.10920v1",
    "title": "Hamilton-Jacobi Based Policy-Iteration via Deep Operator Learning",
    "authors": [
      "Jae Yong Lee",
      "Yeoneung Kim"
    ],
    "abstract": "The framework of deep operator network (DeepONet) has been widely exploited\nthanks to its capability of solving high dimensional partial differential\nequations. In this paper, we incorporate DeepONet with a recently developed\npolicy iteration scheme to numerically solve optimal control problems and the\ncorresponding Hamilton--Jacobi--Bellman (HJB) equations. A notable feature of\nour approach is that once the neural network is trained, the solution to the\noptimal control problem and HJB equations with different terminal functions can\nbe inferred quickly thanks to the unique feature of operator learning.\nFurthermore, a quantitative analysis of the accuracy of the algorithm is\ncarried out via comparison principles of viscosity solutions. The effectiveness\nof the method is verified with various examples, including 10-dimensional\nlinear quadratic regulator problems (LQRs).",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T20, 68U07, 35F21, 49L12, 49L25"
    ],
    "primary_category": "math.OC",
    "comment": "24 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10920v1",
    "published_date": "2024-06-16 12:53:17 UTC",
    "updated_date": "2024-06-16 12:53:17 UTC"
  },
  {
    "arxiv_id": "2406.10918v5",
    "title": "Multi-LLM QA with Embodied Exploration",
    "authors": [
      "Bhrij Patel",
      "Vishnu Sashank Dorbala",
      "Amrit Singh Bedi",
      "Dinesh Manocha"
    ],
    "abstract": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 9 Figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2406.10918v5",
    "published_date": "2024-06-16 12:46:40 UTC",
    "updated_date": "2024-10-18 12:27:07 UTC"
  },
  {
    "arxiv_id": "2406.10893v1",
    "title": "Development and Validation of Fully Automatic Deep Learning-Based Algorithms for Immunohistochemistry Reporting of Invasive Breast Ductal Carcinoma",
    "authors": [
      "Sumit Kumar Jha",
      "Purnendu Mishra",
      "Shubham Mathur",
      "Gursewak Singh",
      "Rajiv Kumar",
      "Kiran Aatre",
      "Suraj Rengarajan"
    ],
    "abstract": "Immunohistochemistry (IHC) analysis is a well-accepted and widely used method\nfor molecular subtyping, a procedure for prognosis and targeted therapy of\nbreast carcinoma, the most common type of tumor affecting women. There are four\nmolecular biomarkers namely progesterone receptor (PR), estrogen receptor (ER),\nantigen Ki67, and human epidermal growth factor receptor 2 (HER2) whose\nassessment is needed under IHC procedure to decide prognosis as well as\npredictors of response to therapy. However, IHC scoring is based on subjective\nmicroscopic examination of tumor morphology and suffers from poor\nreproducibility, high subjectivity, and often incorrect scoring in low-score\ncases. In this paper, we present, a deep learning-based semi-supervised\ntrained, fully automatic, decision support system (DSS) for IHC scoring of\ninvasive ductal carcinoma. Our system automatically detects the tumor region\nremoving artifacts and scores based on Allred standard. The system is developed\nusing 3 million pathologist-annotated image patches from 300 slides, fifty\nthousand in-house cell annotations, and forty thousand pixels marking of HER2\nmembrane. We have conducted multicentric trials at four centers with three\ndifferent types of digital scanners in terms of percentage agreement with\ndoctors. And achieved agreements of 95, 92, 88 and 82 percent for Ki67, HER2,\nER, and PR stain categories, respectively. In addition to overall accuracy, we\nfound that there is 5 percent of cases where pathologist have changed their\nscore in favor of algorithm score while reviewing with detailed algorithmic\nanalysis. Our approach could improve the accuracy of IHC scoring and subsequent\ntherapy decisions, particularly where specialist expertise is unavailable. Our\nsystem is highly modular. The proposed algorithm modules can be used to develop\nDSS for other cancer types.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM",
      "q-bio.TO"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10893v1",
    "published_date": "2024-06-16 10:52:38 UTC",
    "updated_date": "2024-06-16 10:52:38 UTC"
  },
  {
    "arxiv_id": "2406.10890v1",
    "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhitao He",
      "Hongbang Yuan",
      "Jiachun Li",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and\nharmful knowledge from the training corpus; therefore, it is crucial to erase\nthis knowledge from the models. Machine unlearning is a promising solution for\nefficiently removing specific knowledge by post hoc modifying models. In this\npaper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM\nunlearning. RWKU is designed based on the following three key factors: (1) For\nthe task setting, we consider a more practical and challenging unlearning\nsetting, where neither the forget corpus nor the retain corpus is accessible.\n(2) For the knowledge source, we choose 200 real-world famous people as the\nunlearning targets and show that such popular knowledge is widely present in\nvarious LLMs. (3) For the evaluation framework, we design the forget set and\nthe retain set to evaluate the model's capabilities across various real-world\napplications. Regarding the forget set, we provide four four membership\ninference attack (MIA) methods and nine kinds of adversarial attack probes to\nrigorously test unlearning efficacy. Regarding the retain set, we assess\nlocality and utility in terms of neighbor perturbation, general ability,\nreasoning ability, truthfulness, factuality, and fluency. We conduct extensive\nexperiments across two unlearning scenarios, two models and six baseline\nmethods and obtain some meaningful findings. We release our benchmark and code\npublicly at http://rwku-bench.github.io for future work.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "48 pages, 7 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.10890v1",
    "published_date": "2024-06-16 10:47:21 UTC",
    "updated_date": "2024-06-16 10:47:21 UTC"
  },
  {
    "arxiv_id": "2406.10889v2",
    "title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
    "authors": [
      "Darshana Saravanan",
      "Varun Gupta",
      "Darshan Singh",
      "Zeeshan Khan",
      "Vineet Gandhi",
      "Makarand Tapaswi"
    ],
    "abstract": "A fundamental aspect of compositional reasoning in a video is associating\npeople and their actions across time. Recent years have seen great progress in\ngeneral-purpose vision or video models and a move towards long-video\nunderstanding. While exciting, we take a step back and ask: are current models\ngood at compositional reasoning on short videos? To this end, we introduce\nVELOCITI, a benchmark to study Video-LLMs by disentangling and assessing the\ncomprehension of agents, actions, and their associations across multiple\nevents. We adopt the Video-Language Entailment setup and propose StrictVLE that\nrequires correct classification (rather than ranking) of the positive and\nnegative caption. We evaluate several models and observe that even the best,\nLLaVA-OneVision (44.5%) and Gemini-1.5-Pro (49.3%), are far from human accuracy\nat 93.0%. Results show that action understanding lags behind agents, and\nnegative captions created using entities appearing in the video perform worse\nthan those obtained from pure text manipulation. We also present challenges\nwith ClassicVLE and multiple-choice (MC) evaluation, strengthening our\npreference for StrictVLE. Finally, we validate that our benchmark requires\nvisual inputs of multiple frames making it ideal to study video-language\ncompositional reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025. Project Page, see\n  https://katha-ai.github.io/projects/velociti",
    "pdf_url": "http://arxiv.org/pdf/2406.10889v2",
    "published_date": "2024-06-16 10:42:21 UTC",
    "updated_date": "2025-03-30 14:07:22 UTC"
  },
  {
    "arxiv_id": "2406.10878v1",
    "title": "Demonstration Notebook: Finding the Most Suited In-Context Learning Example from Interactions",
    "authors": [
      "Yiming Tang",
      "Bin Dong"
    ],
    "abstract": "Large language models (LLMs) benefit greatly from prompt engineering, with\nin-context learning standing as a pivital technique. While former approaches\nhave provided various ways to construct the demonstrations used for in-context\nlearning, they often ignore the inherent heterogeneity within datasets,\napplying the same demonstrations to all reasoning questions. We observed that\nthe effectiveness of demonstrations varies depending on the specific question.\nThis motivates our exploration of using prompt engineering to select\nappropriate demonstrations. To address the challenge of automatically creating\nand choosing demonstrations tailored to each question, we propose a novel\nprompt engineering workflow built around a novel object called the\n\"demonstration notebook.\" This notebook helps identify the most suitable\nin-context learning example for a question by gathering and reusing information\nfrom the LLM's past interactions. Our experiments show that this approach\noutperforms all existing methods for automatic demonstration construction and\nselection (as far as we know), achieving state-of-the-art results on serveral\nreasoning benchmarks. The method's versatility is further demonstrated by its\nsuccess in text summarization and prompt compression tasks. Additionally, we\ncontribute a rigorous analysis method to reveal the \"demonstrative regime\" of a\ndemonstration, providing valuable insights into how demonstrations relate to\ndifferent question types within a dataset.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10878v1",
    "published_date": "2024-06-16 10:02:20 UTC",
    "updated_date": "2024-06-16 10:02:20 UTC"
  },
  {
    "arxiv_id": "2406.10873v1",
    "title": "Optimizing Automatic Speech Assessment: W-RankSim Regularization and Hybrid Feature Fusion Strategies",
    "authors": [
      "Chung-Wen Wu",
      "Berlin Chen"
    ],
    "abstract": "Automatic Speech Assessment (ASA) has seen notable advancements with the\nutilization of self-supervised features (SSL) in recent research. However, a\nkey challenge in ASA lies in the imbalanced distribution of data, particularly\nevident in English test datasets. To address this challenge, we approach ASA as\nan ordinal classification task, introducing Weighted Vectors Ranking Similarity\n(W-RankSim) as a novel regularization technique. W-RankSim encourages closer\nproximity of weighted vectors in the output layer for similar classes, implying\nthat feature vectors with similar labels would be gradually nudged closer to\neach other as they converge towards corresponding weighted vectors. Extensive\nexperimental evaluations confirm the effectiveness of our approach in improving\nordinal classification performance for ASA. Furthermore, we propose a hybrid\nmodel that combines SSL and handcrafted features, showcasing how the inclusion\nof handcrafted features enhances performance in an ASA system.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10873v1",
    "published_date": "2024-06-16 09:55:21 UTC",
    "updated_date": "2024-06-16 09:55:21 UTC"
  },
  {
    "arxiv_id": "2406.10858v2",
    "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Chengxi Li",
      "Kai Fan"
    ],
    "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has\nproven to be an effective alternative to reinforcement learning from human\nfeedback (RLHF) for fine-tuning preference aligned large language models\n(LLMs). However, the overall preference annotations of responses do not fully\ncapture the fine-grained quality of model outputs in complex multi-step\nreasoning tasks, such as mathematical reasoning. To address this limitation, we\nintroduce a novel algorithm called Step-level Value Preference Optimization\n(SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically\nannotate step-level preferences for multi-step reasoning. Furthermore, from the\nperspective of learning-to-rank, we train an explicit value model to replicate\nthe behavior of the implicit reward model, complementing standard preference\noptimization. This value model enables the LLM to generate higher reward\nresponses with minimal cost during inference. Experimental results demonstrate\nthat our method achieves state-of-the-art performance on both in-domain and\nout-of-domain mathematical reasoning benchmarks. Our code is available at\n\\url{https://github.com/MARIO-Math-Reasoning/Super_MARIO}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera ready version for EMNLP2024-Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.10858v2",
    "published_date": "2024-06-16 09:06:17 UTC",
    "updated_date": "2024-09-27 08:03:07 UTC"
  },
  {
    "arxiv_id": "2406.10855v1",
    "title": "ALPS: An Auto-Labeling and Pre-training Scheme for Remote Sensing Segmentation With Segment Anything Model",
    "authors": [
      "Song Zhang",
      "Qingzhong Wang",
      "Junyi Liu",
      "Haoyi Xiong"
    ],
    "abstract": "In the fast-growing field of Remote Sensing (RS) image analysis, the gap\nbetween massive unlabeled datasets and the ability to fully utilize these\ndatasets for advanced RS analytics presents a significant challenge. To fill\nthe gap, our work introduces an innovative auto-labeling framework named ALPS\n(Automatic Labeling for Pre-training in Segmentation), leveraging the Segment\nAnything Model (SAM) to predict precise pseudo-labels for RS images without\nnecessitating prior annotations or additional prompts. The proposed pipeline\nsignificantly reduces the labor and resource demands traditionally associated\nwith annotating RS datasets. By constructing two comprehensive pseudo-labeled\nRS datasets via ALPS for pre-training purposes, our approach enhances the\nperformance of downstream tasks across various benchmarks, including iSAID and\nISPRS Potsdam. Experiments demonstrate the effectiveness of our framework,\nshowcasing its ability to generalize well across multiple tasks even under the\nscarcity of extensively annotated datasets, offering a scalable solution to\nautomatic segmentation and annotation challenges in the field. In addition, the\nproposed a pipeline is flexible and can be applied to medical image\nsegmentation, remarkably boosting the performance. Note that ALPS utilizes\npre-trained SAM to semi-automatically annotate RS images without additional\nmanual annotations. Though every component in the pipeline has bee well\nexplored, integrating clustering algorithms with SAM and novel pseudo-label\nalignment significantly enhances RS segmentation, as an off-the-shelf tool for\npre-training data preparation. Our source code is available at:\nhttps://github.com/StriveZs/ALPS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10855v1",
    "published_date": "2024-06-16 09:02:01 UTC",
    "updated_date": "2024-06-16 09:02:01 UTC"
  },
  {
    "arxiv_id": "2406.10852v1",
    "title": "IG2: Integrated Gradient on Iterative Gradient Path for Feature Attribution",
    "authors": [
      "Yue Zhuo",
      "Zhiqiang Ge"
    ],
    "abstract": "Feature attribution explains Artificial Intelligence (AI) at the instance\nlevel by providing importance scores of input features' contributions to model\nprediction. Integrated Gradients (IG) is a prominent path attribution method\nfor deep neural networks, involving the integration of gradients along a path\nfrom the explained input (explicand) to a counterfactual instance (baseline).\nCurrent IG variants primarily focus on the gradient of explicand's output.\nHowever, our research indicates that the gradient of the counterfactual output\nsignificantly affects feature attribution as well. To achieve this, we propose\nIterative Gradient path Integrated Gradients (IG2), considering both gradients.\nIG2 incorporates the counterfactual gradient iteratively into the integration\npath, generating a novel path (GradPath) and a novel baseline (GradCF). These\ntwo novel IG components effectively address the issues of attribution noise and\narbitrary baseline choice in earlier IG methods. IG2, as a path method,\nsatisfies many desirable axioms, which are theoretically justified in the\npaper. Experimental results on XAI benchmark, ImageNet, MNIST, TREC questions\nanswering, wafer-map failure patterns, and CelebA face attributes validate that\nIG2 delivers superior feature attributions compared to the state-of-the-art\ntechniques. The code is released at: https://github.com/JoeZhuo-ZY/IG2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "in IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10852v1",
    "published_date": "2024-06-16 08:48:03 UTC",
    "updated_date": "2024-06-16 08:48:03 UTC"
  },
  {
    "arxiv_id": "2406.10847v2",
    "title": "TorchOpera: A Compound AI System for LLM Safety",
    "authors": [
      "Shanshan Han",
      "Zijian Hu",
      "Alay Dilipbhai Shah",
      "Han Jin",
      "Yuhang Yao",
      "Dimitris Stripelis",
      "Zhaozhuo Xu",
      "Chaoyang He"
    ],
    "abstract": "We introduce TorchOpera, a compound AI system for enhancing the safety and\nquality of prompts and responses for Large Language Models. TorchOpera ensures\nthat all user prompts are safe, contextually grounded, and effectively\nprocessed, while enhancing LLM responses to be relevant and high quality.\nTorchOpera utilizes the vector database for contextual grounding, rule-based\nwrappers for flexible modifications, and specialized mechanisms for detecting\nand adjusting unsafe or incorrect content. We also provide a view of the\ncompound AI system to reduce the computational cost. Extensive experiments show\nthat TorchOpera ensures the safety, reliability, and applicability of LLMs in\nreal-world settings while maintaining the efficiency of LLM responses.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10847v2",
    "published_date": "2024-06-16 08:39:19 UTC",
    "updated_date": "2024-10-27 07:53:40 UTC"
  },
  {
    "arxiv_id": "2406.10842v1",
    "title": "Large Language Models for Automatic Milestone Detection in Group Discussions",
    "authors": [
      "Zhuoxu Duan",
      "Zhengye Yang",
      "Samuel Westby",
      "Christoph Riedl",
      "Brooke Foucault Welles",
      "Richard J. Radke"
    ],
    "abstract": "Large language models like GPT have proven widely successful on natural\nlanguage understanding tasks based on written text documents. In this paper, we\ninvestigate an LLM's performance on recordings of a group oral communication\ntask in which utterances are often truncated or not well-formed. We propose a\nnew group task experiment involving a puzzle with several milestones that can\nbe achieved in any order. We investigate methods for processing transcripts to\ndetect if, when, and by whom a milestone has been completed. We demonstrate\nthat iteratively prompting GPT with transcription chunks outperforms semantic\nsimilarity search methods using text embeddings, and further discuss the\nquality and randomness of GPT responses under different context window sizes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10842v1",
    "published_date": "2024-06-16 08:32:22 UTC",
    "updated_date": "2024-06-16 08:32:22 UTC"
  },
  {
    "arxiv_id": "2406.11906v2",
    "title": "NovoBench: Benchmarking Deep Learning-based De Novo Peptide Sequencing Methods in Proteomics",
    "authors": [
      "Jingbo Zhou",
      "Shaorong Chen",
      "Jun Xia",
      "Sizhe Liu",
      "Tianze Ling",
      "Wenjie Du",
      "Yue Liu",
      "Jianwei Yin",
      "Stan Z. Li"
    ],
    "abstract": "Tandem mass spectrometry has played a pivotal role in advancing proteomics,\nenabling the high-throughput analysis of protein composition in biological\ntissues. Many deep learning methods have been developed for \\emph{de novo}\npeptide sequencing task, i.e., predicting the peptide sequence for the observed\nmass spectrum. However, two key challenges seriously hinder the further\nadvancement of this important task. Firstly, since there is no consensus for\nthe evaluation datasets, the empirical results in different research papers are\noften not comparable, leading to unfair comparison. Secondly, the current\nmethods are usually limited to amino acid-level or peptide-level precision and\nrecall metrics. In this work, we present the first unified benchmark NovoBench\nfor \\emph{de novo} peptide sequencing, which comprises diverse mass spectrum\ndata, integrated models, and comprehensive evaluation metrics. Recent\nimpressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo\nand $\\pi$-HelixNovo are integrated into our framework. In addition to amino\nacid-level and peptide-level precision and recall, we evaluate the models'\nperformance in terms of identifying post-tranlational modifications (PTMs),\nefficiency and robustness to peptide length, noise peaks and missing fragment\nratio, which are important influencing factors while seldom be considered.\nLeveraging this benchmark, we conduct a large-scale study of current methods,\nreport many insightful findings that open up new possibilities for future\ndevelopment.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "NeurIPS 2024 D&B track",
    "pdf_url": "http://arxiv.org/pdf/2406.11906v2",
    "published_date": "2024-06-16 08:23:21 UTC",
    "updated_date": "2024-10-31 08:54:52 UTC"
  },
  {
    "arxiv_id": "2406.10840v3",
    "title": "CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph",
    "authors": [
      "Haitao Lin",
      "Guojiang Zhao",
      "Odin Zhang",
      "Yufei Huang",
      "Lirong Wu",
      "Zicheng Liu",
      "Siyuan Li",
      "Cheng Tan",
      "Zhifeng Gao",
      "Stan Z. Li"
    ],
    "abstract": "Structure-based drug design (SBDD) aims to generate potential drugs that can\nbind to a target protein and is greatly expedited by the aid of AI techniques\nin generative models. However, a lack of systematic understanding persists due\nto the diverse settings, complex implementation, difficult reproducibility, and\ntask singularity. Firstly, the absence of standardization can lead to unfair\ncomparisons and inconclusive insights. To address this dilemma, we propose\nCBGBench, a comprehensive benchmark for SBDD, that unifies the task as a\ngenerative heterogeneous graph completion, analogous to fill-in-the-blank of\nthe 3D complex binding graph. By categorizing existing methods based on their\nattributes, CBGBench facilitates a modular and extensible framework that\nimplements various cutting-edge methods. Secondly, a single task on \\textit{de\nnovo} molecule generation can hardly reflect their capabilities. To broaden the\nscope, we have adapted these models to a range of tasks essential in drug\ndesign, which are considered sub-tasks within the graph fill-in-the-blank\ntasks. These tasks include the generative designation of \\textit{de novo}\nmolecules, linkers, fragments, scaffolds, and sidechains, all conditioned on\nthe structures of protein pockets. Our evaluations are conducted with fairness,\nencompassing comprehensive perspectives on interaction, chemical properties,\ngeometry authenticity, and substructure validity. We further provide the\npre-trained versions of the state-of-the-art models and deep insights with\nanalysis from empirical studies. The codebase for CBGBench is publicly\naccessible at \\url{https://github.com/Edapinenut/CBGBench}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages main context",
    "pdf_url": "http://arxiv.org/pdf/2406.10840v3",
    "published_date": "2024-06-16 08:20:24 UTC",
    "updated_date": "2024-10-10 11:22:58 UTC"
  },
  {
    "arxiv_id": "2406.10834v1",
    "title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
    "authors": [
      "Joykirat Singh",
      "Akshay Nambi",
      "Vibhav Vineet"
    ],
    "abstract": "Large Language Models (LLMs) have been applied to Math Word Problems (MWPs)\nwith transformative impacts, revolutionizing how these complex problems are\napproached and solved in various domains including educational settings.\nHowever, the evaluation of these models often prioritizes final accuracy,\noverlooking the crucial aspect of reasoning capabilities. This work addresses\nthis gap by focusing on the ability of LLMs to detect and correct reasoning\nmistakes. We introduce a novel dataset MWP-MISTAKE, incorporating MWPs with\nboth correct and incorrect reasoning steps generated through rule-based methods\nand smaller language models. Our comprehensive benchmarking reveals significant\ninsights into the strengths and weaknesses of state-of-the-art models, such as\nGPT-4o, GPT-4, GPT-3.5Turbo, and others. We highlight GPT-$o's superior\nperformance in mistake detection and rectification and the persistent\nchallenges faced by smaller models. Additionally, we identify issues related to\ndata contamination and memorization, impacting the reliability of LLMs in\nreal-world applications. Our findings emphasize the importance of rigorous\nevaluation of reasoning processes and propose future directions to enhance the\ngeneralization and robustness of LLMs in mathematical problem-solving.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10834v1",
    "published_date": "2024-06-16 08:06:05 UTC",
    "updated_date": "2024-06-16 08:06:05 UTC"
  },
  {
    "arxiv_id": "2406.10831v1",
    "title": "Design and Optimization of Hierarchical Gradient Coding for Distributed Learning at Edge Devices",
    "authors": [
      "Weiheng Tang",
      "Jingyi Li",
      "Lin Chen",
      "Xu Chen"
    ],
    "abstract": "Edge computing has recently emerged as a promising paradigm to boost the\nperformance of distributed learning by leveraging the distributed resources at\nedge nodes. Architecturally, the introduction of edge nodes adds an additional\nintermediate layer between the master and workers in the original distributed\nlearning systems, potentially leading to more severe straggler effect.\nRecently, coding theory-based approaches have been proposed for stragglers\nmitigation in distributed learning, but the majority focus on the conventional\nworkers-master architecture. In this paper, along a different line, we\ninvestigate the problem of mitigating the straggler effect in hierarchical\ndistributed learning systems with an additional layer composed of edge nodes.\nTechnically, we first derive the fundamental trade-off between the\ncomputational loads of workers and the stragglers tolerance. Then, we propose a\nhierarchical gradient coding framework, which provides better stragglers\nmitigation, to achieve the derived computational trade-off. To further improve\nthe performance of our framework in heterogeneous scenarios, we formulate an\noptimization problem with the objective of minimizing the expected execution\ntime for each iteration in the learning process. We develop an efficient\nalgorithm to mathematically solve the problem by outputting the optimum\nstrategy. Extensive simulation results demonstrate the superiority of our\nschemes compared with conventional solutions.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "The paper has been accepted by IEEE Transactions on Communications",
    "pdf_url": "http://arxiv.org/pdf/2406.10831v1",
    "published_date": "2024-06-16 07:52:12 UTC",
    "updated_date": "2024-06-16 07:52:12 UTC"
  },
  {
    "arxiv_id": "2406.10827v1",
    "title": "Algorithm Selection for Optimal Multi-Agent Path Finding via Graph Embedding",
    "authors": [
      "Carmel Shabalin",
      "Omri Kaduri",
      "Roni Stern"
    ],
    "abstract": "Multi-agent path finding (MAPF) is the problem of finding paths for multiple\nagents such that they do not collide. This problem manifests in numerous\nreal-world applications such as controlling transportation robots in automated\nwarehouses, moving characters in video games, and coordinating self-driving\ncars in intersections. Finding optimal solutions to MAPF is NP-Hard, yet modern\noptimal solvers can scale to hundreds of agents and even thousands in some\ncases. Different solvers employ different approaches, and there is no single\nstate-of-the-art approach for all problems. Furthermore, there are no clear,\nprovable, guidelines for choosing when each optimal MAPF solver to use. Prior\nwork employed Algorithm Selection (AS) techniques to learn such guidelines from\npast data. A major challenge when employing AS for choosing an optimal MAPF\nalgorithm is how to encode the given MAPF problem. Prior work either used\nhand-crafted features or an image representation of the problem. We explore\ngraph-based encodings of the MAPF problem and show how they can be used\non-the-fly with a modern graph embedding algorithm called FEATHER. Then, we\nshow how this encoding can be effectively joined with existing encodings,\nresulting in a novel AS method we call MAPF Algorithm selection via Graph\nembedding (MAG). An extensive experimental evaluation of MAG on several MAPF\nalgorithm selection tasks reveals that it is either on-par or significantly\nbetter than existing methods.",
    "categories": [
      "cs.AI",
      "68T20",
      "I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10827v1",
    "published_date": "2024-06-16 07:41:58 UTC",
    "updated_date": "2024-06-16 07:41:58 UTC"
  },
  {
    "arxiv_id": "2406.10819v2",
    "title": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding",
    "authors": [
      "Dongping Chen",
      "Yue Huang",
      "Siyuan Wu",
      "Jingyu Tang",
      "Liuyi Chen",
      "Yilin Bai",
      "Zhigang He",
      "Chenlong Wang",
      "Huichi Zhou",
      "Yiqiang Li",
      "Tianshuo Zhou",
      "Yue Yu",
      "Chujie Gao",
      "Qihui Zhang",
      "Yi Gui",
      "Zhen Li",
      "Yao Wan",
      "Pan Zhou",
      "Jianfeng Gao",
      "Lichao Sun"
    ],
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.10819v2",
    "published_date": "2024-06-16 06:56:53 UTC",
    "updated_date": "2025-03-24 11:46:14 UTC"
  },
  {
    "arxiv_id": "2406.10816v1",
    "title": "Optimization of Armv9 architecture general large language model inference performance based on Llama.cpp",
    "authors": [
      "Longhao Chen",
      "Yina Zhao",
      "Qiangjun Xie",
      "Qinghua Sheng"
    ],
    "abstract": "This article optimizes the inference performance of the Qwen-1.8B model by\nperforming Int8 quantization, vectorizing some operators in llama.cpp, and\nmodifying the compilation script to improve the compiler optimization level. On\nthe Yitian 710 experimental platform, the prefill performance is increased by\n1.6 times, the decoding performance is increased by 24 times, the memory usage\nis reduced to 1/5 of the original, and the accuracy loss is almost negligible.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.AR",
      "cs.PF"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10816v1",
    "published_date": "2024-06-16 06:46:25 UTC",
    "updated_date": "2024-06-16 06:46:25 UTC"
  },
  {
    "arxiv_id": "2406.10815v1",
    "title": "On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning",
    "authors": [
      "Jeongheon Oh",
      "Kibok Lee"
    ],
    "abstract": "Supervised contrastive representation learning has been shown to be effective\nin various transfer learning scenarios. However, while asymmetric\nnon-contrastive learning (ANCL) often outperforms its contrastive learning\ncounterpart in self-supervised representation learning, the extension of ANCL\nto supervised scenarios is less explored. To bridge the gap, we study ANCL for\nsupervised representation learning, coined SupSiam and SupBYOL, leveraging\nlabels in ANCL to achieve better representations. The proposed supervised ANCL\nframework improves representation learning while avoiding collapse. Our\nanalysis reveals that providing supervision to ANCL reduces intra-class\nvariance, and the contribution of supervision should be adjusted to achieve the\nbest performance. Experiments demonstrate the superiority of supervised ANCL\nacross various datasets and tasks. The code is available at:\nhttps://github.com/JH-Oh-23/Sup-ANCL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10815v1",
    "published_date": "2024-06-16 06:43:15 UTC",
    "updated_date": "2024-06-16 06:43:15 UTC"
  },
  {
    "arxiv_id": "2406.10811v1",
    "title": "LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction",
    "authors": [
      "Meiyun Wang",
      "Kiyoshi Izumi",
      "Hiroki Sakaji"
    ],
    "abstract": "Recently, Large Language Models (LLMs) have attracted significant attention\nfor their exceptional performance across a broad range of tasks, particularly\nin text analysis. However, the finance sector presents a distinct challenge due\nto its dependence on time-series data for complex forecasting tasks. In this\nstudy, we introduce a novel framework called LLMFactor, which employs\nSequential Knowledge-Guided Prompting (SKGP) to identify factors that influence\nstock movements using LLMs. Unlike previous methods that relied on keyphrases\nor sentiment analysis, this approach focuses on extracting factors more\ndirectly related to stock market dynamics, providing clear explanations for\ncomplex temporal changes. Our framework directs the LLMs to create background\nknowledge through a fill-in-the-blank strategy and then discerns potential\nfactors affecting stock prices from related news. Guided by background\nknowledge and identified factors, we leverage historical stock prices in\ntextual format to predict stock movement. An extensive evaluation of the\nLLMFactor framework across four benchmark datasets from both the U.S. and\nChinese stock markets demonstrates its superiority over existing\nstate-of-the-art methods and its effectiveness in financial time-series\nforecasting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL(Findings)2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10811v1",
    "published_date": "2024-06-16 06:20:50 UTC",
    "updated_date": "2024-06-16 06:20:50 UTC"
  },
  {
    "arxiv_id": "2406.10809v1",
    "title": "Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations",
    "authors": [
      "Yoonna Jang",
      "Suhyune Son",
      "Jeongwoo Lee",
      "Junyoung Son",
      "Yuna Hur",
      "Jungwoo Lim",
      "Hyeonseok Moon",
      "Kisu Yang",
      "Heuiseok Lim"
    ],
    "abstract": "Despite the striking advances in recent language generation performance,\nmodel-generated responses have suffered from the chronic problem of\nhallucinations that are either untrue or unfaithful to a given source.\nEspecially in the task of knowledge grounded conversation, the models are\nrequired to generate informative responses, but hallucinated utterances lead to\nmiscommunication. In particular, entity-level hallucination that causes\ncritical misinformation and undesirable conversation is one of the major\nconcerns. To address this issue, we propose a post-hoc refinement method called\nREM. It aims to enhance the quality and faithfulness of hallucinated utterances\nby refining them based on the source knowledge. If the generated utterance has\na low source-faithfulness score with the given knowledge, REM mines the key\nentities in the knowledge and implicitly uses them for refining the utterances.\nWe verify that our method reduces entity hallucination in the utterance. Also,\nwe show the adaptability and efficacy of REM with extensive experiments and\ngenerative results. Our code is available at https://github.com/YOONNAJANG/REM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2023",
    "pdf_url": "http://arxiv.org/pdf/2406.10809v1",
    "published_date": "2024-06-16 06:12:47 UTC",
    "updated_date": "2024-06-16 06:12:47 UTC"
  },
  {
    "arxiv_id": "2406.10807v2",
    "title": "Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification",
    "authors": [
      "Oluwaseun T. Ajayi",
      "Yu Cheng"
    ],
    "abstract": "With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19)\npandemic, there are still uncertainties that are yet to be discovered about its\nspread, future impact, and resurgence. In this paper, we present a three-stage\ndata-driven approach to distill the hidden information about COVID-19. The\nfirst stage employs a Bayesian network structure learning method to identify\nthe causal relationships among COVID-19 symptoms and their intrinsic\ndemographic variables. As a second stage, the output from the Bayesian network\nstructure learning, serves as a useful guide to train an unsupervised machine\nlearning (ML) algorithm that uncovers the similarities in patients' symptoms\nthrough clustering. The final stage then leverages the labels obtained from\nclustering to train a demographic symptom identification (DSID) model which\npredicts a patient's symptom class and the corresponding demographic\nprobability distribution. We applied our method on the COVID-19 dataset\nobtained from the Centers for Disease Control and Prevention (CDC) in the\nUnited States. Results from the experiments show a testing accuracy of 99.99%,\nas against the 41.15% accuracy of a heuristic ML method. This strongly reveals\nthe viability of our Bayesian network and ML approach in understanding the\nrelationship between the virus symptoms, and providing insights on patients'\nstratification towards reducing the severity of the virus.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10807v2",
    "published_date": "2024-06-16 05:43:24 UTC",
    "updated_date": "2024-06-18 02:20:19 UTC"
  },
  {
    "arxiv_id": "2406.10806v2",
    "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
    "authors": [
      "Marcos Piau",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "abstract": "Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including pretraining data quality, optimization\nstrategies, and multi-epoch pretraining. Perhaps surprisingly, their impact\nremains subtle compared to our baseline. We release $\\texttt{ptt5-v2}$\npretrained checkpoints and their MonoT5-based finetuned $\\texttt{MonoPTT5}$\nrerankers on HuggingFace in their respective collections at\n\\url{https://huggingface.co/unicamp-dl}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10806v2",
    "published_date": "2024-06-16 05:17:56 UTC",
    "updated_date": "2024-11-18 02:19:02 UTC"
  },
  {
    "arxiv_id": "2406.10803v1",
    "title": "HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
    "authors": [
      "William Watson",
      "Nicole Cho",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "abstract": "A myriad of different Large Language Models (LLMs) face a common challenge in\ncontextually analyzing table question-answering tasks. These challenges are\nengendered from (1) finite context windows for large tables, (2) multi-faceted\ndiscrepancies amongst tokenization patterns against cell boundaries, and (3)\nvarious limitations stemming from data confidentiality in the process of using\nexternal models such as gpt-3.5-turbo. We propose a cooperative game dubbed\n\"HiddenTables\" as a potential resolution to this challenge. In essence,\n\"HiddenTables\" is played between the code-generating LLM \"Solver\" and the\n\"Oracle\" which evaluates the ability of the LLM agents to solve Table QA tasks.\nThis game is based on natural language schemas and importantly, ensures the\nsecurity of the underlying data. We provide evidential experiments on a diverse\nset of tables that demonstrate an LLM's collective inability to generalize and\nperform on complex queries, handle compositional dependencies, and align\nnatural language to programmatic commands when concrete table schemas are\nprovided. Unlike encoder-based models, we have pushed the boundaries of\n\"HiddenTables\" to not be limited by the number of rows - therefore we exhibit\nimproved efficiency in prompt and completion tokens. Our infrastructure has\nspawned a new dataset \"PyQTax\" that spans across 116,671 question-table-answer\ntriplets and provides additional fine-grained breakdowns & labels for varying\nquestion taxonomies. Therefore, in tandem with our academic contributions\nregarding LLMs' deficiency in TableQA tasks, \"HiddenTables\" is a tactile\nmanifestation of how LLMs can interact with massive datasets while ensuring\ndata security and minimizing generation costs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of the 2023 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2023)",
    "pdf_url": "http://arxiv.org/pdf/2406.10803v1",
    "published_date": "2024-06-16 04:53:29 UTC",
    "updated_date": "2024-06-16 04:53:29 UTC"
  },
  {
    "arxiv_id": "2406.10802v1",
    "title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs",
    "authors": [
      "Aihua Pei",
      "Zehua Yang",
      "Shunan Zhu",
      "Ruoxi Cheng",
      "Ju Jia",
      "Lina Wang"
    ],
    "abstract": "Existing frameworks for assessing robustness of large language models (LLMs)\noverly depend on specific benchmarks, increasing costs and failing to evaluate\nperformance of LLMs in professional domains due to dataset limitations. This\npaper proposes a framework that systematically evaluates the robustness of LLMs\nunder adversarial attack scenarios by leveraging knowledge graphs (KGs). Our\nframework generates original prompts from the triplets of knowledge graphs and\ncreates adversarial prompts by poisoning, assessing the robustness of LLMs\nthrough the results of these adversarial attacks. We systematically evaluate\nthe effectiveness of this framework and its modules. Experiments show that\nadversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >\nGPT-3.5-turbo, and the robustness of large language models is influenced by the\nprofessional domains in which they operate.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10802v1",
    "published_date": "2024-06-16 04:48:43 UTC",
    "updated_date": "2024-06-16 04:48:43 UTC"
  },
  {
    "arxiv_id": "2406.10796v2",
    "title": "Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction Data",
    "authors": [
      "Gabe Guo",
      "Tristan Saidi",
      "Maxwell Terban",
      "Michele Valsecchi",
      "Simon JL Billinge",
      "Hod Lipson"
    ],
    "abstract": "A major challenge in materials science is the determination of the structure\nof nanometer sized objects. Here we present a novel approach that uses a\ngenerative machine learning model based on diffusion processes that is trained\non 45,229 known structures. The model factors both the measured diffraction\npattern as well as relevant statistical priors on the unit cell of atomic\ncluster structures. Conditioned only on the chemical formula and the\ninformation-scarce finite-size broadened powder diffraction pattern, we find\nthat our model, PXRDnet, can successfully solve simulated nanocrystals as small\nas 10 angstroms across 200 materials of varying symmetry and complexity,\nincluding structures from all seven crystal systems. We show that our model can\nsuccessfully and verifiably determine structural candidates four out of five\ntimes, with average error among these candidates being only 7% (as measured by\npost-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solving\nstructures from noisy diffraction patterns gathered in real-world experiments.\nWe suggest that data driven approaches, bootstrapped from theoretical\nsimulation, will ultimately provide a path towards determining the structure of\npreviously unsolved nano-materials.",
    "categories": [
      "physics.comp-ph",
      "cond-mat.mes-hall",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10796v2",
    "published_date": "2024-06-16 03:45:03 UTC",
    "updated_date": "2024-10-31 17:29:37 UTC"
  },
  {
    "arxiv_id": "2407.12007v2",
    "title": "People will agree what I think: Investigating LLM's False Consensus Effect",
    "authors": [
      "Junhyuk Choi",
      "Yeseon Hong",
      "Bugeun Kim"
    ],
    "abstract": "Large Language Models (LLMs) have been recently adopted in interactive\nsystems requiring communication. As the false belief in a model can harm the\nusability of such systems, LLMs should not have cognitive biases that humans\nhave. Psychologists especially focus on the False Consensus Effect (FCE), a\ncognitive bias where individuals overestimate the extent to which others share\ntheir beliefs or behaviors, because FCE can distract smooth communication by\nposing false beliefs. However, previous studies have less examined FCE in LLMs\nthoroughly, which needs more consideration of confounding biases, general\nsituations, and prompt changes. Therefore, in this paper, we conduct two\nstudies to examine the FCE phenomenon in LLMs. In Study 1, we investigate\nwhether LLMs have FCE. In Study 2, we explore how various prompting styles\naffect the demonstration of FCE. As a result of these studies, we identified\nthat popular LLMs have FCE. Also, the result specifies the conditions when FCE\nbecomes more or less prevalent compared to normal usage.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2407.12007v2",
    "published_date": "2024-06-16 03:29:28 UTC",
    "updated_date": "2025-02-08 09:21:17 UTC"
  },
  {
    "arxiv_id": "2406.10787v3",
    "title": "Evidential Uncertainty Sets in Deep Classifiers Using Conformal Prediction",
    "authors": [
      "Hamed Karimi",
      "Reza Samavi"
    ],
    "abstract": "In this paper, we propose Evidential Conformal Prediction (ECP) method for\nimage classifiers to generate the conformal prediction sets. Our method is\ndesigned based on a non-conformity score function that has its roots in\nEvidential Deep Learning (EDL) as a method of quantifying model (epistemic)\nuncertainty in DNN classifiers. We use evidence that are derived from the logit\nvalues of target labels to compute the components of our non-conformity score\nfunction: the heuristic notion of uncertainty in CP, uncertainty surprisal, and\nexpected utility. Our extensive experimental evaluation demonstrates that ECP\noutperforms three state-of-the-art methods for generating CP sets, in terms of\ntheir set sizes and adaptivity while maintaining the coverage of true labels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in 13th Symposium on Conformal and Probabilistic Prediction\n  with Applications (COPA2024). To be published in the Proceedings of Machine\n  Learning Research (PMLR), vol. 230, 2024 (25 Pages)",
    "pdf_url": "http://arxiv.org/pdf/2406.10787v3",
    "published_date": "2024-06-16 03:00:16 UTC",
    "updated_date": "2024-07-30 04:00:44 UTC"
  },
  {
    "arxiv_id": "2406.10786v2",
    "title": "Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems at once",
    "authors": [
      "Zhengxiang Wang",
      "Jordan Kodner",
      "Owen Rambow"
    ],
    "abstract": "Recent studies have proposed placing multiple problems in a single prompt to\nimprove input token utilization for a more efficient LLM inference. We call\nthis MPP, in contrast to conventional SPP that prompts an LLM with a single\nproblem at a time. While MPP has been shown to work comparably well or even\nbetter than SPP under few-shot settings, its zero-shot performance is\nunderexplored, which better reveals the innate multiple problem handling\ncapabilities of LLMs. To address that, we study the zero-shot MPP performance\nof various LLMs on 6 classification and 12 reasoning benchmarks and confirm\nthat LLMs are competent zero-shot multi-problem solvers. We also examine the\nconditions of effectiveness of zero-shot MPP and explore several model-level\nfactors that may enable MPP. We observe that LLMs consistently perform worse\nwith selecting indices of texts of a given class label and with multiple\nmixed-source reasoning problems, indicating a lack of true understanding. We\nalso find that instruction tuning is an important factor than enhances MPP.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 11 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.10786v2",
    "published_date": "2024-06-16 02:52:32 UTC",
    "updated_date": "2024-10-21 04:09:33 UTC"
  },
  {
    "arxiv_id": "2406.10785v1",
    "title": "ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation",
    "authors": [
      "Yurun Song",
      "Junchen Zhao",
      "Ian G. Harris",
      "Sangeetha Abdu Jyothi"
    ],
    "abstract": "This study introduces an approach to optimize Parameter Efficient Fine Tuning\n(PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank\nAdaptation (ShareLoRA). By strategically deploying ShareLoRA across different\nlayers and adapting it for the Query, Key, and Value components of\nself-attention layers, we achieve a substantial reduction in the number of\ntraining parameters and memory usage. Importantly, ShareLoRA not only maintains\nmodel performance but also exhibits robustness in both classification and\ngeneration tasks across a variety of models, including RoBERTa, GPT-2, LLaMA\nand LLaMA2. It demonstrates superior transfer learning capabilities compared to\nstandard LoRA applications and mitigates overfitting by sharing weights across\nlayers. Our findings affirm that ShareLoRA effectively boosts parameter\nefficiency while ensuring scalable and high-quality performance across\ndifferent language model architectures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10785v1",
    "published_date": "2024-06-16 02:52:28 UTC",
    "updated_date": "2024-06-16 02:52:28 UTC"
  },
  {
    "arxiv_id": "2406.10775v2",
    "title": "A Rate-Distortion View of Uncertainty Quantification",
    "authors": [
      "Ifigeneia Apostolopoulou",
      "Benjamin Eysenbach",
      "Frank Nielsen",
      "Artur Dubrawski"
    ],
    "abstract": "In supervised learning, understanding an input's proximity to the training\ndata can help a model decide whether it has sufficient evidence for reaching a\nreliable prediction. While powerful probabilistic models such as Gaussian\nProcesses naturally have this property, deep neural networks often lack it. In\nthis paper, we introduce Distance Aware Bottleneck (DAB), i.e., a new method\nfor enriching deep neural networks with this property. Building on prior\ninformation bottleneck approaches, our method learns a codebook that stores a\ncompressed representation of all inputs seen during training. The distance of a\nnew example from this codebook can serve as an uncertainty estimate for the\nexample. The resulting model is simple to train and provides deterministic\nuncertainty estimates by a single forward pass. Finally, our method achieves\nbetter out-of-distribution (OOD) detection and misclassification prediction\nthan prior methods, including expensive ensemble methods, deep kernel Gaussian\nProcesses, and approaches based on the standard information bottleneck.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10775v2",
    "published_date": "2024-06-16 01:33:22 UTC",
    "updated_date": "2024-06-18 12:41:43 UTC"
  },
  {
    "arxiv_id": "2406.10773v1",
    "title": "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles",
    "authors": [
      "Filip Trhlik",
      "Pontus Stenetorp"
    ],
    "abstract": "Large language models (LLMs) are increasingly being utilised across a range\nof tasks and domains, with a burgeoning interest in their application within\nthe field of journalism. This trend raises concerns due to our limited\nunderstanding of LLM behaviour in this domain, especially with respect to\npolitical bias. Existing studies predominantly focus on LLMs undertaking\npolitical questionnaires, which offers only limited insights into their biases\nand operational nuances. To address this gap, our study establishes a new\ncurated dataset that contains 2,100 human-written articles and utilises their\ndescriptions to generate 56,700 synthetic articles using nine LLMs. This\nenables us to analyse shifts in properties between human-authored and\nmachine-generated articles, with this study focusing on political bias,\ndetecting it using both supervised models and LLMs. Our findings reveal\nsignificant disparities between base and instruction-tuned LLMs, with\ninstruction-tuned models exhibiting consistent political bias. Furthermore, we\nare able to study how LLMs behave as classifiers, observing their display of\npolitical bias even in this role. Overall, for the first time within the\njournalistic domain, this study outlines a framework and provides a structured\ndataset for quantifiable experiments, serving as a foundation for further\nresearch into LLM political bias and its implications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10773v1",
    "published_date": "2024-06-16 01:32:04 UTC",
    "updated_date": "2024-06-16 01:32:04 UTC"
  }
]
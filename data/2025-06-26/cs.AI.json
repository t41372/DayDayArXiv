{
  "date": "2025-06-26",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-26 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹ **â€œæ™ºèƒ½æœ¬è´¨â€** çš„æ·±åº¦æ¢è®¨ã€‚å¤§ä½¬ä»¬ï¼ˆå¦‚ Li Fei-Fei, Jitendra Malik, Daniela Rusï¼‰æ‰å †åœ¨å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰å’Œç©ºé—´/ç‰©ç†æ¨ç†ä¸Šå‘åŠ›ï¼›åŒæ—¶ï¼Œç¤¾åŒºå¼€å§‹åæ€ LLM çš„â€œç†è§£â€æ˜¯å¦åªæ˜¯â€œæ³¢å°†é‡‘æ‘ï¼ˆPotemkinï¼‰â€å¼çš„å‡è±¡ï¼Œå¹¶å°è¯•é€šè¿‡å› æœæ¨ç†ã€å±‚æ¬¡åŒ– RNN æ¶æ„ï¼ˆä»…27Må‚æ•°æŒ‘æˆ˜ ARC åŸºå‡†ï¼‰æ¥å¯»æ‰¾çªç ´ã€‚æ­¤å¤–ï¼Œå…³äº AI å¦‚ä½•æ”¹å˜ç§‘å­¦è‹±è¯­ç”¨è¯çš„å…ƒåˆ†æä¹Ÿéå¸¸æœ‰è¶£ã€‚\n\n---\n\n### ğŸš€ é¡¶å°–å­¦è€…ä¸æ ¸å¿ƒçªç ´ (Star Power & Foundations)\n\n**40. Spatial Mental Modeling from Limited Views**\n**(ä»æœ‰é™è§†è§’æ„å»ºç©ºé—´å¿ƒæ™ºæ¨¡å‹)**\n> **Authors:** Baiqiao Yin, ..., Jiajun Wu, Li Fei-Fei\n> **å…³é”®ç‚¹:** æé£é£å›¢é˜Ÿæ–°ä½œï¼Œç›´æŒ‡ VLM çš„ç©ºé—´æ¨ç†çŸ­æ¿ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **MindCube** åŸºå‡†æµ‹è¯•ã€‚äººç±»èƒ½é€šè¿‡æœ‰é™è§†è§’åœ¨è„‘æµ·ä¸­æ„å»ºæœªè§ç©ºé—´çš„â€œå¿ƒæ™ºæ¨¡å‹â€ï¼Œè€Œç°æœ‰çš„ VLM è¡¨ç°æ¥è¿‘éšæœºã€‚ä½œè€…æå‡ºä¸€ç§ **\"Map-then-Reason\" (å…ˆå»ºå›¾åæ¨ç†)** çš„ååŒæ–¹æ³•ï¼Œå¼ºåˆ¶æ¨¡å‹å…ˆç”Ÿæˆè®¤çŸ¥åœ°å›¾ï¼ˆcognitive mapï¼‰å†åŸºäºæ­¤è¿›è¡Œæ¨ç†ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼Œå°†å‡†ç¡®ç‡ä» 37.8% æå‡è‡³ 70.7%ã€‚è¿™è¯æ˜äº†æ˜¾å¼ç»“æ„åŒ–ç©ºé—´è¡¨å¾å¯¹ç†è§£ä¸å¯è§ç©ºé—´çš„é‡è¦æ€§ã€‚\n\n**23. Whole-Body Conditioned Egocentric Video Prediction**\n**(å…¨èº«æ¡ä»¶ä¸‹çš„ç¬¬ä¸€äººç§°è§†é¢‘é¢„æµ‹)**\n> **Authors:** Yutong Bai, ..., Yann LeCun, Trevor Darrell, Jitendra Malik\n> **å…³é”®ç‚¹:** Jitendra Malik å’Œ Yann LeCun å‚ä¸ï¼Œå…·èº«æ™ºèƒ½è§†è§’çš„æœªæ¥é¢„æµ‹ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **PEVA** æ¨¡å‹ï¼Œåˆ©ç”¨ 3D èº«ä½“å§¿æ€ï¼ˆActionï¼‰æ¥é¢„æµ‹æœªæ¥çš„ç¬¬ä¸€äººç§°è§†é¢‘ï¼ˆEgocentric Videoï¼‰ã€‚ä½¿ç”¨è‡ªå›å½’æ¡ä»¶ Diffusion Transformer åœ¨ Nymeria æ•°æ®é›†ä¸Šè®­ç»ƒã€‚è¿™æ˜¯ä¸€ä¸ªå°è¯•æ¨¡æ‹Ÿç‰©ç†äººç±»è¡Œä¸ºå¦‚ä½•å¡‘é€ ç¬¬ä¸€äººç§°è§†è§‰ç¯å¢ƒçš„åˆæ­¥å°è¯•ï¼Œæ˜¯æ„å»ºå…·èº«ä¸–ç•Œæ¨¡å‹çš„é‡è¦ä¸€æ­¥ã€‚\n\n**51. Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models**\n**(åŸºäºåˆ†å±‚è¾“å…¥ä¾èµ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ•´ä½“æ‰‹æœ¯é˜¶æ®µè¯†åˆ«)**\n> **Authors:** Haoyang Wu, ..., Daniela Rus\n> **å…³é”®ç‚¹:** Daniela Rus å‚ä¸ï¼ŒSSM (Mambaç±») åœ¨é•¿è§†é¢‘å¤„ç†ä¸­çš„åº”ç”¨ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹æ‰‹æœ¯è§†é¢‘é•¿åºåˆ—åˆ†æä¸­ Transformer è®¡ç®—é‡è¿‡å¤§çš„é—®é¢˜ï¼Œåˆ©ç”¨ **çŠ¶æ€ç©ºé—´æ¨¡å‹ (State Space Models, SSMs)** çš„çº¿æ€§æ‰©å±•ç‰¹æ€§ã€‚æå‡ºäº†åˆ†å±‚çš„ SSM æ¶æ„ï¼Œåˆ†åˆ«æ•æ‰å±€éƒ¨åŠ¨æ€å’Œå…¨å±€æ—¶é—´ä¾èµ–ï¼Œåœ¨ Cholec80 ç­‰æ•°æ®é›†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰ SOTAã€‚è¿™è¡¨æ˜ SSM åœ¨é•¿è§†é¢‘åŒ»ç–—åˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€å› æœä¸æ–°æ¶æ„ (Reasoning, Causal & New Architectures)\n\n**15. Hierarchical Reasoning Model**\n**(åˆ†å±‚æ¨ç†æ¨¡å‹)**\n> **Authors:** Guan Wang, ..., Yasin Abbasi Yadkori\n> **å…³é”®ç‚¹:** **27M å‚æ•°**çš„å°æ¨¡å‹åœ¨ ARC ä¸ŠåŠæ‰“å¤§æ¨¡å‹ï¼ŸRNN çš„é€†è¢­ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** å—åˆ°äººè„‘åˆ†å±‚å¤„ç†çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹å¾ªç¯æ¶æ„ **HRM**ã€‚å®ƒåŒ…å«è´Ÿè´£æŠ½è±¡è§„åˆ’çš„é«˜å±‚æ¨¡å—å’Œè´Ÿè´£ç»†èŠ‚è®¡ç®—çš„ä½å±‚æ¨¡å—ã€‚æƒŠäººçš„æ˜¯ï¼Œè¿™ä¸ªä»… **27M å‚æ•°** çš„æ¨¡å‹ï¼Œåœ¨æ²¡æœ‰ä»»ä½• CoT æ•°æ®æˆ–é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å¤æ‚çš„æ•°ç‹¬å’Œ **ARC (Abstraction and Reasoning Corpus)** åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¿‡äº†æ‹¥æœ‰æ›´é•¿ä¸Šä¸‹æ–‡çª—å£çš„è¶…å¤§æ¨¡å‹ã€‚è¿™æš—ç¤ºäº†é€šç”¨æ¨ç†å¯èƒ½ä¸éœ€è¦å·¨å¤§çš„å‚æ•°é‡ï¼Œè€Œéœ€è¦æ­£ç¡®çš„å½’çº³åç½®ï¼ˆInductive Biasï¼‰ã€‚\n\n**31. Potemkin Understanding in Large Language Models**\n**(å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„â€œæ³¢å°†é‡‘â€å¼ç†è§£)**\n> **Authors:** Marina Mancoridis, ..., Sendhil Mullainathan\n> **å…³é”®ç‚¹:** LLM æ˜¯çœŸæ‡‚è¿˜æ˜¯å‡æ‡‚ï¼Ÿ\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** â€œæ³¢å°†é‡‘æ‘â€æ¯”å–»è™šå‡çš„ç¹è£ã€‚ä½œè€…è´¨ç–‘åŸºäº Benchmark çš„è¯„ä¼°ï¼Œæå‡ºå¦‚æœ LLM åœ¨æ¦‚å¿µä¸Šçš„è¯¯è§£ä¸äººç±»ä¸ä»…ä¸åŒè€Œä¸”â€œä¸å¯è°ƒå’Œâ€ï¼Œé‚£å°±æ˜¯â€œæ³¢å°†é‡‘å¼ç†è§£â€ï¼ˆå³å‡è±¡ï¼‰ã€‚ç ”ç©¶å‘ç°è¿™ç§å‡è±¡åœ¨è·¨æ¨¡å‹ã€è·¨é¢†åŸŸä¸­æ™®éå­˜åœ¨ï¼Œæ­ç¤ºäº† LLM å†…éƒ¨æ¦‚å¿µè¡¨å¾çš„æ·±å±‚ä¸è¿è´¯æ€§ã€‚\n\n**67. Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?**\n**(æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å› æœæ¨ç†ï¼šç°å®è¿˜æ˜¯å¹»å½±ï¼Ÿ)**\n> **Authors:** Haoang Chi, ..., Bo Han\n> **å…³é”®ç‚¹:** LLM ç›®å‰åªèƒ½åšæµ…å±‚å› æœæ¨ç† (Level-1)ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æŒ‡å‡º LLM ç›®å‰ä¸»è¦ä¾èµ–å‚æ•°ä¸­çš„å› æœçŸ¥è¯†ï¼ˆLevel-1ï¼‰ï¼Œè€Œç¼ºä¹çœŸæ­£çš„äººç±»çº§å› æœæ¨ç†ï¼ˆLevel-2ï¼‰ã€‚æå‡ºäº† **CausalProbe-2024** åŸºå‡†ï¼Œå‘ç° LLM åœ¨æ–°è¯­æ–™ä¸Šè¡¨ç°å¤§å¹…ä¸‹é™ã€‚æå‡ºäº† $G^2$-Reasoner æ–¹æ³•ï¼Œç»“åˆé€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘ Promptï¼Œè¯•å›¾æ¡¥æ¥è¿™ä¸€å·®è·ã€‚\n\n**13. THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?**\n**(THE-Tree: è¿½è¸ªå†å²æ¼”å˜èƒ½å¦å¢å¼ºç§‘å­¦éªŒè¯ä¸æ¨ç†ï¼Ÿ)**\n> **Authors:** Xin Wang, ..., Kaicheng Yu\n> **å…³é”®ç‚¹:** ç”¨ AI æ„å»ºç§‘æŠ€æ ‘æ¥éªŒè¯ AI ç”Ÿæˆçš„ Ideaã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **THE-Tree (Technology History Evolution Tree)**ï¼Œä»æ–‡çŒ®ä¸­æ„å»ºé¢†åŸŸç‰¹å®šçš„æ¼”åŒ–æ ‘ã€‚åˆ©ç”¨â€œæ€è€ƒ-è¡¨è¾¾-å¼•ç”¨-éªŒè¯â€çš„è¿‡ç¨‹ï¼Œæ£€æŸ¥ AI ç”Ÿæˆçš„ç§‘å­¦ Idea çš„æ–°é¢–æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™ç§ç»“æ„åŒ–çš„å†å²æ•°æ®æ¯”å•çº¯çš„å¼•æ–‡ç½‘ç»œæ›´èƒ½é¢„æµ‹æœªæ¥çš„ç§‘å­¦å‘å±•ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸ä¸–ç•Œæ¨¡å‹ (Embodied AI & World Models)\n\n**28. WorldVLA: Towards Autoregressive Action World Model**\n**(WorldVLA: è¿ˆå‘è‡ªå›å½’åŠ¨ä½œä¸–ç•Œæ¨¡å‹)**\n> **Authors:** Jun Cen, ..., Deli Zhao, Hao Chen\n> **å…³é”®ç‚¹:** VLA ä¸ä¸–ç•Œæ¨¡å‹çš„ç»Ÿä¸€ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** å°†è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹é›†æˆåœ¨å•ä¸€æ¡†æ¶ä¸­ã€‚ä¸–ç•Œæ¨¡å‹åˆ©ç”¨åŠ¨ä½œé¢„æµ‹æœªæ¥å›¾åƒï¼ˆå­¦ä¹ ç‰©ç†ï¼‰ï¼ŒåŠ¨ä½œæ¨¡å‹åŸºäºè§‚å¯Ÿç”ŸæˆåŠ¨ä½œã€‚ä¸¤è€…ç›¸äº’å¢å¼ºï¼Œä¼˜äºç‹¬ç«‹çš„æ¨¡å‹ã€‚\n\n**22. SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents**\n**(SEEA-R1: ç”¨äºè‡ªè¿›åŒ–å…·èº«æ™ºèƒ½ä½“çš„æ ‘ç»“æ„å¼ºåŒ–å¾®è°ƒ)**\n> **Authors:** Wanxin Tian, ..., Jian Tang\n> **å…³é”®ç‚¹:** å°† DeepSeek-R1 å¼çš„å¼ºåŒ–å¾®è°ƒå¼•å…¥å…·èº«æ™ºèƒ½ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº†å…·èº«ä»»åŠ¡ä¸­å¥–åŠ±ç¨€ç–å’Œä¾èµ–æ‰‹å·¥å¥–åŠ±å‡½æ•°çš„é—®é¢˜ã€‚æå‡ºäº† Tree-GRPOï¼ˆç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰å’Œå¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆMGRMï¼‰ã€‚åœ¨ ALFWorld åŸºå‡†ä¸Šè¶…è¶Šäº† GPT-4oã€‚\n\n**111. DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing**\n**(DFVEdit: ç”¨äºé›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘çš„æ¡ä»¶ Delta æµå‘é‡)**\n> **Authors:** Lingling Cai, ..., Kejie Huang\n> **å…³é”®ç‚¹:** Video DiTs çš„é«˜æ•ˆç¼–è¾‘ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹è§†é¢‘ Diffusion Transformer (Video DiTs) çš„ç¼–è¾‘æ–¹æ³•ã€‚ä¸éœ€è¦ç¹é‡çš„ Attention ä¿®æ”¹æˆ–å¾®è°ƒï¼Œé€šè¿‡æµå˜æ¢ï¼ˆFlow Transformationï¼‰ç›´æ¥æ“ä½œ Latentã€‚é€Ÿåº¦æå‡ 20 å€ï¼Œæ˜¾å­˜å‡å°‘ 85%ï¼Œå®ç°äº†åœ¨ CogVideoX å’Œ Wan2.1 ä¸Šçš„ SOTA ç¼–è¾‘æ•ˆæœã€‚\n\n---\n\n### ğŸ“Š æœ‰è¶£çš„åˆ†æä¸åº”ç”¨ (Analysis & Applications)\n\n**2. Exploring the Structure of AI-Induced Language Change in Scientific English**\n**(æ¢ç´¢ AI è¯±å¯¼çš„ç§‘å­¦è‹±è¯­è¯­è¨€ç»“æ„å˜åŒ–)**\n> **Authors:** Riley Galpin, ..., Tom S. Juzek\n> **å…³é”®ç‚¹:** ä½ çš„è®ºæ–‡é‡Œæœ‰å¤šå°‘ä¸ª \"Delve\"?\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è‡ª 2022 å¹´ä»¥æ¥ï¼Œç§‘å­¦è®ºæ–‡ä¸­ **\"delve\"**, **\"intricate\"**, **\"crucial\"** ç­‰è¯çš„ä½¿ç”¨é¢‘ç‡æ¿€å¢ã€‚ç ”ç©¶å‘ç°è¿™ä¸ä»…ä»…æ˜¯åŒä¹‰è¯æ›¿æ¢ï¼Œè€Œæ˜¯æ•´ä¸ªè¯­ä¹‰ç°‡ï¼ˆSemantic Clustersï¼‰çš„æ•´ä½“åç§»ã€‚è¿™è¯å®äº† LLM æ­£åœ¨é‡å¡‘ç§‘å­¦ç•Œçš„è¯­è¨€ä¹ æƒ¯ã€‚\n\n**122. LLM-guided Chemical Process Optimization with a Multi-Agent Approach**\n**(åŸºäºå¤šæ™ºèƒ½ä½“æ–¹æ³•çš„ LLM å¼•å¯¼åŒ–å·¥è¿‡ç¨‹ä¼˜åŒ–)**\n> **Authors:** Tong Zeng, ..., Amir Barati Farimani\n> **å…³é”®ç‚¹:** OpenAI o3 æ¨¡å‹åœ¨åŒ–å·¥ä¸­çš„åº”ç”¨ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ä½¿ç”¨åŸºäº OpenAI o3 çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåœ¨æ²¡æœ‰é¢„å®šä¹‰æ“ä½œçº¦æŸçš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»æ¨æ–­çº¦æŸå¹¶ä¼˜åŒ–åŒ–å·¥è¿‡ç¨‹ã€‚ç›¸æ¯”ä¼ ç»Ÿç½‘æ ¼æœç´¢ï¼Œæ—¶é—´å‡å°‘ 31 å€ã€‚è¯æ˜äº†å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼ˆo1, o3ï¼‰åœ¨è§£å†³è¿™ç±»æœªå®šä¹‰çº¦æŸé—®é¢˜æ—¶æ˜¯å¿…é¡»çš„ã€‚\n\n**69. BitMark: Watermarking Bitwise Autoregressive Image Generative Models**\n**(BitMark: æ¯”ç‰¹ä½è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ°´å°æŠ€æœ¯)**\n> **Authors:** Louis Kerner, ..., Adam Dziedzic\n> **å…³é”®ç‚¹:** é˜²æ­¢æ¨¡å‹åå¡Œçš„æ•°æ®æ±¡æŸ“å¯¹ç­–ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹å¦‚ VAR è¿™ç§åŸºäº Token çš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæå‡ºåœ¨æ¯”ç‰¹ï¼ˆBitï¼‰çº§åˆ«åµŒå…¥æ°´å°ã€‚è¿™ç§æ°´å°å…·æœ‰é«˜â€œæ”¾å°„æ€§â€ï¼ˆradioactivityï¼‰ï¼Œå³ä½¿ç”¨å¸¦æ°´å°å›¾ç‰‡è®­ç»ƒçš„æ–°æ¨¡å‹ï¼Œå…¶ç”Ÿæˆçš„å›¾ç‰‡ä¹Ÿä¼šå¸¦æœ‰è¯¥æ°´å°ï¼Œæœ‰åŠ©äºè¿½è¸ªæ•°æ®æ¥æºï¼Œé˜²æ­¢æ¨¡å‹å› è®­ç»ƒè‡ªèº«ç”Ÿæˆçš„æ•°æ®è€Œå‘ç”Ÿåå¡Œã€‚\n\n---\n\n### ğŸ©º åŒ»ç–—ä¸ç§‘å­¦ (Medical & Science)\n\n*   **105. Segment Anything in Pathology Images with Natural Language:** å‘å¸ƒäº† **PathSegmentor** å’Œæœ€å¤§çš„ç—…ç†åˆ†å‰²æ•°æ®é›† PathSeg (275k triples)ï¼Œæ”¯æŒç”¨è‡ªç„¶è¯­è¨€ Prompt åˆ†å‰²ç—…ç†å›¾åƒã€‚\n*   **3. CAT-SG (Cataract Surgery Scene Graph):** ç™½å†…éšœæ‰‹æœ¯åœºæ™¯å›¾æ•°æ®é›†ï¼Œä¸ä»…æ˜¯æ£€æµ‹å·¥å…·ï¼Œè¿˜åŒ…å«å·¥å…·-ç»„ç»‡äº¤äº’å’Œæ—¶åºä¾èµ–ã€‚\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸éšç§ (Security)\n\n*   **8. A Survey on Model Extraction Attacks and Defenses:** å…³äº LLM æ¨¡å‹æå–æ”»å‡»ï¼ˆå·æ¨¡å‹ï¼‰çš„ç»¼è¿°ã€‚\n*   **124. ZKPROV:** åˆ©ç”¨é›¶çŸ¥è¯†è¯æ˜ï¼ˆZero-Knowledge Proofsï¼‰éªŒè¯ LLM çš„è®­ç»ƒæ•°æ®æ¥æºï¼Œè€Œä¸æ³„éœ²æ•°æ®æœ¬èº«ã€‚\n\n### ğŸ® æ¸¸æˆä¸æ¨¡æ‹Ÿ (Simulation)\n\n*   **5. CitySim:** åŸºäº LLM Agent çš„å¤§è§„æ¨¡åŸå¸‚æ¨¡æ‹Ÿå™¨ï¼Œæ¨¡æ‹Ÿæ•°ä¸‡ä¸ª Agent çš„æ—¥å¸¸ï¼Œæ¯”ä¼ ç»Ÿè§„åˆ™æ¨¡å‹æ›´é€¼çœŸã€‚\n*   **36. Ad-Hoc Human-AI Coordination Challenge:** åŸºäº Hanabi æ¸¸æˆçš„è¯„ä¼°ï¼Œè§£å†³äº†äººç±»è¯„ä¼°éš¾çš„é—®é¢˜ï¼Œå‘å¸ƒäº† Human Proxy Agentsã€‚",
  "papers": [
    {
      "arxiv_id": "2507.02937v2",
      "title": "FoGE: Fock Space inspired encoding for graph prompting",
      "title_zh": "FoGEï¼šå—ç¦å…‹ç©ºé—´å¯å‘çš„å›¾æç¤ºç¼–ç ",
      "authors": [
        "Sotirios Panagiotis Chytas",
        "Rudrasis Chakraborty",
        "Vikas Singh"
      ],
      "abstract": "Recent results show that modern Large Language Models (LLM) are indeed capable of understanding and answering questions about structured data such as graphs. This new paradigm can lead to solutions that require less supervision while, at the same time, providing a model that can generalize and answer questions beyond the training labels. Existing proposals often use some description of the graph to create an ``augmented'' prompt fed to the LLM. For a chosen class of graphs, if a well-tailored graph encoder is deployed to play together with a pre-trained LLM, the model can answer graph-related questions well. Existing solutions to graph-based prompts range from graph serialization to graph transformers. In this work, we show that the use of a parameter-free graph encoder based on Fock space representations, a concept borrowed from mathematical physics, is remarkably versatile in this problem setting. The simple construction, inherited directly from the theory with a few small adjustments, can provide rich and informative graph encodings, for a wide range of different graphs. We investigate the use of this idea for prefix-tuned prompts leveraging the capabilities of a pre-trained, frozen LLM. The modifications lead to a model that can answer graph-related questions -- from simple graphs to proteins to hypergraphs -- effectively and with minimal, if any, adjustments to the architecture. Our work significantly simplifies existing solutions and generalizes well to multiple different graph-based structures effortlessly.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FoGEï¼Œä¸€ç§å—æ•°å­¦ç‰©ç†ä¸­Fock Spaceè¡¨ç¤ºæ³•å¯å‘çš„æ— å‚æ•°(parameter-free)å›¾ç¼–ç å™¨ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)å¯¹ç»“æ„åŒ–å›¾æ•°æ®çš„ç†è§£ã€‚FoGEé€šè¿‡å°†æ•°å­¦ç‰©ç†ç†è®ºç›´æ¥åº”ç”¨äºå›¾æç¤ºè¯(graph prompting)ï¼Œä¸ºå†»ç»“çš„é¢„è®­ç»ƒLLMæä¾›ä¸°å¯Œä¸”å…·ä¿¡æ¯é‡çš„å›¾ç¼–ç ç‰¹å¾ã€‚è¿™ç§åŸºäºå‰ç¼€å¾®è°ƒ(prefix-tuned)çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æå°‘ç”šè‡³æ— éœ€è°ƒæ•´æ¶æ„çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆå¤„ç†ä»ç®€å•å›¾åˆ°è›‹ç™½è´¨ç»“æ„åŠè¶…å›¾(hypergraphs)çš„å„ç±»å¤æ‚é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒFoGEæ˜¾è‘—ç®€åŒ–äº†ç°æœ‰çš„å›¾åºåˆ—åŒ–æˆ–å›¾å˜æ¢å™¨(Graph Transformers)æ–¹æ¡ˆï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å±•ç°å‡ºæå¼ºçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸æ˜“ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02937v2",
      "published_date": "2025-06-26 23:48:03 UTC",
      "updated_date": "2025-10-27 19:36:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:46:46.488413+00:00"
    },
    {
      "arxiv_id": "2506.21817v1",
      "title": "Exploring the Structure of AI-Induced Language Change in Scientific English",
      "title_zh": "æ¢ç©¶ç§‘æŠ€è‹±è¯­ä¸­AIè¯±å¯¼çš„è¯­è¨€æ¼”å˜ç»“æ„",
      "authors": [
        "Riley Galpin",
        "Bryce Anderson",
        "Tom S. Juzek"
      ],
      "abstract": "Scientific English has undergone rapid and unprecedented changes in recent years, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing significant spikes in frequency since around 2022. These changes are widely attributed to the growing influence of Large Language Models like ChatGPT in the discourse surrounding bias and misalignment. However, apart from changes in frequency, the exact structure of these linguistic shifts has remained unclear. The present study addresses this and investigates whether these changes involve the replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\" replacing \"essential\" and \"key,\" or whether they reflect broader semantic and pragmatic qualifications. To further investigate structural changes, we include part of speech tagging in our analysis to quantify linguistic shifts over grammatical categories and differentiate between word forms, like \"potential\" as a noun vs. as an adjective. We systematically analyze synonym groups for widely discussed 'spiking words' based on frequency trends in scientific abstracts from PubMed. We find that entire semantic clusters often shift together, with most or all words in a group increasing in usage. This pattern suggests that changes induced by Large Language Models are primarily semantic and pragmatic rather than purely lexical. Notably, the adjective \"important\" shows a significant decline, which prompted us to systematically analyze decreasing lexical items. Our analysis of \"collapsing\" words reveals a more complex picture, which is consistent with organic language change and contrasts with the patterns of the abrupt spikes. These insights into the structure of language change contribute to our understanding of how language technology continues to shape human language.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ª2022å¹´ä»¥æ¥ç”±Large Language Modelså¼•èµ·çš„Scientific Englishè¯­è¨€å˜åŒ–çš„ç»“æ„ï¼Œæ—¨åœ¨åˆ†æè¿™ç§å˜åŒ–æ˜¯æºäºåŒä¹‰è¯æ›¿æ¢è¿˜æ˜¯æ›´å¹¿æ³›çš„è¯­ä¹‰å’Œè¯­ç”¨è°ƒæ•´ã€‚ç ”ç©¶è€…é€šè¿‡åˆ†æPubMedç§‘å­¦æ‘˜è¦ä¸­çš„è¯é¢‘è¶‹åŠ¿ï¼Œå¹¶ç»“åˆPart of Speech taggingæŠ€æœ¯ï¼Œå¯¹\"delve\"ã€\"intricate\"å’Œ\"crucial\"ç­‰é¢‘ç‡æ¿€å¢è¯åŠå…¶åŒä¹‰è¯ç¾¤ç»„è¿›è¡Œäº†ç³»ç»Ÿçš„å®šé‡åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œæ•´ä¸ªSemantic clustersé€šå¸¸ä¼šæ•´ä½“è¿ç§»ï¼Œè¡¨ç°ä¸ºç¾¤ç»„å†…å¤§éƒ¨åˆ†è¯æ±‡çš„ä½¿ç”¨é¢‘ç‡åŒæ­¥å¢åŠ ï¼Œè¿™è¡¨æ˜AIå¼•å¯¼çš„è¯­è¨€å˜åŒ–ä¸»è¦æ˜¯Semanticå’ŒPragmaticå±‚é¢çš„åç§»è€Œéçº¯ç²¹çš„è¯æ±‡æ›¿æ¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡åˆ†æå½¢å®¹è¯\"important\"çš„æ˜¾è‘—ä¸‹é™åŠå…¶ä»–è¯æ±‡çš„Collapsingç°è±¡ï¼Œå‘ç°å…¶æ¼”å˜æ¨¡å¼ä¸çˆ†å‘å¼å¢é•¿è¯æ±‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è¯­è¨€æŠ€æœ¯æŒç»­é‡å¡‘äººç±»è¯­è¨€çš„æ·±å±‚ç»“æ„ï¼Œä¸ºç†è§£äººå·¥æ™ºèƒ½é©±åŠ¨ä¸‹çš„å­¦æœ¯å†™ä½œèŒƒå¼æ¼”å˜æä¾›äº†é‡è¦çš„å®è¯è¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table. Licensed under CC BY-NC-SA 4.0",
      "pdf_url": "https://arxiv.org/pdf/2506.21817v1",
      "published_date": "2025-06-26 23:44:24 UTC",
      "updated_date": "2025-06-26 23:44:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:46:51.689105+00:00"
    },
    {
      "arxiv_id": "2506.21813v1",
      "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery",
      "title_zh": "CAT-SGï¼šé¢å‘ç™½å†…éšœæ‰‹æœ¯ç»†ç²’åº¦ç†è§£çš„å¤§å‹åŠ¨æ€åœºæ™¯å›¾æ•°æ®é›†",
      "authors": [
        "Felix Holm",
        "GÃ¶zde Ãœnver",
        "Ghazal Ghazaei",
        "Nassir Navab"
      ],
      "abstract": "Understanding the intricate workflows of cataract surgery requires modeling complex interactions between surgical tools, anatomical structures, and procedural techniques. Existing datasets primarily address isolated aspects of surgical analysis, such as tool detection or phase segmentation, but lack comprehensive representations that capture the semantic relationships between entities over time. This paper introduces the Cataract Surgery Scene Graph (CAT-SG) dataset, the first to provide structured annotations of tool-tissue interactions, procedural variations, and temporal dependencies. By incorporating detailed semantic relations, CAT-SG offers a holistic view of surgical workflows, enabling more accurate recognition of surgical phases and techniques. Additionally, we present a novel scene graph generation model, CatSGG, which outperforms current methods in generating structured surgical representations. The CAT-SG dataset is designed to enhance AI-driven surgical training, real-time decision support, and workflow analysis, paving the way for more intelligent, context-aware systems in clinical practice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç™½å†…éšœæ‰‹æœ¯æµç¨‹ç†è§£ä¸­ç¼ºä¹å®ä½“é—´è¯­ä¹‰å…³ç³»å’Œæ—¶é—´ä¾èµ–æ€§è¡¨å¾çš„é—®é¢˜ï¼Œæ¨å‡ºäº† CAT-SG æ•°æ®é›†ã€‚ä½œä¸ºé¦–ä¸ªæä¾›å·¥å…·ä¸ç»„ç»‡äº¤äº’(tool-tissue interactions)ã€æ‰‹æœ¯å˜å¼‚åŠæ—¶é—´ä¾èµ–æ€§ç»“æ„åŒ–æ ‡æ³¨çš„å¤§å‹åŠ¨æ€åœºæ™¯å›¾æ•°æ®é›†ï¼ŒCAT-SG é€šè¿‡è¯¦ç»†çš„è¯­ä¹‰å…³ç³»ä¸ºæ‰‹æœ¯å·¥ä½œæµæä¾›äº†å®è§‚è§†è§’ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«æ‰‹æœ¯é˜¶æ®µå’ŒæŠ€æœ¯ã€‚åŒæ—¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º CatSGG çš„æ–°å‹åœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ï¼Œå…¶åœ¨ç”Ÿæˆç»“æ„åŒ–æ‰‹æœ¯è¡¨å¾æ–¹é¢çš„æ€§èƒ½ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚è¯¥æ•°æ®é›†çš„è®¾è®¡æ—¨åœ¨æå‡ AI é©±åŠ¨çš„æ‰‹æœ¯åŸ¹è®­ã€å®æ—¶å†³ç­–æ”¯æŒä»¥åŠå·¥ä½œæµåˆ†æï¼Œä¸ºä¸´åºŠå®è·µä¸­æ„å»ºæ›´æ™ºèƒ½ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥(context-aware)èƒ½åŠ›çš„ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21813v1",
      "published_date": "2025-06-26 23:25:23 UTC",
      "updated_date": "2025-06-26 23:25:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:46:48.729600+00:00"
    },
    {
      "arxiv_id": "2506.22523v3",
      "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çº¢é˜Ÿæµ‹è¯•ï¼šå­¦æœ¯åŒ»ç–—ä¸­å¿ƒç‰ˆæƒä¸“é¡¹æ¼”ç»ƒæŠ¥å‘Š",
      "authors": [
        "James Wen",
        "Sahil Nalawade",
        "Zhiwei Liang",
        "Catherine Bielick",
        "Marisa Ferrara Boston",
        "Alexander Chowdhury",
        "Adele Collin",
        "Luigi De Angelis",
        "Jacob Ellen",
        "Heather Frase",
        "Rodrigo R. Gameiro",
        "Juan Manuel Gutierrez",
        "Pooja Kadam",
        "Murat Keceli",
        "Srikanth Krishnamurthy",
        "Anne Kwok",
        "Yanan Lance Lu",
        "Heather Mattie",
        "Liam G. McCoy",
        "Katherine Miller",
        "Allison C. Morgan",
        "Marlene Louisa Moerig",
        "Trang Nguyen",
        "Alexander Owen-Post",
        "Alex D. Ruiz",
        "Sreekar Reddy Puchala",
        "Soujanya Samineni",
        "Takeshi Tohyama",
        "Varun Ullanat",
        "Carmine Valenza",
        "Camilo Velez",
        "Pengcheng Wang",
        "Anna Wuest",
        "Yuxiang Zhou",
        "Yingde Zhu",
        "Jason M. Johnson",
        "Naomi Lenane",
        "Jennifer Willcox",
        "Francis J. Vitiello",
        "Leo Anthony G. Celi",
        "Renato Umeton"
      ],
      "abstract": "Background: Generative artificial intelligence (AI) deployment in academic medical settings raises copyright compliance concerns. Dana-Farber Cancer Institute implemented GPT4DFCI, an internal generative AI tool utilizing OpenAI models, that is approved for enterprise use in research and operations. Given (1) the exceptionally broad adoption of the tool in our organization, (2) our research mission, and (3) the shared responsibility model required to benefit from Customer Copyright Commitment in Azure OpenAI Service products, we deemed rigorous copyright compliance testing necessary.\n  Case Description: We conducted a structured red teaming exercise in Nov. 2024, with 42 participants from academic, industry, and government institutions. Four teams attempted to extract copyrighted content from GPT4DFCI across four domains: literary works, news articles, scientific publications, and access-restricted clinical notes. Teams successfully extracted verbatim book dedications and near-exact passages through various strategies. News article extraction failed despite jailbreak attempts. Scientific article reproduction yielded only high-level summaries. Clinical note testing revealed appropriate privacy safeguards.\n  Discussion: The successful extraction of literary content indicates potential copyrighted material presence in training data, necessitating inference-time filtering. Differential success rates across content types suggest varying protective mechanisms. The event led to implementation of a copyright-specific meta-prompt in GPT4DFCI; this mitigation has been in production since Jan. 2025.\n  Conclusion: Systematic red teaming revealed specific vulnerabilities in generative AI copyright compliance, leading to concrete mitigation strategies. Academic medical institutions deploying generative AI should implement continuous testing protocols to ensure legal and ethical compliance.",
      "tldr_zh": "ä¸¹å¨œ-æ³•ä¼¯ç™Œç—‡ç ”ç©¶æ‰€ (Dana-Farber Cancer Institute) é’ˆå¯¹å…¶å†…éƒ¨ç”Ÿæˆçš„ GPT4DFCI å¼€å±•äº†ä¸€é¡¹ä»¥ç‰ˆæƒåˆè§„ä¸ºæ ¸å¿ƒçš„çº¢é˜Ÿæµ‹è¯• (Red Teaming) æ¼”ç»ƒï¼Œæ—¨åœ¨è¯„ä¼°å­¦æœ¯åŒ»ç–—æœºæ„éƒ¨ç½²ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) æ—¶çš„æ³•å¾‹ä¸ä¼¦ç†é£é™©ã€‚æ­¤æ¬¡æ¼”ç»ƒç”±æ¥è‡ªå­¦æœ¯ã€å·¥ä¸šå’Œæ”¿åºœæœºæ„çš„42åå‚ä¸è€…åä½œå®Œæˆï¼Œé‡ç‚¹æµ‹è¯•äº†æ–‡å­¦ä½œå“ã€æ–°é—»æ–‡ç« ã€ç§‘å­¦å‡ºç‰ˆç‰©åŠå—é™ä¸´åºŠç¬”è®°å››ä¸ªé¢†åŸŸçš„ç‰ˆæƒå†…å®¹æå–é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæµ‹è¯•å›¢é˜Ÿé€šè¿‡å¤šç§ç­–ç•¥æˆåŠŸæå–äº†æ–‡å­¦ä½œå“çš„é€å­—çŒ®è¯åŠè¿‘ä¹ç²¾ç¡®çš„ç‰‡æ®µï¼Œæ­ç¤ºäº†è®­ç»ƒæ•°æ®ä¸­æ½œåœ¨çš„ç‰ˆæƒåˆè§„æ¼æ´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ–°é—»æ–‡ç« çš„æå–å³ä½¿é€šè¿‡è¶Šç‹± (Jailbreak) å°è¯•ä¹Ÿæœªèƒ½æˆåŠŸï¼Œç§‘å­¦æ–‡ç« ä»…èƒ½äº§ç”Ÿé«˜å±‚çº§æ‘˜è¦ï¼Œè€Œä¸´åºŠç¬”è®°æµ‹è¯•åˆ™éªŒè¯äº†éšç§ä¿æŠ¤æœºåˆ¶çš„ç¨³å¥æ€§ã€‚é’ˆå¯¹æ¼”ç»ƒä¸­å‘ç°çš„é—®é¢˜ï¼Œè¯¥æœºæ„å·²äº2025å¹´1æœˆåœ¨ GPT4DFCI ä¸­æ­£å¼å®æ–½äº†ç‰ˆæƒä¸“ç”¨çš„å…ƒæç¤º (Meta-prompt) ç¼“è§£æªæ–½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç³»ç»ŸåŒ–çº¢é˜Ÿæµ‹è¯•åœ¨è¯†åˆ«ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å…·ä½“æ¼æ´æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å»ºè®®å­¦æœ¯åŒ»ç–—æœºæ„å»ºç«‹æŒç»­çš„æµ‹è¯•åè®®ä»¥ç¡®ä¿é•¿æœŸçš„æ³•å¾‹ä¸ä¼¦ç†åˆè§„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22523v3",
      "published_date": "2025-06-26 23:11:49 UTC",
      "updated_date": "2025-07-02 21:04:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:46:53.940316+00:00"
    },
    {
      "arxiv_id": "2506.21805v1",
      "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation",
      "title_zh": "CitySimï¼šåŸºäºå¤§è§„æ¨¡ LLM é©±åŠ¨æ™ºèƒ½ä½“æ¨¡æ‹Ÿçš„åŸå¸‚è¡Œä¸ºä¸åŸå¸‚åŠ¨æ€å»ºæ¨¡",
      "authors": [
        "Nicolas Bougie",
        "Narimasa Watanabe"
      ],
      "abstract": "Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CitySimï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°çš„å¤§è§„æ¨¡åŸå¸‚è¡Œä¸ºä¸åŠ¨æ€æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿè§„åˆ™å¯¼å‘æ–¹æ³•åœ¨åˆ»ç”»äººç±»å¤æ‚æ„å›¾å’Œè‡ªé€‚åº”è¡Œä¸ºæ–¹é¢çš„å±€é™ã€‚åœ¨ CitySim ä¸­ï¼Œæ™ºèƒ½ä½“åˆ©ç”¨é€’å½’ä»·å€¼é©±åŠ¨æ–¹æ³•ï¼ˆrecursive value-driven approachï¼‰å¹³è¡¡å¼ºåˆ¶æ€§æ´»åŠ¨ã€ä¸ªäººä¹ æƒ¯ä¸æƒ…å¢ƒå› ç´ ï¼Œä»è€Œç”ŸæˆçœŸå®çš„æ—¥å¸¸æ—¥ç¨‹ã€‚ç ”ç©¶é€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“ä¿¡å¿µï¼ˆbeliefsï¼‰ã€é•¿æœŸç›®æ ‡ï¼ˆlong-term goalsï¼‰å’Œç©ºé—´è®°å¿†ï¼ˆspatial memoryï¼‰ï¼Œå®ç°äº†æ›´å…·ç”Ÿå‘½åŠ›çš„é•¿æœŸæ¨¡æ‹Ÿã€‚å®éªŒè¯æ˜ï¼ŒCitySim åœ¨å¾®è§‚å’Œå®è§‚å±‚é¢å‡å±•ç°å‡ºæ¯”ä»¥å¾€å·¥ä½œæ›´è´´è¿‘äººç±»çœŸå®è¡Œä¸ºçš„ç‰¹æ€§ã€‚é€šè¿‡å¯¹æ•°ä¸‡ä¸ªæ™ºèƒ½ä½“åœ¨äººç¾¤å¯†åº¦ä¼°è®¡ã€åœ°ç‚¹æµè¡Œåº¦é¢„æµ‹åŠç¦ç¥‰ï¼ˆwell-beingï¼‰è¯„ä¼°ç­‰ç°å®åœºæ™¯ä¸‹çš„é›†ä½“è¡Œä¸ºè¿›è¡Œæ¨¡æ‹Ÿï¼ŒéªŒè¯äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼ŒCitySim ä¸ºæ·±å…¥ç†è§£å’Œé¢„æµ‹åŸå¸‚å¤æ‚ç°è±¡æä¾›äº†ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•ä¸”çµæ´»çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21805v1",
      "published_date": "2025-06-26 23:11:42 UTC",
      "updated_date": "2025-06-26 23:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:04.452179+00:00"
    },
    {
      "arxiv_id": "2506.21796v1",
      "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning",
      "title_zh": "å±•ç¤ºåŸºäºæœºå™¨å­¦ä¹ çš„å¯äº’æ“ä½œä¿¡é“çŠ¶æ€åé¦ˆå‹ç¼©",
      "authors": [
        "Dani Korpi",
        "Rachel Wang",
        "Jerry Wang",
        "Abdelrahman Ibrahim",
        "Carl Nuzman",
        "Runxin Wang",
        "Kursat Rasim Mestav",
        "Dustin Zhang",
        "Iraj Saniee",
        "Shawn Winston",
        "Gordana Pavlovic",
        "Wei Ding",
        "William J. Hillery",
        "Chenxi Hao",
        "Ram Thirunagari",
        "Jung Chang",
        "Jeehyun Kim",
        "Bartek Kozicki",
        "Dragan Samardzija",
        "Taesang Yoo",
        "Andreas Maeder",
        "Tingfang Ji",
        "Harish Viswanathan"
      ],
      "abstract": "Neural network-based compression and decompression of channel state feedback has been one of the most widely studied applications of machine learning (ML) in wireless networks. Various simulation-based studies have shown that ML-based feedback compression can result in reduced overhead and more accurate channel information. However, to the best of our knowledge, there are no real-life proofs of concepts demonstrating the benefits of ML-based channel feedback compression in a practical setting, where the user equipment (UE) and base station have no access to each others' ML models. In this paper, we present a novel approach for training interoperable compression and decompression ML models in a confidential manner, and demonstrate the accuracy of the ensuing models using prototype UEs and base stations. The performance of the ML-based channel feedback is measured both in terms of the accuracy of the reconstructed channel information and achieved downlink throughput gains when using the channel information for beamforming. The reported measurement results demonstrate that it is possible to develop an accurate ML-based channel feedback link without having to share ML models between device and network vendors. These results pave the way for a practical implementation of ML-based channel feedback in commercial 6G networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ— çº¿ç½‘ç»œä¸­ä½¿ç”¨æœºå™¨å­¦ä¹ (Machine Learning)è¿›è¡Œä¿¡é“çŠ¶æ€åé¦ˆ(channel state feedback)å‹ç¼©çš„é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³å®é™…åº”ç”¨ä¸­ç”¨æˆ·è®¾å¤‡(UE)ä¸åŸºç«™ä¹‹é—´æ¨¡å‹äº’æ“ä½œæ€§çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æœºå¯†è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¼€å‘äº’æ“ä½œçš„å‹ç¼©ä¸è§£å‹ç¼©æ¨¡å‹ï¼Œå¹¶åœ¨åŸå‹UEå’ŒåŸºç«™ä¸Šè¿›è¡Œäº†å®æµ‹æ¼”ç¤ºã€‚å®éªŒç»“æœé€šè¿‡ä¿¡é“ä¿¡æ¯é‡å»ºå‡†ç¡®åº¦å’Œä¸‹è¡Œé“¾è·¯ååé‡(throughput)å¢ç›Šè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†åœ¨ä¸å…±äº«å‚å•†ç§æœ‰æ¨¡å‹çš„å‰æä¸‹æ„å»ºé«˜ç²¾åº¦åé¦ˆé“¾è·¯çš„å¯è¡Œæ€§ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†æœºå™¨å­¦ä¹ ä¿¡é“åé¦ˆåœ¨ç°å®åœºæ™¯ä¸­ç¼ºä¹æ¦‚å¿µéªŒè¯çš„ç©ºç™½ï¼Œä¸ºæœªæ¥6Gç½‘ç»œçš„å•†ä¸šåŒ–éƒ¨ç½²æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2506.21796v1",
      "published_date": "2025-06-26 22:40:03 UTC",
      "updated_date": "2025-06-26 22:40:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:02.069244+00:00"
    },
    {
      "arxiv_id": "2506.21788v1",
      "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data",
      "title_zh": "é¢å‘å¤šæºã€å¤šä¿çœŸåŸå­å»ºæ¨¡æ•°æ®çš„å›¾åŸºç¡€æ¨¡å‹é²æ£’é¢„è®­ç»ƒå¤šä»»åŠ¡å¹¶è¡Œ",
      "authors": [
        "Massimiliano Lupo Pasini",
        "Jong Youl Choi",
        "Pei Zhang",
        "Kshitij Mehta",
        "Rylie Weaver",
        "Ashwin M. Aji",
        "Karl W. Schulz",
        "Jorda Polo",
        "Prasanna Balaprakash"
      ],
      "abstract": "Graph foundation models using graph neural networks promise sustainable, efficient atomistic modeling. To tackle challenges of processing multi-source, multi-fidelity data during pre-training, recent studies employ multi-task learning, in which shared message passing layers initially process input atomistic structures regardless of source, then route them to multiple decoding heads that predict data-specific outputs. This approach stabilizes pre-training and enhances a model's transferability to unexplored chemical regions. Preliminary results on approximately four million structures are encouraging, yet questions remain about generalizability to larger, more diverse datasets and scalability on supercomputers. We propose a multi-task parallelism method that distributes each head across computing resources with GPU acceleration. Implemented in the open-source HydraGNN architecture, our method was trained on over 24 million structures from five datasets and tested on the Perlmutter, Aurora, and Frontier supercomputers, demonstrating efficient scaling on all three highly heterogeneous super-computing architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå­å»ºæ¨¡ä¸­çš„å¤šæºã€å¤šä¿çœŸåº¦æ•°æ®é¢„è®­ç»ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¹¶è¡Œ(Multi-task parallelism)æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå›¾åŸºç¡€æ¨¡å‹(Graph foundation models)çš„é²æ£’æ€§ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ–¹æ³•åº”ç”¨äºå¼€æºçš„HydraGNNæ¶æ„ï¼Œé€šè¿‡åœ¨GPUåŠ é€Ÿçš„è®¡ç®—èµ„æºä¸Šåˆ†å¸ƒä¸åŒçš„è§£ç å¤´(decoding heads)ï¼Œå®ç°äº†åœ¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„é«˜æ•ˆé¢„è®­ç»ƒã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨å…±äº«æ¶ˆæ¯ä¼ é€’å±‚å¤„ç†åŸå­ç»“æ„ï¼Œå¹¶å°†å…¶è·¯ç”±è‡³ç‰¹å®šçš„é¢„æµ‹å¤´ä»¥ç¨³å®šé¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹å‘æœªçŸ¥åŒ–å­¦é¢†åŸŸçš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒé‡‡ç”¨äº†æ¥è‡ªäº”ä¸ªæ•°æ®é›†çš„è¶…è¿‡2400ä¸‡ä¸ªåŸå­ç»“æ„è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨Perlmutterã€Auroraå’ŒFrontierç­‰é¡¶çº§è¶…çº§è®¡ç®—æœºä¸Šå®Œæˆäº†å¤§è§„æ¨¡éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¼‚æ„è¶…ç®—æ¶æ„ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„å¯æ‰©å±•æ€§ï¼Œä¸ºé«˜æ•ˆæ„å»ºå¤§è§„æ¨¡åŸå­å°ºåº¦åŸºç¡€æ¨¡å‹æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.atm-clus"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.21788v1",
      "published_date": "2025-06-26 22:04:05 UTC",
      "updated_date": "2025-06-26 22:04:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:06.543065+00:00"
    },
    {
      "arxiv_id": "2506.22521v1",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ¨¡å‹æŠ½å–æ”»å‡»ä¸é˜²å¾¡ç»¼è¿°",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "tldr_zh": "è¯¥ç»¼è¿°é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰é¢ä¸´çš„æ¨¡å‹æå–æ”»å‡»åŠå…¶é˜²å¾¡æœºåˆ¶è¿›è¡Œäº†å…¨é¢æ¢³ç†ï¼ŒæŒ‡å‡ºæ­¤ç±»æ”»å‡»å¯¹çŸ¥è¯†äº§æƒå’Œç”¨æˆ·éšç§æ„æˆä¸¥é‡å¨èƒã€‚ç ”ç©¶æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„åˆ†ç±»æ³•ï¼Œå°† LLM ç‰¹æœ‰çš„æå–æ”»å‡»åˆ†ä¸ºåŠŸèƒ½æå–ï¼ˆfunctionality extractionï¼‰ã€è®­ç»ƒæ•°æ®æå–ï¼ˆtraining data extractionï¼‰å’Œé’ˆå¯¹æç¤ºè¯çš„æ”»å‡»ï¼ˆprompt-targeted attacksï¼‰ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†åŒ…æ‹¬åŸºäº API çš„çŸ¥è¯†è’¸é¦ï¼ˆknowledge distillationï¼‰ã€ç›´æ¥æŸ¥è¯¢ã€å‚æ•°æ¢å¤ä»¥åŠåˆ©ç”¨ Transformer æ¶æ„ç¼ºé™·çš„æç¤ºè¯çªƒå–ï¼ˆprompt stealingï¼‰ç­‰å¤šç§æ”»å‡»æ–¹æ³•ã€‚åŒæ—¶ï¼Œç ”ç©¶ä»æ¨¡å‹ä¿æŠ¤ã€æ•°æ®éšç§ä¿æŠ¤å’Œæç¤ºè¯ç›®æ ‡ç­–ç•¥ä¸‰ä¸ªç»´åº¦æ¢è®¨äº†é˜²å¾¡æœºåˆ¶ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸“é—¨çš„è¯„ä¼°æŒ‡æ ‡ç”¨äºè¡¡é‡æ”»å‡»å¼ºåº¦å’Œé˜²å¾¡æ•ˆèƒ½ã€‚é€šè¿‡æ­ç¤ºç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œè¯¥å·¥ä½œæå‡ºäº†é›†æˆæ”»å‡»æ–¹æ³•å’Œå¹³è¡¡å®‰å…¨ä¸æ•ˆèƒ½çš„è‡ªé€‚åº”é˜²å¾¡ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸º NLP ç ”ç©¶è€…å’Œå®‰å…¨ä»ä¸šè€…æä¾›äº†å…³é”®çš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22521v1",
      "published_date": "2025-06-26 22:02:01 UTC",
      "updated_date": "2025-06-26 22:02:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:01.694426+00:00"
    },
    {
      "arxiv_id": "2507.00057v2",
      "title": "Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation",
      "title_zh": "ä¸ä¸€è‡´æ€§ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆçš„æ— é¢„è¨€æœºé”™è¯¯åº¦é‡",
      "authors": [
        "Thomas Valentin",
        "Ardi Madadi",
        "Gaetano Sapia",
        "Marcel BÃ¶hme"
      ],
      "abstract": "Generating code from a natural language programming task is one of the most successful applications of Large Language Models (LLMs). Yet, the generated program may be buggy. Without an oracle, such as an existing, correct implementation or a formal specification, can we somehow estimate how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called *incoherence*, that can be estimated efficiently in the absence of an oracle and allows us to establish a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. In our experiments, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives for the average task. In fact, *an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation*. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via incoherence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ç¼ºä¹Oracleï¼ˆå¦‚ç°æœ‰æ­£ç¡®å®ç°æˆ–æ­£å¼è§„èŒƒï¼‰çš„æƒ…å†µä¸‹å¦‚ä½•è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆç¨‹åºå‡†ç¡®æ€§çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºIncoherenceçš„é”™è¯¯è¡¡é‡æŒ‡æ ‡ã€‚Incoherence èƒ½å¤Ÿåœ¨æ— Oracleç¯å¢ƒä¸‹é«˜æ•ˆä¼°ç®—ï¼Œå¹¶ä¸ºLLMç”Ÿæˆç¨‹åºçš„ä¸æ­£ç¡®æ¦‚ç‡å»ºç«‹ä¸‹ç•Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ä»»åŠ¡ä¸­èƒ½è‡ªåŠ¨è¯†åˆ«çº¦ä¸‰åˆ†ä¹‹äºŒçš„é”™è¯¯ç¨‹åºï¼Œä¸”æœªå‡ºç°ä»»ä½•å‡é˜³æ€§ï¼ˆFalse Positivesï¼‰æŠ¥å‘Šã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼ŒåŸºäºIncoherenceçš„è¯„ä¼°å¯ä»¥å¯é åœ°æ›¿ä»£åŸºäºOracleçš„è¯„ä¼°æ–¹å¼ï¼Œå…¶å¯¹ä¸åŒLLMçš„æ€§èƒ½æ’åç»“æœä¸åŸºäºOracleçš„pass@1æ’åå…·æœ‰æå¼ºçš„ä¸€è‡´æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºè‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆè´¨é‡çš„ç¦»çº¿ç›‘æ§å’ŒLLMæ€§èƒ½åŸºå‡†æµ‹è¯•æä¾›äº†æœ‰æ•ˆä¸”ä½æˆæœ¬çš„è¡¡é‡æ‰‹æ®µã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "Accepted at AAAI'26 (extended version). 8 pages + refs and appendix",
      "pdf_url": "https://arxiv.org/pdf/2507.00057v2",
      "published_date": "2025-06-26 22:00:50 UTC",
      "updated_date": "2025-12-13 19:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:08.543157+00:00"
    },
    {
      "arxiv_id": "2506.21785v1",
      "title": "Comparing Learning Paradigms for Egocentric Video Summarization",
      "title_zh": "ç¬¬ä¸€è§†è§’è§†é¢‘æ‘˜è¦å­¦ä¹ èŒƒå¼çš„å¯¹æ¯”",
      "authors": [
        "Daniel Wen"
      ],
      "abstract": "In this study, we investigate various computer vision paradigms - supervised learning, unsupervised learning, and prompt fine-tuning - by assessing their ability to understand and interpret egocentric video data. Specifically, we examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM (state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned pre-trained model), evaluating their effectiveness in video summarization. Our results demonstrate that current state-of-the-art models perform less effectively on first-person videos compared to third-person videos, highlighting the need for further advancements in the egocentric video domain. Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models, emphasizing the limitations of existing approaches in adapting to the unique challenges of first-person perspectives. Although our evaluation is conducted on a small subset of egocentric videos from the Ego-Exo4D dataset due to resource constraints, the primary objective of this research is to provide a comprehensive proof-of-concept analysis aimed at advancing the application of computer vision techniques to first-person videos. By exploring novel methodologies and evaluating their potential, we aim to contribute to the ongoing development of models capable of effectively processing and interpreting egocentric perspectives.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨å¹¶æ¯”è¾ƒäº†ç›‘ç£å­¦ä¹ (Supervised Learning)ã€æ— ç›‘ç£å­¦ä¹ (Unsupervised Learning)å’Œæç¤ºå¾®è°ƒ(Prompt Fine-tuning)ç­‰è®¡ç®—æœºè§†è§‰èŒƒå¼åœ¨ç¬¬ä¸€è§†è§’è§†é¢‘(Egocentric Video)æ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶é€‰å–äº† Shotluck Holmesã€TAC-SUM ä»¥åŠç»è¿‡æç¤ºå¾®è°ƒçš„ GPT-4o æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒå­¦ä¹ æ–¹æ³•ç†è§£ç¬¬ä¸€è§†è§’è§†é¢‘æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›®å‰çš„ SOTA æ¨¡å‹åœ¨ç¬¬ä¸€è§†è§’è§†é¢‘ä¸Šçš„è¡¨ç°æ™®éé€Šè‰²äºå…¶åœ¨ç¬¬ä¸‰è§†è§’è§†é¢‘ä¸Šçš„è¡¨ç°ï¼Œåæ˜ å‡ºè¯¥é¢†åŸŸä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡æç¤ºå¾®è°ƒçš„é€šç”¨å‹ GPT-4o æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¸“é—¨çš„è§†é¢‘æ‘˜è¦æ¨¡å‹ï¼Œæ­ç¤ºäº†ç°æœ‰ä¸“ç”¨æ–¹æ³•åœ¨é€‚åº”ç¬¬ä¸€è§†è§’ç‹¬ç‰¹æ€§æ–¹é¢çš„å±€é™ã€‚å°½ç®¡å—èµ„æºé™åˆ¶ä»…åœ¨ Ego-Exo4D æ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œä½†è¯¥å·¥ä½œä¸ºåº”ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¤„ç†ç¬¬ä¸€è§†è§’è§†é¢‘æä¾›äº†é‡è¦çš„æ¦‚å¿µéªŒè¯ã€‚é€šè¿‡å¯¹ä¸åŒèŒƒå¼çš„æ·±å…¥åˆ†æï¼Œæœ¬ç ”ç©¶ä¸ºæœªæ¥å¼€å‘èƒ½å¤Ÿé«˜æ•ˆè§£è¯»ç¬¬ä¸€è§†è§’åœºæ™¯çš„è§†è§‰æ¨¡å‹æä¾›äº†å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21785v1",
      "published_date": "2025-06-26 21:46:48 UTC",
      "updated_date": "2025-06-26 21:46:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:19.262744+00:00"
    },
    {
      "arxiv_id": "2506.21784v1",
      "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models",
      "title_zh": "MobiVerseï¼šåŸºäºæ··åˆè½»é‡çº§é¢†åŸŸç‰¹å®šç”Ÿæˆå™¨ä¸å¤§è¯­è¨€æ¨¡å‹çš„è§„æ¨¡åŒ–åŸå¸‚ç§»åŠ¨æ€§æ¨¡æ‹Ÿ",
      "authors": [
        "Yifan Liu",
        "Xishun Liao",
        "Haoxuan Ma",
        "Jonathan Liu",
        "Rohan Jadhav",
        "Jiaqi Ma"
      ],
      "abstract": "Understanding and modeling human mobility patterns is crucial for effective transportation planning and urban development. Despite significant advances in mobility research, there remains a critical gap in simulation platforms that allow for algorithm development, policy implementation, and comprehensive evaluation at scale. Traditional activity-based models require extensive data collection and manual calibration, machine learning approaches struggle with adaptation to dynamic conditions, and treding agent-based Large Language Models (LLMs) implementations face computational constraints with large-scale simulations. To address these challenges, we propose MobiVerse, a hybrid framework leverages the efficiency of lightweight domain-specific generator for generating base activity chains with the adaptability of LLMs for context-aware modifications. A case study was conducted in Westwood, Los Angeles, where we efficiently generated and dynamically adjusted schedules for the whole population of approximately 53,000 agents on a standard PC. Our experiments demonstrate that MobiVerse successfully enables agents to respond to environmental feedback, including road closures, large gathering events like football games, and congestion, through our hybrid framework. Its modular design facilitates testing various mobility algorithms at both transportation system and agent levels. Results show our approach maintains computational efficiency while enhancing behavioral realism. MobiVerse bridges the gap in mobility simulation by providing a customizable platform for mobility systems planning and operations with benchmark algorithms. Code and videos are available at https://github.com/ucla-mobility/MobiVerse.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MobiVerseï¼Œä¸€ç§ç»“åˆäº†è½»é‡åŒ–é¢†åŸŸç‰¹å®šç”Ÿæˆå™¨(Domain-Specific Generator)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ··åˆæ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡åŸå¸‚ç§»åŠ¨æ¨¡æ‹Ÿåœ¨è®¡ç®—æ•ˆç‡ä¸é€‚åº”æ€§ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆå™¨çš„é«˜æ•ˆæ€§ç”ŸæˆåŸºç¡€æ´»åŠ¨é“¾ï¼ŒåŒæ—¶å‘æŒ¥LLMsçš„æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›æ¥æ‰§è¡ŒåŠ¨æ€ä¿®æ”¹ã€‚åœ¨æ´›æ‰çŸ¶Westwoodçš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒMobiVerseåœ¨æ ‡å‡†ä¸ªäººç”µè„‘ä¸ŠæˆåŠŸæ¨¡æ‹Ÿäº†çº¦53,000åæ™ºèƒ½ä½“çš„æ´»åŠ¨ï¼Œå¹¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé’ˆå¯¹é“è·¯å°é—­ã€å¤§å‹é›†ä¼šåŠäº¤é€šæ‹¥å µç­‰ç¯å¢ƒåé¦ˆåšå‡ºçœŸå®å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“è¡Œä¸ºçš„çœŸå®æ€§ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä¸ºäº¤é€šç³»ç»Ÿè§„åˆ’å’Œç§»åŠ¨ç®—æ³•æµ‹è¯•æä¾›äº†ä¸€ä¸ªé«˜åº¦å¯å®šåˆ¶çš„å¹³å°ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç°æœ‰ç§»åŠ¨æ¨¡æ‹Ÿå·¥å…·åœ¨å¯æ‰©å±•æ€§ä¸å¤æ‚ç¯å¢ƒé€‚åº”æ€§æ–¹é¢çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21784v1",
      "published_date": "2025-06-26 21:46:18 UTC",
      "updated_date": "2025-06-26 21:46:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:33.585768+00:00"
    },
    {
      "arxiv_id": "2506.21783v1",
      "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„åˆ—è¡¨æ„å»ºä¸æ—¶åºç†è§£èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Alexandru Dumitru",
        "V Venktesh",
        "Adam Jatowt",
        "Avishek Anand"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated immense advances in a wide range of natural language tasks. However, these models are susceptible to hallucinations and errors on particularly temporal understanding tasks involving multiple entities in answers. In such tasks, they fail to associate entities with accurate time intervals, generate a complete list of entities in answers or reason about events associated with specific temporal bounds. Existing works do not extensively evaluate the abilities of the model to perform implicit and explicit temporal understanding in a list answer construction setup. To bridge this gap, we propose the Time referenced List based Question Answering or TLQA benchmark that requires structured answers in list format aligned with corresponding time periods. Our TLQA benchmark, requires both list construction and temporal understanding simultaneously, which to the best of our knowledge has not been explored in prior benchmarks. We investigate the temporal understanding and list construction capabilities of state-of-the-art generative models on TLQA in closed-book and open-domain settings. Our findings reveal significant shortcomings in current models, particularly their inability to provide complete answers and temporally align facts in a closed-book setup and the need to improve retrieval in open-domain setup, providing clear future directions for research on TLQA. The benchmark and code at https://github.com/elixir-research-group/TLQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªå®ä½“çš„å¤æ‚æ—¶é—´ç†è§£ä»»åŠ¡æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†å®ä½“ä¸å‡†ç¡®æ—¶é—´é—´éš”å…³è”ä»¥åŠç”Ÿæˆå®Œæ•´åˆ—è¡¨æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºå¡«è¡¥ç°æœ‰è¯„ä¼°ä½“ç³»çš„ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†TLQA (Time referenced List based Question Answering) åŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹ç”Ÿæˆä¸ç‰¹å®šæ—¶é—´æ®µå¯¹é½çš„ç»“æ„åŒ–åˆ—è¡¨æ ¼å¼ç­”æ¡ˆã€‚è¯¥åŸºå‡†æµ‹è¯•é¦–æ¬¡åŒæ—¶è€ƒå¯Ÿäº†åˆ—è¡¨æ„å»º(list construction)å’Œæ—¶é—´ç†è§£(temporal understanding)èƒ½åŠ›ï¼Œå¹¶åœ¨é—­å·(closed-book)å’Œå¼€æ”¾åŸŸ(open-domain)ä¸¤ç§è®¾ç½®ä¸‹å¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨é—­å·ç¯å¢ƒä¸‹éš¾ä»¥æä¾›å®Œæ•´ç­”æ¡ˆä¸”äº‹å®çš„æ—¶é—´å¯¹é½èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œå¼€æ”¾åŸŸè®¾ç½®åˆ™æ­ç¤ºäº†æå‡æ£€ç´¢è´¨é‡çš„ç´§è¿«æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡TLQAåŸºå‡†åŠå…¬å¼€çš„ä»£ç èµ„æºï¼Œä¸ºå¢å¼ºLLMsåœ¨å¤æ‚æ—¶é—´çº¦æŸä¸‹çš„äº‹å®æ£€ç´¢ä¸ç»“æ„åŒ–ç”Ÿæˆèƒ½åŠ›æŒ‡æ˜äº†æœªæ¥çš„ä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.21783v1",
      "published_date": "2025-06-26 21:40:58 UTC",
      "updated_date": "2025-06-26 21:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:45.222712+00:00"
    },
    {
      "arxiv_id": "2506.21763v2",
      "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?",
      "title_zh": "THE-Treeï¼šè¿½æº¯å†å²æ¼”è¿›èƒ½å¦å¼ºåŒ–ç§‘å­¦éªŒè¯ä¸æ¨ç†ï¼Ÿ",
      "authors": [
        "Xin Wang",
        "Jiyao Liu",
        "Yulong Xiao",
        "Junzhi Ning",
        "Lihao Liu",
        "Junjun He",
        "Botian Shi",
        "Kaicheng Yu"
      ],
      "abstract": "Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow. Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured. This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology \\textbf{H}istory \\textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature. THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\" process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded. We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research. Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8% to 14% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†THE-Tree (Technology History Evolution Tree) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”Ÿæˆç§‘å­¦ææ¡ˆæ—¶é¢ä¸´çš„å¹»è§‰ã€é¢†åŸŸçŸ¥è¯†ç¼ºå¤±åŠéªŒè¯æ»åç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ç§‘å­¦æ–‡çŒ®ä¸­æ„å»ºé¢†åŸŸç‰¹å®šçš„æŠ€æœ¯æ¼”åŒ–æ ‘ï¼Œä¸ºç§‘å­¦æ¼”è¿›æä¾›äº†ç»“æ„åŒ–ã€å¯éªŒè¯ä¸”å…·æœ‰å› æœå…³è”çš„å†å²æ•°æ®ã€‚THE-Tree é‡‡ç”¨äº†åˆ›æ–°çš„ \"Think-Verbalize-Cite-Verify\" æµç¨‹ï¼Œåˆ©ç”¨ LLM æå‡ºæ½œåœ¨ç§‘å­¦è¿›å±•å¹¶å¼•ç”¨æ”¯æŒæ–‡çŒ®ï¼Œå†é€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç† (NLI) æœºåˆ¶ç¡®ä¿æ¯ä¸€æ­¥æ¼”åŒ–é€»è¾‘çš„ä¸€è‡´æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº† 88 ä¸ªè·¨é¢†åŸŸçš„æ¼”åŒ–æ ‘ï¼Œå¹¶å‘å¸ƒäº†åŒ…å« 7.1 ä¸‡é¡¹äº‹å®éªŒè¯çš„åŸºå‡†æ•°æ®é›†ä»¥æ¨åŠ¨åç»­ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›¾è¡¥å…¨ (Graph Completion) ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å°† hit@1 æŒ‡æ ‡æå‡äº† 8% è‡³ 14%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†é¢„æµ‹æœªæ¥ç§‘å­¦å‘å±•çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç»“åˆå…¶ä»–æ–¹æ³•è¯„ä¼°é‡è¦ç§‘å­¦è®ºæ–‡æ—¶ï¼ŒTHE-Tree å¸¦æ¥äº†è¿‘ 100% çš„æ€§èƒ½å¢é•¿ï¼Œè¯æ˜äº†è¿½è¸ªå†å²æ¼”åŒ–å¯¹å¢å¼ºç§‘å­¦éªŒè¯ä¸æ¨ç†çš„æ˜¾è‘—è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21763v2",
      "published_date": "2025-06-26 20:44:51 UTC",
      "updated_date": "2025-07-21 06:49:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:51.203288+00:00"
    },
    {
      "arxiv_id": "2507.02935v1",
      "title": "Theory of Mind in Action: The Instruction Inference Task",
      "title_zh": "å¿ƒç†ç†è®ºçš„å®è·µï¼šæŒ‡ä»¤æ¨ç†ä»»åŠ¡",
      "authors": [
        "Fardin Saad",
        "Pradeep K. Murukannaiah",
        "Munindar P. Singh"
      ],
      "abstract": "The Theory of Mind (ToM) refers to an agent's capacity to infer the mental states of other agents. ToM is essential for effective collaboration. To assess ToM in a dynamic, goal-oriented, and collaborative environment, we introduce a novel task, Instruction Inference, in which an agent assists a principal in reaching a goal by interpreting indirect or ambiguous instructions. We present Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting and responding to the principal's instructions. We implement two variants of Tomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e., few-shot or Fs) demonstrating the requisite structured reasoning (i.e., chain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and information about the problem (i.e., commonsense prompt or CP). We realized both variants of Tomcat on three leading large language models (LLMs), namely, GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat, we conducted a study with 52 human participants in which we provided participants with the same information as the CP variant of Tomcat. We computed intent accuracy, action optimality, and planning optimality to measure the ToM capabilities of Tomcat and our study participants. We found that Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to the human participants, underscoring its ToM potential for human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¿ƒæ™ºç†è®º(Theory of Mind)åœ¨åŠ¨æ€åä½œç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œå¹¶å¼•å…¥äº†ä¸€é¡¹åä¸ºæŒ‡ä»¤æ¨ç†(Instruction Inference)çš„æ–°ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“é€šè¿‡è§£é‡Šé—´æ¥æˆ–æ¨¡ç³Šçš„æŒ‡ä»¤æ¥ååŠ©å§”æ‰˜äººè¾¾æˆç›®æ ‡ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“Tomcatï¼Œå¹¶å®ç°äº†ä¸¤ç§å˜ä½“ï¼šä¸€ç§æ˜¯ç»“åˆå°‘é‡ç¤ºä¾‹ä¸é“¾å¼æ€ç»´æ¨ç†(Chain-of-Thought)çš„Fs-CoTï¼Œå¦ä¸€ç§æ˜¯ä¾èµ–å¸¸è¯†çŸ¥è¯†çš„CPæ¨¡å¼ã€‚é€šè¿‡åœ¨GPT-4oã€DeepSeek-R1å’ŒGemma-3-27Bç­‰æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œå¹¶ä¸52åäººç±»å‚ä¸è€…åœ¨æ„å›¾å‡†ç¡®æ€§(intent accuracy)ã€åŠ¨ä½œæœ€ä¼˜æ€§(action optimality)åŠè§„åˆ’æœ€ä¼˜æ€§(planning optimality)ç­‰æŒ‡æ ‡ä¸Šè¿›è¡Œå¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨Fs-CoTæ¶æ„çš„Tomcatï¼ˆç‰¹åˆ«æ˜¯åŸºäºGPT-4oå’ŒDeepSeek-R1çš„ç‰ˆæœ¬ï¼‰è¡¨ç°å‡ºäº†ä¸äººç±»ç›¸å½“çš„å¿ƒæ™ºç†è®ºæ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†LLMsåœ¨æå‡äººæœºåä½œ(human-AI collaboration)æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to Artificial Intelligence Journal (under review). 51 pages with appendix (28 pages article + 4 pages references + 19 pages appendix), 7 figures (Appendix: 26 Figures), 6 tables. Code available at: https://github.com/fardinsaad/Tomcat-LLM",
      "pdf_url": "https://arxiv.org/pdf/2507.02935v1",
      "published_date": "2025-06-26 20:44:12 UTC",
      "updated_date": "2025-06-26 20:44:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:50.488416+00:00"
    },
    {
      "arxiv_id": "2506.21734v3",
      "title": "Hierarchical Reasoning Model",
      "title_zh": "å±‚çº§æ¨ç†æ¨¡å‹",
      "authors": [
        "Guan Wang",
        "Jin Li",
        "Yuhao Sun",
        "Xing Chen",
        "Changling Liu",
        "Yue Wu",
        "Meng Lu",
        "Sen Song",
        "Yasin Abbasi Yadkori"
      ],
      "abstract": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å±‚æ¬¡åŒ–æ¨ç†æ¨¡å‹(Hierarchical Reasoning Model, HRM)ï¼Œè¿™æ˜¯ä¸€ç§å—äººè„‘å¤šå°ºåº¦å¤„ç†æœºåˆ¶å¯å‘çš„é€’å½’æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸­é¢ä¸´çš„ä»»åŠ¡åˆ†è§£è„†å¼±ã€æ•°æ®éœ€æ±‚å¤§åŠå»¶è¿Ÿé«˜ç­‰ç“¶é¢ˆã€‚HRMé€šè¿‡ä¸¤ä¸ªç›¸äº’ä¾èµ–çš„é€’å½’æ¨¡å—åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å®ç°åºåˆ—æ¨ç†ï¼šä¸€ä¸ªè´Ÿè´£ç¼“æ…¢ã€æŠ½è±¡è§„åˆ’çš„é«˜å±‚æ¨¡å—ï¼Œä»¥åŠä¸€ä¸ªè´Ÿè´£å¿«é€Ÿã€è¯¦ç»†è®¡ç®—çš„ä½å±‚æ¨¡å—ã€‚è¯¥æ¨¡å‹ä»…åŒ…å«2700ä¸‡ä¸ªå‚æ•°ï¼Œåœ¨æ— éœ€é¢„è®­ç»ƒæˆ–CoTæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»…å‡­1000ä¸ªè®­ç»ƒæ ·æœ¬å°±åœ¨æ•°ç‹¬(Sudoku)å’Œå¤§å‹è¿·å®«è·¯å¾„å¯»ä¼˜ç­‰å¤æ‚ä»»åŠ¡ä¸­è¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œåœ¨è¡¡é‡é€šç”¨äººå·¥æ™ºèƒ½èƒ½åŠ›çš„æŠ½è±¡ä¸æ¨ç†åŸºå‡†æµ‹è¯•(Abstraction and Reasoning Corpus, ARC)ä¸Šï¼ŒHRMçš„è¡¨ç°ä¼˜äºè§„æ¨¡æ›´å¤§ä¸”ä¸Šä¸‹æ–‡çª—å£æ›´é•¿çš„æ¨¡å‹ã€‚è¿™äº›ç ”ç©¶ç»“æœå‡¸æ˜¾äº†HRMåœ¨æ¨åŠ¨é€šç”¨è®¡ç®—å’Œé€šç”¨æ¨ç†ç³»ç»Ÿå‘å±•æ–¹é¢çš„å˜é©æ€§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21734v3",
      "published_date": "2025-06-26 19:39:54 UTC",
      "updated_date": "2025-08-04 08:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:51.331708+00:00"
    },
    {
      "arxiv_id": "2506.21732v1",
      "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation",
      "title_zh": "é’ˆå¯¹æ»‘ç§»è½¬å‘è§†è§‰å¯¼èˆªçš„å§¿æ€æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ å®éªŒç ”ç©¶",
      "authors": [
        "Ameya Salvi",
        "Venkat Krovi"
      ],
      "abstract": "Vision-based lane keeping is a topic of significant interest in the robotics and autonomous ground vehicles communities in various on-road and off-road applications. The skid-steered vehicle architecture has served as a useful vehicle platform for human controlled operations. However, systematic modeling, especially of the skid-slip wheel terrain interactions (primarily in off-road settings) has created bottlenecks for automation deployment. End-to-end learning based methods such as imitation learning and deep reinforcement learning, have gained prominence as a viable deployment option to counter the lack of accurate analytical models. However, the systematic formulation and subsequent verification/validation in dynamic operation regimes (particularly for skid-steered vehicles) remains a work in progress. To this end, a novel approach for structured formulation for learning visual navigation is proposed and investigated in this work. Extensive software simulations, hardware evaluations and ablation studies now highlight the significantly improved performance of the proposed approach against contemporary literature.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè§†è§‰çš„è½¦é“ä¿æŒæŠ€æœ¯åœ¨æ»‘ç§»è½¬å‘è½¦è¾†(skid-steered vehicle)ä¸­çš„åº”ç”¨è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œç‰¹åˆ«å…³æ³¨äº†éé“è·¯ç¯å¢ƒä¸‹ç”±äºå¤æ‚çš„skid-slipäº¤äº’ä½œç”¨å¯¼è‡´çš„å»ºæ¨¡ç“¶é¢ˆã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿè§£ææ¨¡å‹çš„å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºå¹¶ç ”ç©¶äº†ä¸€å¥—åŸºäºPose informed reinforcement learningçš„ç»“æ„åŒ–è§†è§‰å¯¼èˆªå­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡å¹¿æ³›çš„è½¯ä»¶ä»¿çœŸã€ç¡¬ä»¶è¯„ä¼°ä»¥åŠæ¶ˆèç ”ç©¶(ablation studies)ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨åŠ¨æ€è¿è¡Œæœºåˆ¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„å½“ä»£æ–‡çŒ®æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œä¸ä»…æœ‰æ•ˆåº”å¯¹äº†æ»‘ç§»è½¬å‘å¹³å°åœ¨è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸­çš„æŒ‘æˆ˜ï¼Œè¿˜é€šè¿‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤„ç†å¤æ‚åœ°å½¢äº¤äº’æ—¶çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21732v1",
      "published_date": "2025-06-26 19:36:49 UTC",
      "updated_date": "2025-06-26 19:36:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:50.695872+00:00"
    },
    {
      "arxiv_id": "2506.21731v2",
      "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis",
      "title_zh": "åŸºäºäº’æ–¥æ¦‚ç‡ç©ºé—´ä¸å±€éƒ¨ç›¸å…³æ€§å‡è®¾çš„å›¾åƒç”Ÿæˆæ¢ç´¢",
      "authors": [
        "Chenqiu Zhao",
        "Anup Basu"
      ],
      "abstract": "A common assumption in probabilistic generative models for image generation is that learning the global data distribution suffices to generate novel images via sampling. We investigate the limitation of this core assumption, namely that learning global distributions leads to memorization rather than generative behavior. We propose two theoretical frameworks, the Mutually Exclusive Probability Space (MEPS) and the Local Dependence Hypothesis (LDH), for investigation. MEPS arises from the observation that deterministic mappings (e.g. neural networks) involving random variables tend to reduce overlap coefficients among involved random variables, thereby inducing exclusivity. We further propose a lower bound in terms of the overlap coefficient, and introduce a Binary Latent Autoencoder (BL-AE) that encodes images into signed binary latent representations. LDH formalizes dependence within a finite observation radius, which motivates our $Î³$-Autoregressive Random Variable Model ($Î³$-ARVM). $Î³$-ARVM is an autoregressive model, with a variable observation range $Î³$, that predicts a histogram for the next token. Using $Î³$-ARVM, we observe that as the observation range increases, autoregressive models progressively shift toward memorization. In the limit of global dependence, the model behaves as a pure memorizer when operating on the binary latents produced by our BL-AE. Comprehensive experiments and discussions support our investigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å­¦ä¹ å…¨å±€æ•°æ®åˆ†å¸ƒè¿™ä¸€æ ¸å¿ƒå‡è®¾çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºè¿‡åˆ†ä¾èµ–å…¨å±€åˆ†å¸ƒä¼šå¯¼è‡´æ¨¡å‹å€¾å‘äºè®°å¿†(memorization)è€ŒéçœŸæ­£çš„ç”Ÿæˆè¡Œä¸ºã€‚ç ”ç©¶è€…æå‡ºäº†äº’æ–¥æ¦‚ç‡ç©ºé—´(Mutually Exclusive Probability Space, MEPS)å’Œå±€éƒ¨ä¾èµ–å‡è®¾(Local Dependence Hypothesis, LDH)ä¸¤ä¸ªç†è®ºæ¡†æ¶è¿›è¡Œæ·±å…¥åˆ†æã€‚MEPS æ­ç¤ºäº†ç¥ç»ç½‘ç»œç­‰ç¡®å®šæ€§æ˜ å°„å¦‚ä½•è¯±å¯¼éšæœºå˜é‡é—´çš„äº’æ–¥æ€§ï¼Œå¹¶æ®æ­¤å¼•å…¥äº†å°†å›¾åƒç¼–ç ä¸ºå¸¦ç¬¦å·äºŒè¿›åˆ¶æ½œè¡¨ç¤ºçš„äºŒè¿›åˆ¶æ½œå˜é‡è‡ªåŠ¨ç¼–ç å™¨(Binary Latent Autoencoder, BL-AE)ã€‚é€šè¿‡å¼€å‘çš„å…·æœ‰å¯å˜è§‚å¯ŸèŒƒå›´çš„ $\\gamma$-è‡ªå›å½’éšæœºå˜é‡æ¨¡å‹($\\gamma$-ARVM)ï¼Œç ”ç©¶å‘ç°éšç€è§‚å¯ŸèŒƒå›´å¢åŠ ï¼Œè‡ªå›å½’æ¨¡å‹ä¼šé€æ¸è½¬å‘è®°å¿†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…¨å±€ä¾èµ–çš„æé™æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨å¤„ç† BL-AE ç”Ÿæˆçš„äºŒè¿›åˆ¶æ½œå˜é‡æ—¶è¡¨ç°ä¸ºçº¯ç²¹çš„è®°å¿†å™¨ã€‚è¯¥å·¥ä½œä¸ºç†è§£ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆèƒ½åŠ›ä¸è®°å¿†è¡Œä¸ºä¹‹é—´çš„å¹³è¡¡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘å’Œå®éªŒè¯æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21731v2",
      "published_date": "2025-06-26 19:32:29 UTC",
      "updated_date": "2025-09-22 22:05:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:47:57.438730+00:00"
    },
    {
      "arxiv_id": "2506.22520v1",
      "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics",
      "title_zh": "æ¢ç´¢äº¤äº’å¼åˆ†å­åŠ¨åŠ›å­¦ç¯å¢ƒä¸‹äººå·¥æ™ºèƒ½å¯¼å¸ˆé˜Ÿå‹çš„é€‚åº”æ€§ï¼šæ¿€å‘æ¢ç´¢å¥½å¥‡å¿ƒå¹¶ä¿ƒè¿›å­¦ä¹ ",
      "authors": [
        "Mustafa Demir",
        "Jacob Miratsky",
        "Jonathan Nguyen",
        "Chun Kit Chan",
        "Punya Mishra",
        "Abhishek Singharoy"
      ],
      "abstract": "This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äº¤äº’å¼åˆ†å­åŠ¨åŠ›å­¦(Interactive Molecular Dynamics, IMD)å­¦ä¹ ç¯å¢ƒä¸‹ï¼Œäººå·¥æ™ºèƒ½å¯¼å¸ˆé˜Ÿå‹(AI tutor teammate)å¯¹å­¦ç”Ÿå¥½å¥‡å¿ƒé©±åŠ¨çš„å‚ä¸åº¦å’Œå­¦ä¹ æ•ˆæœçš„å½±å“ã€‚ç ”ç©¶é‡‡ç”¨Wizard-of-Ozå®éªŒèŒƒå¼ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹(Large Language Model)åŠ¨æ€è°ƒæ•´AIçš„è¡Œä¸ºï¼Œé‡ç‚¹è€ƒå¯Ÿå…¶å¥½å¥‡å¿ƒè§¦å‘(curiosity-triggering)å’Œå›åº”è¡Œä¸ºåœ¨æ¿€å‘å­¦ç”Ÿå¥½å¥‡å¿ƒä¸­çš„ä½œç”¨ã€‚é€šè¿‡æ··åˆæ–¹æ³•æ¢ç´¢æ€§è®¾è®¡å’Œäº¤å‰é€’å½’å®šé‡åˆ†æ(Cross Recurrence Quantification Analysis, CRQA)ï¼Œç ”ç©¶è¯„ä¼°äº†å­¦ç”Ÿä¸AIä¹‹é—´çš„åŠ¨æ€åŒæ­¥æ€§ã€æ²Ÿé€šå¤æ‚æ€§åŠå›¢é˜Ÿè¡¨ç°ã€‚å®éªŒå‘ç°ï¼ŒAIçš„å¥½å¥‡å¿ƒè§¦å‘è¡Œä¸ºä¸å­¦ç”Ÿæå‡ºæ›´é«˜æ°´å¹³çš„é—®é¢˜å‘ˆæ­£ç›¸å…³ï¼Œæ˜¾è‘—æå‡äº†è®¤çŸ¥å¤æ‚æ€§å’Œå­¦ä¹ å‚ä¸åº¦ã€‚CRQAæŒ‡æ ‡æ­ç¤ºäº†å­¦ç”Ÿä¸AIäº¤äº’ä¸­çš„ç»“æ„åŒ–è‡ªé€‚åº”å‚ä¸å¯¹ç»´æŒå¥½å¥‡å¿ƒçš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶è¯æ˜äº†AIä½œä¸ºé˜Ÿå‹å’Œæ•™è‚²è€…çš„åŒé‡èº«ä»½ï¼Œèƒ½é€šè¿‡æä¾›è‡ªé€‚åº”åé¦ˆæ¥æŒç»­æ¿€å‘å­¦ç”Ÿçš„æ¢ç´¢æ€§å¥½å¥‡å¿ƒ(epistemic curiosity)ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå›¢é˜Ÿçš„å­¦ä¹ ç»©æ•ˆå’Œå¯¹çŸ¥è¯†çš„æ·±åº¦ç†è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CE",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22520v1",
      "published_date": "2025-06-26 19:30:25 UTC",
      "updated_date": "2025-06-26 19:30:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:07.158763+00:00"
    },
    {
      "arxiv_id": "2506.21727v1",
      "title": "Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions",
      "title_zh": "å¤šç»´åº¦ä¸å¯åˆ†ç‰©å“çš„åŒæ—¶å…¬å¹³åˆ†é…",
      "authors": [
        "Yasushi Kawase",
        "Bodhayan Roy",
        "Mohammad Azharuddin Sanpui"
      ],
      "abstract": "This paper explores the fair allocation of indivisible items in a multidimensional setting, motivated by the need to address fairness in complex environments where agents assess bundles according to multiple criteria. Such multidimensional settings are not merely of theoretical interest but are central to many real-world applications. For example, cloud computing resources are evaluated based on multiple criteria such as CPU cores, memory, and network bandwidth. In such cases, traditional one dimensional fairness notions fail to capture fairness across multiple attributes. To address these challenges, we study two relaxed variants of envy-freeness: weak simultaneously envy-free up to c goods (weak sEFc) and strong simultaneously envy-free up to c goods (strong sEFc), which accommodate the multidimensionality of agents' preferences. Under the weak notion, for every pair of agents and for each dimension, any perceived envy can be eliminated by removing, if necessary, a different set of goods from the envied agent's allocation. In contrast, the strong version requires selecting a single set of goods whose removal from the envied bundle simultaneously eliminates envy in every dimension. We provide upper and lower bounds on the relaxation parameter c that guarantee the existence of weak or strong sEFc allocations, where these bounds are independent of the total number of items. In addition, we present algorithms for checking whether a weak or strong sEFc allocation exists. Moreover, we establish NP-hardness results for checking the existence of weak sEF1 and strong sEF1 allocations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šç»´ç¯å¢ƒä¸‹çš„ä¸å¯åˆ†å‰²ç‰©å“å…¬å¹³åˆ†é…é—®é¢˜ï¼Œä¸»è¦é’ˆå¯¹äº‘è®¡ç®—èµ„æºåˆ†é…ç­‰éœ€è¦ä¾æ®å¤šä¸ªæ ‡å‡†è¯„ä¼°åˆ†é…æ–¹æ¡ˆçš„å¤æ‚ç°å®åœºæ™¯ã€‚ä¸ºäº†åº”å¯¹ä¼ ç»Ÿä¸€ç»´å…¬å¹³æ€§å®šä¹‰çš„å±€é™ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§å«‰å¦’æ¶ˆé™¤ï¼ˆenvy-freenessï¼‰çš„æ¾å¼›å˜ä½“ï¼šå¼±åŒæ­¥å«‰å¦’æ¶ˆé™¤ï¼ˆweak sEFcï¼‰å’Œå¼ºåŒæ­¥å«‰å¦’æ¶ˆé™¤ï¼ˆstrong sEFcï¼‰ã€‚åœ¨ weak sEFc å®šä¹‰ä¸‹ï¼Œé’ˆå¯¹æ¯ä¸€å¯¹ä»£ç†å’Œæ¯ä¸€ä¸ªç»´åº¦ï¼Œå¯ä»¥é€šè¿‡ä»è¢«å«‰å¦’è€…çš„åˆ†é…ä¸­ç§»é™¤ä¸åŒçš„ä¸€ç»„ç‰©å“æ¥æ¶ˆé™¤å«‰å¦’ï¼›è€Œ strong sEFc åˆ™è¦æ±‚ç§»é™¤å•ä¸€çš„ä¸€ç»„ç‰©å“å³å¯åŒæ—¶æ¶ˆé™¤æ‰€æœ‰ç»´åº¦çš„å«‰å¦’ã€‚ç ”ç©¶ä¸ºä¿è¯è¿™ç±»åˆ†é…å­˜åœ¨çš„æ¾å¼›å‚æ•° c æä¾›äº†ä¸Šç•Œå’Œä¸‹ç•Œï¼Œä¸”è¿™äº›ç•Œé™ä¸ç‰©å“æ€»æ•°æ— å…³ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†æ£€æµ‹ weak æˆ– strong sEFc åˆ†é…å­˜åœ¨æ€§çš„ç®—æ³•ï¼Œå¹¶è¯æ˜äº†æ£€æµ‹ weak sEF1 å’Œ strong sEF1 åˆ†é…å­˜åœ¨æ€§å±äº NP-hardness é—®é¢˜ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21727v1",
      "published_date": "2025-06-26 19:27:22 UTC",
      "updated_date": "2025-06-26 19:27:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:12.042544+00:00"
    },
    {
      "arxiv_id": "2506.21722v1",
      "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration",
      "title_zh": "é˜æ˜å¹¶èµ‹èƒ½é€šç”¨å›¾åƒä¿®å¤çš„æ‰©æ•£è®­ç»ƒèŒƒå¼",
      "authors": [
        "Xin Lu",
        "Xueyang Fu",
        "Jie Xiao",
        "Zihao Fan",
        "Yurui Zhu",
        "Zheng-Jun Zha"
      ],
      "abstract": "While diffusion models demonstrate strong generative capabilities in image restoration (IR) tasks, their complex architectures and iterative processes limit their practical application compared to mainstream reconstruction-based general ordinary IR networks. Existing approaches primarily focus on optimizing network architecture and diffusion paths but overlook the integration of the diffusion training paradigm within general ordinary IR frameworks. To address these challenges, this paper elucidates key principles for adapting the diffusion training paradigm to general IR training through systematic analysis of time-step dependencies, network hierarchies, noise-level relationships, and multi-restoration task correlations, proposing a new IR framework supported by diffusion-based training. To enable IR networks to simultaneously restore images and model generative representations, we introduce a series of regularization strategies that align diffusion objectives with IR tasks, improving generalization in single-task scenarios. Furthermore, recognizing that diffusion-based generation exerts varying influences across different IR tasks, we develop an incremental training paradigm and task-specific adaptors, further enhancing performance in multi-task unified IR. Experiments demonstrate that our method significantly improves the generalization of IR networks in single-task IR and achieves superior performance in multi-task unified IR. Notably, the proposed framework can be seamlessly integrated into existing general IR architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(diffusion models)åœ¨å›¾åƒæ¢å¤(Image Restoration, IR)ä»»åŠ¡ä¸­é¢ä¸´çš„æ¶æ„å¤æ‚åŠè¿­ä»£è¿‡ç¨‹å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†æ‰©æ•£è®­ç»ƒèŒƒå¼(diffusion training paradigm)èå…¥é€šç”¨IRæ¡†æ¶çš„æ–°æ–¹æ¡ˆã€‚é€šè¿‡ç³»ç»Ÿåˆ†ææ—¶é—´æ­¥ä¾èµ–æ€§ã€ç½‘ç»œå±‚çº§ã€å™ªå£°æ°´å¹³å…³ç³»ä»¥åŠå¤šä»»åŠ¡ç›¸å…³æ€§ï¼Œä½œè€…é˜æ˜äº†é€‚é…æ‰©æ•£è®­ç»ƒçš„å…³é”®åŸåˆ™ï¼Œå¹¶å¼•å…¥ä¸€ç³»åˆ—æ­£åˆ™åŒ–ç­–ç•¥ä»¥å¯¹é½æ‰©æ•£ç›®æ ‡ä¸IRä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†å¢é‡è®­ç»ƒèŒƒå¼å’Œä»»åŠ¡ç‰¹å®šé€‚é…å™¨(task-specific adaptors)ï¼Œæ—¨åœ¨æå‡å¤šä»»åŠ¡ç»Ÿä¸€IRçš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†IRç½‘ç»œåœ¨å•ä»»åŠ¡åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œå¹¶åœ¨å¤šä»»åŠ¡è”åˆæ¢å¤ä¸­å–å¾—äº†ä¼˜å¼‚æˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„å„ç§å›¾åƒæ¢å¤æ¶æ„ä¸­ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21722v1",
      "published_date": "2025-06-26 19:14:27 UTC",
      "updated_date": "2025-06-26 19:14:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:09.409592+00:00"
    },
    {
      "arxiv_id": "2506.21718v1",
      "title": "Performance Prediction for Large Systems via Text-to-Text Regression",
      "title_zh": "åŸºäºæ–‡æœ¬åˆ°æ–‡æœ¬å›å½’çš„å¤§å‹ç³»ç»Ÿæ€§èƒ½é¢„æµ‹",
      "authors": [
        "Yash Akhauri",
        "Bryan Lewandowski",
        "Cheng-Hsi Lin",
        "Adrian N. Reyes",
        "Grant C. Forbes",
        "Arissa Wongpanich",
        "Bangding Yang",
        "Mohamed S. Abdelfattah",
        "Sagi Perel",
        "Xingyou Song"
      ],
      "abstract": "In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹ç³»ç»ŸæŒ‡æ ‡é¢„æµ‹ä¸­ä¼ ç»Ÿ Tabular Regression å¤„ç†é…ç½®æ–‡ä»¶æˆ–ç³»ç»Ÿæ—¥å¿—ç­‰å¤æ‚æ•°æ®æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨ä¸”å¯æ‰©å±•çš„ Text-to-Text Regression æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åœ¨ Google çš„å¤§è§„æ¨¡é›†ç¾¤è°ƒåº¦ç³»ç»Ÿ Borg ä¸Šé¢„æµ‹èµ„æºæ•ˆç‡ï¼Œç ”ç©¶é‡‡ç”¨äº†ä»éšæœºåˆå§‹åŒ–è®­ç»ƒçš„ 60M å‚æ•° Encoder-Decoder æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ•´ä¸ªé›†ç¾¤èŒƒå›´å†…å®ç°äº†é«˜è¾¾ 0.99 çš„ç­‰çº§ç›¸å…³ç³»æ•°ï¼ˆå¹³å‡ 0.9ï¼‰ï¼Œä¸” Mean Squared Error (MSE) æ¯”ä¼ ç»Ÿè¡¨æ ¼æ–¹æ³•ä½ 100 å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°äº†æå¼ºçš„é€‚åº”æ€§ï¼Œä»…éœ€ 500 ä¸ª Few-shot æ ·æœ¬å³å¯è¿ç§»è‡³æ–°ä»»åŠ¡ï¼Œå¹¶èƒ½æœ‰æ•ˆæ•æ‰å¤æ‚ç»“æœåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†ä½¿ç”¨ Encodersã€å¢åŠ åºåˆ—é•¿åº¦ä»¥åŠæ¨¡å‹å†…ç½®çš„ Uncertainty Quantification å¯¹é¢„æµ‹æ€§èƒ½çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°ä¸ºæ„å»ºç°å®ä¸–ç•Œç»“æœçš„é€šç”¨æ¨¡æ‹Ÿå™¨ (Universal Simulators) å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF",
        "cs.SE",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Code can be found at https://github.com/google-deepmind/regress-lm",
      "pdf_url": "https://arxiv.org/pdf/2506.21718v1",
      "published_date": "2025-06-26 19:10:08 UTC",
      "updated_date": "2025-06-26 19:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:23.424311+00:00"
    },
    {
      "arxiv_id": "2506.21669v2",
      "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents",
      "title_zh": "SEEA-R1ï¼šé¢å‘è‡ªè¿›åŒ–å…·èº«æ™ºèƒ½ä½“çš„æ ‘çŠ¶ç»“æ„å¼ºåŒ–å¾®è°ƒ",
      "authors": [
        "Wanxin Tian",
        "Shijie Zhang",
        "Kevin Zhang",
        "Xiaowei Chi",
        "Chunkai Fan",
        "Junyu Lu",
        "Yulin Luo",
        "Qiang Zhou",
        "Yiming Zhao",
        "Ning Liu",
        "Siyu Lin",
        "Zhiyuan Qin",
        "Xiaozhu Ju",
        "Shanghang Zhang",
        "Jian Tang"
      ],
      "abstract": "Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (Tree-GRPO) integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (MGRM). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07% (textual) and 46.27% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% (textual) and 44.03% (multi-modal) without ground truth reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SEEA-R1ï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨å®ç°å…·èº«æ™ºèƒ½ä½“(Embodied Agents)è‡ªæˆ‘è¿›åŒ–(Self-evolving)èƒ½åŠ›çš„å¼ºåŒ–å¾®è°ƒ(Reinforcement Fine-Tuning)æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å…·èº«ä»»åŠ¡ä¸­å¤šæ­¥æ¨ç†å¯¼è‡´çš„ä¸­é—´å¥–åŠ±ç¼ºå¤±ä»¥åŠæ‰‹å·¥å¥–åŠ±å‡½æ•°æ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºæ ‘ç»“æ„çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Tree-GRPO)ï¼Œé€šè¿‡é›†æˆè’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search)æ¥æä¾›æ›´å¯†é›†çš„ä¸­é—´å­¦ä¹ ä¿¡å·ã€‚åŒæ—¶ï¼Œç ”ç©¶é€šè¿‡å¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±æ¨¡å‹(Multi-modal Generative Reward Model)å®ç°äº†è·¨åœºæ™¯çš„è‡ªä¸»å¥–åŠ±ä¼°è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEEA-R1åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€è¡¨ç°å‡è¶…è¶Šäº†GPT-4oåŠç°æœ‰å¼€æºæ¨¡å‹ã€‚å³ä½¿åœ¨æ²¡æœ‰çœŸå®å¥–åŠ±(Ground Truth Reward)çš„æƒ…å†µä¸‹ï¼Œè¯¥ç³»ç»Ÿä¾ç„¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æ„å»ºå¯æ‰©å±•è‡ªæˆ‘è¿›åŒ–å…·èº«æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21669v2",
      "published_date": "2025-06-26 18:00:07 UTC",
      "updated_date": "2025-10-27 04:10:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:29.944588+00:00"
    },
    {
      "arxiv_id": "2506.21552v1",
      "title": "Whole-Body Conditioned Egocentric Video Prediction",
      "title_zh": "åŸºäºå…¨èº«æ¡ä»¶çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘é¢„æµ‹",
      "authors": [
        "Yutong Bai",
        "Danny Tran",
        "Amir Bar",
        "Yann LeCun",
        "Trevor Darrell",
        "Jitendra Malik"
      ],
      "abstract": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PEVA (Predict Ego-centric Video from human Actions) æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆå†å²è§†é¢‘å’Œç”± 3D body pose è¡¨ç¤ºçš„åŠ¨ä½œåºåˆ—æ¥é¢„æµ‹ç¬¬ä¸€è§†è§’è§†é¢‘ã€‚è¯¥æ¨¡å‹ä»¥ç¬¦åˆäººä½“å…³èŠ‚å±‚çº§çš„è¿åŠ¨å­¦å§¿æ€è½¨è¿¹ (kinematic pose trajectories) ä¸ºæ¡ä»¶ï¼Œæ¨¡æ‹Ÿäººç±»ç‰©ç†åŠ¨ä½œå¦‚ä½•ä»ç¬¬ä¸€äººç§°è§†è§’å½±å“å¹¶æ”¹å˜ç¯å¢ƒã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ Nymeria å¤§è§„æ¨¡çœŸå®ä¸–ç•Œç¬¬ä¸€è§†è§’è§†é¢‘ä¸èº«ä½“å§¿æ€æ•°æ®é›†ä¸Šï¼Œè®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’æ¡ä»¶æ‰©æ•£å˜æ¢å™¨ (auto-regressive conditional diffusion transformer)ã€‚ä¸ºäº†æ·±å…¥åˆ†ææ¨¡å‹çš„å…·èº«é¢„æµ‹å’Œæ§åˆ¶èƒ½åŠ›ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€å¥—ä»»åŠ¡éš¾åº¦é€’å¢çš„å±‚æ¬¡åŒ–è¯„ä¼°åè®® (hierarchical evaluation protocol)ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†åˆ©ç”¨è§†é¢‘é¢„æµ‹æŠ€æœ¯ä»äººç±»è§†è§’å»ºæ¨¡å¤æ‚çœŸå®ç¯å¢ƒåŠå…·èº«æ™ºèƒ½ä½“ (embodied agent) è¡Œä¸ºçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://dannytran123.github.io/PEVA",
      "pdf_url": "https://arxiv.org/pdf/2506.21552v1",
      "published_date": "2025-06-26 17:59:59 UTC",
      "updated_date": "2025-06-26 17:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:32.037106+00:00"
    },
    {
      "arxiv_id": "2506.21550v1",
      "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale",
      "title_zh": "mTSBenchï¼šå¤§è§„æ¨¡å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸æ¨¡å‹é€‰æ‹©åŸºå‡†",
      "authors": [
        "Xiaona Zhou",
        "Constantin Brif",
        "Ismini Lourentzou"
      ],
      "abstract": "Multivariate time series anomaly detection (MTS-AD) is critical in domains like healthcare, cybersecurity, and industrial monitoring, yet remains challenging due to complex inter-variable dependencies, temporal dynamics, and sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for MTS-AD and unsupervised model selection, spanning 344 labeled time series across 19 datasets and 12 diverse application domains. mTSBench evaluates 24 anomaly detection methods, including large language model (LLM)-based detectors for multivariate time series, and systematically benchmarks unsupervised model selection techniques under standardized conditions. Consistent with prior findings, our results confirm that no single detector excels across datasets, underscoring the importance of model selection. However, even state-of-the-art selection methods remain far from optimal, revealing critical gaps. mTSBench provides a unified evaluation suite to enable rigorous, reproducible comparisons and catalyze future advances in adaptive anomaly detection and robust model selection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† mTSBenchï¼Œè¿™æ˜¯ç›®å‰è§„æ¨¡æœ€å¤§çš„å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ (Multivariate Time Series Anomaly Detection, MTS-AD) å’Œæ— ç›‘ç£æ¨¡å‹é€‰æ‹©åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº† 12 ä¸ªåº”ç”¨é¢†åŸŸçš„ 19 ä¸ªæ•°æ®é›†å’Œ 344 ä¸ªå¸¦æ ‡ç­¾çš„æ—¶é—´åºåˆ—ã€‚mTSBench ç³»ç»Ÿè¯„ä¼°äº†åŒ…æ‹¬åŸºäºå¤§è¯­è¨€æ¨¡å‹ (Large Language Model, LLM) åœ¨å†…çš„ 24 ç§å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå¹¶åœ¨æ ‡å‡†åŒ–æ¡ä»¶ä¸‹å¯¹æ— ç›‘ç£æ¨¡å‹é€‰æ‹©æŠ€æœ¯è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”±äºæ²¡æœ‰ä»»ä½•å•ä¸€æ£€æµ‹å™¨èƒ½åœ¨æ‰€æœ‰æ•°æ®é›†ä¸­è¡¨ç°æœ€ä¼˜ï¼Œæ¨¡å‹é€‰æ‹©å˜å¾—è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„å…ˆè¿›é€‰æ‹©æ–¹æ³•ä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡æä¾›ç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œå®ç°äº†ä¸¥è°¨ä¸”å¯é‡å¤çš„æ€§èƒ½å¯¹æ¯”ï¼Œä¸ºæœªæ¥è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹å’Œç¨³å¥æ¨¡å‹é€‰æ‹©çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21550v1",
      "published_date": "2025-06-26 17:59:58 UTC",
      "updated_date": "2025-06-26 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:32.335187+00:00"
    },
    {
      "arxiv_id": "2506.21546v3",
      "title": "Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination",
      "title_zh": "åäº‹å®åˆ†å‰²æ¨ç†ï¼šåƒç´ å®šä½å¹»è§‰çš„è¯Šæ–­ä¸ç¼“è§£",
      "authors": [
        "Xinzhuo Li",
        "Adheesh Juvekar",
        "Jiaxun Zhang",
        "Xingyou Liu",
        "Muntasir Wahed",
        "Kiet A. Nguyen",
        "Yifan Shen",
        "Tianjiao Yu",
        "Ismini Lourentzou"
      ],
      "abstract": "Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å‰²è§†è§‰è¯­è¨€æ¨¡å‹ (Segmentation VLMs) ä¸­å‡ºç°çš„åƒç´ å®šä½å¹»è§‰ (pixel-grounding hallucinations) é—®é¢˜ï¼Œå³æ¨¡å‹ä¸ºé”™è¯¯æˆ–ä¸å­˜åœ¨çš„å¯¹è±¡ç”Ÿæˆæ©ç çš„ç°è±¡è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…æŒ‡å‡ºå½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æˆ–æ ‡ç­¾æ‰°åŠ¨ï¼Œéš¾ä»¥æ­ç¤ºæ›´å…·æŒ‘æˆ˜æ€§çš„è§†è§‰é©±åŠ¨å¹»è§‰ï¼Œä¸ºæ­¤æ­£å¼æå‡ºäº†åäº‹å®åˆ†å‰²æ¨ç† (Counterfactual Segmentation Reasoning, CSR) ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒè¯¥ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡è¯Šæ–­åŸºå‡† HalluSegBenchï¼Œåˆ©ç”¨å—æ§çš„è§†è§‰åäº‹å®æ ·æœ¬ä»¥åŠæ–°åº¦é‡æŒ‡æ ‡æ¥è¯„ä¼°å¹»è§‰çš„ä¸¥é‡ç¨‹åº¦å¹¶åŒºåˆ†å¤±è´¥æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†é€šè¿‡åäº‹å®å¾®è°ƒ (Counterfactual Fine-Tuning, CFT) è®­ç»ƒçš„ RobustSeg æ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡åœ¨é¢å¯¹åäº‹å®åœºæ™¯æ—¶æ‹’ç»åˆ†å‰²çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRobustSeg åœ¨æ˜¾è‘—æå‡ FP-RefCOCO(+/g) ç­‰æ•°æ®é›†åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å¹»è§‰ç°è±¡å‡å°‘äº† 30%ï¼Œä¸ºå¢å¼ºè§†è§‰å®šä½æ¨¡å‹çš„é²æ£’æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/",
      "pdf_url": "https://arxiv.org/pdf/2506.21546v3",
      "published_date": "2025-06-26 17:59:12 UTC",
      "updated_date": "2025-12-11 19:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:42.154839+00:00"
    },
    {
      "arxiv_id": "2506.21545v1",
      "title": "Data Efficacy for Language Model Training",
      "title_zh": "è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ•°æ®æ•ˆåŠ›",
      "authors": [
        "Yalun Dai",
        "Yangyu Huang",
        "Xin Zhang",
        "Wenshan Wu",
        "Chong Li",
        "Wenhui Lu",
        "Shijie Cao",
        "Li Dong",
        "Scarlett Li"
      ],
      "abstract": "Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œæ•°æ®æ•ˆèƒ½â€(Data Efficacy)çš„æ¦‚å¿µï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–è®­ç»ƒæ•°æ®çš„ç»„ç»‡æ–¹å¼æ¥æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹(Language Model)çš„æ€§èƒ½ï¼Œä»¥æ­¤ä½œä¸ºå¯¹ç°æœ‰æ•°æ®æ•ˆç‡ç ”ç©¶çš„è¡¥å……ã€‚ä½œè€…å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„èŒƒå¼DELTï¼Œè¯¥èŒƒå¼ç”±æ•°æ®è¯„åˆ†(Data Scoring)ã€æ•°æ®é€‰æ‹©(Data Selection)å’Œæ•°æ®æ’åº(Data Ordering)ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚ç ”ç©¶é‡ç‚¹è®¾è®¡äº†å¯å­¦ä¹ æ€§-è´¨é‡è¯„åˆ†(Learnability-Quality Scoring, LQS)ï¼Œä»æ¢¯åº¦ä¸€è‡´æ€§(gradient consistency)çš„è§’åº¦è¯„ä¼°æ•°æ®æ ·æœ¬ï¼Œå¹¶æå‡ºäº†æŠ˜å æ’åº(Folding Ordering, FO)ä»¥è§£å†³æ¨¡å‹é—å¿˜(model forgetting)å’Œæ•°æ®åˆ†å¸ƒåå·®é—®é¢˜ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼ŒDELTåœ¨ä¸å¢åŠ æ•°æ®è§„æ¨¡å’Œæ¨¡å‹å°ºå¯¸çš„æƒ…å†µä¸‹ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ã€‚å…¶ä¸­ï¼ŒLQSä¸æŠ˜å æ’åºçš„ç»„åˆè¾¾åˆ°äº†æœ€ä¼˜æ”¹è¿›æ•ˆæœï¼Œå¹¶è¯æ˜äº†æ•°æ®æ•ˆèƒ½å¯ä»¥ä¸æ•°æ®é€‰æ‹©ç»“åˆä»¥åŒæ—¶å®ç°é«˜æ•ˆè®­ç»ƒã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†æ•°æ®ç»„ç»‡åœ¨æ¨¡å‹è®­ç»ƒä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œä¸ºæ•°æ®æ•ˆèƒ½è¿™ä¸€åŸºç¡€é¢†åŸŸå¥ å®šäº†ç ”ç©¶æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21545v1",
      "published_date": "2025-06-26 17:59:07 UTC",
      "updated_date": "2025-06-26 17:59:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:40.798756+00:00"
    },
    {
      "arxiv_id": "2506.21655v1",
      "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization",
      "title_zh": "APOï¼šé€šè¿‡éå¯¹ç§°ç­–ç•¥ä¼˜åŒ–æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Minjie Hong",
        "Zirun Guo",
        "Yan Xia",
        "Zehan Wang",
        "Ziang Zhang",
        "Tao Jin",
        "Zhou Zhao"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse data, but they often struggle with complex reasoning. While Reinforcement learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky. Common issues include a drop in performance on general tasks and the generation of overly detailed or \"overthinking\" reasoning. Our work investigates how the KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric Policy Optimization (APO) to address these issues, which divides the sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically adjust the KL divergence weight based on their difficulty. This method prevents policy entropy from dropping sharply, improves training stability, utilizes samples better, and preserves the model's existing knowledge. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to penalize overly long responses. This helps mitigate overthinking and encourages more concise reasoning while preserving the model's explorative capacity. We apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B significantly enhances reasoning capabilities, showing an average 7\\% gain over the base model and outperforming larger MLLMs (7-11B) on various reasoning benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade on general tasks, View-R1-3B maintains consistent improvement, demonstrating superior generalization. These results highlight the effectiveness and broad applicability of our DADS and STCR techniques for advancing complex multimodal reasoning in MLLMs. The code will be made available at https://github.com/Indolent-Kawhi/View-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¼‚æ­¥ç­–ç•¥ä¼˜åŒ–(Asymmetric Policy Optimization, APO)æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è§çš„é€šç”¨ä»»åŠ¡æ€§èƒ½ä¸‹é™åŠâ€œè¿‡åº¦æ€è€ƒâ€é—®é¢˜ã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ­£å‘æ ·æœ¬å¼•å…¥äº†éš¾åº¦è‡ªé€‚åº”æ•£åº¦å¡‘å½¢(Difficulty-Adaptive Divergence Shaping, DADS)ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ KL æ•£åº¦æƒé‡æ¥æå‡è®­ç»ƒç¨³å®šæ€§å¹¶ä¿ç•™æ¨¡å‹æ—¢æœ‰çŸ¥è¯†ã€‚é’ˆå¯¹è´Ÿå‘æ ·æœ¬ï¼Œç ”ç©¶æå‡ºäº†æ¬¡ä¼˜è½¨è¿¹å¤æ‚åº¦æ­£åˆ™åŒ–(Suboptimal Trajectory Complexity Regularization, STCR)ï¼Œé€šè¿‡æƒ©ç½šè¿‡é•¿çš„å›å¤æ¥æŠ‘åˆ¶å†—ä½™æ¨ç†ï¼Œç¡®ä¿é€»è¾‘çš„ç®€æ´æ€§ã€‚å®éªŒåŸºäº Qwen2.5-VL-3B æ„å»ºäº† View-R1-3B æ¨¡å‹ï¼Œåœ¨å¤šé¡¹æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ¯”åŸºå‡†æ¨¡å‹å¹³å‡æå‡äº† 7%ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„ 7-11B æ¨¡å‹ã€‚ä¸è®¸å¤šæ¨ç†è°ƒä¼˜åé€šç”¨èƒ½åŠ›ä¸‹é™çš„æ¨¡å‹ä¸åŒï¼ŒView-R1-3B åœ¨å¢å¼ºæ¨ç†çš„åŒæ—¶ä¿æŒäº†ä¼˜ç§€çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº† DADS å’Œ STCR æŠ€æœ¯åœ¨æ¨åŠ¨ MLLMs å¤æ‚å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21655v1",
      "published_date": "2025-06-26 17:57:08 UTC",
      "updated_date": "2025-06-26 17:57:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:39.536376+00:00"
    },
    {
      "arxiv_id": "2506.21539v1",
      "title": "WorldVLA: Towards Autoregressive Action World Model",
      "title_zh": "WorldVLAï¼šè¿ˆå‘è‡ªå›å½’åŠ¨ä½œä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Jun Cen",
        "Chaohui Yu",
        "Hangjie Yuan",
        "Yuming Jiang",
        "Siteng Huang",
        "Jiayan Guo",
        "Xin Li",
        "Yibing Song",
        "Hao Luo",
        "Fan Wang",
        "Deli Zhao",
        "Hao Chen"
      ],
      "abstract": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WorldVLAï¼Œè¿™æ˜¯ä¸€ä¸ªå°†åŠ¨ä½œä¸å›¾åƒçš„ç†è§£ä¸ç”Ÿæˆç›¸ç»Ÿä¸€çš„è‡ªå›å½’åŠ¨ä½œä¸–ç•Œæ¨¡å‹(Autoregressive Action World Model)ã€‚è¯¥æ¡†æ¶åœ¨å•ä¸€ä½“ç³»å†…æ•´åˆäº†è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹(World Model)ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹é¢„æµ‹æœªæ¥å›¾åƒä»¥å­¦ä¹ ç¯å¢ƒç‰©ç†è§„å¾‹ï¼ŒåŒæ—¶é€šè¿‡åŠ¨ä½œæ¨¡å‹ç”Ÿæˆåç»­æ“ä½œæ¥è¾…åŠ©è§†è§‰ç†è§£ä¸ç”Ÿæˆã€‚å®éªŒè¯æ˜WorldVLAä¼˜äºç‹¬ç«‹çš„åŠ¨ä½œæˆ–ä¸–ç•Œæ¨¡å‹ï¼Œä½“ç°äº†ä¸¤è€…é—´çš„ç›¸äº’å¢å¼ºä½œç”¨ã€‚é’ˆå¯¹æ¨¡å‹åœ¨è‡ªå›å½’ç”ŸæˆåŠ¨ä½œåºåˆ—æ—¶å› è¯¯å·®ä¼ æ’­å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›æ©ç (Attention Mask)ç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡åœ¨ç”Ÿæˆå½“å‰åŠ¨ä½œæ—¶é€‰æ‹©æ€§åœ°å±è”½å…ˆéªŒåŠ¨ä½œï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åŠ¨ä½œå—(Action Chunk)ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA",
      "pdf_url": "https://arxiv.org/pdf/2506.21539v1",
      "published_date": "2025-06-26 17:55:40 UTC",
      "updated_date": "2025-06-26 17:55:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:42.970938+00:00"
    },
    {
      "arxiv_id": "2506.21536v1",
      "title": "PsyLite Technical Report",
      "title_zh": "PsyLite æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Fangjun Ding",
        "Renyu Zhang",
        "Xinyu Feng",
        "Chengye Xie",
        "Zheng Zhang",
        "Yanting Zhang"
      ],
      "abstract": "With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score improvement of 2.4\\%). Additionally, the model uses quantization technology (GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PsyLiteï¼Œä¸€ä¸ªåŸºäº InternLM2.5-7B-chat å¼€å‘çš„è½»é‡åŒ–å¿ƒç†å’¨è¯¢å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¯¹è¯å®‰å…¨ã€ç»†èŠ‚åœºæ™¯å¤„ç†åŠéƒ¨ç½²é—¨æ§›æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡ç»“åˆæ··åˆè’¸é¦æ•°æ®å¾®è°ƒä¸ ORPO åå¥½ä¼˜åŒ–çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒPsyLite æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ·±åº¦æ¨ç†ã€å¿ƒç†å’¨è¯¢ä¸“ä¸šåº¦åŠå®‰å…¨å¯¹è¯èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†åˆ›æ–°æ€§çš„ Conditional RAG æœºåˆ¶ï¼Œåœ¨å’¨è¯¢è¿‡ç¨‹ä¸­é€‚æ—¶èå…¥ç›¸å£°å¹½é»˜å…ƒç´ ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶å¼ºåŒ–äº†å¯¹å±é™©è¯·æ±‚çš„æ‹¦æˆªã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒPsyLite åœ¨ CPsyCounE å¿ƒç†å’¨è¯¢ä¸“ä¸šæµ‹è¯„ä¸­å¾—åˆ†æå‡äº† 47.6%ï¼Œå¹¶åœ¨ SafeDialBench å®‰å…¨æµ‹è¯„ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åˆ©ç”¨ GGUF q4_k_m é‡åŒ–æŠ€æœ¯å®ç°äº†ä»…éœ€ 5GB å†…å­˜çš„æä½ç¡¬ä»¶éƒ¨ç½²è¦æ±‚ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¿ƒç†å¥åº·åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21536v1",
      "published_date": "2025-06-26 17:54:42 UTC",
      "updated_date": "2025-06-26 17:54:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:52.248668+00:00"
    },
    {
      "arxiv_id": "2506.21532v3",
      "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets",
      "title_zh": "â€œåŒ»ç”Ÿï¼Œæ€ä¹ˆäº†ï¼Ÿâ€ï¼šå¤§è§„æ¨¡å¯¹è¯å¼äººå·¥æ™ºèƒ½æ•°æ®é›†ä¸­ç”¨æˆ·å¥åº·ä¿¡æ¯å¯»æ±‚è¡Œä¸ºåˆ†æ",
      "authors": [
        "Akshay Paruchuri",
        "Maryam Aziz",
        "Rohit Vartak",
        "Ayman Ali",
        "Best Uchehara",
        "Xin Liu",
        "Ishan Chatterjee",
        "Monica Agrawal"
      ],
      "abstract": "People are increasingly seeking healthcare information from large language models (LLMs) via interactive chatbots, yet the nature and inherent risks of these conversations remain largely unexplored. In this paper, we filter large-scale conversational AI datasets to achieve HealthChat-11K, a curated dataset of 11K real-world conversations composed of 25K user messages. We use HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs when seeking healthcare information in order to systematically study user interactions across 21 distinct health specialties. Our analysis reveals insights into the nature of how and why users seek health information, such as common interactions, instances of incomplete context, affective behaviors, and interactions (e.g., leading questions) that can induce sycophancy, underscoring the need for improvements in the healthcare support capabilities of LLMs deployed as conversational AI. Code and artifacts to retrieve our analyses and combine them into a curated dataset can be found here: https://github.com/yahskapar/HealthChat",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æå¤§è§„æ¨¡å¯¹è¯å¼äººå·¥æ™ºèƒ½(Conversational AI)æ•°æ®é›†ï¼Œæ¢è®¨äº†ç”¨æˆ·åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è·å–åŒ»ç–—å¥åº·ä¿¡æ¯çš„æ¨¡å¼åŠå…¶å›ºæœ‰é£é™©ã€‚ä½œè€…é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œæ„å»ºäº†åŒ…å«1.1ä¸‡åœºçœŸå®å¯¹è¯çš„ç²¾é€‰æ•°æ®é›†HealthChat-11Kï¼Œå¹¶åˆ©ç”¨ä¸´åºŠåŒ»ç”Ÿä¸»å¯¼çš„åˆ†ç±»æ³•(Clinician-driven taxonomy)å¯¹æ¶‰åŠ21ä¸ªåŒ»ç–—ä¸“ä¸šçš„äº¤äº’è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚åˆ†ææ­ç¤ºäº†ç”¨æˆ·å¯»æ±‚å¥åº·ä¿¡æ¯çš„æ·±å±‚åŠ¨æœºï¼Œå¹¶è¯†åˆ«å‡ºä¸Šä¸‹æ–‡ç¼ºå¤±(Incomplete context)ã€æƒ…æ„ŸåŒ–è¡Œä¸ºä»¥åŠæ˜“è¯±å‘æ¨¡å‹äº§ç”Ÿé¡ºä»æ€§(Sycophancy)çš„è¯±å¯¼æ€§æé—®(Leading questions)ç­‰å…³é”®äº¤äº’ç‰¹å¾ã€‚è¯¥ç ”ç©¶æˆæœä¸ä»…ä¸ºç†è§£çœŸå®åœºæ™¯ä¸‹çš„åŒ»ç–—AIäº¤äº’æä¾›äº†å®è¯ä¾æ®ï¼Œä¹Ÿå¼ºè°ƒäº†è¿›ä¸€æ­¥ä¼˜åŒ–å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ”¯æŒé¢†åŸŸä¸“ä¸šæ€§ä¸å¯é æ€§çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Findings - 25 pages, 6 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.21532v3",
      "published_date": "2025-06-26 17:52:18 UTC",
      "updated_date": "2025-09-20 01:36:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:48:56.207545+00:00"
    },
    {
      "arxiv_id": "2506.21521v2",
      "title": "Potemkin Understanding in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ³¢å°†é‡‘ç†è§£",
      "authors": [
        "Marina Mancoridis",
        "Bec Weeks",
        "Keyon Vafa",
        "Sendhil Mullainathan"
      ],
      "abstract": "Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¸å…¶çœŸå®ç†è§£èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºäº† Potemkin Understanding (æ³¢å°†é‡‘ç†è§£)çš„æ¦‚å¿µã€‚ä½œè€…æŒ‡å‡ºï¼Œç”±äºç›®å‰çš„åŸºå‡†æµ‹è¯•(å¦‚ AP exams)å¤šä¸ºäººç±»è®¾è®¡ï¼Œåªæœ‰å½“æ¨¡å‹è¯¯è§£æ¦‚å¿µçš„æ–¹å¼ä¸äººç±»ä¸€è‡´æ—¶ï¼Œè¿™äº›æµ‹è¯•æ‰æœ‰æ•ˆï¼Œå¦åˆ™é«˜åˆ†å¯èƒ½ä»…ä»£è¡¨ä¸€ç§ç†è§£çš„å¹»è±¡ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå½¢å¼åŒ–æ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§ç¨‹åºæ¥é‡åŒ– Potemkin ç†è§£çš„å­˜åœ¨åŠå…¶æµè¡Œç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPotemkin ç†è§£åœ¨å„ç§æ¨¡å‹ã€ä»»åŠ¡å’Œé¢†åŸŸä¸­æ™®éå­˜åœ¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè¿™äº›å¤±æ•ˆä¸ä»…æºäºé”™è¯¯çš„ç†è§£ï¼Œæ›´åæ˜ äº† LLMs åœ¨æ¦‚å¿µè¡¨ç¤º(concept representations)ä¸­å­˜åœ¨æ·±å±‚çš„å†…éƒ¨ä¸ä¸€è‡´æ€§(internal incoherence)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21521v2",
      "published_date": "2025-06-26 17:41:35 UTC",
      "updated_date": "2025-06-29 18:12:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:09.402502+00:00"
    },
    {
      "arxiv_id": "2506.22518v1",
      "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation",
      "title_zh": "Weak-to-Strong GraphRAGï¼šé¢å‘å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¼±æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹é½",
      "authors": [
        "Deyu Zou",
        "Yongqiang Chen",
        "Mufei Li",
        "Siqi Miao",
        "Chenxi Liu",
        "Bo Han",
        "James Cheng",
        "Pan Li"
      ],
      "abstract": "Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals to the LLMs. II) Due to the abstraction of graph data, the retrieved knowledge is often presented in unorganized forms. To mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM feedback to get rid of spurious signals and improve the quality of the supervision. Meanwhile, ReG introduces a structure-aware reorganization module to refactor the retrieval results into logically coherent evidence chains. Experiments on prominent benchmarks demonstrate that ReG significantly and consistently brings improvements across different LLM backbones by up to 10%. The improved supervision quality enables ReG to match the state-of-the-art performance with 5% training data and to transfer to out-of-distribution KGs. Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token cost by up to 30% and improves the performance by up to 4%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Graph-based RAG)ä¸­æ£€ç´¢å™¨(retriever)å› å¼±ç›‘ç£äº§ç”Ÿçš„è™šå‡ä¿¡å·ä»¥åŠæ£€ç´¢çŸ¥è¯†æ— åºç­‰é—®é¢˜ï¼Œæå‡ºäº†æ—¨åœ¨å¯¹é½å¼±æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„Refined Graph-based RAG (ReG)æ¡†æ¶ã€‚ReGé€šè¿‡å¼•å…¥LLMåé¦ˆæœºåˆ¶æ¥æ¶ˆé™¤ä¼ªä¿¡å·å¹¶æé«˜ç›‘ç£è´¨é‡ï¼ŒåŒæ—¶åˆ©ç”¨ç»“æ„æ„ŸçŸ¥é‡ç»„æ¨¡å—(structure-aware reorganization module)å°†æ£€ç´¢ç»“æœé‡æ„ä¸ºé€»è¾‘è¿è´¯çš„è¯æ®é“¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReGåœ¨ä¸åŒLLMåº•åº§ä¸Šå‡å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ï¼Œæœ€é«˜å¢å¹…è¾¾10%ï¼Œä¸”ä»…éœ€5%çš„è®­ç»ƒæ•°æ®å³å¯åŒ¹é…å½“å‰æœ€å…ˆè¿›æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒReGåœ¨å¤„ç†åŸŸå¤–(out-of-distribution)çŸ¥è¯†å›¾è°±æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„è¿ç§»èƒ½åŠ›ï¼Œåº”ç”¨äºæ¨ç†å‹LLMsæ—¶èƒ½åœ¨å‡å°‘30%æ¨ç†Tokenæˆæœ¬çš„åŒæ—¶æå‡4%çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22518v1",
      "published_date": "2025-06-26 17:40:23 UTC",
      "updated_date": "2025-06-26 17:40:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:11.737497+00:00"
    },
    {
      "arxiv_id": "2506.21508v1",
      "title": "skLEP: A Slovak General Language Understanding Benchmark",
      "title_zh": "skLEPï¼šæ–¯æ´›ä¼å…‹è¯­é€šç”¨è¯­è¨€ç†è§£åŸºå‡†",
      "authors": [
        "Marek Å uppa",
        "Andrej Ridzik",
        "Daniel HlÃ¡dek",
        "TomÃ¡Å¡ JavÅ¯rek",
        "ViktÃ³ria OndrejovÃ¡",
        "KristÃ­na SÃ¡sikovÃ¡",
        "Martin Tamajka",
        "MariÃ¡n Å imko"
      ],
      "abstract": "In this work, we introduce skLEP, the first comprehensive benchmark specifically designed for evaluating Slovak natural language understanding (NLU) models. We have compiled skLEP to encompass nine diverse tasks that span token-level, sentence-pair, and document-level challenges, thereby offering a thorough assessment of model capabilities. To create this benchmark, we curated new, original datasets tailored for Slovak and meticulously translated established English NLU resources. Within this paper, we also present the first systematic and extensive evaluation of a wide array of Slovak-specific, multilingual, and English pre-trained language models using the skLEP tasks. Finally, we also release the complete benchmark data, an open-source toolkit facilitating both fine-tuning and evaluation of models, and a public leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering reproducibility and drive future research in Slovak NLU.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†skLEPï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°æ–¯æ´›ä¼å…‹è¯­è‡ªç„¶è¯­è¨€ç†è§£(NLU)æ¨¡å‹çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†åŒ…æ‹¬token-levelã€sentence-pairå’Œdocument-levelåœ¨å†…çš„ä¹é¡¹å¤šæ ·åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨æä¾›å¯¹æ¨¡å‹èƒ½åŠ›çš„å½»åº•è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç­–åˆ’å…¨æ–°çš„æ–¯æ´›ä¼å…‹è¯­åŸå§‹æ•°æ®é›†å¹¶ç²¾å¿ƒç¿»è¯‘æˆç†Ÿçš„è‹±æ–‡NLUèµ„æºï¼Œæ„å»ºäº†è¿™ä¸€èµ„æºåº“ã€‚æ–‡ä¸­è¿˜é¦–æ¬¡å¯¹ä¸€ç³»åˆ—æ–¯æ´›ä¼å…‹è¯­ç‰¹å®šæ¨¡å‹ã€å¤šè¯­è¨€æ¨¡å‹åŠè‹±æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å¹¿æ³›è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼€æºäº†å®Œæ•´çš„åŸºå‡†æ•°æ®ã€æ”¯æŒæ¨¡å‹å¾®è°ƒä¸è¯„ä¼°çš„å·¥å…·åŒ…ä»¥åŠå…¬å¼€æ’è¡Œæ¦œã€‚è¿™ä¸€è´¡çŒ®æ—¨åœ¨ä¿ƒè¿›æ–¯æ´›ä¼å…‹è¯­NLUé¢†åŸŸçš„å¯é‡å¤æ€§ç ”ç©¶å¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.21508v1",
      "published_date": "2025-06-26 17:35:04 UTC",
      "updated_date": "2025-06-26 17:35:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:15.700808+00:00"
    },
    {
      "arxiv_id": "2506.21506v2",
      "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
      "title_zh": "Mind2Web 2ï¼šåŸºäº Agent-as-a-Judge æ¡†æ¶çš„æ™ºèƒ½ä½“æœç´¢è¯„ä¼°",
      "authors": [
        "Boyu Gou",
        "Zanming Huang",
        "Yuting Ning",
        "Yu Gu",
        "Michael Lin",
        "Weijian Qi",
        "Andrei Kopanev",
        "Botao Yu",
        "Bernal JimÃ©nez GutiÃ©rrez",
        "Yiheng Shu",
        "Chan Hee Song",
        "Jiaman Wu",
        "Shijie Chen",
        "Hanane Nour Moussa",
        "Tianshu Zhang",
        "Jian Xie",
        "Yifei Li",
        "Tianci Xue",
        "Zeyi Liao",
        "Kai Zhang",
        "Boyuan Zheng",
        "Zhaowei Cai",
        "Viktor Rozgic",
        "Morteza Ziyadi",
        "Huan Sun",
        "Yu Su"
      ],
      "abstract": "Agentic search such as Deep Research systems-where agents autonomously browse the web, synthesize information, and return comprehensive citation-backed answers-represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of ten frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, highlighting its great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç†æœç´¢(Agentic search)ï¼ˆå¦‚Deep Researchç³»ç»Ÿï¼‰æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§ï¼Œæå‡ºäº†Mind2Web 2åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•åº”å¯¹é•¿æ—¶ç¨‹æœç´¢å’ŒåŠ¨æ€ç­”æ¡ˆçš„é—®é¢˜ã€‚Mind2Web 2åŒ…å«130ä¸ªé«˜è´¨é‡ã€é•¿æ—¶ç¨‹çš„çœŸå®ä»»åŠ¡ï¼Œæ¶‰åŠå®æ—¶ç½‘é¡µæµè§ˆå’Œå¤§è§„æ¨¡ä¿¡æ¯åˆæˆï¼Œç”±è¶…è¿‡1000å°æ—¶çš„äººå·¥åŠ³åŠ¨æ„å»ºè€Œæˆã€‚ä¸ºäº†è¯„ä¼°éšæ—¶é—´å˜åŒ–çš„å¤æ‚ç­”æ¡ˆï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„Agent-as-a-Judgeæ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ ‘çŠ¶ç»“æ„å‡†åˆ™(tree-structured rubric)è®¾è®¡çš„ç‰¹å®šä»»åŠ¡è£åˆ¤ä»£ç†ï¼Œè‡ªåŠ¨è¯„ä¼°ç­”æ¡ˆçš„å‡†ç¡®æ€§åŠå…¶æ¥æºå½’å±(source attribution)ã€‚ç ”ç©¶å¯¹åä¸ªå‰æ²¿ä»£ç†æœç´¢ç³»ç»Ÿå’Œäººç±»è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç›®å‰æ€§èƒ½æœ€ä½³çš„OpenAI Deep Researchåœ¨è€—æ—¶ä»…ä¸ºäººç±»ä¸€åŠçš„æƒ…å†µä¸‹ï¼Œå·²èƒ½è¾¾åˆ°äººç±»æ°´å¹³çš„50-70%ã€‚é€šè¿‡è¯¦ç»†çš„é”™è¯¯åˆ†æï¼Œè¯¥ç ”ç©¶ä¸ºä¸‹ä¸€ä»£ä»£ç†æœç´¢ç³»ç»Ÿçš„å¼€å‘å’ŒåŸºå‡†æµ‹è¯•å¥ å®šäº†åšå®åŸºç¡€ï¼Œå±•ç¤ºäº†è‡ªä¸»Webæ™ºèƒ½ç³»ç»Ÿçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web-2/",
      "pdf_url": "https://arxiv.org/pdf/2506.21506v2",
      "published_date": "2025-06-26 17:32:50 UTC",
      "updated_date": "2025-07-03 15:47:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:20.565048+00:00"
    },
    {
      "arxiv_id": "2506.21502v2",
      "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems",
      "title_zh": "è¿‡ç¨‹æŒ–æ˜é©±åŠ¨çš„å»ºæ¨¡ä¸ä»¿çœŸï¼Œæ—¨åœ¨å¢å¼ºä¿¡æ¯ç‰©ç†ç³»ç»Ÿçš„æ•…éšœè¯Šæ–­",
      "authors": [
        "Francesco Vitale",
        "Nicola Dall'Ora",
        "Sebastiano Gaiardelli",
        "Enrico Fraccaroli",
        "Nicola Mazzocca",
        "Franco Fummi"
      ],
      "abstract": "Cyber-Physical Systems (CPSs) tightly interconnect digital and physical operations within production environments, enabling real-time monitoring, control, optimization, and autonomous decision-making that directly enhance manufacturing processes and productivity. The inherent complexity of these systems can lead to faults that require robust and interpretable diagnoses to maintain system dependability and operational efficiency. However, manual modeling of faulty behaviors requires extensive domain expertise and cannot leverage the low-level sensor data of the CPS. Furthermore, although powerful, deep learning-based techniques produce black-box diagnostics that lack interpretability, limiting their practical adoption. To address these challenges, we set forth a method that performs unsupervised characterization of system states and state transitions from low-level sensor data, uses several process mining techniques to model faults through interpretable stochastic Petri nets, simulates such Petri nets for a comprehensive understanding of system behavior under faulty conditions, and performs Petri net-based fault diagnosis. The method is applied to the Robotic Arm Dataset (RoAD), a benchmark collected from a robotic arm deployed in a scale-replica smart manufacturing assembly line. The application to RoAD demonstrates the method's effectiveness in modeling, simulating, and classifying faulty behaviors in CPSs. The modeling results demonstrate that our method achieves a satisfactory interpretability-simulation accuracy trade-off with up to 0.676 arc-degree simplicity, 0.395 R^2, and 0.088 RMSE. In addition, the fault identification results show that the method achieves an F1 score of up to 98.925%, while maintaining a low conformance checking time of 0.020 seconds, which competes with other deep learning-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Cyber-Physical Systems, CPS)æ•…éšœè¯Šæ–­ä¸­é¢ä¸´çš„äººå·¥å»ºæ¨¡éš¾åº¦å¤§åŠæ·±åº¦å­¦ä¹ ç¼ºä¹å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç”±æµç¨‹æŒ–æ˜(Process mining)é©±åŠ¨çš„å»ºæ¨¡ä¸ä»¿çœŸæ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡å¯¹åº•å±‚ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œæ— ç›‘ç£çŠ¶æ€è¡¨å¾ï¼Œåˆ©ç”¨å¤šç§æµç¨‹æŒ–æ˜æŠ€æœ¯å°†æ•…éšœè¡Œä¸ºæ„å»ºä¸ºå¯è§£é‡Šçš„éšæœºPetriç½‘(Stochastic Petri nets)ã€‚é€šè¿‡æ¨¡æ‹Ÿè¿™äº›Petriç½‘ï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ·±å…¥ç†è§£æ•…éšœæ¡ä»¶ä¸‹çš„ç³»ç»Ÿè¡Œä¸ºï¼Œå¹¶æ®æ­¤å¼€å±•é«˜æ•ˆçš„æ•…éšœè¯Šæ–­ã€‚åœ¨æœºæ¢°è‡‚æ•°æ®é›†(Robotic Arm Dataset, RoAD)ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜ç§€çš„ä»¿çœŸç²¾åº¦ã€‚åœ¨æ•…éšœè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•è·å¾—äº†é«˜è¾¾98.925%çš„F1åˆ†æ•°ï¼Œä¸”ä¸€è‡´æ€§æ£€æŸ¥(conformance checking)æ—¶é—´ä»…ä¸º0.020ç§’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CPSæ•…éšœå»ºæ¨¡ä¸åˆ†ç±»æ–¹é¢å…·æœ‰æé«˜çš„æœ‰æ•ˆæ€§ï¼Œæ€§èƒ½è¶³ä»¥ä¸æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21502v2",
      "published_date": "2025-06-26 17:29:37 UTC",
      "updated_date": "2025-12-14 12:04:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:21.354504+00:00"
    },
    {
      "arxiv_id": "2506.21490v2",
      "title": "Ad-Hoc Human-AI Coordination Challenge",
      "title_zh": "Ad-Hoc äººæœºåä½œæŒ‘æˆ˜",
      "authors": [
        "Tin DizdareviÄ‡",
        "Ravi Hammond",
        "Tobias Gessler",
        "Anisoara Calinescu",
        "Jonathan Cook",
        "Matteo Gallici",
        "Andrei Lupu",
        "Darius Muglich",
        "Johannes Forkel",
        "Jakob Nicolaus Foerster"
      ],
      "abstract": "Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \\textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI ä¸äººç±»åœ¨çœŸå®åœºæ™¯ä¸‹éš¾ä»¥å®ç°æ— ç¼åä½œçš„é—®é¢˜ï¼Œæå‡ºäº† Ad-Hoc Human-AI Coordination Challenge (AH2AC2) è¯„æµ‹æ¡†æ¶ã€‚ä¸ºäº†å…‹æœäººç±»è¯„ä¼° (human evaluation) æˆæœ¬é«˜ä¸”éš¾ä»¥å¤ç°çš„å±€é™ï¼Œç ”ç©¶è€…åŸºäºå¤§è§„æ¨¡äººç±»æ•°æ®é›†å¼€å‘äº† human proxy agentsï¼Œæ—¨åœ¨æä¾›ç¨³å¥ã€å»‰ä»·ä¸”å¯é‡å¤çš„è¯„ä¼°åŸºå‡†ã€‚è¯¥é¡¹æŒ‘æˆ˜ä»¥åˆä½œæ¸¸æˆ Hanabi ä¸ºæµ‹è¯•å¹³å°ï¼Œåˆ©ç”¨å…¶ä¸å®Œå…¨ä¿¡æ¯ (imperfect information) å’Œå¿ƒç†ç†è®º (Theory of Mind) ç­‰ç‰¹æ€§ï¼Œé‡ç‚¹è€ƒå¯Ÿ AI çš„åè°ƒèƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¼€æºäº†ä¸€ä¸ªåŒ…å« 3,079 å±€æ¸¸æˆçš„ Hanabi æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨æ•°æ®é«˜æ•ˆå‹ (data-efficient) æ–¹æ³•çš„å‘å±•ã€‚å®éªŒæä¾›äº†ä¸¤ååŠä¸‰åç©å®¶åœºæ™¯ä¸‹çš„ baseline resultsï¼Œå¹¶é€šè¿‡å—æ§è¯„ä¼°ç³»ç»Ÿç¡®ä¿äº†è¯„ä»·çš„å…¬æ­£æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21490v2",
      "published_date": "2025-06-26 17:19:52 UTC",
      "updated_date": "2025-06-29 10:25:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:28.432648+00:00"
    },
    {
      "arxiv_id": "2506.21484v1",
      "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning",
      "title_zh": "TITANï¼šåŸºäºæŸ¥è¯¢ä»¤ç‰Œçš„é¢†åŸŸè‡ªé€‚åº”å¯¹æŠ—å­¦ä¹ ",
      "authors": [
        "Tajamul Ashraf",
        "Janibul Bashir"
      ],
      "abstract": "We focus on the source-free domain adaptive object detection (SF-DAOD) problem when source data is unavailable during adaptation and the model must adapt to an unlabeled target domain. The majority of approaches for the problem employ a self-supervised approach using a student-teacher (ST) framework where pseudo-labels are generated via a source-pretrained model for further fine-tuning. We observe that the performance of a student model often degrades drastically, due to the collapse of the teacher model, primarily caused by high noise in pseudo-labels, resulting from domain bias, discrepancies, and a significant domain shift across domains. To obtain reliable pseudo-labels, we propose a Target-based Iterative Query-Token Adversarial Network (TITAN), which separates the target images into two subsets: those similar to the source (easy) and those dissimilar (hard). We propose a strategy to estimate variance to partition the target domain. This approach leverages the insight that higher detection variances correspond to higher recall and greater similarity to the source domain. Also, we incorporate query-token-based adversarial modules into a student-teacher baseline framework to reduce the domain gaps between two feature representations. Experiments conducted on four natural imaging datasets and two challenging medical datasets have substantiated the superior performance of TITAN compared to existing state-of-the-art (SOTA) methodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7 percent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºTITANçš„åŸºäºæŸ¥è¯¢ä»¤ç‰Œ(Query-Token)çš„è¿­ä»£å¯¹æŠ—ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³æ— æºåŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹(SF-DAOD)ä¸­å› æ•™å¸ˆæ¨¡å‹å´©æºƒå’Œä¼ªæ ‡ç­¾é«˜å™ªå£°å¯¼è‡´çš„æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å¼•å…¥äº†æ–¹å·®ä¼°è®¡ç­–ç•¥ï¼Œå°†ç›®æ ‡åŸŸå›¾åƒåˆ’åˆ†ä¸ºä¸æºåŸŸç›¸ä¼¼çš„ç®€å•æ ·æœ¬å’Œä¸ç›¸ä¼¼çš„å›°éš¾æ ·æœ¬ï¼Œä»¥æ­¤è·å–æ›´å¯é çš„ä¼ªæ ‡ç­¾ã€‚TITANåœ¨å­¦ç”Ÿ-æ•™å¸ˆ(Student-Teacher)æ¡†æ¶ä¸­é›†æˆäº†åŸºäºQuery-Tokençš„å¯¹æŠ—æ¨¡å—ï¼Œæœ‰æ•ˆç¼©å°äº†ä¸åŒåŸŸä¹‹é—´çš„ç‰¹å¾åˆ†å¸ƒé—´éš™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTITANåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å½±åƒç­‰å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›(SOTA)æŠ€æœ¯ã€‚åœ¨C2Få’ŒC2Bç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹æœ€é«˜å®ç°äº†22.7%çš„å¹³å‡ç²¾åº¦(mAP)æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨åº”å¯¹æ˜¾è‘—åŸŸåç§»æŒ‘æˆ˜æ—¶çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21484v1",
      "published_date": "2025-06-26 17:12:58 UTC",
      "updated_date": "2025-06-26 17:12:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:38.258659+00:00"
    },
    {
      "arxiv_id": "2506.21478v1",
      "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture",
      "title_zh": "SmoothSingerï¼šåŸºäºå¤šåˆ†è¾¨ç‡æ¶æ„çš„æ­Œå£°åˆæˆæ¡ä»¶æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Kehan Sui",
        "Jinxu Xiang",
        "Fang Jin"
      ],
      "abstract": "Singing voice synthesis (SVS) aims to generate expressive and high-quality vocals from musical scores, requiring precise modeling of pitch, duration, and articulation. While diffusion-based models have achieved remarkable success in image and video generation, their application to SVS remains challenging due to the complex acoustic and musical characteristics of singing, often resulting in artifacts that degrade naturalness. In this work, we propose SmoothSinger, a conditional diffusion model designed to synthesize high quality and natural singing voices. Unlike prior methods that depend on vocoders as a final stage and often introduce distortion, SmoothSinger refines low-quality synthesized audio directly in a unified framework, mitigating the degradation associated with two-stage pipelines. The model adopts a reference-guided dual-branch architecture, using low-quality audio from any baseline system as a reference to guide the denoising process, enabling more expressive and context-aware synthesis. Furthermore, it enhances the conventional U-Net with a parallel low-frequency upsampling path, allowing the model to better capture pitch contours and long term spectral dependencies. To improve alignment during training, we replace reference audio with degraded ground truth audio, addressing temporal mismatch between reference and target signals. Experiments on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that SmoothSinger achieves state-of-the-art results in both objective and subjective evaluations. Extensive ablation studies confirm its effectiveness in reducing artifacts and improving the naturalness of synthesized voices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SmoothSingerï¼Œä¸€ç§ä¸“é—¨ä¸ºæ­Œå£°åˆæˆ(Singing Voice Synthesis)è®¾è®¡çš„é«˜è´¨é‡æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å¤æ‚å£°å­¦ç‰¹å¾æ—¶äº§ç”Ÿçš„ä¼ªå½±å’Œè‡ªç„¶åº¦ä¸è¶³é—®é¢˜ã€‚è¯¥æ¨¡å‹æ‘’å¼ƒäº†æ˜“å¯¼è‡´å¤±çœŸçš„ä¼ ç»Ÿä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œé‡‡ç”¨å‚è€ƒå¼•å¯¼çš„åŒåˆ†æ”¯æ¶æ„(reference-guided dual-branch architecture)ï¼Œç›´æ¥åˆ©ç”¨åŸºçº¿ç³»ç»Ÿçš„ä½è´¨é‡éŸ³é¢‘ä½œä¸ºå¼•å¯¼æ¥é©±åŠ¨å»å™ªè¿‡ç¨‹ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æ•è·éŸ³é«˜è½®å»“(pitch contours)å’Œé•¿æœŸé¢‘è°±ä¾èµ–å…³ç³»ï¼ŒSmoothSingeråœ¨å¸¸è§„U-Netä¸­å¼•å…¥äº†å¹³è¡Œçš„ä½é¢‘ä¸Šé‡‡æ ·è·¯å¾„ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨é€€åŒ–çš„çœŸå®éŸ³é¢‘æ›¿æ¢å‚è€ƒéŸ³é¢‘ï¼Œæœ‰æ•ˆè§£å†³äº†å‚è€ƒä¿¡å·ä¸ç›®æ ‡ä¿¡å·ä¹‹é—´çš„æ—¶é—´å¯¹é½é—®é¢˜ã€‚åœ¨Opencpopæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSmoothSingeråœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†State-of-the-artæ°´å¹³ï¼Œæ˜¾è‘—å‡å°‘äº†ä¼ªå½±å¹¶æå‡äº†åˆæˆæ­Œå£°çš„è‡ªç„¶åº¦ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21478v1",
      "published_date": "2025-06-26 17:07:45 UTC",
      "updated_date": "2025-06-26 17:07:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:30.609873+00:00"
    },
    {
      "arxiv_id": "2506.21465v1",
      "title": "Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage",
      "title_zh": "ä¼˜åŒ–å››é˜¶é¾™æ ¼-åº“å¡”æ³•ï¼šä¸€ç§å…¼é¡¾é«˜æ•ˆä¸ä½å­˜å‚¨çš„åŠ¨æ€å¯å‘å¼æ–¹æ³•",
      "authors": [
        "Gavin Lee Goodship",
        "Luis Miralles-Pechuan",
        "Stephen O'Sullivan"
      ],
      "abstract": "Extended Stability Runge-Kutta (ESRK) methods are crucial for solving large-scale computational problems in science and engineering, including weather forecasting, aerodynamic analysis, and complex biological modelling. However, balancing accuracy, stability, and computational efficiency remains challenging, particularly for high-order, low-storage schemes. This study introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL) approach for automated heuristic discovery, optimising low-storage ESRK methods. Unlike traditional approaches that rely on manually designed heuristics or exhaustive numerical searches, our method leverages GA-driven mutations for search-space exploration and an RL-inspired state transition mechanism to refine heuristic selection dynamically. This enables systematic parameter reduction, preserving fourth-order accuracy while significantly improving computational efficiency.The proposed GA-RL heuristic optimisation framework is validated through rigorous testing on benchmark problems, including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes equations. The best-performing heuristic achieves a 25\\% reduction in IPOPT runtime compared to traditional ESRK optimisation processes while maintaining numerical stability and accuracy. These findings demonstrate the potential of adaptive heuristic discovery to improve resource efficiency in high-fidelity simulations and broaden the applicability of low-storage Runge-Kutta methods in real-world computational fluid dynamics, physics simulations, and other demanding fields. This work establishes a new paradigm in heuristic optimisation for numerical methods, opening pathways for further exploration using Deep RL and AutoML-based heuristic search",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Extended Stability Runge-Kutta (ESRK) æ–¹æ³•åœ¨å¹³è¡¡ç²¾åº¦ã€ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ Genetic Algorithm (GA) å’Œ Reinforcement Learning (RL) çš„æ··åˆä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä½å­˜å‚¨ ESRK æ–¹æ³•çš„è‡ªåŠ¨å¯å‘å¼å‘ç°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ GA é©±åŠ¨çš„å˜å¼‚è¿›è¡Œæœç´¢ç©ºé—´æ¢ç´¢ï¼Œå¹¶ç»“åˆå— RL å¯å‘çš„çŠ¶æ€è½¬æ¢æœºåˆ¶åŠ¨æ€ç²¾ç»†åŒ–å¯å‘å¼é€‰æ‹©ï¼Œåœ¨ç³»ç»Ÿæ€§å‡å°‘å‚æ•°çš„åŒæ—¶ä¿ç•™äº†å››é˜¶ç²¾åº¦ã€‚é€šè¿‡åœ¨ 1D/2D Brusselator ç³»ç»Ÿå’Œç¨³æ€ Navier-Stokes equations ç­‰åŸºå‡†é—®é¢˜ä¸Šçš„ä¸¥æ ¼éªŒè¯ï¼Œæ€§èƒ½æœ€ä¼˜çš„å¯å‘å¼æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿä¼˜åŒ–æµç¨‹ä½¿ IPOPT è¿è¡Œæ—¶é—´ç¼©çŸ­äº† 25%ï¼Œä¸”ä¿æŒäº†æ•°å€¼ç¨³å®šæ€§ä¸ç²¾åº¦ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è‡ªé€‚åº”å¯å‘å¼å‘ç°åœ¨æé«˜å¤§è§„æ¨¡é«˜ä¿çœŸæ¨¡æ‹Ÿèµ„æºæ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ•°å€¼æ–¹æ³•çš„å¯å‘å¼ä¼˜åŒ–å»ºç«‹äº†æ–°èŒƒå¼ï¼Œå¹¶ä¸ºæœªæ¥åˆ©ç”¨ Deep RL å’Œ AutoML è¿›è¡Œå¯å‘å¼æœç´¢å¼€è¾Ÿäº†è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21465v1",
      "published_date": "2025-06-26 16:51:22 UTC",
      "updated_date": "2025-06-26 16:51:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:34.199192+00:00"
    },
    {
      "arxiv_id": "2506.21458v1",
      "title": "Spatial Mental Modeling from Limited Views",
      "title_zh": "åŸºäºæœ‰é™è§†è§’çš„ç©ºé—´å¿ƒç†å»ºæ¨¡",
      "authors": [
        "Baiqiao Yin",
        "Qineng Wang",
        "Pingyue Zhang",
        "Jianshu Zhang",
        "Kangrui Wang",
        "Zihan Wang",
        "Jieyu Zhang",
        "Keshigeyan Chandrasegaran",
        "Han Liu",
        "Ranjay Krishna",
        "Saining Xie",
        "Manling Li",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new MindCube benchmark with 21,154 questions across 3,268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using MindCube, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for \"what-if\" movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from a synergistic approach, \"map-then-reason\", that jointly trains the model to first generate a cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) æ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·ä»æœ‰é™è§†è§’æ„å»ºç©ºé—´å¿ƒç†æ¨¡å‹ (Spatial Mental Models)ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†åŒ…å« 21,154 ä¸ªé—®é¢˜çš„ MindCube åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡è®¤çŸ¥æ˜ å°„ (Cognitive Mapping)ã€è§†è§’è½¬æ¢ (Perspective-taking) å’Œå¿ƒç†æ¨¡æ‹Ÿ (Mental Simulation) ç­‰ç»´åº¦ï¼Œè¯¥åŸºå‡†æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç†è§£ä¸å¯è§ç©ºé—´æ–¹é¢çš„å·¨å¤§ç¼ºé™·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§â€œå…ˆæ˜ å°„åæ¨ç†â€(Map-then-reason) çš„ååŒè®­ç»ƒæ–¹æ³•ï¼Œå¼•å¯¼æ¨¡å‹å…ˆç”Ÿæˆå†…éƒ¨è®¤çŸ¥åœ°å›¾å†è¿›è¡Œé€»è¾‘æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†æ¨¡å‹å‡†ç¡®ç‡ä» 37.8% æ˜¾è‘—æå‡è‡³ 60.8%ï¼Œåœ¨å¼•å…¥å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) åæ›´æ˜¯è¾¾åˆ°äº† 70.7%ã€‚è¿™é¡¹ç ”ç©¶çš„æ ¸å¿ƒè§è§£åœ¨äºï¼Œé€šè¿‡ä¸»åŠ¨æ„å»ºå’Œåˆ©ç”¨å†…éƒ¨ç»“æ„åŒ–ç©ºé—´è¡¨ç¤ºåŠå…¶æ¨ç†è¿‡ç¨‹ï¼Œèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹ä¸å¯è§ç©ºé—´çš„ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint version",
      "pdf_url": "https://arxiv.org/pdf/2506.21458v1",
      "published_date": "2025-06-26 16:38:19 UTC",
      "updated_date": "2025-06-26 16:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:44.379500+00:00"
    },
    {
      "arxiv_id": "2506.21443v1",
      "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
      "title_zh": "é¢å‘æ¬ºè¯ˆä¸æ¦‚å¿µæ¼‚ç§»æ£€æµ‹çš„é¢†åŸŸçŸ¥è¯†å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Ali Åenol",
        "Garima Agrawal",
        "Huan Liu"
      ],
      "abstract": "Detecting deceptive conversations on dynamic platforms is increasingly difficult due to evolving language patterns and Concept Drift (CD)-i.e., semantic or topical shifts that alter the context or intent of interactions over time. These shifts can obscure malicious intent or mimic normal dialogue, making accurate classification challenging. While Large Language Models (LLMs) show strong performance in natural language tasks, they often struggle with contextual ambiguity and hallucinations in risk-sensitive scenarios. To address these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework that integrates pretrained LLMs with structured, task-specific insights to perform fraud and concept drift detection. The proposed architecture consists of three main components: (1) a DK-LLM module to detect fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine whether a semantic shift has occurred; and (3) a second DK-LLM module to classify the drift as either benign or fraudulent. We first validate the value of domain knowledge using a fake review dataset and then apply our full framework to SEConvo, a multiturn dialogue dataset that includes various types of fraud and spam attacks. Results show that our system detects fake conversations with high accuracy and effectively classifies the nature of drift. Guided by structured prompts, the LLaMA-based implementation achieves 98% classification accuracy. Comparative studies against zero-shot baselines demonstrate that incorporating domain knowledge and drift awareness significantly improves performance, interpretability, and robustness in high-stakes NLP applications.",
      "tldr_zh": "é’ˆå¯¹åŠ¨æ€å¹³å°ä¸­æ¬ºè¯ˆå¯¹è¯æ£€æµ‹é¢ä¸´çš„è¯­è¨€æ¨¡å¼æ¼”å˜å’Œæ¦‚å¿µæ¼‚ç§»(Concept Drift)éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢†åŸŸçŸ¥è¯†å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹(Domain Knowledge-Enhanced LLM)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†é¢„è®­ç»ƒæ¨¡å‹ä¸ç»“æ„åŒ–çš„ç‰¹å®šä»»åŠ¡è§è§£ç›¸ç»“åˆï¼Œç”±æ£€æµ‹æ¬ºè¯ˆå¯¹è¯çš„DK-LLMæ¨¡å—ã€è¯†åˆ«è¯­ä¹‰åç§»çš„æ¼‚ç§»æ£€æµ‹å•å…ƒ(OCDD)ä»¥åŠåˆ¤æ–­æ¼‚ç§»æ€§è´¨çš„ç¬¬äºŒDK-LLMæ¨¡å—å…±åŒæ„æˆã€‚é€šè¿‡åœ¨è™šå‡è¯„è®ºæ•°æ®é›†å’Œå¤šè½®å¯¹è¯æ•°æ®é›†SEConvoä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«æ¬ºè¯ˆè¡Œä¸ºå¹¶å¯¹æ¼‚ç§»è¿›è¡Œåˆ†ç±»ã€‚åŸºäºLLaMAçš„å®ç°ç‰ˆæœ¬åœ¨ç»“æ„åŒ–æç¤ºè¯å¼•å¯¼ä¸‹è¾¾åˆ°äº†98%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚ä¸é›¶æ ·æœ¬(Zero-shot)åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥é¢†åŸŸçŸ¥è¯†å’Œæ¼‚ç§»æ„ŸçŸ¥ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨é«˜é£é™©è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ€§èƒ½ã€å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21443v1",
      "published_date": "2025-06-26 16:29:45 UTC",
      "updated_date": "2025-06-26 16:29:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:49:56.927687+00:00"
    },
    {
      "arxiv_id": "2506.21408v1",
      "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference",
      "title_zh": "åŸºäºéšæœºå˜åˆ†å­ç©ºé—´æ¨æ–­çš„å¤§è¯­è¨€æ¨¡å‹å¯æ‰©å±•è´å¶æ–¯ä½ç§©è‡ªé€‚åº”",
      "authors": [
        "Colin Samplawski",
        "Adam D. Cobb",
        "Manoj Acharya",
        "Ramneet Kaur",
        "Susmit Jha"
      ],
      "abstract": "Despite their widespread use, large language models (LLMs) are known to hallucinate incorrect information and be poorly calibrated. This makes the uncertainty quantification of these models of critical importance, especially in high-stakes domains, such as autonomy and healthcare. Prior work has made Bayesian deep learning-based approaches to this problem more tractable by performing inference over the low-rank adaptation (LoRA) parameters of a fine-tuned model. While effective, these approaches struggle to scale to larger LLMs due to requiring further additional parameters compared to LoRA. In this work we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By repurposing the LoRA parameters as projection matrices, we are able to map samples from this subspace into the full weight space of the LLM. This allows us to learn all the parameters of our approach using stochastic variational inference. Despite the low dimensionality of our subspace, we are able to achieve competitive performance with state-of-the-art approaches while only requiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to scale up to the largest Bayesian LLM to date, with four times as a many base parameters as prior work.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å­˜åœ¨çš„å¹»è§‰å’Œæ ¡å‡†å·®é—®é¢˜ï¼Œæ¢è®¨äº†ä¸ç¡®å®šæ€§é‡åŒ– (Uncertainty Quantification) åœ¨é«˜é£é™©é¢†åŸŸçš„é‡è¦æ€§ã€‚ç°æœ‰çš„è´å¶æ–¯æ–¹æ³•è™½ç„¶å°è¯•å¯¹ LoRA å‚æ•°è¿›è¡Œæ¨ç†ï¼Œä½†å› å‚æ•°å¼€é”€è¿‡å¤§è€Œéš¾ä»¥æ‰©å±•åˆ°è¶…å¤§è§„æ¨¡æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† ScalaBLï¼Œä¸€ç§é€šè¿‡éšæœºå˜åˆ†å­ç©ºé—´æ¨ç† (Stochastic Variational Subspace Inference) å®ç°çš„å¯æ‰©å±•æ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨ $r$ ç»´å­ç©ºé—´ä¸­æ‰§è¡Œè´å¶æ–¯æ¨ç†ï¼Œå¹¶å°† LoRA å‚æ•°å¤ç”¨ä¸ºæŠ•å½±çŸ©é˜µï¼Œå°†å­ç©ºé—´æ ·æœ¬æ˜ å°„å› LLM çš„å®Œæ•´æƒé‡ç©ºé—´ã€‚é€šè¿‡ç»“åˆéšæœºå˜åˆ†æ¨ç† (Stochastic Variational Inference)ï¼ŒScalaBL åœ¨ä»…éœ€çº¦ 1000 ä¸ªé¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸æœ€å…ˆè¿› (State-of-the-Art) æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯æˆåŠŸæ”¯æ’‘äº†ç›®å‰å‚æ•°é‡æœ€å¤§çš„è´å¶æ–¯ LLMï¼Œå…¶åŸºç¡€æ¨¡å‹è§„æ¨¡è¾¾åˆ°äº†æ­¤å‰åŒç±»ç ”ç©¶çš„å››å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at UAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21408v1",
      "published_date": "2025-06-26 15:54:45 UTC",
      "updated_date": "2025-06-26 15:54:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:02.665322+00:00"
    },
    {
      "arxiv_id": "2506.21393v1",
      "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding",
      "title_zh": "TableMoEï¼šé¢å‘å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä¸­ç»“æ„åŒ–ä¸“å®¶æ¨ç†çš„ç¥ç»ç¬¦å·è·¯ç”±",
      "authors": [
        "Junwen Zhang",
        "Pu Chen",
        "Yin Zhang"
      ],
      "abstract": "Multimodal understanding of tables in real-world contexts is challenging due to the complexity of structure, symbolic density, and visual degradation (blur, skew, watermarking, incomplete structures or fonts, multi-span or hierarchically nested layouts). Existing multimodal large language models (MLLMs) struggle with such WildStruct conditions, resulting in limited performance and poor generalization. To address these challenges, we propose TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture specifically designed for robust, structured reasoning over multimodal table data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which predicts latent semantic token roles (e.g., header, data cell, axis, formula) and dynamically routes table elements to specialized experts (Table-to-HTML, Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed by symbolic reasoning graphs. To facilitate effective alignment-driven pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of 1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and industry, utilized exclusively for model pretraining. For evaluation, we curate and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA, WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models under real-world multimodal degradation and structural complexity. Experimental results demonstrate that TableMoE significantly surpasses existing state-of-the-art models. Extensive ablation studies validate each core component, emphasizing the critical role of Neuro-Symbolic Routing and structured expert alignment. Through qualitative analyses, we further showcase TableMoE's interpretability and enhanced robustness, underscoring the effectiveness of integrating neuro-symbolic reasoning for multimodal table understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TableMoEï¼Œä¸€ç§ä¸“ä¸ºé²æ£’çš„ç»“æ„åŒ–è¡¨æ ¼æ¨ç†è®¾è®¡çš„ç¥ç»ç¬¦å·æ··åˆè¿æ¥å™¨ä¸“å®¶æ¶æ„(Neuro-Symbolic Mixture-of-Connector-Experts, MoCE)ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¤„ç†å¤æ‚ç»“æ„å’Œè§†è§‰é€€åŒ–çš„WildStructè¡¨æ ¼æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ¶æ„å¼•å…¥äº†åˆ›æ–°çš„ç¥ç»ç¬¦å·è·¯ç”±(Neuro-Symbolic Routing)æœºåˆ¶ï¼Œèƒ½å¤Ÿé¢„æµ‹æ½œåœ¨è¯­ä¹‰tokençš„è§’è‰²ï¼Œå¹¶é€šè¿‡åŸºäºç¬¦å·æ¨ç†å›¾çš„ç½®ä¿¡åº¦æ„ŸçŸ¥é—¨æ§ç­–ç•¥å°†è¡¨æ ¼å…ƒç´ åŠ¨æ€è·¯ç”±è‡³Table-to-HTMLã€Table-to-JSONå’ŒTable-to-Codeç­‰ä¸“ä¸šä¸“å®¶ã€‚ä¸ºäº†æ”¯æŒæœ‰æ•ˆå¯¹é½ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«120ä¸‡ç»„æ•°æ®çš„TableMoE-Alignå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åŒæ­¥å‘å¸ƒäº†WMMFinQAã€WMMTatQAç­‰å››ä¸ªé’ˆå¯¹çœŸå®åœºæ™¯ç»“æ„å¤æ‚æ€§çš„æŒ‘æˆ˜æ€§å‹åŠ›æµ‹è¯•åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTableMoEåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆéªŒè¯äº†å°†ç¥ç»ç¬¦å·æ¨ç†é›†æˆåˆ°å¤šæ¨¡æ€è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤æ‚è¡¨æ ¼æ•°æ®æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "43 pages and 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.21393v1",
      "published_date": "2025-06-26 15:41:34 UTC",
      "updated_date": "2025-06-26 15:41:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:03.039850+00:00"
    },
    {
      "arxiv_id": "2506.21384v1",
      "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation",
      "title_zh": "åˆ©ç”¨å¤§æ¨¡å‹è¾…åŠ©æŸ¥è¯¢ç†è§£å®ç°å®æ—¶æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Guanting Dong",
        "Xiaoxi Li",
        "Yuyao Zhang",
        "Mengjie Deng"
      ],
      "abstract": "Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. While RAG enhances large language models (LLMs) with external knowledge, current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data. This paper introduces Omni-RAG, a novel framework designed to improve the robustness and effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries; (2) Intent-Aware Knowledge Retrieval, which performs retrieval for each sub-query from a corpus (i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking and Generation, where a reranker (i.e., BGE) refines document selection before a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œä¸­å®æ—¶æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿåœ¨å¤„ç†å«å™ªå£°ã€æ­§ä¹‰åŠå¤šæ„å›¾æŸ¥è¯¢æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† Omni-RAG æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (LLM) è¾…åŠ©çš„æŸ¥è¯¢ç†è§£æŠ€æœ¯ï¼Œåˆ©ç”¨ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—æå‡äº†ç³»ç»Ÿåœ¨å¼€æ”¾åŸŸç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚å…¶ä¸­ Deep Query Understanding and Decomposition æ¨¡å—è´Ÿè´£å¯¹æŸ¥è¯¢è¿›è¡Œå»å™ªå¹¶å°†å¤æ‚æ„å›¾æ‹†è§£ä¸ºç»“æ„åŒ–å­æŸ¥è¯¢ï¼ŒIntent-Aware Knowledge Retrieval æ¨¡å—åˆ™é’ˆå¯¹å­æŸ¥è¯¢ä» FineWeb è¯­æ–™åº“ä¸­è¿›è¡Œç²¾å‡†æ£€ç´¢ã€‚åœ¨æœ€åçš„ Reranking and Generation é˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡ BGE æ¨¡å‹è¿›è¡Œé‡æ’åºï¼Œå¹¶ç»“åˆ Falcon-10B æ¨¡å‹çš„é“¾å¼æ€ç»´ (Chain-of-Thought) æç¤ºç”Ÿæˆæœ€ç»ˆå›å¤ã€‚Omni-RAG æ—¨åœ¨ç¼©å°å½“å‰ RAG æŠ€æœ¯ä¸ SIGIR 2025 LiveRAG Challenge ç­‰çœŸå®åœºæ™¯éœ€æ±‚ä¹‹é—´çš„å·®è·ï¼Œä¸ºå¤„ç†å¤æ‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”¨æˆ·è¾“å…¥æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)",
      "pdf_url": "https://arxiv.org/pdf/2506.21384v1",
      "published_date": "2025-06-26 15:35:12 UTC",
      "updated_date": "2025-06-26 15:35:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:05.133544+00:00"
    },
    {
      "arxiv_id": "2506.21382v1",
      "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection",
      "title_zh": "é¢å‘åŠ å¯†è´§å¸äº¤æ˜“æ¬ºè¯ˆæ£€æµ‹çš„æ—¶åºæ„ŸçŸ¥å›¾æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Zhi Zheng",
        "Bochuan Zhou",
        "Yuping Song"
      ],
      "abstract": "Cryptocurrency transaction fraud detection faces the dual challenges of increasingly complex transaction patterns and severe class imbalance. Traditional methods rely on manual feature engineering and struggle to capture temporal and structural dependencies in transaction networks. This paper proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that enhances detection performance through three modules: (1) designing an advanced temporal embedding module that fuses multi-scale time difference features with periodic position encoding; (2) constructing a temporal-aware triple attention mechanism that jointly optimizes structural, temporal, and global context attention; (3) employing weighted BCE loss to address class imbalance. Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT achieves an AUC of 0.9130, representing a 9.2% improvement over the best traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This method not only validates the enhancement effect of temporal awareness and triple attention mechanisms on graph neural networks, but also provides financial institutions with more reliable fraud detection tools, with its design principles generalizable to other temporal graph anomaly detection tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ å¯†è´§å¸äº¤æ˜“æ¬ºè¯ˆæ£€æµ‹ä¸­æ—¥ç›Šå¤æ‚çš„äº¤æ˜“æ¨¡å¼å’Œä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡(Class Imbalance)æŒ‘æˆ˜ï¼Œæå‡ºäº†å¢å¼ºå‹æ—¶é—´æ„ŸçŸ¥å›¾æ³¨æ„åŠ›ç½‘ç»œ(ATGAT)ã€‚ATGATé€šè¿‡å…ˆè¿›çš„æ—¶é—´åµŒå…¥æ¨¡å—èåˆå¤šå°ºåº¦æ—¶é—´å·®ç‰¹å¾ä¸å‘¨æœŸæ€§ä½ç½®ç¼–ç ï¼Œå¹¶æ„å»ºäº†æ—¶é—´æ„ŸçŸ¥ä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶(Triple Attention Mechanism)ä»¥å…±åŒä¼˜åŒ–ç»“æ„ã€æ—¶é—´åŠå…¨å±€ä¸Šä¸‹æ–‡æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨åŠ æƒBCEæŸå¤±(Weighted BCE Loss)å¤„ç†æ•°æ®åˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚åœ¨Elliptic++æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒATGATè¾¾åˆ°äº†0.9130çš„AUCï¼Œç›¸è¾ƒäºXGBoostã€GCNå’Œæ ‡å‡†GATåˆ†åˆ«æå‡äº†9.2%ã€12.0%å’Œ10.0%ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†æ—¶é—´æ„ŸçŸ¥å’Œä¸‰é‡æ³¨æ„åŠ›æœºåˆ¶å¯¹å›¾ç¥ç»ç½‘ç»œçš„å¢å¼ºæ•ˆæœï¼Œè¿˜ä¸ºé‡‘èæ¬ºè¯ˆæ£€æµ‹æä¾›äº†é«˜å¯é æ€§çš„å·¥å…·ï¼Œå…¶è®¾è®¡åŸåˆ™å¯æ¨å¹¿è‡³å…¶ä»–æ—¶æ€å›¾å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21382v1",
      "published_date": "2025-06-26 15:34:06 UTC",
      "updated_date": "2025-06-26 15:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:07.640608+00:00"
    },
    {
      "arxiv_id": "2506.21374v2",
      "title": "Pay Attention to Small Weights",
      "title_zh": "å…³æ³¨å°æƒé‡",
      "authors": [
        "Chao Zhou",
        "Tom Jacobs",
        "Advait Gadhikar",
        "Rebekka Burkholz"
      ],
      "abstract": "Finetuning large pretrained neural networks is known to be resource-intensive, both in terms of memory and computational cost. To mitigate this, a common approach is to restrict training to a subset of the model parameters. By analyzing the relationship between gradients and weights during finetuning, we observe a notable pattern: large gradients are often associated with small-magnitude weights. This correlation is more pronounced in finetuning settings than in training from scratch. Motivated by this observation, we propose NANOADAM, which dynamically updates only the small-magnitude weights during finetuning and offers several practical advantages: first, this criterion is gradient-free -- the parameter subset can be determined without gradient computation; second, it preserves large-magnitude weights, which are likely to encode critical features learned during pretraining, thereby reducing the risk of catastrophic forgetting; thirdly, it permits the use of larger learning rates and consistently leads to better generalization performance in experiments. We demonstrate this for both NLP and vision tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹é¢„è®­ç»ƒç¥ç»ç½‘ç»œåœ¨ Finetuning è¿‡ç¨‹ä¸­èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œé€šè¿‡åˆ†ææ¢¯åº¦ä¸æƒé‡çš„å…³ç³»å‘ç°å¤§æ¢¯åº¦å¾€å¾€ä¸å°é‡çº§æƒé‡é«˜åº¦ç›¸å…³ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œä½œè€…æå‡ºäº† NANOADAM ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€åœ°ä»…æ›´æ–°æ¨¡å‹ä¸­çš„å°é‡çº§æƒé‡ã€‚è¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„å®è·µä¼˜åŠ¿ï¼šå…¶å‚æ•°å­é›†çš„ç¡®å®šæ˜¯ Gradient-free çš„ï¼Œä¸”é€šè¿‡ä¿ç•™ç¼–ç é¢„è®­ç»ƒå…³é”®ç‰¹å¾çš„å¤§é‡çº§æƒé‡ï¼Œæœ‰æ•ˆé™ä½äº† Catastrophic Forgetting çš„é£é™©ã€‚æ­¤å¤–ï¼ŒNANOADAM å…è®¸ä½¿ç”¨æ›´å¤§çš„ Learning Rateï¼Œåœ¨ NLP å’Œè§†è§‰ä»»åŠ¡çš„å®éªŒä¸­å‡å±•ç°å‡ºæ›´ä¼˜çš„ Generalization æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21374v2",
      "published_date": "2025-06-26 15:22:55 UTC",
      "updated_date": "2025-10-22 15:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:11.784580+00:00"
    },
    {
      "arxiv_id": "2506.21368v1",
      "title": "Real-time and personalized product recommendations for large e-commerce platforms",
      "title_zh": "é¢å‘å¤§å‹ç”µå•†å¹³å°çš„å®æ—¶ä¸ªæ€§åŒ–å•†å“æ¨è",
      "authors": [
        "Matteo Tolloso",
        "Davide Bacciu",
        "Shahab Mokarizadeh",
        "Marco Varesi"
      ],
      "abstract": "We present a methodology to provide real-time and personalized product recommendations for large e-commerce platforms, specifically focusing on fashion retail. Our approach aims to achieve accurate and scalable recommendations with minimal response times, ensuring user satisfaction, leveraging Graph Neural Networks and parsimonious learning methodologies. Extensive experimentation with datasets from one of the largest e-commerce platforms demonstrates the effectiveness of our approach in forecasting purchase sequences and handling multi-interaction scenarios, achieving efficient personalized recommendations under real-world constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸ºå¤§å‹ç”µå­å•†åŠ¡å¹³å°ï¼ˆç‰¹åˆ«æ˜¯æ—¶å°šé›¶å”®é¢†åŸŸï¼‰æä¾›å®æ—¶ä¸ªæ€§åŒ–äº§å“æ¨èçš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯åœ¨ä¿è¯ç”¨æˆ·æ»¡æ„åº¦çš„å‰æä¸‹ï¼Œå®ç°å‡†ç¡®ä¸”å¯æ‰©å±•çš„æ¨èï¼Œå¹¶æœ€å¤§é™åº¦åœ°ç¼©çŸ­å“åº”æ—¶é—´ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº† Graph Neural Networks (GNN) å’Œ parsimonious learning å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„å¤æ‚æ€§ä¸æ€§èƒ½ã€‚é€šè¿‡åœ¨å¤§å‹ç”µå•†å¹³å°æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é¢„æµ‹è´­ä¹°åºåˆ—å’Œå¤„ç†å¤šäº¤äº’åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„çº¦æŸæ¡ä»¶ä¸‹ï¼Œå®ç°é«˜æ•ˆä¸”ç²¾å‡†çš„ä¸ªæ€§åŒ–æ¨èã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "This paper has been accepted for publication at the International Conference on Artificial Neural Networks (ICANN) 2025. The final authenticated version will be available for purchase through the publisher's website. The conference proceedings will be published by Springer in the Lecture Notes in Computer Science (LNCS) series",
      "pdf_url": "https://arxiv.org/pdf/2506.21368v1",
      "published_date": "2025-06-26 15:16:44 UTC",
      "updated_date": "2025-06-26 15:16:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:11.663381+00:00"
    },
    {
      "arxiv_id": "2506.21367v1",
      "title": "rQdia: Regularizing Q-Value Distributions With Image Augmentation",
      "title_zh": "rQdiaï¼šåŸºäºå›¾åƒå¢å¼ºçš„Qå€¼åˆ†å¸ƒæ­£åˆ™åŒ–",
      "authors": [
        "Sam Lerman",
        "Jing Bi"
      ],
      "abstract": "rQdia regularizes Q-value distributions with augmented images in pixel-based deep reinforcement learning. With a simple auxiliary loss, that equalizes these distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks respectively in the MuJoCo Continuous Control Suite from pixels, and Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured in both sample efficiency and longer-term training. Moreover, the addition of rQdia finally propels model-free continuous control from pixels over the state encoding baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†rQdiaï¼Œè¿™æ˜¯ä¸€ç§åœ¨åŸºäºåƒç´ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (Pixel-based Deep Reinforcement Learning)ä¸­ï¼Œåˆ©ç”¨å›¾åƒå¢å¼º(Image Augmentation)æ¥æ­£åˆ™åŒ–Qå€¼åˆ†å¸ƒ(Q-value Distributions)çš„æ–°æ–¹æ³•ã€‚rQdiaå¼•å…¥äº†ä¸€ç§ç®€å•çš„è¾…åŠ©æŸå¤±(Auxiliary Loss)ï¼Œé€šè¿‡å‡æ–¹è¯¯å·®(MSE)ä½¿ä¸åŒå¢å¼ºå›¾åƒç”Ÿæˆçš„åˆ†å¸ƒä¿æŒä¸€è‡´ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MuJoCoè¿ç»­æ§åˆ¶å¥—ä»¶çš„12é¡¹ä»»åŠ¡ä¸­åˆ†åˆ«æ”¹è¿›äº†DrQå’ŒSACåœ¨9é¡¹å’Œ10é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶åœ¨26ä¸ªAtariæ¸¸æˆç¯å¢ƒä¸­çš„18ä¸ªä¸Šæå‡äº†Data-Efficient Rainbowçš„æ€§èƒ½ã€‚è¿™ç§å¢ç›Šä¸ä»…ä½“ç°åœ¨æ ·æœ¬æ•ˆç‡(Sample Efficiency)ä¸Šï¼Œä¹Ÿä½“ç°åœ¨é•¿æœŸè®­ç»ƒçš„æ•ˆæœä¸­ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒrQdiaçš„åŠ å…¥ä½¿å¾—åŸºäºåƒç´ çš„æ— æ¨¡å‹è¿ç»­æ§åˆ¶(Model-free Continuous Control)æ€§èƒ½é¦–æ¬¡è¶…è¶Šäº†åŸºäºçŠ¶æ€ç¼–ç (State Encoding)çš„åŸºå‡†çº¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21367v1",
      "published_date": "2025-06-26 15:16:35 UTC",
      "updated_date": "2025-06-26 15:16:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:18.504190+00:00"
    },
    {
      "arxiv_id": "2506.21364v1",
      "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection",
      "title_zh": "CA-I2Pï¼šå…·æœ‰å…¨å±€æœ€ä¼˜é€‰æ‹©çš„é€šé“è‡ªé€‚åº”é…å‡†ç½‘ç»œ",
      "authors": [
        "Zhixin Cheng",
        "Jiacheng Deng",
        "Xinjun Li",
        "Xiaotian Yin",
        "Bohao Liao",
        "Baoqun Yin",
        "Wenfei Yang",
        "Tianzhu Zhang"
      ],
      "abstract": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting image and point cloud features for patch-level matching and refining dense pixel-to-point correspondences. However, differences in feature channel attention between images and point clouds may lead to degraded matching results, ultimately impairing registration accuracy. Furthermore, similar structures in the scene could lead to redundant correspondences in cross-modal matching. To address these issues, we propose Channel Adaptive Adjustment Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances intra-modal features and suppresses cross-modal sensitivity, while GOS replaces local selection with global optimization. Experiments on RGB-D Scenes V2 and 7-Scenes demonstrate the superiority of our method, achieving state-of-the-art performance in image-to-point cloud registration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CA-I2Pï¼Œä¸€ç§å…·æœ‰å…¨å±€æœ€ä¼˜é€‰æ‹©åŠŸèƒ½çš„é€šé“è‡ªé€‚åº”é…å‡†ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å›¾åƒä¸ç‚¹äº‘é…å‡†ä¸­ç”±äºç‰¹å¾é€šé“æ³¨æ„åŠ›å·®å¼‚å¯¼è‡´çš„åŒ¹é…æ€§èƒ½ä¸‹é™ä»¥åŠåœºæ™¯ç›¸ä¼¼ç»“æ„å¼•èµ·çš„å†—ä½™å¯¹åº”é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†é€šé“è‡ªé€‚åº”è°ƒæ•´æ¨¡å—(Channel Adaptive Adjustment Module, CAA)ä»¥å¢å¼ºæ¨¡æ€å†…ç‰¹å¾å¹¶æŠ‘åˆ¶è·¨æ¨¡æ€æ•æ„Ÿæ€§ï¼ŒåŒæ—¶å¼•å…¥å…¨å±€æœ€ä¼˜é€‰æ‹©æ¨¡å—(Global Optimal Selection Module, GOS)é‡‡ç”¨å…¨å±€ä¼˜åŒ–ç­–ç•¥æ›¿ä»£å±€éƒ¨é€‰æ‹©ã€‚é€šè¿‡åœ¨RGB-D Scenes V2å’Œ7-Scenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ°ç‚¹äº‘é…å‡†(image-to-point cloud registration)ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯çš„ç»“æœï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„(state-of-the-art)æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 accepted",
      "pdf_url": "https://arxiv.org/pdf/2506.21364v1",
      "published_date": "2025-06-26 15:15:18 UTC",
      "updated_date": "2025-06-26 15:15:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:19.228904+00:00"
    },
    {
      "arxiv_id": "2506.21333v2",
      "title": "A Systematic Review of Human-AI Co-Creativity",
      "title_zh": "äººæœºååŒåˆ›ä½œç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Saloni Singh",
        "Koen Hindriks",
        "Dirk Heylen",
        "Kim Baraka"
      ],
      "abstract": "The co creativity community is making significant progress in developing more sophisticated and tailored systems to support and enhance human creativity. Design considerations from prior work can serve as a valuable and efficient foundation for future systems. To support this effort, we conducted a systematic literature review of 62 papers on co-creative systems. These papers cover a diverse range of applications, including visual arts, design, and writing, where the AI acts not just as a tool but as an active collaborator in the creative process. From this review, we identified several key dimensions relevant to system design: phase of the creative process, creative task, proactive behavior of the system, user control, system embodiment, and AI model type. Our findings suggest that systems offering high user control lead to greater satisfaction, trust, and a stronger sense of ownership over creative outcomes. Furthermore, proactive systems, when adaptive and context sensitive, can enhance collaboration. We also extracted 24 design considerations, highlighting the value of encouraging users to externalize their thoughts and of increasing the system's social presence and transparency to foster trust. Despite recent advancements, important gaps remain, such as limited support for early creative phases like problem clarification, and challenges related to user adaptation to AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹62ç¯‡å…³äºHuman-AI Co-Creativityç³»ç»Ÿçš„è®ºæ–‡è¿›è¡Œäº†ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼Œæ¶µç›–äº†è§†è§‰è‰ºæœ¯ã€è®¾è®¡å’Œå†™ä½œç­‰å¤šç§åº”ç”¨é¢†åŸŸï¼Œå…¶ä¸­AIè¢«è§†ä¸ºåˆ›ä½œè¿‡ç¨‹ä¸­çš„ä¸»åŠ¨åˆä½œä¼™ä¼´è€Œéå•çº¯å·¥å…·ã€‚ç ”ç©¶è¯†åˆ«äº†ç³»ç»Ÿè®¾è®¡çš„å…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬åˆ›ä½œé˜¶æ®µ(creative process)ã€ä»»åŠ¡ç±»å‹(creative task)ã€ä¸»åŠ¨è¡Œä¸º(proactive behavior)ã€ç”¨æˆ·æ§åˆ¶(user control)ã€ç³»ç»Ÿå…·èº«åŒ–(system embodiment)ä»¥åŠAIæ¨¡å‹ç±»å‹(AI model type)ã€‚ç»“æœè¡¨æ˜ï¼Œé«˜ç¨‹åº¦çš„ç”¨æˆ·æ§åˆ¶èƒ½æ˜¾è‘—æå‡ç”¨æˆ·æ»¡æ„åº¦ã€ä¿¡ä»»æ„ŸåŠå¯¹ä½œå“çš„æ‰€æœ‰æƒæ„Ÿï¼Œè€Œå…·å¤‡è‡ªé€‚åº”æ€§çš„ä¸»åŠ¨å‹ç³»ç»Ÿåˆ™èƒ½å¢å¼ºåä½œã€‚é€šè¿‡æå–24é¡¹è®¾è®¡è€ƒé‡ï¼Œç ”ç©¶å¼ºè°ƒäº†é¼“åŠ±ç”¨æˆ·æƒ³æ³•å¤–æ˜¾åŒ–(externalize)ä»¥åŠé€šè¿‡æå‡ç³»ç»Ÿçš„ç¤¾ä¼šå­˜åœ¨æ„Ÿ(social presence)å’Œé€æ˜åº¦æ¥åŸ¹å…»ä¿¡ä»»çš„é‡è¦æ€§ã€‚å°½ç®¡æŠ€æœ¯åœ¨è¿›æ­¥ï¼Œä½†åœ¨æ”¯æŒé—®é¢˜æ¾„æ¸…(problem clarification)ç­‰æ—©æœŸåˆ›ä½œé˜¶æ®µä»¥åŠç”¨æˆ·å¯¹AIç³»ç»Ÿçš„é€‚åº”æ€§(user adaptation)æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾çš„ç ”ç©¶ç©ºç™½ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21333v2",
      "published_date": "2025-06-26 14:44:52 UTC",
      "updated_date": "2025-06-27 09:31:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:23.298649+00:00"
    },
    {
      "arxiv_id": "2506.21330v1",
      "title": "Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models",
      "title_zh": "åŸºäºåˆ†å±‚è¾“å…¥ä¾èµ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ‰‹æœ¯å…¨é˜¶æ®µè¯†åˆ«",
      "authors": [
        "Haoyang Wu",
        "Tsun-Hsuan Wang",
        "Mathias Lechner",
        "Ramin Hasani",
        "Jennifer A. Eckhoff",
        "Paul Pak",
        "Ozanan R. Meireles",
        "Guy Rosman",
        "Yutong Ban",
        "Daniela Rus"
      ],
      "abstract": "Surgical workflow analysis is essential in robot-assisted surgeries, yet the long duration of such procedures poses significant challenges for comprehensive video analysis. Recent approaches have predominantly relied on transformer models; however, their quadratic attention mechanism restricts efficient processing of lengthy surgical videos. In this paper, we propose a novel hierarchical input-dependent state space model that leverages the linear scaling property of state space models to enable decision making on full-length videos while capturing both local and global dynamics. Our framework incorporates a temporally consistent visual feature extractor, which appends a state space model head to a visual feature extractor to propagate temporal information. The proposed model consists of two key modules: a local-aggregation state space model block that effectively captures intricate local dynamics, and a global-relation state space model block that models temporal dependencies across the entire video. The model is trained using a hybrid discrete-continuous supervision strategy, where both signals of discrete phase labels and continuous phase progresses are propagated through the network. Experiments have shown that our method outperforms the current state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available after paper acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–è¾“å…¥ä¾èµ–çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ (Hierarchical Input Dependent State Space Models)ï¼Œæ—¨åœ¨è§£å†³ Transformer æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶æ‰‹æœ¯è§†é¢‘åˆ†ææ—¶å› æ³¨æ„åŠ›æœºåˆ¶äºŒæ¬¡æ–¹å¤æ‚åº¦å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM) çš„çº¿æ€§ç¼©æ”¾ç‰¹æ€§ï¼Œåœ¨å¤„ç†å…¨é•¿è§†é¢‘çš„åŒæ—¶æ•æ‰å±€éƒ¨ä¸å…¨å±€åŠ¨æ€ã€‚å…¶æ ¸å¿ƒåŒ…å«å±€éƒ¨èšåˆ SSM æ¨¡å—å’Œå…¨å±€å…³ç³» SSM æ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£æ•æ‰ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾å’Œè·¨è§†é¢‘çš„æ—¶é—´ä¾èµ–ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨æ··åˆç¦»æ•£-è¿ç»­ç›‘ç£ç­–ç•¥ (hybrid discrete-continuous supervision)ï¼Œé€šè¿‡æ‰‹æœ¯é˜¶æ®µæ ‡ç­¾å’Œé˜¶æ®µè¿›åº¦ä¿¡å·çš„å…±åŒä¼ æ’­æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Cholec80ã€MICCAI2016 å’Œ Heichole æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡äº† 2.8%ã€4.3% å’Œ 12.9%ï¼Œä¸ºé«˜æ•ˆçš„æ‰‹æœ¯æµç¨‹åˆ†ææä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21330v1",
      "published_date": "2025-06-26 14:43:57 UTC",
      "updated_date": "2025-06-26 14:43:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:36.952936+00:00"
    },
    {
      "arxiv_id": "2506.21329v4",
      "title": "Active Inference AI Systems for Scientific Discovery",
      "title_zh": "é¢å‘ç§‘å­¦å‘ç°çš„ä¸»åŠ¨æ¨ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Karthik Duraisamy"
      ],
      "abstract": "The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation -- exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns -- and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles -- rather than a monolithic recipe -- are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system's ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡Active Inference AIç³»ç»Ÿæ¨åŠ¨ç§‘å­¦å‘ç°ï¼Œæ—¨åœ¨å¼¥åˆå½“å‰ç³»ç»Ÿåœ¨æŠ½è±¡ã€æ¨ç†å’Œç»éªŒè½åœ°ï¼ˆempirical groundingï¼‰æ–¹é¢çš„ä¸‰å¤§æ ¸å¿ƒé¸¿æ²Ÿã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§äº’è¡¥çš„è®¤çŸ¥æ¨¡å¼ï¼šä½œä¸ºæ…¢é€Ÿè¿­ä»£å‡è®¾ç”Ÿæˆçš„æ€ç»´ï¼ˆthinkingï¼‰ä¸ä½œä¸ºå¿«é€Ÿç¡®å®šæ€§éªŒè¯çš„æ¨ç†ï¼ˆreasoningï¼‰ï¼Œå¹¶å¼ºè°ƒæ„å»ºå¯å®ç°å› æœå½’å› ï¼ˆcausal attributionï¼‰çš„å¯æ“çºµæ¨¡å‹ã€‚ä½œè€…ä¸ºæ­¤æå‡ºäº†ä¸€ç³»åˆ—è®¾è®¡åŸåˆ™ï¼ŒåŒ…æ‹¬ç”¨äºå†…éƒ¨æ¨¡æ‹Ÿçš„causal, multimodal modelsã€èƒ½å¤ŸåŒºåˆ†å‡è®¾ä¸æ—¢å®šäº‹å®çš„scientific memoryï¼Œä»¥åŠä¸å®éªŒè€¦åˆçš„æ­£å¼éªŒè¯è·¯å¾„ã€‚ç ”ç©¶ç‰¹åˆ«æŒ‡å‡ºï¼Œç”±äºæ¨¡æ‹Ÿä¸å®éªŒåé¦ˆä¸­å›ºæœ‰çš„æ­§ä¹‰æ€§ï¼Œäººç±»åˆ¤æ–­åº”ä½œä¸ºç³»ç»Ÿçš„æ°¸ä¹…æ¶æ„ç»„æˆéƒ¨åˆ†è€Œéä¸´æ—¶è¾…åŠ©ã€‚è¯„ä»·è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ ‡å‡†åœ¨äºå…¶è¯†åˆ«æ–°é¢–ç°è±¡ã€æå‡ºå¯è¯ä¼ªå‡è®¾ï¼ˆfalsifiable hypothesesï¼‰å¹¶å¼•å¯¼å®éªŒç¨‹åºèµ°å‘å®è´¨æ€§å‘ç°çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21329v4",
      "published_date": "2025-06-26 14:43:04 UTC",
      "updated_date": "2025-12-12 22:44:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:38.208014+00:00"
    },
    {
      "arxiv_id": "2506.21310v1",
      "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems",
      "title_zh": "IXAIIï¼šé¢å‘å†³ç­–æ”¯æŒç³»ç»Ÿçš„äº¤äº’å¼å¯è§£é‡Šäººå·¥æ™ºèƒ½ç•Œé¢",
      "authors": [
        "Pauline Speckmann",
        "Mario Nadj",
        "Christian Janiesch"
      ],
      "abstract": "Although several post-hoc methods for explainable AI have been developed, most are static and neglect the user perspective, limiting their effectiveness for the target audience. In response, we developed the interactive explainable intelligent system called IXAII that offers explanations from four explainable AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored views for five user groups and gives users agency over the explanations' content and their format. We evaluated IXAII through interviews with experts and lay users. Our results indicate that IXAII, which provides different explanations with multiple visualization options, is perceived as helpful to increase transparency. By bridging the gaps between explainable AI methods, interactivity, and practical implementation, we provide a novel perspective on AI explanation practices and human-AI interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º IXAII çš„äº¤äº’å¼å¯è§£é‡Šäººå·¥æ™ºèƒ½ç•Œé¢ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ post-hoc è§£é‡Šæ–¹æ³•å› å½¢å¼é™æ€ä¸”å¿½è§†ç”¨æˆ·è§†è§’è€Œå¯¼è‡´çš„æœ‰æ•ˆæ€§å—é™é—®é¢˜ã€‚ç³»ç»Ÿé›†æˆäº† LIMEã€SHAPã€Anchors å’Œ DiCE å››ç§ä¸»æµçš„ explainable AI æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹äº”ç±»ç‰¹å®šç”¨æˆ·ç¾¤ä½“è®¾è®¡äº†å®šåˆ¶åŒ–è§†å›¾ï¼Œèµ‹äºˆç”¨æˆ·å¯¹è§£é‡Šå†…å®¹åŠå±•ç¤ºæ ¼å¼çš„è‡ªä¸»æŒæ§æƒã€‚é€šè¿‡ä¸ä¸“å®¶å’Œæ™®é€šç”¨æˆ·çš„è®¿è°ˆè¯„ä¼°ï¼Œç ”ç©¶å‘ç° IXAII æä¾›çš„å¤šç»´åº¦è§£é‡Šä¸å¯è§†åŒ–é€‰é¡¹èƒ½æœ‰æ•ˆå¢å¼ºå†³ç­–æ”¯æŒç³»ç»Ÿçš„é€æ˜åº¦ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æ•´åˆè§£é‡Šæ–¹æ³•ã€äº¤äº’æ€§ä¸å·¥ç¨‹å®ç°ï¼Œä¸ºäººå·¥æ™ºèƒ½è§£é‡Šå®è·µå’Œ human-AI interaction é¢†åŸŸæä¾›äº†åˆ›æ–°çš„ç ”ç©¶è§†è§’ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track",
      "pdf_url": "https://arxiv.org/pdf/2506.21310v1",
      "published_date": "2025-06-26 14:28:13 UTC",
      "updated_date": "2025-06-26 14:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:42.666284+00:00"
    },
    {
      "arxiv_id": "2507.02933v1",
      "title": "Experiment on creating a neural network with weights determined by the potential of a simulated electrostatic field",
      "title_zh": "# ç¿»è¯‘ç»“æœ ğŸ“„\n\n---\n\nåŸºäºæ¨¡æ‹Ÿé™ç”µåœºç”µåŠ¿ç¡®å®šæƒé‡çš„ç¥ç»ç½‘ç»œæ„å»ºå®éªŒ\n\n---\n\nè¿™ä¸€æ ‡é¢˜ç²¾å‡†åœ°æ¦‚æ‹¬äº†è®ºæ–‡çš„æ ¸å¿ƒç ”ç©¶æ–¹å‘ï¼Œå³åˆ©ç”¨æ¨¡æ‹Ÿç‰©ç†åœºï¼ˆé™ç”µåœºï¼‰çš„ç”µåŠ¿å‚æ•°æ¥ç›´æ¥è®¾å®šç¥ç»ç½‘ç»œæƒé‡ï¼Œä»è€Œçœå»ä¼ ç»Ÿçš„åˆ†æè®¡ç®—ä¸è®­ç»ƒè¿‡ç¨‹ã€‚å¦‚æœæ‚¨éœ€è¦è¿›ä¸€æ­¥ç¿»è¯‘æ‘˜è¦ï¼ˆAbstractï¼‰çš„å†…å®¹ï¼Œæˆ–è€…å¯¹è¿™ç§â€œå³æ—¶è·å–æƒé‡â€çš„ç‰©ç†æ¨¡æ‹Ÿæ–¹æ³•æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶ä¸æˆ‘æ·±å…¥æ¢è®¨ï¼",
      "authors": [
        "Geidarov Polad"
      ],
      "abstract": "This paper explores the possibility of determining the weights and thresholds of a neural network using the potential -- a parameter of an electrostatic field -- without analytical calculations and without applying training algorithms. The work is based on neural network architectures employing metric recognition methods. The electrostatic field is simulated in the Builder C++ environment. In the same environment, a neural network based on metric recognition methods is constructed, with the weights of the first-layer neurons determined by the values of the potentials of the simulated electrostatic field. The effectiveness of the resulting neural network within the simulated system is evaluated using the MNIST test dataset under various initial conditions of the simulated system. The results demonstrated functional viability. The implementation of this approach shows that a neural network can obtain weight values almost instantaneously from the electrostatic field, without the need for analytical computations, lengthy training procedures, or massive training datasets.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§é€šè¿‡æ¨¡æ‹Ÿé™ç”µåœº (Electrostatic Field) çš„ç”µåŠ¿ (Potential) å‚æ•°æ¥ç¡®å®šç¥ç»ç½‘ç»œæƒé‡å’Œé˜ˆå€¼çš„æ–°æ–¹æ³•ï¼Œä¸”æ— éœ€è¿›è¡Œè§£æè®¡ç®—æˆ–åº”ç”¨ä¼ ç»Ÿçš„è®­ç»ƒç®—æ³• (Training Algorithms)ã€‚è¯¥å·¥ä½œåŸºäºé‡‡ç”¨åº¦é‡è¯†åˆ«æ–¹æ³• (Metric Recognition Methods) çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶åœ¨ Builder C++ ç¯å¢ƒä¸­å¯¹é™ç”µåœºè¿›è¡Œä»¿çœŸã€‚åœ¨è¯¥ä»¿çœŸç³»ç»Ÿä¸­æ„å»ºçš„ç¥ç»ç½‘ç»œï¼Œå…¶ç¬¬ä¸€å±‚ç¥ç»å…ƒçš„æƒé‡ç›´æ¥ç”±æ¨¡æ‹Ÿé™ç”µåœºçš„ç”µåŠ¿å€¼å†³å®šã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ MNIST æµ‹è¯•æ•°æ®é›†ï¼Œåœ¨å¤šç§åˆå§‹æ¡ä»¶ä¸‹è¯„ä¼°äº†è¯¥æ¨¡å‹åœ¨ä»¿çœŸç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ¡ˆçš„åŠŸèƒ½å¯è¡Œæ€§ï¼Œè¡¨æ˜ç¥ç»ç½‘ç»œå¯ä»¥å‡ ä¹ç¬æ—¶åœ°ä»é™ç”µåœºä¸­è·å–æƒé‡å€¼ã€‚è¿™ç§æ–¹æ³•å®ç°äº†åœ¨ä¸ä¾èµ–ç¹æ‚çš„è§£æè®¡ç®—ã€å†—é•¿çš„è®­ç»ƒè¿‡ç¨‹æˆ–å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿç¡®å®šç¥ç»ç½‘ç»œæƒé‡çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "14 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.02933v1",
      "published_date": "2025-06-26 14:26:38 UTC",
      "updated_date": "2025-06-26 14:26:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:16.370050+00:00"
    },
    {
      "arxiv_id": "2506.21306v1",
      "title": "On Uniform Weighted Deep Polynomial approximation",
      "title_zh": "è®ºä¸€è‡´åŠ æƒæ·±åº¦å¤šé¡¹å¼é€¼è¿‘",
      "authors": [
        "Kingsley Yeon",
        "Steven B. Damelin"
      ],
      "abstract": "It is a classical result in rational approximation theory that certain non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be efficiently approximated using rational functions with root-exponential convergence in terms of degrees of freedom \\cite{Sta, GN}. In contrast, polynomial approximations admit only algebraic convergence by Jackson's theorem \\cite{Lub2}. Recent work shows that composite polynomial architectures can recover exponential approximation rates even without smoothness \\cite{KY}. In this work, we introduce and analyze a class of weighted deep polynomial approximants tailored for functions with asymmetric behavior-growing unbounded on one side and decaying on the other. By multiplying a learnable deep polynomial with a one-sided weight, we capture both local non-smoothness and global growth. We show numerically that this framework outperforms Taylor, Chebyshev, and standard deep polynomial approximants, even when all use the same number of parameters. To optimize these approximants in practice, we propose a stable graph-based parameterization strategy building on \\cite{Jar}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰éå¯¹ç§°è¡Œä¸ºï¼ˆä¸€ä¾§æ— é™å¢é•¿ï¼Œå¦ä¸€ä¾§è¡°å‡ï¼‰çš„å‡½æ•°ï¼Œæå‡ºäº†ä¸€ç±»åŠ æƒæ·±åº¦å¤šé¡¹å¼è¿‘ä¼¼ (weighted deep polynomial approximants) æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å¤šé¡¹å¼è¿‘ä¼¼åœ¨å¤„ç†éå…‰æ»‘æˆ–å¥‡å¼‚å‡½æ•°æ—¶é€šå¸¸ä»…å…·å¤‡ä»£æ•°çº§æ”¶æ•›é€Ÿåº¦ï¼Œè€Œè¯¥æ¡†æ¶é€šè¿‡å°†å¯å­¦ä¹ çš„æ·±åº¦å¤šé¡¹å¼ä¸å•ä¾§æƒé‡ç›¸ä¹˜ï¼Œæ—¨åœ¨åŒæ—¶æ•æ‰å‡½æ•°çš„å±€éƒ¨éå…‰æ»‘ç‰¹å¾ä¸å…¨å±€å¢é•¿è¶‹åŠ¿ã€‚ä¸ºäº†åœ¨å®è·µä¸­æœ‰æ•ˆä¼˜åŒ–è¿™äº›è¿‘ä¼¼å™¨ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç¨³å®šçš„åŸºäºå›¾çš„å‚æ•°åŒ–ç­–ç•¥ (graph-based parameterization strategy)ã€‚æ•°å€¼å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‚æ•°æ•°é‡ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°ä¼˜äº Taylorã€Chebyshev ä»¥åŠæ ‡å‡†çš„æ·±åº¦å¤šé¡¹å¼è¿‘ä¼¼ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†å¤åˆå¤šé¡¹å¼æ¶æ„åœ¨æ¢å¤æŒ‡æ•°çº§è¿‘ä¼¼é€Ÿç‡æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºå¤„ç†å¤æ‚è¾¹ç•Œæ¡ä»¶çš„å‡½æ•°æä¾›äº†é«˜æ•ˆçš„æ•°å€¼æ–¹æ¡ˆã€‚",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21306v1",
      "published_date": "2025-06-26 14:25:32 UTC",
      "updated_date": "2025-06-26 14:25:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:47.481251+00:00"
    },
    {
      "arxiv_id": "2506.21298v2",
      "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation",
      "title_zh": "æ¢ç©¶ä½èµ„æºéŸ³ä¹ç”Ÿæˆçš„é€‚é…å™¨è®¾è®¡æƒè¡¡",
      "authors": [
        "Atharva Mehta",
        "Shivam Chauhan",
        "Monojit Choudhury"
      ],
      "abstract": "Fine-tuning large-scale music generation models, such as MusicGen and Mustango, is a computationally expensive process, often requiring updates to billions of parameters and, therefore, significant hardware resources. Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based methods, have emerged as a promising alternative, enabling adaptation with minimal trainable parameters while preserving model performance. However, the design choices for adapters, including their architecture, placement, and size, are numerous, and it is unclear which of these combinations would produce optimal adapters and why, for a given case of low-resource music genre. In this paper, we attempt to answer this question by studying various adapter configurations for two AI music models, MusicGen and Mustango, on two genres: Hindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in capturing fine-grained local musical details such as ornamentations and short melodic phrases, while transformer-based adapters better preserve long-range dependencies crucial for structured improvisation. Additionally, we analyze computational resource requirements across different adapter scales, demonstrating how mid-sized adapters (40M parameters) achieve an optimal balance between expressivity and quality. Furthermore, we find that Mustango, a diffusion-based model, generates more diverse outputs with better adherence to the description in the input prompt while lacking in providing stability in notes, rhythm alignment, and aesthetics. Also, it is computationally intensive and requires significantly more time to train. In contrast, autoregressive models like MusicGen offer faster training and are more efficient, and can produce better quality output in comparison, but have slightly higher redundancy in their generations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è§„æ¨¡éŸ³ä¹ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ MusicGen å’Œ Mustangoï¼‰ä¸­ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é€‚é…å™¨ï¼ˆAdapterï¼‰è®¾è®¡åœ¨ä½èµ„æºéŸ³ä¹ç”Ÿæˆä¸­çš„æƒè¡¡ã€‚é€šè¿‡åœ¨ Hindustani Classical å’Œ Turkish Makam ä¸¤ç§æµæ´¾ä¸Šçš„å®éªŒï¼Œç ”ç©¶å‘ç°åŸºäºå·ç§¯ï¼ˆconvolution-basedï¼‰çš„é€‚é…å™¨æ“…é•¿æ•æ‰å±€éƒ¨éŸ³ä¹ç»†èŠ‚å’ŒçŸ­æ—‹å¾‹ï¼Œè€ŒåŸºäº Transformer çš„é€‚é…å™¨åˆ™åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å’Œç»“æ„åŒ–å³å…´åˆ›ä½œæ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œçº¦ 40M å‚æ•°çš„ä¸­å‹é€‚é…å™¨åœ¨æ¨¡å‹è¡¨è¾¾åŠ›ä¸ç”Ÿæˆè´¨é‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¯¹æ¯”äº†ä¸åŒæ¶æ„çš„æ¨¡å‹è¡¨ç°ï¼šåŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆdiffusion-basedï¼‰çš„ Mustango è™½ç„¶ç”Ÿæˆå†…å®¹æ›´å¤šæ ·ä¸”ç¬¦åˆæç¤ºè¯æè¿°ï¼Œä½†åœ¨éŸ³ç¬¦ç¨³å®šæ€§å’ŒèŠ‚å¥å¯¹é½ä¸Šè¾ƒå¼±ï¼Œä¸”è®­ç»ƒæˆæœ¬æ›´é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªå›å½’æ¨¡å‹ï¼ˆautoregressiveï¼‰MusicGen åœ¨è®­ç»ƒæ•ˆç‡å’Œè¾“å‡ºè´¨é‡ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œå°½ç®¡å…¶ç”Ÿæˆå†…å®¹å­˜åœ¨ä¸€å®šçš„é‡å¤æ€§ã€‚è¯¥ç ”ç©¶ä¸ºé’ˆå¯¹ç‰¹å®šä½èµ„æºé¢†åŸŸä¼˜åŒ–ç”Ÿæˆå¼éŸ³ä¹æ¨¡å‹æä¾›äº†é‡è¦çš„æ¶æ„é€‰æ‹©æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.21298v2",
      "published_date": "2025-06-26 14:18:39 UTC",
      "updated_date": "2025-08-11 06:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:49.800581+00:00"
    },
    {
      "arxiv_id": "2506.21294v1",
      "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models",
      "title_zh": "åŸºäºè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¥åœ°å¯¹è¯æŒ‡ä»£è¡¨è¾¾æ£€æµ‹",
      "authors": [
        "Bram Willemsen",
        "Gabriel Skantze"
      ],
      "abstract": "In this paper, we explore the use of a text-only, autoregressive language modeling approach for the extraction of referring expressions from visually grounded dialogue. More specifically, the aim is to investigate the extent to which the linguistic context alone can inform the detection of mentions that have a (visually perceivable) referent in the visual context of the conversation. To this end, we adapt a pretrained large language model (LLM) to perform a relatively course-grained annotation of mention spans in unfolding conversations by demarcating mention span boundaries in text via next-token prediction. Our findings indicate that even when using a moderately sized LLM, relatively small datasets, and parameter-efficient fine-tuning, a text-only approach can be effective, highlighting the relative importance of the linguistic context for this task. Nevertheless, we argue that the task represents an inherently multimodal problem and discuss limitations fundamental to unimodal approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è§†è§‰èƒŒæ™¯å¯¹è¯ï¼ˆVisually Grounded Dialogueï¼‰ä¸­ä½¿ç”¨çº¯æ–‡æœ¬è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆAutoregressive Language Modelsï¼‰æå–æŒ‡ä»£æ€§è¡¨è¾¾ï¼ˆReferring Expressionsï¼‰çš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒåœ¨äºè€ƒå¯Ÿå•çº¯çš„è¯­è¨€ä¸Šä¸‹æ–‡åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½æ”¯æŒè¯†åˆ«å¯¹è¯ä¸­å…·æœ‰è§†è§‰å‚ç…§ç‰©çš„æè¿°ï¼ˆMentionsï¼‰ã€‚ä½œè€…é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåˆ©ç”¨ä¸‹ä¸ªä»¤ç‰Œé¢„æµ‹ï¼ˆNext-token Predictionï¼‰æŠ€æœ¯åœ¨æ–‡æœ¬åºåˆ—ä¸­ç•Œå®šæè¿°èŒƒå›´è¾¹ç•Œã€‚å®éªŒç»“æœè¯æ˜ï¼Œå³ä¾¿ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ã€æœ‰é™çš„æ•°æ®é›†ä»¥åŠå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-efficient Fine-tuningï¼‰ï¼Œçº¯æ–‡æœ¬æ–¹æ¡ˆä¾ç„¶éå¸¸æœ‰æ•ˆã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†è¯­è¨€ä¸Šä¸‹æ–‡åœ¨æŒ‡ä»£æ£€æµ‹ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç ”ç©¶è€…æŒ‡å‡ºè¯¥ä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ï¼ˆMultimodalï¼‰é—®é¢˜ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†å•æ¨¡æ€ï¼ˆUnimodalï¼‰æ–¹æ³•å­˜åœ¨çš„æ ¹æœ¬æ€§å±€é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at XLLM @ ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21294v1",
      "published_date": "2025-06-26 14:14:20 UTC",
      "updated_date": "2025-06-26 14:14:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:56.126005+00:00"
    },
    {
      "arxiv_id": "2506.21288v1",
      "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
      "title_zh": "å°å‹ç¼–ç å™¨åœ¨ä¾æ®æ€§æ£€æµ‹ä¸­å¯æ¯”è‚©å¤§å‹è§£ç å™¨",
      "authors": [
        "Istabrak Abbes",
        "Gabriele Prato",
        "Quentin Fournier",
        "Fernando Rodriguez",
        "Alaa Boukhary",
        "Adam Elwood",
        "Sarath Chandar"
      ],
      "abstract": "Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness - generating responses strictly supported by the context - is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude. The code is available at : https://github.com/chandarlab/Hallucinate-less",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤–éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³æ—¶äº§ç”Ÿçš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºåœ¨ç”Ÿæˆç¯èŠ‚å‰å¯¹Groundednessï¼ˆçœŸå®æ€§/æ ¹æ®æ€§ï¼‰è¿›è¡Œæ£€æµ‹ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ç»è¿‡ä¸“é—¨å¾®è°ƒçš„è½»é‡çº§ç¼–ç å™¨æ¨¡å‹(Encoder Models)ï¼Œå¦‚RoBERTaå’ŒNomicBERTï¼Œæ¥åˆ¤æ–­æŸ¥è¯¢å†…å®¹æ˜¯å¦èƒ½ç”±æ‰€æä¾›çš„ä¸Šä¸‹æ–‡æ”¯æ’‘ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›å°å‹æ¨¡å‹åœ¨Groundednessæ£€æµ‹çš„å‡†ç¡®ç‡ä¸Šè¶³ä»¥åª²ç¾Llama3 8Bå’ŒGPT-4oç­‰å°–ç«¯æ¨¡å‹ã€‚é€šè¿‡è¿™ç§å‰ç½®æ£€æµ‹æœºåˆ¶ï¼Œç³»ç»Ÿå¯ä»¥åœ¨æ‰§è¡Œé«˜æ˜‚çš„ç”Ÿæˆè®¡ç®—å‰è¯†åˆ«æ— æ ¹æ®çš„æ¨æµ‹ï¼Œä»è€Œå°†æ¨ç†å»¶è¿Ÿé™ä½æ•°ä¸ªæ•°é‡çº§ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä»»åŠ¡ç‰¹å®šå‹å°æ¨¡å‹åœ¨æå‡å¤§æ¨¡å‹ç³»ç»Ÿäº‹å®ä¸€è‡´æ€§ã€å¢å¼ºå¯ä¿¡åº¦å¹¶é™ä½èµ„æºæ¶ˆè€—æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21288v1",
      "published_date": "2025-06-26 14:09:41 UTC",
      "updated_date": "2025-06-26 14:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:56.248118+00:00"
    },
    {
      "arxiv_id": "2506.21278v2",
      "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution",
      "title_zh": "åŸºäºé«˜æ•ˆçƒé¢æŸ¯è¥¿åˆ†å¸ƒçš„è¶…çƒé¢å˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Lukas Sablica",
        "Kurt Hornik"
      ],
      "abstract": "We propose a novel variational autoencoder (VAE) architecture that employs a spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy provides a more natural hyperspherical representation of latent variables, better capturing directional data while maintaining flexibility. Its heavy-tailed nature prevents over-regularization, ensuring efficient latent space utilization while offering a more expressive representation. Additionally, spCauchy circumvents the numerical instabilities inherent to vMF, which arise from computing normalization constants involving Bessel functions. Instead, it enables a fully differentiable and efficient reparameterization trick via MÃ¶bius transformations, allowing for stable and scalable training. The KL divergence can be computed through a rapidly converging power series, eliminating concerns of underflow or overflow associated with evaluation of ratios of hypergeometric functions. These properties make spCauchy a compelling alternative for VAEs, offering both theoretical advantages and practical efficiency in high-dimensional generative modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoder, VAE)æ¶æ„ï¼Œé‡‡ç”¨çƒé¢æŸ¯è¥¿åˆ†å¸ƒ(spherical Cauchy, spCauchy)ä½œä¸ºå…¶æ½œç©ºé—´åˆ†å¸ƒã€‚ä¸ä¼ ç»Ÿçš„é«˜æ–¯åˆ†å¸ƒæˆ–å¹¿æ³›ä½¿ç”¨çš„von Mises-Fisher (vMF)åˆ†å¸ƒä¸åŒï¼ŒspCauchyåˆ†å¸ƒä¸ºæ½œå˜é‡æä¾›äº†æ›´è‡ªç„¶çš„è¶…çƒé¢è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å®šå‘æ•°æ®çš„ç‰¹å¾ã€‚ç”±äºspCauchyå…·æœ‰é‡å°¾(heavy-tailed)ç‰¹æ€§ï¼Œå®ƒèƒ½æœ‰æ•ˆé˜²æ­¢è¿‡åº¦æ­£åˆ™åŒ–ï¼Œä»è€Œæé«˜æ½œç©ºé—´åˆ©ç”¨ç‡å¹¶æä¾›æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨æ•°å€¼ç¨³å®šæ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•è§„é¿äº†vMFåˆ†å¸ƒä¸­å› è®¡ç®—è´å¡å°”å‡½æ•°(Bessel functions)å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šé—®é¢˜ã€‚ç ”ç©¶åˆ©ç”¨MÃ¶biuså˜æ¢å®ç°äº†å®Œå…¨å¯å¾®ä¸”é«˜æ•ˆçš„é‡å‚æ•°åŒ–æŠ€å·§(reparameterization trick)ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­çš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒKLæ•£åº¦é€šè¿‡å¿«é€Ÿæ”¶æ•›çš„å¹‚çº§æ•°è¿›è¡Œè®¡ç®—ï¼Œæ¶ˆé™¤äº†è¶…å‡ ä½•å‡½æ•°æ¯”ä¾‹è¯„ä¼°ä¸­å¸¸è§çš„æº¢å‡ºé£é™©ã€‚è¿™äº›ç‰¹æ€§ä½¿spCauchyåˆ†å¸ƒæˆä¸ºé«˜ç»´ç”Ÿæˆå»ºæ¨¡ä¸­æå…·å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ç†è®ºä¼˜åŠ¿å’Œå®é™…æ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21278v2",
      "published_date": "2025-06-26 14:01:51 UTC",
      "updated_date": "2025-07-14 10:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:50:59.717393+00:00"
    },
    {
      "arxiv_id": "2506.22516v1",
      "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis",
      "title_zh": "èƒ½å¦ä»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨çŠ¶æ€ä¸­è§‚æµ‹åˆ°â€œæ„è¯†â€ï¼ŸåŸºäºæ•´åˆä¿¡æ¯ç†è®ºä¸è·¨åº¦è¡¨å¾åˆ†æå¯¹å¿ƒç†ç†è®ºæµ‹è¯•ä¸‹çš„ LLM è¡¨å¾è¿›è¡Œå‰–æ",
      "authors": [
        "Jingkai Li"
      ],
      "abstract": "Integrated Information Theory (IIT) provides a quantitative framework for explaining consciousness phenomenon, positing that conscious systems comprise elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the latest iterations of this framework -- to sequences of Large Language Model (LLM) representations, analyzing data derived from existing Theory of Mind (ToM) test results. Our study systematically investigates whether the differences of ToM test performances, when presented in the LLM representations, can be revealed by IIT estimates, i.e., $Î¦^{\\max}$ (IIT 3.0), $Î¦$ (IIT 4.0), Conceptual Information (IIT 3.0), and $Î¦$-structure (IIT 4.0). Furthermore, we compare these metrics with the Span Representations independent of any estimate for consciousness. This additional effort aims to differentiate between potential \"consciousness\" phenomena and inherent separations within LLM representational space. We conduct comprehensive experiments examining variations across LLM transformer layers and linguistic spans from stimuli. Our results suggest that sequences of contemporary Transformer-based LLM representations lack statistically significant indicators of observed \"consciousness\" phenomena but exhibit intriguing patterns under $\\textit{spatio}$-permutational analyses. The Appendix and code are available as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥ä»å¤§è¯­è¨€æ¨¡å‹(LLM)çš„å†…éƒ¨çŠ¶æ€ä¸­è§‚å¯Ÿåˆ°â€œæ„è¯†â€ï¼Œé‡ç‚¹åˆ©ç”¨é›†æˆä¿¡æ¯ç†è®º(Integrated Information Theory, IIT)å’Œè·¨åº¦è¡¨ç¤º(Span Representation)å¯¹å¿ƒç†ç†è®º(Theory of Mind, ToM)æµ‹è¯•ç”Ÿæˆçš„è¡¨å¾è¿›è¡Œäº†åˆ†æã€‚ç ”ç©¶è€…åº”ç”¨äº†IIT 3.0å’Œ4.0æ¡†æ¶ï¼Œç³»ç»Ÿè€ƒå¯Ÿäº†$Î¦^{\\max}$ã€$Î¦$ã€Conceptual InformationåŠ$Î¦$-structureç­‰æŒ‡æ ‡ä¸ToMæµ‹è¯•è¡¨ç°å·®å¼‚ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†åŒºåˆ†æ½œåœ¨çš„â€œæ„è¯†â€ç°è±¡ä¸è¡¨å¾ç©ºé—´çš„å›ºæœ‰åˆ†ç¦»ï¼Œç ”ç©¶è¿˜å°†è¿™äº›æŒ‡æ ‡ä¸ç‹¬ç«‹äºæ„è¯†ä¼°å€¼çš„Span Representationsè¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚å®éªŒè¦†ç›–äº†Transformeræ¶æ„çš„ä¸åŒå±‚çº§å’Œè¯­è¨€è·¨åº¦ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“ä»£åŸºäºTransformerçš„LLMè¡¨å¾åœ¨ç»Ÿè®¡å­¦ä¸Šç¼ºä¹æ˜¾è‘—çš„â€œæ„è¯†â€ç°è±¡æŒ‡æ ‡ï¼Œä½†åœ¨ç©ºæ—¶ç½®æ¢(spatio-permutational)åˆ†æä¸‹å±•ç°å‡ºäº†ä¸€äº›æœ‰è¶£çš„æ¨¡å¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a journal paper at: https://doi.org/10.1016/j.nlp.2025.100163",
      "pdf_url": "https://arxiv.org/pdf/2506.22516v1",
      "published_date": "2025-06-26 13:59:22 UTC",
      "updated_date": "2025-06-26 13:59:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:08.536480+00:00"
    },
    {
      "arxiv_id": "2506.21269v1",
      "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou",
      "title_zh": "èåˆè½¦è¾†å£°å­¦æ•°æ®ä»¥æå‡åŸå¸‚äº¤é€šç®¡ç†ï¼šSuzhou è½¦é€Ÿåˆ†ç±»ç ”ç©¶",
      "authors": [
        "Pengfei Fan",
        "Yuli Zhang",
        "Xinheng Wang",
        "Ruiyuan Jiang",
        "Hankang Gu",
        "Dongyao Jia",
        "Shangbo Wang"
      ],
      "abstract": "This study presents and publicly releases the Suzhou Urban Road Acoustic Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive data-acquisition protocols and annotation guidelines to ensure transparency and reproducibility of the experimental workflow. To model the coupling between vehicular noise and driving speed, we propose a bimodal-feature-fusion deep convolutional neural network (BMCNN). During preprocessing, an adaptive denoising and normalization strategy is applied to suppress environmental background interference; in the network architecture, parallel branches extract Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features, which are subsequently fused via a cross-modal attention mechanism in the intermediate feature space to fully exploit time-frequency information. Experimental results demonstrate that BMCNN achieves a classification accuracy of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic dataset. Ablation studies and robustness tests on the Suzhou dataset further validate the contributions of each module to performance improvement and overfitting mitigation. The proposed acoustics-based speed classification method can be integrated into smart-city traffic management systems for real-time noise monitoring and speed estimation, thereby optimizing traffic flow control, reducing roadside noise pollution, and supporting sustainable urban planning.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†è‹å·åŸå¸‚é“è·¯å£°å­¦æ•°æ®é›†(SZUR-Acoustic Dataset)ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒæ¨¡æ€ç‰¹å¾èåˆæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(BMCNN)ï¼Œæ—¨åœ¨é€šè¿‡è½¦è¾†å£°å­¦æ•°æ®å®ç°åŸå¸‚äº¤é€šçš„å¢å¼ºç®¡ç†ã€‚è¯¥æ¨¡å‹åœ¨é¢„å¤„ç†é˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”å»å™ªå’Œå½’ä¸€åŒ–ç­–ç•¥æŠ‘åˆ¶èƒŒæ™¯å¹²æ‰°ï¼Œå¹¶é€šè¿‡å¹¶è¡Œåˆ†æ”¯æå–æ¢…å°”é¢‘ç‡å€’è°±ç³»æ•°(MFCCs)å’Œå°æ³¢åŒ…èƒ½é‡ç‰¹å¾ï¼Œéšååˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶(cross-modal attention mechanism)åœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œèåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBMCNNåœ¨SZUR-Acousticæ•°æ®é›†å’Œå…¬å¼€çš„IDMT-Trafficæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†87.56%å’Œ96.28%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æ¶ˆèç ”ç©¶ä¸é²æ£’æ€§æµ‹è¯•è¿›ä¸€æ­¥è¯å®äº†å„åŠŸèƒ½æ¨¡å—åœ¨æ€§èƒ½æå‡å’Œå‡è½»è¿‡æ‹Ÿåˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ™ºæ…§åŸå¸‚æä¾›äº†å®æ—¶çš„å™ªå£°ç›‘æµ‹ä¸é€Ÿåº¦ä¼°è®¡æ–¹æ¡ˆï¼Œæœ‰åŠ©äºä¼˜åŒ–äº¤é€šæµæ§åˆ¶å¹¶æ”¯æŒå¯æŒç»­çš„åŸå¸‚è§„åˆ’å‘å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21269v1",
      "published_date": "2025-06-26 13:53:22 UTC",
      "updated_date": "2025-06-26 13:53:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:29.948264+00:00"
    },
    {
      "arxiv_id": "2506.21263v1",
      "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster",
      "title_zh": "DiLoCoXï¼šé¢å‘å»ä¸­å¿ƒåŒ–é›†ç¾¤çš„ä½é€šä¿¡å¤§è§„æ¨¡è®­ç»ƒæ¡†æ¶",
      "authors": [
        "Ji Qi",
        "WenPeng Zhu",
        "Li Li",
        "Ming Wu",
        "YingJun Wu",
        "Wu He",
        "Xun Gao",
        "Jason Zeng",
        "Michael Heinrich"
      ],
      "abstract": "The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åˆ†å¸ƒå¼è®­ç»ƒå¯¹ä¸­å¿ƒåŒ–é›†ç¾¤é«˜é€šä¿¡å¸¦å®½çš„ä¾èµ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†DiLoCoXï¼Œä¸€ç§é€‚ç”¨äºå»ä¸­å¿ƒåŒ–é›†ç¾¤(Decentralized Cluster)çš„å¤§è§„æ¨¡ä½é€šä¿¡è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°é›†æˆäº†æµæ°´çº¿å¹¶è¡Œ(Pipeline Parallelism)ä¸åŒä¼˜åŒ–å™¨ç­–ç•¥(Dual Optimizer Policy)ï¼Œå¹¶å¼•å…¥äº†é€šä¿¡ä¸æœ¬åœ°è®­ç»ƒçš„ä¸€æ­¥å»¶è¿Ÿé‡å (One-Step-Delay Overlap of Communication and Local Training)æœºåˆ¶ã€‚æ­¤å¤–ï¼ŒDiLoCoXè¿˜ç»“åˆäº†è‡ªé€‚åº”æ¢¯åº¦å‹ç¼©æ–¹æ¡ˆ(Adaptive Gradient Compression Scheme)ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æéªŒè¯äº†å…¶åœ¨æ”¶æ•›æ€§ä¸Šçš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiLoCoXèƒ½å¤Ÿåœ¨1Gbpsçš„ä½é€Ÿç½‘ç»œç¯å¢ƒä¸‹æˆåŠŸé¢„è®­ç»ƒå‚æ•°é‡é«˜è¾¾107Bçš„åŸºç¡€æ¨¡å‹ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„AllReduceæ–¹å¼ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒæ¨¡å‹æ”¶æ•›æ€§èƒ½å‡ ä¹æ— æŸçš„å‰æä¸‹ï¼Œå®ç°äº†è®­ç»ƒé€Ÿåº¦357å€çš„æå‡ã€‚ä½œä¸ºé¦–ä¸ªæˆåŠŸåº”ç”¨äºåƒäº¿å‚æ•°è§„æ¨¡æ¨¡å‹çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶ï¼ŒDiLoCoXæ˜¾è‘—å¢å¼ºäº†è¶…å¤§è§„æ¨¡æ¨¡å‹åœ¨æ…¢é€Ÿç½‘ç»œç¯å¢ƒä¸‹çš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21263v1",
      "published_date": "2025-06-26 13:45:04 UTC",
      "updated_date": "2025-06-26 13:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:31.788184+00:00"
    },
    {
      "arxiv_id": "2506.21252v1",
      "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents",
      "title_zh": "Agent-RewardBenchï¼šé¢å‘ç°å®ä¸–ç•Œå¤šæ¨¡æ€æ™ºèƒ½ä½“æ„ŸçŸ¥ã€è§„åˆ’ä¸å®‰å…¨çš„ç»Ÿä¸€å¥–åŠ±å»ºæ¨¡åŸºå‡†",
      "authors": [
        "Tianyi Men",
        "Zhuoran Jin",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agent-RewardBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å¥–åŠ±å»ºæ¨¡èƒ½åŠ›çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨è‡ªçº é”™å’Œæ³›åŒ–æ–¹é¢é¢ä¸´çš„åé¦ˆç¼ºå¤±æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æ¶µç›–æ„ŸçŸ¥ (Perception)ã€è§„åˆ’ (Planning) å’Œå®‰å…¨æ€§ (Safety) ä¸‰ä¸ªç»´åº¦åŠ 7 ä¸ªçœŸå®åœºæ™¯ï¼Œå®ç°äº†å¯¹æ™ºèƒ½ä½“èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚å…¶æ ¸å¿ƒç‰¹è‰²åœ¨äºæ”¯æŒæ­¥éª¤çº§ (Step-level) çš„å¥–åŠ±è¯„ä»·ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä»»åŠ¡ä¸­çš„å…·ä½“æ­¥éª¤æä¾›ç»†ç²’åº¦çš„æ€§èƒ½åé¦ˆã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä» 10 ä¸ªå¤šæ ·åŒ–æ¨¡å‹ä¸­é‡‡æ ·å¹¶ç»“åˆäººå·¥éªŒè¯ï¼Œç¡®ä¿äº†æ•°æ®é›†çš„é«˜è´¨é‡ä¸éš¾åº¦æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨æ™ºèƒ½ä½“å¥–åŠ±å»ºæ¨¡æ–¹é¢ä»è¡¨ç°æœ‰é™ï¼Œå‡¸æ˜¾äº†é’ˆå¯¹è¯¥é¢†åŸŸè¿›è¡Œä¸“é¡¹è®­ç»ƒçš„ç´§è¿«éœ€æ±‚ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.21252v1",
      "published_date": "2025-06-26 13:36:12 UTC",
      "updated_date": "2025-06-26 13:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:34.930904+00:00"
    },
    {
      "arxiv_id": "2508.13156v1",
      "title": "EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code",
      "title_zh": "EvoVerilogï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ Verilog ä»£ç æ¼”åŒ–",
      "authors": [
        "Ping Guo",
        "Yiting Wang",
        "Wanghao Ye",
        "Yexiao He",
        "Ziyao Wang",
        "Xiaopeng Dai",
        "Ang Li",
        "Qingfu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated great potential in automating the generation of Verilog hardware description language code for hardware design. This automation is critical to reducing human effort in the complex and error-prone process of hardware design.\n  However, existing approaches predominantly rely on human intervention and fine-tuning using curated datasets, limiting their scalability in automated design workflows.\n  Although recent iterative search techniques have emerged, they often fail to explore diverse design solutions and may underperform simpler approaches such as repeated prompting.\n  To address these limitations, we introduce EvoVerilog, a novel framework that combines the reasoning capabilities of LLMs with evolutionary algorithms to automatically generate and refine Verilog code.\n  EvoVerilog utilizes a multiobjective, population-based search strategy to explore a wide range of design possibilities without requiring human intervention.\n  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively. Furthermore, the framework showcases its ability to explore diverse designs by simultaneously generating a variety of functional Verilog code while optimizing resource utilization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EvoVerilogï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›ä¸è¿›åŒ–ç®—æ³• (evolutionary algorithms) çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç° Verilog ç¡¬ä»¶æè¿°è¯­è¨€ä»£ç çš„è‡ªåŠ¨ç”Ÿæˆä¸ä¼˜åŒ–ã€‚é’ˆå¯¹ç°æœ‰ LLM è¾…åŠ©è®¾è®¡æ–¹æ³•å¯¹äººå·¥å¹²é¢„ä¾èµ–åº¦é«˜ã€æœç´¢ç­–ç•¥å¤šæ ·æ€§ä¸è¶³ç­‰å±€é™æ€§ï¼ŒEvoVerilog å¼•å…¥äº†ä¸€ç§å¤šç›®æ ‡ã€åŸºäºç¾¤ä½“çš„æœç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æ¢ç´¢å¹¿æ³›çš„è®¾è®¡å¯èƒ½æ€§ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒEvoVerilog åœ¨ VerilogEval-Machine å’Œ VerilogEval-Human åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº† 89.1 å’Œ 80.2 çš„ pass@10 å¾—åˆ†ï¼Œåˆ·æ–°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½è®°å½•ã€‚è¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æå‡äº†ç¡¬ä»¶ä»£ç ç”Ÿæˆçš„å‡†ç¡®ç‡ï¼Œè¿˜å±•ç¤ºäº†åœ¨ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡çš„åŒæ—¶ï¼ŒåŒæ­¥ç”Ÿæˆå¤šç§åŠŸèƒ½æ€§è®¾è®¡æ–¹æ¡ˆçš„å“è¶Šèƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–ç¡¬ä»¶è®¾è®¡å·¥ä½œæµæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13156v1",
      "published_date": "2025-06-26 13:32:25 UTC",
      "updated_date": "2025-06-26 13:32:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:38.380616+00:00"
    },
    {
      "arxiv_id": "2506.21246v1",
      "title": "From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting",
      "title_zh": "ä»é“¾ä¸Šåˆ°å®è§‚ï¼šè¯„ä¼°æ•°æ®æºå¤šæ ·æ€§åœ¨åŠ å¯†è´§å¸å¸‚åœºé¢„æµ‹ä¸­çš„é‡è¦æ€§",
      "authors": [
        "Giorgos Demosthenous",
        "Chryssis Georgiou",
        "Eliada Polydorou"
      ],
      "abstract": "This study investigates the impact of data source diversity on the performance of cryptocurrency forecasting models by integrating various data categories, including technical indicators, on-chain metrics, sentiment and interest metrics, traditional market indices, and macroeconomic indicators. We introduce the Crypto100 index, representing the top 100 cryptocurrencies by market capitalization, and propose a novel feature reduction algorithm to identify the most impactful and resilient features from diverse data sources. Our comprehensive experiments demonstrate that data source diversity significantly enhances the predictive performance of forecasting models across different time horizons. Key findings include the paramount importance of on-chain metrics for both short-term and long-term predictions, the growing relevance of traditional market indices and macroeconomic indicators for longer-term forecasts, and substantial improvements in model accuracy when diverse data sources are utilized. These insights help demystify the short-term and long-term driving factors of the cryptocurrency market and lay the groundwork for developing more accurate and resilient forecasting models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®æºå¤šæ ·æ€§å¯¹åŠ å¯†è´§å¸é¢„æµ‹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œé€šè¿‡æ•´åˆæŠ€æœ¯æŒ‡æ ‡ã€é“¾ä¸ŠæŒ‡æ ‡(on-chain metrics)ã€æƒ…ç»ªä¸å…³æ³¨åº¦åº¦é‡ã€ä¼ ç»Ÿå¸‚åœºæŒ‡æ•°ä»¥åŠå®è§‚ç»æµæŒ‡æ ‡ç­‰å¤šç»´åº¦æ•°æ®è¿›è¡Œåˆ†æã€‚ç ”ç©¶å¼•å…¥äº†ä»£è¡¨å¸‚å€¼å‰100ç§åŠ å¯†è´§å¸çš„Crypto100æŒ‡æ•°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹ç‰¹å¾è§„çº¦ç®—æ³•(feature reduction algorithm)ä»¥è¯†åˆ«æœ€å…·å½±å“åŠ›å’ŒéŸ§æ€§çš„ç‰¹å¾ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ•°æ®æºçš„å¤šæ ·æ€§æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒæ—¶é—´è·¨åº¦ä¸‹çš„é¢„æµ‹è¡¨ç°ã€‚å…³é”®å‘ç°æŒ‡å‡ºï¼Œé“¾ä¸ŠæŒ‡æ ‡å¯¹äºçŸ­æœŸå’Œé•¿æœŸé¢„æµ‹å‡å…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œè€Œä¼ ç»Ÿå¸‚åœºæŒ‡æ•°å’Œå®è§‚ç»æµæŒ‡æ ‡åœ¨é•¿æœŸé¢„æµ‹ä¸­çš„ç›¸å…³æ€§æ—¥ç›Šå¢å¼ºã€‚è¿™äº›ç ”ç©¶ç»“æœæ­ç¤ºäº†åŠ å¯†è´§å¸å¸‚åœºçš„é•¿çŸ­æœŸé©±åŠ¨å› ç´ ï¼Œå¹¶ä¸ºå¼€å‘æ›´å‡†ç¡®ã€æ›´å…·éŸ§æ€§çš„é¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "q-fin.ST"
      ],
      "primary_category": "q-fin.PM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21246v1",
      "published_date": "2025-06-26 13:29:19 UTC",
      "updated_date": "2025-06-26 13:29:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:41.032878+00:00"
    },
    {
      "arxiv_id": "2506.21230v2",
      "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner",
      "title_zh": "å…·å¤‡ä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›çš„è§„åˆ’å™è¿°å¢å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹è§„åˆ’å™¨",
      "authors": [
        "Junhao Shi",
        "Zhaoye Fei",
        "Siyin Wang",
        "Qipeng Guo",
        "Jingjing Gong",
        "Xipeng Qiu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks but struggle with complex scenarios involving unfamiliar environments and multi-step goals. Current approaches rely on environment-agnostic imitation learning that disconnects instructions from environmental contexts, causing models to struggle with context-sensitive instructions and rely on supplementary cues rather than visual reasoning during long-horizon interactions. In this work, we propose World-Aware Planning Narrative Enhancement (WAP), a framework that infuses LVLMs with comprehensive environmental understanding through four cognitive capabilities (visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding) while developing and evaluating models using only raw visual observations through curriculum learning. Evaluations on the EB-ALFRED benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a 60.7 absolute improvement in task success rates, particularly in commonsense reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced open-source models outperform proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†World-Aware Planning Narrative Enhancement (WAP)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å…·ä½“åŒ–è§„åˆ’(embodied planning)ä»»åŠ¡ä¸­éš¾ä»¥åº”å¯¹å¤æ‚åœºæ™¯å’Œå¤šæ­¥ç›®æ ‡çš„é—®é¢˜ã€‚é’ˆå¯¹ç›®å‰æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ç¼ºä¹ç¯å¢ƒä¸Šä¸‹æ–‡å…³è”çš„ç¼ºé™·ï¼ŒWAPé€šè¿‡æ³¨å…¥è§†è§‰å¤–è§‚å»ºæ¨¡(visual appearance modeling)ã€ç©ºé—´æ¨ç†(spatial reasoning)ã€åŠŸèƒ½æŠ½è±¡(functional abstraction)å’Œè¯­æ³•åŸºç¡€(syntactic grounding)å››é¡¹æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç¯å¢ƒçš„æ·±åº¦ç†è§£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ (curriculum learning)ç­–ç•¥ï¼Œä»…åŸºäºåŸå§‹è§†è§‰è§‚å¯Ÿè¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨é•¿ç¨‹äº¤äº’ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚åœ¨EB-ALFREDåŸºå‡†æµ‹è¯•ä¸­ï¼Œç»è¿‡å¢å¼ºçš„Qwen2.5-VLåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šå®ç°äº†60.7%çš„æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å¸¸è¯†æ¨ç†å’Œé•¿ç¨‹è§„åˆ’æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜ï¼Œè¯¥å¢å¼ºåçš„å¼€æºæ¨¡å‹æ€§èƒ½å¤§å¹…è¶…è¶Šäº†GPT-4oå’ŒClaude-3.5-Sonnetç­‰å•†ä¸šç³»ç»Ÿï¼Œå±•ç¤ºäº†ä¸–ç•Œæ„ŸçŸ¥å™äº‹åœ¨æå‡LVLMsè§„åˆ’æ•ˆèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21230v2",
      "published_date": "2025-06-26 13:20:55 UTC",
      "updated_date": "2025-07-02 15:03:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:02.675715+00:00"
    },
    {
      "arxiv_id": "2506.21215v1",
      "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
      "title_zh": "æ­ç§˜å¤§è¯­è¨€æ¨¡å‹çš„å› æœæ¨ç†ï¼šçœŸç›¸è¿˜æ˜¯å¹»è±¡ï¼Ÿ",
      "authors": [
        "Haoang Chi",
        "He Li",
        "Wenjing Yang",
        "Feng Liu",
        "Long Lan",
        "Xiaoguang Ren",
        "Tongliang Liu",
        "Bo Han"
      ],
      "abstract": "Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å…·å¤‡çœŸæ­£çš„å› æœæ¨ç†ï¼ˆCausal Reasoningï¼‰èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡åˆ†æ Transformer æ¶æ„çš„è‡ªå›å½’ï¼ˆAutoregressionï¼‰æœºåˆ¶å‘ç°ï¼Œæ¨¡å‹ç›®å‰ä»…èƒ½å®ç°åŸºäºå‚æ•°çŸ¥è¯†çš„æµ…å±‚ä¸€çº§ï¼ˆLevel-1ï¼‰æ¨ç†ï¼Œè€Œç¼ºä¹ç±»ä¼¼äººç±»çš„äºŒçº§ï¼ˆLevel-2ï¼‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†å…¨æ–°çš„è¯„æµ‹åŸºå‡† CausalProbe-2024ï¼Œå®éªŒæ˜¾ç¤º LLMs åœ¨å¤„ç†è¿™äº›æœªè§æ•°æ®æ—¶è¡¨ç°å¤§å¹…ä¸‹é™ï¼Œè¿›ä¸€æ­¥è¯å®äº†å…¶å¯¹å·²æœ‰çŸ¥è¯†çš„ä¾èµ–è€Œéé€»è¾‘æ¨ç†ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„å› æœæ¨ç†æ°´å¹³ï¼Œç ”ç©¶è€…æå‡ºäº† G^2-Reasoner æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é€šç”¨çŸ¥è¯†å’Œç›®æ ‡å¯¼å‘ï¼ˆGoal-orientedï¼‰æç¤ºæ¥æ¨¡æ‹Ÿäººç±»æ€ç»´ã€‚å®éªŒè¯æ˜ï¼ŒG^2-Reasoner èƒ½æ˜¾è‘—å¢å¼º LLMs åœ¨æ–°é¢–åœºæ™¯åŠåäº‹å®ï¼ˆCounterfactualï¼‰è¯­å¢ƒä¸‹çš„è¡¨ç°ï¼Œä¸ºå®ç°çœŸæ­£çš„äºŒçº§å› æœæ¨ç†æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, accepted at NeurIPS 2024",
      "pdf_url": "https://arxiv.org/pdf/2506.21215v1",
      "published_date": "2025-06-26 13:11:01 UTC",
      "updated_date": "2025-06-26 13:11:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:47.849744+00:00"
    },
    {
      "arxiv_id": "2506.21211v1",
      "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models",
      "title_zh": "$T^3$ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šå±‚æ ‘å¼è‡ªåŠ¨ç¨‹åºä¿®å¤",
      "authors": [
        "Quanming Liu",
        "Xupeng Bu",
        "Zhichao Yan",
        "Ru Li"
      ],
      "abstract": "Automatic Program Repair (APR) is a core technology in software development and maintenance, with aims to enable automated defect repair with minimal human intervention. In recent years, the substantial advancements in Large Language Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly enhanced the reasoning capabilities of these models. However, due to the complex logic and multi-step reasoning ability needed, the application of CoT techniques in the APR domain remains insufficient. This study systematically evaluates the performance of several common CoT techniques in APR tasks and proposes an innovative framework $T^3$, which integrates the powerful reasoning capabilities of LLMs with tree search, effectively improving the precision of generating candidate repair solutions. Furthermore, $T^3$ provides valuable guidance for optimizing sample selection and repair strategies in APR tasks, establishing a robust framework for achieving efficient automated debugging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨ç¨‹åºä¿®å¤(Automatic Program Repair, APR)ä»»åŠ¡ä¸­å› é€»è¾‘å¤æ‚å’Œå¤šæ­¥æ¨ç†éœ€æ±‚å¯¼è‡´Chain-of-Thought (CoT)æŠ€æœ¯åº”ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º$T^3$çš„å¤šå±‚çº§æ ‘çŠ¶è‡ªåŠ¨ä¿®å¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ä¸æ ‘æœç´¢(Tree Search)ç®—æ³•ç›¸ç»“åˆï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çš„æœç´¢è¿‡ç¨‹æå‡å€™é€‰ä¿®å¤æ–¹æ¡ˆçš„ç”Ÿæˆç²¾åº¦ã€‚$T^3$ä¸ä»…ç³»ç»Ÿè¯„ä¼°äº†ç°æœ‰CoTæŠ€æœ¯åœ¨APRé¢†åŸŸçš„æ€§èƒ½ï¼Œè¿˜ä¸ºä¼˜åŒ–æ ·æœ¬é€‰æ‹©å’Œä¿®å¤ç­–ç•¥æä¾›äº†é‡è¦çš„ç†è®ºä¸å®è·µæŒ‡å¯¼ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ºå®ç°é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è°ƒè¯•å»ºç«‹äº†ç¨³å¥çš„ä½“ç³»ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsåœ¨å¤„ç†å¤æ‚ç¨‹åºé”™è¯¯æ—¶çš„æ¨ç†ä¸ä¿®å¤è¡¨ç°ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21211v1",
      "published_date": "2025-06-26 13:04:28 UTC",
      "updated_date": "2025-06-26 13:04:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:59.016990+00:00"
    },
    {
      "arxiv_id": "2506.21209v2",
      "title": "BitMark: Watermarking Bitwise Autoregressive Image Generative Models",
      "title_zh": "BitMarkï¼šä½çº§è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹æ°´å°æŠ€æœ¯",
      "authors": [
        "Louis Kerner",
        "Michel Meintz",
        "Bihe Zhao",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "abstract": "State-of-the-art text-to-image models generate photorealistic images at an unprecedented speed. This work focuses on models that operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework. Our method embeds a watermark directly at the bit level of the token stream during the image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs. The code is available at https://github.com/sprintml/BitMark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BitMarkï¼Œä¸€ç§é’ˆå¯¹æŒ‰ä½è‡ªå›å½’(bitwise autoregressive)å›¾åƒç”Ÿæˆæ¨¡å‹çš„é²æ£’æ€§æ°´å°æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹å› æ¨¡å‹åœ¨è‡ªèº«ç”Ÿæˆçš„åˆæˆæ•°æ®ä¸Šé‡å¤è®­ç»ƒè€Œå¯¼è‡´çš„æ¨¡å‹å´©æºƒ(model collapse)é—®é¢˜ã€‚BitMarkåœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥åœ¨ä»¤ç‰Œæµ(token stream)çš„ä½çº§åˆ«(bit level)åµŒå…¥æ°´å°ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´æ¯”ç‰¹ä½ï¼Œåœ¨ä¿æŒè§†è§‰ä¿çœŸåº¦(visual fidelity)å’Œç”Ÿæˆé€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹å¤šç§ç§»é™¤æŠ€æœ¯çš„æŠµå¾¡ã€‚è¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ”¾å°„æ€§(radioactivity)ï¼Œå³ä½¿ç”¨å¸¦æ°´å°çš„å›¾åƒè®­ç»ƒæˆ–å¾®è°ƒå…¶ä»–ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ diffusion modelsï¼‰æ—¶ï¼Œæ–°æ¨¡å‹çš„è¾“å‡ºä»ä¼šæºå¸¦å¯æ£€æµ‹çš„æ°´å°ç—•è¿¹ã€‚å®éªŒè¡¨æ˜ï¼ŒBitMarkä¸ºå¯é è¯†åˆ«ç”Ÿæˆå†…å®¹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µï¼Œæ˜¯é˜²æ­¢å›¾åƒç”Ÿæˆæ¨¡å‹å‘ç”Ÿæ€§èƒ½é€€åŒ–çš„é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as a Conference Paper at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21209v2",
      "published_date": "2025-06-26 13:03:13 UTC",
      "updated_date": "2025-12-03 15:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:52.532050+00:00"
    },
    {
      "arxiv_id": "2507.21091v1",
      "title": "The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment",
      "title_zh": "ç”Ÿæˆå¼ AI å¯¹è¯çš„ä»·å€¼ï¼šä¸€ç§è‡ªä¸‹è€Œä¸Šçš„ AI ä»·å€¼å¯¹é½æ¡†æ¶",
      "authors": [
        "Lenart Motnikar",
        "Katharina Baum",
        "Alexander Kagan",
        "Sarah Spiekermann-Hoff"
      ],
      "abstract": "Conversational agents (CAs) based on generative artificial intelligence frequently face challenges ensuring ethical interactions that align with human values. Current value alignment efforts largely rely on top-down approaches, such as technical guidelines or legal value principles. However, these methods tend to be disconnected from the specific contexts in which CAs operate, potentially leading to misalignment with users interests. To address this challenge, we propose a novel, bottom-up approach to value alignment, utilizing the value ontology of the ISO Value-Based Engineering standard for ethical IT design. We analyse 593 ethically sensitive system outputs identified from 16,908 conversational logs of a major European employment service CA to identify core values and instances of value misalignment within real-world interactions. The results revealed nine core values and 32 different value misalignments that negatively impacted users. Our findings provide actionable insights for CA providers seeking to address ethical challenges and achieve more context-sensitive value alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)é©±åŠ¨çš„å¯¹è¯æ™ºèƒ½ä½“(Conversational Agents)åœ¨ç¡®ä¿é“å¾·äº¤äº’ä¸äººç±»ä»·å€¼å¯¹é½(Value Alignment)æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªä¸‹è€Œä¸Š(bottom-up)çš„å¯¹é½æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ISOåŸºäºä»·å€¼å·¥ç¨‹(Value-Based Engineering)æ ‡å‡†çš„ä»·å€¼æœ¬ä½“ï¼Œæ—¨åœ¨å¼¥è¡¥ä¼ ç»Ÿè‡ªä¸Šè€Œä¸‹æ–¹æ³•è„±ç¦»å…·ä½“åº”ç”¨åœºæ™¯çš„ä¸è¶³ã€‚é€šè¿‡åˆ†æä¸€å®¶æ¬§æ´²å¤§å‹å°±ä¸šæœåŠ¡æœºæ„å¯¹è¯æ™ºèƒ½ä½“çš„16,908æ¡å¯¹è¯æ—¥å¿—ï¼Œç ”ç©¶äººå‘˜è¯†åˆ«å‡º593ä¸ªé“å¾·æ•æ„Ÿè¾“å‡ºï¼Œå¹¶ä»ä¸­å½’çº³å‡º9ä¸ªæ ¸å¿ƒä»·å€¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°äº†32ç§å¯¹ç”¨æˆ·äº§ç”Ÿè´Ÿé¢å½±å“çš„ä»·å€¼å¤±é…(Value Misalignment)æƒ…å†µï¼Œä¸ºå¯¹è¯æ™ºèƒ½ä½“æä¾›å•†åº”å¯¹é“å¾·æŒ‘æˆ˜å’Œå®ç°æ›´å…·åœºæ™¯æ•æ„Ÿæ€§çš„ä»·å€¼å¯¹é½æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Thirty-Third European Conference on Information Systems (ECIS 2025), Amman, Jordan",
      "pdf_url": "https://arxiv.org/pdf/2507.21091v1",
      "published_date": "2025-06-26 12:57:07 UTC",
      "updated_date": "2025-06-26 12:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:51:55.513511+00:00"
    },
    {
      "arxiv_id": "2507.02932v1",
      "title": "MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework",
      "title_zh": "MolProphecyï¼šé€šè¿‡å¤šæ¨¡æ€æ¡†æ¶è¿æ¥è¯ç‰©åŒ–å­¦å®¶çŸ¥è¯†ä¸åˆ†å­é¢„è®­ç»ƒæ¨¡å‹",
      "authors": [
        "Jianping Zhao",
        "Qiong Zhou",
        "Tian Wang",
        "Yusi Fan",
        "Qian Yang",
        "Li Jiao",
        "Chang Liu",
        "Zhehao Guo",
        "Qi Lu",
        "Fengfeng Zhou",
        "Ruochi Zhang"
      ],
      "abstract": "MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to integrate chemists' domain knowledge into molecular property prediction models. While molecular pre-trained models have enabled significant gains in predictive accuracy, they often fail to capture the tacit, interpretive reasoning central to expert-driven molecular design. To address this, MolProphecy employs ChatGPT as a virtual chemist to simulate expert-level reasoning and decision-making. The generated chemist knowledge is embedded by the large language model (LLM) as a dedicated knowledge representation and then fused with graph-based molecular features through a gated cross-attention mechanism, enabling joint reasoning over human-derived and structural features. Evaluated on four benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis reveals that chemist knowledge and structural features provide complementary contributions, improving both accuracy and interpretability. MolProphecy offers a practical and generalizable approach for collaborative drug discovery, with the flexibility to incorporate real chemist input in place of the current simulated proxy--without the need for model retraining. The implementation is publicly available at https://github.com/zhangruochi/MolProphecy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º MolProphecy çš„äººæœºåä½œ(human-in-the-loop, HITL)å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨å°†è¯ç‰©åŒ–å­¦å®¶çš„é¢†åŸŸçŸ¥è¯†æ•´åˆè¿›åˆ†å­æ€§è´¨é¢„æµ‹æ¨¡å‹ä¸­ï¼Œä»¥å¼¥è¡¥ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸“å®¶çº§è§£é‡Šæ€§æ¨ç†æ–¹é¢çš„çŸ­æ¿ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ ChatGPT ä½œä¸ºè™šæ‹ŸåŒ–å­¦å®¶æ¨¡æ‹Ÿä¸“å®¶çš„å†³ç­–é€»è¾‘ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)å°†ç”Ÿæˆçš„çŸ¥è¯†è½¬åŒ–ä¸ºä¸“ç”¨åµŒå…¥è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨é—¨æ§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶(gated cross-attention)å°†å…¶ä¸åˆ†å­å›¾ç‰¹å¾è¿›è¡Œæ·±åº¦èåˆã€‚åœ¨ FreeSolvã€BACEã€SIDER å’Œ ClinTox å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMolProphecy æ˜¾è‘—ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ï¼Œå…¶ä¸­åœ¨ FreeSolv ä¸Š RMSE é™ä½äº† 15.0%ï¼Œåœ¨ BACE ä¸Š AUROC æå‡äº† 5.39%ã€‚åˆ†æå‘ç°ï¼ŒåŒ–å­¦å®¶çŸ¥è¯†ä¸åˆ†å­ç»“æ„ç‰¹å¾å…·æœ‰äº’è¡¥æ€§ï¼Œåœ¨æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ä¹Ÿå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ç›´æ¥å¼•å…¥çœŸå®åŒ–å­¦å®¶çš„è¾“å…¥ï¼Œä¸ºè¯ç‰©å‘ç°é¢†åŸŸçš„äººæœºååŒæä¾›äº†ä¸€ç§å®ç”¨ä¸”å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages,7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.02932v1",
      "published_date": "2025-06-26 12:51:59 UTC",
      "updated_date": "2025-06-26 12:51:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:17.289471+00:00"
    },
    {
      "arxiv_id": "2506.21184v1",
      "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
      "title_zh": "é¢å‘ç»æµé«˜æ•ˆé•¿è§†é¢‘ç†è§£çš„ä»»åŠ¡æ„ŸçŸ¥ KV å‹ç¼©",
      "authors": [
        "Minghao Qin",
        "Yan Shu",
        "Peitian Zhang",
        "Kun Lun",
        "Huaying Yuan",
        "Juenjie Zhou",
        "Shitao Xiao",
        "Bo Zhao",
        "Zheng Liu"
      ],
      "abstract": "Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬ä»¥åŠé«˜å‹ç¼©æ¯”ä¸‹çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œæå‡ºäº†åä¸º Video-X^2L çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŒå±‚ KV å‹ç¼©ï¼ˆbi-level KV compressionï¼‰æœºåˆ¶ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µåŒæ—¶ç”Ÿæˆç”¨äºæ•æ‰ç»†èŠ‚çš„ä½å‹ç¼© KVï¼ˆL-KVsï¼‰å’Œæä¾›ç´§å‡‘è¡¨ç¤ºçš„é«˜å‹ç¼© KVï¼ˆH-KVsï¼‰ã€‚åœ¨è§£ç é˜¶æ®µï¼Œé€šè¿‡é€‰æ‹©æ€§ KV é‡åŠ è½½ï¼ˆselective KV re-loadingï¼‰ç­–ç•¥ï¼Œç³»ç»Ÿèƒ½é’ˆå¯¹å…³é”®è§†é¢‘å—åŠ è½½ L-KVsï¼Œè€Œå¯¹å…¶ä»–éƒ¨åˆ†ä½¿ç”¨ H-KVsï¼Œä»è€Œåœ¨ç»´æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ä¿ç•™ä»»åŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚Video-X^2L å…·æœ‰æ— éœ€é¢å¤–è®­ç»ƒä¸”ç›´æ¥å…¼å®¹ç°æœ‰ MLLMs çš„ä¼˜åŠ¿ã€‚åœ¨ VideoMMEã€MLVU ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œæ€§èƒ½è¡¨ç°å¤§å¹…é¢†å…ˆäºç°æœ‰çš„ KV å‹ç¼©æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.21184v1",
      "published_date": "2025-06-26 12:43:43 UTC",
      "updated_date": "2025-06-26 12:43:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:16.170942+00:00"
    },
    {
      "arxiv_id": "2506.21182v1",
      "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks",
      "title_zh": "ç»´æŠ¤ MTEBï¼šè‡´åŠ›äºæå‡åµŒå…¥åŸºå‡†çš„é•¿æœŸå¯ç”¨æ€§ä¸å¯å¤ç°æ€§",
      "authors": [
        "Isaac Chung",
        "Imene Kerboua",
        "Marton Kardos",
        "Roman Solomatin",
        "Kenneth Enevoldsen"
      ],
      "abstract": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation platform for text embedding models. While previous work has established the core benchmark methodology, this paper focuses on the engineering aspects that ensure MTEB's continued reproducibility and extensibility. We present our approach to maintaining robust continuous integration pipelines that validate dataset integrity, automate test execution, and assess benchmark results' generalizability. We detail the design choices that collectively enhance reproducibility and usability. Furthermore, we discuss our strategies for handling community contributions and extending the benchmark with new tasks and datasets. These engineering practices have been instrumental in scaling MTEB to become more comprehensive while maintaining quality and, ultimately, relevance to the field. Our experiences offer valuable insights for benchmark maintainers facing similar challenges in ensuring reproducibility and usability in machine learning evaluation frameworks. The MTEB repository is available at: https://github.com/embeddings-benchmark/mteb",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ç»´æŒ Massive Text Embedding Benchmark (MTEB) çš„é•¿æœŸå¯ç”¨æ€§ä¸å¯é‡å¤æ€§ï¼Œé‡ç‚¹å…³æ³¨äº†ç¡®ä¿è¯„ä¼°æ¡†æ¶æŒç»­æ¼”è¿›çš„å·¥ç¨‹åŒ–æ–¹æ³•ã€‚ä½œè€…è¯¦ç»†ä»‹ç»äº†é€šè¿‡å»ºç«‹ç¨³å¥çš„æŒç»­é›†æˆ (Continuous Integration) æµæ°´çº¿æ¥éªŒè¯æ•°æ®é›†å®Œæ•´æ€§ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œä»¥åŠè¯„ä¼°ç»“æœæ³›åŒ–æ€§çš„å…·ä½“åšæ³•ã€‚æ–‡ç« æ·±å…¥é˜è¿°äº†æ—¨åœ¨å¢å¼ºå¯é‡å¤æ€§å’Œæ˜“ç”¨æ€§çš„è®¾è®¡é€‰æ‹©ï¼Œå¹¶åˆ†äº«äº†æœ‰æ•ˆå¤„ç†ç¤¾åŒºè´¡çŒ®ä»¥åŠæ‰©å±•æ–°ä»»åŠ¡å’Œæ•°æ®é›†çš„ç­–ç•¥ã€‚è¿™äº›å·¥ç¨‹å®è·µåœ¨ MTEB æ‰©å±•è§„æ¨¡çš„è¿‡ç¨‹ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½¿å…¶åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶æ›´å…·é¢†åŸŸå½±å“åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºé¢ä¸´ç±»ä¼¼æŒ‘æˆ˜çš„æœºå™¨å­¦ä¹ è¯„ä¼°æ¡†æ¶ç»´æŠ¤è€…æä¾›äº†å…³äºç¡®ä¿ç³»ç»Ÿç¨³å¥æ€§ä¸å¯æ‰©å±•æ€§çš„é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21182v1",
      "published_date": "2025-06-26 12:40:48 UTC",
      "updated_date": "2025-06-26 12:40:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:33.926184+00:00"
    },
    {
      "arxiv_id": "2506.21167v1",
      "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection",
      "title_zh": "ä¸€ç§é¢å‘å°‘æ•°ç±»ä¹å™¨æ£€æµ‹çš„å±‚çº§æ·±åº¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Dylan Sechet",
        "Francesca Bugiotti",
        "Matthieu Kowalski",
        "Edouard d'HÃ©rouville",
        "Filip Langiewicz"
      ],
      "abstract": "Identifying instrument activities within audio excerpts is vital in music information retrieval, with significant implications for music cataloging and discovery. Prior deep learning endeavors in musical instrument recognition have predominantly emphasized instrument classes with ample data availability. Recent studies have demonstrated the applicability of hierarchical classification in detecting instrument activities in orchestral music, even with limited fine-grained annotations at the instrument level. Based on the Hornbostel-Sachs classification, such a hierarchical classification system is evaluated using the MedleyDB dataset, renowned for its diversity and richness concerning various instruments and music genres. This work presents various strategies to integrate hierarchical structures into models and tests a new class of models for hierarchical music prediction. This study showcases more reliable coarse-level instrument detection by bridging the gap between detailed instrument identification and group-level recognition, paving the way for further advancements in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éŸ³ä¹ä¿¡æ¯æ£€ç´¢(Music Information Retrieval)é¢†åŸŸä¸­è¯†åˆ«éŸ³é¢‘ç‰‡æ®µå†…ä¹å™¨æ´»åŠ¨çš„å…³é”®é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ•°æ®é‡è¾ƒå°‘çš„å°‘æ•°ä¹å™¨ç±»åˆ«ä¸Šè¡¨ç°å—é™çš„æŒ‘æˆ˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚æ·±åº¦å­¦ä¹ (Hierarchical Deep Learning)æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºHornbostel-Sachsåˆ†ç±»æ³•æ„å»ºçš„åˆ†å±‚ç³»ç»Ÿæ¥å¢å¼ºå¯¹ç®¡å¼¦ä¹å™¨æ´»åŠ¨çš„æ£€æµ‹èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å«ä¸°å¯Œä¹å™¨å¤šæ ·æ€§çš„MedleyDBæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å¤šç§å°†å±‚æ¬¡ç»“æ„é›†æˆåˆ°æ¨¡å‹ä¸­çš„ç­–ç•¥ï¼ŒåŒæ—¶æµ‹è¯•äº†ä¸€ç±»æ–°å‹çš„å±‚æ¬¡åŒ–éŸ³ä¹é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡å¼¥åˆè¯¦ç»†ä¹å™¨è¯†åˆ«ä¸ç»„åˆ«çº§è¯†åˆ«ä¹‹é—´çš„å·®è·ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´å¯é çš„ç²—ç²’åº¦ä¹å™¨æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§åˆ†å±‚åˆ†ç±»æ–¹æ³•å³ä½¿åœ¨ç»†ç²’åº¦æ ‡æ³¨æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæå‡æ£€æµ‹å‡†ç¡®æ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–éŸ³ä¹ç¼–ç›®å’Œå‘ç°æŠ€æœ¯æä¾›äº†æ–°çš„æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "International Conference on Digital Audio Effects (DAFx)",
      "pdf_url": "https://arxiv.org/pdf/2506.21167v1",
      "published_date": "2025-06-26 11:56:11 UTC",
      "updated_date": "2025-06-26 11:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:27.280646+00:00"
    },
    {
      "arxiv_id": "2506.21162v1",
      "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation",
      "title_zh": "ä¸€ç§å°†ä¸‰ç»´è¶…å£°é›†æˆäºç»çš®è‚è‚¿ç˜¤æ¶ˆèçš„æ–°å‹æ¡†æ¶",
      "authors": [
        "Shuwei Xing",
        "Derek W. Cool",
        "David Tessier",
        "Elvis C. S. Chen",
        "Terry M. Peters",
        "Aaron Fenster"
      ],
      "abstract": "3D ultrasound (US) imaging has shown significant benefits in enhancing the outcomes of percutaneous liver tumour ablation. Its clinical integration is crucial for transitioning 3D US into the therapeutic domain. However, challenges of tumour identification in US images continue to hinder its broader adoption. In this work, we propose a novel framework for integrating 3D US into the standard ablation workflow. We present a key component, a clinically viable 2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to reduce registration complexity. To facilitate efficient verification of the registration workflow, we also propose an intuitive multimodal image visualization technique. In our study, 2D US-CT/MRI registration achieved a landmark distance error of approximately 2-4 mm with a runtime of 0.22s per image pair. Additionally, non-rigid registration reduced the mean alignment error by approximately 40% compared to rigid registration. Results demonstrated the efficacy of the proposed 2D US-CT/MRI registration workflow. Our integration framework advanced the capabilities of 3D US imaging in improving percutaneous tumour ablation, demonstrating the potential to expand the therapeutic role of 3D US in clinical interventions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°† 3D ultrasound (US) é›†æˆåˆ°æ ‡å‡†ç»çš®è‚è„è‚¿ç˜¤æ¶ˆèå·¥ä½œæµä¸­çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‚¿ç˜¤è¯†åˆ«å›°éš¾åŠä¸´åºŠé›†æˆå—é™çš„é—®é¢˜ã€‚æ ¸å¿ƒç»„ä»¶é‡‡ç”¨äº†ä¸€ç§ä¸´åºŠå¯è¡Œçš„ 2D US-CT/MRI æ³¨å†Œæ–¹æ³•ï¼Œé€šè¿‡å°† 3D US ä½œä¸ºä¸­é—´åª’ä»‹æ¥é™ä½é…å‡†è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚ä¸ºäº†å®ç°å·¥ä½œæµçš„é«˜æ•ˆéªŒè¯ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç›´è§‚çš„å¤šæ¨¡æ€å›¾åƒå¯è§†åŒ–æŠ€æœ¯ (multimodal image visualization technique)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ2D US-CT/MRI æ³¨å†Œçš„æ ‡è®°ç‚¹è·ç¦»è¯¯å·®æ§åˆ¶åœ¨ 2-4 mm ä¹‹é—´ï¼Œä¸”æ¯å¯¹å›¾åƒçš„å¤„ç†æ—¶é—´ä»…ä¸º 0.22sã€‚æ­¤å¤–ï¼Œéåˆšæ€§æ³¨å†Œ (non-rigid registration) ç›¸æ¯”äºåˆšæ€§æ³¨å†Œä½¿å¹³å‡å¯¹é½è¯¯å·®é™ä½äº†çº¦ 40%ã€‚è¯¥æ¡†æ¶å¢å¼ºäº† 3D US æˆåƒåœ¨æ”¹å–„ç»çš®è‚¿ç˜¤æ¶ˆèæ‰‹æœ¯ä¸­çš„èƒ½åŠ›ï¼Œä¸ºæ‰©å¤§ 3D US åœ¨ä¸´åºŠå¹²é¢„æ²»ç–—ä¸­çš„åº”ç”¨æ½œåŠ›æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.21162v1",
      "published_date": "2025-06-26 11:39:08 UTC",
      "updated_date": "2025-06-26 11:39:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:30.520974+00:00"
    },
    {
      "arxiv_id": "2506.21154v1",
      "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation",
      "title_zh": "åŸºäº Transformer çš„æ—¶ç©ºåäº‹å®ç»“æœä¼°è®¡",
      "authors": [
        "He Li",
        "Haoang Chi",
        "Mingyu Liu",
        "Wanrong Huang",
        "Liyang Xu",
        "Wenjing Yang"
      ],
      "abstract": "The real world naturally has dimensions of time and space. Therefore, estimating the counterfactual outcomes with spatial-temporal attributes is a crucial problem. However, previous methods are based on classical statistical models, which still have limitations in performance and generalization. This paper proposes a novel framework for estimating counterfactual outcomes with spatial-temporal attributes using the Transformer, exhibiting stronger estimation ability. Under mild assumptions, the proposed estimator within this framework is consistent and asymptotically normal. To validate the effectiveness of our approach, we conduct simulation experiments and real data experiments. Simulation experiments show that our estimator has a stronger estimation capability than baseline methods. Real data experiments provide a valuable conclusion to the causal effect of conflicts on forest loss in Colombia. The source code is available at https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.",
      "tldr_zh": "ç°å®ä¸–ç•Œå¤©ç„¶å…·æœ‰æ—¶é—´å’Œç©ºé—´ç»´åº¦ï¼Œå› æ­¤ä¼°ç®—å…·æœ‰æ—¶ç©ºå±æ€§çš„åäº‹å®ç»“æœ(counterfactual outcomes)æ˜¯å› æœæ¨æ–­ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹åœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ–°å‹ä¼°ç®—æ¡†æ¶ï¼Œå±•ç°å‡ºæ›´å¼ºçš„å¤æ‚ç‰¹å¾æ•æ‰èƒ½åŠ›ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œè¯¥æ¡†æ¶ä¸‹çš„ä¼°è®¡é‡(estimator)åœ¨æ¸©å’Œå‡è®¾ä¸‹è¢«è¯æ˜å…·æœ‰ä¸€è‡´æ€§(consistent)å’Œæ¸è¿‘æ­£æ€æ€§(asymptotically normal)ã€‚å®éªŒéªŒè¯éƒ¨åˆ†åŒ…æ‹¬æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¼°ç®—èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å¯¹å“¥ä¼¦æ¯”äºšæ­¦è£…å†²çªä¸æ£®æ—æµå¤±ä¹‹é—´å› æœæ•ˆåº”çš„å®é™…æ¡ˆä¾‹åˆ†æï¼Œè¯¥ç ”ç©¶å¾—å‡ºäº†å…·æœ‰ç§‘ç ”ä»·å€¼çš„ç»“è®ºã€‚è¯¥æ¡†æ¶ä¸ºåˆ©ç”¨æ·±åº¦å­¦ä¹ å¤„ç†æ—¶ç©ºå±æ€§çš„åäº‹å®æ¨æ–­æä¾›äº†å…¨æ–°çš„ç†è®ºæ”¯æ’‘å’Œå®è·µèŒƒå¼ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "comment": "24 pages, accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21154v1",
      "published_date": "2025-06-26 11:24:46 UTC",
      "updated_date": "2025-06-26 11:24:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:49.612842+00:00"
    },
    {
      "arxiv_id": "2506.21151v1",
      "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels",
      "title_zh": "å™ªå£°æ ‡ç­¾ä¸‹åŸºäºé²æ£’æ·±åº¦å­¦ä¹ çš„å¿ƒè„ MRI å¿ƒè‚Œç˜¢ç—•åˆ†å‰²",
      "authors": [
        "Aida Moafi",
        "Danial Moafi",
        "Evgeny M. Mirkes",
        "Gerry P. McCann",
        "Abbas S. Alatrany",
        "Jayanth R. Arnold",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "The accurate segmentation of myocardial scars from cardiac MRI is essential for clinical assessment and treatment planning. In this study, we propose a robust deep-learning pipeline for fully automated myocardial scar detection and segmentation by fine-tuning state-of-the-art models. The method explicitly addresses challenges of label noise from semi-automatic annotations, data heterogeneity, and class imbalance through the use of Kullback-Leibler loss and extensive data augmentation. We evaluate the model's performance on both acute and chronic cases and demonstrate its ability to produce accurate and smooth segmentations despite noisy labels. In particular, our approach outperforms state-of-the-art models like nnU-Net and shows strong generalizability in an out-of-distribution test set, highlighting its robustness across various imaging conditions and clinical tasks. These results establish a reliable foundation for automated myocardial scar quantification and support the broader clinical adoption of deep learning in cardiac imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ·±åº¦å­¦ä¹ æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒå…ˆè¿›æ¨¡å‹å®ç°å¿ƒè„ MRI ä¸­ Myocardial Scar Segmentation çš„å…¨è‡ªåŠ¨å¤„ç†ã€‚è¯¥æ–¹æ³•é’ˆå¯¹åŠè‡ªåŠ¨æ ‡æ³¨ä¸­çš„ label noiseã€æ•°æ®å¼‚æ„æ€§å’Œç±»åˆ«ä¸å¹³è¡¡æŒ‘æˆ˜ï¼Œé‡‡ç”¨äº† Kullback-Leibler loss å’Œå¤§è§„æ¨¡æ•°æ®å¢å¼ºæŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†æ€¥æ€§ä¸æ…¢æ€§ç—…ä¾‹æ—¶å‡èƒ½ç”Ÿæˆç²¾ç¡®ä¸”å¹³æ»‘çš„åˆ†å‰²ç»“æœï¼Œå…¶æ€§èƒ½ä¼˜äº nnU-Net ç­‰ä¸»æµæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ out-of-distribution æµ‹è¯•é›†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæˆåƒæ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚è¿™äº›å‘ç°ä¸ºå¿ƒè‚Œç˜¢ç—•çš„è‡ªåŠ¨å®šé‡åˆ†ææä¾›äº†å¯é å·¥å…·ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ åœ¨å¿ƒè„å½±åƒä¸´åºŠåº”ç”¨ä¸­çš„è½åœ°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21151v1",
      "published_date": "2025-06-26 11:21:58 UTC",
      "updated_date": "2025-06-26 11:21:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:42.592616+00:00"
    },
    {
      "arxiv_id": "2506.21146v1",
      "title": "Linearity-based neural network compression",
      "title_zh": "åŸºäºçº¿æ€§åº¦çš„ç¥ç»ç½‘ç»œå‹ç¼©",
      "authors": [
        "Silas Dobler",
        "Florian Lemmerich"
      ],
      "abstract": "In neural network compression, most current methods reduce unnecessary parameters by measuring importance and redundancy. To augment already highly optimized existing solutions, we propose linearity-based compression as a novel way to reduce weights in a neural network. It is based on the intuition that with ReLU-like activation functions, neurons that are almost always activated behave linearly, allowing for merging of subsequent layers. We introduce the theory underlying this compression and evaluate our approach experimentally. Our novel method achieves a lossless compression down to 1/4 of the original model size in over the majority of tested models. Applying our method on already importance-based pruned models shows very little interference between different types of compression, demonstrating the option of successful combination of techniques. Overall, our work lays the foundation for a new type of compression method that enables smaller and ultimately more efficient neural network models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Linearity-based neural network compression çš„æ–°å‹å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨ç¥ç»ç½‘ç»œä¸­çš„çº¿æ€§ç‰¹æ€§æ¥å‡å°‘æƒé‡ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒç›´è§‰åœ¨äºï¼Œåœ¨ä½¿ç”¨ ReLU-like æ¿€æ´»å‡½æ•°æ—¶ï¼Œå‡ ä¹æ€»æ˜¯è¢«æ¿€æ´»çš„ç¥ç»å…ƒè¡¨ç°å‡ºçº¿æ€§è¡Œä¸ºï¼Œä»è€Œå…è®¸å°†åç»­å±‚è¿›è¡Œåˆå¹¶ã€‚ä½œè€…è¯¦ç»†ä»‹ç»äº†è¯¥å‹ç¼©æŠ€æœ¯çš„ç†è®ºæ¡†æ¶ï¼Œå¹¶è¿›è¡Œäº†ç³»ç»Ÿçš„å®éªŒè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¤šæ•°æµ‹è¯•æ¨¡å‹ä¸­å®ç°äº† Lossless compressionï¼Œæ¨¡å‹å¤§å°æœ€é«˜å¯ç¼©å‡è‡³åŸå§‹è§„æ¨¡çš„ 1/4ã€‚æ­¤å¤–ï¼Œå°†è¯¥æ–¹æ³•åº”ç”¨äºå·²ç»è¿‡ Importance-based pruning çš„æ¨¡å‹æ—¶ï¼Œä¸åŒç±»å‹çš„å‹ç¼©æŠ€æœ¯ä¹‹é—´å‡ ä¹æ²¡æœ‰å¹²æ‰°ï¼Œè¯æ˜äº†æŠ€æœ¯ç»„åˆçš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘æ›´å°ã€æ›´é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¨¡å‹å¥ å®šäº†æ–°å‹å‹ç¼©æ–¹æ³•çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21146v1",
      "published_date": "2025-06-26 11:04:12 UTC",
      "updated_date": "2025-06-26 11:04:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:42.930159+00:00"
    },
    {
      "arxiv_id": "2506.21142v1",
      "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks",
      "title_zh": "æ— äººæœºç½‘ç»œæ”»å‡»çš„ç”Ÿæˆå¼å¯¹æŠ—é€ƒé€¸ä¸åˆ†å¸ƒå¤–æ£€æµ‹",
      "authors": [
        "Deepak Kumar Panda",
        "Weisi Guo"
      ],
      "abstract": "The growing integration of UAVs into civilian airspace underscores the need for resilient and intelligent intrusion detection systems (IDS), as traditional anomaly detection methods often fail to identify novel threats. A common approach treats unfamiliar attacks as out-of-distribution (OOD) samples; however, this leaves systems vulnerable when mitigation is inadequate. Moreover, conventional OOD detectors struggle to distinguish stealthy adversarial attacks from genuine OOD events. This paper introduces a conditional generative adversarial network (cGAN)-based framework for crafting stealthy adversarial attacks that evade IDS mechanisms. We first design a robust multi-class IDS classifier trained on benign UAV telemetry and known cyber-attacks, including Denial of Service (DoS), false data injection (FDI), man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN perturbs known attacks to generate adversarial samples that misclassify as benign while retaining statistical resemblance to OOD distributions. These adversarial samples are iteratively refined to achieve high stealth and success rates. To detect such perturbations, we implement a conditional variational autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based regret scores significantly outperform traditional Mahalanobis distance-based detectors in identifying stealthy adversarial threats. Our findings emphasize the importance of advanced probabilistic modeling to strengthen IDS capabilities against adaptive, generative-model-based cyber intrusions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨æ°‘ç”¨é¢†åŸŸé¢ä¸´çš„ç½‘ç»œå®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(cGAN)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆèƒ½å¤Ÿé€ƒé¿å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)çš„éšè”½å¯¹æŠ—æ”»å‡»ã€‚ä½œè€…é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªæ¶µç›–æ‹’ç»æœåŠ¡(DoS)ã€è™šå‡æ•°æ®æ³¨å…¥(FDI)ã€ä¸­é—´äººæ”»å‡»(MiTM)å’Œé‡æ”¾æ”»å‡»çš„å¤šåˆ†ç±»IDSåˆ†ç±»å™¨ï¼Œå¹¶åˆ©ç”¨cGANå¯¹å·²çŸ¥æ”»å‡»è¿›è¡Œæ‰°åŠ¨ï¼Œä½¿å…¶åœ¨ä¿ç•™åˆ†å¸ƒå¤–(OOD)ç‰¹å¾çš„åŒæ—¶è¢«è¯¯åˆ¤ä¸ºæ­£å¸¸ã€‚ä¸ºäº†æœ‰æ•ˆé˜²å¾¡æ­¤ç±»æ”»å‡»ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAE)çš„æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶(Negative Log-Likelihood)æˆåŠŸå°†éšè”½çš„å¯¹æŠ—æ ·æœ¬ä¸çœŸå®çš„OODäº‹ä»¶åŒºåˆ†å¼€æ¥ã€‚å¯¹æ¯”è¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºCVAEçš„æ–¹æ³•åœ¨è¯†åˆ«æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é©¬æ°è·ç¦»(Mahalanobis distance)æ£€æµ‹å™¨ã€‚è¯¥ç ”ç©¶ç»“æœçªæ˜¾äº†å…ˆè¿›æ¦‚ç‡å»ºæ¨¡åœ¨åº”å¯¹åŸºäºç”Ÿæˆæ¨¡å‹çš„è‡ªé€‚åº”ç½‘ç»œå…¥ä¾µã€å¢å¼ºIDSå¼¹æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21142v1",
      "published_date": "2025-06-26 10:56:34 UTC",
      "updated_date": "2025-06-26 10:56:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:44.423071+00:00"
    },
    {
      "arxiv_id": "2506.21140v2",
      "title": "DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding",
      "title_zh": "DBConformerï¼šç”¨äºè„‘ç”µè§£ç çš„åŒåˆ†æ”¯å·ç§¯ Transformer",
      "authors": [
        "Ziwei Wang",
        "Hongbin Wang",
        "Tianwang Jia",
        "Xingyi He",
        "Siyang Li",
        "Dongrui Wu"
      ],
      "abstract": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform spontaneous/evoked neural activity into control commands for external communication. While convolutional neural networks (CNNs) remain the mainstream backbone for EEG decoding, their inherently short receptive field makes it difficult to capture long-range temporal dependencies and global inter-channel relationships. Recent CNN-Transformer (Conformer) hybrids partially address this issue, but most adopt a serial design, resulting in suboptimal integration of local and global features, and often overlook explicit channel-wise modeling. To address these limitations, we propose DBConformer, a dual-branch convolutional Transformer network tailored for EEG decoding. It integrates a temporal Conformer to model long-range temporal dependencies and a spatial Conformer to extract inter-channel interactions, capturing both temporal dynamics and spatial patterns in EEG signals. A lightweight channel attention module further refines spatial representations by assigning data-driven importance to EEG channels. Extensive experiments under four evaluation settings on three paradigms, including motor imagery, seizure detection, and steady-state visual evoked potential, demonstrated that DBConformer consistently outperformed 13 competitive baseline models, with over an eight-fold reduction in parameters than current high-capacity EEG Conformer architecture. Furthermore, the visualization results confirmed that the features extracted by DBConformer are physiologically interpretable and aligned with prior knowledge. The superior performance and interpretability of DBConformer make it reliable for accurate, robust, and explainable EEG decoding. Code is publicized at https://github.com/wzwvv/DBConformer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºè„‘ç”µå›¾(Electroencephalography, EEG)çš„è„‘æœºæ¥å£(BCIs)è§£ç ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDBConformerçš„åŒåˆ†æ”¯å·ç§¯Transformer(Dual-branch Convolutional Transformer)ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)éš¾ä»¥æ•æ‰é•¿ç¨‹æ—¶é—´ä¾èµ–å’Œå…¨å±€é€šé“é—´å…³ç³»çš„é—®é¢˜ã€‚DBConformeré€šè¿‡é›†æˆæ—¶é—´Conformer(Temporal Conformer)å’Œç©ºé—´Conformer(Spatial Conformer)ï¼Œèƒ½å¤ŸåŒæ—¶å»ºæ¨¡é•¿ç¨‹æ—¶é—´åŠ¨æ€å’Œå¤æ‚çš„é€šé“é—´äº¤äº’æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€šé“æ³¨æ„åŠ›æ¨¡å—(Channel Attention Module)ï¼Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ–¹å¼ä¼˜åŒ–ç©ºé—´è¡¨å¾ã€‚åœ¨è¿åŠ¨æƒ³è±¡(Motor Imagery)ã€ç™«ç—«æ£€æµ‹(Seizure Detection)å’Œç¨³æ€è§†è§‰è¯±å‘ç”µä½(SSVEP)ç­‰å¤šé¡¹å®éªŒä¸­ï¼ŒDBConformerçš„æ€§èƒ½å‡æ˜¾è‘—ä¼˜äº13ç§ä¸»æµåŸºçº¿æ¨¡å‹ï¼Œä¸”å…¶å‚æ•°é‡è¾ƒç°æœ‰é«˜æ€§èƒ½Conformeræ¶æ„å‡å°‘äº†8å€ä»¥ä¸Šã€‚æœ€åï¼Œå¯è§†åŒ–ç»“æœéªŒè¯äº†è¯¥æ¨¡å‹æå–çš„ç‰¹å¾ç¬¦åˆç”Ÿç†å¯è§£é‡Šæ€§(Physiologically Interpretable)ï¼Œè¯æ˜äº†DBConformeråœ¨å®ç°å‡†ç¡®ã€é²æ£’ä¸”å¯è§£é‡Šçš„EEGè§£ç æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.21140v2",
      "published_date": "2025-06-26 10:53:24 UTC",
      "updated_date": "2025-09-19 19:25:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:52:47.481554+00:00"
    },
    {
      "arxiv_id": "2506.21138v1",
      "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE",
      "title_zh": "åˆæˆéœ€æ±‚çš„è´¨é‡è¯„ä»·ï¼šé¢å‘ AI4RE çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ•°æ®é›†è¯„ä¼°",
      "authors": [
        "Abdelkarim El-Hajjami",
        "Camille Salinesi"
      ],
      "abstract": "The shortage of publicly available, labeled requirements datasets remains a major barrier to advancing Artificial Intelligence for Requirements Engineering (AI4RE). While Large Language Models offer promising capabilities for synthetic data generation, systematic approaches to control and optimize the quality of generated requirements remain underexplored. This paper presents Synthline v1, an enhanced Product Line approach for generating synthetic requirements data that extends our earlier v0 version with advanced generation strategies and curation techniques. We investigate four research questions assessing how prompting strategies, automated prompt optimization, and post-generation curation affect data quality across four classification tasks: defect detection, functional vs. non-functional, quality vs. non-quality, and security vs. non-security. Our evaluation shows that multi-sample prompting significantly boosts both utility and diversity over single-sample generation, with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic Editing) for automated prompt optimization yields task-dependent results, greatly improving functional classification (+32.5 points) but reducing performance on others. Interestingly, similarity-based curation improves diversity but often harms classification performance, indicating that some redundancy may help ML models. Most importantly, our results show that synthetic requirements can match or outperform human-authored ones for specific tasks, with synthetic data surpassing human data for security (+7.8 points) and defect classification (+15.4 points). These findings offer practical insights for AI4RE and chart a viable path to mitigating dataset scarcity through systematic synthetic generation.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½é©±åŠ¨çš„éœ€æ±‚å·¥ç¨‹(AI4RE)ä¸­å…¬å¼€æ ‡è®°æ•°æ®é›†åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Synthline v1 æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆåˆæˆéœ€æ±‚æ•°æ®çš„å¢å¼ºå‹äº§å“çº¿(Product Line)æ–¹æ³•ã€‚ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†æç¤ºç­–ç•¥ã€åŸºäº PACE (Prompt Actor-Critic Editing) çš„è‡ªåŠ¨æç¤ºä¼˜åŒ–ä»¥åŠç”Ÿæˆåç­–å±•(Curation)å¯¹å››é¡¹éœ€æ±‚åˆ†ç±»ä»»åŠ¡çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ ·æœ¬æç¤º(Multi-sample Prompting)æ˜¾è‘—æå‡äº†æ•°æ®çš„æ•ˆç”¨ä¸å¤šæ ·æ€§ï¼Œä½¿ F1-score å¢ç›Šé«˜è¾¾44ç‚¹ã€‚è™½ç„¶ PACE ä¼˜åŒ–åœ¨åŠŸèƒ½åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å…¶ä»–ä»»åŠ¡ä¸Šå‘ˆç°å‡ºä»»åŠ¡ä¾èµ–æ€§ï¼Œè€ŒåŸºäºç›¸ä¼¼æ€§çš„ç­–å±•è™½èƒ½å¢åŠ å¤šæ ·æ€§ï¼Œå´å¯èƒ½æŸå®³åˆ†ç±»æ€§èƒ½ã€‚æœ€å…³é”®çš„å‘ç°æ˜¯ï¼Œåˆæˆéœ€æ±‚åœ¨å®‰å…¨å’Œç¼ºé™·åˆ†ç±»ä»»åŠ¡ä¸­åˆ†åˆ«æ¯”äººå·¥ç¼–å†™æ•°æ®é«˜å‡º7.8å’Œ15.4åˆ†ï¼Œè¯æ˜äº†åˆæˆæ•°æ®åœ¨ç‰¹å®šä»»åŠ¡ä¸­å…·æœ‰è¶…è¶Šäººå·¥æ•°æ®çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç¼“è§£ AI4RE é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†ç³»ç»ŸåŒ–çš„ç”Ÿæˆè·¯å¾„å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21138v1",
      "published_date": "2025-06-26 10:52:07 UTC",
      "updated_date": "2025-06-26 10:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:07.306143+00:00"
    },
    {
      "arxiv_id": "2506.21129v1",
      "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks",
      "title_zh": "è§‚æµ‹ç©ºé—´æ”»å‡»ä¸‹å®‰å…¨æ— äººæœºå†²çªæ¶ˆè§£çš„è¯¾ç¨‹å¼•å¯¼åè„†å¼±å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Deepak Kumar Panda",
        "Adolfo Perrusquia",
        "Weisi Guo"
      ],
      "abstract": "Reinforcement learning (RL) policies deployed in safety-critical systems, such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation space. These attacks induce distributional shifts that significantly degrade value estimation, leading to unsafe or suboptimal decision making rendering the existing policy fragile. To address this vulnerability, we propose an antifragile RL framework designed to adapt against curriculum of incremental adversarial perturbations. The framework introduces a simulated attacker which incrementally increases the strength of observation-space perturbations which enables the RL agent to adapt and generalize across a wider range of OOD observations and anticipate previously unseen attacks. We begin with a theoretical characterization of fragility, formally defining catastrophic forgetting as a monotonic divergence in value function distributions with increasing perturbation strength. Building on this, we define antifragility as the boundedness of such value shifts and derive adaptation conditions under which forgetting is stabilized. Our method enforces these bounds through iterative expert-guided critic alignment using Wasserstein distance minimization across incrementally perturbed observations. We empirically evaluate the approach in a UAV deconfliction scenario involving dynamic 3D obstacles. Results show that the antifragile policy consistently outperforms standard and robust RL baselines when subjected to both projected gradient descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative reward and over 30% fewer conflict events. These findings demonstrate the practical and theoretical viability of antifragile reinforcement learning for secure and resilient decision-making in environments with evolving threat scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨åŠ¨æ€ç©ºåŸŸå¯¼èˆªä¸­æ˜“å—è§‚æµ‹ç©ºé—´(observation-space)å¯¹æŠ—æ”»å‡»å¯¼è‡´æ€§èƒ½ä¸‹é™çš„è„†å¼±æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¯¾ç¨‹å¼•å¯¼çš„åè„†å¼±å¼ºåŒ–å­¦ä¹ (Antifragile Reinforcement Learning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æ¨¡æ‹Ÿæ”»å‡»è€…ä»¥å¢é‡æ–¹å¼æå‡æ‰°åŠ¨å¼ºåº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨è¶…å‡ºåˆ†å¸ƒ(OOD)çš„è§‚æµ‹ä¸­è¿›è¡Œæ³›åŒ–å¹¶é¢„åˆ¤æœªè§æ”»å‡»ã€‚ç ”ç©¶ä»ç†è®ºä¸Šå°†è„†å¼±æ€§å®šä¹‰ä¸ºä»·å€¼å‡½æ•°åˆ†å¸ƒçš„å•è°ƒå‘æ•£ï¼Œå¹¶åˆ©ç”¨Wassersteinè·ç¦»æœ€å°åŒ–æŠ€æœ¯å®ç°è¿­ä»£çš„ä¸“å®¶å¼•å¯¼è¯„ä»·å™¨å¯¹é½(expert-guided critic alignment)ï¼Œä»è€Œå°†ä»·å€¼åç§»é™åˆ¶åœ¨åè„†å¼±æ€§å®šä¹‰çš„èŒƒå›´å†…ã€‚åœ¨åŒ…å«åŠ¨æ€3Déšœç¢ç‰©çš„æ— äººæœºå†²çªè§£è„±(UAV deconfliction)åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹æŠ•å½±æ¢¯åº¦ä¸‹é™(PGD)å’ŒGPSæ¬ºéª—(GPS spoofing)æ”»å‡»æ—¶ï¼Œç›¸è¾ƒäºæ ‡å‡†å’Œç¨³å¥å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œå…¶ç´¯ç§¯å¥–åŠ±æå‡äº†15%ä¸”å†²çªäº‹ä»¶å‡å°‘äº†30%ä»¥ä¸Šã€‚è¿™ä¸€æˆæœä¸ºåœ¨æ¼”åŒ–å¨èƒç¯å¢ƒä¸‹çš„å®‰å…¨ä¸å¤åŸå†³ç­–æä¾›äº†ç†è®ºä¸å®è·µæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21129v1",
      "published_date": "2025-06-26 10:10:41 UTC",
      "updated_date": "2025-06-26 10:10:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:11.121920+00:00"
    },
    {
      "arxiv_id": "2506.21127v3",
      "title": "Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace",
      "title_zh": "å¯¹æŠ—æ€§ç©ºåŸŸä¸‹æ— äººæœºå®‰å…¨é˜²å†²çªçš„å…ƒç­–ç•¥åˆ‡æ¢",
      "authors": [
        "Deepak Kumar Panda",
        "Weisi Guo"
      ],
      "abstract": "Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨å¯¹æŠ—æ€§é¢†ç©ºä¸­çš„å®‰å…¨å†²çªè§„é¿é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…ƒç­–ç•¥åˆ‡æ¢(meta-policy switching)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (RL)æ¨¡å‹åœ¨é¢å¯¹ä¼ æ„Ÿå™¨æ”»å‡»å’Œåˆ†å¸ƒå¤–(OOD)å¯¹æŠ—åç§»æ—¶éš¾ä»¥æ³›åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒé‡‡ç”¨äº†æŠ˜æ‰£æ±¤æ™®æ£®é‡‡æ ·(discounted Thompson sampling, DTS)æœºåˆ¶ï¼Œå°†åœ¨çº¿ç­–ç•¥é€‰æ‹©å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœº(multi-armed bandit)é—®é¢˜ï¼Œä»è€Œåœ¨å¤šç§é¢„å…ˆè®­ç»ƒçš„é²æ£’ç­–ç•¥ä¹‹é—´è¿›è¡ŒåŠ¨æ€åˆ‡æ¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå…ƒç­–ç•¥èƒ½å¤Ÿæ ¹æ®å®æ—¶çš„å¯¹æŠ—æ€§è§‚æµ‹è‡ªé€‚åº”åœ°ä¼˜åŒ–ç³»ç»Ÿå¼¹æ€§ï¼Œæœ‰æ•ˆåº”å¯¹å¤æ‚çš„æ‰°åŠ¨ç¯å¢ƒã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæœ€å°åŒ–é¢„æœŸé—æ†¾å€¼(regret)ï¼Œå¹¶åœ¨é«˜åº¦ä¸ç¡®å®šçš„ç¯å¢ƒä¸‹å±•ç°å‡ºåè„†å¼±(antifragile)ç‰¹æ€§ã€‚åœ¨å¤æ‚3Déšœç¢ç‰©ç¯å¢ƒä¸‹çš„ä»¿çœŸå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹æŠ•å½±æ¢¯åº¦ä¸‹é™(PGD)å’ŒGPSæ¬ºéª—ç­‰æ”»å‡»æ—¶ï¼Œå…¶å¯¼èˆªæ•ˆç‡å’Œå†²çªè§„é¿æˆåŠŸç‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œä¸ºè‡ªä¸»ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸å¯é æ€§æä¾›äº†é‡è¦ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21127v3",
      "published_date": "2025-06-26 10:06:29 UTC",
      "updated_date": "2025-12-13 03:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:15.195768+00:00"
    },
    {
      "arxiv_id": "2506.21119v1",
      "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models",
      "title_zh": "Progtuningï¼šåŸºäº Transformer çš„è¯­è¨€æ¨¡å‹æ¸è¿›å¼å¾®è°ƒæ¡†æ¶",
      "authors": [
        "Xiaoshuang Ji",
        "Zhendong Zhao",
        "Xiaojun Chen",
        "Xin Zhao",
        "Zeyao Liu"
      ],
      "abstract": "Fine-tuning is a promising technique for leveraging Transformer-based language models in downstream tasks. As model sizes continue to grow, updating all model parameters becomes increasingly costly. Parameter-efficient fine-tuning methods effectively address this issue by selectively updating a small subset of parameters. However, fine-tuning and most existing parameter-efficient fine-tuning methods require updating the same number of parameters as the initial size, ignoring the unequal contribution across Transformer blocks and leading to extremely inefficient allocation of computing resources. In this paper, we propose Progtuning, the novel fine-tuning framework combined with progressive learning for Transformer-based language models. Specifically, Progtuning progressively reduces the number of updated transformer blocks based on the contribution. Remarkably, Progtuning optimizes resource allocation and reduces the number of updated parameters by approximately 25\\%, while still maintaining competitive performance. And it also exhibits high adaptability with parameter-efficient fine-tuning methods, demonstrating excellent performance across various adaptation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer-based language models åœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒè¿‡ç¨‹ä¸­è®¡ç®—èµ„æºåˆ†é…æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Progtuning çš„æ¸è¿›å¼å¾®è°ƒæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿå¾®è°ƒå’Œç°æœ‰ parameter-efficient fine-tuning æ–¹æ³•ä¸åŒï¼ŒProgtuning ç»“åˆäº†æ¸è¿›å¼å­¦ä¹  (progressive learning) ç­–ç•¥ï¼Œæ ¹æ®ä¸åŒ transformer blocks å¯¹ä»»åŠ¡çš„è´¡çŒ®åº¦åŠ¨æ€ä¼˜åŒ–èµ„æºåˆ†é…ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥å‡å°‘éœ€è¦æ›´æ–°çš„æ¨¡å—æ•°é‡ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½ç«äº‰åŠ›çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†æ›´æ–°å‚æ•°é‡é™ä½äº†çº¦ 25%ã€‚å®éªŒè¯æ˜ï¼ŒProgtuning åœ¨å¤šç§é€‚é…åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸å„ç±» parameter-efficient fine-tuning æŠ€æœ¯ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å¤§æ¨¡å‹çš„å¾®è°ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICONIP 2024",
      "pdf_url": "https://arxiv.org/pdf/2506.21119v1",
      "published_date": "2025-06-26 09:37:15 UTC",
      "updated_date": "2025-06-26 09:37:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:14.016277+00:00"
    },
    {
      "arxiv_id": "2506.21116v2",
      "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes",
      "title_zh": "IPFormer-VideoLLMï¼šå¢å¼ºå¤šé•œå¤´åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€è§†é¢‘ç†è§£",
      "authors": [
        "Yujia Liang",
        "Jile Jiao",
        "Xuetao Feng",
        "Zixuan Ye",
        "Yuan Wang",
        "Zhicheng Wang"
      ],
      "abstract": "Video Large Language Models (VideoLLMs) have demonstrated remarkable understanding capabilities, but are found struggling to tackle multi-shot scenarios,e.g., video clips with varying camera angles or scene changes. This challenge can render failures such as instance identity forgetting and key frame negligence. In this work, we first attribute the challenge to the lack of multi-shot annotations among existing datasets and therefore we introduce a new dataset termed MultiClip-Bench, featuring dense descriptions and instruction-based question-answering pairs tailored for multi-shot scenarios. We empirically find that the training set significantly boosts the multi-shot performance, while the testing benchmark provides a reliable measure of the model capability in multi-shot scenarios. By further analyzing and discovering that current models only encode instance features in a discrete or lossy manner, at the risk of missing identity information, we then contribute a new model IPFormer-VideoLLM. Its key idea is the injection of instance-level features as instance prompts through an efficient attention-based connector. This allows for the aggregation of instance-specific information across scenes. Experiments demonstrate that our proposed dataset and model not only enhance the multi-scene video understanding significantly, but also offer distinct advantages across various video benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Video Large Language Models (VideoLLMs) åœ¨å¤„ç†å¤šé•œå¤´ (multi-shot) åœºæ™¯ï¼ˆå¦‚è§†è§’åˆ‡æ¢æˆ–åœºæ™¯å˜æ¢ï¼‰æ—¶å®¹æ˜“å‡ºç°å®ä¾‹èº«ä»½é—å¿˜å’Œå…³é”®å¸§å¿½ç•¥çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…é¦–å…ˆæ¨å‡ºäº†MultiClip-Benchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é’ˆå¯¹å¤šé•œå¤´åœºæ™¯å®šåˆ¶çš„å¯†é›†æè¿°å’ŒæŒ‡ä»¤å¼é—®ç­”å¯¹ã€‚éšåï¼Œç ”ç©¶æå‡ºäº†IPFormer-VideoLLMæ¨¡å‹ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ä¸€ä¸ªé«˜æ•ˆçš„åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è¿æ¥å™¨ (attention-based connector)ï¼Œå°†å®ä¾‹çº§ç‰¹å¾ä½œä¸ºå®ä¾‹æç¤º (instance prompts) æ³¨å…¥æ¨¡å‹ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿè·¨åœºæ™¯èšåˆç‰¹å®šå®ä¾‹çš„ä¿¡æ¯ï¼Œæœ‰æ•ˆè§£å†³äº†èº«ä»½ä¿¡æ¯ä¸¢å¤±çš„é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†å’Œæ¨¡å‹ä¸ä»…æ˜¾è‘—å¢å¼ºäº†å¤šåœºæ™¯è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œè¿˜åœ¨å¤šä¸ªé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºç«äº‰ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21116v2",
      "published_date": "2025-06-26 09:30:57 UTC",
      "updated_date": "2025-07-08 02:46:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:14.592836+00:00"
    },
    {
      "arxiv_id": "2506.21106v1",
      "title": "PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction",
      "title_zh": "PhishKeyï¼šä¸€ç§åŸºäºè‡ªé€‚åº” HTML ç»„ä»¶æå–çš„å¢å¼ºå‹è´¨å¿ƒç½‘ç»œé’“é±¼æ£€æµ‹æ–¹æ³•",
      "authors": [
        "Felipe CastaÃ±o",
        "Eduardo Fidalgo",
        "Enrique Alegre",
        "Rocio Alaiz-RodrÃ­guez",
        "Raul Orduna",
        "Francesco Zola"
      ],
      "abstract": "Phishing attacks pose a significant cybersecurity threat, evolving rapidly to bypass detection mechanisms and exploit human vulnerabilities. This paper introduces PhishKey to address the challenges of adaptability, robustness, and efficiency. PhishKey is a novel phishing detection method using automatic feature extraction from hybrid sources. PhishKey combines character-level processing with Convolutional Neural Networks (CNN) for URL classification, and a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at the word level. CAPE reduces noise and ensures complete sample processing avoiding crop operations on the input data. The predictions from both modules are integrated using a soft-voting ensemble to achieve more accurate and reliable classifications. Experimental evaluations on four state-of-the-art datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1 Score and shows strong resistance to adversarial manipulations such as injection attacks with minimal performance degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PhishKeyï¼Œä¸€ç§åŸºäºæ··åˆæºè‡ªåŠ¨ç‰¹å¾æå–çš„æ–°å‹ç½‘ç»œé’“é±¼æ£€æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ£€æµ‹ç³»ç»Ÿçš„è‡ªé€‚åº”æ€§ã€ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚PhishKey ç»“åˆäº†å­—ç¬¦çº§å¤„ç†ä¸å·ç§¯ç¥ç»ç½‘ç»œ (CNN) è¿›è¡Œ URL åˆ†ç±»ï¼Œå¹¶å¼•å…¥äº†åŸºäºè´¨å¿ƒçš„å…³é”®ç»„ä»¶æå–å™¨ (CAPE) åœ¨å•è¯çº§åˆ«å¤„ç† HTML å†…å®¹ã€‚CAPE èƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤å™ªå£°å¹¶é¿å…å¯¹è¾“å…¥æ•°æ®è¿›è¡Œè£å‰ªæ“ä½œï¼Œä»è€Œç¡®ä¿äº†æ ·æœ¬ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚é€šè¿‡è½¯æŠ•ç¥¨é›†æˆ (soft-voting ensemble) æœºåˆ¶ï¼Œè¯¥æ–¹æ³•æ•´åˆäº†ä¸¤ä¸ªæ¨¡å—çš„é¢„æµ‹ç»“æœï¼Œå®ç°äº†æ›´ç²¾ç¡®ã€å¯é çš„åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhishKey åœ¨å››ä¸ªä¸»æµæ•°æ®é›†ä¸Šè¾¾åˆ°äº† 98.70% çš„ F1 Scoreã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹æ³¨å…¥æ”»å‡»ç­‰å¯¹æŠ—æ€§æ“çºµè¡¨ç°å‡ºæå¼ºçš„æŠµæŠ—åŠ›ï¼Œä¸”æ€§èƒ½æŸè€—æå°ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç½‘ç»œå®‰å…¨ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21106v1",
      "published_date": "2025-06-26 09:04:55 UTC",
      "updated_date": "2025-06-26 09:04:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:19.779712+00:00"
    },
    {
      "arxiv_id": "2506.21102v1",
      "title": "Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning",
      "title_zh": "åŸºäºæ³¨æ„åŠ›å¼•å¯¼å›¾å­¦ä¹ çš„å¯è§£é‡Šå±‚çº§æ¦‚å¿µæ¨ç†",
      "authors": [
        "David Debot",
        "Pietro Barbiero",
        "Gabriele Dominici",
        "Giuseppe Marra"
      ],
      "abstract": "Concept-Based Models (CBMs) are a class of deep learning models that provide interpretability by explaining predictions through high-level concepts. These models first predict concepts and then use them to perform a downstream task. However, current CBMs offer interpretability only for the final task prediction, while the concept predictions themselves are typically made via black-box neural networks. To address this limitation, we propose Hierarchical Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for both concept and task predictions. H-CMR models relationships between concepts using a learned directed acyclic graph, where edges represent logic rules that define concepts in terms of other concepts. During inference, H-CMR employs a neural attention mechanism to select a subset of these rules, which are then applied hierarchically to predict all concepts and the final task. Experimental results demonstrate that H-CMR matches state-of-the-art performance while enabling strong human interaction through concept and model interventions. The former can significantly improve accuracy at inference time, while the latter can enhance data efficiency during training when background knowledge is available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¦‚å¿µæ¨¡å‹(Concept-Based Models, CBMs)ä¸­æ¦‚å¿µé¢„æµ‹è¿‡ç¨‹ä»ä¸ºé»‘ç›’ã€ç¼ºä¹è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†å±‚æ¬¡åŒ–æ¦‚å¿µè®°å¿†æ¨ç†å™¨(Hierarchical Concept Memory Reasoner, H-CMR)ã€‚H-CMRåˆ©ç”¨å­¦ä¹ åˆ°çš„æœ‰å‘æ— ç¯å›¾(Directed Acyclic Graph, DAG)æ¥å»ºæ¨¡æ¦‚å¿µé—´çš„é€»è¾‘å…³ç³»ï¼Œå…¶ä¸­è¾¹ä»£è¡¨å®šä¹‰æ¦‚å¿µçš„é€»è¾‘è§„åˆ™ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç¥ç»æ³¨æ„åŠ›æœºåˆ¶(Neural Attention Mechanism)é€‰æ‹©è§„åˆ™å­é›†å¹¶åˆ†å±‚åº”ç”¨äºæ¦‚å¿µä¸ä»»åŠ¡é¢„æµ‹ï¼Œå®ç°äº†å…¨è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒH-CMRåœ¨è¾¾åˆ°å…ˆè¿›æ€§èƒ½æ°´å¹³çš„åŒæ—¶ï¼Œæ”¯æŒé€šè¿‡æ¦‚å¿µå¹²é¢„(Concept Intervention)æ˜¾è‘—æå‡æ¨ç†å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½åˆ©ç”¨èƒŒæ™¯çŸ¥è¯†è¿›è¡Œæ¨¡å‹å¹²é¢„ä»¥å¢å¼ºè®­ç»ƒé˜¶æ®µçš„æ•°æ®æ•ˆç‡ï¼Œä¸ºäººç±»ä¸å¤æ‚æ¨¡å‹çš„äº¤äº’æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21102v1",
      "published_date": "2025-06-26 08:56:55 UTC",
      "updated_date": "2025-06-26 08:56:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:25.377883+00:00"
    },
    {
      "arxiv_id": "2506.21098v2",
      "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry",
      "title_zh": "ComRAGï¼šé¢å‘å·¥ä¸šé¢†åŸŸå®æ—¶ç¤¾åŒºé—®ç­”ã€ç»“åˆåŠ¨æ€å‘é‡åº“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Qinwen Chen",
        "Wenbiao Tao",
        "Zhiwei Zhu",
        "Mingfan Xi",
        "Liangzhong Guo",
        "Yuan Wang",
        "Wei Wang",
        "Yunshi Lan"
      ],
      "abstract": "Community Question Answering (CQA) platforms can be deemed as important knowledge bases in community, but effectively leveraging historical interactions and domain knowledge in real-time remains a challenge. Existing methods often underutilize external knowledge, fail to incorporate dynamic historical QA context, or lack memory mechanisms suited for industrial deployment. We propose ComRAG, a retrieval-augmented generation framework for real-time industrial CQA that integrates static knowledge with dynamic historical QA pairs via a centroid-based memory mechanism designed for retrieval, generation, and efficient storage. Evaluated on three industrial CQA datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9% improvement in vector similarity, reducing latency by 8.7% to 23.3%, and lowering chunk growth from 20.23% to 2.06% over iterations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ComRAGï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå·¥ä¸šé¢†åŸŸå®æ—¶ç¤¾åŒºé—®ç­”(Community Question Answering)è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ã€æ•´åˆåŠ¨æ€å†å²é—®ç­”ä¸Šä¸‹æ–‡ä»¥åŠæ„å»ºé€‚åº”å·¥ä¸šéƒ¨ç½²çš„å­˜å‚¨æœºåˆ¶æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ComRAGé€šè¿‡ä¸€ç§åˆ›æ–°çš„åŸºäºè´¨å¿ƒçš„å­˜å‚¨æœºåˆ¶(centroid-based memory mechanism)ï¼Œå®ç°äº†é™æ€çŸ¥è¯†ä¸åŠ¨æ€å†å²é—®ç­”å¯¹çš„æœ‰æœºæ•´åˆï¼Œä»è€Œä¼˜åŒ–äº†æ£€ç´¢ã€ç”ŸæˆåŠå­˜å‚¨æ•ˆç‡ã€‚åœ¨ä¸‰ä¸ªå·¥ä¸šçº§CQAæ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒComRAGåœ¨æ€§èƒ½ä¸Šå…¨é¢è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶åœ¨å‘é‡ç›¸ä¼¼åº¦(vector similarity)ä¸Šå®ç°äº†é«˜è¾¾25.9%çš„æå‡ï¼Œå¹¶å°†ç³»ç»Ÿå»¶è¿Ÿ(latency)é™ä½äº†8.7%è‡³23.3%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†è¿­ä»£è¿‡ç¨‹ä¸­çš„æ•°æ®å—å¢é•¿(chunk growth)ä»20.23%å¤§å¹…é™ä½è‡³2.06%ï¼Œä¸ºå®æ—¶å·¥ä¸šé—®ç­”ç³»ç»Ÿæä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„éƒ¨ç½²æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track",
      "pdf_url": "https://arxiv.org/pdf/2506.21098v2",
      "published_date": "2025-06-26 08:48:16 UTC",
      "updated_date": "2025-07-01 06:31:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:32.337326+00:00"
    },
    {
      "arxiv_id": "2506.21095v3",
      "title": "FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation",
      "title_zh": "FeDa4Fairï¼šç”¨äºå…¬å¹³æ€§è¯„ä¼°çš„å®¢æˆ·ç«¯çº§è”é‚¦æ•°æ®é›†",
      "authors": [
        "Xenia Heilmann",
        "Luca Corbucci",
        "Mattia Cerrato",
        "Anna Monreale"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across multiple clients without sharing clients' private data. However, the diverse and often conflicting biases present across clients pose significant challenges to model fairness. Current fairness-enhancing FL solutions often fall short, as they typically mitigate biases for a single, usually binary, sensitive attribute, while ignoring the heterogeneous fairness needs that exist in real-world settings. Moreover, these solutions often evaluate unfairness reduction only on the server side, hiding persistent unfairness at the individual client level. To support more robust and reproducible fairness research in FL, we introduce a comprehensive benchmarking framework for fairness-aware FL at both the global and client levels. Our contributions are three-fold: (1) We introduce \\fairdataset, a library to create tabular datasets tailored to evaluating fair FL methods under heterogeneous client bias; (2) we release four bias-heterogeneous datasets and corresponding benchmarks to compare fairness mitigation methods in a controlled environment; (3) we provide ready-to-use functions for evaluating fairness outcomes for these datasets.",
      "tldr_zh": "è”é‚¦å­¦ä¹ (Federated Learning, FL)ä¸­ï¼Œå„å®¢æˆ·ç«¯ä¹‹é—´å¤šæ ·ä¸”å†²çªçš„åè§å¯¹æ¨¡å‹å…¬å¹³æ€§(Fairness)æå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚é’ˆå¯¹å½“å‰æ–¹æ¡ˆå¾€å¾€åªå…³æ³¨å•ä¸€æ•æ„Ÿå±æ€§ä¸”å¿½ç•¥å®¢æˆ·ç«¯å±‚çº§ä¸å…¬å¹³ç°è±¡çš„å±€é™ï¼Œè¯¥ç ”ç©¶æ¨å‡ºäº† FeDa4Fair æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨å…¨çƒå’Œå®¢æˆ·ç«¯å±‚çº§è¯„ä¼°å…¬å¹³æ„ŸçŸ¥è”é‚¦å­¦ä¹ çš„ç»¼åˆåŸºå‡†æ¡†æ¶ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼šå¼€å‘äº†ä¸€ä¸ªç”¨äºåœ¨å¼‚æ„åè§ç¯å¢ƒä¸‹åˆ›å»ºè¡¨æ ¼æ•°æ®é›†çš„åº“ \\fairdatasetï¼Œå‘å¸ƒäº†å››ä¸ªåè§å¼‚æ„çš„æ•°æ®é›†åŠç›¸åº”åŸºå‡†(Benchmarks)ï¼Œå¹¶æä¾›äº†ç”¨äºè¯„ä¼°å…¬å¹³æ€§ç»“æœçš„ç°æˆå‡½æ•°ã€‚è¯¥æ¡†æ¶é€šè¿‡æä¾›å—æ§çš„å®éªŒç¯å¢ƒï¼Œä¸ºè”é‚¦å­¦ä¹ é¢†åŸŸæ›´å…·é²æ£’æ€§å’Œå¯é‡å¤æ€§çš„å…¬å¹³æ€§ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21095v3",
      "published_date": "2025-06-26 08:43:12 UTC",
      "updated_date": "2025-09-30 17:14:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:31.868220+00:00"
    },
    {
      "arxiv_id": "2506.21085v1",
      "title": "CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions",
      "title_zh": "CovDockerï¼šæ¶µç›–ä»»åŠ¡ã€æ•°æ®é›†ä¸è§£å†³æ–¹æ¡ˆçš„å…±ä»·è¯ç‰©è®¾è®¡åŸºå‡†",
      "authors": [
        "Yangzhe Peng",
        "Kaiyuan Gao",
        "Liang He",
        "Yuheng Cong",
        "Haiguang Liu",
        "Kun He",
        "Lijun Wu"
      ],
      "abstract": "Molecular docking plays a crucial role in predicting the binding mode of ligands to target proteins, and covalent interactions, which involve the formation of a covalent bond between the ligand and the target, are particularly valuable due to their strong, enduring binding nature. However, most existing docking methods and deep learning approaches hardly account for the formation of covalent bonds and the associated structural changes. To address this gap, we introduce a comprehensive benchmark for covalent docking, CovDocker, which is designed to better capture the complexities of covalent binding. We decompose the covalent docking process into three main tasks: reactive location prediction, covalent reaction prediction, and covalent docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer, we establish baseline performances and demonstrate the effectiveness of the benchmark in accurately predicting interaction sites and modeling the molecular transformations involved in covalent binding. These results confirm the role of the benchmark as a rigorous framework for advancing research in covalent drug design. It underscores the potential of data-driven approaches to accelerate the discovery of selective covalent inhibitors and addresses critical challenges in therapeutic development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å­å¯¹æ¥(Molecular docking)ä¸­ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å…±ä»·é”®(Covalent bond)å½¢æˆåŠå…¶ç»“æ„å˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å…±ä»·å¯¹æ¥åŸºå‡†æ¡†æ¶ CovDockerã€‚è¯¥æ¡†æ¶å°†å…±ä»·å¯¹æ¥è¿‡ç¨‹åˆ†è§£ä¸ºååº”ä½ç‚¹é¢„æµ‹(Reactive location prediction)ã€å…±ä»·ååº”é¢„æµ‹(Covalent reaction prediction)å’Œå…±ä»·å¯¹æ¥(Covalent docking)ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡é€‚é… Uni-Mol å’Œ Chemformer ç­‰å…ˆè¿›æ¨¡å‹å»ºç«‹äº†åŸºå‡†æ€§èƒ½ï¼ŒéªŒè¯äº† CovDocker åœ¨å‡†ç¡®é¢„æµ‹ç›¸äº’ä½œç”¨ä½ç‚¹å’Œæ¨¡æ‹Ÿåˆ†å­è½¬åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜è¯¥åŸºå‡†èƒ½ä½œä¸ºä¸€ä¸ªä¸¥è°¨çš„æ¡†æ¶ï¼Œæœ‰æ•ˆæ¨åŠ¨å…±ä»·è¯ç‰©è®¾è®¡é¢†åŸŸçš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨åŠ é€Ÿé«˜é€‰æ‹©æ€§å…±ä»·æŠ‘åˆ¶å‰‚(Covalent inhibitors)å‘ç°æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè§£å†³æ²»ç–—å¼€å‘ä¸­çš„å…³é”®æŒ‘æˆ˜æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted to KDD 2025 Research Track",
      "pdf_url": "https://arxiv.org/pdf/2506.21085v1",
      "published_date": "2025-06-26 08:28:07 UTC",
      "updated_date": "2025-06-26 08:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:30.912217+00:00"
    },
    {
      "arxiv_id": "2506.21080v1",
      "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception",
      "title_zh": "EgoAdaptï¼šé¢å‘é«˜æ•ˆç¬¬ä¸€äººç§°æ„ŸçŸ¥çš„è‡ªé€‚åº”å¤šæ„Ÿå®˜è’¸é¦ä¸ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Sanjoy Chowdhury",
        "Subrata Biswas",
        "Sayan Nag",
        "Tushar Nagarajan",
        "Calvin Murdock",
        "Ishwarya Ananthabhotla",
        "Yijun Qian",
        "Vamsi Krishna Ithapu",
        "Dinesh Manocha",
        "Ruohan Gao"
      ],
      "abstract": "Modern perception models, particularly those designed for multisensory egocentric tasks, have achieved remarkable performance but often come with substantial computational costs. These high demands pose challenges for real-world deployment, especially in resource-constrained environments. In this paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal distillation and policy learning to enable efficient inference across different egocentric perception tasks, including egocentric action recognition, active speaker localization, and behavior anticipation. Our proposed policy module is adaptable to task-specific action spaces, making it broadly applicable. Experimental results on three challenging egocentric datasets EPIC-Kitchens, EasyCom, and Aria Everyday Activities demonstrate that our method significantly enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%, and energy up to 9.6x, while still on-par and in many cases outperforming, the performance of corresponding state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EgoAdaptï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡ç¬¬ä¸€äººç§°æ„ŸçŸ¥ (egocentric perception) ä»»åŠ¡æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”åœ°æ‰§è¡Œ cross-modal distillation å’Œ policy learningï¼Œæœ‰æ•ˆè§£å†³äº†ç°ä»£æ„ŸçŸ¥æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚EgoAdapt æ ¸å¿ƒåŒ…å«ä¸€ä¸ªå¯é€‚é…ç‰¹å®šä»»åŠ¡åŠ¨ä½œç©ºé—´çš„ policy moduleï¼Œä½¿å…¶èƒ½å¹¿æ³›åº”ç”¨äº egocentric action recognitionã€active speaker localization å’Œ behavior anticipation ç­‰ä»»åŠ¡ã€‚åœ¨ EPIC-Kitchensã€EasyCom å’Œ Aria Everyday Activities ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸å½“å‰ state-of-the-art æ¨¡å‹æŒå¹³ç”šè‡³æ›´ä¼˜çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶å‡å°‘äº†é«˜è¾¾ 89.09% çš„ GMACsï¼Œé™ä½äº† 82.02% çš„å‚æ•°é‡ï¼Œå¹¶å°†èƒ½è€—é™ä½äº† 9.6 å€ã€‚è¯¥ç ”ç©¶æˆæœä¸ºåœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²é«˜æ•ˆã€ä½åŠŸè€—çš„å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21080v1",
      "published_date": "2025-06-26 08:09:16 UTC",
      "updated_date": "2025-06-26 08:09:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:42.522081+00:00"
    },
    {
      "arxiv_id": "2506.22515v1",
      "title": "In-context learning for the classification of manipulation techniques in phishing emails",
      "title_zh": "é¢å‘ç½‘ç»œé’“é±¼é‚®ä»¶æ“çºµæŠ€æœ¯åˆ†ç±»çš„ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Antony Dalmiere",
        "Guillaume Auriol",
        "Vincent Nicomette",
        "Pascal Marchand"
      ],
      "abstract": "Traditional phishing detection often overlooks psychological manipulation. This study investigates using Large Language Model (LLM) In-Context Learning (ICL) for fine-grained classification of phishing emails based on a taxonomy of 40 manipulation techniques. Using few-shot examples with GPT-4o-mini on real-world French phishing emails (SignalSpam), we evaluated performance against a human-annotated test set (100 emails). The approach effectively identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's potential for nuanced phishing analysis and provides insights into attacker strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç½‘ç»œé’“é±¼æ£€æµ‹å¾€å¾€å¿½è§†å¿ƒç†æ“çºµçš„é—®é¢˜ï¼Œæ¢ç´¢äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)å¯¹é’“é±¼é‚®ä»¶è¿›è¡Œç»†ç²’åº¦åˆ†ç±»ã€‚ç ”ç©¶åŸºäºåŒ…å«40ç§æ“çºµæŠ€æœ¯çš„åˆ†ç±»ä½“ç³»ï¼Œåœ¨GPT-4o-miniæ¨¡å‹ä¸Šé€šè¿‡å°‘æ ·æœ¬(few-shot)ç¤ºä¾‹å¯¹çœŸå®æ³•è¯­ç½‘ç»œé’“é±¼é‚®ä»¶(SignalSpam)è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äººå·¥æ ‡æ³¨çš„æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†0.76çš„å‡†ç¡®ç‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºè¯±å¯¼(Baiting)ã€å¥½å¥‡å¿ƒè¯±å¯¼(Curiosity Appeal)å’Œè¯·æ±‚å°é¢å¸®åŠ©(Request For Minor Favor)ç­‰æµè¡ŒæŠ€æœ¯ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ (ICL)åœ¨ç»†è‡´åŒ–ç½‘ç»œé’“é±¼åˆ†æä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ·±å…¥ç†è§£æ”»å‡»è€…çš„æ“çºµç­–ç•¥æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22515v1",
      "published_date": "2025-06-26 08:07:30 UTC",
      "updated_date": "2025-06-26 08:07:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:49.403374+00:00"
    },
    {
      "arxiv_id": "2506.21049v1",
      "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification",
      "title_zh": "ç”µå­å•†åŠ¡æŸ¥è¯¢åˆ†ç±»çš„åŠç›‘ç£å¯æ‰©å±•ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Chunyuan Yuan",
        "Chong Zhang",
        "Zheng Fang",
        "Ming Pang",
        "Xue Jiang",
        "Changping Peng",
        "Zhangang Lin",
        "Ching Law"
      ],
      "abstract": "Query classification, including multiple subtasks such as intent and category prediction, is vital to e-commerce applications. E-commerce queries are usually short and lack context, and the information between labels cannot be used, resulting in insufficient prior information for modeling. Most existing industrial query classification methods rely on users' posterior click behavior to construct training samples, resulting in a Matthew vicious cycle. Furthermore, the subtasks of query classification lack a unified framework, leading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework (SSUF), containing multiple enhanced modules to unify the query classification tasks. The knowledge-enhanced module uses world knowledge to enhance query representations and solve the problem of insufficient query information. The label-enhanced module uses label semantics and semi-supervised signals to reduce the dependence on posterior labels. The structure-enhanced module enhances the label representation based on the complex label relations. Each module is highly pluggable, and input features can be added or removed as needed according to each subtask. We conduct extensive offline and online A/B experiments, and the results show that SSUF significantly outperforms the state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå•†æŸ¥è¯¢åˆ†ç±» (Query classification) ä¸­å­˜åœ¨çš„æŸ¥è¯¢çŸ­å°ä¸”ç¼ºä¹ä¸Šä¸‹æ–‡ã€è¿‡åº¦ä¾èµ–ç”¨æˆ·ç‚¹å‡»è¡Œä¸º (Posterior click behavior) ä»¥åŠå­ä»»åŠ¡ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º SSUF (Semi-supervised Scalable Unified Framework) çš„åŠç›‘ç£å¯æ‰©å±•ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†çŸ¥è¯†å¢å¼º (Knowledge-enhanced)ã€æ ‡ç­¾å¢å¼º (Label-enhanced) å’Œç»“æ„å¢å¼º (Structure-enhanced) ä¸‰ä¸ªé«˜åº¦å¯æ’æ‹”çš„æ¨¡å—ï¼Œåˆ†åˆ«åˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†ã€æ ‡ç­¾è¯­ä¹‰å’Œå¤æ‚çš„æ ‡ç­¾å…³ç³»æ¥ä¼˜åŒ–è¡¨ç¤ºã€‚SSUF èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å…ˆéªŒä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å¯¹åéªŒæ ‡ç­¾çš„ä¾èµ–ï¼ŒåŒæ—¶æ”¯æŒæ ¹æ®ä¸åŒå­ä»»åŠ¡éœ€æ±‚çµæ´»é…ç½®è¾“å…¥ç‰¹å¾ã€‚å¤§é‡çš„ç¦»çº¿å®éªŒå’Œåœ¨çº¿ A/B æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒSSUF åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸ºç”µå•†ç¯å¢ƒä¸‹çš„å¤šä»»åŠ¡æŸ¥è¯¢å¤„ç†æä¾›äº†é«˜æ•ˆä¸”ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21049v1",
      "published_date": "2025-06-26 06:52:33 UTC",
      "updated_date": "2025-06-26 06:52:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:53.252284+00:00"
    },
    {
      "arxiv_id": "2506.21045v1",
      "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling",
      "title_zh": "é€šè¿‡å¼•å¯¼ä¸è°ƒåº¦æå‡åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘å¿ å®åº¦",
      "authors": [
        "Hansam Cho",
        "Seoung Bum Kim"
      ],
      "abstract": "Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Faithfulness Guidance and Scheduling (FGS)ï¼Œä¸€ç§æ—¨åœ¨æå‡åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘å¿ å®åº¦ (faithfulness) çš„æ–°æ–¹æ³•ã€‚åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹é€šå¸¸éš¾ä»¥åœ¨å®ç°å¤§å¹…åº¦ä¿®æ”¹çš„ç¼–è¾‘èƒ½åŠ› (editability) ä¸ä¿ç•™åŸå§‹å›¾åƒå…ƒç´ çš„å¿ å®åº¦ä¹‹é—´è¾¾æˆå¹³è¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€æƒè¡¡éš¾é¢˜ï¼ŒFGS å¼•å…¥äº†å¿ å®åº¦å¼•å¯¼æœºåˆ¶ä»¥å¼ºåŒ–å¯¹è¾“å…¥å›¾åƒä¿¡æ¯çš„ä¿ç•™ï¼Œå¹¶ç»“åˆè°ƒåº¦ç­–ç•¥æ¥è§£å†³ç¼–è¾‘èƒ½åŠ›ä¸å¿ å®åº¦ä¹‹é—´çš„å¤±é…é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFGS åœ¨ä¿æŒç¼–è¾‘èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸å¤šç§ç°æœ‰çš„ç¼–è¾‘æ‰‹æ®µå…·æœ‰è‰¯å¥½çš„å…¼å®¹æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ç¼–è¾‘ä»»åŠ¡ä¸­å®ç°ç²¾ç¡®ä¸”é«˜è´¨é‡çš„å›¾åƒä¿®æ”¹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.21045v1",
      "published_date": "2025-06-26 06:46:03 UTC",
      "updated_date": "2025-06-26 06:46:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:53.403898+00:00"
    },
    {
      "arxiv_id": "2506.21044v1",
      "title": "Efficient Skill Discovery via Regret-Aware Optimization",
      "title_zh": "åŸºäºæ‚”å€¼æ„ŸçŸ¥ä¼˜åŒ–çš„é«˜æ•ˆæŠ€èƒ½å‘ç°",
      "authors": [
        "He Zhang",
        "Ming Zhou",
        "Shaopeng Zhai",
        "Ying Sun",
        "Hui Xiong"
      ],
      "abstract": "Unsupervised skill discovery aims to learn diverse and distinguishable behaviors in open-ended reinforcement learning. For existing methods, they focus on improving diversity through pure exploration, mutual information optimization, and learning temporal representation. Despite that they perform well on exploration, they remain limited in terms of efficiency, especially for the high-dimensional situations. In this work, we frame skill discovery as a min-max game of skill generation and policy learning, proposing a regret-aware method on top of temporal representation learning that expands the discovered skill space along the direction of upgradable policy strength. The key insight behind the proposed method is that the skill discovery is adversarial to the policy learning, i.e., skills with weak strength should be further explored while less exploration for the skills with converged strength. As an implementation, we score the degree of strength convergence with regret, and guide the skill discovery with a learnable skill generator. To avoid degeneration, skill generation comes from an up-gradable population of skill generators. We conduct experiments on environments with varying complexities and dimension sizes. Empirical results show that our method outperforms baselines in both efficiency and diversity. Moreover, our method achieves a 15% zero shot improvement in high-dimensional environments, compared to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— ç›‘ç£æŠ€èƒ½å‘ç° (Unsupervised skill discovery) åœ¨é«˜ç»´å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´çš„æ•ˆç‡ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºé—æ†¾æ„ŸçŸ¥ä¼˜åŒ– (Regret-Aware Optimization) çš„é«˜æ•ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æŠ€èƒ½å‘ç°å»ºæ¨¡ä¸ºæŠ€èƒ½ç”Ÿæˆä¸ç­–ç•¥å­¦ä¹ ä¹‹é—´çš„æå¤§æå°åšå¼ˆ (min-max game)ï¼Œå¹¶ç»“åˆæ—¶é—´è¡¨ç¤ºå­¦ä¹  (temporal representation learning) åœ¨ç­–ç•¥å¯æå‡çš„æ–¹å‘ä¸Šæ‰©å±•å·²å‘ç°çš„æŠ€èƒ½ç©ºé—´ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é—æ†¾å€¼ (regret) è¯„ä¼°ç­–ç•¥å¼ºåº¦çš„æ”¶æ•›ç¨‹åº¦ï¼Œä»è€Œå¯¹æŠ—æ€§åœ°å¼•å¯¼æ¢ç´¢è¿‡ç¨‹ï¼Œå³ä¼˜å…ˆæ¢ç´¢å¼ºåº¦è¾ƒå¼±çš„æŠ€èƒ½å¹¶å‡å°‘å¯¹å·²æ”¶æ•›æŠ€èƒ½çš„é‡å¤æ¢ç´¢ã€‚ä¸ºäº†é˜²æ­¢æ¨¡å¼é€€åŒ–ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨äº†å¯å‡çº§çš„æŠ€èƒ½ç”Ÿæˆå™¨ç§ç¾¤æ¥é©±åŠ¨æŠ€èƒ½ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå¤æ‚åº¦çš„ç¯å¢ƒä¸‹å‡åœ¨æ•ˆç‡å’Œå¤šæ ·æ€§ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜ç»´ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æŠ€æœ¯å®ç°äº† 15% çš„é›¶æ ·æœ¬ (zero shot) æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21044v1",
      "published_date": "2025-06-26 06:45:59 UTC",
      "updated_date": "2025-06-26 06:45:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:53:58.280822+00:00"
    },
    {
      "arxiv_id": "2506.21041v2",
      "title": "SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling",
      "title_zh": "SEALï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ä¸è‡ªé€‚åº”é•¿å°¾å»ºæ¨¡çš„ç«¯åˆ°ç«¯å®‰å…¨ååŒè‡ªåŠ¨é©¾é©¶",
      "authors": [
        "Junwei You",
        "Pei Li",
        "Zhuoyu Jiang",
        "Zilin Huang",
        "Rui Gan",
        "Haotian Shi",
        "Bin Ran"
      ],
      "abstract": "Autonomous driving technologies face significant safety challenges while operating under rare, diverse, and visually degraded weather scenarios. These challenges become more critical in cooperative settings, where vehicles and infrastructure jointly perceive and reason across complex environments. To address these issues, we propose SEAL, a vision-language model-based framework with adaptive multimodal learning for robust cooperative autonomous driving under long-tail scenarios. SEAL introduces three core innovations: (i) a prompt-driven long-tail scenario generation and evaluation pipeline that leverages foundation models to synthesize realistic long-tail conditions such as snow and fog across vehicle- and infrastructure-side views, enriching training diversity efficiently; (ii) a gated multi-scenario adaptive attention module that modulates the visual stream using scenario priors to recalibrate ambiguous or corrupted features; and (iii) a multi-task scenario-aware contrastive learning objective that improves multimodal alignment and promotes cross-scenario feature separability. Extensive experiments demonstrate that SEAL significantly outperforms existing baselines in reasoning, safety, and planning accuracy under complex, challenging driving conditions, advancing the safety, robustness, and scalability of autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SEALï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Model)çš„ç«¯åˆ°ç«¯å®‰å…¨ååŒè‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶åœ¨è§†è§‰å—æŸçš„é•¿å°¾åœºæ™¯(Long-Tail Scenarios)ä¸‹é¢ä¸´çš„å®‰å…¨æŒ‘æˆ˜ã€‚SEAL å¼•å…¥äº†æç¤ºé©±åŠ¨çš„é•¿å°¾åœºæ™¯ç”Ÿæˆç®¡çº¿ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹(Foundation Models)åˆæˆé›ªã€é›¾ç­‰çœŸå®ç¯å¢ƒä»¥å¢å¼ºè®­ç»ƒå¤šæ ·æ€§ã€‚æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªé—¨æ§å¤šåœºæ™¯è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—(Gated Multi-Scenario Adaptive Attention Module)ï¼Œé€šè¿‡åœºæ™¯å…ˆéªŒå¯¹æ¨¡ç³Šæˆ–æŸåçš„è§†è§‰ç‰¹å¾è¿›è¡Œé‡æ–°æ ¡å‡†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è®¾è®¡äº†å¤šä»»åŠ¡åœºæ™¯æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ (Multi-Task Scenario-Aware Contrastive Learning)ç›®æ ‡ï¼Œæœ‰æ•ˆæå‡äº†å¤šæ¨¡æ€å¯¹é½åŠè·¨åœºæ™¯ç‰¹å¾çš„å¯åˆ†ç¦»æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSEAL åœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸‹çš„æ¨ç†ã€å®‰å…¨æ€§å’Œè§„åˆ’å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå®ç°æ›´å…·é²æ£’æ€§å’Œå¯æ‰©å±•æ€§çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21041v2",
      "published_date": "2025-06-26 06:42:03 UTC",
      "updated_date": "2025-07-04 17:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:58.298177+00:00"
    },
    {
      "arxiv_id": "2506.21039v1",
      "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning",
      "title_zh": "ä¸¥æ ¼å­ç›®æ ‡æ‰§è¡Œï¼šåˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸­å¯é çš„é•¿æ—¶ç¨‹è§„åˆ’",
      "authors": [
        "Jaebak Hwang",
        "Sanghyeon Lee",
        "Jeongmo Kim",
        "Seungyul Han"
      ],
      "abstract": "Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, they often suffer from subgoal infeasibility and inefficient planning. We introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that enforces single-step subgoal reachability by structurally constraining high-level decision-making. To enhance exploration, SSE employs a decoupled exploration policy that systematically traverses underexplored regions of the goal space. Furthermore, a failure-aware path refinement, which refines graph-based planning by dynamically adjusting edge costs according to observed low-level success rates, thereby improving subgoal reliability. Experimental results across diverse long-horizon benchmarks demonstrate that SSE consistently outperforms existing goal-conditioned RL and hierarchical RL approaches in both efficiency and success rate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Strict Subgoal Execution (SSE)ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³åˆ†å±‚å¼ºåŒ–å­¦ä¹  (Hierarchical Reinforcement Learning) ä¸­é•¿ç¨‹è§„åˆ’æŒ‘æˆ˜çš„å›¾å½¢åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•é’ˆå¯¹é•¿ç¨‹ç›®æ ‡å¯¼å‘ä»»åŠ¡ä¸­ç›®æ ‡é¥è¿œã€å¥–åŠ±ç¨€ç–ä»¥åŠå­ç›®æ ‡ä¸å¯è¡Œç­‰é—®é¢˜ï¼Œé€šè¿‡ç»“æ„æ€§åœ°çº¦æŸé«˜å±‚å†³ç­–ï¼Œå¼ºåˆ¶æ‰§è¡Œå•æ­¥å­ç›®æ ‡çš„å¯è¾¾æ€§ã€‚ä¸ºäº†å¢å¼ºæ¢ç´¢èƒ½åŠ›ï¼ŒSSE é‡‡ç”¨äº†å»è€¦åˆçš„æ¢ç´¢ç­–ç•¥ (decoupled exploration policy) ä»¥ç³»ç»Ÿæ€§åœ°éå†ç›®æ ‡ç©ºé—´ä¸­æœªå……åˆ†æ¢ç´¢çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¤±è´¥æ„ŸçŸ¥è·¯å¾„ä¼˜åŒ– (failure-aware path refinement) æœºåˆ¶ï¼Œé€šè¿‡æ ¹æ®åº•å±‚æˆåŠŸç‡åŠ¨æ€è°ƒæ•´è¾¹æˆæœ¬æ¥æå‡è§„åˆ’çš„å¯é æ€§ã€‚åœ¨å¤šä¸ªé•¿ç¨‹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒSSE åœ¨ä»»åŠ¡æ‰§è¡Œæ•ˆç‡å’ŒæˆåŠŸç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç›®æ ‡å¯¼å‘å¼ºåŒ–å­¦ä¹  (Goal-Conditioned RL) å’Œåˆ†å±‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 technical page followed by references and appendix",
      "pdf_url": "https://arxiv.org/pdf/2506.21039v1",
      "published_date": "2025-06-26 06:35:42 UTC",
      "updated_date": "2025-06-26 06:35:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:08.910217+00:00"
    },
    {
      "arxiv_id": "2507.01982v1",
      "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism",
      "title_zh": "DKGCMï¼šèåˆç©ºé—´èŠ‚ç‚¹èšç±»æ–¹æ³•ä¸å‚…é‡Œå¶åŒå‘ Mamba æœºåˆ¶çš„äº¤é€šæµæ—¶ç©ºé¢„æµ‹æ¨¡å‹",
      "authors": [
        "Siqing Long",
        "Xiangzhi Huang",
        "Jiemin Xie",
        "Ming Cai"
      ],
      "abstract": "Accurate traffic demand forecasting enables transportation management departments to allocate resources more effectively, thereby improving their utilization efficiency. However, complex spatiotemporal relationships in traffic systems continue to limit the performance of demand forecasting models. To improve the accuracy of spatiotemporal traffic demand prediction, we propose a new graph convolutional network structure called DKGCM. Specifically, we first consider the spatial flow distribution of different traffic nodes and propose a novel temporal similarity-based clustering graph convolution method, DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering to group traffic nodes and more effectively capture spatial dependencies. On the temporal scale, we integrate the Fast Fourier Transform (FFT) within the bidirectional Mamba deep learning framework to capture temporal dependencies in traffic demand. To further optimize model training, we incorporate the GRPO reinforcement learning strategy to enhance the loss function feedback mechanism. Extensive experiments demonstrate that our model outperforms several advanced methods and achieves strong results on three public datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DKGCMï¼Œä¸€ç§æ—¨åœ¨æå‡äº¤é€šæµé¢„æµ‹å‡†ç¡®æ€§çš„æ–°å‹æ—¶ç©ºé¢„æµ‹æ¨¡å‹ã€‚åœ¨ç©ºé—´ç»´åº¦ä¸Šï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†åŸºäºæ—¶é—´ç›¸ä¼¼æ€§çš„èšç±»å›¾å·ç§¯æ–¹æ³•DK-GCNï¼Œé€šè¿‡ç»“åˆDynamic Time Warping (DTW)å’ŒK-meansèšç±»ç®—æ³•å¯¹äº¤é€šèŠ‚ç‚¹è¿›è¡Œåˆ†ç»„ï¼Œä»è€Œæ›´ç²¾å‡†åœ°æ•æ‰å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æ—¶é—´å°ºåº¦ä¸Šï¼Œç ”ç©¶å°†Fast Fourier Transform (FFT)ä¸åŒå‘Mambaæ·±åº¦å­¦ä¹ æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆæå–äº¤é€šéœ€æ±‚çš„æ—¶é—´ä¾èµ–ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é‡‡ç”¨äº†GRPOå¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥ä¼˜åŒ–æŸå¤±å‡½æ•°åé¦ˆæœºåˆ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒDKGCMåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œä¸ºè§£å†³äº¤é€šç³»ç»Ÿä¸­çš„æ—¶ç©ºå»ºæ¨¡éš¾é¢˜æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.01982v1",
      "published_date": "2025-06-26 06:33:36 UTC",
      "updated_date": "2025-06-26 06:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:10.154727+00:00"
    },
    {
      "arxiv_id": "2506.21031v1",
      "title": "Large Language Models Acing Chartered Accountancy",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹è®¸ä¼šè®¡å¸ˆè€ƒè¯•ä¸­çš„å“è¶Šè¡¨ç°",
      "authors": [
        "Jatin Gupta",
        "Akhil Sharma",
        "Saransh Singhania",
        "Mohammad Adnan",
        "Sakshi Deo",
        "Ali Imam Abidi",
        "Keshav Gupta"
      ],
      "abstract": "Advanced intelligent systems, particularly Large Language Models (LLMs), are significantly reshaping financial practices through advancements in Natural Language Processing (NLP). However, the extent to which these models effectively capture and apply domain-specific financial knowledge remains uncertain. Addressing a critical gap in the expansive Indian financial context, this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically designed to evaluate the financial, legal, and quantitative reasoning capabilities of LLMs. CA-Ben comprises structured question-answer datasets derived from the rigorous examinations conducted by the Institute of Chartered Accountants of India (ICAI), spanning foundational, intermediate, and advanced CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated using standardized protocols. Results indicate variations in performance, with Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and legal reasoning. Notable challenges emerged in numerical computations and legal interpretations. The findings emphasize the strengths and limitations of current LLMs, suggesting future improvements through hybrid reasoning and retrieval-augmented generation methods, particularly for quantitative analysis and accurate legal interpretation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† CA-Benï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å°åº¦ç‰¹è®¸ä¼šè®¡å¸ˆ (Chartered Accountancy) è€ƒè¯•è®¾è®¡çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è´¢åŠ¡ã€æ³•å¾‹åŠå®šé‡æ¨ç†æ–¹é¢çš„ä¸“ä¸šèƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å°åº¦ç‰¹è®¸ä¼šè®¡å¸ˆåä¼š (ICAI) ä»åŸºç¡€åˆ°é«˜çº§å„é˜¶æ®µçš„è€ƒè¯•å†…å®¹ï¼Œå¹¶å¯¹ GPT-4oã€LLAMA 3.3 70Bã€Claude 3.5 Sonnet ç­‰å…­æ¬¾ä¸»æµæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClaude 3.5 Sonnet å’Œ GPT-4o åœ¨æ¦‚å¿µç†è§£å’Œæ³•å¾‹æ¨ç†ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†åœ¨æ•°å€¼è®¡ç®— (numerical computations) å’Œç²¾ç¡®çš„æ³•å¾‹è§£é‡Š (legal interpretations) æ–¹é¢ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚ç ”ç©¶å¼ºè°ƒäº†å½“å‰ LLMs çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æŒ‡å‡ºæœªæ¥åº”é€šè¿‡æ··åˆæ¨ç† (hybrid reasoning) ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æŠ€æœ¯æ¥æå‡å®šé‡åˆ†æçš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æˆæœå¡«è¡¥äº†ä¸“ä¸šé‡‘èèƒŒæ™¯ä¸‹æ¨¡å‹è¯„ä¼°çš„ç©ºç™½ï¼Œä¸ºå¼€å‘æ›´å¯é çš„é¢†åŸŸä¸“ç”¨æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at MoStart 2025: International Conference on Digital Transformation in Education and Applications of Artificial Intelligence, Bosnia and Herzegovina, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21031v1",
      "published_date": "2025-06-26 06:10:37 UTC",
      "updated_date": "2025-06-26 06:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:21.083564+00:00"
    },
    {
      "arxiv_id": "2507.22900v4",
      "title": "New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants",
      "title_zh": "è¯¾å ‚é‡Œçš„â€œæ–°æˆå‘˜â€ï¼šå­¦ç”Ÿå¯¹ AI ç¼–ç¨‹åŠ©æ‰‹æ„ŸçŸ¥çš„æ¢ç´¢",
      "authors": [
        "Sergio Rojas-Galeano"
      ],
      "abstract": "The arrival of AI coding assistants in educational settings presents a paradigm shift, introducing a \"new kid in the classroom\" for both students and instructors. Thus, understanding the perceptions of these key actors about this new dynamic is critical. This exploratory study contributes to this area by investigating how these tools are shaping the experiences of novice programmers in an introductory programming course. Through a two-part exam, we investigated student perceptions by first providing access to AI support for a programming task and then requiring an extension of the solution without it. We collected Likert-scale and open-ended responses from 20 students to understand their perceptions on the challenges they faced. Our findings reveal that students perceived AI tools as helpful for grasping code concepts and boosting their confidence during the initial development phase. However, a noticeable difficulty emerged when students were asked to work unaided, pointing to potential overreliance and gaps in foundational knowledge transfer. These insights highlight a critical need for new pedagogical approaches that integrate AI effectively while effectively enhancing core programming skills, rather than impersonating them.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è°ƒæŸ¥äº†åˆçº§ç¨‹åºå‘˜åœ¨å…¥é—¨ç¼–ç¨‹è¯¾ç¨‹ä¸­å¯¹ AI Coding Assistants çš„çœ‹æ³•å’Œä½“éªŒã€‚é€šè¿‡ä¸€é¡¹åŒ…å«ä¸¤ä¸ªé˜¶æ®µçš„è€ƒè¯•ç ”ç©¶ï¼Œç ”ç©¶è€…é¦–å…ˆè®©20åå­¦ç”Ÿåˆ©ç”¨ AI è¾…åŠ©å®Œæˆç¼–ç¨‹ä»»åŠ¡ï¼Œéšåè¦æ±‚ä»–ä»¬åœ¨æ²¡æœ‰ AI æ”¯æŒçš„æƒ…å†µä¸‹æ‰©å±•å…¶è§£å†³æ–¹æ¡ˆã€‚è°ƒæŸ¥ç»“æœæ˜¾ç¤ºï¼Œå­¦ç”Ÿæ™®éè®¤ä¸º AI å·¥å…·åœ¨ç†è§£ä»£ç æ¦‚å¿µå’Œå¢å¼ºåˆå§‹å¼€å‘é˜¶æ®µçš„ä¿¡å¿ƒæ–¹é¢éå¸¸æœ‰å¸®åŠ©ã€‚ç„¶è€Œï¼Œåœ¨éšåè¢«è¦æ±‚è„±ç¦» AI ç‹¬ç«‹å·¥ä½œæ—¶ï¼Œå­¦ç”Ÿé‡åˆ°äº†æ˜¾è‘—çš„å›°éš¾ï¼Œè¿™æ­ç¤ºäº†æ½œåœ¨çš„è¿‡åº¦ä¾èµ–ï¼ˆoverrelianceï¼‰ç°è±¡ä»¥åŠåŸºç¡€çŸ¥è¯†è¿ç§»ä¸­å­˜åœ¨çš„ç¼ºå£ã€‚è¯¥ç ”ç©¶æœ€åæŒ‡å‡ºï¼Œæ•™è‚²é¢†åŸŸäºŸéœ€å¼€å‘æ–°çš„æ•™å­¦æ–¹æ³•ï¼Œä»¥å®ç°åœ¨æœ‰æ•ˆæ•´åˆ AI å·¥å…·çš„åŒæ—¶ï¼Œåˆ‡å®å¢å¼ºå­¦ç”Ÿçš„æ ¸å¿ƒç¼–ç¨‹èƒ½åŠ›è€Œéå°†å…¶ç®€å•æ›¿ä»£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "A shorter version of the manuscript (16 pages) has been accepted for publication in the Proceedings of 19th Colombian Conference on Computing, CCC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22900v4",
      "published_date": "2025-06-26 05:59:23 UTC",
      "updated_date": "2025-09-16 15:09:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:54:20.955688+00:00"
    },
    {
      "arxiv_id": "2507.02929v1",
      "title": "OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference",
      "title_zh": "OBSERï¼šé¢å‘é›¶æ ·æœ¬ç¯å¢ƒæ¨ç†çš„åŸºäºç‰©ä½“çš„å­ç¯å¢ƒè¯†åˆ«",
      "authors": [
        "Won-Seok Choi",
        "Dong-Sig Han",
        "Suhyung Choi",
        "Hyeonseo Yang",
        "Byoung-Tak Zhang"
      ],
      "abstract": "We present the Object-Based Sub-Environment Recognition (OBSER) framework, a novel Bayesian framework that infers three fundamental relationships between sub-environments and their constituent objects. In the OBSER framework, metric and self-supervised learning models estimate the object distributions of sub-environments on the latent space to compute these measures. Both theoretically and empirically, we validate the proposed framework by introducing the ($Îµ,Î´$) statistically separable (EDS) function which indicates the alignment of the representation. Our framework reliably performs inference in open-world and photorealistic environments and outperforms scene-based methods in chained retrieval tasks. The OBSER framework enables zero-shot recognition of environments to achieve autonomous environment understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OBSER (Object-Based Sub-Environment Recognition) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç° Zero-Shot Environmental Inference çš„æ–°å‹ Bayesian frameworkã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨æ–­å­ç¯å¢ƒä¸å…¶ç»„æˆå¯¹è±¡ä¹‹é—´çš„ä¸‰ç§åŸºæœ¬å…³ç³»ï¼Œåˆ©ç”¨ metric learning å’Œ self-supervised learning æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ (latent space) ä¸­ä¼°è®¡å¯¹è±¡åˆ†å¸ƒã€‚ç ”ç©¶è€…å¼•å…¥äº† ($Îµ,Î´$) statistically separable (EDS) å‡½æ•°æ¥è¡¡é‡è¡¨ç¤ºå¯¹é½ (representation alignment)ï¼Œå¹¶ä»ç†è®ºå’Œå®è¯ä¸¤æ–¹é¢éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOBSER åœ¨å¼€æ”¾ä¸–ç•Œå’Œå†™å®ç¯å¢ƒä¸­å…·æœ‰å¯é çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”åœ¨ chained retrieval ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ scene-based æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å®ç°ç¯å¢ƒçš„ zero-shot è¯†åˆ«ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººçš„è‡ªä¸»ç¯å¢ƒç†è§£ (autonomous environment understanding) èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "This manuscript was initially submitted to ICCV 2025 and is now made available as a preprint",
      "pdf_url": "https://arxiv.org/pdf/2507.02929v1",
      "published_date": "2025-06-26 05:57:06 UTC",
      "updated_date": "2025-06-26 05:57:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:17.235713+00:00"
    },
    {
      "arxiv_id": "2506.21017v1",
      "title": "Multimodal Prompt Alignment for Facial Expression Recognition",
      "title_zh": "é¢å‘é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„å¤šæ¨¡æ€æç¤ºå¯¹é½",
      "authors": [
        "Fuyan Ma",
        "Yiran He",
        "Bin Sun",
        "Shutao Li"
      ],
      "abstract": "Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs) like CLIP for various downstream tasks. Despite their success, current VLM-based facial expression recognition (FER) methods struggle to capture fine-grained textual-visual relationships, which are essential for distinguishing subtle differences between facial expressions. To address this challenge, we propose a multimodal prompt alignment framework for FER, called MPA-FER, that provides fine-grained semantic guidance to the learning process of prompted visual features, resulting in more precise and interpretable representations. Specifically, we introduce a multi-granularity hard prompt generation strategy that utilizes a large language model (LLM) like ChatGPT to generate detailed descriptions for each facial expression. The LLM-based external knowledge is injected into the soft prompts by minimizing the feature discrepancy between the soft prompts and the hard prompts. To preserve the generalization abilities of the pretrained CLIP model, our approach incorporates prototype-guided visual feature alignment, ensuring that the prompted visual features from the frozen image encoder align closely with class-specific prototypes. Additionally, we propose a cross-modal global-local alignment module that focuses on expression-relevant facial features, further improving the alignment between textual and visual features. Extensive experiments demonstrate our framework outperforms state-of-the-art methods on three FER benchmark datasets, while retaining the benefits of the pretrained model and minimizing computational costs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)ä¸­éš¾ä»¥æ•æ‰ç»†ç²’åº¦æ–‡æœ¬-è§†è§‰å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡æ€æç¤ºå¯¹é½æ¡†æ¶MPA-FERã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤šç²’åº¦ç¡¬æç¤º(hard prompt)ç”Ÿæˆç­–ç•¥ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„è¯¦ç»†æè¿°ä¸ºå­¦ä¹ è¿‡ç¨‹æä¾›ç»†ç²’åº¦çš„è¯­ä¹‰å¼•å¯¼ã€‚é€šè¿‡æœ€å°åŒ–è½¯æç¤º(soft prompt)ä¸ç¡¬æç¤ºä¹‹é—´çš„ç‰¹å¾å·®å¼‚ï¼Œç ”ç©¶æˆåŠŸå°†LLMçš„å¤–éƒ¨çŸ¥è¯†æ³¨å…¥æ¨¡å‹æç¤ºä¸­ã€‚ä¸ºäº†ä¿ç•™é¢„è®­ç»ƒCLIPæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒMPA-FERé‡‡ç”¨äº†åŸå‹å¼•å¯¼çš„è§†è§‰ç‰¹å¾å¯¹é½(prototype-guided visual feature alignment)ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè·¨æ¨¡æ€å…¨å±€-å±€éƒ¨å¯¹é½æ¨¡å—è¿›ä¸€æ­¥å¼ºåŒ–äº†å¯¹è¡¨æƒ…ç›¸å…³é¢éƒ¨ç‰¹å¾çš„æ•æ‰ä¸å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªFERåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨æé«˜è¯†åˆ«ç²¾åº¦å’Œå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆå…¼é¡¾äº†æ¨¡å‹çš„æ³›åŒ–æ€§ä¸è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To appear in ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2506.21017v1",
      "published_date": "2025-06-26 05:28:57 UTC",
      "updated_date": "2025-06-26 05:28:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:17.661791+00:00"
    },
    {
      "arxiv_id": "2506.20993v2",
      "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control",
      "title_zh": "SACï¼šä¸€ç§å…·æœ‰åŠ¨æ€å¼ºåº¦æ§åˆ¶çš„å¤§è¯­è¨€æ¨¡å‹äººæ ¼ç‰¹è´¨æµ‹é‡ä¸è¯±å¯¼æ¡†æ¶",
      "authors": [
        "Adithya Chittem",
        "Aishna Shrivastava",
        "Sai Tarun Pendela",
        "Jagat Sesh Challa",
        "Dhruv Kumar"
      ],
      "abstract": "Large language models (LLMs) have gained significant traction across a wide range of fields in recent years. There is also a growing expectation for them to display human-like personalities during interactions. To meet this expectation, numerous studies have proposed methods for modelling LLM personalities through psychometric evaluations. However, most existing models face two major limitations: they rely on the Big Five (OCEAN) framework, which only provides coarse personality dimensions, and they lack mechanisms for controlling trait intensity. In this paper, we address this gap by extending the Machine Personality Inventory (MPI), which originally used the Big Five model, to incorporate the 16 Personality Factor (16PF) model, allowing expressive control over sixteen distinct traits. We also developed a structured framework known as Specific Attribute Control (SAC) for evaluating and dynamically inducing trait intensity in LLMs. Our method introduces adjective-based semantic anchoring to guide trait intensity expression and leverages behavioural questions across five intensity factors: \\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and \\textit{Willingness}. Through experimentation, we find that modelling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression compared to binary trait toggling. Moreover, we observe that changes in target trait intensity systematically influence closely related traits in psychologically coherent directions, suggesting that LLMs internalize multi-dimensional personality structures rather than treating traits in isolation. Our work opens new pathways for controlled and nuanced human-machine interactions in domains such as healthcare, education, and interviewing processes, bringing us one step closer to truly human-like social machines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨äººæ ¼å»ºæ¨¡ä¸­ä¾èµ–Big Fiveæ¡†æ¶å¯¼è‡´ç»´åº¦ç²—ç•¥ä¸”ç¼ºä¹å¼ºåº¦æ§åˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†SACæ¡†æ¶ï¼Œç”¨äºæµ‹é‡å¹¶è¯±å¯¼å…·æœ‰åŠ¨æ€å¼ºåº¦æ§åˆ¶çš„äººæ ¼ç‰¹è´¨ã€‚ç ”ç©¶è€…é€šè¿‡å°†Machine Personality Inventory (MPI)æ‰©å±•è‡³16 Personality Factor (16PF)æ¨¡å‹ï¼Œå®ç°äº†å¯¹16ä¸ªä¸åŒç‰¹è´¨çš„ç²¾ç»†åŒ–æ§åˆ¶ã€‚SACæ¡†æ¶å¼•å…¥äº†åŸºäºå½¢å®¹è¯çš„è¯­ä¹‰é”šå®š(semantic anchoring)æŠ€æœ¯ï¼Œå¹¶ç»“åˆFrequencyã€Depthã€Thresholdã€Effortå’ŒWillingnessäº”ä¸ªç»´åº¦æ¥åŠ¨æ€è¯±å¯¼ç‰¹è´¨å¼ºåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†ç‰¹è´¨å»ºæ¨¡ä¸ºè¿ç»­è°±ç³»(continuous spectrum)æ¯”ä¼ ç»Ÿçš„äºŒå…ƒåˆ‡æ¢(binary toggling)èƒ½äº§ç”Ÿæ›´ä¸€è‡´ä¸”å¯æ§çš„äººæ ¼è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç›®æ ‡ç‰¹è´¨å¼ºåº¦çš„å˜åŒ–ä¼šç³»ç»Ÿæ€§åœ°å½±å“å¿ƒç†å­¦ç›¸å…³çš„å…¶ä»–ç‰¹è´¨ï¼Œæš—ç¤ºLLMså·²å†…åŒ–äº†å¤šç»´äººæ ¼ç»“æ„è€Œéå­¤ç«‹å¤„ç†å•ä¸€ç‰¹è´¨ã€‚è¯¥å·¥ä½œä¸ºåŒ»ç–—ã€æ•™è‚²å’Œè®¿è°ˆç­‰é¢†åŸŸä¸­æ›´å…·äººæ€§åŒ–ã€æ›´ç»†è…»çš„äººæœºäº¤äº’å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted into 18th Edition of International Conference on Agents and Artificial Intelligence (ICAART)",
      "pdf_url": "https://arxiv.org/pdf/2506.20993v2",
      "published_date": "2025-06-26 04:12:15 UTC",
      "updated_date": "2026-01-12 19:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:15.629168+00:00"
    },
    {
      "arxiv_id": "2506.22512v1",
      "title": "Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions",
      "title_zh": "å…ˆé—®è€Œåå»ºï¼šé‡æ–°æ€è€ƒäººå£è´©å–å¹²é¢„ä¸­çš„â€œAIå‘å–„â€",
      "authors": [
        "Pratheeksha Nair",
        "Gabriel Lefebvre",
        "Sophia Garrel",
        "Maryam Molamohammadi",
        "Reihaneh Rabbany"
      ],
      "abstract": "AI for good initiatives often rely on the assumption that technical interventions can resolve complex social problems. In the context of human trafficking (HT), such techno-solutionism risks oversimplifying exploitation, reinforcing power imbalances and causing harm to the very communities AI claims to support. In this paper, we introduce the Radical Questioning (RQ) framework as a five step, pre-project ethical assessment tool to critically evaluate whether AI should be built at all, especially in domains involving marginalized populations and entrenched systemic injustice. RQ does not replace principles based ethics but precedes it, offering an upstream, deliberative space to confront assumptions, map power, and consider harms before design. Using a case study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural complexities and guides us away from surveillance based interventions toward survivor empowerment tools. While developed in the context of HT, RQ's five step structure can generalize to other domains, though the specific questions must be contextual. This paper situates RQ within a broader AI ethics philosophy that challenges instrumentalist norms and centers relational, reflexive responsibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½å…¬ç›Š(AI-for-Good)é¢†åŸŸä¸­æ™®éå­˜åœ¨çš„â€œæŠ€æœ¯æ–¹æ¡ˆä¸»ä¹‰â€(Techno-solutionism)å€¾å‘ï¼Œæ¢è®¨äº†å…¶åœ¨æ‰“å‡»äººå£è´©å–(Human Trafficking)ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´çš„ç¤¾ä¼šé—®é¢˜ç®€åŒ–ã€æƒåŠ›å¤±è¡¡åŠå¯¹å¼±åŠ¿ç¾¤ä½“çš„æ½œåœ¨ä¼¤å®³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æ¿€è¿›æé—®(Radical Questioning, RQ)æ¡†æ¶ï¼Œä½œä¸ºä¸€ç§åŒ…å«äº”ä¸ªæ­¥éª¤çš„é¢„é¡¹ç›®ä¼¦ç†è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨é¡¹ç›®è®¾è®¡å‰æ‰¹åˆ¤æ€§åœ°è¯„ä¼°æ˜¯å¦åº”å½“æ„å»ºAIç³»ç»Ÿã€‚RQæ¡†æ¶å¹¶ä¸å–ä»£ä¼ ç»Ÿçš„ä¼¦ç†åŸåˆ™ï¼Œè€Œæ˜¯ä½œä¸ºå…¶å‰ç½®ç¯èŠ‚ï¼Œæä¾›äº†ä¸€ä¸ªä¸Šæ¸¸çš„å®¡è®®ç©ºé—´æ¥æŒ‘æˆ˜å‡è®¾ã€æ˜ å°„æƒåŠ›å…³ç³»å¹¶é¢„åˆ¤é£é™©ã€‚é€šè¿‡å¯¹äººå£è´©å–å¹²é¢„çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å¦‚ä½•æ­ç¤ºå¤æ‚çš„ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ï¼Œå¹¶å¼•å¯¼æŠ€æœ¯ä»åŸºäºç›‘è§†çš„å¹²é¢„è½¬å‘å¹¸å­˜è€…èµ‹èƒ½(Survivor Empowerment)å·¥å…·ã€‚è™½ç„¶æºäºç‰¹å®šé¢†åŸŸï¼Œä½†RQçš„äº”æ­¥ç»“æ„å¯æ³›åŒ–è‡³å…¶ä»–æ¶‰åŠç³»ç»Ÿæ€§ä¸å…¬çš„åœºæ™¯ï¼Œå¼ºè°ƒäº†åœ¨AIå¼€å‘ä¸­å»ºç«‹å…³ç³»æ€§ä¸åæ€æ€§è´£ä»»çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22512v1",
      "published_date": "2025-06-26 04:05:15 UTC",
      "updated_date": "2025-06-26 04:05:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:21.445492+00:00"
    },
    {
      "arxiv_id": "2506.20988v2",
      "title": "Segment Anything in Pathology Images with Natural Language",
      "title_zh": "åŸºäºè‡ªç„¶è¯­è¨€çš„ç—…ç†å›¾åƒä¸‡ç‰©åˆ†å‰²",
      "authors": [
        "Zhixuan Chen",
        "Junlin Hou",
        "Liqi Lin",
        "Yihui Wang",
        "Yequan Bie",
        "Xi Wang",
        "Yanning Zhou",
        "Ronald Cheong Kin Chan",
        "Hao Chen"
      ],
      "abstract": "Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg, the largest and most comprehensive dataset for pathology segmentation, built from 21 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentor's outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PathSegmentorï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºç—…ç†å›¾åƒè®¾è®¡çš„æ–‡æœ¬æç¤º(text-prompted)åˆ†å‰²åŸºåº§æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºæ ‡æ³¨æ•°æ®æœ‰é™å’Œç±»åˆ«å®šä¹‰å—é™å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†PathSegæ•°æ®é›†ï¼Œå®ƒæ˜¯ç›®å‰æœ€å¤§ã€æœ€å…¨é¢çš„ç—…ç†åˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª21ä¸ªå…¬å¼€æºçš„27.5ä¸‡ä¸ªå›¾åƒ-æ©ç -æ ‡ç­¾ä¸‰å…ƒç»„ï¼Œæ¶µç›–160ä¸ªä¸åŒç±»åˆ«ã€‚PathSegmentorå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ‰§è¡Œè¯­ä¹‰åˆ†å‰²(semantic segmentation)ï¼Œæ— éœ€å†ä½¿ç”¨ç‚¹æˆ–æ¡†ç­‰å¤æ‚çš„ç©ºé—´è¾“å…¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPathSegmentorçš„æ•´ä½“Diceåˆ†æ•°ä¼˜äºç°æœ‰çš„ç©ºé—´æç¤ºå’Œæ–‡æœ¬æç¤ºæ¨¡å‹ï¼Œåˆ†åˆ«é¢†å…ˆ0.145å’Œ0.429ï¼Œå¹¶åœ¨å¤æ‚ç»“æ„åˆ†å‰²å’Œå¤–éƒ¨æ•°æ®é›†æ³›åŒ–ä¸Šè¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚é€šè¿‡ç‰¹å¾é‡è¦æ€§ä¼°è®¡å’Œå½±åƒç”Ÿç‰©æ ‡å¿—ç‰©(imaging biomarker)å‘ç°ï¼Œè¯¥æ¨¡å‹æå‡äº†è¯Šæ–­çš„å¯è§£é‡Šæ€§ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›äº†å¾ªè¯æ”¯æŒã€‚è¿™ä¸€ç ”ç©¶æˆæœæå¤§åœ°æ¨åŠ¨äº†ç²¾å‡†è‚¿ç˜¤å­¦ä¸­å¯è§£é‡Šäººå·¥æ™ºèƒ½(explainable AI)çš„å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20988v2",
      "published_date": "2025-06-26 04:01:40 UTC",
      "updated_date": "2025-08-19 03:06:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:23.315506+00:00"
    },
    {
      "arxiv_id": "2506.20980v1",
      "title": "Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs",
      "title_zh": "å¢å¼ºåŒè´¨-å¼‚è´¨åˆ†ç¦»ï¼šå¼‚æ„å›¾ä¸­çš„å…³ç³»æ„ŸçŸ¥å­¦ä¹ ",
      "authors": [
        "Ziyu Zheng",
        "Yaming Yang",
        "Ziyu Guan",
        "Wei Zhao",
        "Weigang Lu"
      ],
      "abstract": "Real-world networks usually have a property of node heterophily, that is, the connected nodes usually have different features or different labels. This heterophily issue has been extensively studied in homogeneous graphs but remains under-explored in heterogeneous graphs, where there are multiple types of nodes and edges. Capturing node heterophily in heterogeneous graphs is very challenging since both node/edge heterogeneity and node heterophily should be carefully taken into consideration. Existing methods typically convert heterogeneous graphs into homogeneous ones to learn node heterophily, which will inevitably lose the potential heterophily conveyed by heterogeneous relations. To bridge this gap, we propose Relation-Aware Separation of Homophily and Heterophily (RASH), a novel contrastive learning framework that explicitly models high-order semantics of heterogeneous interactions and adaptively separates homophilic and heterophilic patterns. Particularly, RASH introduces dual heterogeneous hypergraphs to encode multi-relational bipartite subgraphs and dynamically constructs homophilic graphs and heterophilic graphs based on relation importance. A multi-relation contrastive loss is designed to align heterogeneous and homophilic/heterophilic views by maximizing mutual information. In this way, RASH simultaneously resolves the challenges of heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on benchmark datasets demonstrate the effectiveness of RASH across various downstream tasks. The code is available at: https://github.com/zhengziyu77/RASH.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚è´¨å›¾(Heterogeneous Graphs)ä¸­èŠ‚ç‚¹å¼‚è´¨æ€§(Heterophily)å¤„ç†ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºRASH (Relation-Aware Separation of Homophily and Heterophily)çš„å¯¹æ¯”å­¦ä¹ (Contrastive Learning)æ¡†æ¶ã€‚ä¸åŒäºå°†å¼‚è´¨å›¾ç®€åŒ–ä¸ºåŒè´¨å›¾çš„ä¼ ç»Ÿåšæ³•ï¼ŒRASHé€šè¿‡æ˜¾å¼å»ºæ¨¡é«˜é˜¶è¯­ä¹‰ï¼Œæœ‰æ•ˆä¿ç•™äº†ç”±å¼‚è´¨å…³ç³»ä¼ è¾¾çš„æ½œåœ¨æ¨¡å¼ã€‚è¯¥æ¡†æ¶å¼•å…¥åŒå¼‚è´¨è¶…å›¾(Dual Heterogeneous Hypergraphs)æ¥ç¼–ç å¤šå…³ç³»äºŒåˆ†å­å›¾ï¼Œå¹¶æ ¹æ®å…³ç³»é‡è¦æ€§åŠ¨æ€æ„å»ºåŒè´¨å›¾(Homophilic Graphs)å’Œå¼‚è´¨å›¾(Heterophilic Graphs)ã€‚é€šè¿‡è®¾è®¡å¤šå…³ç³»å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ŒRASHèƒ½å¤Ÿé€šè¿‡æœ€å¤§åŒ–äº’ä¿¡æ¯æ¥å¯¹é½å¼‚è´¨è§†å›¾ä¸åŒè´¨/å¼‚è´¨è§†å›¾ï¼Œä»è€ŒåŒæ—¶è§£å†³å›¾çš„å¼‚è´¨æ€§(Heterogeneity)ä¸åŒè´¨å¼‚è´¨æ€§(Heterophily)åˆ†ç¦»æŒ‘æˆ˜ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†RASHåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç›®å‰å…¶ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "accepted by KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.20980v1",
      "published_date": "2025-06-26 03:54:06 UTC",
      "updated_date": "2025-06-26 03:54:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:28.270767+00:00"
    },
    {
      "arxiv_id": "2507.02928v1",
      "title": "Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é€šè¿‡æ¸è¿›å¼æ··æ‚å› ç´ å½’è¡¥ç¼“è§£éšæ€§æ··æ‚",
      "authors": [
        "Hao Yang",
        "Haoxuan Li",
        "Luyu Chen",
        "Haoxiang Wang",
        "Xu Chen",
        "Mingming Gong"
      ],
      "abstract": "Hidden confounding remains a central challenge in estimating treatment effects from observational data, as unobserved variables can lead to biased causal estimates. While recent work has explored the use of large language models (LLMs) for causal inference, most approaches still rely on the unconfoundedness assumption. In this paper, we make the first attempt to mitigate hidden confounding using LLMs. We propose ProCI (Progressive Confounder Imputation), a framework that elicits the semantic and world knowledge of LLMs to iteratively generate, impute, and validate hidden confounders. ProCI leverages two key capabilities of LLMs: their strong semantic reasoning ability, which enables the discovery of plausible confounders from both structured and unstructured inputs, and their embedded world knowledge, which supports counterfactual reasoning under latent confounding. To improve robustness, ProCI adopts a distributional reasoning strategy instead of direct value imputation to prevent the collapsed outputs. Extensive experiments demonstrate that ProCI uncovers meaningful confounders and significantly improves treatment effect estimation across various datasets and LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ProCIï¼ˆProgressive Confounder Imputationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»è§‚æµ‹æ•°æ®ä¸­ä¼°è®¡å¤„ç†æ•ˆåº”ï¼ˆTreatment Effectsï¼‰æ—¶ç”±äºæœªè§‚æµ‹å˜é‡å¯¼è‡´çš„éšæ€§æ··æ‚ï¼ˆHidden Confoundingï¼‰åå·®ã€‚ProCIé¦–æ¬¡å°è¯•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†æ¥è¿­ä»£åœ°ç”Ÿæˆã€å¡«å……å¹¶éªŒè¯éšæ€§æ··æ‚å› ç´ ã€‚é€šè¿‡æŒ–æ˜ç»“æ„åŒ–ä¸éç»“æ„åŒ–è¾“å…¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æ½œåœ¨çš„æ··æ‚å› ç´ ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œæ½œåœ¨æ··æ‚ä¸‹çš„åäº‹å®æ¨ç†ï¼ˆCounterfactual Reasoningï¼‰ã€‚ä¸ºå¢å¼ºé²æ£’æ€§å¹¶é˜²æ­¢è¾“å‡ºå´©æºƒï¼ŒProCIé‡‡ç”¨äº†åˆ†å¸ƒæ¨ç†ç­–ç•¥ï¼ˆDistributional Reasoning Strategyï¼‰è€Œéç›´æ¥çš„æ•°å€¼å¡«å……ã€‚å®éªŒè¯æ˜ï¼ŒProCIèƒ½å¤Ÿæœ‰æ•ˆå‘ç°æœ‰æ„ä¹‰çš„æ··æ‚å› ç´ ï¼Œå¹¶åœ¨å¤šç§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†å¤„ç†æ•ˆåº”ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02928v1",
      "published_date": "2025-06-26 03:49:13 UTC",
      "updated_date": "2025-06-26 03:49:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:33.967347+00:00"
    },
    {
      "arxiv_id": "2506.20977v2",
      "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging",
      "title_zh": "From Cradle to Caneï¼šé«˜ä¿çœŸå…¨ç”Ÿå‘½å‘¨æœŸäººè„¸è€åŒ–çš„åŒé˜¶æ®µæ¡†æ¶",
      "authors": [
        "Tao Liu",
        "Dafeng Zhang",
        "Gengchen Li",
        "Shizhuo Liu",
        "Yongqi Song",
        "Senmao Li",
        "Shiqi Yang",
        "Boqian Li",
        "Kai Wang",
        "Yaxing Wang"
      ],
      "abstract": "Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency. Code is available at https://github.com/byliutao/Cradle2Cane.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººè„¸è€åŒ–(Face Aging)ä¸­å…¨å¯¿å‘½å‘¨æœŸå˜æ¢ä¸çœŸå®ã€å¤§å¹´é¾„è·¨åº¦åŠæç«¯å¤´éƒ¨å§¿æ€ä¸‹éš¾ä»¥å¹³è¡¡å¹´é¾„å‡†ç¡®åº¦ä¸èº«ä»½ä¿æŒ(Age-ID trade-off)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCradle2Caneçš„ä¸¤é˜¶æ®µäººè„¸è€åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºå°‘æ­¥æ–‡å­—ç”Ÿæˆå›¾åƒ(few-step T2I)æ‰©æ•£æ¨¡å‹æ„å»ºï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡å¼•å…¥å—å¹´é¾„å’Œæ€§åˆ«æç¤ºè¯å¼•å¯¼çš„è‡ªé€‚åº”å™ªå£°æ³¨å…¥(AdaNI)æœºåˆ¶ï¼Œä¾§é‡äºè§£å†³å¹´é¾„è½¬æ¢çš„å‡†ç¡®æ€§ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨SVR-ArcFaceå’ŒRotate-CLIPä¸¤ç§èº«ä»½æ„ŸçŸ¥åµŒå…¥(IDEmb)å¯¹æ¨¡å‹è¿›è¡Œè°ƒèŠ‚ï¼Œåœ¨ä¿æŒå¹´é¾„ç‰¹å¾çš„åŒæ—¶æ˜¾è‘—å¢å¼ºèº«ä»½ä¿æŒèƒ½åŠ›ã€‚æ•´ä¸ªæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯(end-to-end)çš„æ–¹å¼è¿›è¡Œè”åˆè®­ç»ƒï¼Œç¡®ä¿äº†å˜æ¢è¿‡ç¨‹çš„è‡ªç„¶ä¸è¿è´¯ã€‚åœ¨CelebA-HQæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCradle2Caneåœ¨å¹´é¾„å‡†ç¡®åº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰äººè„¸è€åŒ–æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 12 figures, NeurIPS 2025 Poster",
      "pdf_url": "https://arxiv.org/pdf/2506.20977v2",
      "published_date": "2025-06-26 03:48:28 UTC",
      "updated_date": "2025-10-20 04:50:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:33.816499+00:00"
    },
    {
      "arxiv_id": "2506.22511v2",
      "title": "Lighting the Night with Generative Artificial Intelligence",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç‚¹äº®é»‘å¤œ",
      "authors": [
        "Tingting Zhou",
        "Feng Zhang",
        "Haoyang Fu",
        "Baoxiang Pan",
        "Renhe Zhang",
        "Feng Lu",
        "Zhixin Yang"
      ],
      "abstract": "The visible light reflectance data from geostationary satellites is crucial for meteorological observations and plays an important role in weather monitoring and forecasting. However, due to the lack of visible light at night, it is impossible to conduct continuous all-day weather observations using visible light reflectance data. This study pioneers the use of generative diffusion models to address this limitation. Based on the multi-band thermal infrared brightness temperature data from the Advanced Geostationary Radiation Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we developed a high-precision visible light reflectance generative model, called Reflectance Diffusion (RefDiff), which enables 0.47~Î¼\\mathrm{m}, 0.65~Î¼\\mathrm{m}, and 0.825~Î¼\\mathrm{m} bands visible light reflectance generation at night. Compared to the classical models, RefDiff not only significantly improves accuracy through ensemble averaging but also provides uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90, with particularly significant improvements in areas with complex cloud structures and thick clouds. The model's nighttime generation capability was validated using VIIRS nighttime product, demonstrating comparable performance to its daytime counterpart. In summary, this research has made substantial progress in the ability to generate visible light reflectance at night, with the potential to expand the application of nighttime visible light data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™æ­¢æ°”è±¡å«æ˜Ÿåœ¨å¤œé—´ç¼ºä¹å¯è§å…‰åå°„æ•°æ®è€Œæ— æ³•è¿›è¡Œå…¨å¤©å€™è§‚æµ‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºReflectance Diffusion (RefDiff) çš„é«˜ç²¾åº¦ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é£äº‘å››å·Bæ˜Ÿ(FY4B)æ­è½½çš„å…ˆè¿›é™æ­¢è½¨é“è¾å°„æˆåƒä»ª(AGRI)æä¾›çš„å¤šæ³¢æ®µçƒ­çº¢å¤–äº®åº¦æ¸©åº¦æ•°æ®ï¼ŒæˆåŠŸå®ç°äº†å¤œé—´ 0.47Î¼mã€0.65Î¼m å’Œ 0.825Î¼m æ³¢æ®µå¯è§å…‰åå°„ç‡çš„åˆæˆã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼ŒRefDiff é€šè¿‡é›†æˆå¹³å‡(ensemble averaging)æ˜¾è‘—æå‡äº†ç”Ÿæˆç²¾åº¦ï¼Œå¹¶å…·å¤‡æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„ SSIM æŒ‡æ ‡å¯è¾¾ 0.90ï¼Œåœ¨å¤„ç†å¤æ‚äº‘ç»“æ„å’Œåšäº‘åŒºåŸŸæ—¶è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚é€šè¿‡ VIIRS å¤œé—´äº§å“çš„äº¤å‰éªŒè¯ï¼ŒRefDiff åœ¨å¤œé—´çš„ç”Ÿæˆæ€§èƒ½ä¸ç™½å¤©è§‚æµ‹ç»“æœç›¸å½“ï¼Œä¸ºå…¨å¤©å€™è¿ç»­æ°”è±¡ç›‘æµ‹åŠå¤œé—´å«æ˜Ÿæ•°æ®çš„æ·±åº¦åº”ç”¨åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Title corrected (Lightning to Lighting); terminology updated (retrieval to generative)",
      "pdf_url": "https://arxiv.org/pdf/2506.22511v2",
      "published_date": "2025-06-26 03:21:08 UTC",
      "updated_date": "2025-07-11 12:03:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:36.764693+00:00"
    },
    {
      "arxiv_id": "2506.22510v1",
      "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning",
      "title_zh": "è¿ˆå‘æ— æ–‡æœ¬å›¾åŸºç¡€æ¨¡å‹ï¼šé‡æ–°æ€è€ƒå¤šé¢†åŸŸå›¾å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Zihao Zhao",
        "Xinlong Zhai",
        "Jinyu Yang",
        "Chuan Shi"
      ],
      "abstract": "Foundation models have achieved great success in natural language processing (NLP) and computer vision (CV). Their success largely stems from the ability to integrate multi-domain knowledge in pre-training and transfer it to target domains. Considering graph data, especially graphs without textual features, is ubiquitous in real-world applications such as social networks and recommendation systems, some researchers have attempted to extend this paradigm to the graph field, aiming to construct graph foundation models. However, unlike CV and NLP, there are huge gaps among the semantics and properties of graphs in different domains, while current works still adopt traditional contrastive pre-training strategies designed in the single-domain scenario, which regard contrastive samples from different domains as equivalent. From experimental investigations, we discovered that inherent domain-specific differences prevent these strategies from effectively absorbing knowledge from different domains to generate informative representations. In this paper, we propose a novel multi-domain pre-training and cross-domain transfer framework, namely MDGCL.In the pre-training stage, we design a contrastive learning strategy to substantially recognize and capture domain differences, and introduce domain tokens to encode domain-level global information. In the downstream stage, we introduce a domain attention mechanism to enable fine-grained domain knowledge transfer. Extensive experiments on five benchmark datasets have demonstrated that our method outperforms state-of-the-art significantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\% on Macro-F1 score.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤ç½‘ç»œå’Œæ¨èç³»ç»Ÿç­‰ç°å®åœºæ™¯ä¸­å¹¿æ³›å­˜åœ¨çš„æ— æ–‡æœ¬ç‰¹å¾å›¾æ•°æ®ï¼Œæ¢è®¨äº†æ„å»ºå›¾åŸºç¡€æ¨¡å‹ (Graph Foundation Models) çš„æŒ‘æˆ˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œç”±äºä¸åŒé¢†åŸŸå›¾æ•°æ®çš„è¯­ä¹‰å’Œå±æ€§å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œç°æœ‰çš„å•é¢†åŸŸå¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥åœ¨å¤„ç†å¤šé¢†åŸŸæ•°æ®æ—¶æ— æ³•æœ‰æ•ˆå¸æ”¶å¼‚æ„çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º MDGCL çš„å¤šé¢†åŸŸé¢„è®­ç»ƒä¸è·¨é¢†åŸŸè¿ç§»æ¡†æ¶ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒMDGCL é€šè¿‡æ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¯†åˆ«å¹¶æ•è·é¢†åŸŸå·®å¼‚ï¼Œå¹¶å¼•å…¥é¢†åŸŸæ ‡è®° (domain tokens) ä»¥ç¼–ç é¢†åŸŸçº§çš„å…¨å±€ä¿¡æ¯ã€‚åœ¨ä¸‹æ¸¸é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢†åŸŸæ³¨æ„åŠ›æœºåˆ¶ (domain attention mechanism) å®ç°ç»†ç²’åº¦çš„é¢†åŸŸçŸ¥è¯†è¿ç§»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå…¶å‡†ç¡®ç‡å’Œ Macro-F1 åˆ†æ•°æœ€é«˜åˆ†åˆ«æå‡äº† 19.33% å’Œ 19.13%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.22510v1",
      "published_date": "2025-06-26 03:14:50 UTC",
      "updated_date": "2025-06-26 03:14:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:39.967529+00:00"
    },
    {
      "arxiv_id": "2506.20967v2",
      "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing",
      "title_zh": "DFVEditï¼šé¢å‘é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘çš„æ¡ä»¶å¢é‡æµå‘é‡",
      "authors": [
        "Lingling Cai",
        "Kang Zhao",
        "Hangjie Yuan",
        "Xiang Wang",
        "Yingya Zhang",
        "Kejie Huang"
      ],
      "abstract": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Video Diffusion Transformers (Video DiTs) åœ¨è§†é¢‘ç¼–è¾‘ä¸­å› æ³¨æ„åŠ›ä¿®æ”¹æˆ–å¾®è°ƒå¸¦æ¥çš„å·¨å¤§è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº† DFVEditï¼Œä¸€ç§ä¸“ä¸º Video DiTs è®¾è®¡çš„é«˜æ•ˆé›¶æ ·æœ¬ (Zero-shot) è§†é¢‘ç¼–è¾‘æ–¹æ³•ã€‚DFVEdit æ‘†è„±äº†å¯¹æ³¨æ„åŠ›ä¿®æ”¹å’Œå¾®è°ƒçš„éœ€æ±‚ï¼Œé€šè¿‡æµå˜æ¢ (flow transformation) ç›´æ¥ä½œç”¨äºå¹²å‡€çš„æ½œç©ºé—´ (clean latents)ï¼Œå°†ç¼–è¾‘ä¸é‡‡æ ·ç»Ÿä¸€åœ¨è¿ç»­æµè§†è§’ä¸‹ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¡ä»¶å¢é‡æµå‘é‡ (Conditional Delta Flow Vector, CDFV) ä»¥å®ç°ç†è®ºæ— åä¼°è®¡ï¼Œå¹¶é›†æˆéšå¼äº¤å‰æ³¨æ„åŠ› (Implicit Cross Attention, ICA) æŒ‡å¼•å’ŒåµŒå…¥å¼ºåŒ– (Embedding Reinforcement, ER) æ¥æå‡ç¼–è¾‘è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDFVEdit åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”åŸºäºæ³¨æ„åŠ›å·¥ç¨‹çš„æ–¹æ³•å¿« 20 å€ä»¥ä¸Šï¼Œä¸”å†…å­˜å ç”¨å‡å°‘äº† 85%ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼åº”ç”¨äº CogVideoX å’Œ Wan2.1 ç­‰ä¸»æµ Video DiTs æ¨¡å‹ï¼Œåœ¨ç»“æ„ä¿çœŸåº¦ã€æ—¶ç©ºä¸€è‡´æ€§å’Œç¼–è¾‘è´¨é‡ä¸Šå‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Zero-shot video editing",
      "pdf_url": "https://arxiv.org/pdf/2506.20967v2",
      "published_date": "2025-06-26 03:10:13 UTC",
      "updated_date": "2025-06-27 08:42:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:54.035058+00:00"
    },
    {
      "arxiv_id": "2506.20966v1",
      "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends",
      "title_zh": "VLAæ¨¡å‹åè®­ç»ƒä¸äººç±»è¿åŠ¨å­¦ä¹ çš„ç±»æ¯”ï¼šç ”ç©¶è¿›å±•ã€æŒ‘æˆ˜ä¸è¶‹åŠ¿",
      "authors": [
        "Tian-Yu Xiang",
        "Ao-Qun Jin",
        "Xiao-Hu Zhou",
        "Mei-Jiang Gui",
        "Xiao-Liang Xie",
        "Shi-Qi Liu",
        "Shuang-Yi Wang",
        "Sheng-Bin Duan",
        "Fu-Chao Xie",
        "Wen-Kai Wang",
        "Si-Cheng Wang",
        "Ling-Yun Li",
        "Tian Tu",
        "Zeng-Guang Hou"
      ],
      "abstract": "Vision-language-action (VLA) models extend vision-language models (VLM) by integrating action generation modules for robotic manipulation. Leveraging strengths of VLM in vision perception and instruction understanding, VLA models exhibit promising generalization across diverse manipulation tasks. However, applications demanding high precision and accuracy reveal performance gaps without further adaptation. Evidence from multiple domains highlights the critical role of post-training to align foundational models with downstream applications, spurring extensive research on post-training VLA models. VLA model post-training aims to address the challenge of improving an embodiment's ability to interact with the environment for the given tasks, analogous to the process of humans motor skills acquisition. Accordingly, this paper reviews post-training strategies for VLA models through the lens of human motor learning, focusing on three dimensions: environments, embodiments, and tasks. A structured taxonomy is introduced aligned with human learning mechanisms: (1) enhancing environmental perception, (2) improving embodiment awareness, (3) deepening task comprehension, and (4) multi-component integration. Finally, key challenges and trends in post-training VLA models are identified, establishing a conceptual framework to guide future research. This work delivers both a comprehensive overview of current VLA model post-training methods from a human motor learning perspective and practical insights for VLA model development. (Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Vision-language-action (VLA) æ¨¡å‹åœ¨ Post-training é˜¶æ®µçš„ç­–ç•¥ï¼Œå¹¶å°†å…¶ä¸äººç±»è¿åŠ¨å­¦ä¹  (Human motor learning) çš„è¿‡ç¨‹è¿›è¡Œç±»æ¯”ã€‚æ–‡ç« é’ˆå¯¹ VLA æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­ç²¾åº¦ä¸è¶³çš„æŒ‘æˆ˜ï¼Œä»ç¯å¢ƒã€å…·èº« (Embodiment) å’Œä»»åŠ¡ä¸‰ä¸ªç»´åº¦ç³»ç»Ÿæ€§åœ°å›é¡¾äº†ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäººç±»å­¦ä¹ æœºåˆ¶çš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº†å¢å¼ºç¯å¢ƒæ„ŸçŸ¥ã€æå‡å…·èº«æ„è¯†ã€æ·±åŒ–ä»»åŠ¡ç†è§£ä»¥åŠå¤šç»„ä»¶é›†æˆç­‰æ ¸å¿ƒæ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯†åˆ«äº†è¯¥é¢†åŸŸå½“å‰çš„å…³é”®æŒ‘æˆ˜ä¸æœªæ¥è¶‹åŠ¿ï¼Œæ—¨åœ¨ä¸ºæ„å»ºé«˜ç²¾åº¦çš„è‡ªä¸»äº¤äº’ç³»ç»Ÿæä¾›ç†è®ºæŒ‡å¯¼ã€‚è¯¥å·¥ä½œä¸º VLA æ¨¡å‹çš„è¿›ä¸€æ­¥å¼€å‘æä¾›äº†å…¨é¢çš„æŠ€æœ¯ç»¼è¿°ä¸å®è·µè§è§£ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20966v1",
      "published_date": "2025-06-26 03:06:57 UTC",
      "updated_date": "2025-06-26 03:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:56.194292+00:00"
    },
    {
      "arxiv_id": "2506.20964v1",
      "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human pathology",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“ Copilot çš„äººç±»ç—…ç†å­¦å¾ªè¯è¯Šæ–­æ¨ç†",
      "authors": [
        "Chengkuan Chen",
        "Luca L. Weishaupt",
        "Drew F. K. Williamson",
        "Richard J. Chen",
        "Tong Ding",
        "Bowen Chen",
        "Anurag Vaidya",
        "Long Phi Le",
        "Guillaume Jaume",
        "Ming Y. Lu",
        "Faisal Mahmood"
      ],
      "abstract": "Pathology is experiencing rapid digital transformation driven by whole-slide imaging and artificial intelligence (AI). While deep learning-based computational pathology has achieved notable success, traditional models primarily focus on image analysis without integrating natural language instruction or rich, text-based context. Current multimodal large language models (MLLMs) in computational pathology face limitations, including insufficient training data, inadequate support and evaluation for multi-image understanding, and a lack of autonomous, diagnostic reasoning capabilities. To address these limitations, we introduce PathChat+, a new MLLM specifically designed for human pathology, trained on over 1 million diverse, pathology-specific instruction samples and nearly 5.5 million question answer turns. Extensive evaluations across diverse pathology benchmarks demonstrated that PathChat+ substantially outperforms the prior PathChat copilot, as well as both state-of-the-art (SOTA) general-purpose and other pathology-specific models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching high accuracy on DDxBench, a challenging open-ended differential diagnosis benchmark, while also capable of generating visually grounded, humanly-interpretable summary reports.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PathChat+ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºäººç±»ç—…ç†å­¦è®¾è®¡çš„æ–°å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ï¼Œé€šè¿‡è¶…è¿‡100ä¸‡ä¸ªç—…ç†ç‰¹å®šæŒ‡ä»¤æ ·æœ¬å’Œè¿‘550ä¸‡è½®é—®ç­”è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤šå›¾åƒç†è§£å’Œè‡ªä¸»è¯Šæ–­æ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚å®éªŒè¯æ˜PathChat+åœ¨å¤šé¡¹ç—…ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…ˆå‰çš„PathChatã€é€šç”¨SOTAæ¨¡å‹åŠå…¶ä»–ç—…ç†ä¸“ç”¨æ¨¡å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†SlideSeekï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºPathChat+çš„æ¨ç†é©±åŠ¨å‹å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œèƒ½å¤Ÿé€šè¿‡è¿­ä»£ã€å±‚æ¬¡åŒ–çš„è¯Šæ–­æ¨ç†è‡ªä¸»è¯„ä¼°åäº¿åƒç´ çº§å…¨æ‰«æåˆ‡ç‰‡å›¾åƒ(WSIs)ã€‚è¯¥ç³»ç»Ÿåœ¨æŒ‘æˆ˜æ€§çš„DDxBenché‰´åˆ«è¯Šæ–­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é«˜å‡†ç¡®ç‡ï¼Œå¹¶èƒ½ç”Ÿæˆå…·æœ‰è§†è§‰å¼•å¯¼ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è¯Šæ–­æ‘˜è¦æŠ¥å‘Šï¼Œä¸ºç—…ç†å­¦çš„æ•°å­—åŒ–è½¬å‹æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20964v1",
      "published_date": "2025-06-26 03:02:16 UTC",
      "updated_date": "2025-06-26 03:02:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:55:59.627510+00:00"
    },
    {
      "arxiv_id": "2506.22509v1",
      "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment",
      "title_zh": "FreeDNAï¼šé€šè¿‡å…è®­ç»ƒé¢†åŸŸå™ªå£°å¯¹é½èµ‹äºˆåŸºäºæ‰©æ•£æ¨¡å‹çš„å¯†é›†é¢„æµ‹é¢†åŸŸè‡ªé€‚åº”èƒ½åŠ›",
      "authors": [
        "Hang Xu",
        "Jie Huang",
        "Linjiang Huang",
        "Dong Li",
        "Yidi Liu",
        "Feng Zhao"
      ],
      "abstract": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FreeDNAï¼Œæ—¨åœ¨ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†é¢„æµ‹(Diffusion-based Dense Prediction, DDP)æ¨¡å‹æä¾›æ— éœ€è®­ç»ƒçš„é¢†åŸŸè‡ªé€‚åº”(Domain Adaptation, DA)èƒ½åŠ›ã€‚ç ”ç©¶è€…å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„æ›å…‰åå·®(exposure bias)æ˜¯å¯¼è‡´é¢†åŸŸåç§»(domain shift)çš„å…³é”®å› ç´ ï¼Œè€Œå™ªå£°é¢„æµ‹ç»Ÿè®¡æ•°æ®èƒ½æœ‰æ•ˆæ•æ‰ä¸åŒé¢†åŸŸçš„ä¿¡æ¯ã€‚åŸºäºæ­¤ï¼ŒFreeDNAå¼•å…¥äº†ä¸€ç§é¢†åŸŸå™ªå£°å¯¹é½(Domain Noise Alignment, DNA)æœºåˆ¶ï¼Œåœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­é€šè¿‡ç¼“è§£å™ªå£°ç»Ÿè®¡çš„æ³¢åŠ¨æ¥å®ç°é¢†åŸŸè‡ªé€‚åº”ã€‚åœ¨æºåŸŸå¯ç”¨æ—¶ï¼Œè¯¥æ–¹æ³•ç›´æ¥å°†ç›®æ ‡åŸŸçš„å™ªå£°ç»Ÿè®¡ä¸æºåŸŸå¯¹é½ï¼›è€Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ— æºé¢†åŸŸè‡ªé€‚åº”(Source-free DA)åœºæ™¯ä¸‹ï¼Œåˆ™åˆ©ç”¨é«˜ç½®ä¿¡åº¦åŒºåŸŸçš„ç»Ÿè®¡ä¿¡æ¯é€æ­¥å¼•å¯¼é‡‡æ ·è¿‡ç¨‹ä¸­çš„å™ªå£°è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeDNAåœ¨å››é¡¹é€šç”¨çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­å‡èƒ½æœ‰æ•ˆå¢å¼ºDDPæ¨¡å‹çš„é¢†åŸŸè‡ªé€‚åº”èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ä»…é¿å…äº†æ˜‚è´µçš„é‡è®­ç»ƒè¿‡ç¨‹ï¼Œä¹Ÿä¸ºè§£å†³æ‰©æ•£æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é²æ£’æ€§é—®é¢˜æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2506.22509v1",
      "published_date": "2025-06-26 02:54:27 UTC",
      "updated_date": "2025-06-26 02:54:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:02.566367+00:00"
    },
    {
      "arxiv_id": "2506.20960v2",
      "title": "OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs",
      "title_zh": "OmniEvalï¼šé¢å‘è§†è§‰ã€å¬è§‰åŠæ–‡æœ¬è¾“å…¥å…¨æ¨¡æ€æ¨¡å‹çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yiman Zhang",
        "Ziheng Luo",
        "Qiangyu Yan",
        "Wei He",
        "Borui Jiang",
        "Xinghao Chen",
        "Kai Han"
      ],
      "abstract": "In this paper, we introduce OmniEval, a benchmark for evaluating omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory, and textual inputs. Compared with existing benchmarks, our OmniEval has several distinctive features: (i) Full-modal collaboration: We design evaluation tasks that highlight the strong coupling between audio and video, requiring models to effectively leverage the collaborative perception of all modalities; (ii) Diversity of videos: OmniEval includes 810 audio-visual synchronized videos, 285 Chinese videos and 525 English videos; (iii) Diversity and granularity of tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended questions and 1205 multiple-choice questions. These questions are divided into 3 major task types and 12 sub-task types to achieve comprehensive evaluation. Among them, we introduce a more granular video localization task named Grounding. Then we conduct experiments on OmniEval with several omni-modality models. We hope that our OmniEval can provide a platform for evaluating the ability to construct and understand coherence from the context of all modalities. Codes and data could be found at https://omnieval-benchmark.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OmniEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åŒ…å«è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬è¾“å…¥çš„å…¨æ¨¡æ€æ¨¡å‹ (Omni-modality models) çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰åŸºå‡†ä¸åŒï¼ŒOmniEval å¼ºè°ƒéŸ³è§†é¢‘ä¹‹é—´çš„å¼ºè€¦åˆæ€§ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰æ¨¡æ€çš„åä½œæ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å« 810 ä¸ªéŸ³è§†é¢‘åŒæ­¥è§†é¢‘ï¼Œæ¶µç›–äº†ä¸­è‹±åŒè¯­èµ„æºï¼Œå¹¶è®¾è®¡äº† 2617 ä¸ªé—®ç­”å¯¹ä»¥å®ç°å¤šç»´åº¦çš„è¯„ä¼°ã€‚ç ”ç©¶å°†å…¶åˆ’åˆ†ä¸º 3 å¤§ç±»å’Œ 12 å°ç±»ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æ›´ä¸ºç²¾ç»†çš„è§†é¢‘å®šä½ä»»åŠ¡ Groundingã€‚é€šè¿‡å¯¹å¤šç§å…¨æ¨¡æ€æ¨¡å‹çš„å®éªŒï¼ŒOmniEval ä¸ºè¯„ä¼°æ¨¡å‹ä»å…¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ„å»ºå’Œç†è§£ä¸€è‡´æ€§çš„èƒ½åŠ›æä¾›äº†é‡è¦å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20960v2",
      "published_date": "2025-06-26 02:54:24 UTC",
      "updated_date": "2025-06-29 15:16:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:02.052989+00:00"
    },
    {
      "arxiv_id": "2506.22508v1",
      "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text",
      "title_zh": "AgentStealthï¼šé¢å‘ç”¨æˆ·ç”Ÿæˆæ–‡æœ¬åŒ¿ååŒ–çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–æ¡†æ¶",
      "authors": [
        "Chenyang Shao",
        "Tianxing Li",
        "Chenhao Pu",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "In today's digital world, casual user-generated content often contains subtle cues that may inadvertently expose sensitive personal attributes. Such risks underscore the growing importance of effective text anonymization to safeguard individual privacy. However, existing methods either rely on rigid replacements that damage utility or cloud-based LLMs that are costly and pose privacy risks. To address these issues, we explore the use of locally deployed smaller-scale language models (SLMs) for anonymization. Yet training effective SLMs remains challenging due to limited high-quality supervision. To address the challenge, we propose AgentStealth, a self-reinforcing LLM anonymization framework.First, we introduce an adversarial anonymization workflow enhanced by In-context Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform supervised adaptation of SLMs using high-quality data collected from the workflow, which includes both anonymization and attack signals. Finally, we apply online reinforcement learning where the model leverages its internal adversarial feedback to iteratively improve anonymization performance. Experiments on two datasets show that our method outperforms baselines in both anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight design supports direct deployment on edge devices, avoiding cloud reliance and communication-based privacy risks. Our code is open-source at https://github.com/tsinghua-fib-lab/AgentStealth.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentStealthï¼Œä¸€ä¸ªè‡ªå¢å¼ºçš„ Large Language Model (LLM) æ–‡æœ¬åŒ¿ååŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·ç”Ÿæˆå†…å®¹ä¸­æ•æ„Ÿå±æ€§æ³„éœ²çš„éšç§é£é™©ã€‚ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•å¯¹äº‘ç«¯æ¨¡å‹çš„ä¾èµ–ä»¥åŠå¯¹æ–‡æœ¬æ•ˆç”¨çš„ç ´åï¼ŒAgentStealth ä¸“æ³¨äºåœ¨æœ¬åœ°éƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹ (SLMs) è¿›è¡Œéšç§ä¿æŠ¤ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç»“åˆ In-context Contrastive Learning å’Œ Adaptive Utility-Aware Control çš„å¯¹æŠ—æ€§åŒ¿ååŒ–å·¥ä½œæµï¼Œå¹¶åˆ©ç”¨è¯¥å·¥ä½œæµäº§ç”Ÿçš„é«˜è´¨é‡ä¿¡å·å¯¹ SLMs è¿›è¡Œç›‘ç£é€‚é…ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨åœ¨çº¿ Reinforcement Learning æŠ€æœ¯ï¼Œé€šè¿‡å†…éƒ¨å¯¹æŠ—åé¦ˆè¿­ä»£ä¼˜åŒ–æ¨¡å‹çš„åŒ¿ååŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentStealth åœ¨åŒ¿ååŒ–æœ‰æ•ˆæ€§å’Œæ•ˆç”¨ä¸Šåˆ†åˆ«æ¯”åŸºçº¿æ¨¡å‹æå‡äº† 12.3% å’Œ 6.8%ã€‚å…¶è½»é‡åŒ–è®¾è®¡æ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡ç›´æ¥éƒ¨ç½²ï¼Œæœ‰æ•ˆè§„é¿äº†äº‘ç«¯é€šä¿¡å¸¦æ¥çš„éšç§é£é™©ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This work has been submitted to NeurIPS 2025. Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.22508v1",
      "published_date": "2025-06-26 02:48:16 UTC",
      "updated_date": "2025-06-26 02:48:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:06.380753+00:00"
    },
    {
      "arxiv_id": "2506.20957v1",
      "title": "Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding",
      "title_zh": "åŸºäºå¤šå°ºåº¦ç­‰å˜å›¾æ‰©æ•£æ¨¡å‹çš„æŠ—ä½“è®¾è®¡ä¸ä¼˜åŒ–ï¼Œå®ç°å¤æ‚æŠ—åŸçš„ç²¾å‡†ç»“åˆ",
      "authors": [
        "Jiameng Chen",
        "Xiantao Cai",
        "Jia Wu",
        "Wenbin Hu"
      ],
      "abstract": "Antibody design remains a critical challenge in therapeutic and diagnostic development, particularly for complex antigens with diverse binding interfaces. Current computational methods face two main limitations: (1) capturing geometric features while preserving symmetries, and (2) generalizing novel antigen interfaces. Despite recent advancements, these methods often fail to accurately capture molecular interactions and maintain structural integrity. To address these challenges, we propose \\textbf{AbMEGD}, an end-to-end framework integrating \\textbf{M}ulti-scale \\textbf{E}quivariant \\textbf{G}raph \\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging advanced geometric deep learning, AbMEGD combines atomic-level geometric features with residue-level embeddings, capturing local atomic details and global sequence-structure interactions. Its E(3)-equivariant diffusion method ensures geometric precision, computational efficiency, and robust generalizability for complex antigens. Furthermore, experiments using the SAbDab database demonstrate a 10.13\\% increase in amino acid recovery, 3.32\\% rise in improvement percentage, and a 0.062~Ã… reduction in root mean square deviation within the critical CDR-H3 region compared to DiffAb, a leading antibody design model. These results highlight AbMEGD's ability to balance structural integrity with improved functionality, establishing a new benchmark for sequence-structure co-design and affinity optimization. The code is available at: https://github.com/Patrick221215/AbMEGD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AbMEGDï¼Œä¸€ä¸ªé›†æˆå¤šå°ºåº¦ç­‰å˜å›¾æ‰©æ•£ï¼ˆMulti-scale Equivariant Graph Diffusionï¼‰çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æŠ—ä½“è®¾è®¡ä¸­å¤æ‚æŠ—åŸç»“åˆç•Œé¢å»ºæ¨¡åŠå‡ ä½•ç‰¹å¾æ•æ‰çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆåŸå­çº§å‡ ä½•ç‰¹å¾ä¸æ®‹åŸºçº§åµŒå…¥ï¼Œåˆ©ç”¨E(3)-ç­‰å˜æ‰©æ•£æ–¹æ³•å®ç°äº†æŠ—ä½“åºåˆ—ä¸ç»“æ„çš„ååŒè®¾è®¡ï¼Œç¡®ä¿äº†å‡ ä½•ç²¾ç¡®æ€§ä¸å¯¹æ–°æŠ—åŸç•Œé¢çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨SAbDabæ•°æ®åº“æµ‹è¯•ä¸­ï¼ŒAbMEGDç›¸æ¯”äºé¢†å…ˆæ¨¡å‹DiffAbï¼Œåœ¨å…³é”®çš„CDR-H3åŒºåŸŸä½¿æ°¨åŸºé…¸å›æ”¶ç‡æå‡äº†10.13%ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å‡æ–¹æ ¹åå·®ï¼ˆRMSDï¼‰ã€‚è¯¥ç ”ç©¶è¯æ˜äº†AbMEGDåœ¨ç»´æŒç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ä¼˜åŒ–åŠŸèƒ½æ€§çš„ä¼˜è¶Šè¡¨ç°ï¼Œä¸ºé«˜äº²å’ŒåŠ›æŠ—ä½“è®¾è®¡å’Œåºåˆ—ç»“æ„ååŒä¼˜åŒ–ç¡®ç«‹äº†æ–°çš„æŠ€æœ¯åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures, accepted at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.20957v1",
      "published_date": "2025-06-26 02:45:38 UTC",
      "updated_date": "2025-06-26 02:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:14.898742+00:00"
    },
    {
      "arxiv_id": "2506.20949v1",
      "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation",
      "title_zh": "è¶…è¶Šååº”å¼å®‰å…¨ï¼šåŸºäºé•¿æ—¶åŸŸæ¨¡æ‹Ÿçš„é£é™©æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹å¯¹é½",
      "authors": [
        "Chenkai Sun",
        "Denghui Zhang",
        "ChengXiang Zhai",
        "Heng Ji"
      ],
      "abstract": "Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models' ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å…¬å…±æ”¿ç­–å’ŒåŒ»ç–—ç­‰é«˜é£é™©å†³ç­–ä¸­çš„å½±å“åŠ›ï¼Œæå‡ºäº†ä¸€ä¸ªé€šè¿‡ Long-Horizon Simulation è¿›è¡Œ Risk-Aware LLM Alignment çš„æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹æ¨¡å‹å»ºè®®åœ¨ç¤¾ä¼šç³»ç»Ÿä¸­çš„é•¿æœŸå®è§‚å½±å“ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„é•¿æœŸå®‰å…¨æ„è¯†ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å« 100 ä¸ªé—´æ¥ä¼¤å®³åœºæ™¯çš„æ•°æ®é›†ï¼Œä¸“é—¨æµ‹è¯•æ¨¡å‹é¢„è§çœ‹ä¼¼æ— å®³çš„æç¤ºæ‰€å¼•å‘çš„éæ˜¾æ€§è´Ÿé¢åæœçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°å»ºæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡ 20% çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ AdvBenchã€SafeRLHF å’Œ WildGuardMix ç­‰ç°æœ‰å®‰å…¨åŸºå‡†ä¸Šå¯¹æ¯”å¼ºåŸºçº¿æ¨¡å‹å–å¾—äº†é€¾ 70% çš„èƒœç‡ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚ã€é•¿ç¨‹ç¤¾ä¼šå½±å“æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘æ›´å®‰å…¨ã€æ›´å…·å‰ç»æ€§çš„æ™ºèƒ½ä½“æä¾›äº†é‡è¦æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20949v1",
      "published_date": "2025-06-26 02:28:58 UTC",
      "updated_date": "2025-06-26 02:28:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:12.279911+00:00"
    },
    {
      "arxiv_id": "2506.20946v1",
      "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models",
      "title_zh": "åŸºäºå‡ ä½•æ„ŸçŸ¥æ‰©æ•£ä¸æ—¶åºè§†é¢‘æ¨¡å‹çš„ä¸€è‡´æ€§é›¶æ ·æœ¬ 3D çº¹ç†åˆæˆ",
      "authors": [
        "Donggoo Kang",
        "Jangyeong Kim",
        "Dasol Jeong",
        "Junyoung Choi",
        "Jeonga Wi",
        "Hyunmin Lee",
        "Joonho Gwon",
        "Joonki Paik"
      ],
      "abstract": "Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† VideoTexï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°æ— ç¼ 3D çº¹ç†åˆæˆçš„åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥è§£å†³ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„ç©ºé—´å’Œæ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº† geometry-aware æ¡ä»¶ï¼Œèƒ½å¤Ÿç²¾ç¡®åˆ©ç”¨ 3D mesh ç»“æ„ï¼Œå…‹æœäº†ä¼ ç»Ÿå›ºå®šè§†è§’ç”Ÿæˆæ¨¡å¼ä¸‹å‡ ä½•ç†è§£ä¸è¶³çš„å±€é™ã€‚æ­¤å¤–ï¼ŒVideoTex æå‡ºäº†ä¸€ç§ structure-wise UV diffusion ç­–ç•¥ï¼Œé€šè¿‡ä¿ç•™è¯­ä¹‰ä¿¡æ¯æœ‰æ•ˆå¢å¼ºäº†é®æŒ¡åŒºåŸŸçš„çº¹ç†ç”Ÿæˆï¼Œä»è€Œå®ç°äº†æ›´å¹³æ»‘ã€è¿è´¯çš„è§†è§‰æ•ˆæœã€‚è¯¥æ¡†æ¶ä¸ä»…åœ¨ UV è¾¹ç•Œå®ç°äº†æ›´è‡ªç„¶çš„è¿‡æ¸¡ï¼Œè¿˜ç¡®ä¿äº†è·¨è§†é¢‘å¸§çš„é«˜è´¨é‡ä¸”å…·æœ‰æ—¶é—´ç¨³å®šæ€§çš„çº¹ç†è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVideoTex åœ¨çº¹ç†ä¿çœŸåº¦ (texture fidelity)ã€æ¥ç¼èåˆ (seam blending) å’Œç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¯¹è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æœ‰æé«˜è¦æ±‚çš„åŠ¨æ€å®æ—¶åº”ç”¨å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20946v1",
      "published_date": "2025-06-26 02:25:16 UTC",
      "updated_date": "2025-06-26 02:25:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:15.931213+00:00"
    },
    {
      "arxiv_id": "2507.02927v1",
      "title": "A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations",
      "title_zh": "é¢å‘å¤šè¯­è¨€å¯¹è¯è¯´è¯äººæ—¥å¿—ä¸è¯­éŸ³è¯†åˆ«çš„ç»Ÿä¸€è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Phurich Saengthong",
        "Boonnithi Jiaramaneepinit",
        "Sheng Li",
        "Manabu Okumura",
        "Takahiro Shinozaki"
      ],
      "abstract": "Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm in recent years, extending the capabilities of traditional LLMs to speech tasks such as automatic speech recognition (ASR) and spoken dialogue modeling. However, their effectiveness in real-world multilingual conversations remains limited by the scarcity of data that captures natural conversational phenomena. To address this, the MLC-SLM Challenge provides a multilingual conversational dataset and evaluates models on two tasks: ASR with oracle segmentation (Task I) and joint diarization and recognition without oracle information (Task II). In this paper, we focus on Task II and propose a unified speech LLM that jointly performs diarization and ASR in an end-to-end manner. By reformulating the training data format and modifying the inference procedure, our model addresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\\% relative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall, despite using a smaller LLM backbone. We also report results from Task I using a fine-tuned speech LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè¯­è¨€å¯¹è¯ä¸­çœŸå®åœºæ™¯æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech LLM)ï¼Œæ—¨åœ¨ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ååŒå®ç°è¯´è¯äººæ—¥å¿—(Diarization)ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ä»»åŠ¡ã€‚ç ”ç©¶é‡ç‚¹èšç„¦äºMLC-SLMæŒ‘æˆ˜èµ›ä¸­çš„è”åˆä»»åŠ¡ï¼Œå³åœ¨ç¼ºä¹é¢„å…ˆåˆ†å‰²ä¿¡æ¯çš„æƒ…å†µä¸‹åŒæ—¶è¿›è¡Œè¯´è¯äººåŒºåˆ†ä¸è½¬å†™ã€‚é€šè¿‡é‡æ–°æ„å»ºè®­ç»ƒæ•°æ®æ ¼å¼å¹¶æ”¹è¿›æ¨ç†è¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆè§£å†³äº†é¢„åˆ†å‰²éŸ³é¢‘ä¸­å›ºæœ‰çš„æ­§ä¹‰æ€§æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡é‡‡ç”¨äº†è¾ƒå°çš„LLMä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œè¯¥æ¨¡å‹åœ¨tcpWER/tcpCERæŒ‡æ ‡ä¸Šä»è¾ƒåŸºçº¿æ¨¡å‹å®ç°äº†54.87%çš„ç›¸å¯¹æå‡ï¼Œå¹¶åœ¨æŒ‘æˆ˜èµ›æ€»æ’åä¸­ä½åˆ—ç¬¬å…«ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å±•ç¤ºäº†è¯¥Speech LLMåœ¨ç»è¿‡å¾®è°ƒåå¤„ç†ç‰¹å®šåˆ†å‰²è¯†åˆ«ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02927v1",
      "published_date": "2025-06-26 01:54:02 UTC",
      "updated_date": "2025-06-26 01:54:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:17.483186+00:00"
    },
    {
      "arxiv_id": "2506.20927v1",
      "title": "Interpretable Representation Learning for Additive Rule Ensembles",
      "title_zh": "åŠ æ€§è§„åˆ™é›†æˆçš„å¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Shahrzad Behzadimanesh",
        "Pierre Le Bodic",
        "Geoffrey I. Webb",
        "Mario Boley"
      ],
      "abstract": "Small additive ensembles of symbolic rules offer interpretable prediction models. Traditionally, these ensembles use rule conditions based on conjunctions of simple threshold propositions $x \\geq t$ on a single input variable $x$ and threshold $t$, resulting geometrically in axis-parallel polytopes as decision regions. While this form ensures a high degree of interpretability for individual rules and can be learned efficiently using the gradient boosting approach, it relies on having access to a curated set of expressive and ideally independent input features so that a small ensemble of axis-parallel regions can describe the target variable well. Absent such features, reaching sufficient accuracy requires increasing the number and complexity of individual rules, which diminishes the interpretability of the model. Here, we extend classical rule ensembles by introducing logical propositions with learnable sparse linear transformations of input variables, i.e., propositions of the form $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$, where $\\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as general polytopes with oblique faces. We propose a learning method using sequential greedy optimization based on an iteratively reweighted formulation of logistic regression. Experimental results demonstrate that the proposed method efficiently constructs rule ensembles with the same test risk as state-of-the-art methods while significantly reducing model complexity across ten benchmark datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿçš„ç¬¦å·è§„åˆ™åŠ æ³•é›†æˆï¼ˆAdditive Rule Ensemblesï¼‰åœ¨ç¼ºä¹é«˜è´¨é‡ç‰¹å¾æ—¶æ¨¡å‹å¤æ‚åº¦é«˜ã€å¯è§£é‡Šæ€§å—æŸçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥åŸºäºè¾“å…¥å˜é‡ç¨€ç–çº¿æ€§å˜æ¢çš„å¯å­¦ä¹ é€»è¾‘å‘½é¢˜ï¼Œæ¨¡å‹èƒ½å¤Ÿæ„å»ºå…·æœ‰æ–œé¢ï¼ˆoblique facesï¼‰çš„é€šç”¨å¤šèƒä½“ï¼ˆgeneral polytopesï¼‰å†³ç­–åŒºåŸŸï¼Œçªç ´äº†ä¼ ç»Ÿè½´å¹³è¡Œï¼ˆaxis-parallelï¼‰åŒºåŸŸçš„é™åˆ¶ã€‚å­¦ä¹ è¿‡ç¨‹é‡‡ç”¨äº†åŸºäºè¿­ä»£é‡æƒé€»è¾‘å›å½’ï¼ˆiteratively reweighted formulation of logistic regressionï¼‰çš„åºåˆ—è´ªå©ªä¼˜åŒ–æŠ€æœ¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡èƒ½å®ç°ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ï¼ˆstate-of-the-artï¼‰ç›¸å½“çš„æµ‹è¯•é£é™©ï¼ˆtest riskï¼‰ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å¤æ‚åº¦ã€‚è¿™ä¸€è¿›å±•æœ‰æ•ˆåœ°æå‡äº†åœ¨å¤æ‚æ•°æ®ç¯å¢ƒä¸‹æ„å»ºç®€æ´ä¸”å…·æœ‰å¼ºè§£é‡Šæ€§é¢„æµ‹æ¨¡å‹çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.20927v1",
      "published_date": "2025-06-26 01:24:08 UTC",
      "updated_date": "2025-06-26 01:24:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:32.211288+00:00"
    },
    {
      "arxiv_id": "2506.20921v2",
      "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“æ–¹æ³•çš„å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼åŒ–å·¥è¿‡ç¨‹ä¼˜åŒ–",
      "authors": [
        "Tong Zeng",
        "Srivathsan Badrinarayanan",
        "Janghoon Ock",
        "Cheng-Kai Lai",
        "Amir Barati Farimani"
      ],
      "abstract": "Chemical process optimization maximizes production efficiency and economic performance, but optimization algorithms, including gradient-based solvers, numerical methods, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable. We present a multi-agent LLM framework that autonomously infers operating constraints from minimal process descriptions, then collaboratively guides optimization. Our AutoGen-based framework employs OpenAI's o3 model with specialized agents for constraint generation, parameter validation, simulation, and optimization guidance. Through autonomous constraint generation and iterative multi-agent optimization, the framework eliminates the need for predefined operational bounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost ratio metrics, the framework achieved competitive performance with conventional methods while reducing wall-time 31-fold relative to grid search, converging in under 20 minutes. The reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. Unlike conventional methods requiring predefined constraints, our approach uniquely combines autonomous constraint generation with interpretable parameter exploration. Model comparison reveals reasoning-capable architectures (o3, o1) are essential for successful optimization, while standard models fail to converge. This approach is particularly valuable for emerging processes and retrofit applications where operational constraints are poorly characterized or unavailable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ–å­¦å·¥è‰ºä¼˜åŒ–ä¸­æ“ä½œçº¦æŸï¼ˆoperating constraintsï¼‰å®šä¹‰ä¸æ¸…å¯¼è‡´ä¼ ç»Ÿç®—æ³•éš¾ä»¥åº”ç”¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº AutoGen çš„å¤šæ™ºèƒ½ä½“ LLM æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ OpenAI çš„ o3 æ¨¡å‹ï¼Œé€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“ååŒå®Œæˆçº¦æŸç”Ÿæˆã€å‚æ•°éªŒè¯ã€æ¨¡æ‹Ÿå’Œä¼˜åŒ–æŒ‡å¯¼ï¼Œå®ç°äº†ä»æç®€å·¥è‰ºæè¿°ä¸­è‡ªä¸»æ¨æ–­çº¦æŸæ¡ä»¶ã€‚åœ¨åŠ æ°¢è„±çƒ·åŸºï¼ˆhydrodealkylationï¼‰å·¥è‰ºçš„å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶åœ¨æˆæœ¬å’Œäº§é‡æŒ‡æ ‡ä¸Šå±•ç°å‡ºç«äº‰ä¼˜åŠ¿ï¼Œä¸”æ¯”ç½‘æ ¼æœç´¢ï¼ˆgrid searchï¼‰æé€Ÿ 31 å€ï¼Œåœ¨ 20 åˆ†é’Ÿå†…å³å¯æ”¶æ•›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¶æ„ï¼ˆå¦‚ o3 å’Œ o1ï¼‰å¯¹äºè¯†åˆ«å…¬ç”¨å·¥ç¨‹æƒè¡¡å’Œåº”ç”¨é¢†åŸŸå¯å‘å¼çŸ¥è¯†è‡³å…³é‡è¦ï¼Œè€Œæ ‡å‡†æ¨¡å‹åˆ™éš¾ä»¥æ”¶æ•›ã€‚è¯¥æ–¹æ³•ç‹¬ç‰¹åœ°ç»“åˆäº†è‡ªä¸»çº¦æŸç”Ÿæˆä¸å¯è§£é‡Šçš„å‚æ•°æ¢ç´¢ï¼Œä¸ºæ“ä½œçº¦æŸç¼ºå¤±çš„æ–°å…´å·¥è‰ºæˆ–æ”¹é€ åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è‡ªåŠ¨åŒ–ä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages (main manuscript without references), 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.20921v2",
      "published_date": "2025-06-26 01:03:44 UTC",
      "updated_date": "2025-10-16 15:31:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:39.214891+00:00"
    },
    {
      "arxiv_id": "2506.20917v1",
      "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective",
      "title_zh": "é¢å‘ä¸‹æ¸¸ä»»åŠ¡çš„è¯­è¨€æ¨¡å‹ä¼˜åŒ–ï¼šåè®­ç»ƒè§†è§’",
      "authors": [
        "Zhengyan Shi"
      ],
      "abstract": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet adapting them efficiently and robustly to specific tasks remains challenging. As their scale and complexity grow, fine-tuning LMs on labelled data often underutilizes available unlabelled data, leads to overfitting on small task-specific sets, and imposes significant computational costs. These limitations hamper their application to the open-ended landscape of real-world language tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream applications. First, we explore strategies for extracting task-relevant knowledge from unlabelled data, introducing a novel continued pre-training technique that outperforms state-of-the-art semi-supervised approaches. Next, we present a parameter-efficient fine-tuning method that substantially reduces memory and compute costs while maintaining competitive performance. We also introduce improved supervised fine-tuning methods that enable LMs to better follow instructions, especially when labelled data is scarce, enhancing their performance across a range of NLP tasks, including open-ended generation. Finally, we develop new evaluation methods and benchmarks, such as multi-hop spatial reasoning tasks, to assess LM capabilities and adaptation more comprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results demonstrate that these approaches substantially improve LM robustness, efficiency, and generalization, making them more adaptable to a broad range of applications. These advances mark a significant step towards more robust and efficient LMs, bringing us closer to the goal of artificial general intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹ (Language Models) åœ¨ä¸‹æ¸¸ä»»åŠ¡é€‚é…ä¸­é¢ä¸´çš„æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨ç‡ä½ã€è¿‡æ‹ŸåˆåŠè®¡ç®—å¼€é”€å¤§ç­‰æŒ‘æˆ˜ï¼Œä»è®­ç»ƒå (Post-Training) çš„è§†è§’æå‡ºäº†ä¸€ç³»åˆ—ä¼˜åŒ–æ–¹æ³•ã€‚é¦–å…ˆï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æŒç»­é¢„è®­ç»ƒ (Continued Pre-training) æŠ€æœ¯ï¼Œé€šè¿‡ä»æ— æ ‡ç­¾æ•°æ®ä¸­æå–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„åŠç›‘ç£å­¦ä¹ æ–¹æ¡ˆã€‚æ¥ç€ï¼Œè®ºæ–‡ä»‹ç»äº†ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-efficient Fine-tuning) æ–¹æ³•ï¼Œåœ¨æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹æ€§èƒ½çš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹è¿›çš„ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning) ç­–ç•¥ï¼Œä½¿æ¨¡å‹åœ¨æœ‰æ ‡ç­¾æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¾ç„¶èƒ½å±•ç°å‡ºå¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œå¼€æ”¾å¼ç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶è€…è¿˜å¼€å‘äº†å¤šæ­¥ç©ºé—´æ¨ç†ç­‰æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œå®éªŒè¯æ˜è¿™äº›ç»¼åˆæ‰‹æ®µæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ã€æ•ˆç‡ä¸æ³›åŒ–æ€§ï¼Œæ¨åŠ¨äº†æ¨¡å‹å‘é€šç”¨äººå·¥æ™ºèƒ½ç›®æ ‡çš„è¿ˆè¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD Thesis",
      "pdf_url": "https://arxiv.org/pdf/2506.20917v1",
      "published_date": "2025-06-26 00:49:35 UTC",
      "updated_date": "2025-06-26 00:49:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:36.240979+00:00"
    },
    {
      "arxiv_id": "2506.20915v2",
      "title": "ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models",
      "title_zh": "ZKPROVï¼šä¸€ç§é¢å‘å¤§è¯­è¨€æ¨¡å‹æ•°æ®é›†æº¯æºçš„é›¶çŸ¥è¯†æ–¹æ³•",
      "authors": [
        "Mina Namazi",
        "Alexander Nemecek",
        "Erman Ayday"
      ],
      "abstract": "As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ZKPROVï¼Œä¸€ç§åˆ›æ–°çš„å¯†ç å­¦æ¡†æ¶ (cryptographic framework)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—ç­‰æ•æ„Ÿé¢†åŸŸä¸­ï¼Œå¦‚ä½•åœ¨ä¸æ³„éœ²è®­ç»ƒæ•°æ®é›†çš„æƒ…å†µä¸‹éªŒè¯å…¶è®¡ç®—æº¯æº (computational provenance) çš„éš¾é¢˜ã€‚ZKPROV å…è®¸ç”¨æˆ·éªŒè¯ LLM çš„å›å¤æ˜¯å¦åŸºäºç»æƒå¨æœºæ„è®¤è¯çš„æ•°æ®é›†è®­ç»ƒï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®é›†å†…å®¹ä¸æŸ¥è¯¢ç›¸å…³ä¸”ä¸æ³„éœ²æ•æ„Ÿä¿¡æ¯æˆ–æ¨¡å‹å‚æ•°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é›¶çŸ¥è¯†è¯æ˜ (Zero-Knowledge Proofs) æŠ€æœ¯å®ç°äº†è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹å‚æ•°ä¸ç”Ÿæˆå“åº”çš„ç»‘å®šï¼Œæœ‰æ•ˆå¹³è¡¡äº†éšç§æ€§ä¸éªŒè¯æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç”Ÿæˆå’ŒéªŒè¯è¯æ˜æ—¶å…·æœ‰æ¬¡çº¿æ€§æ‰©å±• (sublinear scaling) ç‰¹æ€§ï¼Œé’ˆå¯¹å‚æ•°é‡é«˜è¾¾ 8B çš„æ¨¡å‹ï¼Œå…¶ç«¯åˆ°ç«¯å¼€é”€ä½äº 3.3 ç§’ï¼Œå…·å¤‡æé«˜çš„å®ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†æ­£å¼çš„å®‰å…¨ä¿è¯ (security guarantees)ï¼Œåœ¨ç¡®ä¿æ•°æ®é›†æœºå¯†æ€§çš„å‰æä¸‹ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„æ•°æ®é›†æº¯æºæœºåˆ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "16 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.20915v2",
      "published_date": "2025-06-26 00:49:02 UTC",
      "updated_date": "2025-12-18 23:45:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:56:39.373065+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 124,
  "processed_papers_count": 124,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T00:58:03.767192+00:00"
}